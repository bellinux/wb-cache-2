7867|10000|Public
5|$|Under the synthesis, debates {{have become}} less ideological and more methodological. Business cycle modelers {{can be broken}} into two camps: those in favor of {{calibration}} and those in favor of estimation. When models are calibrated, the modeler selects parameter values based on other studies or casual empirical observation. Instead of using statistical diagnostics to evaluate models, the model's operating characteristics determine {{the quality of the}} model. Kydland and Prescott (1982) offered no formal evaluation of their model, but noted how variables like hours worked did not match real data well while the variances of other elements of the model did. When <b>estimation</b> <b>methods</b> are used, models are evaluated based on standard statistical goodness of fit criteria. Calibration is generally associated with real business cycle modelers of the new classical school, but methodological differences cut across ideology. While Lucas, Prescott, and Kydland are calibration advocates, another prominent new classical, Sargent, favors estimation.|$|E
25|$|The lowest {{incidence}} of atherosclerotic events over time occurs within the <20% group, with increased {{rates for the}} higher groups. Multiple other measures, including particle sizes, small LDL particle concentrations, large total and HDL particle concentrations, along with estimations of insulin resistance pattern and standard cholesterol lipid measurements (for comparison of the plasma data with the <b>estimation</b> <b>methods</b> discussed above) are also routinely provided.|$|E
25|$|The UNFCCC has {{accepted}} the Revised 1996 IPCC Guidelines for National Greenhouse Gas Inventories, developed {{and published by}} the Intergovernmental Panel on Climate Change (IPCC) as the emission <b>estimation</b> <b>methods</b> that must {{be used by the}} parties to the convention to ensure transparency, completeness, consistency, comparability and accuracy of the national greenhouse gas inventories. These IPCC Guidelines are the primary source for default emission factors. Recently IPCC has published the 2006 IPCC Guidelines for National Greenhouse Gas Inventories. These and many more greenhouse gas emission factors can be found on IPCC's Emission Factor Database. Commercially applicable organisational greenhouse gas emission factors {{can be found on the}} search engine, EmissionFactors.com.|$|E
30|$|In this section, we {{describe}} the widely used 2 -D channel <b>estimation</b> <b>method</b> and the general idea of DD channel <b>estimation</b> <b>method.</b>|$|R
40|$|This paper {{provides}} a simple <b>estimation</b> <b>method</b> for an error component regression model with general MA(q) remainder disturbances. The <b>estimation</b> <b>method</b> utilizes the transformation derived by Baltagi and Li [3] for an error component model with autoregressive remainder disturbances, {{and a standard}} orthogonalizing algorithm for the general MA(q) model. This <b>estimation</b> <b>method</b> is computationally simple utilizing only least-squares regressions. This is important for panel data regressions where brute force GLS is in many cases not feasible. This <b>estimation</b> <b>method</b> performs well relative to true GLS in Monte-Carlo experiments. ...|$|R
3000|$|This paper {{considered}} {{the detection of}} deterministic SCI in a baseband OFDM architecture. The detection performance of a time-domain correlation method is investigated and compared against the conventional ML <b>estimation</b> <b>method.</b> A key benefit of the time-domain <b>estimation</b> <b>method</b> is that it requires no channel estimation at the receiver. The detection performance of the time-domain <b>estimation</b> <b>method</b> {{is found to be}} largely dependent on the value of N [...]...|$|R
25|$|Members {{of these}} various {{workshops}} and study tours {{have been responsible}} for the identification of effective strategies that have increasingly {{been at the forefront of}} HIV control policy in China. They have also contributed to the development of strategic documents, including the Medium- and Long-Term Strategic Plan for HIV/AIDS (1998–2010), the Action Plan on HIV/AIDS Prevention and Containment (2001–2005), and the AIDS Regulations. Other key documents warned of the potential epidemic in China and might have influenced the attitudes of policymakers. China's Titanic Peril, published by the UN in 2002, made the unsubstantiated prediction that China could have 10 million HIV-infected individuals by 2010, a figure that has been repeatedly misused in discussions of China's HIV future. A Joint Assessment of HIV/AIDS Prevention, Treatment and Care in China (2004), developed jointly by UNAIDS and the State Council of China, estimated that China had 840,000 people living with HIV/AIDS. This figure has been revised down to 650,000 in 2005 in light of more representative data collection and more appropriate <b>estimation</b> <b>methods.</b> Although this figure represented a prevalence of about 0.05%, it was substantially higher than previous government estimates (300,000 in 1998) and provided the impetus for immediate scale-up of prevention and control strategies.|$|E
500|$|In 1962, {{scientists}} at Los Alamos created a mockup of Little Boy known as [...] "Project Ichiban" [...] {{in order to}} answer some of the unanswered questions, but it failed to clear up all the issues. In 1982, Los Alamos created a replica Little Boy from the original drawings and specifications. This was then tested with enriched uranium but in a safe configuration that would not cause a nuclear explosion. A hydraulic lift was used to move the projectile, and experiments were run to assess neutron emission. Based on this and the data from The Great Artiste, the yield was estimated at 16.6 ± 0.3 kilotons. After considering many <b>estimation</b> <b>methods,</b> a 1985 report concluded that the yield was 15 kilotons ± 20%.|$|E
500|$|Carnotaurus {{was a large}} but lightly built predator. The {{only known}} {{individual}} was about [...] in length, making Carnotaurus {{one of the largest}} abelisaurids. While Ekrixinatosaurus and possibly Abelisaurus, highly incomplete, would have been similar or larger in size, a 2016 study found that only Pycnonemosaurus, at , was longer than Carnotaurus, which was estimated at [...] Its mass is estimated to have been [...] and [...] in separate studies that used different <b>estimation</b> <b>methods.</b> Carnotaurus was a highly specialized theropod, as seen especially in characteristics of the skull, the vertebrae and the forelimbs. The pelvis and hindlimbs, on the other hand, remained relatively conservative, resembling those of the more basal Ceratosaurus. Both the pelvis and hindlimb bones were long and slender. The left thigh bone of the individual measures 103cm in length, but shows an average diameter of only 11cm.|$|E
3000|$|... (2)Reference [6] {{proposed}} a secure <b>estimation</b> <b>method</b> for steganographic capacity {{based on the}} DCT domain. It only proves the influence of image complexity on payload by doing some experiments but has not worked out the specific capacity <b>estimation</b> <b>method.</b>|$|R
30|$|The {{most widely}} used <b>method</b> for the <b>estimation</b> of {{parameters}} of distribution is the maximum likelihood <b>estimation</b> <b>method</b> (MLE) and the moment method. We employ the maximum likelihood <b>estimation</b> <b>method</b> MLE to estimate the unknown parameter of BBX distribution.|$|R
40|$|Abstract — In this paper, {{we present}} a {{distributed}} <b>estimation</b> <b>method</b> in wireless sensor networks (WSNs) based on decisions transmitted over Rayleigh fading channels. The fusion centre can uses either coherent receiver or non-coherent receiver to acquire decisions transmitted over Rayleigh fading channels. The <b>estimation</b> <b>method</b> using coherent receiver and the <b>estimation</b> <b>method</b> using non-coherent receiver are presented and the Cramer-Rao lower bounds (CRLBs) are derived. Simulation results showed that in ideal situations, the RMS errors given by the distributed <b>estimation</b> <b>method</b> were close to the CRLB. Moreover, simulation results highlighted {{the importance of the}} number of sensors, channel SNR, and accurate channel SNR information known to the fusion centre on estimation performance...|$|R
500|$|Several {{unconfirmed}} {{reports of}} great white sharks caught {{in modern times}} with have been estimated {{to be more than}} [...] long, but these claims have received some criticism. However, J.E. Randall believed that great white shark may have exceeded [...] in length. A great white shark was captured near Kangaroo Island in Australia on 1 April 1987. This shark was estimated to be more than [...] long by Peter Resiley, and has been designated as KANGA. Another great white shark was caught in Malta by Alfredo Cutajar on 16 April 1987. This shark was also estimated to be around [...] long by John Abela and has been designated as MALTA. However, Cappo drew criticism because he used shark size <b>estimation</b> <b>methods</b> proposed by J.E. Randall to suggest that the KANGA specimen was [...] long. In a similar fashion, I.K. Fergusson also used shark size <b>estimation</b> <b>methods</b> proposed by J.E. Randall to suggest that the MALTA specimen was [...] long. However, photographic evidence suggested that these specimens were larger than the size estimations yielded through Randall's methods. Thus, a team of scientists—H.F. Mollet, G.M. Cailliet, A.P. Klimley, D.A. Ebert, A.D. Testi, and L.J.V. Compagno—reviewed the cases of the KANGA and MALTA specimens in 1996 to resolve the dispute by conducting a comprehensive morphometric analysis of the remains of these sharks and re-examination of photographic evidence in an attempt to validate the original size estimations and their findings were consistent with them. The findings indicated that estimations by P.Resiley and J.Abela are reasonable and could not be ruled out. A particularly large female great white nicknamed [...] "Deep Blue", estimated measuring at [...] was filmed off Guadalupe during shooting for the 2014 episode of Shark Week [...] "Jaws Strikes Back". Deep Blue would also later gain significant attention when she was filmed interacting with researcher Mauricio Hoyas Pallida in a viral video that Mauricio posted on Facebook on 11 June 2015. A particularly infamous great white shark, supposedly of record proportions, once patrolled the area that comprises False Bay, South Africa, was said to be well over [...] during the early 1980s. This shark, known locally as the [...] "Submarine", had a legendary reputation that was supposedly well founded. Though rumors have stated this shark was exaggerated in size or non-existent altogether, witness accounts by the then young Craig Anthony Ferreira, a notable shark expert in South Africa, and his father indicate an unusually large animal of considerable size and power (though it remains uncertain just how massive the shark was as it escaped capture each time it was hooked). Ferreira describes the four encounters with the giant shark he participated in with great detail in his book [...] "Great White Sharks On Their Best Behavior".|$|E
2500|$|National Air Pollution Emission Inventories are {{required}} annually under {{the provisions of}} the UNECE Convention on Long-Range Transboundary Air Pollution (LRTAP). Emission <b>estimation</b> <b>methods</b> and the associated emission factors for air pollutants have been developed by the [...] Task Force on Emission Inventories and Projections (...) and are published in the [...]|$|E
2500|$|Maximum-likelihood estimators have no optimum {{properties}} for finite samples, {{in the sense}} that (when evaluated on finite samples) other estimators may have greater concentration around the true parameter-value. However, like other <b>estimation</b> <b>methods,</b> maximum likelihood estimation possesses a number of attractive limiting properties: As the sample size increases to infinity, sequences of maximum likelihood estimators have these properties: ...|$|E
40|$|This paper {{describes}} a multiple time interval (“multi-interval”) parameter <b>estimation</b> <b>method.</b> The multi-interval parameter <b>estimation</b> <b>method</b> estimates a parameter {{from a new}} multi-interval prediction error polynomial that can simultaneously consider multiple time intervals. The root of the multi-interval prediction error polynomial includes the effect on each time interval, and the important mode can be estimated by solving one polynomial for multiple time intervals or signals. The algorithm of the multi-interval parameter <b>estimation</b> <b>method</b> proposed in this paper {{is applied to the}} test function and the data measured from a PMU (phasor measurement unit) installed in the KEPCO (Korea Electric Power Corporation) system. The results confirm that the proposed multi-interval parameter <b>estimation</b> <b>method</b> accurately and reliably estimates important parameters...|$|R
40|$|The {{paper is}} {{dedicated}} to the <b>estimation</b> <b>method</b> in accounting, especially to its basis, representatives and practical use. The paper is written from the theoretical definition of the generally accepted accounting principles, characterizing the accrual basis, the going concern and the prudence principle as a background of the <b>estimation</b> <b>method.</b> Furthermore, this work characterizes accruals and allowances as manifestations of the <b>estimation</b> <b>method.</b> Both expressions are first defined in general, than in the Czech legislation and there is outlined the approach in IAS / IFRS. Subsequently, the thesis analyzes the issue of accruals and allowances in the existing enterprise Pražské vodovody a kanalizace, a. s and focuses {{on the use of the}} <b>estimation</b> <b>method</b> in practice...|$|R
30|$|The maximum {{likelihood}} <b>estimation</b> <b>method</b> {{was used in}} this study for the parameter estimation of the optimal distribution. The basic principle of this method is as follows: assuming the known population distribution and an unknown parameter θ, one value θ̂ is chosen from all possible values, which can result in the maximal probability of the observed results. θ̂ is then defined as the {{maximum likelihood}} estimation value of θ, and the parameter <b>estimation</b> <b>method</b> was named as maximum likelihood <b>estimation</b> <b>method</b> [17].|$|R
2500|$|The {{method of}} maximum {{likelihood}} corresponds to many well-known <b>estimation</b> <b>methods</b> in statistics. For example, {{one may be}} interested in the heights of adult female penguins, but is unable to measure the height of every single penguin in a population due to cost or time constraints. [...] Assuming that the heights are normally distributed with some unknown mean and variance, the mean and variance can be estimated with MLE while only knowing the heights of some sample of the overall population. [...] MLE would accomplish this by taking the mean and variance as parameters and finding particular parametric values that make the observed results the most probable given the model.|$|E
2500|$|For {{items such}} as {{multiple}} choice items, the parameter [...] is used in attempt {{to account for the}} effects of guessing on the probability of a correct response. It indicates the probability that very low ability individuals will get this item correct by chance, mathematically represented as a lower asymptote. A four-option multiple choice item might have an IRF like the example item; there is a 1/4 chance of an extremely low ability candidate guessing the correct answer, so the [...] would be approximately 0.25. This approach assumes that all options are equally plausible, because if one option made no sense, even the lowest ability person would be able to discard it, so IRT parameter <b>estimation</b> <b>methods</b> take this into account and estimate a [...] based on the observed data.|$|E
2500|$|In most countries, {{students}} use calculators for schoolwork. There was some initial {{resistance to the}} idea {{out of fear that}} basic or elementary arithmetic skills would suffer. There remains disagreement {{about the importance of the}} ability to perform calculations in the head, with some curricula restricting calculator use until a certain level of proficiency has been obtained, while others concentrate more on teaching <b>estimation</b> <b>methods</b> and problem-solving. Research suggests that inadequate guidance in the use of calculating tools can restrict the kind of mathematical thinking that students engage in. Others have argued that calculator use can even cause core mathematical skills to atrophy, or that such use can prevent understanding of advanced algebraic concepts. In December 2011 the UK's Minister of State for Schools, Nick Gibb, voiced concern that children can become [...] "too dependent" [...] on the use of calculators. As a result, the use of calculators is to be included as part of a review of the Curriculum. In United States, many math educators and boards of education enthusiastically endorsed the National Council of Teachers of Mathematics (NCTM) standards and actively promoted the use of classroom calculators from kindergarten through high school.|$|E
40|$|The Best linear {{unbiased}} estimators of Buys-Ballot estimates when trend-cycle {{component is}} linear {{are discussed in}} this paper. The estimates are those proposed by Iwueze and Nwogu (2004). Discussed are the Chain Based <b>Estimation</b> <b>method</b> and the Fixed Based <b>Estimation</b> <b>method.</b> The variates for the Chain Based <b>Estimation</b> <b>method</b> {{were found to have}} constant mean and variance but are correlated with only one significant autocorrelation coefficient at lag one. The variates for the Fixed Based <b>Estimation</b> <b>method</b> were found to have constant mean, non-constant variance but with constant autocorrelation coefficient at all lags. Best Linear unbiased estimators of the slope and intercept for the Chain Based Estimation variates only were derived since they exhibit stationarity. Numerical examples were used to illustrate the methods...|$|R
40|$|We {{introduce}} a nonparametric quantile <b>estimation</b> <b>method</b> by applying a level crossing empirical function {{which will be}} defined in this paper, and also {{introduce a}} computational method for the new estimator. A comparison of the new quantile <b>estimation</b> <b>method</b> with the usual kernel quantile <b>estimation</b> <b>method</b> based on the classical empirical distribution function is included. Computational {{results show that the}} new method is more efficient than the usual method in many cases. Kernel quantile estimators Sample quantiles Order statistics Level crossing empirical distribution function...|$|R
40|$|This paper {{considers}} {{a class of}} nonstationary Gaussian processes with possible long-range dependence (LRD) and intermittency. The author proposes a new <b>estimation</b> <b>method</b> to simultaneously estimate both the LRD and intermittency parameter. An application of the proposed <b>estimation</b> <b>method</b> to a continuous-time financial model is discussed. ...|$|R
2500|$|Carl Sverdrup, using {{a variety}} of sources and <b>estimation</b> <b>methods,</b> gives the number of 75,000 for the Mongol army. Sverdrup also {{estimates}} the Khwarezmian army at 40,000 (excluding certain city-restricted militias), and emphasizes that all contemporary sources are in agreement that, if nothing else, the Mongol army was the larger of the two. He states that he came to 40,000 by first calculating the size of the Mongol army based on their historical records, and then assuming the Kwharezmian army was exaggerated by the pro-Mongol historians such as Rashid Al-Din to about the same magnitude as the Mongol army was by both Rashid Al-Din and anti-Mongol chroniclers such as Juzjani. McLynn also says that 400,000 is a massive exaggeration, but considers 200,000 to be closer to the truth (including garrisons). As for the Mongols, he estimates them at 120,000 effectives, out of a total Mongol strength of 200,000 (including troops nominally on the campaign but never engaged, and those in China).Ibid, p. 268 Genghis brought along his most able generals, besides Muqali to aid him. Genghis also brought a large body of foreigners with him, primarily of Chinese origin. These foreigners were siege experts, bridge-building experts, doctors and a variety of specialty soldiers.|$|E
2500|$|Lack {{of perfect}} {{multicollinearity}} in the predictors. [...] For standard least squares <b>estimation</b> <b>methods,</b> the design matrix X must have full column rank p; otherwise, {{we have a}} condition known as perfect multicollinearity in the predictor variables. [...] This can be triggered by having two or more perfectly correlated predictor variables (e.g. if the same predictor variable is mistakenly given twice, either without transforming one of the copies or by transforming one of the copies linearly). It can also happen if there is too little data available compared {{to the number of}} parameters to be estimated (e.g. fewer data points than regression coefficients). In the case of perfect multicollinearity, the parameter vector β will be non-identifiable—it has no unique solution. [...] At most {{we will be able to}} identify some of the parameters, i.e. narrow down its value to some linear subspace of Rp. See partial least squares regression. [...] Methods for fitting linear models with multicollinearity have been developed; some require additional assumptions such as [...] "effect sparsity"—that a large fraction of the effects are exactly zero. Note that the more computationally expensive iterated algorithms for parameter estimation, such as those used in generalized linear models, do not suffer from this problem.|$|E
2500|$|Constant {{variance}} (a.k.a. homoscedasticity). [...] This {{means that}} different {{values of the}} response variable have the same variance in their errors, regardless {{of the values of}} the predictor variables. In practice this assumption is invalid (i.e. the errors are heteroscedastic) if the response variable can vary over a wide scale. In order to check for heterogeneous error variance, or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a [...] "fanning effect" [...] between residual error and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predictive variables. Errors will not be evenly distributed across the regression line. Heteroscedasticity will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear regression line, and the mean squared error for the model will be wrong. Typically, for example, a response variable whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000 (a standard deviation of around $20,000), while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, which would imply their actual income would vary anywhere between -$10,000 and $30,000. (In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression <b>estimation</b> <b>methods</b> give less precise parameter estimates and misleading inferential quantities such as standard errors when substantial heteroscedasticity is present. However, various estimation techniques (e.g. weighted least squares and heteroscedasticity-consistent standard errors) can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g. fit the logarithm of the response variable using a linear regression model, which implies that the response variable has a log-normal distribution rather than a normal distribution).|$|E
40|$|Abstract. This paper {{introduces}} a new parameter <b>estimation</b> <b>method,</b> E-Bayesian <b>estimation</b> <b>method,</b> to estimate failure rate. The definition, properties, E-Bayesian estimation and hierarchical Bayesian estimation of failure rate are given. A example is also discussed. Through the example {{the efficiency and}} easiness of operation of this method are commended...|$|R
40|$|Abstract. An {{optimized}} battery {{state of}} charge (SOC) <b>estimation</b> <b>method</b> has been proposed in this paper. The method is based on extended Kalman filter (EKF) and combines Ah counting method and open-circuit voltage (OCV) method. According to the current excitation-response of a battery, the internal parameters of the battery model were identified by the method of least squares. Then the proposed <b>estimation</b> <b>method</b> is verified by experiments. The results show that the <b>estimation</b> <b>method</b> can reduce the cumulative error caused by long discharge and it can estimate the battery SOC effectively and accurately...|$|R
30|$|The {{proposed}} <b>estimation</b> <b>method</b> can {{be described}} as follows.|$|R
50|$|Bayesian <b>estimation</b> <b>methods.</b>|$|E
5000|$|Some of the <b>estimation</b> <b>methods</b> for multivariable {{linear models}} are ...|$|E
50|$|<b>Estimation</b> <b>methods</b> for {{functional}} {{single and}} multiple index models are available.|$|E
40|$|Abstract. This paper {{introduces}} a new <b>method,</b> named E-Bayesian <b>estimation</b> <b>method,</b> to estimate failure probability. In {{the case of}} zero-failure data, the definition of E-Bayesian estimation of failure probability is provided; moreover, the formulas of E-Bayesian estimation and hierarchical Bayesian estimation and the property of E-Bayesian estimation of the failure probability are also provided. For the estimate failure probability, {{in the following sections}} we will see simple the E-Bayesian <b>estimation</b> <b>method</b> is method than hierarchical Bayesian <b>estimation</b> <b>method.</b> Finally, the calculated results of bearing show that the proposed method is feasible and convenient in engineering application...|$|R
40|$|In this paper, I {{estimate}} {{the effect of}} public expenditure on the household welfare in context of Aschauer (1985) by considering the substitution effect of public expenditure for private expenditure. The estimation is not trivial in that the specification is not linear in parameters. In this paper I found Aschauer’s <b>estimation</b> <b>method</b> is stronger than what his theory requires to be. Instead of MLE which requires additional assumption, I propose to use alternative <b>methods</b> including nonlinear <b>estimation,</b> <b>method</b> of moments and integrated conditional moment. Moreover I perform the specification test using ICM test for each proposed <b>estimation</b> <b>method...</b>|$|R
40|$|This paper {{presents}} an optimal <b>estimation</b> <b>method</b> for nonlinear mechanical systems. The a priori {{knowledge of the}} system {{in the form of}} a nonlinear model structure is taken as a starting point. The method determines estimates of the parameters and estimates of the positions, velocities, accelerations, and inputs of the system. The optimal <b>estimation</b> <b>method</b> is applied to an experimental mechanical system. The unknown parameters in this system relate to inertia, friction and elastic deformation. It is shown that the optimal <b>estimation</b> <b>method</b> on the basis of a relatively simple model structure can lead to a useful description of the syste...|$|R
