1|46|Public
40|$|We {{design an}} incentive-compatible {{mechanism}} for scheduling n non-malleable parallel jobs on a parallel system comprising m identical processors. Each job {{is owned by}} a selfish user who is rational: she performs actions that maximize her welfare even though doing so may cause system-wide suboptimal performance. Each job is characterized by four parameters: value, <b>deadline,</b> <b>number</b> of processors, and execution time. The user’s welfare increases by the amount indicated by the value if her job can {{be completed by the}} deadline. The user declares the parameters to the mechanism which uses them to compute the schedule and the payments. The user can misreport the parameters, but since the mechanism is incentive-compatible, she chooses to truthfully declare them. We prove the properties of the mechanism and perform a study by simulation. 1...|$|E
40|$|Abstract—eCommerce is an {{area where}} an Autonomic Computing system could be very {{effectively}} deployed. The growth of eCommerce has created demand for services with financial incentives for service providers. Revenues accrue if the admitted requests are processed within the specified deadline and costs are incurred if the admitted requests are not processed within the <b>deadline.</b> The <b>number</b> of requests to be admitted depends on system capacity and workload. This paper describes an approach to automate the admission control decisions in an Apache web server for maximizing profit. A fuzzy controller that implements hill climbing logic is used. This is an illustration of the self-optimizing characteristic of an autonomic computing system. Index Terms—autonomic computing, fuzzy control I...|$|R
40|$|This paper {{surveys the}} {{deterministic}} scheduling of jobs m uniprocessor, multiprocessor, and job-shop environments. The survey {{begins with a}} brief introduction to the representation of task or job sets, followed by a discussion of classification categories. These categories include number of processors, task interruptlbility, job periodicity, <b>deadlines,</b> and <b>number</b> of resources. Results are given for single-processor schedules in job-shop and multIprogramming environments, flow-shop schedules, and multiprocessor schedules. They are {{stated in terms of}} optimal constructive algorithms and suboptimal heuristics. In most cases the latter are stated in terms of performance bounds related to optimal results. Annotations for most of the references are provided {{in the form of a}} table classifying the referenced studies m terms of various parameters...|$|R
40|$|Abstract—In {{this paper}} we study the multi-robot de-ployment problem under hard {{temporal}} constraints. After proposing {{a model for}} this task, we consider the simplest deployment algorithm and we analyze the relationship between three fundamental parameters, the temporal dead-line, the probability of success, {{and the number of}} robots. Because an exact analysis of even the simplest algorithm is computationally intractable, we derive an approximate bound leading to performance curves useful to answer design questions (how many robots are needed to get a certain performance guarantee?) or analysis questions (what is the probability of success given a certain <b>deadline</b> and <b>number</b> of robots?) Simulations show that the bounds are sharp and provide a useful tool to predict team deployment performance and tradeoffs. I...|$|R
40|$|AbstractIn this paper, {{we study}} cost efcient multi-copy {{spraying}} algorithm for routing in Delay Tolerant Networks (DTN) in which source-to-destination path {{does not exist}} most of the time. We present a novel idea and the corresponding algorithm for achieving the average minimum cost of packet transmission while maintaining the desired delivery rate by the given <b>deadline.</b> The <b>number</b> of message copies in the network depends on the urgency of meeting the delivery deadline for that message. We nd the efcient copying strategy analytically and validate the analytical results with simulations. The results demonstrate that our time dependent spraying algorithm achieves lower cost of message copying than the original spraying algorithm while maintaining the desired delivery rate by the deadline. I...|$|R
25|$|In March, Radiohead {{moved to}} Medley Studios in Copenhagen for two weeks. According to O'Brien, the {{sessions}} produced about 50 reels of tape each containing 15 minutes of music, with nothing finished. In April, Radiohead resumed recording in a Gloucestershire mansion. The lack of <b>deadline</b> and the <b>number</b> of incomplete ideas {{made it hard}} for the band to focus, and they agreed to disband if they could not agree on an album worth releasing.|$|R
40|$|In this paper, {{we study}} the {{performance}} and impact of maintaining temporal consistency on a recently proposed concurrency control protocol for processing transactions in broadcast environments. This protocol offers autonomy between mobile clients and the server such that mobile clients can read consistent data off the air without contacting the server. However, most of the existing mobile computing applications, such as information dispersal systems for stock prices and weather information, are comprised of real-time read only transactions. In order to deliver timely and useful results, real-time transactions must read temporal consistent data in addition to completing the execution before their <b>deadlines.</b> A <b>number</b> of approaches to maintaining temporal consistency are studied {{through a series of}} simulation experiments. Results show that taking advantage of data semantics and temporal consistency requirement can improve the performance of mobile read only transactions in broadcast environments. I...|$|R
5000|$|Following {{the release}} of their first {{extended}} play This Is What the Edge of Your Seat Was Made For in October 2004, Bring Me the Horizon toured extensively while writing new material for their full-length debut album. Due {{to the number of}} shows the band were playing at the time, much of the material was written quickly before recording was due to begin - drummer Matt Nicholls claimed that three songs were written in the space of two days due to the upcoming <b>deadline.</b> A <b>number</b> of songs are re-recordings of early demos that the band had recorded for a demo titled The Bedroom Sessions and in a broadcast session for the UK station Radio 1, namely [...] "(I Used to Make Out With) Medusa", [...] "Off the Heezay" [...] and [...] "Liquor & Love Lost" [...] (then known as [...] "Dragon Slaying").|$|R
500|$|Because Perfect Dark Zero was {{intended}} to be an Xbox 360 launch title, the last stage of development was very challenging and several features had to be canceled so that the game could meet the launch <b>deadline.</b> The <b>number</b> of players in multiplayer matches had to be reduced from 50 to 32, and a [...] "dataDyne TV" [...] mode that would have allowed players to upload and watch multiplayer matches over Xbox Live was eventually rejected. Final development for the Xbox 360 was very rushed. The order was given to produce the discs five days before the Microsoft certification was complete. Rare later stated they felt very confident they would pass, but it was a significant risk producing 700,000 disks if a bug turned up. According to Botwood, [...] "very few people believed we could make launch, but everything came together in time and it was out there for day one." ...|$|R
40|$|Most HPC {{platforms}} require {{users to}} submit a pre-determined number of computation requests (also called jobs). Unfortunately, this is cumbersome {{when some of the}} computations are optional, i. e., they are not critical, but their completion would improve results. For example, given a <b>deadline,</b> the <b>number</b> of requests to submit for a Monte Carlo experiment is difficult to choose. The more requests are completed, the better the results are, however, submitting too many might overload the platform. Conversely, submitting too few requests may leave resources unused and misses an opportunity to improve the results. This paper introduces and solves the problem of scheduling optional computations. An architecture which auto-tunes the number of requests is proposed, then implemented in the DIET GridRPC middleware. Real-life experiments show that several metrics are improved, such as user satisfaction, fairness and the number of completed requests. Moreover, the solution is shown to be scalable...|$|R
40|$|The paper {{focuses on}} method {{invocation}} of real-time objects in a CAN-based distributed real-time system. A simple object model is introduced, {{which allows the}} convenient modelling of hardware and software components. Related to the object model, two issues are discussed. Firstly, a model is introduced which allows to form and address object groups. This reflects a basic need in a real-time system to distribute information to multiple clients efficiently. Secondly, we discuss an approach to express timing requirements for object invocations. To achieve distributed consensus on communication resource access, an EDF-like approach is introduced, which takes advantage of knowledge about <b>deadlines,</b> the <b>number</b> of remaining communication activities, and the remaining worst-case execution time for the invoked method at each point of time. 1. Introduction Future computer systems will, to a large extent, monitor and control real-world processes. This results in an inevitable demand for timelines [...] ...|$|R
5000|$|Because Perfect Dark Zero was {{intended}} to be an Xbox 360 launch title, the last stage of development was very challenging and several features had to be canceled so that the game could meet the launch <b>deadline.</b> The <b>number</b> of players in multiplayer matches had to be reduced from 50 to 32, and a [...] "dataDyne TV" [...] mode that would have allowed players to upload and watch multiplayer matches over Xbox Live was eventually rejected. Final development for the Xbox 360 was very rushed. The order was given to produce the discs five days before the Microsoft certification was complete. Rare later stated they felt very confident they would pass, but it was a significant risk producing 700,000 disks if a bug turned up. According to Botwood, [...] "very few people believed we could make launch, but everything came together in time and it was out there for day one." ...|$|R
50|$|The Reform Government Surveillance coalition, which {{includes}} major tech firms Microsoft, Facebook, Yahoo!, Twitter, and LinkedIn, has indicated {{its opposition to}} the order. By March 3, the <b>deadline,</b> a large <b>number</b> of amicus curiae briefs were filed with the court, with numerous technology firms supporting Apple's position, including a joint brief from Amazon.com, Box, Cisco Systems, Dropbox, Evernote, Facebook, Google, Lavabit, Microsoft, Mozilla, Nest Labs, Pinterest, Slack Technologies, Snapchat, WhatsApp, and Yahoo!. Briefs from the American Civil Liberties Union, the Electronic Frontier Foundation, Access Now, and the Center for Democracy and Technology also supported Apple.|$|R
40|$|With the {{increasing}} use of computers in research contributions, added need for faster processing has become an essential necessity. Parallel Processing refers to the concept of running tasks that can be run simultaneously on several processors. There are conditions that tasks have deadlines for scheduling. Therefore, the tasks should be scheduled before <b>deadlines.</b> May <b>number</b> of tasks before scheduling reached their deadline, Therefore, these tasks lost. These conditions are unavoidable. Thus, parallel multi-processor system tasks should be scheduled in a way, minimizing lost tasks. On the other hand, achieving good response times is necessary. this is an NP-Complete problem. In this article, we introduce a method based on genetic algorithms for scheduling tasks on parallel heterogeneous multi-processor systems for tasks with deadlines. The results of the simulations indicate reduced number of lost tasks in comparison with the LPT and SPT algorithms. Moreover, the response time of the proposed method due to its number of processing tasks, is appropriate, in comparison with the algorithm LPT and SPT...|$|R
40|$|HPC {{platforms}} require {{users to}} submit a pre-determined number of computation requests (also called jobs). Unfortunately, this is cumbersome {{when some of the}} computations are optional, i. e., they are not critical, but their completion would improve results. For example, given a <b>deadline,</b> the <b>number</b> of requests to submit for a Monte Carlo experiment is difficult to choose. The more requests are completed, the better the results are, however, submitting too many might overload the platform. Conversely, submitting too few requests may leave resources unused and misses an opportunity to improve the results. This paper introduces and solves the problem of scheduling optional computations. An architecture which auto-tunes the number of requests is proposed, then implemented in the DIET GridRPC middleware. Real-life experiments show that several metrics are improved, such as user satisfaction, fairness and the number of completed requests. Moreover, the solution is shown to be scalable. Key-words: HPC; GridRPC; malleable applications; Grid’ 500...|$|R
40|$|Delay Tolerant Network (DTN) {{has been}} {{proposed}} to deliver data packets in an intermittently connected network by the store and carry technique. Among many existing routing protocols in DTN, spray and wait and its variants are based on replication by allowing the number of copies per message in the network but they still include some problems. To address energy issue and delivery ratio, we have presented the spray and fuzzy forwarding (S&FF) to employ the fuzzy inference systems (FIS). However, since our previous performance comparisons are simply evaluated over few cases, more extensive simulation scenarios need to be executed for accurate performance comparison. Based on this demand, in this paper, we compare S&FF with some variants of spray and wait in diverse aspects and provide analysis for their simulation results. Through the simulation results, we can observe that delivery ratio is acceptable while extending network lifetime in S&FF rather than comparable protocols under varying <b>deadlines,</b> the <b>number</b> of nodes and velocities.  </p...|$|R
40|$|In many {{parallel}} processing systems, particularly real-time systems, {{it is desirable}} for jobs to finish {{as close to a}} target time as possible. This work examines a method of controlling the variance of job completion times by dynamically allocating resources to jobs that are behind schedule and taking resources from jobs that are ahead of schedule. Emphasis is placed on where variance enters the system and how well it can be controlled. Controlling the variance also helps meet <b>deadlines</b> because the <b>number</b> of standard deviations between the target and the deadline determines the probability of missing the deadline. ...|$|R
40|$|The use of deadline-based {{scheduling}} {{in support}} of real-time delivery of application data units (ADUs) in a packet-switched network is investigated. Of interest is priority schedul-ing where a packet with a smaller ratio of T/H (time until delivery <b>deadline</b> over <b>number</b> of hops remaining) is given a higher priority. We refer to this scheduling algorithm as the T/H algorithm. T/H has time complexity of O(logN) for a backlog of N packets and was shown to achieve good performance {{in terms of the}} percentage of ADUs that are delivered on time. We develop a new and e±cient algorithm, called T/H-p, that has O(1) time complexity. The performance di®erence of T/H, T/H-p and FCFS are evaluated by simulation. Implementations of T/H and T/H-p in high-speed routers are also discussed. We show through simulation that T/H-p is superior to FCFS but not as good as T/H. In view of the constant time complexity, T/H-p is a good candidate for high-speed routers when both performance and implementation cost are taken into consideration. Keywords: Real-time data delivery, deadline-based scheduling, packet-switched networks, perfor-mance evaluatio...|$|R
50|$|A {{submission}} {{period was}} opened by NRK on 9 June 2016 and lasted until 11 September 2016. Prior to the <b>deadline,</b> a record-breaking <b>number</b> of 1,035 entries were submitted to NRK. Songwriters of any nationality {{were allowed to}} submit entries, while the performers of the selected songs will be chosen by NRK {{in consultation with the}} songwriters. In addition to the public call for submissions, NRK reserves the right to directly invite certain artists and composers to compete. In late 2016 and early 2017, ten songs from different genres was chosen to participate, and the selected singers, entries and composers were revealed on 7 February.|$|R
40|$|Abstract–The use of {{deadline}} based channel scheduling in sup-port of {{real time}} delivery of application data units (ADU’s) is in-vestigated. Of interest is priority scheduling where a packet {{with a smaller}} ratio of delivery <b>deadline</b> over <b>number</b> of hops to destina-tion is given a higher priority. It {{has been shown that}} a variant of this scheduling algorithm, based on head-of-the-line priority, is ef-ficient and effective in supporting real time delivery of ADU’s. In this variant, packets with a ratio smaller than or equal to a given threshold are sent to the higher priority queue. We first present a technique to select this threshold dynamically. The effectiveness of our technique is evaluated by simulation. We then study the performance of deadline based channel scheduling for large net-works, with multiple autonomous systems. For this case, accurate information on number of hops to destination may not be avail-able. A technique to estimate this distance metric is presented. The effectiveness of our algorithm with this estimated distance metric is evaluated. In addition, we study the performance of a multi-service scenario where {{only a fraction of the}} routers deploy deadline based channel scheduling. I...|$|R
5000|$|The Govt. of the National Capital Territory of Delhi {{took the}} {{initiative}} of launching the first Pulse Polio Immunisation in 1994. The Govt. of India followed suit a year later in 1995-96, by designating two National Immunisation Days (NIDs). Since then series of NIDs have been conducted successfully. In 1999, this programme was intensified {{in order to meet}} the global <b>deadline.</b> The <b>number</b> of NIDs were increased and [...] "House to House" [...] strategy was adopted to reach every child. In 2001 the strategy of intensive Mop-up immunisation was also introduced to interrupt the wild Polio virus circulation in endemic districts. In 2001-02 as many as 159 million children between the ages of 0 to 5 were immunised. In 2002 again, 1600 cases were reported which was a major outbreak that originated in western Uttar Pradesh and spread to many other states, most of which had been free of polio for more than one year. In February 2016 Livermore Rotary Club member Beth Wilson recently participated in a Rotary-sponsored trip to India to help inoculate some of the 170 million children there who were given polio drops in its twice-yearly National Immunization Day.|$|R
40|$|BOINC, a {{middleware}} {{system for}} volunteer computing, allows hosts to {{be attached to}} multiple projects. Each host periodically requests jobs from project servers and executes the jobs. This process involves three interrelated policies: 1) of the runnable jobs on a host, which to execute? 2) when and from what project should a host request more work? 3) what jobs should a server send {{in response to a}} given request? 4) How to estimate the remaining runtime of a job? In this paper, we consider several alternatives for each of these policies. Using simulation, we study various combinations of policies, comparing them on the basis of several performance metrics and over a range of parameters such as job length variability, <b>deadline</b> slack, and <b>number</b> of attached projects. ...|$|R
40|$|With {{increasing}} {{costs of}} energy consumption and cooling, power management in server clusters {{has become an}} increasingly important design issue. Current clusters for real-time applications are designed to handle peak loads, where all servers are fully utilized. In practice, peak load conditions rarely happen and clusters are {{most of the time}} underutilized. This creates the opportunity for using slower frequencies, and thus smaller energy consumption, with little or no impact on the Quality of Service (QoS), for example, performance and timeliness. In this work we present a cluster-wide QoS-aware technique that dynamically reconfigures the cluster to reduce energy consumption during periods of reduced load. Moreover, we also investigate the effects of local QoS-aware power management using Dynamic Voltage Scaling (DVS). Since most real-world clusters consist of machines of different kind (in terms of both performance and energy consumption) we focus on heterogeneous clusters. For validation, we describe and evaluate an implementation of the proposed scheme using the Apache Webserver in a small realistic cluster. Our experimental results show that using our scheme it is possible to save up to 45 % of the total energy consumed by the servers, maintaining average response time within the specified <b>deadlines</b> and <b>number</b> of dropped requests within the required amount. ...|$|R
25|$|There {{has been}} much debate in Australia over whether the F-35 is the most {{suitable}} aircraft for the RAAF. It has been claimed that the F-35's performance is inferior to Russian-built fighters operated by countries near Australia (such as the Su-27 and Su-30 in Indonesia), that it cannot meet the RAAF's long-range strike requirement, and that further delays to the F-35 program may result in the RAAF experiencing a shortage of combat aircraft. The RAAF has stated that it believes that the F-35 will meet Australia's needs, and both of Australia's major political parties currently support the development and purchase of the aircraft (though differences remain on the <b>deadline</b> and the <b>number</b> of aircraft). Former defence minister Joel Fitzgibbon has charged the defence chiefs with an obsession for the JSF.|$|R
40|$|Software {{development}} projects {{are subject to}} external and internal risks that cause delays, budget overrun and poor quality. Portfolio management {{can be used to}} alleviate this problem, as it pools resources together and allows for resource sharing among projects. Consequently, projects are more likely to succeed. However, portfolio management using only <b>deadlines</b> and the <b>number</b> of employees to improve probability of success is still confined. This paper proposes integrating portfolio management with COCOMO II that offers more management flexibility. Managers can adjust other resources, such as tools, staff capability, communication support, etc. to improve the project’s success. The proposed method can also be applied despite limited historical data and expert judgment. In addition, this paper introduces time constraints into portfolio management without assuming unrealistic linearity between effort and time...|$|R
40|$|This CEPS Policy Brief {{discusses}} the milestones {{of the new}} EU Regulation on the Registration, Evaluation, Authorisation and Restriction of Chemical substances (REACH) and the difficulties of its implementation. In a reader-friendly, non-technical fashion, the authors sketch out the main properties and ensuing obligations of REACH and survey its present status. They demonstrate that the Regulation suffers from overly ambitious <b>deadlines</b> and a <b>number</b> of technical and administrative uncertainties that are placing a higher burden of implementation for companies and for the European Chemicals Agency (ECHA) in Helsinki than originally expected. Moreover, their analysis attempts to look {{one or two years}} ahead to discuss crucial issues for the short run and also comprises some reflections on the complicated cost/benefit structure of REACH which can only be appreciated when assuming a long-run perspective...|$|R
50|$|There {{has been}} much debate in Australia over whether the F-35 is the most {{suitable}} aircraft for the RAAF. It has been claimed that the F-35's performance is inferior to Russian-built fighters operated by countries near Australia (such as the Su-27 and Su-30 in Indonesia), that it cannot meet the RAAF's long-range strike requirement, and that further delays to the F-35 program may result in the RAAF experiencing a shortage of combat aircraft. The RAAF has stated that it believes that the F-35 will meet Australia's needs, and both of Australia's major political parties currently support the development and purchase of the aircraft (though differences remain on the <b>deadline</b> and the <b>number</b> of aircraft). Former defence minister Joel Fitzgibbon has charged the defence chiefs with an obsession for the JSF.|$|R
50|$|HOPE {{continued}} to soldier on. In 2000, an agreement was signed {{to land the}} returning vehicle at Christmas Island's Aeon Airstrip. The High Speed Flight Demonstration project consisted of 25% scale models of HOPE-X to test navigation technologies and flight characteristics. As the 2003 <b>deadline</b> approached a <b>number</b> of debates broke out about the launcher profile, with many arguing that the H-2 should be replaced with a jet-powered cargo aircraft for an air-start. The first flight was pushed back further to 2004. Before this milestone was reached a major re-organization of NASDA took place {{in order to address}} its obvious overcommitment in light of Japan's economic contraction, especially now that there were demands for a crash program to develop spy satellites in order to track North Korean nuclear efforts. JAXA was formed, and HOPE was cancelled during this process.|$|R
40|$|Recently a new metaheuristic called harmony {{search was}} developed. It mimics the {{behaviors}} of musicians improvising to find the better state harmony. In this paper, this algorithm is described and applied to solve the container storage problem in the harbor. The objective of this problem is to determine a valid containers arrangement, which meets customers delivery <b>deadlines,</b> reduces the <b>number</b> of container rehandlings and minimizes the ship idle time. In this paper, an adaptation of the harmony search algorithm to the container storage problem is detailed and some experimental results are presented and discussed. The proposed approach was compared to a genetic algorithm previously applied to the same problem and recorded a good results. Comment: 7 pages, 8 th International Conference of Modeling and Simulation - MOSIM 10 - May 10 - 12, 2010 - Hammamet - Tunisia. arXiv admin note: text overlap with arXiv: 1305. 725...|$|R
40|$|Reliable {{and timely}} {{delivery}} of messages between processing nodes {{is essential in}} distributed real-time systems Failure to deliver a message within its deadline usually forces the system to undertake a recovery action, which introduces some cost (or overhead) to the system This recovery cost can be very high, especially when the recovery action fails {{due to lack of}} time or resources. Proposed in this paper is a scheme to minimize the expected cost recurred as a result of messages failing to meet their deadhnes. The scheme is mtended for distributed real-time systems, especially with a pomt-to-point interconnection topology The goal of minimizing the expected cost M achieved by sending multiple copies of a message through disjoint routes and thus mcreasmg the probability of successful message delivery within the deadhne. However, as the number of copies increases, the message traffic on the network increases, thereby increasing the dehvery time for each of the copies. There m therefore a tradeoff between the number of copies of each message and the expected cost incurred as a result of messages missmg their <b>deadlines</b> The <b>number</b> of copies of each message to be sent is determined by optimizing this tradeoff Simulation results for a hexagonal mesh and a hypercube topology indicate that the expected cost can be lowered substantially by the proposed scheme. Categories and subject Descriptors: C 21 [Computer-Communication Networks]: Network Architecture–store and forward networks C. 2 4 [Computer-Communication Networks]: Dis...|$|R
40|$|In a {{real-time}} database system, {{an application}} may assign a value to a transaction {{to reflect the}} return it expects to receive if the transaction commits before its deadline. Most prior research on real-time database systems has focused on systems where all transactions are assigned the same value, with the performance goal being to minimize the <b>number</b> of missed <b>deadlines.</b> When transactions may be assigned different values, {{the goal of the}} system shifts to maximizing the sum of the values of those transactions that commit by their <b>deadlines.</b> Minimizing the <b>number</b> of missed <b>deadlines</b> becomes a secondary concern in such systems. In this paper, we address the problem of establishing a priority ordering among transactions characterized by both values and deadlines that results in maximizing the realized value. Of particular interest is the tradeoff that needs to be established between these values and deadlines in constructing the priority ordering. Using a detailed simulation model, we ev [...] ...|$|R
40|$|Multiprocesssor {{systems have}} emerged as an {{important}} computing means for real-time applications and have received increasing attention than before. However, until now, {{little research has been}} done on the problem of on-line scheduling of parallel tasks with deadlines in partitionable multiprocessor systems. Meshes and hypercubes belong to this class of multiprocessors. In this paper, we propose a new on-line scheduling algorithm, called Deferred Earliest Deadline First (DEDF) for hypercube systems. The main idea of the DEDF algorithm is to defer the scheduling as late as possible, so that a set of jobs is scheduled at a time instead of one at a time. For processor allocation using DEDF, we have introduced a new concept - Available Time Window (ATW). By using ATW, the system utilization can be improved and thereby the <b>deadlines</b> of more <b>number</b> of tasks can be met. Simulation results indicate that the DEDF algorithm performs significantly better than the earlier proposed Buddy/RT and Stac [...] ...|$|R
40|$|I {{conduct a}} {{laboratory}} experiment to analyse {{the effect of}} deadlines and deadline length on charitable giving. Individuals may postpone or procrastinate making a donation, and then forget about doing so due to inattention. This behavioural problem is called inertia. In other contexts, deadlines are a useful tool to prevent inertia. I examine their use {{in the context of}} charitable giving using a dictator game where the recipient is a local charity. Participants are either constrained by a one week deadline, a two week deadline, or no deadline. I find no statistically significant evidence of an inertia effect in charitable giving. Furthermore, I find no evidence that the use of a <b>deadline</b> increases the <b>number</b> of donations, or the average donation of participants. The length of the deadline does not change this result. Examining positive donations, there is a significantly higher average donation {{with the use of a}} two week deadline compared to no deadline, but this result does not carry through to other comparisons. Overall, I find that deadlines do not appear to help, nor hinder, charitable campaigns...|$|R
40|$|These last years, we have {{witnessed}} {{a dramatic increase in}} the number of cores available in computational platforms. Concurrently, a new coding paradigm dividing tasks into smaller execution instances called threads, was developed to take advantage of the inherent parallelism of multiprocessor platforms. However, only few methods were proposed to efficiently schedule hard real-time multi-threaded tasks on multiprocessor. In this paper, we propose techniques optimizing the number of processors needed to schedule such sporadic parallel tasks with constrained deadlines. We first define an optimization problem determining, for each thread, an intermediate (artificial) <b>deadline</b> minimizing the <b>number</b> of processors needed to schedule the whole task set. The scheduling algorithm can then schedule threads as if they were independent sequential sporadic tasks. The second contribution is an efficient and nevertheless optimal algorithm that can be executed online to determine the thread's deadlines. Hence, it can be used in dynamic systems were all tasks and their characteristics are not known a priori. We finally prove that our techniques achieve a resource augmentation bound of 2 when the threads are scheduled with algorithms such as U-EDF, PD 2, LLREF, DP-Wrap, etc. © 2012 IEEE. SCOPUS: cp. pinfo:eu-repo/semantics/publishe...|$|R
40|$|Abstract. In {{this paper}} we {{investigate}} a scheduling problem {{motivated by a}} variety of practical applications: We are given jobs with integer release times, deadlines, and processing times. The goal is to find a non-preemptive schedule such that all jobs meet their <b>deadlines</b> and the <b>number</b> of machines used to process all jobs is minimum. If all jobs have equal release times and equal deadlines, we have the classical bin packing problem. Therefore, we are interested in solving this problem for instances where the window (interval from release time to deadline) is just slightly larger than the processing time. For the case that this difference is at most, we present a polynomial-time algorithm, on the other hand we show that the problem becomes -complete already if differences up to are allowed. Moreover, we present two dynamic programs and several approximation algorithms. We explain how filling machine by machine leads to an -approximation and develop a greedy approximation algorithm which has a constant approximation ratio if the problem instance is restricted. For general instances we show that its solution can differ from the optimum solution by a factor of ff. Finally, we present constant approximation algorithms for instances with restrictions on the release times and deadlines...|$|R
40|$|Abstract. We study a {{model in}} which a group of agents make a {{sequence}} of collective decisions on whether {{to remain in the}} current state of the system or switch to an alternative state, as proposed by one of them. Examples for instantiations of this model include the step-wise refinement of a bill of law by means of amendments to be voted on, as well as resource allocation problems, where agents successively alter the current allocation by means of a sequence of deals. We specifically focus on cases where the majority rule is used to make each of the collective decisions, as well as variations of the majority rule where different quotas need to be met to get a proposal accepted. In addition, we allow for cases in which the same proposal may be made more than once. As this can lead to infinite sequences, we investigate the effects of introducing a <b>deadline</b> bounding the <b>number</b> of proposals that can be made. We use both analytical and experimental means to characterise situations in which we can expect to see a convergence effect, in the sense that the expected payoff of each agent will become independent from the initial state of the system, as long as the deadline is chosen large enough. ...|$|R
