79|74|Public
25|$|Brief {{activation}} of an excitatory pathway can produce {{what is known}} as long-term depression (LTD) of synaptic transmission {{in many areas of the}} brain. LTD is induced by a minimum level of postsynaptic depolarization and simultaneous increase in the intracellular calcium concentration at the postsynaptic neuron. LTD can be initiated at inactive synapses if the calcium concentration is raised to the minimum required level by heterosynaptic activation, or if the extracellular concentration is raised. These alternative conditions capable of causing LTD differ from the <b>Hebb</b> <b>rule,</b> and instead depend on synaptic activity modifications. D-serine release by astrocytes has been found to lead to a significant reduction of LTD in the hippocampus.|$|E
5000|$|... #Caption: Fig.4 LEFT: This picture {{illustrates}} a macro-column with three mini-columns {{in which the}} <b>Hebb</b> <b>rule</b> {{is applied to the}} neighboring neurons of the most resonant neuron. The applied LSUP value is 1 (i.e., +/-1) in all of the mini-columns. The number of involved synapses for any neuron of mini-column 2 is equal to 2 * LSUP 1 + 1 (=3), while the number of involved synapses for any neuron of the mini-column 3 is equal to 2 * LSUP 1 + 1 * LSUP 2 + 1 (=6). RIGHT: Three macro-columns composed of three mini-columns. The WTA on the category layer is driven by the delays in the minicolumns.|$|E
50|$|Brief {{activation}} of an excitatory pathway can produce {{what is known}} as long-term depression (LTD) of synaptic transmission {{in many areas of the}} brain. LTD is induced by a minimum level of postsynaptic depolarization and simultaneous increase in the intracellular calcium concentration at the postsynaptic neuron. LTD can be initiated at inactive synapses if the calcium concentration is raised to the minimum required level by heterosynaptic activation, or if the extracellular concentration is raised. These alternative conditions capable of causing LTD differ from the <b>Hebb</b> <b>rule,</b> and instead depend on synaptic activity modifications. D-serine release by astrocytes has been found to lead to a significant reduction of LTD in the hippocampus.A LTD was evidenced in 2011 for the electrical synapses (modification of Gap Junctions efficacy through their activity).|$|E
5000|$|The {{synaptic}} weight is changed {{by using a}} learning rule, {{the most basic of}} which is <b>Hebb's</b> <b>rule,</b> which is usually stated in biological terms as ...|$|R
5000|$|The {{simplest}} learning <b>rule</b> {{known is}} <b>Hebb's</b> <b>rule,</b> which states in conceptual terms that neurons that fire together, wire together. In component form as a difference equation, {{it is written}} ...|$|R
50|$|Oja's rule {{requires}} {{a number of}} simplifications to derive, but in its final form it is demonstrably stable, unlike <b>Hebb's</b> <b>rule.</b> It is a single-neuron special case of the Generalized Hebbian Algorithm. However, Oja's rule can also be generalized in other ways to varying degrees of stability and success.|$|R
40|$|In this thesis, {{the storage}} {{capacities}} of the Bidirectional Associative Memories (BAM) and the Hopfield {{network and the}} applications of the multi-layer feed-forward network are studied and presented. There are four sub-topics in this thesis, {{the first one is}} the use of ring and cascade architectures in storing temporal or ordered patterns. A comparison of these architectures is presented. The second sub-topic is to use some new methods to increase the storage capacities of the BAM and the Hopfield network. The uses of a modified <b>Hebb</b> <b>rule</b> and multi-threshold values are studied. Comparisons among the <b>Hebb</b> <b>rule,</b> the delta rule and these new methods are also presented. Results show that these new methods can store ten 35 -pixel images while the <b>Hebb</b> <b>rule</b> cannot. The third sub-topic is the use of a multi-layer feed-forward network with the back-propagation training to do pulse compression. Results of this approach are presented. Lastly, the use of a multi-layer feed-forward network to do filtering is also studied and the results are presented. Source: Masters Abstracts International, Volume: 30 - 03, page: 0840. Supervisor: H. K. Kwan. Thesis (M. A. Sc.) [...] University of Windsor (Canada), 1990...|$|E
40|$|We {{show that}} a {{straightforward}} extension of a simple learning model based on the <b>Hebb</b> <b>rule,</b> the previously introduced Association-Reinforcement-Hebb-Rule, can cope with "delayed", unspecific reinforcement also {{in the case of}} structured data and lead to perfect generalization. Comment: 13 pages, 3 figure...|$|E
40|$|Abstract. A new a m to the {{asymptotic}} {{analysis of}} autoassociation properties in recurrent McCulloch-Pitts {{networks in the}} range of low activity is proposed. Using information theory, this method examines the sialic SlNCtUTe of stable states imprinted by a Hebbian storing process. In addition to the definition of cfitical pattern capacity usually considered in the analysis of the Hopfield model. we introduce the definition of information capacity which guarantees content adressability and is a stricter upper hound of the information really accessible in an autoassociation process. We calculate these two types of capacities faor two types of local learning rules which are very effective for sparsely coded patterns: the <b>Hebb</b> <b>rule</b> and the clipped <b>Hebb</b> <b>rule.</b> It tums out that for both rules the information capacity is exactly half the pattern capacity. 1...|$|E
50|$|However, <b>Hebb's</b> <b>rule</b> has problems, {{namely that}} it has no {{mechanism}} for connections to get weaker and no upper bound for how strong they can get. In other words, the model is unstable, both theoretically and computationally. Later modifications gradually improved <b>Hebb's</b> <b>rule,</b> normalizing it and allowing for decay of synapses, where no activity or unsynchronized activity between neurons results in a loss of connection strength. New biological evidence brought this activity to a peak in the 1970s, where theorists formalized various approximations in the theory, {{such as the use of}} firing frequency instead of potential in determining neuron excitation, and the assumption of ideal and, more importantly, linear synaptic integration of signals. That is, there is no unexpected behavior in the adding of input currents to determine whether or not a cell will fire.|$|R
5000|$|Hebbian {{theory is}} a theory in {{neuroscience}} that proposes {{an explanation for the}} adaptation of neurons in the brain during the learning process, describing a basic mechanism for synaptic plasticity, where an increase in synaptic efficacy arises from the presynaptic cell's repeated and persistent stimulation of the postsynaptic cell. Introduced by Donald Hebb in his 1949 book The Organization of Behavior, the theory is also called <b>Hebb's</b> <b>rule,</b> <b>Hebb's</b> postulate, and cell assembly theory. Hebb states it as follows: ...|$|R
50|$|Mental {{models are}} built through repetition, variation, and association. Experiencing {{something}} multiple times {{is said to}} lead to an internal representation. Experiencing sequences of things associates our internal representations in a mental map according to how those things occur in reality. Donald <b>Hebb's</b> <b>rule</b> put it as: Things that fire together, wire together.|$|R
40|$|Based on neurophysiological {{observations}} {{of the behavior of}} synapses, Spike Time Dependent Hebbian Plasticity is a novel extension to the modeling of the <b>Hebb</b> <b>Rule</b> [6]. This rule has enormous importance in the learning of Spiking Neural Networks (SNN) but its mechanisms and computational properties are still to be explored...|$|E
40|$|We derive {{analytical}} expressions for {{the connections}} of large perceptrons, {{by studying the}} fixed points of the perceptron learning rule. If the training set consists of all possible input vectors, we can calculate (for large systems) the connections as a series expansion in the system size. The leading term in this expansion {{turns out to be}} either the <b>Hebb</b> <b>rule</b> (for unbiased distributions) or the biased <b>Hebb</b> <b>rule</b> (for biased distributions). The performance of our asymptotic expressions (and finite size corrections) on small systems is studied numerically. For the more realistic case of having an extensive training set (patterns learned with training noise) we derive a self-consistent set of coupled non-linear equations for the connections. In the limit of zero training noise, the solution of these equations is shown to give the connections with maximal stability in the Gardner sense...|$|E
40|$|The {{problem of}} {{learning}} to see is the problem {{of learning to}} extract information from a bewilderingly complex input. To effectively utilize visual data, an organism must reduce the staggering dimensionality of that data by extracting relevant features. Learning to recognize the regularities of visual data is a paridigmatic problem of unsupervised learning. Previous models of visual development were often based on (sometimes fairly ad-hoc) variations on the <b>Hebb</b> <b>rule</b> combined with inhibitory dynamics, which are essentially applications of the classic competitive learning approach to unsupervised learning. This approach is typied, for instance, by Von der Malsberg's models and various models based on Kohonen maps. Statistically, the <b>Hebb</b> <b>rule</b> performs something like principal component analysis, clustering similar inputs together in order to compress away redundancy and highlight important shared features. This serves to maximize the variance at individual nodes...|$|E
5000|$|<b>Hebb's</b> <b>rule</b> has {{synaptic}} weights approaching infinity with {{a positive}} learning rate. We can stop this by normalizing the weights so that each weight's magnitude is restricted between 0, corresponding to no weight, and 1, corresponding to being the only input neuron with any weight. We do this by normalizing the weight vector to be of length one: ...|$|R
40|$|Abstract. In {{this paper}} {{we present a}} new {{self-organizing}} neural network, which builds a spatiotemporal model of an input temporal sequence inductively. The network {{is an extension of}} Kohonen’s Self-organizing Map with a modified <b>Hebb’s</b> <b>rule</b> for update of temporal synapses. The model building behavior is shown on inductive learning of a transition matrix from a data generated by a Markov Chain...|$|R
40|$|The {{interface}} between so-called activity-dependent and activity-independent mechanisms of circuit development is discussed here {{in light of}} recent findings that question the role of activity in brain development. This debate is presented simplistically here in terms of Sperry's chemoaffinity hypothesis versus <b>Hebb's</b> <b>rules</b> of correlation-based synaptic change, which are often presented as being mutually exclusive - much like oil and vinegar...|$|R
40|$|A new {{description}} of the neural activity is introduced by the neuro-flow dynamics and the extended <b>Hebb</b> <b>rule.</b> The remarkable characteristics of the neuro-flow dynamics, such as the primacy and the recency effect during awakeness or sleep, are pointed out. Comment: 8 pages, 10 Postscript figures, LaTeX file, to appear in Chaos, Solitons and Fractal...|$|E
40|$|A set {{of fixed}} {{points of the}} Hopfield type neural network was under investigation. Its {{connection}} matrix is constructed {{with regard to the}} <b>Hebb</b> <b>rule</b> from a highly symmetric set of the memorized patterns. Depending on the external parameter the analytic description of the fixed points set had been obtained. And as a conclusion, some exact results of Hopfield neural networks were gained...|$|E
40|$|A new {{local and}} {{incremental}} learning rule is examined {{for its ability}} to store patterns from a time series in an attractor neural network. This learning rule has a higher capacity than the <b>Hebb</b> <b>rule,</b> and suffers significantly less capacity loss as the correlation between patterns increases. 1 Introduction There are two rules regularly used for training Hopfield networks. The most common of these, the <b>Hebb</b> <b>rule,</b> suffers severe degradation when training patterns are correlated. As a result, practitioners have generally reverted to using the pseudo-inverse rule in such circumstances. However the pseudo-inverse method suffers from significant problems. Firstly, it is not incremental: if a new pattern is to be trained, all the old patterns have to be retrained. Secondly it is not local: the network cannot naturally be trained in a parallel manner, and so is not easily amenable to high speed parallel techniques. Lastly the training method is slow because it involves inverting an m Θ m [...] ...|$|E
40|$|Neural network models offer a {{theoretical}} testbed {{for the study}} of learning at the cellular level. The only experimentally verified learning <b>rule,</b> <b>Hebb's</b> <b>rule,</b> is extremely limited in its ability to train networks to perform complex tasks. An identified cellular mechanism responsible for Hebbian-type long-term potentiation, the NMDA receptor, is highly versatile. Its function and efficacy are modulated by a wide variety of compounds and conditions and are likely to be directed by non-local phenomena. Furthermore, it has been demonstrated that NMDA receptors are not essential for some types of learning. We have shown that another neural network learning rule, the chemotaxis algorithm, is theoretically much more powerful than <b>Hebb's</b> <b>rule</b> and is consistent with experimental data. A biased random-walk in synaptic weight space is a learning rule immanent in nervous activity and may account for some types of learning [...] notably the acquisition of skilled movement. Comment: In press: Progress in Neural Network...|$|R
50|$|Oja's {{learning}} rule, {{or simply}} Oja's rule, named after Finnish computer scientist Erkki Oja, {{is a model}} of how neurons in the brain or in artificial neural networks change connection strength, or learn, over time. It is a modification of the standard <b>Hebb's</b> <b>Rule</b> (see Hebbian learning) that, through multiplicative normalization, solves all stability problems and generates an algorithm for principal components analysis. This is a computational form of an effect which is believed to happen in biological neurons.|$|R
5000|$|... and the {{previous}} section's simplification takes both the learning rate and the input weights to be 1. This version of the rule is clearly unstable, as in any network with a dominant signal the synaptic weights will increase or decrease exponentially. However, it can be shown that for any neuron model, <b>Hebb's</b> <b>rule</b> is unstable. Therefore, network models of neurons usually employ other learning theories such as BCM theory, Oja's rule, or the Generalized Hebbian Algorithm.|$|R
40|$|Neural {{networks}} with synaptic weights constructed {{according to}} the weighted <b>Hebb</b> <b>rule</b> are studied {{in the presence of}} noise (finite temperature), when the number of stored patterns is finite. Although, for arbitrary weights not all of the stored patterns are global minima, there exists a temperature range in which only the stored patterns are minima of the free energy. In particular, a detailed analysis reveals that {{in the presence of a}} single extra pattern stored with an appropriate weight in the synaptic rule, the temperature at which the spurious minima of the free energy are eliminated is significantly lower than for a similar network without this extra pattern. The convergence time of the network, together with the overlaps of the equilibria of the network with the stored patterns, can thereby be improved considerably. 1 Introduction The statistical mechanics of large neural networks with the <b>Hebb</b> <b>rule</b> prescription for the synaptic weights has been studied in detail and is now w [...] ...|$|E
40|$|Computational {{models of}} {{self-organizing}} neural networks and associative memory depend on hebbian synaptic plasticity, a local learning rule for increasing synaptic strengths based on coincident electrical activity in presynaptic and postsynaptic neurons. Some forms of long-term potentiation (L TP) {{studied in the}} hippocampus and neocortex are hebbian. Learning rules in network models also incorporate long-lasting decreases in synaptic strengthsfor stability and efSiciency. We review recent evidence for long-term depression (L TD) in the hippocampus and neocortex. Key words: synaptic plasticity /long-term depression / long-term potentiation 1 glutamate receptor / metabotropic receptor / hippocampus / network models / associative memory LONG-TERM potentiation 1 at some synapses in the hippocampus depends on coincident presynaptic and postsynaptic a~tivit~~. ~ (see also Teyler et ~ 1 and, ~ Gustafsson and Wigstrom, this issue 7). An increase in synaptic efficacy under these conditions, suggested by D. 0. Hebb, 8 is termed hebbiang?l 0 (see also McNaughton and Barnes, this issuel 1). One of the most thoroughly explored uses of the simple <b>Hebb</b> <b>rule</b> is {{in the formation of}} associations between patterns of neural activity. 12 -lg The <b>Hebb</b> <b>rule</b> is appealing for this purpose because it provides a way of forming global associations between macroscopic patterns of activity in assemblies of neurons using only the local information available at individual synapses. However, modifications of the simple <b>Hebb</b> <b>rule</b> to include long-term depression can improve the efficiency and performance of associative recall in neural network models. l 7 lZ 0 Several types of depression have been studied in the hippocampus. Stimulating the perforant pathwa...|$|E
40|$|A set {{of fixed}} {{points of the}} Hopfield type neural network is under investigation. Its {{connection}} matrix is constructed {{with regard to the}} <b>Hebb</b> <b>rule</b> from a highly symmetric set of the memorized patterns. Depending on the external parameter the analytic description of the fixed points set has been obtained. Comment: 2 pages, Latex, sumbitted to Conference of Computational Physics, September, 2 - 5 1998 (Granada, Espana...|$|E
40|$|Abstract. As an {{alternative}} to the conventional Hebb-type unsupervised learning, differential learning was studied in the domain of <b>Hebb’s</b> <b>rule</b> [1] and decorrelation [2]. In this paper we present an ICA algorithm which employs differential learning, thus named as differential ICA. We derive a differential ICA algorithm in the framework of maximum likelihood estimation and random walk model. Algorithm derivation using the natural gradient and local stability analysis are provided. Usefulness of the algorithm is emphasized in the case of blind separation of temporally correlated sources and is demonstrated through a simple numerical example. ...|$|R
40|$|Abstract. Recent {{experimental}} results on spike-timing-dependent plasticity (STDP) and heterosynaptic inter-action in various systems have revealed new {{temporal and spatial}} properties of activity-dependent synaptic plasticity. These results challenge the conventional understanding of <b>Hebb’s</b> <b>rule</b> and raise intriguing questions regarding the fundamental processes of cellu-lar signaling. In this article, I review these new findings that lead to formulation of {{a new set of}} cellular rules. Emphasis is on evaluating potential molecular and cellular mechanisms that may underlie the spike-timing window of STDP and different patterns of heterosynap-tic modifications. I also highlight several unresolved issues, and suggest future lines of research. ...|$|R
40|$|Abstract. The Herault-Jutten (HJ) {{algorithm}} is a neuromimetic structure capable to perform blind source separation (BSS) of a linear mixture from {{an array of}} sensors without knowing the transmission characteristics of the channels, nor the inputs. The learning algorithm developed by Herault and Jutten {{is based on the}} generalized <b>Hebb’s</b> <b>rule</b> {{in such a way that}} each output signal will be proportional to only one source by cancelling the influence of the other source. In this article, we show how theoretic stability conditions can be used for parameter estimation to restore the primary sources via interval computations. ...|$|R
40|$|A set {{of fixed}} {{points of the}} Hopfield type neural network was under investigation. Its {{connection}} matrix is constructed {{with regard to the}} <b>Hebb</b> <b>rule</b> from a highly symmetric set of the memorized patterns. Depending on the external parameter the analytic description of the fixed points set had been obtained. And as a conclusion, some exact results of Hopfield neural networks were gained. Comment: 4 pages, latex, no figure...|$|E
40|$|The {{nature of}} the basins of {{attraction}} of a Hopfield network {{is as important as}} the capacity. Here a new learning rule is re-introduced. This learning rule has a higher capacity than that of the <b>Hebb</b> <b>rule,</b> and still keeps important functionality, such as incrementality and locality, which the pseudo-inverse lacks. However the basins of attraction of the fixed points of this learning rule have not yet been studied...|$|E
40|$|Qubit {{networks}} with long-range interactions {{inspired by}} the <b>Hebb</b> <b>rule</b> {{can be used as}} quantum associative memories. Starting from a uniform superposition, the unitary evolution generated by these interactions drives the network through a quantum phase transition at a critical computation time, after which ferromagnetic order guarantees that a measurement retrieves the stored memory. The maximum memory capacity p of these qubit networks is reached at a memory density p/n= 1. Comment: To appear in Physical Review Letter...|$|E
40|$|The main {{advantage}} of artificial neural networks (ANN) {{in recognition of}} the cottages, is in their functioning like a human brain. The paper deals with image recognition neuron Hopfield’s networks, a comparative analysis of the recognition images by a projection’s method and the <b>Hebb’s</b> <b>rule.</b> For these purposes, was developed program with C# in Microsoft Visual Studio 2012. In this article to recognition for images with different levels of distortion were used. The analysis of results of recognition of images has shown that the method of projections allows to restore strongly distorted images (level of distortions up to 25 – 30 percent...|$|R
40|$|The Herault-Jutten (HJ) {{algorithm}} is a neuromimetic structure capable to perform blind source separation (BSS) of a linear mixture from {{an array of}} sensors without knowing the transmission characteristics of the channels, nor the inputs. The learning algorithm developed by Herault and Jutten {{is based on the}} generalized <b>Hebb's</b> <b>rule</b> {{in such a way that}} each output signal will be proportional to only one source by cancelling the influence of the other source. In this article, we show how theoretic stability conditions can be used for parameter estimation to restore the primary sources via interval computations...|$|R
30|$|The {{emergence}} of cognitive and emotional states {{is made possible}} by brain dynamics, which can be modeled by neural networks. In complex dynamical systems, the rules of locally interacting elements (for example, <b>Hebb’s</b> <b>rules</b> of synaptic interaction) may be simple and programmed in a computer model. But their nonlinear dynamics can generate complex patterns and system states, which cannot be forecast {{in the long run}} without increasing loss of computability and information. Thus, artificial minds could have their own intentionality, cognitive and emotional states, which cannot be forecast and computed similar {{as is the case with}} natural minds. Limitations of computability are characteristic features of complex systems.|$|R
