12|47|Public
50|$|The UIS publishes {{statistical}} {{and analytical}} reports, {{in addition to}} methodological guides and international classifications. The Institute’s flagship publication, the Sustainable Development <b>Data</b> <b>Digest,</b> is available in several UN languages.|$|E
50|$|Most of {{the above}} {{scenarios}} can be emulated by modifying {{the content of the}} output PDU. For example, to emulate a <b>data</b> <b>digest</b> error, we just need to change the DataDigest field in the PDU.|$|E
5000|$|<b>Data</b> <b>Digest</b> {{points that}} {{out by the}} {{development}} of the CDO - and for that matter the CAO and the Chief Digital Officer. Could they all blend into one? Or will they be obsolete in a few years and replaced with a new emerging C-suite? ...|$|E
40|$|Distributed query {{processing}} is {{of paramount}} importance in next-generation distribution services, such as Internet of Things (IoT) and cyber-physical systems. Even if several multi-attribute range queries supports have been proposed for peer-to-peer systems, these solutions must be rethought to fully meet the requirements of new computational paradigms for IoT, like fog computing. This paper proposes dragon, an ecient support for distributed multi-dimensional range query processing targeting ecient query resolution on highly dynamic data. In dragon nodes {{at the edges of the}} network collect and publish multi-dimensional data. The nodes collectively manage an aggregation tree storing <b>data</b> <b>digests</b> which are then exploited, when resolving queries, to prune the sub-trees containing few or no relevant matches. Multi-attribute queries are managed by linearising the attribute space through space lling curves. We extensively analysed dierent aggregation and query resolution strategies in a wide spectrum of experimental set-ups. We show that dragon manages eciently fast changing data values. Further, we show that dragon resolves queries by contacting a lower number of nodes when compared to a similar approach in the state of the art...|$|R
5000|$|Sisson, with Richard Canning, started one of {{the first}} {{consulting}} firms devoted exclusively to electronic data processing, Canning, Sisson, and Associates. Canning and Sisson also published {{one of the}} earliest computer periodicals, <b>Data</b> Processing <b>Digest,</b> starting in 1955. [...] Sisson went on to write a number of noted books on the subject of EDP, including The Management of Data Processing, and A Manager’s Guide to Data Processing. He wrote an early and influential paper in the field of Operations Research, [...] "Methods of Sequencing in Job Shops" [...] in the journal Operations Research in 1959.|$|R
50|$|ISRAFAX is the institute's {{quarterly}} research print publication. It provides CIJR {{members with}} <b>data</b> and a <b>digest</b> of international analysis and opinion on issues, mixing original content with articles from newspapers, magazines, scholarly journals, official documents, and websites {{from around the}} world. The Israeli and Arab media are also scanned for reports, opinions, and other documents. The magazine is distributed internationally.|$|R
5000|$|We {{do wrong}} things {{and see if}} the DUT can detect and behave accordingly.In this case, we must modify the PDU {{sequence}} sent to the DUT to some extent (e.g., change CmdSN of a command, set an invalid <b>data</b> <b>digest...),</b> and verify the DUT can react according to the protocol (e.g., send a Reject PDU, close the connection...).|$|E
5000|$|The iSCSI Engine core is {{in charge}} of sending/receiving iSCSI PDUs to/from DUT (Device Under Test). On one side, it accepts input from test script, sending PDUs or verifying check points; on the other, it should {{understand}} iSCSI protocol to a certain level so that necessary automation can be achieved, e.g., the automatic generation of iSCSI PDUs (Login/Text Request during negotiation, Nop-Out ping response, [...] ) and some PDU fields (ITT, TTT, CmdSN, <b>data,</b> <b>digest...).</b>|$|E
50|$|Initially Prescient Software also {{provided}} Innovation and new product design and new product strategy consulting {{based on the}} SWIFT method. When Prescient Software released MergeRight as open source and wound down its software sales side, the remaining consulting business was renamed the SWIFT Design Group. This consulting practice grew to also include interim and part-time executive management service to several early stage Silicon Valley start-ups and led to early employee or founder roles at several of these companies including <b>Data</b> <b>Digest</b> (predictive analytics), SeeItFirst (Streaming Video), Mobile Anywhere (Wired, WiFI and Cellular mobile device network management), Immunet (distributed network security), Light Crafts (Distributed Photography) and XooXooX (Web Advertising).|$|E
40|$|Cloud {{computing}} is {{most widely}} used as the next generation architecture of IT enterprises, that provide convenient remote access to data storage and application services. This cloud storage can potentially bring great economical savings for data owners and users, but due to wide concerns of data owners that their private data may be exposed or handled by cloud providers. Hence end-to-end encryption techniques and fuzzy fingerprint technique {{have been used as}} solutions for secure cloud data storage. In this project we use searchable encryption techniques, which allows encrypted data to be searched by users without leaking information about the data itself and user’s queries. We build a secure searchable index, and develop a one to many order preserving mapping technique to protect those sensitive score information. The resulting design is able to facilitate efficient server side ranking without losing keyword privacy. Hence to avoid loose and loss of data, we use privacy preserving data-leak detection (DLD) solution to solve the issue where sensitive <b>data</b> <b>digests</b> is used in detection. The advantage of this method is that it enables the data owner to safely delegate the detection operation to a semi honest provider without revealing the sensitive data to the provider. In this project, we identify the challenges towards achieving privacy in searchable outsourced cloud data services and we use the DLD solution which helps to detect the leak data. This technique helps us to save securely our sensitive data in cloud storage and retrieve this data while we required without leaking our private data through Information Security and Secure Search over Encrypted Data in Cloud Storage Services...|$|R
40|$|The {{applications}} of E-commerce {{rely on the}} security of communication net foundations. A secure E-commerce system must satisfy the essential demands of information security, integrality, being undeniable and available and keeping bargainer’s identity certifiable. Therefore, all kinds of techniques for information security should be applied, including the <b>data</b> encryption, digital <b>digest,</b> digital signature and digital ID etc. The wide application of these techniques into the secure transaction standards would insure the secure development of E-commerce...|$|R
40|$|The 16 S- 23 S ribosomal RNA spacer {{regions of}} Acetobacter europaeus DSM 6160, A. xylinum NCIB 11664 and A. xyUnion CL 27 were amplified by PCR. Specific PCR {{products}} {{were obtained from}} each strain and their nucleotide sequences determined. The spacer region of A. europaeus comprises 768 nucleotides (nt), that of A. xylinum 778 nt and that of A. xylinum CL 27 759 nt. Genes encoding tRNAIle and tRNAAla were identified. Putative antitermination sequences {{were found between the}} tRNAAla sequence and the 5 ′-terminus of the 23 S rRNA coding sequence. The boxA element has the nucleotide sequence TGCTCTTTGATA. Based on hybridization <b>data</b> of <b>digested</b> chromosomal DNA with spacer-specific probes, the copy number of the rrn operons on the chromosome of Acetobacter strains is estimated to be fou...|$|R
5000|$|In 2000, McGregor joined <b>Data</b> <b>Digest,</b> {{where he}} led the {{engineering}} team developing of an Bayesian Network predictive analytic engine which analyzed arbitrary databases or tables of transactional data to find hidden predictive relationships. Initially named [...] "Business Navigator / BN-5", the product was later sold to Decision-Q Corporation who renamed it FasterAnalytics. The analytic engine is general purpose and was initially used to for analyze data {{for a wide range}} of problems, from predicting response levels to offers for a mail order record and video club to predicting protein-folding in drug discovery research. It is notable that with this work, McGregor returned to the roots of his earlier Prescient Agent work of a decade earlier which also relied on Bayesian prediction. However, a decade of computer performance improvements now made technology that had once challenged $100,000 workstations well within the capabilities of desktop office machines.|$|E
40|$|We {{propose a}} new method to build {{persistent}} suffix trees for indexing the genomic data. Our algorithm DiGeST (Disk-Based Genomic Suffix Tree) improves significantly over previous work {{in reducing the}} random access to the in-put string and performing only two passes over disk <b>data.</b> <b>DiGeST</b> {{is based on the}} two-phase multi-way merge sort paradigm using a concise binary representation of the DNA alphabet. Furthermore, our method scales to larger genomic data than managed before...|$|E
30|$|Type_of_message {{indicates}} {{the type of}} the sent message. For this case, the node who receives the messages detects {{that it is a}} request to choose a cryptographic suite. Mid is the message identifier. SAi states the cryptographic algorithms supported by the core node for encryption and signature. The previous two payloads are encrypted using the previously negotiated key GKDH. This ensures the confidentiality of the transmitted <b>data.</b> <b>digest</b> is a digest of the global message and is encrypted using the negotiated key GKDH. This payload ensures the integrity and authentication of the sent data.|$|E
40|$|A biophysical and {{immunological}} {{method for}} characterization of ribonucleic acid type C virus suspensions is described. The method provides {{a relationship to}} the total viral mass concentration of the particle titer, the gs antigen titer, and the ultraviolet absorbance (268 nm) of 2 % sodium dodecyl sulfate <b>digests.</b> <b>Data</b> for murine, rat, feline, and hamster viruses are shown to be analogous within the test limitations. From these data, {{an assessment of the}} viral purity can be made, the structural integrity can be evaluated, an approximate molecular weight can be computed, and the mole ratio of gs antigen can be determined...|$|R
40|$|Vols. for called ed. Vols. for 1977 - 1978, 1983 - 1984 and 1985 - 1986 issued together. Mode of access: Internet. Vols. for 1980 - 1983 / 84, 1988 - {{issued by}} the U. S. Dept. of Education, Office of Educational Research and Improvement, National Center for Education Statistics; 1985 / 86 by: U. S. Dept. of Education, Office of Educational Research and Improvement, Center for Statistics; 1987 by: U. S. Dept. of Education, Office of Educational Research and Improvement, Center for Education Statistics. <b>Data</b> {{summarized}} in: <b>Digest</b> of education statistics. Pocket digest; and, 1993 - in: Mini-digest of education statistics...|$|R
50|$|Commander Maury, {{who held}} the {{position}} of Hydrographer of the Navy from 1842-1861, is credited with founding the science of oceanography. His system for collecting and using oceanographic data revolutionized navigation of the seas. Maury assumed command of the Navy's Depot of Charts and Instruments in 1842. Possessing an active, scientific mind, he immediately recognized possibilities for expanding the services of the Depot. He suggested that, if all shipmasters would submit reports of their experiences to a central agency, the <b>data</b> could be <b>digested,</b> compiled and published {{for the benefit of}} all. This idea became the basic formula of hydrographic offices throughout the world, making Maury's contributions a milestone in naval oceanography.|$|R
40|$|In {{the study}} of network {{forensics}} Mostly tools give you view of movement in real time, Butmonitoring in real- time at any stage requires hardware resources and human significant, and doesn'tratio to workgroups larger than a single network. It is normally more practical to archive allintercommunication service and analyse subsets as necessary. This process is called as network forensics, orreconstructive traffic analysis. Practically, it is frequently limited to <b>data</b> <b>digest</b> and packet-level inspectionsupervision; in spite of, a network monitoring forensics tool can endow a richer look of the data gathered,permission you to inspect the intercommunication service from further up the protocol stack? the forensicnetwork is very simple to monitor and identify problem conveniently. It is very useful to rummage securityinfringement. It is completely analysing the record of network traffic...|$|E
40|$|Abstract—We {{consider}} an underwater {{wireless sensor network}} where baseline communication happens over acoustic, multihop routes from the underwater nodes to an on-shore station. The data collected by the nodes greatly exceeds the baseline communication capability. At best, the nodes can transmit digests of their full observations. In order for the sink to receive all sensed data, an autonomous underwater vehicle (AUV) is sent to each node for collecting data over short-distance, high data rate optical connections. The AUV then offloads all collected information to the terrestrial station via wireless communication when it surfaces. The observations made by the nodes vary in size and urgency. The information they provide has an associated value. Given a path of the AUV, we design scheduling strategies for the nodes to decide when and how much information (i. e., which digest) to transmit via acoustic routes so {{that the value of}} information reaching the terrestrial station is maximized. These strategies are compared via simulations on realistic scenarios. Our results show that scheduling algorithms that are able to locally estimate the value of information of a <b>data</b> <b>digest</b> provide the delivery of data with a significantly higher value of information. In contrast, uninformed algorithms, i. e., strategies that do not consider the value of information at the node level, provide only a marginal increase over the benchmark case of using only the AUV for data collection. I...|$|E
40|$|ABSTRACT. Acomputer {{algorithm}} for restriction-site mapping {{consists of}} a generator of partial maps and a consistency checker. This paper examines consistency checking and argues that a method based on separation theory extracts {{the maximum amount of}} information from fragment lengths in <b>digest</b> <b>data.</b> It results in the minimum number of false maps being generated. 1. Introduction. Restriction-site mapping involves locating certain restriction sites on a circular plasmid, on a linear phage or on some other sequence of DNA. The raw data for the problem consist of fragment lengths from one or more digests carried out on the DNA. Deducing the map is a combinatorial problem often performed {{with the aid of a}} computer program [1 − 9]. While there is only one real map, the data ca...|$|R
40|$|AbstractThe Partial Digest problem {{asks for}} the {{coordinates}} of m points {{on a line}} such that the pairwise distances of the points form a given multiset of m 2 distances. Partial Digest is a well-studied problem with important applications in physical mapping of DNA molecules. Its computational complexity status is open. Input <b>data</b> for Partial <b>Digest</b> from real-life experiments are always prone to error, which suggests to study variations of Partial Digest that take this fact into account. In this paper, we study the computational complexity of Partial Digest variants that model three different error types that can occur in the data: additional distances, missing distances, and erroneous fragment lengths. We show that these variations are NP-hard, hard to approximate, and strongly NP-hard, respectively...|$|R
40|$|In this paper, {{we propose}} a robust {{statistical}} framework for extracting scenes from a baseball broadcast video. We apply multistream hidden Markov models (HMMs) {{to control the}} weights among different features. To achieve a large robustness against new scenes, we used a common simple structure for all the HMMs. In addition, scene segmentation and unsupervised adaptation were applied to achieve greater robustness against di#erences in environmental conditions among games. The F-measure of scene-extracting experiments for eight types of scene from 4. 5 hours of <b>digest</b> <b>data</b> was 77. 4 % and was increased to 78. 7 % by applying scene segmentation. Furthermore, the unsupervised adaptation method improved precision by 2. 7 points to 81. 4 %. These results confirm the effectiveness of our framewor...|$|R
40|$|International audienceBusiness Process Outsourcing is {{nowadays}} {{common in}} companies. In particular, {{the distribution of}} such business processes encompasses the inclusion of external service providers in the overall process {{as well as the}} usage of external infrastructures like clouds. Both of these approaches lead to decentralization and outsourcing of a part of the global workflow, resulting in a complexified management of the global orchestration. As a matter of fact, the overall management data are decentralized among different domains and must, most of the time, be gathered manually. This paper presents a framework that eases multi-domain orchestration management. Our approach extracts, gathers and <b>digests</b> <b>data</b> from the decentralized processes in order to provide an unified and global view of a distributed orchestration...|$|R
40|$|This paper {{proposes a}} robust {{statistical}} framework to extract highlights from a baseball broadcast video. We applied multistream Hidden Markov Models (HMMs) {{to control the}} weights among different features. To achieve robustness against new highlights, we used a common simple structure for all the HMMs. In addition, scene segmentation and unsupervised adaptation were applied to achieve more robustness against the differences of environmental conditions among games. The precision rate of highlight extracting experiments for eight kinds of highlights from 4. 5 hours of <b>digest</b> <b>data</b> was 77. 4 % and was increased to 78. 7 % by applying scene segmentation. Futhermore, the unsupervised adaptation method improved precision by 2. 7 points to 81. 4 %. These results confirm the effectiveness of our framework...|$|R
40|$|The Partial Digest problem {{asks for}} the {{coordinates}} of m points {{on a line}} such that the pairwise distances of the points form a given multiset of � m � 2 distances. Partial Digest is a well-studied problem with important applications in physical mapping of DNA molecules. Its computational complexity status is open. Input <b>data</b> for Partial <b>Digest</b> from real-life experiments are always prone to error, which suggests to study variations of Partial Digest that take this fact into account. In this paper, we study the computational complexity of Partial Digest variants that model three different error types that can occur in the data: additional distances, missing distances, and erroneous fragment lengths. We show that these variations are NP-hard, hard to approximate, and strongly NP-hard, respectively...|$|R
40|$|Gross primary {{production}} (GPP) and community respiration (R) are increasingly calculated from high-frequency measurements of dissolved O 2 (DO) by fitting dynamic metabolic models to the observed DO time series. Since {{different combinations of}} metabolic components result in the same DO time series, theoretical problems burden this inverse modeling approach. Bayesian parameter inference could improve identification of processes by including independent knowledge in the estimation procedure. This, however, requires model development, because parameters of existing metabolic models are too abstract to achieve a significant improvement. As algal biomass is a key determinant of GPP and R, and high-frequency data on phytoplankton biomass are increasingly available, coupling DO and biomass time series within a Bayesian framework has a high potential to support identification of individual metabolic components. We demonstrate this in three lakes where both high frequency DO and chlorophyll fluorescence data were available. Phytoplankton <b>data</b> were <b>digested</b> via a sequential Bayesian learning procedure coupled with an error model that accounted for systematic errors caused by structural deficiencies of the metabolic model. This method provided ecologically coherent and therefore presumably robust estimates for biomass-specific metabolic rates. This can contribute {{to a better understanding}} of metabolic responses to natural and anthropogenic changes...|$|R
40|$|Abstract—An {{efficient}} {{digital watermarking}} scheme to transmit the medical image which embeds an encrypted data is proposed in this paper. We substitute the non-significant LSB bitplane {{of the image}} with encrypted data composed of the patient <b>data</b> and a <b>digest.</b> The latter is composed of two numbers representing the sum of detected image edge pixels without LSB bitplane, using Canny and Laplacien of Gaussian (LoG) operators respectively. On the receiving side, after decrypting data contained in the LSB bitplane, {{a comparison of the}} digest number saved on the watermarked image and the digest computed on the received image without LSB bitplane. The equality of the two digests, saved in the LSB bitplane and computed on the received image, proves the integrity and the authenticity of the medical image...|$|R
40|$|Abstract — In Unattended Wireless Sensor Networks(UWSNs), sensed {{data are}} stored locally {{for a long}} term till to collector’s retrieval. It is {{motivated}} by the scenarios where only historical information or <b>digest</b> <b>data,</b> not real-time data, are of interest. Such paradigm indeed has been attracted more and more interests in research communities recently. Data storage in UWSNs should be dependable to defend random failure or node compromise, {{as well as the}} efficiency of communication and storage should be maintained. In this paper, we propose a dependable and efficient data survival scheme to maximize the data survival degree upon data retrieval. Our scheme makes use of computational secret sharing to achieve fault tolerance and compromise resilience, and takes advantages of network coding to further improve communication efficiency. As justified by our extensive analysis, the proposed scheme has the most advantages in terms of robustness and lightweight...|$|R
40|$|Administrators survey, 6 {{percent of}} school districts closed or {{consolidated}} schools during the 2008 - 2009 school year, double {{the rate of}} the previous year. Another 11 percent are considering school closings or consolidations in 2010 - 2011. According to <b>data</b> from the <b>Digest</b> of Educational Statistics, total enrollments in PK– 12 increased 11. 5 % between 1993 – 2006, with public school enrollments increasing 13. 4 % and private schools decreasing slightly by 1. 2 % during this period. The same data projected total enrollments to increase an additional 8. 3 % by 2018, with public school increasing 9. 5 %, and private schools decreasing 1. 8 % during this period. These data are displayed in Figure 1. During the past year, school districts around the nation have announced major school closings. Here are some examples of school districts that have recently closed school buildings...|$|R
40|$|We propose an {{algorithm}} which efficiently constructs {{a physical}} map of DNA from a fingerprinted library of random clones. The algorithm clusters clones into contiguous regions using restriction enzyme <b>digest</b> <b>data.</b> Clones which contain overlapping genomic regions are detected and are grouped into {{an island in}} such a way that disjoint islands which can be linked by the clones are joined together automatically. This approach solves a common problem in fingerprinting [...] linking disjoint islands becomes more difficult and slow as mapping goes on. A map is produced in O(km 2 n) time where k is the number of cleavage sites of a clone, m is the number of clones in a library, and n is the number of libraries. Our approach provides a general and efficient means for constructing long-range physical maps in large-scale genome mapping. key words: physical map, fingerprinting, restriction enzymes, DNA 2 1 Introduction The molecular characterization of a genome involves the construction of a ph [...] ...|$|R
40|$|Accurately {{measured}} peptide masses can be {{used for}} large-scale protein identification from bacterial whole-cell digests as an alternative to tandem mass spectrometry (MS/MS) provided mass measurement errors of a few parts-per-million (ppm) are obtained. Fourier transform ion cyclotron resonance (FTICR) mass spectrometry (MS) routinely achieves such mass accuracy either with internal calibration or by regulating the charge in the analyzer cell. We have developed a novel and automated method for internal calibration of liquid chromatography (LC) /FTICR <b>data</b> from whole-cell <b>digests</b> using peptides in the sample identified by concurrent MS/MS together with ambient polydimethyl-cyclosiloxanes as internal calibrants in the mass spectra. The method reduced mass measurement error from 4. 3 +/- 3. 7 ppm to 0. 3 +/- 2. 3 ppm in an E. coli LC/FTICR dataset of 1000 MS and MS/MS spectra and is applicable to all analyses of complex protein digests by FTICRMS. Copyright (c) 2006 John Wiley & Sons, Ltd...|$|R
40|$|AbstractData is a {{valuable}} asset to our society. Effective use of data can enhance productivity of business and create economic benefit to customers. However with data growing at unprecedented rates, organisations are struggling {{to take full advantage}} of available data. One main reason for this is that data is usually originated from disparate sources. This can result in data heterogeneity, and prevent <b>data</b> from being <b>digested</b> easily. Among other techniques developed, ontology based approaches is one promising method for overcoming heterogeneity and improving data interoperability. This paper contributes a formal and semi-automated approach for ontology development based on Formal Concept Analysis (FCA), with the aim to integrate data that exhibits implicit and ambiguous information. A case study has been carried out on several non-trivial industrial datasets, and our experimental results demonstrate that proposed method offers an effective mechanism that enables organisations to interrogate and curate heterogeneous data, and to create the knowledge that meets the need of business...|$|R
40|$|College {{students}} select majors for {{a variety}} of reasons, including expected returns in the labor market. This paper demonstrates an empirical method linking a census of US degrees and fields of study with measures of the knowledge content of jobs. The study combines individual wage and employment data from the Current Population Survey (CPS) with ratings on 27 knowledge content areas from the Occupational Information Network (O*NET), thus providing measures of the economy-wide knowledge content of jobs. Fields of study and corresponding BA degree <b>data</b> from the <b>Digest</b> of Education Statistics for 1976 - 1977 through 2001 - 2002 are linked to these 27 content areas. We find that the choice of college major is responsive to changes in the knowledge composition of jobs and, more problematically, the wage returns to types of knowledge. Women's degree responsiveness to knowledge content appears to be stronger than men's, but their response to wage returns is weak. ...|$|R
40|$|With {{abundant}} aggregate network bandwidth, {{continuous data}} streams {{are commonly used}} in scientific and commercial applications. Correspondingly, there is an increasing demand of authenticating these data streams. Existing strategies explore data stream authentication by using message authentication codes (MACs) on {{a certain number of}} data packets (a data block) to generate a message digest, then either embedding the digest into the original data, or sending the digest out-of-band to the receiver. Embedding approaches inevitably change the original data, which is not acceptable under some circumstances (e. g., when sensitive information is included in the <b>data).</b> Sending the <b>digest</b> out-of-band incurs additional communication overhead, which consumes more critical resources (e. g., power in wireless devices for receiving information) besides network bandwidth. In this paper, we propose a novel strategy, DaTA, which effectively authenticates data streams by selectively adjusting some interpacket delay. This authentication scheme requires no change to the original data and no additional communication overhead. Modeling-based analysis and experiments conducted on an implemented prototype system in an LAN and over the Internet show that our proposed scheme is efficient and practical...|$|R
50|$|During the {{organizational}} and inspecting meetings, thirty formal sessions {{were held in}} addition to informal meetings and conferences. At several of these sessions the entire Commission were present; at no session were there fewer than five Commissioners; the average attendance was over seven. During the meetings for report preparation there were 27 sessions, with an average attendance of seven. Experts on matters entrusted to the Commission were present by invitation at 24 sessions; of these experts there were 24, several of whom attended two or more sessions, with most being former or current attaches of the Corps of Engineers. Outside formal sessions, the Commissioners devoted much time to the consideration of the waterways and related matters. Two or three Commissioners jointly inspected the upper Missouri, the Columbia and Snake, the Sacramento and San Joaquin, and major tributaries; several Commissioners employed agencies under their direction in collating and <b>digesting</b> <b>data</b> relating to canals, water transportation, etc. Most of the Commissioners attended conventions and other meetings connected {{with the development of}} waterways and related interests.|$|R
