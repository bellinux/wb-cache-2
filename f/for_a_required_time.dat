4|10000|Public
40|$|Anycast routing {{offers a}} {{flexible}} paradigm for resource constrained, ad hoc sensor networks, where sensors can route data to any available server. The selection policy of servers in an anycast scenario could assist {{in keeping the}} network operating quiescently <b>for</b> <b>a</b> <b>required</b> <b>time</b> frame. This paper proposes a novel load balancing model to make anycast routing decisions based on route efficiency, and remaining life-times of the servers. The model makes use of the historical load and channel capacity profiles to accurately estimate the remaining life of each server. The {{results show that the}} proposed technique achieves effective reconfiguration performance to meet the network time-to-live requirement and also maintains better load-balance with anycast routing protoco...|$|E
40|$|SYNOPSIS: The {{excavation}} of an underground metro frequently involves materials of poor geotechnical characteristics. Under certain conditions, {{improvement of the}} strength and stiffness of the ground is recommended to facilitate the excavation. Drainage of silty/clayey layers is generally reckoned as an efficient action before excavation. Boreholes for drainage can be drilled and activated <b>for</b> <b>a</b> <b>required</b> <b>time.</b> Dissipation of pore pressures {{may lead to a}} sufficient increase of short-term resistance, {{which is going to be}} mobilized during excavation. However, drainage may induce surface settlements, which can affect the buildings in the area of the construction works. In order to compensate these settlements grout injections can be carried out to produce an artificial volumetric increase. Sometimes the propagation may result uncontrolled as a consequence of the heterogeneity of the soil mass. Therefore, especially when historical buildings are involved, volume-controlled grouting may necessitate avoiding damage. As innovative solution, grout can be injected along a rod inflating pockets of special tissue and a prescribed volumetric expansion can be given. In this note the results of a numerical analysis are illustrated to demonstrate the efficiency of volume-controlled grouting with reference to an hypothetical case where enlargement of an existing tunnel section is planned. 1...|$|E
40|$|The {{miniaturization}} of electronics, {{sensors and}} actuators has enabled the growing use of CubeSats and sub- 20 kg spacecraft. Their reduced mass and volume {{has the potential}} to translate into significant reductions in required propellant and launch mass for interplanetary missions, earth observation and for astrophysics applications. There is an important need to optimize the design of these spacecraft to better ascertain their maximal capabilities by finding optimized solution, where mass, volume and power is a premium. Current spacecraft design methods require a team of experts, who use their engineering experience and judgement to develop a spacecraft design. Such an approach can miss innovative designs not thought of by a human design team. In this work we present a compelling alternative approach that extends the capabilities of a spacecraft engineering design team to search for and identify near-optimal solutions using machine learning. The approach enables automated design of a spacecraft that requires specifying quantitative goals, requiring reaching a target location or operating at a predetermined orbit <b>for</b> <b>a</b> <b>required</b> <b>time.</b> Next a virtual warehouse of components is specified that be selected to produce a candidate design. Candidate designs are produced using an artificial Darwinian approach, where fittest design survives and reproduce, while unfit individuals are culled off. Our past work in space robotic has produced systems designs and controllers that are human competitive. Finding a near-optimal solution presents vast improvements over a solution obtained through engineering judgment and point design alone. The approach shows a credible pathway to identify and evaluate many more candidate designs than it would be otherwise possible with a human design team alone. Comment: 6 pages, 11 figures, Proceedings of the International Astronautical Congress, 201...|$|E
3000|$|... 1 has <b>an</b> {{invocation}} history <b>for</b> <b>a</b> <b>required</b> response <b>time</b> of 2 s {{and we have}} a given workload configuration, we {{can test}} the compliance of the configuration with a pattern using the matrix.|$|R
3000|$|... 2 has no {{invocation}} history <b>for</b> <b>a</b> <b>required</b> response <b>time</b> of 2 s, we can utilise {{collaborative filtering}} for {{the prediction of}} settings, i.e., use similar services to determine patterns for the given service [11, 12].|$|R
60|$|This was one {{of those}} vast propositions which Mr Willet had never {{considered}} <b>for</b> <b>an</b> instant, and <b>required</b> <b>time</b> to 'tackle.' Wherefore he made no answer.|$|R
40|$|Abstract In this thesis, alkali {{activation}} of fayalite slag was investigated. The slag utilized {{is a waste}} by product from nickel production, the {{activation of}} {{which resulted in the}} formation of a geopolymer binder of acceptable properties. The need for a reduction in overall waste output as well as the cementitious properties exhibited by this slag motivated this study. The literature section of this research provides an insight on the previous work in the area of geopolymerization, the source materials used, the activators employed as well as the properties exhibited by various geopolymer products. The experimental section reports the particle size distribution and particle size optimization as well as geopolymer synthesis. The size reduction was carried out by milling <b>for</b> <b>a</b> <b>required</b> <b>time</b> while the particle size optimization and mix design was done with “Elkem Material Mixture Analyser (EMMA). The milling reduced the particle size of fayalite slag to 10. 08 µm after 3 hours which is sufficient fineness for most geopolymer precursors. Different mix compositions were activated with potassium silicate and sodium hydroxide at varying mass ratio with the one activated with 50 : 50 mass ratio of NaOH /K₂SiO₃ had the highest mechanical strength. The workability and setting time were good for the mixtures with liquid to solid ratio from 0. 1 – 0. 2. Compressive strength test were carried out on various mix compositions and the result shows the maximum compressive strength of 16. 5 MPa was achieved at room temperature after 28 days. The water absorption was very low (5 %). It was also noticed that the strength after water absorption was higher than the unconfined compressive strength due to subjection of slag geopolymer to elevated temperature before water absorption. The alkali activation of fayalite slag has shown promising properties which could further be improved for better mechanical performance...|$|E
50|$|The {{book was}} <b>for</b> <b>a</b> long <b>time</b> <b>required</b> reading in {{secondary}} schools in Ireland. As a book with arguably sombre themes (its latter half cataloguing a string of family misfortunes), its presence on the Irish syllabus was criticised for some years.|$|R
40|$|Flight {{management}} systems and control methods <b>for</b> meeting <b>a</b> <b>required</b> <b>time</b> of arrival (RTA) with reduced fuel burn. An example method {{can account for}} probabilistic wind forecast uncertainty in RTA calculations by reformulating the speed and thrust profile problem as a multi-stage stochastic program, using a wind forecast uncertainty model to generate scenario sets for the fuel optimization problem. The method can iteratively calculate a fuel-efficient advised air speed <b>for</b> achieving <b>an</b> RTA over a flight path with an arbitrary number of recourse points. Georgia Tech Research Corporatio...|$|R
5000|$|Like many ranchos east of San Diego it was {{attacked}} by the Kumeyaay in 1837 and abandoned <b>for</b> <b>a</b> <b>time</b> <b>requiring</b> <b>a</b> new grant to be made later. It was formally granted to Juan Ignacio Lopez in June 11, 1840 as Rancho Toljol. [...] The Expediente for Rancho Joljol successfully submitted, formal title to the grant was recorded in July 12, 1840, however it was strangely recorded as Toljol.|$|R
5000|$|In {{comparison}} to flash, {{the advantages are}} much more obvious. Whereas the read operation {{is likely to be}} similar in performance, the charge pump used <b>for</b> writing <b>requires</b> <b>a</b> considerable <b>time</b> to [...] "build up" [...] current, a process that FeRAM does not need. Flash memories commonly need a millisecond or more to complete a write, whereas current FeRAMs may complete a write in less than 150 ns.|$|R
50|$|The Green Goblin {{developed}} a gas that temporarily deadened all of Spider-Man's powers, especially his spider-sense. This same gas was later enhanced {{and used by}} Roderick Kingsley, in his guise as the villain Hobgoblin. A variant of this attack was used by Phil Urich as the Hobgoblin, causing Spider-Man's spider-sense to be so sensitive that he was alerted to even minor 'threats' such as staplers <b>for</b> <b>a</b> <b>time,</b> <b>requiring</b> him to concentrate far more to determine what was or wasn't a true threat.|$|R
40|$|It is shown, {{under the}} {{assumption}} of possibility to perform an arbitrary local operation, that all nonlocal variables related to two or more separate sites can be measured instantaneously, except <b>for</b> <b>a</b> finite <b>time</b> <b>required</b> <b>for</b> bringing to one location the classical records from these sites which yield {{the result of the}} measurement. It is a verification measurement: it yields reliably the eigenvalues of the nonlocal variables, but it does not prepare the eigenstates of the system. Comment: 4 pages, revised version, to be published in PR...|$|R
40|$|Modern {{complex and}} {{information}} intensive computer applications involve multi-window operations. To simultaneously visualize {{all the necessary}} information <b>for</b> <b>a</b> task <b>requires</b> <b>time</b> [...] consuming window management operations by the user. Since these activities are {{not directly related to}} the user's task domain, time spent on window management results in loss of user's mental context and an increase in the actual task completion time. We propose an Adaptive Window Manager (AWM) which automates the layout of the windows on the display screen according to the current user and his current task domain and learns the user's layout requirements. In this paper, we shortly address what user model we used for building AWM. KEYWORDS Adaptive Window Management, Adaptive User Interfaces, User Model 1. Introduction With an ever [...] increasing rate of advances in computers, a user is burdened with a cluttered multi [...] window desktop and has to deal with rearranging the display layout (that is, positioning an [...] ...|$|R
40|$|Some applications, like {{medical or}} {{satellite}} imagery systems <b>for</b> example, <b>require</b> <b>a</b> real <b>time</b> {{implementation of the}} JPEG 2000 coder. This paper describes a possible implementation on a platform based on the MPC 74 XX processor, and <b>a</b> complexity study <b>for</b> this platform is presented. After a brief summary of the different components of a JPEG 2000 coder, {{a description of the}} MPC 74 XX processor and an example will be given to illustrate its possibilities. Then some measurements and evaluations of the complexity of JPEG 2000 will be described. 1...|$|R
5000|$|In {{order to}} prevent {{the loss of the}} experts, Africa has {{observed}} the [...] "friends and relatives effect", which identifies professional, societal and personal factors as the three imperatives underlying the decision of African students in the United States to return home. In addition, the most widespread instrument used by African countries to combat the brain drain is bonding, which obligates a graduate to return home <b>for</b> <b>a</b> <b>required</b> period of <b>time</b> before s/he can emigrate or to also have bilateral agreements with developed countries, which will require them to return home immediately upon graduation. These approaches are influenced from the policies that exist and worked in Asian countries.|$|R
5000|$|Generally, bread flours {{are high}} in gluten (hard wheat); pastry flours have a lower gluten content. Kneading {{promotes}} the formation of gluten strands and cross-links, creating baked products that are chewier (in contrast to crumbly). The [...] "chewiness" [...] increases as the dough is kneaded for longer times. An increased moisture content in the dough enhances gluten development, and very wet doughs left to rise <b>for</b> <b>a</b> long <b>time</b> <b>require</b> no kneading (see no-knead bread). Shortening inhibits formation of cross-links and is used, along with diminished water and less kneading, when a tender and flaky product, such as a pie crust, is desired.|$|R
40|$|Gameplay is {{commonly}} {{considered to be}} a voluntary activity. Game designers generally believe that voluntary gameplay is essentially different from mandatory gameplay. Such a belief may be <b>a</b> challenge <b>for</b> serious games, as instruction is usually mandatory. The article describes the outcomes of two experiments on the impact of voluntariness on the learning effect and enjoyment of a serious game. In the first experiment freedom of choosing to play a serious game was studied, with participants who had volunteered to participate. The results suggested that, contrary to the opinion of many game designers, being <b>required</b> to play <b>a</b> serious game does not automatically take the fun out of the game. The second experiment had voluntary participants and mandatory participants, who had to participate as part of a homework assignment. The outcomes show that mandatory participants enjoyed the game as much as the voluntary participants, even if they had to play the game <b>for</b> <b>a</b> minimum <b>required</b> <b>time.</b> These studies indicate that mandatory gameplay does not reduce enjoyment and learning effect. </p...|$|R
40|$|There is <b>a</b> need <b>for</b> <b>a</b> fuel-optimal <b>required</b> <b>time</b> {{of arrival}} (RTA) mode for {{aircraft}} flight management systems capable of enabling controlled {{time of arrival}} functionality {{in the presence of}} wind speed forecast uncertainty. A computationally tractable two-stage stochastic algorithm utilizing a data-driven, location-specific forecast uncertainty model to generate forecast uncertainty scenarios is proposed as a solution. Three years of Aircraft Communications Addressing and Reporting Systems (ACARS) wind speed reports are used in conjunction with corresponding wind speed forecasts from the Rapid Update Cycle (RUC) forecast product to construct an inhomogeneous Markov model quantifying forecast uncertainty characteristics along specific route through the national airspace system. The forecast uncertainty modeling methodology addresses previously unanswered questions regarding the regional uncertainty characteristics of the RUC model, and realizations of the model demonstrate a clear tendency of the RUC product to be positively biased along routes following the normal contours of the jet stream. A two-stage stochastic algorithm is then developed to calculate the fuel optimal stage one cruise speed given <b>a</b> <b>required</b> <b>time</b> of arrival at a destination waypoint and wind forecast uncertainty scenarios generated using the inhomogeneous Markov model. The algorithm utilizes a quadratic approximation of aircraft fuel flow rate as a function of cruising Mach number to quickly search for the fuel-minimum stage one cruise speed while keeping computational footprint small and ensuring RTA adherence. Compared to standard approaches to the problem utilizing large scale linear programming approximations, the algorithm performs significantly better from a computational complexity standpoint, providing solutions in fractional power time while maintaining computational tractability in on-board systems. Ph. D...|$|R
40|$|Abstract: Suppose we need {{to watch}} a set of targets {{continuously}} <b>for</b> <b>a</b> <b>required</b> period of <b>time,</b> and suppose we choose any number of sensors from a fixed set of sensor types and place them at selected sites. We {{want to find a}} sensor arrangement to achieve the required coverage lifetime such that the total (monetary) cost of the chosen sensors is minimum. This is an NP-hard problem. We approach this problem by modelling it as an Integer Linear Programming problem. We devise a polynomial-time approximation algorithm to this problem with a proven approximation guarantee. We observe that the actual approximation ratios are small in practice. We then present a time-slotted scheduling to produce <b>a</b> timetable <b>for</b> each sensor. We also consider a special case with only one type of sensor that can watch one target at a time. We present <b>a</b> different method <b>for</b> solving this problem...|$|R
50|$|Heathcliff and Cathy {{are inseparable}} and their habit of {{wandering}} unchecked on their beloved moors {{results in an}} accidental injury. This invalids Cathy <b>for</b> <b>a</b> <b>time,</b> <b>requiring</b> <b>a</b> stay at Thrushcross Grange, {{the home of the}} Lintons (the cultured Edgar, and his sister Isabella). This contact with a more refined world seduces Cathy, spurring her to rein in her wild passions. The dazzle of wealth broadens her horizons, and her first taste of gentility results in the careless abandonment of her relationship with Heathcliff. The possibility of <b>a</b> future together <b>for</b> them in <b>a</b> world that <b>required</b> more prudent choices be made is despaired. Her acceptance of Edgar Linton's (Darryl Knock) marriage proposal throws Heathcliff into a tormented rage. In order to avoid daily confrontation with his loss of Cathy and to punish her with his absence, he travels abroad in an effort to improve his life, to match that of her husband.|$|R
40|$|The paper {{considers}} {{distributed applications}} where interactions between constituent services take place via messages in an asynchronous environment with unpredictable communication and processing delays; further, interacting parties {{are not required}} to be online at the same time. Message-oriented middleware (MoM) is commonly used for connecting such loosely coupled distributed applications. Despite loose coupling, many service interactions have temporal and message validation constraints. A failure to deliver a valid message within its time constraint could cause mutually conflicting views of an interaction (one party regarding it as timely whilst the other party regarding it as untimely) leading to application level inconsistencies. In a loosely coupled system, such inconsistencies could remain undetected <b>for</b> <b>a</b> long <b>time,</b> <b>requiring</b> costly application level recovery procedures. This paper describes how synchronisation support providing multilateral consistency guarantees can be provided using the underlying MoM to prevent inconsistencies from reaching application level. 1...|$|R
40|$|Kernel {{means are}} {{frequently}} {{used to represent}} probability distributions in machine learning problems. In particular, the well known kernel density estimator and the kernel mean embedding both have {{the form of a}} kernel mean. Unfortunately, kernel means are faced with scalability issues. A single point evaluation of the kernel density estimator, <b>for</b> example, <b>requires</b> <b>a</b> computation <b>time</b> linear in the training sample size. To address this challenge, we present a method to efficiently construct a sparse approximation of a kernel mean. We do so by first establishing an incoherence-based bound on the approximation error, and then noticing that, for the case of radial kernels, the bound can be minimized by solving the $k$-center problem. The outcome is a linear time construction of a sparse kernel mean, which also lends itself naturally to an automatic sparsity selection scheme. We show the computational gains of our method by looking at three problems involving kernel means: Euclidean embedding of distributions, class proportion estimation, and clustering using the mean-shift algorithm...|$|R
50|$|It {{was after}} {{her return to}} Canada that she wrote The Stone Angel, the book for which she is best known. Set in a fictional Manitoba small town called Manawaka, the novel is narrated {{retrospectively}} by Hagar Shipley, a ninety-year-old woman living in her eldest son’s home in Vancouver. Published in 1964, the novel is of the literary form that looks at the entire life of a person, and Laurence produced a novel from a Canadian experience. After finishing school, the narrator moves from Toronto to Manitoba, and marries a rough-mannered homesteader, Bram Shipley, against the wishes of her father, who then disinherits her — disinheritance is a recurring theme in much of Laurence's fiction. The couple struggles through the economic hardship and climatic challenges of Canadian frontier existence, and Hagar, unhappy in the relationship, leaves Bram, moving with her son John to Vancouver where she works as <b>a</b> domestic <b>for</b> many years, betraying her social class and upbringing. The novel was <b>for</b> <b>a</b> <b>time</b> <b>required</b> reading in many North American school systems and colleges.|$|R
40|$|Recent {{observations}} {{show that}} the CLR 1 is small (a few Θ 10 pc), has large column densities (N ß 10 22 cm Γ 2), and lies close to and is probably photoionized by the AGN. Coronal lines give information on Ne, Mg, Si, S, Ca, Fe, abundances, on the dust content and on {{the dynamics of the}} CLR, and can be used to trace hidden AGN. If photoionized, coronal lines could be used to reconstruct the AGN spectrum at 100 [...] 400 eV. However, a proper understanding of the results require careful use of photoionization codes, since the ionization parameter at the inner face is probably meaningless for CLR models, and a settlement of the `highly variable' atomic parameters <b>for</b> coronal lines. <b>A</b> shock model <b>for</b> the CLR in NGC 1068 needs collisional ionization of 600 M fi /yr of dense (n ? 10 4 cm Γ 3) gas within ! 30 pc. Maintaining a shock excited CLR <b>for</b> <b>a</b> sensible <b>time</b> <b>requires</b> <b>a</b> total fuel of ? 10 10 M fi which must be continuously supplied to the CLR. 1. Introduction Coron [...] ...|$|R
40|$|Our work uses 1080 images {{sequence}} {{obtained from}} "in vitro" samples taken every 4 min from a microscope under phase contrast technique. These images are in JPEG format and are 500 × 700 pixels size with a compression rate of 3 : 1. We developed an algorithm and characterize it over several image operations against the tracking effectiveness and its robustness respect mitosis and cell shape change. Image equalization, dilation and erosion were the image processing procedures founded to provide best tracking results. Equalization procedure, <b>for</b> example, <b>required</b> <b>a</b> <b>time</b> delay of 5 sec <b>for</b> <b>a</b> size target of 60 x 90 pixels and 9 sec for size target of 89 x 100 pixels. This algorithm was implemented into a FPGA which controlled our optical correlator {{in order to}} performance all Fourier operations by optical method. Our {{results showed that the}} use of the optical correlator can reduce the time consuming in the image process until for 90 % which able us to track cells in vascular structure. © 2009 SPIE. SCOPUS: cp. pinfo:eu-repo/semantics/publishe...|$|R
40|$|Spent fuel is an {{inevitable}} residue {{of the production}} of electricity by nuclear power. The main strategies of spent fuel management include the spent fuel direct disposal route and the reprocessing route. Today a new strategy of partitioning and transmutation is being pursued {{for the management of}} long lived actinides and fission products, that can in the future be separated. However, even if this route is successful, it will never be possible to burn all of them in advanced fuel cycles. In the reprocessing route, almost 100 % of the fissile material is separated {{from the rest of the}} fission products and other actinides generated by nuclear transformations. Taken all together these constitute the high level wastes, which are to be conditioned in a glass matrix in view of their final disposal in deep geological formations. The concept of direct disposal of unprocessed spent fuel consists of final disposal, once encapsulated in proper disposal canisters, in the same geological formations as the vitrified wastes. It is important that the conditioned spent fuel and other conditioned high level wastes have a certain stability and maintain it <b>for</b> <b>a</b> <b>required</b> period of <b>time,</b> that is several thousand years. The changes in the physical and chemical characteristics that may happen in contac...|$|R
30|$|Agaricus bisporus is {{the fourth}} {{mushroom}} species cultivated in the world, with 15 % of global production (34  ×  106  t edible mushrooms, Royse et al. 2016). The method usually used to cultivate the white button mushroom was described by Sinden and Hauser (1950) and improved through the years by {{a great number of}} research findings (Wuest 1982). The process involves two composting phases and is used worldwide because of excellent results. Phase I is <b>a</b> composting treatment <b>requiring</b> between 6 and 14  days, according to raw materials used. Phase I helps soften straw and other raw materials, break down soluble sugars and lower the C/N ratio. Biological (bacterial) and chemical (ammonia) activities increase during Phase I composting (Straatsma et al. 1995; Gerrits et al. 1995). This phase brings environmental challenges to growers because of odors and slurries (Mamiro et al. 2007; Beyer 2017). Phase II is a pasteurization process devoted to reduce competitor microbiota and give the substrate its biological selectivity. In Phase II, ammonia is reduced to levels that are non-toxic to A. bisporus (Laborde et al. 1993). Besides environmental problems, the Phase II technology <b>for</b> producing <b>A.</b> bisporus <b>requires</b> <b>time,</b> labor and investment (Miller et al. 1990). The loss of matter during composting is also <b>an</b> argument <b>for</b> the development of mushroom cultivation alternative methods.|$|R
30|$|A {{rectangular}} Reactor {{with external}} dimensions of height =  30  cm, width =  7  cm, length =  11  cm and wall thickness =  10  mm was constructed with glass. The experimental set up {{consisted of a}} glass chamber as a Reactor with a capacity of 2.0  L sample. Each time, the BTMW sample of 2.0  L was collected and placed in an electrolytic cell (Chopra and Sharma 2012). Al–Al electrode combination was connected to their respective anode and cathode leading to the D.C Power supply (LMC electronics, India 0 – 500  V and 0 – 2  A) and energized <b>for</b> <b>a</b> <b>required</b> duration of <b>time</b> at different voltages and currents. All the experiments were performed at room temperature (30  ±  2  °C) and at a constant stirring speed (100  rpm) to maintain the uniform mixing of BTMW sample during the EC. Before conducting the experiment, the electrodes were washed with water, dipped into diluted HCl (5  % v/v) for 5  min, thoroughly washed with water and then finally rinsed twice with distilled water. Electrodes were dipped into BTMW samples with different electrode areas (80, 120 and 160  cm 2) and different inter electrode distances (0.5, 1.0, 1.5, 2.0 and 2.5). The different voltages (5 – 40  V) were applied for different operating times (10 – 80  min). After applying the particular voltage <b>for</b> <b>a</b> particular time period, i.e., after each batch experiment, the treated samples were allowed to settle for different times (30, 60 and 90  min).|$|R
40|$|The {{thermoplastic}} formability (TPF) of {{metallic glasses}} {{was found to}} be related to the calorimetrically measured crystallization temperature minus the glass transition temperature, Tg - Tx = ΔT. Alloy development in the ZrTiBe system identified a composition with ΔT = 120 °C. Many alloys with ΔT > 150 °C and one alloy, Zr 35 Ti 30 Be 27. 5 Cu 7. 5, with ΔT = 165 °C were discovered by substituting Be with small amounts of fourth alloying elements. The viscosity as a function of temperature, η(T), and time temperature transformation (TTT) measurements for the new alloy are presented and combined to create ηTT plots (viscosity time transformation) that are useful in determining what viscosities are available <b>for</b> <b>a</b> <b>required</b> processing <b>time.</b> ηTT plots are created for many alloys used in TPF in the literature and it is found that for processes requiring 60 - 300 s, Zr 35 Ti 30 Be 27. 5 Cu 7. 5 provides an order of magnitude lower viscosity for processing than the other metallic glasses. Injection molding is demonstrated with Zr 35 Ti 30 Be 27. 5 Cu 7. 5 and the part shows improved mechanical properties over die cast specimens of the same geometry. Changes of slope in η(T) measurements were observed and investigated in some quaternary compositions and found to be present in ternary compositions as well. Traditionally metallic glasses show a single discontinuity in heat capacity at the glass transition temperature. Alloys with the changes in slope of η(T) were found to show two discontinuities in heat capacity with the changes in slope of η(T) roughly correlating with the observed Tg values. These two Tg values were assumed to arise from two glassy phases present in the alloy. Further heat capacity analysis found systematic trends in the magnitude of the heat capacity discontinuities with composition and the single phase compositions of a metastable miscibility gap were discovered. Microscopic evidence of the two phases is lacking so we must limit our claims to evidence of two relaxation phenomena existing and can’t definitively claim two phases. The alloy development led to the discovery of alloys with densities near Ti that are among the highest strength to weight ratio materials known. Alloys with corrosion resistances in simulated sea water 10 x greater than other Zr based glasses and commonly used marine metals were discovered. Glasses spanning 6 orders of magnitude in corrosion resistance to 37...|$|R
40|$|Simultaneous {{observation}} of short-period seismic noise {{were carried out}} on the observation points where the distances from the national road and the expresshig highway in Morioka area. Investigation was like-wise conducted {{with respect to the}} ground vibration characteristics excited by the cars and vehicles and their influential range. On the other hand, the investigation was also made to see whether directly-generated noise by vehicular traffic was included in the redults of short-period seismic noise observation, approximately 490 points in Morioka area. Short-period, seismic noise was measured by 3 -component seismometers with a natural period of 1 see. Power spectra were calculated by means of FFT from 2048 degitized data of wave forms. Results of the investigation are summarized as follows: 1) Due to passage of cars and vehicles, ground vibration in the frequency band between 5 Hz and 15 HZ is much exicited and its directly-generated influence is exercised upon a range no less than 300 m from the road. 2) Amplitude and predominant frequency of the short-period seismic noise while none of cars and vehicles pass through make no change in accordance with the distance from the road. 3) With respect to the amplitude of the short-period seismic noise observed in Morioka area at midnight especially while none of cars and vehicles pass through, none of significant difference is seen in either the district close to the road with in 200 m from the trunk-line road where huge traffic volume or the region excluding the said district. Therefore it is suggested that vibration characteristics of the ground peculiar to the site can be observed even in the vicinity of trunk-line road, provided that passage of cars and vehicles is ceased <b>for</b> <b>a</b> settled <b>time</b> <b>required</b> <b>for</b> andysis...|$|R
40|$|Offline {{handwriting}} recognition is usually performed by first extracting {{a sequence of}} features from the image, then using either a hidden Markov model (HMM) [9] or an HMM / neural network hybrid [10] to transcribe the features. However a system trained directly on pixel data has several potential advantages. One is that defining input features suitable <b>for</b> <b>an</b> HMM <b>requires</b> considerable <b>time</b> and expertise. Furthermore, the features must be redesigned for every different al-phabet. In contrast, a system trained on raw images can be applied with equal ease to, for example, Arabic and English. Another potential benefit is that using raw data allows the visual and sequential aspects of {{handwriting recognition}} to be learned together, rather than treated as two separate problems. This kind of ‘end-to-end’ training is often beneficial for machine learning algorithms, since it allows them more freedom {{to adapt to the}} task [13]. Furthermore, recent results suggest that recurrent neural networks (RNNs) may be preferable to HMMs for sequence labelling tasks such as speech [5] and online handwriting recognition [6]. One possible {{reason for this is that}} RNNs are trained discriminatively, whereas HMMs are generative. Although generative approaches offer more insight into the data, discriminative methods tend to perform better at tasks such as classification and labelling, at least when large amounts of data are available [15]. Indeed much work has been in recent years to introduce discrimi-native training to HMMs [11]. Another important difference is that RNNs, unlike HMMs, do not assume successive data points to be conditionally independent given some discrete internal state, which is often unrealistic for cursive handwriting. This chapter will describe an offline handwriting recognition system based on recurrent neural networks. The system is trained directly on raw images, with no manual feature extraction. It won several prizes at the 2009 International Conference on Document Analysis and Recognition, including first place in the offline Arabic handwriting recognition competition [14]...|$|R
40|$|Determining {{the fatigue}} {{properties}} (Manson-Coffin and Ramberg-Osgood parameters) <b>for</b> <b>a</b> steel material <b>requires</b> <b>time</b> consuming and expensive testing. In {{the early stages}} of a design process, it is not feasible to perform this testing. To help solve this problem numerous researchers have developed estimation methods to estimate the Manson-Coffin parameters from monotonic properties data. Additionally, other researchers have compared the results from these various estimation methods for large material classifications. However, a comprehensive comparison of these estimation methods has not been made for steels in different heat treatment states. More accurate results for the best estimation method can be made with smaller classifications, which have more consistent properties. In this research, best estimation methods are determined for six steel heat treatments. In addition to looking at steel heat treatment classifications, the estimation of the Ramberg-Osgood parameters is also examined through the compatibility conditions. Without them, the approach of estimating the fatigue properties using the estimation methods would not be practically useful. Finally, in the comparison of the estimation methods, an appropriate statistical comparison methodology is utilized; multiple contrasts comparison. This methodology is implemented into the comparison of the different estimation methods, by comparing the estimated lives and the experimental lives as a regression so that the entire life range can be considered. The estimation methods can also be utilized to get estimates of the variability of the fatigue properties given the variability of the monotonic properties data, since there is a functional relationship developed between the two sets of material properties. This variability is necessary <b>for</b> <b>a</b> stochastic design process, in order to obtain a more optimally designed component or structure. Overall the estimation methods have a number of practical applications within a fatigue design process. Their use and implementation needs to be supplemented by the appropriate knowledge of their limitations and for what classifications they give the best results. An expert system is developed to summarize this knowledge to assist an engineer. This research aims to provide this knowledge and expands their use to account for variability in fatigue properties for stochastic analysis. 1 yea...|$|R
50|$|In law, a default is {{the failure}} to do {{something}} required by law or to appearat <b>a</b> <b>required</b> <b>time</b> in legal proceedings.|$|R
30|$|Do {{not forget}} {{to include in}} your {{selection}} criteria the availability and timing of resources. Many otherwise successful projects fail because they cannot be completed within <b>a</b> <b>required</b> <b>time</b> period.|$|R
50|$|Where {{the court}} imposes a fine and {{specifies}} <b>a</b> <b>required</b> <b>time</b> period {{to pay the}} fine, the accused may apply to the court <b>for</b> <b>an</b> extension of the period to pay a fine. Courts will usually only grant the variation, however, if the accused has made a reasonable attempt to pay the fine or has <b>a</b> reasonable excuse <b>for</b> failure to do so.|$|R
