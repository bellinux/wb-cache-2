5|104|Public
5000|$|PuTTY 'sshzlib.c': a {{standalone}} implementation, {{capable of}} <b>full</b> <b>decode,</b> but static tree only creation, by Simon Tatham. MIT licensed.|$|E
5000|$|The DVD video {{may then}} be re-encoded by the application's VBR encoder. The user {{is able to}} choose how the {{compression}} is to be distributed across the DVD. DVD Shrink can automatically re-compress video, to as little as 39% of its original size, depending on the aspect ratio of the original DVD (with a corresponding loss in quality) {{to allow it to}} fit on a standard DVD±R(W), a Dual-Layer DVD+R, or any user-defined custom size. When [...] "Deep Analysis" [...] and [...] "Adaptive Error Compensation" [...] options are selected, the quality of the resulting DVD is improved. These options are more time-consuming because DVD Shrink runs through the DVD once doing the analysis, and then again doing the transcoding. The transcoder in DVD Shrink was unusual {{at the time of its}} release in that it employs compressed domain video processing technology to avoid a <b>full</b> <b>decode</b> and re-encode of the video stream. This boosts performance significantly as only part of the video stream is decoded and scraped.|$|E
40|$|A generic {{cooperative}} MIMO BICM {{system is}} described. Achievable rates are computed {{based on the}} extended equivalent binary input channel model of the original BICM system. <b>Full</b> <b>decode</b> and forward is assumed at the relay node. Two types of two-phased transmission/reception protocols are employed to establish orthogonal transmission/reception of the relay node. The achievable rate results are provided for different combinations of modulation orders {{and the number of}} antennas used at the source and relay nodes. Quantitative results provided in this paper could serve as a guide on when to engage cooperative transmission and how to choose proper constellations and puncturing ratios for the practical BICM coded systems. Comparison of the considered BICM system with other possible cooperative coded systems is also crucial that this paper due to lack of space for exposition misses to address...|$|E
5000|$|MPlayer (also with GUI under Windows and Mac OS X): <b>full</b> <b>decoding</b> quality ...|$|R
5000|$|... iTunes 9.2 and iOS 4 include <b>full</b> <b>decoding</b> of HE-AAC v2 {{parametric}} stereo streams.|$|R
50|$|<b>Full</b> <b>decoding</b> is also {{unnecessary}} {{for certain}} editing operations such as cropping, horizontal or vertical flips, or cardinal rotations.|$|R
40|$|Efficient bitrate {{reduction}} of video content {{is necessary in}} order to satisfy the different constraints imposed by decoding devices and transmission networks. Requantization is a fast technique for bitrate reduction, and has been successfully applied for MPEG- 2 bitstreams. Because of the newly introduced intra prediction in H. 264 /AVC, the existing techniques are rendered useless. In this paper we examine requantization transcoding of H. 264 /AVC bitstreams, focusing on the intra 4 x 4 prediction modes. Two architectures are proposed, one in the pixel domain and the other in the frequency domain, that compensate the drift introduced by the requantization of intra 4 x 4 predicted blocks. Experimental results show that these architectures perform approximately equally well as the <b>full</b> <b>decode</b> and recode architecture for low to medium bitrates. Because of the reduced computational complexity of these architectures, in particular the frequency-domain compensation architecture, they are highly suitable for real-time adaptation of video content...|$|E
40|$|IPTV video {{services}} {{are increasingly being}} considered for delivery to mobile devices over broadband wireless access networks. The IPTV streams or channels are multiplexed together for transport across an IP core network prior to distribution across the access network. According {{to the type of}} access network, prior bandwidth constraints exist that restrict the multiplex data-rate. This paper presents a bandwidth allocation scheme based on content complexity to equalize the overall video quality of the IPTV sub-streams, in effect a form of statistical multiplexing. Bandwidth adaptation is achieved through a bank of bit-rate transcoders. Complexity metrics serve to estimate the appropriate bandwidth share for each stream, prior to distribution over a wireless or ADSL access network. These metrics are derived after entropy decoding of the input compressed bit-streams, without the delay resulting from a <b>full</b> <b>decode.</b> Fuzzy-logic control serves to adjust the balance between spatial and temporal coding complexity. The paper examines constant and varying bandwidth scenarios. Experimental results show a significant overall gain in video quality in comparison to a fixed bandwidth allocation...|$|E
50|$|The tile {{structure}} {{for access to}} image regions can also be changed without <b>full</b> <b>decoding</b> and without introducing distortion.|$|R
5000|$|DVMP Basic & DVMP Pro: <b>full</b> <b>decoding</b> quality. Plays AVI (inc DVCPRO25 and DVCAM) and.dv files. Also {{displays}} the DV meta-information (e.g. timecode, date/time, f-stop, shutter speed, gain, white balance etc.) ...|$|R
50|$|MPC-HC as of SVN 2071 {{and higher}} builds {{supports}} WebM playback with internal VP8 decoder based on FFmpeg's code. The <b>full</b> <b>decoding</b> support for WebM {{is available in}} MPC-HC since version 1.4.2499.0.|$|R
50|$|In JPEG XR, <b>full</b> <b>decoding</b> of {{the image}} is {{unnecessary}} for converting an image from a lossless to lossy encoding, reducing the fidelity of a lossy encoding, or reducing the encoded image resolution.|$|R
40|$|In this paper, we {{investigate}} communication {{strategies for the}} multiple access channel with feedback and correlated sources (MACFCS). The MACFCS models a wireless sensor network scenario in which sensors distributed throughout an arbitrary random field collect correlated measurements and transmit them to a common sink. We derive achievable rate regions for the three-node MACFCS. First, we study the strategy when source coding and channel coding are combined, which we term <b>full</b> <b>decoding</b> at sources. Second, we look at several strategies when source coding and channel coding are separated, which we term <b>full</b> <b>decoding</b> at destination. From numerical computations on Gaussian channels, we see that different strategies perform better under certain source correlations and channel setups. ...|$|R
40|$|Abstract—The reductioninbothuser {{and control}} planelatency {{is a major}} goal for next {{generation}} (4 G) cellular networks, specifically the Long Term Evolution-Advanced (LTE-A) standard. At the same time, relay stations, which introduce additional latency into transmissions, {{are seen as a}} potential means to improve cell spectral efficiency and coverage. This paper extends a number of existing relaying schemes tothe case where a stricter latency constraint is applied. These low latency schemes are first evaluated for convolutional codes, for which they quickly approach the performance of <b>full</b> <b>decoding</b> at the relay with little additional latency over memoryless relaying. The schemes are then adapted for the turbo code in the LTE standard and shown to be preferable to <b>full</b> <b>decoding</b> and memoryless relaying for certain LTE-A relay types. I...|$|R
40|$|Abstract — In this paper, we {{investigate}} communication {{strategies for the}} multiple access channel with feedback and correlated sources (MACFCS). The MACFCS models a wireless sensor network scenario in which sensors distributed throughout an arbitrary random field collect correlated measurements and transmit them to a common sink. We derive achievable rate regions for the three-node MACFCS. First, we study the strategy when source coding and channel coding are combined, which we term <b>full</b> <b>decoding</b> at sources. Second, we look at several strategies when source coding and channel coding are separated, which we term <b>full</b> <b>decoding</b> at destination. From numerical computations on Gaussian channels, we see that different strategies perform better under certain source correlations and channel setups. Fig. 1. sources. The 3 -node multiple access channel with feedback and correlated I...|$|R
40|$|The {{existing}} thumbnail extraction method {{generates a}} thumbnail {{by reducing the}} reconstructed frame after a <b>full</b> <b>decoding</b> process. This method is complex and requires considerable time, particularly for higher resolution video. To alleviate these issues, a fast thumbnail extraction method which utilises an intra-prediction mode for high-efficiency video coding (HEVC) is presented. The proposed method reconstructs only the 4 x 4 boundary pixels needed for a thumbnail image based on the intra-prediction mode. Experimental {{results indicate that the}} proposed method significantly reduces the computational complexity and extraction time for a thumbnail image. The visual quality of thumbnail images obtained with this method does not differ significantly from that of images extracted after a <b>full</b> <b>decoding</b> process. Brain Korea 21 plus (BK 21 plus) NRF of Korea - Korean Government (MSIP...|$|R
40|$|We {{consider}} a CDMA system on an additive white Gaussian noise (AWGN) channel with K concurrent users. Each user transmits equal-rate data and employs an LDPC error control code (ECC). Each user’s bits are modulated with a random spreading waveform. These spreading waveforms are furthermore partitioned in M sections {{which are in}} turn interleaved and spread in time. This creates a rate 1 /M repetition code which interfaces the LDPC-ECC with the CDMA channel. This method, called partitioned spreading, significantly improves convergence properties and maximum system load in conjunction with iterative (turbo) detection. Different decoding iteration schedules, viz. a low-complexity two-stage schedule which separates CDMA detection and LDPC decoding into two successive processes, and a full iteration schedule (<b>full</b> <b>decoding)</b> which invokes all message exchanges in a parallel decoding approach are presented and analyzed. We show that, under certain power and rate conditions, two-stage decoding achieves virtually identical performance as <b>full</b> <b>decoding.</b> I...|$|R
40|$|This paper {{presents}} a fast thumbnail extraction method for HEVC. The proposed method reconstructs only the 4 × 4 boundary pixels requested for a thumbnail image. We {{found that the}} proposed partial decoding method reduced decoding time significantly compared to the conventional method. In addition, the visual quality of the obtained thumbnail images was nearly identical to that of thumbnail images extracted after a <b>full</b> <b>decoding</b> process...|$|R
40|$|In this paper, some fast feature {{extraction}} algorithms are addressed for joint retrieval of images compressed in JPEG and JPEG 2000 formats. In {{order to avoid}} <b>full</b> <b>decoding,</b> three fast algorithms that convert block-based discrete cosine transform (BDCT) into wavelet transform are developed, so that wavelet-based features can be extracted from JPEG images as in JPEG 2000 images. The first algorithm exploits the similarity between the BDCT and the wavelet packet transform. For {{the second and third}} algorithms, the first algorithm or an existing algorithm known as multiresolution reordering is first applied to obtain bandpass subbands at fine scales and the lowpass subband. Then for the subbands at the coarse scale, a new filter bank structure is developed to reduce the mismatch in low frequency features. Compared with the extraction based on <b>full</b> <b>decoding,</b> there is more than 72 % reduction in computational complexity. Retrieval experiments also show that the three proposed algorithms can achieve higher precision and recall than the multiresolution reordering, especially around the typical range of compression ratio. Department of Electronic and Information Engineerin...|$|R
50|$|DeepProbe is an {{intelligent}} passive <b>full</b> <b>decoding</b> probe, {{often used in}} distributed surveillance environments, which are typically large, complex networks, or networks requiring significant application-level monitoring. Examples of such networks would be nationwide surveillance solutions with probes installed at the key service providers and gateways, critical, secure government networks, or large communications service providers. DeepProbe functions as a passive monitoring system, generally {{under the control of}} a separate surveillance element such as a mediation system or a Security Information and Event Management system.|$|R
3000|$|The goal of {{this step}} is to refine the {{detection}} achieved at the first level. Speech segments that passed the filtering process are submitted to the ASR system for a <b>full</b> <b>decoding</b> pass. In order {{to be sure that}} the speech segment contains the full targeted speech utterance even if only a part of the phonetic string is spotted (due to smart queries), we enlarge the segment before and after the spotted area. In our experiments, we used an offset of [...]...|$|R
40|$|The {{increasing}} {{availability of}} forensic audio surveillance recordings covering {{days or weeks}} of time makes human audition impractical. This paper describes the rationale and potential application of several techniques for high-speed automated search and classification of sound sources and sound events in long-term forensic audio recordings. Methods that can operate directly on perceptually compressed bitstreams without <b>full</b> <b>decoding</b> are of particular interest. Example applications include identification of aircraft overflights, the presence of human speech utterances, gunshot sounds, and other forensically relevant audio signals...|$|R
30|$|The coded {{bit-error rate}} (BER) {{performance}} of the proposed detection algorithm was evaluated through simulations of a MIMO system employing four transmit and four receive antennas. The channel encoder {{is based on the}} LTE turbo encoder specification [52] with interleaver length 1, 024, using 16 -QAM and 64 -QAM modulation constellations. The channel entries are assumed to be independent and identically distributed (i.i.d.) complex Gaussian random variables. At the receiver end, we assume perfect channel knowledge. The turbo decoder implements the true A Posteriori Probability algorithm, and performs four (<b>full)</b> <b>decoding</b> iterations.|$|R
30|$|Researchers in {{watermarking}} domain {{have focused}} their works on two fundamental issues: watermark detection and watermark decoding (extraction). In the latter, usually {{referred to as}} multibit watermarking, a <b>full</b> <b>decoding</b> is carried out to extract the hidden message, which can be an ownership identifiers, transaction dates, a serial numbers, and so forth. Such a watermarking {{can be found in}} fingerprinting, steganography, and the protection of intellectual property rights. In multibit watermarking, errors may occur when extracting the hidden message. Error probability {{can be used as a}} measure of the watermarking system performance.|$|R
40|$|A {{two stage}} {{decoding}} scheme {{for use with}} the Nordstrom Robinson code is proposed which partitions the code {{into a number of}} subsets. The subset containing the codeword can be found using a simple Reed Muller decoder. Soft decision decoding can be performed at both stages with near maximum likelihood performance but decoding complexity of about one third of that of a <b>full</b> <b>decoding.</b> The performance of the decoder is also compared to that the 15, 7, 5 BCH and is found to be comparable but with approximately half the capacity...|$|R
50|$|The {{compiler}} {{was used}} on the Countess to produce a version for the CDC 1604, and later self-compiled on that machine. Many other versions were produced for commercial computer products like the UNIVAC 1107, UNIVAC 490 and UNIVAC 418 including versions for IBM 704 and 709. The production version of NELIAC was a second generation system (for the USQ-20, a modernized and militarized version of the USQ-17), compiled by the first version, but including <b>full</b> <b>decoding</b> of algorithmic expressions and, later, an input-output system missing on all other versions.|$|R
40|$|Indexing {{and editing}} digital video {{directly}} in the compressed domain offer many advantages in terms of storage efficiency and processing speed. We have designed automatic tools in the compressed domain for extracting key visual features such as scene cut, dissolve, camera operations (zoom, pan), and moving object detection and tracking. In addition, we have developed algorithms to solve the decoder buffer control problems and allow users to “cut, copy and paste ” arbitrary compressed video segments {{directly in the}} compressed domain. The compressed-domain approach does not require <b>full</b> <b>decoding.</b> Thus fast software implementations can be achieved. Our compressed video editing techniques will enhance the reusability of existing compressed videos...|$|R
40|$|Video {{surveillance}} is ubiquitous and {{it produces}} {{a large amount of}} visual data H. 264 /AVC is currently the most advanced and the most popular video coding standard Automatic and efficient moving object segmentation and tracking can facilitate the content analysis and management of surveillance videos RELATED WORK Pixel domain methods • Carry out a <b>full</b> <b>decoding</b> • Rely on raw pixels for further analysis Compressed domain methods • Carry out a partial decoding • Analyze the information produced by video encoder, e. g. motion vectors, transform coefficients, and prediction modes Shortcomings of existing methods • Encode with small intra frame period and thus compromise the coding efficiency • Rarely take advantage of the prediction mode...|$|R
40|$|For {{applications}} involving video streaming, <b>full</b> <b>decoding</b> {{is usually}} not acceptable for quality assessment. To address the inherent challenges, an efficient method for coding distortion assessment is proposed in this paper. Building on empirical analysis, the proposed method employs a linear model to assess the coding distortion using the quantization scale. Furthermore, {{the characteristics of the}} human visual system are exploited by taking into account the spatial and temporal masking. To estimate the required spatial and temporal complexities in absence of sufficient information, a rate-distortion model is theoretically derived to formulate their relationship with the coding bit-rate. Extensive experimental results have demonstrated the effectiveness of the proposed method for quality assessment with respect to perceived coding distortion. </p...|$|R
5000|$|SCART {{enables a}} device to command the TV to very quickly switch between signals, {{in order to create}} {{overlays}} in the image. In order to implement captioning or subtitles, a SCART set-top box does not have to process and send back a complete new video signal, which would require <b>full</b> <b>decoding</b> and re-encoding of the color information, a signal-degrading and costly process, especially given the presence of different standards in Europe. The box can instead ask the TV to stop displaying the normal signal and display a signal it generates internally for selected image areas, with pixel-level granularity. This can also be driven by the use of a [...] "transparent" [...] color in a teletext page.|$|R
40|$|Abstract—For {{applications}} involving video streaming, <b>full</b> <b>decoding</b> {{is usually}} not acceptable for quality assessment. To address the inherent challenges, an efficient method for coding distortion assessment is proposed in this paper. Building on empirical analysis, the proposed method employs a linear model to assess the coding distortion using the quantization scale. Furthermore, {{the characteristics of the}} human visual system are exploited by taking into account the spatial and temporal masking. To estimate the required spatial and temporal complexities in absence of sufficient information, a rate-distortion model is theoretically derived to formulate their relationship with the coding bit-rate. Extensive experimental results have demonstrated the effectiveness of the proposed method for quality assessment with respect to perceived coding distortion. Index Terms—coding distortion, video quality assessment, streaming video I...|$|R
40|$|An {{approach}} to construct irregular repeat accumulate (RA) codes from circulant permutation matrices is presented. The constructed irregular RA codes can be encoded with shift registers and decoded in the turbo decoding fashion, {{which leads to}} a much faster convergence rate compared to the conventional <b>full</b> parallel <b>decoding</b> fashion...|$|R
40|$|Abstract—This paper {{studies the}} {{performance}} limits of two-way relay channel (TWRC) at finite signal-to-noise ratio (SNR) in Rayleigh fading environment. A two-phase decode-and-forward (DF) protocol is considered. We first derive closed-form ex-pressions for both outage probability and diversity-multiplexing tradeoff (DMT). Our results are general and suitable for any time sharing and any rate allocation in the two-way relay protocol. It is found that DF outperforms amplify-and-forward (AF) when either multiplexing gain or SNR is small enough, otherwise, DF is inferior to AF {{in term of}} outage probability. Meanwhile, finite-SNR DMT of DF is always {{lower than that of}} AF regardless of SNR due to the additional sum-rate constraint imposed on the relay node for <b>full</b> <b>decoding.</b> Furthermore, the optimum relay location for any given combination of time sharing and rate allocation is presented. I...|$|R
40|$|Speech {{recognition}} {{systems for}} conversational telephone speech require the audio {{data to be}} automatically divided into regions of speech and non-speech. The quality of this audio segmentation affects the recognition accuracy. This paper describes several approaches to segmentation and compares the resulting recogniser performance. It is shown that using Gaussian Mixture Models outperforms an energy-detection method and using the output from the speech recogniser itself increases performance further. An upper bound on possible performance was obtained when deriving a segmentation from a forced alignment of the reference words and this outperformed using manually marked word times. Finally the correlation between an appropriately defined segmentation score and WER is shown to be over 0. 95 across three data sets, suggesting that segmentations can be evaluated directly {{without the need for}} <b>full</b> <b>decoding</b> runs...|$|R
40|$|<b>Full</b> soft <b>decoding</b> {{of product}} codes is optimal, {{but it is}} impractical. Although, the {{performance}} of the one dimension soft decoding is reasonable, it can be improved. We propose a modified algorithm, which improves that performance. The modified algorithm provides 0. 7 dB gain over the old one for AWGN channel...|$|R
40|$|AbstractWe {{examine the}} p-ary codes, for any prime p, {{that can be}} {{obtained}} from incidence matrices and line graphs of the Hamming graphs, H(n,m), obtaining the main parameters of these codes. We show that the codes from the incidence matrices of H(n,m) can be used for <b>full</b> permutation <b>decoding</b> for all m,n≥ 3...|$|R
