22|10000|Public
40|$|Hallucinating <b>high</b> <b>frequency</b> <b>image</b> {{details in}} single image super-resolution is a {{challenging}} task. Traditional super-resolution methods tend to produce oversmoothed output images {{due to the}} ambiguity in mapping between low and high resolution patches. We build on recent success in deep learning based texture synthesis and show that this rich feature space can facilitate successful transfer and synthesis of <b>high</b> <b>frequency</b> <b>image</b> details to improve the visual quality of super-resolution results {{on a wide variety}} of natural textures and images. Comment: 13 pages, 11 figure...|$|E
40|$|In image {{compression}} the main {{challenge is to}} efficiently encode and represent <b>high</b> <b>frequency</b> <b>image</b> structural components such as patterns, edges and textures. In this work, we develop an efficient {{image compression}} scheme based on similar structure block prediction. This so-called similar structure block prediction is motivated by motion prediction in video coding, attempting to find an optimal prediction of structure components within previously encoded image regions...|$|E
40|$|Abstract — In image {{compression}} the key {{challenge is to}} efficiently encode and represent <b>high</b> <b>frequency</b> <b>image</b> structural components such as patterns, edges and textures. In this w ork, w e develop an efficient {{image compression}} scheme based on super-spatial prediction of structural units. This so-called similar structure block prediction is motivated by motion prediction in video coding, attempting to find an optimal prediction of structure components w ithin previously encoded image regions...|$|E
50|$|Alternatively, {{a system}} {{may have no}} {{reconstruction}} filter and simply tolerate some energy being wasted reproducing <b>higher</b> <b>frequency</b> <b>images</b> of the primary signal spectrum.|$|R
40|$|Abstract — Classification of high {{resolution}} SAR images is difficult {{due to the}} strong presence of speckle noise. We propose to use a multiscale decomposition where this decomposition is interpreted {{as a way to}} achieve different trade-off between spatial precision (resolution) and radiometric uncertainty (noise reduction). Classification decisions at large scale are certain but spatially imprecise whereas decisions at {{high resolution}} are uncertain but spatially precise. We first decompose the SAR images in low and <b>high</b> <b>frequency</b> <b>images</b> at different scales using a stationnary wavelet transformation. Then low pass images are classified by maximum likelihood based on a gaussian mixture estimation. Wavelet coefficients in <b>high</b> <b>frequency</b> <b>images</b> enable us to identify stationnary homogeneous regions within the image where classification decisions are expected to be stable across scales. Decisions at different scales are merged using Dempster-shafer theory which gives us an adequate framework to manipulate both uncertainty and imprecision. Finally, resulting multiscale decisions are injected in a stochastic classification algorithm (MPM) as a hidden ”evidential ” Markov random field. The proposed algorithm is evaluated on artificial SAR images. We also propose to filter wavelet coefficients based on the resulting multiscale confidence map. I...|$|R
2500|$|If {{he wants}} to {{calculate}} how fast his twin was aging when the image was transmitted, he adjusts for the Doppler shift. For example, when he receives <b>high</b> <b>frequency</b> <b>images</b> (showing his twin aging rapidly) with frequency , he does not conclude that the twin was aging that rapidly when the image was generated, {{any more than he}} concludes that the siren of an ambulance is emitting the frequency he hears. He knows that the Doppler effect has increased the <b>image</b> <b>frequency</b> by the factor 1 / (1 − v/c). Therefore, he calculates that his twin was aging at the rate of ...|$|R
40|$|Abstract: To {{improve the}} {{efficiency}} of facial expression recognition, this paper puts forward a kind of recognition algo-rithm based on local binary pattern (LBP) and empirical mode decomposition (EMD). First of all, process the empirical mode decomposition into the preprocessing facial image, and bring forward many a high frequency images instead of the original image; then, divide the sub domain of the <b>high</b> <b>frequency</b> <b>image</b> and obtain the sub domain LBP histogram and full face histogram; finally, identify {{the expression of the}} generated LBP histogram. Through the experiment on JAFFE database, it shows that the method is effective for facial expression recognition...|$|E
40|$|Classification Abstract. An {{efficient}} ear recognition {{method by}} weighted wavelet transformation and Bi-Directional {{principal component analysis}} was proposed. First, each ear image was decomposed into four sub-images by wavelet transformation,the four sub-images were low frequency image, vertical detail image,horizontal detail image and <b>high</b> <b>frequency</b> <b>image.</b> Then the low frequency image was decomposed into four sub-images, the four-images were weighted by different coefficients, then,the four sub-images were reconstructed into a image. On this basis,the feature was extraction by the BDPCA method,and then we use the k-Nearest Neighbor Classification to recognition. Experimental {{results show that the}} method have high recognition rate and shorted training time...|$|E
40|$|Object {{movement}} during exposure generates blur. Removing blur {{is challenging}} because {{one has to}} estimate the motion blur, which can spatially vary over the image. Even if the motion is successfully identified, blur removal can be unstable because the blur kernel attenuates <b>high</b> <b>frequency</b> <b>image</b> contents. We {{address the problem of}} removing blur from objects moving at constant velocities in arbitrary 2 D directions. Our solution captures two images of the scene with a parabolic motion in two orthogonal directions. We show that our strategy near-optimally preserves image content, and allows for stable blur inversion. Taking two images of a scene helps us estimate spatially varying object motions. We present a prototype camera and demonstrate successful motion deblurring on real motions. 1...|$|E
40|$|Abstract. In this paper, {{we propose}} a novel learning-based image {{restoration}} scheme for compressed images by suppressing compression artifacts and recovering <b>high</b> <b>frequency</b> components with the priors {{learned from a}} training set of natural images. Specifically, Deblocking is performed to alleviate the blocking artifacts. Moreover, consistency of the primitives is enhanced by estimating the <b>high</b> <b>frequency</b> components, which are simply truncated during quantization. Furthermore, {{with the assumption that}} small image patches in the enhanced and real <b>high</b> <b>frequency</b> <b>images</b> form manifolds with similar local geometry in the corresponding image feature spaces, a neighboring embedding-based mapping strategy is utilized to reconstruct the target <b>high</b> <b>frequency</b> components. And experimental results have demonstrated that the proposed scheme can reproduce higher-quality images in terms of visual quality and PSNR, especially the regions relating to the contours...|$|R
40|$|Abstract: The paper takes {{mathematical}} morphology into locomotive {{image by}} operating the <b>high</b> <b>frequency</b> sub <b>image</b> after wavelet first level decomposition, finding the license rectangular area, extracting the area position by area researching, and then further {{making sure the}} license area by line and row projecting. 1. introduction License location is a process {{to make sure the}} license area which only has front information to extract license area image. Fast and precise license location is an important step to license recognition. There are many algorithms raised by many scholar to fast and precise location license. Many algorithms use license self feature and different background area to locate, which include colorful feature, geometrical characteristic, textural features after processing and operating, the license innate features are fixed and so on. The common location method such as edge detection method, based on license color feature method has common specialty that carries out by scanning whole image. To real time condition the algorithm is hard fitted with speed demand. Based on wavelet first level decomposition extracting <b>high</b> <b>frequency</b> sub <b>image</b> can stand out the license area much more conspicuous. And it only has to scan <b>high</b> <b>frequency</b> sub <b>image</b> to take up the 1 / 4 size o...|$|R
40|$|In {{this paper}} an image {{transmission}} {{system has been}} proposed where Joint Photographic Experts Group (JPEG) algorithm is used as an image coder and Rate Compatible Punctured Convolution (RCPC) channel coder is used for transmission of coded image over wireless channels (AWGN and Rayleigh fading). JPEG bit stream is partitioned into DC and AC bit streams. AC bit stream further classified using edge density property of block. Priorities based Unequal error Protection (UEP) applied to bit stream. Distortion analysis (MSE) is given for proposed image transmission scheme. The simulation results shows reduction in distortion compared to conventional Equal Error Protection (EEP). This proposed algorithm can be applied for low frequency as well as <b>high</b> <b>frequency</b> <b>images...</b>|$|R
40|$|Abstract – Remote sensing {{applications}} like disaster or mass event monitoring {{need the}} acquired data and extracted information {{within a very}} short time span. Airborne sensors can acquire the data quickly and on-board processing combined with data downlink is the fastest possibility to achieve this requirement. For this purpose, a new low-cost airborne frame camera system has been developed at the German Aerospace Center (DLR) named 3 K-camera. The pixel size and swath width range between 15 cm to 50 cm and 2. 5 km to 8 km respectively. Within two minutes an area of approximately 10 km x 8 km can be monitored. Image data are processed onboard on five computers using data from a real time GPS/IMU system including direct georeferencing. Due to <b>high</b> <b>frequency</b> <b>image</b> acquisition (3 images/second) the monitoring of moving objects like vehicles and people is performed allowing wide area detailed traffic monitoring...|$|E
40|$|Scanned {{halftone}} {{images are}} degraded {{for the presence}} of screen patterns. It’s a challenge to automatically detect the halftone images and remove the noises on the fly. This paper proposes a novel adaptive real-time descreening method based on Support Vector Machine (SVM) and modified Smoothing over Univalue Segment Assimilating Nucleus (SUSAN) filter for imaging devices, including scanners and multifunction printers. The proposed algorithm contains two major steps: image classification and adaptive descreening. The image classification uses SVM methods to accurately classify the scanned images into three categories: continuous tone, amplitude modulation (AM) halftone or frequency modulation (FM) halftone. The halftone images are needed to descreen. The proposed descreening method is based on modified SUSAN filter. It considers screen cell size to choose the optimal filter parameters which can preserve more <b>high</b> <b>frequency</b> <b>image</b> detail. The experiment results show that the algorithm is effective and fully automatism, and maintains higher image quality...|$|E
40|$|Intra {{prediction}} {{is a fundamental}} tool in video coding with hybrid block-based architecture. Recent investigations have shown {{that one of the}} most beneficial elements for a higher compression performance in high-resolution videos is the incorporation of larger block structures. Thus in this work, we investigate the performance of novel intra prediction modes based on different image completion techniques in a new video coding scheme with large block structures. Image completion methods exploit the fact that <b>high</b> <b>frequency</b> <b>image</b> regions yield high coding costs when using classical H. 264 /AVC prediction modes. This problem is tackled by investigating the incorporation of several intra predictors using the concept of Laplace partial differential equation (PDE), Least Square (LS) based linear prediction and the Auto Regressive model. A major aspect of this article is the evaluation of the coding performance in a qualitative (i. e. coding efficiency) manner. Experimental resul ts show significant improvements in compression (up to 7. 41 %) by integrating the LS-based linear intra prediction...|$|E
40|$|Abstract — In this {{proposed}} reconstructed scheme the <b>high</b> <b>frequency</b> subband <b>images</b> are constructed by taking LWT of Luminance component of LR image. The edges are enhanced by introducing an intermediate stage by using stationary wavelet transform (SWT). LWT is applied {{in order to}} decompose an input image into different subbands. Then the <b>high</b> <b>frequency</b> subbands are interpolated. The estimated <b>high</b> <b>frequency</b> subbands are being modified by using <b>high</b> <b>frequency</b> subband obtained through SWT. Then to preserve the more information sparse representation is applied to Luminance component of LR <b>image.</b> Then modified <b>high</b> <b>frequency</b> subband <b>images</b> and sparesed image are combined to generate a Luminance component of high resolution image by using inverse SWT (ISWT). Then the Cb and Cr component of LR image is interpolated using bicubic interpolation. And at last the YCbCr to RGB conversion is done and High Resolution Colour Image can be generated. The quantitative and visual results are showing {{the superiority of the}} proposed technique over the conventional and state-of-art image resolution enhancement techniques...|$|R
25|$|After {{the ship}} has reached its {{cruising}} speed of 0.8c, each twin would see 1second {{pass in the}} received image of the other twin for every 3seconds of his own time. That is, each would see {{the image of the}} other's clock going slow, not just slow by the ε factor 0.6, but even slower because light-time-delay is increasing 0.8seconds per second. This is shown in the figures by red light paths. At some point, the images received by each twin change so that each would see 3seconds pass in the image for every second of his own time. That is, the received signal has been increased in frequency by the Doppler shift. These <b>high</b> <b>frequency</b> <b>images</b> are shown in the figures by blue light paths.|$|R
40|$|Abstract — SAR {{images are}} {{disturbed}} by a multiplicative noise {{depending on the}} signal (the ground reflectivity) due to the radar wave coherence. Images have a strong variability from one pixel to another reducing essentially {{the efficiency of the}} algorithms of detection and classification. In this study, we propose to filter this noise with a multiresolution analysis of the image. The wavelet coefficient of the reflectivity is estimated with a bayesian model, maximizing the a posteriori probability density function. The different probability density function are modeled with the Pearson system of distributions. The resulting filter combines the classical adaptive approach with wavelet decomposition where the local variance of <b>high</b> <b>frequency</b> <b>images</b> is used in order to segment and filter wavelet coefficients...|$|R
40|$|In this paper, {{we present}} an {{evolution}} of IFS-based image compression schemes, well adapted to high frequency contents. We propose to substitute the usual affine mass map {{which tends to}} smooth irregular surfaces, by an harmonic based map. This change implies {{the creation of a}} new IFS determination algorithm for solving the inverse problem, and a new way to quantize and encode coefficients. 1 INTRODUCTION It may be surprising to note that IFS in the family of Jacquin's coding schemes [11] suffer from annoying distortions when dealing with <b>high</b> <b>frequency</b> <b>image</b> contents reconstruction [16] [21]. In fact, some of them are able to generate many rich and different textures [1] at a very high compression rate, but under condition they are fractal [14]. Generally, stationary textures do not possess the self-similarity property on which block-based fractal coding methods rest. Nevertheless, some fractal tools are very efficient for common textures manipulation. This is the case of the multif [...] ...|$|E
40|$|Texture {{enhancement}} is {{an important}} component of image processing that finds extensive application in science and engineering. The quality of medical images, quantified using the imaging texture, plays {{a significant role in the}} routine diagnosis performed by medical practitioners. Most image texture enhancement is performed using classical integral order differential mask operators. Recently, first order fractional differential operators were used to enhance images. Experimentation with these methods led to the conclusion that fractional differential operators not only maintain the low frequency contour features in the smooth areas of the image, but they also nonlinearly enhance edges and textures corresponding to <b>high</b> <b>frequency</b> <b>image</b> components. However, whilst these methods perform well in particular cases, they are not routinely useful across all applications. To this end, we apply the second order Riesz fractional differential operator to improve upon existing approaches of texture enhancement. Compared with the classical integral order differential mask operators and other first order fractional differential operators, we find that our new algorithms provide higher signal to noise values and superior image quality...|$|E
40|$|Satellite {{images are}} {{corrupted}} by noise in its acquisition and transmission. The removal of {{noise from the}} image by attenuating the <b>high</b> <b>frequency</b> <b>image</b> components, removes some important details as well. In order to retain the useful information and improve the visual appearance, an effective denoising and resolution enhancement techniques are required. In this research, Hybrid Directional Lifting (HDL) technique is proposed to retain the important details of the image and improve the visual appearance. The Discrete Wavelet Transform (DWT) based interpolation technique is developed for enhancing {{the resolution of the}} denoised image. The performance of the proposed techniques are tested by Land Remote-Sensing Satellite (LANDSAT) images, using the quantitative performance measure, Peak Signal to Noise Ratio (PSNR) and computation time to show the significance of the proposed techniques. The PSNR of the HDL technique increases 1. 02 dB compared to the standard denoising technique and the DWT based interpolation technique increases 3. 94 dB. From the experimental results it reveals that newly developed image denoising and resolution enhancement techniques improve the image visual quality with rich textures...|$|E
40|$|This paper {{represents}} {{an approach to}} implement image resolution enhancement i. e. Stationary wavelet decomposition and Discrete Wavelet Decomposition. An image resolution enhancement technique based on interpolation of the <b>high</b> <b>frequency</b> subband <b>images</b> obtained by input image and the Discrete Wavelet Transform. These two type of wavelet transforms are used in several type of applications in image processing...|$|R
2500|$|The twin on {{the ship}} sees low <b>frequency</b> (red) <b>images</b> for 3 years. During that time, he would see the Earth twin in the image grow older by [...] He then sees <b>high</b> <b>frequency</b> (blue) <b>images</b> during the back trip of 3years. During that time, he would see the Earth twin in the image grow older by [...] When the journey is finished, {{the image of the}} Earth twin has aged by ...|$|R
50|$|After {{the ship}} has reached its {{cruising}} speed of 0.8c, each twin would see 1 second {{pass in the}} received image of the other twin for every 3 seconds of his own time. That is, each would see {{the image of the}} other's clock going slow, not just slow by the ε factor 0.6, but even slower because light-time-delay is increasing 0.8 seconds per second. This is shown in the figures by red light paths. At some point, the images received by each twin change so that each would see 3 seconds pass in the image for every second of his own time. That is, the received signal has been increased in frequency by the Doppler shift. These <b>high</b> <b>frequency</b> <b>images</b> are shown in the figures by blue light paths.|$|R
40|$|Equalizing {{image noise}} {{is shown to}} be an {{important}} step in the automatic detection of microcalcifications in digital mammography. This study extends a well established film-screen noise equalization scheme developed by Veldkamp et al. for application to full-field digital mammogram (FFDM) images. A simple noise model is determined {{based on the assumption that}} quantum noise is dominant in direct digital X-ray imaging. Estimation of the noise as a function of the gray level is improved by calculating the noise statistics using a truncated distribution method. Experimental support for the quantum noise assumption is presented for a set of step wedge phantom images. Performance of the noise equalization technique is also tested as a preprocessing stage to a microcalcification detection scheme. It is shown that the square root model based approach which FFDM allows leads to a robust estimation of the <b>high</b> <b>frequency</b> <b>image</b> noise. This provides better microcalcification detection performance when compared to the film-screen noise equalization method developed by Veldkamp. Substantially better results are obtained than when noise equalization is omitted. A database of 124 direct digital mammogram images containing 28 microcalcification clusters was used for evaluation of the method...|$|E
40|$|MR imaging data is {{sometimes}} {{presented in a}} "patchwork quilt" format with individual pixels visible as squares of uniform intensity. This phenomenon often arises by default from an image space convolution (performed implicitly by the graphics system) used to convert the sparse point sampling of the spatial domain offered by the discrete Fourier transform (DFT) into a sufficiently dense sampling to allow assignment of an intensity value to each addressable point on the display device. Typical examples are fMRI maps, spectroscopic images and zoomed-in views. These square patches are image structure not present in the object, i. e., artifacts. This form of image display is studied by both an image analysis method and by Fourier analysis. Image formation by display of the 2 D DFT of an acquired k-space matrix as a 2 D pixel array is a poor reconstruction {{because it does not}} ensure a faithful representation of the spatial frequency content actually present in the data. By analysis of the visual appearance of 2 D pixel arrays we show that there are two principal effects: (a) attenuation of higher spatial frequencies (i. e., low-pass filtering); (b) introduction of artifactual <b>high</b> <b>frequency</b> <b>image</b> structure. These effects can lead to very poor performance with an artifact/signal ratio of over 200...|$|E
40|$|One of {{the long-standing}} {{challenges}} in photography is motion blur. Blur artifacts are generated from relative motion between {{a camera and}} a scene during exposure. While blur can be reduced by using a shorter exposure, this comes at an unavoidable trade-off with increased noise. Therefore, it is desirable to remove blur computationally. To remove blur, we need to (i) estimate how the image is blurred (i. e. the blur kernel or the point-spread function) and (ii) restore a natural looking image through deconvolution. Blur kernel estimation is challenging because the algorithm needs to distinguish the correct image– blur pair from incorrect ones that can also adequately explain the blurred image. Deconvolution is also difficult because the algorithm needs to restore <b>high</b> <b>frequency</b> <b>image</b> contents attenuated by blur. In this dissertation, we address a few aspects of these challenges. We introduce an insight that a blur kernel can be estimated by analyzing edges in a blurred photograph. Edge profiles in a blurred image encode projections of the blur kernel, from which we can recover the blur using the inverse Radon transform. This method is computationally attractive and is well suited to images with many edges. Blurred edge profiles can also serv...|$|E
5000|$|The twin on {{the ship}} sees low <b>frequency</b> (red) <b>images</b> for 3 years. During that time, he would see the Earth twin in the image grow older by 3/3 [...] 1 years. He then sees <b>high</b> <b>frequency</b> (blue) <b>images</b> during the back trip of 3 years. During that time, he would see the Earth twin in the image grow older by 3 × 3 [...] 9 years. When the journey is finished, {{the image of the}} Earth twin has aged by 1 + 9 [...] 10 years.|$|R
40|$|<b>High</b> <b>frequency</b> transducers, up to 35 - 50 MHz, {{are widely}} used in {{ophthalmic}} echography to image fine eye structures. At such a <b>high</b> <b>frequency,</b> mechanical scanning probes are the only practical choice. A good probe positioning-image evaluation feedback requires an image refresh-rate of 15 - 30 frames per second. We present the probe design and construction, the experimental characterization and a first <b>high</b> <b>frequency</b> echographic <b>image</b> obtained with a prototype probe based on an ultrasound motor. The prototype probe reaches a scanning rate of 15 frames per second, with very silent operation and little weight...|$|R
40|$|As of February 2012, {{approximately}} 46 % of American adults own a smartphone. The graphics {{quality of}} these devices gets better each year. However, they still have many more limitations in graphics processing and storage space than desktop computers. This means that applications on these devices should focus on optimizing their file sizes and graphics quality {{in order to maximize}} the number of devices that can run and store them. Unfortunately, there is no defined metric for graphics resolution on smartphones. This thesis explores what users believe to be the minimum acceptable graphics quality in smartphone games and graphics applications. By using a testing program we designed in OpenGL, we were able to find at what point in an image’s degradation users found it graphically unappealing and found the app unacceptable. Participants gauged four images that degraded over time. For our two <b>high</b> <b>frequency</b> <b>images,</b> participants found the minimum acceptable graphics quality to occur at 43 pixels per inch (ppi), while in low <b>frequency</b> <b>images</b> they found minimum acceptable graphics quality to occur at around 31 ppi, with the average minimum being 37 ppi...|$|R
40|$|Recent advancements in {{computer}} technology have ensured that early detection of breast cancer, via computer aided detection (CAD) schemes, has become a rapidly expanding field of research. There is a desire to improve the detection accuracy of breast cancer without {{increasing the number of}} falsely identified cancers. The CAD scheme considered here is intended to assist radiologists in the detection of micro calcification clusters, providing a real contribution to the mammography screening process. Factors that affect the detection accuracy of micro calcifications in digital mammograms include the presence of high spatial frequency noise, and locally linear high intensity structures known as curvilinear structures (CLS). The two issues considered are how to compensate for the <b>high</b> <b>frequency</b> <b>image</b> noise and how to detect CLS thus removing their influence on micro calcification detection. First, an adaptive approach to modelling the image noise is adopted. This is derived directly from each mammogram and is adaptable to varying imaging conditions. It is found that compensating for the <b>high</b> <b>frequency</b> <b>image</b> noise significantly improves micro calcification detection accuracy. Second, due to the varying size and orientation of CLS in mammogram images, a shape parameter is designed for their detection using a multiresolution wavelet filter bank. The shape parameter leads to an efficient way of distinguishing curvilinear structures from faint micro calcifications. This improves micro calcification detection performance by reducing the number of false positive detections related to CLS. The detection and segmentation of micro calcification clusters is achieved by the development of a stochastic model, which classifies individual pixels within a mammogram into separate classes based on Bayesian decision theory. Both the high frequency noise model and CLS shape parameters are used as input to this segmentation process. The CAD scheme is specifically designed to be independent of the modality used, simultaneously exploiting the image data and prior knowledge available for micro calcification detection. A new hybrid clustering scheme enables the distinction between individual and clustered micro calcifications, where clustered micro calcifications are considered more clinically suspicious. The scheme utilises the observed properties of genuine clusters (such as a uniform distribution) providing a practical approach to the clustering process. The results obtained are encouraging with a high percentage of genuine clusters detected at the expense of very few false positive detections. An extensive performance evaluation of the CAD scheme helps determine the accuracy of the system and hence the potential contribution to the mammography screening process. Comparing the CAD scheme developed with previously developed micro calcification detection schemes shows that the performance of this method is highly competitive. The best results presented here give a sensitivity of 91 % at an average false positive detection rate of 0. 8 false positives per image...|$|E
40|$|The photogrammetric bundle {{adjustment}} of line scanner image data requires a precise {{description of the}} time-dependent image orientation. For this task exterior orientation parameters of discrete points are used to model position and viewing direction of a camera trajectory via polynomials. This paper investigates {{the influence of the}} distance between these orientation points on the quality of trajectory modeling. A new method adapts the distance along the trajectory to the available image information. Compared to a constant distance as used previously, a better reconstruction of the exterior orientation is possible, especially when image quality changes within a strip. In our research we use image strips of the High Resolution Stereo Camera (HRSC), taken to map the Martian surface. Several experiments on the global image data set have been carried out to investigate how the bundle adjustment improves the image orientation, if the new method is employed. For evaluation the forward intersection errors of 3 D points derived from HRSC images, as well as their remaining height differences to the MOLA DTM are used. In 13. 5 % (515 of 3, 828) of the image strips, taken during this ongoing mission over the last 12 years, <b>high</b> <b>frequency</b> <b>image</b> distortions were found. Bundle adjustment with a constant orientation point distance was able to reconstruct the orbit in 239 (46. 4 %) cases. A variable orientation point distance increased this number to 507 (98. 6 %) ...|$|E
40|$|Quality {{assessment}} {{plays an}} improle in image analysis. The study highlight wavelet transformation based technique {{which is basically}} a time frequency transformation and compares it with traditional methods like PSNR, MSE, SSIM, MSSIM, UQI, NO REF BASED. By applying the wavelet decomposition to the image. The low-freq. components and the high-frequency components of the image are successively obtained. The bases of the wavelet decomposition can be selected from Haar, Daubechies (N= 3), and Daubechies (N= 5). The original image is reconstructed from the low-freq. and high-freq. components. This procedure is called the inverse wavelet transform. By reconstructing the image only from high frequency component, <b>high</b> <b>frequency</b> <b>image</b> is obtained and this image {{can be used for}} edge detection. Daubechies wavelet metric ((DWM) uses the concept of sharpness and zero crossing. Zero crossing provide location of sharp signal variation. Sharpness is defined by the boundaries between zones of different tones or colors. Four bands are extracted from reference and distorted image by employing 2 -D Daubechies wavelet decomposition namely: LL, LH, HL, and HH. Sharpness of both reference and distorted can be calculated from energy in wavelet sub bandand then sharpness similarity of both original and distorted image is measured. By calculatingnumber of edge points in original and distorted image edge structural similarity is measured. Zero crossing is found from edge structural similarity. By combining zero crossing and structural similarity DWM is obtained. We performed our experiment on 9 publicly available images on which blurring of different types JPEG compression and JPEG compression with blurrin...|$|E
40|$|This thesis {{addresses}} {{the problem of}} image super-resolution from a low resolution image. Spatial resoltuion of images are restricted {{by the size of}} CMOS sensors. Spatial resolution can be increased by increasing no of COMS sensors resuling in decrease in size of CMOS sensors which cause shot noise. In this thesis {{attempts have been made to}} enhance the spatial resolution of different images. Two schemes are proposed for this purpose. The basic idea behind both the techniques is to utilize the <b>high</b> <b>frequency</b> subband <b>images</b> derived using lifting wavelet transform. In the first scheme the <b>high</b> <b>frequency</b> subband <b>images</b> are interpolated using surface fitting. In another scheme lifting wavelet transform and stationary wavelet transform are used along with surface fitting interpolation to increase the spatial resolution in the frequency domain. Each technique is studied separately, and experiments are conducted to evaluate their performances. The visual, blind image quality index, visual image fidelity index as well as the peak signal to noise ratio (PSNR in dB) of high resolution images are compared with competent recent schemes. Experimental results demonstrate that the proposed approaches are very effective in increasing resolution and compare favorably to state-of-the-art super-resolution algorithms...|$|R
40|$|In this paper, {{an image}} super-resolution {{technique}} is proposed {{which is based}} on interpolation of <b>high</b> <b>frequency</b> sub-band <b>images</b> obtained by Discrete Wavelet Transform on input image. In those sub-bands edges are enhanced by introducing an intermediate stage by using Stationary Wavelet Transform. The wavelet transform is applied in order to decompose image into different sub-bands. In those sub-bands, the <b>high</b> <b>frequency</b> sub-bands are interpolated and then these estimated <b>high</b> <b>frequency</b> sub-bands are modified by using <b>high</b> <b>frequency</b> sub-bands obtained through SWT. These all sub-bands are fused to generate a new high resolution by using inverse Wavelet transform techniques. The proposed results depict the conventional and state of art image resolution enhancement techniques...|$|R
40|$|ABSTRACT: In this paper,the authors propose {{an image}} {{resolution}} enhancement technique based on interpolation of the <b>high</b> <b>frequency</b> subband <b>images</b> obtained by discrete wavelet transform (DWT) and the input image. The edges are enhanced by introducing an intermediate stage by using stationary wavelet transform (SWT). DWT is applied {{in order to}} decompose an input image into different subbands. Then the <b>high</b> <b>frequency</b> subbands {{as well as the}} input image are interpolated. The estimated <b>high</b> <b>frequency</b> subbands are being modified by using <b>high</b> <b>frequency</b> subband obtained through SWT. Then all these subbands are combined to generate a new high resolution image by using inverse DWT (IDWT). The quantitative and visual results are showing the superiority of the proposed technique over the conventional and state-of-art image resolution enhancement techniques...|$|R
