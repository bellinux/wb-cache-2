2|2|Public
40|$|Abstract: High-quality {{mixed reality}} {{rendering}} {{is the art}} of combining synthetic and photographic images in a photorealistically consistent way. Most rendering methods used in this area are based on ray tracing algorithms and take hours of computing time. With interactive ray tracing having become available in recent years it is obvious to explore its potential for mixed reality applications, like virtual TV studios featuring photorealistic live insertion of human actors in rendered background scenes. In this paper we present a method to seamlessly integrate live actor performance into the distributed interactive ray tracing framework OpenRT. An image based visual <b>hull</b> <b>shader</b> is used to create a 3 D representation of the actor. This allows for the full spectrum of ray tracing based effects like reflection, refraction, and shadows in the composite image...|$|E
40|$|The {{appearance}} of woven fabrics is intrinsically {{determined by the}} geometric details of their meso/micro scale structure. In this paper, we propose a multiscale representation and tessellation approach for woven fabrics. We extend the Displaced Subdivision Surface (DSS) to a representation named Interlaced/Intertwisted Displacement Subdivision Surface (IDSS). IDSS maps the geometric detail, scale by scale, onto a ternary interpolatory subdivision surface that is approximated by Bezier patches. This approach is designed for woven fabric rendering on DX 11 GPUs. We introduce the Woven Patch, a structure based on DirectX's new primitive, patch, to describe an area of a woven fabric {{so that it can}} be easily implemented in the graphics pipeline using a <b>hull</b> <b>shader,</b> a tessellator and a domain shader. We can render a woven piece of fabric at 25 frames per second on a low-performance NVIDIA 8400 MG mobile GPU. This allows for large-scale representations of woven fabrics that maintain the geometric variances of real yarn and fiber. Institute of Textiles and ClothingDepartment of Computin...|$|E
5000|$|As of OpenGL 4.0 and Direct3D 11, a new shader class {{called a}} {{tessellation}} shader has been added. It adds two new shader stages {{to the traditional}} model: tessellation control shaders (also known as <b>hull</b> <b>shaders)</b> and tessellation evaluation shaders (also known as Domain Shaders), which together allow for simpler meshes to be subdivided into finer meshes at run-time according to a mathematical function. The function {{can be related to}} a variety of variables, most notably the distance from the viewing camera to allow active level-of-detail scaling. This allows objects close to the camera to have fine detail, while further away ones can have more coarse meshes, yet seem comparable in quality. It also can drastically reduce mesh bandwidth by allowing meshes to be refined once inside the shader units instead of downsampling very complex ones from memory. Some algorithms can upsample any arbitrary mesh, while others allow for [...] "hinting" [...] in meshes to dictate the most characteristic vertices and edges.|$|R
50|$|HLSL {{programs}} come in five forms: pixel shaders (fragment in GLSL), vertex shaders, geometry shaders, compute shaders and tessellation <b>shaders</b> (<b>Hull</b> and Domain <b>shaders).</b> A vertex shader is {{executed for}} each vertex that is {{submitted by the}} application, and is primarily responsible for transforming the vertex from object space to view space, generating texture coordinates, and calculating lighting coefficients such as the vertex's tangent, binormal and normal vectors. When a group of vertices (normally 3, to form a triangle) come through the vertex shader, their output position is interpolated to form pixels within its area; this process is known as rasterisation. Each of these pixels comes through the pixel shader, whereby the resultant screen colour is calculated.|$|R

