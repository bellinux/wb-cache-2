25|7|Public
6000|$|Among books, Plato only is {{entitled}} to Omar's fanatical compliment to the Koran, when he said, [...] "Burn the libraries; for, their value is in this book." [...] These sentences contain the culture of nations; these are the corner-stone of schools; these are the fountain-head of literatures. A discipline it is in logic, arithmetic, taste, symmetry, poetry, language, rhetoric, ontology, morals, or practical wisdom. There was never such range of speculation. Out of Plato come all things that are still written and debated among men of thought. Great havoc makes he among our originalities. We have reached the mountain from which all these drift bowlders were detached. The Bible of the learned for twenty- two hundred years, every brisk young man, who says in succession fine things to each reluctant generation,--Boethius, Rabelais, Erasmus, Bruno, Locke, Rousseau, Alfieri, Coleridge,--is some reader of Plato, translating into the vernacular, wittily, his good things. Even the men of grander proportion suffer some deduction from the misfortune (shall I say?) of coming after this exhausting <b>generalizer.</b> St. Augustine, Copernicus, Newton, Behmen, Swedenborg, Goethe, are likewise his debtors, and must say after him. For {{it is fair to}} credit the broadest <b>generalizer</b> with all the particulars deducible from his thesis.|$|E
60|$|The Observer of Peoples {{has to be}} a Classifier, a Grouper, a Deducer, a <b>Generalizer,</b> a Psychologizer; and, {{first and}} last, a Thinker. He {{has to be a}}ll these, and when he is at home, observing his own folk, he is often able to prove competency. But history has shown that when he is abroad observing {{unfamiliar}} peoples the chances are heavily against him. He is then a naturalist observing a bug, with no more than a naturalist's chance of being able to tell the bug anything new about itself, and no more than a naturalist's chance of being able to teach it any new ways which it will prefer to its own.|$|E
60|$|The {{events of}} Mr. James's life--as we agree to {{understand}} events--may be told {{in a very}} few words. His race is Irish on his father's side and Scotch on his mother's, to which mingled strains the <b>generalizer</b> may attribute, if he likes, that union of vivid expression and dispassionate analysis which has characterized his work from the first. There are none of those early struggles with poverty, which render the lives of so many distinguished Americans monotonous reading, to record in his case: the cabin hearth-fire did not light him to the youthful pursuit of literature; he had from the start all those advantages which, when they go too far, become limitations.|$|E
30|$|World {{knowledge}} aggregated to mined {{data will}} generate redundancy, since wide-ranging attributes and specific attributes will coexist. Wide-ranging attributes, such as price range of items, pattern <b>generalizers,</b> or grouping representatives, are called dependents under the information storage point of view, since once a specific (or determinant) attribute is known, the wide-ranging (or dependent) attribute is also known.|$|R
40|$|Abstract: The {{method of}} {{combining}} classifier {{can be done}} in various ways of which the most competent of them are Stacking and Voting method. Stacking is a way of combining multiple <b>generalizers</b> one after another where the output of the first classifier is considered as an input to the next one, whereas Voting method works on the principle of best result oriented <b>generalizers.</b> In this paper the author have used a large dataset i. e. KDD which helps in anomaly detection methods. In this paper the author has explained the paramount of Voting method over stacking method in the combination algorithm named as boostSVM for the above mentioned specified dataset. The classifiers used are Support Vector Machine (SVM) and AdaBoost where the AdaBoost algorithm boosts SVM to debase the error rates. SVM is mainly chosen as it provides a global maxima instead of local minima’s. ROC curves are also being used to justify the results as it helps in evaluating the performance efficviently...|$|R
40|$|ABSTRACT: We {{show that}} the minimum {{possible}} size of an n-superconcentrator with depth 2 k~ 4 is e(nX(k, n)), where k(k,.) is the inverse of a certain function at the k-th level of the primitive recursive hierarchy. It follows that the minimum possible depth of an n-superconcentrator with linear size is 8 (~(n)), where ~ is the inverse of a function growing more_rapidly than any primitive recursive function. Similar results hold for <b>generalizers.</b> We give a simple explicit construction for a (dl [...] . dk) -generalizer with depth k and size (dl+ [...] . +dk) dl [...] . dk. This is applied to give a simple explicit construction for a generalized n-connector with depth 2 k- 3 and size (2 dl+ 3 d 2 + [...] . + 3 dk_l+ 2 dk) dl [...] . d k. These are the best explicit construction...|$|R
40|$|For any {{real-world}} generalization problem, {{there are}} always many generalizers which {{could be applied to}} the problem. This paper discusses some algorithmic techniques for dealing with this multiplicity of possible generalizers. All of these techniques rely on partitioning the provided learning set in two, many different times. The first technique discussed is cross-validation, which is a winner-takes-all strategy (based on the behavior of the generalizers on the partitions of the learning set, it picks one single <b>generalizer</b> from amongst the set of candidate generalizers, and tells you to use that <b>generalizer).</b> The second technique discussed, the one this paper concentrates on, is an extension of cross-validation called stacked generalization. As opposed to cross-validation's winnertakes -all strategy, stacked generalization uses the partitions of the learning set to combine the generalizers, in a non-linear manner, via another <b>generalizer</b> (hence the term "stacked generalization"). Af [...] ...|$|E
40|$|This paper {{introduces}} stacked generalization, {{a scheme}} for minimizing the generalization error {{rate of one}} or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization {{can be seen as a}} more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation 's crude winner-takes-all for combining the individual generalizers. When used with a single <b>generalizer,</b> stacked generalization is a scheme for estimating (and then correcting for) the error of a <b>generalizer</b> which has been trained on a particular learning set and then asked a particular question. After [...] ...|$|E
40|$|This {{paper is}} about explanation-based learn-ing for {{heuristic}} problem solvers which "build" solutions using schemata (frames like scripts) as both "bricks " and "mortar". The {{heart of the}} paper is {{a description of a}} generalization method which is designed to extract as much information as pos-sible from examples of successful problem solving behavior. A ' related <b>generalizer,</b> (less powerful but more efficient), has been implemented as part of an experimental apprentice. *...|$|E
40|$|In this paper, {{we propose}} {{a method for}} {{learning}} a classifier which combines outputs {{of more than one}} Japanese named entity extractors. The proposed combination method belongs to the family of stacked <b>generalizers,</b> which is in principle a technique of combining outputs of several classifiers at the first stage by learning a second stage classifier to combine those outputs at the first stage. Individual models to be combined are based on maximum entropy models, one of which always considers surrounding contexts of a fixed length, while the other considers those of variable lengths according to the number of constituent morphemes of named entities. As an algorithm for learning the second stage classifier, we employ a decision list learning method. Experimental evaluation shows that the proposed method achieves improvement over the best known results with Japanese named entity extractors based on maximum entropy models...|$|R
30|$|The {{idea behind}} SemPrune comes from {{observations}} of ecommerce transactions. Consider a dataset of online sales transactions. The simplest way of using subjacent knowledge is to group attributes into value ranges such as by grouping purchasable items by price range (cheap, normal price, expensive), individuals by height (short, medium, tall), or workers by productivity (low, medium, high, excellent). Comprehensive variables, {{such as the}} price range of items, <b>generalizers,</b> or grouping representatives, are those which, according to the information-storage perspective, are called dependent, since whenever a specific (or determinant) variable is known, the comprehensive (or dependent) variable is also known. Each type of relationship has its representation and resolution dependency mechanisms. Usually the determinant attributes (which give rise to aggregating attributes) occur in small numbers. This type of analysis is often used {{in the case of}} value ranges of attributes and derived aggregator attributes. A pre-processing activity would normally create new rules with the dependent variables established in a file of dependencies. The present research applies the semantic enhancement of rule mining along with the pruning treatment.|$|R
40|$|The last {{third of}} the twentieth century was ushered in by a set of events—wars, rebellions, finally {{economic}} crisis—that dealt a crushing blow to the previously dominant paradigm in U. S. social science, the structural-functionalist modernizationism elaborated by Talcott Parsons and his students. Within sociology, the site of much social science innovation in the post-war period, a notable splintering occurred. Already underway in the early nineteen-sixties, this splintering was accelerated by the tumultuous events later in that decade. One of the splinters which has grown and developed most rapidly and fruitfully over the past decade has been the world-systems perspective, a formidable synthesis of continental historicism, “Third World ” radicalism, and Marxism. The principal exponent of this perspective has been Immanuel Wallerstein (b. 1930). whose work has built upon and in turn has stimulated advances in both historical sociology and the study of contemporary “development. ” Most important and innovative about Wallerstein’s effort are the reconceptualization of social change in terms of totalities as units of analysis, the attempt to historicize the social sciences and overcome the split between the universalizing <b>generalizers</b> (theory) and “idiographic ” particularizers (his...|$|R
40|$|Thesis (Sarjana Pendidikan (Pengajaran Bahasa Inggeris sebagai Bahasa Kedua)) - Universiti Teknologi Malaysia, 2013 One of {{the major}} {{challenges}} in polymerization industry {{is the lack of}} online instruments to measure polymer end-used properties such as melt flow index (MFI). As an alternative to the online instruments and conventional laboratory tests, these properties can be estimated by using a model based-soft sensor. This research presents models for soft sensors to measure MFI in industrial polypropylene loop reactors by using the artificial neural network (ANN), hybrid FP-ANN (HNN) and stacked neural network (SNN) models. The ANN model of the two loop reactors was developed by employing the concept of Feed-Forward Back Propagation (FFBP) network architecture using Levenberg-Marquardt training method. Serial hybrid FP-ANN (HNN) models were developed in this study. The error between actual MFI and simulation MFI from FP model was fed into the HNN model as one of the input variables. To construct the stacked neural network (SNN) model, two layers were needed: 1) level- 0 <b>generalizer</b> output comes from a number of diverse ANN models and 2) level- 1 <b>generalizer</b> was developed using the results of level- 0 <b>generalizer</b> with additional input variables. All models were developed and simulated in MATLAB 2009 a environment. The simulation results of the MFI based on the ANN, HNN, and SNN models were compared and analyzed. The HNN model is the best model in predicting all range of MFI with the lowest root mean square error (RMSE) value, 0. 010848, followed by ANN model (RMSE= 0. 019366) and SNN model (RMSE= 0. 059132). When these three models (ANN, HNN, and SNN) were compared, the SNN model shows the lower RMSE for each type of MFI studied...|$|E
40|$|It {{is shown}} how ideas adapted from recent work on explanation-based {{generalization}} {{can be used}} to allow a logic grammar to "learn" useful derived grammar rules by generalizing them from example sentences. The method is presented {{in the form of a}} small Prolog meta-interpreter, and its soundness is formally proved. Examples are given showing the application of the <b>generalizer,</b> first to a toy grammar with 40 rules and then to a largish independantly developed system which involves non-trivial syntactic and semantic analysis...|$|E
40|$|The {{study that}} is {{presented}} here concerns {{the learning of}} algebra in a computer algebra environment and, more specific, the learning {{of the concept of}} parameter. Students of 14 – 15 years old used a TI- 89 symbolic calculator during a five-week period. They studied the parameter in different roles such as placeholder, changing quantity and <b>generalizer.</b> The results indicate that using parameters in the computer algebra environment requires a clear view on the roles of the different letters. Also, the reification of a formula seems to be important for an appropriate instrumentation...|$|E
6000|$|The Cabalists had a notion, {{that whoever}} {{found out the}} mystic word for {{anything}} attained to absolute mastery over that thing. The reverse of this is certainly true of poetic expression; for he who is thoroughly possessed of his thought, who imaginatively conceives an idea or image, becomes master of the word that shall most amply and fitly utter it. Heminge and Condell tell us, accordingly, that there was scarce a blot in the manuscripts they received from Shakespeare; {{and this is the}} natural corollary from the fact that such an imagination as his is as unparalleled as the force, variety, and beauty of the phrase in which it embodied itself.[121] We believe that Shakespeare, like all other great poets, instinctively used the dialect which he found current, and that his words are not more wrested from their ordinary meaning than followed necessarily from the unwonted weight of thought or stress of passion they were called on to support. He needed not to mask familiar thoughts in the weeds of unfamiliar phraseology; for the life that was in his mind could transfuse the language of every day with an intelligent vivacity, that makes it seem lambent with fiery purpose, and at each new reading a new creation. He could say with Dante, that [...] "no word had ever forced him to say what he would not, though he had forced many a word to say what it would not,"--but only {{in the sense that the}} mighty magic of his imagination had conjured out of it its uttermost secret of power or pathos. When I say that Shakespeare used the current language of his day, I mean only that he habitually employed such language as was universally comprehensible,--that he was not run away with by the hobby of any theory as to the fitness of this or that component of English for expressing certain thoughts or feelings. That the artistic value of a choice and noble diction was quite as well understood in his day as in ours is evident from the praises bestowed by his contemporaries on Drayton, and by the epithet [...] "well-languaged" [...] applied to Daniel, whose poetic style is as modern as that of Tennyson; but the endless absurdities about the comparative merits of Saxon and Norman-French, vented by persons incapable of distinguishing one tongue from the other, were as yet unheard of. Hasty <b>generalizers</b> are apt to overlook the fact, that the Saxon was never, to any great extent, a literary language. Accordingly, it held its own very well in the names of common things, but failed to answer the demands of complex ideas, derived from them. The author of [...] "Piers Ploughman" [...] wrote for the people,--Chaucer for the court. We open at random and count the Latin[122] words in ten verses of the [...] "Vision" [...] and ten of the [...] "Romaunt of the Rose," [...] (a translation from the French,) and find the proportion to be seven in the former and five in the latter.|$|R
40|$|Stacked Generalization: {{when does}} it work? Stacked {{generalization}} {{is a general}} method of using a high-level model to combine lowerlevel models to achieve greater predictive accuracy. In this paper we address two crucial issues which have been {{considered to be a}} 'black art ' in classification tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of <b>generalizer</b> that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms. ...|$|E
40|$|We study numerically the {{properties}} of the bayesian perceptron through a gradient descent on the optimal cost function. The theoretical distribution of stabilities is deduced. It predicts that the optimal <b>generalizer</b> lies close to the boundary of the space of (error-free) solutions. The numerical simulations are in good agreement with the theoretical distribution. The extrapolation of the generalization error to infinite input space size agrees with the theoretical results. Finite size corrections are negative and exhibit two different scaling regimes, depending on the training set size. The variance of the generalization error vanishes for N →∞ confirming the property of self-averaging. Comment: RevTeX, 7 pages, 7 figures, submitted to Phys. Rev. ...|$|E
40|$|Stacked {{generalization}} is {{a general}} method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we resolve two crucial issues which have been {{considered to be a}} ‘black art’ in classification tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of <b>generalizer</b> that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms, and also for combining models of the same type derived from a single learning algorithm in a multiple-data-batches scenario. We also compare the performance of stacked generalization with published results of arcing and bagging...|$|E
40|$|Abstract: This paper {{introduces}} stacked generalization, {{a scheme}} for minimizing the generalization error {{rate of one}} or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization {{can be seen as a}} more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-vali-dation’s crude winner-takes-all for combining the individual generalizers. When used with a single <b>generalizer,</b> stacked generalization is a scheme for estimating (and then correcting for) the error of a <b>generalizer</b> which has been trained on a particular learning set and then asked a particular ques-tion. After introducing stacked generalization and justifying its use, this paper presents two numer-ical experiments. The first demonstrates how stacked generalization improves upon a set of sepa-rate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other ex-perimental evidence in the literature, the usual arguments supporting cross-validation, and the ab-stract justifications presented in this paper, the conclusion is that for almost any real-world gener-alization problem one should use some version of stacked generalization to minimize the general-ization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory. Key Words: generalization and induction, combining generalizers, learning set pre-processing, cross-validation, error estimation and correction...|$|E
40|$|Abstract: This paper uses off-training set (OTS) error to {{investigate}} the assumption-free relationship between learning algorithms. It is shown, loosely speaking, that for any two algorithms A and B, {{there are as many}} targets (or priors over targets) for which A has lower expected OTS error than B as vice-versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation ” (choose the <b>generalizer</b> with largest cross-validation error). On the other hand, for loss functions other than zero-one (e. g., quadratic loss), there are a priori distinctions between algorithms. However even for such loss functions, any algorithm is equivalent on average to its “randomized ” version, and in this still has no first principles justification in terms of average error. Nonetheless, it may be that (for example) cross-validation has better minimax properties than anti-cross-validation, even for zero-one loss. This paper also analyzes averages over hypotheses rather than targets. Such analyses hold for all possible priors. Accordingly they prove, as a particular example, that cross-validation can not be justified as a Bayesian procedure. In fact, for a very natural restriction of the class of learning algorithms, one should use anti-crossvalidation rather than cross-validation (!). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one can not say: if empirical misclassification rate is low; the VC dimension of your <b>generalizer</b> is small; and the training set is large, then with high probability your OTS error is small. Other implications for “membership queries” algorithms and “punting ” algorithms are also discussed. “Even after the observation of the frequent conjunction of objects, {{we have no reason to}} draw any inference concerning any object beyond those of which we have had experience. ”- David Hume, in A Treatise of Human Nature, Book I, part 3, Section 12. ...|$|E
40|$|This paper {{introduces}} a new metaobject, the <b>generalizer,</b> which complements the existing specializer metaobject. With {{the help of}} examples, we show that this metaobject allows for the efficient implementation of complex non-class-based dispatch {{within the framework of}} existing metaobject protocols. We present our modifications to the generic function invocation protocol from the Art of the Metaobject Protocol; in combination with previous work, this produces a fully-functional extension of the existing mechanism for method selection and combination, including support for method combination completely independent from method selection. We discuss our implementation, within the SBCL implementation of Common Lisp, and in that context compare the performance of the new protocol with the standard one, demonstrating that the new protocol can be tolerably efficient. Comment: 8 pages; version accepted for presentation at 2014 European Lisp Symposium. [URL]...|$|E
40|$|Stacked {{generalization}} is {{a general}} method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we address two crucial issues which have been {{considered to be a}} `black art ' in classication tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of <b>generalizer</b> that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We nd that best results are obtained when the higher-level model combines the con dence (and not just the predictions) of the lower-level ones. We demonstrate the eectiveness of stacked generalization for combining three dierent types of learning algorithms for classication tasks. We also compare the performance of stacked generalization with majority vote and published results of arcing and bagging. 1...|$|E
40|$|We {{introduce}} two boosting algorithms {{that aim}} {{to increase the}} generalization accuracy of a given classifier by incorporating it as a level- 0 component in a stacked <b>generalizer.</b> Both algorithms construct a complementary level- 0 classifier that can only generate coarse hypotheses for the training data. We show that the two algorithms boost generalization accuracy on a representative collection of data sets. The two algorithms are distinguished in {{that one of them}} modifies the class targets of selected training instances in order to train the complementary classifier. We show that the two algorithms achieve approximately equal generalization accuracy, but that they create complementary classifiers that display different degrees of accuracy and diversity. Our study provides evidence that it may be useful to investigate families of boosting algorithms that incorporate varying levels of accuracy and diversity, so as to achieve an appropriate mix for a given task and domain...|$|E
40|$|Recent {{developments}} {{have shown the}} usefulness of combinations of classifiers. This paper presents a 3 -Level Stacked <b>Generalizer</b> consisting of evolutionary created Artificial Neural Networks with different architectures and weight settings. Several combination and new selection methods are discussed and their usefulness is shown in practical examples. It can be shown {{that it is more}} efficient to use information obtained from the complete population rather than taking the best individual. Keywords: Ensembles, Evolutionary Algorithms, Artificial Neural Networks, Stacked Generalization 1 Introduction One way {{to improve the quality of}} a classifier is to optimize the underlying algorithm or to search for new algorithms. A more simple approach is to combine the results of several different classifiers by a combination algorithm. Many scientists have presented methods to obtain useful Ensembles of classifiers. The main objective is to create Ensembles of highly accurate members, which ar [...] ...|$|E
40|$|Abstract—The main {{principle}} of stacked generalization (or Stacking) {{is using a}} second-level <b>generalizer</b> to combine the outputs of base classifiers in an ensemble. In this paper, we investigate different combi-nation types under the stacking framework; namely weighted sum (WS), class-dependent weighted sum (CWS) and linear stacked generalization (LSG). For learning the weights, we propose using regularized empirical risk minimization with the hinge loss. In addition, we propose using group sparsity for regularization to facilitate classifier selection. We per-formed experiments using two different ensemble setups with differing diversities on 8 real-world datasets. Results show the power of regular-ized learning with the hinge loss function. Using sparse regularization, {{we are able to}} reduce the number of selected classifiers of the diverse ensemble without sacrificing accuracy. With the non-diverse ensembles, we even gain accuracy on average by using sparse regularization. Index Terms—classifier combination, classifier selection, regularized empirical risk minimization, hinge loss, group sparsity...|$|E
40|$|This paper {{introduces}} the Recurrence Surface Approximation, an inductive learning method based on linear programming that predicts recurrence times using censored training examples, that is, examples {{in which the}} available training output may be only a lower bound on the "right answer. " This approach is augmented with a feature selection method that chooses an appropriate feature set {{within the context of}} the linear programming <b>generalizer.</b> Computational results in the field of breast cancer prognosis are shown. A straightforward translation of the prediction method to an artificial neural network model is also proposed. 1 INTRODUCTION Machine learning methods have been successfully applied to the analysis of many different complex problems in recent years, including many biomedical applications. One field which can benefit from this type of approach is the analysis of survival or lifetime data (Lee, 1992; Miller Jr., 1981), in which the objective can be broadly defined as predicting [...] ...|$|E
40|$|The main {{principle}} of stacked generalization (or Stacking) {{is using a}} second-level <b>generalizer</b> to combine the outputs of base classifiers in an ensemble. In this paper, we investigate different combination types under the stacking framework; namely weighted sum (WS), class-dependent weighted sum (CWS) and linear stacked generalization (LSG). For learning the weights, we propose using regularized empirical risk minimization with the hinge loss. In addition, we propose using group sparsity for regularization to facilitate classifier selection. We performed experiments using two different ensemble setups with differing diversities on 8 real-world datasets. Results show the power of regularized learning with the hinge loss function. Using sparse regularization, {{we are able to}} reduce the number of selected classifiers of the diverse ensemble without sacrificing accuracy. With the non-diverse ensembles, we even gain accuracy on average by using sparse regularization. Comment: 8 pages, 3 figures, 6 tables, journa...|$|E
40|$|The main {{principle}} of stacked generalization {{is using a}} second-level <b>generalizer</b> to combine the outputs of base classifiers in an ensemble. In this paper, after presenting a short survey {{of the literature on}} stacked generalization, we propose to use regularized empirical risk minimization (RERM) as a framework for learning the weights of the combiner which generalizes earlier proposals and enables improved learning methods. Our main contribution is using group sparsity for regularization to facilitate classifier selection. In addition, we propose and analyze using the hinge loss instead of the conventional least squares loss. We performed experiments on three different ensemble setups with differing diversities on 13 real-world datasets of various applications. Results show the power of group sparse regularization over the conventional l 1 norm regularization. We are able {{to reduce the number of}} selected classifiers of the diverse ensemble without sacrificing accuracy. Corresponding author...|$|E
40|$|This paper uses off-training set (OTS) error to {{investigate}} the assumption-free relationship between learning algorithms. It is shown, loosely speaking, that for any two algorithms A and B, {{there are as many}} targets (or priors over targets) for which A has lower expected OTS error than B as vice-versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is "anti-cross-validation" (choose the <b>generalizer</b> with largest cross-validation error). On the other hand, for loss functions other than zero-one (e. g., quadratic loss), there are a priori distinctions between algorithms. However even for such loss functions, any algorithm is equivalent on average to its "randomized" version, and in this still has no first principles justification in terms of average error. Nonetheless, it may be that (for example) cross-validation has better minimax properties than anti-cross-validation, even for zero-one loss. This paper also analyzes averages over hyp [...] ...|$|E
40|$|This is {{the first}} of two papers that use off-training set (OTS) error to {{investigate}} the assumption -free relationship between learning algorithms. This first paper discusses the senses in which there are no a priori distinctions between learning algorithms. (The second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are "as many" targets (or priors over targets) for which A has lower expected OTS error than B as vice-versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is "anti-cross-validation" (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one can not say: if empirical misclassification rate is low; the Vapnik-Chervonenkis dimension of your <b>generalizer</b> is small; and the trainin [...] ...|$|E
40|$|A {{classification}} of the extensions of degree p 2 over Qp whose normal closure is a p-extension par Luca CAPUTO Résumé. Soit k une extension finie de Qp et soit Ek l’ensemble des extensions de degre ́ p 2 sur k dont la clôture normale est une p-extension. Pour chaque discriminant fixé, nous calculons le nombre des éléments de EQp qui ont un tel discriminant et nous donnons les discriminants et les groupes de Galois (avec leur fil-trations des groupes de ramification) de leurs clôtures normales. Nous montrons aussi que l’on peut <b>generalizer</b> cette méthode pour obtenir une classification des extensions qui appartiennent a ̀ Ek. Abstract. Let k be a finite extension of Qp and Ek be {{the set of}} the extensions of degree p 2 over k whose normal closure is a p-extension. For a fixed discriminant, we show how many exten-sions there are in EQp with such discriminant and we give the dis-criminant and the Galois group (together with its filtration of the ramification groups) of their normal closure. We show how this method can be generalized to get a {{classification of}} the extensions in Ek. 1. Notation, preliminaries and results. Throughout this paper, p is an odd prime and k will be a fixed p-adic field of degree d over Qp which does not contain any primitive p-th root of unity. If E is a p-adic field and L|E is a finite extension, then we say that L|E is a p-extension if it is Galois and its degree is a power of p. The aim of the present paper is to give a classification of the extensions of degree p 2 over Qp whose normal closure is a p-extension. This classification is based on the discriminant of the extension and on the Galois group and the discriminant of its normal closure. Let Ek be {{the set of the}} extensions of degree p 2 over k whose normal closure is a p-extension. Then for every L ∈ Ek, there exists a cyclic extension K|k of degree p, K ⊆ L and L|K is cyclic (of degree p). Furthermore, the converse is true: if K|k is a cyclic extension of degree p, then every cyclic extension L of degree p over K is an extension of degree p 2 over k whose normal closure is a p-extension (see Prop. 2. 1). Therefore, if K|k is a cyclic extension of degree p, we can ha l-...|$|E

