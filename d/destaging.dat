10|12|Public
50|$|A system crash {{or other}} {{interruption}} of a write operation {{can result in}} states where the parity is inconsistent with the data due to non-atomicity of the write process, such that the parity cannot be used for recovery {{in the case of}} a disk failure (the so-called RAID 5 write hole). The RAID write hole is a known data corruption issue in older and low-end RAIDs, caused by interrupted <b>destaging</b> of writes to disk.|$|E
5000|$|A {{functionality}} optimizing data paths {{within a}} metro- or geo-distance Stretched Cluster (see above), helpful when bandwidth between sites is scarce and cross-site traffic must be minimized. SVC {{will attempt to}} use the shortest path for reads and writes. For instance, cache write <b>destaging</b> to storage devices is always performed by the most nearby cache copy, unless its peer cache copy is down. Two node pairs are the recommended minimum for an Enhanced Stretched Cluster.|$|E
50|$|The MSS (as it was known) {{consisted}} of a library of cylindrical plastic cartridges, two inches wide and 4 in long, each holding a spool of tape 770 in long storing 50MB; each virtual disk required a pair of cartridges. These cartridges were held in a hexagonal array of bins in the IBM 3851 Mass Storage Facility. New cartridges were rolled into the facility and were automatically stored in a vacant bin. The data were accessed via virtual IBM 3330 disk drives, and physically cached {{on a combination of}} 3330 and 3350 staging drives, the data being transferred automatically between cartridge and disk drive in processes called staging and <b>destaging.</b> These were all connected together with the IBM 3830 Storage Control (also used for disk storage alone), the entire system making up a 3850 unit.|$|E
40|$|Non-volatile {{write-back}} caches enable storage controllers {{to provide}} quick write response times by hiding the latency of the disks. Managing a write cache well {{is critical to}} the performance of storage controllers. Over two decades, various algorithms have been proposed, including the most popular, LRW, CSCAN, and WOW. While LRW leverages temporal locality in the workload, and CSCAN creates spatial locality in the <b>destages,</b> WOW combines the benefits of both temporal and spatial localities in a unified ordering for <b>destages.</b> However, there remains an equally important aspect of write caching to be considered, namely, the rate of <b>destages.</b> For the best performance, it is important to <b>destage</b> at a steady rate while making sure that the write cache is not under-utilized or over-committed. Most algorithms have not seriously considered this problem, and as a consequence, forgo {{a significant portion of the}} performance gains that can be achieved. We propose a simple and adaptive algorithm, STOW, which not only exploits both spatial and temporal localities in a new order of <b>destages,</b> but also facilitates and controls the rate of <b>destages</b> effectively. Further, STOW partitions the write cache into a sequential queue and a random queue, and dynamically and continuously adapts their relative sizes. Treating the two kinds of writes separately provides for better <b>destage</b> rate control, resistance to one-time sequential requests polluting the cache, and a workload-responsive write caching policy. STOW represents a leap ahead of all previously proposed write cache management algorithms. As anecdotal evidence, with a write cache of 32 K pages, serving a 4 +P RAID- 5 array, using an SPC- 1 Like Benchmark, STO...|$|R
40|$|In a {{disk array}} with a {{nonvolatile}} write cache, <b>destages</b> from the cache to the disk are {{performed in the}} background asynchronously while read requests from the host system are serviced in the foreground. In this paper, we study a number of algorithms for scheduling <b>destages</b> in a RAID- 5 system. We introduce a new scheduling algorithm, called linear threshold scheduling, that adaptively varies the rate of <b>destages</b> to disks based on the instantaneous occupancy of the write cache. The performance of the algorithm is {{compared with that of}} a number of alternative scheduling approaches such as least-cost scheduling and high/low mark. The algorithms are evaluated in terms of their effectiveness in making <b>destages</b> transparent to the servicing of read requests from the host, disk utilization, and their ability to tolerate bursts in the workload without causing an overflow of the write cache. Our results show that linear threshold scheduling provides the best read performance of all the algorithm [...] ...|$|R
40|$|Abstract — Write caches using fast, {{non-volatile}} storage are now {{widely used in}} modern storage controllers since they enable hiding latency on writes. Effective algorithms for write cache management are extremely important since (i) in RAID- 5, due to read-modify-write and parity updates, each write may cause up to four separate disk seeks while a read miss causes only a single disk seek; and (ii) typically, write cache size is {{much smaller than the}} read cache size – a proportion of 1 : 16 is typical. A write caching policy must decide: what data to <b>destage.</b> On one hand, to exploit temporal locality, we would like to <b>destage</b> data that is least likely to be re-written soon with the goal of minimizing the total number of <b>destages.</b> This is normally achieved using a caching algorithm such as LRW (least recently written). However, a read cache has a ver...|$|R
40|$|Abstract: 2 ̆ 2 This paper {{describes}} a programming model and system support for clean construction of disk maintenance applications. Such applications expose the disk activity to be done, and then process completed requests {{as they are}} reported. The system ensures that these applications make steady forward progress without competing for disk access with a system 2 ̆ 7 s primary applications. It opportunistically completes maintenance requests by using disk idle time and free-block scheduling. In this paper, three disk maintenance applications (backup, write-back cache <b>destaging,</b> and disk layout reorganization) are adapted to the system support and evaluated on a FreeBSD implementation. All are shown to successfully execute in busy systems with minimal (e. g., 3 ̆c 2...|$|E
40|$|This paper {{describes}} a programming model and system support for clean construction of disk maintenance applications. Such applications expose the disk activity to be done, and then process completed requests {{as they are}} reported. The system ensures that these applications make steady forward progress without competing for disk access with a system's primary applications. It opportunistically completes maintenance requests by using disk idle time and freeblock scheduling. In this paper, three disk maintenance applications (backup, write-back cache <b>destaging,</b> and disk layout reorganization) are adapted to the system support and evaluated on a FreeBSD implementation. All are shown to successfully execute in busy systems with minimal (e. g., < 2 %) impact on foreground disk performance. In fact, by modifying FreeBSD's cache to write dirty blocks for free, the average read cache miss response time is decreased by 15 [...] 30 %...|$|E
40|$|This paper {{describes}} a programming framework for clean construction of disk maintenance applications. They {{can use it}} to expose the disk activity to be done, and then process completed requests as they are reported. The system ensures that these applications make steady forward progress without competing for disk access with a system’s primary applications. It opportunistically completes maintenance requests by using disk idle time and freeblock scheduling. In this paper, three disk maintenance applications (backup, write-back cache <b>destaging,</b> and disk layout reorganization) are adapted to the system support and evaluated on a FreeBSD implementation. All are shown to successfully execute in busy systems with minimal (e. g., � 2 %) impact on foreground disk performance. In fact, by modifying FreeBSD’s cache to write dirty blocks for free, the average read cache miss response time is decreased by 15 – 30 %. For non-volatile caches, the reduction is almost 50 %. ...|$|E
50|$|When free {{disk space}} was {{required}} {{a group of}} cylinders were selected to be <b>destaged</b> to tape, these were transferred with minimal or no change of format. Each tape could store 202 cylinder images of 19 tracks each, half of the 404 cylinders in a 3330 disk pack. Cylinder locations on the tape were fixed and identified by markers along the edge.|$|R
5000|$|Writes from hosts are {{acknowledged}} {{once they}} have been committed into the SVC mirrored cache, but prior to being <b>destaged</b> to the underlying storage controllers. Data is protected by replication to the peer node in an I/O group (cluster node pair). Cache size {{is dependent on the}} SVC hardware model and installed options. Fast-write cache is especially useful to increase performance in midrange storage configurations.|$|R
40|$|A bstract — RAID- 5 arrays need 4 a%k accesses to upa%te a nkzta block [...] 2 to read old a%ta and parity, and 2 to wn”te new a%ta andparity. Schemes {{previously}} {{proposed to}} improve the upalzte performance of such arrays are the Log-Structured File System [1 OJ and the Floating Parity Approach [6]. Here, we consider a third approach, caIled Fast Wn”te, which eiim”nates disk time from the host response time to a write, by using a Non-Volatile Cache in the disk array controller. We exanu”ne three ahernatives for handling Fast Writes and describe a hierarchy of <b>destage</b> algorithms with increasing robustness to fm”lures. These <b>destage</b> algo-rithms are compared agm”nst those {{that would be used}} by a disk controller employing m“rroring. We show that array controllers require considerably more (2 to 3 times more) bus bandwidth and memory bandwidth thun do disk controi?ers that employ nu”rroring. So, array controllers that use parity {{are likely to be more}} expensive than controllers thut do m“rroring, though rm”rroring is more expensive when both controllers and disks are considered. 1...|$|R
40|$|Flash memory, in {{the form}} of Solid State Drive (SSD), is being {{increasingly}} employed in mobile and enterprise-level storage systems due to its superior features such as high energy efficiency, high random read performance and small form factor. However, SSD suffers from the erase-before-write and endurance problems, which limit the direct deployment of SSD in enterprise environment. Existing studies either develop SSD-friendly on-board buffer management algorithms, or design sophisticated Flash Translation Layers (FTL) to ease the erase-before-write problem. This dissertation addresses the two issues and consists of two parts. ^ The first part focuses on the white-box approaches that optimize the internal design of SSD. We design a write buffer management algorithm on top of the log-block FTL, which not only optimizes the write buffer effect by exploiting both the recency and frequency of blocks in the write buffer, but also minimizes the <b>destaging</b> overhead by maximizing the number of valid pages of the destaged block. We further identify that the low garbage collection efficiency problem has a significantly negative impact to the performance of the page-mapped SSD. We design a GC-Aware RAM management algorithm that improves the GC efficiency even if the workloads do not have updating requests by dynamically evaluating the benefits of different <b>destaging</b> policies and adaptively adopting the best one. Moreover, this algorithm minimizes the address translation overhead by exploiting the interplay between the buffer component and the FTL component. ^ The second part focuses on the black-box approaches that optimize the SSD performance externally. As an increasing number of applications deploy SSD in enterprise environment, understanding the performance characteristics of SSD in enterprise environment is becoming critically important. We identify several performance anomalies of SSDs and their performance and endurance impacts on SSD employed in enterprise environment by evaluating several commercial SSDs. Our study provides insights and suggestions to both system developers and SSD vendors. Further, based on the performance anomalies identified, we design an IO scheduler that takes advantage of the SSD features and evaluate its performance on SSD. The scheduler is shown to improve performance in terms of bandwidth and average response time. ...|$|E
40|$|In this paper, we show {{detailed}} analysis and performance {{evaluation of the}} Dynamic Hybrid GRACE Hash Join Method (DHGH Method) when the tuple distribution in buckets is unbalanced. The conventional Hash Join Methods specify the tuple distribution in buckets statically. However it may differ from estimation since join operations are applied with selection operations. When the tuple distribution in buckets is unbalanced, the processing cost of join operation becomes more costly than the ideal case when you use Hybrid Hash Join Method (HH Method). On the other hand, when you use the DHGH Method, the <b>destaging</b> buckets are selected dynamically, gives the same performance as the ideal case even if the tuple distribution in buckets is unbalanced such as Zipf-like distributions. We analyze the total I/O cost of a join operation at various number of buckets. The result shows {{that we have to}} determine the number of buckets baaed on the tuple distribution in buckets rather than the size of the source relation. It is shown that we had better partition the source relation using a large number of small buckets instead of the smaller number of buckets almost filling the whole main memory adopted in the HH Method. ...|$|E
40|$|Most {{file system}} {{performance}} enhancing techniques, {{such as the}} I/O buffer cache and the Log-structured File Systems (LFS), re-lying on caching data in volatile RAM {{for a period of}} time be-fore <b>destaging</b> them to disk. Such approaches pose a reliability problem. Non-volatile RAM (NVRAM) caches can be used to enhance the reliability. However, large NVRAM caches are very expensive and not as reliable as disks. This paper presents the design and implementation of a dif-ferent approach called RAPID-Cache (Redundant, Asymmetri-cally Parallel and Inexpensive Disk Cache) for highly-reliable systems. The new approach provides an inexpensive way to make the OS buffer cache non-volatile and highly reliable. Re-cent researches using simulation suggest that as a cache inside a disk array controller, RAPID-Cache has great reliability/cost advantages over conventional write cache structures. In order to validate whether RAPID-Cache can really achieve satisfac-tory performance in real systems, and to see if RAPID-Cache can also improve the performance/reliability of the OS buffer cache, we have designed and implemented RAPID-Cache for the Linux system. Measurement results using realistic benchmarks show very promising results. Using only 3. 25 MB of NVRAM, a RAPID-Cache outperforms normal NVRAM caches of 64 MB or even 128 MB. RAPID-Cache also provide better reliability than normal NVRAM caches. ...|$|E
40|$|Abstract—Flash memory SSDs pose a {{well-known}} challenge, that is, the erase-before-write problem. Researchers {{try to solve}} this inherent problem from two different angles by either designing sophisticated Flash Translation Layer (FTL) schemes to postpone and minimize erasures or designing flash-aware buffer management algorithms to absorb unnecessary erasures. Our experimental results show that buffer management inside SSD is necessary and indispensable. Further, based on our observation that TPC and some server workloads have strong temporal locality, this paper proposes a new flash-aware write-buffer management algorithm, called PUD-aware LRU algorithm (PUD-LRU), based on the Predicted average Update Distance (PUD) as the key block replacement criterion on top of log-block FTL schemes. The main idea of PUD-LRU is to differentiate blocks and judiciously <b>destage</b> blocks based on their frequency and recency {{so as to avoid}} the unnecessary erasures due to repetitive updates. To take advantage of the characteristics of log-block FTL and increase the erase efficiency, PUD-LRU maximizes the number of valid pages in the <b>destaged</b> block in each erase operation. Our trace-driven experimental results show that the PUD-LRU algorithm can reduce the number of erasures of the state-of-the-art buffer management algorithm BPLRU by up to 42 %, and average response time by up to 56 %, while reducing the two measures of the state-of-the-art page-mapping FTL scheme DFTL by up to 73 % and 75 %, respectively. I...|$|R
40|$|Abstract—Distributed sparing is {{a method}} to improve the {{performance}} of RAID 5 disk arrays {{with respect to a}} dedicated sparing system with N + 2 disks (including the spare disk), since it utilizes the bandwidth of all N + 2 disks. We analyze the performance of RAID 5 with distributed sparing in normal mode, degraded mode, and rebuild mode in an OLTP environment, which implies small reads and writes. The analysis in normal mode uses an M/G/ 1 queuing model, which takes into account the components of disk service time. In degraded mode, a low-cost approximate method is developed to estimate the mean response time of fork-join requests resulting from accesses to recreate lost data on the failed disk. Rebuild mode performance is analyzed by considering an M/G/ 1 vacationing server model with multiple vacations of different types to take into account differences in processing requirements for reading the first and subsequent tracks. An iterative solution method is used to estimate the mean response time of disk requests, as well as the time to read each disk, which is shown to be quite accurate through validation against simulation results. We next compare RAID 5 performance in a system 1) without a cache; 2) with a cache; and 3) with a nonvolatile storage (NVS) cache. The last configuration, in addition to improved read response time due to cache hits, provides a fast-write capability, such that dirty blocks can be <b>destaged</b> asynchronously and at a lower priority than read requests, resulting in an improvement in read response time. The small write penalty is also reduced due to the possibility of repeated writes to dirty blocks in the cache and by taking advantage of disk geometry to efficiently <b>destage</b> multiple blocks at a time. Index Terms—RAID 5 disk arrays, dedicated sparing, distributed sparing, disk failures, fault-tolerance, operation in degraded mode...|$|R
40|$|The {{following}} proposed wording represents {{changes to}} SAM- 3 {{to enable the}} transmission of priority information on a per-command basis. This proposal standardizes the interface by which device servers can offer differentiated quality of service to different applications associated with the same initiator. Examples of its use would include offering lower priority on IO associated with background <b>destage</b> writes within a storage controller or on IO associated with background applications, so that response time may be reduced for those IO operations that directly affect the responsiveness offered to the end user. The method defined in this proposal to accomplish this involves changes to the protocol standards to accommodate an extension to the task attribute field to allow different priorities to be assigned to simple task attributes. 2 Additions to SAM- 3 2. 1 The Execute Command procedure call An application client requests the processing of a SCSI command by invoking the SCSI transport protocol services described in 2. 4, the collective operation of which is conceptually modeled in the following procedure call...|$|R
40|$|Abstract. In an IaaS (Infrastructure-as-a-Service) cloud services, storage {{needs of}} VM (Virtual Machine) {{instances}} are met through virtual disks (i. e. virtual block devices). However, it is nontrivial to provide virtual disks to VMs in an efficient and scalable {{way for a}} couple of reasons. First, a VM host may be required to provide virtual disks for a large number of VMs. It is difficult to ascertain the largest possible storage demands and physically provision them all in the host machine. On the other hand, if the storage spaces for virtual disks are provided through remote storage servers, aggregate network traffic due to storage accesses from VMs can easily deplete the network bandwidth and cause congestion. We propose a system, vStore, which overcomes these issues by utilizing the host’s limited local disk space as a block-level cache for the remote storage in order to absorb network traffics from storage accesses. This allows the VMM (Virtual Ma-chine Monitor) to serve VMs ’ disk I/O requests from the host’s local disks most of the time, while providing the illusion of much larger storage space for creating new virtual disks. Caching is a well-studied topic in many different contexts, but caching virtual disks at block-level poses special challenges in achieving high performance while maintaining virtual disk semantics. First, after a disk write operation finishes from the VM’s perspective, the data should survive even if the host immediately encounters a power failure. Second, as disk I/O performance is dominated by disk seek times, {{it is important to keep}} a virtual disk as sequential as possible in the limited cache space. Third, the <b>destaging</b> operation that sends dirty pages back to the remote storage server should be self-adaptive and mini-mize the impact on the foreground traffic. We propose techniques to address these challenges and implemented them in Xen. Our evaluation shows that vStore pro-vides the illusion of unlimited storage space, significantly reduces network traffic, and incurs a low disk I/O performance overhead...|$|E
40|$|Recent {{studies show}} that disk-based storage {{subsystems}} account for a non-trivial portion of energy consumption in both low-end and high-end servers. Current energyefficient solutions work either at a disk drive level or at a storage system cache level without the knowledge of redundant information inside RAID systems and thus have certain limitations. In this paper, we develop a novel energy-efficient RAID system architecture called EERAID to significantly conserve energy {{by taking advantage of}} redundant information. To give a proof-of-concept solution, we develop new energy-efficient, redundancyaware I/O scheduling and controller-level cache management schemes for EERAID 1 and EERAID 5 respectively. EERAID 1 implements two new policies – Windows Round-Robin (WRR) and Power and Redundancy-Aware Flush (PRF), while EERAID 5 develops two novel schemes–Transformable Read (TRA) and Power and Redundancy-aware <b>Destage</b> (PRD). Comprehensive trace-driven simulation experiments have been conducted by replaying two real-world server traces and a wide spectrum server-side synthetic traces. Experimental results showed that, 1) for single-speed (conventional) disks, EERAID 1 and EERAID 5 can achieve up to 30 % and 11 % energy savings respectively, and 2) For multi-speed disks, compared with DRPM, EERAID 1 and EERAID 5 can achieve 22 % and 11 % extra energy savings. There is little performance degradation or even better performance in EERAID 1 and 5. ...|$|R
40|$|GÁŠEK Tomáš: The order {{processing}} by firm with usage of project management The project laboured {{within the field}} of study with the name "Production tech-nology and industrial management" describes some versions of the potential job order's progress in the manufactory at the firm "Speciální strojírna, a. s. ". A customer asks for the batch production of the casts, that he will be order step by step for working in regular intervals. The part of the customer's plan is the optimalization of the production and the press of the price, that should be lower after the finishing of the production technology. Along with the correction of the price, a customer expects cutting of the delivery time with the fully quali-ty of the working. In the first version, without a wide preparation, is all working realized on the general-purpose machines. The second version focuses on the production with use of drilling jigs, that is constructed and produced by the firm itself. The input costs of this preparation at the production will be compensated with the major production times, that will be <b>destage</b> from the expensive borer to the cheaper drilling-machine. The last third version supposes setting of the CNC machining centres and therethrough it will be reach to huge saving of the major and minor production times by the working...|$|R
40|$|Recent {{studies show}} that disk-based I/O {{subsystems}} account for a non-trivial portion of energy consumption in data-intensive environment such as storage servers and data centers. Previous powerefficient I/O solutions for a single disk drive or mobile computers {{cannot be applied to}} data-intensive environment where the I/O load is much more intensive. Current solutions seek help from multi-speed disks but this type of disk will not be on the market soon. Solutions have to be sought on extensions on conventional storage system techniques such as RAIDs. This paper seeks a novel way to build Energy-Efficient, high-performance RAID system called EERAID by exploiting inherent redundant information such as mirroring and parity at the RAID controller level. As representative mirroring and parity redundant RAID systems, RAID 1 and RAID 5 are particularly studied. For RAID 1, we develop a Windows Round Robin (WRR) request scheduling policy. For RAID 5, we introduce a Transformable Read policY(TRY) to transform disk requests from spun-down disks to active disks, and develop a Power and Redundant Aware <b>Destage</b> policy (PRAD) to defer writes to work together with TRY. Comprehensive trace-driven simulations show that EERAID can significantly conserve energy with a minor performance impairment, by 12 - 30 % energy saving for both small-scale and large scale RAID level 1, and 10 - 20 % energy saving for small-scale RAID level 5...|$|R

