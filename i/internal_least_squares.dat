0|10000|Public
40|$|Abstract:Through {{theoretical}} derivation, some {{properties of}} the total <b>least</b> <b>squares</b> estimation are found. The total <b>least</b> <b>squares</b> estimation is the linear transformation of the <b>least</b> <b>squares</b> estimation, and the total <b>least</b> <b>squares</b> estimation is unbiased. The condition number of the total <b>least</b> <b>squares</b> estimation {{is greater than the}} <b>least</b> <b>squares</b> estimation, so the total <b>least</b> <b>squares</b> estimation is easier to be affected by the data error than the <b>least</b> <b>squares</b> estimation. Then through the further derivation, the relationships of solutions, residuals and unit weight variance estimations between the total <b>least</b> <b>squares</b> and the <b>least</b> <b>squares</b> are given...|$|R
40|$|AbstractIn this note, {{we present}} two {{results on the}} scaled total <b>least</b> <b>squares</b> problem. First, we discuss the {{relation}} between the scaled total <b>least</b> <b>squares</b> and the <b>least</b> <b>squares</b> problems. We derive an upper bound for the difference between the scaled total <b>least</b> <b>squares</b> solution and the <b>least</b> <b>squares</b> solution and establish a quantitative relation between the scaled total <b>least</b> <b>squares</b> residual and the <b>least</b> <b>squares</b> residual. Second, we give a perturbation analysis of the scaled total <b>least</b> <b>squares</b> problem. Numerical experiments in comparing our results with existing results are demonstrated...|$|R
30|$|In this section, we {{investigate}} widely used fuzzy regression methods of Fuzzy <b>Least</b> <b>Squares</b> (FLS), General Fuzzy <b>Least</b> <b>Squares</b> (GFLS), Sakawa–Yano (SY), Hojati–Bector–Smimou (HBS), Approximate-Distance Fuzzy <b>Least</b> <b>Squares</b> (ADFLS) and Interval-Distance Fuzzy <b>Least</b> <b>Squares</b> (IDFLS).|$|R
40|$|Abstract: This Paper {{describes}} {{an application of}} Structured Total <b>Least</b> <b>Squares</b> method to the interpolation of terrain data. We briefly review the ideas of <b>Least</b> <b>Squares,</b> Total <b>Least</b> <b>Squares,</b> and Structured Total <b>Least</b> <b>Squares.</b> We illustrate the use of Structured Total <b>Least</b> <b>Squares</b> in the approximation of terrain surfaces using a novel discrete surface, the Triangular Regular Network. The Structured Total <b>Least</b> <b>Squares</b> algorithm allows us to deal with data corrupted by noise in every coordinate (x; y; z) ...|$|R
50|$|In {{mathematical}} statistics, polynomial <b>least</b> <b>squares</b> {{refers to}} {{a broad range of}} statistical methods for estimating an underlying polynomial that describes observations. These methods include polynomial regression, curve fitting, linear regression, <b>least</b> <b>squares,</b> ordinary <b>least</b> <b>squares,</b> simple linear regression, linear <b>least</b> <b>squares,</b> approximation theory and method of moments. Polynomial <b>least</b> <b>squares</b> has applications in radar trackers, estimation theory, signal processing, statistics, and econometrics.|$|R
40|$|In {{this paper}} we {{investigate}} the finite sample risk performance of feasible generalised <b>least</b> <b>squares</b> estimators applied in models with serially correlated error terms. The risk {{functions of the}} ordinary <b>least</b> <b>squares,</b> generalised <b>least</b> <b>squares</b> and feasible generalised <b>least</b> <b>squares</b> estimators are derived under the asymmetric Linear-Exponential loss function. A numerical evaluation using simulation is {{used to compare the}} risk functions. Our numerical results show that the relative risk gains of the feasible generalised <b>least</b> <b>squares</b> estimators over the ordinary <b>least</b> <b>squares</b> estimator increases with higher loss asymmetry, particularly for larger serial correlation coefficients. [URL]...|$|R
40|$|We {{show that}} the {{generalized}} total <b>least</b> <b>squares</b> (GTLS) problem with a singular noise covariance matrix {{is equivalent to the}} restricted total <b>least</b> <b>squares</b> (RTLS) problem and propose a recursive method for its numerical solution. The method is based on the generalized inverse iteration. The estimation error covariance matrix and the estimated augmented correction are also characterized and computed recursively. The algorithm is cheap to compute and is suitable for online implementation. Simulation results in <b>least</b> <b>squares</b> (LS), data <b>least</b> <b>squares</b> (DLS), total <b>least</b> <b>squares</b> (TLS), and RTLS noise scenarios show fast convergence of the parameter estimates to their optimal values obtained by corresponding batch algorithms. Index Terms total <b>least</b> <b>squares</b> (TLS), generalized total <b>least</b> <b>squares</b> (GTLS), restricted total <b>least</b> <b>squares</b> (RTLS), recursive estimation, subspace tracking, system identification. I...|$|R
40|$|The {{behavior}} of the t test in small samples for coefficient significance in time-series regressions is examined after using the Prais-Winsten (PW) and Cochrane-Orcutt (CO) corrections for autocorrelation. Results are compared to ordinary <b>least</b> <b>squares</b> and generalized <b>least</b> <b>squares.</b> Key words: First-order autocorrelation generalized <b>least</b> <b>squares,</b> ordinary <b>least</b> <b>squares,</b> Prais-Winsten...|$|R
40|$|This article studies weighted, generalized, <b>least</b> <b>squares</b> estimators {{in simple}} linear {{regression}} with serially correlated errors. Closed-form expressions of weighted <b>least</b> <b>squares</b> estimators and variances are presented under some common stationary autocorrelation settings, a first-order autoregression and a first-order moving-average. These explicit expressions also have appealing applications, including an efficient weighted <b>least</b> <b>squares</b> computation method and a new sufficient and necessary condition on the equality of weighted <b>least</b> <b>squares</b> estimators and ordinary <b>least</b> <b>squares</b> estimators...|$|R
5000|$|Two-stage <b>least</b> <b>squares,</b> three-stage <b>least</b> <b>squares,</b> and {{seemingly}} unrelated regressions.|$|R
5000|$|Finding {{the tree}} and branch lengths {{minimizing}} the <b>least</b> <b>squares</b> residual is an NP-complete problem. However, for a given tree, the optimal branch lengths can be determined in [...] time for ordinary <b>least</b> <b>squares,</b> [...] time for weighted <b>least</b> <b>squares,</b> and [...] time for generalised <b>least</b> <b>squares</b> (given the inverse of the covariance matrix).|$|R
50|$|For details {{concerning}} nonlinear {{data modeling}} see <b>least</b> <b>squares</b> and non-linear <b>least</b> <b>squares.</b>|$|R
40|$|Preface Examples of the General Linear Model Introduction One-Sample Problem Simple Linear Regression Multiple Regression One-Way ANOVA First Discussion The Two-Way Nested Model Two-Way Crossed Model Analysis of Covariance Autoregression Discussion The Linear <b>Least</b> <b>Squares</b> Problem The Normal Equations The Geometry of <b>Least</b> <b>Squares</b> Reparameterization Gram-Schmidt Orthonormalization Estimability and <b>Least</b> <b>Squares</b> Estimators Assumptions for the Linear Mean Model Confounding, Identifiability, and Estimability Estimability and <b>Least</b> <b>Squares</b> Estimators...|$|R
40|$|AbstractThis article {{surveys the}} history, development, and {{applications}} of <b>least</b> <b>squares,</b> including ordinary, constrained, weighted, and total <b>least</b> <b>squares.</b> The presentation includes proofs {{of the basic}} theory, in particular, unitary factorizations and singular-value decompositions of matrices. Numerical examples with real data demonstrate {{how to set up}} and solve several types of problems of <b>least</b> <b>squares.</b> The bibliography lists comprehensive sources for more specialized aspects of <b>least</b> <b>squares...</b>|$|R
40|$|<b>Least</b> <b>squares</b> estimations {{have been}} used {{extensively}} in many applications system identification and signal prediction. These applications, the <b>least</b> <b>squares</b> estimators can usually be found by solving Toeplitz <b>least</b> <b>squares</b> problems. We present fast algorithms for solving the Toeplitz <b>least</b> <b>squares</b> problems. The algorithm is derived by using the displacement representation of the normal equations matrix. Numerical experiments show that these algorithms are efficient. published_or_final_versio...|$|R
40|$|The paper {{presents}} a comparison {{study for the}} performances of seven wind estimation algorithms for spaceborne scatterometers. These algorithms are weighted <b>least</b> <b>square</b> in log domain, maximum-likelihood, <b>least</b> <b>square</b> weighted <b>least</b> <b>square,</b> adjustable weighted <b>least</b> <b>square,</b> L 1 norm, and <b>least</b> wind speed <b>square</b> algorithms using radar scatterometer measurements. For each algorithm, the system performance simulation results are presented for the NASA scatterometer system planned to be launched in the 1990 's...|$|R
40|$|The {{performance}} of several algorithms for positioning a single sensor node based on distance estimates to {{it from a}} number of nodes at known positions (anchor nodes) is compared when the distance estimates are obtained from a measurement campaign. The distance estimates are based on timeof-arrival measurements done by ultrawideband devices in an indoor office environment. The compared algorithms are based on nonlinear <b>least</b> <b>squares,</b> <b>least</b> <b>squares</b> and total <b>least</b> <b>squares</b> after data preprocessing, and projection methods. No algorithm is uniformly best; however, <b>least</b> <b>squares</b> and total <b>least</b> <b>squares</b> after data preprocessing show higher mean squared errors in almost all cases, while the nonlinear <b>least</b> <b>squares</b> and projection methods have similar performance; projection methods performs slightly better. 1...|$|R
50|$|The {{discrete}} <b>least</b> <b>squares</b> meshless (DLSM) {{method is}} a newly introduced meshless method {{based on the}} <b>least</b> <b>squares</b> concept. The method {{is based on the}} minimization of a <b>least</b> <b>squares</b> functional defined as the weighted summation of the squared residual of the governing differential equation and its boundary conditions at nodal points used to discretize the domain and its boundaries. While most of the existing meshless methods need background cells for numerical integration, DLSM did not require numerical integration procedure due to the use of discrete <b>least</b> <b>squares</b> method to discretize the governing differential equation. A Moving <b>least</b> <b>squares</b> (MLS) approximation method is used to construct the shape function making the approach a fully <b>least</b> <b>squares</b> based approach.|$|R
40|$|In this correspondence, we {{introduce}} a minimax regret criteria to the <b>least</b> <b>squares</b> problems with bounded data uncertainties and solve it using semi-definite programming. We investigate a robust minimax <b>least</b> <b>squares</b> approach that minimizes a worst case difference regret. The regret {{is defined as}} the difference between a squared data error and the smallest attainable squared data error of a <b>least</b> <b>squares</b> estimator. We then propose a robust regularized <b>least</b> <b>squares</b> approach to the regularized <b>least</b> <b>squares</b> problem under data uncertainties by using a similar framework. We show that both unstructured and structured robust <b>least</b> <b>squares</b> problems and robust regularized <b>least</b> <b>squares</b> problem can be put in certain semi-definite programming forms. Through several simulations, we demonstrate the merits of the proposed algorithms with respect to the the well-known alternatives in the literature. Comment: Submitted to the IEEE Transactions on Signal Processin...|$|R
40|$|In {{previous}} work we introduced a construction to produce biorthogonal multiresolutions from given subdivisions. The approach involved estimating {{the solution to}} a <b>least</b> <b>squares</b> problem {{by means of a}} number of smaller <b>least</b> <b>squares</b> approximations on local portions of the data. In this work we use a result by Dahlquist, et. al. on the method of averages to make observational comparisons between this local <b>least</b> <b>squares</b> estimation and full <b>least</b> <b>squares</b> approximation. We have explored examples in two problem domains: data reduction and data approximation. We observe that, particularly for design matrices with a repetitive pattern of column entries, the <b>least</b> <b>squares</b> solution is often well estimated by local <b>least</b> <b>squares,</b> that the estimation rapidly improves with the size of the local <b>least</b> <b>squares</b> problems, and that the quality of the estimate is largely independent {{of the size of the}} full problem...|$|R
5000|$|The {{recursive}} <b>least</b> <b>squares</b> algorithm considers {{an online}} {{approach to the}} <b>least</b> <b>squares</b> problem. It can be shown that by initialising [...] and , {{the solution of the}} linear <b>least</b> <b>squares</b> problem given in the previous section can be computed by the following iteration: ...|$|R
40|$|It {{is shown}} that, in {{comparison}} to the results obtained from a conventional <b>least</b> <b>squares</b> approach, a total <b>least</b> <b>squares</b> solution leads to significant improvements in the geometry and appearance of images synthesised in a linear combination of views procedure. Use of the total <b>least</b> <b>squares</b> criterion is appropriate when errors on the control points are independently and identically distributed between the basis images and the target image being synthesised. When this is not be the case it is pointed out that the generalised total <b>least</b> <b>squares</b> procedure should be used. A synthetic object is used to evaluate the improvement in geometric accuracy obtained by use of the total <b>least</b> <b>squares</b> solution {{in comparison to}} a classical <b>least</b> <b>squares</b> method. Simulated and real images of laboratory test objects are similarly used to illustrate the improvement in appearance of images reconstructed by means of the total <b>least</b> <b>squares</b> procedure. ...|$|R
40|$|Critical random {{coefficient}} AR(1) {{processes are}} investigated where the random coefficient is binary, taking values - 1 and 1. Asymptotic behavior of <b>least</b> <b>squares</b> estimator for {{the mean of}} the random coefficient is discussed. Ordinary <b>least</b> <b>squares</b> estimator is shown to be consistent. Weighted <b>least</b> <b>squares</b> estimator turns out to be asymptotically normally distributed. This enables us to present a unified limit result for the weighted <b>least</b> <b>squares</b> estimator valid for the stationary, explosive and critical cases. Also, a test of criticality is discussed. Critical process Random coefficient AR(1) Test of criticality Weighted and ordinary <b>least</b> <b>squares...</b>|$|R
50|$|Total <b>Least</b> <b>Squares</b> DMD: Total <b>Least</b> <b>Squares</b> DMD is {{a recent}} {{modification}} of Exact DMD meant to address issues of robustness to measurement noise in the data. In, the authors interpret the Exact DMD as a regression problem that is solved using ordinary <b>least</b> <b>squares</b> (OLS), which assumes that the regressors are noise free. This assumption creates a bias in the DMD eigenvalues when it is applied to experimental data sets where all of the observations are noisy. Total <b>least</b> <b>squares</b> DMD replaces the OLS problem with a total <b>least</b> <b>squares</b> problem, which eliminates this bias.|$|R
40|$|M. Com. (Econometrics) The {{objective}} {{of this study is}} to evaluate different estimation techniques that can be used to estimate the coefficients of a model. The estimation techniques were applied to empirical data drawn from the South African economy. The Monte Carlo studies are unique in that data was statistically generated for the experiments. This approach was due to the fact that actual observations on economic variables contain several econometric problems, such as autocorrelation and MUlticollinearity, simultaneously. However, the approach in this study differs in that empirical data is used to evaluate the estimation techniques. The estimation techniques evaluated are : • Ordinary <b>least</b> <b>squares</b> method • Two stage <b>least</b> <b>squares</b> method • Limited information maximum likelihood method • Three stage <b>least</b> <b>squares</b> method • Full information maximum likelihood method. The estimates of the different coefficients are evaluated on the following criteria : • The bias of the estimates • The variance of the estimates • t-values of the estimates • The root mean square error. The ranking of the estimation techniques on the bias criterion is as follows : 1 Full information maximum likelihood method. 2 Ordinary <b>least</b> <b>squares</b> method 3 Three stage <b>least</b> <b>squares</b> method 4 Two stage <b>least</b> <b>squares</b> method 5 Limited information maximum likelihood method The ranking of the estimation techniques on the variance criterion is as follows : 1 Full information maximum likelihood method. 2 Ordinary <b>least</b> <b>squares</b> method 3 Three stage <b>least</b> <b>squares</b> method 4 Two stage <b>least</b> <b>squares</b> method 5 Limited information maximum. likelihood method All the estimation techniques performed poorly with regard to the statistical significance of the estimates. The ranking of the estimation techniques on the t-values of the estimates is thus as follows 1 Three stage <b>least</b> <b>squares</b> method 2 ordinary <b>least</b> <b>squares</b> method 3 Two stage <b>least</b> <b>squares</b> method and the limited information maximum likelihood method 4 Full information maximum likelihood method. The ranking of the estimation techniques on the root mean square error criterion is as follows : 1 Full information maximum likelihood method and the ordinary <b>least</b> <b>squares</b> method 2 Two stage <b>least</b> <b>squares</b> method 3 Limited information maximum likelihood method and the three stage <b>least</b> <b>squares</b> method The results achieved in this study are very similar to those of the Monte Carlo studies. The only exception is the ordinary <b>least</b> <b>squares</b> method that performed better on every criteria dealt with in this study. Though the full information maximum likelihood method performed the best on two of the criteria, its performance was extremely poor on the t-value criterion. The ordinary <b>least</b> <b>squares</b> method is shown, in this study, to be the most constant performer...|$|R
40|$|AbstractWe {{study the}} {{convergence}} of discrete and penalized <b>least</b> <b>squares</b> spherical splines in spaces with stable local bases. We derive a bound for error in the approximation of a sufficiently smooth function by the discrete and penalized <b>least</b> <b>squares</b> splines. The error bound for the discrete <b>least</b> <b>squares</b> splines is explicitly dependent on the mesh size of the underlying triangulation. The error bound for the penalized <b>least</b> <b>squares</b> splines additionally depends on the penalty parameter...|$|R
40|$|This paper {{focuses on}} the {{iterative}} identification problems for a class of Hammerstein nonlinear systems. By decomposing the system into two fictitious subsystems, a decomposition-based <b>least</b> <b>squares</b> iterative algorithm is presented for estimating the parameter vector in each subsystem. Moreover, a data filtering-based decomposition <b>least</b> <b>squares</b> iterative algorithm is proposed. The simulation {{results indicate that the}} data filtering-based <b>least</b> <b>squares</b> iterative algorithm can generate more accurate parameter estimates than the <b>least</b> <b>squares</b> iterative algorithm...|$|R
40|$|Partial <b>least</b> <b>squares</b> {{regression}} {{has been}} an alternative to ordinary <b>least</b> <b>squares</b> for handling multicollinearity in several areas of scientific research since the 1960 s. It has recently gained much attention {{in the analysis of}} high dimensional genomic data. We show that known asymptotic consistency of the partial <b>least</b> <b>squares</b> estimator for a univariate response does not hold with the very large "p" and small "n" paradigm. We derive a similar result for a multivariate response regression with partial <b>least</b> <b>squares.</b> We then propose a sparse partial <b>least</b> <b>squares</b> formulation which aims simultaneously to achieve good predictive performance and variable selection by producing sparse linear combinations of the original predictors. We provide an efficient implementation of sparse partial <b>least</b> <b>squares</b> regression and compare it with well-known variable selection and dimension reduction approaches via simulation experiments. We illustrate the practical utility of sparse partial <b>least</b> <b>squares</b> regression in a joint analysis of gene expression and genomewide binding data. Copyright Journal compilation (c) 2010 Royal Statistical Society. ...|$|R
40|$|Generalisation {{properties}} of support vector machines, orthogonal <b>least</b> <b>squares</b> and other variants of the orthogonal <b>least</b> <b>squares</b> algorithms are studied in this paper. In particular the zero-order regularised orthogonal <b>least</b> <b>squares</b> algorithm {{that has been}} proposed in (Chen et al. 1996) and the first order regularised orthogonal <b>least</b> <b>squares</b> algorithm which can be obtained using the cost function support vector machines will be discussed. Simple noisy sine and sinx functions are used to show that overfitting in the orthogonal <b>least</b> <b>squares</b> algorithm can be greatly reduced if the free parameters of the algorithm are selected properly. Results on three chaotic time series show that the orthiogonal <b>least</b> <b>squares</b> algorithm is slightly inferior {{compared to the other}} three algorithms. However, the strength of the orthogonal <b>least</b> <b>squares</b> algorithm lies in the ability to obtain a very concise or parsimonious model and the algorithm has the fewest number of free parameters compared to the other algorithms...|$|R
5000|$|Regression: <b>least</b> <b>squares,</b> ridge regression, <b>least</b> angle regression, elastic net, kernel ridge regression, support vector {{machines}} (SVM), partial <b>least</b> <b>squares</b> (PLS) ...|$|R
40|$|This paper {{constructs}} root-n consistent weighted <b>least</b> <b>squares</b> estimates with random weights of the finite-dimensional parameter in the partly {{linear regression}} model with heteroscedastic errors. These new estimates have smaller asymptotic dispersion than the <b>least</b> <b>squares</b> type estimates previously constructed in these models. Weighted <b>least</b> <b>squares</b> spline estimates Heteroscedasticity...|$|R
40|$|We {{present a}} new {{approach}} to univariate partial <b>least</b> <b>squares</b> regression (PLSR) based on directional signal-to-noise ratios (SNRs). We show how PLSR, unlike principal components regression, takes into account the actual value and not only the variance of the ordinary <b>least</b> <b>squares</b> (OLS) estimator. We find an orthogonal sequence of directions associated with decreasing SNR. Then, we state partial <b>least</b> <b>squares</b> estimators as <b>least</b> <b>squares</b> estimators constrained to be null on the last directions. We also give another procedure that shows how PLSR rebuilds the OLS estimator iteratively by seeking at each step the direction with the largest difference of signals over the noise. The latter approach does not involve any arbitrary scale or orthogonality constraints. Biased regression Constrained <b>least</b> <b>squares</b> Regression on components Partial <b>least</b> <b>squares</b> Principal components Shrinkage...|$|R
40|$|Numerical {{methods for}} the {{solution}} of the structured data <b>least</b> <b>squares</b> problem with special application in digital filtering are investigated. While the minimum mean-square error, i. e. ordinary <b>least</b> <b>squares</b> formulation, solves the linear system of equations for the case of noise in the right hand side, data <b>least</b> <b>squares</b> is formulated for the problem with noise in the coefficient matrix. For {{the solution of}} the channel equalization problem of a linear time invariant channel, the coefficient matrix, and hence the error in the coefficient matrix, possesses Hankel structure. Experimental verification demonstrates that the imposition {{of the structure of the}} error in the formulation, structured data <b>least</b> <b>squares,</b> generates more accurate solutions than are achieved by either standard ordinary <b>least</b> <b>squares</b> or data <b>least</b> <b>squares,</b> for signals with high signal to noise ratios...|$|R
3000|$|The {{goodness}} of the <b>least</b> <b>square</b> fitting is typically measured using the estimated Chi-square value, {{that is the}} <b>least</b> <b>squared</b> value, [...]...|$|R
40|$|For {{testing the}} {{equality}} of variances for correlated variables, Harris (1985) suggested Wald type test statistics. In this note we show that this test coincides with the generalized <b>least</b> <b>squares</b> statistic employed {{in the analysis of}} covariance structures. We derive the generalized <b>least</b> <b>squares</b> estimator of the common variance. This estimator is compared with the average sample variances which is the ordinary <b>least</b> <b>squares</b> estimator for this problem. Covariance structures generalized <b>least</b> <b>squares</b> test of homogeneity...|$|R
40|$|The {{standard}} {{approaches to}} solving an overdetermined linear system Ax ≈ b find minimal corrections to the vector b and/or the matrix A {{such that the}} corrected system is consistent, such as the <b>least</b> <b>squares</b> (LS), the data <b>least</b> <b>squares</b> (DLS) and the total <b>least</b> <b>squares</b> (TLS). The scaled total <b>least</b> <b>squares</b> (STLS) method unifies the LS, DLS and TLS methods. The classical normwise condition numbers for the LS problem have been widely studied. However, ther...|$|R
