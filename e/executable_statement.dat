10|26|Public
500|$|The DATA step has {{executable}} {{statements that}} result in the software taking an action, and declarative statements that provide instructions to read a data set or alter the data's appearance. The DATA step has two phases: compilation and execution. In the compilation phase, declarative statements are processed and syntax errors are identified. Afterwards, the execution phase processes each <b>executable</b> <b>statement</b> sequentially. Data sets are organized into tables with rows called [...] "observations" [...] and columns called [...] "variables". Additionally, each piece of data has a descriptor and a value.|$|E
50|$|Blocks can be nested - i.e. {{because a}} block is an <b>executable</b> <b>statement,</b> it can appear in another block {{wherever}} an <b>executable</b> <b>statement</b> is allowed. A block can {{be submitted to}} an interactive tool (such as SQL*Plus) or embedded within an Oracle Precompiler or OCI program. The interactive tool or program runs the block once. The block is not stored in the database, and for that reason, it is called an anonymous block (even {{if it has a}} label).|$|E
50|$|An {{action is}} the {{specification}} of an <b>executable</b> <b>statement</b> {{and is the}} fundamental unit of processing or behavior in an activity node that represents some transformation in the modeled system.|$|E
5000|$|The {{order of}} <b>executable</b> <b>statements</b> must be {{explicitly}} represented and well defined.|$|R
5000|$|Note {{that this}} sample {{includes}} only the <b>executable</b> <b>statements</b> of the program, the [...] section. The record fields [...] and [...] {{would have been}} defined in the [...] section, which did not use English-like syntax.|$|R
50|$|Second, FLOW-MATIC was {{the first}} system to {{distinctly}} separate the description of data from the operations on it. Its data definition language, unlike its <b>executable</b> <b>statements,</b> was not English-like; rather, data structures were defined by filling in pre-printed forms.|$|R
5000|$|... <<label>> [...] -- this is {{optional}}DECLARE-- {{this section}} is optional number1 NUMBER(2); number2 number1%TYPE := 17; -- value default text1 VARCHAR2(12) := 'Hello world'; text2 DATE := SYSDATE; -- current date and timeBEGIN-- this section is mandatory, must contain {{at least one}} <b>executable</b> <b>statement</b> SELECT street_number INTO number1 FROM address WHERE name = 'INU';EXCEPTION-- this section is optional WHEN OTHERS THEN DBMS_OUTPUT.PUT_LINE('Error Code is ' || TO_CHAR(sqlcode [...] ) [...] ); DBMS_OUTPUT.PUT_LINE('Error Message is ' || sqlerrm [...] );END; ...|$|E
5000|$|Each step {{consists}} {{of a series of}} statements. The DATA step has executable statements that result in the software taking an action, and declarative statements that provide instructions to read a data set or alter the data's appearance. The DATA step has two phases, compilation and execution. In the compilation phase, declarative statements are processed and syntax errors are identified. Afterwards, the execution phase processes each <b>executable</b> <b>statement</b> sequentially. Data sets are organized into tables with rows called [...] "observations" [...] and columns called [...] "variables". Additionally, each piece of data has a descriptor and a value.|$|E
5000|$|In Pascal {{there is}} no return statement. (However, in newer Pascals, the [...] {{can be used to}} return a value immediately. Without parameters, it just breaks out the procedure.) A {{subroutine}} automatically returns when execution reaches its last <b>executable</b> <b>statement.</b> Values may be returned by assigning to an identifier that has the same name as the subroutine, a function in Pascal terminology. This way the function identifier is used for recursive calls and as result holder; this is syntactically similar to an explicit output parameter. The same syntax is used in Fortran 66 and Fortran 77. In some other languages a user defined result variable is used instead of the function identifier.|$|E
40|$|AbstractThe Lanczos {{algorithm}} {{is used to}} compute some eigenvalues of a given symmetric matrix of large order. At {{each step of the}} Lanczos algorithm it is valuable to know which eigenvalues of the associated tridiagonal matrix have stabilized at eigenvalues of the given symmetric matrix. We present a robust algorithm which is fast (20 j to 40 j operations at the jth Lanczos step), uses about 30 words of extra storage, and has a fairly short program (approxiamately 200 <b>executable</b> <b>statements...</b>|$|R
40|$|It {{is shown}} that MS Fortran- 77 compilers allow to {{construct}} recursive subroutines. The recursive one-dimensional adaptive quadrature subroutine is considered in particular. Despite its extremely short body (only eleven <b>executable</b> <b>statements)</b> the subroutine {{proved to be}} very effective and competitive. It was tested on various rather complex integrands. The possibility of function calls number minimization by choosing the optimal number of Gaussian abscissas is considered. The proposed recursive procedure can be effectively applied for creating more sophisticated quadrature codes (one- or multi-dimensional) and easily incorporated into existing programs. ...|$|R
40|$|Based on an {{empirical}} study {{of more than}} 10, 000 lines of program text written in a GOTO-less language, a machine architecture specifically designed for structured programs is proposed. Since assignment, CALL, RETURN, and IF statements together account for 93 percent of all <b>executable</b> <b>statements,</b> special care is given to ensure that these statements can be implemented efficiently. A highly compact instruction encoding scheme is presented, which can reduce program size by a factor of 3. Unlike a Huffman code, which utilizes variable length fields, this method uses only fixed length (1 -byte) opcode and address fields. The most frequent instructions consist of a single 1 -byte field. As a consequence, instruction decoding time is minimized, and the machine is efficient with respect to both space and time. Â© 1978, ACM. All rights reserved...|$|R
3000|$|... (1) Is Solved, (2) LOC, (3) Boolean Expression Complexity, (4) Class Data Abstraction Coupling, (5) Class Fan Out Complexity, (6) Cyclomatic Complexity, (7) <b>Executable</b> <b>Statement</b> Count, (8) Max Len file, (9) Max Len method, (10) Max Line Len, (11) Max Outer Types, (12) Max Param, (13) NCSS Class, (14) NCSS File, (15) NCSS Method, (16) Npath Complexity, and (17) Too Many Methods.|$|E
40|$|Since both cost/quality and {{production}} environments differ, this study presents an approach for customizing a characteristic set of software metrics to an environment. The approach is {{applied in the}} Software Engineering Laboratory (SEL), a NASA Goddard production environment, to 49 candidate process and product metrics of 652 modules from six (51, 000 to 112, 000 lines) projects. For this particular environment, the method yielded the characteristic metric set (source lines, fault correction effort per <b>executable</b> <b>statement,</b> design effort, code effort, number of I/O parameters, number of versions). The uses examined for a characteristic metric set include forecasting the effort for development, modification, and fault correction of modules based on historical data...|$|E
40|$|We have {{developed}} a system for clinical trial eligibility determination where patients or primary care providers can enter clinical information about a patient and obtain a ranked list of clinical trials for which the patient {{is likely to be}} eligible. We used clinical trial eligibility information from the National Cancer Institute's Physician Data Query (PDQ) database. We translated each free-text eligibility criterion into a machine <b>executable</b> <b>statement</b> using a derivation of the Arden Syntax. Clinical trial protocols were then structured as collections of these eligibility criteria using XML. The application compares the entered patient information against each of the eligibility criteria and returns a numerical score. Results are displayed in order of likelihood of match. We have tested our system using all phase II and III clinical trials for treatment of metastatic breast cancer found in the PDQ database. Preliminary results are encouraging...|$|E
40|$|We {{investigate}} {{the evolution of}} a medium sized software package, LAPACK, through its public releases over the last six years and establish a correlation, at a subprogram level, between a simply computable software metric value and the number of coding errors detected in the released routines. We also quantify the code changes made between issues of the package and attempt to categorize the reasons for these changes. We then consider the testing strategy used with LAPACK. Currently this consists {{of a large number of}} mainly self-checking driver programs along with sets of configuration files. These suites of test codes run a very large number of test cases and consume significant amounts of cpu time. We attempt to quantify how successful this testing strategy is from the viewpoint of the coverage of the <b>executable</b> <b>statements</b> within the routines being tested...|$|R
40|$|In the correct-by-construction {{programming}} methodology, {{programs are}} incrementally derived from their formal specifications, by repeatedly applying transformations to partially derived programs. At an intermediate {{stage in a}} derivation, users may have to make certain assumptions to proceed further. To ensure that the assumptions hold true {{at that point in}} the program, certain other assumptions may need to be introduced upstream as loop invariants or preconditions. Typically these other assumptions are made in an ad hoc fashion and may result in unnecessary rework, or worse, complete exclusion of some of the alternative solutions. In this work, we present rules for propagating assumptions through annotated programs. We show how these rules can be integrated in a top-down derivation methodology to provide a systematic approach for propagating the assumptions, materializing them with <b>executable</b> <b>statements</b> at a place different from the place of introduction, and strengthening of loop invariants with minimal additional proof efforts...|$|R
40|$|Software {{engineers}} {{have been trying}} for years to develop a software synthesis system that can transform a formal specification model to a design model from which executable code can be generated. AFIT wide spectrum object modeling environment (AWESOME) is one result of their research. AWESOME presents a formal model as an abstract syntax tree. This model consists mainly of object class specifications. The methods in these classes are specified using pre and post-conditions. The intent of this thesis is to support the transformation of post-conditions to code statements. A post-condition is first categorized as dependent or independent relative to other post-conditions. Post-conditions are further divided into actions and constraints. Actions {{can be converted to}} <b>executable</b> <b>statements.</b> Constraints can be converted to pre-conditions using weakest pre-condition analysis. Functions have been designed to categorize the post-conditions. Transforms have been designed to simplify the post-conditions and to determine the weakest pre-condition and add it to the method. The result is a design model from which executable code can be generated...|$|R
40|$|Similar code {{may exist}} in large {{software}} projects due to some common software engineering practices, such as copying and pasting code and n-version programming. Although previous work has studied syntactic equivalence and small-scale, coarse-grained program-level and function-level semantic equivalence, {{it is not}} known whether significant fine-grained, code-level semantic duplications exist. Detecting such semantic equivalence is also desirable because it can enable many applications such as code understanding, maintenance, and optimization. In this paper, we introduce the first algorithm to automatically mine functionally equivalent code fragments of arbitrary size - down to an <b>executable</b> <b>statement.</b> Our notion of functional equivalence is based on input and output behavior. Inspired by Schwartz 2 Ì 7 s randomized polynomial identity testing, we develop our core algorithm using automated random testing: (1) candidate code fragments are automatically extracted from the input program; and (2) random inputs are generated to partition the code fragments based on their output values on the generated inputs. We implemented the algorithm and conducted a large-scale empirical evaluation of it on the Linux kernel 2. 6. 24. Our results show that there exist many functionally equivalent code fragments that are syntactically different (i. e., they are unlikely due to copying and pasting code). The algorithm also scales to million-line programs; it was able to analyze the Linux kernel with several days of parallel processing...|$|E
40|$|Transforming {{guideline}} recommendations into <b>executable</b> <b>statements</b> for computerized {{decision support}} systems requires {{a clear understanding}} of what tasks must be performed. We sought (a) to determine whether a limited set of action types could be defined to comprehensively categorize activities recommended by the majority of clinical guidelines, (b) to describe the relative frequency of these action types, and (c) to create a library of recommendations for future validation activities. We randomly selected test and validation sets of 50 recommendations each from the National Guideline Clearinghouse and randomly extracted 3 recommendations from each guideline. We tested the ability of a preliminary palette of action types to categorize guideline-prescribed activities and expanded it to accommodate several unanticipated actions. Ultimately, the following actions were sufficient to categorize all 405 actions: Prescribe, Perform therapeutic procedure, Educate/Counsel, Test, Dispose, Refer/Consult, Conclude, Monitor, Document, Advocate, Prepare, and No recommendation. These action types can be used to construct a framework for design of clinical decision support systems...|$|R
40|$|This paper {{describes}} the features {{and implementation of}} our automatic data distribution research tool. The tool (DDT) accepts programs written in Fortran 77 and generates HPF directives and <b>executable</b> <b>statements.</b> DDT works by identifying a set of computational phases (procedures and loops). The algorithm builds a search space of candidate solutions for these phases which is explored looking for their combination that minimize the overall cost; this cost includes movement cost and computation cost. The data movement cost includes the cost of executing each phase with a given mapping and the remapping costs {{that have to be}} paid in order to execute each phase with the mapping selected. The computation cost includes the cost of executing each phase in parallel according to the mapping selected and the owner computes rule. Control flow information is used to identify how phases are sequenced during the execution of the application. 1 Introduction Data distribution is one of the topics of [...] ...|$|R
40|$|This paper {{presents}} an efficient classification algorithm for categorizing evolutionary organisms using slicing techniques. Dynamic slicing excels in tracing out dependencies between <b>executable</b> <b>statements.</b> The {{nature of these}} dependencies aids in the determination of control statements in a program. Dynamic slicing technique imbibes the run time execution trace based on a slicing criterion. Dynamic slicing algorithms can trace both the backward and forward dependencies. The UML model is automatically generated from the source code to validate the forward and backward dynamic slicing algorithm. This paper shows the algorithmic implementation in NetBeans IDE 7. 4. It provides a new platform for automated software engineering. The algorithm efficiently discovers the evolutionary relationship between organisms. Forward dynamic slicing algorithm helps in identifying the successors of the organisms and the backward dynamic slicing algorithm finds out the predecessors of the evolutionary organisms. Both the algorithms are based on dynamic slicing criterion at the run time execution trace. The integration of these phylogenetic algorithms deciphers the building complexity of the evolutionary organisms. It proves to have an advantageous classification encasement for jeopardized species...|$|R
40|$|This article {{describes}} the main features and implementation of our automatic data distribution research tool. The tool (DDT) accepts programs written in Fortran 77 and generates High Performance Fortran (HPF) directives to map arrays onto {{the memories of the}} processors and parallelize loops, and <b>executable</b> <b>statements</b> to remap these arrays. DDT works by identifying a set of computational phases (procedures and loops). The algorithm builds a search space of candidate solutions for these phases which is explored looking for the combination that minimizes the overall cost; this cost includes data movement cost and computation cost. The movement cost reflects the cost of accessing remote data during the execution of a phase and the remapping costs that have to be paid in order to execute the phase with the selected mapping. The computation cost includes the cost of executing a phase in parallel according to the selected mapping and the owner computes rule. The tool supports interprocedural analysis and uses control flow information to identify how phases are sequenced during the execution of the application. Peer ReviewedPostprint (published version...|$|R
40|$|A {{conceptual}} model is discussed {{which allows the}} hierarchic definition of high-level input driven objects, called input-output tools, from any set of basic input primitives. An input-output tool {{is defined as a}} named object. Its most important elements are the input rule, output rule, internal tool definitions, and a tool body consisting of <b>executable</b> <b>statements.</b> The input rule contains an expression with tool designators as operands and with operators allowing for sequencing, selection, interleaving, and repetition. Input rules are similar in appearance to production rules in grammars. The input expression specifies one or more input sequences, or input patterns, in terms of tool designators. An input parser tries, at run-time, to match (physical) input tokens against active input sequences. If a match between an input token and a tool designator is found, the corresponding tool body is executed, and the output is generated according to specifications in the tool body. The control structures in the input expression allow a variety of input patterns from any number of sources. Tool definitions may occur in-line or be stored in a library. All tools are ultimately encompassed in one tool representing the program...|$|R
40|$|The basic {{features}} of object-oriented software i. e. encapsulation, inheritance and poly- morphism makes it dicult to apply traditional testing methods to them. Traditional testing {{methods have been}} successfully implemented in procedural systems. One {{of the most commonly}} used example of white-box testing is basis path testing which ensures that every path of a program is executed at least once. Control Flow Graph(CFG) is a very well-known model that is used for identication of basis paths in procedural systems. McCabes cyclomatic complexity(CC) metric deter- mines that number of linearly independent paths through a piece of software using the control ow graph(CFG) to determine a set of test cases which will cause <b>executable</b> <b>statements</b> to be executed at least once. The major challenge here is calculation of cyclomatic complexity is easy for procedural systems, but due to basic properties of object-oriented system it is dicult. My work implements a new model named as Extended Control Flow Graph for code based analysis of object-oriented software. ECFG is a layered CFG where nodes refer to methods rather than statements. My work also implements the calculation of a new metric Extended Cyclomatic Complexity (E-CC) ...|$|R
50|$|The {{selection}} structure {{contains two}} execution sequences, each {{preceded by a}} double colon. One sequence from the list will be executed. A sequence can be selected only if its first <b>statement</b> is <b>executable.</b> The first <b>statement</b> of a control sequence is called a guard.|$|R
40|$|A b s t r a c t Objective: A gap {{exists between}} the {{information}} contained in published clinical practice guidelines and the knowledge and information that are necessary to implement them. This work describes a process to systematize and make explicit the translation of document-based knowledge into workflow-integrated clinical decision support systems. Design: This approach uses the Guideline Elements Model (GEM) to represent the guideline knowledge. Imple-mentation requires a number of steps to translate the knowledge contained in guideline text into a computable format and to integrate the information into clinical workflow. The steps include: (1) selection of a guideline and specific recommendations for implementation, (2) markup of the guideline text, (3) atomization, (4) deabstraction and (5) disambiguation of recommendation concepts, (6) verification of rule set completeness, (7) addition of explanations, (8) building <b>executable</b> <b>statements,</b> (9) specification of origins of decision variables and insertions of recommended actions, (10) definition of action types and selection of associated beneficial services, (11) choice of interface components, and (12) creation of requirement specification. Results: The authors illustrate these component processes using examples drawn from recent experience translating recommendations from the National Heart, Lung, and Blood Instituteâs guideline on management of chronic asthma into a workflow-integrated decision support system that operates within the Logician electronic health record system...|$|R
40|$|In {{this paper}} we propose a novel {{approach}} to specification, development, and verification of object-oriented frameworks employing separate interface inheritance and implementation inheritance hierarchies. In particular, we illustrate how our method of framework specification and verification can be used to specify Java Collections Framework, which {{is a part of the}} standard Java Development Kit 2. 0, and ensure its correctness. We propose to associate with Java interfaces formal descriptions of the behavior that classes implementing these interfaces and their subinterfaces must deliver. Verifying behavioral conformance of classes implementing given interfaces to the specifications integrated with these interfaces allows us to ensure correctness of the system. The characteristic feature of our specification methodology is that the specification language used combines standard <b>executable</b> <b>statements</b> of the Java language with possibly nondeterministic specification statements. A specification of the intended behavior of a particular interface given in this language can serve asa precise documentation guiding implementation development. Since subtyping polymorphism in Java is based on interface inheritance, behavioral conformance of subinterfaces to their superinterfaces is essential for correctness of object substitutability in clients. As we view interfaces augmented with formal specifications as abstract classes, verifying behavioral conformance amounts to proving class refinement between specifications of superinterfaces and subinterfaces. Moreover, the logic frameworkthatwe use also allows verification of behavioral conformance between specifications of interfaces and classes implementing these interfaces. The uniform treatment of specifications and implementations and the relationships between them permits verifying correctness of the whole framework and its extensions...|$|R
40|$|Automatically {{generating}} a program from its specification eliminates a large source of errors {{that is often}} unavoidable in a manual approach. While a general purpose code generator is impossible to build, {{it is possible to}} build a practical code generator for a specific domain. This thesis investigates the theory behind Booster â a domain specific, object based specification language and automatic code generator. The domain of Booster is information systems â systems that consist of a rich object model in which the objects refer to each other to form a complicated network of associations. The operations of such systems are conceptually simple (changing the attributes of objects, adding or removing new objects and creating or destroying associations) but they are tricky to implement correctly. The thesis focuses on the theoretical foundation of the Booster approach, in particular on three contributions: semantics, model completion, and code generation. The semantics of a Booster model is a single abstract data type (ADT) where the invariants and the methods of all the classes in the model are promoted {{to the level of the}} ADT. This is different from the traditional view that considers each class as a separate ADT. The thesis argues that the Booster semantics is a better model of object oriented systems. The second important contribution is the idea of model completion â a process that augments the postconditions of methods with additional predicates that follow from the systemâs invariant and the methodâs original intention. The third contribution describes a simple but effective code generation technique that is based on interpreting postconditions as <b>executable</b> <b>statements</b> and uses weakest preconditions to ensure that the generated code refines its specification. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|As {{service design}} has gained {{interest}} in the last years, so has gained one of its primary tools: the Service Blueprint. In essence, a service blueprint is a graphical tool {{for the design of}} business models, specifically for the design of business service operations. Despite its level of adoption, tool support for service design tasks is still on its early days and available tools for service blueprint modeling are mainly focused on enhancing usability and enabling collaborative edition, disregarding the formal aspects of modeling. In this paper we present a way to support the validation of service blueprint models by simulation. This approach is based on annotating the models with formal semantics, so that each task can be translated into formal logics, and from them, to <b>executable</b> SQL <b>statements.</b> This works opens a new direction in the way to bridge formal techniques and creative service design processes. Peer ReviewedPostprint (author's final draft...|$|R
40|$|We {{look at how}} both logical {{restructuring}} and improvements available from successive versions of Fortran allow us to reduce the complexity (measured {{by a number of}} the com-monly used software metrics) of the Level 1 BLAS code used to compute the modified Givens transformation. With these reductions in complexity we claim that we have improved both the maintainability and clarity of the code; in addition, we report a fix to a minor problem with the original code. The performance of two commercial Fortran restructuring tools is also reported. 1 In t roduct ion The Level 1 BLAS [LHKK 79], originally published in Fortran 66 [ANS 66], implemented a number of common vector operations and were designed to be used as building blocks for linear algebra software. Hopkins [Hop 96] used knot counts [WHH 79] and path counts [Nej 88] to identify routines from the Level 1 BLAS which might benefit from code restructuring Two sets of routines, *NRM 2, used to compute the Euclidean orm of a vector and *ROTMG, for computing the modified Givens transformation, were identified as having Extremely high metric values given their relatively low number of <b>executable</b> <b>statements.</b> The restructuring ofthe *NRM 2 routines, along with a dramatic decrease in the metric values, was reported by Hopkins [Hop 96]; the *ROTMG routines are considered here. Following {{a brief description of the}} software metrics used to compare versions of the *ROTMG routines, we present a fiowgraph of the published code and look at how two Fortran code restruc-turing tools fared on this original source. We then compare the metric values obtained for the original and automatically restructured code with hand-coded Fortran 66 and Fortran 77 versions. Section 5 looks at how the metric values may be reduced further by using Fortran 90 and we show how the use of some of the new facilities available in Fortran 90 may be used to improve these routines further. Finally we look briefly at the testing of the new routine and report a fix to a minor problem in the original code. 2 Mod i f ied G ivens Rotat ion Mat r ix The input values to *ROTMG, dl, d 2, Xl and Yl, define a two-vector [al, a 2] T in the partitioned form as a 2 0 d ~ Y...|$|R
40|$|Verifying {{critical}} numerical software {{involves the}} generation of test data for floating-point intensive programs. As the symbolic execution of floating-point computations presents significant difficulties, existing approaches usually resort to random or search-based test data generation. However, without symbolic reasoning, {{it is almost impossible}} to generate test inputs that execute many paths with floating-point computations. Moreover, constraint solvers over the reals or the rationals do not handle the rounding errors. In this paper, we present a new version of FPSE, a symbolic evaluator for C program paths, that specifically addresses this problem. The tool solves path conditions containing floating-point computations by using correct and precise projection functions. This version of the tool exploits an essential filtering property based on the representation of floating-point numbers that makes it suitable to generate path-oriented test inputs for complex paths characterized by floating-point intensive computations. The paper reviews the key implementation choices in FPSE and the labeling search heuristics we selected to maximize the benefits of enhanced filtering. Our experimental results show that FPSE can generate correct test inputs for selected paths containing several hundreds of iterations and thousands of <b>executable</b> floating-point <b>statements</b> on a standard machine: this is currently outside the scope of any other symbolic-execution test data generator tool...|$|R
40|$|There are {{two main}} {{approaches}} in studying human values: the macro-global approach and the micro or individual approach. Within the first approach there are three major models. The first views values as absolute universals that should be: the second views values as the actual relative patterns of behavior: and the third scales values to natural self-actualizing, modern or urban humans. Within the micro-approach, there are four models. The first places more focus on the affective psycho-dynamic part of values: the second views values as cognitive representations. The decision making model views values as independent dimensions which guide decisions, and the action model reveals the built-in discrepancy between the espoused values and values in use. Most of these models lack the dynamic and the comprehensive perspective which combines the micro and macro analytic levels. The alternative model proposed in this thesis suggests three levels for rediscovering value dynamics and four approaches to measure them. The first level is the signification process that triggers global activation spreadings with intensity equal to {{the significance of the}} perceived object to the individual's survival and growth values or their derivatives. There are three kinds of signification: object-signification, values-signification, and self-signification. Self-signification is the core of valuation dynamics at this level and the source of valuepathy and psychological health. The second level in addressing value dynamics may be performed by analyzing the value system as self programming mechanisms. Values and their derivatives, which are the reference criteria in the signification process, are materialized in micro, or intrasystem, and macro, or inter-system, programming. On the micro-processing level {{there are two kinds of}} dynamic processes: the valuative states formation, like anticipation and familiarity and their specific derivatives, such as attitudes, preferences, and likings. The second is the value programming and packaging mechanisms. Within these packaging mechanisms there are different kinds of internal processing subroutines and algorithms. Within these programming mechanisms there are two kinds of statements: declarative, or non-executable, statements, which are these kinds of abstract universal values, and <b>executable,</b> or command, <b>statements,</b> which are either negative or positive, imperative or conditional self-orders. At this level, values are selfprescriptive programs and algorithms that function to secure a base line of readiness for effective coping. The third level of addressing value dynamics is the level of real time macro-processing. The individual develops through significating and coping to outside events his network of automated and non-automated personal survival and growth strategies which are dependent on both his valuative competencies and environmental availabilities. Flexibility, which is characterized by the selectivity syndrome inherent in it, is the source of different valuative competencies to plunge, to transcend, to balance, and to automate. Four measurement approaches have been proposed to check the validity and practicality of this model. The first centers on the assessment of valuative maturities and competencies. The second focuses on examining value processing dynamics. The third emphasizes evaluating valuepathy and its manifestation. The fourth combines all the available techniques in a value contextual analysis through task and case studies. Four extensions, or clarifications, of this model have been proposed to extend it to cover and explain macro-analysis, value contents, supernatural and natural belief systems, and to develop it to be more specific through sub-modeling of the specific areas of behavior...|$|R

