1572|1203|Public
25|$|The {{development}} of a criterion that can be evaluated to determine when the solution with the minimum error has been achieved. Laplace tried to specify a mathematical form of the probability density for the errors and define a method of estimation that minimizes the error of estimation. For this purpose, Laplace used a symmetric two-sided exponential distribution we now call Laplace distribution to model the <b>error</b> <b>distribution,</b> and used the sum of absolute deviation as error of estimation. He felt these to be the simplest assumptions he could make, and {{he had hoped to}} obtain the arithmetic mean as the best estimate. Instead, his estimator was the posterior median.|$|E
2500|$|In two {{important}} papers in 1810 and 1811, Laplace first developed the characteristic {{function as a}} tool for large-sample theory and proved the first general central limit theorem. Then in a supplement to his 1810 paper written after he had seen Gauss's work, he showed that the central limit theorem provided a Bayesian justification for least squares: if one were combining observations, each one of which was itself the mean {{of a large number of}} independent observations, then the least squares estimates would not only maximise the likelihood function, considered as a posterior distribution, but also minimise the expected posterior error, all this without any assumption as to the <b>error</b> <b>distribution</b> or a circular appeal to the principle of the arithmetic mean. In 1811 Laplace took a different non-Bayesian tack. Considering a linear regression problem, he restricted his attention to linear unbiased estimators of the linear coefficients. After showing that members of this class were approximately normally distributed if the number of observations was large, he argued that least squares provided the [...] "best" [...] linear estimators. Here it is [...] "best" [...] in the sense that it minimised the asymptotic variance and thus both minimised the expected absolute value of the error, and maximised the probability that the estimate would lie in any symmetric interval about the unknown coefficient, no matter what the <b>error</b> <b>distribution.</b> His derivation included the joint limiting distribution of the least squares estimators of two parameters.|$|E
2500|$|Here and below, [...] means proportional; a pdf {{is always}} scaled {{so that its}} {{integral}} over the whole space is one. This , called the prior, was evolved in time by running the model and now is to be updated to account for new data. It is natural {{to assume that the}} <b>error</b> <b>distribution</b> of the data is known; data have to come with an error estimate, otherwise they are meaningless. Here, the data [...] is assumed to have Gaussian pdf with covariance [...] and mean , where [...] is the so-called observation matrix. The covariance matrix [...] describes the estimate of the error of the data; if the random errors in the entries of the data vector [...] are independent, [...] is diagonal and its diagonal entries are the squares of the standard deviation (“error size”) of the error of the corresponding entries of the data vector [...] The value [...] is what the value of the data would be for the state [...] in the absence of data errors. Then the probability density [...] of the data [...] conditional of the system state , called the data likelihood, is ...|$|E
30|$|We {{formulated}} {{a method}} for minimising the expected procurement cost of electricity and we reported the results of simulations of the proposed procurement method using actual data. From these simulations, we found that this method minimised the procurement cost. Based on the above discussion, to reduce the procurement cost {{it was necessary to}} estimate not only the accuracy of predictions but also the prediction <b>error</b> <b>distributions.</b> In this paper, the prediction <b>error</b> <b>distributions</b> were assumed to be normal distributions, but the actual prediction errors did not always become normal distributions. To achieve further cost reduction, an estimation of the prediction <b>error</b> <b>distributions</b> is indispensable.|$|R
40|$|In this paper, we {{proposed}} the error growth curve {{model for the}} integration of intertemporal measure errors correlation which usually exist in quality control process. This model can work well in the usual <b>error</b> <b>distributions,</b> such as normal, uniform, Rayleigh and some other <b>error</b> <b>distributions.</b> Simulation {{results show that the}} proposed estimators of the model parameters perform well especially in small sample situations...|$|R
30|$|The multi-dimensional {{scenario}} is generated {{with respect to}} the <b>error</b> <b>distributions</b> and the copula function.|$|R
5000|$|The Skewed Generalized <b>Error</b> <b>Distribution</b> has the pdf:where gives {{a mean of}} [...] Alsogives a {{variance}} of [...]|$|E
5000|$|The Generalized <b>Error</b> <b>Distribution</b> (also {{known as}} the {{generalized}} normal distribution) has the pdf:where gives a variance of [...]|$|E
50|$|The affine {{transform}} ax + b {{yields a}} relocation and scaling {{of the original}} distribution. The following are self-replicating:Normal distribution, Cauchy distribution, Logistic distribution, <b>Error</b> <b>distribution,</b> Power distribution, Rayleigh distribution.|$|E
40|$|Nonnested {{models are}} {{sometimes}} tested using a simulated reference {{distribution for the}} uncentred log likelihood ratio statistic. This approach has been recommended for the specific problem of testing linear and logarithmic regression models. The general asymptotic validity of the reference distribution test under correct choice of <b>error</b> <b>distributions</b> is questioned. The asymptotic behaviour of the test under incorrect assumptions about <b>error</b> <b>distributions</b> is also examined. In order to complement these analyses, Monte Carlo results for the case of linear and logarithmic regression models are provided. The finite sample properties of several standard tests for testing these alternative functional forms are also studied, under normal and nonnormal <b>error</b> <b>distributions.</b> These regression-based variable-addition tests are implemented using asymptotic and bootstrap critical values. Bootstrap, Nonnested hypotheses, Nonnormality,...|$|R
30|$|Inverted 1 -yr afterslip {{distributions}} {{following the}} 2003 Tokachi-oki earthquake {{together with their}} <b>error</b> <b>distributions</b> (1 -σ standard deviations) are described below.|$|R
30|$|To {{explain the}} dual LM {{of the mean}} and {{volatility}} of platinum and palladium return series, we fitted ARFIMA–FIGARCH type models under heavy tailed <b>error</b> <b>distributions</b> including the Normal distribution. We used an ARFIMA model for modelling squared log returns and for volatility we used FIGARCH, FIEGARCH, FIAPARCH and HYGARCH models under heavy tailed <b>error</b> <b>distributions</b> bench marking them with the Normal distribution. Distributions considered are the Normal, Student, Generalized extreme distribution (GED), and the skewed Student distribution.|$|R
50|$|Special and {{limiting}} {{cases of the}} skewed generalized t distribution include the skewed generalized <b>error</b> <b>distribution,</b> the generalized t distribution introduced by McDonald and Newey, the skewed t proposed by Hansen, the skewed Laplace distribution, the generalized <b>error</b> <b>distribution</b> (also known as the generalized normal distribution), a skewed normal distribution, the student t distribution, the skewed Cauchy distribution, the Laplace distribution, the uniform distribution, the normal distribution, and the Cauchy distribution. The graphic below, adapted from Hansen, McDonald, and Newey, shows which parameters should be set to obtain some of the different special values of the skewed generalized t distribution.|$|E
50|$|The {{basic curve}} for {{calculation}} is the Gauss <b>error</b> <b>distribution</b> curve. For calculation, an integral {{is necessary to}} determine the expectation of winnings. Only chess results against competitors with a DWZ are taken into account.|$|E
50|$|His {{research}} includes (1) {{the study}} of models for the distribution of income and of stock returns and (2) partially adaptive estimators of various econometric models which are robust to many types of misspecification of the <b>error</b> <b>distribution.</b>|$|E
40|$|The mean squared {{errors of}} various estimators of slope, intercept, and mean {{response}} in the simple linear regression problem are compared in a simulation study. A weighted median estimator of slope proposed by Sievers (1978) and Scholz (1978) and two intercept estimators based upon it are found to perform well for most <b>error</b> <b>distributions</b> studied. Theil's (1950) estimator of slope and two intercept estimators based on it are preferable for certain heavily contaminated <b>error</b> <b>distributions.</b> 1...|$|R
50|$|Tweedie {{distributions}} are {{a special}} case of exponential dispersion models, a class of models used to describe <b>error</b> <b>distributions</b> for the generalized linear model.|$|R
50|$|For <b>error</b> <b>distributions</b> {{that belong}} to the {{exponential}} family, a link function {{may be used to}} transform the parameters under the Generalized linear model framework.|$|R
50|$|Known also as the {{exponential}} power distribution, or {{the generalized}} <b>error</b> <b>distribution,</b> {{this is a}} parametric family of symmetric distributions. It includes all normal and Laplace distributions, and as limiting cases it includes all continuous uniform distributions on bounded intervals of the real line.|$|E
50|$|Multiplying the {{variable}} by any positive real constant yields a scaling {{of the original}} distribution.Some are self-replicating, meaning that the scaling yields the same family of distributions, albeit with a different parameter:Normal distribution, Gamma distribution, Cauchy distribution, Exponential distribution, Erlang distribution, Weibull distribution, Logistic distribution, <b>Error</b> <b>distribution,</b> Power distribution, Rayleigh distribution.|$|E
5000|$|... where , [...] is the {{conditional}} variance, , , , [...] and [...] are coefficients. [...] {{may be a}} standard normal variable or come from a generalized <b>error</b> <b>distribution.</b> The formulation for [...] allows the sign and the magnitude of [...] to have separate effects on the volatility. This is particularly useful in an asset pricing context.|$|E
40|$|A {{comprehensive}} {{study on the}} role of the phase <b>errors</b> <b>distribution</b> on the performances of the phased array systems has been produced. The analysis is brought out using a complete and behavioural model for radiation-pattern characteristics. The study has shown how the phase <b>errors</b> <b>distribution</b> actually affects the performances demonstrating that the rms phase error is a valuable figure of merit of phased array systems but it is not sufficient to completely describe the behaviour of a real system. The paper demonstrates that the antennas array beam shape is depending by the actual <b>error</b> phase <b>distribution</b> and that a good phase shifter has to have the phase errors as constant as possible and with a low rms value...|$|R
40|$|Quantile {{estimation}} in deconvolution {{problems is}} studied comprehensively. In particular, the more realistic setup of unknown <b>error</b> <b>distributions</b> is covered. Our plug-in method {{is based on}} a deconvolution density estimator and is minimax optimal under minimal and natural conditions. This closes an important gap in the literature. Optimal adaptive estimation is obtained by a data-driven bandwidth choice. As a side result we obtain optimal rates for the plug-in estimation of distribution functions with unknown <b>error</b> <b>distributions.</b> The method is applied to a real data example...|$|R
40|$|The {{bootstrap}} {{statistical method}} {{is applied to}} the discrepancy in the l-charged particle decay modes of the tau lepton. This eliminates questions about the correctness of the errors ascribed to the branching fraction measurements and the use of gaussian <b>error</b> <b>distributions</b> for systematic <b>errors.</b> The discrepancy is still seen when the results of the bootstrap analysis are combined with other measurements and with deductions from theory. But the bootstrap method assigns less statistical significance to the discrepancy compared to a method using gaussian <b>error</b> <b>distributions...</b>|$|R
50|$|The most {{commonly}} used formula for a binomial confidence interval relies on approximating the distribution of error about a binomially-distributed observation, , with a normal distribution. However, although this distribution is frequently confused with a binomial distribution, {{it should be noted}} that the <b>error</b> <b>distribution</b> itself is not binomial, and hence other methods (below) are preferred.|$|E
50|$|Generalized {{linear models}} (GLMs) provide a {{flexible}} generalization of ordinary linear regression {{that allows for}} response variables that have <b>error</b> <b>distribution</b> models other than a normal distribution. GLMs generalize allow the linear model {{to be related to}} the response variable via a link function and allow the magnitude of the variance of each measurement to be a function of its predicted value.|$|E
5000|$|The least-squares {{approach}} implicitly {{assumes that}} the errors in the image data have a Gaussian distribution with zero mean. If one expects the window to contain {{a certain percentage of}} [...] "outliers" [...] (grossly wrong data values, that do not follow the [...] "ordinary" [...] Gaussian <b>error</b> <b>distribution),</b> one may use statistical analysis to detect them, and reduce their weight accordingly.|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedTropical cyclone (TC) track forecasts will always contain uncertainty. This thesis relates ranges (bins) of uncertainty measurements with historical TC track forecast errors, to provide statistically distinct <b>error</b> <b>distributions</b> {{for use with}} the Monte Carlo (MC) method. T-test and Kolmogorov-Smirnov tests are used to confirm distinctness among <b>error</b> <b>distributions</b> associated with the bins of either European Center for Medium-Range Weather Forecasts (ECMWF) ensemble spread or TVCN Goerss Predicted Consensus Error (GPCE). The statistical tests indicate that distinct <b>error</b> <b>distributions</b> (consisting of official TC forecast error, ECMWF ensemble mean [EMN] error, or TVCN error) exist when using four bins of uncertainty (of either uncertainty measurement). Furthermore, <b>error</b> <b>distributions</b> of ECMWF EMN error are distinct with five bins of ECMWF ensemble spread. Along- and cross-track official errors could not be directly related to either measurement of uncertainty at even three bins. These {{results suggest that the}} National Hurricane Center test and evaluate the use of four bins of uncertainty for operational use with the MC method to further improve its Wind Speed Probability products and overall TC track forecasts. TC forecasters should also exploit the more impressive relationship established using five bins ECMWF ensemble spread with ECMWF EMN error. Captain, United States Air Forc...|$|R
5000|$|Inferences from ordinal data, {{to produce}} {{estimates}} on an interval/ratio scale, require assumptions about <b>error</b> <b>distributions</b> and the respondent's decision rule (functional {{form of the}} utility function); ...|$|R
40|$|We characterise the {{convergence}} of the Gibbs sampler which samples from the joint posterior distribution of parameters and missing data in hierarchical linear models with arbitrary symmetric <b>error</b> <b>distributions.</b> We show that {{the convergence}} can be uniform, geometric or sub-geometric depending on the relative tail behaviour of the <b>error</b> <b>distributions,</b> and on the parametrisation chosen. Our theory is applied to characterise {{the convergence of}} the Gibbs sampler on latent Gaussian process models. We indicate how the theoretical framework we introduce will be useful in analyzing more complex models...|$|R
50|$|In statistics, the {{generalized}} linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have <b>error</b> <b>distribution</b> models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model {{to be related to}} the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.|$|E
5000|$|The {{security}} of the protocol is proven based on the hardness of solving LWE problem. In 2014, Peikert presented a key transport scheme following the same basic idea of Ding's, where the new idea of sending additional 1 bit signal for rounding in Ding's construction is also utilized. The [...] "new hope" [...] implementation selected for Google's post quantum experiment, uses Peikert's scheme with variation in the <b>error</b> <b>distribution.</b>|$|E
50|$|Empirical {{likelihood}} (EL) is an {{estimation method}} in statistics. Empirical likelihood estimates require few {{assumptions about the}} <b>error</b> <b>distribution</b> compared to similar methods like maximum likelihood. EL can handle data well {{as long as it}} is independent and identically distributed (iid). EL performs well even when the distribution is asymmetric or censored. EL methods are also useful since they can easily incorporate constraints and prior information. Art Owen pioneered work in this area with his 1988 paper.|$|E
40|$|The present article {{considers}} the linear ultrastructural model which encompasses the two popular forms of measurement error models, viz., the functional and structural models. The measurement error variance {{associated with the}} explanatory variable {{is assumed to be}} known which leads to consistent estimators of parameters. The efficiency properties of the thus obtained estimators of slope parameter, intercept term and the measurement error variance of study variable are derived under non-normal <b>error</b> <b>distributions</b> and the effects of departures from symmetry and peakedness of the <b>error</b> <b>distributions</b> are studied. Immaculate estimator Measurement errors Ultrastructural model...|$|R
5000|$|... some systems {{monitor the}} actual cross-track and {{along-track}} errors individually, whereas others monitor the radial NSE {{to simplify the}} monitoring and eliminate dependency on the aircraft track, e.g. based on typical elliptical 2-D <b>error</b> <b>distributions.</b>|$|R
30|$|For the {{selected}} models the platinum model has intercept estimate of 0.0005 and the palladium model has intercept estimate of − 0.00032, hence the platinum model underestimates volatility while the palladium model overestimates volatility. The null hypothesis {{of a unit}} slope is not rejected at 5 % level of significance for all models. This tells us that our forecasts from the models explains the observed values. In summary, the ARFIMA–FIGARCH type models under heavy tailed <b>error</b> <b>distributions</b> show an improvement of forecasts {{as compared to the}} assumption of Normally distributed errors, and further ARFIMA–FIAPARCH models proved to explain platinum and palladium return series better under non Normal <b>error</b> <b>distributions.</b>|$|R
