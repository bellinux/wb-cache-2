319|1176|Public
25|$|Edelsbrunner and Welzl (1986) first {{studied the}} problem of {{constructing}} all k-sets of an <b>input</b> <b>point</b> set, or dually of constructing the k-level of an arrangement. The k-level version of their algorithm {{can be viewed as}} a plane sweep algorithm that constructs the level in left-to-right order. Viewed in terms of k-sets of point sets, their algorithm maintains a dynamic convex hull for the points on each side of a separating line, repeatedly finds a bitangent of these two hulls, and moves each of the two points of tangency to the opposite hull. Chan (1999) surveys subsequent results on this problem, and shows that it can be solved in time proportional to Dey's O(nk1/3) bound on the complexity of the k-level.|$|E
500|$|Initialize the set {{of active}} {{clusters}} to consist of [...] one-point clusters, one for each <b>input</b> <b>point.</b>|$|E
2500|$|... {{which are}} the {{distances}} from the fulcrum to the <b>input</b> <b>point</b> A and to the output point B, respectively.|$|E
40|$|The goal of {{supervised}} learning is to estimate an underlying input-output function from its input-output training samples so that output values for unseen test <b>input</b> <b>points</b> can be predicted. A common assumption in {{supervised learning}} {{is that the}} training <b>input</b> <b>points</b> follow the same probability distribution as the test <b>input</b> <b>points.</b> However, this assumption is not satisfied, for example, when outside of the training region is extrapolated. The situation where the training and test <b>input</b> <b>points</b> follow different distributions while the conditional distribution of output values given <b>input</b> <b>points</b> is unchanged is called covariate shift. Since almost all existing learning methods assume that the training and test samples are drawn from the same distribution, their fundamental theoretical properties such as consistency or efficiency no longer hold under covariate shift. In this chapter, we review recently proposed techniques for covariate shift adaptation. ...|$|R
5000|$|... (a {{property}} of Delaunay triangulations): If {{there is a}} circle {{with two of the}} <b>input</b> <b>points</b> on its boundary which contains no other <b>input</b> <b>points,</b> the line between those two points is an edge of every Delaunay triangulation.|$|R
40|$|In model {{selection}} {{procedures in}} supervised learning, a model is usually chosen {{so that the}} expected test error over all possible test <b>input</b> <b>points</b> is minimized. On the other hand, when the test <b>input</b> <b>points</b> (without output values) are available in advance, it is more effetive to choose a model so that the test error only at the test <b>input</b> <b>points</b> at hand is minimized. In this paper, we follow this idea and derive an estimator of the test error at the given test <b>input</b> <b>points</b> for linear regression. Our estimator is {{proved to be an}} unbiased estimator of the test error at the given test <b>input</b> <b>points</b> under certain conditions. Through the simulations with artificial and standard benchmark data sets, we show that the proposed method is successfully applied in test error estimation and is compared favorably to the standard cross-validation and an empirical Bayesian method in ridge parameter selection...|$|R
2500|$|... that is, {{one that}} {{contains}} no other <b>input</b> <b>point.</b> The original {{solution to the}} happy ending problem {{can be adapted to}} show that any five points in general position have an empty convex quadrilateral, as shown in the illustration, and any ten points in general position have an empty convex pentagon. However, there exist arbitrarily large sets of points in general position that contain no empty convex heptagon.|$|E
2500|$|This {{equation}} {{shows that}} if the distance a from the fulcrum to the point A where the input force is applied {{is greater than the}} distance b from fulcrum to the point B where the output force is applied, then the lever amplifies the input force. [...] If the opposite is true that the distance from the fulcrum to the <b>input</b> <b>point</b> A is less than from the fulcrum to the output point B, then the lever reduces the magnitude of the input force.|$|E
5000|$|... {{preprocessing}} time: , where [...] is {{the time}} to evaluate a function [...] on an <b>input</b> <b>point</b> ...|$|E
50|$|If d is not {{a square}} in K and , then there are no {{exceptional}} points: the denominators 1 + dx1x2y1y2 and 1 − dx1x2y1y2 are always nonzero. Therefore, the Edwards addition law is complete when d {{is not a}} square in K. This means that the formulas work for all pairs of <b>input</b> <b>points</b> on the Edwards curve with no exceptions for doubling, no exception for the neutral element, no exception for negatives, etc. In other words, it is defined for all pairs of <b>input</b> <b>points</b> on the Edwards curve over K and the result gives {{the sum of the}} <b>input</b> <b>points.</b>|$|R
40|$|A common {{assumption}} in {{supervised learning}} {{is that the}} <b>input</b> <b>points</b> in the training set follow the same probability distribution as the <b>input</b> <b>points</b> that will be given in the future test phase. However, this assumption is not satisfied, for example, when {{the outside of the}} training region is extrapolated. The situation where the training <b>input</b> <b>points</b> and test <b>input</b> <b>points</b> follow different distributions while the conditional distribution of output values given <b>input</b> <b>points</b> is unchanged is called the covariate shift. Under the covariate shift, standard model selection techniques such as cross validation do not work as desired since its unbiasedness is no longer maintained. In this paper, we propose a new method called importance weighted cross validation (IWCV), for which we prove its unbiasedness even under the covariate shift. The IWCV procedure {{is the only one that}} can be applied for unbiased classification under covariate shift, whereas alternatives to IWCV exist for regression. The usefulness of our proposed method is illustrated by simulations, and furthermore demonstrated in the brain-computer interface, where strong non-stationarity effects can be seen between training and test sessions. c 2000 Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert Müller...|$|R
40|$|Supervised {{learning}} is obtaining an underlying rule from training examples {{made up of}} <b>input</b> <b>points</b> and corresponding output values. If the input-Output rule is successfully acquired, then we can estimate appropriate output values corresponding to unknown <b>input</b> <b>points.</b> This ability is called the generalization capability. It is known that higher levels of the generalization capability can b...|$|R
5000|$|Initialize the set {{of active}} {{clusters}} to consist of [...] one-point clusters, one for each <b>input</b> <b>point.</b>|$|E
5000|$|... {{designed}} a fast algorithm {{to determine the}} face of an arrangement of hyperplanes containing an <b>input</b> <b>point.</b>|$|E
5000|$|The {{resulting}} coordinate (x', y', ...) is {{then used}} {{in order to}} determine which skewed unit hypercube cell the <b>input</b> <b>point</b> lies in, (xb'=floor(x'), yb'=floor(y'), ...), and its internal coordinates (xi'=x'-xb', yi'=y'-yb', ...).|$|E
5000|$|Volume {{regulation}} (to {{ensure that}} <b>input</b> <b>points</b> are controlled to avoid overloading system) ...|$|R
40|$|We {{present the}} first truly polynomial-time {{approximation}} scheme (PTAS) for the minimum-cost k-vertex- (or, k- edge-) connected spanning subgraph problem for complete Euclidean graphs in R d : Previously {{it was known}} for every positive constant " how to construct in a polynomial time a graph on a superset of the <b>input</b> <b>points</b> which is k-vertex connected {{with respect to the}} <b>input</b> <b>points,</b> and whose cost is within (1 + ") of the minimum-cost of a k-vertex connected graph spanning the <b>input</b> <b>points.</b> We subsume that result by showing for every positive constant " how to construct in a polynomial-time a k-connected subgraph spanning the <b>input</b> <b>points</b> without any Steiner points and having the cost within (1 + ") of the minimum. We also study hardness of approximations for the minimum-cost k-vertex- and k-edge-connected spanning subgraph problems. The only inapproximability result known so far for the minimum-cost k-vertex- and k-edge- connected spanning subgraph problems states that the k- e [...] ...|$|R
50|$|A circle {{circumscribing}} any Delaunay triangle {{does not}} contain any other <b>input</b> <b>points</b> in its interior.|$|R
5000|$|Treat each <b>input</b> <b>point</b> as {{separate}} cluster, compute u.closest for each u and then insert each cluster into the heap Q. (clusters {{are arranged in}} increasing order of distances between u and u.closest).|$|E
50|$|Let the {{coordinate}} vector of {{the point}} P that defines the fulcrum be rP, and introduce the lengths which are the distances from the fulcrum to the <b>input</b> <b>point</b> A and to the output point B, respectively.|$|E
5000|$|Shamos and Hoey {{proposed}} an O(n log n) time algorithm {{for the problem}} based on the observation that {{the center of the}} smallest enclosing circle must be a vertex of the farthest-point Voronoi diagram of the <b>input</b> <b>point</b> set.|$|E
50|$|Newton's {{method has}} been {{successfully}} used to give rapid convergence for all pairs of <b>input</b> <b>points</b> (Karney, 2013).|$|R
40|$|We {{describe}} {{the use of}} splines for solving nonlinear model estimation problems, in which nonlinear functions with unknown shapes and values are involved, by converting the nonlinear estimation problems into linear ones at a higherdimensional space. This contrasts with the typical use of the splines [1]–[3] for function interpolation where the functional values at some <b>input</b> <b>points</b> are given and the values corresponding to other <b>input</b> <b>points</b> are sought for vi...|$|R
50|$|The {{discussion}} above {{considers the}} case when all <b>input</b> <b>points</b> are known in advance. One may consider two other settings.|$|R
50|$|The nearest {{neighbour}} search (NN) algorithm aims to {{find the}} point in the tree that is nearest to a given <b>input</b> <b>point.</b> This search can be done efficiently by using the tree properties to quickly eliminate large portions of the search space.|$|E
5000|$|For every cluster u (each <b>input</b> <b>point),</b> in u.mean and u.rep {{store the}} mean of the points in the cluster and a set of c {{representative}} points of the cluster (initially c = 1 since each cluster has one data point). Also u.closest stores the cluster closest to u.|$|E
5000|$|The {{classical}} {{supervised learning}} problem requires estimating the output {{for some new}} <b>input</b> <b>point</b> [...] by learning a scalar-valued estimator [...] {{on the basis of}} a training set [...] consisting of [...] input-output pairs, [...] [...] Given a symmetric and positive bivariate function [...] called a kernel, one of the most popular estimators in machine learning is given by ...|$|E
30|$|We {{proposed}} a new method for extracting elliptic arcs from a spatially connected point sequence. Assuming that <b>input</b> <b>points</b> are a spatially connected sequence of edge points, we fit an ellipse {{to it and}} automatically segment it into partial arcs at the intersection points of the fitted ellipse. Then, we compute residuals of the fitted ellipse for all <b>input</b> <b>points</b> and select elliptic arcs among the segmented arcs by checking curvatures of the residual graph.|$|R
40|$|In {{this section}} we discuss {{strengths}} {{and limitations of}} possible approaches of solving the ALMS problem (3). Direct batch method A naive and direct {{solution to this problem}} would be to simultaneously optimize Å and �. However, this direct approach may not be possible due to the ALMS dilemma: when selecting training <b>input</b> <b>points</b> with existing AL methods, model Å must have been fixed; and when choosing a model with existing MS methods training <b>input</b> <b>points</b> � must have been fixed and corresponding output values must have been gathered. Therefore, this direct batch approach may not be possible. Sequential method A typical approach of coping with the above ALMS dilemma is with the sequential method: iteratively choosing model and training <b>input</b> <b>points</b> (e. g., [7]). At each step, the most promising model is chosen b...|$|R
30|$|Another {{limitation}} is that: {{when all the}} <b>input</b> <b>points</b> are initially extreme points, e.g., when all points exactly locate on a circle, two rounds of discarding interior points will be wasteful {{since there are no}} interior points that can be found and removed. All the <b>input</b> <b>points</b> will be kept and used to calculate the convex hull on the CPU using the Melkman’s algorithm (Melkman 1987). Hence, the overall execution in this case might be very slow.|$|R
50|$|Once {{the points}} are in sorted order, two {{properties}} {{make it easy}} to build a quadtree: The first is that the points contained in a square of the quadtree form a contiguous interval in the sorted order. The second is that if more than one child of a square contains an <b>input</b> <b>point,</b> the square is the derived square for two adjacent points in the sorted order.|$|E
5000|$|Finding {{the best}} BST {{execution}} for the input sequence [...] {{is equivalent to}} finding the minimum cardinality superset of points (that contains the input in geometric representation) that is arborally satisfied. The more general problem of finding the minimum cardinality arborally satisfied superset of a general set of input points (not limited to one <b>input</b> <b>point</b> per [...] coordinate), {{is known to be}} NP-complete.|$|E
5000|$|The {{problem to}} be solved must first be {{formulated}} as a set intersection problem in Euclidean space: find an [...] in the intersection of sets [...] and [...] Another prerequisite is an implementation of the projections [...] and [...] that, given an arbitrary <b>input</b> <b>point</b> , return {{a point in the}} constraint set [...] or [...] that is nearest to [...] One iteration of the algorithm is given by the mapping: ...|$|E
5000|$|Dynamic {{convex hull}} maintenance: The <b>input</b> <b>points</b> may be {{sequentially}} inserted or deleted, and the convex hull must be updated after each insert/delete operation.|$|R
40|$|Tom Vierjahn 1, 2, 3 Niklas Henrich 1 Klaus Hinrichs 2 Sina Mostafawy 2, 3 In {{this report}} we present an online {{algorithm}} to reconstruct a triangulated surface from an unorganized point cloud. Our algorithm called sgng, {{which is based}} on an artificial neural network, does not make any assumptions about the technique used to acquire the <b>input</b> <b>points.</b> Furthermore, sgng can update a previously reconstructed surface incrementally when previous <b>input</b> <b>points</b> are moved or deleted or new <b>input</b> <b>points</b> are added. Even arbitrary topology can be learnt without relying on predefined heuristics. In contrast to existing similar algorithms, sgng is able to create all triangles while learning without post-processing. All learning decisions are solely based on a local neighbourhood to keep overall complexity low. We demonstrate that the new algorithm efficiently and robustly reconstructs 3 d surfaces from even huge, arbitrary unorganized point clouds. ...|$|R
40|$|Strategyproof {{classification}} {{deals with}} a setting where a decision-maker must classify a set of <b>input</b> <b>points</b> with binary labels, while minimizing the expected error. The labels of the <b>input</b> <b>points</b> are reported by self-interested agents, who might lie {{in order to obtain}} a classifier that more closely matches their own labels, thus creating a bias in the data; this motivates the design of truthful mechanisms that discourage false reports. Previous work [Meir et al., 2008] investigated both decisiontheoretic and learning-theoretic variations of the setting, but only considered classifiers that belong to a degenerate class. In this paper we assume that the agents are interested in a shared set of <b>input</b> <b>points.</b> We show that this plausible assumption leads to powerful results. In particular, we demonstrate that variations of a truthful random dictator mechanism can guarantee approximately optimal outcomes with respect to any class of classifiers. ...|$|R
