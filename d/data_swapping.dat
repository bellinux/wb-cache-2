50|114|Public
2500|$|... of multiplications {{associated}} with the 3-D DCT VR algorithm is less than that {{associated with}} the RCF approach by more than 40%. In addition, the RCF approach involves matrix transpose and more indexing and <b>data</b> <b>swapping</b> than the new VR algorithm. This makes the 3-D DCT VR algorithm more efficient and better suited for 3-D applications that involve the 3-D DCT-II such as video compression and other 3-D image processing applications. The main consideration in choosing a fast algorithm is to avoid computational and structural complexities. As the technology of computers and DSPs advances, the execution time of arithmetic operations (multiplications and additions) is becoming very fast, and regular computational structure becomes the most important factor. Therefore, although the above proposed 3-D VR algorithm does not achieve the theoretical lower bound {{on the number of}} multiplications, it has a simpler computational structure as compared to other 3-D DCT algorithms. It can be implemented in place using a single butterfly and possesses the properties of the Cooley–Tukey FFT algorithm in 3-D. Hence, the 3-D VR presents a good choice for reducing arithmetic operations in the calculation of the 3-D DCT-II while keeping the simple structure that characterize butterfly style Cooley–Tukey FFT algorithms.The image to the right shows a combination of horizontal and vertical frequencies for an 8 x 8 (...) two-dimensional DCT. Each step from left to right and top to bottom is an increase in frequency by 1/2 cycle.|$|E
50|$|Speech and <b>data</b> <b>swapping</b> {{during a}} call, i.e. {{alternate}} speech and data.|$|E
50|$|Supports {{unlimited}} terrain size using custom <b>data</b> <b>swapping</b> {{mechanism to}} hard disk space. Terrain made of multiple terrain zones supporting variable resolution and automatic border stitching.|$|E
25|$|The CFTC oversees {{designated}} contract markets (DCMs) or exchanges, swap execution facilities (SEFs), derivatives clearing organizations, <b>swap</b> <b>data</b> repository, <b>swap</b> dealers, futures commission merchants, {{commodity pool}} operators and other intermediaries. The CFTC {{falls under the}} oversight of the Senate Agriculture Committee.|$|R
40|$|AbstractA {{lower bound}} is derived {{for the time}} {{required}} to perform an update in an implicit data structure that is stored in a recursively rotated order. The update is shown to require Ω(22 logn (log n) 12) <b>data</b> <b>swaps,</b> where n is the number of elements in the structure. This matches an upper bound derived in an earlier paper (Frederickson, 1983) ...|$|R
40|$|We {{show that}} {{primitive}} <b>data</b> <b>swaps</b> or moves {{are the only}} moves {{that have to be}} included in a Markov basis that links all the contingency tables having a set of fixed marginals when this set of marginals induces a decomposable independence graph. We give formulae that fully identify such Markov bases and show how to use these formulae to dynamically generate random moves...|$|R
5000|$|In 2002, the GAO {{confirmed}} that the Census Bureau has authority to conduct the survey and [...] "require responses from the public." [...] All individual American Community Survey responses are kept private and are used (along with other ACS responses) to create estimates of demographic characteristics for various geographies. Because of <b>data</b> <b>swapping</b> techniques to ensure confidentiality, {{it is impossible to}} figure out how individual people responded based on data from published ACS estimates.|$|E
50|$|In {{virtual memory}} systems, {{thrashing}} {{may be caused}} by programs or workloads that present insufficient locality of reference: if the working set of a program or a workload cannot be effectively held within physical memory, then constant <b>data</b> <b>swapping,</b> i.e., thrashing, may occur. The term was first used during the tape operating system days to describe the sound the tapes made when data was being rapidly written to and read.An example of this sort of situation occurred on the IBM System/370 series mainframe computer, in which a particular instruction could consist of an execute instruction (which crosses a page boundary) that points to a move instruction (which itself also crosses a page boundary), targeting a move of data from a source that crosses a page boundary, to a target of data that also crosses a page boundary. The total number of pages thus being used by this particular instruction is eight, and all eight pages must be present in memory at the same time. If the operating system allocates fewer than eight pages of actual memory, when it attempts to swap out some part of the instruction or data to bring in the remainder, the instruction will again page fault, and it will thrash on every attempt to restart the failing instruction.|$|E
5000|$|The {{conventional}} {{method to}} calculate MD-DCT-II {{is using a}} Row-Column-Frame (RCF) approach which is computationally complex and less productive on most advanced recent hardware platforms. The number of multiplications required to compute VR DIF Algorithm when compared to RCF algorithm {{are quite a few}} in number. The number of Multiplications and additions involved in RCF approach are given by [...] and [...] respectively. From Table 1, {{it can be seen that}} the total numberof multiplications associated with the 3-D DCT VR algorithm is less than that associated with the RCF approach by more than 40%. In addition, the RCF approach involves matrix transpose and more indexing and <b>data</b> <b>swapping</b> than the new VR algorithm. This makes the 3-D DCT VR algorithm more efficient and better suited for 3-D applications that involve the 3-D DCT-II such as video compression and other 3-D image processing applications. The main consideration in choosing a fast algorithm is to avoid computational and structural complexities. As the technology of computers and DSPs advances, the execution time of arithmetic operations (multiplications and additions) is becoming very fast, and regular computational structure becomes the most important factor. Therefore, although the above proposed 3-D VR algorithm does not achieve the theoretical lower bound on the number of multiplications, it has a simpler computational structure as compared to other 3-D DCT algorithms. It can be implemented in place using a single butterfly and possesses the properties of the Cooley-Tukey FFT algorithm in 3-D. Hence, the 3-D VR presents a good choice for reducing arithmetic operations in the calculation of the 3-D DCT-II while keeping the simple structure that characterize butterfly style Cooley-Tukey FFT algorithms.The image to the right shows a combination of horizontal and vertical frequencies for an 8 x 8 (...) two-dimensional DCT. Each step from left to right and top to bottom is an increase in frequency by 1/2 cycle.For example, moving right one from the top-left square yields a half-cycle increase in the horizontal frequency. Another move to the right yields two half-cycles. A move down yields two half-cycles horizontally and a half-cycle vertically. The source data (8x8) is transformed to a linear combination of these 64 frequency squares.|$|E
50|$|As of 2014 the CFTC oversees 'designated {{contract}} markets' (DCMs) or exchanges, swap execution facilities (SEFs), derivatives clearing organizations, <b>swap</b> <b>data</b> repository, <b>swap</b> dealers, futures commission merchants, {{commodity pool}} operators and other intermediaries. The CFTC coordinates its work with foreign regulators, {{such as its}} UK counterpart, the Financial Conduct Authority, which supervises the London Metal Exchange.|$|R
40|$|<b>Data</b> about <b>swap</b> {{rates and}} {{impinging}} variables {{were taken from}} multiple sources and examined using regression analysis. Results show that the identified variables, including corporate default spreads, Treasury rates, Treasury yield curve, interest rate volatility, and eurodollar rates explain changes in interest rate swap spreads...|$|R
50|$|Devices may be hosts or peripherals. Some, such as mobile phones, {{can take}} either role {{depending}} on what kind is detected on the other end. These types of ports are called Dual-Role-Data (DRD). When two such devices are connected, the roles are randomly assigned but a swap can be commanded from either end. Furthermore, dual-role devices that implement USB Power Delivery may independently and dynamically <b>swap</b> <b>data</b> and power roles using the <b>Data</b> Role <b>Swap</b> or Power Role Swap processes. This allows for charge-through hub or docking station applications where the Type-C device acts as a USB data host while acting as a power consumer rather than a source.|$|R
40|$|The pre-tabular {{statistical}} disclosure control (SDC) {{method of}} <b>data</b> <b>swapping</b> {{is the preferred}} method for protecting Census tabular data in some National Statistical Institutes, including the United States and Great Britain. A pre-tabular SDC method has the advantage that it only needs {{to be carried out}} once on the microdata and all tables released (under the conditions of the output strategies, eg. fixed categories of variables, minimum cell size and population thresholds) are considered protected. In this paper, we propose a method for targeted <b>data</b> <b>swapping.</b> The method involves a probability proportional to size selection strategy of high risk households for <b>data</b> <b>swapping.</b> The selected households are then paired with other households having the same control variables. In addition, the distance between paired households is determined by the level of risk with respect to the geographical hierarchies. The strategy is compared to a random <b>data</b> <b>swapping</b> strategy in terms of the disclosure risk and data utility...|$|E
40|$|Researchers from {{a growing}} range of fields and {{industries}} rely on public-access census data. These data are altered by census-taking agencies {{to minimize the}} risk of identification; one such disclosure avoidance measure is the <b>data</b> <b>swapping</b> procedure. I study the effects of <b>data</b> <b>swapping</b> on contingency tables using a dummy dataset, public-use American Community Survey (ACS) data, and restricted-use ACS data accessed within the U. S. Census Bureau. These simulations demonstrate that as the rate of swapping is varied, the effect on joint distributions of categorical variables is no longer understandable when the <b>data</b> <b>swapping</b> procedure attempts to target at-risk individuals for swapping using a simple targeting criterion. Comment: 19 pages, 7 figures. Submitted to the Journal of Privacy and Confidentialit...|$|E
40|$|<b>Data</b> <b>swapping</b> is {{a common}} {{technique}} for statistical disclosure limitation, but its effects on real data are not understood completely. In this paper, we consider measures {{that can be used}} to quantify distortion to the data engendered by <b>data</b> <b>swapping</b> when the variables in the data set are categorical. These measures are applied to a data set derived from the Current Population Survey. Their behavior is studied and compared for various values of the swapping rate and different choice of the variable swapped...|$|E
30|$|Simply {{having a}} number of disks running in a storage system is not a {{reflection}} of RAID implementation. Such disks act independent of each other, and are usually used to host meant for spooling <b>data</b> or <b>swap</b> files. These disks are not managed by any RAID level, and are defined as “Just a Bunch of Disks” or JBOD.|$|R
5000|$|The [...] "x2F" [...] , as in Model A2F, unit is {{a normal}} x2 unit, but its two HDAs also have a Fixed Head area over the first five cylinders, thereby {{reducing}} seek time to zero for these five cylinders. This fixed head area {{is intended to be}} allocated to the frequently accessed HASP or JES2 checkpoint area and thus greatly reduce head motion on the SPOOL device. The fixed head area can also be utilized for TSO <b>swap</b> <b>data</b> (MVT and SVS) and system <b>swap</b> <b>data</b> (MVS) wherein the <b>swap</b> <b>data</b> for SVS and MVS consist of blocks of pages that have been in memory when an address space is selected for swap-out; those pages need not be contiguous and in general do not include pages that have not been modified since their last page-in. This system architecture greatly improves context switches between TSO users or batch regions.|$|R
5000|$|When a {{certificate}} [...] fails, the <b>data</b> structure must <b>swap</b> [...] and [...] in the heap, and update the certificates {{that each of}} them was present in.|$|R
40|$|Abstract. <b>Data</b> <b>swapping,</b> a term {{introduced}} in 1978 by Dalenius and Reiss {{for a new}} method of statistical disclosure protection in confidential data bases, has taken on new meanings and been linked to new statistical methodologies over the intervening twenty-five years. This paper revisits the original (1982) published version of the the Dalenius-Reiss <b>data</b> <b>swapping</b> paper and then traces the developments of statistical disclosure limitation methods that {{can be thought of}} as rooted in the original concept. The emphasis here, as in the original contribution, is on both disclosure protection and the release of statistically usable data bases...|$|E
40|$|<b>Data</b> <b>swapping</b> is one {{of several}} {{disclosure}} avoidance methods that the Census Bureau implements to uphold confidentiality mandated by law. The Center for Disclosure Avoidance Research (CDAR) is currently studying the use of n-cycle swapping as a means to protect respondent identity in large-scale data. N-cycle swapping, a variant of <b>data</b> <b>swapping,</b> uses permutations of size 'n' to swap data records rather than swapping them in pairs. In this talk, we will discuss the processes surrounding n-cycle swapping, CDAR's current studies and challenges, and future projects and data products involving this disclosure avoidance technique. This research is supported by the NCRN Coordinating Office...|$|E
40|$|<b>Data</b> <b>swapping</b> is a {{statistical}} disclosure limitation method {{used to protect}} the confidentiality of data by interchanging variable values between records. We propose a risk-utility framework for selecting an optimal swapped data release when considering several swap variables and multiple swap rates. Risk and utility values associated with each such swapped data file are traded off along a frontier of undominated potential releases, which contains the optimal release(s). Current Population Survey data are used to illustrate the framework for categorical <b>data</b> <b>swapping.</b> Key words: constrained swaps; data confidentiality; Hellinger distance; optimal release; risk measure; risk-utility frontier; statistical disclosure limitation; swap rate; swapping attribute; unconstrained swaps, utility measure. ...|$|E
5000|$|Tullett Prebon: The company {{operates}} as an intermediary in wholesale financial markets. Many of its clients are commercial and investment banks. It operates in eight product areas: Volatility, Rates, Credit, Treasury, Non Banking, Energy, Equities and Property. The company also has specialist trading desks including, for example, Insurance Linked Securities where it facilitates Catastrophe Bond secondary market trading {{and provides a}} broking service to arrange Primary and Private Market transactions. It also provides data information services for financial institutions, covering areas such as Solvency II risk <b>data,</b> Credit Default <b>Swaps</b> <b>data</b> and FX Options data.|$|R
40|$|Abstract. In {{this paper}} we {{investigate}} trading with optimal mean reverting portfolios subject to cardinality constraints. First, we identify {{the parameters of}} the underlying VAR(1) model of asset prices and then the quantities of the corresponding Ornstein-Uhlenbeck (OU) process are estimated by pattern matching techniques. Portfolio optimization is performed according to two approaches: (i) maximizing the predictability by solving the generalized eigenvalue problem or (ii) maximizing the mean return. The optimization itself is carried out by stochastic search algorithms and Feed Forward Neural Networks (FFNNs). The presented solutions satisfy the cardinality constraint thus providing sparse portfolios to minimize the transaction costs and to maximize interpretability of the results. The performance has been tested on historical <b>data</b> (<b>SWAP</b> rates, SP 500, and FOREX). The proposed trading algorithms have achieved 29. 57 % yearly return on average, on the examined data sets. The algorithms prove to be suitable for high frequency, intraday trading as they can handle financial data up to the arrival rate of every second...|$|R
50|$|Data and La Forge {{determine}} that the power surge causes segments of the main computer and <b>Data's</b> processes to <b>swap</b> memories, and they start a memory purge to restore both to normal operation.|$|R
40|$|Abstract. Data {{perturbation}} is {{a method}} of reference control in statistical database. Data modification and <b>data</b> <b>swapping</b> are two methods of data perturbation. Data modification replaces the original data with the sample data sequence after deciding probability distribution function of the original data and generating sample data sequence. <b>Data</b> <b>swapping</b> interchanges attribute values of records. While the algorithm of the former is complex, this paper presents a new method in which the sample sequence generation is skipped. We simply sort the original data, divide them into different groups and exchange attribute values of the same group. Experiment show that the proposed method can effectively simplify the former method and guarantee {{the safety of the}} statistical data at the same time...|$|E
40|$|<b>Data</b> <b>Swapping</b> is {{a popular}} value-invariant data {{perturbation}} technique. The quality of a <b>data</b> <b>swapping</b> method is measured by how well it preserves data privacy and data utility. As swapping data globally is computationally impractical, to guarantee its performance in these metrics appropriate, localization schemes are often conducted in advance. Equi-depth partitioning is preferred {{by most of the}} existing data perturbation techniques as it provides uniform privacy protection for each data tuple. However, this method performs ineffectively for two types of applications: one is to maintain statistics based on equi-width partitioning, such as the multivariate histogram with equal bin width, and the other is to preserve parametric statistics, such as covariance, in the context of sparse data with non-uniform distribution. As a natural solution for the above application, this paper explores the possibility of using <b>data</b> <b>swapping</b> with equi-width partitioning for private data publication, which has been little used in data perturbation due to the difficulty of preserving data privacy. With extensive theoretical analysis and experimental results, we show that, Equi-Width Swapping (EWS) can achieve a similar performance in privacy preservation to that of Equi-Depth Swapping (EDS) if the number of partitions is sufficiently large (e. g. Ã Â¿ = Ã Â¿N, where N is the size of dataset). Our experimental results in both synthetic and real-world data validate our theoretical analysis. Yidong Li and Hong She...|$|E
40|$|Statistical {{agencies}} seek {{to disseminate}} useful data while keeping low {{the risk of}} statistical confidentiality disclosure. Recognizing that deidentification of data is generally inadequate to protect its confidentiality against attack by a data snooper, agencies restrict the data they release for general use. Typically, these restricted data procedures have involved transformation or masking of the original, collected data through such devices as adding noise, topcoding, <b>data</b> <b>swapping,</b> and recoding. Recently, proposals have been put forth {{for the release of}} synthetic data, simulated from models constructed from the original data. This paper gives a framework for the comparison of masking and synthetic data as two approaches to disclosure limitation. Particular attention is paid to data utility and disclosure risk. Examples of instantiations of masking and of synthetic data construction are provided to illustrate the concepts. Particular attention is paid to <b>data</b> <b>swapping.</b> Insights drawn from the Bayesian paradigm are provided...|$|E
50|$|Permutation invariance: Where {{a set of}} data values can be {{represented}} by a statistical model that they are outcomes from independent and identically distributed random variables, {{it is reasonable to}} impose the requirement that any estimator of any property of the common distribution should be permutation-invariant: specifically that the estimator, considered {{as a function of the}} set of data-values, should not change if items of <b>data</b> are <b>swapped</b> within the dataset.|$|R
40|$|With the {{increasing}} physical event rate {{and number of}} electronic channels, traditional readout scheme meets the challenge of improving readout speed caused by the limited bandwidth of crate backplane. In this paper, a high-speed data readout method based on Ethernet is designed for each module to have capability of transmitting data to DAQ. Features of explicitly parallel data transmitting and distributed network architecture make the readout system has advantage of adapting varying requirements of particle physics experiments. Furthermore, to guarantee the readout performance and flexibility, a standalone embedded CPU system is utilized for network protocol stack processing. To receive customized data format and protocol from front-end electronics, a {{field programmable gate array}} (FPGA) is used for logic reconfiguration. To optimize the interface and improve the <b>data</b> <b>swap</b> speed between CPU and FPGA, a sophisticated method based on SRAM is presented in this paper. For the purpose of evaluating this high-speed readout method, a simplified readout module is designed and implemented. Test results show that this module can support up to 70 Mbps data throughput from the readout module to DAQ smoothly...|$|R
40|$|Analytic {{solutions}} are found for prices of variance and volatility swaps {{under a new}} time-dependent stochastic model for the dynamics of variance. The main features of the new stochastic differential equation are (1) an empirically validated cν 3 / 2 diffusion term and (2) a free function of time as a moving target in a reversion term, allowing additional flexibility for model calibration against market <b>data.</b> variance <b>swap,</b> volatility swap, stochastic variance,...|$|R
40|$|Many {{papers have}} speculated on the {{possibility}} of applying peer-to-peer networking concepts to networks that exist in the physical world such as financial markets, business or personal communication and ad hoc networking. One such application that has been discussed in the literature has been the application of peer-to-peer to corporate supply chains to provide a flexible communication medium that may overcome some classical problems in supply chain management. This thesis presents the design, development and evaluation of a system which implements a peer-to-peer supply chain system. A general, flexible peer-to-peer network was developed which serves as a foundation to build peer-to-peer <b>data</b> <b>swapping</b> applications on top of. It provides simple network management, searching and <b>data</b> <b>swapping</b> services which form the basis of many peer-to-peer systems. Using the developed framework, a supply chain focussed application was built to test the feasibility of applying peer-to-peer networking to supply chain management. Results and discussion are presented of a scenario analysis which yielded positive results. Several future directions for research in this area are also discussed...|$|E
40|$|Application Scenario � Governmental and {{commercial}} organizations need to disseminate data for research or business-related applications. � Data owners {{are concerned about}} the privacy of their data, and not willing to release it in plain. � Data perturbation (randomization) strives to provide a solution to this dilemma. � Existing Perturbation Approach � Additive noise perturbation, data condensation, data anonymization, <b>data</b> <b>swapping,</b> sampling, etc. � They do not preserve Euclidean distance of the original data exactly. ...|$|E
30|$|Methods for {{providing}} individual anonymity {{are discussed in}} the field of privacy-preserving data publishing (Fung et al. 2010; Wong and Fu 2010). A plenty of methods have been proposed over the years, some of which are randomization (Evfimievski 2002), microaggregation (Domingo-Ferrer and Mateo-Sanz 2002), <b>data</b> <b>swapping</b> (Fienberg and McIntyre 2005), differential privacy (Dwork 2006), etc. A comprehensive overview of recent developments in the field can be found in Sowmyarani and Srinivasan (2012) and Rashid and Yasin (2015).|$|E
5000|$|One {{provision}} of the bill was {{the inclusion of the}} provisions of , the <b>Swap</b> <b>Data</b> Repository and Clearinghouse Indemnification Correction Act of 2013. This previous bill dealt with [...] "issues surrounding the indemnification provisions and confidentiality requirements of Dodd-Frank." ...|$|R
40|$|CFTC’’) {{is proposing}} rules to {{implement}} new statutory provisions enacted by Title VII of the Dodd-Frank Wall Street Reform and Consumer Protection Act. These proposed rules apply to <b>swap</b> <b>data</b> recordkeeping and reporting requirements for <b>swap</b> <b>data</b> repositories, derivatives clearing organizations, designated contract markets, swap execution facilities, swap dealers, major swap participants, and swap counterparties who are neither swap dealers nor major swap participants (including counterparties who {{qualify for the}} end user exception with respect to particular swaps). DATES: Comments must be received on or before February 7, 2011. ADDRESSES: You may submit comments, identified by RIN number 3038 –AD 19, {{by any of the}} following methods: • Agency Web site, via its Comments Online process...|$|R
30|$|The main {{advantage}} of random swap clustering {{is that it}} is extremely simple to implement. If k-means can be implemented for the <b>data,</b> random <b>swap</b> is only a small extension. K-means consists of two steps (partition step and centroid step), and the random swap method has only one additional step: prototype swap. In most cases, this step is independent on the data and the objective function. It is also trivial to implement, which makes it highly useful for practical applications.|$|R
