2|43|Public
40|$|AbstractBackgroundOne of {{the major}} {{concerns}} of the biomedical community is the increasing prevalence of antimicrobial resistant microorganisms. Recent findings show that the diversification of colony morphology may be indicative of the expression of virulence factors and increased resistance to antibiotic therapeutics. To transform these findings, and upcoming results, into a valuable clinical decision making tool, colony morphology characterisation should be standardised. Notably, {{it is important to}} establish the minimum experimental information necessary to contextualise the environment that originated the colony morphology, and describe the main morphological features associated unambiguously. ResultsThis paper presents MorphoCol, a new ontology-based tool for the standardised, consistent and machine-interpretable description of the morphology of colonies formed by human pathogenic bacteria. The Colony Morphology Ontology (CMO) is the first controlled vocabulary addressing the specificities of the morphology of clinically significant bacteria, whereas the MorphoCol publicly Web-accessible knowledgebase is an <b>end-user</b> <b>means</b> to search and compare CMO annotated colony morphotypes. Its ultimate aim is to help correlate the morphological alterations manifested by colony-forming bacteria during infection with their response to the antimicrobial treatments administered. ConclusionsMorphoCol is the first tool to address bacterial colony morphotyping systematically and deliver a free of charge resource to the community. Hopefully, it may introduce interesting features of analysis on pathogenic behaviour and {{play a significant role in}} clinical decision making. Database URL[URL]...|$|E
40|$|Background One of {{the major}} {{concerns}} of the biomedical community is the increasing prevalence of antimicrobial resistant microorganisms. Recent findings show that the diversification of colony morphology may be indicative of the expression of virulence factors and increased resistance to antibiotic therapeutics. To transform these findings, and upcoming results, into a valuable clinical decision making tool, colony morphology characterisation should be standardised. Notably, {{it is important to}} establish the minimum experimental information necessary to contextualise the environment that originated the colony morphology, and describe the main morphological features associated unambiguously. Results This paper presents MorphoCol, a new ontology-based tool for the standardised, consistent and machine-interpretable description of the morphology of colonies formed by human pathogenic bacteria. The Colony Morphology Ontology (CMO) is the first controlled vocabulary addressing the specificities of the morphology of clinically significant bacteria, whereas the MorphoCol publicly Web-accessible knowledgebase is an <b>end-user</b> <b>means</b> to search and compare CMO annotated colony morphotypes. Its ultimate aim is to help correlate the morphological alterations manifested by colony-forming bacteria during infection with their response to the antimicrobial treatments administered. Conclusions MorphoCol is the first tool to address bacterial colony morphotyping systematically and deliver a free of charge resource to the community. Hopefully, it may introduce interesting features of analysis on pathogenic behaviour and {{play a significant role in}} clinical decision making. The authors thank the project PTDC/SAU-ESA/ 646091 / 2006 /FCOMP- 01 - 0124 -FEDER- 007480 FCT, the Strategic Project PEst-OE/EQB/LA 0023 / 2013, the Project "BioHealth - Biotechnology and Bioengineering approaches to improve health quality", Ref. NORTE- 07 - 0124 -FEDER- 000027, co-funded by the Programa Operacional Regional do Norte (ON. 2 - O Novo Norte), QREN, FEDER, the project "RECI/BBB-EBI/ 0179 / 2012 - Consolidating Research Expertise and Resources on Cellular and Molecular Biotechnology at CEB/IBB", Ref. FCOMP- 01 - 0124 -FEDER- 027462, FEDER, and the Agrupamento INBIOMED from DXPCTSUG-FEDER unha maneira de facer Europa (2012 / 273). The research leading to these results has received funding from the European Union's Seventh Framework Programme FP 7 /REGPOT- 2012 - 2013. 1 under grant agreement no 316265, BIOCAPS. This document reflects only the author's views and the European Union is not liable for any use that may be made of the information contained herein. The authors also acknowledge PhD Grant of Ana Margarida Sousa SFRH/BD/ 72551 / 2010...|$|E
40|$|Everyone uses {{hand tools}} {{in their daily}} life, like knife and fork. Moreover, many people use hand tools in their {{profession}} as well as during leisure time. It is important that they can work with hand tools that provide comfort. Until now, the avoidance of discomfort was emphasized during the design process of hand tools, like screwdrivers, hand saws and paint brushes. In the near future, the focus will shift towards providing comfort. However, some questions need to be answered to make this shift, like: What does the <b>end-user</b> <b>mean</b> with comfort in using hand tools? How can we translate this into hand tool design and the design proces? How can we evaluate hand tools on comfort? These questions are answered in the current thesis. Industrial Design Engineerin...|$|R
30|$|Enterprise wiki systems [35] aim at facilitating {{contributions}} to and collaboration on hypertextual information artifacts. By providing <b>end-users</b> with <b>means</b> for creating situational applications, enterprise mashup systems [36] adopt this idea for technological artifacts. Evolutionary information systems [4] aim at combining these properties, hence providing highly introspectable, tailorable technology [37], thus ultimately enabling secondary design at all conceptual {{layers of the}} information system.|$|R
40|$|Scientific numeric {{databases}} {{are defined}} and their utility as research tools illustrated using three practical examples. This leads naturally to {{a summary of}} their capabilities and of the benefits that accrue to the scientist or engineer <b>end-user.</b> Practical <b>means</b> of online access are discussed, with primary emphasis on terminal scientific numeric database activity in Canada is given, along with an outline of those databases that are available online internationally from Canada. Peer reviewed: YesNRC publication: Ye...|$|R
40|$|M. Com. (Computer Auditing) Abstract: Mobile {{applications}} {{have grown}} to be the preferred mode of the banking sector and <b>end-user’s</b> <b>means</b> of conducting transactions due to benefits of ease of use and cost. The proliferation of mobile applications {{increases the likelihood that}} some may include IT security vulnerabilities. The objective {{of this paper is to}} examine the impact that mobile applications’ IT security risks have on the IT security controls in the South African (SA) banking sector – and the frameworks used by the organisations to assess the IT security controls related to mobile applications. An electronically administered questionnaire was sent to IT security analysts who are responsible for assessing IT security risks at the big four banking organisations in SA. The findings of this paper reveal that a number of IT security risks in mobile banking applications are related to inadequate software coding. Software programmers are more concerned with mobile application functionality than with IT security and this is the root cause of the noted finding. Banking organisations should ensure that mobile applications are secure before deployment to proactively prevent prospective attacks on their organisation’s IT control environment. This can be realised by conducting IT security audits, vulnerability assessments, and penetration testing throughout the software development lifecycle...|$|R
40|$|New {{directions}} {{in the provision}} of <b>end-user</b> computing experiences <b>mean</b> that we need to determine the best way to share data between small mobile computing devices. Partitioning large structures {{so that they can be}} shared efficiently provides a basis for data-intensive applications on such platforms. In conjunction with such an approach, dictionarybased compression techniques provide additional benefits and help to prolong battery life...|$|R
40|$|Our {{purpose is}} to provide <b>end-users</b> with a <b>means</b> to query {{ontology}} based knowledge bases using natural language queries and thus hide the complexity of formulating a query expressed in a graph query language such as SPARQL. The main originality of our approach lies {{in the use of}} query patterns. Our contribution is materialized in a system named SWIP, standing for Semantic Web Interface Using Patterns. The demo will present use cases of this system...|$|R
40|$|International audienceThis paper {{presents}} an agent-oriented visual programming approach which aims at providing MABS <b>end-users</b> with a <b>means</b> to easily elaborate artificial autonomous behaviors {{according to a}} targeted domain, namely situational programming (SP). More specifically, SP defines design principles {{which could be used}} to develop MABS visual programming toolkits suited for non developers and MABS novices. This paper presents SP and how it is used to build a MABS video game which can be played by MABS novices, that is any Internet user...|$|R
40|$|The rapid {{development}} of new technologies {{in the field of}} hardware, software and telecommunications allowed {{the creation of a new}} mobile device generation (smartphones and tablets) characterized by interesting and attractive features such as touch or multi-touch displays and embedded Operating Systems (OSs). Mobile OSs offer several functional and playful features to <b>end-users</b> by <b>means</b> of mobile applications. Such characteristics led to a worldwide increase in smartphones’ sales, resulting in Android being the world’s best-selling mobile OS. However, due to its popularity, Android is also the first mobile open source OS with a large number of vulnerabilities, cyber-attacks and malicious apps. This paper aims at presenting a review of recent patents, and state of art solutions as well, in the field of information security in mobile devices. Finally, a discussion pointing out the current and future developments in this area is presented...|$|R
40|$|This licentiate thesis {{addresses}} {{design and}} development problems that arise when service providers, and service end-users face {{the variety of}} computing devices available on the market. The devices are designed for many types of use in various situations and settings, {{which means that they}} have different capabilities in terms of presentation, interaction, memory, etc. Service providers often handle these differences by creating a new version for each device. This creates a lot of development and maintenance work, and often leads to restrictions on the set of devices that services are developed for. For service <b>end-users,</b> this <b>means</b> that {{it can be difficult to}} combine devices that fit the intended usage context and services that provide the needed content. New development methods that target multiple devices from the start are needed. The differences between devices call for services that can adapt to various devices, and present themselves with device specific user interfaces...|$|R
40|$|<b>End-user</b> {{development}} <b>means</b> {{the active}} participation of end {{users in the}} software development process. In this perspective, tasks that are traditionally performed by professional software developers are transferred to end users, {{who need to be}} specifically supported in performing these tasks. We have developed a methodology that supports user work practice and metadesign, allowing experts in a domain to personalize and evolve their own software environments. In this article we illustrate how this methodology is applied to a project for the development of an interactive system in the medical domain. Physicians and their activities have been carefully analyzed through a field study that is reported in the article, in order to provide them with computer systems that may improve their work practice and determine an increase in their productivity and performance, that is, a better quality of diagnosis and medical cure, with the achievement of competitive advantage for the organization they work in...|$|R
40|$|For over a decade, dCache {{has been}} synonymous with large-capacity, {{fault-tolerant}} storage using commodity hardware that supports seamless data migration {{to and from}} tape. Over that time, it has satisfied the requirements of various demanding scientific user communities to store their data, transfer it between sites and fast, site-local access. When the dCache project started, {{the focus was on}} managing a relatively small disk cache in front of large tape archives. Over the project's lifetime storage technology has changed. During this period, technology changes have driven down the cost-per-GiB of harddisks. This resulted in a shift towards systems where the majority of data is stored on disk. More recently, the availability of Solid State Disks, while not yet a replacement for magnetic disks, offers an intriguing opportunity for significant performance improvement if they can be used intelligently within an existing system. New technologies provide new opportunities and dCache user communities' computing models are changing. The traditional data models, in which tape is used as an active storage, are being revised with tape adopting a more archival model. The symbiotic relationship between dCache and the <b>end-users</b> <b>means</b> that dCache is both driven by and facilitating these changes. Recently, dCache introduced support for WebDAV and the NFS 4. 1 /pNFS protocols. This move away from bespoke protocols towards standards {{is the result of the}} availability of protocols that support large storage systems. dCache's adoption of standards allows end-users to use their favourite desktop data-transfer clients or unmodified analysis software. This keeps dCache competitive with industry solutions. Hadoop FS (HDFS) provides an easy-to-maintain backend storage that is showing promise as an easy-to-maintain storage system. dCache is adopting HDFS as an alternative to local filesystem storage. Since HDFS doesn't offer file system semantics, integrating support into dCache provides some challenges. Once solved, this work will allow dCache integration with other storage technologies such as object stores and cloud storage. We present a short summary of what dCache is providing in new long-term support release (the next "Golden Release") and offers a glimpse into the future of dCache with the emerging storage technology...|$|R
40|$|International audienceOur {{purpose is}} to provide <b>end-users</b> with a <b>means</b> to query {{ontology}} based knowledge bases using natural language queries and thus hide the complexity of formulating a query expressed in a graph query language such as SPARQL. The main originality of our approach lies {{in the use of}} query patterns. In this article we justify the postulate supporting our work which claims that queries issued by real life end-users are variations of a few typical query families. We also explain how our approach is designed to be adaptable to different user languages. Evaluations on the QALD- 3 data set have shown the relevancy of the approach...|$|R
40|$|This paper {{gives an}} {{overview}} of research within the ALADIN project, which aims to develop an assistive vocal interface for people with a physical impairment. In contrast to existing ap-proaches, the vocal interface is trained by the <b>end-user</b> himself, which <b>means</b> {{it can be used}} with any vocabulary and grammar, and that it is maximally adapted to the — possibly dysarthric — speech of the user. This paper describes the overall learn-ing framework, the user-centred design and evaluation aspects, database collection and approaches taken to combat problems such as noise and erroneous input. Index Terms: vocal user interface, user-centred design, self-taught learning, speech database, dysarthric speec...|$|R
40|$|Current Computer-Aided Architectural Design (CAAD) systems fail to {{represent}} buildings in-use before their realization. This failure prevents testing {{the extent to}} which a proposed setting supports the activities of its intended users. We present a novel approach to human behavior simulation based on a thorough representation of <b>end-user</b> activities by <b>means</b> of events – computational constructs that simulate users’ individual and group activities to achieve a specific goal. Human behavior narratives result from a combination of top-down (planned) and bottom-up (unplanned) sequences of events, as a reaction to timebased schedules and to social and environmental stimuli, respectively. A narrative management system orchestrates the narrative developments and resolves conflicts that may arise among competing events...|$|R
40|$|Femtocells are {{primarily}} used to extend high-quality radio coverage in indoor environments. Dense femtocell access point (FAP) deployment however, {{can lead to}} severe interference on femtocell user equipment (FUE) from neighbouring FAP, when they operate on the same channel. The problem is compounded as FAP are deployed in an uncoordinated manner by the <b>end-user,</b> which <b>means</b> either avoiding or minimising interference is an essential prerequisite to achieve successful femtocell operation. This paper adopts a new strategy towards radio resource distribution and management by creating virtual clusters of femtocells which are managed by a logical entity called the Virtual Cluster Controller(VCC). Each VCC is situated between the FAPs and Radio Network Controller (RNC) and has a set of sub-channels which it can allocate amongst its members. A Virtual Cluster Formation (VCF) algorithm exploits location information to assign each FAP to a VCC so that the minimum distance between a FAP and the nearest FAP allocated to the same VCC is maximised, thereby ensuring minimal inter-FAP interference. Simulation results corroborate the significant performance improvement achieved by this virtual clustering model {{in terms of both}} interference minimisation and throughput for various FAP deployment scenarios...|$|R
40|$|DOI: 10. 1016 /j. jnca. 2016. 02. 006 Filiació URV: SIUsers are unceasingly {{relying on}} {{personal}} clouds (like Dropbox, Box, etc) to store, edit and retrieve their files stored in remote servers. These systems generally follow a client–server model {{to distribute the}} files to <b>end-users.</b> This <b>means</b> that they require {{a huge amount of}} bandwidth to meet the requirements of their clients. Personal clouds with limited bandwidth budget can benefit from the upload speed of the clients interested in the same content {{to improve the quality of}} service. This can be done by introducing a peer-to-peer protocol, BitTorrent for instance, when the load on a certain content becomes high. The main challenge is to decide when to switch to BitTorrent and how to allocate the cloud's available bandwidth to the different clients. In this paper, we propose an algorithm for the allocation of the cloud׳s bandwidth. Based on the current load and the predefined quality of service constraints, the algorithm identifies the most suitable protocol for each swarm and provides the corresponding bandwidth allocation. We validate the algorithm using a real trace of the Ubuntu One system and the results show important gains in the download times experienced by the clients...|$|R
40|$|<b>End-user</b> {{development}} <b>means</b> {{the active}} participation of end {{users in the}} software development process. In this perspective, tasks that are traditionally performed by professional software developers at design time are transferred to end users at use time. This creates a new challenge for software engineers: designing software systems that can be evolved by end users. Metadesign, a new design paradigm discussed in this chapter, {{is regarded as a}} possible answer to this challenge. In this line, we have developed a metadesign methodology, called Software Shaping Workshop methodology, that supports user work practice and allows experts in a domain to personalize and evolve their own software environments. We illustrate the Software Shaping Workshop methodology and describe its application to a project in the medical domain. The work proposes a new perspective on system personalization, distinguishing between customization and tailoring of software environments. The software environments are customized by the design team to the work context, culture, experience, and skills of the user communities; they are also tailorable by end users at runtime in order to adapt them to the specific work situation and users’ preferences and habits. The aim is to provide the physicians with software environments that are easy to use and adequate for their tasks, capable to improve their work practice and determine an increase in their productivity and performance...|$|R
40|$|Environment) is {{addressing}} the still unsolved problem of designing, developing and putting into operation efficient and innovative mobile Service creation/execution platforms for networks beyond 3 G. The IST project SPICE {{is part of}} the Wireless World Initiative (WWI). With the growing diversity of services, devices and connectivity means, Service Platforms such as the SPICE platform will provide (mobile) <b>end-users</b> with communication <b>means</b> and tailored applications anywhere, anytime and on any device; and service providers, SMEs and nonprofessional users with service enablers that ease and quicken (context-aware) application development. Operators – like the ones in the SPICE consortium – will take up the role of Service Provider (/Service Platform Provider) and research and develop an advanced B 3 G service delivery environment. The key principles of the SPICE project will be described, and the technical approach of the project will be presented in this paper...|$|R
40|$|Research on digital {{platform}} evolution is largely {{focused on how}} platform-owners leverage boundary resources to facilitate and control contributions from external developers to extend the functional diversity and scope of a digital device. However, {{our knowledge of the}} {{digital platform}}s that carve out their existence exclusively in the service layer of industry architectures, i. e. without proprietary device connections, is limited. The concept of digital service platforms directs attention to such platforms, the role of end-users as value co-creators, and devices as requisite, but not necessarily proprietary, distribution mechanisms for service. Based on a longitudinal case study of Spotify, this paper contributes by demonstrating that digital service platform evolution is characterized by specific architectural conditions that rationalize the use of boundary resources for extending scale rather than scope, and for resourcing and controlling not only developers but also <b>end-users</b> as a <b>means</b> to strategically adjust the evolutionary process...|$|R
40|$|Electric {{power supply}} {{companies}} increasingly rely on enterprise IT systems {{to provide them}} with a comprehensive view {{of the state of the}} distribution network. Within a utility-wide network, enterprise IT systems collect data from various metering devices. Such data can be effectively used for the prediction of power supply network vulnerability. The purpose of this paper is to present the Enterprise Service Bus (ESB) -based Sensor Web integration solution that we have developed with the purpose of enabling prediction of power supply network vulnerability, in terms of a prediction of defect probability for a particular network element. We will give an example of its usage and demonstrate our vulnerability prediction model on data collected from two different power supply companies. The proposed solution is an extension of the GinisSense Sensor Web-based architecture for collecting, processing, analyzing, decision making and alerting based on the data received from heterogeneous data sources. In this case, GinisSense has been upgraded to be capable of operating in an ESB environment and combine Sensor Web and GIS technologies to enable prediction of electric power supply system vulnerability. Aside from electrical values, the proposed solution gathers ambient values from additional sensors installed in the existing power supply network infrastructure. GinisSense aggregates gathered data according to an adapted Omnibus data fusion model and applies decision-making logic on the aggregated data. Detected vulnerabilities are visualized to <b>end-users</b> through <b>means</b> of a specialized Web GIS application...|$|R
40|$|The current {{ubiquitous}} {{network access}} and increase in network bandwidth are driving {{the sales of}} mobile location-aware user devices and, consequently, the development of context-aware applications, namely location-based services. The goal of this project is to provide consumers of location-based services with a richer <b>end-user</b> experience by <b>means</b> of service composition, personalization, device adaptation and continuity of service. Our approach relies on a multi-agent system composed of proxy agents that act as mediators and providers of personalization meta-services, device adaptation and continuity of service for consumers of pre-existing location-based services. These proxy agents, which have Web services interfaces to ensure {{a high level of}} interoperability, perform service composition and take in consideration the preferences of the users, the limitations of the user devices, making the usage of different types of devices seamless for the end-user. To validate and evaluate the performance of this approach, use cases were defined, tests were conducted and results gathered which demonstrated that the initial goals were successfully fulfilled...|$|R
40|$|Spreadsheet {{workbook}} contents {{are simple}} programs. Because of this, probabilistic programming techniques {{can be used}} to perform Bayesian inversion of spreadsheet computations. What is more, existing execution engines in spreadsheet applications such as Microsoft Excel can be made to do this using only built-in functionality. We demonstrate this by developing a native Excel implementation of both a particle Markov Chain Monte Carlo variant and black-box variational inference for spreadsheet probabilistic programming. The resulting engine performs probabilistically coherent inference over spreadsheet computations, notably including spreadsheets that include user-defined black-box functions. Spreadsheet engines that choose to integrate the functionality we describe in this paper will give their users the ability to both easily develop probabilistic models and maintain them over time by including actuals via a simple user-interface mechanism. For spreadsheet <b>end-users</b> this would <b>mean</b> having access to efficient and probabilistically coherent probabilistic modeling and inference for use in all kinds of decision making under uncertainty...|$|R
40|$|Abstract — In this paper, we {{introduce}} {{a novel approach}} intended to simplify the production of multimedia content from real objects {{for the purpose of}} knowledge sharing, which is particularly appropriate to the cultural heritage field. It consists in a pipeline that covers all steps from the digitization of the objects up to the Web publishing of the resulting digital copies. During a first stage, the digitization is performed by a high speed 3 D scanner that recovers the object’s geometry. A second stage then extracts from the recovered data a color texture as well as a texture of details, in order to enrich the acquired geometry in a more realistic way. Finally, a third stage converts these data so that they are compatible with the recent WebGL paradigm, then providing 3 D multimedia content directly exploitable by <b>end-users</b> by <b>means</b> of standard Internet browsers. The pipeline design is centered on automation and speed, {{so that it can be}} used by non expert users to produce multimedia content from potentially large object’s collections, like it may be the case in cultural heritage. The choice of a high speed scanner is particularly adapted for such a design, since this kind of devices has the advantage of being fast and intuitive. Processing stages that follow the digitization are both completely automatic and “seamless”, in the sense that it is not incumbent upon the user to perform tasks manually, nor to use external softwares that generally need additional operations to solve compatibility issues. I...|$|R
40|$|In this paper, we {{introduce}} {{a novel approach}} intended to simplify the production of multimedia content from real objects {{for the purpose of}} knowledge sharing, which is particularly appropriate to the cultural heritage field. It consists in a pipeline that covers all steps from the digitization of the objects up to the Web publishing of the resulting digital copies. During a first stage, the digitization is performed by a high speed 3 D scanner that recovers the object 2 ̆ 7 s geometry. A second stage then extracts from the recovered data a color texture as well as a texture of details, in order to enrich the acquired geometry in a more realistic way. Finally, a third stage converts these data so that they are compatible with the recent WebGL paradigm, then providing 3 D multimedia content directly exploitable by <b>end-users</b> by <b>means</b> of standard Internet browsers. The pipeline design is centered on automation and speed, {{so that it can be}} used by non expert users to produce multimedia content from potentially large object 2 ̆ 7 s collections, like it may be the case in cultural heritage. The choice of a high speed scanner is particularly adapted for such a design, since this kind of devices has the advantage of being fast and intuitive. Processing stages that follow the digitization are both completely automatic and "seamless 2 ̆ 72 ̆ 7, in the sense that it is not incumbent upon the user to perform tasks manually, nor to use external softwares that generally need additional operations to solve compatibility issues...|$|R
40|$|One {{important}} aspect in non-invasive brain-computer interface (BCI) {{research is to}} acquire the electroencephalogram (EEG) in a proper way. From an <b>end-user</b> perspective this <b>means</b> with maximum comfort and without any extra inconveniences (e. g., washing the hair). Whereas from a technical perspective, the signal quality has to be optimal to make the BCI work effectively and efficiently. In this work we evaluated three different commercially available EEG acquisition systems that differ {{in the type of}} electrode (gel-, water-, and dry-based), the amplifier technique, and the data transmission method. Every system was tested regarding three different aspects, namely, technical, BCI effectiveness and efficiency (P 300 communication and control), and user satisfaction (comfort). We found that the water-based system had the lowest short circuit noise level, the hydrogel-based system had the highest P 300 spelling accuracies, and the dry electrode system caused the least inconveniences. Therefore, building a reliable BCI is possible with all evaluated systems and it is on the user to decide which system meets the given requirements best...|$|R
40|$|The project SPICE (Service Platform for Innovative Communication Environment) is {{addressing}} the still unsolved problem of designing, developing and putting into operation efficient and innovative mobile Service creation/execution platforms for networks beyond 3 G. The IST project SPICE {{is part of}} the Wireless World Initiative (WWI). With the growing diversity of services, devices and connectivity means, Service Platforms (such as the SPICE platform) will provide (mobile) <b>end-users</b> with communication <b>means</b> and tailored applications anywhere, anytime and on any device; and service providers, SMEs and non-professional users with service enablers that facilitate and quicken (context-aware) application development. Operators – such as in the SPICE consortium – will take up the role of Service Provider (/Service Platform Provider) and research and develop an advanced B 3 G service delivery environment. In this paper we present our research on a Service Platform in B 3 G environment. We sketch the key issues of such a Service Platform. Current technological solutions are discussed and the specific SPICE approach to address these challenges is described. I...|$|R
40|$|This licentiate thesis {{addresses}} {{design and}} development problems that arise when service providers, and service end-users face {{the variety of}} computing devices available on the market. The devices are designed for many types of use in various situations and settings, {{which means that they}} have different capabilities in terms of presentation, interaction, memory, etc. Service providers often handle these differences by creating a new version for each device. This creates a lot of development and maintenance work, and often leads to restrictions on the set of devices that services are developed for. For service <b>end-users,</b> this <b>means</b> that {{it can be difficult to}} combine devices that fit the intended usage context and services that provide the needed content. New development methods that target multiple devices from the start are needed. The differences between devices call for services that can adapt to various devices, and present themselves with device specific user interfaces. We propose a way of developing device independent services by using interaction acts to describe user-service interaction. Devices would interpret the interaction acts and generate user interfaces according to their own specific capabilities. Additional presentation information can be encoded in customization forms, to further control how the user interface would be generated. Different devices would generate different user interfaces from the same interaction acts, and a device could generate different user interfaces from the same interaction acts combined with different customization forms. In this thesis, the interaction act and customization form concepts are described in detail. A system prototype for handling them and two sample services have been implemented. Preliminary evaluations indicate that interaction acts and customization forms constitute a feasible approach for developing services with multiple user interfaces. The thesis concludes with a discussion of the problems arising when evaluating this kind of systems, and some conclusions on how to continue the evaluation process. ...|$|R
40|$|The {{purpose of}} this paper is to examine small- to {{medium-sized}} enterprises (SMEs) partnerships and co-operation utilization within innovation processes. Industrial firms are gaining ideas for innovation from various sources and their innovative performance depends, besides their internal knowledge resources, also on how successful they are at appropriating knowledge from external sources. This seems to be true also with smaller, low tech and remote firms. According to analysis of Finnish data small low tech companies have a growth oriented innovation activity. The main findings from Finnish cases show that when the renewal is important at the firm level only, cooperation with business partners (consultants, suppliers) is emphasized; however, when the renewal is important at market level then the public sector cooperation with universities etc. is emphasized. 25 % of the studied companies had university co-operation. Informal co-operation and short-term education was seen the most important forms of co-operation. The study of SME innovation in Eastern Finland is included in which implemented innovations during 2003 ? 2005 were investigated. This paper is based on a quantitative study of a sample of SMEs located in the Eastern Finland region in Finland. The entrepreneurs completed a research questionnaire which was sent to 3226 entrepreneurs. 381 completed answers were received and the response rate was modest 11, 8 %, the final analyzed data contains 370 completed answers. The results suggest that the largest backlog appears to be in utilizing universities and public research organizations. The choices of knowledge sources can be attributed to different capabilities of firms and network partners (consumers, universities etc.) in creating and utilizing (exploitation and exploration) respective innovation-enhancing knowledge. Universities and consulting firms also need to exploit new knowledge created in science, practice and by <b>end-users.</b> This <b>means</b> a challenge for universities and other actors to develop their services and capabilities to meet the needs of SMEs and the manner of SMEs to implement innovation processes hand in hand with daily business...|$|R
40|$|Experts {{in a field}} {{regularly}} apply {{a defined}} set of rules or procedures {{to carry out a}} problem-solving task or analysis on a given problem. Often the problem can be represented as a computer model, be it mathematical, chemical, or physics based, and so on. It would certainly be advantageous for a domain expert who is not proficient in software development to express solutions to problems in a domain-specific notation that can be executed as a program. Many new ideas aim to make software development easier and shift the development role closer to the <b>end-user.</b> One such <b>means</b> of development is the use of a small, intuitive programming language called a Domain-Specific Language (DSL.) This dissertation examines a generic approach to constructing a Virtual Machine (VM) to provide the runtime semantics for a particular DSL. It proposes a generic, object-oriented framework, called a VM Framework, in which to build a VM by subtyping abstract instruction and environment classes {{that are part of the}} VM Framework. The subtyped classes constitute an environment and an interface called an instruction set architecture and the instructions can access and operate on the environment in a deterministic way to provide the runtime semantics of a DSL program. Both instruction classes and environment classes encapsulat...|$|R
40|$|Personal {{cloud storage}} {{services}} offer {{a large amount}} of space and the ease to synchronize devices with great simplicity. They help in sharing content and in backing up data by relying on the cloud to seamlessly store users' files. Despite the high public interest in such services, little information is available about design, implementation choices and, most of all, actual performance implications. In this work, we design a methodology to run benchmarks of cloud storage services. We unveil how they are implemented, where their servers are located, and measure implication on performance as seen by <b>end-users.</b> By <b>means</b> of repeatable and customizable tests, we identify eventual advanced capabilities the cloud client implements, and measure performance implications. We consider realistic workloads (e. g., the exchange of text and binary files, compressed archives, the presence of file replicas, etc.) and network accesses (e. g., high speed university campus, or 3 G mobile connectivity). We use then the benchmarking methodology to compare 11 cloud services, including popular solutions like Dropbox or Google Drive, and two private storage solutions, i. e., the open source ownCloud and the commercial VMware Horizon, that we installed and configured in our campus network. We take the perspective of a customer located in Europe, and we benchmark each service. Our case study reveals interesting differences in design choices. Results show no clear winner, with all services having potential for performance improvements. Some are limited by design choices, e. g., by artificially throttling upload and download speed, or by long application timers that slow down synchronization procedures. Others suffer TCP performance issues due to their data centers being located in other continents. In some scenarios, the synchronization of the same set of files can take 20 times longer. In other cases, we observe a wastage of twice as much network capacity, questioning the design of some services, especially in a bandwidth constrained scenario like 3 G/ 4 G connectivity. Our results show the implications of design choices on performance, and of the tradeoffs faced when building cloud storage services. The developed methodology and the collected results are useful both as benchmarks and as guidelines for system design. In addition, they help the prospected customer in the choice of the best service by allowing the execution of independent performance tests before purchasing a storage offer...|$|R
30|$|The {{home care}} service {{continues}} to grow as the overall population ages, and as the economic situation requires parents and other adults of the family all out for work and for longer hours. At the same time, Hong Kong, following the global trend, is shifting its care provision more and more from the expensive institutional-based model to the more economical community-based and family-based model. Accidents could happen when the elderly just alone at home and {{they might not be}} able to send out help-seeking alerts to their care-givers immediately, which could cause the risk of safety. An activity monitoring system would use various infrared sensors to monitor the activity patterns of the client in a non-obtrusive manner within an apartment. The system would prompt the user for response if certain patterns of vital signs and activities deemed critical appear. The proposed TeleCare technology for iHome would offer to the <b>end-users</b> as a <b>mean</b> to alert their care-providers when they are in certain critical situations. The deliverables would have impacts on the 500, 000 elderly in Hong Kong who live singly or pseudo-singly (when other family members are all out for work), plus other vulnerable who are staying at home often by themselves, such as people with disabilities and chronic illness, summing to another 500, 000 in the territory. Many of them would appreciate some forms of care service in case of emergency. This development facilitates care at home without requiring other family members and care-providers to stay home all the time.|$|R
40|$|End-user {{development}} is a very common but often largely overlooked phenomenon in information systems research and practice. <b>End-user</b> development <b>means</b> that regular people, the end-users of software, and not professional developers are doing software development. A {{large number of people}} are directly or indirectly impacted by the results of these non-professional development activities. The numbers of users performing end-user development activities are difficult to ascertain precisely. But it is very large, and still growing. Computer adoption is growing towards 100 % and many new types of computational devices are continually introduced. In addition, other devices not previously programmable are becoming so. This means that, at this very moment, {{hundreds of millions of people}} are likely struggling with development problems. Furthermore, software itself is continually being adapted for more flexibility, enabling users to change the behaviour of their software themselves. New software and services are helping to transform users from consumers to producers. Much of this is now found on-line. The problem for the end-user developer is that little of this {{development is}} supported by anyone. Often organisations do not notice end-user development and consequently neither provide support for it, nor are equipped to be able to do so. Many end-user developers do not belong to any organisation at all. Also, the end-user development process may be aggravating the problem. End-users are usually not really committed to the development process, which tends to be more iterative and ad hoc. This means support becomes a distant third behind getting the job done and figuring out the development issues to get the job done. Sometimes the software itself may exacerbate the issue by simplifying the development process, deemphasising the difficulty of the task being undertaken. On-line support could be the lifeline the end-user developer needs. Going online one can find all the knowledge one could ever need. However, that does still not help the end-user apply this information or knowledge in practice. A virtual community, through its ability to adopt the end-user’s specific context, could surmount this final obstacle. This thesis explores the concept of end-user development and how it could be supported through on-line sources, in particular virtual communities, which it is argued here, seem to fit the end-user developer’s needs very well. The experiences of real end-user developers and prior literature were used in this process. Emphasis has been on those end-user developers, e. g. small business owners, who may have literally nowhere to turn to for support. Adopting the viewpoint of the end-user developer, the thesis examines the question of how an end-user could use a virtual community effectively, improving the results of the support process. Assuming the common situation where the demand for support outstrips the supply...|$|R
40|$|Purpose - New {{directions}} {{in the provision}} of <b>end-user</b> computing experiences <b>mean</b> {{that the best way to}} share data between small mobile computing devices needs to be determined. Partitioning large structures so that they can be shared efficiently provides a basis for data-intensive applications on such platforms. The partitioned structure can be compressed using dictionary-based approaches and then directly queried without firstly decompressing the whole structure. Design/methodology/approach - The paper describes an architecture for partitioning XML into structural and dictionary elements and the subsequent manipulation of the dictionary elements to make the best use of available space. Findings - The results indicate that considerable savings are available by removing duplicate dictionaries. The paper also identifies the most effective strategy for defining dictionary scope. Research limitations/implications - This evaluation is based on a range of benchmark XML structures and the approach to minimising dictionary size shows benefit in the majority of these. Where structures are small and regular, the benefits of efficient dictionary representation are lost. The authors' future research now focuses on heuristics for further partitioning of structural elements. Practical implications - Mobile applications that need access to large data collections will benefit from the findings of this research. Traditional client/server architectures are not suited to dealing with high volume demands from a multitude of small mobile devices. Peer data sharing provides a more scalable solution and the experiments that the paper describes demonstrate the most effective way of sharing data in this context. Social implications - Many services are available via smartphone devices but users are wary of exploiting the full potential because of the need to conserve battery power. The approach mitigates this challenge and consequently expands the potential for users to benefit from mobile information systems. This will have impact in areas such as advertising, entertainment and education but will depend on the acceptability of file sharing being extended from the desktop to the mobile environment. Originality/value - The original work characterises the most effective way of sharing large data sets between small mobile devices. This will save battery power on devices such as smartphones, thus providing benefits to users of such devices...|$|R
40|$|Growth” is {{endemic to}} our {{economic}} system. Economies of scale, long-held as critical means to efficiency and profitability, {{have taken on}} new meanings: whereas they once relied on {{the skills of the}} mechanical engineer to realize wide-scaling manufacturing capabilities, our cultural shift towards the individual means a focus placed on customization of both product research and design. In the age of networked information and networked publics—built on the “solutions” of large, multinational companies like Cisco and EMC, as well as small, connected product and service startups— this paradoxical emphasis on broad ranging customization is resolved through the automated tools at our disposal today: platforms, data, and algorithms, to name a few. Throughout the decades, however, scalability and growth has always been perceived as essential by investors and the public alike. As commercial practitioners, we are often tasked with the design and development of projects meant to reach a large audience—at the time of release or further down the line. For instance, a website for a large NGO must scale in terms of both content and reach, accommodating a broad swath of information types for a global audience. Increasingly, as the integration of social streams and other “open” sources of content becomes valued by clients, the access of publicly available APIs requires an accommodation of the parameters set by those sources—often multi-billion dollar corporations. To develop designs and technologies that scale, we build on or develop our own platforms. These platforms include those with which we are familiar and interact every day, such as YouTube (and its API), Twitter, and various content management systems such as SiteCore or WordPress. The way in which we are able to design for scale today is enabled by our ability to capture a tremendous (and often overwhelming) amount of data. “Big Data” has become common parlance. We use the data that we capture to make inferences about the users for whom we design, giving us the ability to scale solutions across geographies, demographics, and markets. Algorithms are pervasive in today’s experience of designing at scale, especially as the time and cognition required to process the volume of information with which we interact increases. Once the sources of our data and content are identified, in order to present that information back to our <b>end-user</b> in a <b>means</b> unique to our project, we must process it. Infusing this data with value requires moving it through algorithms—ones that aggregate, analyze, modify, and more...|$|R
