710|0|Public
5|$|The Minnesota Territorial Legislature {{recognized}} Saint Anthony as {{a town in}} 1855 and Minneapolis in 1856. Boundaries {{were changed}} and Minneapolis was incorporated as a city in 1867. Minneapolis and Saint Anthony joined in 1872. Minneapolis changed more than 100 road names in 1873, including <b>deduplication</b> of names {{between it and the}} former Saint Anthony.|$|E
5|$|Alongside the Unix File System, {{which is}} {{typically}} the default file system on BSDs, DragonFly BSD supports HAMMER file system. It was developed specifically for DragonFly BSD {{to provide a}} feature-rich yet better designed analogue of the increasingly popular ZFS. HAMMER supports configurable file system history, snapshots, checksumming, data <b>deduplication</b> and other features typical for file systems of its kind.|$|E
5|$|Some NTFS {{features}} are not supported in ReFS, including object IDs, short names, file compression, file level encryption (EFS), user data transactions, hard links, extended attributes, and disk quotas. Sparse files are supported. Support for named streams is not implemented in Windows 8 and Windows Server 2012, {{though it was}} later added in Windows 8.1 and Windows Server 2012 R2. ReFS does not itself offer data <b>deduplication.</b> Dynamic disks with mirrored or striped volumes are replaced with mirrored or striped storage pools provided by Storage Spaces. In Windows Server 2012, automated error-correction with integrity streams is only supported on mirrored spaces; automatic recovery on parity spaces was added in Windows 8.1 and Windows Server 2012 R2. Booting from ReFS is not supported either.|$|E
25|$|Effective use of <b>deduplication</b> {{may require}} large RAM capacity; {{recommendations}} range between 1 and 5GB of RAM for every TB of storage. Insufficient physical memory {{or lack of}} ZFS cache can result in virtual memory thrashing when using <b>deduplication,</b> which can either lower performance or result in complete memory starvation. Solid-state drives (SSDs) {{can be used to}} cache <b>deduplication</b> tables, thereby speeding up <b>deduplication</b> performance.|$|E
25|$|Native data {{compression}} and <b>deduplication,</b> although {{the latter is}} largely handled in RAM and is memory hungry.|$|E
25|$|Data <b>deduplication</b> {{capabilities}} {{were added}} to the ZFS source repository at the end of October 2009, and relevant OpenSolaris ZFS development packages have been available since December 3, 2009 (build 128).|$|E
25|$|On December 9, 2008, Dell and EMC {{announced}} the multi-year extension, through 2013, {{of the strategic}} partnership with EMC. In addition, Dell expanded its product lineup by adding the EMC Celerra NX4 storage system to the portfolio of Dell/EMC family of networked storage systems and partnered on {{a new line of}} data <b>deduplication</b> products as part of its TierDisk family of data storage devices.|$|E
500|$|<b>Deduplication</b> for VHD: Reduces {{the storage}} space for VHD files with largely similar {{contents}} by storing the similar contents only once ...|$|E
2500|$|Other storage vendors use {{modified}} {{versions of}} ZFS to achieve very high data compression ratios. Two examples in 2012 were GreenBytes and Tegile. [...] In May 2014, Oracle bought GreenBytes for its ZFS <b>deduplication</b> and replication technology.|$|E
2500|$|A pool {{can contain}} datasets, which are [...] {{containers}} storing a native ZFS file system. Datasets can contain other datasets ("nested datasets"), which are transparent for file system purposes. A dataset within another dataset is treated {{much like a}} directory {{for the purposes of}} file system navigation, but it allows a branch of a file system to have different settings for compression, <b>deduplication</b> and other settings. This is because file system settings are per-dataset (and can be inherited by nested datasets).|$|E
2500|$|OpenZFS on OSX (abbreviated to O3X) is an {{implementation}} of ZFS for macOS. [...] O3X is under active development, with close relation to ZFS on Linux and illumos' ZFS implementation, while maintaining feature flag compatibility with ZFS on Linux. [...] O3X implements zpool version 5000, and includes the Solaris Porting Layer (SPL) originally authored for MacZFS, {{which has been}} further enhanced to include a memory management layer based on the illumos kmem and vmem allocators. [...] O3X is fully featured, supporting LZ4 compression, <b>deduplication,</b> ARC, L2ARC, and SLOG.|$|E
2500|$|A pool {{can also}} contain volumes (also known as zvols), {{which can be}} used as block storage devices by other systems. An example of a volume would be an iSCSI or Fibre Channel target for another system, used to create NAS, a SAN, or any other ZFS-backed raw block storage capability. The volume will be seen by other systems as a bare storage device which they can use as they like. Capabilities such as snapshots, redundancy, [...] "scrubbing" [...] (data {{integrity}} and repair checks), <b>deduplication,</b> compression, cache usage, and replication are operational but not exposed to the remote system, which [...] "sees" [...] only a bare file storage device. Because ZFS does not create a file storage system on the block device or control how the storage space is used, it cannot create nested ZFS datasets or volumes within a volume.|$|E
50|$|Scaling {{has also}} been a {{challenge}} for <b>deduplication</b> systems because ideally, the scope of <b>deduplication</b> needs to be shared across storage devices. If there are multiple disk backup devices in an infrastructure with discrete <b>deduplication,</b> then space efficiency is adversely affected. A <b>deduplication</b> shared across devices preserves space efficiency, but is technically challenging from a reliability and performance perspective.|$|E
5000|$|NetVault SmartDisk Data <b>Deduplication</b> offers disk-based backup, data compression, {{and data}} <b>deduplication</b> {{to reduce the}} backend storage footprint. It uses byte-level, {{variable}} block-based software <b>deduplication</b> and is hardware-agnostic, so no specialized drives or appliances are needed.|$|E
5000|$|Client or {{server-side}} <b>deduplication</b> via data <b>deduplication</b> {{engine that}} can {{see into the}} backup streams ...|$|E
5000|$|... content-aware data <b>deduplication</b> - a data <b>deduplication</b> {{method that}} leverages {{knowledge}} of specific application data formats.|$|E
50|$|Effective use of <b>deduplication</b> {{may require}} large RAM capacity; {{recommendations}} range between 1 and 5 GB of RAM for every TB of storage. Insufficient physical memory {{or lack of}} ZFS cache can result in virtual memory thrashing when using <b>deduplication,</b> which can either lower performance or result in complete memory starvation. Solid-state drives (SSDs) {{can be used to}} cache <b>deduplication</b> tables, thereby speeding up <b>deduplication</b> performance.|$|E
50|$|The {{advantage}} of in-line <b>deduplication</b> over post-process <b>deduplication</b> {{is that it}} requires less storage, since duplicate data is never stored. On the negative side, it is frequently argued that because hash calculations and lookups take so long, data ingestion can be slower, thereby reducing the backup throughput of the device. However, certain vendors with in-line <b>deduplication</b> have demonstrated equipment with similar performance to their post-process <b>deduplication</b> counterparts.|$|E
5000|$|... content-agnostic data <b>deduplication</b> - a data <b>deduplication</b> {{method that}} does not require {{awareness}} of specific application data formats.|$|E
50|$|NetVault Backup {{can also}} be used with NetVault SmartDisk, which is a disk storage {{repository}} and <b>deduplication</b> product. NetVault SmartDisk can perform post-process data <b>deduplication.</b> It represents a contiguous storage pool on disk without being subdivided into virtual tapes or slots for easier management and allows for byte-level, variable block-based software <b>deduplication.</b>|$|E
50|$|Data <b>deduplication</b> {{technology}} {{can be used to}} very efficiently track and remove duplicate blocks of data inside a storage unit. There are a multitude of implementations, each with their separate advantages and disadvantages. <b>Deduplication</b> is most efficient at the shared storage layer, however, implementations in software and even databases exist. The most suitable candidates for <b>deduplication</b> are backup and platform virtualization, because both applications typically produce or use a lot of almost identical copies. However, some vendors are now offering in-place <b>deduplication,</b> which deduplicates primary storage.|$|E
5000|$|Target <b>deduplication</b> is {{the process}} of {{removing}} duplicates when the data was not generated at that location. Example of this would be a server connected to a SAN/NAS, The SAN/NAS would be a target for the server (Target <b>deduplication).</b> The server is not aware of any <b>deduplication,</b> the server is also the point of data generation.|$|E
50|$|The Dell DR {{appliance}} is a hardware-based, inline <b>deduplication</b> storage product. Its <b>deduplication</b> {{engine was}} originally developed by Ocarina Networks. The NetVault and Ocarina engineering teams worked {{to integrate the}} two technologies.|$|E
50|$|HP Data Protector offers Data <b>deduplication</b> {{capabilities}} {{with the}} ability to compare blocks of data being written to a backup device with data blocks previously stored on the device. For data <b>deduplication,</b> HP Data Protector allows any disk-based data storage to be the target storage. Tape drives cannot be used as <b>deduplication</b> targets because of their sequential nature opposed to the block-oriented nature of disk-devices.|$|E
50|$|To date, data <b>deduplication</b> has {{predominantly}} {{been used}} with secondary storage systems. The {{reasons for this}} are two-fold. First, data <b>deduplication</b> requires overhead to discover and remove the duplicate data. In primary storage systems, this overhead may impact performance. The second reason why <b>deduplication</b> is applied to secondary data, is that secondary data tends to have more duplicate data. Backup application in particular commonly generate significant portions of duplicate data over time.|$|E
5000|$|SD3 Appliances - InQuinox’s US-built, SD3 {{appliances}} {{are intended}} to optimize the performance and simplify the deployment of Symantec’s NetBackup <b>Deduplication</b> (PureDisk) software. The SD3 is the first <b>deduplication</b> appliance to claim to surpassed the 1TBpm (TeraByte Per Minute) threshold. [...] The SD3 family consists of four models from a 1U systems with 1.3TB of space to their largest system with 192TB of usable space. InQuinox claims the largest system has 2880TB of equivalent space based on a 15:1 <b>deduplication</b> rate.|$|E
50|$|The data {{collection}} phase can also involve {{a process called}} <b>deduplication,</b> which involves removing redundant information from a data set in order to lower costs and improve user experience. The <b>deduplication</b> process filters and compresses the data that has been collected before it gets uploaded.|$|E
5000|$|Another way to {{classify}} data <b>deduplication</b> methods is according {{to where they}} occur. <b>Deduplication</b> occurring close to where data is created, {{is often referred to}} as [...] "source deduplication". When it occurs near where the data is stored, it is commonly called [...] "target deduplication".|$|E
5000|$|Intelligent Data <b>Deduplication</b> and Auto Image Replication (AIR) ...|$|E
50|$|Component {{techniques}} of Branch WAN Optimization include <b>deduplication,</b> wide area file services (WAFS), SMB proxy, HTTPS Proxy, media multicasting, web caching, and bandwidth management. Requirements for DC2DC WAN Optimization also center around <b>deduplication</b> and TCP acceleration, however these must {{occur in the}} context of multi-gigabit data transfer rates.|$|E
50|$|Post-process and in-line <b>deduplication</b> {{methods are}} often heavily debated.|$|E
5000|$|Efficient storage {{management}} using <b>deduplication</b> for disk and tape.|$|E
50|$|Commercial <b>deduplication</b> {{implementations}} {{differ by}} their chunking methods and architectures.|$|E
50|$|Quest offers data <b>deduplication,</b> and {{protection}} for NAS filers (NDMP).|$|E
5000|$|MasterData Server: a {{software}} product for data corrections and <b>deduplication</b> ...|$|E
5000|$|RAM <b>deduplication</b> (Page Fusion) for Windows {{guests on}} 64-bit hosts ...|$|E
