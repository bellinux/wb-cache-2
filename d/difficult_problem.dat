6364|7555|Public
5|$|Before {{the local}} protein {{synthesis}} hypothesis gained significant support, there was general {{agreement that the}} protein synthesis underlying L-LTP occurred in the cell body. Further, there was thought that the products of this synthesis were shipped cell-wide in a nonspecific manner. It thus became necessary to explain how protein synthesis could occur in the cell body without compromising LTP's input specificity. The synaptic tagging hypothesis attempts to solve the cell's <b>difficult</b> <b>problem</b> of synthesizing proteins in the cell body but ensuring they only reach synapses that have received LTP-inducing stimuli.|$|E
5|$|N. R. Malkani finds Aurobindo's {{theory of}} {{creation}} to be false, as the theory talks about experiences and visions which are beyond normal human experiences. He says {{the theory is}} an intellectual response to a <b>difficult</b> <b>problem</b> and that Aurobindo uses the trait of unpredictability in theorising and discussing things not based upon truth of existence. Malkani says that awareness is already a reality and suggests {{there would be no}} need to examine the creative activity subjected to awareness.|$|E
5|$|Besides the {{problems}} of direct election, the new Constitution was seen as such a radical break with the old system, by which delegates were elected to the Confederation Congress by state legislatures, that the Convention agreed to retain this method of electing senators to make the constitutional change less radical. The more <b>difficult</b> <b>problem</b> was the issue of apportionment. The Connecticut delegation offered a compromise, whereby the number of representatives for each state in the lower house would be apportioned based on the relative size of the state's population, {{while the number of}} representatives in the upper house would be the same for all of the states, irrespective of size. The large states, fearing a diminution of their influence in the legislature under this plan, opposed this proposal. Unable to reach agreement, the delegates decided to leave this issue for further consideration later during the meeting.|$|E
40|$|Previous {{approaches}} to bidirectional search require exponential space, {{and they are}} either less efficient than unidirectional search for finding optimal solutions, or they cannot even find such solutions for <b>difficult</b> <b>problems.</b> Based on a memory-bounded unidirectional algorithm for trees (SMA*), we developed a graph search extension, and we used it to construct a very efficient memory-bounded bidirectional algorithm. This bidirectional algorithm can be run for <b>difficult</b> <b>problems</b> with bounded memory. In addition, {{it is much more}} efficient than the corresponding unidirectional search algorithm also for finding optimal solutions to <b>difficult</b> <b>problems.</b> In summary, bidirectional search appears to be the best approach to solving <b>difficult</b> <b>problems,</b> and this indicates the extreme usefulness of a paradigm that was neglected for long. Notation s,t l- 1 (4 ra (4 d d’ d(n) hf (4 il$f F$) c...|$|R
2500|$|Among {{the most}} <b>difficult</b> <b>problems</b> in {{knowledge}} representation are: ...|$|R
50|$|The most <b>difficult</b> <b>problems</b> in P are P-complete problems.|$|R
5|$|Computer systems {{make use}} of caches—small and fast {{memories}} located close to the processor which store temporary copies of memory values (nearby in both the physical and logical sense). Parallel computer systems have difficulties with caches that may store the same value {{in more than one}} location, with the possibility of incorrect program execution. These computers require a cache coherency system, which keeps track of cached values and strategically purges them, thus ensuring correct program execution. Bus snooping {{is one of the most}} common methods for keeping track of which values are being accessed (and thus should be purged). Designing large, high-performance cache coherence systems is a very <b>difficult</b> <b>problem</b> in computer architecture. As a result, shared memory computer architectures do not scale as well as distributed memory systems do.|$|E
5|$|The GCD of two numbers a and b is {{the product}} of the prime factors shared by the two numbers, where a same prime factor can be used {{multiple}} times, but only as long as the product of these factors divides both a and b. For example, since 1386 can be factored into 2×3×3×7×11, and 3213 can be factored into 3×3×3×7×17, the greatest common divisor of 1386 and 3213 equals 63=3×3×7, the product of their shared prime factors. If two numbers have no prime factors in common, their greatest common divisor is 1 (obtained here as an instance of the empty product), in other words they are coprime. A key advantage of the Euclidean algorithm is that it can find the GCD efficiently without having to compute the prime factors. Factorization of large integers is believed to be a computationally very <b>difficult</b> <b>problem,</b> and the security of many widely used cryptographic protocols is based upon its infeasibility.|$|E
25|$|Henriques {{argues that}} the most <b>difficult</b> <b>problem</b> in {{psychology}} as a discipline is that while there is incredible diversity offered by different approaches to psychology, {{there is no consensus}} model of what psychology actually is.|$|E
50|$|Attract {{investment}} {{from outside the}} sector that takes new approaches to <b>difficult</b> <b>problems.</b>|$|R
50|$|There {{are also}} more <b>difficult</b> <b>problems</b> that are NP-easy. See NP-equivalent for an example.|$|R
5000|$|O'Farrell, Ursula and Sarah McLoughlin. (2008). Families in Focus: Finding Solutions to <b>Difficult</b> <b>Problems</b> (Veritas Books [...] ) ...|$|R
25|$|Gaussian {{elimination}} {{does not}} generalize in any {{simple way to}} higher order tensors (matrices are array representations of order 2 tensors); even computing the rank of a tensor of order greater than 2 is a <b>difficult</b> <b>problem.</b>|$|E
25|$|Romanization of the Sinitic languages, {{particularly}} Mandarin, {{has proved}} a very <b>difficult</b> <b>problem,</b> although {{the issue is}} further complicated by political considerations. Because of this, many romanization tables contain Chinese characters plus one or more romanizations or Zhuyin.|$|E
25|$|Dwingeloo 1 is {{a highly}} {{obscured}} galaxy, which makes distance determination a <b>difficult</b> <b>problem.</b> The initial estimate, made soon after the discovery {{and based on the}} Tully–Fisher relation, was about 3Mpc. Later, this value was slightly increased to 3.5–4Mpc.|$|E
5000|$|The Evaporating Cloud tool is {{intended}} to similarly [...] "vapourize" [...] <b>difficult</b> <b>problems</b> by resolving an underlying conflict.|$|R
500|$|Algorithms {{originally}} developed by AI researchers {{began to appear}} as parts of larger systems. AI had solved {{a lot of very}} <b>difficult</b> <b>problems</b> ...|$|R
2500|$|... 32 Hamlet – one of {{the most}} <b>difficult</b> <b>problems</b> in the First Folio: {{probably}} typeset from some combination of Q2 and manuscript sources.|$|R
25|$|Reliability of {{high-power}} {{diode laser}} pump bars (used to pump solid-state lasers) remains a <b>difficult</b> <b>problem</b> {{in a variety}} of applications, in spite of these proprietary advances. Indeed, the physics of diode laser failure is still being worked out and research on this subject remains active, if proprietary.|$|E
25|$|PID tuning is a <b>difficult</b> <b>problem,</b> {{even though}} there are only three {{parameters}} and in principle is simple to describe, because it must satisfy complex criteria within the limitations of PID control. There are accordingly various methods for loop tuning, and more sophisticated techniques are the subject of patents; this section describes some traditional manual methods for loop tuning.|$|E
25|$|The {{economics}} {{of global warming}} concerns the economic aspects of global warming; this can inform policies that governments might consider in response. A number of factors make this a <b>difficult</b> <b>problem</b> from both economic and political perspectives: it is a long-term, intergenerational problem; benefits and costs are distributed unequally both within and across countries; and scientific and public opinions may diverge.|$|E
3000|$|From the few {{examples}} presented, {{it is clear}} that a lot of <b>difficult</b> <b>problems</b> have to be solved mostly by proper waste management: [...]...|$|R
50|$|Programmable calculators {{allow the}} user to write and store {{programs}} in the calculator in order to solve <b>difficult</b> <b>problems</b> or automate an elaborate procedure.|$|R
25|$|No {{neural network}} has solved such {{computationally}} <b>difficult</b> <b>problems</b> {{such as the}} n-Queens problem, the travelling salesman problem, or the problem of factoring large integers.|$|R
25|$|By definition, the {{algorithm}} {{used to create}} the CAPTCHA must be made public, {{though it may be}} covered by a patent. This is done to demonstrate that breaking it requires the solution to a <b>difficult</b> <b>problem</b> in the field of artificial intelligence (AI) rather than just the discovery of the (secret) algorithm, which could be obtained through reverse engineering or other means.|$|E
25|$|In {{cellular}} networks, {{the channel}} from the users (or phones) {{to the base}} station(s) {{is known as the}} uplink channel. Conversely, the downlink channel is from he base station(s) to the users. The downlink channel is the most studied with stochastic geometry models while models for the uplink case, which is a more <b>difficult</b> <b>problem,</b> are starting to be developed.|$|E
25|$|Second, {{creating}} lifelike {{simulations of}} human beings is a <b>difficult</b> <b>problem</b> on its own that {{does not need to}} be solved to achieve the basic goals of AI research. Believable human characters may be interesting in a work of art, a game, or a sophisticated user interface, but they are not part of the science of creating intelligent machines, that is, machines that solve problems using intelligence.|$|E
25|$|The Alcubierre drive, however, {{remains a}} {{hypothetical}} concept with seemingly <b>difficult</b> <b>problems,</b> though {{the amount of}} energy required is no longer thought to be unobtainably large.|$|R
40|$|A {{substantial}} amount of economic activity involves problem solving, yet economics has few, if any, formal models to address how agents of limited abilities find good solutions to <b>difficult</b> <b>problems.</b> In this paper, we construct a model of heterogeneous agents of bounded abilities confronting <b>difficult</b> <b>problems</b> and analyze their individual and collective performance. By heterogeneity, we mean differences in how individuals represent problems internally, their perspectives, and in the algorithms they use to generate solutions, their heuristics. With this model, we find that a collection of bounded but diverse agents can locate optimal solutions to very <b>difficult</b> <b>problems.</b> We can also calculate the marginal benefits to adding additional problem solvers. We find that problem solving firms can exhibit arbitrary returns to scale, that the order that problem solvers are applied to a problem can matter, and that the standard story of decreasing returns to scale is unlikely...|$|R
40|$|This paper {{presents}} {{a method of}} producing solutions to <b>difficult</b> <b>problems</b> based on the laws of natural selection. The method, known as the genetic algorithm, is described in detail and applied to the cart pole control problem. The future of genetic algorithms is {{discussed in terms of}} potential commercial application. Key Words/Phrases: genetic algorithms, chromosomes, genes, crossover, mutation, generic genetic algorithm engines, cart pole problem 1. 0 INTRODUCTION Producing solutions to standard business problems (such as developing and maintaining payroll systems) have long been the staple activity of the computer industry. However as more of these systems are prepared, this is leaving more <b>difficult</b> <b>problems</b> to solve using computer implementation. <b>Difficult</b> <b>problems</b> are those which cannot be readily solved using conventional techniques/algorithms. Such problems involve finding solutions which are non-linear (i. e. there may not be a discernible relationship between input and output) [...] ...|$|R
25|$|As the {{oscillation}} {{takes some}} time to set up, and is inherently random at the start, subsequent startups will have different output parameters. Phase is almost never preserved, which makes the magnetron difficult to use in phased array systems. Frequency also drifts from pulse to pulse, a more <b>difficult</b> <b>problem</b> for a wider array of radar systems. Neither of these present a problem for continuous-wave radars, nor for microwave ovens.|$|E
25|$|Tableaux with {{unification}} can {{be proved}} complete: if {{a set of}} formulae is unsatisfiable, it has a tableau-with-unification proof. However, actually finding such a proof may be a <b>difficult</b> <b>problem.</b> Contrarily to the case without unification, applying a substitution can modify the existing part of a tableau; while applying a substitution closes at least a branch, it may make other branches impossible to close (even if the set is unsatisfiable).|$|E
25|$|As noted at the beginning, {{the effects}} of the atomic {{structure}} of materials are disregarded in the relatively simple treatments of field electron emission discussed here. Taking atomic structure properly into account is a very <b>difficult</b> <b>problem,</b> and only limited progress has been made. However, it seems probable that the main influences on the theory of Fowler-Nordheim tunneling will (in effect) be to change the values of P and ν in eq. (15), by amounts that cannot easily be estimated at present.|$|E
40|$|Abstract We {{show the}} {{applicability}} of neural networks for distance estimation in classical search problems. First, we present and evaluate different techniques which are able to sample training data from <b>difficult</b> <b>problems</b> of arbitrary domains. Afterwards, an empirical investigation on good neural network configurations for learning a goal dependent heuristic is performed. Finally, the trained networks are evaluated as heuristics in actual searches and compared to {{state of the art}} techniques. We have observed that for <b>difficult</b> <b>problems</b> the neural networks perform faster searches and generate better plans than other state of the art techniques...|$|R
40|$|In {{this paper}} a new {{evolutionary}} paradigm, called Multi-Expression Programming (MEP), intended for solving computationally <b>difficult</b> <b>problems</b> is proposed. A new encoding method is designed. MEP individuals are linear entities that encode complex computer programs. In this paper MEP {{is used for}} solving some computationally <b>difficult</b> <b>problems</b> like symbolic regression, game strategy discovering, and for generating heuristics. Other exciting applications of MEP are suggested. Some of them are currently under development. MEP is compared with Gene Expression Programming (GEP) by using a well-known test problem. For the considered problems MEP performs better than GEP...|$|R
40|$|The authors {{describe}} and test spai_ 1. 1, a parallel MPI {{implementation of the}} sparse approximate inverse (SPAI) preconditioner. They show that SPAI can be very effective for solving a set of very large and <b>difficult</b> <b>problems</b> on a Cray T 3 E. The results clearly show the value of SPAI (and approximate inverse methods in general) as the viable alternative to ILU-type methods when facing very large and <b>difficult</b> <b>problems.</b> The authors strengthen this conclusion by showing that spai_ 1. 1 also has very good scaling behavior...|$|R
