16|20|Public
40|$|Formal {{methods have}} been {{traditionally}} used to model and verify operating systems. Different methods verify different operating systems properties, such as process management, mutual exclusion and inter-process communication. Moreover, various methods may capture different design errors, such as deadlocks or unspecified receptions. The system kernel supports higher-level system services. Hence, kernel verification {{is essential for the}} proper operation of the system. In addition, providing clear <b>kernel</b> <b>specification</b> improves the interoperability between its various implementations. In this paper, we describe commonly used methods for <b>kernel</b> <b>specification</b> and verification. Some methods provide a mathematical model, and use logic to prove properties of interest. These include PVS and Boyer-Moore logic. Others use a programming language to simulate the system, then apply verification tools to capture system errors. These include the SPIN tool. Distributed operating systems are susceptible [...] ...|$|E
40|$|The ITRON Project is to {{standardize}} real-time operating system and related specifications for embedded systems. We have defined and published {{the series of}} ITRON real-time kernel specifications. Of these, the ITRON real-time <b>kernel</b> <b>specification,</b> which was designed for consumer electronic applications and other small-scale embedded systems, has been implemented for many kind of processors and adopted in numerous end products, making it an industry standard in this field. Based on the achievements, we have broadened the scope of our standardization efforts beyond the kernel specifications to related aspects. This paper briefly outlines the history and {{the current status of}} the ITRON project and introduces its recent results, including the ITRON TCP/IP API specification, an application program interface (API) specification for TCP/IP protocol stacks suitable for embedded systems, and the JTRON 2. 0 specification, an interface definition for hybrid environments of Java Runtime Environment and ITRON-specification kernel. We also describe the current standardization activities in the project, focusing on ITRON 4. 0, the next generation ITRON real-time <b>kernel</b> <b>specification.</b> ...|$|E
40|$|This paper proposes and {{implements}} a consumption-based pricing kernel (stochastic discount factor) {{testing methodology}} {{that focuses on}} the covariance between the pricing kernel and asset squared excess returns. This covariance has an intuitive economic interpretation as a risk-neutral variance risk-premium, i. e. the difference between the risk-neutral return variance and the objective return variance. In the same way that an asset riskpremium puzzle is due to a failure of the pricing kernel to adequately covary with asset excess returns, a riskneutral variance puzzle is due to a failure of the pricing kernel to adequately covary with asset squared excess returns. This paper tests a consumption-based pricing <b>kernel</b> <b>specification</b> that is compatible with habit formation, consumption durability, and constant relative risk-aversion over a range of plausible preference parameter values. The difference between consumption-based and semi-parametric option-based estimates of unconditional risk-neutral S&P 500 return variance is used as a pricing <b>kernel</b> <b>specification</b> test statistic. Evidence is found of a risk-neutral S&P 500 return variance puzzle if constant relative risk-aversion is assumed. The puzzle is resolved when the pricing kernel is allowed to exhibit habit formation. The acceptable habit pricing kernels exhibit higher habit levels, higher utility function concavity, and lower rates of timepreference than estimates in related papers. When the full history of consumption data is used, the preference parameter estimates are more similar to those of related papers...|$|E
40|$|This paper {{examines}} {{the relationship between}} consumption-based and option-based risk-neutral moments, providing a technique to explore consumption-based pricing <b>kernel</b> <b>specifications</b> {{using data from the}} options markets. Estimators for average risk-neutral moments of each type are proposed and implemented. ...|$|R
40|$|This paper {{examines}} {{the relationship between}} consumption-based and option-based risk-neutral moments, providing a technique to explore consumption-based pricing <b>kernel</b> <b>specifications</b> {{using data from the}} options markets. Estimators for average risk-neutral moments of each type are proposed and implemented. Option-based average risk-neutral moments are estimated for S&P 500 returns using S&P 500 futures options data; consumption-based moments are estimated using aggregate consumption data, the timeseries of S&P 500 returns, and pricing kernels characterized by constant relative risk aversion, consumption durability, and habit persistence. Moment comparisons indicate that there is a “risk-neutral standard deviation puzzle. ” All of the pricing <b>kernel</b> <b>specifications,</b> even at very high levels of relative risk aversion, significantly underestimate option-based risk-neutral standard deviation or interquartile range...|$|R
40|$|We study regression-based estimators for beta {{representations}} of dynamic asset pricing models with affine and exponentially affine pricing <b>kernel</b> <b>specifications.</b> These estimators extend static cross-sectional asset pricing estimators to settings where prices of risk vary with observed state variables. We identify {{conditions under which}} four-stage regression-based estimators are efficient and also present alternative, closed-form linearized maximum likelihood (LML) estimators. We provide multi-stage standard errors necessary to conduct inference for asset pricing tests. In empirical applications, we find that time-varying prices of risk are pervasive, thus favoring dynamic cross-sectional asset pricing models over standard unconditional specifications. Asset pricing; Econometric models; Risk; Regression analysis...|$|R
40|$|A novel point process {{framework}} {{to examine the}} links between transaction data across equity markets is proposed. Moving beyond a simple exponential <b>kernel</b> <b>specification,</b> it is shown that the kernel matrix can be estimated by solving a system of integral equations which is uniquely characterised by second order cumulants. The cumulant based estimator is shown to be asymptotically normally distributed and consistent and is shown to perform well in a small simulation study. Applying this method to data from U. S and U. K. equity markets when both are open, reveals that two-way interaction between trades is significant. Moreover, this interaction is characterised by both complex short term dynamics and long memory, which cannot be captured by conventioanl exponential kernels...|$|E
40|$|Some popular {{parametric}} diffusion processes {{have been}} assumed as such underlying diffusion processes. This paper considers an important case where both the drift and volatility {{functions of the}} underlying diffusion process are unknown functions of the underlying process, and then proposes using two novel testing procedures for the parametric specification of both the drift and diffusion functions. The finite-sample properties of the proposed tests are assessed through using data generated from four popular parametric models. In our implementation, we suggest using a simulated critical value for each case {{in addition to the}} use of an asymptotic critical value. Our detailed studies show that there is little size distortion when using a simulated critical value while the proposed tests have some size distortions when using an asymptotic critical value in each case. Continuous-time model, Financial econometrics, Nonparametric <b>kernel,</b> <b>Specification</b> testing,...|$|E
40|$|Abstract ASL+ [SST 92] is a <b>kernel</b> <b>specification</b> {{language}} with higherorder parametrisation for programs and specifications, {{based on a}} dependently typed λ-calculus. ASL+ has an institution-independent semantics, which leaves the underlying programming language and specification logic unspecified. To complete the definition, and in particular, to study the type checking problem for ASL+, the language ASL+ FPC was conceived. It is {{a modified version of}} ASL+ for FPC, and institution based on the paradigmatic programming calculus FPC. The institution FPC is notable for including sharing equations inside signatures, reminiscent of so-called manifest types or translucent sums in type systems for programming language modules [Ler 94,HL 94]. This allows type equalities to be propagated when composing modules. This paper introduces FPC and ASL+ FPC and their type checking systems. 1 Program Development with Institutions A simple setup for program development with institutions [GB 92] is to conside...|$|E
40|$|This paper {{presents}} a method based on formal specifications for building robust real-time microkernels. Temporal logic {{is used to}} specih the functional and temporal properties of real-time kernels {{with respect to their}} main services (e. g., scheduling, time, synchronization, and clock interrupts). As an example of a synchronization mechanism, the specification of the Priority Ceiling Protocol is provided The objective is to veri & kernel properties at runtime in order to improve the internal kernel’s detection mechanisms and complement their weaknesses. The core of this paper is a complete description of the temporal logic formulas corresponding to real-time <b>kernel</b> <b>specifications.</b> The formulas developed in this paper are the basis for the implementation of fault containment wrappers. The combination of COTS microkernels and wrappers leak to the notion of robust microkernels. The provided case study illustrates the approach on top of an instance of the Chorus microkernel. ...|$|R
40|$|The paper {{presents}} {{a case study}} of application of the VDM formal method to specification and verification of a simple real-time <b>kernel.</b> <b>Specifications</b> of selected external services of the kernel are presented. Then the verification methodology is introduced by demonstrating its basic steps in relation to verification of a selected function - a process waiting for a signal on a condition variable. The experience from the study is subsummed in the last section. 1 Introduction Typically, embedded systems are based on a small kernel which provides process and interrupt handling services. This article reports on a case study made using the VDM [3] notation, a mathematical specification language, to specify and verify a kernel which supports the concurrent programming laboratory. Previously the kernel was used to support a telecommunication switching system. The kernel supports the following services: ffl Process Management: creation, start and termination of processes, ffl Time Managemen [...] ...|$|R
40|$|Abstract. We {{recall the}} <b>kernel</b> {{algebraic}} <b>specification</b> language ASL and outline its main {{features in the}} context of the state of research on algebraic specification at the time it was conceived in the early 1980 s. We discuss the most significant new ideas in ASL and the influence they had on subsequent developments in the field and on our own work in particular. ...|$|R
40|$|In this talk, I {{introduce}} some improved {{programs for}} nonparametric smoothing that originally {{were written in}} a very simple manner. These updated ado-files are simple too, but they are more versatile and more “Stata-like” than the original versions. The ado-files include, for density traces, boxdent (boxcar weight function) and dentrace (boxcar and cosine weight functions); for choosing the smoothing parameter in density-frequency estimation, bandw (which permits <b>kernel</b> <b>specification</b> with automatic bandwidth adjustment); for direct and discretized variable bandwidth density estimation, varwiker and varwike 2, respectively; for finding critical bandwidth for a specified number of modes, critiband; and for nonparametric assessment of multimodality, bootsamb (to use {{in conjunction with the}} boot command). In spite of its simplicity, this collection of commands has proved to be very useful in the analysis of biological (and other kinds of) data, saving the analyst considerabe amounts of time and effort. ...|$|E
40|$|This paper {{presents}} and discusses the LOTOS specification of a real-time parallel kernel. The {{purpose of this}} specification exercise has been to evaluate LOTOS with respect to its capabilities to model real-time features with a realistic industrial product. LOTOS was used to produce the formal specification of TRANS-RTXC, which is a real-time parallel kernel developed by Intelligent Systems international. This paper shows that although timing constraints cannot be explicitly represented in LOTOS, the language is suitable for the specification of co-ordination of real-time tasks, which is the main functionality of the real-time kernel. This paper also discusses the validation process of the <b>kernel</b> <b>specification</b> {{and the role of}} tools in this validation process. We believe that our experience (use of structuring techniques, use of validation methods and tools, etc) is valuable for designers who want to apply formal models in their design or analysis tasks...|$|E
40|$|We {{have been}} {{conducting}} {{a research and}} development project, called the TRON Project, for realizing the computer augmented environments. The final goal of the project is to realize highly functionally distributed systems (HFDS) in which every kind of objects around our daily life are augmented with embedded computers, be connected with networks, and cooperate each other to provide better living environments for human beings. One of the key technologies towards HFDS is the realization methods of compact and low-cost distributed computing systems with dependability and real-time property. In this paper, the overview of the TRON Project are introduced and various research and development activities for realizing HFDS, especially the subprojects on a standard realtime <b>kernel</b> <b>specification</b> for small-scale embedded systems called ITRON and on a low-cost real-time LAN called ¯ITRON bus, are described. We also describe the future directions of the project centered on the TRON-concept Computer [...] ...|$|E
40|$|An {{attempt is}} made to apply ideas about {{algebraic}} specification {{in the context of}} a programming language. Standard ML with modules is extended by allowing axioms in module interface specifications and in place of code. The resulting specification language, called Extended ML, is given a semantics based on the primitive specification-building operations of the <b>kernel</b> algebraic <b>specification</b> language ASL. Extended ML provides a framework for the formal development of Programs from specifications by stepwise refinement, which is illustrated by means of a simple example. From its semantic basis Extended ML inherits complete independence from the logical system (institution) used to write specifications. This allows different styles of specification as we! 1 as different programming languages to be accommodated. ...|$|R
40|$|Abstract In this paper, {{we present}} PATUS, a code gener-ation and auto-tuning {{framework}} for stencil computations targeted at multi- and manycore processors, such as mul-ticore CPUs and graphics processing units. PATUS, {{which stands for}} “Parallel Autotuned Stencils, ” generates a com-pute <b>kernel</b> from a <b>specification</b> of the stencil operation and a strategy which describes the parallelization and optimiza-tion to be applied, and leverages the autotuning methodol-ogy to optimize strategy-specific parameters for the given hardware architecture...|$|R
5000|$|The Multiboot Specification {{is an open}} {{standard}} {{describing how}} a boot loader can load an x86 operating system <b>kernel.</b> [...] The <b>specification</b> allows any compliant boot loader implementation to boot any compliant operating system kernel. Thus, it allows different operating systems and boot loaders {{to work together and}} interoperate, without the need for operating system - specific boot loaders. As a result, it also allows easier coexistence of different operating systems on a single computer, which is also known as multi-booting.|$|R
40|$|The {{performance}} of many scientific applications {{depends on a}} small number of key computational kernels which require a level of efficiency rarely satisfied by existing native compilers. We present a new approach to high performance kernel optimization, where a general-purpose transformation engine automates the production of highly efficient library routines. Our framework requires only an annotated <b>kernel</b> <b>specification</b> and can automatically produce optimized implementations based on tuning parameters controlled by a search driver. The transformation engine includes an extensive suite of optimizations which can be easily expanded using a custom transformation description language. We have applied our transformation engine to generate highly tuned code for key linear algebra kernels used in the ATLAS tuning framework. The time required to produce specifications for these kernels is orders of magnitude less than that required to hand-craft kernel implementations, and yet our framework has achieved similar performance to ATLAS’s highly tuned kernels. ...|$|E
40|$|A <b>kernel</b> <b>specification</b> {{language}} called ASL is presented. ASL comprises five fundamental {{but powerful}} specificationbuilding operations {{and has a}} simple semantics. Behavioural abstraction {{with respect to a}} set of observable sorts san be expressed, and (recurstve) parameterisad specifications can be defined using a more powerful and more expressive parameterisation mechanism than usual. A simple notion of implementation permitting vertical and horizontal composition (i. e. it is transitive and monotonic) is adopted and compared with previous more elaborate notions. A In recent years there has {{been a great deal of}} work on developing the algebraic approach to specification of data types and programs. Guttag [Gut 75] and others began by viewing an abstract data type as a class of heterogeneous algebras and showing how such a type can be specified by a signature (a collection of sorts and operators) together with a set of axioms. For quite simple data types (e. g. natural numbers) such an approach ca...|$|E
40|$|A set of {{operations}} for constructing algebraic specifications in an arbitrary logical system is presented. This builds on the framework provided by Goguen and Burstall's work {{on the notion of}} an institution as a formalisation of the concept of a logical system for writing specifications. We show how to introduce free variables into the sentences of an arbitrary institution and how to add quantifiers which bind them. We use this foundation to define a set of primitive operations for building specifications in an arbitrary institution based loosely on those in the ASL <b>kernel</b> <b>specification</b> language, We examine the set {{of operations}} which results when the definitions are instantiated in an institution of first-order logic and compare these with the operations found in existing specification languages. The result of instantiating the operations in an institution of partial first-order logic is also discussed, Much work has been done on algebraic specifications in the past ten years. Althoug...|$|E
40|$|Cyber-Physical Systems need {{to handle}} {{increasingly}} complex tasks, which additionally, may have variable operating conditions over time. Therefore, dynamic resource management {{to adapt the}} system to different needs is required. In this paper, a new bus-based architecture, called ARTICo 3, which by means of Dynamic Partial Reconfiguration, allows the replication of hardware tasks to support module redundancy, multi-thread operation or dual-rail solutions for enhanced side-channel attack protection is presented. A configuration-aware data transaction unit permits data dispatching {{to more than one}} module in parallel, or provide coalesced data dispatching among different units to maximize the advantages of burst transactions. The selection of a given configuration is application independent but context-aware, which may be achieved by the combination of a multi-thread model similar to the CUDA <b>kernel</b> model <b>specification,</b> combined with a dynamic thread/task/kernel scheduler. A multi-kernel application for face recognition is used as an application example to show one scenario of the ARTICo 3 architecture...|$|R
40|$|In {{addition}} to these, several interesting practical lessons are noted in doing the two-stage PLR model estimation. First, the cross validation (CV) {{used in the}} PLR model literature can fail if the mean-independence is ignored. Second, high order kernels can make the CV criterion function ill behaved. Third, product kernels {{work as well as}} spherically symmetric <b>kernels.</b> Fourth, nonparametric <b>specification</b> tests may work poorly due to a sample splitting problem with outliers in the data or due to choosing more than one bandwidth; in this regard, a test suggested by Stute (1997) and Stute et al. (1998) is recommended. Copyright Royal Economic Society, 2003...|$|R
40|$|Abstract of M. Sc. thesis Jo Erskine Hannay; August 1, 1995 The thesis {{deals with}} topics {{in the field}} of formal {{abstract}} specification and verification of pro-grams,particularly within the framework of algebraic methods restricted to equational logic. The discussion views equational specification and the closely related topic of simple term-rewriting as more concrete and nearer to implementation than specification in general. There-fore initial and final models are considered, rather than complete model-classes. Also mostly constructive, “executable” equational specifications are considered giving an inclination to-wards viewing specifications as abstract programs. The thesis is structured around the follow-ing two topics: 1. The method of equational algebraic specification is generalized in a manner allowing a certain mode of modular specification. The generalization has two forms accommodating initial and final algebra semantics respectively. The generalization is modular in the sense that complex specifications may be constructed stepwise from simpler kernel specifica-tions. Each construction step provides the choice of initial or final generalized form. The resulting complex equational specifications differ fromgeneral case predicate logic hierar-chical specifications, {{in the sense that the}} polarization into initial and final semantics gives inhomogeneous specifications; a complex specification cannot in principle be viewed as a non-hierarchical simple specification. A concept of consistency relative to <b>kernel</b> <b>specifications</b> is developed and it is possible to reason about such relative consistency without specific knowledge about the kernel spec-ification. The possibility of analogously constructing complex formal-mechanical proof methods from simpler methods is briefly discussed. 2. A variant of (initial) algebraic specification called indirect specification is developed. Motivation and a foundation for indirect specification is given by syntactical specifier functions; i. e. terms are to be understood identical iff the their values under some given function(with syntactical codomain) are identical. A special and interesting case of syntactical specifier functions are functions giving canonical representatives in some well-defined sense. Besides providing the specification language with additional means of specifying equal-ity,indirect specification expands the class of congruences (over ground terms) decidable by simple term-rewriting. In addition, although the class of initial congruences equation-ally specifiable is identical to the class specifiable by syntactical specifier functions with canonical representatives as codomain, there exist syntactical specifier functions which do not give canonical representatives. (Such a function may for instance give a representa-tive in some other class than in which the argument belongs.) It may therefore be the fact that the class of congruences indirectly specifiable is greater than the class equationally specifiable. Characteristically, indirect specification represents a more operational mode of specification than does usual algebraic specification. Indirect specification introduces further inhomogeneousity to equational algebraic spec-ification. The modularity of the generalized specification strategy discussed in part 1 is used to encapsulate indirect specification in the hierarchical framework. It is then shown that a trivial augmentation of indirect specification can be reduced to standard non-generalized initial or final semantics specification; under certain interesting circumstances. It is also shown that Knuth-Bendix completion of an augmentation of in-direct specification, if successful and under a certain congruency condition, will give an equivalent standard algebraic specification. Viewing equational specifications as abstract programs, this might be seen as a program transformation. (Amore applicable strategy for automatic transformation in this sense is sketched just as an idea without any sort of further proof. The overall structure sees part 1 primarily as giving a framework for the discussion in part 2. 1 The subject of consistency permeates the entire discussion. Consistency is viewed as related to basic ideas of some semantic domain which are expressed as untamperable presuppositions in the act of specifying. For example, the basic idea that a mathematical proposition cannot simultaneously be true and false is presupposed in predicate calculus by predefined interpretations of the symbols true and fal se, and by referring to a set of axioms as inconsistent iff the predicate true = false is deducible from the axioms. The concept of (in) consistency seen always relatively to such presuppositions allows a generalized notion of consistency in hierarchical <b>specification</b> relative to <b>kernel</b> <b>specifications</b> as discussed in part 1. (This also generalizes Guttag’s notion of consistency.) Some simple methods for detecting (generalized) inconsistency and for establishing consistency are briefly presented. The notion of artificial inconsistency is introduced, as inconsistency due to auxiliary functions; i. e. functions helpful or necessary during constructive definition or implementation, but oth-erwise really not belonging to the semantical objects under implementation. Formalisms and results are developed showing that auxiliary functions and hence artificial inconsistency can be hidden from the formal reasoning, still allowing for the full implementatory benefits. This goes beyond the model-theoretical notions of hidden sorts and symbols. The theory developed can probably be used and implemented by modifying existing proof methods (based on the concepts of proof by consistency and inductive completion). Finally an extension of Knuth-Bendix completion is presented as a step towards mechanical generation of constructive proofs in the input theory given to the process. Constructive proofs may give deeper insight into a theory. It also turns out that the extension of Knuth-Bendix completion under certain circumstances can be used in establishing consistency in conjunction with indirect specification (part 2). The extension also turns out to be vital in proving that hiding of auxiliary functions and artificial inconsistency may be implemented in proof methods based on inductive completion...|$|R
40|$|AbstractA {{formalism}} {{for constructing}} and using axiomatic specifications in an arbitrary logical system is presented. This {{builds on the}} framework provided by Goguen and Burstall's work {{on the notion of}} an institution as a formalisation of the concept of a logical system for writing specifications. We show how to introduce free variables into the sentences of an arbitrary institution and how to add quantifiers which bind them. We use this foundation to define a set of primitive operations for building specifications in an arbitrary institution based loosely on those in the ASL <b>kernel</b> <b>specification</b> language. We examine the set of operations which results when the definitions are instantiated in institutions of total and partial first-order logic and compare these with the operations found in existing specification languages. We present proof rules which allow proofs to be conducted in specifications built using the operations we define. Finally, we introduce a simple mechanism for defining and applying parameterised specifications and briefly discuss the program development process...|$|E
40|$|A {{formalism}} {{for constructing}} and using axiomatic specifications in an arbitrary logical system is presented. This {{builds on the}} framework provided by Goguen and Burstall’s work {{on the notion of}} an institution as a formalisation of the concept of a logical system for writing specifications. We show how to introduce free variables into the sentences of an arbitrary institution and how to add quantitiers which bind them. We use this foundation to define a set of primitive operations for building specifications in an arbitrary institution based loosely on those in the ASL <b>kernel</b> <b>specification</b> language. We examine the set of operations which results when the definitions are instantiated in institutions of total and partial tirst-order logic and compare these with the operations found in existing specification languages. We present proof rules which allow proofs to be conducted in specifications built using the operations we define. Finally, we introduce a simple mechanism for defining and applying parameterised specifications and briefly discuss the program development process. 1 1988 Academic Press. Inc. 1...|$|E
40|$|This {{document}} describes our {{achievements in}} work package 4 (<b>kernel</b> <b>specification</b> and verification) of the Robin project towards the verification of selected {{parts of the}} Nova micro-hypervisor. Despite organizational difficulties that were beyond our control (see below) {{we were able to}} finish our task successfully. In this line of work we achieved the following results. 1. A precise formalization of the IA 32 hardware in the interactive theorem prover PVS, see Sections 4. 1 – 4. 6. The formalization faithfully models all the peculiarities of the real hardware that can lead to subtle errors in kernel programming. It includes a novel approach for formally describing memory-mapped devices. Wellbehaved and well-typed memory (which are necessary to reason efficiently about the Nova source code) are established as an invariant and a set of theorems on top of the low-level formalization. Well-behaved and well-typed memory therefore rest only on properties that the Nova hypervisor itself ensures and not on additional assumptions. 2. A formal semantics in PVS of a sufficiently rich subset of C++, see Section 4. 7...|$|E
40|$|Abstract. KVEST – <b>Kernel</b> VErification and <b>Specification</b> Technology – {{is based}} on {{automated}} test generation from formal specifications. The technology was developed under a contract with Nortel Networks and {{is based on}} experience gained in academic research [1]. By 2000 the methodology and the toolset have been applied in 6 industrial projects dealing with the verification of large-scale telecommunication software. The first project, named Kernel Verification project, gave its name to the methodology and the toolset as a whole. The results of this project are presented in the Formal Method Europe Application database [28]. It {{is one of the largest}} formal method applications presented in the database. This paper provides a brief description of the approach, a comparison to related research, and prospects for the future work*...|$|R
40|$|In this paper, {{we propose}} a {{combined}} regression estimator {{by using a}} parametric estimator and a nonparametric estimator of the regression function. The asymptotic distribution of this estimator is obtained for cases where the parametric regression model is correct, incorrect, and approximately correct. These distributional results imply that the combined estimator is superior to the kernel estimator {{in the sense that}} it can never do worse than the kernel estimator in terms of convergence rate and it has the same convergence rate as the parametric estimator in the case where the parametric model is correct. Unlike the parametric estimator, the combined estimator is robust to model misspecification. In addition, we also establish the asymptotic distribution of the estimator of the weight given to the parametric estimator in constructing the combined estimator. This can be used to construct consistent tests for the parametric regression model used to form the combined estimator. asymptotic distribution convex combination <b>kernel</b> estimation model <b>specification</b> testing parametric estimation semiparametric estimation...|$|R
40|$|Operating systems {{provide an}} {{abstract}} {{view of the}} underlying hardware to applications by dening an interface between the two. Traditionally, monolithic <b>kernels</b> hide hardware <b>specifications</b> and other useful information behind abstractions like processes, virtual memory, files and interprocess communication, and provide a fixed interface to the physical resources. This interface, being overly general, can significantly limit the performance, flexibility and functionality of applications. Research in operating systems consisted mainly of adding more functionality to them or devising new methods to improve their performance, {{until a few years}} ago. There is a growing drive to move system services to user domain where applications can extend or customize them to suit their needs. Several new kernel models have been proposed in recent years to meet these goals. They use techniques like moving system services to trusted user-level servers (e. g. microkernels), moving management of resources to untrusted user-level libraries (e. g. exokernels), or downloading code into kernel (e. g SPIN). This paper studies the extensibility features and methodologies of some of the proposed models of operating systems...|$|R
40|$|Kernel {{methods are}} a popular choice in solving {{a number of}} {{problems}} in statistical machine learning. In this thesis, we propose new methods for two important kernel based classification problems: 1) learning from highly unbalanced large-scale datasets and 2) selecting a relevant subset of input features for a given <b>kernel</b> <b>specification.</b> The first problem is known as the rare class problem, which is characterized by a highly skewed or unbalanced class distribution. Unbalanced datasets can introduce significant bias in standard classification methods. In addition, due to the increase of data in recent years, large datasets with millions of observations have become commonplace. We propose an approach to address both the problem of bias and computational complexity in rare class problems by optimizing area under the receiver operating characteristic curve and by using a rare class only kernel representation, respectively. We justify the proposed approach theoretically and computationally. Theoretically, we establish an upper bound on the difference between selecting a hypothesis from a reproducing kernel Hilbert space and a hypothesis space which can be represented using a subset of kernel functions. This bound shows that for a fixed number of kernel functions, it is optimal to first include functions corresponding to rare class samples. We also discuss the connection of a subset kernel representation with the Nystrom method for a general class of regularized loss minimization methods. Computationally, we illustrate that the rare class representation produces statistically equivalent test error results on highly unbalanced datasets compared to using the full kernel representation, but with significantly better time and space complexity. Finally, we extend the method to rare class ordinal ranking, and apply it to a recent public competition problem in health informatics. The second problem studied in the thesis is known as the feature selection problem in literature. Embedding feature selection in kernel classification leads to a non-convex optimization problem. We specify a primal formulation and solve the problem using a second-order trust region algorithm. To improve efficiency, we use the two-block Gauss-Seidel method, breaking the problem into a convex support vector machine subproblem and a non-convex feature selection subproblem. We reduce possibility of saddle point convergence and improve solution quality by sharing an explicit functional margin variable between block iterates. We illustrate how our algorithm improves upon state-of-the-art methods...|$|E
40|$|Formal Semantics is a {{topic of}} major {{importance}} {{in the study of}} programming language design. Action semantics is a recently developed framework for the specification of formal semantics which allows understandable, modular and reusable semantic descriptions of programming languages. Action laws are algebraic properties of primitive actions and action combinators which can be used to prove the existence of semantic equivalence between pairs of constructs, expressions etc. of programming language. This thesis endeavours to show how action semantics can be formalised computationally by reporting on the representation of the kernel of action notation in CAML. CAML is a functional language whose type systems allow the user to define his/her own data structures. It allows the definitions of functions manipulating these data structures with the security provided by strict type verification. The representation of the <b>kernel</b> in the <b>specification</b> language of the Coq development system is also outlined. The Coq system is an implementation of the Calculus of Inductive Constructions and provides goal-directed tactic-driven proof search. The proof engine of the Coq system is then used to prove various action laws...|$|R
40|$|Environmental {{economists have}} {{advocated}} {{the use of}} choice modelling in environmental valuation. Standard approaches employ choice sets including one alternative depicting the status-quo, yet the effects of explicitly accounting for systematic differences in preferences for non status-quo alternatives in the econometric models are not well understood. We explore three different ways of addressing such systematic differences using data from two choice modelling studies designed to value the provision of environmental goods. Preferences for change versus status-quo are explored with standard conditional logit with alternative-specific constant for status-quo, nested logit and a less usual mixed logit error component <b>specification</b> (<b>kernel</b> logit). Our empirical {{results are consistent with}} the hypothesis that alternatives offering changes from status-quo do not share the same preference structure as status-quo alternatives, as found by others in the marketing literature, in the environmental economic literature and in food preference studies. To further explore the empirical consequences of such mis-specification we report on a series of Monte Carlo experiments. Evidence from the experiments indicates that the expected bias in estimates ignoring the status-quo effect is substantial, and—more interestingly—that error component specifications with status-quo alternative specific-constant are efficient even when biased. These findings have significant implications for practitioners and their stance towards the strategies for the econometric analysis of choice modelling data for the purpose of valuation...|$|R
40|$|AbstractAn {{evolutionarily}} conserved subcircuit (kernel) {{dedicated to}} a specific developmental function is found {{at the top of}} the gene regulatory networks (GRNs) hierarchy. Here we comprehensively demonstrate that a pan-deuterostome endoderm <b>specification</b> <b>kernel</b> exists in zebrafish. We analyzed interactions among gata 5, gata 6, otx 2 and prdm 1 a using specific morpholino knockdowns and measured the gene expression profiles by quantitative real-time RT-PCR and in situ hybridization. The mRNA rescue experiment validated the specificity of the morpholino knockdown. We found that the interactions among gata 5, gata 6, otx 2 and prdm 1 a determine the initial specification of the zebrafish endoderm. Although otx 2 can activate both gata 5 and gata 6, and the prdm 1 a/krox homologue also activates some endoderm transcription factors, a feedback loop from Gata to otx 2 and prdm 1 a is missing. Furthermore, we found the positive regulation between gata 5 and gata 6 to further lock-on the mesendoderm specification by the Gata family. Chromatin immunoprecipitation was used to further validate the recruitment of Otx 2 to the gata 5 and gata 6 loci. Functional assays revealed that module B of gata 6 and the basal promoter of gata 5 drive the gene at the mesendoderm, and mutational analysis demonstrated that Otx 2 and Gata 5 / 6 contribute to reporter gene activation. This is the first direct evidence for evolutionarily conserved endoderm specification across echinoderms and vertebrates...|$|R
40|$|Summary. We {{estimate}} hedonic price indices (HPI) for rental {{offices in}} Tokyo {{for the period}} 1985 â€ 1991. We take a partially linear regression (PLR) model, linear in x (year dummies) and nonparametric in z (office quality characteristics), as our main model; the usual linear model is used as well. Since x consists of year dummies, the linearity in x is not a restriction in the PLR model; the only restriction is that of no interaction between x and z. For the PLR model, the HPI are estimated -consistently with a two-stage procedure. For our data, x {{turns out to be}} (almost) mean-independent of z. This implies that least squares estimation (LSE) for models with a misspecified function for z is still consistent. The mean-independence also leads to an efficiency result that, under heteroskedasticity of unknown form, the two-stage PLR model estimator is at least as efficient as any LSE for models specifying (rightly or wrongly) the part for z. In addition to these, several interesting practical lessons are noted in doing the two-stage PLR model estimation. First, the cross validation (CV) used in the PLR model literature can fail if the mean-independence is ignored. Second, high order kernels can make the CV criterion function ill behaved. Third, product kernels work as well as spherically symmetric <b>kernels.</b> Fourth, nonparametric <b>specification</b> tests may work poorly due to a sample splitting problem with outliers in the data or due to choosing more than one bandwidth; in this regard, a test suggested by Stute (1997) and Stute et al. (1998) is recommended...|$|R
40|$|Transcription factors (TFs) play a {{fundamental}} role in cellular regulation by binding to promoter regions of target genes (TGs) {{in order to}} control their gene expression. TF-TG networks are widely used as representations of regulatory mechanisms, e. g. for modeling the cellular response to input signals and perturbations. As the experimental identification of regulatory interactions is time consuming and expensive, one tries to use knowledge from related species when studying an organism of interest. Here, we present ConReg, an interactive web application to store regulatory relations for various species and to investigate their level of conservation in related species. Currently, ConReg contains data for eight model organisms. The regulatory relations stored in publicly available databases cover only a small fraction both of the actual interactions and also of the regulatory relations described in the scientific literature. Therefore, we included regulatory relations extracted from PubMed and PubMedCentral using sophisticated text-mining approaches and from binding site predictions into ConReg. We applied ConReg for the investigation of conserved regulatory motifs in D. melanogaster. From the 471 regulatory relations in REDfly our system was able to identify 66 confirmed conserved regulations in at least one vertebrate model organism (H. sapiens, M. musculus, R. norvegicus, D. rerio). The conserved network consists among others of the well studied motifs for eye-development and the pan-bilaterian <b>kernel</b> for heart <b>specification,</b> which are well-known examples for conserved regulatory relations between different organisms. ConReg is available at [URL] and can be used to analyze and visualize regulatory networks and their conservation among eight model organisms. It also provides direct links to annotations including literature references to potentially conserved regulatory relations...|$|R
