145|305|Public
5000|$|A {{write to}} this {{register}} will reset bits 5 through 7 of SKSTAT which are latches to 1. The latches flag keyboard overrun, Serial data input overrun, and Serial data <b>input</b> <b>frame</b> error.|$|E
50|$|The LDPC code, in contrast, uses many low depth {{constituent}} codes (accumulators) in parallel, each {{of which}} encode {{only a small portion}} of the <b>input</b> <b>frame.</b> The many constituent codes can be viewed as many low depth (2 state) 'convolutional codes' that are connected via the repeat and distribute operations. The repeat and distribute operations perform the function of the interleaver in the turbo code.|$|E
5000|$|Latency: {{measures}} the round-trip time {{taken by a}} test frame to travel through a network device or across the network {{and back to the}} test port. Latency is the time interval that begins when the last bit of the <b>input</b> <b>frame</b> reaches the input port and ends when the first bit of the output frame is seen on the output port. It is the time taken by a bit to go through the network and back. Latency variability can be a problem. With protocols like voice over Internet protocol (VoIP), a variable or long latency can cause degradation in voice quality.|$|E
30|$|There are {{number of}} ways to combine SR {{algorithm}} with JPEG 2000 compression. One way is to apply AWF SR after performing compression on the LR <b>input</b> <b>frames.</b> We also consider performing AWF SR first and then compressing the resulting SR images. When using compression on multiple <b>input</b> <b>frames,</b> we consider both individual- and difference-frame methods as described below.|$|R
30|$|The {{classifier}} uses three input data: the <b>input</b> <b>frames,</b> a foreground mask, and {{the corresponding}} background model frame.|$|R
3000|$|Both MCTI and DCVP {{have the}} same complexity. The only {{difference}} between both techniques is the <b>input</b> <b>frames.</b> For each block match, [...]...|$|R
50|$|QuickTime Animation uses run-length {{encoding}} and conditional replenishment for compression. When encoding, the <b>input</b> <b>frame</b> is scanned pixel-wise in raster-scan {{order and}} processed line-wise. Within a line, pixels are segmented into runs, {{the length of}} which is variable and signaled in the bitstream. For each run, one of three coding modes is used: same color, skip, or PCM. In same color mode, a run of pixels is represented by a single color in a run-length encoding fashion. If pixels with different colors are joined into a run (of a single color) by the encoder, the coding process is lossy, otherwise it is lossless. The lossless mode is used at the 100% quality level. In skip mode, the run of pixels is left unchanged from the previous frame (conditional replenishment). In PCM mode, the color of each pixel is written to the bitstream, without any compression.|$|E
50|$|The output video {{switches}} {{width and}} number of frames from the input - number of frames in the output is {{the width of the}} input, while width of the output is number of frames in the input. For a given resolution, the resulting output video has a fixed height {{and number of}} frames, and variable width (depending on the duration of the input video). The duration of the output video is determined by the width (x-resolution) of the <b>input</b> <b>frame,</b> which form the frames of the output video, and the frame rate (fps) of the output video, which need not be related to the frame rate of the input video. For example, 1920 × 1080 input images (as in 1080p) output at 24 fps yield 1920/24 = 80 seconds, while at 60 fps yields 1920/60 = 32 seconds. The width of the output video is exactly the number of frames in the input video, hence the frame rate times the duration. A single frame input video (a photo) thus yields a single line of output video, while a very long input video yields a very wide output video, though in both cases they last the same time (assuming the same output frame rate).|$|E
30|$|Convert the <b>input</b> <b>frame</b> from an RGB image into an HSV image.|$|E
40|$|Standard {{video frame}} {{interpolation}} methods first estimate optical flow between <b>input</b> <b>frames</b> and then synthesize an intermediate frame guided by motion. Recent approaches merge these two steps {{into a single}} convolution process by convolving <b>input</b> <b>frames</b> with spatially adaptive kernels that account for motion and re-sampling simultaneously. These methods require large kernels to handle large motion, which limits the number of pixels whose kernels can be estimated at once due to the large memory demand. To address this problem, this paper formulates frame interpolation as local separable convolution over <b>input</b> <b>frames</b> using pairs of 1 D kernels. Compared to regular 2 D kernels, the 1 D kernels require significantly fewer parameters to be estimated. Our method develops a deep fully convolutional neural network that takes two <b>input</b> <b>frames</b> and estimates pairs of 1 D kernels for all pixels simultaneously. Since our method is able to estimate kernels and synthesizes the whole video frame at once, it allows for the incorporation of perceptual loss to train the neural network to produce visually pleasing frames. This deep neural network is trained end-to-end using widely available video data without any human annotation. Both qualitative and quantitative experiments show that our method provides a practical solution to high-quality video frame interpolation. Comment: ICCV 2017, [URL]...|$|R
3000|$|... {{represent}} the frame number of a segment, {{the total number}} of segments, and {{the total number of}} <b>input</b> <b>frames,</b> respectively. The core algorithm of Segmental CDP is shown in Algorithm 1.|$|R
30|$|To {{estimate}} and {{compensate the}} motions existing among the <b>input</b> <b>frames</b> modeled by M(t), we adopt the image registration method in frequency-domain [18] {{since it is}} simple and accurate for translational motions. It estimates the horizontal and vertical shifts in spatial domain by computing the phase shift in the frequency domain. Moreover, the frequency-domain approach benefits when the aliasing effect exists in <b>input</b> LR <b>frames.</b>|$|R
30|$|Convert the <b>input</b> <b>frame</b> {{from the}} RGB image into a grey image.|$|E
3000|$|... [...]) {{indicates}} that {{it is likely to}} exist an object in the background model and do not in the <b>input</b> <b>frame,</b> i.e., the background model does not correspond to the reality and the referred blob is a removed one. While a positive value {{indicates that}} the object is likely to be in the <b>input</b> <b>frame</b> and do not in the background model, i.e., an abandoned object.|$|E
3000|$|The oldest {{frame is}} removed by using (22) and (25), {{and the new}} <b>input</b> <b>frame</b> is added by using (24) and (26). Hence, the {{template}} matrix for each new <b>input</b> <b>frame</b> can be computed recursively without matrix inversion by using the results generated by previous input frames. Since both frequency response and intensity of a note evolve slowly in a short time, C [...]...|$|E
3000|$|... <b>input</b> <b>frames.</b> As {{soon as the}} new {{segment is}} constructed, CDP is {{performed}} for the segment and all other previously constructed segments toward the subsequent data, as shown by (II) and (III) in Figure 2.|$|R
40|$|This paper {{describes}} prototyping of a form {{processing system}} employing dot texture for printing <b>input</b> <b>frames</b> of the form. The dot texture is the texture composed of small dots. It eases {{the separation of}} handwritings from the <b>input</b> <b>frames</b> even under monochrome printing/reading environments and makes the system to process the handwritings according to the information embedded in the dot texture of the frames. The embedded information in the form dictates how to process the form so that we call the form "active form" being opposite to the passive form processed by the program stored in a document reader. This method {{can also be used}} to embed other information such as attribute of handwriting and so on. This paper presents the design, prototyping and some preliminary evaluation...|$|R
30|$|The object {{detector}} uses {{a background}} subtraction algorithm that only uses <b>input</b> <b>frames</b> periodically {{at a fixed}} spacing. Only once it is filled a buffer of frames is it able to produce an output, and so until then the tracker, and thus the system, has no output.|$|R
30|$|In this experiment, we {{used the}} {{foreground}} mask produced with the SuBSENSE [45] segmenter. SuBSENSE does not maintain a single background model frame. Instead, it manages a set of samples for each pixel. Then, for each pixel, we extracted a background frame by choosing the sample that best fits each corresponding pixel from <b>input</b> <b>frame</b> and {{used it as a}} running average background model. This procedure was repeated for each <b>input</b> <b>frame.</b>|$|E
3000|$|The {{target set}} (codomain) of both {{features}} is [0, 1]. First, we evaluate each feature at the <b>input</b> <b>frame</b> (F [...]...|$|E
3000|$|... be {{transform}} and {{inverse transform}} respectively. We perform wave atom transform to <b>input</b> <b>frame</b> Y and get coefficient matrix C=F(Y). C [...]...|$|E
5000|$|As with static strip photography, videos can be {{produced}} both in [...] "tableau" [...] format (conventional almost square aspect ratio), or in [...] "strip" [...] format (very wide), and in fact can have exactly the same dimensions as the input video if <b>input</b> <b>frames</b> = <b>input</b> x-resolution (so the [...] array is square in the [...] dimensions); in this case if <b>input</b> and output <b>frame</b> rate are the same, then the input and output videos {{will have the same}} duration as well.|$|R
5000|$|Input for the encoder {{consists}} of <b>input</b> <b>frames</b> each of 24 8-bit symbols (12 16-bit {{samples from the}} A/D converter, 6 each from left and right data (sound) sources). A frame can be represented by [...] where [...] and [...] are bytes from {{the left and right}} channels from the [...] sample of the frame.|$|R
30|$|The DAE is {{fine-tuned}} using reverberant {{speech as}} the input and clean speech as the target. The <b>input</b> <b>frames</b> and the output frames {{for the training}} were adjusted to be time-aligned in the multi-condition training-data generation process. The last portions of reverberant speech utterance files exceeding {{the length of the}} clean speech were trimmed to equalize the lengths of input and output.|$|R
30|$|Henceforth for {{a simpler}} notation, unless {{otherwise}} specified, neighborhood of foreground blobs means the corresponding neighbor {{region in the}} <b>input</b> <b>frame,</b> not in the foreground mask.|$|E
3000|$|... in {{the rest}} of this paper. After finding these ghost maps for an <b>input</b> <b>frame,</b> the overall ghost map for the frame is {{constructed}} as g=g [...]...|$|E
3000|$|... (Bg)). A {{high value}} of <b>input</b> <b>frame</b> {{features}} {{indicates that the}} blob is likely to correspond to a removed object. A low value indicates an abandoned one. The inverse is also true for the background features.|$|E
40|$|Video frame {{interpolation}} typically involves two steps: {{motion estimation}} and pixel synthesis. Such a two-step approach heavily {{depends on the}} quality of motion estimation. This paper presents a robust video frame interpolation method that combines these two steps into a single process. Specifically, our method considers pixel synthesis for the interpolated frame as local convolution over two <b>input</b> <b>frames.</b> The convolution kernel captures both the local motion between the <b>input</b> <b>frames</b> and the coefficients for pixel synthesis. Our method employs a deep fully convolu- tional neural network to estimate a spatially-adaptive con- volution kernel for each pixel. This deep neural network can be directly trained end to end using widely available video data without any difficult-to-obtain ground-truth data like optical flow. Our experiments show that the formula- tion of video interpolation as a single convolution process allows our method to gracefully handle challenges like oc- clusion, blur, and abrupt brightness change and enables high-quality video frame interpolation...|$|R
40|$|This paper {{presents}} {{the use of}} a newly created network structure known as a Self-Delaying Dynamic Network (SDN) to create a high resolution image from a set of time stepped <b>input</b> <b>frames.</b> These SDNs are non-recurrent temporal neural networks which can process time sampled data. SDNs can store input data for a lifecycle and feature dynamic logic based connections between layers. Several low resolution images and one high resolution image of a scene were presented to the SDN during training by a Genetic Algorithm. The SDN was trained to process the <b>input</b> <b>frames</b> in order to recreate the high resolution image. The trained SDN was then used to enhance a number of unseen noisy image sets. The quality of high resolution images produced by the SDN is compared to that of high resolution images generated using Bi-Cubic interpolation. The SDN produced images are superior in several ways to the images produced using Bi-Cubic interpolation...|$|R
30|$|Note {{that such}} a PBFDF scheme is {{computationally}} efficient because each <b>input</b> signal <b>frame’s</b> DFT has to be computed only once.|$|R
40|$|An {{embodiment}} of an analysis filterbank for filtering {{a plurality of}} time domain input frames, wherein an <b>input</b> <b>frame</b> comprises a number of ordered input samples, comprises a windower configured to generating a plurality of windowed frames, wherein a windowed frame comprises a plurality of windowed samples, wherein the windower is configured to process the plurality of input frames in an overlapping manner using a sample advance value, wherein the sample advance value {{is less than the}} number of ordered input samples of an <b>input</b> <b>frame</b> divided by two, and a time/frequency converter configured to providing an output frame comprising a number of output values, wherein an output frame is a spectral representation of a windowed frame...|$|E
3000|$|... (i) {{denotes the}} {{corresponding}} pixel in the ideal <b>input</b> <b>frame.</b> The noise {{is represented by}} n_k(i) ∼N (0, σ _n^ 2), which {{is assumed to be}} samples of a zero-mean independent and identically distributed Gaussian random variable, with variance σ _n^ 2.|$|E
3000|$|As output, n Temp_filters {{matrices}} of {{the same}} dimensions as each <b>input</b> <b>frame</b> are generated {{as a result of}} the convolution of each frame with the corresponding convolution filter. These matrices will be the input for the second stage (spatial filtering.) [...]...|$|E
30|$|The {{usage of}} {{blocking}} and double-buffering is required. This involves {{the allocation of}} the current block of each frame to be processed and the next block which is being transferred through DMA while CPU computation is in progress. This technique effectively hides memory latencies, improving the overall throughput. In our case, we have mapped the temporal buffers that accommodate blocks of the <b>input</b> <b>frames</b> to the on-chip MSMC memory, {{in order to improve}} memory throughput in the computation stage.|$|R
40|$|A {{robust and}} {{efficient}} anomaly detection technique is proposed, capable {{of dealing with}} crowded scenes where traditional tracking based approaches tend to fail. Initial foreground segmentation of the <b>input</b> <b>frames</b> confines the analysis to foreground objects and effectively ignores irrelevant background dynamics. <b>Input</b> <b>frames</b> are split into nonoverlapping cells, followed by extracting features based on motion, size and texture from each cell. Each feature type is independently analysed {{for the presence of}} an anomaly. Unlike most methods, a refined estimate of object motion is achieved by computing the optical flow of only the foreground pixels. The motion and size features are modelled by an approximated version of kernel density estimation, which is computationally efficient even for large training datasets. Texture features are modelled by an adaptively grown codebook, with the number of entries in the codebook selected in an online fashion. Experiments on the recently published UCSD Anomaly Detection dataset show that the proposed method obtains considerably better results than three recent approaches: MPPCA, social force, and mixture of dynamic textures (MDT). The proposed method is also several orders of magnitude faster than MDT, the next best performing method. 1...|$|R
40|$|Large display environments {{allow to}} support {{collective}} visualization of Virtual Reality applications. As interaction in those environments can be troublesome, we considered {{the integration of}} gestural information into corresponding interaction techniques. To get rid of trackers and {{of some of the}} typical devices weaknesses, we investigated the use of video tracking. The realtime system we proposed combines a full use of graphics hardware and a parametric 3 D model. 2 D images are synthesized and compared to <b>input</b> <b>frames.</b> The latest improvement to our system lies in the collaboration between two traditionally distinct approaches. Indeed, model-based and image-based algorithms are in general concurrent. However, image-based preprocessing of <b>input</b> video <b>frames</b> enables us to bypass some of our initial limitations. Image moments permit to speed up hand position extraction, while image differencing cancels the minimization of some degrees of freedom...|$|R
