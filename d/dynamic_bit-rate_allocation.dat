0|30|Public
40|$|International audienceThis paper {{questions}} {{the existence of}} factors influencing {{the quality of the}} synthesized views, in the context of multi-view video plus depth coding (MVC). The issue of <b>bit-rate</b> <b>allocation</b> between texture and depth data in MVC is still open, despite the many efforts already raised for the development of optimization techniques. The originality of this study lies in the investigation of direct relationships between the best <b>bit-rate</b> <b>allocation,</b> in terms of objective quality of synthesized views, and the sequence characteristics (entropy of depth maps, depth complexity and camera baseline distance, background/foreground contrast areas). The results confirm our assumptions regarding the impact of the sequence features on the <b>bit-rate</b> <b>allocation.</b> The results and the limitations of the study are also discussed...|$|R
40|$|This paper {{questions}} {{the existence of}} factors influencing {{the quality of the}} synthesized views, in the context of multi-view video plus depth coding (MVC). The issue of <b>bit-rate</b> <b>allocation</b> between texture and depth data in MVC is still open, despite the many efforts already raised for the development of optimization techniques. The originality of this study lies in the investigation of direct relationships between the best <b>bit-rate</b> <b>allocation,</b> in terms of objective quality of synthesized views, and the sequence characteristics (entropy of depth maps, depth complexity and camera baseline distance, background/foreground contrast areas). The results confirm our assumptions regarding the impact of the sequence features on the <b>bit-rate</b> <b>allocation.</b> The results and the limitations of the study are also discussed. Index Terms — 3 DTV, MVD, 3 D video, MVC, quality assessment 1...|$|R
40|$|The {{efficient}} {{compression of}} multi-view-video-plus-depth (MVD) data raises the <b>bit-rate</b> <b>allocation</b> {{issue for the}} com-pression of texture and depth data. This question has not been solved yet because not all surveys reckon on a shared frame-work. This paper studies the impact of <b>bit-rate</b> <b>allocation</b> for texture and depth data relying {{on the quality of}} an interme-diate synthesized view. The results show that depending on the acquisition configuration, the synthesized views require a different ratio between the depth and texture bit-rate: be-tween 40 % and 60 % of the total bit-rate should be allocated to depth...|$|R
40|$|With the {{advancement}} of digital video technology in recent years, {{there has been an}} enormous surge in the amount of video data sent across networks. In many cases, a transmission link is shared by more than one video stream. Applications where multiple compressed video streams are transmitted simultaneously through a shared channel include direct broadcast satellite (DBS), cable TV, video- on-demand services, disaster relief response, and video surveillance. Some commercial applications are YouTube and instant video streaming by content providers, such as Netflix, where multiple streams are transmitted simultaneously, and in many cases, these streams share a common transmission channel. Recently, in cognitive radio technology, the secondary or unlicensed users share a pool of bandwidth that is temporarily going unused by the primary or licensed users. In such cases, {{it has been shown that}} joint <b>bit-rate</b> <b>allocation</b> schemes for multiple streams can perform better than an equal <b>bit-rate</b> <b>allocation.</b> In this dissertation, we consider the problem of <b>bit-rate</b> <b>allocation</b> for multiple video users sharing a common transmission channel. We consider two separate objectives for <b>bit-rate</b> <b>allocation</b> among multiple video users: (a) improving the video quality averaged across all the users, and (b) improving the video quality of each individual user, compared to the <b>bit-rate</b> <b>allocation</b> for the users when acting independently. We use dual-frame video with high-quality Long-Term Reference (LTR) frames, and propose multiplexing methods to reduce the sum of Mean Squared Error (MSE) for all the users. We make several improvements to dual-frame video coding by selecting the location and quality of LTR frames. An adaptive buffer- constrained rate-control algorithm is devised to accommodate the extra bits of the high-quality LTR frames. Multiplexing of video streams was studied under the constraint of a video encoder delay buffer. The high- quality LTR frames are offset in time among different video streams. This provides the benefit of dual-frame video coding with high-quality LTR frames while still fitting under the constraint of an output delay buffer. The multiplexing methods show considerable improvement over conventional rate control when the video streams are encoded independently, and over multiplexing methods proposed previously in the literature. While the average video quality is improved for multiple video users, such methods often rely on identifying the relative complexity of the video streams. In such methods, not all the videos benefit from the multiplexing process. Typically, the quality of high motion videos is improved at the expense of a reduction in the quality of low motion videos. We use a competitive equilibrium <b>allocation</b> of <b>bit-rate</b> to improve the quality of each individual video stream by finding trades between videos across time. A central controller collects rate-distortion information from each video user and makes a joint <b>bit-rate</b> <b>allocation</b> decision. The proposed method uses information about not only the differing complexity of the different video streams at a given instant of time, but also the differing complexity of each stream over time. Using the competitive equilibrium <b>bit-rate</b> <b>allocation</b> approach for multiple video streams, we show that all the video streams perform better or at least as well as with individual encoding. The centralized <b>bit-rate</b> <b>allocation</b> methods share the video characteristics and involve high computational complexity. In our pricing-based method, we present an informationally decentralized <b>bit-rate</b> <b>allocation</b> for multiple users where a user only needs to inform his demand to an allocator. Each user separately calculates his bit-rate demand based on his video complexity and bit- rate price, where the bit-rate price is announced by the allocator. The allocator adjusts the bit-rate price based o...|$|R
40|$|International audienceThe {{efficient}} {{compression of}} multi-view-video-plus-depth (MVD) data raises the <b>bit-rate</b> <b>allocation</b> {{issue for the}} compression of texture and depth data. This question has not been solved yet because not all surveys reckon on a shared framework. This paper studies the impact of <b>bit-rate</b> <b>allocation</b> for texture and depth data relying {{on the quality of}} an intermediate synthesized view. The results show that depending on the acquisition configuration, the synthesized views require a different ratio between the depth and texture bit-rate: between 40 % and 60 % of the total bit-rate should be allocated to depth...|$|R
40|$|Due to the {{scarcity}} of wireless resources, efficient resource allocation {{is essential to the}} success of cellular systems. With the proliferation of bandwidth-hungry multimedia applications with diverse traffic characteristics and quality of service requirements, the resource management is becoming particularly challenging. In this thesis, we address some of the key link-layer resource allocation mechanisms that affect the performance of video streaming in cellular systems: <b>bit-rate</b> <b>allocation,</b> opportunistic scheduling, and statistical multiplexing. The <b>bit-rate</b> <b>allocation</b> problem involves the distortion-optimal assignment of source, channel, and pilot data rates under link capacity constraints. We derive an analytical model that captures the video distortion as a function of these data rates and, based on it, we study various <b>bit-rate</b> <b>allocation</b> strategies. The opportunistic scheduling problem addresses the throughput-optimal assignment of time-slots among users with diverse channel conditions under certain fairness constraints. We focus on two aspects of the opportunistic scheduling: the performance of delay-constrained streaming applications and possible extensions of the opportunistic concepts to multicast scenarios. Finally, the statistical multiplexing is a resource-efficient method for smoothing out the extreme burstiness of video streams. We study possible statistical multiplexing gains of H. 264 video streams in the context of E-MBMS architecture. QC 2010112...|$|R
50|$|Rate-distortion theory {{gives an}} {{analytical}} expression {{for how much}} compression can be achieved using lossy compression methods. Many of the existing audio, speech, image, and video compression techniques have transforms, quantization, and <b>bit-rate</b> <b>allocation</b> procedures that capitalize on the general shape of rate-distortion functions.|$|R
40|$|One of {{the central}} {{problems}} in video transmission over lossy channels is the choice of source and channel coding rates to allocate the available transmission rate optimally. In this paper, we present a structural distortion model for video streaming over time-varying fading channels. Based on this model we study the average video distortion for various <b>bit-rate</b> <b>allocation</b> strategies and channel conditions. We argue that sensitivity to channel variations {{should be one of}} the selection criteria when choosing an optimal <b>bit-rate</b> <b>allocation.</b> We demonstrate that a simple bitrate allocation strategy, which aims to minimize current distortion, is not necessarily optimal in terms of time-average distortion. System robustness is crucial for the streaming performance when frequent allocation updates are not feasible. It is achieved at the expense of higher source distortion in the encoder...|$|R
40|$|In motion-compensated wavelet based video coders (MCWT), it {{is known}} that a precise motion {{estimation}} is necessary to minimize the wavelet coefficients energy. However, a motion vectors field of high precision is expensive in binary resources compared to the wavelet subbands. In order to reduce the quantity of bits required to represent these vectors, we proposed in a previous work [1] to quantize the motion vectors using a scalable and open-loop lossy coder, while controlling the rate-distortion trade-off. Obviously, this lossy motion coding has an impact on the decoded sequence. In this paper, we propose to evaluate this impact by establishing a theoretical distortion model of motion coding error, including also the subbands quantization noise. This model will allow to realize an optimal modelbased <b>bit-rate</b> <b>allocation</b> between wavelet coefficients and motion information. Experimental validation of the model gives interesting results. Index Terms — Video coding, temporal wavelet transform, motion vectors coding, coding error, distortion modeling, <b>bit-rate</b> <b>allocation</b> 1...|$|R
40|$|International audienceMulti-view video plus depth (MVD) data offer a {{reliable}} representation of three-dimensional (3 D) scenes for 3 D video applications. This {{is a huge}} amount of data whose compression is an important challenge for researchers at the current time. Consisting of texture and depth video sequences, the question of the relationship between these two types of data regarding <b>bit-rate</b> <b>allocation</b> often raises. This paper questions the required ratio between texture and depth when encoding MVD data. In particular, the paper investigates the elements impacting on the best bit-rate ratio between depth and color: total bit-rate budget, input data features, encoding strategy, and assessed view...|$|R
40|$|In {{order to}} support {{increasing}} traffic loads, mobile operators need cost-effective solutions to improve the spectral efficiency of their cellular networks, or to off-load them by diverting some of the load to other networks. Advances in the radio resource management may to some extent {{reduce the need for}} costly new deployments. The resource management should not only focus on spectrum efficiency—it should try to meet the service requirements of applications that are expected to contribute large data volumes, such as video streaming. Many of those applications are multicast/broadcast in nature (e. g., mobile TV, data podcasting). Our focus in this thesis is on resource allocation mechanisms that exploit the mobility of users. The mobility induces channel quality fluctuations and creates intermittent connectivity, which both can be used to improve the resource efficiency of wireless multimedia systems. The thesis concentrates on two areas: link-layer resource allocation for video streaming in cellular networks and mobility-assisted content distribution in hybrid cellular/ad-hoc networks. In the area of wireless video streaming, we study <b>bit-rate</b> <b>allocation,</b> statistical multiplexing, and channel-aware scheduling. The <b>bit-rate</b> <b>allocation</b> should provide a distortion-optimal assignment of source, channel, and pilot bit-rates under link capacity constraints. We derive an analytical model that captures the video distortion {{as a function of the}} bit-rates and, based on it, we study various <b>bit-rate</b> <b>allocation</b> strategies and their robustness to varying radio conditions. The statistical multiplexing can be used to smooth out the burstiness of video streams and avoid over-provisioning of transport channels. We study the statistical multiplexing gains of H. 264 video streams, both in terms of bit-rate requirements and video quality. When multiple flows are multiplexed on a shared transport channel, multi-user scheduling becomes crucial for the performance. Channel-aware scheduling exploits fluctuations in radio conditions to optimize the assignment of channel resources. We study the impact of channel-aware scheduling on the performance of delay-sensitive applications and possible extensions of channel-aware schemes to multicast scenarios. In the area of mobility-assisted content distribution, we study the resource efficiency of mobility-assisted podcasting and we propose an analytical model for pedestrian content distribution. The mobility-assisted podcasting exploits random encounters of mobile terminals equipped with short range radios to forward the podcast episodes, thereby relieving the strain on cellular networks. We provide results on the achievable spectrum and energy savings of such scheme. Finally, we introduce the “street model”, the first building block in a conceived library of analytical models that would be used to study the performance of pedestrian content distribution in some common case scenarios of urban mobility. Based on the “street model”, we study how various system parameters and node mobility affect the efficiency of content distribution in a grid of streets that represents a part of Stockholm’s downtown area. QC 2010061...|$|R
40|$|Abstract—Real-time video {{applications}} require tight bounds on end-to-end delay. Hierarchical {{bidirectional prediction}} requires buffering frames in the encoder input buffer, thereby contributing to encoder input delay. Long-term frame prediction with pulsed quality requires buffering at the encoder output, increasing the output buffer delay. Both hierarchical B-pictures and pulsed-quality coders involve uneven <b>bit-rate</b> <b>allocation.</b> Both the encoder and decoder buffering requirements {{depend on the}} rate allocation. We derive an efficient rate allocation for hierarchical B-pictures using the power spectral density of a wide-sense stationary process. In addition, we discuss important aspects of hierarchical predictive coding, such as {{the effect of the}} temporal prediction distance and delay tradeoffs for prediction branch truncation. Finally, we investigate experimentally the tradeoff between delay and compression efficiency. Index Terms—Bidirectional prediction, end-to-end delay, H. 264 /AVC, hierarchical B-pictures, motion-compensated tempora...|$|R
40|$|A precise motion {{estimation}} is necessary, in motion-compensated wavelet based video coders (MCWT), {{in order to}} minimize the wavelet coefficients energy. Nevertheless, a motion vectors field of high precision is expensive in binary resources compared to wavelet subbands and it is thus necessary to optimize the rate-distortion trade-off between motion information and wavelet coefficients. To this end, we have proposed in previous works to quantize the mo-tion vectors using a scalable and open-loop lossy coder [1] and, to evaluate the impact of this lossy motion coding on the decoded sequence, we have established a theoretical distortion model of the motion coding error [2]. We present in this paper an approach to realize an optimal model-based <b>bit-rate</b> <b>allocation</b> between motion and wavelet subbands. This method is based on the total distortion model of coding error on several decomposition levels, including both motion information and subbands quantization noise. Experi-mental validations are satisfactory. 1...|$|R
40|$|In this paper, {{we propose}} a {{practical}} coding approach {{for the problem}} of distributed compression of multi-view images. Our coding technique {{is based on a}} tree structured compression algorithm that guarantees an optimal rate-distortion behaviour for piecewise polynomial signals. We model the different views using a piecewise polynomial function whose singularity positions are shifted from one view to the others according to the constraints imposed by the structure of the plenoptic function [1]. We show that, starting from the optimal tree decompositions of the different views, only partial information from each tree is necessary at the decoder in order to reconstruct all the different approximations. We first present our approach in the more intuitive 1 D case and show that it can be used with arbitrary <b>bit-rate</b> <b>allocation.</b> Then, we propose a construction for the case of N different views that satisfies a certain bit conservation principle. Finally, we show how our approach can be extended to the 2 D case. 1...|$|R
40|$|When video coders {{communicate}} at very-low bit-rate, it {{is often}} difficult for them to preserve an acceptable global quality of the images. A selection of the key regions according to the semantic relevance may therefore be useful for improving the quality, as it would permit an adaptive bit allocation, which is important for good subjective quality at (very-) low bit-rate: the essential features could be extracted and coded with a good quality, while the remaining portions of the image would be coarsely transmitted. This paper describes tools to perform classification of regions according to the subjective priority, and a generic algorithm to optimally share out the bit-rate. Among others, an important contribution of this paper is the use of Fuzzy Logic in order to model human subjective knowledge about the attribution of priorities to picture regions. An original <b>bit-rate</b> <b>allocation</b> method is also proposed as a solution to the optimization problem posed by the rate-distortion theory. The paper concludes presenting some preliminary results. (C) 1997 Elsevier Science B. V...|$|R
40|$|This paper {{addresses}} {{the problem of}} rate distortion analysis {{in the context of}} multi-view image coding, where images are predicted via disparity compensation based on depth map. We first present an analytical model for the variance of the residual error in a predicted frame when the prediction is done {{with the help of a}} compressed depth map. This residual variance model presents a convenient expression that separates the different error origins (reference frame quantization, depth map coding, and motion activity). We then validate the novel analytical model by testing separately its different underlying hypotheses. Finally, we illustrate an application of our analytical model in a simple bit allocation problem where the objective is to determine the optimal distribution of a global bit budget among reference frame, depth map and disparity-compensated frame. We observe that the optimal allocation given by the analytical model corresponds in practice to the best rate distribution for high bitrate, which confirms the potential of the proposed model in the design of rate-controlled multi-view coding algorithms. Index Terms — Video-plus-depth, motion vectors, <b>bit-rate</b> <b>allocation</b> 1...|$|R
40|$|Abstract—Opportunistic routing (OR) schemes, such as ExOR, {{have been}} shown to provide {{significant}} throughput gains over traditional best-path routing schemes for wireless networks. Though the performance of OR schemes depend on the bitrate, they currently use a fixed rate for transmitting packets. While several schemes have been proposed for selecting bit-rate for unicast transmission to a single receiver, none of them are suitable for broadcast transmission to multiple receivers under OR. This paper attempts to maximize the benefits of OR with <b>dynamic</b> <b>bit-rate</b> selection. We first define a new metric, Expected Anypath Communication Time (ExACT), that captures the time to deliver a packet to destination with a given rate at each hop under OR. We then propose Bit-rate Selection for Opportunistic Routing (BitSOR) algorithm that minimizes ExACT for each pair of nodes in the network. We evaluate the performance of BitSOR using MIT Roofnet trace and demonstrate significant potential improvement with dynamic rate over OR with the best fixed rate. I...|$|R
40|$|Abstract — This paper proposes new {{scheme for}} {{efficient}} rate allocation {{in conjunction with}} reducing peak-to-average power ratio (PAPR) in orthogonal frequency-division multiplexing (OFDM) system. Modification of the set partitioning in hierarchical trees (SPIHT) image coder is proposed to generate four different groups of bit-stream relative to its significances. The significant bits, the sign bits, the set bits and the refinement bits are transmitted in four different groups. The proposed method for reducing the PAPR utilizes twice the unequal error protection (UEP) using the Read-Solomon codes (RS) in conjunction with <b>bit-rate</b> <b>allocation.</b> The output bit-stream from the source code (SPIHT) will be started by the most significant types of bits (first group of bits). The optimal unequal error protection (UEP) of the four groups is proposed. As a result, the proposed structure gives a significant improvement in bit error rate (BER) performance. Performed computer simulations {{have shown that the}} proposed scheme outperform the performance of most of the recent PAPR reduction techniques in most cases. Moreover, the simulation results indicate that the proposed scheme provides significantly better PSNR performance in comparison to well-known robust coding schemes. Index Term — SPIHT coding, unequal error protectio...|$|R
40|$|An {{efficient}} {{bit allocation}} algorithm {{based on a}} novel view synthesis distortion model is proposed for the rate-distortion optimized coding of multiview video plus depth sequences in this paper. We decompose an input frame into nonedge blocks and edge blocks. For each nonedge block, we linearly approximate its texture and disparity values, and derive a view synthesis distortion model, which quantifies the impacts of the texture and depth distortions on the qualities of synthesized virtual views. On the other hand, for each edge block, we use its texture and disparity gradients for the distortion model. In addition, we formulate a <b>bit-rate</b> <b>allocation</b> problem {{in terms of the}} quantization parameters for texture and depth data. By solving the problem, we can optimally divide a limited bit budget between the texture and depth data, in order to maximize the qualities of synthesized virtual views, {{as well as those of}} encoded real views. Experimental results demonstrate that the proposed algorithm yields the average PSNR gains of 1. 98 and 2. 04 dB in two-view and three-view scenarios, respectively, as compared with a benchmark conventional algorithm. close 0...|$|R
40|$|There is interest, {{supported}} by successful field trials, {{in the use}} of satellite communications at the Ka band (30 / 20 GHz) and beyond to meet emerging demand for broadband interactive multimedia services. The key advantages of operation at Ka band are availability of bandwidth and favorable implications for terminal size, cost and mobility. We study two problems related to bandwidth management of the uplink in a multibeam, CDMAbased, GEO satellite. Our focus is on the delivery of data services with rigid constraints on bit-error rate and elastic constraints on data rate. The first of the two problems concerns the design of the coverage areas of the satellite beams. We were interested specifically in the adaptation of beam shape to inhomogeneity in the geographic distribution of the user population, and in the impact of beam shaping on the set of transmission rates that are compatible with prescribed constraints on transmission powers and signal-to-interference ratios. Assuming that the spatial distribution of users is known, we construct an algorithm which computes beam coverage regions to equilibrate the per-beam user populations. The impact on the set of feasible <b>bit-rate</b> <b>allocations</b> i...|$|R
40|$|Abstract-This paper proposes new {{scheme for}} {{efficient}} rate allocation {{in conjunction with}} reducing peak-to-average power ratio (PAPR) in orthogonal frequency-division multiplexing (OFDM) system. Modification of the set partitioning in hierarchical trees (SPIHT) image coder is proposed to generate four different groups of bit-stream relative to its significances. The significant bits, the sign bits, the set bits and the refinement bits are transmitted in four different groups. The proposed method for reducing the PAPR utilizes twice the unequal error protection (UEP) using the Read-Solomon codes (RS) in conjunction with <b>bit-rate</b> <b>allocation.</b> The output bit-stream from the source code (SPIHT) will be started by the most significant types of bits (first group of bits). The optimal unequal error protection (UEP), based on fuzzy linear optimization technique, of the four groups is proposed. As a result, the proposed structure provides a significant improvement in bit error rate (BER) performance. Performed computer simulations {{have shown that the}} proposed scheme outperform the performance of most of the recent PAPR reduction techniques in most cases. Moreover, the simulation results indicate that the proposed scheme provides significantly better PSNR performance in comparison with the well-known robust transmission schemes. Keywords- SPIHT coding, unequal error protection (UEP) rate allocation, RS codes, OFDM, PAPR I...|$|R
40|$|This paper proposes new {{scheme for}} {{efficient}} rate allocation {{in conjunction with}} reducing peak-to-average power ratio (PAPR) in orthogonal frequency-division multiplexing (OFDM) systems. Modification of the set partitioning in hierarchical trees (SPIHT) image coder is proposed to generate four different groups of bit-stream relative to its significances. The significant bits, the sign bits, the set bits and the refinement bits are transmitted in four different groups. The proposed method for reducing the PAPR utilizes twice the unequal error protection (UEP), using the Read-Solomon codes (RS), in conjunction with <b>bit-rate</b> <b>allocation</b> and selective interleaving to provide minimum PAPR. The output bit-stream from the source code (SPIHT) will be started by the most significant types of bits (first group of bits). The optimal unequal error protection (UEP) of the four groups is proposed based on the channel destortion. The proposed structure provides significant improvement in bit error rate (BER) performance. Performed computer simulations {{have shown that the}} proposed scheme outperform the performance of most of the recent PAPR reduction techniques in most cases. Moreover, the simulation results indicate that the proposed scheme provides significantly better PSNR performance in comparison to well-known robust coding schemes. Comment: Submitted to Journal of Telecommunications, see [URL]...|$|R
40|$|A {{bit rate}} {{allocation}} (BRA) strategy {{is needed to}} optimally compress three-dimensional (3 -D) data on a per-slice basis, treating it {{as a collection of}} two-dimensional (2 -D) slices/components. This approach is compatible with the framework of JPEG 2000 Part 2 which includes the option of pre-processing the slices with a decorrelation transform in the cross-component direction so that slices of transform coefficients are compressed. In this paper, we illustrate the impact of a recently developed inter-slice rate-distortion optimal <b>bit-rate</b> <b>allocation</b> approach that is applicable to this compression system. The approach exploits the MSE optimality of all JPEG 2000 bit streams for all slices when each is produced in the quality progressive mode. Each bit stream can be used to produce a rate-distortion curve (RDC) for each slice that is MSE optimal at each bit rate of interest. The inter-slice allocation approach uses all RDCs for all slices to optimally select an overall optimal set of bit rates for all the slices using a constrained optimization procedure. The optimization is conceptually similar to Post-Compression Rate-Distortion optimization that is used within JPEG 2000 to optimize bit rates allocated to codeblocks. Results are presented for two types of data sets: a 3 -D computed tomography (CT) medical image, and a 3 -D metereological data set derived from a particular modeling program. For comparison purposes, compression results are also illustrated for the traditional log-variance approach and for a uniform allocatio...|$|R
40|$|For the 21 st century, the Internet {{has matured}} into {{the de facto}} data {{transport}} platform that analog media such as videotape, film, and broadcast will be supplanted by a digital media infrastructure. This digital infrastructure will allow data to be transferred between any two computing machines on the planet in a heterogeneous network environment. In the case where data is transmitted from the SONET network to PSTN network, <b>dynamic</b> <b>bit-rate</b> adaptation/reduction at the gateways is then required due to the transmission media has a lower capacity than the capacity required by the bitstream. There are many different approaches to this problem of bit rate conversion. A robust scheme is to implement a transcoder module to perform dynamic adjustments of bit rate of the coded video bitstream to the desired transmission rate. The emphasis in this thesis is {{on the study of}} the transcoder's Motion Compensation module of MPEG- 4 compressed video stream in the DCT domain on both algorithmic and implementation level. For the algorithmic level of Motion Compensation in DCT-domain (MC-DCT) design, the 3 - 2 - 1 partial information scheme is integrated into the DCT Coefficient Translation and Truncation Transformation Matrix (DCTTTM) -based algorithm with two bit precision on the element of DCT-Constant Matrix (DCT-CM) to process the MC-DCT operation. For the VHDL hardware-level design, data-dependent signal processing has been applied to the MC-DCT module to reduce power consumption via various manoeuvres such as zero bypassing, custom handling of internal bandwidth, the implementation of multiplication-free module, the 3 - 2 Wallace tree propagation-delay-free addition map and the logic-based addition module. Keywords. MPEG- 4, Motion Compensation, transcoder, drift error correction, discrete cosine transform (DCT) domain, requantization, data-dependent logic, low powe...|$|R
40|$|There is interest, {{supported}} by successful field trials, {{in the use}} of satellite communications at the Ka band (30 / 20 GHz) and beyond to meet emerging demand for broadband interactive multimedia services. The key advantages of operation at Ka band are availability of bandwidth and favorable implications for terminal size, cost and mobility. We study two problems related to bandwidth management of the uplink in a multibeam, CDMA-based, GEO satellite. Our focus is on the delivery of data services with rigid constraints on bit-error rate and elastic constraints on data rate. The first of the two problems concerns the design of the coverage areas of the satellite beams. We were interested specifically in the adaptation of beam shape to inhomogeneity in the geographic distribution of the user population, and in the impact of beam shaping on the set of transmission rates that are compatible with prescribed constraints on transmission powers and signal-to-interference ratios. Assuming that the spatial distribution of users is known, we construct an algorithm which computes beam coverage regions to equilibrate the per-beam user populations. The impact on the set of feasible <b>bit-rate</b> <b>allocations</b> is quantified through numerical experiments. Comparison with uniform beam shapes suggests that the adaptive approach is superior {{in terms of the number}} of concurrent transmissions that can be supported. The second problem concerns the allocation of bit rates in a setting where user bit-rate requirements are assumed defined by averages over moving windows of constant length. We use a frame-based channel model characterized by fading coefficients which, though statistically variable, are assumed known to the controller at the start of each frame. The implied temporal elasticity in quality-of-service provides opportunity to achieve economies in transmitted power. The value of such opportunity is quantified by comparison of two extreme cases. We develop an approximate system model which allows optimization of the rate allocation when the number of users is small, and a heuristic which is useful when the number of users is not small. The associated performance results confirm the inverse relationship between the per-bit energy required for transmission and the length of the averaging window...|$|R
40|$|The {{transmission}} of compressed video over channels with different capacities {{may require a}} reduction in bit rate if the transmission channel has a lower capacity than the capacity required by the video bit-stream, or when the channel capacity is changing over time. The process of converting a compressed video format into another compressed format is known as transcoding. This thesis addresses the specific transcoding problem of <b>dynamic</b> <b>bit-rate</b> adaptation for transmission over low bandwidth wireless channels. Transmitting compressed video over lower bandwidth wireless channels require accurate and efficient rate-control schemes. In this thesis, we propose several techniques to improve transcoding performance. Based on our experimental results, we present an approximate linear bit allocation model and macroblock layer rate-control algorithm, which can achieve accurate transcoding bit-rate. By reusing useful statistics information from the incoming compressed video, the bit-rate of the transcoded video can be determined according to the video scene context. Considering a specific bursty error wireless channel, we propose a solution which combines video transcoding and an ARQ protocol to transmit compressed video over this channel. In {{order to make sure}} that the end decoder can decode and play the transcoded video within the required end-to-end delay, we analyze the rate and buffer constraints of the transcoder and derive the conditions that have to be met by the transcoder. In order to test the proposed solution, we use a statistical channel model to simulate the wireless channel and use this model and channel observation to estimate the effective channel bandwidth, which will be fed back to the transcoder for better rate control. In this thesis, we discuss two applications. For real time video communication over wireless channel, we propose an algorithm that determines the transcoding scaling factor considering end-to-end delay, buffer fullness and effective channel bandwidth. For pre-encoded video distribution over wireless channels, we propose an algorithm which can determine the transcoding bit budget based on end-to-end delay, effective bandwidth, and original video bit profile. The proposed algorithm outperforms H. 263 TMN 8 in terms of video quality and buffer behavior with the same computational requirements...|$|R
40|$|Mobile {{communications}} {{has gained}} a growing interest from both customers and service providers alike in the last 1 - 2 decades. Visual information is used in many application domains such as remote health care, video –on demand, broadcasting, video surveillance etc. In order to enhance the visual effects of digital video content, the depth perception needs to be provided with the actual visual content. 3 D video has earned a significant interest from the research community in recent years, due to the tremendous impact it leaves on viewers and its enhancement of the user’s quality of experience (QoE). In the near future, 3 D video {{is likely to be}} used in most video applications, as it offers a greater sense of immersion and perceptual experience. When 3 D video is compressed and transmitted over error prone channels, the associated packet loss leads to visual quality degradation. When a picture is lost or corrupted so severely that the concealment result is not acceptable, the receiver typically pauses video playback and waits for the next INTRA picture to resume decoding. Error propagation caused by employing predictive coding may degrade the video quality severely. There are several ways used to mitigate the effects of such transmission errors. One widely used technique in International Video Coding Standards is error resilience. The motivation behind this research work is that, existing schemes for 2 D colour video compression such as MPEG, JPEG and H. 263 cannot be applied to 3 D video content. 3 D video signals contain depth as well as colour information and are bandwidth demanding, as they require the transmission of multiple high-bandwidth 3 D video streams. On the other hand, the capacity of wireless channels is limited and wireless links are prone to various types of errors caused by noise, interference, fading, handoff, error burst and network congestion. Given the maximum bit rate budget to represent the 3 D scene, optimal <b>bit-rate</b> <b>allocation</b> between texture and depth information rendering distortion/losses should be minimised. To mitigate the effect of these errors on the perceptual 3 D video quality, error resilience video coding needs to be investigated further to offer better quality of experience (QoE) to end users. This research work aims at enhancing the error resilience capability of compressed 3 D video, when transmitted over mobile channels, using Multiple Description Coding (MDC) in order to improve better user’s quality of experience (QoE). Furthermore, this thesis examines the sensitivity of the human visual system (HVS) when employed to view 3 D video scenes. The approach used in this study is to use subjective testing in order to rate people’s perception of 3 D video under error free and error prone conditions {{through the use of a}} carefully designed bespoke questionnaire. EThOS - Electronic Theses Online ServicePetroleum Technology Development Fund (PTDF) GBUnited Kingdo...|$|R
40|$|This thesis was {{submitted}} for {{the degree of}} Doctor of Philosophy and awarded by Brunel University. Mobile communications has gained a growing interest from both customers and service providers alike in the last 1 - 2 decades. Visual information is used in many application domains such as remote health care, video –on demand, broadcasting, video surveillance etc. In order to enhance the visual effects of digital video content, the depth perception needs to be provided with the actual visual content. 3 D video has earned a significant interest from the research community in recent years, due to the tremendous impact it leaves on viewers and its enhancement of the user’s quality of experience (QoE). In the near future, 3 D video {{is likely to be}} used in most video applications, as it offers a greater sense of immersion and perceptual experience. When 3 D video is compressed and transmitted over error prone channels, the associated packet loss leads to visual quality degradation. When a picture is lost or corrupted so severely that the concealment result is not acceptable, the receiver typically pauses video playback and waits for the next INTRA picture to resume decoding. Error propagation caused by employing predictive coding may degrade the video quality severely. There are several ways used to mitigate the effects of such transmission errors. One widely used technique in International Video Coding Standards is error resilience. The motivation behind this research work is that, existing schemes for 2 D colour video compression such as MPEG, JPEG and H. 263 cannot be applied to 3 D video content. 3 D video signals contain depth as well as colour information and are bandwidth demanding, as they require the transmission of multiple high-bandwidth 3 D video streams. On the other hand, the capacity of wireless channels is limited and wireless links are prone to various types of errors caused by noise, interference, fading, handoff, error burst and network congestion. Given the maximum bit rate budget to represent the 3 D scene, optimal <b>bit-rate</b> <b>allocation</b> between texture and depth information rendering distortion/losses should be minimised. To mitigate the effect of these errors on the perceptual 3 D video quality, error resilience video coding needs to be investigated further to offer better quality of experience (QoE) to end users. This research work aims at enhancing the error resilience capability of compressed 3 D video, when transmitted over mobile channels, using Multiple Description Coding (MDC) in order to improve better user’s quality of experience (QoE). Furthermore, this thesis examines the sensitivity of the human visual system (HVS) when employed to view 3 D video scenes. The approach used in this study is to use subjective testing in order to rate people’s perception of 3 D video under error free and error prone conditions {{through the use of a}} carefully designed bespoke questionnaire. Petroleum Technology Development Fund (PTDF...|$|R
40|$|Abstract—In this paper, we {{consider}} the downlink rate control problem in a wireless channel. A dynamic programming optimization method is introduced to obtain the optimal bit-rate/delay control policy in the downlink for packet transmission in wireless networks with fading channels. We assume that the base station is capable of transmitting data packets in the downlink with different bit rates, H I I. It is assumed that the symbol rate is fixed in the system, and different bit rates are achieved by choosing the transmitted symbols from the appropriate signal constellation (adaptive modulation). The derived optimal rate control policy, in each time slot, selects the highest possible bit rate which minimizes the delay {{and at the same}} time minimizes the number of rate switchings in the network. The optimal bit-rate control problem is an important issue, especially in packet data networks, where we need to guarantee a quality of service (QoS) in the network. Our analytical as well as simulation results confirm that there is an optimal threshold policy to switch between different rates. Index Terms—Adaptive modulation, <b>dynamic</b> programming, optimal <b>bit-rate</b> control, wireless packet networks. I...|$|R
40|$|The {{widespread}} {{proliferation of}} {{wireless sensor networks}} is revolutionizing our capabilities of monitoring and controlling the environment. The wireless connection of spatially distributed sensors, controllers and actuators poses challenges to the control system, due to packet drops, delays and measurements quantization, {{as well as to}} the wireless network resource allocator. These challenges push for a cross-layer design of communication and estimation/control systems. In this paper, assuming a TCP-like protocol between controller and actuator, we solve the problem of optimum control around a target state for a stable system in case of both packet drops and signal quantization. Generalization for unstable systems is also given in case of negligible quantization error. Moreover we propose a general framework for cross-layer optimization of signal quantization and network resource allocation. Here our main contributions are on i) quantizer design (how many bits to allocate for signal quantization), ii) network resource <b>allocation</b> (<b>bit-rate</b> for each radio link) and iii) choice of the transmission mode (constellation and channel coding rate). As application example, we consider a simple scalar, stable system and compare network resource allocation in the presence of i) low-cost sensors using a fix modulation and ii) long-term future sensors capable of rate adaptation. Interestingly, almost optimal control is achievable with small bandwidth transmissions and simple BPSK, supporting the use of low-cost sensors in applications dealing with state control around a target state trajectory...|$|R

