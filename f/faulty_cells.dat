58|96|Public
30|$|The {{second test}} {{shown in this}} {{subsection}} considers two <b>faulty</b> <b>cells</b> that are not cosited cells. The selected <b>faulty</b> <b>cells</b> are the original cell (i.e., cell 11) {{and one of its}} main neighboring cells (i.e., cell 3). The selected neighboring cells for cell 3 are 1, 2, 6, 10, 11, and 32. The neighboring cell 11 is not considered as neighboring cell for the compensation since in this test it is also affected by the fault. The algorithm settings for cell 3 are the same as for cell 10 and described before (i.e., Thr 3 = 1 and Thr 4 = 2).|$|E
40|$|Abstract — As {{dimensions}} of MOS devices have been scaled down, new reliability problems {{are coming into}} effect. One of these emerging reliability issues is aging effects which result in device performance degradation over time. NBTI (Negative biased temperature instability) is a well known aging phenomenon which is a limiting factor for future scaling of devices. NBTI results in the generation of trapped charges which cause Vt (threshold voltage) degradation of PMOS. It is observed that a sharp Vt shift occurs {{in just a few}} seconds after turning on the MOSFET. In nano-scale CMOS technologies, process (threshold voltage) and temperature variations are also crucial reliability concerns. On the other hand, NBTI itself is dependent on temperature and threshold voltage. In this paper, the combined effect of NBTI, process and temperature variations on the reliability of the 6 T SRAM (Static Random Access Memory) in 32 nm CMOS technology is analyzed. It is observed that: (1) Vt abruptly increases initially and afterwards Vt shift is very small, even for prolonged time; (2) Low Vt transistors age faster than high Vt transistors; and (3) NBTI Vt degradation is more significant at higher temperature. Along with these observations, we also quantified our results in terms of number of <b>faulty</b> <b>cells</b> in SRAM array. It is observed that: (1) number of <b>faulty</b> <b>cells</b> rises over time (8. 2 % rise in <b>faulty</b> <b>cells</b> for the inter-die nominal Vt chip over 2 years) due to SNM degradation; (2) {{rise in the number of}} <b>faulty</b> <b>cells</b> over time due to write failures under NBTI effect is practically negligible; (3) Leakage (in the worst case condition) and access time are not impacted by NBTI. I...|$|E
40|$|This paper {{deals with}} {{efficient}} methods for mapping arbitrary parallel algorithms onto faulty general purpose VLSI/WSI data-driven array. First, {{a brief overview}} of several architectural designs of the array is given. Next, three directions for the algorithmic improvement of a certain mapping scheme are presented. None of these directions takes into account the possibility of the defects in the array. Therefore, we present two methods which can be used to adapt any of the above algorithmic improvements for the case where defects are present in the array. In the first Map-onto-faulty-array method, <b>faulty</b> <b>cells</b> are taken into consideration during all the phases of the mapping/improvement process. In contrast, the second Map-and-correct method initially ignores <b>faulty</b> <b>cells</b> and takes care of them in the correction phase following the mapping/improvement process...|$|E
30|$|Other {{alternative}} {{to reduce the}} coverage area of the <b>faulty</b> <b>cell</b> is to increase its antenna tilt (i.e., downtilt the <b>faulty</b> <b>cell).</b> The channel quality of the neighboring cells could be improved by downtilting the <b>faulty</b> <b>cell</b> antenna because the interference produced by the <b>faulty</b> <b>cell</b> can be reduced. However, the expected improvement will be small since the transmission power of the <b>faulty</b> <b>cell</b> has been reduced so that the interference level produced in the neighboring cell is small too. In addition, when the <b>faulty</b> <b>cell</b> antenna tilt is increased, a deterioration of the cell edge of the <b>faulty</b> <b>cell</b> may be caused so that the SINR 50 of the <b>faulty</b> <b>cell</b> cannot be increased.|$|R
30|$|Finally, a last {{test has}} been carried out in order to compare the {{proposed}} algorithm with another approach. The considered method for the comparison is based on tilt modifications since it is the method most frequently used for outage compensation. In particular, a sensitivity study {{has been carried}} out. In this case, the <b>faulty</b> <b>cell</b> is cell 11 and the selected neighboring cells are cells 2, 3, 10, 12, 13, and 15. The initial antenna tilt angle of these cells are 4 °, 2 °, 5 °, 4 °, 5 °, and 4 °, respectively. The step used in the simulations is 1 ° because bigger values may produce significant variations in the KPIs. The minimum angle is limited to 0 ° in order to avoid negative values of the tilt angle.Several simulations have been executed in order to analyze different combinations of tilt changes for the <b>faulty</b> <b>cell</b> and the neighboring cells. The strategy that obtains the best results is based on downtilting neighboring cells. Such changes produce an interference reduction in the <b>faulty</b> <b>cell</b> and allow to increase the SINR 50 even when it is accepting more users from the neighboring cells. The reason for that is that the experienced quality in the cell edge of the <b>faulty</b> <b>cell</b> is improved.|$|R
3000|$|..., as {{presented}} in Fig.  1. This {{is one of}} the main characteristics of the proposed approach, as it allows the monitoring of a possible <b>faulty</b> <b>cell</b> [...]...|$|R
40|$|A new {{algorithm}} for the Optimal Spare Allocation {{problem in}} reconfigurable arrays is presented. It {{is based on}} a previously published branch and bound search method [4]. Enhanced with a strong heuristic function, it yields an effective A ∗ search, which performs more efficiently in the iterative deepening implementation (IDA ∗). A parallel implementation of the algorithm for a distributed shared memory machine using TreadMarks TM was used for evaluation. The results demonstrate the efficiency of the scheme and show how computationally intensive problems can be handled by appropriate domain heuristics and cooperative distributed computational power. 1 The Optimal Spare Allocation Problem The Optimal Spare Allocation (OSA) problem (or equivalently Vertex Cover in Bipartite Graphs) [4] is an NP-HARD problem arising in fault tolerant computing. It deals with the optimal allocation of spare rows and columns over a two-dimensional array of cells where some cells are faulty. The cells can be processors, memory units or VLSI components in practical applications. A number of spare rows and a number of spare columns are given, with two costs associated to spare rows and columns respectively. One spare row (column) can replace any row (column) in the array, repairing all the <b>faulty</b> <b>cells</b> along this row (column). The purpose is to repair all the <b>faulty</b> <b>cells</b> using spares, with the minimum possible overall cost. The sample instance [4] given below, consists of a (10 × 10) array with 12 <b>faulty</b> <b>cells.</b> There are 3 spare rows with a cost of 8 associated to each one of them and 3 spare columns with a cost of 15 associated to each one of them. An optimal solution with a cost of 39 would include the spare row allocations { 4, 2, 8 } and the spare column allocation { 9 }...|$|E
40|$|This paper {{deals with}} active {{diagnosis}} {{for a class}} of switched systems which may not satisfy the classical diagnosability conditions. A Mealy Machine modeling is used to define an appropriate diagnoser which reduces the uncertain state subset. An algorithm combining the proposed diagnoser and a testing procedure is introduced to solve the fault identification problem. A study on the multicellular converter is carried out to detect and isolate <b>faulty</b> <b>cells.</b> Simulation results show {{the effectiveness of the}} proposed scheme...|$|E
40|$|Since {{the minimum}} feature size of dynamic RAM has been scaled down, {{several studies have}} been carried out to sense the <b>faulty</b> <b>cells.</b> In the field of testing, more {{appropriate}} test algorithms are required to detect the faults in the cells. In this paper, various test algorithms such as March tests, word line pulsing technique, large Vds Data Retention Test Pattern, interleaving test algorithm are discussed. These algorithms allow the screening of faults in the cells...|$|E
3000|$|... b {{shows how}} the ThrUL 50 for the <b>faulty</b> <b>cell</b> is {{increased}} due to a coverage area reduction while the ThrUL 50 for the neighboring cells remains stable.|$|R
30|$|In summary, {{the weak}} {{coverage}} fault causes a degradation in the SINR 50 of the <b>faulty</b> <b>cell,</b> which was successfully {{overcome by the}} proposed method, without degrading neighboring cells.|$|R
30|$|When a cell {{suffers a}} weak {{coverage}} problem, its users experience {{a reduction in}} the received signal quality. The objective of the compensation algorithm is to improve the quality experience of the affected users by modifying some configuration parameters of the serving and neighboring cells. In this work, the control parameter that has been modified for the <b>faulty</b> <b>cell</b> and its neighboring cells is the HOM. With these modifications, the algorithm tries to force users in the cell edge of the <b>faulty</b> <b>cell</b> to move to a neighboring cell. As a consequence, the service area of the <b>faulty</b> <b>cell</b> is reduced and its SINR 50 would be improved. Once the compensation algorithm is activated, changes applied to the HOM may {{have a significant impact on}} neighboring cell performance. These changes may produce a degradation in some KPIs such as retainability. For this reason, the proposed algorithm includes the retainability from the neighboring cells as input of the algorithm.|$|R
40|$|As microfluidics-based biochips {{become more}} complex, {{manufacturing}} yield will have {{significant influence on}} production volume and product cost. We propose an interstitial redundancy approach to enhance the yield of biochips {{that are based on}} droplet-based microfluidics. In this design method, spare cells are placed in the interstitial sites within the microfluidic array, and they replace neighboring <b>faulty</b> <b>cells</b> via local reconfiguration. The proposed design method is evaluated using a set of concurrent real-life bioassays. Comment: Submitted on behalf of EDAA ([URL]...|$|E
40|$|Abstract {{attributed}} to the high regularity of memories, PAs and As microfluidics-based biochips become more complex, manufacturing yield will have significant influence on production volume and product cost. We propose an interstitial redundancy approach to enhance the yield of biochips {{that are based on}} droplet-based microfluidics. In this design method, spare cells are placed in the interstitial sites within the microfluidic array, and they replace neighboring <b>faulty</b> <b>cells</b> via local reconfiguration. The proposed design method is evaluated using a set of concurrent real-life bioassays. 1...|$|E
40|$|Abstract. Despite being staggeringly error prone, {{spreadsheets}} can {{be viewed}} as a highly flexible end-users programming environment. As a consequence, spreadsheets are widely adopted for decision making by end-users, and may have a serious economical impact for the business. Hence, approaches for aiding the process of pinpointing the <b>faulty</b> <b>cells</b> in a spreadsheet are of great value. In this paper we present a constrain-based approach for debugging spreadsheets. We coin the approach Con-Bug. Essentially, the approach takes as input a (faulty) spreadsheet and a test case that reveals the fault (a test case specifies values for the input cells as well as the expected values for the output cells) and computes a set of diagnosis candidates for the debugging problem we are trying to solve. To compute the set of diagnosis candidates we convert the spreadsheet and test case to a constraint satisfaction problem (CSP), modeled using the state-of-the-art constraint solver MINION. We use a case study, in particular using a spreadsheet taken from the well-known EUSES Spread-sheet Corpus, to better explain the different phases of the approach as well as to measure the efficiency of ConBug. We conclude that ConBug can be of added value for the end user in order to pinpoint <b>faulty</b> <b>cells.</b> ...|$|E
40|$|Fast and {{accurate}} fault detection {{is the primary}} step {{and one of the}} most important tasks in fault tolerant converters. In this paper, a fast and simple method is proposed to detect and diagnosis the <b>faulty</b> <b>cell</b> in a cascaded H-bridge multilevel inverter under a short circuit fault. In this method, the reference voltage is calculated using switching control pulses and DC-Link voltages. The comparison result of the output voltage and the reference voltage is used in conjunction with active cell pulses to detect the <b>faulty</b> <b>cell.</b> To achieve this goal, the cell which is active when the Fault signal turns to “ 0 ” is detected as the <b>faulty</b> <b>cell.</b> Furthermore, consideration of generating the active cell pulses is completely described. Since the main advantage of this method is its simplicity, it can be easily implemented in a programmable digital device. Experimental results obtained with an 11 -level inverter prototype confirm the effectiveness of the proposed fault detection technique. In addition, they show that the diagnosis method is unaffected by variations of the modulation index...|$|R
5|$|Most {{eukaryotic}} cell types {{usually have a}} single nucleus, but some have no nuclei, while others have several. This can result from normal development, as in the maturation of mammalian red blood <b>cells,</b> or from <b>faulty</b> <b>cell</b> division.|$|R
3000|$|One {{possible}} undesired {{effect may}} be a degradation in the uplink of the neighboring cells, since the changes made by the compensation algorithm provoke an increase of the service area of the neighboring cells. Along the iterations, the changes made in the HOM allow users from the <b>faulty</b> <b>cell</b> to change to a neighboring cell. These users are further than the other users of the neighboring cell so that they produce an increase of its service area. This increase may affect the uplink performance since these users may be interfered by the users from the <b>faulty</b> <b>cell.</b> However, the proposed algorithm is able to achieve a compensation situation without degrading the uplink of the neighboring cells. Figure 5 [...]...|$|R
40|$|Abstract: Stress {{optimization}} for {{memory devices}} {{is a complex}} process due to the continuous space of possible optimization values for relevant parameters. This paper uses a method based on electrical Spice simulation to perform this optimization process for DRAM devices. The paper presents a case-study performed in Qimonda to optimize the timing and temperature stresses for the strap problem in defective memory cells. The paper also considers the impact of bit line coupling effects on the faulty behavior and identifies the worst case coupling background needed to detect the <b>faulty</b> <b>cells...</b>|$|E
40|$|Abstract—Online Testability is used {{to detect}} bit error of Reversible Circuit at runtime using Check circuit to {{propagate}} all errors into single line. In this paper, we propose an improved design of check circuit, Modified Test Cell (MTC) can detect error in short time and also able to identify affected cell(s) simultane-ously. We also present a cost effective design of Online Testable Ripple Carry Adder (RCA) with proposed MTC. Proposed design of check circuit is able to enhance the reusability of circuit by replacing corresponding <b>faulty</b> <b>cell(s).</b> I...|$|E
40|$|This study {{addresses}} {{the problem of}} efficient fault simulation and test generation in circuits using multi-output combinational logic cells. A symbolic fault simulation algorithm is proposed to exploit bit-level parallelism in order to represent the propagation of the output value of <b>faulty</b> <b>cells</b> throughout the circuit, thus accounting for different faulty behaviours in a single simulation step. A satisfiability (SAT) -based test generation procedure is also provided and it early discovers sets of undetectable behaviours. Results {{for a set of}} combinational benchmarks show the feasibility of the proposed approach...|$|E
30|$|This test {{tries to}} {{evaluate}} the proposed algorithm when different levels of degradation affect a certain cell. Several values for the offsetTx parameter have been defined in order to implement different levels of degradation. Specifically, the simulations {{have been carried out}} with values of 7 dB (fault case 1), 10 dB (fault case 2), and 13 dB (fault case 3). The <b>faulty</b> <b>cell</b> is cell 11 (Fig. 4).|$|R
30|$|First, {{this paper}} {{considers}} a weak coverage fault (as a different problem as the cell outage problem commonly addressed in SON literature). As explained before, when a cell is in outage, it cannot carry out traffic. Therefore, {{the main effect}} caused by a cell outage is the total loss of service in the problematic area. In such a situation, {{it is not possible}} to consider the <b>faulty</b> <b>cell</b> as a part of the compensation algorithm. This paper presents a compensation algorithm with the aim of mitigating the effects caused by a weak coverage fault considering modifications of the <b>faulty</b> <b>cell</b> parameters as part of the compensation method. Moreover, the degradation produced by the fault will be different to the one produced by a cell outage.|$|R
5000|$|Again in {{the days}} of direct current mains, power {{stations}} often had large lead-acid batteries for load balancing. These supplemented the steam-powered generators during peak periods and were re-charged off-peak. Sometimes one cell in the battery would become [...] "sick" [...] (faulty, reduced capacity) and a [...] "milking booster" [...] would be used to give it an additional charge and restore it to health. The milking booster was so-called because it [...] "milked" [...] the healthy cells in the battery to give an extra charge to the faulty one. The motor side of the booster was connected across the whole battery but the generator side was connected only across the <b>faulty</b> <b>cell.</b> During discharge periods the booster supplemented the output of the <b>faulty</b> <b>cell.</b>|$|R
40|$|As digital {{microfluidic}} biochips become {{widespread in}} safety-critical biochemical applications, system dependability {{emerges as a}} critical performance parameter. The dynamic reconfigurability inherent in digital microfluidic biochips can be utilized to bypass <b>faulty</b> <b>cells,</b> thereby supporting defect/fault tolerance. In this paper, we propose three different reconfiguration techniques and the corresponding defect/fault tolerance methodologies for digital microfluidic biochips. The proposed schemes ensure that the bioassays mapped to a droplet-based microfluidic array can still be executed on a defective biochip. Real-life biochemical assays, namely multiplexed diagnostics on human physiological fluids, are {{used to evaluate the}} proposed reconfiguration techniques. 1...|$|E
40|$|Fault {{tolerance}} {{techniques are}} important to increase the yield of the VLSI chips in advanced fabrication technologies. In regular structure like FPGA, redundancy is commonly used for fault tolerance. Most of the techniques found so far in literature talks about software based change in configuration data. Here in this work we present a solution in which configuration bit stream of FPGA is changed by a hardware controller that is present on the FPGA chip itself. The technique uses redundant columns for replacing <b>faulty</b> <b>cells.</b> Two variation of the technique is presented and thereafter a qualitative analysis of the tradeoffs is done. ...|$|E
40|$|This paper {{presents}} a first-order analytical model {{for determining the}} performance degradation caused by permanently <b>faulty</b> <b>cells</b> in archi-tectural and non-architectural arrays. We refer to this degradation as the performance vulnerability factor (PVF). The study assumes a future where cache blocks with <b>faulty</b> <b>cells</b> are disabled resulting in less cache capacity and extra misses while faulty predictor cells are still used but cause additional mispredictions. For a given program run, random probability of permanent cell failure, and processor configuration, the model can rapidly provide the expected PVF as well as lower and upper PVF probability distri-bution bounds for an individual array or array combination. The model is used to predict the PVF for the three predictors and the last level cache, used in this study, {{for a wide range}} of cell failure rates. The analysis reveals that for cell failure rate of up to 1. 5 e- 6 the expected PVF is very small. For higher failure rates the expected PVF grows noticeably mostly due to the extra misses in the last level cache. The expected PVF of the predictors remains small even at high failure rates but the PVF distribution reveals cases of significant performance degradation with a non-negligible probability. These results suggest that designers of future processors can lever-age trade-offs between PVF and reliability to sustain area, perfor-mance and energy scaling. The paper demonstrates this approach by exploring the implications of different cell size on yield and PVF. 1...|$|E
30|$|In {{the context}} of Self-Organizing Networks (SON), this paper {{presents}} a novel cell degradation compensation algorithm based on handover margin modifications. The fault considered in this paper is a weak coverage use case so that the <b>faulty</b> <b>cell</b> will be active during the compensation, but its power will be abnormally lower than it should. Unlike the extensively studied cell outage use case, in this case, the modification of the <b>faulty</b> <b>cell’s</b> parameters {{can be considered as}} part of the compensation algorithm. In particular, this paper proposes to modify the handover margins for cell degradation compensation. To analyze the proposed algorithm, a set of simulations has been carried out using an LTE simulator with a realistic scenario. Results show that the proposed algorithm is able to improve the degradation caused by the fault with a low effect in the neighboring cells under different conditions.|$|R
30|$|Second, {{this paper}} proposes a {{compensation}} algorithm based on HO margin modifications, including the <b>faulty</b> <b>cell</b> and its neighbors. The {{use of this}} parameter with a compensation objective is the second main contribution of this paper. The HO margin has been extensively used with optimization purposes, {{but it has not}} previously considered for fault compensation.|$|R
50|$|Klerokinesis (from the Greek {{root for}} allotted inheritance) has been {{claimed to be}} a new form of cell {{division}} in human cells. It is thought to serve as a natural back-up mechanism during <b>faulty</b> <b>cell</b> division (Cytokinesis), thus preventing some cells from going down a path that can lead to cancer. It {{is considered to be a}} recovery mechanism.|$|R
40|$|Multilevel {{converters}} use a {{large amount}} of semiconductors, allowing the reconfigurate of the converter to work even on internal fault condition. This paper presents a method to detect <b>faulty</b> <b>cells</b> in a cascaded multicell converter requiring just one voltage measurement per output phase. The method is based on high-frequency harmonic analysis, using a dynamic prediction of their behavior, avoiding erroneous detection on transients while keeping the precision under real fault events. Once the faulty cell is detected, it can be bypassed allowing the converter to keep working according to previously reported techniques. Experimental results confirm accurate and fast fault detection, with a good rejection to normal operation transients...|$|E
40|$|Spreadsheet programs, {{artifacts}} {{developed by}} non-programmers, {{are used for}} a variety of important tasks and decisions. Yet a significant proportion of them have severe quality problems. To address this issue, our previous work presented an interval-based testing methodology for spreadsheets. Interval-based testing rests on the observation that spreadsheets are mainly used for numerical computations. It also incorporates ideas from symbolic testing and interval analysis. This paper addresses the issue of efficiently debugging spreadsheets. Based on the interval-based testing methodology, this paper presents a technique for tracing faults in spreadsheet programs. The fault tracing technique proposed uses the dataflow information and cell marks to identify the most influential <b>faulty</b> <b>cell(s)</b> for a given formula cell containing a propagated fault. Comment: 13 Pages, 4 figue...|$|E
40|$|Microfluidics-based biochips for {{biochemical}} {{analysis are}} currently receiving much attention. They automate highly repetitive laboratory procedures by replacing cumbersome equipment with miniaturized and integrated systems. As these microfluidics-based microsystems become more complex, manufacturing yield will have {{significant influence on}} production volume and product cost. We propose an interstitial redundancy approach to enhance the yield of biochips {{that are based on}} droplet-based digital microfluidics. In this design method, spare cells are placed in the interstitial sites within the microfluidic array, and they replace neighboring <b>faulty</b> <b>cells</b> via local reconfiguration. The proposed design method is evaluated using a set of concurrent real-life bioassays. The defect-tolerant design approach based on space redundancy and local reconfiguration is expected to facilitate yield enhancement of microfluidics-based biochips, especially for the emerging marketplace...|$|E
40|$|Self-replicating {{structures}} in cellular automata have been extensively {{studied in the}} past as models of Artificial Life. However, CAs, unlike the biological cellular model, are very brittle: any <b>faulty</b> <b>cell</b> usually leads to the complete destruction of any emerging structures. In this paper, we propose a method, inspired by error-correcting-code theory, to develop fault-resistant rules at, almost, no extra cost. We then propose fault-tolerant substructures necessary to future fault-tolerant self-replicating structure...|$|R
50|$|One {{exception}} {{is that in}} rare cases <b>faulty</b> <b>cell</b> division may leave an extra X chromosome {{in one of the}} gametes that produced the male cat. That extra X then is reproduced in each of his cells, a condition referred to as XXY, or Klinefelter syndrome. Such a combination of chromosomes could produce tortoiseshell or calico markings in the male, {{in the same way as}} XX chromosomes produce them in the female.|$|R
30|$|However, {{there are}} many other network {{failures}} that can occur and affect the network performance [20]. These other faults (e.g., overshooting and weak coverage) may provoke an important degradation in the network so that the application of a compensation method is needed while the fault is identified and repaired. In these cases, the <b>faulty</b> <b>cell</b> is still active and that allows to consider the modification of its own parameters, and not only neighboring cells’ parameters, as part of the compensation method. This paper presents a degraded cell compensation algorithm, an issue that has not been previously considered in the literature. Specifically, the analyzed degradation consists of a coverage deterioration in a cell due to a reduction of its transmission power in the downlink. The transmission power reduction may be caused by wiring problems or a wrong parameter configuration. In this situation, the <b>faulty</b> <b>cell</b> is still carrying traffic although its coverage area is reduced due to the fault. Thus, the <b>faulty</b> <b>cell</b> can be considered for the compensation. This paper proposes to use the handover (HO) margin as a new parameter for cell degradation compensation. This parameter has been extensively used for network performance optimization. In that context, the objective of the HO margin modification typically has been the HO process optimization [21] or load balancing in case of a congestion situation [22] in macrocell scenarios. There are other works that investigate the HO management in other scenarios such as small cell or femtocell scenarios [23 – 26]. However, the HO margins have not been previously considered for fault compensation. Even if both types of algorithms (i.e., compensation and coverage optimization) tune the same parameters (e.g., HO margins), in most cases, the changes made by each algorithm will be different, since their objectives are different.|$|R
