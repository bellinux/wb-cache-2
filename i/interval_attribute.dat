2|93|Public
40|$|Abstract—Partner {{selection}} {{is an important}} step of the supply chain companies. This issue belongs to the multi-attribute Decision-making Problem. This work is based on the framework of MAS-based dynamic supply chain. The basic idea of Agent is to make the software to simulate organizational behavior and relationships,so using agent to select the partner is a trend. It applies TOPSIS method in the partner selection. The feature of this study is {{how to deal with the}} <b>interval</b> <b>attribute</b> in partner selection. Finally, this work uses an examples to illustrate how Enterprise Agent to select thepartners that the attributes are intervalin the partner selection. Keywords-Dynamic supply chain,Partner selection, TOPSIS (Technique for order preference by similarity to ideal solution), MAS(Multi-Agents) I...|$|E
40|$|Each tuple in a valid-time {{relation}} {{includes an}} <b>interval</b> <b>attribute</b> T {{that represents the}} tuple's valid time. The overlap join between two valid-time relations determines all pairs of tuples with overlapping intervals. Although overlap joins are common, existing partitioning and indexing schemes are inefficient if the data includes long-lived tuples or if intervals intersect partition boundaries. We propose Overlap Interval Partitioning (OIP), a new partitioning approach for data with an interval. OIP divides the time range of a relation into k base granules and defines overlapping partitions for sequences of contiguous granules. OIP is the first partitioning method for interval data that gives a constant clustering guarantee: the difference in duration between the interval of a tuple and the interval of its partition is independent of {{the duration of the}} tuple's interval. We offer a detailed analysis of the average false hit ratio and the average number of partition accesses for queries with overlap predicates, and we prove that the average false hit ratio is independent of the number of short- and long-lived tuples. To compute the overlap join, we propose the Overlap Interval Partition Join (OIPJoin), which uses OIP to partition the input relations on-the-fly. Only the tuples from overlapping partitions have to be joined to compute the result. We analytically derive the optimal number of granules, k, for partitioning the two input relations, from the size of the data, the cost of CPU operations, and the cost of main memory or disk IOs. Our experiments confirm the analytical results and show that the OIPJoin outperforms state-of-the-art techniques for the overlap join...|$|E
40|$|We {{have carried}} out {{experiments}} to re-evaluate {{the influence of}} ketoconazole (400 mg kg- 1,p. o.) {{on the effects of}} ebastine, terfenadine and loratadine on the QTc interval in conscious guinea-pigs. Following a previously described protocol of oral drug administration, but using telemetric recording of the ECG, we have found that the prolongation of the QTc <b>interval</b> <b>attributed</b> to ebastine and terfenadine is in fact entirely due to ketoconazole, and that neither terfenadine, ebastine nor loratadine produce any additional effects on subsequent administration...|$|R
40|$|Abstract. In view {{of target}} {{attribute}} value for different sector number, moreover, also attaches a target constraint condition kind of mix sector multi-attribute decision making question, {{this paper presents}} set pair analysis decision-making method. Firstly this paper puts forward three typical <b>interval</b> type <b>attribute</b> value representation; Then using set pair analysis theory, the <b>interval</b> type <b>attribute</b> value unified convert the correlate form, Finally has given complex decision-making criterion function, which collected Conformity degree criteria and Criteria for membership degree. Through the construction plan changes decision-making example analysis shows that this method is a simple and effective method for solving multiple attribute decision making...|$|R
40|$|The Lower Aptian Sarastarri Limestone unit (Aralar, nor thern Spain), mainly {{consists}} of rudist and coral micrites and is nearly 200 m thick. The uppermost 30 m {{of the unit}} show sharp-based marly <b>intervals</b> <b>attributed</b> to two drowning events. Both marly intervals are immediately preceeded by laminated bioclastic rudstone facies, attributed to quick shoaling processes ending with subaerial exposure. A change of local subsidence conditions from uniform during the Sarastarri Limestone episode, to varying with distance during the Lareo Marl succeeding episode, is invoked to explain both drownings. Influence of oceanographic causes reported in other contemporaneous carbonate platforms is, however, not exclude...|$|R
40|$|With the {{increasing}} occurrence of {{temporal and spatial}} data in present-day database applications, the interval data type is adopted by more and more database systems. For an efficient support of queries that contain selections on <b>interval</b> <b>attributes</b> as well as simple-valued attributes (e. g. numbers, strings) at the same time, special index structures are required supporting both types of predicates in combination. Based on the Relational Interval Tree, we present various indexing schemes that support such combined queries and can be integrated in relational database systems with minimum effort. Experiments on different query types show superior performance for the new techniques in comparison to competing access methods. 1...|$|R
40|$|In present study, I {{proposed}} a node-similarity based algorithm for prediction of missing {{connections in the}} network. In this algorithm, whether a node vk can connect to vi or not, depending on the similarity between vk and vi, the similarities between vi and its adjacent nodes, the similarities between vk and the adjacent nodes of vi, {{and the degree of}} node vi, and vice versa. Pearson correlation measure, cosine measure, and (negative) Euclidean distance measure (the three measures are for <b>interval</b> <b>attributes),</b> contingency correlation measure (for nominal attributes), and Jaccard coefficient measure (for binary attributes) were used as the between-node similarity. Two application examples showed a better prediction of the algorithm (approximately 60...|$|R
40|$|This paper {{describes}} a heuristic for the equitable partitioning problem, which involves classifying individual elements, so that classes are similar. The paper presents three extensions to a heuristic algorithm, developed in earlier work, which dealt with binary-valued attributes only, The first extension illustrates how changing the coding {{of the data}} without changing the problem improves the quality of solutions obtained. The second extension allows the algorithm to deal with different scales of measurement; data sets consisting of binary valued, multi-valued nominal and <b>interval</b> <b>attributes</b> are tested and the results presented, The third extension allows the algorithm to deal with problems involving classes of different sizes. The revised heuristic {{is applied to the}} real life problem of allocating university student accommodation. (C) 1997 Elsevier Science Ltd. ...|$|R
40|$|Most of {{the common}} {{proposals}} for temporal extensions of SQL (e. g., ATSQL 2 or SQL/Temporal) use explicit interval-based references to time (<b>interval</b> <b>attributes).</b> In this {{paper we propose a}} different approach: we use point-based references to time as a basis for a temporal extension of SQL. The proposed language [...] -SQL/TP [...] -extends the syntax and semantics of SQL/ 92 in a very natural way: by adding a single new data type that represents the universe of individual time instants. Such an extension allows the users to write temporal queries in a customary fashion and vastly simplifies the semantics of the proposed language: we can essentially use the common SQL semantics. This way SQL/TP also fixes many problems present in query languages that use explicit interval-based temporal attributes. In addition, SQL/TP queries can still be efficiently evaluated over a compact interval-based encoding of possibly infinite temporal relations. The query evaluation is based on a compilation technique tha [...] ...|$|R
40|$|AbstractThe {{description}} of the attributes or characteristics of the individual parts in a feature-based clustering system is frequently vague, and linguistic, fuzzy number or fuzzy coding is ideally suited to represent these attributes. However, due to the vagueness of the description, the resulting fuzzy membership functions are usually very approximate. Neural network learning to improve the fuzzy representation {{was used in this}} investigation to overcome these difficulties. In particular, Kohonen's self-organizing map network combined with fuzzy membership functions was used to classify the different parts based on their various attributes. The network can simultaneously deal with crisp <b>attributes,</b> <b>interval</b> <b>attributes,</b> and fuzzy attributes. Due to the fuzzy input and fuzzy weights, a revised weight updating rule was proposed. Various approaches have been proposed to define the distance or ranking of fuzzy numbers, which is essential in order to use the Kohonen map. The overall existence measurement was used in the present investigation. To illustrate the approach, parts based on two attributes were classified and discussed...|$|R
40|$|Seismic {{attributes}} {{have come}} a long way since their intro d u ction in the early 1970 s and have become an integral part of seismic interpretation projects. To d a y, they are being used widely for lithological and petrophysical prediction of re s e rvoirs and various methodologies have been developed for their application to broader hydrocarbon exploration and development decision making. Beginning with the digital re c o rding of seismic data in the early 1960 s and the ensuing bright spot analysis, the 1970 s saw the introduction of complex trace attributes and seismic inversion along with their color displays. This was followed by the development of response attributes, introduction of texture analysis, 2 D <b>attributes,</b> horizon and <b>interval</b> <b>attributes</b> and the pervasive use of c o l o r. 3 D seismic acquisition dominated the 1990 s as the most successful exploration technology of several decades an...|$|R
40|$|In this paper, {{we develop}} {{a method of}} multi-attribute {{decision-making}} with both weights and attribute ratings expressed by single valued neutrosophic sets(SVN-sets). The method is called linear weighted averaging method of SVN-sets. Then, we present a sensitivity analysis of attribute weights which give changing <b>intervals</b> of <b>attribute</b> weights in which the ranking order of the alternatives is required to remain unchanging. Finally, validity and applicability of the proposed method are illustrated with a real application...|$|R
40|$|In {{databases}} {{with time}} <b>interval</b> <b>attributes,</b> query processing techniques {{that are based}} on sort-merge or sort-aggregate deteriorate. This happens because for intervals no total order exists and either the start or end point is used for the sorting. Doing so leads to inefficient solutions with lots of unproductive comparisons that do not produce an output tuple. Even if just one tuple with a long interval is present in the data, the number of unproductive comparisons of sort-merge and sort-aggregate gets quadratic. In this paper we propose disjoint interval partitioning (DIP), a technique to efficiently perform sort-based operators on interval data. DIP divides an input relation into the minimum number of partitions, such that all tuples in a partition are non-overlapping. The absence of overlapping tuples guarantees efficient sort-merge computations without backtracking. With DIP the number of unproductive comparisons is linear in the number of partitions. In contrast to current solutions with inefficient random accesses to the active tuples, DIP fetches the tuples in a partition sequentially. We illustrate the generality and efficiency of DIP by describing and evaluating three basic database operators over interval data: join, anti-join and aggregation...|$|R
40|$|In {{this paper}} we {{investigate}} {{the problem of}} processing multi-way interval joins on map-reduce platform. We look at join queries formed by interval predicates as defined by Allen’s interval algebra. These predicates can be classified in two groups: colocation based predicates and sequence based predicates. A colocation predicate requires two intervals to share at least one common point while a sequence predi-cate requires two intervals to be disjoint. An interval join query can therefore {{be thought of as}} belonging to one of the three classes: (a) queries containing only colocation based predicates, (b) queries containing only sequence based pred-icates and (c) queries containing both classes of predicates. We address these three classes of join queries, discuss the challenges and present novel approaches for processing these queries on map-reduce platform. We also discuss why the current approaches developed for handling join queries on real-valued data can not be directly used to handle inter-val joins. We finally extend the approaches developed to handle join queries containing multiple <b>interval</b> <b>attributes</b> as well as join queries containing both interval as well as non-interval attributes. Through experimental evaluations both on synthetic and real life datasets, we demonstrate that the proposed approaches comfortably outperform naive ap-proaches. 1...|$|R
40|$|Fig. 1 : The EventFlow display {{including}} {{the control panel}} and legend (left), the aggregated record display (middle), and the individual record display (right). Abstract — Our work on searching point-based event sequences with the Align, Rank, Filter, and Summary concepts produced a powerful tool (Lifelines 2) that is being applied in an increasing set of medical and other applications. The LifeFlow aggregation tool expanded the capabilities of Lifelines 2 to show common patterns of events on a single-screen display, resulting {{in the ability to}} summarize millions of individual patient records. However, users found that point-based event sequences limited their capacity to solve problems that had inherently <b>interval</b> <b>attributes,</b> for example, the 3 -month interval during which patients took a medication. This paper reports on our development of EventFlow, an application that integrates interval-based events into the original LifeFlow mechanisms. Interval events represent a fundamental increase in complexity at every level of the application, from the input and data structure to the eventual questions that a user might ask of the data. Our goal was to accomplish this integration in a way that appeared to users as a simple and intuitive extension of the original LifeFlow tool. In this paper, we present novel solutions for displaying interval events, simplifying their visual impact, and incorporating them into meaningful queries. Index Terms—EventFlow, temporal event sequence, temporal event querying. ...|$|R
40|$|Physical time <b>intervals</b> are <b>attributes</b> {{of single}} {{physical}} object whereas physical space intervals are a relational attribute of two physical objects. Some {{consequences of the}} breaking of the space-time exchange symmetry inherent in the Lorentz transformation following from the above distinction are investigated. In particular, it is shown that the relativity of simultaneity and length contraction effects which naively follow from space-time symmetry of the Lorentz transformation do not occur. Seven laws describing the relation between observations of space intervals, time intervals and velocities in different reference frames are given. Only two of these laws are respected by conventional special relativity theory. Comment: 15 pages, 2 figure...|$|R
40|$|Abstract. The {{technique}} {{of a new}} extension of fuzzy rough theory using partition of interval set-valued is proposed for granular computing during knowledge discovery in this paper. The natural <b>intervals</b> of <b>attribute</b> values in decision system to be transformed into multiple sub-interval of [0, 1]are given by normalization. And some characteristics of interval set-valued of decision systems in fuzzy rough set theory are discussed. The correctness and effectiveness of the approach are shown in experiments. The approach {{presented in this paper}} can also be used as a data preprocessing step for other symbolic knowledge discovery or machine learning methods other than rough set theory...|$|R
40|$|The Lotena Group is a clastic to evaporitic unit up to 650 m {{thick that}} {{accumulated}} during the Middle to Late Jurassic in the Neuquén Basin, western Argentina. Extensive field work {{carried out in}} the Sierra de la Vaca Muerta and Arroyo Covunco areas, including the measurement of seven detailed stratigraphic sections and geological mapping allow the discrimination of six unconformity-bounded units or sequences. The first sequence is composed of red beds and evaporites belonging to the Tábanos Formation that unconformably overlies strata of the Lower to Middle Jurassic Cuyo Group. Sequences 2 to 5 are shallow marine and display a basal sandstone <b>interval</b> <b>attributed</b> to confined shelfal sandstone lobes. These grade vertically into unconfined shelfal sandstone lobes, and terminate with carbonate deposits. The basal interval is restricted to the thickest areas of each sequence, a relationship attributed to structural relief. Sequence 6 has a very irregular shape and strongly truncates the underlying deposits. It is composed almost entirely of massive carbonate strata that were deposited by density currents. Facies analysis and stratigraphic mapping suggest periodic recycling of previous accumulations. Stratigraphic evidence suggests that the Lotena Group in the Sierra de la Vaca Muerta and adjacent areas probably accumulated over a tectonically unstable basement. Sequences 1, 2 and 3 display evidence of accumulation in an extensional tectonic setting, while sequences 4, 5 and 6 experienced a northward shift of their depocentres associated with extensive erosional truncation of the marginal areas, suggesting that accumulation was affected by early stages of growth of the Covunco anticline...|$|R
40|$|It is {{well known}} that Pearson linear {{correlations}} between more than two attributes (nodes, taxa, variables, etc) can be adjusted to partial linear correlations for eliminating indirect between-attribute interactions of other attributes not being tested. In present study I first proposed three correlation measures, revised Dice coefficient, overlap coefficient, and proportion correlation. In addition, I proposed partial correlation measures for some correlation measures, of which Jaccard correlation, revised Dice coefficient, overlap coefficient, and point correlation are for binary attributes; Spearman rank correlation and proportion correlation are for <b>interval</b> value <b>attributes.</b> The full algorithm and Matlab codes (Pearson linear correlation is included also) are given. Users can add other general correlation measures in the Matlab codes. ...|$|R
40|$|In {{this paper}} we {{describe}} Constraint-based <b>Attribute</b> and <b>Interval</b> Planning (CAIP), a paradigm for representing and reasoning about plans. The paradigm enables {{the description of}} planning domains with time, resources, concurrent activities, mutual exclusions among sets of activities, disjunctive preconditions and conditional effects. We provide a theoretical foundation for the paradigm, based on temporal <b>intervals</b> and <b>attributes.</b> We then show how the plans are naturally expressed by networks of constraints, and show {{that the process of}} planning maps directly to dynamic constraint reasoning. In addition, we de ne compatibilities, a compact mechanism for describing planning domains. We describe how this framework can incorporate the use of constraint reasoning technology to improve planning. Finally, we describe EUROPA, an implementation of the CAIP framework...|$|R
40|$|Tracking sea bed {{topography}} in the Jurassic. The Lotena Group in the Sierra de la Vaca Muerta (Neuquén Basin, Argentina) The Lotena Group is a clastic to evaporitic unit up to 650 m {{thick that}} accumulated during the Middle to Late Jurassic in the Neuquén Basin, western Argentina. Extensive field work {{carried out in}} the Sierra de la Vaca Muerta and Arroyo Covunco areas, including the measurement of seven detailed stratigraphic sections and geo-logical mapping allow the discrimination of six unconformity-bounded units or sequences. The first sequence is composed of red beds and evaporites belonging to the Tábanos Formation that unconformably overlies strata of the Lower to Middle Jurassic Cuyo Group. Sequences 2 to 5 are shallow marine and display a basal sandstone <b>interval</b> <b>attributed</b> to confined shelfal sandstone lobes. These grade vertically into unconfined shelfal sandstone lobes, and terminate with carbonate deposits. The basal interval is restricted to the thickest areas of each sequence, a relationship attributed to structural relief. Sequence 6 has a very irregular shape and strongly trun-cates the underlying deposits. It is composed almost entirely of massive carbonate strata that were deposited by density currents. Facies analysis and stratigraphic mapping suggest periodic recycling of previous accumula-tions. Stratigraphic evidence suggests that the Lotena Group in the Sierra de la Vaca Muerta and adjacent areas probably accumulated over a tectonically unstable basement. Sequences 1, 2 and 3 display evidence of accumu-lation in an extensional tectonic setting, while sequences 4, 5 and 6 experienced a northward shift of their depocentres associated with extensive erosional truncation of the marginal areas, suggesting that accumulation was affected by early stages of growth of the Covunco anticline Lotena Formation. Contained flows. Jurassic. Neuquén Basin...|$|R
40|$|Nucleation {{of silver}} {{nanoparticles}} (NPs) in Tm(3 +) doped PbO-GeO(2) (PGO) glass is reported. The {{influence of the}} heat treatment on the nucleation of silver NPs is studied by means of transmission electron microscopy and optical spectroscopy. Two heat treatment procedures were applied in order to compare their performance. Observation of infrared-to-visible frequency upconversion (UC) luminescence of Tm(3 +) ions is reported and correlated with the heat-treatment procedure. Enhancement of the UC emission for samples heat treated during various time <b>intervals</b> is <b>attributed</b> to the increased local field {{in the vicinity of}} the NPs. Quenching of the UC signal was also observed and correlated with the growth of NPs amount and size. Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPq) Fundacao de Amparo a Ciencia e Tecnologia do Estado de Pernambuco (FACEPE...|$|R
50|$|RFM (customer value) is a {{model that}} diversifies {{considerable}} customers from a mass of data by three <b>attributes.</b> <b>Interval</b> of customer consumption, frequency and payment value, are these three factors. Hughes (1994) considered that the three variables are equal in the importance; therefore, the weights of the three variables are identical. On the other hand, Stone (1995) indicated that the three variables are different in the importance due to the characteristic of industry. Thus, the weights of the three variables are not equal.|$|R
40|$|The Triassic - Jurassic {{boundary}} {{is marked}} {{by one of the}} ‘big five’ mass extinctions of the Phanerozoic. This boundary event was accompanied by several carbon cycle pert urbations, potentially induced by the opening of the Central Atlantic and associated volcanism, and accompanied by an ocean acidification event. Continuous carbonate successions covering this interval of environmental change are however rare. Here data fro m a shallow - marine equatorial mixed carbonate - siliciclastic succession is presented, that was studied on a regional scale. Four sections that are 48 km apart were examined on the Musandam Peninsula (United Arab Emirates and Sultanate of Oman). The system w as analysed for its sedimentology, vertical and lateral facies changes, and stable carbon and oxygen isotopes. Strontium isotope analysis was used to determine the position of the Triassic - Jurassic boundary horizon. The studied ramp experienced an episode of demise during the Late Triassic, followed by a restricted microbialite dominated ramp, containing large amounts of siliciclastic facies. During the Latest Triassic the diverse carbonate factory revived and flourished across the Triassic - Jurassic boundar y. No clear evidence for a biocalcification crisis or an ocean acidification event across the Triassic - Jurassic boundary is visible. Lateral facies heterogeneities can be observed across the studied <b>interval,</b> <b>attributed</b> to hydrodynamic activity, including tropical storms, crossing the extensive shelf area. Although evidence for synsedimentary tectonic activity is present, the vertical stacking pattern is largely controlled by changes in relative sea level. The refined chronostratigraphy accompanied by the d etailed environment of deposition analysis allows for a refinement of the regional palaeogeography. The neritic equatorial carbonate ramp has archived a negative carbon isotope excursion preceding the Triassic - Jurassic boundary that has also been reported from other study sites. The lack of evidence for a biocalcification crisis across the equatorial Triassic - Jurassic boundary indicates that the Tethys did not experience a distinct global acidification event...|$|R
40|$|International audienceThis paper {{addresses}} the important problem of efficiently mining numerical data with formal concept analysis (FCA). Classically, {{the only way}} to apply FCA is to binarize the data, thanks to a so-called scaling procedure. This may either involve loss of information, or produce large and dense binary data known as hard to process. In the context of gene expression data analysis, we propose and compare two FCA-based methods for mining numerical data and we show that they are equivalent. The first one relies on a particular scaling, encoding all possible <b>intervals</b> of <b>attribute</b> values, and uses standard FCA techniques. The second one relies on pattern structures without a priori transformation, and is shown to be more computationally efficient and to provide more readable results. Experiments with real-world gene expression data are discussed and give a practical basis for the comparison and evaluation of the methods...|$|R
40|$|Abstract. Attribute-based signatures, {{introduced}} by Maji et al., are signatures that prove that an authority has issued the signer “attributes” that satisfy some specified predicate. In existing attribute-based signa-ture schemes, keys are valid indefinitely once issued. In this paper, we initiate {{the study of}} incorporating time into attribute-based signatures, where a time instance is embedded in every signature, and attributes are restricted to producing signatures with times that fall in designated validity intervals. We provide three implementations that vary in granu-larity of assigning validity <b>intervals</b> to <b>attributes,</b> including a scheme in which each attribute has its own independent validity interval, a scheme in which all attributes share a common validity interval, and a scheme in which sets of <b>attributes</b> share validity <b>intervals.</b> All of our schemes provide anonymity to a signer, hide the attributes used to create the signature, and provide collusion-resistance between users...|$|R
40|$|This article proposes an {{approach}} to handle multi-attribute decision making (MADM) problems under the interval-valued intuitionistic fuzzy environment, in which both assessments of alternatives on attributes (hereafter, referred to as attribute values) and attribute weights are provided as interval-valued intuitionistic fuzzy numbers (IVIFNs). The notion of relative closeness is extended to interval values to accommodate IVIFN decision data, and fractional programming models are developed based on the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) method to determine a relative closeness <b>interval</b> where <b>attribute</b> weights are independently determined for each alternative. By employing a series of optimization models, a quadratic program is established for obtaining a unified attribute weight vector, whereby the individual IVIFN attribute values are aggregated into relative closeness intervals to the ideal solution for final ranking. An illustrative supplier selection problem is employed to demonstrate how to apply the proposed procedure...|$|R
40|$|Abstract. In {{this paper}} we {{describe}} Constraint-based <b>Attribute</b> and <b>Interval</b> Planning (CAIP), a paradigm for representing and reasoning about plans. The paradigm enables {{the description of}} planning domains with time, resources, concurrent activities, mutual exclusions among sets of activities, disjunctive preconditions and conditional effects. We provide a theoretical foundation for the paradigm, based on temporal <b>intervals</b> and <b>attributes.</b> We then show how the plans are naturally expressed by networks of constraints, and show {{that the process of}} planning maps directly to dynamic constraint reasoning. In addition, we define compatibilities, a compact mechanism for describing planning domains. We describe how this framework can incorporate the use of constraint reasoning technology to improve planning. Finally, we describe EUROPA, an implementation of the CAIP framework. 1. What Should a Planner Do? In recent years, planning has been applied to complex domains, including the sequencing of commands for spacecraft both on the ground and on-board (Jónsson et al., 2000). The domain of spacecraft operation...|$|R
40|$|Abstract—Data mining is most {{commonly}} used in attempts to induce association rules from database. Recently, some researchers have suggested the extraction of association rules as a multi-objective problem, removing some {{of the limitations of}} current approaches. In this way, we can jointly optimize quality measures which can present different degrees of tradeoff depending on the database used and the type of information can be extracted from it. In this work, we extend the well-known multi-objective evolutionary algorithms NSGA-II to perform an evolutionary learning of the <b>intervals</b> of <b>attributes</b> and a condition selection in order to mine a set of quantitative association rules with a good trade-off between interpretability and accuracy. To do that, this method considers three objectives, maximize the interestingness, comprehensibility and performance. Moreover, this method follows a database-independent approach which does not rely upon minimum support and minimum confidence thresholds. The results obtained over two real-world databases demonstrate the effectiveness of the proposed approach...|$|R
40|$|This paper {{presents}} {{an application of}} two ML models {{to the analysis of}} residential demand of water - the heterogeneity and the two-error model, both apt to model demand in presence of a kinked budget constraint. The heterogeneity model is especially suitable when the distribution is characterized by a strong clustering around the kinks. Since in practice observations can be very close, but not exactly at the kink, its application may require the definition of an interval of data around the kink, so that the observations falling inside this <b>interval</b> are <b>attributed</b> to the kink. We propose a procedure, based upon the estimates obtained from the twoerror model, to define this interval. In this application we find that the heterogeneity model allows to obtain more efficient estimates than the two-error model for the parameter of principal interest, i. e. the coefficient of the price variable. water demand, block pricing, kinked budget, constraint, maximum likelihood, discrete-continuous choice, hausman model...|$|R
40|$|This paper {{addresses}} the important problem of efficiently mining numerical data with formal concept analysis (FCA). Classically, {{the only way}} to apply FCA is to binarize the data, thanks to a so-called scaling procedure. This may either involve loss of information, or produce large and dense binary data known as hard to process. In the context of gene expression data analysis, we propose and compare two FCA-based methods for mining numerical data and we show that they are equivalent. The first one relies on a particular scaling, encoding all possible <b>intervals</b> of <b>attribute</b> values, and uses standard FCA techniques. The second one relies on pattern structures without a priori transformation, and is shown to be more computationally efficient and to provide more readable results. Experiments with real-world gene expression data are discussed and give a practical basis for the comparison and evaluation of the methods. Key words: Formal concept analysis, conceptual scaling, numerical data, pattern structures, gene expression dat...|$|R
40|$|The {{movement}} of 47 barchan dunes {{on the west}} side of Salton Sea, California, ranged from 325 to 925 feet over the 7 years between 1956 and 1963, an average of 82 feet per year. During the 15 years between 1941 and 1956, the {{movement of}} 34 of these dunes ranged between 350 and 1200 feet, an average of 50 feet per year. This difference in average movement during the two <b>intervals</b> is <b>attributed</b> primarily to increased sand supply. Movement (D) plotted against height of slip face (H) for the Salton dunes fits reasonably well with Finkel's reciprocal curve (1 /D = n + kH) or with a law of the type D = Pe^(rH). Factors other than height affect the rate of movement, one of the more important being the state of the dune, whether growing or steady state. Barchan shape, whether fat or slim, may also influence the rate of movement of growing dunes. None of these dunes needs be more than 300 years old...|$|R
40|$|Sediment {{from two}} deep {{boreholes}} (400 m) approximately 90 km apart in southern Bangladesh was analyzed by X-ray absorption spectroscopy (XAS), total chemical analyses, chemical extractions, and electron probe microanalysis {{to establish the}} importance of authigenic pyrite as a sink for arsenic in the Bengal Basin. Authigenic framboidal and massive pyrite (median values 1500 and 3200 ppm As, respectively), is the principal arsenic residence in sediment from both boreholes. Although pyrite is dominant, ferric oxyhydroxides and secondary iron phases contain a large fraction of the sediment-bound arsenic between approximately 20 and 100 m, which is the depth range of wells containing {{the greatest amount of}} dissolved arsenic. The lack of pyrite in this <b>interval</b> is <b>attributed</b> to rapid sediment deposition and a low sulfur flux from riverine and atmospheric sources. The ability of deeper aquifers (3 ̆e 150 m) to produce ground water with low dissolved arsenic in southern Bangladesh reflects adequate sulfur supplies and sufficient time to redistribute the arsenic into pyrite during diagenesis...|$|R
40|$|This paper {{presents}} {{the analysis of}} relationships among different interestingness measures of quality of association rules as first step to select the best objectives {{in order to develop}} a multi-objective algorithm. For this purpose, the discovering of association rules is based on evolutionary techniques. Specifically, a genetic algorithm has been used in order to mine quantitative association rules and determine the <b>intervals</b> on the <b>attributes</b> without discretizing the data before. The algorithm has been applied in real-word climatological datasets based on Ozone and Earthquake data. Ministerio de Ciencia y Tecnología TIN 2007 - 68084 -C- 00 Junta de Andalucía P 07 -TIC- 0261...|$|R
40|$|Discovering {{optimized}} <b>intervals</b> of numeric <b>attributes</b> {{in association}} rule mining {{has been recognized}} as an influential research problem over the last decade. There have been several stochastic optimization approaches such as evolutionary and swarm methods which try to find good intervals. One drawback of these approaches is sequential nature which requires multiple runs to find all rules. This paper presents multi agent architecture to find optimized rules simultaneously using a dynamic priority schema. The Practical Swarm Optimization (PSO) Variant is modeled and implemented in JADE framework and tested with synthetic datasets. The results confirm finding the same sequential results in parallel...|$|R
40|$|This paper {{presents}} an extension to Ant-Miner, named cAnt-Miner (Ant-Miner coping with continuous attributes), which incorporates an entropy-based discretization method {{in order to}} cope with continuous attributes during the rule construction process. By having {{the ability to create}} discrete <b>intervals</b> for continuous <b>attributes</b> "on-the-fly", cAnt-Miner does not requires a discretization method in a preprocessing step, as Ant-Miner requires. cAnt-Miner has been compared against Ant-Miner in eight public domain datasets with respect to predictive accuracy and simplicity of the discovered rules. Empirical results show that creating discrete intervals during the rule construction process facilitates the discovery of more accurate and significantly simpler classification rule...|$|R
