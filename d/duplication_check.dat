5|14|Public
30|$|Li et al. [28] have {{proposed}} DeKey [29], an efficient and reliable key management scheme for block-level deduplication. In DeKey, each client distributes the convergent key shares across multiple servers {{based on the}} ramp secret sharing scheme. Zhou et al. [61] proposed a more fine-grained key management scheme called SecDup, which mitigates the key generation overhead by exploiting hybrid deduplication policies. Li et al. [30] proposed a fine-grained deduplication mechanism based on user privileges. A client can perform a <b>duplication</b> <b>check</b> only for the files marked with matching privileges. Li et al. [28] designed a distributed reliable deduplication scheme, which can achieve data reliability and secure deduplication simultaneously by dispersing the data shares across multiple cloud servers. Chen et al. [13] proposed a novel storage-efficient deduplication scheme, called block-level message-locked encryption (BL-MLE), in which the block keys are encapsulated into the block tag to reduce metadata storage space.|$|E
40|$|In Wireless Body Area Network (WBAN), {{detection}} of fault node improves reliability and security for long range transmission. In this paper, we propose a combined approach for reliable and secured data transmission in WBAN. The proposed architecture consists of sensor nodes, relay nodes, the intermediate processing nodes and body area network (BAN) coordinator where the nodes are modeled to have direct and relay mode. The secured communication is ensured among the node and BAN coordinator by following simple protocol. The {{secure data transmission}} is proposed through authentication check, <b>duplication</b> <b>check</b> and faulty node detection. The proposed method is applicable to long ranges of transmission. It is also supporting a retransmission concept. Advancement of work to secure level checking provides a prohibition unwanted responses of WBAN and retransmission improves the probability of sending all most all data. Faulty node detection powers our security checking methodology further. By simulation results we prove that the proposed approach reduces the packet drop, energy consumption and the delay...|$|E
3000|$|In <b>Duplication</b> <b>Check,</b> SIFT {{features}} are extracted from an image, and then, the local feature regions are generated. Let I be an image captured in a robot’s workspace. Let [...] F_A = { f^(A)_ 1, [...] f^(A)_ 2, [...]..., [...] f^(A)_N} be one local feature region extracted from I, where [...] f is a feature vector {{that corresponds to}} a feature point. Similarly, let [...] F_B = { f^(B)_ 1, [...] f^(B)_ 2, [...]..., [...] f^(B)_M} be another local feature region, where N < M. To calculate the similarity between [...] F_A and [...] F_B, a feature vector [...] f^(A)_n is specified from [...] F_A and the Euclidean distances with all of feature vectors in [...] F_B are calculated. A feature vector [...] f^(B)_m with the minimum distance from [...] f^(A)_n is specified. If the distance {{is less than a}} pre-defined threshold, [...] f^(A)_n is considered to have correspondence. For all feature vectors in [...] F_A, if the number of correspondences is greater than the pre-defined threshold, the two feature regions are eliminated because they are too similar to represent an independent region.|$|E
40|$|For {{efficient}} large-scale Web crawlers, URL <b>duplication</b> <b>checking</b> is {{an important}} technique since it is a significant bottleneck. In this paper, we propose a new URL du-plication checking technique for a parallel Web crawler; we call it full-coverage two-level URL <b>duplication</b> <b>checking</b> (full-coverage- 2 L-UDC). Full-coverage- 2 L-UDC provides efficient URL <b>duplication</b> <b>checking</b> while ensuring maximum coverage. First, we propose two-level URL <b>duplication</b> <b>checking</b> (2 L-UDC). It provides efficiency in URL <b>duplication</b> <b>checking</b> by communicating at the Web site level {{rather than at the}} Web page level. Second, we present a solution for the so-called coverage problem, which {{is directly related to the}} recall of the search engine. It is the first solution for the coverage problem in the centralized parallel architecture. Third, we propose an architecture, FC 2 L-UDCbot, for a centralized parallel crawler using full-coverage- 2 L-UDC. We build a seven-agent FC 2 L-UDCbot for extensive experiments. We show that the crawling speed of FC 2 L-UDCbot is approximately proportional to the number of agents (i. e., FC 2 L-UDCbot is faster than a single-machine crawler by 6. 9 times). Full-coverage- 2 L-UDC allows FC 2 L-UDCbot to be scalable to the number of agents since it effectively deals with the overheads incurred in a parallel environment. Through an in-depth analysis, we construct a cost model for estimating the crawling speed of a scaled-up crawler. Using the model, we show that FC 2 L-UDCbot can crawl Google-scale Web pages within several days using dozens of agents...|$|R
40|$|A linear graph is a graph whose {{vertices}} {{are totally}} ordered. Biological and linguistic sequences with interactions among symbols are naturally represented as linear graphs. Examples include protein contact maps, RNA secondary structures and predicate-argument structures. Our algorithm, linear graph miner (LGM), leverages the vertex order for efficient enumeration of frequent subgraphs. Based {{on the reverse}} search principle, the pattern space is systematically traversed without expensive <b>duplication</b> <b>checking.</b> Disconnected subgraph patterns are particularly important in linear graphs due to their sequential nature. Unlike conventional graph mining algorithms detecting connected patterns only, LGM can detect disconnected patterns as well. The utility and efficiency of LGM are demonstrated in experiments on protein contact maps. Comment: This paper {{is going to be}} published in proceedings of 15 th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2011...|$|R
40|$|Most of the sequent/tableau based proof {{systems for}} the modal logic S 4 need to {{duplicate}} formulas and thus are required to adopt some method of loop checking [7, 13, 10]. In what follows we present a tableau-like proof system for S 4, based on D’Agostino and Mondadori’s classical KE [3], which is free of <b>duplication</b> and loop <b>checking.</b> The ke...|$|R
40|$|In {{regards to}} {{increase}} in use of digital information users prefer to store information in cloud system. In cloud storage system many users can store same type of data leading to data duplication causing a high utilization of bandwidth. Some techniques are proposed for making cloud more efficient and effective regarding to storage and bandwidth. In current time data de-duplication is effective technique to avoid such data duplication caused due to privileged as well as non-privileged user. To save bandwidth to transact data when replicating it offsite for disaster recovery huge organization, companies and all education institutes supports de-duplication technique. With the help of “Content hash keying ” data confidentiality is provided. Using this technique data is first encrypted and the encrypted data is outsourced to the client. To {{address the problem of}} authorized access in our proposed system de-duplication check technique is introduced and privileged access which is different from traditional data de-duplication check system. Log based approach for unauthorized data <b>duplication</b> <b>check</b> in hybrid cloud architecture is also explored. For privileged as well as for non-privileged users de-duplication can be managed using the above technique. Proposed system provides clever solution for duplication of data and also works on bandwidth efficiency...|$|E
40|$|A {{letter report}} {{issued by the}} Government Accountability Office with an {{abstract}} that begins "The Department of Agriculture's (USDA) Agricultural Research Service (ARS) and the National Institute of Food and Agriculture (NIFA) generally focus {{on many of the}} same broad topics and rely on agency safeguards, {{as well as on the}} scientific community's professional norms, to prevent inadvertent duplication of research projects within and between the agencies. Shortcomings with certain agency safeguards, however, may increase the potential risk of project duplication within or between the two agencies. ARS and NIFA built in their own safeguards to help prevent project duplication, such as (1) panels of independent external scientists who review proposed projects and (2) agency requirements for staff to ensure that proposed work is relevant, including checking the Current Research Information System (CRIS) [...] USDA's primary system containing project-level information on its ongoing and completed research projects [...] for potentially duplicative research projects in both agencies. The agencies also rely on professional norms to safeguard against duplication, such as the peer review process used by scientific journals to limit the publication of unnecessarily duplicative research. Indeed, agency officials and stakeholders could not provide recent examples of duplication within or between the two agencies, and GAO's review of 20 randomly selected research projects did not identify duplicative projects. Nevertheless, GAO identified a few shortcomings that somewhat limit the utility of certain agency safeguards. First, information in CRIS about ARS projects was typically at least 6 months out-of-date when uploaded, which undermines CRIS's utility as a safeguard. ARS officials said that the agency now expects staff to provide ARS project information on a quarterly basis, but ARS has not issued guidance about this expectation. Second, NIFA directs staff to conduct a CRIS <b>duplication</b> <b>check</b> for projects that accounted for about two-thirds of the funding it awarded for competitive grants; as a result, about one-third of its competitive grants are not subject to this safeguard against duplication. NIFA recently convened a task force to study, among other issues, whether the directive to check CRIS should be extended to all competitive grants. ...|$|E
50|$|Instead, the Visitor pattern can be applied. It encodes {{a logical}} {{operation}} {{on the whole}} hierarchy into one class containing one method per type. In the CAD example, each save function would be implemented as a separate Visitor subclass. This would remove all <b>duplication</b> of type <b>checks</b> and traversal steps. It would also make the compiler complain if a shape is omitted.|$|R
40|$|A well-designed {{implementation}} of medication concepts, records, and lists in an {{electronic medical record}} (EMR) system allows it to successfully perform many functions vital {{for the provision of}} quality health care. A controlled medication terminology provides the foundation for decision support services, such as <b>duplication</b> <b>checking,</b> allergy checking, and drug-drug interaction alerts. Clever modeling of medication records makes it easy to provide a history of any medication the patient is on and to generate the patient's medication list for any arbitrary point in time. Medication lists that distinguish between description and prescription and that are exportable in a standard format can play an essential role in medication reconciliation and contribute to the reduction of medication errors. At present, there is no general agreement on how to best implement medication concepts, records, and lists. The underlying implementation in an EMR often reflects the needs, culture, and history of both the developers and the local users. survey of a sample of medication terminologies (COSTAR Directory, the MDD, NDDF Plus, and RxNorm) and EMR implementations of medication records (OnCall, LMR, and the Benedum EMR) reveals {{the advantages and disadvantages of}} each. There is no medication system that would fit perfectly in every single context, but some features should strongly be considered in the development of any new system. (cont.) A survey of a sample of medication terminologies (COSTAR Directory, the MDD, NDDF Plus, and RxNorm) and EMR implementations of medication records (OnCall, LMR, and the Benedum EMR) reveals the advantages and disadvantages of each. There is no medication system that would fit perfectly in every single context, but some features should strongly be considered in the development of any new system. by Jaime Chang. Thesis (S. M.) [...] Harvard-MIT Division of Health Sciences and Technology, 2006. Includes bibliographical references...|$|R
40|$|The ATLAS EventIndex System, {{developed}} {{for use in}} LHC Run 2, is designed to index every processed event in ATLAS, replacing the TAG System used in Run 1. Its storage infrastructure, based on Hadoop, necessitates revamping how information in this system relates to other ATLAS systems. In addition, {{the scope of this}} new application is {{different from that of the}} TAG System. It will store fewer derived quantities, but store more indexes since the fundamental mechanisms for retrieving these indexes will be better integrated into all stages of processing, allowing more events from later stages of processing to be indexed than was possible with the previous system. Connections with other systems are fundamentally critical to assess dataset completeness, identify data <b>duplication,</b> and <b>check</b> data integrity, but also needed to enhance user and system interfaces accessing information in EventIndex. This presentation will give an overview of the ATLAS systems involved, the relevant metadata, and describe the technologies we are deploying to complete these connections...|$|R
40|$|Abstract Bug {{reports are}} {{essential}} software artifacts that describe software bugs, especially in open-source software. Lately, {{due to the}} availability {{of a large number of}} bug reports, a considerable amount of research has been carried out on bug-report analysis, such as automatically <b>checking</b> <b>duplication</b> of bug reports and localizing bugs based on bug reports. To review the work on bug-report analysis, this paper presents an exhaustive survey on the existing work on bug-report analysis. In particular, this paper first presents some background for bug reports and gives a small empirical study on the bug reports on Bugzilla to motivate the necessity for work on bug-report analysis. Then this paper summaries the existing work on bug-report analysis and points out some possible problems in working with bug-report analysis...|$|R
40|$|As {{society has}} become more reliant on electronics, the need for fault {{tolerant}} ICs has increased. This has resulted in signi cant research into both fault tolerant controller design, and mechanisms for datapath fault tolerance insertion. By treating these two issues separately, previous work has failed to address compatibility issues, as well as e cient codesign methodologies. In this paper, we present a uni ed approach to detecting control and datapath faults through the datapath, along with a method for fault identi cation and recon guration. By detecting control faults in the datapath, we avoid the area and performance overhead of detecting control faults through <b>duplication</b> or error <b>checking</b> codes. The result is a complete design methodology for self recovering architectures capable of far more e cient solutions than previous approaches. ...|$|R
40|$|Most of the sequent/tableau based proof {{systems for}} the modal logic S 4 need to {{duplicate}} formulas and thus are required to adopt some method of loop checking. In what follows we present a tableau-like proof system for S 4, based on D 2 ̆ 7 Agostino and Mondadori 2 ̆ 7 s classical KE, which is free of <b>duplication</b> and loop <b>checking.</b> The key feature of this system (let us call it KES 4) consists in its use of (i) a label formalism which models the semantics of the modal operators according to the usual conditions for S 4; and (ii) a label unification scheme which tells us when two labels "denote" the same world in the S 4 -model(s) generated {{in the course of}} proof search. Moreover, it uses special closure conditions to check models for putative contradictions...|$|R
40|$|Abstract — Crossbar nano-architectures {{based on}} selfassembled nano-structures are {{promising}} alternatives for current CMOS technology, which is facing serious challenges for further down-scaling. One {{of the major}} challenges in this nanotechnology is elevated failure rate due to atomic device sizes and inherent lack of control in self-assembly fabrication. Therefore, high permanent and transient failure rates lead to multiple faults during lifetime operation of crossbar nano architectures. In this paper, we present a concurrent multiple error detection scheme for multistage crossbar nano-architectures based on dual-rail implementations of logic functions. We prove the detectability of all single faults {{as well as most}} classes of multiple faults in this scheme. Based on statistical multiple fault injection, we compare the proposed technique with other online error detection and masking techniques such as Triple Module Redundancy (TMR), <b>duplication,</b> and parity <b>checking,</b> in terms of fault coverage as well as area and delay overhead. I...|$|R
40|$|Introduction Most of the sequent/tableau based proof {{systems for}} the modal logic S 4 need to {{duplicate}} formulas and thus are required to adopt some method of loop checking [7, 13, 10]. In what follows we present a tableau-like proof system for S 4, based on D'Agostino and Mondadori's classical KE [3], which is free of <b>duplication</b> and loop <b>checking.</b> The key feature of this system (let us call it KES 4) consists in its use of (i) a label formalism which models the semantics of the modal operators according to the usual conditions for S 4; and (ii) a label unification scheme which tells us when two labels "denote" the same world in the S 4 -model(s) generated {{in the course of}} proof search. Moreover, it uses special closure conditions to check models for putative contradictions. 2 Label Formalism Let #C = 1, w 2,... be a non empty set of constant world symbols, and let # V = {W 1, W 2,... be a non empty set of variable world symbols. The set is now defined as follows...|$|R
40|$|As the {{semiconductor}} roadmap reaches smaller feature sizes {{and the end}} of Dennard Scaling, design goals change, and managing the power envelope often dominates delay minimization. Voltage scaling remains a powerful tool to reduce energy. We find that it results in about 60 % geomean energy reduction on top of other common low-energy optimizations with 22 nm CMOS technology. However, when voltage is reduced, it becomes easier for noise and particle strikes to upset a node, potentially causing Silent Data Corruption (SDC). The 60 % energy reduction, therefore, comes with a significant drop in reliability. <b>Duplication</b> with <b>checking</b> and triple-modular redundancy are traditional approaches used to combat transient errors, but spending 2 – 3 x the energy for redundant computation can diminish or reverse the benefits of voltage scaling. As an alternative, we explore the opportunity to use checking operations that are cheaper than the base computation they are guarding. We devise a classification system for applications and their lightweight checking characteristics. In particular, we identify and evaluate the effectiveness of lightweight checks in a broad set of common tasks in scientific computing and signal processing. We find that the lightweight checks cost {{only a fraction of the}} base computation (0 - 25 %) and allow us to recover the reliability losses from voltage scaling. Overall, we show about 50 % net energy reduction without compromising reliability compared to operation at the nominal voltage. We use FPGAs (Field-Programmable Gate Arrays) in our work, although the same ideas can be applied to different systems. On top of voltage scaling, we explore other common low-energy techniques for FPGAs: transmission gates, gate boosting, power gating, low-leakage (high-Vth) processes, and dual-V dd architectures. ^ We do not scale voltage for memories, so lower voltages help us reduce logic and interconnect energy, but not memory energy. At lower voltages, memories become dominant, and we get diminishing returns from continuing to scale voltage. To ensure that memories do not become a bottleneck, we also design an energy-robust FPGA memory architecture, which attempts to minimize communication energy due to mismatches between application and architecture. We do this alongside application parallelism tuning. We show our techniques on a wide range of applications, including a large real-time system used for Wide-Area Motion Imaging (WAMI). ...|$|R
40|$|Arc-consistency {{algorithms}} are the workhorse of many backtrack algorithms. Most {{research on}} arc-consistency algorithms {{is focusing on}} the design of algorithms that are optimal when it comes to worst case scenarios. This report will provide experimental evidence that, despite common belief to the contrary, the ability to deal efficiently with such worst case scenarios may not be a prerequisite for solving quickly. It will compare on the one hand AC- 2001, which has an optimal worst case time-complexity and is considered efficient, and on the other AC- 3 d, which is not optimal when it comes to its worst case time-complexity, but which has a better space-complexity than AC- 2001. Both algorithms will be compared for MAC search and for stand alone arc-consistency (the task of making a single CSP arc-consistent). For stand alone arc-consistency AC- 3 d is the better algorithm when it comes to time but there is no clear winner when it comes to minimising the number of checks. For search the results are more interesting. MAC- 2001 is by far the better algorithm when it comes to minimising the number of checks. However, MAC- 3 d is considerably faster on average. For difficult random problems, that took between minutes and 1. 5 hour to solve, MAC- 3 d was about 1. 5 times faster on average than MAC- 2001. As soon as MAC- 2001 starts to become successful in avoiding the <b>duplication</b> of many <b>checks</b> it begins to invest much more additional solution time. These observations suggest that being worst case optimal may come at a price of being less efficient on average in search and that algorithms like MAC- 3 d are promising. Contents...|$|R
40|$|In this thesis, {{we present}} {{techniques}} and algorithms for analysis and synthesis of synchronous Boolean and multiple-valued networks. Synchronous Boolean and multiple-valued networks are a discrete-space discrete-time model of gene regulatory networks. Their cycle of states, called attractors, {{are believed to}} give a good indication of the possible functional modes of the system. This motivates research on algorithms for finding attractors. Existing decision diagram-based approaches have limited capacity due to the excessive memory requirements of decision diagrams. Simulation-based approaches {{can be applied to}} large networks, however, their results are incomplete. In {{the first part of this}} thesis, we present an algorithm, which uses a SAT-based bounded model checking approach to find all attractors in a multiple-valued network. The efficiency of the presented algorithm is evaluated by analysing 30 network models of real biological processes as well as 35000 randomly generated 4 -valued networks. The results show that our algorithm has a potential to handle an order of magnitude larger models than currently possible. One of the characteristic features of genetic regulatory networks is their inherent robustness, that is, their ability to retain functionality in spite of the introduction of random faults. In the second part of this thesis, we focus on the robustness of a special kind of Boolean networks called Balanced Boolean Networks (BBNs). We formalize the notion of robustness and introduce a method to construct BBNs for 2 -singleton attractors Boolean networks. The experiment results show that BBNs are capable of tolerating single stuck-at faults. Our method improves the robustness of random Boolean networks by at least 13 % on average, and in some special case, up to 61 %. In the third part of this thesis, we focus on a special type of synchronous Boolean networks, namely Feedback Shift Registers (FSRs). FSR-based filter generators are used as a basic building block in many cryptographic systems, e. g. stream ciphers. Filter generators are popular because their well-defined mathematical description enables a detailed formal security analysis. We show how to modify a filter generator into a nonlinear FSR, which is faster, but slightly larger, than the original filter generator. For example, the propagation delay can be reduced 1. 54 times at the expense of 1. 27 % extra area. The presented method might be important for applications, which require very high data rates, e. g. 5 G mobile communication technology. In the fourth part of this thesis, we present a new method for detecting and correcting transient faults in FSRs based on <b>duplication</b> and parity <b>checking.</b> Periodic fault detection of functional circuits is very important for cryptographic systems because a random hardware fault can compromise their security. The presented method is more reliable than Triple Modular Redundancy (TMR) for large FSRs, while the area overhead of the two approaches are comparable. The presented approach might be important for cryptographic systems using large FSRs. QC 20151120 </p...|$|R

