39|742|Public
5000|$|... the Damerau-Levenshtein {{distance}} allows insertion, <b>deletion,</b> <b>substitution,</b> and the transposition of two adjacent characters; ...|$|E
50|$|The flight {{instrumentation}} system {{is composed of}} a primary instrumentation package and an auxiliary package. The primary package instrumentation measures those parameters critical to all engine static firings and subsequent vehicle launches. These include some 70 parameters such as pressures, temperatures, flows, speeds, and valve positions for the engine components, with the capability of transmitting signals to a ground recording system or a telemetry system, or both. The {{instrumentation system}} is designed for use throughout {{the life of the}} engine, from the first static acceptance firing to its ultimate vehicle flight. The auxiliary package is designed for use during early vehicle flights. It may be deleted from the basic engine instrumentation system after the propulsion system has established its reliability during research and development vehicle flights. It contains sufficient flexibility to provide for <b>deletion,</b> <b>substitution,</b> or addition of parameters deemed necessary as a result of additional testing. Eventual deletion of the auxiliary package will not interfere with the measurement capability of the primary package.|$|E
40|$|The aim of {{this paper}} is to handle the errors, due to insertion, <b>deletion,</b> <b>substitution,</b> letter {{sequencing}} and typing in the lexical analysis phase of compiler. Fuzzy keywords, their fuzzy regular expressions and minimized fuzzy deterministic automaton are constructed. The issue of membership of fuzzy keyword is successfully tackled with the help of an algorithm. Full implementation of fuzzy lexical analyzer is also described...|$|E
50|$|Since DNA {{frequently}} undergoes insertions, <b>deletions,</b> <b>substitutions,</b> and transpositions, {{and each}} of these operations occurs on approximately the same timescale, the Damerau-Levenshtein distance is an appropriate metric of the variation between two strands of DNA. More common in DNA, protein, and other bioinformatics related alignment tasks {{is the use of}} closely related algorithms such as Needleman-Wunsch algorithm or Smith-Waterman algorithm.|$|R
40|$|This {{software}} {{comes with}} ABSOLUTELY NO WARRANTY. It is subject {{only to the}} terms of the ML-Yacc NOTICE, LICENSE, and DISCLAIMER (in the file COPYRIGHT distributed with this software). New in this version: Improved error correction directive %change that allows multi-token insertions, <b>deletions,</b> <b>substitutions.</b> Explanation of how to build a parser (Section 5) and the Calc example (Section 7) revised for SML/NJ Version 110 and the use of CM...|$|R
40|$|This work explores {{methods of}} automat-ically {{detecting}} corrections of individual mistakes in sentence revisions for ESL students. We have trained a classifier {{that specializes in}} determining whether consecutive basic-edits (word insertions, <b>deletions,</b> <b>substitutions)</b> address the same mistake. Experimental result shows that the proposed system achieves an F 1 -score of 81 % on correction detection and 66 % for the overall system, out-performing the baseline by a large margin. ...|$|R
40|$|SV 40 DNA is cleaved by the Eco RII and Hae {{restriction}} endonucleases to {{give rise}} to two different se t s of 16 frag-ments each. These fragments have been ordered by analysis of the products of redigestlon of one set of fragments with another restriction enzyme. The c leavage s i t e s have been precisely mapped based on length measurements of the pro-duct s of each c leavage. This provides a convenient group of small DNA fragments suitable for sequence analysts In-vestigation of the transcripts present In Infected c e l l s, or construction of <b>deletion</b> <b>substitution</b> variants of the virus...|$|E
40|$|Abstract. Studies using {{deletion}} mutagenesis {{indicate that}} a processing recognition site lies proximal to the normal cleavage position between gin 32 and se # 3 of pre-ornithine carbamyl transferase (pOeT). pOCT cDNA was manipulated to delete codons specifying amino acids 22 - 30 of the signal sequence. The mutant precursor, designated pOCTA 22 - 30, was imported to the matrix compartment by purified mitochondria, but remained largely unprocessed; {{the low level of}} processing that was observed did not involve the normal cleavage site. Several manipulations performed down-stream of the normal pOC'r processing site (<b>deletion,</b> <b>substitution,</b> and hybrid protein constructions) affected neither import nor correct processing. Our data suggest (a) that domains specifying impor...|$|E
40|$|Graph edit {{distance}} {{measures the}} distance between two graphs {{as the number of}} elementary operations (vertex/edge insertion, <b>deletion,</b> <b>substitution)</b> required to transform the first graph into the second one. Such a distance allows to define a metric between graphs and has many applications in the structural pattern recognition framework. However, the complexity of the computation of this distance is exponential in the size of both graphs to be compared. In this technical report, we focus our attention on applications where families of graphs to be considered have a finite set of structures. We then investigate under which relationships between the costs of the different elementary operations, such a priori knowledge may be used to pre compute most of the optimal edit path between any two graphs...|$|E
5000|$|We {{can store}} {{the number of}} insertions, <b>deletions,</b> and <b>substitutions</b> separately, or even the {{positions}} at which they occur, which is always [...]|$|R
40|$|Error {{correction}} of noisy reads obtained from high-throughput DNA sequencers {{is an important}} problem since read quality significantly affects downstream analyses such as detection of genetic variation and the complexity and success of sequence assembly. Most of the current error correction algorithms are only capable of recovering substitution errors. In this work, Pindel, an algorithm that simultaneously corrects insertion, <b>deletion</b> and <b>substitution</b> errors in reads from next generation DNA sequencing platforms is presented. Pindel corrects insertion, <b>deletion</b> and <b>substitution</b> errors by modelling the sequencer output as emissions of an appropriately defined Hidden Markov Model (HMM). Reads are corrected to the corresponding maximum likelihood paths using an appropriately modified Viterbi algorithm. When compared with Karect and Fiona, the top two current algorithms capable of correcting insertion, <b>deletion</b> and <b>substitution</b> errors, Pindel exhibits superior accuracy {{across a range of}} datasets...|$|R
5000|$|We {{can give}} {{different}} penalty costs to insertion, <b>deletion</b> and <b>substitution.</b> We can also give penalty costs {{that depend on}} which characters are inserted, deleted or substituted.|$|R
40|$|This paper {{examined}} the segmental phonology of 25 purposively sampled bilingual Nigerian adult Broca’s aphasics from a Nigerian teaching hospital. Data {{were collected by}} tape-recording the speech {{of each of the}} aphasic. The data were analysed perceptually, complemented with frequency count and simple percentage. The way the subjects ranked constraints were then examined, using Optimality Theory. Three main forms of deviation were noticed in their speech: <b>deletion,</b> <b>substitution</b> and epenthesis, with substitution having the highest frequency, followed by deletion. The deviation affected consonants more than vowels. Plosives and alveolars were more affected by the brain damage than any other sound. Some of the effects of Nigerian English on the speech of the subjects were still retained after the brain damage. The Broca’s aphasics sampled ranked constraints {{in such a way that}} markedness dominated faithfulness...|$|E
40|$|This {{paper is}} {{an inquiry into}} the {{translation}} strategies that Nayla Naguib used in rendering the famous Egyptian drama "The Jailer and the Prisoner" into English. While translation strategies are often employed to maintain an acceptable level of faithfulness to the target text, the present translator seems to have applied some translation strategies: <b>deletion,</b> <b>substitution,</b> and addition {{in such a way}} that distorted the meaning of the original text and brought about false impressions about the original text. The paper shows that the inclusions of such strategies by the translator has added less clarity to the expressions in question and concealed the cultural element to a high degree. The paper concludes by asserting the importance of context and pragmatic situation in bringing about better understanding of the text and appropriate use of strategies...|$|E
40|$|This thesis {{addresses}} {{the problem of}} Recognizing Textual Entailment (i. e. recognizing that {{the meaning of a}} text entails the meaning of another text) using a Tree Edit Distance algorithm between the syntactic trees of the two texts. A key aspect of the approach is the estimation of the cost for the edit-ing operations (i. e. Insertion, <b>Deletion,</b> <b>Substitution)</b> among words. Our aim is to compare the contribution of different resources providing entail-ment rules, including lexical rules from WordNet and the UniAlberta the-saurus, and syntactic rules automatically acquired by the Dirt and TEASE systems. We carried out a number of experiments over the PASCAL-RTE dataset in order to estimate the contribution of different combinations of the available resources. In addition, we have developed and evaluated an Answer Vali-dation module for Question Answering and a Relation Extraction system...|$|E
30|$|If IND 1 {{is used as}} a {{reference}} sequence, JF 926558, JF 926556 and IND 3 haplotypes differ by one substitution at base positions 306, 534 and 553 respectively. Haplotypes JF 926562 and JF 926557 showed three and six substitutions respectively. The remaining five haplotypes are more diverse showing JF 926561 one <b>deletion</b> and 10 <b>substitutions,</b> CM 8 two <b>deletions</b> and 34 <b>substitutions,</b> JF 926560 three <b>deletions</b> and 34 <b>substitutions,</b> JF 926559 three <b>deletions</b> and 39 <b>substitutions</b> and CMJ 1 two deletions, one insertion and 28 substitutions (Additional file 1 : Table S 1). Haplotypes CM 8 and JF 926560 are separated from JF 926559 by the same five substitutions, in the base positions 261, 266, 267, 270, 272; only one deletion was detected between CM 8 and JF 926560.|$|R
50|$|The Damerau-Levenshtein {{distance}} {{differs from}} the classical Levenshtein distance by including transpositions among its allowable operations {{in addition to the}} three classical single-character edit operations (insertions, <b>deletions</b> and <b>substitutions).</b>|$|R
40|$|The Walking Tree Method [3, 4, 5, 18] is an {{approximate}} string alignment method {{that can handle}} insertions, <b>deletions,</b> <b>substitutions,</b> translocations, {{and more than one}} level of inversions all together. Moreover, it tends to highlight gene locations, and helps discover unknown genes. Its recent improvements in runtime and space use extends its capability in exploring large strings. We will briefly describe the Walking Tree Method with its recent improvements [18], and demonstrate its speed and ability to align real complete genomes such as Borrelia burgdorferi (910724 base pairs of its single chromosome) and Chlamydia trachomatis (1042519 base pairs) in reasonable time, and to locate and verify genes. 1...|$|R
40|$|The {{sieve element}} {{occlusion}} (SEO) gene family includes several members that are expressed specifically in immature sieve elements (SEs) {{in the developing}} phloem of dicotyledonous plants. To determine how this restricted expression profile is achieved, we analysed the SE-specific Medicago truncatula SEO-F 1 promoter (PMtSEO-F 1) by constructing <b>deletion,</b> <b>substitution</b> and hybrid constructs and testing them in transgenic tobacco plants using green fluorescent protein as a reporter. This revealed four promoter regions, each containing cis-regulatory elements that activate transcription in SEs. One of these segments also contained sufficient information to suppress PMtSEO-F 1 transcription in the phloem companion cells (CCs). Subsequent in silico analysis revealed several candidate cis-regulatory elements that PMtSEO-F 1 shares with other SEO promoters. These putative sieve element boxes (PSE boxes) are promising candidates for cis-regulatory elements controlling the SE-specific expression of PMtSEO-F 1...|$|E
40|$|Studies using {{deletion}} mutagenesis {{indicate that}} a processing recognition site lies proximal to the normal cleavage position between gln 32 and ser 33 of pre-ornithine carbamyl transferase (pOCT). pOCT cDNA was manipulated to delete codons specifying amino acids 22 - 30 of the signal sequence. The mutant precursor, designated pOCT delta 22 - 30, was imported to the matrix compartment by purified mitochondria, but remained largely unprocessed; {{the low level of}} processing that was observed did not involve the normal cleavage site. Several manipulations performed downstream of the normal pOCT processing site (<b>deletion,</b> <b>substitution,</b> and hybrid protein constructions) affected neither import nor correct processing. Our data suggest that domains specifying import and processing site recognition may be functionally segregated within the signal peptide; that processing is not requisite for import of pOCT; and that a proximal region, not involving the normal signal peptide cleavage site, is required for processing site recognition...|$|E
40|$|Vibrio vulnificus is an {{etiological}} agent causing serious systemic {{infections in}} the immunocompromised humans or cultured eels. This species commonly produces a hemolytic toxin {{consisting of the}} cytolysin domain and the lectin-like domain. For hemolysis, the lectin-like domain specifically binds to cholesterol in the erythrocyte membrane, and to form a hollow oligomer, the toxin is subsequently assembled on the membrane. The cytolysin domain {{is essential for the}} process to form the oligomer. Three-dimensional structure model revealed that two domains connected linearly and the C-terminus was located near to the joint of the domains. Insertion of amino acid residues between two domains was found to cause inactivation of the toxin. In the C-terminus, <b>deletion,</b> <b>substitution</b> or addition of an amino acid residue also elicited reduction of the activity. However, the cholesterol-binding ability was not affected by the mutations. These results suggest that mutation of the C- or N-terminus of the lectin-like domain may result in blockage of the toxin assembly...|$|E
5000|$|The minimum string {{distance}} (MSD) is {{the number}} of [...] "primitives" [...] which {{is the number}} of insertions, <b>deletions,</b> or <b>substitutions</b> to transform one string into another. The following equation was found for the MSD Error Rate ...|$|R
3000|$|..., i.e. {{the minimum}} number of {{character}} insertions, <b>deletions</b> and <b>substitutions</b> needed to transform one string into the other. Note that edit distance is well defined on larger alphabet and variable length strings. The scheme can be extended to these cases.|$|R
40|$|ABSTRACT: Normal {{spontaneous}} speech {{is characterized by}} hesitation (silent pauses, filled pauses and prolongations) and self-repairs (repetitions, <b>deletions,</b> <b>substitutions</b> and insertions). The timings of self-repairs have been investigated to explain the processes involved in self-monitoring and self-repair in the speech production process. Analyses of timings from the onset of an error to the interruption point (error-to-cut-off), and from the interruption point to the onset of a self-repair (cut-off-to-repair) found in the speech of 67 callers to a radio programme, support pre-articulatory monitoring of speech (Levelt, 1983, 1989). There {{also seems to be}} evidence of the planning of self-repairs taking place before the interruption point as indicated by a large number of cut-off-to-repair intervals of 0 ms...|$|R
40|$|Abstract. The Burrows-Wheeler Transform is a {{building}} block for many text compression applications and self-index data structures. It reorders {{the letters of}} a text T to obtain a new text bwt(T) which can be better compressed. This forward transform has been intensively studied over the years, but a major problem still remains: bwt(T) has to be entirely recomputed whenever T is modified. In this article, we are considering standard edit operations (insertion, <b>deletion,</b> <b>substitution</b> of a letter or a factor) that are transforming a text T into T ′. We are studying {{the impact of these}} edit operations on bwt(T) and are presenting an algorithm that converts bwt(T) into bwt(T ′). Moreover, we show that we can use this algorithm for converting the suffix array of T into the suffix array of T ′. Even if the theoretical worst-case time complexity is O(|T |), the experiments we conducted indicate that it performs really well in practice. ...|$|E
40|$|The {{speech of}} aphasics {{is a kind}} of {{language}} in which constraints are ranked differently from what obtains in the speech of non-aphasics. Bilingual aphasics rank constraints in ways that reflect that they have more than one language in their language faculty. Therefore, this paper examined the manner in which constraints are ranked among Nigerian adult aphasics, using 40 purposively sampled Yoruba-English aphasics from the University College Hospital, Ibadan, Nigeria. A normative text was given these subjects to read, and their speeches were tape-recorded. The data were analyzed perceptually. The frequency and percentage of each form of deviation noticed in their speeches were calculated. Optimality Theory was then used to explicate the way constraints were ranked by these subjects before those forms of deviation emerged as the optimal candidates. Three forms of deviation were discovered at the segmental level of their phonology, namely <b>deletion,</b> <b>substitution</b> and epenthesis. Generally, the subjects ranked constraints in ways opposite those of non-aphasics. Markedness dominates faithfulness in all their rankings...|$|E
40|$|The Burrows-Wheeler Transform is a {{building}} block for many text compression applications and self-index data structures. It reorders {{the letters of}} a text T to obtain a new text bwt(T) which can be better compressed. This forward transform has been intensively studied over the years, but a major problem still remains: bwt(T) has to be entirely recomputed whenever T is modified. In this article, we are considering standard edit operations (insertion, <b>deletion,</b> <b>substitution</b> of a letter or a factor) that are transforming a text T into T ′. We are studying {{the impact of these}} edit operations on bwt(T) and are presenting an algorithm that converts bwt(T) into bwt(T ′). Moreover, we show that we can use this algorithm for converting the suffix array of T into the suffix array of T ′. Even if the theoretical worst-case time complexity is O(|T |), the experiments we conducted indicate that it performs really well in practice...|$|E
40|$|The aim of {{the current}} study is to {{amplification}} the regularity region encompassed the mutations changing the Myostatin gene express and also to determine sequential analysis. The blood samples taken from 12 Najdi Cattles in Najdi Cattle station in Shushtar city, Khuzestan province. Then the DNA extracted to amplification 730 bp and 561 bp fragments. Sequencing was performed after the precision in PCR products put-upon Agarose gel 1 % and Myostatin gene pro-motor was determined. In Myostatin gene’s pro-motor region, three mutations, Insertion, <b>Deletion</b> and <b>Substitution</b> in 561 bp fragment and the two mutations, <b>Deletion</b> and <b>Substitution</b> in 730 bp were appeared, results pointed out. Reported mutations on boxes and pro-motor components did not influence upon Myostatin gene...|$|R
30|$|PER is the {{industry}} standard for measuring the accuracy of acoustic models and is calculated by the the Levenshtein distance [35] where the number of insertions, <b>deletions,</b> and <b>substitutions</b> are added and divided {{by the total number}} of phonetic units in the string.|$|R
40|$|In {{this paper}} we propose a {{multi-step}} system for the semiautomatic detection and annotation of disfluencies in spoken corpora. A set of rules, statistical models and machine learning techniques are applied to the input, which is a transcription aligned to the speech signal. The system uses the results of an automatic estimation of prosodic, part-of-speech and shallow syntactic features. We present a detailed coding scheme for simple disfluencies (filled pauses, mispronunciations, false starts, drawls and intra-word pauses), structured disfluencies (repetitions, <b>deletions,</b> <b>substitutions,</b> insertions) and complex disfluencies. The system is trained and evaluated on a transcribed corpus of spontaneous French speech, consisting of 112 different speakers and balanced for speaker age and sex, covering 14 different varieties of French spoken in Belgium, France and Switzerland...|$|R
40|$|Motivation: The {{evolution}} of viruses is very rapid {{and in addition}} to local point mutations (insertion, <b>deletion,</b> <b>substitution)</b> it also includes frequent recombinations, genome rearrangements, and horizontal transfer of genetic materials (HGTS). Evolutionary analysis of viral sequences is therefore a complicated matter for two main reasons: First, due to HGTs and recombinations, the right model of evolution is a network and not a tree. Second, due to genome rearrangements, an alignment of the input sequences is not guaranteed. These facts encourage developing methods for inferring phylogenetic networks that do not require aligned sequences as input. Results: In this work we present the first computational approach which deals with both genome rearrangements and horizontal gene transfers and does not require a multiple alignment as input. We formalize a new set of computational problems which involve analyzing such complex models of evolution. We investigate their computational complexity, and devise algorithms for solving them. Moreover, we demonstrate the viability of our methods on several synthetic datasets as well as four biological datasets. Availability: The code is available from the authors upon request. Contact...|$|E
40|$|Pattern-based {{synthesis}} {{has drawn}} wide interest from researchers {{who tried to}} utilize the regularity in applications for design optimizations. In this paper we present a general pattern-based behavior synthesis framework which can efficiently extract similar structures in programs. Our approach is very scalable in benefit of advanced pruning techniques that include locality sensitive hashing and characteristic vectors. The similarity of structures is captured by a mismatch-tolerant metric: graph edit distance. The edit distance between two graphs is the minimum number of vertex/edge insertion, <b>deletion,</b> <b>substitution</b> operations to transform one graph into the other. Graph edit distance can naturally handle various program variations such as bit-width, structure, and port variations. In addition, we apply our pattern-based synthesis system to FPGA resource optimization with the observation that multiplexors are particularly expensive on FPGA platforms. Considering knowledge of discovered patterns, the resource binding step can intelligently generate the data-path to reduce interconnect costs. Experiments show our approach can, on average, reduce the total area by about 20 % with 7 % latency overhead on the Xilinx Virtex- 4 FPGAs, compared to the traditional behavior synthesis flow...|$|E
40|$|AbstractWe {{present a}} four-stage {{algorithm}} that updates the Burrows–Wheeler Transform {{of a text}} T, when this text is modified. The Burrows–Wheeler Transform is used by many text compression applications and some self-index data structures. It operates by reordering the letters of a text T to obtain a new text bwt(T) which can be better compressed. Even though recent advances are offering this structure new applications, a major bottleneck still exists: bwt(T) has to be entirely reconstructed from scratch whenever T is modified. We study how standard edit operations (insertion, <b>deletion,</b> <b>substitution</b> of a letter or a factor) that transform a text T into T′ impact bwt(T). Then we present an algorithm that directly converts bwt(T) into bwt(T′). Based on this algorithm, we also sketch a method for converting the suffix array of T into the suffix array of T′. We finally show, based on the experiments we conducted, that this algorithm, whose worst-case time complexity is O(|T|log|T|(1 +logσ/loglog|T|)), performs really well in practice and replaces advantageously the traditional approach...|$|E
40|$|Political {{scientists}} {{often find}} themselves tracking amendments to political texts. As different actors weigh in, texts change as they are drafted and redrafted, reflecting political preferences and power. This study provides a novel solution to the prob- lem of detecting amendments to political text based upon minimum edit distances. We demonstrate the usefulness of two language-insensitive, transparent, and efficient minimum-edit-distance algorithms suited for the task. These algorithms are capable of providing {{an account of the}} types (insertions, <b>deletions,</b> <b>substitutions,</b> and trans- positions) and substantive amount of amendments made between version of texts. To illustrate the usefulness and efficiency of the approach we replicate two existing stud- ies from the field of legislative studies. Our results demonstrate that minimum edit distance methods can produce superior measures of text amendments to hand-coded efforts in a fraction of the time and resource costs...|$|R
40|$|Abstract. The task of {{approximate}} {{string matching}} {{is to find}} all locations at which a pattern string p of length m matches a substring of a text string t of length n with at most k differences. It is common to use Levenshtein distance [5], which allows the differences to be single-character insertions, <b>deletions,</b> <b>substitutions.</b> Recently, in [3], the IndelMYE, IndelWM and IndelBYN algorithms where introduced as {{modified version of the}} bit-parallel algorithms of Myers [6], Wu&Manber [10] and Baeza-Yates&Navarro [1], respectively. These modified versions where made to support the indel distance (only single-character insertions and/or deletions are allowed). In this paper we present an improved version of IndelMYE that makes a better use of the bit-operations and runs 24. 5 percent faster in practice. In the end we present a complete set of experimental results to support our findings. ...|$|R
40|$|The {{tremendous}} {{growth in}} digital imagery {{is driving the}} need for more sophisticated methods for automatic image analysis, cataloging, and searching. We present a method for classifying and querying images based on the spatial orderings of regions or objects using composite region templates (CRTs). The CRTs capture the spatial information statistically and provide a robust way to measure similarity in the presence of region insertions, <b>deletions,</b> <b>substitutions,</b> replications and relocations. The CRTs can be used for classifying and annotating images by assigning symbols to the regions or objects and by extracting symbol strings from spatial scans of the images. The symbol strings can be decoded using a library of annotated CRTs to automatically label and classify the images. The CRTs can also be used for searching bysketch or example by measuring image similarity based on relative counts of the CRTs...|$|R
