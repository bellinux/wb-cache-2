13|29|Public
50|$|Here {{the aim is}} to {{link between}} an unknown genome and the {{concealed}} identity of the <b>data</b> <b>originator</b> by accumulating quasi-identifiers − residual pieces of information that are embedded in the dataset − and to gradually narrow down the possible individuals that match the combination of these quasi-identifiers.|$|E
50|$|The {{originator}} of {{the data}} to be transmitted is provided in the TSR. The Link 22 system ensures that this <b>Data</b> <b>Originator</b> Identification is delivered along with the data, so that any unit receiving it knows which unit originated the data regardless of its route through the system.|$|E
50|$|Some of {{the main}} {{principles}} are that data and metadata should not be managed centrally, but by the <b>data</b> <b>originator</b> and/or owner, and that tools and services connect via computer networks to the various sources. A GIS is often the platform for deploying an individual node within an SDI. To achieve these objectives, good coordination between all the actors is necessary and the definition of standards is very important.|$|E
50|$|Swift Ref, {{the global}} payment {{reference}} data utility, is SWIFT’s unique reference data service. Swift Ref sources data direct from <b>data</b> <b>originators,</b> including central banks, code issuers and banks {{making it easy}} for issuers and <b>originators</b> to maintain <b>data</b> regularly and thoroughly. SWIFTRef constantly validates and cross-checks data across the different data sets.|$|R
40|$|We are {{entering}} the era of ubiquitous genetic information for research, clinical care, and personal curiosity. Sharing these datasets is vital for rapid progress in understanding the genetic basis of human diseases. However, one growing concern {{is the ability to}} protect the genetic privacy of the <b>data</b> <b>originators.</b> Here, we technically map threats to genetic privacy and discuss potential mitigation strategies for privacy-preserving dissemination of genetic data. Comment: Draft for comment...|$|R
40|$|With {{the growing}} {{complexity}} of data acquisition and processing methods, {{there is an}} increasing demand in understanding which data is outdated and how to have it as fresh as possible. Staleness {{is one of the}} key, time-related, data quality characteristics, that represents a degree of synchronization between <b>data</b> <b>originators</b> and information systems possessing the data. However, nowadays there is no common and pervasive notion of data staleness, as well as methods for its measurement in a wide scope of applications. Our work provides a definition of a data-driven notion of staleness for information systems with frequently updatable data. For such a data, we demonstrate an efficient exponential smoothing method of staleness measurement, compared to naïve approaches, using the same limited amount of memory, based on averaging of frequency of updates. We present experimental results of staleness measurement algorithms that we run on history of updates of articles from Wikipedia...|$|R
50|$|The S-63 {{standard}} secures data by encrypting {{the basic}} transfer database using the Blowfish algorithm, SHA-1-hashing the data {{based on a}} random key and adding a CRC32 check. The standard also defines the systems to develop permit files that are delivered to end-users of ENC data enabling them to decrypt the data and use it for navigation. It also defines the use of DSA format signatures to authenticate the <b>data</b> <b>originator,</b> however because of poor implementation of the standard by ECDIS hardware manufacturers, virtually all signing is performed centrally by the IHO which acts as the scheme administrator. Exceptions to this are a few smaller resellers such as AUSRenc operated by AHS.|$|E
50|$|The Network of Networks {{strategy}} proposed within OCJR {{recommended the}} creation of specific networks based upon these Communities of Interest which were joined together through data interchange gateways supporting common standards. Under this approach networks would be arranged by data type and business functions such as Criminal Justice, Health and Social Care, Defence and Intelligence or Public Finance rather than solely on established departmental boundaries.Within a COI, trust relationships and data interchange are readily supported, enabling data sharing without a need to cross network boundaries and providing benefits of scale without the challenges and compromises intrinsic to homogeneous cross sector networks. Data is made available without a need to transport it between organisations and control is retained by the <b>data</b> <b>originator.</b>|$|E
40|$|A simple {{technique}} {{is presented to}} reduce power consumption in wireless sensor networks. This {{technique is}} based on reducing data size and sending smaller size packets which will in turn result in power savings. Target applications are assumed to involve environments where variations in data are typically continuous in time. Experimental analyses show that the energy saving can be up to 50 % if the proposed technique is employed. Further, energy saving grows linearly {{with the number of}} hops between the source and sink nodes. In a sensor network tiny sensor nodes have a capability to sense and process sensed data, and perform communication among them [1, 2]. Typically these nodes are deployed in a large sensor field. Each node in a sensor network plays the dual role of a <b>data</b> <b>originator</b> and data router. Sensor nodes are prone to failure, and therefore the sensor network topology changes frequently. If a few nodes die, this may caus...|$|E
50|$|This project aims {{to provide}} a web service {{permitting}} to retrieve validated datasets (temperature, oxygen, salinity, nutrients, etc.) from 45 different National Data Centers of 35 countries having coasts along European seas. Therefore SeaDataNet is a standardized system for managing the large and diverse data sets collected by the oceanographic fleets and the automatic observation systems. Additional objectives consist in creating product with aggregated data such as climatological descriptions. This European funded project has started in 2004, the project is currently in its second phase with fundings for 2012 to 2016. Most of the datasets are free of access, but some are restricted to institutes. In term of harmonization SeaDataNet has chosen standards, vocabularies, tools {{that are used in}} the different NODC(National Oceanographic Data Center). For example they use Ocean Data View to validate or visualize datasets, they also use DIVA software to perform objective analysis. Datasets are covering the years 1800 up to 2012. In 2012 400 <b>data</b> <b>originators</b> are registered into Seadatanet project.|$|R
50|$|Following LCM Partners’ {{consultations}} it {{was decided}} that the European Data Warehouse would receive the loan-level <b>data</b> from the <b>originators</b> and would then check it for compliance against the ECB templates.|$|R
50|$|Furthermore, the CSDB is able {{to support}} {{technical}} authors or operators with a checking environment.The checks result in standardized files like CSV or plain text, to ensure an easy transfer to the originator of the checked <b>data.</b> The <b>originator</b> may be the technical author during the authoring process or in case of international data exchange in accordance with ASD/AIA S1000D, an industrial partner respective a sub-contractor who delivers documentation.|$|R
40|$|Abstract — Corporate IT {{as well as}} {{individuals}} show increasing interest in reliable outsourcing of storage infras-tructure. Decentralized solutions with their resilience against partial outages {{are among the most}} attractive approaches. Irrespective of the form of the relationship, be it based on a contract or on the more flexible cooperative model, the problem of verifying whether someone promising to store one’s data actually does so remains to be solved, especially in the presence of multiple replicas. In this paper, we introduce a lightweight mechanism that allows the <b>data</b> <b>originator</b> or a dedicated verification agent to build up trust in the replica holder by means of protocols that do not require prior trust or key establishment. We show how naive versions of the protocol do not prevent cheating, and then strengthen it by adding means that make it economically attractive to be honest. This provides a foundation for further work in providing trustworthy distributed storage. I...|$|E
40|$|Abstract. Use of {{intermediary}} hosts as stepping {{stones to}} conceal tracks {{is common in}} Internet misuse. It is therefore desirable to find a method to detect whether the originating party is using an intermediary host. Such a detection technique would allow the activation {{of a number of}} countermeasures that would neutralize the effects of misuse, and make it easier to trace a perpetrator. This work explores a new approach in de-termining if a host communicating via TCP is the <b>data</b> <b>originator</b> or if it is acting as a mere TCP proxy. The approach is based on measuring the inter packet arrival time at the receiving end of the connection only, and correlating the observed results with the network latency between the receiver and the proxy. The results presented here indicate that deter-mining the use of a proxy host is possible, if the network latency between the originator and proxy is larger than the network latency between the proxy and the receiver. We show that this technique has potential to be used to detect connections were data is sent through a TCP proxy, such as remote login through TCP proxies, or rejecting spam sent through a bot network...|$|E
40|$|This {{inquiry is}} {{prompted}} by the unprecedented policy of the European Medicines Agency that enables the disclosure of clinical trial reports submitted for drug marketing authorization, effective as of January 1, 2015. It addresses the question whether such practice is {{in compliance with the}} international standard of clinical data protection under Article 39. 3 of the TRIPS Agreement. Most scholarly and policy debate regarding this provision analyzes whether it precludes the referential use of data to facilitate the approval of a generic drug. Rather than focusing on a particular use, this Article seeks to identify the principle underlying the protection obligation by which the legitimacy of “use X” can be evaluated. In doing so, it interprets the provision from literal, historical and teleological perspectives, and it analyzes a peculiar overlap between three legal regimes: unfair competition, trade secret, and sui generis data protection. The proposed principle allows avoidance of situations where, due to the ambiguous notion of unfair commercial use, the protection of data under the TRIPS Agreement can be stretched indefinitely. With regard to data disclosure for experimental use, it is argued that the protection obligation under 39. 3 TRIPS does not justify monopoly type protection of clinical trial data, neither does it require the reservation of experimental use exclusively for the <b>data</b> <b>originator,</b> even if such use can have commercial benefits for competitors...|$|E
40|$|Many network {{applications}} (such as swarming downloads, peer-to-peer {{video streaming}} and file sharing) are {{made possible by}} using large groups of peers to distribute and process data. Securing data in such a system requires not just <b>data</b> <b>originators,</b> but also those “distributors, ” to enforce access control, verify integrity, or make other content-specific security decisions for the replicated or adapted data. In this paper, we introduce the concepts of cooperative policy enforcement and request type checking, and propose an implementation framework Q which uses these approaches to secure data in peer-to-peer systems. The Q framework associates every data object with relocatable policy descriptors which distributors can use {{to determine whether a}} request for that object should be granted and whether a data transfer meets a request. With minimal changes to the application or the framework, Q can define and enforce arbitrarily sophisticated policies across a wide range of applications. Policies can be written to work across applications, or to include application-specific criteria and behavior. We will also discuss integrating Q with several peer-to-peer applications, including Gnutella, distributed hash tables such as CAN and Chord, peer-to-peer video streaming, HTTP swarming and application-level routing...|$|R
40|$|A model which {{integrates}} particulate {{primary production}} over time (light-day) and depth (euphonic layer) was used extensively for calculations {{of the primary}} production results of the Fladen Ground Experiment (FLEX' 76), and is described here in detail. The photosynthesis-light relationship occupies a central position in the computational scheme. The choice of an adequate relationship (here, {{a variant of the}} Vollenweider's (1965) expression), the properties of such a formula, and the relevant parameters are discussed in the paper. The proposed approach is useful whenever specific problems imply a reduction to elemental processes, as it was the case during the Fladen Ground Experiment. Thus, the difficulty of integrating primary production over a heterogeneous light field (depth and time) and a heterogeneous biomass field (depth) can be easily overcome. Also, comparability problems arising from differences in presentation of incubation results provided by various <b>data</b> <b>originators</b> can be minimized. The calculations for RV "Meteor", station 446, 23 May 1976, are given as an example, and the results of several simulation scenarios are compared. In the author's view, the calculation of primary production is here merely a side issue since the real goal of this approach is to work with parameters which have an ecological meaning. Only such parameters could be included in larger phytoplankton or ecosystem models...|$|R
40|$|The European Union set {{the ambitious}} {{objective}} to reach within 2020 {{the goal of}} Good Environmental Status. The European Commission (2008) represents the legislative framework that drives member state efforts to reach it. The Integrated Maritime Policy supported {{the need to provide}} a European knowledge base able to drive sustainable development by launching in 2009 a new European Marine Observation and Data Network (EMODnet). Through a stepwise approach, EMODnet Chemistry aims to provide high-quality marine environmental data and related products at the scale of regions and sub-regions defined by the Marine Strategy Framework Directive. The chemistry lot takes advantage and further develops the SeaDataNet pan-European infrastructure and the distributed approach, linking together a network of more than 100 National Oceanographic Data Centres providing data from more than 500 <b>data</b> <b>originators.</b> The close interaction with EEA, RSCs, ICES and EMODnet–MSFD coordination group facilitated the identification of the most appropriate set of information required for the MSFD process. EMODnet Chemistry provides aggregated and validated regional data collections for nutrients, dissolved gasses, chlorophyll, and contaminants, properly visualized with OGC WMS and WPS viewing services. Concentration maps with 10 -year moving window from 1960 to 2014, by season and for selected vertical layers, are computed and made available...|$|R
40|$|Abstract. A mixed-criticality {{system is}} one that must meet {{multiple}} assurance requirements, in particular safety and security requirements. Ideally, such a system could be constructed with components that have been individually certified to meet safety or security requirements relevant to their function. The challenge is that the components need {{to communicate with each}} other. In this case, the safety and security properties of a certified component can be affected by changes in other components that it is linked to. In this paper we propose a method to alleviate this problem, by moving from a component-interaction model to a data-centric model. A data-centric system is based on definitions and characterization of the data that flows through it, which are in turn used as a foundation for individual components. For instance, in an airplane, airspeed could be safety-critical data, while images from an on-board camera could be security-critical data, and aircraft position could be both safety and security-critical. Once the data is defined, system components and their functions are defined as well. Each component has to specify what data it requires and produces, and align its safety and security properties with the data it produces. With this approach, components are decoupled from each other – they are <b>data</b> <b>originator</b> agnostic, as long as data is delivered on time and meets the assurance requirements. This paper outlines a data-centric approach for modular assurance, and a specific instantiation of it, based on the RTI implementation of the Object Management Group (OMG) Data Distribution Services (DDS) [3, 4] standard middleware, which defines and disseminates data, and the Wind River VxWorks MILS platform [2], which ensures component separation...|$|E
40|$|In October 2000, the {{personnel}} responsible for {{administration of the}} corporate computers managed by the Scientific Computing Department assembled to reengineer {{the process of creating}} and deleting users' computer accounts. Using the Carnegie Mellon Software Engineering Institute (SEI) Capability Maturity Model (CMM) for quality improvement process, the team performed the reengineering by way of process modeling, defining and measuring the maturity of the processes, per SEI and CMM practices. The computers residing in the classified environment are bound by security requirements of the Secure Classified Network (SCN) Security Plan. These security requirements delimited the scope of the project, specifically mandating validation of all user accounts on the central corporate computer systems. System administrators, in addition to their assigned responsibilities, were spending valuable hours performing the additional tacit responsibility of tracking user accountability for user-generated data. For example, in cases where the <b>data</b> <b>originator</b> was no longer an employee, the administrators were forced to spend considerable time and effort determining the appropriate management personnel to assume ownership or disposition of the former owner's data files. In order to prevent this sort of problem from occurring and to have a defined procedure {{in the event of an}} anomaly, the computer account management procedure was thoroughly reengineered, as detailed in this document. An automated procedure is now in place that is initiated and supplied data by central corporate processes certifying the integrity, timeliness and authentication of account holders and their management. Automated scripts identify when an account is about to expire, to preempt the problem of data becoming ''orphaned'' without a responsible ''owner'' on the system. The automated account-management procedure currently operates on and provides a standard process for all of the computers maintained by the Scientific Computing Department...|$|E
40|$|This thesis {{addresses}} {{the problem of}} traffic transfer in wireless sensor networks (WSN). In such networks, the foremost challenge {{in the design of}} data communication techniques is that the sensor's transceiver circuitry consumes the major portion of the available power. Thus, due to stringent limitations on the nodes' hardware and power resources in WSN, data transmission must be power-efficient {{in order to reduce the}} nodes' power consumption, and hence to maximize the network lifetime while satisfying the required data rate. The transmit power is itself under the influence of data rate and source-destination distance. Thanks to the dense deployment of nodes in WSN, multi-hop communication can be applied to mitigate the transmit power for sending bits of information, i. e., gathered data by the sensor nodes to the destination node (gateway) compared to single-hop scenarios. In our approach, we achieve a reasonable trade-off between power-efficiency and transmission data rate by devising cooperative communication strategies through which the network traffic (i. e. nodes' gathered information) is relayed hop-by-hop to the gateway. In such strategies, the sensor nodes serve as <b>data</b> <b>originator</b> as well as data router, and assist the data transfer from the sensors to the gateway. We develop several data transmission schemes, and we prove their capability in transmitting the data from the sensor nodes at the highest possible rates allowed by the network limitations. In particular, we consider that (i) network has linear or quasi-linear topology, (ii) nodes are equipped with half-duplex radios, implying that they cannot transmit and receive simultaneously, (iii) nodes transmit their traffic at the same average rate. We compute the average data rate corresponding to each proposed strategy. Next, we take an information-theoretic approach and derive an upper bound to the achievable rate of traffic transfer in the networks under consideration, and analyze its tightness. We show that our proposed strategies outperform the conventional multi-hop scheme, and their average achievable rate approaches the upper bound at low levels of signal to noise ratio...|$|E
40|$|A well documented, {{publicly}} available, {{global data}} set of surface ocean carbon dioxide (CO 2) parameters {{has been called}} for by international groups for nearly two decades. The Surface Ocean CO 2 Atlas (SOCAT) project was initiated by the international marine carbon science community in 2007 {{with the aim of}} providing a comprehensive, publicly available, regularly updated, global data set of marine surface CO 2, which had been subject to quality control (QC). Many additional CO 2 data, not yet made public via the Carbon Dioxide Information Analysis Center (CDIAC), were retrieved from <b>data</b> <b>originators,</b> public websites and other data centres. All data were put in a uniform format following a strict protocol. Quality control was carried out according to clearly defined criteria. Regional specialists performed the quality control, using state-of-the-art web-based tools, specially developed for accomplishing this global team effort. SOCAT version 1. 5 was made public in September 2011 and holds 6. 3 million quality controlled surface CO 2 data points from the global oceans and coastal seas, spanning four decades (1968 – 2007). Three types of data products are available: individual cruise files, a merged complete data set and gridded products. With the rapid expansion of marine CO 2 data collection and the importance of quantifying net global oceanic CO 2 uptake and its changes, sustained data synthesis and data access are prioritie...|$|R
40|$|The {{ability to}} measure the use and impact of {{published}} data sets {{is key to the}} success of the open data / open science paradigm. A direct measure of impact would require tracking data (re) use in the wild, which however is difficult to achieve. This is therefore commonly replaced by simpler metrics based on data download and citation counts. In this paper we describe a scenario where it is possible to track the trajectory of a dataset after its publication, and we show how this enables the design of accurate models for ascribing credit to <b>data</b> <b>originators.</b> A <b>Data</b> Trajectory (DT) is a graph that encodes knowledge of how, by whom, and in which context data has been re-used, possibly after several generations. We provide a theoretical model of DTs that is grounded in the W 3 C PROV data model for provenance, and we show how DTs can be used to automatically propagate a fraction of the credit associated with transitively derived datasets, back to original data contributors. We also show this model of transitive credit in action by means of a Data Reuse Simulator. Ultimately, our hope is that, in the longer term, credit models based on direct measures of data reuse will provide further incentives to data publication. We conclude by outlining a research agenda to address the hard questions of creating, collecting, and using DTs systematically across a large number of data reuse instances, in the wild...|$|R
40|$|The GEOTRACES Intermediate Data Product 2014 (IDP 2014) is {{the first}} {{publicly}} available data product of the international GEOTRACES programme, and contains data measured and quality controlled {{before the end of}} 2013. It consists of two parts: (1) a compilation of digital data for more than 200 trace elements and isotopes (TEIs) as well as classical hydrographic parameters, and (2) the eGEOTRACES Electronic Atlas providing a strongly inter-linked on-line atlas including more than 300 section plots and 90 animated 3 D scenes. The IDP 2014 covers the Atlantic, Arctic, and Indian oceans, exhibiting highest data density in the Atlantic. The TEI data in the IDP 2014 are quality controlled by careful assessment of intercalibration results and multi-laboratory data comparisons at cross-over stations. The digital data are provided in several formats, including ASCII spreadsheet, Excel spreadsheet, netCDF, and Ocean Data View collection. In addition to the actual data values the IDP 2014 also contains data quality flags and 1 -? data error values where available. Quality flags and error values are useful for data filtering. Metadata about <b>data</b> <b>originators,</b> analytical methods and original publications related to the data are linked to the data in an easily accessible way. The eGEOTRACES Electronic Atlas is the visual representation of the IDP 2014 data providing section plots and a new kind of animated 3 D scenes. The basin-wide 3 D scenes allow for viewing of data from many cruises at the same time, thereby providing quick overviews of large-scale tracer distributions. In addition, the 3 D scenes provide geographical and bathymetric context that is crucial for the interpretation and assessment of observed tracer plumes, as well as for making inferences about controlling processes...|$|R
40|$|Every {{organisation}} {{needs to}} exchange and disseminate data constantly amongst its employees, members, customers and partners. Disseminated data is often sensitive or confidential {{and access to}} it should be restricted to authorised recipients. Several enterprise rights management (ERM) systems and data protection solutions have been proposed by both academia and industry to enable usage control on disseminated data, i. e. to allow <b>data</b> <b>originators</b> to retain control over whom accesses their information, under which circumstances, {{and how it is}} used. This is often obtained by means of cryptographic techniques and thus by disseminating encrypted data that only trustworthy recipients can decrypt. Most of these solutions assume data recipients are connected to the network and able to contact remote policy evaluation authorities that can evaluate usage control policies and issue decryption keys. This assumption oversimplifies the problem by neglecting situations where connectivity is not available, as often happens in crisis management scenarios. In such situations, recipients {{may not be able to}} access the information they have received. Also, while using data, recipients and their applications can create new derived information, either by aggregating data from several sources or transforming the original data’s content or format. Existing solutions mostly neglect this problem and do not allow originators to retain control over this derived data despite the fact that it may be more sensitive or valuable than the data originally disseminated. In this thesis we propose an ERM architecture that caters for both derived data control and usage control in partially disconnected networks. We propose the use of a novel policy lattice model based on information flow and mandatory access control. Sets of policies controlling the usage of data can be specified and ordered in a lattice according to the level of protection they provide. At the same time, their association with specific data objects is mandated by rules (content verification procedures) defined in a data sharing agreement (DSA) stipulated amongst the organisations sharing information. When data is transformed, the new policies associated with it are automatically determined depending on the transformation used and the policies currently associated with the input data. The solution we propose takes into account transformations that can both increase or reduce the sensitivity of information, thus giving originators a flexible means to control their data and its derivations. When data must be disseminated in disconnected environments, the movement of users and the ad hoc connections they establish can be exploited to distribute information. To allow users to decrypt disseminated data without contacting remote evaluation authorities, we integrate our architecture with a mechanism for authority devolution, so that users moving in the disconnected area can be granted the right to evaluate policies and issue decryption keys. This allows recipients to contact any nearby user that is also a policy evaluation authority to obtain decryption keys. The mechanism has been shown to be efficient so that timely access to data is possible despite the lack of connectivity. Prototypes of the proposed solutions that protect XML documents have been developed. A realistic crisis management scenario has been used to show both the flexibility of the presented approach for derived data control and the efficiency of the authority devolution solution when handling data dissemination in simulated partially disconnected networks. While existing systems do not offer any means to control derived data and only offer partial solutions to the problem of lack of connectivity (e. g. by caching decryption keys), we have defined a set of solutions that help <b>data</b> <b>originators</b> faced with the shortcomings of current proposals to control their data in innovative, problem-oriented ways. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Ongoing {{zooplankton}} {{research at}} the Plymouth Marine Laboratory has established a time series of zooplankton species since 1988 at L 4, a coastal station off Plymouth. Samples were collected by vertical net hauls (WP 2 net, mesh 200 µm; UNESCO 1968) from the sea floor (approximately 50 m) {{to the surface and}} stored in 4 % formalin. Much of the zooplankton analysis has been to the level of "major taxonomic groups" only, and a number of different analysts have participated over the years. The level of expertise has generally been consistent, but the user should be aware that levels of taxonomic discrimination may vary {{during the course of the}} dataset. The dominant calanoid copepods are generally well discriminated to species throughout. Calanus has not been routinely examined for species determination, the assumption being that the local population is entirely composed of Calanus helgolandicus. In certain years there has been a particular interest in Temora stylifera, Centropages cherchiae and other species reflected in the dataset. The lack of records in other previous years does not necessarily reflect species absence. We view it as essential for all users of L 4 plankton data to establish and maintain contact with the nominated current <b>data</b> <b>originators</b> as well as fully consulting the metadata. While not impinging on free data access, this ensures that this large, species-rich but slightly complex species database is being used in the correct way, and any potential issues with the data are clarified. Furthermore, a proper dialogue with these local experts on the time series will enable where appropriate the most recent sampling timepoints to be used. The data can be downloaded from BODC or from doi: 10. 1594 /PANGAEA. 778092 as files for each year by searching for "L 4 zooplankton". The most comprehensive dataset is the version downloadable directly from this page. The entire set of zooplankton samples is stored at the Plymouth Marine Laboratory in buffered formalin, and may be available for further taxonomic analysis on request...|$|R
40|$|This study {{comprises}} part of {{a research}} program to investigate the degree of correlation attainable between flight test measured airplane drag levels and the full scale drag that would be predicted {{on the basis of}} wind tunnel <b>data.</b> "Issued by <b>originator</b> as Lockheed-Georgia Co. report no. ER- 10153. ""NASA CR- 1558. ""June 1970 " [...] Cover. Includes bibliographical references (p. 53 - 54). This study comprises {{part of a}} research program to investigate the degree of correlation attainable between flight test measured airplane drag levels and the full scale drag that would be predicted on the basis of wind tunnel data. Prepared by Lockheed-Georgia Company, Marietta, Ga., under Contract no. Mode of access: Internet...|$|R
40|$|Wireless sensor {{networks}} are an emerging technology that has recently gained attention for their potential use in many applications such disaster management, combat field reconnaissance, border protection, object localization, harbors, coal mines, and so on. Sensors in {{these kind of}} applications {{are expected to be}} remotely deployed and to operate autonomously in unattended environments. Since sensors typically operate on batteries and are often deployed in harsh environment where human operators cannot access them easily, much of the research on wireless sensor networks has focused on the energy depletion in order to achieve energy efficiency to extend the network lifetime. In multihop wireless networks that are often characterized by many to one traffic patterns, it is very common to find problems related to energy depletion. Along the network, sensors experiment different traffic intensities and energy depletion rates. Usually, the sensors near the sink tend to deplete their energy sooner because they act as <b>data</b> <b>originators</b> and <b>data</b> relayers and are required to forward a large amount of traffic of the most remote sensors to the sink while the sensors located in the periphery of the network remain much of the time inactive. Therefore, these sensors located close to the sink tend to die early, leaving areas of the network completely disconnected from the sink reducing the functional network lifetime. In order to achieve equal power consumption at different levels of our network, we have decided to add extra relay nodes to reduce and balance the traffic load that normal nodes have to carry. As mentioned above, each level within the network faces a different amount of traffic, which becomes more intense as we approach the interior levels. This behavior causes that the external nodes, with less traffic to handle, stay more time at rest while the nodes in the inner rings face a great amount of traffic which forces them to be more active, generating a more accelerated exhaustion, reason why nodes located in the inner rings exhaust its battery faster causing the lifetime of the network to come to an end. This work presents a comprehensive analysis on the maximum achievable sensor network lifetime for different deployment strategies (linear, quadratic, and exponential) in order to equalize the energy consumption rates of all nodes. More specifically the deployment of extra relay nodes around the sink in order to solve the energy imbalanced problem and guarantee that all nodes have balanced energy consumption and die almost at the same time...|$|R
40|$|Désirée Caselli, 1 Simone Cesaro, 2 Maurizio Aricò 1 1 Medical Department, Pediatric Unit, Azienda Sanitaria Provinciale Ragusa, Ragusa, 2 Department of Pediatrics, Pediatric Hematology Oncology, Azienda Ospedaliera Universitaria Integrata, Verona, Italy Abstract: Advances in {{chemotherapy}} and surgery allows {{the majority of}} patients to survive cancer diseases. Yet, the price may be a proportion of patients dying of complications due to treatment-induced infectious complications, such as neutropenia. With the aim of decreasing morbidity and mortality related to infectious complications, recombinant human granulocyte colony-stimulating factor (G-CSF), filgrastim, and pegylated filgrastim {{have been used to}} reduce time and degree of neutropenia. A biosimilar is a copy of an approved original biologic medicine whose data protection has expired. The patent for filgrastim expired in Europe in 2006 and in the US in 2013. This review analyses the available evidence to be considered in order to design a strategy of use of G-CSF and its biosimilars. The clinical and safety outcomes of biosimilars are well within the range of historically reported <b>data</b> for <b>originator</b> filgrastim. This underscores the clinical effectiveness and safety of biosimilar filgrastim in daily clinical practice. Biosimilars can play an important role by offering the opportunity to reduce costs, thus contributing to the financial sustainability of treatment programs. Keywords: neutropenia, filgrastim, biosimilars, G-CSF, fever, prophylaxi...|$|R
40|$|The thesis {{begins with}} {{liability}} definition, its types (civil, criminal, administrative) and their particular occurences in legal protection of intelectual property on Internet. Then it groups internet plant participants into some categories (users, ISP, definition authorities) and analyzes the liability {{issue from the}} point of view of each single category. The same chapter explores participant's liability for internet plant and <b>data</b> content, whose <b>originator</b> is someone else, and for ISP role it analyzes its limitation incorporated in legal institute called safe harbor. It describes safe harbor from historical and international perspective and demonstrates it on US and German legislatures. Lastly this chapter compares legal regulations of safe harbour in various countries and adjudicates Czech transposition of its EU framework. As a practical application this theses deals with P 2 P networks theme (chiefly with BitTorrent) and in the conclusion it proposes legislative changes (de lege ferenda) constiting mainly in objective liability for IP adress...|$|R
40|$|OBJECTIVE: To {{analyze the}} effect of multiple-source drug entry on price {{competition}} after patent expiration in the pharmaceutical industry. <b>DATA</b> SOURCES: <b>Originators</b> and their multiple-source drugs selected from the 35 chemical entities whose patents expired from 1984 through 1987. Data were obtained from various primary and secondary sources for the patents' expiration dates, sales volume and units sold, and characteristics of drugs in the sample markets. STUDY DESIGN: The {{study was designed to}} determine significant factors using the study model developed under the assumption that the off-patented market is an imperfectly segmented market. PRINCIPAL FINDINGS: After patent expiration, the originators' prices continued to increase, while the price of multiple-source drugs decreased significantly over time. By the fourth year after patent expiration, originators' sales had decreased 12 percent in dollars and 30 percent in quantity. Multiple-source drugs increased their sales twofold in dollars and threefold in quantity, and possessed about one-fourth (in dollars) and half (in quantity) of the total market three years after entry. CONCLUSION: After patent expiration, multiple-source drugs compete largely with other multiple-source drugs in the price-sensitive sector, but indirectly with the originator in the price-insensitive sector. Originators have first-mover advantages, and therefore have a market that is less price sensitive after multiple-source drugs enter. On the other hand, multiple-source drugs target the price-sensitive sector, using their lower-priced drugs. This trend may indicate that the off-patented market is imperfectly segmented between the price-sensitive and insensitive sector. Consumers as a whole can gain from the entry of multiple-source drugs because the average price of the market continually declines after patent expiration...|$|R
40|$|The Cassini Archive Tracking System (CATS) is a {{computer}} program that enables tracking of scientific <b>data</b> transfers from <b>originators</b> to the Planetary Data System (PDS) archives. Without CATS, there is no systematic means of locating products in the archive process or ensuring their completeness. By keeping a database of transfer communications and status, CATS enables the Cassini Project and the PDS to efficiently and accurately report on archive status. More importantly, problem areas are easily identified through customized reports that can be generated on the fly from any Web-enabled computer. A Web-browser interface and clearly defined authorization scheme provide safe distributed access to the system, where users can perform functions such as create customized reports, record a transfer, and respond to a transfer. CATS ensures that Cassini provides complete science archives to the PDS on schedule and that those archives are available to the science community by the PDS. The three-tier architecture is loosely coupled and designed for simple adaptation to multimission use. Written in the Java programming language, it is portable and can be run on any Java-enabled Web server...|$|R
40|$|Since the {{adoption}} of the WTO-TRIPS Agreement in 1994, there has been significant controversy over the impact of pharmaceutical patent protection on the access to medicines in the developing world. In addition to the market exclusivity provided by patents, the pharmaceutical industry has also sought to further extend their monopolies by advocating the need for additional ‘regulatory’ protection for new medicines, known as data exclusivity. Data exclusivity limits the use of clinical trial data that need to be submitted to the regulatory authorities before a new drug can enter the market. For a specified period, generic competitors cannot apply for regulatory approval for equivalent drugs relying on the <b>originator's</b> <b>data.</b> As a consequence, data exclusivity lengthens the monopoly for the original drug, impairing the availability of generic drugs. This article illustrates how the pharmaceutical industry has convinced the US and the EU to impose data exclusivity on their trade partners, many of them developing countries. The key arguments formulated by the pharmaceutical industry in favor of adopting data exclusivity and their underlying ethical assumptions are described in this article, analyzed, and found to be unconvincing. Contrary to industry's arguments, it is unlikely that data exclusivity will promote innovation, especially in developing countries. Moreover, the industry's appeal to a property rights claim over clinical test data and the idea that data exclusivity can prevent the generic competitors from ‘free-riding’ encounters some important problems: Neither legitimize excluding all others...|$|R
40|$|Abstract. WEB-IS, Web-based Integrated System, allows remote, {{interactive}} {{visualization of}} large-scale 3 -D data over the Internet, along with data analysis and data mining capabilities. In this paper, {{we discuss the}} overall view for WEB-IS. We have developed three sub-modules in WEB-IS: WEB-IS 1 allows users to navigate through their rendered 3 -D data and interactively analyze the data for statistics or apply data mining techniques, such as cluster analysis. WEB-IS 2 allows user to manipulate Amira (a powerful 3 -D visualization package that has been employed recently by the science and engineering communities to gain insight into their data) controls remotely and to analyze, render and view large datasets through the internet. WEB-IS 3 is an imaging service that displays selected features from a lowresolution environment to one with increased resolution by zooming into the data. In the near future, we propose to integrate the three components together through the middleware, called NaradaBrokering (iNtegrated Asynchronous Real-time Adaptive Distributed Architecture, a distributed messaging infrastructure {{that can be used}} to intelligently route <b>data</b> between the <b>originators</b> and registered consumers), without regard for time or location. NaradaBrokering, as the middleware, makes WEB-IS more scalable and fault-tolerant. WEB-IS uses a combination of Java, C++ and NaradaBrokering to seamlessly integrate the server-side processing and user interaction utilities on the client. The server takes care of the processor intensive tasks, such as visualization and data mining, and sends the resulting image buffer or statistic result to middleware over the Internet. And a Java applet on the client provides the front-end interface: respond...|$|R
40|$|Abstract. WEB-IS, Web-based Integrated System, allows remote, {{interactive}} {{visualization of}} large-scale 3 -D data over the Internet, along with data analysis and data mining. In this paper, {{we discuss the}} overall structure of WEB-IS. Up until now we have developed three sub-modules geared towards geophysical problems. WEB-IS 1 allows geoscientists to navigate through their 3 -D geophysical data, such as seismic structures or numerical simulations, and interactively analyze the statistics or apply data-mining techniques, such has cluster analysis. WEB-IS 2 lets a user control Amira (a powerful 3 -D visualization package) remotely and analyze, render and view large datasets across the Internet. WEB-IS 3 is an imaging service that enables the user to control the scale of features to view through interactive zooming. In the near future, we propose to integrate the three components together through a middleware framework, called NaradaBrokering (iNtegrated Asynchronous Real-time Adaptive Distributed Architecture, a distributed messaging infrastructure {{that can be used}} to intelligently route <b>data</b> between the <b>originators</b> and regis-tered consumers), without regard for time or location. As a result, WEB-IS becomes more scalable and fault-tolerant. WEB-IS uses a combination of Java, C++,, and through the use of NaradaBrokering will seamlessly integrate the server-side processing and user interaction utilities on the client. The server takes care of the processor intensive tasks, such as visualization and data mining, and sends either the resulting bitmap image or statistical results to the middleware across the Internet for viewing. WEB-IS is an easy-to-use service, which will eventually help geoscientists collaborate from different sites in a natural manner. It wil...|$|R
40|$|Thesis (MBA) [...] Stellenbosch University, 2011. This study {{analyses}} {{the effects}} of generic medicine competition on the market share growth and pricing of originator brand medicine in the South African private pharmaceutical market. The study is based on five years (2005 to 2011) of IMS Health market share <b>data</b> for 39 <b>originator</b> brand drugs that {{have been exposed to}} competition from generic substitutes from 2001. The results show that, for all the drug molecules included in the study pooled together, the price of an originator brand medicine relative to the weighted average price of its generics has a significant negative impact on the change of its market share. Results for the molecules pooled according to anatomical classes, as well as each molecule separately, show that in four out of the nine classes represented in the study and nine out of the 39 molecules the relative price of the originator brand medicine had a significant negative impact on its change in market share. The manufacturers and marketers of generic medicines would be well advised to offer their medicines at significantly discounted prices compared to the originator brands, as the results suggest that the market penetration of the generic product may depend heavily on the price the generics are offered at. Investigations into the prices of the originator brands in relation with the number of generic equivalents in the market show that the number of generics available in a specific market has a significant positive impact on the relative price of originators, thereby making originators relatively more expensive compared with their generic competitors, {{while at the same time}} the results show that the absolute price of the originator brand medicines declines as the number of generic equivalents in the market increases. This indicates that, from a policy perspective, reducing the barriers to entry for generic medicine once originator patents expire may have a significant role to play in reducing the cost of pharmaceutical drugs in the South African market...|$|R
