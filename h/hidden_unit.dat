213|1026|Public
25|$|The {{need for}} deep {{learning}} with real-valued inputs, as in Gaussian restricted Boltzmann machines, {{led to the}} spike-and-slab RBM (ssRBM), which models continuous-valued inputs with strictly binary latent variables. Similar to basic RBMs and its variants, a spike-and-slab RBM is a bipartite graph, while like GRBMs, the visible units (input) are real-valued. The difference is in the hidden layer, where each <b>hidden</b> <b>unit</b> has a binary spike variable and a real-valued slab variable. A spike is a discrete probability mass at zero, while a slab is a density over continuous domain; their mixture forms a prior.|$|E
50|$|Sparsity may be {{achieved}} by additional terms in the loss function during training (by comparing the probability distribution of the <b>hidden</b> <b>unit</b> activations with some low desired value), or by manually zeroing all but the few strongest <b>hidden</b> <b>unit</b> activations (referred to as a k-sparse autoencoder).|$|E
5000|$|Since the RBM has {{the shape}} of a bipartite graph, with no intra-layer connections, the <b>hidden</b> <b>unit</b> activations are {{mutually}} independent given the visible unit activations and conversely, the visible unit activations are mutually independent given the <b>hidden</b> <b>unit</b> activations. That is, for [...] visible units and [...] hidden units, the conditional probability of a configuration of the visible units , given a configuration of the hidden units , is ...|$|E
40|$|The <b>hidden</b> <b>units</b> in {{multi-layer}} perceptrons {{are believed}} to act as feature extractors. In other words, the outputs of the <b>hidden</b> <b>units</b> represent the features in a more traditional statistical classification paradigm. This viewpoint offers a statistical, objective approach to determining the optimal number of <b>hidden</b> <b>units</b> required. This approach {{is based on an}} F-ratio test, and proceeds in an iterative fashion. The method and its application to simulated time-series data are presented...|$|R
50|$|By {{imposing}} sparsity on the <b>hidden</b> <b>units</b> {{during training}} (whilst having {{a larger number}} of <b>hidden</b> <b>units</b> than inputs), an autoencoder can learn useful structures in the input data. This allows sparse representations of inputs. These are useful in pretraining for classification tasks.|$|R
40|$|For neural {{networks}} {{with a wide}} class of weight priors, it can be shown that {{in the limit of}} an infinite number of <b>hidden</b> <b>units,</b> the prior over functions tends to a gaussian process. In this article, analytic forms are derived for the covariance function of the gaussian processes corresponding to networks with sigmoidal and gaussian <b>hidden</b> <b>units.</b> This allows predictions to be made efficiently using networks with an infinite number of <b>hidden</b> <b>units</b> and shows, somewhat paradoxically, that it may be easier to carry out Bayesian prediction with infinite networks rather than finite ones...|$|R
5000|$|SD Gundam G Generation MONOEYE GUNDAMS (first G Generation {{to include}} {{original}} characters {{with their own}} unique storyline. Also the first G Generation to include Gundam SEED {{in the form of}} an early conceptual design of the Strike Gundam, placed in the game as a <b>hidden</b> <b>unit.)</b> ...|$|E
5000|$|The {{standard}} type of RBM has binary-valued (Boolean/Bernoulli) hidden and visible units, {{and consists of}} a matrix of weights [...] (size m√ón) associated with the connection between <b>hidden</b> <b>unit</b> [...] and visible unit , as well as bias weights (offsets) [...] for the visible units and [...] for the hidden units. Given these, the energy of a configuration (pair of boolean vectors) [...] is defined as ...|$|E
5000|$|A Swedish {{court found}} Asian Tribune Editor KT Rajasingham and the World Institute for Asian Studies guilty of [...] "gross defamation". Norway {{journalist}} Nadarajah Sethurupan, who filed the lawsuit, claimed that Rajasingham {{called him and}} asked him to work with his secret unit to attack the Sri Lanka peace process. When Sethurupan refused to join, the <b>hidden</b> <b>unit</b> started attacking him as a [...] "terrorist" [...] on the Asian Tribune website. The attacks continued from 2005 to 2008.|$|E
40|$|This paper {{demonstrates}} that a skeletal {{structure of a}} network emerges when independent noises {{are added to the}} inputs of the <b>hidden</b> <b>units</b> of Multilayer Perceptron during the learning by error backpropagation. By analyzing the average behavior of the error backpropagation algorithm to such noises, it is shown that the weights from the <b>hidden</b> <b>units</b> to the output units tend to get smaller and the outputs of the <b>hidden</b> <b>units</b> tend to be 0 or 1. Such tendency have been demonstrated by experiments of learning of pattern classification problem. I...|$|R
40|$|There is a {{widespread}} misconception that the delta-rule is in some sense guaranteed to work on networks without <b>hidden</b> <b>units.</b> As previous authors have mentioned, {{there is no such}} guarantee for classification tasks. We will begin by presenting explicit counter-examples illustrating two different interesting ways in which the delta rule can fail. We go on to provide conditions which do guarantee that gradient descent will successfully train networks without <b>hidden</b> <b>units</b> to perform two-category classification tasks. We discuss the generalization of our ideas to networks with <b>hidden</b> <b>units</b> and to multi...|$|R
5000|$|Although {{learning}} is impractical in general Boltzmann machines, {{it can be}} made quite efficient in an architecture called the [...] "restricted Boltzmann machine" [...] or [...] "RBM" [...] which does not allow intralayer connections between <b>hidden</b> <b>units.</b> After training one RBM, the activities of its <b>hidden</b> <b>units</b> can be treated as data for training a higher-level RBM. This method of stacking RBMs makes it possible to train many layers of <b>hidden</b> <b>units</b> efficiently {{and is one of the}} most common deep learning strategies. As each new layer is added the overall generative model gets better.|$|R
5000|$|A {{restricted}} Boltzmann machine [...] is an undirected {{graphical model}} with stochastic visible variable and stochastic hidden variables. Each visible variable {{is connected to}} each hidden variable. The energy function of the model is defined as where [...] are model parameters: [...] represents the symmetric interaction term between visible unit [...] and <b>hidden</b> <b>unit</b> [...] and [...] are bias terms. The joint distribution {{of the system is}} defined as where [...] is a normalizing constant.The conditional distribution over hidden [...] and [...] can be derived as logistic function in terms of model parameters.|$|E
50|$|The {{need for}} deep {{learning}} with real-valued inputs, as in Gaussian restricted Boltzmann machines, {{led to the}} spike-and-slab RBM (ssRBM), which models continuous-valued inputs with strictly binary latent variables. Similar to basic RBMs and its variants, a spike-and-slab RBM is a bipartite graph, while like GRBMs, the visible units (input) are real-valued. The difference is in the hidden layer, where each <b>hidden</b> <b>unit</b> has a binary spike variable and a real-valued slab variable. A spike is a discrete probability mass at zero, while a slab is a density over continuous domain; their mixture forms a prior.|$|E
50|$|Cascade {{correlation}} is {{an architecture}} and supervised learning algorithm. Instead of just adjusting the weights {{in a network}} of fixed topology, Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new <b>hidden</b> <b>unit</b> {{has been added to}} the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages: It learns quickly, determines its own size and topology, retains the structures it has built even if the training set changes and requires no backpropagation.|$|E
40|$|We {{present a}} {{mathematical}} construction for the restricted Boltzmann machine (RBM) that doesn't require specifying {{the number of}} <b>hidden</b> <b>units.</b> In fact, the hidden layer size is adaptive and can grow during training. This is obtained by first extending the RBM {{to be sensitive to}} the ordering of its <b>hidden</b> <b>units.</b> Then, thanks to a carefully chosen definition of the energy function, we show that the limit of infinitely many <b>hidden</b> <b>units</b> is well defined. As with RBM, approximate maximum likelihood training can be performed, resulting in an algorithm that naturally and adaptively adds trained <b>hidden</b> <b>units</b> during learning. We empirically study the behaviour of this infinite RBM, showing that its performance is competitive to that of the RBM, while not requiring the tuning of a hidden layer size. Comment: 25 pages, 8 figure...|$|R
5000|$|The {{conditional}} distributions {{over the}} visible and <b>hidden</b> <b>units</b> are ...|$|R
40|$|We {{consider}} feed-forward {{neural networks}} with one hidden layer, tree architecture and a fixed hidden-to-output Boolean function. Focusing on the saturation {{limit of the}} storage problem the influence of replica symmetry breaking {{on the distribution of}} local fields at the <b>hidden</b> <b>units</b> is investigated. These field distributions determine the probability for finding a specific activation pattern of the <b>hidden</b> <b>units</b> as well as the corresponding correlation coefficients and therefore quantify the division of labor among the <b>hidden</b> <b>units.</b> We find that although modifying the storage capacity and the distribution of local fields markedly replica symmetry breaking has only a minor effect on the correlation coefficients. Detailed numerical results are provided for the PARITY, COMMITTEE and AND machines with K= 3 <b>hidden</b> <b>units</b> and nonoverlapping receptive fields. Comment: 9 pages, 3 figures, RevTex, accepted for publication in Phys. Rev. ...|$|R
50|$|A Boltzmann {{machine is}} a type of {{stochastic}} neural network invented by Geoffrey Hinton and Terry Sejnowski in 1985. Boltzmann machines {{can be seen as the}} stochastic, generative counterpart of Hopfield nets. They are named after the Boltzmann distribution in statistical mechanics. The units in Boltzmann machines are divided into two groups-visible units and hidden units. General Boltzmann machines allow connection between any units. However, learning is impractical using general Boltzmann Machines because the computational time is exponential to the size of the machine. A more efficient architecture is called restricted Boltzmann machine where connection is only allowed between <b>hidden</b> <b>unit</b> and visible unit, which is described in the next section.|$|E
5000|$|Units {{in towns}} and wooded hexes were {{invisible}} unless an enemy unit was directly adjacent to them, even though those units may {{have moved to}} that position {{in full view of}} the enemy, and fired from it as well. There is an optional rule called opportunity fire in which a unit moving in the line-of-sight of an enemy unit may be fired upon by that unit. This ability of units to hop from one woods hex to another without being attacked was called [...] "Panzerbush Syndrome", and [...] "Panzerbush" [...] became a scornful nickname for the game itself. The game provided a cumbersome optional rule to overcome this, but the later versions of the system (Panzer Leader and The Arab-Israeli Wars) provided much better solutions, such as the optional opportunity fire and more realistic rules for spotting and visibility. In these systems, a <b>hidden</b> <b>unit</b> that fires on the enemy becomes seen and can be fired upon in return. A common practice for those who desire more realism is to play PanzerBlitz with the Panzer Leader spotting rules.|$|E
50|$|Many of {{the early}} {{techniques}} in reducing representational overlap involved making either the input vectors or the <b>hidden</b> <b>unit</b> activation patterns orthogonal to one another. Lewandowsky and Li (1995) noted that the interference between sequentially learned patterns is minimized if the input vectors are orthogonal to each other. Input vectors {{are said to be}} orthogonal to each other if the pairwise product of their elements across the two vectors sum to zero. For example, the patterns 0,0,1,0 and 0,1,0,0 are said to be orthogonal because (0&times;0 + 0&times;1 + 1&times;0 + 0&times;0) = 0. One of the techniques which can create orthogonal representations at the hidden layers involves bipolar feature coding (i.e., coding using -1 and 1 rather than 0 and 1). Orthogonal patterns tend to produce less interference with each other. However, not all learning problems can be represented using these types of vectors and some studies report that the degree of interference is still problematic with orthogonal vectors. Simple techniques such as varying the learning rate parameters in the backpropagation equation were not successful in reducing interference. Varying the number of hidden nodes has also been used to try and reduce interference. However, the findings have been mixed, with some studies finding that more hidden units decrease interference and other studies finding it does not.|$|E
40|$|This paper {{presents}} an algorithm to prune {{feedforward neural network}} architectures using sensitivity analysis. Sensitivity Analysis is used to quantify the relevance of input and <b>hidden</b> <b>units.</b> A new statistical pruning heuristic is proposed, based on variance analysis, to decide which units to prune. Results are presented {{to show that the}} pruning algorithm correctly prunes irrelevant input and <b>hidden</b> <b>units...</b>|$|R
40|$|Conventional wisdom {{states that}} {{as the number of}} <b>hidden</b> <b>units</b> H in a {{supervised}} regression network is increased, the generalization error, beyond a certain point, gets worse, so that the number of <b>hidden</b> <b>units</b> should be carefully controlled. However, Neal (1995) has shown theoretically that if an appropriate Gaussian prior is applied to the network weights, then as the number of <b>hidden</b> <b>units</b> is increased to infinity, the complexity of the probabilistic model does not increase. The distribution over functions tends to a Gaussian process whose properties are solely determined by three parameters of the Gaussian prior. Thus in an ideal Bayesian implementation, the number of <b>hidden</b> <b>units</b> should not be important. In this paper we reconcile these two apparently conflicting points of view. We emphasize the importance of the log predictive probability which takes into account error bars on the network's predictions, as a generalization measure, in contrast to the traditional test error. 1 Intr [...] ...|$|R
40|$|In this paper, {{we present}} two {{learning}} mechanisms for artificial neural networks (ANNs) {{that can be}} applied to solve classification problems with binary outputs. These mechanisms are used {{to reduce the number of}} <b>hidden</b> <b>units</b> of an ANN when trained by the cascade-correlation learning algorithm (CAS). Since CAS adds <b>hidden</b> <b>units</b> incrementally as learning proceeds, it is difficult to predict the number of <b>hidden</b> <b>units</b> required when convergence is reached. Further, learning must be restarted when the number of <b>hidden</b> <b>units</b> is larger than expected. Our key idea in this paper is to provide alternatives in the learning process and to select the best alternative dynamically based on run-time information obtained. Mixed-mode learning (MM), our first algorithm, provides alternative output matrices so that learning is extended to find one of the many one-to-many mappings instead of finding a unique one-to-one mapping. Since the objective of learning is relaxed by this transformation, the number o [...] ...|$|R
5000|$|The MSZ-010 ŒñŒñ Gundam (pronounced Double Zeta (Daburu Zƒìta)), {{designed}} by Makoto Kobayashi, is a fictional weapon from the Universal Century timeline of the anime Gundam metaseries. Its popularity {{has led to}} many variations, upgrades, redesigns and influenced later design works such as S Gundam. It is named after the third of the Gundam series Mobile Suit Gundam ZZ, sequel to Mobile Suit Zeta Gundam and {{is presented in the}} middle of the story as a new weapon of the protagonist Judau Ashta replacing the battle-worn Zeta Gundam. First portrayed as a multipurpose fighting machine, later variations such as the FAZZ (a test production unit) was portrayed as a less-mobile, long-range fire-support unit. The unit has also appeared in many of the Super Robot Wars series, famous for featuring different giant robot mechas of different franchise together in one mixed story, from the first of the series to Super Robot Wars Alpha 3, a total of 26 games excluding remakes. Inside the Gundam Franchise, it has been featured in the PS2 Game Mobile Suit Gundam Z: AEUG vs. Titans as a <b>hidden</b> <b>unit</b> not being able to purchase in the in-game shop until certain requirements are met. It is also one of the featured units in the March 2007 release PS3/Xbox 360 game Dynasty Warriors: Gundam, a spinoff series of the Koei game Dynasty Warriors. [...] It is also featured in Gundam Evolve 10. [...] In the SD Gundam manga Double Zeta Kun kokoniari(„ÉÄ„Éñ„É´„ÇºÔºç„Çø„Åè„Çì„Åì„Åì„Å´„ÅÇ„É™, Little Double Zeta is here), Double ZZ Kun is the main character.|$|E
5000|$|To {{test their}} hypothesis, McRae and Hetherington (1993) {{compared}} {{the performance of}} a na√Øve and pre-trained auto-encoder backpropagation network on three simulations of verbal learning tasks. The pre-trained network was trained using letter based representations of English monosyllabic words or English word pairs. All three tasks involved the learning of some consonant-vowel-consonant (CVC) strings or CVC pairs (list A), followed by training on a second list of these items (list B). Afterwards, the distributions of the hidden node activations were compared between the na√Øve and pre-trained network. In all three tasks, the representations of a CVC in the na√Øve network tended to be spread fairly evenly across all hidden nodes, whereas most hidden nodes were inactive in the pre-trained network. Furthermore, in the pre-trained network the representational overlap between CVCs was reduced compared to the na√Øve network. The pre-trained network also retained some similarity information as the representational overlap between similar CVCs, like [...] "JEP" [...] and [...] "ZEP", was greater than for dissimilar CVCs, such as [...] "JEP" [...] and [...] "YUG". This suggests that the pre-trained network had a better ability to generalize, i.e. notice the patterns, than the na√Øve network. Most importantly, this reduction in <b>hidden</b> <b>unit</b> activation and representational overlap resulted in significantly less forgetting in the pre-trained network than the na√Øve network, essentially eliminating catastrophic interference. Essentially, the pre-training acted to create internal orthogonalization of the activations at the hidden layer, which reduced interference. Thus, pre-training is a simple way to reduce catastrophic forgetting in standard backpropagation networks.|$|E
5000|$|Many {{researchers}} {{have suggested that}} the main cause of catastrophic interference is overlap in the representations at the hidden layer of distributed neural networks. In a distributed representation any given input will tend to create changes in the weights to many of the nodes. Catastrophic forgetting occurs because when many of the weights, where 'knowledge is stored, are changed it is impossible for prior knowledge to be kept intact. During sequential learning, the inputs become mixed with the new input being superimposed over top of the old input. Another way to conceptualize this is through visualizing learning as movement through a weight space. This weight space can be likened to a spatial representation of all of the possible combinations of weights that the network can possess. When a network first learns to represent a set of patterns, it has found a point in weight space which allows it to recognize all of the patterns that it has seen. However, when the network learns a new set of patterns sequentially it will move to a place in the weight space that allows it to only recognize the new pattern. To recognize both sets of patterns, the network must find a place in weight space that can represent both the new and the old output. One {{way to do this is}} by connecting a <b>hidden</b> <b>unit</b> to only a subset of the input units. This reduces the likelihood that two different inputs will be encoded by the same hidden units and weights, and so will decrease the chance of interference. [...] Indeed, a number of the proposed solutions to catastrophic interference involve reducing the amount of overlap that occurs when storing information in these weights.|$|E
40|$|This paper {{demonstrates}} that a skeletal {{structure of a}} network emerges when independent noises {{are added to the}} inputs of the <b>hidden</b> <b>units</b> of Multilayer Perceptron during the learning by error backpropagation. By analyzing the average behavior of the error backpropagation algorithm to such noises, it is shown that the weights from the <b>hidden</b> <b>units</b> to the output units tend to get smaller and the outputs of the <b>hidden</b> <b>units</b> tend to be 0 or 1. Such tendency have been demonstrated by experiments of learning of pattern classification problem. I. Introduction Multilayer Perceptron (MLP) with error backpropagation learning algorithm (BP) [1, 2] have been successfully applied to many problems. It is also shown that MLP is capable of approximating arbitrary nonlinear mappings if we can use arbitrarily many <b>hidden</b> <b>units</b> [3]. However, in general, {{it is difficult to determine}} the best size of the network for a given problem in advance. In weight decay [4] the effective number of weights in the n [...] ...|$|R
40|$|We {{consider}} two-layered perceptrons {{consisting of}} N binary input <b>units,</b> K binary <b>hidden</b> <b>units</b> and one binary output unit, in the limit N AE K 1. We {{prove that the}} weights of a regular irreducible network are uniquely determined by its input-output map up to some obvious global symmetries. A network is regular if its K weight vectors from the input layer to the K <b>hidden</b> <b>units</b> are linearly independent. A (single layered) perceptron {{is said to be}} irreducible if its output depends on everyone of its input units; 2 and a two-layered perceptron is irreducible if the K + 1 perceptrons that constitute such network are irreducible. By global symmetries we mean, for instance, permuting the labels of the <b>hidden</b> <b>units.</b> Hence, two irreducible regular two-layered perceptrons that implement the same Boolean function must have the same number of <b>hidden</b> <b>units,</b> and must be composed of equivalent perceptrons. 1 Introduction In most applications dealing with learning and pattern classification, neur [...] ...|$|R
50|$|In Gaussian-Bernoulli RBM, {{the visible}} <b>unit</b> {{conditioned}} on <b>hidden</b> <b>units</b> is modeled as a Gaussian distribution.|$|R
3000|$|The dropout {{process is}} that each <b>hidden</b> <b>unit</b> is {{randomly}} omitted from the deep architecture with a probability such that a <b>hidden</b> <b>unit</b> cannot rely on other hidden units being presented, based on the observation that this parameter is correlating with the deep architecture (Fig. 4 [...]...|$|E
40|$|A fast {{algorithm}} is presented {{for the training}} of multilayer perceptron neural networks, which uses separate error functions for each <b>hidden</b> <b>unit</b> and solves multiple sets of linear equations. The algorithm builds upon two previously described techniques. In each training iteration, output weight optimization (OWO) solves linear equations to optimize output weights, which are those connecting to output layer net functions. The method of hidden weight optimization (HWO) develops desired <b>hidden</b> <b>unit</b> net signals from delta functions. The resulting <b>hidden</b> <b>unit</b> error functions are minimized with respect to hidden weights, which are those feeding into <b>hidden</b> <b>unit</b> net functions. An {{algorithm is}} described for calculating the learning factor for hidden weights. We show that the combined technique, OWO-HWO is superior in terms of convergence to standard OWO-BP (output weight optimization-backpropagation) which uses OWO to update output weights and backpropagation to update hidden weights. We also [...] ...|$|E
40|$|We can {{generate}} a secret key using neural cryptography, {{which is based}} on synchronization of Tree Parity Machines (TPMs) by mutual learning. In the proposed TPMs random inputs are replaced with queries which are considered. The queries depend on the current state of A and B TPMs. Then, TPMs hidden layer of each output vectors are compared. That is, the output vectors of <b>hidden</b> <b>unit</b> using Hebbian learning rule, left-dynamic <b>hidden</b> <b>unit</b> using Random walk learning rule and rightdynamic <b>hidden</b> <b>unit</b> using Anti-Hebbian learning rule are compared. Among the compared values, one of the best values is received by the output layer. The queries fix the security against majority flipping and geometric attacks are shown in this paper. The new parameter H can accomplish a higher level of security for the neural key-exchange protocol without altering the average synchronization time...|$|E
30|$|The first setup (setup 1) {{contained}} a 3 -layer vanilla RNN with 512 <b>hidden</b> <b>units</b> per layer, for the language model. The same network {{was used for}} the acoustic model. As per MSTT model, we decided to choose a 2 -layer RNN, with 1024 <b>hidden</b> <b>units</b> per layer. Descriptions for setup 2 and setup 3 are analogical to setup 1 and are shown in Table¬† 1.|$|R
30|$|Neural {{networks}} {{perform better}} for EEG classifications compared to {{linear discriminant analysis}} since {{it can be used}} to implement boundaries for nonlinear classifications. Nevertheless, to acquire the desired level of accuracy, it is important to choose a suitable number of <b>hidden</b> <b>units,</b> which can become problematic. Having a larger number of <b>hidden</b> <b>units</b> than required results in memorising the training set which causes poor generalisation [63].|$|R
40|$|Deep {{learning}} is the state-of-the-art in {{fields such as}} visual object recognition and speech recognition. This learning uses {{a large number of}} layers, huge number of units, and connections. Therefore, overfitting is a serious problem. To avoid this problem, dropout {{learning is}} proposed. Dropout learning neglects some inputs and <b>hidden</b> <b>units</b> in the learning process with a probability, p, and then, the neglected inputs and <b>hidden</b> <b>units</b> are combined with the learned network to express the final output. We find that the process of combining the neglected <b>hidden</b> <b>units</b> with the learned network can be regarded as ensemble learning, so we analyze dropout learning from this point of view. Comment: 9 pages, 8 figures, submitted to Conferenc...|$|R
