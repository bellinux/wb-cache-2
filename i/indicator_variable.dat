364|1311|Public
50|$|Thus where w denotes equals {{weekly wage}} of the i, the {{individual}} in t years, Xis the control variable (education, experience, gender, marriage, part-time work, census divisions, and twelve occupations). Ij is the <b>indicator</b> <b>variable</b> for industry j, U is the <b>indicator</b> <b>variable</b> for union membership and ε is the regression error. The πjt is the union wage premium in industry j in year t.|$|E
50|$|To {{formulate}} {{this as a}} 0-1 integer program, form an <b>indicator</b> <b>variable</b> xi {{for each}} set Si, that takes the value 1 when Si belongs to the chosen subfamily and 0 when it does not. Then a valid cover can be described by an assignment of values to the indicator variables satisfying the constraints(that is, only the specified <b>indicator</b> <b>variable</b> values are allowed) and, for each element ej of the union of F,(that is, each element is covered). The minimum set cover corresponds to the assignment of indicator variables satisfying these constraints and minimizing the linear objective functionThe linear programming relaxation of the set cover problem describes a fractional cover in which the input sets are assigned weights such that the total weight of the sets containing each element {{is at least one}} and the total weight of all sets is minimized.|$|E
50|$|In {{contrast}} to the traditional Bayesian paradigm Bayes linear statistics following de Finetti uses Prevision or subjective expectation as a primitive, probability is then defined as the expectation of an <b>indicator</b> <b>variable.</b> Instead of specifying a subjective probability for every element in the partition D &times; B the analyst specifies subjective expectations {{for just a few}} quantities that they are interested in or feel knowledgeable about. Then instead of conditioning an adjusted expectation is computed by a rule that is a generalization of Bayes' rule that is based upon expectation.|$|E
40|$|This paper {{develops}} a multiperiod agency model {{to study the}} use of leading <b>indicator</b> <b>variables</b> in managerial performance measures. In addition to the familiar moral hazard problem, the principal faces the task of motivating a manager to undertake "soft" investments. These investments are not directly contractible, but the principal can instead rely on leading <b>indicator</b> <b>variables</b> which provide a noisy forecast of the investment returns to be received in future periods. Our analysis relates the role of leading <b>indicator</b> <b>variables</b> to {{the duration of the}} manager's incentive contract. With short-term contracts, leading <b>indicator</b> <b>variables</b> are essential in mitigating a hold up problem resulting from the fact that investments are sunk {{at the end of the}} first period. With long-term contracts, leading <b>indicator</b> <b>variables</b> will be valuable if the manager's compensation schemes are not stationary over time. The leading <b>indicator</b> <b>variables</b> then become an instrument for matching the future investment return with the current investment expenditure. We identify conditions under which the optimal long-term contract induces larger investments and less reliance on the leading <b>indicator</b> <b>variables</b> in comparison to short-term contracts. Under certain conditions, though, the principal does better with sequence of one-period contracts than with a long-term contract. ...|$|R
3000|$|Specification 1 : Adds {{a set of}} <b>indicator</b> <b>variables</b> {{for whether}} the {{individual}} applied and was admitted {{to each of the}} eight UC campuses (sixteen <b>indicator</b> <b>variables</b> in all) to the baseline specification.|$|R
40|$|In {{this article}} we develop a multiperiod agency model to study the role of leading <b>indicator</b> <b>variables</b> in {{managerial}} performance measures. In addition to the familiar moral hazard problem, the principal faces the task of motivating a manager to undertake "soft" investments. These investments are not directly contractible, but the principal can instead rely on leading <b>indicator</b> <b>variables</b> that provide a noisy forecast of the investment returns to be received in future periods. Our analysis relates the role of leading <b>indicator</b> <b>variables</b> to {{the duration of the}} manager's incentive contract. With short-term contracts, leading <b>indicator</b> <b>variables</b> are essential in mitigating a holdup problem resulting from the fact that investments are sunk {{at the end of the}} first period. With long-term contracts, leading <b>indicator</b> <b>variables</b> will be valuable if the manager's compensation schemes are not stationary over time. The leading <b>indicator</b> <b>variables</b> then become an instrument for matching the future investment return with the current investment expenditure. We identify conditions under which the optimal long-term contract induces larger investments and less reliance on the leading <b>indicator</b> <b>variables</b> as compared with short-term contracts. Under certain conditions, though, the principal does better with a sequence of one-period contracts than with a long-term contract. Copyright 2003 Institute of Professional Accounting, University of Chicago. ...|$|R
50|$|Factor loadings: Commonality is {{the square}} of {{standardized}} outer loading of an item. Analogous to Pearson's r, the squared factor loading is the percent of variance in that <b>indicator</b> <b>variable</b> explained by the factor. To get the percent of variance in all the variables accounted for by each factor, add {{the sum of the}} squared factor loadings for that factor (column) and divide by the number of variables. (Note the number of variables equals the sum of their variances as the variance of a standardized variable is 1.) This is the same as dividing the factor's eigenvalue by the number of variables.|$|E
5000|$|The intuition {{behind this}} is that [...] is an <b>indicator</b> <b>variable</b> for whether edge (i, j) {{is part of the}} {{shortest}} path: 1 when it is, and 0 if it is not. We wish to select the set of edges with minimal weight, subject to the constraint that this set forms a path from s to t (represented by the equality constraint: for all vertices except s and t the number of incoming and outcoming edges {{that are part of the}} path must be the same (i.e., that it should be a path from s to t).|$|E
5000|$|A further {{financial}} correlation measure, [...] is the binomial correlation {{approach of}} Lucas (1995). We define the binomial events [...] and [...] where [...] is the default time of entity [...] and [...] is the default time of entity [...] Hence if entity [...] defaults before or at time , the random <b>indicator</b> <b>variable</b> [...] {{will take the}} value in 1, and 0 otherwise. The same applies to [...] Furthermore, [...] and [...] is the default probability of [...] and [...] respectively, and [...] is the joint probability of default. The standard deviation of a one-trial binomial event is , where P is the probability of outcome X. Hence, we derive the joint default dependence coefficient of the binomial events and [...] as ...|$|E
40|$|We employ {{artificial}} {{neural networks}} using macro-financial variables to predict recessions. We model {{the relationship between}} <b>indicator</b> <b>variables</b> and recessions to periods into the future and employ a procedure that penalizes a misclassified recession more than a misclassified non-recession. Our results reveal that among 16 models that we constructed from <b>indicator</b> <b>variables</b> and their combinations, the <b>indicator</b> <b>variables</b> Spread, -year bond rates, -year bond rates, monetary base, industrial production are candidate variables for predicting recessions ranging to periods in the future. However, most <b>indicator</b> <b>variables</b> become candidate for predicting recessions when misclassified recessions are penalized heavily than misclassified non-recessions. business cycles; neural networks; out-of-sample forecasts; recession; real GDP...|$|R
40|$|To {{know the}} {{correlation}} between <b>indicator</b> <b>variables</b> and latent variables that the influence students’ achievement in majors of mathematics of FMIPA UNSRI used Confirmatory Factor Analysis by maximum likelihood method to estimate the model parameters. Confirmatory Factor Analysis {{is one of the}} methods of multivariate analysis used to confirm whether or not the model that is build is match the hypothesis. The result of tained that the latent variable of has a background of family could be measured by <b>indicator</b> <b>variables</b> of father’s education (x 1), mother’s education (x 2) and parent’s income (x 3), where <b>indicator</b> <b>variables</b> that give great contribution is mother’s education is 0. 84. The latent variables of learning environment of campus (ξ 2) could be measured by <b>indicator</b> <b>variables</b> of time using far house to campus (x 4), learning facilities at home (x 5) and learning concentration (x 8), where <b>indicator</b> <b>variables</b> that give great contribution is learning facilities at home is 0. 80. The latent variables of attitude toward almamater (ξ 3) could be measured by <b>indicator</b> <b>variables</b> of classroom facilities in major (x 11), library facilities (x 12) and computer facilities (x 13), where <b>indicator</b> <b>variables</b> that give great contribution is classroom facilities in major is 1. 08. The latent variable of perception toward lecturers (ξ 4) could be measured by indicator of evaluation system given by lecturer (x 16), learning system given by lecturer (x 17), assignment system given by the lecturer (x 18), and the relationship with academic advisory (x 19), where <b>indicator</b> <b>variables</b> that give great contribution is learning system is 0. 73...|$|R
40|$|Abstract. This paper {{analyses}} {{the relationship}} between financial stress <b>indicator</b> <b>variables</b> and monetary policy in South Africa with emphasis on how robust these variables {{are related to the}} monetary policy interest rate. The financial stress <b>indicator</b> <b>variables</b> comprise a set of variables from the main segments of the South African financial market that include the bond and equity securities markets, the commodity market and the exchange rate market. The empirical results show that the set of financial stress <b>indicator</b> <b>variables</b> from the bond and equity securities markets as well as those from credit markets and property markets are robustly associated with the monetary policy interest rate, while the set of financial stress <b>indicator</b> <b>variables</b> from commodity markets and the exchange rate market are weakly associated with the monetary policy interest rate. Keywords. Financial stressindicator variables, Monetary policy. JEL. C 32, C 51, E 52, E 61, G 01, G 10...|$|R
5000|$|In {{statistics}} and econometrics, particularly in regression analysis, a dummy variable (also {{known as an}} <b>indicator</b> <b>variable,</b> design variable, Boolean indicator, categorical variable, binary variable, or qualitative variable) is one that takes the value 0 or 1 to indicate the absence or presence of some categorical effect that {{may be expected to}} shift the outcome. Dummy variables are used as devices to sort data into mutually exclusive categories (such as smoker/non-smoker, etc.). [...] For example, in econometric time series analysis, dummy variables may be used to indicate the occurrence of wars or major strikes. A dummy variable can thus {{be thought of as a}} truth value represented as a numerical value 0 or 1 (as is sometimes done in computer programming).|$|E
5000|$|Any given clause c is {{unsatisfied}} only {{if all of}} its k constituent literals evaluates to false. Because each literal {{within a}} clause has a [...] chance of evaluating to true independently {{of any of the}} truth value of any of the other literals, the probability that they are all false is [...] Thus, the probability that c is indeed satisfied is , so the <b>indicator</b> <b>variable</b> [...] (that is 1 if c is true and 0 otherwise) has expectation [...] The sum of all of the indicator variables over all [...] clauses is , so by linearity of expectation we satisfy a [...] fraction of the clauses in expectation. Because the optimal solution can't satisfy more than all [...] of the clauses, we have that , so the algorithm finds a [...] approximation to the true optimal solution in expectation.|$|E
5000|$|The Jensen-Shannon {{divergence}} is {{the mutual}} information between a random variable [...] associated to a mixture distribution between [...] and [...] and the binary <b>indicator</b> <b>variable</b> [...] {{that is used}} to switch between [...] and [...] to produce the mixture. Let [...] be some abstract function on the underlying set of events that discriminates well between events, and choose the value of [...] according to [...] if [...] and according to [...] if [...] That is, we are choosing [...] according to the probability measure [...] , and its distribution is the mixture distribution. We compute It follows from the above result that the Jensen-Shannon divergence is bounded by 0 and 1 because mutual information is non-negative and bounded by [...] The JSD is not always bounded by 0 and 1: the upper limit of 1 arises here because we are considering the specific case involving the binary variable [...]|$|E
40|$|The use of <b>indicator</b> <b>variables</b> for {{computing}} {{predictions for}} the linear {{model is a}} well known technique. Fuller (Fuller, W. A. (1980). The use of <b>indicator</b> <b>variables</b> in computing predictions. J. Econometrics 2 : 231 – 243.) extends this to predictions for models with a general covariance structure and nonlinear models. In this work we use <b>indicator</b> <b>variables</b> for spatial data models with trend and a parametrized but unknown covariance function. We show that Restricted Maximum Likelihood (REML) estimates are a natural way to estimate the covariance parameters under this schema. We use dummy variables to predict the response at any number of sites, on a random Gaussian field. A simulation {{study was conducted to}} study the performance of the estimate and predictor when we consider <b>indicator</b> <b>variables</b> in the model. *Correspondence: Graciela Gonza ´ lez-Farı ´ as, Centro de Investigacio ´ n e...|$|R
30|$|The {{probability}} of informal employment was estimated {{in the first}} stage controlling for sex, age and its squared, civil status, <b>indicator</b> <b>variables</b> for education level and household head status, <b>indicator</b> <b>variables</b> for industry affiliation and time periods, and industry tariffs. In the second stage, we estimate two wage models, for formal and informal workers, including the inverse Mills ratio as an additional control variable (λ (z)).|$|R
3000|$|... − 1) as {{suggested}} by Halvorsen and Palmquist (1980) when interpreting <b>indicator</b> <b>variables</b> in a semi-log specification.|$|R
5000|$|One type {{of sample}} mean is the mean of an <b>indicator</b> <b>{{variable}},</b> which takes on the value 1 for true and the value 0 for false. The mean of such a variable {{is equal to the}} proportion that has the variable equal to one (both in the population and in any sample). This is a useful property of indicator variables, especially for hypothesis testing. To apply the central limit theorem, one must use a large enough sample. A rough rule of thumb is that one should see at least 5 cases in which the indicator is 1 and at least 5 in which it is 0. Confidence intervals constructed using the above formulae may include negative numbers or numbers greater than 1, but proportions obviously cannot be negative or exceed 1. Additionally, sample proportions can only take on a finite number of values, so the central limit theorem and the normal distribution are not the best tools for building a confidence interval. See [...] "Binomial proportion confidence interval" [...] for better methods which are specific to this case.|$|E
5000|$|Lack of {{multicollinearity}} in the predictors. For standard {{least squares}} estimation methods, the design matrix X must have full column rank p; otherwise, {{we have a}} condition known as multicollinearity in the predictor variables. This can be triggered by having two or more perfectly correlated predictor variables (e.g. if the same predictor variable is mistakenly given twice, either without transforming one of the copies or by transforming one of the copies linearly). It can also happen if there is too little data available compared {{to the number of}} parameters to be estimated (e.g. fewer data points than regression coefficients). In the case of multicollinearity, the parameter vector β will be non-identifiable—it has no unique solution. At most {{we will be able to}} identify some of the parameters, i.e. narrow down its value to some linear subspace of Rp. See partial least squares regression. Methods for fitting linear models with multicollinearity have been developed; some require additional assumptions such as [...] "effect sparsity"—that a large fraction of the effects are exactly zero. Note that the more computationally expensive iterated algorithms for parameter estimation, such as those used in generalized linear models, do not suffer from this problem—and in fact it's quite normal when handling categorically valued predictors to introduce a separate <b>indicator</b> <b>variable</b> predictor for each possible category, which inevitably introduces multicollinearity.|$|E
5000|$|For the set cover problem, Lovász {{proved that}} the integrality gap for an {{instance}} with n elements is Hn, the nth harmonic number. One can turn the linear programming relaxation for this problem into an approximate solution of the original unrelaxed set cover instance via the technique of randomized rounding [...] Given a fractional cover, in which each set Si has weight wi, choose randomly the value of each 0-1 <b>indicator</b> <b>variable</b> xi to be 1 with probability wi &times;(ln n +1), and 0 otherwise. Then any element ej has probability less than 1/(e&times;n) of remaining uncovered, so with constant probability all elements are covered. The cover generated by this technique has total size, with high probability, (1+o(1))(ln n)W, where W is the total weight of the fractional solution. Thus, this technique leads to a randomized approximation algorithm that finds a set cover within a logarithmic factor of the optimum. As [...] showed, both the random part of this algorithm {{and the need to}} construct an explicit solution to the linear programming relaxation may be eliminated using the method of conditional probabilities, leading to a deterministic greedy algorithm for set cover, known already to Lovász, that repeatedly selects the set that covers the largest possible number of remaining uncovered elements. This greedy algorithm approximates the set cover to within the same Hn factor that Lovász proved as the integrality gap for set cover. There are strong complexity-theoretic reasons for believing that no polynomial time approximation algorithm can achieve a significantly better approximation ratio [...]|$|E
40|$|A Bayesian {{analysis}} is given {{for a state}} space model with errors that are finite mixtures of normals and with coefficients that can assume {{a finite number of}} different values. A sequence of <b>indicator</b> <b>variables</b> determines which components the errors belong to and the values of the coefficients. The computation is carried out using Markov chain Monte Carlo, with the <b>indicator</b> <b>variables</b> generated without conditioning on the states. Previous approaches use the Gibbs sampler to generate the <b>indicator</b> <b>variables</b> conditional on the states. In many problems, however, there is a strong dependence between the <b>indicator</b> <b>variables</b> and the states causing the Gibbs sampler to converge unacceptably slowly, or even not to converge at all. The new sampler is implemented in 0 {n) operations, where n is the sample size, permitting an exact Bayesian analysis of problems that previously had no computationally tractable solution. We show empirically that the new sampler can be much more efficient than previous approaches, and illustrate its applicability to robust nonparametric regression with discontinuities and to a time series change point problem...|$|R
3000|$|... is a vector of <b>indicator</b> <b>variables</b> that {{identify}} the ethnic group {{to which the}} individual belongs, and Z [...]...|$|R
3000|$|Ethnic origin: two <b>indicator</b> <b>variables</b> for {{the birth}} place of the parents, {{indicating}} whether the parents are from non-European countries.|$|R
3000|$|... is an individual’s {{composite}} wage in year t and exp and s are {{as defined}} above 14. The letter I denotes an immigrant <b>indicator</b> <b>variable.</b> Qualitative matching is incorporated into the model with indicator variables, i.e., some is an <b>indicator</b> <b>variable</b> for respondents in a “somewhat related” job, while close is an <b>indicator</b> <b>variable</b> for respondents in a “closely related” job. The omitted category refers to those in a job “not at all related” to their education.|$|E
30|$|The process generates ten PV {{for each}} <b>indicator</b> <b>variable.</b>|$|E
40|$|BACKGROUNDS/OBJECTIVES: The aim of {{this paper}} is to verify the {{performance}} of the frequency of consumption as variable for prediction of the usual intakes of foods. SUBJECTS/METHODS: In total, 725 individuals who answered two nonconsecutive 24 -h recall and one food frequency questionnaire (FFQ) in the 'Healthy Survey-Sao Paulo-Brazil'. An additional <b>indicator</b> <b>variable</b> indicating if one is usual consumer was created before analyzing. The Multiple Source Method and National Cancer Institute method were used to estimate usual intake of selected food considering different models of prediction: with no covariates; with FFQ; with FFQ plus indicator variable; and with only <b>indicator</b> <b>variable.</b> RESULTS: For foods that are consumed every day or almost every day, the inclusion of the FFQ and/or the <b>indicator</b> <b>variable</b> as covariates resulted in similar percentiles of consumption when compared with the model with no covariates. For episodically consumed foods, the models with FFQ plus <b>indicator</b> <b>variable</b> and with only <b>indicator</b> <b>variable</b> estimated similar percentiles of intake. CONCLUSIONS: The use of the <b>indicator</b> <b>variable</b> instead the FFQ appears as a good alternative to estimate usual intake of episodically consumed foods. Fundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP) Fundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP) [2009 / 11239 - 9, 2009 / 15831 - 0]Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPq) [503128 / 2010 - 4]Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPq...|$|E
3000|$|Specification 3 : Adds <b>indicator</b> <b>variables</b> for {{the highest}} ranked campus where the {{individual}} {{was admitted to the}} baseline specification.|$|R
3000|$|Career choices due {{to family}} responsibilities: two <b>indicator</b> <b>variables</b> for parental leave {{following}} {{the birth of}} the first (parental leave 1) and the second child (parental leave 2), taking the value one if the person takes, parental leave at the birth if the first or the second child and zero otherwise; two <b>indicator</b> <b>variables</b> for being employed at {{the birth of the}} first (employed 1) and the second child (employed 2), taking the value one if the person was employed at the birth if the first or the second child and zero otherwise; two <b>indicator</b> <b>variables</b> for having resigned at the birth of the first (resign 1) and the second child (resign 2), taking the value one if the person resigned at the birth the first or second child and zero otherwise.|$|R
40|$|In {{response}} to the perceived instability of the relations between traditional monetary aggregates and nominal aggregate demand a number of nonstandard <b>indicator</b> <b>variables</b> {{have been developed to}} enable monetary policy to respond to, and counteract, incipient inflationary pressures before much inflation has developed. While the Federal Reserve Board is believed to pay increasing attention to such <b>indicator</b> <b>variables,</b> it is unclear which ones are perceived as particularly important. In this note, we present a variation of the monetary impulse measure (MIM), which was recently developed by McCallum and Hargraves (Staff studies for the World Economic Outlook, 1995). Modifying the original specification of the measure, we show that the new MIM's performance in explaining actual Fed decisions is clearly superior to other <b>indicator</b> <b>variables,</b> which are widely believed to guide US monetary policy. ...|$|R
30|$|DrugsMissing An <b>indicator</b> <b>variable</b> {{which is}} 1 if all {{pharmacy}} claims {{for the patient}} were unavailable.|$|E
30|$|A {{calculation}} {{with the}} I 1 <b>indicator</b> <b>variable</b> returned five concentrations as useful variables for classification, those are: iron, silver, bismuth, lead and tin. Based on maximizing the variance {{between the groups}} and minimizing it within the six groups in the classification matrix we could classify the 257 coins into three groups. By doing so, we {{have shown that the}} construction of the I 2 <b>indicator</b> <b>variable</b> is valid.|$|E
40|$|Instructional dataset, Accompanying An Introduction to Classical Econometric Theory Paul A. Ruud, Oxford University Press, 2 d ed. (c) 2000 Datasets also {{accessible}} in ASCII from [URL] An {{extract from}} the March, 1995 Current Population Survey of the U. S. Census Bureau. 1289 observations, 8 variables, including w (wage), fe female <b>indicator</b> <b>variable),</b> nw (nonwhite indicator), un ((union indicator), ed (years of schooling), ex (years of potential experience), age (age), wk (weekly earnings <b>indicator</b> <b>variable)</b> ...|$|E
30|$|We used partial {{least squares}} {{modeling}} with the ‘PLS-PM’ package (Sanchez et al. 2013) in R v. 3.4. 4 (R Core Team 2018), which is non-parametric {{and does not}} have any distributional assumptions, to explain the effects of economic and environmental drivers and of forest management on biodiversity, here defined by the abundance of central European forest specialist bird species from 1980 to 2014. As these variables are theoretical concepts, i.e. not measured directly, we created latent variables for each. We used the wood price index and gross domestic product (GDP) as <b>indicator</b> <b>variables</b> for ‘Economy’, CO 2 concentrations and N deposition (standardized using 1990 as baseline) as <b>indicator</b> <b>variables</b> for ‘Environment’, cuttings per area per year and the percent of mixed forest area as <b>indicator</b> <b>variables</b> for ‘Forest management’, and the abundances of 12 forest specialist bird species as <b>indicator</b> <b>variables</b> for ‘Bird Abundance’. We fit the PLS-PM using 1000 bootstrap samples to estimate model parameters. To evaluate the fitted PLS-PM, we first assessed the reflective measurement model using Cronbach’s alpha and the first eigenvalue to ensure that the selected <b>indicator</b> <b>variables</b> agree with their corresponding latent variable. The latent variables were unidimensional, meaning that the <b>indicator</b> <b>variables</b> are strongly associated with their latent variable, as indicated by Cronbach’s alpha and the first eigen-values for each latent variable. Cronbach’s alpha was greater than 0.7 for all latent variables except ‘Economy’ (Cronbach’s alpha[*]=[*] 0.57) and all first eigen-values were[*]≥[*] 1. We then assessed the structural part of the PLS-PM using goodness-of-fit for the entire model and R 2 for endogenous variables. Goodness-of-fit, which is an index of prediction of the entire model, was 0.72, and the mean bootstrapped R 2 values of the latent variables ranged from 0.81 to 0.92. Together, this information indicates that the PLS-PM fit the data well and has reasonable predictive power. In order to handle the multiple arrays of parameters, latent variables were defined as Economy, Environment, and Forest Management, which are quantified by <b>indicator</b> <b>variables</b> containing the actual time series data. After running the model for non-migratory forest birds, we fit the first model again, but then added separate latent variable for the two other groups of birds. We analyzed temporal trends in bird abundance using linear regression analysis.|$|R
30|$|The use of step {{functions}} as each tree is grown {{can produce a}} very large number of new predictors. A single predictor such as age, might ultimately be represented by many <b>indicator</b> <b>variables</b> for different break points and many indicators for interaction effects. A search using, for example, 20 identified predictors such as gender and prior record, may be implemented with several hundred <b>indicator</b> <b>variables.</b> As a result, information in the initial 20 predictors can be more effectively exploited.|$|R
30|$|Iiyes, yes, Iiyes, no, Iino, yes, and Iino,  no are <b>indicator</b> <b>variables</b> {{equal to}} 0 or 1, {{depending}} on the outcome for each participant.|$|R
