7|67|Public
25|$|From {{the point}} of view of the author of a routine, raising an {{exception}} is a useful way to signal that a routine could not execute normally - for example, when an input argument is invalid (e.g. value is outside of the domain of a function) or when a resource it relies on is unavailable (like a missing file, a hard <b>disk</b> <b>error,</b> or out-of-memory errors). In systems without exceptions, routines would need to return some special error code. However, this is sometimes complicated by the semipredicate problem, in which users of the routine need to write extra code to distinguish normal return values from erroneous ones.|$|E
5000|$|... is used {{to verify}} the {{integrity}} of files, as virtually any change to a file will cause its MD5 hash to change. Most commonly, [...] {{is used to}} verify that a file has not changed {{as a result of}} a faulty file transfer, a <b>disk</b> <b>error</b> or non-malicious meddling. The [...] program is included in most Unix-like operating systems, or compatibility layers such as Cygwin.|$|E
50|$|From {{the point}} of view of the author of a routine, raising an {{exception}} is a useful way to signal that a routine could not execute normally - for example, when an input argument is invalid (e.g. value is outside of the domain of a function) or when a resource it relies on is unavailable (like a missing file, a hard <b>disk</b> <b>error,</b> or out-of-memory errors). In systems without exceptions, routines would need to return some special error code. However, this is sometimes complicated by the semipredicate problem, in which users of the routine need to write extra code to distinguish normal return values from erroneous ones.|$|E
40|$|Data {{recoverability}} in {{the face}} of partial <b>disk</b> <b>errors</b> is an important prerequisite in modern storage. We have designed and implemented a prototype disk system that automatically ensures the integrity of stored data, and transparently recovers vital data in the event of integrity violations. We show that by using pointer knowledge, effective integrity assurance can be performed inside a block-based disk with negligible performance overheads. We also show how semantics-aware replication of blocks can help improve the recoverability of data in the event of partial <b>disk</b> <b>errors</b> with small space overheads. Our evaluation results show that for normal user workloads, our disk system has a performance overhead of only 1 – 5 % compared to traditional disks...|$|R
25|$|Because DOS was not {{designed}} for multitasking purposes, Windows versions such as 9x that are DOS-based lack File System security, such as file permissions. Further, if the user uses 16-bit DOS drivers, Windows can become unstable. Hard <b>disk</b> <b>errors</b> often plague the Windows 9x series.|$|R
40|$|ABSTRACT Data {{recoverability}} in {{the face}} of partial <b>disk</b> <b>errors</b> is an importantprerequisite in modern storage. We have designed and implemented a prototype disk system that automatically ensures the integrity ofstored data, and transparently recovers vital data in the event of integrity violations. We show that by using pointer knowledge, ef-fective integrity assurance can be performed inside a block-based disk with negligible performance overheads. We also show howsemantics-aware replication of blocks can help improve the recoverability of data in the event of partial <b>disk</b> <b>errors</b> with small spaceoverheads. Our evaluation results show that for normal user workloads, our disk system has a performance overhead of only 1 - 5 %compared to traditional disks. Categories and Subject Descriptors D. 4. 2 [Operating Systems]: Storage Management [...] Secondary Stor-ag...|$|R
5000|$|The drive-head {{mechanism}} {{installed in}} the early production years is notoriously easy to misalign. The {{most common cause of}} the 1541's drive head knocking and subsequent misalignment is copy-protection schemes on commercial software. The main cause {{of the problem is that}} the disk drive itself does not feature any means of detecting when the read/write head reaches track zero. Accordingly, when a disk is not formatted or a <b>disk</b> <b>error</b> occurs, the unit tries to move the head 40 times in the direction of track zero (although the 1541 DOS only uses 35 tracks, the drive mechanism itself is a 40-track unit, so this ensured track zero would be reached no matter where the head was before). Once track zero is reached, every further attempt to move the head in that direction would cause it to be rammed against a solid stop: for example, if the head happened to be on track 18 (where the directory is located) before this procedure, the head would be actually moved 18 times, and then rammed against the stop 22 times. This ramming gives the characteristic [...] "machine gun" [...] noise and sooner or later throws the head out of alignment.|$|E
40|$|We {{introduce}} {{a new type of}} cubature formula for the evaluation of an integral over the disk with respect to a weight function. The method is based on an analysis of the Fourier series of the weight function and a reduction of the bivariate integral into an infinite sum of univariate integrals. Several experimental results show that the accuracy of the method is superior to standard cubature formula on the <b>disk.</b> <b>Error</b> estimates provide the theoretical basis for the good performance of the new algorithm. Comment: 22 page...|$|E
40|$|Abstract-In most {{real-time}} multimedia systems, major design bottlenecks {{are related}} to memory organisation and management related issues. Usually, {{in order to meet}} design constraints at low cost, designers of the memory management units directly think about lowlevel (register-transfer level) implementation aspects of the architecture. However, this approach typically results into time-consuming global design iterations. In contrast, we propose a high-level design exploration methodology where design bottlenecks related especially to the address generation issues are exposed as early as possible in the design trajectory, hence reducing design time considerably. The effectiveness of our approach is illustrated on the memory processor of an optical <b>disk</b> <b>error</b> correction application. Using our approach, a substantial part of the design search space has been explored in a few man-weeks instead of the months of effort required by more conventional design flows. Furthermore, not only design time is dramatically improved but also implementation cost and system performance, hence resulting in a highly productive design flow...|$|E
50|$|Officially, SDMAC rev 02 {{requires}} a Ramsey 04, and SDMAC 04 a Ramsey 07 counterpart. but SDMAC 04 + Ramsey 04 combinations {{have been reported}} to work as well.A combination of SDMAC 02 + Ramsey 07 generally works, but major hard <b>disk</b> <b>errors</b> have been reported.|$|R
5000|$|Disk mirroring. This {{was done}} in the {{filesystem}} and not the device driver, so that slightly (or even completely) different devices could still be mirrored together. Mirroring a small hard disk to the floppy was a popular way to test mirroring as ejecting the floppy was an easy way to induce <b>disk</b> <b>errors.</b>|$|R
25|$|Some drive {{mechanisms}} {{such as the}} Apple II 5.25 inch drive without a track zero sensor, produce very characteristic mechanical noises when trying to move the heads past the reference surface. This physical striking {{is responsible for the}} 5.25 inch drive clicking during the boot of an Apple II, and the loud rattles of its DOS and ProDOS when <b>disk</b> <b>errors</b> occurred and track zero synchronization was attempted.|$|R
40|$|Radio signal {{strength}} (RSS) {{is an attractive}} property since it can be obtained without additional hardware. Based on various signal propagation models that describe the relationship between RSS and distance, RSS can be employed in network connectivity, localization, and link quality issues in wireless sensor networks (WSNs). However, the deterministic propagation models, in which RSS is only determined by distance, are far from reality. We carefully design a series of experiments using MICA 2 nodes in real environments to investigate the parameters besides distance–– frequency, variation of transceivers, antenna orientation, battery voltage, temporal-spatial properties of environment, and environmental dynamics. All the parameters contribute to RSS irregularity, which can highly affect the design of localization and topology control algorithms in wireless sensor networks. We derive the log-normal error model for RSS-based distance estimation from the underlying propagation models and {{find out that the}} popular noisy <b>disk</b> <b>error</b> model always underestimates distance error in localization. A quality-based localization algorithm is proposed to handle the large error in RSS-based distance estimation. In WSNs, topology control achieves energy-efficiency by turning off redundant nodes, while still satisfying the given network requirement. We first propose several connectivity-based topology control algorithms, which assume that links are either connected or disconnected. As our experiments show that, besides the connected and disconnected region, a large number of links reside in the transitional region with fluctuating link quality. We then propose a link-quality-based topology control algorithm, which employ opportunistic transmission to catch the best transmission opportunities on transitional links. Our simulations demonstrate that opportunistic transmission can significantly improve energy-efficiency in topology control with low communication overhead. To the best of our knowledge, we are the first to consider link quality and apply opportunistic communication in topology control for WSNs...|$|E
5000|$|A small {{resident}} monitor which handles <b>disk</b> input/output, <b>error</b> recovery, and job-to-job transition, {{and loads}} programs {{to be executed}} from the core-image library on disk.|$|R
40|$|SD {{codes are}} erasure codes {{that address the}} mixed failure mode of current RAID systems. Rather than {{dedicate}} entire disks to erasure coding, as done in RAID- 5, RAID- 6 and Reed-Solomon coding, an SD code dedicates entire disks, plus individual sectors to erasure coding. The code then tolerates combinations of <b>disk</b> and sector <b>errors,</b> rather than solely <b>disk</b> <b>errors.</b> It is been an open problem to construct general codes that have the SD property, and previous work has relied o nMonte Carlo searches. In this paper, we present two general constructions that address the cases with one disk and two sectors, and two disks and two sectors. Additionally, we make an observation about shortening SD codes {{that allows us to}} prune Monte Carlo searches...|$|R
50|$|The {{most popular}} feature of Fast Hack'em was {{its ability to}} produce copies of copy-protected {{commercial}} software. When using the nibbler, disk copying was done on a very low level, bit-by-bit rather than using standard Commodore DOS commands. This effectively nullified the efficacy of deliberate <b>disk</b> <b>errors,</b> non-standard track layouts, and related forms of copy prevention. Copying a protected disk took approximately 60 seconds if being copied directly to another disk drive, or 3 minutes (plus several disk swaps) if performed using a single disk drive.|$|R
2500|$|Hard <b>disk</b> drive <b>error</b> rates statistic). We found {{consistently}} {{across all}} models that the geometric distribution {{is a poor}} ﬁt, while the Pareto distribution provides the best ﬁt.}} ...|$|R
50|$|Part of the Fast Hack'em {{disk copy}} {{software}} was a nibbler {{used to produce}} copies of copy protected Commodore 64 commercial software. When using the nibbler, disk copying was done on a very low level, bit-by-bit rather than using standard Commodore DOS commands. This effectively nullified the efficacy of deliberate <b>disk</b> <b>errors,</b> non-standard track layouts, and related forms of copy prevention. Copying a protected disk took approximately 60 seconds if being copied directly to another disk drive, or 3 minutes (plus several disk swaps) if performed using a single disk drive.|$|R
40|$|This paper {{analyzes}} the error {{behavior of a}} 3. 2 TB disk storage system. We report reliability data for 18 months of the prototype’s operation, and analyze 6 months of error logs from nodes in the prototype. We found that the disks drives {{were among the most}} reliable components in the system. We were also able to divide errors into eleven categories, comprising <b>disk</b> <b>errors,</b> network errors and SCSI errors that appeared repeatedly across all nodes. We also gained insight into the types of error messages reported by devices in various conditions, and the effects of these events on the operating system. We also present data from four cases of disk drive failures. These results and insights should be useful to any designer of a fault tolerant storage system. 1. ...|$|R
5000|$|Data {{scrubbing}} {{is another}} method {{to reduce the}} likelihood of data corruption, as <b>disk</b> <b>errors</b> are caught and recovered from before multiple errors accumulate and overwhelm the number of parity bits. Instead of parity being checked on each read, the parity is checked during a regular scan of the disk, often done as a low priority background process. Note that the [...] "data scrubbing" [...] operation activates a parity check. If a user simply runs a normal program that reads data from the disk, then the parity would not be checked unless parity-check-on-read was both supported and enabled on the disk subsystem.|$|R
40|$|Although rare in {{absolute}} terms, undetected CPU, memory, and <b>disk</b> <b>errors</b> occur often enough at datacenter scale to significantly affect overall system reliability and availability. In this paper, we propose a new failure model, called Machine Fault Tolerance, {{and a new}} abstraction, a replicated write-once trusted table, to provide improved resilience to these types of failures. Since most machine failures manifest in application server and operating system code, we assume a Byzantine model for {{those parts of the}} system. However, by assuming that the hypervisor and network are trustworthy, we are able to reduce the overhead of machine-fault masking to be close to that of non-Byzantine Paxos. 1...|$|R
40|$|High {{performance}} computing (HPC) systems frequently suffer errors and failures from hardware components that negatively impact {{the performance of}} jobs run on these systems. We analyzed system logs from two HPC systems at Purdue University and created statistical models for memory and hard <b>disk</b> <b>errors.</b> We created a small-scale error injection testbed—using a customized QEMU build, libvirt, and Python—for HPC application programmers to test and debug their programs in a faulty environment so that programmers can write more robust and resilient programs before deploying them on an actual HPC system. The deliverables for this project are the fault injection program, the modified QEMU source code, and the statistical models used for driving the injection. ...|$|R
50|$|In 2012, Phoronix {{wrote an}} {{analysis}} of ReFS vs Btrfs, a copy-on-write file system for Linux. Their features are similar, with both supporting checksums, RAID-like use of multiple <b>disks,</b> and <b>error</b> detection/correction. However, ReFS lacks deduplication, copy-on-write snapshots, and compression, all found in Btrfs and ZFS.|$|R
30|$|This error {{plays an}} {{important}} role while operating the WTGs under fluctuating wind conditions. When sudden changes in wind direction happen (like gusty winds), it leads to erratic rotation of wind turbines which may affect the sensor that is located near the <b>disk.</b> This <b>error</b> occurs where maximum variation of wind fluctuation occurs.|$|R
5000|$|BitDynamics {{provides}} the storage cluster with scalability and self healing. [...] It performs proactive self verification of <b>disk</b> bit <b>errors</b> and then automatically heals that disk block or entire disk as needed. This aims {{to reduce the}} amount of work that BitSpread needs to do on a component failure and is expected to improve overall system reliability.|$|R
40|$|Recent {{research}} has shown that even modern hard disks have complex failure modes that do not conform to “failstop” operation. Disks exhibit partial failures like block access errors and block corruption. Commodity operating systems are required to deal with such failures as commodity hard disks are known to be failure-prone. An important operating system component that is exposed to disk failures is the virtual memory system. In this paper, we examine the failure handling policies of different virtual memory systems for different classes of partial <b>disk</b> <b>errors.</b> We use type and context aware fault injection to explore as many of the internal code paths as possible. From experiments, we find that failure handling policies in current virtual memory systems are at best simplistic, and often inconsistent or even absent. Our fault injection technique also identifies bugs in the failure handling code in these systems. The study identifies possible reasons for poor failure handling, which can help in the design of a failure-aware virtual memory system. 1...|$|R
40|$|Abstract. Large-scale TPC workloads are {{critical}} {{for the evaluation of}} datacenter-scale storage systems. However, these workloads have not been previously characterized, in-depth, and modeled in a DC environment. In this work, we categorize the TPC workloads into storage threads that have unique features and characterize the storage activity of TPCC, TPCE and TPCH based on I/O traces from real server installations. We also propose a framework for modeling and generation of large-scale TPC workloads, which allows us to conduct a wide spectrum of storage experiments without requiring knowledge on the structure of the application or the overhead of fully deploying it in different storage configurations. Using our framework, we eliminate the time for TPC setup and reduce the time for experiments by two orders of magnitude, due to the compression in storage activity enforced by the model. We demonstrate the accuracy of the model and the applicability of our method to significant datacenter storage challenges, including identification of early <b>disk</b> <b>errors,</b> and SSD caching...|$|R
40|$|This paper {{discusses}} {{the results of}} a measurement-based analysis of real error data collected from a DEC VAXcluster multicomputer system. In addition to evaluating basic system dependability characteristics such as error and failure distributions and hazard rates for both individual machines and for the VAXcluster, reward models were developed to analyze the impact of failures on the system as a whole. The results show that more than 46 percent of all failures were due to errors in shared resources. This is despite the fact that these errors have a recovery probability greater than 0. 99. The hazard rate calculations show that not only errors, but also failures occur in bursts. Approximately 40 percent of all failures occur in bursts and involved multiple machines. This result indicates that correlated failures are significant. Analysis of rewards shows that software errors have the lowest reward (0. 05 vs 0. 74 for <b>disk</b> <b>errors).</b> The expected reward rate (reliability measure) of the VAXcluster drops to 0. 5 in 18 hours for the 7 -out-of- 7 model and in 80 days for the 3 -out-of- 7 model...|$|R
5000|$|Retry (shortcut key R): Attempts the {{operation}} again. [...] "Retry" [...] made sense if the user could rectify the problem. For example, if the user simply forgot {{to close the}} drive latch, they could close it, retry, and the system would continue where it left off. On early hardware, retrying a <b>disk</b> read <b>error</b> would sometimes be successful, but as disk drives improved, this became far less likely, although the message itself appeared less often.|$|R
5000|$|Ignore (shortcut key I): Return success {{status to}} the calling program or routine, despite {{the failure of}} the operation. For instance, a <b>disk</b> read <b>error</b> could be ignored and DOS would return {{whatever}} data was in the read buffer, which might contain some of the correct data from the disk. [...] "Ignore" [...] did not appear in cases where it was impossible for the data to be used; for instance, a missing disk could not be ignored.|$|R
50|$|On modern {{operating}} systems with graphical user interfaces, error messages are often displayed using dialog boxes. Error messages are used when user intervention is required, {{to indicate that}} a desired operation has failed, or to relay important warnings (such as warning a computer user that they are almost out of hard <b>disk</b> space). <b>Error</b> messages are seen widely throughout computing, and are part of every operating system or computer hardware device. Proper design of error messages is an important topic in usability and other fields of human-computer interaction.|$|R
40|$|Modern storage systems orchestrate a {{group of}} disks to achieve their {{performance}} and reliability goals. Even though such systems are designed to withstand the fail-ure of individual disks, failure of multiple disks poses {{a unique set of}} challenges. We empirically investigate disk failure data from a large number of production systems, specifically focusing on the impact of disk failures on RAID storage systems. Our data covers about one million SATA disks from 6 disk models for periods up to 5 years. We show how observed disk failures weaken the protection provided by RAID. The count of reallocated sectors correlates strongly with impending failures. With these findings we designed RAIDSHIELD, which consists of two components. First, we have built and evaluated an active defense mechanism that moni-tors the health of each disk and replaces those that are predicted to fail imminently. This proactive protection has been incorporated into our product and is observed to eliminate 88 % of triple <b>disk</b> <b>errors,</b> which are 80 % of all RAID failures. Second, we have designed and simulated a method of using the joint failure probability to quantify and predict how likely a RAID group is to face multi-ple simultaneous disk failures, which can identify disks that collectively represent a risk of failure even when no individual disk is flagged in isolation. We find in sim-ulation that RAID-level analysis can effectively identify most vulnerable RAID- 6 systems, improving the cover-age to 98 % of triple errors. ...|$|R
2500|$|If bit 14 (on FAT16) or bit 26 (on FAT32) is cleared, the {{operating}} system has encountered <b>disk</b> I/O <b>errors</b> on startup, a possible indication for bad sectors. Operating systems aware of this extension will interpret this as a recommendation {{to carry out a}} surface scan (SCANDISK) on the next boot. (A similar set of bitflags exists in the FAT12/FAT16 EBPB at offset 0x1A or the FAT32 EBPB at offset 0x36. While the cluster 1 entry can be accessed by file system drivers once they have mounted the volume, the EBPB entry is available even when the volume is not mounted and thus easier to use by disk block device drivers or partitioning tools.) ...|$|R
40|$|For {{contact surface}} measurement, {{estimating}} {{the profile of}} actual surface from the locus of the center point of the stylus is an inverse problem. The measurement process which transforms the profile of actual surface to the locus of the center point is a dilation operation with a <b>disk</b> with <b>error.</b> An erosion operation can be taken as the inverse mapping of the dilation to estimate the actual profile. By using category theory, erosion and dilation can be formulated as two functors between two categories of sets of vectors. Each category has sets of vectors as objects and inclusion functions between sets as morphisms. The functor of erosion is left adjoint to the functor of dilatio...|$|R
40|$|A {{methodology}} {{is presented}} for application of stress results from three dimensional {{finite element models}} for use in fracture mechanics computations. It {{is based on the}} assumption that a fatigue crack propagates in a plane normal to the maximum principal stress in a region that can be idealized as a rectangular plate. The methodology, recently implemented in the DARWIN ® probabilistic fracture mechanics program, is demonstrated for (1) a finite width plate with a centrally located hole and (2) an aircraft gas turbine engine rotor <b>disk.</b> Computational <b>error</b> associated with the methodology is less than 1 % compared to analytical and finite element solutions. The results can be applied to the probabilistic life prediction of components subjected to surface damage...|$|R
40|$|The SATA advertised {{bit error}} rate of one error in 10 terabytes is frightening. We moved 2 PB through {{low-cost}} hardware and saw five <b>disk</b> read <b>error</b> events, several controller failures, and many system reboots caused by security patches. We conclude that SATA uncorrectable read errors are not yet a dominant system-fault source - they happen, but are rare compared to other problems. We also conclude that UER (uncorrectable error rate) is not the relevant metric for our needs. When an uncorrectable read error happens, there are typically several damaged storage blocks (and many uncorrectable read errors.) Also, some uncorrectable read errors may be masked by the operating system. The more meaningful metric for data architects is Mean Time To Data Loss (MTTDL. ...|$|R
