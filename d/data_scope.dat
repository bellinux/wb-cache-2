36|427|Public
5000|$|<b>Data</b> <b>scope</b> (single point, history {{based on}} time range, history based on sample count), ...|$|E
5000|$|It isn't {{possible}} to do a cross-database query on 70% of company <b>data</b> (<b>scope)</b> ...|$|E
50|$|The <b>data</b> <b>scope</b> of {{the version}} 1.0 of EDILABO {{specifications}} covers all water themes (surface and ground water, drinking water, waste water,...) for all {{information related to}} physico-chemical and microbiological analysis.|$|E
5000|$|... default: {{allows the}} {{programmer}} {{to state that}} the default <b>data</b> <b>scoping</b> within a parallel region will be either shared, or none for C/C++, or shared, firstprivate, private, or none for Fortran. The none option forces the programmer to declare each variable in the parallel region using the data sharing attribute clauses.|$|R
40|$|This memo initiates the {{planning}} and participant appointment process for SEDAR 16, which will assess the king mackerel stock of the South Atlantic and Gulf of Mexico. SEDAR 16 workshops will begin in early 2008, though initial <b>data</b> <b>scoping</b> efforts should begin as early as October 2007. Councils are asked to begin planning for this project at their next meeting to meet the deadlines listed below...|$|R
30|$|In {{response}} to the above <b>data</b> <b>scoping</b> challenge, we allow the datascope annotation to take the following values: (i) middleware when data is purely middleware specific and {{it should not be}} exposed to the application-layer mediator synthesis; (ii) application when data belongs to the application layer, and must be forwarded to the application-layer mediator; (iii) replay-only when application layer data should only be shared between the set of operations from the same component; (iv) operation-only when application layer data may only be included in certain operations; (v) one-way when application layer data may only flow in one direction, i.e., only Request or Response messages may include this data.|$|R
5000|$|DataBoy: This {{cartridge}} {{plugs into}} the GBA game slot and converts the Game Boy into an RS-232 <b>data</b> <b>scope</b> (also known as serial line monitor or protocol analyzer). Users can play GB games, GBC games, and GBA games on it.|$|E
50|$|Data has a {{characteristic}} called a scope, which describes where {{in a program}} the data may be used. <b>Data</b> <b>scope</b> is either global (outside the scope of any function and with an indefinite extent), or local (created each time a function is called and destroyed upon exit).|$|E
50|$|The <b>data</b> <b>scope</b> of {{the version}} 2.0, under {{development}} (2008-2009), will also {{take into account}} biological sampling operations related to application of specific French methods (IBGN for oligochetes, IBD for diatoms,…) {{for the assessment of}} water biological quality, including the definition of queries and transfer of corresponding faunistic / floristic lists detailed results.|$|E
40|$|To ascertain, {{whether the}} {{reported}} <b>data</b> covers the <b>scope</b> of examination {{required for the}} Preservice (PSI) Inspection of general components listed in the tables of IWB- 2500, IWC- 2500, and IWD- 2500 of ASME Section XI, as described in: the applicable ASME Code, Section III and Section XI; the technical requirements manual (TRM) and/or site director procedures manual (SDPM); and the PSI program filed with the NRC. To ascertain whether the reported <b>data</b> covers the <b>scope</b> of examination required for th...|$|R
3000|$|... (ii) {{explaining}} the different trade-offs which includes tenant isolation versus (resource sharing, {{the number of}} users/requests, customizability, the size of generated <b>data,</b> the <b>scope</b> of control of the cloud application stack and business constraints) {{to be considered for}} optimal deployment of components with a guarantee of the required degree of tenant isolation (see “Results” and “Analysis” sections).|$|R
30|$|The site {{importing}} module described takes 2 D {{images of}} the site plan as input <b>data.</b> The <b>scope</b> of this research did not include an in-depth look at the impact of differing object heights {{in order to simplify}} the input process and facilitate computation. The system simply generated columns with a default height to represent the obstacles and boundaries appearing on the site.|$|R
5000|$|Historically, {{the term}} data {{presentation}} architecture {{is attributed to}} Kelly Lautt: [...] "Data Presentation Architecture (DPA) is a rarely applied skill set critical for the success and value of Business Intelligence. Data presentation architecture weds the science of numbers, data and statistics in discovering valuable information from data and making it usable, relevant and actionable with the arts of data visualization, communications, organizational psychology and change management {{in order to provide}} business intelligence solutions with the <b>data</b> <b>scope,</b> delivery timing, format and visualizations that will most effectively support and drive operational, tactical and strategic behaviour toward understood business (or organizational) goals. DPA is neither an IT nor a business skill set but exists as a separate field of expertise. Often confused with data visualization, data presentation architecture is a much broader skill set that includes determining what data on what schedule and in what exact format is to be presented, not just the best way to present data that has already been chosen. Data visualization skills are one element of DPA." ...|$|E
40|$|Knowledge {{discovery}} from <b>data</b> <b>scope</b> {{is quite}} a young concept but {{is becoming more and}} more popular. This bachelor's thesis is engaged in mining multiple level association rules. There are four different methods of investigation discussed. These offer an insight into the problem of concept and implementation of application. At the end we have confronted and reviewed the results of these four algorithms...|$|E
40|$|A {{digestive}} system disorders {{are the very}} important problem in human being universally. As apanacea, a preliminary investigation method is proposed before the endoscopy procedure is calledElectrogastrography. Electrogastrogram [EGG] is obtained from the human being cutaneously, which is noninvasive,easy procedure over endoscopy. EGG recorded for a 180 human being includes the normalindividual and patients with {{digestive system}} disorder such us Nausea, Dyspepsia, Vomiting, ulcer, etc. Adatabase is created for the analysis purpose. During the acquisition of EGG, the analog signal’s numericvalues are recorded as. bio file format using the <b>data</b> <b>scope.</b> Wavelet transform {{is used for the}} analysis ofEGG to find the deviation in frequency and Power from the power spectrum estimate plot obtained fordisorders patients compare to normal Individual. Wavelet Transform analysis includes the principlecomponent analysis, denosing of the signal and power spectrum estimation. In the proposed waveletanalysis, the data of a patient or normal individual is included for analysis. The EGG signal is reconstructedwith a data obtained from the <b>data</b> <b>scope.</b> This signal undergoes principle component analysis to remove thenoise in the EGG signal to obtain de-noised signal and it is plotted for power spectral density estimation withWelch power spectral Density Estimation. As a result of the proposed method the power variation is found tobe a range of 40 - 52 dB and frequency is detected as 0. 06 Hz to 0. 075 Hz for ulcer patient 0. 02 Hz to 0. 04 Hzfor dyspepsia patient and 0. 05 Hz to 0. 058 Hz for Normal Individual. The above said finding supports thephysician in the diagnosis of digestive system disorders at fair amount of accuracy...|$|E
50|$|Queries can be {{designed}} visually or in built-in dedicated SQL editor, then they can be executed. There is support for parametrized queries and searching through the <b>data.</b> The <b>scope</b> of the SQL supported is limited compared to raw database engines (MySQL and even SQLite) but the SQL dialect supported by Kexi is (by design) common to all supported database engines. This can simplify switching between engines.|$|R
50|$|MDL {{provides}} several enhancements {{to classical}} Lisp. It supports several built-in data types, including lists, strings and arrays, and user-defined data types. It offers multithreaded expression evaluation and coroutines. Variables can carry both a local value within a scope, and a global value, for passing <b>data</b> between <b>scopes.</b> Advanced built-in functions supported interactive debugging of MDL programs, incremental development, and reconstruction of source programs from object programs.|$|R
40|$|The MDO {{process of}} {{products}} can {{be supported by}} automation of analysis and optimisation steps. A Design and Engineering Engine (DEE) is a useful concept to structure this automation. To power the automatic analysis an agent based framework has been developed to support human and agent teams. The agent-based framework seeks to integrate the human and computer engineer into a hybrid design and built team, providing engineering services to the product design team. In this perspective four levels of scoping are identified; organisational scoping level, framework or integration level, tool or engineering service level and <b>data</b> <b>scoping</b> level. These four scoping levels are a good frame of reference to link the identified actors, the four main established functions of a framework and the recent contributions in engineering framework development. In order to demonstrate {{the application of the}} described methodologies, the design of a 6 persons high speed aircraft wingbox has been used as an example. The results of the feasilisation process are validated using FEM analysis and two ways of representing the results data are described. 1...|$|R
30|$|The {{theoretical}} {{foundation of}} transfer learning is data-size independent and, although not extensively investigated, may {{be applied to}} big data [9, 10] {{to achieve the same}} benefits as within normal data environments. Specifically, both heterogeneous and homogeneous transfer learning methods are applicable to big data scenarios. This is because one can leverage these methods to enhance a target task in a big data environment with a source domain. As described by [7], transfer learning is especially attractive in the big data environment because, due to the growth of big data repositories, one can enhance their machine learning task by using an available dataset from a similar domain. In doing so, one can avoid the costly effort of collecting new labeled data which is especially apparent in the big <b>data</b> <b>scope.</b>|$|E
40|$|The level- 1 {{results of}} {{probabilistic}} safety assessments (PSA) for 7 pressurized water reactors (PWR) and 5 boiling water reactors (BWR) were evaluated {{with regard to}} accident initiators, reliability <b>data,</b> <b>scope</b> and extent of operator actions and accident management (AM) measures, and core damage states. The objective was the comparison of the principal results of the studies and of the insights derived therefrom, {{as well as the}} identification of generic problems common to all studies and of approaches to their solution. By performing a top down comparison with identification of dominant sequences and system unavailabilities, insights of general interest for the use and improvement of PSAs and their application in safety relevant decision making were generated. (orig. /HP) SIGLEAvailable from TIB Hannover: RO 3190 (429) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|E
40|$|Target of cluster {{analysis}} is to group data {{represented as a}} vector of measurements or a point in a multidimensional space such that the most similar objects {{belong to the same}} group or cluster. The greater the similarity within a cluster and the greater the dissimilarity between clusters, the better the clustering task has been performed. Starting from the 1990 s, {{cluster analysis}} has emerged as an important interdisciplinary field, applied to several heterogeneous domains with numerous applications, including among many others social sciences, information retrieval, natural language processing, galaxy formation, image segmentation, and biological <b>data.</b> <b>Scope</b> {{of this paper is to}} provide an overview of the main types of criteria adopted to classify and partition the data and to discuss properties and state-of-the-art solution approaches, with special emphasis to the combinatorial optimization and operational research perspective...|$|E
40|$|We {{discuss the}} design of a quasi-statically typed {{language}} for XML in which data may be associated with different structures and different algebras in different scopes, whilst preserving identity. In declarative <b>scopes,</b> <b>data</b> are trees and may be queried with the full flexibility associated with XML query algebras. In procedural <b>scopes,</b> <b>data</b> have more conventional structures, such as records and sets, and can be manipulated with the constructs normally found in mainstream languages. For its original form of structural polymorphism, the language offers integrated support for the development of hybrid applications over XML, where data change form to re flct programming expectations and enable their enforcement...|$|R
40|$|Kenya has {{undertaken}} {{much work to}} date on data and knowledge issues, and has advanced this through the Kenya Data Foruma national initiative managed by the Deputy President's office that aims {{to develop and implement}} a long term sustainable local data strategy. Kenya, however, lacks an organized framework for collecting reliable and comparable data on philanthropy in the country; to help meet this need, the Philanthropy Sector in Kenya has come together over the decades through the creation of the East Africa Association of Grantmakers (EAAG) and more recently the Kenya Philanthropy Forum (KPF) and its Data Sub-group. In June 2015 the KPF organized a Philanthropy Data Management convening that brought together over 30 foundations and trusts to explore opportunities for strengthening data collection, management, and sharing data in the philanthropy forum for greater impact and influence on national development efforts in Kenya. As a result, certain outcomes and aspirations were agreed upon. They included: Establishing the principles for data management for philanthropy. Expanding the forum so that participation reflects the size and diversity of existing forms of philanthropy. Developing a standardized tool for data collection. Actively engaging in the existing philanthropy data initiative. Partnering with the Kenya National Bureau of Statistics (KNBS) to establish data sets that effectively capture the contribution of philanthropy in Kenya's development. The June 2015 meeting highlighted the urgent need for philanthropic data and that "Kenya lacks an organized framework for collecting reliable and comparable data on philanthropy in the country. " To begin systematically addressing these recommendations, Foundation Center (a philanthropic support organisation based in New York) designed a multi-stage Data Strategy and Capacity Building Program, working in partnership with KPF, EAAG, Kenya Community Development Foundation (KCDF), and the Sustainable Development Goals Philanthropy Platform (SDGPP). This initiative was kicked off at a special "Data Scoping Meeting" of the KPF on 28 April 2016, attended by a total of 51 participants representing across section of Kenyan foundations, trusts, and support organisations. As reflected in the meeting agenda (see Appendix A), the objectives of the <b>Data</b> <b>Scoping</b> Meeting were as follows: 1. Establish principles for collaborative data and knowledge management 2. Understand the core data needs of philanthropy in Kenya 3. Leverage available technologies for collecting and sharing data and knowledge 4. Leverage global knowledge for local purposes 5. Identify data challenges and set local goals This report summarizes the outcomes of the <b>Data</b> <b>Scoping</b> Meeting and outlines next steps in preparation for a follow-up meeting on Data Capacity Building in the coming months...|$|R
30|$|From the {{perspective}} of honeypots, the IP address, timestamp, and Internet protocol are data collected in all honeypots. Due to the abovementioned broad definition of personal data, all of this should be considered personal <b>data</b> within the <b>scope</b> of the <b>Data</b> Protection Directive.|$|R
40|$|We {{conducted}} six between-subjects survey {{experiments to}} examine how judgments of cyber-crime vary {{as a function of}} characteristics of the crime. The experiments presented vignettes that described a fictional cybercrime in which someone broke into an organization’s network and downloaded data records. In each experiment, we manipulated the vignettes according to one dimension per experiment: type of <b>data,</b> <b>scope,</b> motivation, the organization’s co-responsibility for the crime, consequences, and context. Participants were U. S. residents recruited via Amazon Mechanical Turk. We find that scope (the number of records downloaded) and the attacker’s motivation had significant effects on the perceived seriousness of the crime. Participants also recommended harsher punishments when the monetary costs of the cybercrime were higher. Furthermore, participants considered cybercrimes committed by activists to be significantly less blameworthy, and deserving of significantly lighter sentences, than cybercrimes committed for profit—contrary to the position sometimes taken by U. S. prosecutors. ...|$|E
40|$|Abstract SEMATECH, a US based {{consortium}} of ten major semiconductor manufacturers, {{is developing a}} comprehensive system {{for the design of}} ICs below. 25 µm, which exploits hierarchy, constraint directives, incremental processing, and concurrent design and analysis. This development of SEMATECH's Chip Hierarchical Design System (CHDS) includes major technological investments in algorithms for design planning, parasitic extraction, and signal integrity verification. The foundation of CHDS is an open design model and API upon which these tools are integrated. This model must support a number of critical requirements including: • A <b>data</b> <b>scope</b> that includes connectivity, electrical data, physical data, and timing; • Design hierarchy and incremental access to data; • A central delay architecture; • Efficient, application-selected views of the data. This paper introduces the Integrated Data Model technology being used for CHDS Beta development and its use {{as the basis for the}} design of an industry-open specification called the CHDS Technical Data Specification (CHDStd). I...|$|E
40|$|Abstract: Following our {{previous}} research on developing a dynamic virtual city system (Sheffield Urban Contextual Databank, SUCoD), the paper reports on a study of applying the virtual city resources to an undergraduate urban design course. The study focuses on how the multi-dimensional and multiple types of urban contextual data {{can be used by}} student designers directly for urban visual analysis and design development. A link is made with the Serial Vision in Townscape first proposed by Gordon Cullen, which sets out an experiential approach to how a living city environment should be read and understood. Drawing on the project works produced by the students, some patterns of generating urban narratives and 3 D spatial designs were observed. Although the current experiment with SUCoD is limited in terms of <b>data</b> <b>scope</b> and modelling capabilities, it reveals a future direction to follow that can turn conventional virtual cities into Web-based online services capable of supporting urban design analyses and syntheses directly. ...|$|E
50|$|Authors submit data to Dryad either {{when the}} {{associated}} article is under review or {{has been accepted}} for publication. The choice depends on whether the journal includes <b>data</b> within the <b>scope</b> of peer reviewer. Authors may also submit data after an article has been published.|$|R
40|$|As part of Uganda's {{commitment}} to the Sustainable Development Agenda, the country has made substantial progress toward improved national development data—including the launch of a Development Data Hub supported by Development Initiatives and a review of open data readiness jointly undertaken {{by the government and}} the World Bank. Uganda however, lacks an organized framework for collecting and sharing reliable and comparable data on philanthropy. As such, the newly established Uganda National Philanthropy Forum (UPF) represents a key mechanism for the sector to consolidate its e orts and hone its contributions to national development. The forum was established in October 2015, facilitated by the East Africa Association of Grantmakers (EAAG), in partnership with Independent Development Fund (IDF), Development Network of Indigenous Voluntary Associations (DENIVA) and GoBig Hub. Its objective is to explore strategies for consolidating and organizing the philanthropy sector in Uganda. As a follow up to the UPF agenda on advancing philanthropy data in Uganda, EAAG and the Foundation Center in partnership with IDF and DENIVA hosted a <b>Data</b> <b>Scoping</b> Meeting on October 25 th 2016. The objective of the meeting was to explore opportunities to strengthen data sharing and management to enhance the sector's coordination and in uence on national development policy. The meeting brought together 35 foundations, trusts and other local philanthropy organizations...|$|R
40|$|In this study, we {{used the}} {{approach}} of topic modeling to uncover the possible structure of research topics {{in the field of}} Informetrics, to explore the distribution of the topics over years, and to compare the core journals. In order to infer the structure of the topics in the field, the data of the papers published in the Journal of Informetricsand Scientometrics during 2007 to 2013 are retrieved from the database of the Web of Science as input of the approach of topic modeling. The results of this study show that when the number of topics was set to 10, the topic model has the smallest perplexity. Although <b>data</b> <b>scopes</b> and analysis methodsare different to previous studies, the generating topics of this study are consistent with those results produced by analyses of experts. Empirical case studies and measurements of bibliometric indicators were concerned important in every year during the whole analytic period, and the field was increasing stability. Both the two core journals broadly paid more attention to all of the topics in the field of Informetrics. The Journal of Informetricsput particular emphasis on construction and applications ofbibliometric indicators and Scientometrics focused on the evaluation and the factors of productivity of countries, institutions, domains, and journals...|$|R
40|$|Velocity in carbonates {{is largely}} {{controlled}} by porosity and pore types while in siliciclastic rocks the velocity {{is controlled by}} porosity, mineralogy and overburden pressure. In mixed carbonate-siliciclastic systems these controlling factors compete with each other. Few data sets exists that systematically assess the individual factors with the same methodology (eg., Anselmetti et al. 1997; et al. 1997). To close this knowledge gap we plan to undertake a comprehensive study of mixed carbonate-siliciclastic rocks. The project will utilize the newly acquired samples from the Neuquen Basin (outcrop and subsurface), the CSL database, and published literature data to assess how intrinsic factors (mineralogy, diagenesis) influence acoustic properties in mixed-carbonate siliciclastic rocks. The findings will have implications for general prediction of lithology and pore type from acoustic <b>data.)</b> <b>Scope</b> of Work This project will study the acoustic behavior of mixed carbonate-siliciclastic rocks of the Lower Cretaceous Quintuco Formation from the Neuquen Basin (Argentina) and dat...|$|E
40|$|The {{past years}} have shown an {{enormous}} advancement in sequencing and array-based technologies, producing supplementary or alternative {{views of the}} genome stored in various formats and databases. Their sheer volume and different <b>data</b> <b>scope</b> pose a challenge to jointly visualize and integrate diverse data types. We present AmalgamScope a new interactive software tool focusing on assisting scientists with the annotation {{of the human genome}} and particularly the integration of the annotation files from multiple data types, using gene identifiers and genomic coordinates. Supported platforms include next-generation sequencing and microarray technologies. The available features of AmalgamScope range from the annotation of diverse data types across the human genome to integration of the data based on the annotational information and visualization of the merged files within chromosomal regions or the whole genome. Additionally, users can define custom transcriptome library files for any species and use the file exchanging distant server options of the tool...|$|E
40|$|As a kind {{of novel}} {{feedforward}} neural network with single hidden layer, ELM (extreme learning machine) neural networks are studied for the identification and control of nonlinear dynamic systems. The property of simple structure and fast convergence of ELM can be shown clearly. In this paper, {{we are interested in}} adaptive control of nonlinear dynamic plants by using OS-ELM (online sequential extreme learning machine) neural networks. Based on <b>data</b> <b>scope</b> division, the problem that training process of ELM neural network is sensitive to the initial training data is also solved. According to the output range of the controlled plant, the data corresponding to this range will be used to initialize ELM. Furthermore, due to the drawback of conventional adaptive control, when the OS-ELM neural network is used for adaptive control of the system with jumping parameters, the topological structure of the neural network can be adjusted dynamically by using multiple model switching strategy, and an MMAC (multiple model adaptive control) will be used to improve the control performance. Simulation results are included to complement the theoretical results...|$|E
40|$|Opening Remarks: Data Mining {{has been}} developed, though vigorously, under rather ad hoc and vague concepts. For further development, a close {{examination}} on its foundations seems necessary. The central goal in this workshop {{is to explore}} various fundamental issues of <b>data</b> mining. The <b>scope</b> of the workshop includes...|$|R
50|$|In {{this kind}} of file structure, each piece of data is {{embedded}} in a container that somehow identifies the <b>data.</b> The container's <b>scope</b> can be identified by start- and end-markers of some kind, by an explicit length field somewhere, or by fixed requirements of the file format's definition.|$|R
40|$|Abstract—This paper {{describes}} work {{in progress}} whereby a dynamic data replication scheme, under market-based control is applied to a proposed autonomic distributed data layer for managing configuration management <b>data.</b> The <b>scope</b> of the proposed autonomic system is described and also some experimental work presented. Analytic approximations of the performance achieved for management requests under various static data replication schemes are compared with event-based simulations of the same system under dynamic market-based replication control. The purpose of this comparison is to evaluate the performance and suitability of a market-based control approach for such autonomic replication systems...|$|R
