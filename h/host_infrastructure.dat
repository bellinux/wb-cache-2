4|252|Public
40|$|This {{dissertation}} {{studies the}} mitigation {{of the performance}} and security interference between guest virtual machines (VMs) in public clouds. The goals are to characterize the impact of VM interference, uncover {{the root cause of}} the negative impact, and design novel techniques to mitigate such impact. The central premise of this dissertation is that by identifying the shared resources that cause the VM interference and by exploiting the properties of the workloads that share these resources with adapted scheduling policies, public cloud services can reduce conflicts of resource usage between guests and hence mitigate their interference. Current techniques for conflict reduction and interference mitigation overlook the virtualization semantic gap between the cloud <b>host</b> <b>infrastructure</b> and guest virtual ma- chines and the unique challenges posed by the multi-tenancy service model necessary to support public cloud services. This dissertation deals with both performance and security interference problems. It characterizes the impact of VM interference on inter-VM network latency using live measurements in a real public cloud and studies the root cause of the negative impact with controlled experiments on a local testbed. Two methods of improving the inter-VM net- work latency are explored. The first approach is a guest-centric solution that exploits the properties of application workloads to avoid interference without any support from the underlying <b>host</b> <b>infrastructure.</b> The second approach is a host-centric solution that adapts the scheduling policies for the contented resources that cause the interference without guest cooperation. Similarly, the characteristics of cache-based cross-VM attacks are studied in detail using both live cloud measurements and testbed experiments. To mitigate this security interference, a partition-based VM scheduling system is designed to reduce the effectiveness of these cache-based attacks...|$|E
40|$|Loose {{coupling}} {{and preserving}} safe changes are two key criteria for composing self-managing services. Composition of adaptive control components with business services without {{interfering with the}} original service operations {{is complicated by the}} dynamic and highly distributed nature of service-oriented systems. Essentially, encapsulating control logic into abstract logical models enables a clear separation of concerns, with states and transitions indicating the logical control flow. The challenge is to seamlessly integrate these models with services and their <b>host</b> <b>infrastructure</b> as aunified self-managing environment. In this paper, we present an architectural solution towards process-based composition and coordination of self-managing services. This architecture framework leverages business process models to produce declarative and executable control models. We discuss the problem context and outline the research challenges of such an approach...|$|E
40|$|We {{recently}} installed {{an automated}} practitioner order entry system on our busiest inpatient wards and critical care units. The installation followed 20 months preparation {{in which we}} created the workstation, network, and <b>host</b> <b>infrastructure,</b> developed requisite policies, recruited personnel to support the system, and installed the software {{in areas where the}} pace of order entry was less intense. Since implementing automated order entry, we have experienced problems such as an increase in time required for practitioners to enter orders, workflow changes on inpatient units, difficulties with patient transfers, and others. Our user support system has been heavily used during the transition period. Software tailoring and enhancements designed to address these problems are planned, as is installation of the order entry system in remaining clinical units in our medical centers...|$|E
40|$|Large-scale <b>hosting</b> <b>infrastructures</b> require {{automatic}} system anomaly management to achieve continuous system operation. In this paper, {{we present a}} novel adaptive runtime anomaly prediction system, called ALERT, to achieve robust <b>hosting</b> <b>infrastructures.</b> In contrast to traditional anomaly detection schemes, ALERT aims at raising advance anomaly alerts to achieve just-in-time anomaly prevention. We propose a novel context-aware anomaly prediction scheme to improve prediction accuracy in dynamic <b>hosting</b> <b>infrastructures.</b> We have implemented the ALERT system and deployed it on several production <b>hosting</b> <b>infrastructures</b> such as IBM System S stream processing cluster and PlanetLab. Our experiments show that ALERT can achieve high prediction accuracy {{for a range of}} system anomalies and impose low overhead to the <b>hosting</b> <b>infrastructure...</b>|$|R
30|$|Currently, Internet {{users can}} access content {{anywhere}} and anytime, without needing {{to consider the}} <b>hosting</b> <b>infrastructure.</b> Such <b>hosting</b> <b>infrastructure</b> consists of various machines with various capabilities that are maintained and managed by the service provider. Cloud computing enhances the capabilities of such infrastructure, which can access the Internet. Cloud service providers earn profits by providing services to cloud service users.|$|R
50|$|Libscore was {{developed}} via a co-sponsorship by Stripe (a payments processor) and DigitalOcean (a <b>hosting</b> <b>infrastructure</b> provider).|$|R
40|$|Public clouds {{have become}} a popular {{platform}} for building Internet-scale applications. Using virtualization, public cloud services grant customers full control of guest operating systems and applications, while service providers still retain the management of their <b>host</b> <b>infrastructure.</b> Because applications built with public clouds are often highly sensitive to response time, infrastructure builders strive to reduce the latency of their data center’s internal network. However, most existing solutions require modification to the software stack controlled by guests. We introduce a new host-centric solution for improving latency in virtualized cloud environments. In this approach, we extend a classic scheduling principle—Shortest Remaining Time First—from the virtualization layer, through the host network stack, to the network switches. Experimental and simulation results show that our solution can reduce median latency of small flows by 40 %, with improvements in the tail of almost 90 %, while reducing throughput of large flows by less than 3 %...|$|E
50|$|Encrypted data at Oystor follows {{security}} protocols at {{the physical}} <b>hosting</b> <b>infrastructure,</b> application and data transfer protocols, and the user interface.|$|R
5000|$|OneNeck {{currently}} provides hosted application management, managed <b>hosting,</b> <b>infrastructure</b> {{services and}} hybrid IT cloud services and has divided those offerings into five segments: ...|$|R
50|$|This {{enabled the}} company to provide dating software, {{membership}} database, payment processing, customer support, <b>hosting</b> <b>infrastructure</b> and tax processing to partners looking to set up dating sites.|$|R
40|$|Recent {{studies show}} that a {{significant}} part of Internet traffic is delivered through Web-based applications. To cope with the increasing demand for Web content, large scale content <b>hosting</b> and delivery <b>infrastructures,</b> such as data-centers and content distribution networks, are continuously being deployed. Being able to identify and classify such <b>hosting</b> <b>infrastructures</b> is helpful not only to content producers, content providers, and ISPs, but also to the research community at large. For example, to quantify the degree of <b>hosting</b> <b>infrastructure</b> deployment in the Internet or the replication of Web content. In this paper, we introduce Web Content Cartography, i. e., the identification and classification of content <b>hosting</b> and delivery <b>infrastructures.</b> We propose a lightweight and fully automated approach to discover <b>hosting</b> <b>infrastructures</b> based only on DNS measurements and BGP routing table snapshots. Our experimental results show that our approach is feasible even with a limited number of well-distributed vantage points. We find that some popular content is served exclusively from specific regions and ASes. Furthermore, our classification enables us to derive content-centric AS rankings that complement existing AS rankings and shed light on recent observations about shifts in inter-domain traffic and the AS topology...|$|R
40|$|Abstract—Quality-of-service (QoS) {{management}} {{often requires}} a continuous monitoring service to provide updated information about different hosts and network links in the managed system. However, it is a challenging task to achieve both scalability and precision for monitoring various intra-node and inter-node metrics (e. g., CPU, memory, disk, network delay) in a large-scale <b>hosting</b> <b>infrastructure.</b> In this paper, {{we present a}} novel OnLine Information Compression (OLIC) system to achieve scalable fine-grained <b>hosting</b> <b>infrastructure</b> monitoring. OLIC models continuous snapshots of a <b>hosting</b> <b>infrastructure</b> as a sequence of images and performs online monitoring data compression to significantly reduce the monitoring cost. We have implemented a prototype of the OLIC system and deployed it on the PlanetLab and NCSU’s virtual computing lab (VCL). We have conducted extensive experiments using a set of real monitoring data from VCL, Planetlab, and a Google cluster {{as well as a}} real Internet traffic matrix trace. The experimental results show that OLIC can achieve much higher compression ratios with several orders of magnitude less overhead than previous approaches. I...|$|R
50|$|TokBox is a PaaS (Platform as a Service) {{company that}} {{provides}} <b>hosted</b> <b>infrastructure,</b> APIs and tools required to deliver enterprise-grade WebRTC capabilities. It does so primarily through its proprietary OpenTok video platform for commercial application.|$|R
40|$|Web hosting service, {{web server}} farm, web server cluster, load balancing, scalability, {{performance}} analysis, SpecWeb Web <b>hosting</b> is an <b>infrastructure</b> service that allows to design, integrate, {{operate and maintain}} all of the infrastructure components required to run web-based applications. It includes Web server farms, network access, data staging tools and security firewalls. Web server farms are used in a Web <b>hosting</b> <b>infrastructure</b> {{as a way to}} create scalable and highly availabl...|$|R
40|$|Although cloud {{computing}} service providers offer opportunities {{for improving the}} administration, reliability, and maintenance of hosted services, they also concentrate network resources and data in {{a small number of}} cloud service providers. The concentration of data and resources introduces various associated risks, including sharing the underlying infrastructure with unknown (and untrusted) tenants and relying on the availability and security of the underlying infrastructure itself. These security risks represent some of the most significant barriers to the adoption of cloud-based services. To begin tackling these risks, a cloud <b>hosting</b> <b>infrastructure</b> should provide strong guarantees for resource and data isolation. This paper examines isolation problems with today’s cloud <b>hosting</b> <b>infrastructures</b> and proposes SilverLine, a collection of techniques to improve data and network isolation for a cloud tenant’s service. 1...|$|R
40|$|Hosting network {{services}} on virtual machines has become appealing for provisioning resources, saving power and enabling migration {{in case of}} service disruption, especially in data centers. Network services implemented widely over the Internet may enjoy similar benefits, because general-purpose <b>hosting</b> <b>infrastructures</b> such as PlanetLab and Amazon EC 2 are available to host them. However, such infrastructures are piecemeal and heterogeneous in terms of virtualization technologies, which {{makes it hard to}} use them all together to their full potential. To ease this challenge, we implemented Mobitopolo, a portable infrastructure service to deploy and migrate distributed {{network services}} spanning over heterogeneous <b>hosting</b> <b>infrastructures</b> while preserving the logical connections between the service components. To the best of our knowledge, Mobitopolo is the first virtualized execution environment to integrate all these attributes into one system...|$|R
50|$|HostPapa has web <b>hosting</b> <b>infrastructure,</b> {{including}} Hewlett-Packard {{hardware and}} servers, networked by Cisco equipment, in various tier one data centers in Canada {{and around the}} world. The company operates North American-based call centers for customer support in English, French, and Spanish. Its hosting services use Sucuri server software and CloudProxy.|$|R
40|$|Continued {{improvements}} in network bandwidth, cost, and ubiquitous access are enabling service providers to host desktop computing environments {{to address the}} complexity, cost, and mobility limitations of today’s personal computing infrastructure. However, distributed denial of service attacks can deny use of such services to users. We present A²M, a secure and attack-resilient desktop computing <b>hosting</b> <b>infrastructure.</b> A²M combines a stateless and secure communication protocol, a single-hop Indirection-based network (IBN) and a remote display architecture to provide mobile users with continuous access to their desktop computing sessions. Our architecture protects both the <b>hosting</b> <b>infrastructure</b> and the client’s connections against {{a wide range of}} service disruption attacks. Unlike any other DoS protection system, A²M takes advantage of its low-latency remote display mechanisms and asymmetric traffic characteristics by using multipath routing to send a small number of replicas of each packet transmitted from client to server. This packet replication through different paths, diversifies the client-server communication, boosting system resiliency and reducing end-toend latency. Our analysis and experimental results on PlanetLab demonstrate that A²M significantly increases the <b>hosting</b> <b>infrastructure’s</b> attack resilience even for wireless scenarios. Using conservative ISP bandwidth data, we show that we can protect against attacks involving thousands (150, 000) attackers, while providing good performance for multimedia and web applications and basic GUI interactions even when up to 30 % and 50 %, respectively, of indirection nodes become unresponsive...|$|R
5000|$|In 2005, SDS is {{purchased}} in transaction valued at $11.3 billion, {{by a team}} led by SilverLake. [...] Additional SDS acquisitions followed, including their 2007 purchase of Vericenter [...] to provide managed hosting and production availability and a 2010 acquisition of 365 Hosting Limited, an Ireland-based <b>hosted</b> <b>infrastructure</b> provider of Managed Services and Cloud offerings.|$|R
40|$|Abstract. Continued {{improvements}} in network bandwidth, cost, and ubiquitous access are enabling service providers to host desktop computing environments {{to address the}} complexity, cost, and mobility limitations of today’s personal computing infrastructure. However, distributed denial of service attacks can deny use of such services to users. We present A 2 M, a secure and attack-resilient desktop computing <b>hosting</b> <b>infrastructure.</b> A 2 M combines a stateless and secure communication protocol, a single-hop Indirection-based network (IBN) and a remote display architecture to provide mobile users with continuous access to their desktop computing sessions. Our architecture protects both the <b>hosting</b> <b>infrastructure</b> and the client’s connections against {{a wide range of}} service disruption attacks. Unlike any other DoS protection system, A 2 M takes advantage of its low-latency remote display mechanisms and asymmetric traffic characteristics by using multipath routing to send a small number of replicas of each packet transmitted from client to server. This packet replication through different paths, diversifies the client-server communication, boosting system resiliency and reducing end-toend latency. Our analysis and experimental results on PlanetLab demonstrate that A 2 M significantly increases the <b>hosting</b> <b>infrastructure’s</b> attack resilience even for wireless scenarios. Using conservative ISP bandwidth data, we show that we can protect against attacks involving thousands (150, 000) attackers, while providing good performance for multimedia and web applications and basic GUI interactions even when up to 30 % and 50 %, respectively, of indirection nodes become unresponsive. ...|$|R
30|$|Many DNS cache {{poisoning}} attacks occur by subverting the <b>hosting</b> <b>infrastructure</b> of DNS, e.g., {{domain registrar}} or name servers. Indeed, {{there is an}} increasing number of attacks by compromising the hosting side of DNS which allows to take over victim domains. Subverting a registrar or a name server is a lucrative avenue for cache poisoning when the attacker is not a MitM.|$|R
5000|$|The {{main feature}} {{highlighted}} in reviews {{is the fact}} that the Webydo code generator manages to replace the developer in the web design process, an achievement described as a [...] "web design revolution". Other remarked features are the rich, intuitive and easy-to-use web design software, providing ample room to manage the professional web design process and the good <b>hosting</b> <b>infrastructure.</b>|$|R
50|$|Typically, most <b>hosting</b> <b>infrastructures</b> {{are based}} on the {{paradigm}} of using a single physical machine to host multiple hosted services, including web, database, email, FTP and others. A single physical machine is not only a single point of failure, but also has finite capacity for traffic, that in practice can be troublesome for a busy website or for a website that is experiencing transient bursts in traffic.|$|R
5000|$|Logicworks {{specializes in}} HIPAA and PCI {{compliance}} <b>hosting</b> and <b>infrastructure</b> design on AWS and Private Cloud environments.|$|R
30|$|By {{supporting}} recontextualization, {{the proposed}} contextualization framework offers unambiguous means of triggering internal system events {{as a result}} of external events such as VM migration or a change in the surrounding <b>hosting</b> <b>infrastructure.</b> In the distributed file system case, recontextualization can be used to trigger a new optimization phase post-migration, either by calling designated functions in the file system client (if supported), or simply by restarting the client process.|$|R
50|$|Because {{objects are}} {{accessible}} by unmodified HTTP clients, S3 {{can be used}} to replace significant existing (static) web <b>hosting</b> <b>infrastructure.</b> The Amazon AWS Authentication mechanism allows the bucket owner to create an authenticated URL with time-bounded validity. That is, someone can construct a URL that can be handed off to a third-party for access for a period such as the next 30 minutes, or the next 24 hours.|$|R
50|$|Furthermore, in {{contrast}} to the multi-tenancy approaches to SaaS, a virtual appliance can also be deployed on-premises for customers that need local network access to the running application, or have security requirements that a third-party hosting model does not meet. The underlying virtualization technology also allows for rapid movement of virtual appliances instances between physical execution environments. Traditional approaches to SaaS fix the application in place on the <b>hosted</b> <b>infrastructure.</b>|$|R
40|$|Large-Scale {{distributed}} <b>hosting</b> <b>infrastructures</b> {{have become}} the basic platforms for several real-world production systems. But a challenging task is to achieve both scalability and high precision while monitoring {{a large number of}} intra-node attributes that contain information relating to each node and inter-node attribute that denote measurements between different nodes. This paper presents a new distributed monitoring framework Based on video coding techniques of named RBOIC (Replica Based Online Informatio...|$|R
40|$|Abstract—Large-scale <b>hosting</b> <b>infrastructures</b> {{have become}} the {{fundamental}} platforms for many real world systems such as cloud computing infrastructures, enterprise data centers, and massive data processing systems. However, it is a challenging task to achieve both scalability and high precision while monitoring {{a large number of}} intra-node and inter-node attributes (e. g., CPU usage, free memory, free disk, inter-node network delay). In this paper, we present the design and implementation of a Resilient self-Compressive Monitoring (RCM) system for large-scale <b>hosting</b> <b>infrastructures.</b> RCM achieves scalable distributed monitoring by performing online data compression to reduce remote data collection cost. RCM provides failure resilience to achieve robust monitoring for dynamic distributed systems where host and network failures are common. We have conducted extensive experiments using a set of real monitoring data from NCSU’s virtual computing lab (VCL), PlanetLab, a Google cluster, and real Internet traffic matrices. The experimental results show that RCM can achieve up to 200 % higher compression ratio and several orders of magnitude less overhead than the existing approaches. Index Terms—Online data compression, distributed system monitoring...|$|R
50|$|Twilio uses Amazon Web Services to <b>host</b> {{telephony}} <b>infrastructure</b> {{and provide}} connectivity between HTTP {{and the public}} switched telephone network (PSTN) through its APIs.|$|R
5000|$|Incorporated in April 2003, California-based Jupiter Hosting was {{acquired}} by NaviSite in 2007. Jupiter Hosting provides dedicated and complex managed <b>hosting</b> and <b>infrastructure</b> services.|$|R
30|$|We review DNS and DNS cache {{poisoning}} in Section 2. We discuss {{threats from}} (1) MitM adversaries and how attackers can obtain MitM capabilities (Section 2) and (2) vulnerabilities in name servers and zones <b>hosting</b> <b>infrastructure</b> (Section 2). We then review the dependencies within the DNS infrastructure in Section 2. We discuss DNSSEC and report on our measurements of DNSSEC adoption rate and deployment challenges. We then review application of DNSSEC for forensic analysis (Section 2). We conclude this work in Section 2.|$|R
30|$|Many DNS cache {{poisoning}} attacks occur by subverting the <b>hosting</b> <b>infrastructure</b> of DNS, e.g., {{domain registrar}} or name servers. By exploiting social engineering or vulnerabilities in domains registration interface, {{an attack against}} an infrastructure, run by a registrar, may expose all of the domains using it to domain hijacking attacks. Subverting a registrar or a name server is a lucrative avenue for cache poisoning when the attacker is not a MitM. Indeed, there is {{an increasing number of}} attacks by compromising the hosting side of DNS.|$|R
50|$|The {{organisation}} {{tends to}} support its aims by <b>hosting</b> <b>infrastructure</b> for semi-independent projects to develop. This approach to organising was hinted {{as one of its}} earliest projects was a project management service called KnowledgeForge, which runs on the KForge platform. KnowledgeForge allows sectoral working groups to have space to manage projects related to open knowledge. More widely, the project infrastructure includes both technical and face-to-face aspects. The organisation hosts several dozen mailing lists for virtual discussion, utilises IRC for real-time communications and also hosts events.|$|R
50|$|The Institute for New Culture Technologies/t0 was {{established}} in 1993 as an arts and culture related international competence platform for the critical use of information and communication technologies. Over the years it has pursued {{a broad range of}} transdisciplinary activities. From producing and <b>hosting</b> <b>infrastructure</b> to organizing conferences, festivals and exhibitions, local interventions and skill transfer, as well as international research and publishing. Konrad Becker and Francisco de Sousa Webber, who founded the institute, currently form the board of directors together with Felix Stalder.|$|R
30|$|The tourist {{supply is}} mostly {{concentrated}} in the eastern coastal part of Lefkada, where {{the main body of}} hotel infrastructures and rooms to let are located (Fig.  2 c). The emphasis placed on the tourist sector has resulted in a certain increase of <b>hosting</b> <b>infrastructures</b> (hotels and family-run rooms to let) during the last few years, resulting in a significant environmental degradation but also in conflicts against other sectors’ interests. The low class level of the majority of these infrastructures reflects the low budget tourist flows attracted in the area.|$|R
