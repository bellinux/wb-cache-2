0|10000|Public
50|$|<b>File</b> <b>system</b> types can be {{classified}} into disk/tape <b>file</b> <b>systems,</b> network <b>file</b> <b>systems</b> and special-purpose <b>file</b> <b>systems.</b>|$|R
40|$|From its inception, UNIX {{has been}} built around two {{fundamental}} entities: processes and files. In this chapter, {{we look at the}} implementation of files in Solaris and discuss the framework for <b>file</b> <b>systems.</b> 14. 1 <b>File</b> <b>System</b> Framework Solaris OS includes a framework, the virtual <b>file</b> <b>system</b> framework, under which multiple <b>file</b> <b>system</b> types are implemented. Earlier implementations of UNIX used a single <b>file</b> <b>system</b> type for all of the mounted <b>file</b> <b>systems,</b> typically, the UFS <b>file</b> <b>system</b> from BSD UNIX. The virtual <b>file</b> <b>system</b> framework was developed to allow Sun’s distributed computing <b>file</b> <b>system</b> (NFS) to coexist with the UFS <b>file</b> <b>system</b> in SunOS 2. 0; it became a standard part of System V in SVR 4 and Solaris OS. We can categorize Solaris <b>file</b> <b>systems</b> into the following types: Storage-based. Regular <b>file</b> <b>systems</b> that provide facilities for persistent storage and management of data. The Solaris UFS and PC/DOS <b>file</b> <b>systems</b> are examples. Network <b>file</b> <b>systems.</b> <b>File</b> <b>systems</b> that provide <b>files</b> that are accessible in a local directory structure but are stored on a remote network server; for example, NFS. Pseudo <b>file</b> <b>systems.</b> <b>File</b> <b>systems</b> that present various abstractions as files in a <b>file</b> <b>system.</b> The /proc pseudo <b>file</b> <b>system</b> represents the address space of a process as a series of files. 657 658 Chapter 14 <b>File</b> <b>System</b> Framework The framework provides a single set of well-defined interfaces that are <b>file</b> <b>system</b> independent; the implementation details of each <b>file</b> <b>system</b> are hidden behind these interfaces. Two key objects represent these interfaces: the virtual file, or vnode, and the virtual <b>file</b> <b>system,</b> or vfs objects. The vnode interfaces implement file-related functions, and the vfs interfaces implement <b>file</b> <b>system</b> management functions. The vnode and vfs interfaces direct functions to specific <b>file</b> <b>systems,</b> {{depending on the type of}} <b>file</b> <b>system</b> being operated on. Figure 14. 1 shows the <b>file</b> <b>system</b> layers. File-related functions are initiated through a system call or from another kernel subsystem and are directed to the appropriate <b>file</b> <b>system</b> by the vnode/vfs layer...|$|R
50|$|Distributed <b>file</b> <b>systems</b> can be {{optimized}} for different purposes. Some, {{such as those}} designed for internet services, including GFS, are {{optimized for}} scalability. Other designs for distributed <b>file</b> <b>systems</b> support performance-0intensive applications usually executed in parallel. Some examples include: MapR <b>File</b> <b>System</b> (MapR-FS), Ceph-FS, Fraunhofer <b>File</b> <b>System</b> (BeeGFS), Lustre <b>File</b> <b>System,</b> IBM General Parallel <b>File</b> <b>System</b> (GPFS), and Parallel Virtual <b>File</b> <b>System.</b>|$|R
50|$|For example, {{to migrate}} a FAT32 <b>file</b> <b>system</b> to an ext2 <b>file</b> <b>system.</b> First {{create a new}} ext2 <b>file</b> <b>system,</b> then copy the data to the <b>file</b> <b>system,</b> then delete the FAT32 <b>file</b> <b>system.</b>|$|R
5000|$|Blue Whale Clustered <b>file</b> <b>system</b> (BWFS) is {{a shared}} disk <b>file</b> <b>system</b> (also called {{clustered}} <b>file</b> <b>system,</b> shared storage <b>file</b> <b>systems</b> or SAN <b>file</b> <b>system)</b> made by Tianjin Zhongke Blue Whale Information Technologies Company in China.|$|R
5000|$|Virtual <b>file</b> <b>system</b> (VFS): A VFS is a <b>file</b> <b>system</b> used to {{help the}} user to hide the {{different}} <b>file</b> <b>systems</b> complexities. A user can use the same standard <b>file</b> <b>system</b> related calls to access different <b>file</b> <b>systems.</b>|$|R
5000|$|Use default {{settings}}. Default {{settings are}} defined per <b>file</b> <b>system</b> at the <b>file</b> <b>system</b> level. For ext3 <b>file</b> <b>systems</b> {{these can be}} set with the tune2fs command. The normal default for Ext3 <b>file</b> <b>systems</b> is equivalent to (no acl support). Modern Red Hat based systems set acl support as default on the root <b>file</b> <b>system</b> but not on user created Ext3 <b>file</b> <b>systems.</b> Some <b>file</b> <b>systems</b> such as XFS enable acls by default. Default <b>file</b> <b>system</b> mount attributes can be overridden in /etc/fstab.|$|R
40|$|Abstract. HFS+ <b>file</b> <b>system</b> is a <b>file</b> <b>system</b> of the Mac OS. In {{order to}} achieve data {{manipulation}} of the <b>file</b> <b>system</b> based on the Windows OS for further computer forensics, {{not only do we}} introduce the principle and structure of HFS+ <b>file</b> <b>system,</b> but also propose a efficient method to analyze the <b>file</b> <b>system.</b> Research contains the exploration of the <b>file</b> <b>system</b> and program implementation to analyze the <b>file</b> <b>system...</b>|$|R
40|$|With the {{emergence}} of Storage Networking, distributed <b>file</b> <b>systems</b> that allow data sharing through shared disks will become vital. We refer to Cluster <b>File</b> <b>Systems</b> as a distributed <b>file</b> <b>systems</b> optimized for environments of clustered servers. The requirements such <b>file</b> <b>systems</b> is that they guarantee <b>file</b> <b>systems</b> consistency while allowing shared access from multiple nodes in a shared-disk environment. In this paper we evaluate three approaches for designing a cluster <b>file</b> <b>system</b> - conventional client/server distributed <b>file</b> <b>systems,</b> symmetric shared <b>file</b> <b>systems</b> and asymmetric shared <b>file</b> <b>systems.</b> These alternatives are considered by using our prototype cluster <b>file</b> <b>system,</b> HAMFS (Highly Available Multi-server <b>File</b> <b>System).</b> HAMFS is classified as an asymmetric shared <b>file</b> <b>system.</b> Its technologies are incorporated into our commercial cluster <b>file</b> <b>system</b> product named SafeFILE. SafeFILE offers a disk pooling facility that supports off-the-shelf disks, and balances file load across these disks automatically and dynamically. From our measurements, we identify the required disk capabilities, such as multi-node tag queuing. We also identify the advantages of an asymmetric shared <b>file</b> <b>system</b> over other alternatives...|$|R
50|$|There {{are various}} User Mode <b>File</b> <b>System</b> (FUSE)-based <b>file</b> <b>systems</b> for Unix-like {{operating}} systems (Linux, etc.) {{that can be}} used to mount an S3 bucket as a <b>file</b> <b>system.</b> Note that as the semantics of the S3 <b>file</b> <b>system</b> are not that of a Posix <b>file</b> <b>system,</b> the <b>file</b> <b>system</b> may not behave entirely as expected.|$|R
50|$|Other Unix virtual <b>file</b> <b>systems</b> {{include the}} <b>File</b> <b>System</b> Switch in System V Release 3, the Generic <b>File</b> <b>System</b> in Ultrix, and the VFS in Linux. In OS/2 and Microsoft Windows, the virtual <b>file</b> <b>system</b> {{mechanism}} {{is called the}} Installable <b>File</b> <b>System.</b>|$|R
40|$|Abstract—Researches on {{technologies}} about testing {{aggregate bandwidth}} of <b>file</b> <b>systems</b> in cloud storage systems. Through the memory <b>file</b> <b>system,</b> network <b>file</b> <b>system,</b> parallel <b>file</b> <b>system</b> theory analysis, {{according to the}} cloud storage system polymerization bandwidth and concept, developed to cloud storage environment <b>file</b> <b>system</b> polymerization bandwidth test software called FSPoly. In this paper, use FSpoly to luster <b>file</b> <b>system</b> testing, find reasonable test methods, and then evaluations latest development in cloud storage <b>system</b> <b>file</b> <b>system</b> performance by using FSPoly. Keywords-cloud storage, aggregate bandwidth, <b>file</b> <b>system,</b> performance evaluation I...|$|R
40|$|We propose and {{evaluate}} an approach for decoupling persistent-cache management from general <b>file</b> <b>system</b> design. Several distributed <b>file</b> <b>systems</b> maintain a persistent cache {{of data to}} speed up accesses. Most of these <b>file</b> <b>systems</b> retain complete control over various aspects of cache management, such as granularity of caching, and policies for cache placement and eviction. Hardcoding cache management into the <b>file</b> <b>system</b> often results in sub-optimal performance as the clients of the <b>file</b> <b>system</b> are prevented from exploiting information about their workload in order to tune cache management. We introduce xCachefs, a framework that allows clients to transparently augment the cache management of the <b>file</b> <b>system</b> and customize the caching policy based on their resources and workload. xCachefs {{can be used to}} cache data persistently from any slow <b>file</b> <b>system</b> to a faster <b>file</b> <b>system.</b> It mounts over two underlying <b>file</b> <b>systems,</b> which can be local disk <b>file</b> <b>systems</b> like Ext 2 or remote <b>file</b> <b>systems</b> like NFS. xCachefs maintains the same directory structure as in the source <b>file</b> <b>system,</b> so that disconnected reads are possible when the source <b>file</b> <b>system</b> is down. ...|$|R
40|$|In this note, we {{introduce}} a simple <b>file</b> <b>system</b> implementation, known as vsfs (the Very Simple <b>File</b> <b>System).</b> This <b>file</b> <b>system</b> is a simplified {{version of a}} typical UNIX <b>file</b> <b>system</b> and thus serves to introduce {{some of the basic}} on-disk structures, access methods, and policies that you will find in many <b>file</b> <b>systems</b> today. The <b>file</b> <b>system</b> is pure software; unlike our development of CPU and memory virtualization, we will not be adding hardware features to make some aspect of the <b>file</b> <b>system</b> work better (though we will want to pay attention to device characteristics to make sure the <b>file</b> <b>system</b> works well). Because of the great flexibility we have in building a <b>file</b> <b>system,</b> many different ones have been built, literally from AFS (the Andrew <b>File</b> <b>System)</b> to ZFS (Sun’s Zettabyte <b>File</b> <b>System).</b> All of these <b>file</b> <b>systems</b> have different data structures and and do some things better or worse than their peers. Thus, the way we will be learning about <b>file</b> <b>systems</b> is through case studies: first, a simple <b>file</b> <b>system</b> (vsfs) in this chapter to introduce most concepts, and then a series of studies of real <b>file</b> <b>systems</b> to understand how they can differ in practice...|$|R
5000|$|FFS2, Unix <b>File</b> <b>System,</b> Berkeley Fast <b>File</b> <b>System,</b> the BSD Fast <b>File</b> <b>System</b> or FFS ...|$|R
40|$|File Allocation Table (FAT) <b>file</b> <b>system</b> is {{the most}} common <b>file</b> <b>system</b> used in {{embedded}} devices such as smart phones, digital cameras, smart TVs, tablets, etc. Typically these embedded devices use Solid State Drives (SSD) as storage devices. The ExFAT <b>file</b> <b>system</b> is future <b>file</b> <b>system</b> for embedded devices and it is optimal for SSDs. This paper discourses the methodologies for Geotagging as a <b>file</b> <b>system</b> metadata instead of file data in FAT and ExFAT <b>file</b> <b>systems.</b> The designed methodologies of this paper adheresthe compatibility with the FAT <b>file</b> <b>system</b> specification and existing ExFAT <b>file</b> <b>system</b> implementations...|$|R
40|$|Abstract—As <b>file</b> <b>system</b> {{capacities}} {{reach the}} petascale, {{it is becoming}} increasingly difficult for users to organize, find, and manage their data. <b>File</b> <b>system</b> search has the potential to greatly improve how users manage and access files. Unfortunately, existing <b>file</b> <b>system</b> search is designed for smaller scale systems, making it difficult for existing solutions to scale to petascale <b>files</b> <b>systems.</b> In this paper, we motivate the importance of <b>file</b> <b>system</b> search in petascale <b>file</b> <b>systems</b> and present a new fulltext <b>file</b> <b>system</b> search design for petascale <b>file</b> <b>systems.</b> Unlike existing solutions, our design exploits <b>file</b> <b>system</b> properties. Using a novel index partitioning mechanism that utilizes <b>file</b> <b>system</b> namespace locality, we are able to improve search scalability and performance and we discuss how such a design can potentially improve search security and ranking. We describe how our design can be implemented within the Ceph petascale <b>file</b> <b>system.</b> I...|$|R
40|$|HDFS is a {{distributed}} <b>file</b> <b>system</b> {{designed to}} hold very large amounts of data (terabytes or even petabytes), and provide high-throughput access to this information. Files are stored in a redundant fashion across multiple machines to ensure their durability to failure and high availability to very parallel applications. This paper includes the step by step introduction to the <b>file</b> <b>system</b> to distributed <b>file</b> <b>system</b> and to the Hadoop Distributed <b>File</b> <b>System.</b> Section I introduces What is <b>file</b> <b>System,</b> Need of <b>File</b> <b>System,</b> Conventional <b>File</b> <b>System,</b> its advantages, Need of Distributed <b>File</b> <b>System,</b> What is Distributed <b>File</b> <b>System</b> and Benefits of Distributed <b>File</b> <b>System.</b> Also the analysis of large dataset and comparison of mapreducce with RDBMS, HPC and Grid Computing communities have been doing large-scale data processing for years. Sections II introduce the concept of Hadoop Distributed <b>File</b> <b>System.</b> Lastly section III contains Conclusion followed with the References...|$|R
40|$|In {{this paper}} we {{describe}} an architecture for extensible <b>file</b> <b>systems.</b> The architecture enables {{the extension of}} <b>file</b> <b>system</b> functionality by composing (or stacking) new <b>file</b> <b>systems</b> on top of existing <b>file</b> <b>systems.</b> A <b>file</b> <b>system</b> that is stacked {{on top of an}} existing <b>file</b> <b>system</b> can access the existing <b>file</b> <b>system's</b> <b>files</b> via a well-defined naming interface and can share the same underlying file data in a coherent manner. We describe extending <b>file</b> <b>systems</b> {{in the context of the}} Spring operating <b>system.</b> Composing <b>file</b> <b>systems</b> in Spring is facilitated by basic Spring features such as its virtual memory architecture, its strongly-typed well-defined interfaces, its location-independent object invocation mechanism, and its flexible naming architecture. <b>File</b> <b>systems</b> in Spring can reside in the kernel, in user-mode, or on remote machines, and composing them can be done in a very flexible manner...|$|R
50|$|Modern {{journaling}} <b>file</b> <b>systems</b> for Amiga are the Smart <b>File</b> <b>System</b> (SFS) and Professional <b>File</b> <b>System</b> (PFS).|$|R
5000|$|File systems: High Reliability <b>File</b> <b>System</b> (HRFS), FAT-based <b>file</b> <b>system</b> (DOSFS), Network <b>File</b> <b>System</b> (NFS), and TFFS ...|$|R
40|$|The Hurricane <b>File</b> <b>System</b> (HFS) {{is a new}} <b>file</b> <b>system</b> being {{developed}} for large-scale shared memory multiprocessors with distributed disks. The main goal of this <b>file</b> <b>system</b> is scalability; that is, the <b>file</b> <b>system</b> is designed to handle demands {{that are expected to}} grow linearly with the number of processors in the system. To achieve this goal, HFS is designed using a new structuring technique called Hierarchical Clustering. HFS is also designed to be flexible in supporting a variety of policies for managing file data and for managing <b>file</b> <b>system</b> state. This flexibility is necessary to support in a scalable fashion the diverse workloads we expect for a multiprocessor <b>file</b> <b>system.</b> 1 Introduction The Hurricane <b>File</b> <b>System</b> (HFS) is a new <b>file</b> <b>system</b> {{being developed}} for large-scale shared memory multiprocessors. In this paper the goals and basic architecture of this <b>file</b> <b>system</b> are introduced. The main goal of this <b>file</b> <b>system</b> is scalability; we expect the <b>file</b> <b>system</b> load to grow linearly [...] ...|$|R
40|$|Conference Name:International Conference on Materials Science and Engineering Science. Conference Address: Shenzhen, PEOPLES R CHINA. Time:DEC 11 - 12, 2010. HFS+ <b>file</b> <b>system</b> is a <b>file</b> <b>system</b> of the Mac OS. In {{order to}} achieve data {{manipulation}} of the <b>file</b> <b>system</b> based on the Windows OS for further computer forensics, {{not only do we}} introduce the principle and structure of HFS+ <b>file</b> <b>system,</b> but also propose a efficient method to analyze the <b>file</b> <b>system.</b> Research contains the exploration of the <b>file</b> <b>system</b> and program implementation to analyze the <b>file</b> <b>system...</b>|$|R
40|$|Semantic <b>file</b> <b>systems</b> enhance {{standard}} <b>file</b> <b>systems</b> with {{ability of}} file searching based on file semantics. In this paper, we propose to integrate support for ontologies into a <b>file</b> <b>system</b> to build efficient semantic <b>file</b> <b>systems</b> whose <b>file</b> semantics can be shared by users, applications and semantic <b>file</b> <b>systems</b> themselves. We call it ontology-based <b>file</b> <b>system.</b> We identify three existent types of file semantics: property-based semantics, content-based semantics and context-based semantics and adopt multi-ontology layer approach to share them between agents. The challenges on ontology-based <b>file</b> <b>system</b> are also discussed...|$|R
5000|$|Cryptographic <b>file</b> <b>systems</b> are {{specialized}} (not general-purpose) <b>file</b> <b>systems</b> {{that are}} specifically designed with encryption and security in mind. They usually encrypt all the data they contain - including metadata. Instead of implementing an on-disk format and their own block allocation, these <b>file</b> <b>systems</b> are often layered on top of existing <b>file</b> <b>systems</b> e.g. residing in a directory on a host <b>file</b> <b>system.</b> Many such <b>file</b> <b>systems</b> also offer advanced features, such as deniable encryption, cryptographically secure read-only <b>file</b> <b>system</b> permissions and different views of the directory structure depending on the key or user ...|$|R
40|$|In this chapter, we {{introduce}} a simple <b>file</b> <b>system</b> implementation, known as vsfs (the Very Simple <b>File</b> <b>System).</b> This <b>file</b> <b>system</b> is a simplified {{version of a}} typical UNIX <b>file</b> <b>system</b> and thus serves to introduce {{some of the basic}} on-disk structures, access methods, and various policies that you will find in many <b>file</b> <b>systems</b> today. The <b>file</b> <b>system</b> is pure software; unlike our development of CPU and memory virtualization, we will not be adding hardware features to make some aspect of the file systemwork better (thoughwewill want to pay at-tention to device characteristics to make sure the <b>file</b> <b>system</b> works well). Because of the great flexibility we have in building a <b>file</b> <b>system,</b> many different ones have been built, literally from AFS (the Andrew File Sys-tem) [H+ 88] to ZFS (Sun’s Zettabyte <b>File</b> <b>System)</b> [B 07]. All of these <b>file</b> <b>systems</b> have different data structures and do some things better or worse than their peers. Thus, the way we will be learning about <b>file</b> <b>systems</b> is through case studies: first, a simple <b>file</b> <b>system</b> (vsfs) in this chapter to introduce most concepts, and then a series of studies of real <b>file</b> <b>systems</b> to understand how they can differ in practice. THE CRUX: HOW TO IMPLEMENT A SIMPLE <b>FILE</b> <b>SYSTEM</b> How can we build a simple <b>file</b> <b>system?</b> What structures are needed on the disk? What do they need to track? How are they accessed? 40. 1 The Way To Think To think about <b>file</b> <b>systems,</b> we usually suggest thinking about two different aspects of them; if you understand both of these aspects, you probably understand how the <b>file</b> <b>system</b> basically works. The first is the data structures of the <b>file</b> <b>system.</b> In other words, what types of on-disk structures are utilized by the <b>file</b> <b>system</b> to organize its data and metadata? The first <b>file</b> <b>systems</b> we’ll see (including vsfs below) employ simple structures, like arrays of blocks or other objects, whereas...|$|R
50|$|Log-structured <b>file</b> <b>systems</b> {{have all}} the {{desirable}} properties for a flash <b>file</b> <b>system.</b> Such <b>file</b> <b>systems</b> include JFFS2 and YAFFS.|$|R
5000|$|Very large <b>file</b> <b>systems,</b> {{embodied}} by applications like Apache Hadoop and Google <b>File</b> <b>System,</b> use some database <b>file</b> <b>system</b> concepts.|$|R
50|$|The CMS <b>file</b> <b>system</b> is {{the native}} <b>file</b> <b>system</b> of IBM's Conversational Monitor System (CMS), a {{component}} of VM/370. It was the only <b>file</b> <b>system</b> for CMS until {{the introduction of the}} CMS Shared <b>File</b> <b>System</b> with VM/SP.|$|R
40|$|Implementing and {{maintaining}} <b>file</b> <b>systems</b> is painful. OS functionality is notoriously difficult {{to develop and}} debug, and <b>file</b> <b>systems</b> are more so than most because of their size and interactions with other OS components. In-kernel <b>file</b> <b>systems</b> must adhere to {{a large number of}} internal OS interfaces. Though difficult during initial <b>file</b> <b>system</b> development, these dependencies particularly complicate porting a <b>file</b> <b>system</b> to different OSs or even across OS versions. This dissertation describes an architecture that addresses the <b>file</b> <b>system</b> portability problem. Virtual machines are used to decouple the OS on which a <b>file</b> <b>system</b> runs from the OS on which user applications run. The <b>file</b> <b>system</b> is distributed as a <b>file</b> <b>system</b> virtual appliance (FSVA), a virtual machine running the <b>file</b> <b>system</b> developers’ preferred OS (version). Users runs their applications in a separate virtual machine, using their preferred OS (version). An FSVA design and implementation is described that maintains fil...|$|R
40|$|Stackable <b>file</b> <b>systems</b> {{promise to}} ease the {{development}} of <b>file</b> <b>systems.</b> Operating system vendors, however,resist making extensive changes to support stacking, because of the impact on performance and stability. Existing <b>file</b> <b>system</b> interfaces differ from system to system and they support extensibility poorly. Consequently, extending <b>file</b> <b>system</b> functionality across platforms is difficult. We propose a new language, FiST, to describe stackable <b>file</b> <b>systems.</b> FiST uses operations common to <b>file</b> <b>system</b> interfaces. From a single description, FiST's compiler produces <b>file</b> <b>system</b> modules for multiple platforms. The generated code handles many kernel details, freeing developers {{to concentrate on the}} main issues of their <b>file</b> <b>systems.</b> This paper describes the design, implementation, and evaluation of FiST. We extended <b>file</b> <b>system</b> functionality in a portable way without changing existing kernels. We built several <b>file</b> <b>systems</b> using FiST on Solaris, FreeBSD, and Linux. Our experiences with these examples shows the following benefits of FiST: average code size over other stackable <b>file</b> <b>systems</b> is reduced ten times; average development time is reduced seven times;performance overhead of stacking is 1 - 2 %...|$|R
40|$|<b>File</b> <b>systems</b> have {{traditionally}} been a major area of research and development. This {{is evident from the}} existence of over 50 <b>file</b> <b>systems</b> of varying popularity in the current version of the Linux kernel. They represent a complex subsystem of the kernel, with each <b>file</b> <b>system</b> employing different strategies for tackling various issues. Although there are many <b>file</b> <b>systems</b> in Linux, there has been no prior work (to the best of our knowledge) on understanding how <b>file</b> <b>systems</b> evolve. We believe that such information would be useful to the <b>file</b> <b>system</b> community allowing developers to learn from previous experiences. This paper looks at six <b>file</b> <b>systems</b> (Ext 2, Ext 3, Ext 4, JFS, ReiserFS, and XFS) from a historical perspective (between kernel versions 1. 0 to 2. 6) to get an insight on the <b>file</b> <b>system</b> development process. We study the source code of these <b>file</b> <b>systems</b> over years to observe changes in complexity of the system and relate these metrics to the features and stability of <b>file</b> <b>systems.</b> We identified that only a few components in <b>file</b> <b>systems</b> are difficult to get right the first time. Also, <b>file</b> <b>system</b> developers do not learn from each others mistake. As a result they end up introducing the same bugs in <b>file</b> <b>systems.</b> 1...|$|R
25|$|A FAT <b>file</b> <b>system</b> is a {{specific}} type of computer <b>file</b> <b>system</b> architecture and a family of industry-standard <b>file</b> <b>systems</b> utilizing it.|$|R
5000|$|The MooseFS follows similar design {{principles}} as Fossil (<b>file</b> <b>system),</b> Google <b>File</b> <b>System,</b> Lustre or Ceph. The <b>file</b> <b>system</b> comprises three components: ...|$|R
40|$|Stackable <b>file</b> <b>systems</b> are modular <b>file</b> <b>systems</b> {{that can}} be mounted to enhance the {{functionality}} of the underlying <b>file</b> <b>systems.</b> Here, we present the design and implementation of a stackable fan-out caching <b>file</b> <b>system,</b> which caches <b>files</b> and directories in a source branch to a cache branch in a flat hierarchy. Typically, the Source Branch will come from a slower <b>file</b> <b>system</b> (e. g. a Network <b>File</b> <b>System)</b> and Cache Branch will come from a faster <b>file</b> <b>system</b> (local disk based <b>file</b> <b>system).</b> Apart from making file access faster through caching files, the flat hierarchy of the cache branch simplifies bookkeeping of the content, hence {{making it easier to}} administer the cached files...|$|R
40|$|Today <b>file</b> <b>system</b> {{tools and}} file-system aware storage {{applications}} are tightly coupled with <b>file</b> <b>system</b> implementations. Developing these applications is challenging {{because it requires}} detailed knowl-edge of the <b>file</b> <b>system</b> format, and the code for interpreting <b>file</b> <b>system</b> metadata has to be written manually. This code is complex and file-system specific, and so the application requires significant re-engineering to support different <b>file</b> <b>systems.</b> We propose a <b>file</b> <b>system</b> annotation language for specifying a <b>file</b> <b>system’s</b> on-disk metadata format. <b>File</b> <b>system</b> developers are asked to annotate the data structure definitions of a <b>file</b> <b>system’s</b> metadata. The annotated code is parsed and used by tool-specific code templates to create interpretation routines (e. g., a metadata parser) for the desired <b>file</b> <b>system</b> tool. The benefit is that different tools can reuse the interpretation routines, and they are much less dependent on <b>file</b> <b>system</b> formats and implementations. We show the feasibility of this approach by implementing a compiler that generates a runtime metadata interpreter for an annotated toy <b>file</b> <b>system.</b> The generated code has low overhead (roughly 3 %) com-pared to a hand-written {{version of the same}} application. 1...|$|R
