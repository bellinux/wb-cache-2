11|34|Public
40|$|The {{combination}} of different text representations and search strategies {{has become a}} standard technique for improving the effectiveness of information retrieval. Combination, for example, has been studied extensively in the TREC evaluations and {{is the basis of}} the “meta-search” engines used on the Web. This paper examines the development of this technique, including both experimental results and the retrieval models that have been proposed as formal frameworks for combination. We show that combining approaches for information retrieval can be modeled as combining the outputs of multiple classifiers based on one or more representations, and that this simple model can provide explanations for many of the experimental results. We also show that this view of combination is very similar to the <b>inference</b> <b>net</b> model, and that a new approach to retrieval based on language models supports combination and can be integrated with the <b>inference</b> <b>net</b> model...|$|E
40|$|Relevance feedback, which modijfes queries 14 sing {{judgments}} of {{the relevance of}} a few highlvranked documents, lzas historically been an important method for increasing the per-jlormance of information retrieval systems. In this papec we ex-~ertd the inference network model introduced by Turtle and Croft to include relevance feedback techniques. The difference between relet’ancefeedback on text abstracts and full text collections is studied. Preliminary results for relevance feedback on the structured queries supported by the <b>inference</b> <b>net</b> model are also reported. ...|$|E
40|$|It {{has been}} known that {{different}} reference structure retrieve different sets of structures. Recent works in similarity searching have suggested that significant improvements in retrieval effectiveness {{can be achieved by}} combining results from different reference structures. One of an important characteristic of the Bayesian inference network (BIN) model is that permits the combining of multiple reference structures. In this paper we introduce a formal <b>inference</b> <b>net</b> model to directly combine the contributions of multiple reference structures, and propose a novel approach to the combination of information from various reference structures. The <b>inference</b> <b>net</b> model of similarity, which was designed from this point of view, treats similarity searching as an evidential reasoning process where multiple sources of evidence about target structure are combined to estimate similarity scores. In this paper, we have compared BIN with other similarity searching methods when multiple bioactive reference structures are available. Six different 2 D fingerprints were used in combination with data fusion (DF) and nearest neighbor (NN) approaches as search tools and also as descriptors for BIN. Our empirical results show that the BIN consistently outperformed all conventional approaches such as DF and NN, regardless of the fingerprints that were tested. The superiority of BIN over conventional approaches is ascribed to the fact that BIN understands the content of the descriptors of the structures and references and used this understanding to infer the direct relationship between structures and references...|$|E
40|$|Emitter {{and system}} {{hierarchies}} {{are represented by}} <b>inference</b> <b>nets</b> and propositional relationships. Emitters are the primitive objects of the domain and systems consist of relationships among emitters. Evidence gathered concerning the identification of emitters must be used to classify both emitters and systems. Evidential reasoning and <b>inference</b> <b>nets</b> are used to to combine information at each level. Methods of direct and indirect transfer of evidence between levels are presented...|$|R
40|$|An {{intelligent}} {{cardiovascular disease}} (CVD) diagnosis system using hemodynamic parameters (HDPs) derived from sphygmogram (SPG) signal {{is presented to}} support the emerging patient-centric healthcare models. To replicate clinical approach of diagnosis through a staged decision process, the Bayesian <b>inference</b> <b>nets</b> (BIN) are adapted. New approaches to construct a hierarchical multistage BIN using defined function formulas and a method employing fuzzy logic (FL) technology to quantify inference nodes with dynamic values of statistical parameters are proposed. The suggested methodology is validated by constructing hierarchical Bayesian fuzzy <b>inference</b> <b>nets</b> (HBFIN) to diagnose various heart pathologies from the deduced HDPs. The preliminary diagnostic {{results show that the}} proposed methodology has salient validity and effectiveness in the diagnosis of cardiovascular disease...|$|R
40|$|License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. An intelligent cardiovascular disease (CVD) diagnosis systemusing hemodynamic parameters (HDPs) derived from sphygmogram (SPG) signal {{is presented to}} support the emerging patient-centric healthcare models. To replicate clinical approach of diagnosis through a staged decision process, the Bayesian <b>inference</b> <b>nets</b> (BIN) are adapted. New approaches to construct a hierarchical multistage BIN using defined function formulas and a method employing fuzzy logic (FL) technology to quantify inference nodes with dynamic values of statistical parameters are proposed. The suggested methodology is validated by constructing hierarchical Bayesian fuzzy <b>inference</b> <b>nets</b> (HBFIN) to diagnose various heart pathologies from the deduced HDPs. The preliminary diagnostic {{results show that the}} proposed methodology has salient validity and effectiveness in the diagnosis of cardiovascular disease. 1...|$|R
40|$|Harvard Community Health Plan is {{exploring}} emerging information technologies for means {{to use the}} text portion of its 25 year old computerized medical record system. The Center for Intelligent Information Retrieval is developing systems to answer the question: {{to what extent can}} automated information systems replace manual chart review of encounter notes? INQUERY, a probabilistic <b>inference</b> <b>net</b> information retrieval system, and FIGLEAF, an inductive decision tree text classifier are applied to the problem of classifying electronic encounter notes to identify acute exacerbations in pediatric asthmatics. Both systems achieve average precisions of greater than 80 %, with a new enhancement to INQUERY's relevance feedback the top performer. Refinement of the systems and plans for their integration are discussed...|$|E
40|$|Results from {{research}} in information retrieval {{have suggested that}} significant improvements in retrieval effectiveness {{can be obtained by}} combining results from multiple index representations, query formulations, and search strategies. The <b>inference</b> <b>net</b> model of retrieval, which was designed from this point of view, treats information retrieval as an evidential reasoning process where multiple sources of evidence about document and query content are combined to estimate relevance probabilities. In this paper, we use a system based on this model to study the retrieval effectiveness benefits of combining the types of document and query information that are found in typical commercial databases and information services. The results indicate that substantial real benefits are possible...|$|E
40|$|A {{technological}} {{project of}} workplace {{is a fairly}} busy activity which expects professional knowledge, large number of input information and knowledge of standards, rudders, directives. Our research team solves the problems related to automation of technological workplace design {{by means of an}} expert system. This expert system will increase project quality and cut down the time of project processing. The knowledge base and <b>inference</b> <b>net</b> of the system are based on the model of main activities related to design of workplace. The methods and procedures are included in processing knowledge base and rudders and standards are included in the database part of the knowledge base. A control program leads a designer from the setting of the task to its accomplishment. Key words: expert system, computer support, workplace design, ecology, ergonomy 1...|$|E
40|$|Search {{engines are}} a {{critical}} tool for intelligence analysis. A number of innovations for search {{have been introduced}} since research {{with an emphasis on}} analyst needs began in the TIPSTER project. For example, the Inquery search engine introduced support for specification of complex queries in a probabilistic inference network framework. Recent research on language modeling has {{led to the development of}} Indri, a search engine that combines the best features of <b>inference</b> <b>nets</b> and language modeling in an architecture designed for large-scale applications. In this paper, we describe the Indri system and show how the query language is designed to support modern language technologies. We also present results demonstrating that Indri is both effective and efficient. 1...|$|R
40|$|This thesis {{examines}} {{the problems of}} designing decision trees and expert systems from an information-theoretic viewpoint. A well-known greedy algorithm using mutual information for tree design is analysed. A basic model for tree design is developed leading {{to a series of}} bounds relating tree performance parameters. Analogies with prefix-coding and rate-distortion theory lead to interesting interpretations and results. The problem of finding termination rules for such greedy algorithms is discussed {{in the context of the}} theoretical models derived earlier, and several experimentally observed phenomena are explained in this manner. In two classification experiments, involving alphanumeric LEDS and local edge detection, the hierarchical approach is seen to offer significant advantages over alternative techniques. The second part of the thesis begins by analysing the difficulties in designing rule-based expert systems. The inability to model uncertainty in an effective manner is identified as a key limitation of existing approaches. Accordingly, an information-theoretic model for rules and rule-based systems is developed. From a simple definition of rule information content, the ability to specialise and generalise (akin to cognitive processes) in a quantitative manner is demonstrated. The problem of generalised rule induction is posed and the ITRULE algorithm is described which derives optimal rule sets from data. The problem of probabilistic updating in <b>inference</b> <b>nets</b> is discussed and a new maximum-likelihhod rule is proposed based on bounded probabilities. Utility functions and statistical decision theory concepts are used to develop a model of implicit control for rule-based inference. The theory is demonstrated by deriving rules from expert-supplied data and performing backward and forward chaining based on decision-theoretic criteria. The thesis concludes by outlining the many problems which remain to be solved in this area, and by briefly discussing the analogies between rule-based <b>inference</b> <b>nets</b> and neural networks...|$|R
40|$|Data {{classification}} {{is one of}} {{the primary}} tasks in Geocomputation. It is regularly used by the Geoscience professional to categorise datasets for further analysis such as land management, potential mapping, forecast analysis and soil assessment, to name a few. Traditionally, data classification tasks are based on statistical methodologies such as minimum distance-to-mean (MDM), maximum likelihood classification (MLC) and linear discrimination analysis (LDA). These classifiers have developed over the last century from the mathematical disciplines of set theory and control theory. Over the last 20 years, classification tools have also developed from the emerging fields of connectionism and <b>inference</b> <b>nets</b> within the discipline of Artificial Intelligence (AI); the more notable being the neural network based multi-layered perceptron (MLP), the decision tree and genetic algorithms (GA) such as differential evolution (DE). This paper seeks to compare {{the advantages and disadvantages of}} the various classifier types. The results show that, for simple tasks, MDM, LDA and similar classifiers are the best compromise of efficiency and classification ability, whilst for more complex datasets, variants based on the MLP and decision trees are the classifiers of choice...|$|R
40|$|An {{approach}} to investigating the human decision cycle, particularly that employed by {{individuals and organizations}} during crisis, is presented. The collaborative approach described here is especially beneficial in today’s world of rapidly evolving, global situations within which U. S. security policies and operational plans are generated. This paper continues the documentation {{of research in the}} field of Influence Net modeling. Specifically, we will address the capabilities required of an automated system to encourage and facilitate the collaboration, both real-time and evolutionary, of decision makers and their supporting experts. We present our research results that extend traditional Bayesian <b>inference</b> <b>net</b> structure to allow for interactive use by modelers unfamiliar with probability theory or who are unwilling to spend the excessive time required to specify the traditional Bayesian model. The results of this research, called Causal Strengths (CAST) Logic, have been implemented as software applications by the authors and their colleagues. ...|$|E
40|$|Document {{management}} systems {{are needed for}} many business applications. This type of system would com-bine the functionality of a database system, (for de-scribing, storing and maintaining documents with com-plex structure and relationships) with a text retrieval system (for effective retrieval based on full text). The retrieval model for a document management system {{is complicated by the}} variety and complexity of the ob-jects that are represented. In this paper, we describe an approach to complex object retrieval using a prob-abilistic <b>inference</b> <b>net</b> model, and an implementation of this approach using a loose coupling of an object-oriented database system (IRIS) and a text retrieval system based on inference nets (IN QUERY). The re-sulting system is used to store long, structured docu-ments and can retrieve document components (sections, figures, etc.) based on their text contents or the con-tents of related components. The lessons learnt from the implementation are discussed. ...|$|E
40|$|Relevance feedback, which modifies queries using judgements of the {{relevance}} of a few, highly-ranked documents, has historically been an important method for increasing the performance of information retrieval systems. In this paper, we extend the inference network model introduced by Turtle and Croft to include relevance feedback techniques. The difference between relevance feedback on text abstracts and full text collections is studied. Preliminary results for relevance feedback on the structured queries supported by the <b>inference</b> <b>net</b> model are also reported. 1 Introduction Relevance feedback methods in information retrieval attempt to improve performance for a particular query by modifying the query, based on the user's reaction to the initial retrieved documents. Specifically, the user's judgements of {{the relevance}} or non-relevance {{of some of the}} documents retrieved are used to add new terms to the query and to reweight query terms. For example, if all the documents, that the user [...] ...|$|E
40|$|Intelligent Information Retrieval is {{concerned}} with the application of intelligent techniques, like for example semantic networks, neural networks and <b>inference</b> <b>nets</b> to Information Retrieval. The eld of research has seen a number of applications of Constrained Spreading Activation (CSA) techniques on domain knowledge networks. However, there has never been any application of these techniques to the World Wide Web. The Web is a very important information resource, but users nd that looking for a relevant piece of information in the Web can be like "looking for a needle in a haystack". We were therefore motivated to design and develop a prototype system, WebSCSA (Web Search by CSA), that applies a CSA technique to retrieve information from the Web using an ostensive approach to querying similar to query-by-example. In this paper we describe the system and its underlying model. Furthermore, we report on an experiment carried out with human subjects to evaluate the e ectiveness of WebSCSA. We tested whether WebSCSA improves retrieval of relevant information on top of Web search engines results and how well WebSCSA serves as an agent browser for the user. The results of the experiments are promising, and show that there is much potential for further research on the use of CSA techniques to search the Web...|$|R
40|$|Abstract: Bayes-Nets are a {{suitable}} means for probabilistic <b>inference.</b> Such <b>nets</b> are very restricted concerning the com-munication language with the user, however. MinREnt-inference in a conditional environment {{is a powerful}} counterpart to this concept. Here conditional expressions of high complexity instead of mere potential tables in a directed acyclic graph, permit rich communication between system and user. This is true as well for knowledge acquisition as for query and response. For any such step of probabilistic reasoning, processed information is measurable in the information theoretical unit [bit]. The expert-system-shell SPIRIT is a pro-fessional tool for such inference and allows realworld (decision-) models with umpteen variables and hundreds of rules...|$|R
40|$|Abstract. The {{formalism}} of credal networks {{can be used}} {{to represent}} imprecision in multivariate probabilistic models. Currently, the algo-rithms for <b>inference</b> with credal <b>nets</b> allow to compute posterior proba-bility intervals given specific evidence. However, they do not deal with soft evidence. This paper presents an approach to integrate soft evi-dence in credal networks. The proposal is to convert the soft evidence into constraints that are appended to a multilinear program to perform inferences. ...|$|R
40|$|Introduction The WIN {{retrieval}} {{engine is}} West's {{implementation of the}} inference network retrieval model [Tur 90]. The <b>inference</b> <b>net</b> model ranks documents based on the combination of different evidence, e. g., text representations, such as words, phrases, or paragraphs, in a consistent probabilistic framework [TC 91]. WIN {{is based on the}} same retrieval model as the INQUERY system that has been used in previous TREC competitions [BCC 93, Cro 93, CCB 94]. The two retrieval engines have common roots but have evolved separately [...] WIN has focused on the retrieval of legal materials from large (? 50 gigabyte) collections in a commercial online environment that supports both Boolean and natural language retrieval [Tur 94]. For TREC- 3 we decided to run an essentially unmodified version of WIN to see how well a state-of-the-art commercial system compares to state-of-the-art research systems. Some modifications to WIN were required to handle the TREC topics, which bear little rese...|$|E
40|$|Knowledge base {{development}} forms {{the most difficult}} {{step in the process}} of knowledge based system design. The most typical form of knowledge acquisition - an interview with a domain expert - is a difficult and time consuming process. That is why various alternative solutions are often employed, including inductive learning from typical examples. The aim {{of this paper is to}} present an attempt to apply genetic algorithms in knowledge induction for a classification task. There are no training examples required by this approach. Only test examples are used. A typical input example can be described by a set of attribute-value pairs. Simultaneously, each example contains a notification of its classification class. Goal hypotheses and possible questions can be derived from these input examples. A chromosome represents one version of the induced knowledge base. The generated knowledge base has the form of an <b>inference</b> <b>net</b> connecting all hypotheses with relevant question nodes. Only standard ge [...] ...|$|E
40|$|Graduation date: 1991 There {{are three}} {{families}} of exact methods used for probabilistic <b>inference</b> in belief <b>nets.</b> It {{is necessary to}} compare them and analyze the advantages and the disadvantages of each algorithm, and know the time cost of making inferences in a given belief network. This paper discusses {{the factors that influence}} the computation time of each algorithm, presents the predictive model of the time complexity for each algorithm and shows the statistical results of testing the algorithms with randomly generated belief networks...|$|R
40|$|For target {{recognition}} under complex environment, {{a target}} fusion recognition method based fuzzy sets and Petri net was brought out. The method first process the target data using fuzzy sets theory, including divide the characters into different status respectively and calculate {{the degree of}} membership function. Then the results of fuzzy classify and fuzzy recognition rules are modeled with fuzzy Petri nets. The target type estimation was gained through fuzzy Petri <b>net</b> <b>inference.</b> A simulation example demonstrates {{the validity of the}} model and the reliability of the inference results...|$|R
40|$|We {{demonstrate}} {{and critique}} the new Bayesian <b>inference</b> package Infer. <b>NET</b> {{in terms of}} its capacity for statistical analyses. Infer. NET differs from the well-known BUGS Bayesian inference packages in that its main engine is the variational Bayes family of deterministic approximation algorithms rather than Markov chain Monte Carlo. The underlying rationale is that such deterministic algorithms can handle bigger problems due to their increased speed, despite some loss of accuracy. We find that Infer. NET is a well-designed computational framework with intuitive syntax. Nevertheless, the current release is limited in terms of the breadth of models it can handle, and speed of execution on standard hardware platforms...|$|R
40|$|We {{present a}} {{generalization}} {{of the local}} expression language used in the Symbolic Probabilistic Inference (SPI) approach to <b>inference</b> in belief <b>nets</b> [1 l, [8]. The local expression language in SPI is the language in which the dependence of a node on its antecedents is described. The original language represented the dependence as a single monolithic conditional probability distribution. The extended language provides a set of operators (*, +, and -) {{which can be used}} to specify methods for combining partial conditional distributions. As one instance of the utility of this extension, we show how this extended language can be used to capture the semantics, representational advantages, and inferential complexity advantages of the "noisy or" relationship. Comment: Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI 1991...|$|R
40|$|The use of {{hot water}} to extract lunar samples, {{followed}} by the hydrolysis of the aqueous extract, {{appears to be the}} method of choice for identification and quantitation of amino acid precursors in extraterrestrial sources. The <b>net</b> <b>inferences</b> from the analyses to date are (1) that amino acid precursors are verifiably present in lunar dust, and (2) that they are quite certainly not the consequence of contamination by terrestrial organisms, including man. It is suggested that prebiotic evolutionary pathways such as have been traversed on the earth were terminated on the moon for lack of sufficient water. Although {{some or all of the}} amino acid precursors may be indigenous, the low level observed suggests that they may also result from onfall of organic compounds from interstellar matter, comets, tails, solar wind, or meteorites...|$|R
40|$|Derivations, Equational Logic and Interpolation p. 173 Elaine Pimentel, Simona Ronchi della Rocca and Luca Roversi: Intersection Types: a Proof-Theoretical Approach p. 189 Joao Rasga: A Cut Elimination in Propositional Based Logics p. 205 Beyond Deduction Modulo Claude Kirchner INRIA & LORIA Nancy, France From Deep <b>Inference</b> to Proof <b>Nets</b> Universitat des Saarlandes [...] - Informatik [...] - Programmiersysteme Postfach 15 11 50 [...] - 66041 Saarbrucken [...] - Germany [URL] Abstract. This paper {{shows how}} derivations in (a {{variation}} of) SKS {{can be translated}} into proof nets. Since an SKS derivation contains more information about a proof than the corresponding proof net, we observe a loss of information which {{can be understood as}} "eliminating bureaucracy ". Technically this is achieved by cut reduction on proof nets. As an intermediate step between the two extremes, SKS derivations and proof nets, we will see nets representing derivations in "Formalism A"...|$|R
40|$|Many {{real-world}} data are maintained in relational format, with different tables storing information about entities and their links or relationships. The structure (schema) of the database is essentially {{that of a}} logical language, with variables ranging over individual entities and predicates for relationships and attributes. Our work combines the graphical structure of Bayes nets with the logical structure of relational databases to achieve knowledge discovery in databases. We introduce Join Bayes nets, {{a new type of}} Bayes nets for representing and learning class-level dependencies between attributes from the same table and from different tables; such dependencies are important for policy making and strategic planning. Focusing on class-level dependencies brings advantages in terms of the simplicity of the model and the tractability of inference and learning. As usual with Bayes nets, the graphical structure supports efficient inference and reasoning. We show that applying standard Bayes <b>net</b> <b>inference</b> algorithms to the learned models provides fast and accurate probability estimates for queries that involve attributes and relationships from multiple tables. ...|$|R
40|$|Background and objective: In human-machine (HM) hybrid control systems, human {{operator}} and machine cooperate {{to achieve the}} control objectives. To enhance the overall HM system performance, the discrete manual control task-load by the operator must be dynamically allocated in accordance with continuous-time fluctuation of psychophysiological functional status of the operator, so-called operator functional state (OFS). The behavior of the HM system is hybrid in nature due to the co-existence of discrete task-load (control) variable and continuous operator performance (system output) variable. Methods: Petri net is an effective tool for modeling discrete event systems, but for hybrid system involving discrete dynamics, generally Petri net model has to be extended. Instead of using different tools to represent continuous and discrete components of a hybrid system, this paper proposed a method of fuzzy <b>inference</b> Petri <b>nets</b> (FIPN) to represent the HM hybrid system comprising a Mamdani-type fuzzy model of OFS and a logical switching controller in a unified framework, in which the task-load level is dynamically reallocated between the operator and machine based on the model-predicted OFS. Furthermore, this paper used a multi-model approach to predict the operator performance based on three electroencephalographic (EEG) input variables (features) via the Wang-Mendel (WM) fuzzy modeling method. The membership function parameters of fuzzy OFS model for each experimental participant were optimized using artificial bee colony (ABC) evolutionary algorithm. Three performance indices, RMSE, MRE, and EPR, were computed to evaluate the overall modeling accuracy. Results: Experiment data from six participants are analyzed. The {{results show that the}} proposed method (FIPN with adaptive task allocation) yields lower breakdown rate (from 14. 8...|$|R
40|$|When {{analyzing}} the causal e§ect of a treatment on an outcome {{it is important}} to un- derstand the mechanisms or channels through which the treatment works. In this paper we study net and mechanism average treatment e§ects (NATE and MATE, respectively), which provide an intuitive decomposition of the total average treatment e§ect (ATE) that enables learning about how the treatment a§ects the outcome. We derive informative non- parametric bounds for these two e§ects allowing for heterogeneous e§ects and without re- quiring the use of an instrumental variable or having an outcome with bounded support. We employ assumptions requiring weak monotonicity of mean potential outcomes within or across subpopulations deÖned by the potential values of the mechanism variable under each treatment arm. We illustrate the identifying power of our bounds by analyzing what part of the ATE of a training program on weekly earnings and employment is due to the obtainment of a GED, high school, or vocational degree. causal <b>inference,</b> treatment effects, <b>net</b> effects, direct effects, nonparametric bounds, principal stratification...|$|R
40|$|Many {{real-world}} problems, including <b>inference</b> in Bayes <b>Nets,</b> can {{be reduced}} to #SAT, the problem of counting the number of models of a propositional theory. This has motivated the need for efficient #SAT solvers. Currently, such solvers utilize a modified version of DPLL that employs decomposition and caching, techniques that significantly increase {{the time it takes to}} process each node in the search space. In addition, the search space is significantly larger than when solving SAT since we must continue searching even after the first solution has been found. It has previously been demonstrated that the size of a DPLL search tree can be significantly reduced by doing more reasoning at each node. However, for SAT the reductions gained are often not worth the extra time required. In this paper we verify the hypothesis that for #SAT this balance changes. In particular, we show that additional reasoning can reduce the size of a #SAT solver’s search space, that this reduction cannot always be achieved by the already utilized technique of clause learning, and that this additional reasoning can be cost effective...|$|R
40|$|Many {{databases}} {{store data}} in relational format, with {{different types of}} entities and information about links between the entities. The field of statistical-relational learning has developed {{a number of new}} statistical models for such data. Instead of introducing a new model class, we propose using a standard model class—Bayes nets—in a new way: Join Bayes nets contain nodes that correspond to the descriptive attributes of the database tables, plus Boolean relationship nodes that indicate the presence of a link. Join Bayes nets are class-level models whose random variables describe attributes of generic individuals (e. g., age(P) rather than age(Jack) where P stands for a randomly selected person). As Join Bayes nets are just a special type of Bayes net, their semantics is standard (edges denote direct associations, d-separation implies probabilistic independence etc.), and Bayes <b>net</b> <b>inference</b> algorithms can be used “as is ” to answer probabilistic queries involving relations. We present a dynamic programming algorithm for estimating the parameters of a Join Bayes net and discuss how Join Bayes Nets model various well-known statistical-relational phenomena like autocorrelation and aggregation. ...|$|R
40|$|Church is a Turing-complete {{probabilistic}} programming language, {{designed for}} inference. By allowing for easy description and manipulation of distributions, it {{allows one to}} describe classical Al models in compact ways, providing a language for very rich expression. However, for <b>inference</b> in Bayes <b>nets,</b> Hidden Markov Models, and topic models, the very settings for which probabilistic programming languages like Church were designed for, researchers typically instead write special cased algorithms in regular programming languages to maximize performance. In this paper, we argue that an extremely general language can still support very fast inference. We first introduce the theoretical aspects of our Church-like language, including many implementation tradeoffs. While the language is extremely hands-off and easy to use, we also allow for more detailed specification of the inference. Lastly, we demonstrate empirical results displaying our progress towards general languages which can perform inference quickly, and point out many future directions for research. by Jeff Wu. Thesis: M. Eng., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2013. Cataloged from PDF version of thesis. "February 2013. "Includes bibliographical references (page 32) ...|$|R
40|$|In this study, we infer {{the breast}} cancer gene {{regulatory}} network from gene expression data. This network is obtained from {{the application of the}} BC 3 <b>Net</b> <b>inference</b> algorithm to a large-scale gene expression data set consisting of $ 351 $ patient samples. In order to elucidate the functional relevance of the inferred network, we are performing a Gene Ontology (GO) analysis for its structural components. Our analysis reveals that most significant GO-terms we find for {{the breast cancer}} network represent functional modules of biological processes that are described by known cancer hallmarks, including translation, immune response, cell cycle, organelle fission, mitosis, cell adhesion, RNA processing, RNA splicing and response to wounding. Furthermore, by using a curated list of census cancer genes, we find an enrichment in these functional modules. Finally, we study cooperative effects of chromosomes based on information of interacting genes in the beast cancer network. We find that chromosome $ 21 $ is most coactive with other chromosomes. To our knowledge this is the first study investigating the genome-scale breast cancer network...|$|R
40|$|Inverse {{dynamics}} joint kinetics {{are often}} used to infer contributions from underlying groups of muscle-tendon units (MTUs). However, such interpretations are confounded by multiarticular (multi-joint) musculature, which can cause inverse dynamics to over- or under-estimate net MTU power. Misestimation of MTU power could lead to incorrect scientific conclusions, or to empirical estimates that misguide musculoskeletal simulations, assistive device designs, or clinical interventions. The objective {{of this study was}} to investigate the degree to which ankle joint power overestimates net plantarflexor MTU power during the Push-off phase of walking, due to the behavior of the flexor digitorum and hallucis longus (FDHL) -multiarticular MTUs crossing the ankle and metatarsophalangeal (toe) joints. We performed a gait analysis study on six healthy participants, recording ground reaction forces, kinematics, and electromyography (EMG). Empirical data were input into an EMG-driven musculoskeletal model to estimate ankle power. This model enabled us to parse contributions from mono- and multi-articular MTUs, and required only one scaling and one time delay factor for each subject and speed, which were solved for based on empirical data. Net plantarflexing MTU power was computed by the model and quantitatively compared to inverse dynamics ankle power. The EMG-driven model was able to reproduce inverse dynamics ankle power across a range of gait speeds (R 2 ≥ 0. 97), while also providing MTU-specific power estimates. We found that FDHL dynamics caused ankle power to slightly overestimate net plantarflexor MTU power, but only by ~ 2 - 7 %. During Push-off, FDHL MTU dynamics do not substantially confound the <b>inference</b> of <b>net</b> plantarflexor MTU power from inverse dynamics ankle power. However, other methodological limitations may cause inverse dynamics to overestimate net MTU power; for instance, due to rigid-body foot assumptions. Moving forward, the EMG-driven modeling approach presented could be applied to understand other tasks or larger multiarticular MTUs...|$|R
40|$|ABSTRACT: The mean grain size. sorting, and {{skewness}} of a {{sedimentary deposit}} {{are dependent on}} the sediment grain size distribution of its source and the sedimentary processes of i) winnowing (erosion), ii) selective deposition of the grain size distribution i transport, and iii) total deposition of the sediment in transport. If a source sediment undergoes erosion, and the resultant sediment in transport is deposited completely, the deposit must be finer, better sorted, and more negatively skewed than the source. This trend {{is referred to as}} Case I. The lag remaining after erosion, on the other hand, must be coarser, better sorted, and more positively skewed (Case ll). If sediment in transport undergoes selective deposition, the resultant deposit can either be finer (Case IlIA) or coarser (Case nlB) than the source, but the sorting will be better and the skew more positive, Although exceptions to these trends may occur, they suggest that comparison of one sediment must be made with another for the proper identification of the sedimentary process, and therefore it is not possible for a single grain-size distribution to identify the depositional environment. The trends also suggest that the skewness of a grain-size distribution has been widely misinterpreted and implies neither the truncation of one of the tails nor the mixing of more than one mode. Rather, a skewed sediment is the natural result of the sedimentary process. In a system of related environments, these trends can be used to identify both the probable source and the probable deposit and. by <b>inference,</b> the <b>net</b> sediment transport paths among sedimentary deposits. Such an analysis provides a rapid understanding of the sedimentary processes, identifies patterns of erosion and accretion, and may suggest transport processes...|$|R
40|$|This {{describes}} a knowledge base (KB) partitioning approach {{to solve the}} problem of real-time performance using the CLIPS AI shell when containing large numbers of rules and facts. This work is funded under the joint USAF/NASA Advanced Launch System (ALS) Program as applied research in expert systems to perform vehicle checkout for real-time controller and diagnostic monitoring tasks. The Expert System advanced development project (ADP- 2302) main objective is to provide robust systems responding to new data frames of 0. 1 to 1. 0 second intervals. The intelligent system control must be performed within the specified real-time window, in order to meet the demands of the given application. Partitioning the KB reduces the complexity of the <b>inferencing</b> Rete <b>net</b> at any given time. This reduced complexity improves performance but without undo impacts during load and unload cycles. The second objective is to produce highly reliable intelligent systems. This requires simple and automated approaches to the KB verification & validation task. Partitioning the KB reduces rule interaction complexity overall. Reduced interaction simplifies the V&V testing necessary by focusing attention only on individual areas of interest. Many systems require a robustness that involves a large number of rules, most of which are mutually exclusive under different phases or conditions. The ideal solution is to control the knowledge base by loading rules that directly apply for that condition, while stripping out all rules and facts that are not used during that cycle. The practical approach is to cluster rules and facts into associated 'blocks'. A simple approach has been designed to control the addition and deletion of 'blocks' of rules and facts, while allowing real-time operations to run freely. Timing tests for real-time performance for specific machines under R/T operating systems have not been completed but are planned as part of the analysis process to validate the design...|$|R
