12|10000|Public
5000|$|... 1988, Digiport - first {{satellite}} telecommunications <b>data</b> <b>processing</b> <b>operation</b> - Montego Bay ...|$|E
50|$|Stage III is a {{reaction}} against excessive and uncontrolled expenditures {{of time and money}} spent on computer systems, and the major problem for management is the organization of tasks for control of computer operating costs. In this stage, project management and management report systems are organized, which leads to development of programming, documentation, and operation standards. During Stage III, a shift occurs from management of computers to management of data resources. This shift is an outcome of analysis of how to increase management control and planning in expending data processing operations. Also, the shift provides flexibility in data processing that is needed in a case of management’s new controls. The major characteristic of Stage III is reconstruction of <b>data</b> <b>processing</b> <b>operation.</b>|$|E
40|$|This {{document}} {{has been}} prepared {{in response to}} the continuing concern of the United Nations Fund for Population Activities and the United Nations Statistical Office to provide appropriate and cost effective advice to developing countries in the preparation for and execution of their censuses of population and housing. The rapid advances of computing technology in the last decade, especially those emerging in the area of microcomputer technology, provide significant new opportunities for developing countries to improve substantially the effectiveness of their statistical <b>data</b> <b>processing</b> <b>operation...</b>|$|E
50|$|An FSMD is {{a digital}} system {{composed}} of a finite-state machine, which controls the program flow, and a datapath, which performs <b>data</b> <b>processing</b> <b>operations.</b>|$|R
50|$|A {{datapath}} is {{a collection}} of functional units (such as arithmetic logic units or multipliers, that perform <b>data</b> <b>processing</b> <b>operations),</b> registers, and buses. Along with the control unit it composes the central processing unit (CPU).|$|R
30|$|In the 1960 ’s Pearsall was busily at work {{starting}} up his first successful manufacturing company called Statitrol Corporation. His product {{was designed to}} eliminate static electricity problems in industries like newspaper printing, photo and <b>data</b> <b>processing</b> <b>operations.</b>|$|R
40|$|Governmental, {{business}} and scientific activities have been placing {{and will continue}} to place heavy reliance on data processing equipment and procedures. Because of increasing complexity and speed of equipment and sophistication in procedures, utilization of this powerful tool is highly dependent upon the quality of management of the <b>data</b> <b>processing</b> <b>operation,,</b> Management of this function depends to a great extent on several major factors This paper describes and examines five key elements of successful data processing management—organization, personnel, documentation, control, and standards. [URL] United States NavyLieutenant Commander, United States Nav...|$|E
40|$|Demo. We {{demonstrate}} Argos, {{a framework}} to automatically generate data processing workflows. First, we show how to assign formal semantics to data and operations using to a domain ontology. Specifically, we define data contents using relational descriptions in an expressive logic. Second, we show a novel planner that uses relational subsumption {{to connect the}} output of a <b>data</b> <b>processing</b> <b>operation</b> with the input of another. Our modeling methodology has the significant advantage that the planner can automatically insert adaptor operations wherever necessary to bridge the inputs and outputs of operations in the workflow. We have implemented the approach in a transportation modeling domain...|$|E
40|$|The data {{processing}} system for the Helios experiment 6, measuring energetic charged particles of solar, planetary and galactic origin in the inner solar system, is described. The aim of this experiment is to extend knowledge on origin and propagation of cosmic rays. The different programs for data reduction, analysis, presentation, and scientific evaluation are described as well as hardware and software of the {{data processing}} equipment. A chronological presentation of the <b>data</b> <b>processing</b> <b>operation</b> is given. Procedures and methods for data analysis which were developed {{can be used with}} minor modifications for analysis of other space research experiments...|$|E
5000|$|In {{their vision}} the CIO was [...] "a senior-level {{executive}} responsible for overseeing network and <b>data</b> <b>processing</b> <b>operations,</b> and crafting an information systems strategy {{that would help}} a corporation achieve its business objectives through the innovative use of technology." ...|$|R
50|$|Essential to {{realizing the}} target state, Data {{architecture}} describes how data is processed, stored, and utilized {{in a given}} system. It provides criteria for <b>data</b> <b>processing</b> <b>operations</b> that {{make it possible to}} design data flows and also control the flow of data in the system.|$|R
40|$|The {{increase}} of luminosity of the LHC in 2011 also introduced an {{increase of}} computing requirements for <b>data</b> <b>processing.</b> This paper describes the <b>data</b> <b>processing</b> <b>operations</b> during 2011 prompt reconstruction {{as well as the}} end of year re-processing of the full data sample. It further gives an outlook to next evolutionary steps in the LHCb computing model for 2012 <b>data</b> <b>processing</b> and beyond...|$|R
40|$|Removing {{redundant}} {{content is}} an important <b>data</b> <b>processing</b> <b>operation</b> in search engines and other web applications. An offline approach can be important for reducing the engine’s cost, but {{it is challenging to}} scale such an approach for a large data set which is updated continuously. This paper discusses our experience in developing a scalable approach with parallel clustering that detects and removes near duplicates incrementally when processing billions of web pages. It presents a multidimensional mapping to balance the load among multiple machines. It further describes several approximation techniques to efficiently manage distributed duplicate groups with transitive relationship. The experimental results evaluate the efficiency and accuracy of the incremental clustering, assess the effectiveness of the multidimensional mapping, and demonstrate the impact on online cost reduction and search quality...|$|E
40|$|In Opinion 1 / 2010, the Article 29 Data Protection Working Party has {{provided}} additional guidance concerning {{the concepts of}} ‘controller’ and ‘processor’ contained in Directive 95 / 46 /EC. This guidance aims to assist practitioners in their determination of whether an entity is acting as a controller or as a processor towards a particular <b>data</b> <b>processing</b> <b>operation.</b> Despite {{the fact that this}} opinion is informative, the existing framework still appears to leave room for a considerable amount of legal uncertainty. This uncertainty is attributable in part {{to the nature of the}} existing concepts, but also (and perhaps to a larger extent) to their apparent misalignment with current processing realities. In this paper, the author seeks to articulate why the existing concepts often remain difficult to apply in practice, in order to enable a constructive reflection on how these issues might be addressed in the future. status: publishe...|$|E
40|$|EU data {{protection}} law has, to date, been monitored and enforced in a decentralised way by independent supervisory authorities in each Member State. While the independence of these supervisory authorities is {{an essential element of}} EU {{data protection}} law, this decentralised governance structure has led to competing claims from supervisory authorities regarding the national law applicable to a <b>data</b> <b>processing</b> <b>operation</b> and the national authority responsible for enforcing the data protection rules. These competing claims, evident in investigations conducted into the data protection compliance of Google and Facebook, jeopardise the objectives of the EU data protection regime. The new General Data Protection Regulation will revolutionise data protection governance by providing for a centralised decision-making body, the European Data Protection Board. While this agency will ensure the ‘Europeanisation’ of {{data protection law}}, given the nature and the extent of this Board’s powers it marks another significant shift in the EU’s agency-creating process and must therefore also be considered in its broader EU context...|$|E
50|$|Essential to {{realizing the}} target state, Data Architecture {{describes}} how data is processed, stored, and utilized in an information system. It provides criteria for <b>data</b> <b>processing</b> <b>operations</b> {{so as to}} make it possible to design data flows and also control the flow of data in the system.|$|R
50|$|In 1997, Polk {{acquired}} the MSS division of Automatic <b>Data</b> <b>Processing's</b> European <b>Operations.</b>|$|R
40|$|IDSISM {{services}} are a central source of bulk U. S. and Canadian securities information for <b>data</b> <b>processing</b> <b>operations,</b> brokerage accounting, trust operations, and portfolio accounting at banks, brokerage firms, insurance companies, and other securities-related businesses. IDSI provides data {{for a range}} of security types, delivered via a variety of options, including...|$|R
40|$|Scientists, economists, and {{planners}} in government, {{industry and}} academia {{spend much of}} their time accessing, integrating, and analyzing data. However, many of their studies are one-of-a-kind with little sharing and reuse for subsequent endeavors. The Argos project seeks to improve the productivity of analysts by providing a framework that encourages reuse of data sources and data processing operations, and by developing tools to generate data processing workflows. In this paper, we present an approach to automatically generate data processing workflows. First, we define a methodology for assigning formal semantics to data and operations according to a domain ontology, which allows sharing and reuse. Specifically, we define data contents using relational descriptions in an expressive logic. Second, we develop a novel planner that uses relational subsumption to connect the output of a <b>data</b> <b>processing</b> <b>operation</b> with the input of another. Our modeling methodology has the significant advantage that the planner can automatically insert adaptor operations wherever necessary to bridge the inputs and outputs of operations in the workflow. We have implemented the approach in a transportation modeling domain...|$|E
30|$|The {{proposed}} ISP {{is implemented}} on an SRP {{in accordance with}} the test of the preliminary version of the proposed ISP [1]. Since the SRP can support both of the parallel processing modes SIMD and VLIW, the proposed ISP is accelerated by the implementation of numerous key operations so that it can run in parallel. The SRP supports the following three operation modes: SIMD, VLIW, and scalar. As the SRP configuration that is used in this study can process 128 bits at a time with 16 functional units, it supports SIMD configurations that can process four 32 -bit, eight 16 -bit, or 16 8 -bit data at the one time. In the VLIW mode, eight of the function units can be operational at the same time, whereby up to eight operations can be executed in parallel. Since the routing channel of the SRP comprises independent configurations for the SIMD, VLIW, and scalar modes, the three modes cannot be used in combination; however, the SRP can switch among the three operational modes dynamically while the ISP software is processed. While the sequential codes in the complex control sequences of the algorithm run in Scalar mode, the parallel codes of the massive image <b>data</b> <b>processing</b> <b>operation</b> are accelerated in the SIMD mode or the VLIW mode.|$|E
40|$|Cloud {{computing}} {{has opened}} a new paradigm {{in the area of}} distributed computing in which users lease computing resources from large scale data centers operated by service providers. It delivers the large scale computation and <b>data</b> <b>processing</b> <b>operation</b> in a scalable and flexible manner. It provides the opportunity for small organizations and individuals to deploy applications by paying a minimal cost of actual resource usage [8]. Cloud resources such as virtual machine [52] can be provisioned on-demand on a Pay-As-You-Go basis [9]. Current resources provisioning approaches [9] [11] rely on monitoring the state of system resources such as CPU or memory utilization of virtual machines to determine the required size of the system. Most cloud service providers use machine virtualization to provide flexible and cost effective resource sharing. The cloud service provider is responsible to make the needed resources available on demand to the cloud users. Out of the three types (Infrastructure-as-a-Service IaaS, Platform-as-a-Service PaaS and Software-as-a-Service SaaS) of computing capacities as a service [12] in different abstraction levels, this thesis focused on Infrastructure-as-a-Service (IaaS) [5] only to provide computing resources as a service to customers. It introduces dynamic resource allocation mechanisms for infrastructure provisioning where there is a fixed time-limit as well as a resource budget for a particular task...|$|E
40|$|In view {{of knowing}} {{the essence of}} {{phenomena}} {{it is necessary to}} perform statistical <b>data</b> <b>processing</b> <b>operations.</b> This allows for shifting from individual data to derived, synthetic indicators that highlight the essence of various phenomena. The high volume and diversity of <b>processing</b> <b>operations</b> presuppose developing plans of computerised <b>data</b> <b>processing.</b> To identify distinct and homogenous groups and classes it is necessary to realise well-pondered groupings and classifications that presuppose to comply with the requirements presented in the article. computerized <b>data</b> <b>processing,</b> statistics, synthetic indicators...|$|R
40|$|Inputs from {{prospective}} LANDSAT-C data {{users are}} requested to aid NASA in defining LANDSAT-C mission and data requirements and {{in making decisions}} regarding the scheduling of satellite <b>operations</b> and ground <b>data</b> <b>processing</b> <b>operations.</b> Design specifications, multispectral band scanner performance characteristics, satellite schedule operations, and types of available data products are briefly described...|$|R
40|$|This paper {{discusses}} the device attributes {{that may be}} obtained by using advanced technology components in an embodiment that is deemed most suitable for computer data storage applications requiring high reliability, flexibility of <b>data</b> <b>processing</b> <b>operations,</b> economical storage costs, economical maintenance and operations costs, and data rate parity {{with other members of}} the storage hierarchy...|$|R
50|$|On April 1987, {{parent company}} TOKAI handed its {{remaining}} computer-related branches, such as electronic calculation and operational sections, to Vic Tokai, thus making the latter company {{able to handle}} <b>data</b> <b>processing</b> <b>operations</b> independently. Vic Tokai expanded in 1988 when it acquired another telecommunication company. The company also launched its first system consulting software the same year.|$|R
40|$|The Europol Regulation 1 {{entered into}} {{force on the}} 1 May 2017. The {{instrument}} has however received little attention and the content was hardly debated. Yet, it brings forth {{a major change in}} terms of data protection: the principle of purpose limitation is not implemented anymore through the so-called ‘silo-based approach’. The focus of the Regulation is not on databases but on <b>data</b> <b>processing</b> <b>operations.</b> This article reviews the impact of this shift, in particular as purpose limitation is restated as fundamental principle in the field of law enforcement cooperation, both because of the opacity of the personal <b>data</b> <b>processing</b> <b>operations</b> and their significant impact on the lives and freedoms of individuals. To that end, it first describes the context and content of the reform before highlighting the limits of this approach. It finally formulates some recommendations for the implementation of purpose limitation in this new contextstatus: publishe...|$|R
30|$|This article {{addresses}} {{new methods}} for performing error correction when real convolution codes are involved. Real convolution codes can provide effective protection for <b>data</b> <b>processing</b> <b>operations</b> at the data-parity level. <b>Data</b> <b>processing</b> implementations are protected against both hard and soft errors. The <b>data</b> <b>processing</b> system is protected through parity sequences specified {{by a high}} rate real convolution code. Parity comparisons provide error detection, while output data correction is affected by a decoding method that includes both round-off error and computer-induced errors. The error detection structures are developed and they not only detected subsystem errors, but also corrected errors introduced in the <b>data</b> <b>processing</b> system. Concurrent parity values techniques are very useful in detecting numerical error in the <b>data</b> <b>processing</b> <b>operations,</b> where a single error can propagate to many output errors. Parity values are the most effective tools used to detect burst errors occurring in the code stream. The detection performance in the <b>data</b> <b>processing</b> system depends on the detection threshold, which is determined by round-off tolerances. The structures have been tested using MATLAB programs and compute error detecting performance of the concurrent parity values method and simulation results are presented.|$|R
40|$|All-optical free-space {{architecture}} {{can find}} many interesting applications in optical communications {{in case of}} packet-oriented <b>data</b> <b>processing.</b> An example of an all-optical system able to perform <b>data</b> <b>processing</b> <b>operations</b> is described and experimented. Logic schemes characterized by high modularity and flexibility are reported. An optical implementation is proposed by using an elementary switching unit realized with CdTe:In crystal. The unit is optically controlled and operates with beams at typical communication wavelengths. Preliminary experimentation in ns regime at 1550 nm is shown...|$|R
40|$|<b>Data</b> <b>processing</b> in {{environmental}} science {{is essential for}} doing science. The heterogeneity of <b>data</b> sources, <b>data</b> <b>processing</b> <b>operations</b> and infrastructures results {{in a lot of}} manual data and process integration work done by each scientist individually. This is very inefficient and time consuming. The aim is to provide a view based approach on accessing and <b>processing</b> <b>data</b> supporting a more generic infrastructure to integrate processing steps from different organizations, systems and libraries. We propose an approach modeled in Colored Place/Transition Nets which has been implemented in a Web Service infrastructure...|$|R
40|$|Function creep, i. e. {{when the}} purpose {{specification}} principle is breached, {{is a major}} challenge for personal <b>data</b> <b>processing</b> <b>operations.</b> This is especially a clear risk {{in the field of}} Identity Management when biometric data are deployed. The concept of privacy by design, set forth in the data protection reform, could, in principle, contribute to mitigating function creep. An imple-mentation is discussed hereunder in relation to Automated Border Control (‘ABC’). status: publishe...|$|R
40|$|Examine how the map-reduce {{programming}} {{model can}} be applied to NetFlow processing in order to efficiently analyze very large sets of NetFlow data. Implement a number of common NetFlow <b>data</b> <b>processing</b> <b>operations</b> 1) using a map-reduce framework, and 2) using an optimal approach without any framework constraints. Execute relevant benchmarks and compare the efficiency and scalability of the implemented systems. Based on this, evaluate the viability of using general frameworks for distributed NetFlow processing...|$|R
40|$|Part 3 : Privacy by Design and Privacy PatternsInternational audienceFunction creep, i. e. {{when the}} purpose {{specification}} principle is breached, {{is a major}} challenge for personal <b>data</b> <b>processing</b> <b>operations.</b> This is especially a clear risk {{in the field of}} Identity Management when biometric data are deployed. The concept of privacy by design, set forth in the data protection reform, could, in principle, contribute to mitigating function creep. An implementation is discussed hereunder in relation to Automated Border Control (‘ABC’) ...|$|R
50|$|The second version, VP2, was {{released}} in 1998 {{as the basis of}} {{a new version of the}} portable multimedia platform, first known as Elate and then as intent. VP2 supports the same data types and <b>data</b> <b>processing</b> <b>operations</b> as VP1, but has additional features for better support of high level languages such as demarcation of subroutines, by-value parameters, and a very large theoretical maximum number of registers local to the subroutine for use as local variables.|$|R
30|$|Although MapReduce {{provides}} {{basic functionality}} for many <b>data</b> <b>processing</b> <b>operations,</b> users are limited since {{they need to}} adapt their applications to the MapReduce model to achieve parallelism. This can include the implementation of multiple sequenced operations which can add overhead to the overall processing time. In addition, many <b>processing</b> <b>operations</b> do not naturally fit into the group by-aggregation model using single key-value pairs. Even simple applications such as selection and projection must fit into this model and users need to provide custom MapReduce functions for such operations, which is more error prone and limits re-usability [24].|$|R
40|$|The paper {{presents}} {{objectives and}} tasks of the EC project BALTPORTS-IT, aimed at promoting and supporting dissemination, industrial customisation and transfer of technologies and tools gained during execution of the successfully completed EC projects AMCAI and DAMAC-HP. Customisation of a simulation model of container terminal operation, {{as well as of}} maritime information systems, is discussed. Use of a special technology for development and customisation of maritime information systems, that is based on infological models of <b>data</b> <b>processing</b> <b>operations,</b> is presented...|$|R
40|$|The {{rating system}} for <b>data</b> <b>processing</b> <b>operations</b> is Composite- 2 {{similar to the}} "Uniform Interagency Bank Rating Data centers in this group are {{fundamentally}} sound, System, " which is used for federally regulated but may reflect modest weaknesses. Deficiencies are financial institutions and is based upon an evaluation generally corrected in {{the normal course of}} business. of the overall financial institution performance. The Therefore, the need for supervisory response is IS rating system is explained in FFIEC SP- 2 Uniform usually limited...|$|R
