18|171|Public
5|$|Independently of the ANSI committee, the CODASYL Programming Language Committee {{was working}} on {{improving}} the language. They described new versions in 1968, 1969, 1970 and 1973, including changes such as new inter-program communication, debugging and <b>file</b> <b>merging</b> facilities as well as improved string-handling and library inclusion features.|$|E
50|$|Following {{is a list}} of {{companies}} providing software and/or services for <b>file</b> <b>merging,</b> mail presorting and/or data cleansing for variable data printing.|$|E
50|$|Bitcask is an Erlang {{application}} that provides an API for storing and retrieving key/value data into a log-structured hash table. The design owes {{a lot to}} the principles found in log-structured file systems and draws inspiration from a number of designs that involve log <b>file</b> <b>merging.</b>|$|E
5000|$|If the merged {{output is}} to be piped into another program, the <b>file</b> <b>merge</b> {{sequence}} 2>&1 must precede the pipe symbol, thus: ...|$|R
50|$|The {{program is}} also capable of {{performing}} automatic 3-way <b>file</b> <b>merges.</b> It {{is available for}} Windows, Mac OS, Linux, and Unix operating systems.|$|R
5000|$|... iView - A iSeries - System i native {{document}} archive - retrieval solution performing content management {{functions to}} enable the capture, storage, indexing and retrieval of spool <b>files,</b> <b>merged</b> documents and scanned forms so they are accessible via the Web.|$|R
50|$|OneDrive {{includes}} an online text editor that {{allows users to}} view and edit files in plain text format, such as text files and batch files. Syntax highlighting and code completion is available {{for a number of}} programming and markup languages, including C#, Visual Basic, JavaScript, Windows PowerShell, CSS, HTML, XML, PHP and Java. This online editor includes a find-and-replace feature and a way to manage <b>file</b> <b>merging</b> conflicts.|$|E
50|$|Independently of the ANSI committee, the CODASYL Programming Language Committee {{was working}} on {{improving}} the language. They described new versions in 1968, 1969, 1970 and 1973, including changes such as new inter-program communication, debugging and <b>file</b> <b>merging</b> facilities as well as improved string-handling and library inclusion features.Although CODASYL was independent of the ANSI committee, the CODASYL Journal of Development was used by ANSI to identify features that were popular enough to warrant implementing.The Programming Language Committee also liaised with ECMA and the Japanese COBOL Standard committee.|$|E
50|$|In Perforce Helix, the {{operation}} that merges changes from one branch to another is called integration. Integration propagates changes from {{a set of}} donor files into a set of corresponding target files; optional branch views can store customized donor - target mappings. By default, integration propagates all outstanding donor changes. Donor changes can be limited or cherry-picked by changelist, date, label, filename, or filename pattern-matching. The system records all integrations, uses them to select common ancestors for <b>file</b> <b>merging,</b> and does not by default perform redundant or unnecessary integrations.|$|E
5000|$|Three-way text <b>file</b> merging; <b>merge</b> {{tracking}} and re-merge prevention; common ancestor detection ...|$|R
5000|$|Mix PDF files where {{a number}} of PDF <b>files</b> are <b>merged</b> {{together}} taking pages alternately from them ...|$|R
5000|$|The default {{extension}} for {{the policy}} file is [...]POL.The policy file filters the settings it enforces by user and by group (a [...] "group" [...] is a defined set of users). To do {{that the policy}} <b>file</b> <b>merges</b> into the Registry, preventing users from circumventing it by simply changing back the settings.The policy file is usually distributed through a LAN, but can {{be placed on the}} local computer.|$|R
40|$|IBM SPSS Statistics 19 Made Simple is a {{book that}} {{presents}} step-by-step implementations {{of a number of}} statistical techniques. No prior knowledge of SPSS is needed. The book begins with an introduction to SPSS, shares datahandling features of the software, and provides some guidance on the choice of statistical models. Throughout the book, annotated screen shots of SPSS dialog boxes and output of each model help the reader to easily follow the process. In this new edition, more emphasis is placed on <b>file</b> <b>merging</b> and aggregation, and chart editing. Exercises from previous editions of the book are available at the book's websit...|$|E
40|$|During {{the last}} two decades, {{computerized}} data acquisition systems (DASs) have been applied at magnetic confinement fusion devices. Present-day data acquisition is done by means of distributed computer systems and transient recorders in CAMAC systems. The development of DASs has been technology driven; the emphasis {{has been on the}} development of computer hardware and system software. For future DASs, challenging problems are to be solved. The DASs have to be better optimized with respect {{to the needs of the}} users. Existing bottlenecks, such as CAMAC-computer coupling or pulse <b>file</b> <b>merging,</b> need to be eliminated. Continuous or long-pulse operation will require the introduction of event abstraction in DAS design...|$|E
40|$|This MCPL version brings several improvements. 	Improve {{unit vector}} packing scheme for higher {{precision}} in stored particle directions (older files using the previous packing scheme {{are of course}} still readable). 	Improve MCNP converters ssw 2 mcpl and mcpl 2 ssw to support more files (including MCNP 5 files and files from in-house MCNP builds). 	Interfaces for <b>file</b> <b>merging</b> is now more user-friendly. 	Add option for universal weights, allowing smaller files in case of unweighted particles. 	Improve documentation in release and add more build mechanisms for "fat" files. 	Add small sample MCPL file (example/example. mcpl) to release. 	And much more. Refer to the CHANGELOG file for details...|$|E
3000|$|... 1 In the {{terminology}} {{of the data}} producer, we have used the Integrated <b>File</b> that <b>merges</b> all countries with comparable data, edition 4.3.|$|R
40|$|Which <b>merges</b> <b>files</b> better: the SAS � DATA Step or SAS SQL? Traditionally, {{the only}} way to <b>merge</b> <b>files</b> in SAS was via the SAS DATA Step. Now SAS {{provides}} a Structured Query Language (SQL) facility which also <b>merges</b> <b>files.</b> This tutorial compares and contrasts these two merge facilities. It examines {{the pros and cons of}} each merge technique. It looks at DATA Step code to perform specific merges and then looks at the corresponding SQL code to perform the same merges...|$|R
40|$|This column {{will focus}} {{on how to improve}} your fluency in Stata. Over the next issues we will look at Stata {{problems}} of intermediate size which turn out to be soluble with a few command lines. As an introduction, systematic ways of repeating the same or similar operations are surveyed to give one overview of the territory to be covered. Copyright 2001 by Stata Corporation. append, by, collapse, contract, do-files, egen, for, foreach, forvalues, log <b>files,</b> <b>merge,</b> naming conventions, programs, repetition, reshape, statsby, subset or group structure, tabulations...|$|R
40|$|In {{ecological}} {{studies the}} recent emphasis on larger study areas over longer time spans has {{coincided with the}} development of geographical information systems GISs are a set of computer hardware and software for analyzing and displaying spatially referenced features,points, lines, polygons) with non-geographic attributes species, age). In the fields of natural resources management and ecology the GIS has been used most frequently for 1) derivation of area or length measures, 2) spatial intersection functions such as <b>file</b> <b>merging,</b> analysis of spatial coincidence and detection of temporal change, 3) proximity analyses, and 4) derivation of data for input in simulation or growth models or calculation of specific metrics. Several cur- rent applications of GISs in ecology and natural resources are reviewed. 1...|$|E
40|$|The ATLAS event store {{employs a}} {{persistence}} framework with extensive navigational capabilities. These include real-time back navigation to upstream processing stages, externalizable data object references, navigation from any data object {{to any other}} both within a single file and across files, and more. The 2013 - 2014 shutdown of the Large Hadron Collider provides an opportunity to enhance this infrastructure in several ways that both extend these capabilities and allow the collaboration to better exploit emerging computing platforms. Enhancements include redesign with efficient <b>file</b> <b>merging</b> in mind, content-based indices in optimized reference types, and support for forward references. The latter provide the potential to construct valid references to data before those data are written, a capability that is useful {{in a variety of}} multithreading, multiprocessing, distributed processing, and deferred processing scenarios. This paper describes the architecture and design of the next generation of ATLAS navigational infrastructure...|$|E
40|$|Cloud storage {{has become}} an {{important}} part of a cloud system nowadays. Most current cloud storage systems perform well for large files but they cannot manage small file storage appropriately. With the development of cloud services, more and more small files are emerging. Therefore, we propose an optimized data replication approach for small files in cloud storage systems. A small <b>file</b> <b>merging</b> algorithm and a block replica placement algorithm are involved in this approach. Small files are classified into four types according to their access frequencies. A number of small files will be merged into the same block based on which type they belong to. And the replica placement algorithm helps to improve the access efficiencies of small files in a cloud system. Related experiment results demonstrate that our proposed approach can effectively shorten the time spent reading and writing small files, and it performs better than the other two already known data replication algorithms: HAR and SequenceFile...|$|E
5000|$|Union mount, {{describing}} {{the concept of}} <b>merging</b> <b>file</b> system branches ...|$|R
40|$|Record linkage is {{used for}} {{preparing}} sampling frames, deduplication of lists and combining information on the same object from two different databases. If the identifiers of the same objects in two different databases have error free unique common identifiers like personal identification numbers (PID), record linkage is a simple <b>file</b> <b>merge</b> operation. If the identifiers contain errors, record linkage is a challenging task. In many applications, the files have widely different numbers of observations, for example a few thousand records of a sample survey and a few million records of an administrative database of social security numbers. Available software, privacy issues and future research topics are discussed. " [author's abstract...|$|R
5000|$|A {{component}} of software configuration management, version control, {{also known as}} revision control or source control, is the management of changes to documents, computer programs, large web sites, and other collections of information. Changes are usually identified by a number or letter code, termed the [...] "revision number", [...] "revision level", or simply [...] "revision". For example, an initial set of files is [...] "revision 1". When the first change is made, the resulting set is [...] "revision 2", and so on. Each revision {{is associated with a}} timestamp and the person making the change. Revisions can be compared, restored, and with some types of <b>files,</b> <b>merged.</b>|$|R
40|$|Abstract — In {{the recent}} years, {{the use of}} {{internet}} get increases, so all user wish to store data on cloud computing platform. Most of the time user‟s files are small in size, so it leads to small files. Hadoop is developed as a software structure for distributed processing of large datasets across large clusters of computers. Hadoop structure consists of Hadoop Distributed file system (HDFS) and Execution engine called Map Reduce layers. HDFS has the property of handling very large datasets whose sizes are in Megabytes, Gigabytes and Terabytes, but the performance of Hadoop Distributed file system degrades when handling large amount of small size files. The massive numbers of small files impose heavy burden to the NameNode of HDFS, correlations between small files were not considered for data storage. In this paper, an efficient approach is designed to improve the storage and access efficiencies of small files, small files are classified based on file correlation features and <b>File</b> <b>merging</b> and prefetching technique is applied for structurally-related small files. AVRO technique is applied to further improve the storage and access efficiency of small files. Experimental procedures show that the proposed technique effectively improves the storage and access efficiency of small files {{when compared with the}} original HDFS...|$|E
40|$|Hadoop {{distributed}} {{file system}} (HDFS) is widely adopted to support Internet services. Unfortunately, native HDFS does not perform well for large numbers but small size files, which has attracted significant attention. This paper firstly analyzes {{and points out}} the reasons of small file problem of HDFS: (1) large numbers of small files impose heavy burden on NameNode of HDFS; (2) correlations between small files are not considered for data placement; and (3) no optimization mechanism, such as prefetching, is provided to improve I/O performance. Secondly, {{in the context of}} HDFS, the clear cut-off point between large and small files is determined through experimentation, which helps determine ‘how small is small’. Thirdly, according to file correlation features, files are classified into three types: structurally-related files, logically-related files, and independent files. Finally, based on the above three steps, an optimized approach is designed to improve the storage and access efficiencies of small files on HDFS. <b>File</b> <b>merging</b> and prefetching scheme is applied for structurally-related small files, while file grouping and prefetching scheme is used for managing logically-related small files. Experimental results demonstrate that the proposed schemes effectively improve the storage and access efficiencies of small files, compared with native HDFS and a Hadoop file archiving facility...|$|E
40|$|HDFS(Hadoop Distributed File System) 凭借其高容错、可伸缩和廉价存储的优点,在当前面向云计算的应用场景中得到了广泛应用。然而,HDFS设计的初衷是存储超大文件,对于海量小文件,由于NameNode内存开销等问题,其存储和读取性能并不理想。提出一种基于小文件合并的方法 HIFM(Hierarchy Index File Merging),综合考虑小文件之间的相关性和数据的目录结构,来辅助将小文件合并成大文件,并生成分层索引。采用集中存储和分布式存储相结合的方式管理索引文件,并实现索引文件预加载。此外,HIFM采用数据预取的机制,提高顺序访问小文件的效率。实验结果表明,HIFM方法能够有效提高小文件存储和读取效率,显著降低NameNode和DataNode的内存开销,适合应用在有一定目录结构的海量小文件存储的应用场合。新闻出版重大科技工程项目(0610 - 1041 BJNF 2328 / 23) |国家科技支撑计划课题(2011 BAH 14 B 02) |中国科学院知识创新工程方向性项目课题(KGCX 2 -YW- 174) Benefiting {{from its}} {{advantages}} of high fault-tolerance, scalability and low-cost storage capability, HDFS (Hadoop distributed file system) {{has been gaining}} widely application in current cloud computing-based applied scenes. However, HDFS is primarily designed for streaming access of ultra-large files and suffers the performance penalty in both storage and accessing while managing massive small files due to the memory overhead problem of NameNode. In this paper, an approach based on combining small files, called HIFM (hierarchy index <b>file</b> <b>merging),</b> is proposed. In it, the correlations between small files and the directory structure of data are comprehensively considered to assist the small files to be merged into large ones and to generate hierarchical index. Centralised storage and distributed storage methods are jointly used in index files management, and the preload of index files is implemented. Besides, {{in order to improve}} the efficiency of sequentially ?accessing? the small files, HIFM adopts data prefetching mechanism. Experimental results show that HIFM can improve the efficiency of ?storing? and accessing small files effectively, and mitigate the memory overhead of NameNode and DataNode obviously. It is suitable for the applications which have massive structured small files storage...|$|E
40|$|TRecord linkage is {{used for}} {{preparing}} sampling frames, deduplication of lists and combining information on the same object from two different databases. If the identifiers of the same objects in two different databases have error free unique common identifiers like personal identification numbers (PID), record linkage is a simple <b>file</b> <b>merge</b> operation. If the identifiers contains errors, record linkage is a challenging task. In many applications, the files have widely different numbers of observations, for example a few thousand records of a sample survey and a few million records of an administrative database of social security numbers. Available software, privacy issues and future research topics are discussed. Record-Linkage, Data-mining, Privacy preserving protocols...|$|R
40|$|Merge {{tools are}} an {{increasingly}} used feature in collaborative environments. Being able to combine versions gives {{greater access to}} contributors as they can work independently on the same work. However, merge tools for working with data beyond textual comparisons have less research devoted to them. This thesis focuses {{on the creation of}} merging functionality for report layouts in Report Definition Language (RDL) through the design of an intuitive interface and effective merge algorithm. Interviews were conducted to gather requirements from the application users. A prototype was implemented for a two <b>file</b> <b>merge</b> with user interaction. The approach taken was to analyze the layout and find the corresponding matches. The interface was designed to maximize usability and included features using colors and viewing windows for comparison...|$|R
5000|$|... meld, KDiff3 - {{visualize}} differences {{very much}} like Kompare, but also allow <b>merging</b> <b>files</b> and editing details of the text.|$|R
40|$|The authors {{abstracted}} {{a sample}} of 7, 536 hospital medical records to validate {{the accuracy of the}} coding of obstetric information on 1) birth certificates, 2) a statewide computerized hospital discharge abstract data system, and 3) a linked <b>file</b> <b>merging</b> birth certificates and the hospital abstract data for Washington State deliveries occurring in 1989. Measures of accuracy of coding of delivery method and obstetric procedures varied greatly among the 23 hospitals that participated in the study. Computerized hos-pital discharge data were generally more complete and accurate than were birth cer-tificate data. The linked file was more likely to identify obstetric procedures than was either source alone. For example, only 84. 1 % of cesarean deliveries noted in the hospital charts were identified on birth certificates (range among hospitals, 37 - 100 %). Using the linked file, the authors identified 99. 8 % of cesarean deliveries (range, 97 - 100 %). Linked birth certificate-hospital abstract files may become an excellent source of data for ep-idemiologic and health care studies; however, further training of medical record per-sonnel and standardization of coding are needed {{to improve the quality of}} computerized data on obstetric events. Am J Epidemiol 1993; 138 : 119 - 27...|$|E
40|$|The thesis {{discusses}} {{implementation and}} use of a testing tool for testing implementations of the OpenCL standard. The standard defines an API interface, runtime environment and OpenCL C programming language, which together standardize programming of all processors supported by the implementation. Implementation of such a comprehensive standard is difficult and prone to errors, and therefore requires a suitable test tool. The testing tool written {{in the context of}} the thesis and sponsorship of the X. Org community represents an open-source solution of the problem. The developed tool enables quick writing of tests and is easily extendable. It introduces a test runner and types of tests which, depending on the configuration of a test, set a suitable OpenCL runtime environment and execute a test in it. Tests written for the tool are minimal and divided into a configuration structure for setting a runtime environment and a test function, or OpenCL C test program, for testing required functionality specified in the standard. With the addition of new types of tests the tool can be arbitrarily extended. The most advanced part of the tool allows implicit writing of tests for programming language OpenCL C where the configuration structure and the test are located in the OpenCL C source <b>file.</b> <b>Merging</b> of the testing tool with the Piglit testing tool adds automatic execution of groups of tests and graphical presentation of results. ...|$|E
40|$|The High Performance Computing (HPC) {{market has}} made a {{significant}} shift from large, monolithic, specialized systems to networked clusters of workstations. This has been precipitated by the continuing upward movement of the price/performance ratio of commodity computing hardware. The fast growth of this market has presented {{a challenge to the}} open source community. Software has not necessarily kept up with the growth. The Parallel Toolkit Library provides support for common design patterns used throughout parallel programs. It includes both PVM and MPI versions. The examples given help users understand how to use the library functions. The data sharing patterns of gather, scatter, and all to all are fully supported. They allow users the flexibility of having odd amounts of data that are not evenly divisible by the number of processes. The two-dimensional versions allow the user to share 2 ̆ 2 ragged 2 ̆ 2 arrays of data. These elements are not provided by PVM or MPI. The <b>file</b> <b>merging</b> functionality automates a common cluster task. The workpools remove a significant layer of detail from writing workpool code. The user of the workpool needs to provide the library with functions for processing tasks and results. The library takes care of sending and receiving tasks and results. Most importantly it handles termination detection, which can be quite cumbersome to design and write. The testing and benchmarking results are consistent with expectations. The library does not add a significant amount of overhead. In some cases, it may be more efficient than code that users would write, because time may not be taken in non-library code to incorporate some efficiencies {{that are part of the}} toolkit library. The library and example code is available at [URL] Libraries such as the toolkit are critical to making clusters easier to write programs for. The toolkit removes a layer of detail for the programmer to need to understand. This will make writing parallel programs easier and faster. The toolkit also provides a tested set of features that will make users 2 ̆ 7 programs more robust...|$|E
50|$|In most cases, users don't {{work with}} {{deployment}} manifest as one big YAML file. Instead, deployment manifest are split into smaller files that {{are easier to}} maintain. These separate <b>files</b> are <b>merged</b> by tools like spiff or spruce, right before they get uploaded to the BOSH server and deployed.|$|R
50|$|When {{two people}} have made changes to copies of the same file, diff3 can produce a merged output that {{contains}} both sets of changes together with warnings about conflicts. diff3 can merge three or more sets of changes to a <b>file</b> by <b>merging</b> two change sets at a time.|$|R
3000|$|... 8 Extracts {{from several}} Social Security {{administrative}} <b>files</b> were <b>merged</b> {{to create the}} TRF, including the Master Beneficiary Record, Supplemental Security Record, Numerical Identification System (Numident) file, the 831 and 832 / 33 Disability files, the Disability Control File, monthly snapshot files, and files from the payment history update system.|$|R
