553|3056|Public
25|$|With color {{motion picture}} film, {{information}} about {{the color of the}} light at each <b>image</b> <b>point</b> is also captured. This is done by analyzing the visible spectrum of color into several regions (normally three, commonly referred to by their dominant colors: red, green and blue) and recording each region separately.|$|E
25|$|Affine shape {{adaptation}} is {{a methodology}} for iteratively adapting {{the shape of}} the smoothing kernels in an affine group of smoothing kernels to the local image structure in neighbourhood region of a specific <b>image</b> <b>point.</b> Equivalently, affine shape adaptation can be accomplished by iteratively warping a local image patch with affine transformations while applying a rotationally symmetric filter to the warped image patches. Provided that this iterative process converges, the resulting fixed point will be affine invariant. In the area of computer vision, this idea has been used for defining affine invariant interest point operators as well as affine invariant texture analysis methods.|$|E
25|$|Coxeter–Dynkin {{diagrams}} can explicitly enumerate {{nearly all}} classes of uniform polytope and uniform tessellations. Every uniform polytope with pure reflective symmetry (all {{but a few}} special cases have pure reflectional symmetry) can be represented by a Coxeter–Dynkin diagram with permutations of markups. Each uniform polytope can be generated using such mirrors and a single generator point: mirror images create new points as reflections, then polytope edges can be defined between points and a mirror <b>image</b> <b>point.</b> Faces can be constructed by cycles of edges created, etc. To specify the generating vertex, one or more nodes are marked with rings, meaning that the vertex {{is not on the}} mirror(s) represented by the ringed node(s). (If two or more mirrors are marked, the vertex is equidistant from them.) A mirror is active (creates reflections) only with respect to points not on it. A diagram needs at least one active node to represent a polytope. An unconnected diagram (subgroups separated by order-2 branches, or orthogonal mirrors) requires at least one active node in each subgraph.|$|E
5000|$|Zero padding: Extend {{the input}} image {{sufficiently}} by adding extra points outside the original image which {{are set to}} zero. The loops over the <b>image</b> <b>points</b> described above visit only the original <b>image</b> <b>points.</b>|$|R
5000|$|... #Caption: An {{illustration}} of some 3D points and their corresponding <b>image</b> <b>points</b> {{as described by}} the pinhole camera model. As the 3D points are moving in space, the corresponding <b>image</b> <b>points</b> are also moving. The motion field consists of the motion vectors in the <b>image</b> for all <b>points</b> in the <b>image.</b>|$|R
5000|$|... for {{corresponding}} <b>image</b> <b>points</b> {{represented in}} normalized image coordinates [...] The problem which the algorithm solves {{is to determine}} [...] {{for a set of}} matching <b>image</b> <b>points.</b> In practice, the image coordinates of the <b>image</b> <b>points</b> are affected by noise and the solution may also be over-determined which means that it may not be possible to find [...] which satisfies the above constraint exactly for all points. This issue is addressed in the second step of the algorithm.|$|R
2500|$|The {{idea of a}} {{projective}} space relates to perspective, more precisely to the way an eye or a camera projects a 3D scene to a 2D image. All points that lie on a projection line (i.e., a [...] "line of sight"), intersecting with the entrance pupil of the camera, are projected onto a common <b>image</b> <b>point.</b> In this case, the vector space is R3 with the camera entrance pupil at the origin, and the {{projective space}} corresponds to the image points.|$|E
2500|$|An {{optical system}} {{typically}} has many openings or structures that limit the ray bundles (ray bundles are {{also known as}} pencils of light). [...] These structures may be {{the edge of a}} lens or mirror, or a ring or other fixture that holds an optical element in place, or may be a special element such as a diaphragm placed in the optical path [...] to limit the light admitted by the system. [...] In general, these structures are called stops, and the aperture stop is the stop that primarily determines the ray cone angle and brightness at the <b>image</b> <b>point.</b>|$|E
2500|$|SPIRE (Spectral and Photometric Imaging Receiver): An {{imaging camera}} and low-resolution {{spectrometer}} covering 194 to 672 micrometre wavelength. The spectrometer had a resolution between R=40 and R=1000 at a wavelength of 250 micrometres {{and was able}} to <b>image</b> <b>point</b> sources with brightnesses around 100millijanskys (mJy) and extended sources with brightnesses of around 500 mJy. The imaging camera had three bands, centred at 250, 350 and 500 micrometres, each with 139, 88 and 43 pixels respectively. It was able to detect point sources with brightness above 2mJy and between 4 and 9mJy for extended sources. A prototype of the SPIRE imaging camera flew on the BLAST high-altitude balloon. NASA's Jet Propulsion Laboratory in Pasadena, Calif., developed and built the [...] "spider web" [...] bolometers for this instrument, which is 40 times more sensitive than previous versions. The Herschel-SPIRE instrument was built by an international consortium comprising more than 18 institutes from eight countries, of which Cardiff University was the lead institute.|$|E
50|$|Starting with a 2D <b>image,</b> <b>image</b> <b>points</b> are {{extracted}} which {{correspond to}} corners in an image. The projection rays from the <b>image</b> <b>points</b> are reconstructed from the 2D points {{so that the}} 3D points, which must be incident with the reconstructed rays, can be determined.|$|R
50|$|The {{pseudo code}} given above {{suggests}} that a neighborhood operation is implemented {{in terms of an}} outer loop over all <b>image</b> <b>points.</b> However, since the results are independent, the <b>image</b> <b>points</b> can be visited in arbitrary order, or can even be processed in parallel. Furthermore, in the case of linear shift-invariant operations, the computation of f at each point implies a summation of products between the image data and the filter coefficients. The implementation of this neighborhood operation can then be made by having the summation loop outside the loop over all <b>image</b> <b>points.</b>|$|R
5000|$|... #Caption: The {{object and}} {{corresponding}} <b>image</b> <b>points</b> can be interchanged ...|$|R
50|$|A common {{example of}} feature vectors appears when each <b>image</b> <b>point</b> {{is to be}} {{classified}} as belonging to a specific class. Assuming that each <b>image</b> <b>point</b> has a corresponding feature vector based on a suitable set of features, meaning that each class is well separated in the corresponding feature space, the classification of each <b>image</b> <b>point</b> can be done using standard classification method.|$|E
5000|$|... #Caption: [...] Figure 1: In {{this phase}} shift image of cells in culture, the height {{and color of}} an <b>image</b> <b>point</b> {{correspond}} to the measured phase shift. The phase shift induced by an object in an <b>image</b> <b>point</b> depends only on the object's thickness and the relative refractive index of the object in the <b>image</b> <b>point.</b> The volume of an object can therefore be determined from a phase shift image when the difference in refractive index between the object and the surrounding media is known.|$|E
5000|$|... = {{undistorted}} <b>image</b> <b>point</b> as projected by {{an ideal}} pinhole camera, ...|$|E
5000|$|If {{the points}} xL and xR are known, their {{projection}} lines are also known. If the two <b>image</b> <b>points</b> {{correspond to the}} same 3D point X the projection lines must intersect precisely at X. This means that X can be calculated from the coordinates of the two <b>image</b> <b>points,</b> a process called triangulation.|$|R
40|$|Aiming to {{the fast}} {{automatic}} matching of artificial target points in digital close range photogrammetry, a new matching method basing on two-image space intersection {{is presented in}} this paper, thinking over the epipolar line constraint in object space. Firstly, a set of initially matched <b>image</b> <b>points</b> is found by calibrating the shortest distance between two image rays. Secondly, the coordinate of corresponding object point is calculated through two-image space intersection, and these points are grouped according to distances between each other. Finally, eliminate those <b>image</b> <b>points</b> whose coordinate errors are too great and {{who are in the}} same image, and find out homologous <b>image</b> <b>points</b> by numbers of <b>image</b> <b>points</b> corresponding with object points. Both experiences prove the advantages of the matching method, which are high speed, high matching quotient and low miss matching quotient. It can also provide precise initial values for the following bundle adjustment. 1...|$|R
2500|$|... {{which is}} the {{constraint}} that the essential matrix defines between corresponding <b>image</b> <b>points.</b>|$|R
5000|$|... = {{distorted}} <b>image</b> <b>point</b> as {{projected on}} image plane using specified lens, ...|$|E
50|$|Mathematically, the {{gradient}} of a two-variable function (here {{the image}} intensity function) is at each <b>image</b> <b>point</b> a 2D vector with the components {{given by the}} derivatives in the horizontal and vertical directions. At each <b>image</b> <b>point,</b> the gradient vector points {{in the direction of}} largest possible intensity increase, and the length of the gradient vector corresponds to the rate of change in that direction. This implies that the result of the Prewitt operator at an <b>image</b> <b>point</b> which is in a region of constant image intensity is a zero vector and at a point on an edge is a vector which points across the edge, from darker to brighter values.|$|E
5000|$|Every {{point of}} [...] {{is within the}} {{constant}} distance [...] of an <b>image</b> <b>point.</b> More formally: ...|$|E
2500|$|Given {{a set of}} {{corresponding}} <b>image</b> <b>points</b> it {{is possible}} to estimate an essential matrix which satisfies the defining epipolar constraint for all the points in the set. [...] However, if the <b>image</b> <b>points</b> are subject to noise, which is the common case in any practical situation, {{it is not possible to}} find an essential matrix which satisfies all constraints exactly.|$|R
50|$|This {{generally}} {{creates a}} network of critical curves, lines connecting <b>image</b> <b>points</b> of infinite amplification.|$|R
50|$|In practice, {{however, the}} {{coordinates}} of <b>image</b> <b>points</b> cannot be measured with arbitrary accuracy. Instead, {{various types of}} noise, such as geometric noise from lens distortion or interest point detection error, lead to inaccuracies in the measured image coordinates. As a consequence, the lines generated by the corresponding <b>image</b> <b>points</b> do not always intersect in 3D space. The problem, then, {{is to find a}} 3D point which optimally fits the measured <b>image</b> <b>points.</b> In the literature there are multiple proposals for how to define optimality and how to find the optimal 3D point. Since they are based on different optimality criteria, the various methods produce different estimates of the 3D point x when noise is involved.|$|R
50|$|Mathematically, the {{gradient}} of a two-variable function (here {{the image}} intensity function) at each <b>image</b> <b>point</b> is a 2D vector with the components {{given by the}} derivatives in the horizontal and vertical directions. At each <b>image</b> <b>point,</b> the gradient vector points {{in the direction of}} largest possible intensity increase, and the length of the gradient vector corresponds to the rate of change in that direction.|$|E
5000|$|All rays [...] "originating" [...] {{from any}} object point {{converge}} {{to a single}} <b>image</b> <b>point</b> (Imaging is stigmatic).|$|E
50|$|In certain implementations, this {{separable}} computation may be advantageous {{since it}} implies fewer arithmetic computations for each <b>image</b> <b>point.</b>|$|E
5000|$|Assign all <b>image</b> <b>points</b> to {{foreground}} or background by {{a weighted}} nearest neighbor search {{in the color}} signatures.|$|R
5000|$|... #Caption: Long Point Peninsula {{with bright}} {{sediment}} plumes. (The {{top of the}} <b>image</b> <b>points</b> southwest, rather than north.) Source: NASA ...|$|R
5000|$|... #Caption: A {{panoramic}} view {{from inside the}} Kalalau valley. The center of the <b>image</b> <b>points</b> toward the ocean in the northwest.|$|R
5000|$|A camera model maps {{each point}} [...] in 3D space to a 2D <b>image</b> <b>point</b> [...] {{according}} to some mapping functions : ...|$|E
5000|$|... where [...] is the {{filtering}} parameter (i.e., standard deviation) and [...] is {{the local}} mean value of the <b>image</b> <b>point</b> values surrounding [...]|$|E
5000|$|... #Caption: In 1675, {{a pencil}} was {{interpreted}} as a double cone of rays, as from an object point, through a lens, to an <b>image</b> <b>point.</b>|$|E
40|$|This paper {{proposes a}} novel {{calibration}} method for fisheye cameras using sphere images {{which is an}} extension of the calibration method for central catadioptric cameras. We show that, each sphere image under fisheye cameras is tangent to the modified calibrating conic (MCC) at two double-contact <b>image</b> <b>points.</b> The calibration method is proposed by first finding the double-contact <b>image</b> <b>points,</b> then fitting the MCC, and final obtaining the intrinsic parameters using the LDLT factorization of the MCC. 1...|$|R
50|$|Therefore, {{once the}} {{coordinates}} of <b>image</b> <b>points</b> is known, besides {{the parameters of}} two cameras, the 3D coordinate of the point can be determined.|$|R
25|$|The {{essential}} matrix {{can be seen}} as {{a precursor}} to the fundamental matrix. Both matrices can be used for establishing constraints between matching <b>image</b> <b>points,</b> but the essential matrix can only be used in relation to calibrated cameras since the inner camera parameters must be known in order to achieve the normalization. If, however, the cameras are calibrated the essential matrix can be useful for determining both the relative position and orientation between the cameras and the 3D position of corresponding <b>image</b> <b>points.</b>|$|R
