1|25|Public
40|$|Abstract. Aiming at {{the problem}} of the {{harmonic}} of the space vector pulse width modulation (SVPWM), the analysis methods to SVPWM are studied. After introducing the principle of SVPWM, the paper discusses the harmonic distortion factor (HDF) that is an important assessing index of the harmonics. The two analysis methods- ripple current method and <b>error</b> <b>triangle</b> method – are discussed, and the relation between them is revealed. Finally, the micro HDF of the random zero distribution PWM (RZDPWM) is computed based on the two methods and analyzed...|$|E
40|$|In many {{applications}} one {{is concerned with}} the approximation of functions from a finite set of scattered data sites with associated function values. We describe a scheme for constructing a hierarchy of triangulations that approximates a given data set at varying levels of resolution. Intermediate triangulations can be associated with a particular level of a hierarchy by considering their approximation errors. We present a data-dependent triangulation scheme using a Sobolev norm to measure error instead of the more commonly used root-mean-square (RMS) <b>error.</b> <b>Triangles</b> are split by selecting points in a triangle, or its neighbors, that are in areas of potential discontinuites or areas of hight gradients. We call such points "significant points. "...|$|R
40|$|The paper {{discusses}} {{single and}} double curvature centrifugal turbo machine rotor blades which {{can use a}} set of profiles, the fundamental characteristics of which were derived from experiments in the wind tunnel of the large experimental laboratory of NACA, now NASA. A criterion is formulated for extending {{the use of these}} profiles of four digits to radial flow rotors. The novelty of the method is not in the circumferential design of the profiles but as a plane development of the sections, characteristic of the <b>error</b> <b>triangles</b> method. The results of the numerical investigations, compared with more traditional solutions such as those with multiple arcs of a circle, confirm the efficiency and flexibility of this new procedure. This possibility of adopting profiles of known geometrical configuration has permitted automatic generation of the blade surface, and also further assisted design of even more complex blades such as those of multi-stage machine diffusors...|$|R
40|$|We are {{developing}} a multilingual machine translation system to provide foreign tourists with a multilingual speech translation service in the Winter Olympic Games that {{will be held in}} Korea in 2018. For a knowledge learning to make the multilingual expansibility possible, we needed large bilingual corpus. In Korea {{there were a lot of}} Korean-English bilingual corpus, but Korean-French bilingual corpus and Korean-Spanish bilingual corpus lacked absolutely. Korean-English-French and Korean-English-Spanish triangle corpus were constructed by crowdsourcing translation using the existing large Korean-English corpus. But we found a lot of translation <b>errors</b> from the <b>triangle</b> corpora. This paper aims at filtering of translation <b>errors</b> in large <b>triangle</b> corpus constructed by crowdsourcing translation to reduce the translation loss of triangle corpus with English as a pivot language. Experiment shows that our method improves + 0. 34 BLEU points over the baseline system. ...|$|R
40|$|The error {{will inevitably}} appear in {{triangle}} location in WSNs (wireless sensor networks), and {{the analysis of}} its character {{is very important for}} a reliable location algorithm. To research the <b>error</b> in <b>triangle</b> location algorithm, firstly, a triangle location model is set up; secondly, the condition of the minimal error is got based on the analysis of the location model; at last, by analyzing the condition, the law of error’s changing is ab-stracted, and its correctness is validated by some testing. The conclusions in this paper are all proved, and they provide a powerful foundation and support for reliable location algorithm’s research...|$|R
30|$|Figure  7 {{shows the}} robot arm {{positioning}} error at different model placement angles. According {{to the visual}} robot’s positioning error for different angles of different models, this paper calculates the positioning error of visual robots for different models. The average positioning error of the regular quadrilateral is 0.86  mm, the average positioning error of the circular shape is 0.8  mm, and the average positioning <b>errors</b> for the <b>triangle</b> and the hexagon are 0.93  mm and 1.27  mm, respectively. The data {{results show that the}} robot system will produce different positioning errors for different shape models. The mean value of the positioning error of the visual robot for the regular quadrilateral and the circle is significantly smaller than the mean value of the positioning <b>error</b> for the <b>triangle</b> and the hexagon. Therefore, the use of a regular quadrilateral and circular model with low positioning error rate can improve the accuracy of positioning.|$|R
5000|$|At the {{southwest}} corner of Seventh Avenue and Christopher Street is a triangular mosaic which reads [...] "Property of the Hess Estate Which Has Never Been Dedicated for Public Purpose". David Hess was a property owner whose building was condemned to make room for a subway line in the early 1900s. Due to a surveying <b>error,</b> this small <b>triangle</b> remained in the possession of Hess's heirs, who refused to donate it to the city.|$|R
40|$|Abstract. Aiming at the {{nonlinear}} {{and time}} variant {{characteristics of a}} ramp control system, a fuzzy control method is applied to freeway ramp metering in this paper. A cell transmission model (CTM) to describe the freeway flow process is firstly established. Based on the model and in conjunction with nonlinear feedback theory, fuzzy control based ramp controllers are then designed. The ramp metering rates are determined by fuzzy control according to density tracking errors and <b>error</b> increments. <b>Triangle</b> curves are used for the membership functions of the fuzzy variables. The rule base including 56 fuzzy rules is also established. Finally, the controllers are simulated in MATLAB software. The results show that this method can achieve a perfect density tracking performance and eliminate traffic congestion. This approach is quite effective to freeway ramp metering...|$|R
40|$|Network {{coordinates}} {{provide a}} mechanism for selecting and placing servers efficiently in a large distributed system. This approach works well {{as long as the}} coordinates continue to accurately reflect network topology. We conducted a long-term study of a million-plus node coordinate system and found that it exhibited some of the problems for which network coordinates are frequently criticized, for example, inaccuracy and fragility in the presence of violations of the triangle inequality. Fortunately, we show that several simple techniques remedy many of these problems. Using the Azureus BitTorrent network as our testbed, we show that live, large-scale network coordinate systems behave differently than their tame PlanetLab and simulation-based counterparts. We find higher relative <b>errors,</b> more <b>triangle</b> inequality violations, and higher churn. We present and evaluate a number of techniques that, when applied to Azureus, efficiently produce accurate and stable network coordinates. ...|$|R
40|$|The theorem of Steinhagen {{establishes}} a relation between inradius and {{width of a}} convex set. The half of the width {{can be interpreted as}} the minimum of inradii of all 1 -dimensional orthogonal projections of a convex set. By considering i-dimensional projections we obtain series of i-dimensional inradii. In this paper we study some relations between these inradii and by this we find a natural generalization of Steinhagen’s theorem. Further we show in the course of our investigation that the minimal <b>error</b> of the <b>triangle</b> inequality for a set of vectors cannot be too large...|$|R
40|$|Discretized Marching Cubes (DMC) is a {{standard}} method in computer graphics and visualization for constructing 3 D surfaces in data represented on a regular grid. After thresholding, it builds high-resolution surfaces by tiling surface patches halfway between objects and background in the data. This paper shows that if surfaces are built locally, in a high-resolution sub-grid of a cell instead of directly in a cell, sharp surfaces can be generated {{in order to preserve}} concave and convex object features. The main advantage is the improved geometric models that are extracted. This makes lower approximation <b>errors</b> and lower <b>triangle</b> counts possible...|$|R
40|$|In {{this paper}} {{we present a}} method for the {{adapting}} a geometrical deformable model (GDM) for reconstruction of real-world objects from multiple range images. Our approach registers the range images simultaneously, carves out an immediate volume and finally generates an accurate, sparse triangle mesh. The proposed GDM scheme refines an initial roughly approximated mesh by deformation and adaptive subtriangulation. Even {{in the case of}} very large data sets our approach presents an efficient method of surface reconstruction due to adaptive improvement to the desired degree of accuracy. Since the root mean square approximation <b>error</b> of each <b>triangle</b> is minimized in an iterative procedure, the mesh quality is higher than that of previous approaches...|$|R
40|$|AbstractThis is an {{adaptive}} subroutine that computes an approximation to the integral of a function f(x, y) over a two dimensional region {{made up of}} triangles. Lyness–Jespersen rules {{form the basis for}} a local quadrature module that is used to estimate the integral and the <b>error</b> over each <b>triangle.</b> The triangle with the largest error is subdivided and the local quadrature module is applied to each sub-triangle to obtain new estimates of the integral and the error. This process is repeated until either (1) an error tolerance is satisfied, (2) the number of triangles exceeds an input parameter MAXTRI, (3) the number of integrand evaluations exceeds an input parameter MEVALS, or (4) the subroutine senses that round-off error is beginning to contaminate the result...|$|R
40|$|We {{propose the}} LEPP-surface {{triangulation}} method {{that starts with}} a coarse initial triangu-lation of input grid terrain data, and incrementally adds data points that reduce the worst edge approximation error in the mesh. The method generalizes a previous LEPP-centroid method in two dimensions as follows: for the edge e, having highest error in the mesh, one or two points close to (one or two) terminal edges associated to e, are inserted in the mesh. The edge error is computed by adding the <b>triangle</b> approximation <b>errors</b> of the two triangles that share e, while each <b>triangle</b> <b>error</b> in L 2 -norm is computed by using a curvature tensor (good approximation of the surface) at a representative point associated to both triangles. The method produces trian-gular approximations that capture well the relevant features of the terrain surface by naturally producing well-shaped triangles. ...|$|R
40|$|In this paper, since {{importance}} (i. e., light map texel density) {{is constant}} in a triangle primitive, it is precomputed {{and stored in}} each triangle for acceleration. In theory, such importance I(t) for a triangle t can be computed as follows: I(t) = Al(t), (1) Ao(t) where Ao(t) is {{the area of the}} triangle in object space and Al(t) is the area of the triangle in light map space. However, it can lead to large <b>errors</b> for small <b>triangles</b> in practice, as shown in Figure 2 (a). UV unwrapping tools can produce noise caused by lack of computation precision which is generally undetectable to artists. Even if the area of a triangle in light map space is imperceptible, it can be much larger than the area in object space for practical scenes. To avoid noise, an image space technique is employed i...|$|R
40|$|This work {{considers}} {{the application of}} the finite element method to a variety of two-dimensional partial differential equations using unstructured meshes of triangles. Conventional mesh refinement strategies involve estimating the local <b>error</b> on each <b>triangle</b> and then subdividing those triangles for which the error exceeds some predefined tolerance. Whilst this form of adaptivity (h-refinement) is generally quite robust, the work described in this paper demonstrates that the additional use of mesh movement via nodal relocation (r-refinement) can significantly enhance the overall efficiency of the adaptive method. Moreover, these efficiency gains may be obtained very cheaply in terms of the extra software development overhead. Key words. Finite Element Methods, mesh adaptivity, mesh quality, stability, efficiency. 1 Introduction In this paper we consider the use of adaptive finite element algorithms for the solution of a variety of steady, elliptic-type, partial differential e [...] ...|$|R
40|$|We propose Graph Priority Sampling (GPS), a new {{paradigm}} for order-based reservoir sampling from massive streams of graph edges. GPS provides a general way to weight edge sampling according to auxiliary and/or size variables so as to accomplish various estimation goals of graph properties. In the context of subgraph counting, we show how edge sampling weights can be chosen so as to minimize the estimation variance of counts of specified sets of subgraphs. In distinction with many prior graph sampling schemes, GPS separates the functions of edge sampling and subgraph estimation. We propose two estimation frameworks: (1) Post-Stream estimation, to allow GPS to construct a reference sample of edges to support retrospective graph queries, and (2) In-Stream estimation, to allow GPS to obtain lower variance estimates by incrementally updating the subgraph count estimates during stream processing. Unbiasedness of subgraph estimators is established through a new Martingale formulation of graph stream order sampling, which shows that subgraph estimators, written {{as a product of}} constituent edge estimators are unbiased, even when computed at different points in the stream. The separation of estimation and sampling enables significant resource savings relative to previous work. We illustrate our framework with applications to triangle and wedge counting. We perform a large-scale experimental study on real-world graphs from various domains and types. GPS achieves high accuracy with less than 1 % <b>error</b> for <b>triangle</b> and wedge counting, while storing {{a small fraction of the}} graph with average update times of a few microseconds per edge. Notably, for a large Twitter graph with more than 260 M edges, GPS accurately estimates triangle counts with less than 1 % error, while storing only 40 K edges...|$|R
40|$|The {{standard}} {{method of building}} compact triangulated surface approximations to terrain surfaces (TINs) from dense digital elevation models(DEMs) adds points to an initial sparse triangulation or removes points from a dense initial mesh. Typically, in each triangle in the current TIN, the worst fitting point, in terms of vertical distance, is selected. The order of insertion of the points {{is determined by the}} magnitude of the maximum vertical difference. This measure produces triangulations that minimize the maximum vertical distance between the TIN and the source DEM. Other approximation criteria are often used, however, including the root-mean-squared error or the mean absolute error, both for the vertical difference and normal difference, i. e. the distance {{in the direction of the}} normal to the triangular approximation. For these approximation criteria, we still select the worst fit point, but determine the insertion order by various sums of <b>errors</b> over the <b>triangle.</b> Experiments show that using these better evaluation measures significantly reduces the size of the TIN for a given approximation error...|$|R
40|$|We {{present a}} new {{algorithm}} for extracting adaptive multiresolution triangle meshes from volume datasets. The algorithm guarantees that the topological genus of the generated mesh {{is the same}} as the genus of the surface embedded in the volume dataset at all levels of detail. In addition to this "hard constraint" on the genus of the mesh, the user can choose to specify some number of soft geometric constraints, such as triangle aspect ratio, minimum or maximum total number of vertices, minimum and/or maximum triangle edge lengths, maximum magnitude of various <b>error</b> metrics per <b>triangle</b> or vertex, including maximum curvature (area) error, maximum distance to the surface, and others. The mesh extraction process is fully automatic and does not require manual adjusting of parameters to produce the desired results as long as the user does not specify incompatible constraints. The algorithm robustly handles special topological cases, such as trimmed surfaces (intersections of the surface with the volume boundary), and manifolds with multiple disconnected components (several closed surfaces embedded in the same volume dataset). The meshes may self-intersect at coarse resolutions. However, the self-intersections are corrected automatically as the resolution of the meshes increase. We show several examples of meshes extracted from complex volume datasets...|$|R
40|$|We propose {{and discuss}} a new Lepp-surface method {{able to produce}} a small {{triangular}} approximation of huge sets of terrain grid data by using a two-goal strategy that assures both small approximation error and well-shaped 3 D triangles. This is a refinement method which starts with a coarse initial triangulation of the input data, and incrementally selects and adds data points into the mesh as follows: for the edge e having the highest error in the mesh, one or two points close to (one or two) terminal edges associated with e are inserted in the mesh. The edge error is computed by adding the <b>triangle</b> approximation <b>errors</b> of the two triangles that share e, while each L 2 -norm <b>triangle</b> <b>error</b> is computed by using a curvature tensor (a good approximation of the surface) at a representative point associated with both triangles. The method produces triangular approximations that capture well the relevant features of the terrain surface by naturally producing well-shaped triangles. We compare our method with a pure L 2 -norm optimization methodThe first, second and fourth authors were partially supported by the Spanish Ministerio de Educacion y Ciencia under grant TIN 2010 - 20590 -C 02 - 0...|$|R
40|$|Figure 1 : Simplifying the zero-isosurface of a {{directed}} distance {{volume of}} 256 3 using our topology-preserving isosurface simplification algorithm. Although the cylinders {{and the box}} {{are very close to}} each other, they don’t touch, and thus there are several disconnected surface components in this volume. Note that disconnected surface components are assigned different materials and are clustered independently. (a) δ 2 = 0 and t = 97 K. (b) δ 2 = 10 − 6 and t = 23 K. (c) δ 2 = 10 − 4 and t = 2494. (δ 2 : quadric <b>error</b> threshold, t: <b>triangle</b> count.) We present a fast, topology-preserving approach for isosurface simplification. The underlying concept behind our approach is to preserve the disconnected surface components in cells during isosurface simplification. We represent isosurface components in a novel representation, called enhanced cell, where each surface component in a cell is represented by a vertex and its connectivity information. A topology-preserving vertex clustering algorithm is applied to build a vertex octree. An enhanced dual contouring algorithm is applied to extract error-bounded multiresolution isosurfaces from the vertex octree while preserving the finest resolution isosurface topology. Cells containing multiple vertices are properly handled during contouring. Our approach demonstrates better results than existing octree-based simplification techniques...|$|R
40|$|This paper {{presents}} {{the application of}} a real-time closed-loop control for the quasistatic axis of electrostatic micro scanning mirrors. In comparison to resonantly driven mirrors, the quasistatic comb drive allows arbitrary motion profiles with frequencies up to its eigenfrequency. A current mirror setup at Fraunhofer IPMS is manufactured with a staggered vertical comb (SVC) drive and equipped with an integrated piezo-resistive deflection sensor, which can potentially be used as position feedback sensor. The control design is accomplished based on a nonlinear mechatronic system model and the preliminary parameter characterization. In previous papers [1, 2] we have shown that jerk-limited trajectories, calculated offline, provide a suitable method for parametric trajectory design, taking into account physical limitations given by the electrostatic comb and thus decreasing the dynamic requirements. The open-loop control shows in general unfavorable residual eigenfrequency oscillations leading to considerable tracking <b>errors</b> for desired <b>triangle</b> trajectories [3]. With real-time closed-loop control, implemented on a dSPACE system using an optical feedback, we can significantly reduce these errors and stabilize the mirror motion against external disturbances. In this paper we compare linear and different nonlinear closed-loop control strategies as well as two observer variants for state estimation. Finally, we evaluate the simulation and experimental results in terms of steady state accuracy and the concept feasibility for a low-cost realization...|$|R
40|$|Automatic mesh {{optimization}} algorithms {{suffer from}} the problem that humans are not uniformly sensitive to changes on {{different parts of the}} body. This is a problem because when a mesh optimization algorithm typically measures <b>errors</b> caused by <b>triangle</b> reductions, the <b>errors</b> are strictly geometrical, and an error of a certain magnitude on the thigh of a 3 D model will be perceived by a human as less of an error than one of equal geometrical significance introduced on the face. The partial solution to this problem proposed in this paper consists of detecting the faces of the 3 D assets to be optimized using conventional, existing 2 D face detection algorithms, and then using this information to selectively and automatically preserve the faces of 3 D assets that are to be optimized, leading to a smaller perceived error in the optimized model, albeit not necessarily a smaller geometrical error. This is done by generating a set of per-vertex weights that are used to scale the errors measured by the reduction algorithm, hence preserving areas with higher weights. The final optimized meshes produced by using this method is found to be subjectively closer to the original 3 D asset than their non-weighed counterparts, and if the input meshes conform to certain criteria this method is well suited for inclusion in a fully automatic mesh decimation pipelin...|$|R
40|$|For any {{triangulation}} {{of a given}} polygonal region, {{consider the}} piecewise linear least squares approximation of a given smooth function u. The problem is to characterize triangulations for which the global error of approximation is minimized {{for the number of}} triangles. The analogous problem in one dimension has been thoroughly analyzed, but in higher dimensions one has also to consider the shapes of the subregions, and not only their relative size. After establishing the existence of such an optimal triangulation, the local problem of best triangle shape is considered. Using an expression for the error of approximation involving the matrix H of second derivatives, the best shaped triangle is seen to be an equilateral transformed by a matrix related to H. This triangle is long in the direction of minimum curvature and narrow in the direction of maximum curvature, as one would expect. For the global problem, a series of two lower bounds on the approximation error are obtained, which suggest an asymptotic error estimate for optimal triangulation. The error estimate is shown to hold, and the conditions for attaining the lower bounds characterize the sizes and shapes of the triangles in the optimal triangulation. The shapes are seen to approach the optimal shapes described in the local analysis, and the <b>errors</b> on the <b>triangles</b> are seen to be asymptotically balance...|$|R
40|$|A key task {{in social}} network and other complex network {{analysis}} is role analysis: describing and categorizing nodes {{according to how}} they interact with other nodes. Two nodes have the same role if they interact with equivalent sets of neighbors. The most fundamental role equivalence is automorphic equivalence. Unfortunately, the fastest algorithms known for graph automorphism are nonpolynomial. Moreover, since exact equivalence may be rare, a more meaningful task is to measure the role similarity between any two nodes. This task {{is closely related to}} the structural or link-based similarity problem that SimRank attempts to solve. However, SimRank and most of its offshoots are not sufficient because they do not fully recognize automorphically or structurally equivalent nodes. In this paper we tackle two problems. First, what are the necessary properties for a role similarity measure or metric? Second, how can we derive a role similarity measure satisfying these properties? For the first problem, we justify several axiomatic properties necessary for a role similarity measure or metric: range, maximal similarity, automorphic equivalence, transitive similarity, and the triangle inequality. For the second problem, we present RoleSim, a new similarity metric with a simple iterative computational method. We rigorously prove that RoleSim satisfies all the axiomatic properties. We also introduce an iceberg RoleSim algorithm which can guarantee to discover all pairs with RoleSim score no less than a user-defined threshold θ without computing the RoleSim for every pair. We demonstrate the superior interpretative power of RoleSim on both both synthetic and real datasets. Comment: 17 pages, twocolumn Version 2 of this technical report fixes minor <b>errors</b> in the <b>Triangle</b> Inequality proof, grammatical errors, and other typos. Edited and more polished version to be published in KDD' 11, August 201...|$|R
40|$|Background: The {{vestibular}} {{system has}} been shown to contribute to mechanisms of locomotion such as distance perception. Galvanic vestibular stimulation (GVS) is a tool used to perturb the vestibular system, and causes significant deviations in path trajectory during locomotion. Previous research has suggested that applying GVS during straight-line locomotion tasks is not sufficient {{to determine the effects of}} the vestibular system on locomotion. However, spatial navigation challenges one’s ability to navigate throughout the environment using idiothetic cues to constantly update one’s position. The purpose of the current study was to determine the effects of GVS on both path trajectory and body rotation during a task of spatial navigation in the absence of visual cues, and how accuracy of this task is affected by dance training. It was hypothesized that the delivery of GVS would significantly increase <b>errors</b> during the <b>triangle</b> completion task, and this increase would be more pronounced in the control participants compared to the dancers. Methods: Participants (n= 34, all female, 18 - 30 years) were divided into two groups: controls (n= 18) had no experience with sport-specific training while dancers (n= 16) had previously experienced dance training (M = 15. 6 years, SD = ± 4. 1) and were still currently training in dance (M = 11. 5 hours/week, SD = ± 7. 3). Monofilament testing (Touch-Test Six Piece Foot Kit) was used to determine the plantar surface cutaneous sensitivity threshold and a joint angle-matching task was used to quantify the proprioceptive awareness of each individual. Participants completed trials of the triangle completion task in VR (via Oculus Rift DK 2), during which they would navigate along the first two legs of one of two triangles using visual input, and then accurately navigate back to their initial position with the use of vision. GVS was delivered at three times the participant’s threshold in either the left or right direction prior to the final body rotation and until the participant reached their end position. The task was completed six times for each of the GVS conditions (with and without GVS) with the experimental GVS condition being further divided into right and left perturbation trials, for each of the two triangles, in both the right and left triangle directions, for a total of 48 trials (six trials x 2 GVS conditions x 2 triangles x 2 directions). Whole body kinematic data were collected at 60 Hz using an NDI Optotrak motion tracking system. Results: No significant differences were observed between control subjects and dancers with respect to arrival error, angular error, path variability, cutaneous sensitivity or proprioceptive awareness. However, there was a significant effect of GVS on both arrival error and angular error. Conditions without GVS had significantly smaller angular error than both conditions with GVS. In addition, GVS conditions with the perturbation in the same direction as the final body rotation had significantly greater arrival error than both the condition without GVS and with the current in the opposite direction of the final body rotation. There was no significant difference between GVS conditions in path variability during the return to the initial position. Conclusions: The significant effect of GVS on both arrival error and angular rotation demonstrates that vestibular perturbation reduced the accuracy of the triangle completion task. These findings suggest that the vestibular system plays a major role in both path trajectory and body rotation during tasks of spatial navigation in the absence of vision...|$|R

