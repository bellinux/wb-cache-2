57|20|Public
25|$|The {{user can}} save the {{workspace}} with all values, programs, and <b>execution</b> <b>status.</b>|$|E
50|$|Hinemos Job map:An {{option to}} add an {{interface}} to see the job configuration and Job <b>execution</b> <b>status</b> graphically.|$|E
50|$|Project {{controlling}} {{is represented}} by feedback, <b>execution</b> <b>status</b> of tasks, financial control and variance analysis. In addition, email notification feature helps to keep the important due dates in mind.|$|E
40|$|Abstract—In this paper, {{we present}} an {{architecture}} that encapsulates system hardware inside a software component used for job <b>execution</b> and <b>status</b> monitoring. The develop-ment of this interface has enabled system simulation, which yields {{a number of}} novel benefits, including dramatically improved debug and testing capabilities. I...|$|R
40|$|Exploit {{parallel}} processing OSG resources � Simplify submission to hide details (RSL/targeting) � Integrate with existing submission models � Explore MPI delivery and <b>execution</b> � <b>Status</b> � 8 -way jobs are the practical upper bound � About {{a half dozen}} sites are HTPC enabled � Implementing discoverable GIP configuration Steven Cox...|$|R
40|$|In the cross-organization Emergency Response System (ERS), some tasks {{may depend}} on the {{coordination}} between various organizations in the disposal process of an accident. So monitoring and commanding the task-flow during the process is needed. In this study, firstly, we analyze the basic principles and define task-flow model in cross-organization emergency response system. Then we design and develop a visual tool to manage and monitor cross-organization workflow, tasks operations and cooperative control relationships among different organizations. The study introduces the architecture and the basic functions of the tool in detail and illustrates the feature technologies, such as the synchronization mechanism between different workflow editors, multi-view representations of workflow, workflow recommendation and dynamic monitoring over tasks’ <b>execution</b> <b>statuses,</b> etc. Finally, the study shows the significance and value of the tool in the emergency disposal process of a coal mine accident instance...|$|R
50|$|Most modern shells support command history. Shells which support command {{history in}} general also {{supports}} completion from history {{rather than just}} recallingcommands from the history. In addition to the plain command text, PowerShell also records execution start- and end time and <b>execution</b> <b>status</b> in the command history.|$|E
50|$|Rational Quality Manager {{includes}} {{a set of}} predefined reports to give {{the status of the}} project. In addition, live, test <b>execution</b> <b>status</b> can be seen by opening a test plan or by browsing a list of test plans and opening the execution view. The relationship between test artifacts, requirements, and development artifacts can be traced by browsing a list of test artifacts and opening the traceability view.|$|E
50|$|Pipeline {{tracking}} {{is one of}} the {{key features}} of an accounting system and software for asset management. This provides summarized information on all the details pertaining to the potential investments that are being monitored. The system and software will organize the pipeline and record the source, <b>execution</b> <b>status,</b> approval status, feasible investment capital and the targeted purchase price. It provides an efficient analysis of the best deals, timing and price for the utilization of the investment team. Pipeline tracking provides tracking of the source, history and status. It also provides customized classifications and categories. The system can easily execute a cash flow model and create return assumptions.|$|E
30|$|The {{independent}} simulations configure the climatology as a BOT {{procedure and}} {{a good example of}} intensive computing. The portal serves for simulations configuration and submission. Once submitted to the computational grid, a scheduler assigns the simulation workflow to an HPC cluster that executes the sequential and parallel components of the workflow. A database stores information of each job (job <b>execution</b> time and <b>status),</b> with access through the portal.|$|R
40|$|Abstract—Due to {{increasing}} hardware capacity, computing grids have been handling and processing more data. This {{has led to}} higher amount of energy being consumed by grids; hence the necessity for strategies to reduce their energy consumption. Scheduling is a process carried out to define in which node tasks will be executed in the grid. This process can significantly impact the global system performance, including energy consumption. This paper focuses on a scheduling model for opportunistic grids that considers network traffic, distance between input files and execution node {{as well as the}} <b>execution</b> node <b>status.</b> The model was tested in a simulated environment created using GreenCloud. The simulation results of this model compared to a usual approach show a total power consumption savings of 7. 10 %...|$|R
40|$|The thesis {{contains}} a historical {{overview of the}} origin and development institute of protection {{of the rights of}} third parties in enforcement proceedings. In this section, {{the focus is on the}} period of the Roman Empire and then follows the development from the 19 th century untiltoday. The fundamental part of the work focuses on the description of the current state of enforcement proceedings under the Act no. 120 / 2001 Coll. The Executors and <b>Execution,</b> his <b>status</b> in the legal order of the Czech Republic, the basic principles of management and everything in relation to the protection of the rights of third parties in enforcement proceedings. Distributes third party to various groups and describes their rights and obligations under the legislation. It focuses on the means of protection and defense. In conclusion provides an overview of the most important novels enforceable rights in relation...|$|R
30|$|My-favorite webscript adds {{documents}} and folders to “favorites”, deletes them, returns the <b>execution</b> <b>status.</b>|$|E
30|$|In {{the main}} fuzzing loop, target {{programs}} are executed repeatedly. Information of program <b>execution</b> <b>status</b> are extracted {{and used to}} improve the program execution. Two key problems involved in the execution phase is how to guide the fuzzing process and how to explore new path.|$|E
30|$|Methods {{to support}} {{education}} are mainly divided into student support and teacher support. Many {{studies have focused}} on student support in programming education such as the visualization program <b>execution</b> <b>status</b> (Ishizue et al. 2017 b; 2018) and a method to learn a language based on another language already learned (Li et al. 2017). This study focuses on teacher support.|$|E
30|$|CSP {{is a new}} {{application}} concept which utilizes the standardized communication protocol and the secure information network available in a digital substation. The Process Level accomplishes synchronous sample value acquisition, action command <b>execution</b> and breaker <b>status</b> uploading. The Bay Level consists {{of a number of}} IEDs which work in a harmonious and coordinated manner to provide reliable fault detection. The coordination of IEDs includes information sharing and coordinated backup tripping. The Station Level integrates useful data and events for substation management and provides communication with the control center or a SDH network.|$|R
40|$|Debarment {{action in}} {{execution}} proceedings The thesis targets {{the interpretation of}} debarment actions in execution proceedings. Generally the thesis provides a definition of debarment actions as a legal institution {{for the protection of}} third party rights and of debarment action execution proceedings. On the other hand the thesis is very particular in terms of the specifics of individual forms of debarment actions. The interpretation does presented also refers to current jurisprudence as applied by Czech courts as regards debarment actions, as well as in further related cases. The legal interpretation points out to the possibility of future amendments to certain legal provisions. The first Chapter of the thesis defines fundamental legal terms and institutions, which create a broader base for debarment action interpretations. The thesis focuses on the definition of notions of <b>execution,</b> the <b>status</b> of bailiffs, <b>execution</b> proceedings and the parties thereto as well as the sources of law including the definition of the inter relationship between Civil Procedure Code and Execution Procedure, which are cardinal for the main subject of the thesis. The interpretation contained in Chapter two aims at general description of debarment actions, their subjects and status within incidence disputes and it also [...] ...|$|R
40|$|The Hanford Site Environment, Safety and Health (ES&H) Budget-Risk Management Summary {{report is}} {{prepared}} to support the annual request to sites in the U. S. Department of Energy (DOE) Complex by DOE, Headquarters. The request requires sites to provide supplementary crosscutting information related to ES&H activities and the ES&H resources that support these activities. The report includes the following: (1) A summary status of fiscal year (FY) 1999 ES&H performance and ES&H <b>execution</b> commitments; (2) <b>Status</b> and plans of Hanford Site Office of Environmental Management (EM) cleanup activities; (3) Safety and health (S&H) risk management issues and compliance vulnerabilities of FY 2001 Target Case and Below Target Case funding of EM cleanup activities; (4) S&H resource planning and crosscutting information for FY 1999 to 2001; and (5) Description of indirect-funded S&H activities...|$|R
40|$|We explain {{dialogue}} management tech-niques for collaborative {{activities with}} hu-mans, involving multiple concurrent tasks. Conversational context for multiple con-current activities is represented using a “Dialogue Move Tree ” and an “Activity Tree ” which support multiple interleaved threads of dialogue about different activi-ties and their <b>execution</b> <b>status.</b> We also de-scribe the incremental message selection, aggregation, and generation method em-ployed in the system. ...|$|E
40|$|What actions {{must take}} place when {{subprogram}}s are called {{and when they}} terminate? – calling a subprogram has several associated actions: • calling subprogram’s environment must be saved – subprogram’s local variables, <b>execution</b> <b>status</b> return location • handle parameter passing to called subprogram • allocation and storage of local variables • transfer control to subprogram – subprogram termination then has the following actions: • return parameters that are out-mode • deallocation of local variables • return value from subprogram (if it is a function) • restore calling subprogram’s environment • transfer control to calling subprogram at the proper place • Generically, {{this is known as}} subprogram linkage and in most languages, this is performed through the run-time stack by using activation record instancesFORTRAN I- 77 • We first examine FORTRAN first since it is simplest – early FORTRAN had no recursion, all memory allocation was set up at compile time • so all variable and parameter names had specific memory locations known at compile time – subprogram call • save <b>execution</b> <b>status</b> of current program unit • carry out parameter passing – pass by copy required copying parameters – pass by reference required substituting an address for the value • pass return address to called subprogram • start execution of the subprogram – subprogram return • if pass-by-result parameters, copy current values to corresponding actual parameters and if subprogram is a function, value is moved to a place accessible by the caller • <b>execution</b> <b>status</b> of caller is restored • control is transferred back to caller...|$|E
30|$|As a {{cloud service}} {{operation}} is executed, certain data {{response would be}} returned from its CSP, informing the execution result, e.g. <b>execution</b> <b>status,</b> obtained service information, newly altered service entities, etc. This operation element is represented as SROutcome. Typically, {{for the majority of}} operation requests, SROutcome reveals the expected service entities to be returned from the respected CSP. For some simple SMR operations, SROutcome would only be the (expected) success response.|$|E
40|$|Interrupt {{handling}} in {{out-of-order execution}} processors requires complex hardware schemes {{to maintain the}} sequential state. The amount of hardware will be substantial in VLIW architectures {{due to the nature}} of issuing a very large number of instructions in each cycle. It is hard to implement precise interrupts in out-of-order execution machines, especially in VLIW processors. In this paper, we will apply the reorder buffer with future file and the history buffer methods to a VLIW platform, and present a novel scheme, called the Current-State Buffer, which employs modest hardware with compiler support. Unlike the other interrupt handling schemes, the Current-State Buffer does not keep history state, result buffering or bypass mechanisms. It is a fast interrupt handling scheme with a relatively small buffer that records the <b>execution</b> and exception <b>status</b> of operations. It is suitable for embedded processors that require a fast interrupt handling mechanism with modest hardware. I. INTRODU [...] ...|$|R
40|$|Abstract This study aims {{to design}} a Severity-Adjusted LOS(Length of Stay) Model in order to {{efficiently}} manage LOS of AMI(Acute Myocardial Infarction) patients. We designed a Severity-Adjusted LOS Model with using data-mining methods(multiple regression analysis, decision trees, and neural network) which covered 6, 074 AMI patients who showed the diagnosis of I 21 from 2004 - 2009 Korean National Hospital Discharge in-depth Injury Survey. A decision tree model was chosen for the final model that produced superior results. This study discovered that the <b>execution</b> of CABG, <b>status</b> at discharge(alive or dead), comorbidity index, etc. were major factors affecting a Sevirity-Adjustment of LOS of AMI patients. The difference between real LOS and adjusted LOS resulted from hospital location and bed size. The efficient management of LOS of AMI patients requires {{that we need to}} perform various activities after identifying differentiating factors. These factors can be specifie...|$|R
40|$|Abstract. The {{primary purpose}} of this {{research}} is to resolve the problem of ETL operation failure in execution of ETL (Extraction, Transformation and Loading) by the medical decision-making system due to data content, system factors and defective program design, thereby affect online daily operation of the application system and even customer complaint. This research first research and develop how to record in database, whether successful or not, the number of ETL file conversion program (including tool and self-wrote PL/SQL program) <b>execution</b> process, data <b>status,</b> and <b>execution</b> time; followed by designing control mechanism and write Script for voluminous table restoration for automatic execution by the system; afterwards followed by research and develop system automatic execution of restoration to stop only the affected application programs of the table and design a restoration mechanism. Lastly, through verification, this R&D result would correctly restore the data and table required by ETL procedure...|$|R
40|$|The {{physicists}} {{must be able}} {{to monitor}} the <b>execution</b> <b>status,</b> application and grid-level messages of their tasks that may run at any site within the CMS Virtual Organisation. The CMS Dashboard Task Monitoring project provides this information towards individual analysis users by collecting and exposing a user-centric set of information regarding submitted tasks including reason of failure, distribution by site and over time, consumed time and efficiency. The development was user-drive...|$|E
40|$|We {{introduce}} the c 1 ass of destructive run-time errors as those errors {{after which the}} <b>execution</b> <b>status</b> of the program can be so damaged that it is usually impossible to perform any kind of program-driven recovery. Programming errors causing this destructive events are analysed, with particular attention to those cases {{not related to the}} use of explicitly "dangerous" low level constructs. It is shown how the risk of these errors can easily underestimated...|$|E
40|$|We {{claim that}} a natural {{dialogue}} interface to a semiautonomous intelligent agent has important advantages, especially when operating in real-time complex dynamic environments involving multiple concurrent tasks and activities. We discuss some of the requirements of such a dialogue interface, and describe some {{of the features of}} a working system built at CSLI, focusing on the data-structures and techniques used to manage multiple interleaved threads of conversation about concurrent activities and their <b>execution</b> <b>status...</b>|$|E
5000|$|A PLC program {{generally}} loops i.e. executes repeatedly, {{as long as}} {{the controlled}} system is running. At the start of each <b>execution</b> loop, the <b>status</b> of all physical inputs are copied to an area of memory, sometimes called the [...] "I/O Image Table", which is accessible to the processor. The program then runs from its first instruction rung down to the last rung. It takes some time for the processor of the PLC to evaluate all the rungs and update the I/O image table with the status of outputs. Scan times of a few milliseconds may be encountered for small programs and fast processors, but for older processors and very large programs much longer scan times (on the order of 100 ms) may be encountered. Excessively long scan times may mean the response of the PLC to changing inputs or process conditions is too slow to be useful.|$|R
40|$|I {{have chosen}} the topic of this diploma thesis for its relevance. The number of ordered executions is still high and the ratio between rights and {{obligations}} {{on the part of}} individual subjects of the execution proceeding is unclear. The diploma thesis discusses subjects of the execution proceeding according to the Execution Procedure Act. It analyses who can be subject and participant of the <b>execution</b> proceeding, the <b>status</b> of the subjects, their role in execution proceeding and also their rights and duties. The aim of the diploma thesis is to compare the position of individual subjects, describe how the execution can affect lives not only of the obligated and entitled subjects but also lives of third parties and to elaborate on future legislation. The diploma thesis is, except for the introduction and the conclusion, divided into ten chapters. The beginning is devoted to general questions regarding subjects and participants of the proceeding and the proceeding itself. Then it discusses individual subjects of the execution proceeding. The last chapter focuses on the discussion about future legislation...|$|R
40|$|During project <b>execution,</b> the <b>status</b> of {{the project}} is {{periodically}} evaluated, using traditional methods or standard practices. However, these traditional methods or standard practices may not adequately identify certain issues, such as lack of sufficient identification of warning signs that predict potential project failure. Current methods may lack the ability to provide real time indications of emerging problems that impact project outcomes in a timely manner. To address this problem, the Construction Industry Institute (CII) formed a research team {{to develop a new}} tool that can forecast the potential risk of not meeting specific project outcomes based on assessing leading indicators. Thus, the leading indicators were identified and then the new tool was developed and validated. A screening process was conducted through industry surveys after identifying potential leading indicators. Each time, industry professionals were asked to evaluate the negative impact of leading indicators on project outcomes that were identified to measure the impact of leading indicators on project health. Through this process, forty-three leading indicators were acquired finally. Using descriptive statistics, the amount of negative impact of each leading indicator on project outcomes was identified after the analysis of the survey results. Based on these impacts, the tool development was initiated. The tool concept is that no indication of problems based on assessing leading indicators results in the tool output score high. To comply with this concept, specific weights were assigned to each leading indicator to reflect the impact on each project outcome. By this procedure, the Project Health Indicator (PHI) tool was developed. The validation process of the PHI tool was conducted using completed projects and finally negative correlation was observed between project outcomes and health scores generated by the PHI tool...|$|R
30|$|The {{execution}} module (CG-EM) executes a CPG for {{a specific}} patient, considering the patient’s data (retrieved from the Patient DB). The schema of the Patient DB mirrors the schema of the Clinical DB. Therefore, the inter-action with the Clinical DB during the acquisition phase {{makes it possible to}} automatically retrieve data from the Patient DB at execution time. CG-EM stores the <b>execution</b> <b>status</b> in another DB (CG Instances) and interacts with the user-physician via a graphical interface (CG-IM).|$|E
40|$|Abstract. With {{the rapid}} {{advancement}} of e-service technology, {{there is a}} need to manage business relationships among business entities such as service providers, service consumers, and internal departments. In this paper, we have proposed a novel approach to business relationship management using business commitments and associated business commitment hubs. Business commitments are commitments related to business issues such as service levels in service agreements, and terms and conditions in procurement contracts. The concept of business commitments has captured the essence of business relationships in e-services. Based on case studies, we have envisioned the need of establishing a business commitment hub to centrally manage the external relationships with trading partners and the internal relationships with internal departments. A language called Business Commitment Language (BCL) has been proposed to specify business commitments. These business commitments are used to monitor and control the <b>execution</b> <b>status</b> of e-services. Conceptually, the business commitment hub has two related subsystems: the active subsystem and the dashboard subsystem. The active subsystem responds to business events received from business entities. The dashboard subsystem visually displays the key data and the <b>execution</b> <b>status</b> of business commitments. ...|$|E
40|$|This paper {{describes}} a timeline-based, domain independent deliberative layer, based on E SA APSI technology, {{deployed in the}} context of the Goal Oriented Autonomous Controller (G OAC) project. In particular the paper {{describes a}} new controller composed by (1) a planning module that exploits the timeline-based approach provided by the APSI - TRF and is able to model and solve planning problems, (2) a module that dispatches planned timelines, supervises their <b>execution</b> <b>status</b> and entails continuous planning and re-planning. An example will illustrate both modules at work...|$|E
40|$|Background National {{oral health}} policy was conscripted by the Indian Dental Association (IDA) in 1986 and was {{accepted}} {{as an integral part}} of National Health Policy (NHP) by the Central Council of Health and Family Welfare in one of its conferences in the year 1995. Objectives of this paper were to find out the efforts made or going on towards its <b>execution,</b> its current <b>status</b> and recent oral health-related affairs or programs, if any. Methods Literature search was done using the institutional library, web-based search engines like ‘Google’ and ‘PubMed’ and also by cross referencing. It yielded 108 articles, of which 50 were excluded as they were not pertinent to the topic. Twenty-four were of global perspective rather than Indian and hence were not taken into account and finally 34 articles were considered for analyses. Documents related to central and state governments of India were also considered. Results All the articles considered for analysis were published within the past 10 years with gradual increase in number which depicts the researchers’ increasing focus towards oral health policy. Criticisms, suggestions and recommendations regarding national oral health programs, dental manpower issues, geriatric dentistry, public health dentistry, dental insurance, oral health inequality, and public-private partnerships have taken major occupancies in the articles. Proposals like “model for infant and child oral health promotion” and “oral health policy phase 1 for Karnataka” were among the initiatives towards national oral health policy. Conclusion The need for implementation of the drafted oral health policy with modification that suits the rapidly changing oral health system of this country is inevitable...|$|R
40|$|The {{correlator}} is {{the signal}} processing {{engine of the}} Very Long Baseline Array (VLBA). Radio signals are recorded on special wideband (128 Mb/s) digital recorders at the 10 telescopes, with sampling times controlled by hydrogen maser clocks. The magnetic tapes are shipped to the Array Operations Center in Socorro, New Mexico, where they are played back simultaneously into the correlator. Real-time software and firmware controls the playback drives to achieve synchronization, compute models of the wavefront delay, control the numerous modules of the correlator, and record FITS files of the fringe visibilities at the back-end of the correlator. In addition {{to the more than}} 3000 custom VLSI chips which handle the massive data flow of the signal processing, the correlator contains a total of more than 100 programmable computers, 8 -, 16 - and 32 -bit CPUs. Code is downloaded into front-end CPU's dependent on operating mode. Low-level code is assembly language, high-level code is C running under a RT OS. We use VxWorks on Motorola MVME 147 CPU's. Code development is on a complex of SPARC workstations connected to the RT CPU's by Ethernet. The overall management of the correlation process is dependent on a database management system. We use Ingres running on a Sparcstation- 2. We transfer logging information from the database of the VLBA Monitor and Control System to our database using Ingres/NET. Job scripts are computed and are transferred to the real-time computers using NFS, and correlation job <b>execution</b> logs and <b>status</b> flow back by the route. Operator status and control displays use windows on workstations, interfaced to the real-time processes by network protocols. The extensive network protocol support provided by VxWorks is invaluable. The VLBA Correlator's dependence on network protocols {{is an example of the}} radical transformation of the real-time world over the past five years. Real-time is becoming more like conventional computing. Paradoxically, 'conventional' computing is also adopting practices from the real-time world: semaphores, shared memory, light-weight threads, and concurrency. This appears to be a convergence of thinking...|$|R
40|$|This report {{describes}} a conceptual design for automation of the scheduling of airlift activities {{as part of}} the current upgrade of the MAC C 2 System. It defines the airlift scheduling problem in generic terms before reviewing the current procedures used by MAC; and then a new scheduling system aimed at handling a very busy and dynamic wartime scenario, is introduced. The new system proposes "Airlift Scheduling Workstations" where MAC Airlift Schedulers would be able to manipulate symbolic information on a computer display to create and quickly modify schedules for aircraft, crews, and stations. For certain sub-problems in generating schedules, automated decision support algorithms would be used interactively to speed the search for feasible and efficient solutions. Airlift Scheduling Workstations are proposed to exist at each "Scheduling Cell", a conceptual organizational unit which has been given sole and complete responsibility for developing the schedule of activities for a specific set of airlift resources-aircraft by tail number, aircrew by name, and stations by location. A Mission Scheduling Database is located at each cell to support the Airlift Scheduling Workstation, and requires information communicated by Airlift Task Planners, and, Airlift Operators at many other locations. These locations would have smaller workstations with local databases, and database management software to assist Task Planners and Operators in viewing current committed and planned schedule information of particular interest to them, and to allow them to send information to the Mission Scheduling Database. The Command and Control processes for Airlift have been structured into a three level hierarchy in this report: Task Planning, Mission Scheduling, and Schedule Execution. Task Planners deal with Airlift Users and Mission Schedulers, but not Airlift Operators. Task Planning has three sub-processes: Processing User Requests; Assigning Requirements and Resources; and Monitoring Task Status. Task planning does not create missions, schedule the missions, or route aircraft. Mission Schedulers deal with Task Planners and Airlift Operators, but not Airlift Users. Mission Scheduling combines several sub-processes to allow efficient schedules to be quickly generated at the ASW (Airlift Scheduling Workstation). These sub-processes are: Mission Generation, Schedule Map Generation (for each type of aircraft), Crew Mission Sequence Generation, Station Schedule Generation, Management of Schedule Status, and Monitoring Schedule <b>Execution</b> and Resource <b>Status.</b> It is important that all these processes be co-located and processed by the Airlift Scheduling Cell. Schedule Execution is performed by Airlift Operators assigned by the scheduling process. It has three sub-processes: Monitor Assigned Schedules, Report Resources Assigned to Schedule, Report Local Capability Status. The assignment of local resources such as aircraft by tail, and crew by name is actually another scheduling process, but has not been studied in this report. Airlift Operators do not deal with Task Planners, but may deal with Airlift Users to finalize details of the scheduled operations. This three level hierarchy is compatible with the current organizational structures of MAC Command and Control. However, it is clear that both the current organizational structures and procedures of MAC Command and Control for both tactical and strategic airlift will be significantly affected by the introduction of the automated scheduling systems envisioned here. These changes will occur in an evolutionary manner after the upgraded MAC C 2 system is introduced. April 1984 Prepared for the Electronic Systems Division, Air Force Systems Command, USAF, Hanscom Air Force Base, Bedford, M...|$|R
