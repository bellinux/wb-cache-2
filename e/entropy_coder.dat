151|7|Public
50|$|<b>Entropy</b> <b>coder</b> {{allows the}} {{encoding}} of {{a sequence of}} symbols using approximately Shannon entropy bits/symbol. For example ANS could be directly used to enumerate combinations: assign a different natural number to every sequence of symbols having fixed proportions in nearly optimal way.|$|E
50|$|For general {{constraints}} defining which {{symbols are}} allowed after a given symbol, the maximal information rate {{can be obtained}} by first finding the optimal transition probabilities using Maximal Entropy Random Walk, then use <b>entropy</b> <b>coder</b> (with switched encoder with decoder) to encode a message as a sequence of symbols fulfilling the found optimal transition probabilities.|$|E
50|$|To {{exploit the}} {{redundancy}} between neighboring block vectors, (e.g. {{for a single}} moving object covered by multiple blocks) {{it is common to}} encode only the difference between the current and previous motion vector in the bit-stream. The result of this differencing process is mathematically equivalent to a global motion compensation capable of panning.Further down the encoding pipeline, an <b>entropy</b> <b>coder</b> will take advantage of the resulting statistical distribution of the motion vectors around the zero vector to reduce the output size.|$|E
40|$|We {{introduce}} a novel, adaptive image representation using spatially varying wavelet packets (WPs), Our adaptive representation uses the fast double-tree algorithm introduced previously (Herley et al., 1993) to optimize an operational rate-distortion (R-D) cost function, as {{is appropriate for}} the lossy image compression framework. This involves jointly determining which filter bank tree (WP frequency decomposition) to use, and when to change the filter bank tree (spatial segmentation). For optimality, the spatial and frequency segmentations must be done jointly, not sequentially. Due to computational complexity constraints, we consider quadtree spatial segmentations and binary WP frequency decompositions (corresponding to two-channel filter banks) for application to image coding. We present results verifying the usefulness and versatility of this adaptive representation for image coding using both a first-order <b>entropy</b> rate-measure-based <b>coder</b> {{as well as a}} powerful space-frequency quantization-based (SPQ-based) wavelet coder introduced by Xiong et al. (1993...|$|R
40|$|Perhaps, Sarnoff Corporation's zerotree <b>entropy</b> (ZTE) <b>coder</b> is {{the most}} {{successful}} wavelet video coder published so far which exploits the statistical properties of wavelet-transformed images by utilizing novel data representation and organization strategies. In this paper, a high performance hybrid video coding algorithm termed video signi cance-linked connected component analysis (VSLCCA) is developed. It is quite encouraging that, at least empirically convinced, the wavelet transform with aids of those recently published innovative data representation and organization methods can be an invaluable asset in video coding if motion-compensated error frames are coherent. In VSLCCA, time domain motion estimation followed by exhaustive overlapped block motion compensation is utilized to ensure coherency, and then wavelet transform is applied to each error frame with significant wavelet coefcients being encoded by highly efficient SLCCA technique. Experimental results on standard MPEG- 4 test sequences show that VSLCCA is superior to H. 263 and ZTE by 0. 48 dB and 0. 77 dB on average, respectively...|$|R
40|$|This paper proposes {{particle}} swarm optimization {{method to}} design M channel near perfect reconstructionpseudo QMF banks used in transforming stage of image coder. The filter bank is designed to have highest <b>entropy</b> based <b>coder.</b> To achieve high energy compaction and least distortion, design problem is formulated as {{a combination of the}} coding gain, low dc leakage conditions and stopband attenuation. For distortion free signal representation perfect reconstruction and good visual quality measures are imposed as constraints. The design problem is solved using ({{particle swarm optimization}}) PSO technique for minimizing filter tap weights. The technique find out solution by searching feasible solutions that achieve the best solution for the objectives criteria mentioned above. The performance of this optimization technique in filter bank design for image compression is evaluated in terms of both objective quality via coding gain, PSNR measures and subjective visual quality measure using both JPEG baseline image coder and an Embedded Zerotree Wavelet (EZW) coder. For comparison same test images for approximately same conditions and characteristics are used to measure compression ratio and peak signal to noise ratio (PSNR) for lower bit rates...|$|R
5000|$|Like other {{lossless}} codecs for continuous-tone images, FELICS operates by decorrelating {{the image}} and encoding it with an <b>entropy</b> <b>coder.</b> The decorrelation is the context [...] where and [...] where [...] are the pixel's two nearest neighbors (causal, already coded and known at the decoder) used for providing the context to code thepresent pixel [...]Except {{at the top and}} left edges, these are the pixel above and the pixel to the left.For example, the neighbors of pixel X in the diagram are A and B, but if X were at the left side, its neighbors would be B and D.|$|E
5000|$|The {{basic idea}} is to encode {{information}} into a single natural number [...]In the standard binary number system, we can add a bit [...] of information to by appending [...] {{at the end of}} [...] which gives us [...]For an <b>entropy</b> <b>coder,</b> this is optimal if [...]ANS generalizes this process for arbitrary sets of symbols [...] with an accompanying probability distribution [...]In ANS, if [...] is the result of appending the information from [...] to , then [...] Equivalently, , where [...] is the amount of bits of information stored in number [...] and [...] is the amount of bits contained in symbol [...]|$|E
50|$|Calculating the bit cost is {{made more}} {{difficult}} by the entropy encoders in modern video codecs, requiring the rate-distortion optimization algorithm to pass each block of video {{to be tested}} to the <b>entropy</b> <b>coder</b> to measure its actual bit cost. In MPEG codecs, the full process consists of a discrete cosine transform, followed by quantization and entropy encoding. Because of this, rate-distortion optimization is much slower than most other block-matching metrics, such as the simple sum of absolute differences (SAD) and sum of absolute transformed differences (SATD). As such it is usually used only for the final steps of the motion estimation process, such as deciding between different partition types in H.264/AVC.|$|E
40|$|Abstract | In this paper, {{we present}} a new {{compression}} scheme based on applying the 3 -D Fast Wavelet Transform, to code medical video. This video has special features such as its representation in gray scale, {{the small amount of}} interframe variations, and the quality re-quirements of the reconstructed images. These characteristics as well as the social impact of de-sired applications deserve the design and imple-mentation of coding schemes especially oriented to exploit its features. We analyze dierent parameters of the codication process, such as the utilization of dierent wavelets functions, the number of steps this function is applied, the way the thresholds are chosen, and the selected methods in the quantization and <b>entropy</b> encoder. Our <b>coder</b> achieves a good trade-o between com-pression ratio and quality of the reconstructed video. These results are better than MPEG- 2, without the complexity of motion compensation...|$|R
40|$|Compression of {{magnetic}} resonance images (MRI) {{has proved to}} be more difficult than other medical imaging modalities, and attempts at utilizing inter-slice dependencies for more efficient coding have so far met with little success. On the other hand, the increasing amounts of MRI data generated every day in hospitals makes this particular data compression problem very important. In this paper, we present an adaptive, <b>entropy</b> constrained, transform <b>coder</b> that also employs a new inter-slice estimator. Previous attempts at inter-slice coding of MRI have all used a piece-wise uniform, discontinuous translational model. We propose a continuous piece-wise affine model for inter-slice dependencies, whose implementation is performed through a triangle-based matching (TBM) algorithm. The residue frames from the inter-slice estimator are coded through an entropy constrained quantizer, applied to the block discrete cosine transformed (DCT) residue frame. Realizing that the statistics of the slice [...] ...|$|R
40|$|This paper {{describes}} a fast, low-complexity, <b>entropy</b> efficient video <b>coder</b> for wavelet pyramids. This coder approaches the entropy-limited coding rate of video wavelet pyramids, is fast in both {{hardware and software}} implementations, and has low complexity (no multiplies) for use in ASICs. It consists of a modified Z-coder used to code the zero/non-zero significance function and Huffman coding for the non-zero coefficients themselves. Adaptation is not required. There is a strong speed-memory trade-off for the Huffman tables allowing the coder to be customized {{to a variety of}} platform parameters. 1. Introduction An image transform codec consists of three steps: 1) a reversible transform, often linear, of the pixels for the purpose of decorrelation, 2) quantization of the transform values, and 3) entropy coding of the quantized transform coefficients. This paper presents an entropy codec which is fast, efficient in silicon area, coding-wise efficient, and practical when the transform i [...] ...|$|R
40|$|Abstract. Entropy coding {{is one of}} {{the most}} {{important}} techniques in video codec. Two main criteria to assess an <b>entropy</b> <b>coder</b> are coding efficiency and friendly realization characteristic. In the recent development of the H. 264 /AVC standard, a sophisticated <b>entropy</b> <b>coder,</b> named Context-based Adaptive Variable Length Coding (CAVLC), has been invented, which supplies higher coding efficiency than other VLC-based entropy coders in previous video coding standards. But due to its algorithm’s inherit traits CAVLC must be executed in a sequential manner, which results in relative low throughput rate. In this paper, a new well-designed context-based VLC <b>entropy</b> <b>coder</b> for transform coefficients is presented. It exploits coefficient block’s inner context information to obtain high coding efficiency, and at the same time context models for successive coding elements are designed to be dependent-free so that the coder is more apt to be parallel. Experimental results show that the proposed <b>entropy</b> <b>coder</b> can exhibit the same coding efficiency as CAVLC. Therefore, a new high performance <b>entropy</b> <b>coder</b> with characteristics of parallel orientation and high coding efficiency is supplied. ...|$|E
3000|$|... of the {{operational}} bit-rates {{except in the}} region, where the packet loss rates approach and exceed the critical point, where the system becomes unstable. This excess 1 bit accounts for the theoretical loss of an <b>entropy</b> <b>coder.</b> While we have not applied actual entropy coding, {{it is well known}} that the loss of the <b>entropy</b> <b>coder</b> diminishes at moderate to large bit rates.|$|E
30|$|Format compliance: the {{proposed}} scheme isfully compliant to any compression algorithm that uses VLC or FLC <b>entropy</b> <b>coder.</b>|$|E
40|$|In recent years, a {{tremendous}} success in wavelet image coding has been achieved. It is mainly attributed to innovative strategies for data organization and representation of wavelet-transformed images. However, {{there have been}} only a few successful attempts in wavelet video coding. The most successful one is perhaps Sarno Corporation's zerotree <b>entropy</b> (ZTE) video <b>coder.</b> In the paper, a novel hybrid wavelet video coding algorithm termed video significance-linked connected component analysis (VSLCCA) is developed for very low bit rate applications. It has also been empirically evidenced that wavelet transform combined with those innovative data organization and representation strategies can be an invaluable asset in very low bit rate video coding as long as motion-compensated error frames are ensured to be blocking-effect-free or coherent. In the proposed VSLCCA codec, first, fine-tuned motion estimation based on H. 263 Recommendation is developed to reduce temporal redundancy and exhaustive overlapped block motion compensation is utilized to ensure the coherency in motion-compensated error frames, second, wavelet transform is applied to each coherent motion-compensated error frame to attain global energy compaction, third, significant fields of wavelet-transformed error frame are organized an...|$|R
40|$|Vector {{quantization}} (VQ) {{has been}} used extensively {{in the past for}} image compression. The quantized image can be further compressed via a standard <b>entropy</b> <b>coder</b> (such as the arithmetic coder). In this paper, we present a simple equivalent to VQ, where unsupervised neural nets (NN) are used to find the appropriate codevectors. Furthermore, by imposing additional constraints to the VQ-NN system, we match the <b>entropy</b> <b>coder</b> characteristics and improve the overall image compression by an additional 10 %...|$|E
3000|$|... 4 The excess 1 bit {{is due to}} the {{conservative}} estimate of the loss of the <b>entropy</b> <b>coder,</b> which is characterized by 1 bit.|$|E
40|$|In this paper, {{we present}} a new <b>entropy</b> <b>coder,</b> Contextbased Bit Plane Golomb Coder (CB-BPGC) for {{scalable}} image coding, which achieves better coding performance with lower complexity compared to the state-of-the-art JPEG 2000 <b>entropy</b> <b>coder</b> EBCOT. Because of the direct output lazy bit planes, applying the partial decoding on the corrupted bit planes, and the better compression ratio which may lead to corruption to the less important codestream, CB-BPGC appears more resilient to errors when simulated on the wireless channel based on Rayleigh fading model. 1...|$|E
40|$|In {{this paper}} we propose a {{geometrical}} <b>entropy</b> <b>coder</b> to compress images with geometry. Our proposed scheme works on quantized coefficients of the discrete wavelet transform (DWT). The geometry inherent in those coefficients is exploited by means of directional, predictive coding. We accomplish directional prediction with the aid of the coefficients of the undecimated wavelet transform (UDWT), which are estimated from the available DWT ones both at the encoder and decoder. Following directional prediction, a simple <b>entropy</b> <b>coder</b> takes care of the remaining redundancy. The resulting codec attains competitive performance...|$|E
3000|$|All the SI {{generation}} {{methods were}} implemented on the KTA software implementation of H. 264 /AVC [32]. In our entire tests, we use fast motion estimation, the CAVLC <b>entropy</b> <b>coder,</b> no rate-distortion optimization, 16 [...]...|$|E
40|$|This paper {{presents}} an algorithm that jointly optimizes a lattice vector quantizer (LVQ) and an <b>entropy</b> <b>coder</b> in a subband coding at all ranges of bit rate. Estimation formulas for both entropy and distortion of lattice quantized subband images are derived. From these estimates, we then develop dynamic algorithm optimizing the LVQ and <b>entropy</b> <b>coder</b> {{together for a}} given entropy rate. Compared to previously reported min [...] max approaches, or approaches using asymptotic distortion bounds, the approach reported here quickly designs a highly accurate optimal entropy-constrained LVQ. The corresponding waveletbased image coder also has better coding performance compared to other subband coders that use entropy-constrained LVQ, especially at low bit rates...|$|E
40|$|The Block Sorting {{process of}} Burrows and Wheeler {{can be applied}} to any {{sequence}} in which symbols are (or might be) conditioned upon each other. In particular, it is possible to parse text into a stream of words, and then employ block sorting to identify and so exploit any conditioning relationships between words. In this paper we build upon the previous work of two of the authors, describing several further recency rank transformations, and considering also the role of the <b>entropy</b> <b>coder.</b> By combining the best of the new recency transformations with an <b>entropy</b> <b>coder</b> that conditions ranks upon gross characteristics of previous ones, we are able to obtain improved compression on typical text files...|$|E
40|$|Compression {{ratio and}} {{computational}} complexity {{are two major}} factors for a successful image coder. By exploring the Laplacian distribution of the wavelet coefficients, a new bit plane <b>entropy</b> <b>coder</b> is proposed in this paper. Compared with the state-of-the-art JPEG 2000 <b>entropy</b> <b>coder</b> (EBCOT), the proposed coder achieves a 0. 75 % better lossless performance for 5 level 5 / 3 wavelet decomposition at block size 64 × 64 and 2. 56 % at block size 16 × 16. Experimental results also show PSNR improvements of about 0. 13 dB at 1 bpp and 0. 25 dB at 2 bpp on average for lossy compression. However, the gain in coding performance {{is not based on}} increasing computational complexity but instead a reduction by using a static arithmetic coder which avoids complicated adaptive procedure. 1...|$|E
40|$|The Minimum-Entropy Clustering (MEC) {{algorithm}} {{proposed in}} this paper provides an optimal method for addressing the non-stationarity of a source with respect to entropy coding. This algorithm clusters a set of vectors (where each vector consists of a xed number of contiguous samples from a discrete source) using a minimum entropy criterion. In {{a manner similar to}} Classi ed Vector Quantization (CVQ), a given vector is rst classi ed into the class which leads to the lowest entropy and then its samples are coded by the <b>entropy</b> <b>coder</b> designed for that particular class. In this paper the MEC algorithm is used in the design of a lossless, predictive image coder. The MEC-based coder is found to sigi cantly outperform the single <b>entropy</b> <b>coder</b> {{as well as the other}} popular lossless coders reported in the literature. ...|$|E
40|$|A new <b>entropy</b> <b>coder,</b> the RI-coder. {{based on}} the {{classical}} TRL code, is proposed. Unlike the ATRL coder, which is blockwise adaptive, the M-coder is a bit by bit adaptive coder. To accomplish this, the M-coder requires a probability estimation scheme for the symbols to be coded. In this way, the M-coder {{can be compared to}} the Q-coder, with an easier implementation...|$|E
40|$|This paper {{presents}} a CMOS image sensor with focal-plane compression. The design has a column-level architecture {{and it is}} based on predictive coding techniques for image decorrelation. The prediction operations are performed in the analog domain to avoid quantization noise and to decrease the area complexity of the circuit, The prediction residuals are quantized and encoded by a joint quantizer/coder circuit. To save area resources, the joint quantizerlcoder circuit exploits common circuitry between a single-slope analog-to-digital converter (ADC) and a Golomb-Rice <b>entropy</b> <b>coder.</b> This combination of ADC and encoder allows the integration of the <b>entropy</b> <b>coder</b> at the column level. A prototype chip was fabricated in a 0. 35 pm CMOS process. The output of the chip is a compressed bit stream. The test chip occupies a silicon area of 2. 60 mm x 5. 96 mm which includes an 80 X 44 APS array. Tests of the fabricated chip demonstrate the validity of the design...|$|E
40|$|This report {{introduces}} a new lossless asymmetric single instruction multiple data codec designed for extremely efficient decompression of large satellite images. A throughput {{in excess of}} 3 GB/s allows decompression to proceed in parallel with asynchronous transfers from fast block devices such as disk arrays. This is {{made possible by a}} simple and fast single instruction multiple data <b>entropy</b> <b>coder</b> that removes leading null bits. Our main contribution is a new approach for vectorized prediction and encoding. Unlike previous approaches that treat the <b>entropy</b> <b>coder</b> as a black box, we account for its properties {{in the design of the}} predictor. The resulting compressed stream is 1. 2 to 1. 5 times as large as JPEG- 2000, but can be decompressed 100 times as quickly - even faster than copying uncompressed data in memory. Applications include streaming decompression for out of core visualization. To the best of our knowledge, this is the first entirely vectorized algorithm for lossless compression...|$|E
3000|$|Whether {{or not the}} {{algorithm}} and parameters used are generalizable to those {{that will be used}} in practice (e.g. much of the literature evaluates novel or proprietary rather than standard algorithms, and even if of the same family (e.g. wavelet) results in terms of compression ratio or bit rate may not be directly comparable with algorithms available for operational use (e.g. different basis function, different <b>entropy</b> <b>coder,</b> etc.) [...]...|$|E
3000|$|From the {{analysis}} of the MJLS in Section 3, {{it is not easy to}} assess the computational burden required, when using the proposed system in practice. In this section, we provide a brief overview of the complexity of the encoder and decoder. The encoder includes the controller, quantizer, <b>entropy</b> <b>coder,</b> and channel (FEC) coder. The decoder includes channel decoder, entropy decoder, buffering, and selection of the control values: [...]...|$|E
30|$|The rate {{term in the}} R-D {{functional}} {{represents an}} estimate {{for the number of}} coded bits produced by the <b>entropy</b> <b>coder.</b> Unlike H. 264 /AVC, the context adaptive variable length coding (CAVLC) is not supported in HEVC. It only defines context adaptive binary arithmetic coding (CABAC), which involves three elementary steps: binarization, context modeling, and binary arithmetic coding (Marpe et al. 2003), as the <b>entropy</b> <b>coder.</b> The binarization step maps the non-binary valued syntax elements (SEs), which will be represented in the bitstream and describe how the video sequence can be reconstructed at the decoder, to binary symbols. This step will prolong the encoding pipelines, for it typically maps one element to a bin string. The modeling stage assigns a model probability distribution which was updated using the statistics of the already coded neighboring symbols to binary symbols. In arithmetic coding stage, the actual coding engine is driven by the probability model. Based on recursive interval division and selection, the coding procedure generates a sequence of bits for representing the SEs.|$|E
40|$|In {{this paper}} we motivate {{the need for}} {{lookahead}} search in a context based <b>entropy</b> <b>coder.</b> An efficient algorithm based on modeling of the context coder as a finite state machine is presented. A key contribution {{of this paper is}} the use of the per survivor processing principle (PSP) to enable a lookahead search in scenarios where adaptive entropy coding is used. Our results show that lookahead searches based on PSP result in performance improvements over traditional schemes. 1...|$|E
40|$|This paper {{presents}} {{a method to}} find the operational rate-distortion optimal solution for an overcomplete signal decomposition. The idea of using overcomplete dictionaries, or frames, {{is to get a}} sparse representation of the signal. Traditionally, suboptimal algorithms, such as matching pursuit (MP), are used for this purpose. When using frames in a lossy compression scheme, the major issue is to find the best possible rate-distortion (RD) tradeoff. Given the frame and the variable length code (VLC) table embedded in the <b>entropy</b> <b>coder,</b> the {{solution to the problem of}} establishing the best RD tradeoff is highly complex. The proposed approach reduces this complexity significantly by structuring the solution approach such that the dependent quantizer allocation problem reduces to an independent one. In addition, the use of a solution tree further reduces the complexity. It is important to note that this large reduction in complexity is achieved without sacrificing optimality. The optimal rate-distortion solution depends on the selection of the frame and the VLC table embedded in the <b>entropy</b> <b>coder.</b> Thus, frame design and VLC optimization is part of this work. We experimentally demonstrate that the new approach outperforms rate-distortion optimized (RDO) matching pursuit, previously proposed by Gharavi-Alkhansari...|$|E
40|$|The 2 D-wavelet {{transform}} {{of images}} is, among others, used for image compression (either for still images or for video sequences). A wavelet transformed image has the special property {{that the image}} content is split into a low-pass component and a high-pass component. The highpass component usually consists mostly of small coefficients and can be compressed efficiently using an <b>entropy</b> <b>coder.</b> A Quadtree coder [9], [6], [5] is such an <b>entropy</b> <b>coder</b> which achieves good compression rates and in addition {{has a number of}} scalability features. In this article we present the results of a hardware implementation of the Quadtree algorithm. We find that the progressive coding and the reordering of the video stream during Quadtree coding requires a tailored approach to its hardware implementation in order to achieve an efficient memory use and a high compression speed. We present a number of techniques to deal with these challenges and give simulation results of the expected performance of the Quadtree coder. Compared to the original algorithm in software, processed on a PentiumIV 2, 4 Ghz, we reach a speedup by a factor of 9...|$|E
40|$|Abstract. In this paper, a block-edge based Single-Pass Perceptual Embedded Zero-tree Coding (SPPEZC) {{method is}} {{proposed}} and implemented on the DSP-based platform. SPPEZC combines two novel compression concepts which are Block-Edge Detection (BED) and Low-Complexity and Low-Memory <b>Entropy</b> <b>Coder</b> (LLEC) for the coding efficiency and quality. Besides, the proposed SPPEZC is implemented as fixed-point version and optimized on the DSP-based platform based {{on both the}} presented platform-independent and platform-dependent optimization technologies. The performance including compression quality and efficiency is validated by experimental results...|$|E
40|$|In this paper, a new fast error {{resilient}} <b>entropy</b> <b>coder</b> (FEREC) for robust {{image transmission}} over wireless fading channels is proposed. A slow, frequency non-selective Rayleigh fading channel model is used. The proposed FEREC algorithm is observed {{to be almost}} {{twice as fast as}} EREC [3] in encoding the data and hence the error resilience capability is also significantly better. Upto 2 dB improvement in the peak signal to noise ratio of the received image is achieved when compared to EREC...|$|E
