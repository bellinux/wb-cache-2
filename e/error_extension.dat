10|128|Public
3000|$|When {{the last}} state {{transitions}} of PU are exactly estimated, the simplified algorithm {{is equal to}} MAP algorithm, and it has optimal detection performance. However, from (16), spectrum detection errors in state transition moment will cause <b>error</b> <b>extension</b> in the upcoming periods. To avoid this unfavorable situation, following measures are suggested when PU has entered one state and lasted beyond its mean duration. ([...] [...]...|$|E
40|$|This ISR {{describes}} {{a number of}} improvements to calnica, the STSDAS task that performs routine instrumental calibration of NICMOS raw images. We report two major {{and a number of}} minor updates to the code. The major updates consist of changes in the way the count rate is calculated from up-the-ramp fitting and changes in the cosmic rays rejection algorithm. All updates have been extensively tested and analysis shows improvement in the S/N of output images by 6 - 15 % compared to the previous calnica version. A further improvement is that the <b>error</b> <b>extension</b> in the calibrated images now represents the true errors in a more consistent way compared to the old implementation of the software...|$|E
40|$|Abstract—Bit {{dependency}} {{of image}} compression algorithms introduce <b>error</b> <b>extension</b> effects in transmission of compressed images and considerably reduce the received image quality. Channel coding provides protection against these errors {{but at the}} cost of increased bandwidth. Asymmetric modulation methods like QAM and QPSK provide alternative means of equal error protection to all the coded bits without increasing the bandwidth. This paper examines the suitability of Hierarchical QAM (HQAM) where non-uniform signal constellation is used to provide different degrees of protection to the significant and nonsignificant bits in the compressed image data at lower channel Signal to Noise Ratio (SNR) and compares it with that of QAM. The performance of HQAM is evaluated using gray test image for different values of the modulation parameter...|$|E
5000|$|MS-WEBDAVE: Web Distributed Authoring and Versioning <b>Error</b> <b>Extensions</b> Protocol Specification. This SharePoint Front-End Protocol {{describes}} extended {{error codes}} and extended error handling mechanism specified in MS-WDV to enable compliant servers to report error condition details on a server response.|$|R
40|$|We study {{nonlinear}} {{systems with}} observation errors. The main problem {{addressed in this}} paper is the design of feedbacks for globally asymptotically controllable (GAC) control affine systems that render the closed loop systems input to state stable with respect to actuator <b>errors.</b> <b>Extensions</b> for fully nonlinear GAC systems with actuator errors are also discussed. ...|$|R
40|$|International audienceThe {{main problem}} {{addressed}} in this paper is the design of feedbacks for globally asymptotically controllable (GAC) control aﬃne systems that render the closed-loop systems input-to-state stable (ISS) with respect to actuator <b>errors.</b> <b>Extensions</b> for fully nonlinear GAC systems with actuator errors are also discussed. Our controllers have the property that they tolerate small observation noise as well...|$|R
40|$|Due to the {{incorrect}} filling {{order and}} the fixed size of patch, the traditional examplar-based image inpainting algorithm tends to cause the image structure fracture, texture <b>error</b> <b>extension</b> and so on. So in this paper, it proposes an improved Criminisi algorithm with adaptive adjustment with gradient variation to color image inpainting algorithm. Firstly, to overcome the discontinuity of the edge structure caused by the incorrect filling order, using curvature of isophotes to constraint the filling order. Secondly, in order to solve the lack of the step effect in rich texture region, it adaptively adjusts the sample patch size according to the variation of local gradient. Finally, the local search method is used {{to find the best}} matching patch. The experimental results show that the proposed algorithm’s PSNR increased by 1 - 3 dB and obtain better results in terms of different types of images...|$|E
40|$|Abstract−The {{optimized}} watermarking transform {{come into}} view to commence extremely low distortion which outperforms the fundamental Difference expansion transform of Tian {{and the traditional}} median edge detector and gradient-adjusted Predictor <b>error</b> <b>extension</b> transforms. The median edge detector predictor is used in JPEG-LS, as a substitute of the simple difference between adjacent pixels. The gradient-adjusted Predictor used in context-based adaptive lossless image coding algorithm appeared to provide better results than the median edge detector predictor in watermarking schemes. Uncomplicated linear predictors are considered as an alternative for looking an elevated-complication predictor and optimize the data embedding procedure. Extremely low distortion renovate for prediction-error expansion reversible watermarking has been predictable. The Proposed technique outperforms a representative previous art of Difference expansion change, and it is shown that Tian’s transform is equal to a prediction-error expansion of a simple linear predictor with enhanced embedding. Sensible reversible watermarking algorithms based on the proposed transform have been considered. The suitable application areas comprise low bit-rate image annotation for captioning and cataloging...|$|E
40|$|Following {{a review}} of the key {{determinants}} of successful rowing, a wireless body sensor network was developed to monitor boat and body segment acceleration and surface electromyography in major muscles recruited during the rowing stroke cycle. Its design was optimised to yield maximum information about the rowing stroke cycle from fewest sensors and minimise the power consumption of the nodes. The system was validated against the Qualisys motion capture and high-speed camera system with most Pearson correlation coefficients in excess of r = 0. 8. On-land ergometer experimentation allowed muscle recruitment over the stroke cycle to be studied, with data from multiple experiments combined using correlation of the acceleration signatures of back and thigh nodes (r = 0. 95). It was demonstrated {{that it was possible to}} identify one of the common rowing errors of ‘shooting-the-slide’ from the data collected, and that a marked decrease in correlation of good-to-bad technique over the drive phase of the stroke (0. 95 reducing to 0. 34 in the experiment undertaken) could be used to indicate the presence of this <b>error.</b> <b>Extension</b> of the wireless body sensor network to encompass boat and two oarsmen was demonstrated, allowing correlation of their rowing signatures to be studied, indicating their cohesion as a crew...|$|E
40|$|Brief Overview of Partial Differential Equations The {{parabolic}} equations The wave equations The elliptic equations Differential equations in broader areasA quick {{review of}} numerical methods for PDEsFinite Difference Methods for Parabolic Equations Introduction Theoretical issues: stability, consistence, and convergence 1 -D parabolic equations 2 -D and 3 -D parabolic equationsNumerical examples with MATLAB codesFinite Difference Methods for Hyperbolic Equations IntroductionSome basic difference schemes Dissipation and dispersion <b>errors</b> <b>Extensions</b> to conservation lawsThe second-order hyperbolic PD...|$|R
40|$|This paper {{introduces}} two new nonparametric estimators for {{probability density}} functions which have {{support on the}} non-negative half line. These kernel estimators are based on some inverse Gaussian and reciprocal inverse Gaussian probability density functions used as kernels. We show that they share the same properties as those of gamma kernel estimators : they are free of boundary bias, always non-negative, and achieve the optimal rate of convergence for the mean integrated squared <b>error.</b> <b>Extensions</b> to regression curve estimation and hazard rate estimation under random censoring are briefly discussed. Monte Carlo results concerning finite sample properties are reported for different distributions...|$|R
40|$|This article {{provides}} {{an introduction to}} the major types of nonparametric regression techniques, including kernel, spline and orthogonal projection methods. Practical aspects of the methods and their applicability in environmental statistics are emphasized through examples and discussion. Topics covered include bandwidth selection, nonparametric regression in multiple dimensions, and methods for handling data exhibiting correlated <b>errors.</b> <b>Extensions</b> to generalized regression models and semiparametric regression are also discussed. 1 Introduction Nonparametric regression is a rapidly growing and exciting branch of statistics, both because of recent theoretical developments and because of more widespread use of fast and inexpensive computers. Many methods are currently available, including kernel-based methods, regression splines, smoothing splines and wavelet and Fourier series expansions. In this article, we introduce the main types of smoothing methods and discuss the usefulne [...] ...|$|R
40|$|AbstractIn this paper, {{we address}} a {{fundamental}} problem {{related to the}} induction of Boolean logic: Given a set of data, represented {{as a set of}} binary “truen-vectors” (or “positive examples”) and a set of “falsen-vectors” (or “negative examples”), we establish a Boolean function (or an extension) f, so thatfis true (resp., false) in every given true (resp., false) vector. We shall further require that such an extension belongs to a certain specified class of functions, e. g., class of positive functions, class of Horn functions, and so on. The class of functions represents our a priori knowledge or hypothesis about the extensionf, which may be obtained from experience or from the analysis of mechanisms {{that may or may not}} cause the phenomena under consideration. The real-world data may contain errors, e. g., measurement and classification errors might come in when obtaining data, or there may be some other influential factors not represented as variables in the vectors. In such situations, we have to give up the goal of establishing an extension that is perfectly consistent with the given data, and we are satisfied with an extensionfhaving the minimum number of misclassifications. Both problems, i. e., the problem of finding an extension within a specified class of Boolean functions and the problem of finding a minimum <b>error</b> <b>extension</b> in that class, will be extensively studied in this paper. For certain classes we shall provide polynomial algorithms, and for other cases we prove their NP-hardness...|$|E
40|$|Conventional {{compressed}} sensing theory assumes signals have sparse representations in a known, finite dictionary. Nevertheless, in many {{practical applications}} such as direction-of-arrival (DOA) estimation and line spectral estimation, the sparsifying dictionary is usually characterized {{by a set of}} unknown parameters in a continuous domain. To apply the conventional compressed sensing technique to such applications, the continuous parameter space has to be discretized to a finite set of grid points, based on which a "presumed dictionary" is constructed for sparse signal recovery. Discretization, however, inevitably incurs errors since the true parameters do not necessarily lie on the discretized grid. This error, also referred to as grid mismatch, may lead to deteriorated recovery performance or even recovery failure. To address this issue, in this paper, we propose a generalized iterative reweighted L 2 method which jointly estimates the sparse signals and the unknown parameters associated with the true dictionary. The proposed algorithm is developed by iteratively decreasing a surrogate function majorizing a given objective function, resulting in a gradual and interweaved iterative process to refine the unknown parameters and the sparse signal. A simple yet effective scheme is developed for adaptively updating the regularization parameter that controls the tradeoff between the sparsity of the solution and the data fitting <b>error.</b> <b>Extension</b> of the proposed algorithm to the multiple measurement vector scenario is also considered. Numerical results show that the proposed algorithm achieves a super-resolution accuracy and presents superiority over other existing methods. Comment: arXiv admin note: text overlap with arXiv: 1401. 431...|$|E
40|$|The {{dissertation}} {{addresses the}} formulation of Large-Eddy Simulations (LES) with direct consideration of a base finite difference scheme, and {{with the intent of}} reducing numerical error influences on the closure model and ultimately the solution. As such, spectral characteristics of the explicitly-defined LES filter are considered with respect to the discretization method’s spectral accuracy (i. e., resolvability). Analysis and development of discrete filtering stencils is undertaken, placing emphasis on the ability to specify desired scale-separation (e. g., cut-off wavenumber and scale-discriminant attenuation) relative to the computational grid. Assessment of the LES procedure is preceded by the establishment of a suitable base scheme, comprised of high-order discretizations and the addition of stabilization presented in a filter-based artificial dissipation form. Subsequent robustness and preservation of the overall solution accuracy is achieved by tuning the dissipation according to the dispersion characteristics of the underlying numerical method and seeking to deliberately remove the effects of discretization <b>error.</b> <b>Extension</b> to LES is then established by properly defining the explicit filter in relation to these numerical characteristics. Effectiveness of the procedure is evaluated by means of a priori and a posteriori inspection of turbulence calculations for the Burgers and Navier-Stokes equations, wherein the impacts of discretization and filter cut-off are assessed in light of scale-similarity and “perfect” modeling (i. e., a DNS-based closure). Results demonstrate the benefits of employing mutually tuned high-order discretizations and filters in the limit of the idealized “perfect” model, yet highlight the likely possibility of modeling error overshadowing such gains when actual closures, such as scale-similarity models, are used. In an attempt to enhance the scale-similarity models considered herein, the filter-based artificial dissipation is employed in order to enforce the prescribed LES field, and is shown to reduce overall model error. Meanwhile, its use is shown to be beneficial even in the presence of “perfect” modeling, wherein the dissipation can be tuned to specifically target discretization errors...|$|E
2500|$|When {{comparing}} two means, concluding {{the means}} were different when in reality {{they were not}} different would be a Type I error; concluding the means were not different when in reality they were different would be a Type II <b>error.</b> Various <b>extensions</b> have been suggested as [...] "Type III errors", though none have wide use.|$|R
40|$|Multi-resolution image {{analysis}} utilizes subsampled image representations for {{applications such as}} image coding, hierarchical image segmentation and fast image smoothing. An anti-aliasing filter {{may be used to}} insure that the sampled signals adequately represent the frequency components/features of the higher resolution signal. Sampling theories associated with linear anti-aliasing filtering are well-defined and conditions for nonlinear filters are emerging. This paper analyzes sampling conditions associated with anisotropic diffusion, an adaptive nonlinear filter implemented by partial differential equations (PDEs). Sampling criteria will be defined within the context of edge causality, and conditions will be prescribed that guarantee removal of all features unsupported in the sample domain. Initially, sampling definitions will utilize a simple, piecewise linear approximation of the anisotropic diffusion mechanism. Results will then demonstrate the viability of the sampling approach through the computation of reconstruction <b>errors.</b> <b>Extension</b> to more practical diffusion operators will also be considered...|$|R
40|$|This paper {{describes}} some extensions to the Geographer Module of Huey. Keeping {{track of}} and handling motion errors and incorporating {{the uncertainty of}} a feature detector into the decision procedure are explained. We also introduce {{a new set of}} geometric features and add a new action for reducing positional <b>errors.</b> These <b>extensions</b> provide efficiency and robustness to the Geographer Module...|$|R
40|$|A {{complete}} {{classification of}} the perfect binary one-error-correcting codes of length 15 {{as well as their}} extensions of length 16 was recently carried out in [P. R. J. Östergård and O. Pottonen, "The perfect binary one-error-correcting codes of length 15 : Part I [...] Classification," IEEE Trans. Inform. Theory vol. 55, pp. 4657 [...] 4660, 2009]. In the current accompanying work, the classified codes are studied in great detail, and their main properties are tabulated. The results include the fact that 33 of the 80 Steiner triple systems of order 15 occur in such codes. Further understanding is gained on full-rank codes via switching, as it turns out that all but two full-rank codes can be obtained through a series of such transformations from the Hamming code. Other topics studied include (non) systematic codes, embedded one-error-correcting codes, and defining sets of codes. A classification of certain mixed perfect codes is also obtained. Comment: v 2 : fixed two <b>errors</b> (<b>extension</b> of nonsystematic codes, table of coordinates fixed by symmetries of codes), added and extended many other result...|$|R
40|$|Abstract. [Purpose] This {{study was}} {{performed}} to determine the difference in thoracic repositioning sense in young people with and without thoracic flexion syndrome (TFS) in target positions of half extension. [Subjects] People with TFS (n = 15; 7 men and 8 women) and people without TFS (n = 15; 7 men and 8 women) were recruited from three universities. Subjects were guided into a sitting extension target posture {{and were asked to}} move from a neutral position (2 s) to an extension target position (2 s); 10 trials were performed. [Results] People with TFS showed a significantly higher thoracic repositioning <b>error</b> in the <b>extension</b> target position than people without TFS. [Conclusion] People with TFS show a higher thoracic spine repositioning <b>error</b> in <b>extension</b> than people without TFS. A rehabilitation program to treat TFS should be implemented for individuals with decreased position sense of the thoracic spine...|$|R
40|$|Thermal <b>extension</b> <b>error</b> of {{boring bar}} in z-axis {{is one of}} the key factors that have a bad {{influence}} on the machining accuracy of boring machine, so how to exactly establish the relationship between the thermal extension length and temperature and predict the changing rule of thermal error are the premise of thermal <b>extension</b> <b>error</b> compensation. In this paper, a prediction method of thermal extension length of boring bar in boring machine is proposed based on principal component analysis (PCA) and least squares support vector machine (LS-SVM) model. In order to avoid the multiple correlation and coupling among the great amount temperature input variables, firstly, PCA is introduced to extract the principal components of temperature data samples. Then, LS-SVM is used to predict the changing tendency of the thermally induced thermal <b>extension</b> <b>error</b> of boring bar. Finally, experiments are conducted on a boring machine, the application results show that Boring bar axial thermal elongation error residual value dropped below 5 μm and minimum residual error is only 0. 5 μm. This method not only effectively improve the efficiency of the temperature data acquisition and analysis, and improve the modeling accuracy and robustness...|$|R
40|$|The aim of {{this paper}} is to {{accelerate}} division, square root and square root reciprocal computations, when Goldschmidt method is used on a pipelined multiplier. This is done by replacing the last iteration by the addition of a correcting term that can be looked up during the early iterations. We describe several variants of the Goldschmidt algorithm assuming 4 -cycle pipelined multiplier and discuss obtained number of cycles and <b>error</b> achieved. <b>Extensions</b> to other than 4 -cycle multipliers are given...|$|R
40|$|The hybrid {{coding scheme}} is {{employed}} in all established coding standards. A {{forward motion vector}} field is estimated and applied for motion compensation. The remaining prediction error and the motion vectors are transmitted to the decoder. The discrete cosine transform is used for transform coding of the prediction <b>error.</b> The <b>extension</b> of this coding scheme to scalability is not easily achieved {{and the performance of}} standard video coders when using scalability options can often be reduced to the performance of simulcast coding...|$|R
40|$|Abstract [...] In this paper, we {{describe}} image-based point rendering (IBPR) for multiple range images from 3 D scanners. Our {{approach is a}} natural extension of the method called pull-push so as to render scanned points with some measurement <b>errors.</b> Several <b>extensions</b> for rendering range images are proposed. One is a seamless rendering of a whole object even for points that have measurement errors. The other is a high quality rendering to reduce blurring. Our method is suitable for roughly checking the shape from range images...|$|R
40|$|The aim of {{this paper}} is {{discussion}} on particular aspects of the extension of a classic example in the design of experiments under the presence of correlated <b>errors.</b> Such <b>extension</b> allows us to study the effect of the correlation range on the design. We discuss the dependence of the information gained by the D-optimum design on the covariance bandwidth and also we concentrate to some technical aspects that occurs in such settings. (author's abstract) Series: Research Report Series / Department of Statistics and Mathematic...|$|R
40|$|Kalman smoothers {{reconstruct}} {{the state of}} a dynamical system starting from noisy output samples. While the classical estimator relies on quadratic penalization of process deviations and measurement <b>errors,</b> <b>extensions</b> that exploit Piecewise Linear Quadratic (PLQ) penalties have been recently proposed in the literature. These new formulations include smoothers robust with respect to outliers in the data, and smoothers that keep better track of fast system dynamics, e. g. jumps in the state values. In addition to L 2, well known examples of PLQ penalties include the L 1, Huber and Vapnik losses. In this paper, we use a dual representation for PLQ penalties to build a statistical modeling framework and a computational theory for Kalman smoothing. We develop a statistical framework by establishing conditions required to interpret PLQ penalties as negative logs of true probability densities. Then, we present a computational framework, based on interior-point methods, that solves the Kalman smoothing problem with PLQ penalties and maintains the linear complexity {{in the size of}} the time series, just as in the L 2 case. The framework presented extends the computational efficiency of the Mayne-Fraser and Rauch-Tung-Striebel algorithms to a much broader non-smooth setting, and includes many known robust and sparse smoothers as special cases. Comment: 8 page...|$|R
40|$|A new {{multiple}} testing procedure, {{the generalized}} augmentation procedure (GAUGE), is introduced. The procedure {{is shown to}} control the false discovery exceedance and to be competitive in terms of power. It is also shown how to apply the idea of GAUGE to achieve control of other <b>error</b> measures. <b>Extensions</b> to dependence are discussed, together with a modification valid under arbitrary dependence. We present an application to an original study on prostate cancer and on a benchmark data set on colon cancer. Copyright (c) 2009 Board of the Foundation of the Scandinavian Journal of Statistics. ...|$|R
40|$|An {{interpolated}} 3 -D digital waveguide mesh {{algorithm is}} elaborated. We introduce an optimized technique that improves a formerly proposed interpolated 3 -D mesh and renders the 3 -D mesh more homogeneous in different directions. Frequency-warping techniques {{are used to}} shift the frequencies of the output signal of the mesh in order to cancel the effect of dispersion <b>error.</b> The <b>extensions</b> improve the accuracy of 3 -D digital waveguide mesh simulations enough {{so that in the}} future it can be used for acoustical simulations needed in the design of listening rooms, for example...|$|R
40|$|This article {{explores the}} configural {{weighted}} average (CWA) hypothesis suggesting that extension biases, like conjunction and disjunction errors, occur because people estimate compound probabilities {{by taking a}} CWA of the constituent probabilities. The hypothesis suggests a process consistent with well-known cognitive constraints, which nonetheless achieves high robustness and bounded rationality in noisy real-life environments. Predictions by the CWA hypothesis are that in error-free data, conjunction and disjunction errors should be the rule {{rather than the exception}} when pairs of statements are randomly sampled from an environment, the rate of <b>extension</b> <b>errors</b> should increase when noise in data is decreased, and that adding a likely component should increase the probability of a conjunction. Four experiments generally verify the predictions by the hypothesis, demonstrating that <b>extension</b> <b>errors</b> are frequent also when tasks are selected according to representative design...|$|R
40|$|This paper {{proposes a}} self-supervised model which enables a {{humanoid}} robot {{to learn to}} reach to visual targets. Only 400 training samples are used to learn a forward kinematic model of the 6 degree-of-freedom (DOF) arm. The forward model is represented compactly with just 150 hidden neurons and enables high accuracy reaching in real-time. We provide an optimization process for the learning parameters and a careful analysis of reaching <b>errors.</b> An <b>extension</b> of the model is presented to address additional DOFs in the neck. The consistency of the model with physiological and psychological observations is elaborated...|$|R
40|$|Using many valid {{instrumental}} variables has {{the potential}} to improve efficiency but makes the usual inference procedures inaccurate. We give corrected standard <b>errors,</b> an <b>extension</b> of Bekker (1994) to nonnormal disturbances, that adjust for many instruments. We find that this adjustment is useful in empirical work, simula-tions, and in the asymptotic theory. Use of the corrected standard errors in t-ratios leads to an asymptotic approximation order that is the same when the number of instrumental variables grows as when the number of instruments is fixed. We also give a version of the Kleibergen (2002) weak instrument statistic that is robust to many instruments...|$|R
30|$|The WGPS [26] {{that deals}} with burst and location-dependent channel <b>error</b> is the <b>extension</b> of general {{processor}} sharing. When the errors happen, WGPS does not provide the service for the flow in an error state. Afterwards the flow's channel becomes good; the flow will receive the lost service that flows in the dirty channel.|$|R
40|$|In {{this paper}} we {{consider}} position control of underwater vehicles through inversion of differential kinematics based on uncalibrated, {{relative to the}} water, velocity sensors and unknown marine current. An estimation algorithm, based on the above measurements, estimates calibration parameters and marine current, assuring convergence of the estimated velocities to the true quantities. A kinematic control algorithm assures convergence to zero of the position tracking <b>error.</b> An <b>extension</b> of the basic estimation algorithm has been considered, in which position measurements are considered sampled at low rate and randomly spaced in time. Computer simulations are given of the proposed position tracking control of an underactuated underwater vehicle...|$|R
40|$|This lecture {{considers}} a-posteriori error {{estimates for}} the numerical solution of conservation laws with time invariant constraints such as those arising in magnetohydrodynamics (MHD) and gravitational physics. Using standard duality arguments, a-posteriori error {{estimates for the}} discontinuous Galerkin finite element method are then presented for MHD with solenoidal constraint. From these estimates, a procedure for adaptive discretization is outlined. A taxonomy of Green's functions for the linearized MHD operator is given which characterizes the domain of dependence for pointwise <b>errors.</b> The <b>extension</b> to other constrained systems such as the Einstein equations of gravitational physics are then considered. Finally, future directions and open problems are discussed...|$|R
40|$|This work {{consists}} of a review of online algorithms for URL classification followed by some extensions and tweaking of these methods to make them efficient in terms of computational time and memory. We {{found out that the}} trade-off in <b>error</b> for these <b>extensions</b> are fairly comparable for some of these algorithms. We also applied two more kernel based methods namely Forgetron and Projectron. 1...|$|R
5000|$|DisplayPort version 1.4 was {{published}} March 1, 2016. [...] No new transmission modes are defined, so HBR3 (32.4Gbit/s) as introduced in version 1.3 still remains {{as the highest}} available mode. DisplayPort 1.4 adds support for Display Stream Compression 1.2 (DSC), Forward <b>Error</b> Correction, HDR10 <b>extension</b> defined in CTA-861.3, the Rec. 2020 color space, and extends {{the maximum number of}} inline audio channels to 32.|$|R
