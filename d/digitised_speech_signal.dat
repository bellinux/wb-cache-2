2|7596|Public
40|$|Automatically {{generated}} detailed transcriptions {{should be}} helpful for transcribers anno-tating speech databases with detailed transcriptions. In this paper, a method for creating detailed transcriptions using a <b>digitised</b> <b>speech</b> <b>signal,</b> word boundary annotation, canon-ical transcriptions and näıve pronunciation rules is described. Some initial results using word boundaries from an automatic alignment system are presented. Although this data {{has been based on}} partly erroneous alignments, the results seem plausible. The sys-tem will be developed so that it can use manually annotated word boundaries for forced alignment of canonical transcriptions. This will probably enhance system performance considerably. ...|$|E
40|$|Acoustic {{analysis}} {{as used in}} the vocal pathology literature has come to mean any spectrum or waveform measurement taken from the <b>digitised</b> <b>speech</b> <b>signal.</b> The purpose of the work {{as set out in}} the present thesis is to investigate the currently available acoustic measures, to test their validity and to introduce new measures. More specifically, pitch extraction techniques and perturbation measures have been tested, several harmonic to noise ratio techniques have been implemented and thoroughly investigated (three of which are new) and cepstral and other spectral measures have been examined. Also, ratios relevant to voice source characteristics and perceptual correlation have been considered in addition to the tradition harmonic to noise ratios. A study of these approaches has revealed that many measurement problems arise and that the separation of the indices into independent measures is not a simple issue. The most commonly used acoustic measures for diagnosis o f vocal pathology are jitter, shimmer and the harmonic to noise ratio. However, several researchers have shown that these measures are not independent and therefore may give ambiguous information. For example, the addition of random noise causes increased jitter measurements and the introduction of jitter causes a reduced harmonic to noise ratio. Recent {{studies have shown that the}} glottal waveform and hence vibratory pattern of the vocal folds may be estimated in terms of spectral measurements. However, in order to provide spectral characterisation of the vibratory pattern in pathological voice types the effects of jitter and shimmer on the speech spectrum must firstly be removed. These issues are thoroughly addressed in this thesis. The foundation has been laid for future studies that will investigate the vibratory pattern of the vocal folds based on spectral evaluation of tape recorded data. All analysis techniques are tested by initially running them on specially designed synthesis data files and on a group of 13 patients with varying pathologies and a group of twelve normals. Finally, the possibility of using digital spectrograms for speaker identification purposes has been addressed...|$|E
40|$|This paper investigates {{lossless}} coding of wideband speech {{by adding a}} lossless enhancement layer to the lossy baselayer produced by a standardised wideband speech coder. Both the ITU-T G. 722 and G. 722. 2 speech coders are examined. Entropy results show that potential compression rates are dependent on the type and bit rate of the baselayer coder {{as well as the}} symbol size used by the lossless coder. Higher compression rates were obtained by adding a decorrelation stage prior to lossless encoding. The resulting lossless speech coder operates at a bit rate that is approximately 58 % of the bit rate of original <b>digitised</b> wideband <b>speech</b> <b>signal.</b> 1...|$|R
40|$|In {{application}} areas {{which involve}} <b>digitised</b> <b>speech</b> and audio <b>signals,</b> such as coding, digital remastering of old recordings {{and recognition of}} speech, it is often desirable to reduce the effects of noise {{with the aim of}} enhancing intelligibility and perceived sound quality. We consider the case where noise sources contain non-Gaussian, impulsive elements superimposed upon a continuous Gaussian background. Such a situation arises in areas such as communications channels, telephony and gramophone recordings where impulsive effects might be caused by electromagnetic interference (lightning strikes), electrical switching noise or defects in recording media, while electrical circuit noise or the combined effect of many distant atmospheric events lead to a continuous Gaussian component. In this paper we discuss the background to this type of noise degradation and describe briefly some existing statistical techniques for noise reduction. We propose new methods for enhancement based upon Mark [...] ...|$|R
50|$|Squish'em, {{also known}} as Squish'em Sam, is a 1983 video game {{designed}} by Tony Ngo and published by Sirius Software for the Atari 8-bit family, VIC-20, Commodore 64, MSX and the Colecovision game console. The Colecovision version features <b>digitised</b> <b>speech</b> without additional hardware and was published as Squish'em Featuring Sam. The game is the sequel to Sewer Sam.|$|R
40|$|In mobile devices, {{perceived}} <b>speech</b> <b>signal</b> degrades {{significantly in}} the presence of background noise as it reaches directly at the listener‟s ears. There is a need to improve the intelligibility and quality of the received <b>speech</b> <b>signal</b> in noisy environments by incorporating speech enhancement algorithms. This paper focuses on speech enhancement method including auditory masking properties of the human ear to improve the intelligibility and quality of the <b>speech</b> <b>signal</b> {{in the presence of}} near-end noise. Implemented by dynamically enhancing the <b>speech</b> <b>signal</b> when the near-end noise dominates. Intelligibility and quality of enhanced <b>speech</b> <b>signal</b> are measured using SII and PESQ. Experimental results show improvement in the intelligibility and quality of the enhanced <b>speech</b> <b>signal</b> with the proposed approach over the unprocessed <b>speech</b> <b>signal.</b> This particular approach is far more efficient in overcoming the degradation of <b>speech</b> <b>signals</b> in noisy environments...|$|R
30|$|Speech {{interferences}} (N 7 : <b>Speech</b> <b>signal</b> {{uttered by}} a woman 1, N 8 : <b>Speech</b> <b>signal</b> uttered by a man 2 and N 9 : <b>Speech</b> <b>signal</b> uttered {{by a woman}} 2).|$|R
40|$|Abstract: The {{adaptive}} noise cancellation {{system by}} LMS algorithm need {{not to know}} the prior knowledge of input <b>speech</b> <b>signal</b> and noise, and can carry out denoise. In this paper, we present a general approach to using Simulink to build adaptive filter which may denoise for noise added <b>speech</b> <b>signal.</b> Simulation results show that this method has the good suppression ability for the noise of collection <b>speech</b> <b>signal.</b> Because <b>speech</b> <b>signal</b> influenced inevitably by surrounding environment, communications equipment internal electrical noise and so on makes the receiver receive <b>speech</b> <b>signal</b> pollute...|$|R
40|$|This paper {{discusses}} a {{new kind}} of <b>Speech</b> <b>Signal</b> Compression Coding Algorithm based on Half-waveform. According to different characteristics of different <b>speech</b> <b>signal</b> parts, before we encode the <b>Speech</b> <b>Signal,</b> we segment <b>Speech</b> <b>Signal</b> into three kinds of segments: Silence segment, Unvoiced sound segment, Voiced sound segment. As such we encode each kind of speech segment and allocate different bit rate to each kind of speech segment to save the channel sources by different principles. Then we can get these advantages: low bit rate, high compression ratio, high quality of reestablished <b>speech</b> <b>signal...</b>|$|R
40|$|A {{new method}} for {{encoding}} <b>speech</b> <b>signals</b> in automatic voice recognition is proposed. To represent <b>speech</b> <b>signals</b> with minimum redundancy we use independent component analysis to adapt features (basis functions) that efficiently encode the <b>speech</b> <b>signals.</b> The learned basis functions are localized {{in time and}} frequency and resemble Gabor-like filters. In encoding the <b>speech</b> <b>signals,</b> the ICA features capture the statistical essence of <b>speech</b> <b>signals</b> with fewer basis functions than traditional methods such as Gabor filters and the Fourier basis. A speech recognizer can be trained based on those features and the recognition rate is improved and better than the recognition rates obtained by the conventional mel-frequency cepstral features. Our {{results suggest that the}} obtained higher-order structure of <b>speech</b> <b>signals</b> {{plays an important role in}} efficient speech coding. Index Terms Gabor filter, independent component analysis, <b>speech</b> <b>signal</b> processing, feature extraction. I. Introd [...] ...|$|R
5000|$|If {{measurement}} matrix [...] {{satisfies the}} restricted isometric property (RIP) and is incoherent with dictionary matrix [...] then the reconstructed signal is {{much closer to}} the original <b>speech</b> <b>signal.</b> Different types of measurement matrices like random matrices can be used for speech signals.Estimating the sparsity of <b>speech</b> <b>signal</b> is a problem since <b>speech</b> <b>signal</b> highly varies over time and thus sparsity of <b>speech</b> <b>signal</b> also varies highly over time. If sparsity of <b>speech</b> <b>signal</b> can be calculated over time without much complexity that will be best. If this is not possible then worst-case scenario for sparsity can be considered for a given <b>speech</b> <b>signal.</b> Sparse vector (...) for a given <b>speech</b> <b>signals</b> is reconstructed from less number of measurements (...) using [...] minimization. Then original <b>speech</b> <b>signal</b> is reconstructed form the calculated sparse vector [...] using the fixed dictionary matrix as [...] as [...] = [...] [...] Estimation of both the dictionary matrix and sparse vector from just random measurements only has been done iteratively in.The <b>speech</b> <b>signal</b> reconstructed from estimated sparse vector and dictionary matrix is {{much closer to the}} original signal.Some more iterative approaches to calculate both dictionary matrix and <b>speech</b> <b>signal</b> from just random measurements of <b>speech</b> <b>signal</b> are shown in.Th application of structured sparsity for joint speech localization-separation in reverberant acoustics has been investigated for multiparty speech recognition. Further applications of the concept of sparsity are yet to be studied in the field of speech processing. The idea behind CS for <b>speech</b> <b>signals</b> is that can we come up with some algorithms or methods where we only use those random measurements (...) to do some application-based processing like speaker recognition, speech enhancement, etc.|$|R
40|$|Abstract — A {{humanoid}} robot must recognize a target <b>speech</b> <b>signal</b> while {{people around the}} robot chat with them in realworld. To recognize the target <b>speech</b> <b>signal,</b> robot has to separate the target <b>speech</b> <b>signal</b> among other <b>speech</b> <b>signals</b> and recognize the separated <b>speech</b> <b>signal.</b> As separated signal includes distortion, automatic speech recognition (ASR) performance degrades. To avoid the degradation, we trained an acoustic model from non-clean <b>speech</b> <b>signals</b> to adapt acoustic feature of distorted signal and adding white noise to separated <b>speech</b> <b>signal</b> before extracting acoustic feature. The issues are (1) To determine optimal noise level to add the training <b>speech</b> <b>signals,</b> and (2) To determine optimal noise level to add the separated signal. In this paper, we investigate how much noises should be added to clean speech data for training and how speech recognition performance improves for different positions of three talkers with soft masking. Experimental {{results show that the}} best performance is obtained by adding white noises of 30 dB. The ASR with the acoustic model outperforms with ASR with the clean acoustic model by 4 points. I...|$|R
40|$|Noise {{estimation}} and suppression is {{very important}} for improving the quality of <b>speech</b> <b>signal.</b> Noises exist in almost all places. In reality, more than one noise degrades the <b>speech</b> <b>signal.</b> It is hard to find and supress various types of noise that affect the speech quality. This paper proposed a method for noise estimation of mixed non-stationary noisy <b>speech</b> <b>signal.</b> This method uses Spectral properties of the noisy <b>speech</b> <b>signal</b> to detect the frequency regions of noise signal. Highest frequency of <b>speech</b> <b>signal</b> is calculated and it is considered as the threshold value for separating noise <b>signal</b> and clean <b>speech</b> <b>signal.</b> Using Spectral subtraction, Standard deviation of noise spectrum is subtracted with noisy spectrum to acquire enhanced <b>speech</b> <b>signal.</b> Performance of the method is evaluated using SNR and Spectrogram. The main focus {{of this paper is to}} propose an independent method which estimates the noise of any type and nature...|$|R
40|$|Abstract. The paper {{addresses}} {{the problem of}} discrimination of homographs when a lengthy segment of an uttered word is missing. The considered discrimination procedure is done by recognizer that operates on cepstrum coefficients extracted from the <b>speech</b> <b>signal.</b> For restoration of the missing speech segment rather than use of the known <b>speech</b> <b>signal,</b> it has been proposed to calculate <b>speech</b> <b>signal</b> characteristics: the period of fundamental frequency and intensity. By experimentation {{it has been shown}} that the polynomial approximation of <b>speech</b> <b>signal</b> characteristics improves homograph discrimination results. An extra computational burden associated with the proposed method is not high because it involves recalculation of the already extracted Fourier coefficients. Key words: homograph discrimination, <b>speech</b> recognition, <b>speech</b> <b>signal</b> processing, <b>speech</b> <b>signal</b> characteristics, restoration, approximation...|$|R
30|$|The {{transform}} coefficients {{based on}} the spectral characteristics of unvoiced <b>speech</b> <b>signals</b> are nearly uniformly distributed in the frequency domain with no obvious decay. Consequently, the sparsity of unvoiced <b>speech</b> <b>signal</b> {{with respect to the}} DCT basis is undesirable. Furthermore, we have not found a satisfactory sparsifying matrix for unvoiced <b>speech</b> <b>signals.</b> Therefore, the usual practice in the framework of CS is to apply the scheme to entire <b>speech</b> <b>signals</b> and not to distinguish voiced <b>speech</b> <b>signals</b> and unvoiced <b>speech</b> <b>signals</b> in advance. Moreover, we find that the overall performance has not been greatly influenced, which can be verified by the simulation results in the following subsection. The reason is that the proportion of voiced speech is more than seventy percent and voiced speech bears dominating information of speech. Certainly, it is of great significance for us to seek to construct a basis or a redundant dictionary for unvoiced <b>speech</b> <b>signals,</b> which is the focus of our future work.|$|R
40|$|Abstract. A new {{efficient}} {{code for}} <b>speech</b> <b>signals</b> is proposed. To represent <b>speech</b> <b>signals</b> with minimum redundancy we use independent component analysis to adapt features (basis vec-tors) that efficiently encode the <b>speech</b> <b>signals.</b> The learned basis vectors are sparsely distrib-uted and localized in both time and frequency. Time-frequency analysis of basis vectors shows the property similar with the critical bandwidth of human auditory system. Our {{results suggest that}} the obtained codes of <b>speech</b> <b>signals</b> are sparse and biologically plausible...|$|R
40|$|This paper {{presents}} the denoising technique based on spectral subtraction for speech {{synthesis of the}} Marathi numerals. Numerals are recorded through mice and normalized the signals with PRAAT tools. Different form of <b>speech</b> <b>signals</b> were analyzed by added noise in original <b>speech</b> <b>signals.</b> The voice activity detection (VAD) algorithm estimate the noise spectrum of original <b>speech</b> <b>signal.</b> Spectral Subtraction technique is adopted to reduce the noise. It exploits the ability of actively unwanted <b>signals</b> of <b>speech.</b> Spectral Subtraction method has reduced noise and improved the quality of <b>speech</b> <b>signals.</b> This paper concentrates on the application of quality <b>speech</b> <b>signals</b> for <b>speech</b> synthesis and the results found to be satisfactory...|$|R
5000|$|Consider a <b>speech</b> <b>signal</b> , {{which can}} be {{represented}} in a domain [...] such that , where <b>speech</b> <b>signal</b> [...] , dictionary matrix [...] and the sparse coefficient vector [...] This <b>speech</b> <b>signal</b> {{is said to be}} sparse in domain , if number of significant (non zero) coefficients in sparse vector [...] are , where [...]|$|R
5000|$|Exile {{offers the}} option of playing an {{enhanced}} version of the game on a BBC Micro upgraded with a 16 kB page of sideways RAM. These enhancements include sampled sound effects and <b>digitised</b> <b>speech</b> ("Welcome {{to the land of}} the Exile." [...] and [...] "Alien die!"), as well as a larger visible screen area (eight physical colours; 128 pixels across &times; 256 lines down = 16 kB screen memory).|$|R
5000|$|In the Colecovision {{version of}} the game <b>digitised</b> <b>speech</b> is {{employed}} when Sam performs certain actions, for instance he exclaims [...] "squish 'em" [...] after successfully attacking an enemy and [...] "money, money, money" [...] after collecting the suitcase {{at the top of}} each building. It {{is one of the few}} Colecovision games to contain speech, as unlike the rival Intellivision console the Colecovision lacks a speech module.|$|R
30|$|As expected, the time-frequency {{characteristics}} of the watermark follow those components of the <b>speech</b> <b>signal.</b> Consequently, the watermark is inaudible within the <b>speech</b> <b>signal.</b>|$|R
40|$|In this work, <b>speech</b> <b>signals</b> are modeled {{by means}} of the {{so-called}} pre-defined "signature functions". The pre-defined signature functions are generated using the statistical properties of the <b>speech</b> <b>signals.</b> It has been exhibited that, with a few basic signature functions, any <b>speech</b> <b>signal</b> can be generated within a tolerable error. Publisher's Versio...|$|R
40|$|Abstract:- Enhancement of <b>speech</b> <b>signals</b> {{recorded}} using signal channel {{devices such}} as mobile phones is of prime interest. It is because for these devices, {{it is not possible}} to record noise signals separately, and the surrounding background noises are picked up by their microphone simultaneously with the <b>speech</b> <b>signal.</b> This may even completely fade-in the <b>speech</b> <b>signal,</b> depending upon the signal-to-noise ratio (SNR). Therefore to address this problem, number of algorithms and techniques has been developed. However, the existing methods are not able to perform homogenously across all noise types. The auto-correlation function of a noisy <b>speech</b> <b>signal</b> is usually confined to lower time lag and is very small or zero for higher time lag. Therefore, the higher-lag auto-correlation coefficients are relatively robust to additive noise distortion. This paper is focused on enhancing the noisy <b>speech</b> <b>signal</b> from single channel devices by using only the higher-lag auto-correlation coefficients. The efficiency of the algorithm is evaluated in terms of energy, zero crossings and intelligibility of <b>speech</b> <b>signal.</b> Keywords:- Single channel speech enhancement; speech processing; spectral subtraction autocorrelation and <b>Speech</b> <b>signals</b> SNR I...|$|R
30|$|X and R {{denote the}} pure sent <b>speech</b> <b>signal</b> and the {{received}} <b>speech</b> <b>signal,</b> respectively. Four {{cases of the}} OED of received speech are discussed below.|$|R
3000|$|... where y(t) is the {{degraded}} <b>speech</b> <b>signal,</b> s(t) {{represents the}} clean signal, d(t) is the additive noise, which is uncorrelated with the <b>speech</b> <b>signal</b> and unknown.|$|R
2500|$|Speech {{processing}} [...] {{study of}} <b>speech</b> <b>signals</b> and the processing methods of these signals. The signals are usually processed {{in a digital}} representation, so speech processing {{can be regarded as}} a special case of digital signal processing, applied to <b>speech</b> <b>signal.</b> Aspects of <b>speech</b> processing includes the acquisition, manipulation, storage, transfer and output of digital <b>speech</b> <b>signals.</b>|$|R
40|$|This paper {{describes}} {{analysis of}} a speaker recognition model based on Generalized Gamma Distribution (GGD) using PCA. The proposed work mainly concentrates on the feature vectors that are generated from the <b>speech</b> <b>signals</b> contain high dimension data, but to model a speech and recognize a speaker finite speech samples which plays significant role in speech analysis are sufficient, hence it necessary to reduce dimension of the data. The PCA is considered for this purpose, it converts high dimension <b>speech</b> <b>signal</b> in to a low dimension <b>speech</b> <b>signal</b> by transforming the un-correlated components of the <b>speech</b> <b>signal.</b> PCA not only reduces the correlation among feature vectors but also the <b>speech</b> <b>signal.</b> The feature vectors are modeled by extracting MFCC followed by PCA for dimensionality reduction...|$|R
40|$|International audienceIn {{this paper}} a new {{geometrical}} approach for separating <b>speech</b> <b>signals</b> is presented. This {{approach can be}} directly applied to separate more than two <b>speech</b> <b>signals.</b> It is based on clustering the observation points,and then fitting aline(hyper-plane) ontoeach cluster. The algorithm quality is shown to be improved by using DCT coefficients of <b>speech</b> <b>signals,</b> as opposed to using speech samples...|$|R
50|$|The {{study of}} {{phonetics}} grew {{quickly in the}} late 19th century {{partly due to the}} invention of the phonograph, which allowed the <b>speech</b> <b>signal</b> to be recorded. Phoneticians were able to replay the <b>speech</b> <b>signal</b> several times and apply acoustic filters to the signal. By doing so, they were able to more carefully deduce the acoustic nature of the <b>speech</b> <b>signal.</b>|$|R
40|$|Abstract—Today’s {{telecommunications}} systems use a limited audio signal bandwidth. A typical bandwidth is 0. 3 – 3. 4 kHz, but recently {{it has been}} suggested that mobile phone networks will facilitate an audio signal bandwidth of 50 Hz– 7 kHz. This is suggested since an increased bandwidth will increase the sound quality of the <b>speech</b> <b>signals.</b> Since only few telephones initially will have this facility, a method extending the conventional narrow frequency-band <b>speech</b> <b>signal</b> into a wide-band <b>speech</b> <b>signal</b> utilizing the receiving telephone only is suggested. This will give the impression of a wide-band <b>speech</b> <b>signal.</b> The proposed <b>speech</b> bandwidth extension method is based on models of speech acoustics and fundamentals of human hearing. The extension maps each speech feature separately. Care has been taken to deal with implementation aspects, such as noisy <b>speech</b> <b>signals,</b> <b>speech</b> <b>signal</b> delays, computational complexity, and processing memory usage. Index Terms—Speech analysis, speech enhancement, speech synthesis. I...|$|R
40|$|<b>Speech</b> <b>signal</b> {{enhancement}} {{techniques have}} reached a considerable research attention because of its significant need in several signal processing applications. Various techniques {{have been developed for}} improving the <b>speech</b> <b>signals</b> in adverse conditions. In order to apply a good <b>speech</b> <b>signal</b> enhancement technique, an extensive comparison of the algorithms has always been necessary. Therefore, the performance evaluations of eight <b>speech</b> <b>signal</b> enhancement techniques are implemented and assessed based on various <b>speech</b> <b>signal</b> quality measures. In this paper, the Geometric Spectral Subtraction (GSS), Recursive Least Squares (RLS) Adaptive Filtering, Wavelet Filtering, Kalman Filtering, Ideal Binary Mask (IBM), Phase Spectrum Compensation (PSC), Minimum Mean Square Error estimator Magnitude Squared Spectrum incorporating SNR Uncertainty (MSS-MMSE-SPZC), and MMSE-MSS using SNR Uncertainty (MSS-MMSE-SPZC-SNRU) algorithms are implemented. These techniques are evaluated based on six objective speech quality measures and one subjective quality measure. Based on the experimental outcomes, the optimal <b>speech</b> <b>signal</b> enhancement technique which is suitable for all types of noisy conditions is exposed...|$|R
40|$|Some {{traditional}} {{digital signal}} processing techniques encompass enhancement, filtering, coding, compression, detection and recognition. Recently, it has been presented a new hypothesis of signal processing known as the ability of adaptation of speech signals: an original <b>speech</b> <b>signal</b> may sound similar to a target <b>speech</b> <b>signal</b> if a relocation process of its wavelet coefficients is applied. This hypothesis is true under some conditions theoretically defined. In this paper we present the basic idea behind the hypothesis of adaptation and moreover, we test the hypothesis within four cases: <b>speech</b> <b>signals</b> with the same gender and language, <b>speech</b> <b>signals</b> with the same gender but different language, <b>speech</b> <b>signals</b> with the same language but different gender, and <b>speech</b> <b>signals</b> with different gender and language. It is found that the hypothesis is true if the requirements are satis® ed, even if the gender or {{the language of the}} original and target signals are not the same. Peer ReviewedPostprint (published version...|$|R
40|$|Speech {{denoising}} is {{the process}} of removing unwanted sounds from the <b>speech</b> <b>signal.</b> In the presence of noise, it is difficult for the listener to understand the message of the <b>speech</b> <b>signal.</b> Also, the presence of noise in <b>speech</b> <b>signal</b> will degrade the performance of various signal processing tasks like speech recognition, speaker recognition, speaker verification etc. Many methods have been widely used to eliminate noise from <b>speech</b> <b>signal</b> like linear and nonlinear filtering methods, total variation denoising, wavelet based denoising etc. This paper addresses the problem of reducing additive white Gaussian noise from <b>speech</b> <b>signal</b> while preserving the intelligibility and quality of the <b>speech</b> <b>signal.</b> The method is based on Savitzky-Golay smoothing filter, which is basically a low pass filter that performs a polynomial regression on the signal values. The results of S-G filter based denoising method are compared against two widely used enhancement methods, Spectral subtraction method and Total variation denoising. Objective and subjective quality evaluation are performed for the three speech enhancement schemes. The results show that S-G based method is ideal for the removal of additive white Gaussian noise from the <b>speech</b> <b>signals...</b>|$|R
40|$|Abstract:-Speech {{processing}} is {{the study}} of <b>speech</b> <b>signals,</b> and the methods used to process them. In application such as speech coding, speech synthesis, speech recognition and speaker recognition technology, speech processing is employed. In speech classification, the computation of prosody effects from <b>speech</b> <b>signals</b> plays a major role. In emotional <b>speech</b> <b>signals</b> pitch and frequency is a most important parameters. Normally, the pitch value of sad and happy <b>speech</b> <b>signals</b> has a great difference and the frequency value of happy is higher than sad speech. But, in some cases the frequency of happy speech is nearly similar to sad speech or frequency of sad speech is similar to happy speech. In such situation, it is difficult to recognize the exact <b>speech</b> <b>signal.</b> To reduce such drawbacks, in this paper we propose a Telugu speech emotion classification system with three features and use neural network for the classification. Features are extracted with optimal window size from the <b>speech</b> <b>signals</b> and given to the FFBNN. The well trained FFBNN is tested with more number of <b>speech</b> <b>signals</b> with prosody effects. The implementation result shows the effectiveness of proposed speech emotion classification system in classifying the Telugu <b>speech</b> <b>signals</b> based on their prosody effects. The performance of the proposed speech emotion classification system is evaluated by change the learning error rate of the Back Propagation Network till the recognition rate reaches to optimum value...|$|R
40|$|This paper {{addresses}} {{the issue of}} balancing the acoustic hole and sparsing the <b>speech</b> <b>signal</b> enrollment for training and testing in Automatic Speaker Recognition (ASR) system. Sparsing techniques involve the representation of {{a small number of}} coefficients that hold a large amount of the energy. Sparsity can {{play a major role in}} resolving the issue of dealing with big data in ASR by applying speech compression techniques and information storage in databases. Spectral domain compression of the <b>speech</b> <b>signal</b> using novel sparsing algorithms that balance the sparsity of <b>speech</b> <b>signal</b> with the acoustic hole is proposed. The <b>speech</b> <b>signal</b> is converted to a spectral domain using the Discrete Rajan Transform (DRT) and only first and mid-spectrum component in each block of size 8 x 1 retained forcing the remaining component to zero. The <b>speech</b> <b>signal</b> spectrum can be maximally compressed at 8 : 1 ratio to the unique one with balancing acoustic hole and synthesized <b>speech</b> <b>signal,</b> which can be used in ASR systems. A balanced spectrally compressed <b>speech</b> <b>signal</b> can be stored in database as a speaker representative and during training and testing time it can be synthesized using the Inverse Discrete Rajan Transform (IDRT). Simulation results, shows acceptable <b>speech</b> <b>signal</b> spectral compression that balances sparsity and the generation of the acoustic hole is 75...|$|R
40|$|Abstract. In {{this paper}} a new {{geometrical}} approach for separating <b>speech</b> <b>signals</b> is presented. This {{approach can be}} directly applied to separate more than two <b>speech</b> <b>signals.</b> It is based on clustering the observation points, and then fitting a line (hyper-plane) onto each cluster. The algorithm quality is shown to be improved by using DCT coefficients of <b>speech</b> <b>signals,</b> as opposed to using speech samples. ...|$|R
