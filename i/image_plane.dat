2789|994|Public
5|$|The {{detection}} uses {{pattern recognition}} of regular shapes (rectangles, circles, ellipses). The 3D {{model of the}} element to be inspected can be projected in the <b>image</b> <b>plane</b> for more complex shapes. The evaluation is based on indices such as the uniformity of segmented regions, convexity of their forms, or periodicity of the image pixels' intensity.|$|E
5|$|For visual localization, {{the robot}} {{estimates}} its position {{relative to the}} aircraft using visual elements (doors, windows, tires, static ports etc.) of the aircraft. During {{the evolution of the}} robot, these visual elements are extracted from a three-dimensional virtual model of the aircraft and projected in the <b>image</b> <b>plane</b> of the cameras. The projected shapes are used for pattern recognition to detect those visual elements. The other detection method used is based on the extraction of features with a Speeded Up Robust Features (SURF) approach. A pairing is performed between images of each element to be detected and the actual scene experienced.|$|E
25|$|When {{the lens}} axis is {{perpendicular}} to the <b>image</b> <b>plane,</b> as is normally the case, the plane of focus (POF) {{is parallel to the}} <b>image</b> <b>plane,</b> and the DOF extends between parallel planes {{on either side of the}} POF. When the lens axis is not {{perpendicular to the}} <b>image</b> <b>plane,</b> the POF is no longer parallel to the image plane; the ability to rotate the POF is known as the Scheimpflug principle. Rotation of the POF is accomplished with camera movements (tilt, a rotation of the lens about a horizontal axis, or swing, a rotation about a vertical axis). Tilt and swing are available on most view cameras, and are also available with specific lenses on some small- and medium-format cameras.|$|E
5000|$|... 16 {{of these}} become <b>image</b> <b>planes</b> {{with the other}} 8 used as {{moveable}} [...] "overlay" [...] planes. Another 2 hardware colormaps become available, providing 2 for the <b>image</b> <b>planes</b> and 2 for the overlay planes.|$|R
50|$|Microscopes using Köhler {{illumination}} must be routinely {{checked for}} correct alignment. The realignment procedure tests whether the correct optical components are in focus {{at the two}} sets of conjugate image planes; the light source <b>image</b> <b>planes</b> and the specimen <b>image</b> <b>planes.</b>|$|R
50|$|Object planes {{perpendicular}} to the optical axis are conjugate to <b>image</b> <b>planes</b> {{perpendicular to}} the axis.|$|R
25|$|Besides {{providing}} a uniform description of circles, ellipses, parabolas, and hyperbolas, conic sections {{can also be}} understood as a natural model of the geometry of perspective in the case where the scene being viewed consists of circles, or more generally an ellipse. The viewer is typically a camera or the human eye and the image of the scene a central projection onto an <b>image</b> <b>plane,</b> i.e., all projection rays pass a fixed point O, the center. The lens plane is a plane parallel to the <b>image</b> <b>plane</b> at the lens O.|$|E
25|$|The fringes in {{the picture}} were {{obtained}} using the yellow light from a sodium light (wavelength = 589nm), with slits separated by 0.25mm, and projected directly onto the <b>image</b> <b>plane</b> of a digital camera.|$|E
25|$|Moritz von Rohr {{also used}} an object field method, but unlike Merklinger, {{he used the}} {{conventional}} criterion of a maximum circle of confusion diameter in the <b>image</b> <b>plane,</b> leading to unequal front and rear depths of field.|$|E
30|$|Misidentifying uterine {{vasculature}} as a thin-walled hydrosalpinx on cross-sectional <b>image</b> <b>planes.</b> Doppler evaluation {{can usually}} resolve this issue.|$|R
5000|$|... #Caption: The optical {{setup and}} light path of {{critical}} illumination showing the conjugate <b>image</b> <b>planes</b> {{of the various}} optical components.|$|R
40|$|The {{radiation}} from {{a printed circuit}} board (PCB) is one of the critical problems regarding the design and functioning of VLSI electronic devices. This work deals with the influence of two finite width, finite thickness, not perfectly conductive <b>image</b> <b>planes</b> on the {{radiation from}} a trace placed between them. The effects of the surface currents on both sides of each plane are taken into account. The radiated emission of this structure has been calculated by solving a set of Electric Field Integral Equations by the Method of Moments. The current distributions on the <b>image</b> <b>planes</b> as functions of various parameters are analyzed to have a guidance to an optimal design. The radiation patterns in different configurations of the <b>image</b> <b>planes</b> are also considered. The diffusion of the electromagnetic field through the finite thickness of the boards is modelled and its influence discussed...|$|R
25|$|Sub pixel {{displacement}} values allow a {{high degree}} of accuracy, since each vector is the statistical average for many particles within a particular tile. Displacement can typically be accurate down to 10% of one pixel on the <b>image</b> <b>plane.</b>|$|E
25|$|These {{results can}} be {{understood}} if one recognizes that the projection process {{can be seen in}} two steps: 1) circle c and point O generate a cone which is 2) cut by the <b>image</b> <b>plane,</b> in order to generate the image.|$|E
25|$|Ray tracing is a {{technique}} for generating an image by tracing the path of light through pixels in an <b>image</b> <b>plane.</b> The technique is capable of producing a very high degree of photorealism; usually {{higher than that of}} typical scanline rendering methods, but at a greater computational cost.|$|E
50|$|Where the <b>image</b> <b>planes</b> are not co-planar image {{rectification}} {{is required to}} adjust the images {{as if they were}} co-planar. This may be achieved by a linear transformation.|$|R
40|$|AbstractMatch stimuli {{presented}} {{on one side}} of a contextual image were adjusted to have the same appearance as test stimuli {{presented on}} the other side. Both full color and isochromatic contextual images were used. Contextual image pairs were constructed that had identical S-cone <b>image</b> <b>planes,</b> while their L- and M-cone <b>image</b> <b>planes</b> differed. The data show that the S-cone component of the matches depends on the L- and M-cone planes of the contextual image. This dependence means that matches obtained using isochromatic stimuli (lightness matches) may not be used directly to predict full color matches...|$|R
5000|$|These {{components}} lie in {{this order}} between the light source and the specimen {{and control the}} illumination of the specimen. The collector/field lenses act to collect light from the light source and focus it at {{the plane of the}} condenser diaphragm. The condenser lens acts to project this light, without focusing it, through the sample. This illumination scheme creates two sets of conjugate <b>image</b> <b>planes,</b> one with the light source image and one with the specimen. These two sets of <b>image</b> <b>planes</b> are found at the following points: ...|$|R
25|$|Optical flowto determine, {{for each}} {{point in the}} image, how that point is moving {{relative}} to the <b>image</b> <b>plane,</b> i.e., its apparent motion. This motion is a result both of how the corresponding 3D point {{is moving in the}} scene and how the camera is moving relative to the scene.|$|E
25|$|An air-wedge {{shearing}} interferometer {{is described}} in and was employed in set of experiments described in. This interferometer consists of two optical glass wedges (~2-5deg), pushed together and then slightly separated {{from one side to}} create a thin air-gap wedge. This air-gap wedge has a unique property- it is very thin (micrometer scale) and it has perfect flatness (~λ/10). There are four nearly equal intensity Fresnel reflections (~4% for refraction coefficient 1.5) from the air-wedge interferometer (Fig.1): 1-2 from the first glass wedge, and 3-4 from the second one. The angle between beams 1-2 and 3-4 is non adjustable and depends only on the glass wedge. The angle between beams 2-3 is easily adjusted by varying the air-wedge angle. The distance between the air-wedge and an <b>image</b> <b>plane</b> should be long enough to spatially separate reflections 1 from 2 and 3 from 4. The overlap of beams 2 and 3 in the <b>image</b> <b>plane</b> creates an interferogram.|$|E
25|$|This type {{of camera}} matrix is {{referred}} to as a normalized camera matrix, it assumes focal length = 1 and that image coordinates are measured in a coordinate system where the origin is located at the intersection between axis X3 and the <b>image</b> <b>plane</b> and has the same units as the 3D coordinate system. The resulting image coordinates are referred to as normalized image coordinates.|$|E
5000|$|So {{assuming}} {{the cameras are}} level, and <b>image</b> <b>planes</b> are flat on the same plane, the displacement in the y axis between the same pixel in the two images is, ...|$|R
40|$|Match stimuli {{presented}} {{on one side}} of a contextual image were adjusted to have the same appearance as test stimuli {{presented on}} the other side. Both full color and isochromatic contextual images were used. Contextual image pairs were constructed that had identical S-cone <b>image</b> <b>planes,</b> while their L- and M-cone <b>image</b> <b>planes</b> differed. The data show that the S-cone component of the matches depends on the L- and M-cone planes of the contextual image. This dependence means that matches obtained using isochromatic stimuli (lightness matches) may not be used directly to predict full color matches. © 2000 Elsevier Science Ltd. All rights reserved...|$|R
40|$|In {{this paper}} we propose a new Computer Vision {{technique}} to reconstruct the vascular wall in space using a deformable model-based technique and compounding methods, based in biplane angiography and intravascular ultrasound data jicsion. It is also proposed a generalpurpose three-dimensional guided interpolation method. The three dimensional centerline of the vessel is reconstructed from geometrically corrected biplane angiographies using automatic segmentation methods and snakes. The IVUS <b>image</b> <b>planes</b> are located in the threedimensional space and correctly oriented. A led interpolation method based in B-SurJaces and snakes is used to fill the gaps among <b>image</b> <b>planes</b> 1...|$|R
25|$|The focus spread {{is related}} to the depth of focus. Ray (2000, 56) gives two {{definitions}} of the latter. The first is the tolerance of the position of the <b>image</b> <b>plane</b> for which an object remains acceptably sharp; the second is that the limits of depth of focus are the image-side conjugates of the near and far limits of DOF. With the first definition, focus spread and depth of focus are usually close in value though conceptually different. With the second definition, focus spread and depth of focus are the same.|$|E
25|$|Dark field {{microscopy}} is {{a technique}} for improving the contrast of unstained, transparent specimens. Dark field illumination uses a carefully aligned light source to minimize the quantity of directly transmitted (unscattered) light entering the <b>image</b> <b>plane,</b> collecting only the light scattered by the sample. Dark field can dramatically improve image contrast – especially of transparent objects – while requiring little equipment setup or sample preparation. However, the technique suffers from low light intensity in final image of many biological samples {{and continues to be}} affected by low apparent resolution.|$|E
25|$|The {{experiment}} uses a setup {{similar to}} that for the double-slit experiment. In Afshar's variant, light generated by a laser passes through two closely spaced circular pinholes (not slits). After the dual pinholes, a lens refocuses the light so {{that the image of}} each pinhole falls on separate photon-detectors (Fig. 1). A photon that goes through pinhole number one impinges only on detector number one, and similarly, if it goes through pinhole two it impinges only on detector number two, which is why we see the pinholes separately in the <b>image</b> <b>plane</b> close to the mirrors before the photon-detectors.|$|E
30|$|For all scan series, {{we report}} the average volume of {{interest}} (VOI) value (reconstructed counts) calculated from central circles with a radius of 5  cm placed on 11 central <b>image</b> <b>planes</b> of the phantom.|$|R
40|$|In {{stereoscopic}} images, {{the behavior}} of a curve in space is related to the appearance of the curve in the left and right <b>image</b> <b>planes.</b> Formally, this relationship is governed by the projective geometry induced by the stereo camera configuration and by the differential structure of the curve in the scene. In this thesis, I propose that the correspondence problem [...] -matching corresponding points in the <b>image</b> <b>planes</b> [...] -can be solved by relating the differential structure in the left and right <b>image</b> <b>planes</b> to the geometry of curves in space. Specifically, the compatibility between two pairs of corresponding points and tangents at those points is related to the local approximation of a space curve using an osculating helix. To guarantee robustness against small changes in the camera parameters, I introduce the principle of differential invariance and show how this principle resolves a natural ambiguity in selecting the osculating helix. A relaxation labeling network demonstrates that the compatibilities can be used to infer the appropriate correspondences in a scene. Finally, parallels between the proposed solution and binocular processes in primate vision are discussed...|$|R
3000|$|... [...]. In a multicamera scenario, the {{projective}} transformation {{between each}} camera {{and the world}} plane is different. Hence, the mapping from the individual <b>image</b> <b>planes</b> to the world planes is given {{by a set of}} matrices [...]...|$|R
25|$|The PlayStation Move motion {{controller}} {{features an}} orb {{at the head}} which can glow in any of {{a full range of}} colors using RGB light-emitting diodes (LEDs). Based on the colors in the user environment captured by the camera, the system dynamically selects an orb color that can be distinguished {{from the rest of the}} scene. The colored light serves as an active marker, the position of which can be tracked along the <b>image</b> <b>plane</b> by the camera. The uniform spherical shape and known size of the light also allows the system to simply determine the controller's distance from the camera through the light's image size, thus enabling the controller's position to be tracked in three dimensions with high precision and accuracy. The simple sphere-based distance calculation allows the controller to operate with minimal processing latency, as opposed to other camera-based control techniques on the PlayStation 3.|$|E
25|$|Typically a TEM {{consists}} of three stages of lensing. The stages are the condenser lenses, the objective lenses, and the projector lenses. The condenser lenses are responsible for primary beam formation, while the objective lenses focus the beam that comes through the sample itself (in STEM scanning mode, there are also objective lenses above the sample to make the incident electron beam convergent). The projector lenses are used to expand the beam onto the phosphor screen or other imaging device, such as film. The magnification of the TEM {{is due to the}} ratio of the distances between the specimen and the objective lens' <b>image</b> <b>plane.</b> Additional stigmators allow for the correction of asymmetrical beam distortions, known as astigmatism. It is noted that TEM optical configurations differ significantly with implementation, with manufacturers using custom lens configurations, such as in spherical aberration corrected instruments, or TEMs using energy filtering to correct electron chromatic aberration.|$|E
25|$|In {{this system}} a {{filament}} {{is located at}} one focus of an ellipsoidal reflector and has a condenser lens {{at the front of}} the lamp. A shade is located at the <b>image</b> <b>plane,</b> between the reflector and lens, and the projection of the top edge of this shade provides the low-beam cutoff. The shape of the shade edge and its exact position in the optical system determine the shape and sharpness of the cutoff. The shade may be lowered by a solenoid actuated pivot to provide low beam, and removed from the light path for high beam. Such optics are known as BiXenon or BiHalogen projectors. If the cutoff shade is fixed in the light path, separate high-beam lamps are required. The condenser lens may have slight fresnel rings or other surface treatments to reduce cutoff sharpness. Modern condenser lenses incorporate optical features specifically designed to direct some light upward towards the locations of retroreflective overhead road signs.|$|E
2500|$|Two {{normalized}} cameras {{project the}} 3D world onto their respective <b>image</b> <b>planes.</b> Let the 3D coordinates {{of a point}} P be [...] and [...] relative to each camera's coordinate system. Since the cameras are normalized, the corresponding image coordinates are ...|$|R
40|$|The {{substitution}} of physical bank check exchange by electronic check image transfer brings agility, security and cost reduction to the clearing system. In this paper, we propose {{a model for}} the electronic representation of bank checks based on the Mixed Raster Content (MRC) model for compression of color and gray-scale images. The binary image is sent first and only if necessary the other MRC <b>image</b> <b>planes</b> are sent to reconstruct the color image of the check. Careful subjective evaluation helped us indicate the best binarization technique and studies led us to use JPEG 2000 to compress the MRC <b>image</b> <b>planes.</b> Furthermore, we propose a new watermarking technique to embed a digital signature into the check image for protection. 1...|$|R
30|$|Artefacts {{caused by}} CS are unpredictable, often not easily {{recognisable}} and of unfamiliar nature. Furthermore, they often appear with predilection {{in certain parts}} of the images or in selected <b>image</b> <b>planes</b> thus resulting in noise enhancement and global ringing [1, 7, 9].|$|R
