90|243|Public
500|$|One of {{the first}} {{consistency}} models was Leslie Lamport's sequential consistency model. Sequential consistency is the property of a parallel program that its parallel execution produces the same results as a sequential program. Specifically, a program is sequentially consistent if [...] "… the results of any execution {{is the same as}} if the operations of all the processors were executed in some sequential order, and the operations of each <b>individual</b> <b>processor</b> appear in this sequence in the order specified by its program".|$|E
5000|$|..... {{the result}} of any {{execution}} {{is the same as}} if the operations of all the processors were executed in some sequential order, and the operations of each <b>individual</b> <b>processor</b> appear in this sequence in the order specified by its program." ...|$|E
5000|$|In {{addition}} to cache state, a directory must track which processors have data {{when in the}} shared state. This is required to for sending invalidation and intervention requests to the <b>individual</b> <b>processor</b> caches which have the cache block in shared state. Few of the popular implementation approaches are: ...|$|E
50|$|Local coscheduling allows <b>individual</b> <b>processors</b> to {{schedule}} the processing independently.|$|R
50|$|Dynamic (or implicit) coscheduling {{is a form}} of coscheduling where <b>individual</b> <b>processors</b> {{can still}} {{schedule}} processing independently, but they make scheduling decisions in cooperation with other processors.|$|R
30|$|Examples of {{positive}} relationships between operators include those wherein cooperatives provide their members with technical assistance or inputs such as feed and labor and those wherein <b>individual</b> <b>processors</b> {{take part in}} farm management decisions or offer services such as milk-quality analysis to their suppliers.|$|R
50|$|In {{the full}} bit vector format, for each {{possible}} cache line in memory, a bit {{is used to}} track whether every <b>individual</b> <b>processor</b> has that line stored in its cache. The full bit vector format is the simplest structure to implement, but the least scalable. The SGI Origin 2000 uses a combination of full bit vector and coarse bit vector depending {{on the number of}} processors.|$|E
5000|$|Initially, memory {{locations}} [...] and [...] both {{hold the}} value [...] The program running on processor #1 loops while {{the value of}} [...] is zero, then it prints the value of [...] The program running on processor #2 stores the value [...] into [...] and then stores the value [...] into [...] Pseudo-code for the two program fragments is shown below. The steps of the program correspond to <b>individual</b> <b>processor</b> instructions.|$|E
5000|$|One of {{the first}} {{consistency}} models was Leslie Lamport's sequential consistency model. Sequential consistency is the property of a parallel program that its parallel execution produces the same results as a sequential program. Specifically, a program is sequentially consistent if [...] "… the results of any execution {{is the same as}} if the operations of all the processors were executed in some sequential order, and the operations of each <b>individual</b> <b>processor</b> appear in this sequence in the order specified by its program".|$|E
40|$|Proposed {{processor}} architecture {{would have}} flexibility of a multi-processor and computational {{power of a}} lockstep array. Using an efficient interconnection network, it accomodates {{a large number of}} <b>individual</b> <b>processors</b> and memory modules. Array architecture would be suitable for very large scientific simulation problems and other applications...|$|R
50|$|Scheduler {{switches}} contexts at equal {{intervals of}} time. This interval {{depends on the}} speed of <b>individual</b> <b>processors,</b> memory-cache hierarchy state and system load. Even on the same processor, under the same load, the interval varies slightly due to minor variations in frequency of the system clock.|$|R
40|$|Utilization bounds for Earliest Deadline First(EDF) and Rate Monotonic(RM) {{scheduling}} {{are known}} and well understood for uniprocessor systems. In this paper, we derive limits on similar bounds for the multiprocessor case, when the <b>individual</b> <b>processors</b> {{need not be}} identical. Tasks are partitioned among the processors and RM scheduling {{is assumed to be}} the policy used in <b>individual</b> <b>processors.</b> A minimum limit on the bounds for a 'greedy' class of algorithms is given and proved, since the actual value of the bound depends on the algorithm that allocates the tasks. We also derive the utilization bound of an algorithm which allocates tasks in decreasing order of utilization factors. Knowledge of such bounds allows us to carry out very fast schedulability tests although we are constrained {{by the fact that the}} tests are sufficient but not necessary to ensure schedulability...|$|R
50|$|In-target probe, or ITP is {{a device}} used in {{computer}} hardware and microprocessor design, {{to control a}} target microprocessor or similar ASIC at the register level. It generally allows full control of the target device and allows the computer engineer access to <b>individual</b> <b>processor</b> registers, program counter, and instructions within the device. It allows the processor to be single-stepped or for breakpoints to be set. Unlike an in-circuit emulator (ICE), an In-Target Probe uses the target device to execute, rather than substituting for the target device.|$|E
5000|$|The {{sequential}} consistency {{model is}} a weaker memory model than strict consistency. A write to a variable {{does not have to}} be seen instantaneously, however, writes to variables by different processors have to be seen in the same order by all processors. As defined by Lamport(1979), Sequential Consistency is met if [...] "the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each <b>individual</b> <b>processor</b> appear in this sequence in the order specified by its program." ...|$|E
50|$|A system {{exhibits}} Processor Consistency if {{the order}} in which other processors see the writes from any <b>individual</b> <b>processor</b> {{is the same as the}} order they were issued. Because of this, Processor Consistency is only applicable to systems with multiple processors. It is weaker than the Causal Consistency model because it does not require writes from all processors to be seen in the same order, but stronger than the PRAM Consistency model because it requires Cache Coherence. Another difference between Causal Consistency and Processor Consistency is that Processor Consistency removes the requirements for loads to wait for stores to complete, and for Write Atomicity. Processor Consistency is also stronger than Cache Consistency because Processor Consistency requires all writes by a processor to be seen in order, not just writes to the same memory location.|$|E
50|$|The 167-processor AsAP 2 chip enables <b>individual</b> <b>processors</b> to make {{extremely}} fast (on {{the order}} of 1-2ns) and locally controlled changes to their own supply voltages. Processors connect their local power grid to either a higher (VddHi) or lower (VddLow) supply voltage, or can be cut off entirely from either grid to dramatically cut leakage power.|$|R
40|$|Amorphous {{computing}} is {{the study}} of programming ultra-scale computing environments of smart sensors and actuators [1]. The individual elements are identical, asynchronous, randomly placed, unreliable, embedded and communicate with a small local neighborhood via wireless broadcast. In such environments, where <b>individual</b> <b>processors</b> have limited resources, aggregating the processors into groups is useful for specialization, increased robustness, and efficient resource allocation...|$|R
50|$|The 36-processor AsAP 1 chip {{is among}} the first {{multi-core}} processor chips to support completely unconstrained clock operation (requiring only that frequencies are below the maximum allowed) including arbitrary changes in frequency, starts, and stops. The 167-processor AsAP 2 chip is the first multi-core processor chip which enables <b>individual</b> <b>processors</b> to make fully unconstrained changes to their own clock frequencies.|$|R
30|$|After {{performing}} gate-level simulation on all our benchmarks, architectures, and optimization combinations {{as described}} in Section 5, we collected power data for <b>individual</b> <b>processor</b> components. We computed the energy of <b>individual</b> <b>processor</b> components using the common formula Energy=Power×Cycles/Frequency. We computed the averages for all benchmarked applications to focus on overall trends, in addition to individual benchmarks.|$|E
40|$|Recent {{trend in}} {{development}} of computer hardware points toward increased parallelism instead of speeding up <b>individual</b> <b>processor</b> cores. This introduces new challenges to developers of computationally intensive software. This includes Bayesian inference and learning that often require extensive computation {{for anything but}} the simplest models...|$|E
40|$|Abstract A {{sea change}} within the {{computing}} discipline occurred last year {{which is not}} widely recognized nor fully understood. <b>Individual</b> <b>processor</b> chips {{are not going to}} be faster in the future. The implications of this fact are discussed and changes to the undergraduate computing curriculum are proposed. ...|$|E
5000|$|Most of {{the early}} massive {{parallel}} processing machines were built out of <b>individual</b> serial <b>processors,</b> including: ...|$|R
40|$|A {{parallel}} processing method of rotating and comparing three-dimensional objects is presented. The multi-processor has an abstract structure {{in which the}} <b>Individual</b> <b>processors</b> are located at the vertices of a geodesic dome. The design has been tested on {{the same type of}} object matching problems as Shepard (3) used in his mental rotation experiments with human subjects. The same linear &quot;reaction time* * behaviour is found*...|$|R
40|$|In this study, a new online task {{assignment}} {{scheme is}} presented for multiprocessor systems where <b>individual</b> <b>processors</b> execute the rate-monotonic scheduling algorithm. The computational {{complexity of the}} task assignment scheme grows linearly {{with the number of}} tasks, and its performance is shown to be signi cantly better than previously existing schemes. The superiority ofourscheme is achieved by anewschedulability condition derived for the rate-monotonic scheduling discipline. ...|$|R
40|$|Graduation date: 1993 An {{experimental}} {{investigation of}} factors effecting scheduling {{a system of}} parallel, non-identical processors using a series of experimental designs was carried out. System variables included were processor capacities relationships, sequencing and assignment rules, job size, and product demand distributions. The effect of the variables was measured by comparing mean flow times, proportion of jobs tardy, and processor utilization spread. Results of {{the study found that}} system loading and set-up times {{play a major role in}} system performance. Grouping jobs by product will minimize set-up times and hence mean flow time and tardiness at the expense of controlling <b>individual</b> <b>processor</b> usage. Factors involving processor capacities and assignment rules tend to have no affect on any of the system performance measures. Variability in job size and product demand tended to give flexibility in controlling <b>individual</b> <b>processor</b> utilization...|$|E
40|$|Some of the {{fundamental}} issues and tradeoffs for distributed execution systems for the Ada language are examined. Steps {{that need to be}} taken to deal with heterogeneity of addressing program objects, of processing resources, and of the <b>individual</b> <b>processor</b> environment are considered. The ways in which program elements can be assigned are examined in the context of four issues: implied remote object access, object visibility and recursive execution, task termination problems, and distributed types...|$|E
40|$|A polynomial-time {{algorithm}} is presented for partitioning {{a collection of}} sporadic tasks among the processors of an identical multiprocessor platform with static-priority scheduling on each <b>individual</b> <b>processor.</b> Since the partitioning problem is easily seen to be NP-hard in the strong sense, this {{algorithm is}} not optimal. A quantitative characterization of its worst-case performance is provided in terms of sufficient conditions and resource augmentation approximation bounds. The partitioning algorithm is also evaluated over randomly generated task systems. ...|$|E
40|$|Program for Analysis and Resizing of Structures (PARS) determines optimum {{resizing}} {{of structures}} subject to stress, displacement, and flutter constraints. Is an efficient code for sizing large- or small-scale finite-element models in presence of strength, thermal, and aeroelastic constraints with minimum and maximum bounds on structural dimensions. PARS {{is composed of}} <b>individual</b> <b>processors</b> that are executed in a logical sequence to perform analysis or synthesis...|$|R
40|$|Multithreading {{has become}} a {{dominant}} paradigm in general purpose MIMD parallel computation. To execute a multithreaded computation on a parallel computer, a scheduler must order and allocate threads to run on the <b>individual</b> <b>processors.</b> The scheduling algorithm dramatically affects both the speedup attained and the space used when executing the computation. We consider the problem of scheduling multithreaded computations to achieve linear speedup without using significantly more space-per-processor than required for a single-processor execution...|$|R
50|$|Several early massively {{parallel}} computers used 1-bit architectures for the processors as well. Examples include the Goodyear MPP and the Connection Machine. By using a 1-bit architecture for the <b>individual</b> <b>processors</b> {{a very large}} array (e.g.: the Connection Machine had 65,536 processors) could be constructed with the chip technology available at the time. In this case the slow computation of a 1-bit processor was traded off against {{the large number of}} processors.|$|R
40|$|Suitable {{parallel}} architectural concepts {{for reducing}} data volume for video signals in orthogonal functional space (transform coding) are presented. This {{is followed by}} their translations into circuit structures as regular as possible for VLSI implementation. The proposed structure is characterized by optimal regularity with short connections between the <b>individual</b> <b>processor</b> units and enables calculations of the Discrete Cosine Transform coefficients in real time for use in HDTV systems with pixel frequencies up to 70 MHz. The device can be implemented in 1. 5 mu m CMOS technology...|$|E
30|$|Overall, Figure  6 {{shows that}} the {{connectivity}} reduction indeed leads to reduction in instruction width and successfully removes {{a large number of}} socket-to-bus connections and that the software bypassing produces large reduction in dynamic register reads and writes as well as large drop in cycle counts, with eventually simpler architectures with a single read and single write port in the register file outperforming much larger architectures with multi-ported register files without bypassing. The combination of these reductions has an effect on the power of <b>individual</b> <b>processor</b> components and results in energy reduction.|$|E
40|$|The use of {{simplified}} error {{models to}} accurately simulate {{and evaluate the}} performance of an optical linear-algebra processor is described. The optical architecture used to perform banded matrix-vector products is reviewed, along with a linear dynamic finite-element case study. The laboratory hardware and ac-modulation technique used are presented. The <b>individual</b> <b>processor</b> error-source models and their simulator implementation are detailed. Several significant simplifications are introduced to ease the computational requirements {{and complexity of the}} simulations. The error models are verified with a laboratory implementation of the processor, and are used to evaluate its potential performance...|$|E
40|$|This paper {{presents}} a general methodology for the efficient parallelization of existing data cubeconstruction algorithms. We describe two different partitioning strategies, one for top-down {{and one for}} bottomup cube algorithms. Both partitioning strategies assign subcubes to <b>individual</b> <b>processors</b> in suchaway that the loads assigned to the processors are balanced. Our methods reduce inter-processor communication overhead by partitioning the load in advance instead of computing each individual group-by in parallel as is done in previous parallel approaches...|$|R
40|$|Conventional {{workload}} distribution {{schemes for}} software distributed shared mem-ory (DSM) systems simply distribute the program threads {{in accordance with}} the CPU power of the <b>individual</b> <b>processors</b> or the data-sharing characteristics of the application. Although these schemes aim to minimize the program execution time by reducing the computation and communication costs, memory access costs also have a major influence on the overall program performance. If a processor has insufficient physical memory space to cache all of the data required by its local working threads, it must perform a se-ries of page replacements if it is to complete its thread executions. Although these page replacements enable the threads to complete their tasks, thread execution is inevitably delayed by the latency of the page swapping operations. Consequently, the current study proposes a novel workload distribution scheme for DSM systems which considers not only the CPU power and data-sharing characteristics, but also the physical memory ca-pabilities of the <b>individual</b> <b>processors.</b> The present results confirm the importance of considering memory resources when establishing an appropriate workload distribution for DSM systems and indicate that the proposed scheme is more effective than schemes which consider only CPU resources or memory resources, respectively...|$|R
40|$|This paper {{addresses}} {{how to meet}} a real-time constraint as {{the load}} on a distributed system increases, without increasing {{the capacity of the}} <b>individual</b> <b>processors.</b> The application we study is a real-time resource counter with a probabilistic correctness criterion, and is motivated by the problem of implementing resource management in the telephone network. We introduce a model that combines distributed computing, real-time constraints, probabilistic correctness, and large system size; as far as we know, no previous work addresses this combination...|$|R
