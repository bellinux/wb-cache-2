10000|10000|Public
5|$|Since 2001, <b>data</b> <b>quality</b> {{improvements}} focussing on {{road and}} address range currency {{has been the}} primary area of updates to the National Geographic Database. It is used by Elections Canada to generate electoral district maps, and by Statistics Canada for census activities. The maintenance costs from the 2000 general election to 2005 was $16.6 million, requiring 34 full-time employees.|$|E
5|$|Paxata is {{a privately}} owned {{software}} company headquartered in Redwood City, California. It develops self-service data preparation software that gets data ready for data analytics software. Paxata's software {{is intended for}} business analysts, as opposed to technical staff. It is used to combine data from different sources, then check it for <b>data</b> <b>quality</b> issues, such as duplicates and outliers. Algorithms and machine learning automate certain aspects of data preparation and users work with the software through a user-interface similar to Excel spreadsheets.|$|E
25|$|Temporal Evaluation of DFAT <b>Data</b> <b>Quality,</b> Levi Smith, IEST/ESTECH 2012, May 2012.|$|E
50|$|Environmental Data Statistics : CPCB manages {{environmental}} data statistic in which air <b>quality</b> <b>data</b> and water <b>quality</b> <b>data</b> comes through. In {{the case of}} air <b>quality</b> <b>data,</b> it measures the level of SO2, NO2, RSPM and SPM. CPCB measure and maintains water <b>quality</b> <b>data</b> as well. <b>Quality</b> level of river and ponds are the major fields which comes under the water <b>quality</b> <b>data</b> criteria.|$|R
40|$|This {{volume of}} the report {{represents}} the technical findings of a study conducted on the ecological makeup of the Trinity River. The data presented includes: ambient toxicity test results and survival rates, United States Geological Survey (USGS) flow <b>data,</b> water <b>quality</b> <b>data,</b> as well as sediment <b>quality</b> <b>data...</b>|$|R
5000|$|Data architecture: The data {{structures}} used by {{a business}} and/or its applications. Descriptions of data in storage and data in motion. Descriptions of data stores, data groups and data items. Mappings of those data artifacts to <b>data</b> <b>qualities,</b> applications, locations etc.|$|R
25|$|Strictly {{standardized}} {{mean difference}} (SSMD) {{has recently been}} proposed for assessing <b>data</b> <b>quality</b> in HTS assays.|$|E
25|$|The mission's post-operations phase will {{continue}} until 2017. The main tasks are consolidation and refinement of instrument calibration, to improve <b>data</b> <b>quality,</b> and data processing, {{to create a}} body of scientifically validated data.|$|E
25|$|Business Intelligence: Introduced in SQL Server 2012 and {{focusing}} on Self Service and Corporate Business Intelligence. It includes the Standard Edition capabilities and Business Intelligence tools: PowerPivot, Power View, the BI Semantic Model, Master Data Services, <b>Data</b> <b>Quality</b> Services and xVelocity in-memory analytics.|$|E
50|$|A data {{architecture}} describes the data structures {{used by a}} business and/or its applications. There are descriptions of data in storage and data in motion; descriptions of data stores, data groups and data items; and mappings of those data artifacts to <b>data</b> <b>qualities,</b> applications, locations etc.|$|R
40|$|ABSTRACT Needs for {{a quality}} {{perspective}} {{in the management}} of data resources are becoming increasingly critical. This paper investigates how to associate <b>data</b> with <b>quality</b> information that can help users make judgments of the <b>quality</b> of <b>data.</b> Specifically, we propose the concept of <b>quality</b> <b>data</b> object and investigate its structure and behavior. The structure of the <b>quality</b> <b>data</b> object includes a description of the datum object, its corresponding quality description object, and a mechanism to associate the datum object with its quality description object. The behavior of the quality object includes a set of methods to measure quality dimensions (such as timeliness, completeness, credibility). In addition, we have developed a <b>quality</b> <b>data</b> object algebra that includes quality comparison methods and an algebra that extends the relational algebra to the <b>quality</b> <b>data</b> object domain. It allows for a systematic construction of retrieval methods for <b>quality</b> <b>data</b> objects. The concept of <b>quality</b> <b>data</b> object presented in the paper is a first step toward the design and manufacture of data products. We envision that the quality dat...|$|R
5000|$|Provide <b>data</b> for <b>quality</b> {{improvements}} to the manufacturing process.|$|R
25|$|A {{statistical}} analysis of misleading data produces misleading conclusions. The issue of <b>data</b> <b>quality</b> can be more subtle. In forecasting for example, {{there is no agreement}} on a measure of forecast accuracy. In the absence of a consensus measurement, no decision based on measurements will be without controversy.|$|E
25|$|A Cochrane review {{concluded}} that aripiprazole {{is similar to}} other typical and atypical antipsychotics with respect to benefit. Compared to typical antipsychotics, there are fewer extrapyramidal side effects, but higher rates of dizziness. With respect to other atypicals, {{it is difficult to}} determine differences in adverse effects as <b>data</b> <b>quality</b> is poor.|$|E
25|$|The primary {{objective}} of the GDDS is to encourage member countries to build a framework to improve <b>data</b> <b>quality</b> and statistical capacity building to evaluate statistical needs, set priorities in improving the timeliness, transparency, reliability and accessibility of financial and economic data. Some countries initially used the GDDS, but later upgraded to SDDS.|$|E
5000|$|Provides <b>data</b> for <b>quality</b> {{assurance}} {{studies and}} shows progress toward expected outcomes.|$|R
40|$|International audienceThis study {{evaluates the}} {{performances}} of a recently introduced 1 mm HR[small mu]MAS NMR probe for metabolic profiling of nanoliter samples. It examines essential NMR criteria such as spectral <b>data</b> <b>qualities</b> and repeatability, and experimental practicality. The report also discusses the difficulties related to the sample preparation in HR[small mu]MAS experiments and considers possible solutions and improvements...|$|R
5000|$|<b>Data</b> quality: <b>quality</b> and {{reliability}} of data is enhanced and data becomes a real asset.|$|R
25|$|The {{application}} of continuous delivery and DevOps to data analytics has been termed DataOps. DataOps seeks to integrate data engineering, data integration, <b>data</b> <b>quality,</b> data security, and data privacy with operations. It applies principles from DevOps, Agile Development and the statistical process control, used in lean manufacturing, {{to improve the}} cycle time of extracting value from data analytics.|$|E
25|$|A {{performance}} based {{quality assurance}} process (QA) {{is used by}} the Mussel Watch Program to maintain <b>data</b> <b>quality.</b> The National Institute of Standards and Technology (NIST) and the National Research Council of Canada (NRC) assist analytic laboratories in exercises to ensure that data collected from all labs have comparable accuracy and precision. The QA process reduces intralaboratory and interlaboratory variation.|$|E
25|$|High-quality HTS assays are {{critical}} in HTS experiments. The development of high-quality HTS assays requires {{the integration of}} both experimental and computational approaches for quality control (QC). Three important means of QC are (i) good plate design, (ii) the selection of effective positive and negative chemical/biological controls, and (iii) the development of effective QC metrics to measure the degree of differentiation so that assays with inferior <b>data</b> <b>quality</b> can be identified.|$|E
30|$|One of the {{peculiarity}} of the Energy ADE is its flexibility: {{as it must}} cover different (energy) application scenarios, it {{must deal}} with heterogeneous <b>data</b> <b>qualities</b> and modelling paradigms. For simplified energy assessments, a building modelled as single-zone with simplified thermal properties (U-values, g-values, etc.) might suffice, but for more complex dynamic simulations, multi-zone models with detailed geometries, layered constructions and complex occupancy schedules are required.|$|R
40|$|Water <b>quality</b> <b>data</b> {{are often}} {{collected}} at different sites {{over time to}} improve water quality management. Water <b>quality</b> <b>data</b> usually exhibit the following characteristics: non-normal distribution, presence of outliers, missing values, values below detection limits (censored), and serial dependence. It is essential to apply appropriate statistical methodology when analyzing water <b>quality</b> <b>data</b> to draw valid conclusions and hence provide useful advice in water management. In this chapter, we will provide and demonstrate various statistical tools for analyzing such water <b>quality</b> <b>data,</b> and will also introduce {{how to use a}} statistical software R to analyze water <b>quality</b> <b>data</b> by various statistical methods. A dataset collected from the Susquehanna River Basin will be used to demonstrate various statistical methods provided in this chapter. The dataset can be downloaded from website [URL]...|$|R
50|$|<b>Data</b> {{resource}} <b>quality</b> is {{a measure}} of how well the organization's data resource supports the current and the future business information demand of the organization. The data resource cannot support just the current business information demand while sacrificing the future business information demand. It must support both the current and the future business information demand. The ultimate <b>data</b> resource <b>quality</b> is stability across changing business needs and changing technology.|$|R
25|$|Effective {{analytic}} QC methods {{serve as}} a gatekeeper for excellent quality assays. In a typical HTS experiment, {{a clear distinction between}} a positive control and a negative reference such as a negative control is an index for good quality. Many quality-assessment measures have been proposed to measure the degree of differentiation between a positive control and a negative reference. Signal-to-background ratio, signal-to-noise ratio, signal window, assay variability ratio, and Z-factor have been adopted to evaluate <b>data</b> <b>quality.</b>|$|E
25|$|The {{majority}} of issues with tissue residue {{arise in the}} interpretation of tissue residue data. Interpretation complication can be caused by choice of endpoints, species choice, life stage sensitivity, <b>data</b> <b>quality,</b> and toxicity data extrapolation. Choice of tissue for tissue residue analysis can also be difficult and has an effect on tissue residue data. When choosing tissue, a scientist needs to consider: mode and mechanism of action of chemical being tested, site of toxic action for the chemical and species combination being studied and strength of the tissue residue-response relationship. There is also a lack of reliable tissue residue relationships available for comparison.|$|E
25|$|In 2011 {{the level}} of non-revenue water was {{estimated}} to be 32% in Sana'a, 33% in Aden, 22% in Taiz and 35% in Mukalla. This is an improvement compared to previous levels. In 2001 non-revenue water {{was estimated to be}} around 50 percent. According to the joint annual review of the water and sanitation sector for 2007, average non-revenue water was down to 28%. In Sana'a non-revenue water had declined from about 50% in 1999 to an estimated 38% in 2007. In 2007, among the larger utilities the lowest level was achieved in Ibb with 20% and the highest in Hodeidah with 43%. The authors of the report caution that the <b>data</b> <b>quality</b> may be poor.|$|E
5000|$|The Earthscope USArray Array Network Facility (ANF): Metadata, Network and <b>Data</b> Monitoring, <b>Quality</b> Assurance as We Start to Roll (2008) ...|$|R
30|$|To {{study the}} {{temporal}} variation in groundwater {{quality of the}} study area, water <b>quality</b> <b>data</b> have been collected from Water Testing Laboratory, Public Health Division (PWD), Puducherry for the year 2004 and 2008 for which the complete dataset is available. The water <b>quality</b> <b>data</b> collected include pH, EC, TDS, Na, K, Ca, Mg and Cl. The average rainfall and water <b>quality</b> <b>data</b> of the year 2004 and 2008 have been tabulated in the TableÂ  7.|$|R
5000|$|The Earthscope USArray Array Network Facility (ANF): Metadata, Network and <b>Data</b> Monitoring, <b>Quality</b> Assurance During the Second Year of Operations (2005) ...|$|R
500|$|The {{software}} has a spreadsheet-based user interface. Patterns and anomalies in {{the data}} are color-coded in the spreadsheet. Then users are provided with {{instructions on how to}} resolve <b>data</b> <b>quality</b> issues or to supplement the data with contextual information. Data sets and related quality issues can also be addressed in a collaborative environment through the [...] "Paxata Share" [...] feature. It runs on Apache Spark.|$|E
500|$|In its 2014 report [...] "Cool Vendors in Data Integration and Data Quality", Gartner praised Paxata for {{developing}} a [...] "business-user-friendly" [...] <b>data</b> <b>quality</b> product that does not use code. Ventana Research said its spreadsheet-based user interface [...] "should resonate well with business analysts," [...] who are resistant {{to move away from}} familiar Excel-like programs. Gartner also said Paxata was recognized in the report due to its automated, algorithm-based features and how it tracks any changes made to the data.|$|E
500|$|Paxata {{refers to}} its suite of {{cloud-based}} <b>data</b> <b>quality,</b> integration, enrichment and governance products as [...] "Adaptive Data Preparation." [...] The software {{is intended for}} business analysts, who need to combine data {{from a variety of}} sources, then check the data for duplicates, empty fields, outliers, trends and integrity issues before conducting analysis or visualization in a third-party software tool. It uses algorithms and machine-learning to automate certain aspects of data preparation. For example, it may automatically detect records belonging to the same person or address, even if the information is formatted differently in each record in different data sets.|$|E
5000|$|The CERTAIN Collaborative. Preparing Electronic Clinical <b>Data</b> for <b>Quality</b> Improvement and Comparative Effectiveness Research: The SCOAP CERTAIN Automation and Validation Project. eGEMs. 2013 Oct; 1(1):16.|$|R
40|$|Workshop Extended Abstracts, Geological Society of America Annual Meeting, Denver, Colorado-October 2013. This {{workshop}} {{is designed}} for those constructing 3 D geological maps and numerical models. Our objective is to bring together people dealing with large datasets, and who must integrate variable <b>quality</b> <b>data</b> with high <b>quality</b> <b>data</b> to construct 3 D geological models for application such as hydrogeology, engineering, and energy resource assessment. Topics include (1) methods of model construction, (2) managing diverse <b>data</b> of variable <b>quality,</b> (3) ensuring <b>data</b> interoperability, (4) visualization tools, and (5) interaction between mappers, hydrogeologists, energy and mineral resource geologists, engineering geologists, and engineers. The emphasis is on deposits that host potable groundwater, as well as sedimentary basins as a whole. Geological Society of Americ...|$|R
40|$|<b>Data</b> <b>Qualities</b> (DQ) issues {{become a}} major concern over decade. Even with an {{enhancements}} and introduction of new technologies, it is stills not good if systems are lack of <b>qualities</b> of <b>data.</b> Data warehouses (DW) are complex systems that have to deliver highly- aggregated, high <b>quality</b> <b>data</b> from heterogeneous sources to decision makers. It involves a lot of integration of sources system to support business operations. This paper propose a framework for implementing DQ in DW systems architecture using Metadata Analysis Technique and Base Analysis Techniques to perform comparison between target value and current value gain from the systems. A prototype using PHP is develops to support Base Analysis Techniques. This paper also emphasize on dimension need to be consider to perform DQ processes...|$|R
