2632|2|Public
25|$|A {{special case}} of {{generalized}} least squares called weighted least squares occurs {{when all the}} off-diagonal entries of Ω (the correlation matrix of the residuals) are null; the variances of the observations (along the covariance matrix diagonal) may still be unequal (<b>heteroscedasticity).</b>|$|E
25|$|Homoscedasticity: , {{which means}} that the error term has the same {{variance}} σ2 in each observation. When this requirement is violated this is called <b>heteroscedasticity,</b> in such case a more efficient estimator would be weighted least squares. If the errors have infinite variance then the OLS estimates will also have infinite variance (although by the law of large numbers they will nonetheless tend toward the true values so long as the errors have zero mean). In this case, robust estimation techniques are recommended.|$|E
2500|$|When [...] is the {{identity}} matrix (such {{that there is}} no correlation or <b>heteroscedasticity),</b> the model is called independent probit.|$|E
2500|$|Residuals {{against the}} {{explanatory}} {{variables in the}} model. A non-linear relation between these variables suggests that the linearity of the conditional mean function may not hold. [...] Different levels of variability in the residuals for different levels of the explanatory variables suggests possible <b>heteroscedasticity.</b>|$|E
2500|$|Various {{models have}} been created that allow for <b>heteroscedasticity,</b> i.e. the errors for {{different}} response variables may have different variances. [...] For example, weighted least squares is a method for estimating linear regression models when the response variables may have different error variances, possibly with correlated errors. (See also Weighted linear least squares, and Generalized least squares.) Heteroscedasticity-consistent standard errors is an improved method for use with uncorrelated but potentially heteroscedastic errors.|$|E
2500|$|When {{only one}} {{dependent}} variable is being modeled, a scatterplot will suggest {{the form and}} strength {{of the relationship between}} the dependent variable and regressors. It might also reveal outliers, <b>heteroscedasticity,</b> and other aspects of the data that may complicate the interpretation of a fitted regression model. [...] The scatterplot suggests that the relationship is strong and can be approximated as a quadratic function. OLS can handle non-linear relationships by introducing the regressor HEIGHT2. [...] The regression model then becomes a multiple linear model: ...|$|E
2500|$|Constant {{variance}} (a.k.a. homoscedasticity). [...] This {{means that}} different {{values of the}} response variable have the same variance in their errors, regardless {{of the values of}} the predictor variables. In practice this assumption is invalid (i.e. the errors are heteroscedastic) if the response variable can vary over a wide scale. In order to check for heterogeneous error variance, or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a [...] "fanning effect" [...] between residual error and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predictive variables. Errors will not be evenly distributed across the regression line. <b>Heteroscedasticity</b> will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear regression line, and the mean squared error for the model will be wrong. Typically, for example, a response variable whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000 (a standard deviation of around $20,000), while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, which would imply their actual income would vary anywhere between -$10,000 and $30,000. (In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression estimation methods give less precise parameter estimates and misleading inferential quantities such as standard errors when substantial <b>heteroscedasticity</b> is present. However, various estimation techniques (e.g. weighted least squares and heteroscedasticity-consistent standard errors) can handle <b>heteroscedasticity</b> in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g. fit the logarithm of the response variable using a linear regression model, which implies that the response variable has a log-normal distribution rather than a normal distribution).|$|E
50|$|The econometrician Robert Engle won the 2003 Nobel Memorial Prize for Economics for {{his studies}} on {{regression}} {{analysis in the}} presence of <b>heteroscedasticity,</b> which led to his formulation of the autoregressive conditional <b>heteroscedasticity</b> (ARCH) modeling technique.|$|E
50|$|There {{are several}} methods {{to test for}} the {{presence}} of <b>heteroscedasticity.</b> Although tests for <b>heteroscedasticity</b> between groups can formally be considered as a special case of testing within regression models, some tests have structures specific to this case.|$|E
5000|$|Regressions with <b>heteroscedasticity</b> and serial-correlation correction.|$|E
5000|$|Heteroscedasticity-consistent {{standard}} errors (HCSE), while still biased, improve upon OLS estimates. [...] HCSE is a consistent estimator of {{standard errors}} in regression models with <b>heteroscedasticity.</b> This method corrects for <b>heteroscedasticity</b> without altering {{the values of}} the coefficients. This method may be superior to regular OLS because if <b>heteroscedasticity</b> is present it corrects for it, however, if the data is homoscedastic, the standard errors are equivalent to conventional standard errors estimated by OLS. Several modifications of the White method of computing heteroscedasticity-consistent standard errors have been proposed as corrections with superior finite sample properties.|$|E
5000|$|There {{are four}} common {{corrections}} for <b>heteroscedasticity.</b> They are: ...|$|E
50|$|White test — a {{test for}} whether <b>heteroscedasticity</b> is present.|$|E
5000|$|Weighting {{algorithms}} {{are provided}} {{to optimize the}} curve fit and account for <b>heteroscedasticity</b> ...|$|E
50|$|<b>Heteroscedasticity</b> often {{occurs when}} there is a large {{difference}} among the sizes of the observations.|$|E
50|$|Step 3: Select the {{equation}} {{with the highest}} R2 and lowest standard errors to represent <b>heteroscedasticity.</b>|$|E
5000|$|In {{regression}} analysis, <b>heteroscedasticity</b> {{refers to}} unequal variances of the random error terms εi, such that ...|$|E
5000|$|The GARCH-in-mean (GARCH-M) model adds a <b>heteroscedasticity</b> term {{into the}} mean equation. It has the specification: ...|$|E
50|$|Step 2: Regress the {{absolute}} value |ei| on the explanatory variable {{that is associated with}} the <b>heteroscedasticity.</b>|$|E
5000|$|For any {{non-linear}} model (for instance Logit and Probit models), however, <b>heteroscedasticity</b> has more severe consequences: the maximum likelihood {{estimates of the}} parameters will be biased (in an unknown direction), as well as inconsistent (unless the likelihood function is modified to correctly {{take into account the}} precise form of <b>heteroscedasticity).</b> As pointed out by Greene, “simply computing a robust covariance matrix for an otherwise inconsistent estimator does not give it redemption.” ...|$|E
50|$|Under certain assumptions, the OLS {{estimator}} has {{a normal}} asymptotic distribution when properly normalized and centered (even when the data {{does not come}} from a normal distribution). This result is used to justify using a normal distribution, or a chi square distribution (depending on how the test statistic is calculated), when conducting a hypothesis test. This holds even under <b>heteroscedasticity.</b> More precisely, the OLS estimator in the presence of <b>heteroscedasticity</b> is asymptotically normal, when properly normalized and centered, with a variance-covariance matrix that differs from the case of homoscedasticity. In 1980, White proposed a consistent estimator for the variance-covariance matrix of the asymptotic distribution of the OLS estimator. This validates the use of hypothesis testing using OLS estimators and White's variance-covariance estimator under <b>heteroscedasticity.</b>|$|E
50|$|<b>Heteroscedasticity</b> {{is also a}} major {{practical}} issue {{encountered in}} ANOVA problems.The F test can still be used in some circumstances.|$|E
5000|$|When [...] is the {{identity}} matrix (such {{that there is}} no correlation or <b>heteroscedasticity),</b> the model is called independent probit.|$|E
50|$|Because <b>heteroscedasticity</b> {{concerns}} {{expectations of}} the second moment of the errors, its presence {{is referred to as}} misspecification of the second order.|$|E
5000|$|It {{is easy to}} {{see many}} kinds of {{failures}} of the model or violations of the underlying assumptions (nonlinearity, <b>heteroscedasticity,</b> unusual patterns)[...]|$|E
50|$|The {{existence}} of <b>heteroscedasticity</b> {{is a major}} concern in the application of regression analysis, including the analysis of variance, as it can invalidate statistical tests of significance that assume that the modelling errors are uncorrelated and uniform—hence that their variances do not vary with the effects being modeled. For instance, while the ordinary least squares estimator is still unbiased in the presence of <b>heteroscedasticity,</b> it is inefficient because the true variance and covariance are underestimated. Similarly, in testing for differences between sub-populations using a location test, some standard tests assume that variances within groups are equal.|$|E
5000|$|The {{study of}} <b>heteroscedasticity</b> has been {{generalized}} to the multivariate case, {{which deals with}} the covariances of vector observations instead of the variance of scalar observations. One version {{of this is to}} use covariance matrices as the multivariate measure of dispersion. Several authors have considered tests in this context, for both regression and grouped-data situations. Bartlett's test for <b>heteroscedasticity</b> between grouped data, used most commonly in the univariate case, has also been extended for the multivariate case, but a tractable solution only exists for 2 groups. Approximations exist for more than two groups, and they are both called Box's M test ...|$|E
50|$|In econometrics, the Park test is a {{test for}} <b>heteroscedasticity.</b> The test {{is based on the}} method {{proposed}} by Rolla Edward Park for estimating linear regression parameters in the presence of heteroscedastic error terms.|$|E
5000|$|In statistics, {{the theory}} of minimum norm {{quadratic}} unbiased estimation (MINQUE) was developed by C.R. Rao. Its application was originally {{to the problem of}} <b>heteroscedasticity</b> and the estimation of variance components in random effects models.|$|E
5000|$|Integrated Generalized Autoregressive Conditional <b>heteroscedasticity</b> I GARCH is a {{restricted}} {{version of the}} GARCH model, where the persistent parameters sum up to one, and imports a unit root in the GARCH process. The condition for this is ...|$|E
5000|$|Mean-independence: [...] {{the errors}} are mean-zero for every {{value of the}} latent regressor. This is a less {{restrictive}} assumption than the classical one, as it allows {{for the presence of}} <b>heteroscedasticity</b> or other effects in the measurement errors.|$|E
50|$|A {{special case}} of {{generalized}} least squares called weighted least squares occurs {{when all the}} off-diagonal entries of Ω (the correlation matrix of the residuals) are null; the variances of the observations (along the covariance matrix diagonal) may still be unequal (<b>heteroscedasticity).</b>|$|E
5000|$|It {{is assumed}} that E(εi) = 0. The above {{variance}} varies with i, or the ith trial in an experiment or the ith case or observation in a dataset. Equivalently, <b>heteroscedasticity</b> refers to unequal conditional variances in the response variables Yi, such that ...|$|E
5000|$|Residuals {{against the}} {{explanatory}} {{variables in the}} model. A non-linear relation between these variables suggests that the linearity of the conditional mean function may not hold. Different levels of variability in the residuals for different levels of the explanatory variables suggests possible <b>heteroscedasticity.</b>|$|E
50|$|Whereas GLS is more {{efficient}} than OLS under <b>heteroscedasticity</b> or autocorrelation, {{this is not true}} for FGLS. The feasible estimator is, provided the errors covariance matrix is consistently estimated, asymptotically {{more efficient}}, but for a small or medium size sample, it can be actually less efficient than OLS. This is why, some authors prefer to use OLS, and reformulate their inferences by simply considering an alternative estimator for the variance of the estimator robust to <b>heteroscedasticity</b> or serial autocorrelation.But for large samples FGLS is preferred over OLS under heteroskedasticity or serial correlation. A cautionary note is that the FGLS estimator is not always consistent. One case in which FGLS might be inconsistent is if there are individual specific fixed effects.|$|E
50|$|In summary, {{to ensure}} {{efficient}} inference {{of the regression}} parameters and the regression function, the <b>heteroscedasticity</b> must be accounted for. Variance functions quantify {{the relationship between the}} variance and the mean of the observed data and hence {{play a significant role in}} regression estimation and inference.|$|E
