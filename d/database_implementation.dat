138|428|Public
5000|$|Physical : Models all the {{information}} necessary for <b>database</b> <b>implementation</b> ...|$|E
50|$|HSQLDB - Lightweight Java <b>database</b> <b>implementation</b> {{used for}} storage of {{configuration}} data and internal user database (when used).|$|E
5000|$|... "An Extended Model for <b>Database</b> <b>Implementation</b> (P. De and C. Kriebel), Information Systems, Vol. 7, No. 7, 1982, pp. 139-145.|$|E
5000|$|<b>Database</b> <b>implementations</b> {{typically}} store {{variable length}} fields in ways such as ...|$|R
5000|$|Document <b>database</b> <b>implementations</b> offer {{a variety}} of ways of {{organizing}} documents, including notions of ...|$|R
5000|$|Most SQL <b>database</b> <b>implementations</b> {{extend their}} SQL {{capabilities}} by providing imperative, i.e. procedural languages. Examples {{of these are}} Oracle's PL/SQL and DB2's SQL_PL.|$|R
5000|$|However, {{the basic}} terms [...] "record" [...] and [...] "field" [...] {{are used in}} nearly every flat file <b>database</b> <b>implementation.</b>|$|E
50|$|The central {{concept of}} a document-oriented {{database}} {{is the notion of}} a document. While each document-oriented <b>database</b> <b>implementation</b> differs on the details of this definition, in general, they all assume documents encapsulate and encode data (or information) in some standard format or encoding. Encodings in use include XML, YAML, JSON, and BSON, as well as binary forms like PDF and Microsoft Office documents (MS Word, Excel, and so on).|$|E
5000|$|An ER {{model is}} {{typically}} implemented as a database. In a simple relational <b>database</b> <b>implementation,</b> each row {{of a table}} represents one instance of an entity type, and each field in a table represents an attribute type. In a relational database a relationship between entities is implemented by storing the primary key of one entity as a pointer or [...] "foreign key" [...] in the table of another entity ...|$|E
40|$|International audienceInformation {{retrieval}} from textual data {{focuses on}} the construction of vocabularies that contain weighted term tuples. Such vocabularies can then be exploited by various text analysis algorithms to extract new knowledge, e. g., top-k keywords, top-k documents, etc. Top-k keywords are casually used for various purposes, are often computed on-the-fly, and thus must be efficiently computed. To compare competing weighting schemes and <b>database</b> <b>implementations,</b> benchmarking is customary. To the best of our knowledge, no benchmark currently addresses these problems. Hence, in this paper, we present a top-k keywords benchmark, T²K², which features a real tweet dataset and queries with various complexities and selectivities. T²K² helps evaluate weighting schemes and <b>database</b> <b>implementations</b> in terms of computing performance. To illustrate T²K²'s relevance and genericity, we successfully performed tests on the TF-IDF and Okapi BM 25 weighting schemes, on one hand, and on different relational (Oracle, PostgreSQL) and document-oriented (MongoDB) <b>database</b> <b>implementations,</b> on the other hand...|$|R
40|$|Information {{retrieval}} from textual data {{focuses on}} the construction of vocabularies that contain weighted term tuples. Such vocabularies can then be exploited by various text analysis algorithms to extract new knowledge, e. g., top-k keywords, top-k documents, etc. Top-k keywords are casually used for various purposes, are often computed on-the-fly, and thus must be efficiently computed. To compare competing weighting schemes and <b>database</b> <b>implementations,</b> benchmarking is customary. To the best of our knowledge, no benchmark currently addresses these problems. Hence, in this paper, we present a top-k keywords benchmark, T${}^ 2 $K${}^ 2 $, which features a real tweet dataset and queries with various complexities and selectivities. T${}^ 2 $K${}^ 2 $ helps evaluate weighting schemes and <b>database</b> <b>implementations</b> in terms of computing performance. To illustrate T${}^ 2 $K${}^ 2 $'s relevance and genericity, we successfully performed tests on the TF-IDF and Okapi BM 25 weighting schemes, on one hand, and on different relational (Oracle, PostgreSQL) and document-oriented (MongoDB) <b>database</b> <b>implementations,</b> on the other hand...|$|R
5|$|This is {{complicated}} by the fact that in some database interface programs (or even <b>database</b> <b>implementations</b> like Oracle's), NULL is reported as an empty string, and empty strings may be incorrectly stored as NULL.|$|R
50|$|TnFOX {{optionally}} includes {{copies of}} the OpenSSL library and the SQLite library in order to implement its strong encryption and its default SQL <b>database</b> <b>implementation</b> respectively. It can be built modularly {{as a set of}} separate DLLs or monolithically. It also has full portable support for host operating system ACL security and knows how to protect sensitive data from entering the swap file, plus automatically shredding any deleted portions. It can access which user is running the process and how to escalate its privileges as necessary.|$|E
50|$|A {{physical}} data model (or database design) is {{a representation of}} a data design as implemented, or intended to be implemented, in a database management system. In the lifecycle of a project it typically derives from a logical data model, {{though it may be}} reverse-engineered from a given <b>database</b> <b>implementation.</b> A complete {{physical data}} model will include all the database artifacts required to create relationships between tables or to achieve performance goals, such as indexes, constraint definitions, linking tables, partitioned tables or clusters. Analysts can usually use a physical data model to calculate storage estimates; it may include specific storage allocation details for a given database system.|$|E
50|$|The CADM was {{initially}} published in 1997 as a logical data model for architecture data. It was revised in 1998 {{to meet all}} {{the requirements of the}} C4ISR Architecture Framework Version 2.0.1 As a logical data model, the initial CADM provided a conceptual view of how architecture information is organized. It identified and defined entities, attributes, and relations. The CADM has evolved since 1998, so that it now has a physical view providing the data types, abbreviated physical names, and domain values that are needed for a <b>database</b> <b>implementation.</b> Because the CADM is also a physical data model, it constitutes a database design and can be used to automatically generate databases.|$|E
5000|$|Transactions are {{available}} in most SQL <b>database</b> <b>implementations,</b> though with varying levels of robustness. (MySQL, for example, began supporting transactions from version 5.5, with the switch to the InnoDB storage engine. The previously used storage engine, MyISAM did not support transactions.) ...|$|R
50|$|The file's name {{originates}} {{from one}} of its initial functions as it contained the data used to verify passwords of user accounts. However, on modern Unix systems the security-sensitive password information is instead often stored in a different file using shadow passwords, or other <b>database</b> <b>implementations.</b>|$|R
5000|$|Each edition has {{separate}} database libraries, {{despite the}} common branding. The {{first is the}} traditional Berkeley DB, written in C. It contains several <b>database</b> <b>implementations,</b> including a B-tree and one built around extensible hashing. It supports multiple language bindings, including C/C++, Java (via JNI), C# [...]NET, Perl and Python.|$|R
50|$|Overhead: Because stored {{procedure}} {{statements are}} stored {{directly in the}} database, they may remove {{all or part of}} the compiling overhead that is typically needed in situations where software applications send inline (dynamic) SQL queries to a database. (However, most database systems implement statement caches and other methods to avoid repetitively compiling dynamic SQL statements.) Also, while they avoid some pre-compiled SQL, statements add to the complexity of creating an optimal execution plan because not all arguments of the SQL statement are supplied at compile time. Depending on the specific <b>database</b> <b>implementation</b> and configuration, mixed performance results will be seen from stored procedures versus generic queries or user defined functions.|$|E
5000|$|The central {{concept of}} a {{document}} store {{is the notion of}} a [...] "document". While each document-oriented <b>database</b> <b>implementation</b> differs on the details of this definition, in general, they all assume that documents encapsulate and encode data (or information) in some standard formats or encodings. Encodings in use include XML, YAML, and JSON as well as binary forms like BSON. Documents are addressed in the database via a unique key that represents that document. One of the other defining characteristics of a document-oriented database is that in addition to the key lookup performed by a key-value store, the database offers an API or query language that retrieves documents based on their contents.|$|E
5000|$|... ● Steve Marshall {{found the}} Federal Circuits efforts at {{harmonization}} a failure {{and saw the}} two opinions as addressing similar technologies but treating them disparately:Despite the attempted harmonizing discussion of Enfish in TLI, the latter exposes several inconsistencies between the opinions as well as potential flaws in the reasoning of Enfish. As an initial matter, the Federal Circuits descriptions of the claimed technologies {{in each of the}} opinions share similarities in areas that impacted the legal analysis. Each involved a <b>database</b> <b>implementation</b> on a commodity computer. Also, the benefits of each purport to include increased search speed and dynamic configuration of data files. Additionally, the disclosure of each was largely functional with little to no emphasis on new physical components.|$|E
50|$|GeoSciML is {{intended}} for use by data portals publishing data for customers in GeoSciML, for interchanging data between organisations that use different <b>database</b> <b>implementations</b> and software/systems environments, and in particular, for use in geoscience web services. In this way, GeoSciML allows applications to utilize globally distributed geoscience data and information.|$|R
5000|$|GeoInformatics Focus area: Geo-spatial Data Management, Data Warehousing, <b>Database</b> System <b>Implementation</b> ...|$|R
25|$|Both {{computer}} {{hardware and software}} also use internal representations which are effectively decimal for storing decimal values and doing arithmetic. Often this arithmetic is done on data which are encoded using some variant of binary-coded decimal, especially in <b>database</b> <b>implementations,</b> {{but there are other}} decimal representations in use (such as in the new IEEE 754 Standard for Floating-Point Arithmetic).|$|R
50|$|She {{played a}} {{fundamental}} {{role in the}} development of System R, a pioneering relational <b>database</b> <b>implementation,</b> and wrote the canonical paper on relational query optimization. She is a pioneer in relational database management and inventor of the technique of cost-based query optimization. She was a key member of the original System R team that created the first relational database research prototype. The dynamic programming algorithm for determining join order proposed in that paper still forms the basis for most of the query optimizers used in modern relational systems. She also established and led IBM’s Database Technology Institute, considered one of the most successful examples of a fast technology pipeline from research to development and personally has technical contributions in the areas of database optimization, data parallelism, distributed data, and unstructured data management. Before her retirement from IBM, she was the Vice President of Data Management Architecture and Technology at IBM.|$|E
30|$|We {{thank all}} staffs of CVL Hospitals {{who contributed to}} the Hospital Discharge <b>Database</b> <b>implementation.</b> We {{would like to thank}} Kimberly A. Barker for the English editing.|$|E
3000|$|All {{resources}} of the <b>database</b> <b>implementation</b> of the Energy ADE (DDL procedures, documentation, test data, etc.) are already freely available and can be downloaded from GitHub. 12 [...]...|$|E
40|$|Effective {{querying}} {{and managing}} of temporal databases represent an unanswered {{challenge to the}} modern research community. In this paper, we introduce a temporal preprocessor {{that can be used}} to aid in creation of temporal applications and for testing various methods and approaches in temporal <b>databases</b> <b>implementations.</b> We outline our proposal, current results and directions for further research and development. ...|$|R
5000|$|Amazon {{published}} {{the paper on}} Dynamo, but never released their implementation. The index layer of Amazon S3 implements and extends many of the core features of a Dynamo. Since then, several implementations have been created based on the paper. The paper also inspired many other NoSQL <b>database</b> <b>implementations.</b> [...] Here are some projects that implement it or were inspired by it.|$|R
5000|$|Some <b>database</b> <b>implementations</b> {{adopted the}} term [...] "Upsert" [...] (a {{portmanteau}} of update and insert) to a database statement, or combination of statements, that inserts a record {{to a table}} in a database if the record does not exist or, if the record already exists, updates the existing record. It {{is also used to}} abbreviate the [...] "MERGE" [...] equivalent pseudo-code.|$|R
40|$|This paper {{describes}} {{the implementation of}} the hidrography category information the conceptual model, about the Technical Specification for the Structuring of Data Vectorial Geospatial (ET-EDGV), in a spatial relational database management system. The physical modelling of the spatial-temporal database was developed with the Sybase PowerDesigner CASE tool. The spatial-temporal <b>database</b> <b>implementation</b> was done using Structured Query Language. The contribution of this work is the spatial-temporal <b>database</b> <b>implementation,</b> hidrography category information from ET-EDGV version 2. 1. 3, for the spatial relational database management system PostgreSQL version 9. 1 and PostGIS 2. 0. It was concluded that ET-EDGV adoption enables the geographical data standardization and normalization stores in the spatial relational database management system. Pages: 2471 - 247...|$|E
40|$|This thesis {{suggests}} an improved parameter <b>database</b> <b>implementation</b> {{for one of}} Ericsson products. The parameter database is used during the initialization of the system as well as during the later operation. The database size is constantly growing because the parameter database {{is intended to be}} used with different hardware configurations. When a new technology platform is released, multiple revisions with additional features and functionalities are later created, resulting in introduction of new parameters or changes to their values. Ericsson provides support for old and new products. The entire parameter database is currently stored in DRAM memory as a hash map. Therefore an optimal parameter <b>database</b> <b>implementation</b> should have low memory footprint. The search speed and initialization speed for the target system are also important to allow high system availability and low downtime, since a reboot is a common fix for many problems. As many optimizations have to consider memory size – speed tradeoff, it has been decided to give preference to reducing memory footprint. This research seeks to: Analyze data-structures suitable for parameter <b>database</b> <b>implementation</b> (Hash map, Sparsehash, Judy hash, Binary search tree, Treap, Skip List, AssocVector presorted using std::map, Burst trie). Propose the best data-structure in terms of used memory area and speed. If possible, further optimize it for database size in memory and access speed. Create a prototype implementation. Test the performance of the new implementation. The results indicate that a more compact <b>database</b> <b>implementation</b> can be achieved using alternative data structures such as Presorted AssocVector an Sparsehash, however some search speed and build speed is lost when using these data structures instead of the original Gnu Hash Map implementation...|$|E
40|$|This paper {{presents}} a new query algebra based on fold iterations that facilitates <b>database</b> <b>implementation.</b> An algebraic normalization algorithm is introduced that reduces any program {{expressed in this}} algebra to a canonical form that generates no intermediate data structures and has no more nested iterations than the initial program. Given any inductive data type, our system can automatically synthesize {{the definition of the}} fold operator that traverses instances of this type, and, more importantly, it can produce the necessary transformations for optimizing expressions involving this fold operator. <b>Database</b> <b>implementation</b> in our framework is controlled by userdefined mappings from abstract types to physical structures. The optimizer uses this information to translate abstract programs and queries into concrete algorithms that conform to the type transformation. Database query optimization {{can be viewed as a}} search over the reduced space of all canonical forms which are equivalent to t [...] ...|$|E
5000|$|Widom, Jennifer; (with H. Garcia-Molina and J.D. Ullman). <b>Database</b> System <b>Implementation,</b> New Jersey: Prentice-Hall, 2000.|$|R
50|$|One {{reasonable}} hypothesis {{for this}} data model lasting 50 years, with new <b>database</b> <b>implementations</b> {{of the model}} even {{in the 21st century}} is that it provides inexpensive database solutions. Historically, with industry benchmarks tied to SQL transactions, this has been a difficult hypothesis to test, although there are considerable anecdotes of failed attempts to get the functionality of a MultiValue application into a relational database framework.|$|R
40|$|The {{need for}} {{databases}} {{occurs in the}} moment when takes place an informatics system development. Moreover, databases are {{an important step in}} this process. For this reason, this paper deals with object-relational <b>databases</b> <b>implementation</b> as part of informatics systems development. The practical implementation is made on a decision support system (DSS) prototype, which can be applied in the uncertain and unpredictable environments, like the production and the prediction of the wind energ...|$|R
