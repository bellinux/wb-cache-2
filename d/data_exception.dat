7|299|Public
5000|$|The {{most common}} were ABEND 0C7 (<b>Data</b> <b>exception)</b> and ABEND 0CB (Division by zero) ...|$|E
50|$|The decimal {{arithmetic}} feature provides {{instructions that}} operate on packed decimal data. A packed decimal number has 1-31 decimal digits {{followed by a}} 4-bit sign. All of the decimal arithmetic instructions except PACK and UNPACK generate a <b>Data</b> <b>exception</b> if a digit {{is not in the}} range 0-9 or a sign is not in the range A-F.|$|E
40|$|Abstract. The key {{technology}} in monitoring and alarming was investigated, aiming at the <b>data</b> <b>exception</b> and system fault existing in digital campus information system. Data {{monitoring and management}} flow was proposed and a data monitoring platform combining monitoring and alarming as well as data management was established. According to the actual requirement of universities, a 3 -layers data monitoring system on digital campus was designed and implemented. By accomplishing about 100 data monitoring tasks in multiple business activities, it met the expected result in the establishment and operation of digital campus. ...|$|E
50|$|PathGuide OASYS is a {{software}} system that captures employee time and labor data {{in real time}} online {{using a variety of}} tools. OASYS supports complex pay rules and specialized accounting requirements for organizations with thousands of employees. It automatically flags all time and labor <b>data</b> <b>exceptions,</b> and computes overtime and shift differentials according to a company's pay rules.|$|R
40|$|A Doctoral Thesis. Submitted in partial {{fulfilment}} of {{the requirements}} for the award of Doctor of Philosophy of Loughborough UniversityThe Internet and associated network technologies are an increasingly integral part of modem day working practices. With this increase in use comes an increase in dependence. For some time commentators have noted that given the level of reliance on data networks, there is a paucity of monitoring tools and techniques to support them. As this area is addressed, more data regarding network perfonnance becomes available. However, a need to automatically analyse and interpret this perfonnance data now becomes imperative. This thesis takes one-way latency as an example perfonnance metric. The tenn 'Data Exception' is then employed to describe delay data that is unusual or unexpected due to some {{fundamental change in the}} underlying network perfonnance. <b>Data</b> <b>Exceptions</b> can be used to assess the effect of network modifications and failures and can also help in the diagnosis of network faults and perfonnance trends. The thesis outlines how <b>Data</b> <b>Exceptions</b> can be identified by the use of a two-stage approach. The Kolmogorov-Smirnov test can initially be applied to detect general changes in the delay distribution, and where such a change has taken place, a neural network can then be used to categorise the change. This approach is evaluated using both a network simulation and a test network to generate a range of delay <b>Data</b> <b>Exceptions...</b>|$|R
25|$|Abort mode: A {{privileged mode}} that is entered {{whenever}} a prefetch abort or <b>data</b> abort <b>exception</b> occurs.|$|R
40|$|We {{have studied}} the decay widths Γ_ππ, Γ_K K̅, Γ_πη, Γ_γγ, of the f_ 0 (980) and a_ 0 (980) mesons by {{assuming}} that they are largely dominated by KK̅ components. By using threshold K K̅ kinematics and lowest order of chiral perturbation theory {{we are able to}} write these widths in terms of only one unknown magnitude for each meson. The results obtained by using the experimental values of the masses are in qualitative agreement with the experimental <b>data</b> <b>exception</b> made of Γ_K K̅ for the a_ 0, where there is also disagreement between present data...|$|E
40|$|Service-Oriented Computing (SOC) has {{recently}} gained attention both within industry and academia; however, its characteristics cannot be easily solved using existing distributed computing technologies. Composition and interaction {{issues have been}} the central concerns, because SOC applications are composed of heterogeneous and distributed processes. To tackle the complexity of inter-organizational service integration, the authors propose a methodology to decompose complex process requirements into different types of flows, such as control, <b>data,</b> <b>exception,</b> and security. The subset of each type of flow necessary for the interactions with each partner can be determined in each service. These subsets collectively constitute a process view, based on which interactions can be systematically designed and managed for system integration through service composition. The authors illustrate how the proposed SOC middleware, named FlowEngine, implements and manages these flows with contemporary Web services technologies. An experimental case study in an e-governmental environment further demonstrates how the methodology can facilitate the design of complex inter-organizational processes...|$|E
40|$|Workflow {{technology}} {{has recently been}} employed {{as a framework for}} the integration of heterogeneous and complex information systems. This typically requires collaborative enactment of complex processes across multiple organizations over the Internet. To tackle the complexity of these cross-organizational interactions, we propose a methodology for the decomposition of complex process requirements into five types of elementary flows: control, <b>data,</b> <b>exception,</b> semantics, and security. Then, we can determine the subset of each type of flows (i. e., flow views) necessary for the interactions with each type of collaboration partners in each service. These five subsets collectively constitute a process view, based on which interactions can be systematically designed and managed for heterogeneous and complex system integration. We show with a case study in a governmental environment not only can our methodology facilitate organizations to design each of their own services-based integration infrastructures, but also new cross-organizational information systems can be formulated through effective composition over such an infrastructure. We further illustrate how these flows can be implemented with various contemporary Web services standard technologies...|$|E
50|$|This is {{the newest}} {{approach}} in data mapping and involves simultaneously evaluating actual data values in two data sources using heuristics and statistics to automatically discover complex mappings between two data sets. This approach is used to find transformations between two data sets and will discover substrings, concatenations, arithmetic, case statements {{as well as other}} kinds of transformation logic. This approach also discovers <b>data</b> <b>exceptions</b> that do not follow the discovered transformation logic.|$|R
40|$|A Doctoral Thesis. Submitted in partial {{fulfillment}} of the requirements for the award of Doctor of Philosophy of Loughborough University. There is an increased demand for higher levels of network availability and reliability. Effective monitoring is necessary to help meet this demand. Loughborough University's High speed Network (HSN) group and many other research groups have preformed significant research related to the monitoring of communication networks and the subsequent processing of the information collected in a meaningful way. This thesis takes latency as an example performance metric. The term 'Data Exception' is then employed to describe delay data that is unusual or unexpected due to some {{fundamental change in the}} underlying network performance. Examples of such changes include significant changes in usage patterns or planned alterations. The objective of this work is to process and interpret such communication network performance data at higher levels of understanding, and will focus on three main points:- • Developing a rule based algorithm to automate the detection of Delay <b>Data</b> <b>Exceptions.</b> • Correlating Delay <b>Data</b> <b>Exceptions</b> in different routes in a network to detect the location and the characteristic of the event that caused these Exceptions. • Predicting the effect of an external event on network performance. In addition to the above three points, the research started by improving a previously published technique for detection and classification of Delay <b>Data</b> <b>Exceptions.</b> The nature of the delay patterns in a commercial communication network was the key issue in developing the algorithm for the first section of the work, and a Neural Network was used in the last two research areas. The monitored delay data used in this work was obtained from different sources; the historical performance data of a commercial network, data from simulation and monitoring of test network in previous related research, and also by monitoring two experimental test networks built in the laboratory. The results of the detection algorithm show an improvement in detection performance, and provide more generality and independency of the source of the delay data. The outputs of the approaches used in the event detection and the performance predictions work give good results, and show potentially the ability to locate the underlying events...|$|R
40|$|There is an {{increased}} demand for higher levels of network availability and reliability. Effective monitoring is necessary to help meet this demand. Loughborough University's High speed Network (HSN) group and many other research groups have preformed significant research related to the monitoring of communication networks and the subsequent processing of the information collected in a meaningful way. This thesis takes latency as an example performance metric. The term 'Data Exception' is then employed to describe delay data that is unusual or unexpected due to some {{fundamental change in the}} underlying network performance. Examples of such changes include significant changes in usage patterns or planned alterations. The objective of this work is to process and interpret such communication network performance data at higher levels of understanding, and will focus on three main points:- • Developing a rule based algorithm to automate the detection of Delay <b>Data</b> <b>Exceptions.</b> • Correlating Delay <b>Data</b> <b>Exceptions</b> in different routes in a network to detect the location and the characteristic of the event that caused these Exceptions. • Predicting the effect of an external event on network performance. In addition to the above three points, the research started by improving a previously published technique for detection and classification of Delay <b>Data</b> <b>Exceptions.</b> The nature of the delay patterns in a commercial communication network was the key issue in developing the algorithm for the first section of the work, and a Neural Network was used in the last two research areas. The monitored delay data used in this work was obtained from different sources; the historical performance data of a commercial network, data from simulation and monitoring of test network in previous related research, and also by monitoring two experimental test networks built in the laboratory. The results of the detection algorithm show an improvement in detection performance, and provide more generality and independency of the source of the delay data. The outputs of the approaches used in the event detection and the performance predictions work give good results, and show potentially the ability to locate the underlying events. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Tak-Chung Lau. Thesis (M. Phil.) [...] Chinese University of Hong Kong, 2004. Includes bibliographical {{references}} (leaves 190 - 198). Abstracts in English and Chinese. Abstract [...] - p. iAcknowledgement [...] - p. ivChapter 1 [...] - Introduction [...] - p. 1 Chapter 1. 1 [...] - Background Information [...] - p. 4 Chapter 1. 2 [...] - Importance of the Problem [...] - p. 6 Chapter 1. 3 [...] - Problem Definition and Proposed Algorithm Outline [...] - p. 7 Chapter 1. 4 [...] - Simple Illustration [...] - p. 10 Chapter 1. 5 [...] - Outline of the Thesis [...] - p. 12 Chapter 2 [...] - Survey [...] - p. 14 Chapter 2. 1 [...] - Introduction [...] - p. 14 Chapter 2. 2 [...] - Dynamic Programming (DP) [...] - p. 15 Chapter 2. 2. 1 [...] - Introduction [...] - p. 15 Chapter 2. 2. 2 [...] - Algorithm [...] - p. 15 Chapter 2. 2. 3 [...] - Example [...] - p. 16 Chapter 2. 2. 4 [...] - Complexity Analysis [...] - p. 20 Chapter 2. 2. 5 [...] - Summary [...] - p. 21 Chapter 2. 3 [...] - General Alignment Tools [...] - p. 21 Chapter 2. 4 [...] - K-Nearest Neighbor (KNN) [...] - p. 22 Chapter 2. 4. 1 [...] - Value of K [...] - p. 22 Chapter 2. 4. 2 [...] - Example [...] - p. 23 Chapter 2. 4. 3 [...] - Variations in KNN [...] - p. 24 Chapter 2. 4. 4 [...] - Summary [...] - p. 24 Chapter 2. 5 [...] - Decision Tree [...] - p. 25 Chapter 2. 5. 1 [...] - General Information of Decision Tree [...] - p. 25 Chapter 2. 5. 2 [...] - Classification in Decision Tree [...] - p. 26 Chapter 2. 5. 3 [...] - Disadvantages in Decision Tree [...] - p. 27 Chapter 2. 5. 4 [...] - Comparison on Different Types of Trees [...] - p. 28 Chapter 2. 5. 5 [...] - Conclusion [...] - p. 29 Chapter 2. 6 [...] - Hidden Markov Model (HMM) [...] - p. 29 Chapter 2. 6. 1 [...] - Markov Process [...] - p. 29 Chapter 2. 6. 2 [...] - Hidden Markov Model [...] - p. 31 Chapter 2. 6. 3 [...] - General Framework in HMM [...] - p. 32 Chapter 2. 6. 4 [...] - Example [...] - p. 34 Chapter 2. 6. 5 [...] - Drawbacks in HMM [...] - p. 35 Chapter 2. 7 [...] - Chapter Summary [...] - p. 36 Chapter 3 [...] - Related Work [...] - p. 37 Chapter 3. 1 [...] - Resonant Recognition Model (RRM) [...] - p. 37 Chapter 3. 1. 1 [...] - Introduction [...] - p. 37 Chapter 3. 1. 2 [...] - Encoding Stage [...] - p. 39 Chapter 3. 1. 3 [...] - Transformation Stage [...] - p. 41 Chapter 3. 1. 4 [...] - Evaluation Stage [...] - p. 43 Chapter 3. 1. 5 [...] - Important Conclusion in RRM [...] - p. 47 Chapter 3. 1. 6 [...] - Summary [...] - p. 48 Chapter 3. 2 [...] - Motivation [...] - p. 49 Chapter 3. 2. 1 [...] - Example [...] - p. 51 Chapter 3. 3 [...] - Chapter Summary [...] - p. 53 Chapter 4 [...] - Group Classification [...] - p. 54 Chapter 4. 1 [...] - Introduction [...] - p. 54 Chapter 4. 2 [...] - Design [...] - p. 55 Chapter 4. 2. 1 [...] - Data Preprocessing [...] - p. 55 Chapter 4. 2. 2 [...] - Encoding Stage [...] - p. 58 Chapter 4. 2. 3 [...] - Transformation Stage [...] - p. 63 Chapter 4. 2. 4 [...] - Evaluation Stage [...] - p. 64 Chapter 4. 2. 5 [...] - Classification [...] - p. 72 Chapter 4. 2. 6 [...] - Summary [...] - p. 75 Chapter 4. 3 [...] - Experimental Settings [...] - p. 75 Chapter 4. 3. 1 [...] - "Statistics from Database of Secondary Structure in Pro- teins (DSSP) [27], [54]" [...] - p. 76 Chapter 4. 3. 2 [...] - Parameters Used [...] - p. 77 Chapter 4. 3. 3 [...] - Experimental Procedure [...] - p. 79 Chapter 4. 4 [...] - Experimental Results [...] - p. 79 Chapter 4. 4. 1 [...] - Reference Group - Neurotoxin [...] - p. 80 Chapter 4. 4. 2 [...] - Reference Group - Biotin [...] - p. 82 Chapter 4. 4. 3 [...] - Average Results {{of all the}} Groups [...] - p. 84 Chapter 4. 4. 4 [...] - Conclusion in Experimental Results [...] - p. 88 Chapter 4. 5 [...] - Discussion [...] - p. 89 Chapter 4. 5. 1 [...] - Discussion on the Experimental Results [...] - p. 89 Chapter 4. 5. 2 [...] - Complexity Analysis [...] - p. 94 Chapter 4. 5. 3 [...] - Other Discussion [...] - p. 99 Chapter 4. 6 [...] - Chapter Summary [...] - p. 102 Chapter 5 [...] - Individual Classification [...] - p. 103 Chapter 5. 1 [...] - Design [...] - p. 103 Chapter 5. 1. 1 [...] - Group Profile Generation [...] - p. 104 Chapter 5. 1. 2 [...] - Preparation of Each Testing Examples [...] - p. 104 Chapter 5. 2 [...] - Design with Clustering [...] - p. 104 Chapter 5. 2. 1 [...] - Motivation [...] - p. 105 Chapter 5. 2. 2 [...] - <b>Data</b> <b>Exception</b> [...] - p. 105 Chapter 5. 2. 3 [...] - Clustering Technique [...] - p. 110 Chapter 5. 2. 4 [...] - Classification [...] - p. 116 Chapter 5. 3 [...] - Hybridization of Our Approach and Sequence Alignment [...] - p. 116 Chapter 5. 3. 1 [...] - AlignRemove and AlignChange [...] - p. 117 Chapter 5. 3. 2 [...] - Classification [...] - p. 119 Chapter 5. 4 [...] - Experimental Settings [...] - p. 120 Chapter 5. 4. 1 [...] - Parameters Used [...] - p. 120 Chapter 5. 4. 2 [...] - Choosing of Protein Functional Groups [...] - p. 121 Chapter 5. 5 [...] - Experimental Results [...] - p. 122 Chapter 5. 5. 1 [...] - Experimental Results Setup [...] - p. 122 Chapter 5. 5. 2 [...] - Receiver Operating Characteristics (ROC) Curves [...] - p. 123 Chapter 5. 5. 3 [...] - Interpretation of Comparison Results [...] - p. 125 Chapter 5. 5. 4 [...] - Area {{under the}} Curve [...] - p. 138 Chapter 5. 5. 5 [...] - Classification with KNN [...] - p. 141 Chapter 5. 5. 6 [...] - Three Types of KNN [...] - p. 142 Chapter 5. 5. 7 [...] - Results in Three Types of KNN [...] - p. 143 Chapter 5. 6 [...] - Complexity Analysis [...] - p. 144 Chapter 5. 6. 1 [...] - Complexity in Individual Classification [...] - p. 144 Chapter 5. 6. 2 [...] - Complexity in Individual Clustering Classification [...] - p. 146 Chapter 5. 6. 3 [...] - Complexity of Individual Classification in DP [...] - p. 148 Chapter 5. 6. 4 [...] - Conclusion [...] - p. 148 Chapter 5. 7 [...] - Discussion [...] - p. 149 Chapter 5. 7. 1 [...] - Domain Expert Opinions [...] - p. 149 Chapter 5. 7. 2 [...] - Choosing the Threshold [...] - p. 149 Chapter 5. 7. 3 [...] - Statistical Support in an Individual Protein [...] - p. 150 Chapter 5. 7. 4 [...] - Discussion on Clustering [...] - p. 151 Chapter 5. 7. 5 [...] - Poor Performance in Hybridization [...] - p. 154 Chapter 5. 8 [...] - Chapter Summary [...] - p. 155 Chapter 6 [...] - Application [...] - p. 157 Chapter 6. 1 [...] - Introduction [...] - p. 157 Chapter 6. 1. 1 [...] - Construct the Correlation Graph [...] - p. 157 Chapter 6. 1. 2 [...] - Minimum Spanning Tree (MST) [...] - p. 161 Chapter 6. 2 [...] - Application in Group Classification [...] - p. 164 Chapter 6. 2. 1 [...] - Groups with Weak Relationship [...] - p. 164 Chapter 6. 2. 2 [...] - Groups with Strong Relationship [...] - p. 166 Chapter 6. 3 [...] - Application in Individual Classification [...] - p. 168 Chapter 6. 4 [...] - Chapter Summary [...] - p. 171 Chapter 7 [...] - Discussion on Other Analysis [...] - p. 172 Chapter 7. 1 [...] - Distanced MLN Encoding Scheme [...] - p. 172 Chapter 7. 2 [...] - Unique Encoding Method [...] - p. 174 Chapter 7. 3 [...] - Protein with Multiple Functions? [...] - p. 175 Chapter 7. 4 [...] - Discussion on Sequence Similarity [...] - p. 176 Chapter 7. 5 [...] - Functional Blocks in Proteins [...] - p. 177 Chapter 7. 6 [...] - Issues in DSSP [...] - p. 178 Chapter 7. 7 [...] - Flexible Encoding [...] - p. 179 Chapter 7. 8 [...] - Advantages over Dynamic Programming [...] - p. 179 Chapter 7. 9 [...] - Novel Research Direction [...] - p. 180 Chapter 8 [...] - Future Works [...] - p. 182 Chapter 8. 1 [...] - Improvement in Encoding Scheme [...] - p. 182 Chapter 8. 2 [...] - Analysis on Primary Protein Sequences [...] - p. 183 Chapter 8. 3 [...] - In Between Spectrum Scaling [...] - p. 184 Chapter 8. 4 [...] - Improvement in Hybridization [...] - p. 185 Chapter 8. 5 [...] - Fuzzy Threshold Boundaries [...] - p. 185 Chapter 8. 6 [...] - Optimal Parameters Setting [...] - p. 186 Chapter 8. 7 [...] - Generalization Tool [...] - p. 187 Chapter 9 [...] - Conclusion [...] - p. 188 Bibliography [...] - p. 190 Chapter A [...] - Fourier Transform [...] - p. 199 Chapter A. 1 [...] - Introduction [...] - p. 199 Chapter A. 2 [...] - Example [...] - p. 201 Chapter A. 3 [...] - Physical Meaning of Fourier Transform [...] - p. 20...|$|E
50|$|The {{final step}} is to {{identify}} components that provide {{a solution to the}} problem domain. This would include databases to hold the <b>data,</b> security, <b>exception</b> handling, and communication between processes or programs.|$|R
5000|$|Exception {{handling}} - translating <b>data</b> access related <b>exception</b> to a Spring {{data access}} hierarchy ...|$|R
40|$|Abstract: <b>Data</b> <b>exceptions</b> often reflect {{potential}} problems or dangers {{in the management}} of corporation. Analysts often need to identify these exceptions from large amount of data. A recent proposed approach automatically detects and marks the exceptions for the user and reduces the reliance on manual discovery. However, the efficiency and scalability of this method are not so satisfying. According to these disadvantages, the optimizations are investigated to improve it. A new method that pushes several constraints into the mining process is proposed in this paper. By enforcing several user-defined constraints, this method first restricts the multidimensional space to a small constrained-cube and then mines exceptions on it. Experimental results show that this method is efficient and scalable...|$|R
50|$|There {{are rarely}} {{constants}} (i.e. characters {{that are not}} formatting placeholders) in a format string, mainly because a program is usually not designed to read known <b>data.</b> The <b>exception</b> is one or more whitespace characters, which discards all whitespace characters in the input.|$|R
40|$|The {{positive}} experiences {{from the}} usage of Ada in a safety critical flight control system are described in this report. It shows that preemptive scheduling implemented with tasking can be combined with high requirements on reliability and deterministic behavior. 1. 1 Keywords Flight control system, tasking, <b>data</b> consistency, <b>exception</b> handling. 2...|$|R
50|$|Adeptia is a Chicago-based {{software}} company. It provides {{application to}} exchange business data with other companies using a self-service integration approach. This business software helps organizations quickly create automated data connections {{to their customers}} and partners and automate pre-processing and post-processing steps such as <b>data</b> validation, <b>exception</b> handling and back-end data integration.|$|R
40|$|We study eA {{scattering}} {{in a model}} {{where the}} total nuclear structure function is a convolution of components W_N for an isolated nucleon and W_ for a nucleus composed of point-nucleons. W_N is represented by its elastic and lowest-energy inelastic parts, while W_ is computed from its asymptotic limit, supplemented by final state interactions due to binary collisions between the knocked-out and core nucleons. (q/M) W_ per nucleon appears to be practically independent of mass number A and momentum transfer q. Consequently, predicted inclusive cross sections per nucleon for given kinematic conditions hardly depend on the target and {{the same can be}} extracted from the <b>data.</b> <b>Exceptions</b> are low energy loss regions, where relatively small cross sections are shown to be sensitive to details of the single nucleon momentum distribution and to FSI involving multiple collisions. The agreement with the data is good...|$|R
40|$|This report {{documents}} the {{successful completion of}} testing for the In Situ Vapor Sampling (ISVS) system. The report includes the test procedure (WHC-SD-WM-OTP- 196, Rev OA), <b>data</b> sheets, <b>exception</b> resolutions, and a test report summary. This report conforms to the guidelines established in WHC-IP- 1026, `Engineering Practice Guidelines,` Appendix L, `Operability Test Procedures and Reports. ...|$|R
40|$|Includes {{such topics}} as variables, expressions, statements, typing scope, procedures, <b>data</b> types, <b>exception</b> {{handling}} and concurrency. By understanding these concepts {{and how they are}} realized in different programming language, the reader is provided with a framework for understanding future language design and a level of comprehension far greater then one gets by writing different programs in different language...|$|R
40|$|University of Twente; Centre for Telematics and Information Technology; Netherlands Organisation for Scientific Research; Jacquard; CapgeminiOne major {{contribution}} of data warehouses {{is to support}} better decision making by facilitating data analysis, and therefore data quality is of primary importance. ETL is the process that extracts, transforms, and ultimately loads data into target warehouses. Although ETL workflows can be designed by ETL tools, <b>data</b> <b>exceptions</b> are largely left to human analysis and handled inadequately. Early detection of exceptions helps to improve the stability and efficiency of ETL workflows. To achieve this goal, a novel approach, Backwards Constraint Propagation (BCP), is proposed that automatically analyzes ETL workflows and verifies the target-end restrictions at their earliest points. BCP builds an ETL graph out of a given ETL workflow, encodes the target-end restrictions as integrity constraints, and propagates them backwards from target to sources through the ETL graph by applying constraint projection rules. It is showed that BCP supports most relational algebra operators and data transformation functions. © 2009 Springer Berlin Heidelberg...|$|R
50|$|The {{community}} openly discuss, {{create and}} support the resources available online. All <b>data,</b> with few <b>exceptions,</b> is licensed under an open license: the UniLang Public License. Members are organized into related groups and coordinate themselves the projects of their interest.|$|R
40|$|This report {{summarizes}} {{the results of}} characterizing and monitoring the following sources during calendar year 1994 : liquid waste streams from Buildings 306, 320, 324, 326, 331, and 3720 in the 300 Area of Hanford Site and managed by the Pacific Northwest Laboratory; treated and untreated Columbia River water (influent); and water at {{the confluence of the}} waste streams (that is, end-of-pipe). Data were collected from March to December before the sampling system installation was completed. Data from this initial part of the program are considered tentative. Samples collected were analyzed for chemicals, radioactivity, and general parameters. In general, the concentrations of chemical and radiological constituents and parameters in building wastewaters which were sampled and analyzed during CY 1994 were similar to historical <b>data.</b> <b>Exceptions</b> were the occasional observances of high concentrations of chloride, nitrate, and sodium that are believed to be associated with excursions that were occurring when the samples were collected. Occasional observances of high concentrations of a few solvents also appeared to be associated with infrequent building r eases. During calendar year 1994, nitrate, aluminum, copper, lead, zinc, bis(2 -ethylhexyl) phthalate, and gross beta exceeded US Environmental Protection Agency maximum contaminant levels...|$|R
40|$|Systematic risk {{analysis}} can {{be based on}} causal analysis of business exceptions. In this paper we describe the concepts of automatic analysis for the exceptional patterns which are hidden in a large set of business <b>data.</b> These <b>exceptions</b> are interesting to be investigated further for their causes and explanations. The analysis process is driven by diagnostic drill-down operations following the equations of the information structure in which the data are organised. Using business intelligence, the analysis method can generate explanations supported by the data...|$|R
30|$|Referring to {{students}} (see Fig.  5 a, c), the median of water consumptions “purified” by losses (see mark C) was as follows: 7.9 and 7.2  L/day/student considering all the experimentation data and all <b>data</b> with the <b>exception</b> of the outlier, respectively. Furthermore, differently from Almeida et al. (2014) and Farina et al. (2011), the interquartile range {{was very small}} highlighting how 50  % of the observations were concentrated around the medians. Also referring {{to students}} (see Fig.  5 a, c), the median of water consumptions “inclusive” of losses (see mark D) was as follows: 14.2 and 13.6  L/day/student considering all the experimentation data and all <b>data</b> with the <b>exception</b> of the outlier, respectively. Compared with the previous case, it was observed how the interquartile range was roughly constant while the upper whisker is varied.|$|R
40|$|In {{this paper}} we {{describe}} the concepts of automatic analysis for the exceptional patterns which are hidden in a large set of business <b>data.</b> These <b>exceptions</b> are interesting to be investigated further for their causes and explanations, as they provide important decision support. The analysis process is driven by diagnostic drill-down operations following the equations of the information structure in which the data are organised. Using business intelligence, the analysis method can generate explanations supported by the data. The methodology was tested on a case study and was reflected in considering the practical aspects of its application procedure...|$|R
50|$|This figure {{illustrates}} the high-level paradigm for remote interprocess communications using CORBA. The CORBA specification further addresses <b>data</b> typing, <b>exceptions,</b> network protocols, communication timeouts, etc. For example: Normally the server side has the Portable Object Adapter (POA) that redirects calls {{either to the}} local servants or (to balance the load) to the other servers. The CORBA specification (and thus this figure) leaves various aspects of distributed system to the application to define including object lifetimes (although reference counting semantics are available to applications), redundancy/fail-over, memory management, dynamic load balancing, and application-oriented models such as the separation between display/data/control semantics (e.g. see Model-view-controller), etc.|$|R
3000|$|In {{each of the}} 23 input–output {{networks}} {{there are}} 59 nodes corresponding to the sector classification defined in the <b>data</b> section. One <b>exception</b> is the French input–output network which has 57 nodes because there are two sectors disconnected {{from the rest of}} the economy: uranium and thorium ores, and recovered secondary raw materials. 6 [...]...|$|R
40|$|Data mining {{has become}} an {{increasingly}} popular activity {{in all areas of}} research, from business to science, biometrics being no <b>exception.</b> <b>Data</b> mining is the computer-intensive activity of exploring large data sets with the purpose of discovering, within a subset of data, some relationship of patterns or hypothesis that may be worthy of further stud...|$|R
50|$|Scalar {{functions}} {{return a}} single data value (not a table) with RETURNS clause. Scalar functions can use all scalar <b>data</b> types, with <b>exception</b> of timestamp and user-defined data types. Inline table-valued functions return the result set {{of a single}} SELECT statement. Multistatement table-valued functions return a table, which was built with many TRANSACT-SQL statements.|$|R
40|$|Abstract: In this paper, we survey efforts {{devoted to}} {{discovering}} interesting <b>exceptions</b> from <b>data</b> in <b>data</b> mining. An <b>exception</b> {{differs from the}} rest of data and thus is interesting and can be a clue for further discoveries. We classify methods into exception instance discovery, exception rule discovery, and exception structured-rules discovery and give a condensed and comprehensive introduction...|$|R
40|$|Simulated sea ice {{thickness}} in the ORCA 2 -LIM coupled sea ice [...] ocean {{model is}} compared with thicknesses from the ASPeCt (Antarctic Sea Ice Processes and Climate) data base. We find a qualitative {{agreement of the}} large-scale patterns of ice thickness distribution. Regional averages for the various sectors of the Southern Ocean yield a very good correspondence between observations and model <b>data.</b> <b>Exceptions</b> are the eastern Bellingshausen and northwestern Weddell Seas. A poor representation of the Antarctic Peninsula in the atmospheric forcing data and the related overestimation of westerly winds in this region lead to a spurious accumulation of sea ice {{on the western side}} of the Peninsula, and to an underestimation of sea ice coverage on the eastern side. Since the spatial scale of observations is not comparable {{to the size of a}} model grid cell, there is little agreement between individual observations and the corresponding model ice thicknesses. A model analysis of the seasonal and interannual variability indicates that the ASPeCt data underestimate the climatological ice thickness in the central and southern Weddell Sea and the eastern Ross Sea by up to 1 m. Due to a winter bias in the observations, an overestimation of similar magnitude is expected in the Bellingshausen Sea. Ice thickness data in most of the Indo-Pacific sector appear to be representative for the long-term climatology. A model estimate of the bias is used to compute a revised distribution of climatological sea ice thickness...|$|R
40|$|Pay-for-performance {{programs}} are often aimed {{to improve the}} management of chronic dis-eases. We evaluate {{the impact of a}} local pay for performance programme (QOF+), which rewarded financially more ambitious quality targets (‘stretch targets’) than those used na-tionally in the Quality and Outcomes Framework (QOF). We focus on targets for intermedi-ate outcomes in patients with cardiovascular disease and diabetes. A difference-in-difference approach is used to compare practice level achievements before and after the in-troduction of the local pay for performance program. In addition, we analysed patient-level <b>data</b> on <b>exception</b> reporting and intermediate outcomes utilizing an interrupted time series analysis. The local pay for performance program led to significantly higher target achieve...|$|R
50|$|Data Quality (DQ) is a niche area {{required}} for {{the integrity of the}} data management by covering gaps of data issues. This {{is one of the key}} functions that aid data governance by monitoring <b>data</b> to find <b>exceptions</b> undiscovered by current data management operations. Data Quality checks may be defined at attribute level to have full control on its remediation steps.|$|R
