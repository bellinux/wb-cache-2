25|22|Public
50|$|Storage server {{software}} also implements Smart <b>Flash</b> <b>Cache,</b> Columnar <b>Flash</b> <b>Cache,</b> Flash Logging and Write Back <b>Flash</b> <b>Cache</b> routines {{that use}} flash storage to improve I/O response times. I/O Resource Management allocates I/O bandwidth to databases or workloads according to specified priorities. Lastly, decompression of Hybrid Columnar Compressed data may {{be performed in}} Exadata storage servers.|$|E
50|$|There are {{two choices}} for Exadata storage servers; Extreme Flash and High Capacity. X6-2 Extreme Flash is an all-flash storage server {{containing}} 8 PCIe flash drives {{for a total}} of 25.6 terabytes of raw storage capacity. The X6-2 High Capacity storage servers contain 12 disks, 8TB each, {{for a total of}} 96 terabytes of rawstorage capacity. To improve I/O response times, High-Capacity storage servers also employ 12.8 terabytes of PCIe flash to cache active data blocks. Exadata’s Smart <b>Flash</b> <b>Cache,</b> Smart Flash Log, Columnar <b>Flash</b> <b>Cache</b> and Write Back <b>Flash</b> <b>Cache</b> features determine how and when to use flash (see Storage Server Software).|$|E
5000|$|NetApp Filer {{can have}} PAM ( [...] Performance Accelerate Module [...] ) or <b>Flash</b> <b>Cache</b> (PAM II) which can reduce read latencies {{and allows the}} filer to support more read {{intensive}} work without adding any further disk to the underlying RAID.|$|E
5000|$|Microsoft ReadyBoost allows {{personal}} computers to USB <b>flash</b> drives as <b>cache.</b>|$|R
25|$|Windows ReadyDrive enables systems {{equipped}} with a hybrid drive or other <b>flash</b> memory <b>caches</b> such as Intel Turbo Memory to boot up faster, resume from hibernation in less time, and preserve battery power.|$|R
50|$|Competing VMware-focused flash {{virtualization}} technologies include Virtunet Systems' VirtuCache, SanDisk’s FlashSoft, Proximal Data's Autocache, and VMware's own vSphere <b>Flash</b> Read <b>Cache</b> (vFRC).PernixData's FVP and Virtunet Systems' VirtuCache distinguish {{themselves from}} VMware's own vFRC by adding write caching and clustering.|$|R
50|$|It was {{reported}} in eWeek that the technology is not being utilized to full extent {{due to lack of}} hybrid drive-specific drivers for the hybrid drives and instead delegated the job to the device manufacturers. However, Microsoft rebuffed the suggestion that it was not providing specialized drivers for hybrid systems. Also, in June 2006, David Morgenstern wrote an article for eWeek suggesting that ReadyDrive might sacrifice data integrity for speed and battery savings. Documentation from Microsoft, however, claims that a copy of the data is always maintained on the hard disk, so there is no question of data loss even if the <b>flash</b> <b>cache</b> fails.|$|E
5000|$|By {{chipsets}} {{external to}} the individual storage drives. An example {{is the use of}} <b>flash</b> <b>cache</b> modules (FCMs). FCMs combine the use of separate SSD (usually an mSATA SSD module) and HDD components, while managing performance optimizations via host software, device drivers, or a combination of both. One example is Intel Smart Response Technology (SRT), which is implemented through a combination of certain Intel chipsets and Intel storage drivers, is the most common implementation of FCM hybrid systems today. What distinguished this dual-drive system from an SSHD system is that each drive maintains its ability to be addressed independently by the operating system if desired.|$|E
5000|$|Released in 2007 the D630 is {{an update}} of the D620 design. It {{differed}} most significantly in being {{based on the}} newer [...] "Santa Rosa" [...] (mobile 965) chipset, which supported the 800MT/s models of the mobile Core 2 Duo (both the Merom 7xx0 series and later the Penryn-based 8x00/9x00 series.) It also had newer versions of the graphics processor options, support for Intel's [...] "Turbo Memory" [...] <b>flash</b> <b>cache</b> (although this uses the same card slot as the mobile broadband card), and support for internal Wireless-N. It also added a 4-pin Firewire IEEE1394 port. It uses DDR2 memory and is compatible with PC2-5300 (667 MHz), and PC2-6400 (800 MHz) memory will work at PC2-5300 speeds.|$|E
5000|$|Oracle Corporation markets {{products}} such as Exadata Smart <b>Cache</b> <b>Flash,</b> and the FS1 flash storage system.|$|R
40|$|Cache {{replacement}} algorithms {{have focused}} on man-aging caches {{that are in the}} datapath. In datapath caches, every cache miss results in a cache update. Cache up-dates are expensive because they induce cache insertion and cache eviction overheads which can be detrimental to both cache performance and cache device lifetime. Non-datapath caches, such as host-side <b>flash</b> <b>caches,</b> allow the flexibility of not having to update the cache on each miss. We propose the multi-modal adaptive replacement cache (mARC), a new cache replacement algorithm that extends the adaptive replacement cache (ARC) algorithm for non-datapath caches. Our initial trace-driven sim-ulation experiments suggest that mARC improves the cache performance over ARC while significantly reduc-ing the number of cache updates for two sets of storage I/O workloads from MSR Cambridge and FIU. ...|$|R
40|$|Hybrid storage {{solutions}} use NAND {{flash memory}} based Solid State Drives (SSDs) as non-volatile cache and tra-ditional Hard Disk Drives (HDDs) as lower level stor-age. Unlike a typical <b>cache,</b> internally, the <b>flash</b> memory <b>cache</b> {{is divided into}} cache space and over-provisioned space, used for garbage collection. We show that bal-ancing the two spaces appropriately helps improve the performance of hybrid storage systems. We show that contrary to expectations, the cache need not be filled with data to the fullest, but may {{be better served by}} reserving space for garbage collection. For this balancing act, we present a dynamic scheme that further divides the cache space into read and write caches and manages the three spaces according to the workload characteristics for op-timal performance. Experimental results show that our dynamic scheme improves performance of hybrid stor-age solutions up to the off-line optimal performance of a fixed partitioning scheme. Furthermore, as our scheme makes efficient use of the <b>flash</b> memory <b>cache,</b> it re-duces the number of erase operations thereby extending the lifetime of SSDs. ...|$|R
40|$|Classic caching {{algorithms}} leverage recency, access count, and/or other {{properties of}} cached blocks at per-block gran-ularity. However, for media such as flash which have perfor-mance and wear penalties for small overwrites, implement-ing cache policies at a larger granularity is beneficial. Re-cent {{research has focused}} on buffering small blocks and writ-ing in large granularities, called containers, but it has not explored the ramifications and best strategies for caching compound blocks consisting of logically distinct, but physi-cally co-located, blocks. Containers may have highly diverse blocks, with mixtures of frequently accessed, infrequently accessed, and invalidated blocks. We propose and evaluate Pannier, a <b>flash</b> <b>cache</b> middle-ware that provides high performance while extending flash lifespan. Pannier uses three main techniques: (1) leverag-ing block access counts to manage cache containers, (2) in-corporating block liveness as a property to improve <b>flash</b> <b>cache</b> space efficiency, and (3) designing a multi-step feed-back controller to ensure a <b>flash</b> <b>cache</b> does not wear out in its lifespan while maintaining performance. Our evaluation shows that Pannier improves <b>flash</b> <b>cache</b> performance and extends lifespan beyond previous per-block and container-aware caching policies. More fundamentally, our investiga-tion highlights the importance of creating new policies for caching compound blocks in flash...|$|E
40|$|Flash {{memory has}} {{recently}} become popular as a caching medium. Most uses to date {{are on the}} storage server side. We investigate a different structure: flash as a cache on the client side of a networked storage environment. We use trace-driven simulation to explore the design space. We consider {{a wide range of}} configurations and policies to determine the potential client-side caches might offer and how best to arrange them. Our results show that the <b>flash</b> <b>cache</b> writeback policy does not significantly affect performance. Write-through is sufficient; this greatly simplifies cache consistency handling. We also find that the chief benefit of the <b>flash</b> <b>cache</b> is its size, not its persistence. Cache persistence offers additional performance benefits at system restart at essentially no runtime cost. Finally, for some workloads a large <b>flash</b> <b>cache</b> allows using miniscule amounts of RAM for file caching (e. g., 256 KB) leaving more memory available for application use. ...|$|E
40|$|Host-side flash caching {{has lately}} {{emerged as a}} {{suitable}} and e↵ective means of accelerating enterprise workloads. How-ever, cache management for flash-based caching is di↵erent from traditional DRAM-based caching. A <b>flash</b> <b>cache</b> sits underneath the DRAM cache. Its position in the hierarchy combined with the unique characteristics of flash, calls for a di↵erent cache management solution. Specifically, cache population, an aspect of cache management which is not at-tributed much importance in DRAM caches, becomes cru-cial in flash-based caches. In this paper, we first present a performance evaluation of three popular open-source <b>flash</b> <b>cache</b> implementations: flashcache, bcache, and EnhanceIO. We evaluate them un-der an industry-standard database benchmark and identify their limitations. We demonstrate that several shortcom-ings are due to sub-optimal cache population. We propose a novel set of techniques for cache population, and present {{the design of the}} Scalable Cache Engine (SCE) – a new <b>flash</b> <b>cache</b> solution that incorporates our cache population tech-niques. We demonstrate that SCE remarkably outperforms the existing open-source solutions: 45 % higher throughput, 55 % lower latency, 12 ⇥ faster cache warm-up than flash-cache, and 95 % less memory usage than EnhanceIO. 1...|$|E
40|$|Despite recent {{increases}} in the processing power of handheld computers, often called palmtops, the devices continue to function primarily as personal information organizers. We believe adding mass storage to handheld computers will expand {{the usefulness of the}} devices. We propose a storage hierarchy, involving a hard disk and large software controlled <b>flash</b> memory <b>cache,</b> that lowers the power cost of accessing the data from the disk, but retains the storage capacity benefits of the disk. We investigate how the hoarding of files into flash memory during a power abundant docked state can drastically reduce the power consumption of the hard disk. ...|$|R
5000|$|ReadyDrive (codenamed Piton) is {{a feature}} of Windows Vista that enables Windows Vista {{computers}} equipped with a hybrid drive or other <b>flash</b> memory <b>caches</b> (such as Intel Turbo Memory) to boot up faster, resume from hibernation in less time, and preserve battery power. Hybrid hard drives are {{a new type of}} hard disk that integrates non-volatile flash memory with a traditional hard drive. The drive-side functionality is expected to be standardized in ATA-8. When a hybrid hard drive is installed in a Windows Vista machine, the operating system will display a new [...] "NV Cache" [...] property tab as part of the drive's device properties within the Device Manager.|$|R
40|$|International audienceMore {{and more}} {{enterprise}} servers storage systems are migrating toward flash based drives (Solid State Drives) {{thanks to their}} attractive characteristics. They are lightweight, power efficient and supposed to outperform traditional disks. The two main constraints of flash memories are: 1) {{the limited number of}} achievable write operations beyond which a given cell can no more retain data, and 2) the erase-before-write rule decreasing the write performance. A RAM cache can help to reduce this problem; they are mainly used to increase performance and lifetime by absorbing flash write operations. RAM caches being very costly, their dimensioning is critical. In this paper, we explore some OLTP I/O workload characteristics with regards to <b>flash</b> memory <b>cache</b> systems structure and configuration. We try, throughout I/O workload analysis to reveal some important elements to take into account to allow a good dimensioning of those embedded caches...|$|R
40|$|Considering {{the current}} price gap between disk and flash memory drives, for {{applications}} dealing with large scale data, {{it will be}} economically more sensible to use flash memory drives to supplement disk drives rather than to replace them. This paper presents FaCE, which is a new low-overhead caching strategy that uses flash memory as an extension to the DRAM buffer. FaCE aims at improving the transaction throughput as well as shortening the recovery time from a system failure. To achieve the goals, we propose two novel algorithms for <b>flash</b> <b>cache</b> management, namely, Multi-Version FIFO replacement and Group Second Chance. One striking result from FaCE is that using a small flash memory drive as a caching device could deliver even higher throughput than using a large flash memory drive to store the entire database tables. This was possible due to flash write optimization as well as disk access reduction obtained by the FaCE caching methods. In addition, FaCE {{takes advantage of the}} non-volatility of flash memory to fully support database recovery by extending the scope of a persistent database to include the data pages stored in the <b>flash</b> <b>cache.</b> We have implemented FaCE in the PostgreSQL open source database server and demonstrated its effectiveness for TPC-C benchmarks. Comment: VLDB 201...|$|E
40|$|Abstract. Emerging {{portable}} devices relay on DRAM/flash memory system to satisfy requirements on fast and large data storage and low-energy consumption. This paper presents {{a novel approach}} to reduce energy of memory system, which unlike others, lowers energy of refresh operation in DRAM. The approach is based on two key ideas: (1) DRAM-based <b>flash</b> <b>cache</b> that keeps dirty pages {{to reduce the number}} of accesses to flash memory; and (2) OS-controlled page allocation/aging to stop the refresh operations in banks, whose pages are clean and not accessed for a long time. Simulations show that by using this technique we can decrease the overall energy consumption of DRAM/flash memory on video applications by 8 - 26 % while reducing the DRAM refresh energy by 59 - 74 %...|$|E
40|$|Gurevich's Abstract State Machines (ASM) {{constitute}} a high-level specification language {{for a wide}} range of applications. The existing tool support for ASM was extended, in a previous work, to support computer-aided verification, in particular by model checking. In this paper we discuss the applicability of the model checking approach in general and describe the steps that are necessary to fit different kinds of ASM models for the model checking process. Along the example of the <b>FLASH</b> <b>cache</b> coherence protocol we show how model checking can support development and debugging of ASM models. We show the necessary refinement for the message passing behaviour in the protocol and give examples for errors found by model checking the resulting model. We conclude with some general remarks on the existing transformation algorithm...|$|E
25|$|In September 2009, Apple {{discontinued}} {{the original}} 40 GB Apple TV and now continues {{to produce and}} sell the 160 GB Apple TV. On September 1, 2010, Apple released a completely redesigned Apple TV. The new device is 1/4 the size, runs quieter, and replaces {{the need for a}} hard drive with media streaming from any iTunes library on the network along with 8 GB of <b>flash</b> memory to <b>cache</b> media downloaded. Like the iPad and the iPhone, Apple TV runs on an A4 processor. The memory included in the device is half of that in the iPhone 4 at 256 MB; the same as the iPad, iPhone 3GS, third and fourth-generation iPod Touch.|$|R
40|$|Abstract—Recent {{breakthroughs}} in circuit and process tech-nology have enabled new usage models for non-volatile memory {{technologies such as}} Flash and phase change RAM (PCRAM) in the general purpose computing environment. These technologies display high density and low power consumption as well as persistency that are appealing properties in a memory device. This paper summarizes our earlier work on improving NAND <b>Flash</b> based disk <b>caches</b> and extends it to consider PCRAM. We first present the primary challenges in reliably managing non-volatile memories such as NAND Flash, reviewing our past work on architectural support for Flash manageability. We then provide a preliminary analysis of how our current Flash manageability architecture may be simplified when we replace Flash with PCRAM. Our evaluations on PCRAM shows a potential {{for more than a}} 65 % throughput improvement for a disk-intensive database workload. Although more detailed studies are needed, we conclude that PCRAM is a strong contender to replace Flash if it becomes cost-effective. I...|$|R
40|$|International audienceFlash {{memories}} based storage {{systems have}} some specific constraints leading designers to encapsulate some management services into a hardware/software layer called the Flash Translation Layer (FTL). The performance of flash based storage {{systems such as}} Solid State Drives (SSDs) are strongly driven by the FTL intricacies and also by a cache system placed {{on top of the}} FTL. Those systems are generally developed independently. In order to accelerate I/O request processing, FTLs use some space of the flash memory called the over-provisioning space. The over-provisioning space is thus not dedicated to data storage and should be small and of fixed size. This paper presents MaCACH, a maximum page-mapped region usage, cache-aware, and configurable hybrid mapping scheme. MaCACH design is based on two motivations: (1) the FTL should make full profit of the fixed size over-provisioning space to accelerate I/O processing, (2) as in most cases cache systems are put on top of FTLs, the latter should use information about the former in order to optimize data management. MaCACH is mainly based on two solutions: (1) it uses a proportional–integral–derivative (PID) feedback control system to keep the over-provisioning space fully used whatever the I/O workload characteristics, making it more efficient, (2) it is cache-aware as it uses a common feature of <b>flash</b> specific <b>caches</b> in order to route evicted data toward a page-mapped or block-mapped area which helps in optimizing the write operation costs. The performance evaluation shows very good behavior of MaCACH as compared to state-of-the-art FTLs in addition to a high flexibility as MaCACH has a large configuration space...|$|R
40|$|Introduction This {{project is}} about verifying the cache {{coherency}} of the Avalanche migratory cache protocol [1]. The verification is {{performed in a}} manner similar to the <b>FLASH</b> <b>cache</b> coherency verification by Park and Dill [2]. The rest of the report is organised as follows: we describe the general verification methodology in section 2, followed by a section on the Avalanche migratory protocol. Section 4 describes the specifics of this project and section 5 gives the conclusions. 2 Verification Methodology The verification method [2] begins with two state graphs - one that of the implementation and other that of the specification and attempts to establish a correspondence between these two graphs. Specifically, let Q denote the set of all implementation states and F denote the set of all implementation transitions. We similarly have Q' and F ' denoting the specication state sets and specification stat...|$|E
40|$|This {{paper is}} devoted to the {{specification}} of the Stanford <b>FLASH</b> <b>cache</b> coherence protocol within the ASM formalism. Correctness proofs related to data consistency are presented. Corner cases that leads to unlikely situations are exhibited. 1 Introduction It is well known that many challenging problems emerge in the design and the verification of cache coherence protocols. Real-life applications in that domain are complex. They generally involve a great number of different agents and rules thus making correctness proofs combinatorially difficult. In particular, methods based only on exhaustive enumeration of states of the system are often hopeless. A frequent requirement in multiprocessor protocol design is flexibility and scalability: variety of protocols are aimed to be implemented on larger and larger systems. This emphasizes on the necessity of specifying protocols in a clear and readable fashion that makes future developments easier. In this paper, we describe a first attempt on [...] ...|$|E
40|$|Modern disk-based storage {{systems are}} not energy proportional, because disks consume almost as much power when idle (but spinning) as they do when {{actively}} accessing data. We combine a power-aware, solidstate (<b>flash)</b> <b>cache</b> and a reliability-aware disk spindown mechanism to significantly improve storage energy proportionality without hurting disk reliability, data integrity, or performance. We evaluated the resulting power- and reliabilityaware hybrid flash-disk RAID storage array {{and found that it}} reduces energy consumption by 85 % compared to a similar-cost, similar-performance typical configuration of all SAS drives that are never spun down. Our design also achieves almost 50 % energy savings compared to hybrid flash-disk systems tuned for performance or that do not take full advantage of opportunities for safe spindown. Further, unlike most previous work that exploits spindown to save energy, we limit the rate at which disks are spun down to avoid premature mechanical failures, whereas reliability-unaware spindown algorithms can exceed manufacturer waranteed lifetime spindown limits in as little as one year...|$|E
40|$|Facebook uses flash devices {{extensively}} in its photo-caching stack. The key design challenge for an efficient photo <b>cache</b> on <b>flash</b> at Facebook is its workload: many small random writes are generated by inserting cache-missed content, or updating cache-hit content for ad-vanced caching algorithms. The Flash Translation Layer on flash devices performs poorly {{with such a}} workload, lowering throughput and decreasing device lifespan. Ex-isting coping strategies under-utilize the space on <b>flash</b> devices, sacrificing <b>cache</b> capacity, or are limited to sim-ple caching algorithms like FIFO, sacrificing hit ratios. We overcome these limitations with the novel Re-stricted Insertion Priority Queue (RIPQ) framework that supports advanced caching algorithms with large cache sizes, high throughput, and long device lifespan. RIPQ aggregates small random writes, co-locates similarly pri-oritized content, and lazily moves updated content to fur-ther reduce device overhead. We show that two fam-ilies of advanced caching algorithms, Segmented-LRU and Greedy-Dual-Size-Frequency, can be easily imple-mented with RIPQ. Our evaluation on Facebook’s photo trace shows that these algorithms running on RIPQ in-crease hit ratios up to ~ 20 % over the current FIFO sys-tem, incur low overhead, and achieve high throughput. ...|$|R
40|$|This paper {{introduces}} proximal I/O, a {{new technique}} for improving random disk I/O performance in file systems. The key enabling technology for proximal I/O {{is the ability of}} disk drives to retire multiple I/Os, spread across dozens of tracks, in a single revolution. Compared to traditional update-in-place or write-anywhere file systems, this technique can provide a nearly seven-fold improvement in random I/O performance while maintaining (near) sequential on-disk layout. This paper quantifies proximal I/O performance and proposes a simple data layout engine that uses a <b>flash</b> memory-based write <b>cache</b> to aggregate random updates until they have sufficient density to exploit proximal I/O. The results show that with cache of just 1 % of the overall disk-based storage capacity, it is possible to service 5. 3 user I/O requests per revolution for random updates workload. On an aged file system, the layout can sustain serial read bandwidth within 3 % of the best case. Despite using flash memory, the overall system cost is just one third of that of a system with the requisite number of spindles to achieve the equivalent number of random I/O operations. ...|$|R
40|$|International audienceMany hybrid Flash Translation Layer (FTL) schemes {{have been}} {{proposed}} to leverage the erase-before-write and limited lifetime constraints of flash memories. Those schemes try to approach page mapping performance and flexibility while seeking block mapping memory usage. Furthermore, flash-specific cache systems were designed (1) to maximize lifetime by absorbing some erase operations, and (2) to reveal sequentiality from random write operations. Indeed, random writes represent the Achilles' heel of <b>flash</b> memories. Both <b>cache</b> systems and FTL schemes were designed independently from each other. This paper presents a scalable (in terms of mapping table size) and flexible (in terms of I/O workload support) Cache-Aware Configurable Hybrid (CACH) FTL. CACH-FTL uses a common feature of flashspecific cache systems that is flushing groups of pages from the same block. CACH-FTL partitions the flash memory space into two regions: (1) a data Block Mapped Region (BMR) collecting large groups of pages from the above cache (sequential I/Os), and (2) a small Page Mapped overprovisioning Region (PMR) which purpose is to collect/buffer small groups of pages coming from the cache (random I/Os) before moving them to BMR. CACH-FTL is flexible as it offers many configuration possibilities and can be adapted according to the I/O workload. CACH-FTL approaches the ideal page mapping FTL performance as it gives less than 15 % performance difference in most cases...|$|R
40|$|State Machine based {{run time}} {{analysis}} with structural design and verification methods. (Joint work with Wolfram Schulte (Microsoft Research Redmond), {{to appear in}} TSE 2000.) Methodology for Model Checking ASM: Lessons learned from the FLASH Case Study Kirsten Winter GMD First, Berlin Gurevich's Abstract State Machines (ASM) constitute a high-level specification language {{for a wide range}} of applications. The existing tool support for ASM was extended, in a previous work, to support computer-aided verification, in particular by model checking. In this paper we discuss the applicability of the model checking approach in general and describe the steps that are necessary to fit different kinds of ASM models for the model checking process. Along the example of the <b>FLASH</b> <b>cache</b> coherence protocol we show how model checking can support development and debugging of ASM models. We show the necessary refinement for the message passing behavior in the protocol and give examples for errors foun [...] ...|$|E
40|$|The {{focus of}} this thesis is on systems that employ both flash and {{magnetic}} disks as storage media. Considering the widely disparate I/O costs of flash disks currently on the market, our approach is a cost-aware one: we explore techniques that exploit the I/O costs of the underlying storage devices to improve I/O performance. We also study the asymmetric I/O properties of magnetic and flash disks and propose algorithms that {{take advantage of this}} asymmetry. Our work is geared towards database systems; however, most of the ideas presented in this thesis can be generalised to any data-intensive application. For the case of low-end, inexpensive flash devices with large capacities, we propose using them at the same level of the memory hierarchy as magnetic disks. In such setups, we study the problem of data placement, that is, on which type of storage medium each data page should be stored. We present a family of online algorithms {{that can be used to}} dynamically decide the optimal placement of each page. Our algorithms adapt to changing workloads for maximum I/O efficiency. We found that substantial performance benefits can be gained with such a design, especially for queries touching large sets of pages with read-intensive workloads. Moving one level higher in the storage hierarchy, we study the problem of buffer allocation in databases that store data across multiple storage devices. We present our novel approach to per-device memory allocation, under which both the I/O costs of the storage devices and the cache behaviour of the data stored on each medium determine the size of the main memory buffers that will be allocated to each device. Towards informed decisions, we found that the ability to predict the cache behaviour of devices under various cache sizes is of paramount importance. In light of this, we study the problem of efficiently tracking the hit ratio curve for each device and introduce a lowoverhead technique that provides high accuracy. The price and performance characteristics of high-end flash disks make them perfectly suitable for use as caches between the main memory and the magnetic disk(s) of a storage system. In this context, we primarily focus on the problem of deciding which data should be placed in the <b>flash</b> <b>cache</b> of a system: how the data flows from one level of the memory hierarchy to the others is crucial for the performance of such a system. Considering such decisions, we found that the I/O costs of the <b>flash</b> <b>cache</b> play a major role. We also study several implementation issues such as the optimal size of flash pages and the properties of the page directory of a <b>flash</b> <b>cache.</b> Finally, we explore sorting in external memory using external merge-sort, as the latter employs access patterns that can take full advantage of the I/O characteristics of flash memory. We study the problem of sorting hierarchical data, as such is necessary for a wide variety of applications including archiving scientific data and dealing with large XML datasets. The proposed algorithm efficiently exploits the hierarchical structure in order to minimize the number of disk accesses and optimise the utilization of available memory. Our proposals are not specific to sorting over flash memory: the presented techniques are highly efficient over magnetic disks as well. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|Abstract—The {{adoption}} of flash memory in high volume consumer {{products such as}} cell phones, tablet computers, digital cameras, and portable music players has driven down flash costs and increased flash quality. This trend is pushing flash memory into new applications, including enterprise computing. In enterprise data centers, servers containing flash-based Solid-State Drives (SSDs) are becoming common. However, data center architects prefer to deploy shared storage over direct-attached storage (DAS). Shared storage offers superior manageability, availability, and scalability compared to DAS. For these reasons, system designers want {{to reap the benefits}} of direct attached flash memory without decreasing the value of shared storage systems. Our solution is Mercury, a persistent, write-through host-side cache for flash memory. By designing Mercury as a hypervisor cache, we simplify integration and deployment into host environments. This paper presents our experience building a host-side <b>flash</b> <b>cache,</b> an architectural analysis of possible cache attachment points, and a performance evaluation using enterprise workloads. Our results show a 26 % improvement in the bandwidth observed by the Jetstress benchmark and a 500 % improvement in the I/O rate of an enterprise workload. I...|$|E
40|$|Flash is {{a widely}} used storage device that {{provides}} high density and low power, appealing properties for general purpose computing. Today, its usual application is in portable special purpose devices such as MP 3 players. In this paper we examine its use in the server domain— a more general purpose environment. Aggressive process scaling {{and the use of}} multi-level cells continues to improve density ahead of Moore’s Law predictions, making Flash even more attractive as a general purpose memory solution. Unfortunately, reliability limits the use of Flash. To seriously consider Flash in the server domain, architectural support must exist to address this concern. This paper first shows how Flash can be used in today’s server platforms as a disk cache. It then proposes two improvements. The first improves performance and reliability by splitting <b>Flash</b> based disk <b>caches</b> into separate read and write regions. The second improves reliability by employing a programmable Flash memory controller. It can change the error code strength (number of correctable bits) and the number of bits that a memory cell can store (cell density) according {{to the demands of the}} application. Our studies show that Flash reduces overall power consumed by the system memory and hard disk drive up to 3 times while maintaining performance. We also show that Flash lifetime can be improved by a factor of 20 when using a programmable Flash memory controller, if some performance degradation (below 5 %) is acceptable. ...|$|R
40|$|Computing system power {{consumption}} {{is a concern}} as it has financial and environmental implications. These concerns will increase in the future due to the current trends in data growth, information availability requirements, and increases {{in the cost of}} energy. Data growth is compounded daily because of the accessibility of portable devices, increased connectivity to the Internet, and a trend toward storing information electronically. These three factors also result in an increased demand for the data to be available for access at all times which results in more electronic devices requiring power. As more electricity is required the overall cost of energy increases due to demand and limited resource availability. The environment also suffers as most electricity is generated from fossil fuels which increase emission of carbon dioxide into the atmosphere. In order {{to reduce the amount of}} energy required while maintaining data availability researchers have focused on changing how data is accessed from hard drives. Hard drives have been found to consume 10 to 86 percent of a system 2 ̆ 7 s energy. Through changing the way data is accessed by implementing multi speed hard drives, algorithms that prefetch, cache, and batch data requests, or by implementing <b>flash</b> drive <b>caches</b> researchers have been able to reduce the energy required from hard drive operation. However, these approaches often result in reduced I/O performance or reduced data availability. This dissertation provides a new method of reducing hard drive energy consumption by implementing a prefetching technique that predicts a chain of future requests based upon previous request observations. The files to be prefetched are given to a caching system which uses a flash memory device for caching. This caching system implements energy sensitive algorithms to optimize the value of files stored in the flash memory device. Through prefetching files the hard drive on a system can be placed in a low power sleep state. This results in reduced {{power consumption}} while providing high I/O performance and data availability. Analysis of simulator results confirmed that this new method increased I/O performance and data availability over previous studies while also providing a higher level of energy savings. Out of 30 scenarios, the new method displayed better energy savings in 26 scenarios and better performance in all 30 scenarios over previous studies. The new method also displayed it could achieve results of 50. 9 percent less time and 34. 6 percent less energy for a workload over previous methodologies...|$|R
40|$|We present FlashStore, a high {{throughput}} persistent key-value store, {{that uses}} flash memory as a non-volatile cache between RAM and hard disk. FlashStore {{is designed to}} store the working set of key-value pairs on flash and use one flash read per key lookup. As the working set changes over time, space is made for the current working set by destag-ing recently unused key-value pairs to hard disk and recy-cling pages in the flash store. FlashStore organizes key-value pairs in a log-structure on flash to exploit faster sequential write performance. It uses an in-memory hash table to index them, with hash collisions resolved by a variant of cuckoo hashing. The in-memory hash table stores compact key sig-natures instead of full keys so as to strike tradeoffs between RAM usage and false flash read operations. FlashStore {{can be used as}} a high throughput persistent key-value storage layer for a broad range of server class ap-plications. We compare FlashStore with BerkeleyDB, an embedded key-value store application, running on hard disk and flash separately, so as to bring out the performance gain of FlashStore in not only using <b>flash</b> as a <b>cache</b> above hard disk but also in its use of flash aware algorithms. We use real-world data traces from two data center applica-tions, namely, Xbox LIVE Primetime online multi-player game and inline storage deduplication, to drive and evalu-ate the design of FlashStore on traditional and low power server platforms. FlashStore outperforms BerkeleyDB by up to 60 x on throughput (ops/sec), up to 50 x on energy efficiency (ops/Joule), and up to 85 x on cost efficiency (ops/sec/dollar) on the evaluated datasets. 1...|$|R
