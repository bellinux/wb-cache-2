495|499|Public
25|$|For 3D {{animation}} Bézier curves {{are often}} used to define 3D paths as well as 2D curves for <b>keyframe</b> interpolation.. Bézier curves are now very frequently used to control the animation easing in CSS and JavaScript.|$|E
25|$|Virtual {{entities}} {{may contain}} and {{be controlled by}} assorted attributes, such as transform values (location, orientation, and scale) stored in an object's transformation matrix. Animation is the change of an attribute over time. Multiple methods of achieving animation exist; the rudimentary form {{is based on the}} creation and editing of keyframes, each storing a value at a given time, per attribute to be animated. The 2D/3D graphics software will change with each <b>keyframe,</b> creating an editable curve of a value mapped over time, in which results in animation. Other methods of animation include procedural and expression-based techniques: the former consolidates related elements of animated entities into sets of attributes, useful for creating particle effects and crowd simulations; the latter allows an evaluated result returned from a user-defined logical expression, coupled with mathematics, to automate animation in a predictable way (convenient for controlling bone behavior beyond what a hierarchy offers in skeletal system set up).|$|E
25|$|Director Robert Zemeckis drew {{inspiration}} for the visual effects of Beowulf from experience with The Polar Express, which used motion capture technology to create three-dimensional images of characters. Appointing Jerome Chen, whom Zemeckis worked with on The Polar Express, the two decided to chart realism as their foremost goal. Over 450 graphic designers were chosen for the project, the largest team ever assembled for an Imageworks-produced movie as of 2007. Designers at Imageworks generated new animation tools for facial, body, and cloth design especially for the movie, and elements of <b>keyframe</b> animation were incorporated into the movie to capture the facial expressions of the actors and actresses. The mead hall battle scene {{near the beginning of}} the film, among others, required numerous props that served as additional markers; these markers allowed for a more accurate manifestation of a battlefield setting as the battle progressed. However, the data being collected by the markers slowed down the studios' computer equipment, and five months were spent developing a new save/load system that would increase the efficiency of the studios' resources. To aid in the process of rendering the massive quantities of information, the development team used cached data. In the cases that using cached data was not possible, the scenes were rendered using foreground occlusion, which involves the blurring of different overlays of a single scene in an attempt to generate a single scene film.|$|E
50|$|Recording is {{used for}} {{adjusting}} an object over a specific amount of time by placing and manipulating <b>keyframes.</b> The recording button is a red dot button adjacent to the play/pause features. When the button is selected, it lights up red and the dot turns white. Any adjustments made when the button is selected are saved as <b>keyframes.</b> <b>Keyframes</b> act as placeholders that solidify an object's characteristics at a single frame (anything from position and rotation to cropping and size). Using multiple recordings, an object shall reorient itself between the two <b>keyframes</b> to match each set characteristics. Recording can act {{as an alternative to}} movement behaviors that allow more in-depth adjustments.|$|R
40|$|In {{accessing}} large {{collections of}} digitized videos, {{it is often}} difficult to find both the appropriate video file and the portion of the video that is of interest. This paper describes a novel technique for determining <b>keyframes</b> that are different from each other and provide a good representation of the whole video. We use <b>keyframes</b> to distinguish videos from each other, to summarize videos, and to provide access points into them. The technique can determine any number of <b>keyframes</b> by clustering the frames in a video and by selecting a representative frame from each cluster. Temporal constraints are used to filter out some clusters and to determine the representative frame for a cluster. Desirable visual features can be emphasized in the set of <b>keyframes.</b> An application for browsing a collection of videos makes use of the <b>keyframes</b> to support skimming and to provide visual summaries...|$|R
30|$|This {{algorithm}} {{is able to}} summarize each shot in <b>keyframes</b> depending on the intensity of action of the scene in a computationally efficient manner. Moreover it provides low-motion activity <b>keyframes</b> that usually yields a better performance in the person detection phase.|$|R
500|$|In August 2016, Mappa Producer Masao Maruyama Said in an interview: [...] "For 4~5 years, I kept {{searching for}} a {{suitable}} director to complete Kon's work. Before his death, the storyboard and script, even part of the <b>keyframe</b> film was already completed. Then I thought, even if someone can mimic Kon's work, {{it would still be}} clear that it's only an imitation. For example, if Mamoru Hosoda took the director's position, the completed Dreaming Machine would still be a good piece of work. However, that would make it Hosoda's movie, not Kon's. Dreaming Machine should be Kon's movie, him and only him, not someone else's. That means we cannot and should not [...] "compromise" [...] only to finish it. I spent years to finally reach this hard conclusion. Instead, we should take only Kon's [...] "original concept", and let somebody turn it into a feature film. By doing so, the completed piece could 100% be that person's work, and I'm OK with that. I also considered about doing a documentary of Kon." ...|$|E
50|$|Max <b>Keyframe</b> {{interval}} - typically {{this is done}} by multiplying the framerate of {{the movie}} (e.g. 23.976 or 25 or 29.97) by 10 (e.g. 240, 250, 300), to deliver an automatic <b>keyframe</b> insert every 10 seconds. Typically left at the default value under DivX.|$|E
50|$|Animating {{the object}} is similar to motion {{tweening}} in Macromedia Flash. A <b>keyframe</b> is created with the object in the desired position. A second <b>keyframe</b> is then created and {{the object is}} moved to the new desired position. One can then preview this with the convenient Preview button.|$|E
40|$|In this paper, {{we develop}} a new model for {{recognizing}} human actions. An action is modeled as a very sparse sequence of temporally local discriminative <b>keyframes</b> – collections of partial key-poses of the actor(s), depicting key states in the action sequence. We cast the learning of <b>keyframes</b> in a max-margin discriminative framework, where we treat <b>keyframes</b> as latent variables. This allows us to (jointly) learn a set of most discriminative <b>keyframes</b> while also learning the local temporal context between them. <b>Keyframes</b> are encoded using a spatially-localizable poselet-like representation with HoG and BoW components learned from weak annotations; we rely on structured SVM formulation to align our components and mine for hard negatives to boost localization performance. This results in a model that supports spatio-temporal localization and is insensitive to dropped frames or partial observations. We show classification performance that is competitive {{with the state of}} the art on the benchmark UT-Interaction dataset and illustrate that our model outperforms prior methods in an on-line streaming setting. 1...|$|R
50|$|Example motions {{are often}} created through <b>keyframing</b> or motion capture. However, <b>keyframing</b> is labor-intensive and lacks {{varieties}} of motion, and both processes result in motions that are time-consuming to alter.Motion interpolation provides a much faster alternative to creating new motions {{through the same}} means.|$|R
5000|$|<b>Keyframed,</b> {{procedural}} {{and real}} time physics based animation support ...|$|R
5000|$|Animation: armatures, forward kinematics, inverse kinematics, morph and <b>keyframe</b> ...|$|E
5000|$|<b>Keyframe</b> {{animation}} with linear, Bézier, and TCB animation curves.|$|E
5000|$|<b>KeyFrame</b> ‘Tweening:’ The {{automatic}} {{creation of}} averaged in-between frames creating camera and actor movement animations ...|$|E
5000|$|... scenes can {{be placed}} as <b>keyframes</b> along an {{animation}} timeline ...|$|R
40|$|This paper {{presents}} {{methods of}} generating compact pictorial sum-marizations of video. By calculating {{a measure of}} shot impor-tance video can be summarized by de-emphasizing or discarding less important information, such as repeated or common scenes. In contrast to other approaches that present <b>keyframes</b> for each shot, this measure allows summarization by presenting only the most important shots. Selected <b>keyframes</b> can also be resized depending on their relative importance. We present an efficient packing algorithm that constructs a pictorial representation from differently-sized <b>keyframes.</b> This results in a compact and visu-ally pleasing summary reminiscent of a comic book. 1...|$|R
40|$|We {{discuss a}} method for {{creating}} animations that allows the animator to sketch an animation by setting {{a small number of}} <b>keyframes</b> on a fraction of the possible degrees of freedom. Motion capture data is then used to enhance the animation. Detail is added to degrees of freedom that were <b>keyframed,</b> a process we call texturing. Degrees of freedom that were not <b>keyframed</b> are synthesized. The method takes advantage of the fact that joint motions of an articulated figure are often correlated, so that given an incomplete data set, the missing degrees of freedom can be predicted from those that are present...|$|R
5000|$|Pinky Dinky Doo (2005-2011) co-production with Sesame Workshop, <b>Keyframe</b> Digital Prod. Inc., and Abrams Gentile Entertainment ...|$|E
5000|$|... #Caption: Rear of Australia Fair. Note fanfold book playing through <b>keyframe</b> in centre {{and belt}} drive.|$|E
50|$|On May 1, 2012, {{journalist}} David Hudson, {{formerly of}} GreenCine and Mubi, joined <b>Keyframe</b> as chief correspondent.|$|E
5000|$|Animation - <b>Keyframed</b> {{animation}} and raw channel manipulation (CHOPs), {{motion capture}} support ...|$|R
40|$|International audienceIn this paper, {{we propose}} a new {{strategy}} for near-duplicate video retrieval {{that is based on}} shot aggregation. We investigate different methods for shot aggregation with the main objective to solve the difficult trade-off between performance, scalability and speed. The proposed short aggregation is based on two steps. The first step consists of <b>keyframes</b> selection. And the second one is the aggregation of the <b>keyframes</b> per shot. The aggregation is performed by applying Fisher vector on the descriptors computed on the selected <b>keyframes.</b> We demonstrate that the scalability and the speed are tackled by a sparse video analysis approach (i. e. extracting only few <b>keyframes)</b> combined with shot aggregation, while the performance is discussed around the choice of the aggregation strategy. The performance is evaluated on the CC_WEB_VIDEO dataset that is designed for the near-duplicate video retrieval assessment and for which some experiments have been conducted by different authors...|$|R
40|$|In this paper, {{we propose}} a new {{bi-directional}} 2 -D mesh representation of video ob- jects, which utilizes {{forward and backward}} reference frames (<b>keyframes).</b> This frame- work extends the previous uni-directional mesh representation to enable efficient rendering, editing, and superresolution of video objects {{in the presence of}} occlusion by allowing bi-directional texture mapping as in MPEG B-frames. The video object of interest is tracked between two successive <b>keyframes</b> (which can be automatically or interactively selected) both in forward and backward directions. <b>Keyframes</b> provide the texture of the video object, whereas its motion is modeled by forward and backward 2 -D meshes. In addition, we employ "validity maps," associated with each 2 -D mesh, which allow selective texture mapping from the <b>keyframes.</b> Experimental results for efficient video object editing and object-based video resolution enhancement in the presence of self occlusion are presented to demonstrate the effectiveness of the proposed representation...|$|R
5000|$|... "In the Time of Fear: On the Films of Ingmar Bergman and Derek Jarman," [...] <b>Keyframe,</b> 2011 ...|$|E
5000|$|A module {{provides}} {{support for}} JavaScript, CSS3 transition and CSS3 <b>keyframe</b> animation hooks within existing core and custom directives.|$|E
5000|$|<b>Keyframe</b> {{animation}} is {{the least}} automated of the processes to create animation data although it delivers {{the maximum amount of}} control over the animation. It is often used in combination with other techniques to deliver the final polish to the animation. The <b>keyframe</b> data can be made of scalar values defining the morph targets coefficients or rotation and translation values of the bones in models with a bone based rig. Often to speed up the <b>keyframe</b> animation process a control rig is used by the animation. The control rig represents a higher level of abstraction that can act on multiple morph targets coefficients or bones at the same time. For example, a [...] "smile" [...] control can act simultaneously on the mouth shape curving up and the eyes squinting.|$|E
5000|$|All {{resulting}} constrained animation can be {{collapsed into}} standard <b>keyframes</b> for further editing.|$|R
40|$|This paper {{considers}} {{the design of}} a camera path through a scene. Given a valid set of <b>keyframes</b> for the camera, we want to build a camera path that avoids collisions with the scene. The first problem is to define a camera path that interpolates the <b>keyframes.</b> We build a rational interpolating motion using rational interpolation of both position and orientation. The second problem is to detect collisions of the planned path with the scene. A spatial decomposition is used to accelerate this detection. The third problem is to correct the path to avoid the detected collisions. <b>Keyframes</b> are added in the neighbourhood of the collision to correct the collision...|$|R
40|$|In {{a simple}} {{approach}} to the affine motion interpolation problem, an affine spline motion is generated that interpolates a given sequence of <b>keyframes</b> and approximately satisfies rigidity constraints and certain optimization criteria. An affine spline motion is generated so as to interpolate the given <b>keyframes,</b> and then it is progressively refined by knot insertion and degree elevation using an iterative optimization procedure...|$|R
5000|$|In {{order to}} {{facilitate}} instant random access to any frame during editing, all SheerVideo codecs use only intraframe compression, meaning that every frame is a <b>keyframe.</b>|$|E
50|$|On June 22, 2010, <b>Keyframe</b> Digital Productions Inc. {{reported}} that they had been given the contract for visual effects on the first thirteen episodes of Lost Girl.|$|E
5000|$|<b>Keyframe</b> Digital Productions Inc. - Imaginext (2002), Pinky Dinky Doo (2002), The Littlest Light On The Christmas Tree (2004), Peggy's Little Harbour (2012), Terrific Trucks (2016), Ollie (2017), ...|$|E
40|$|This work {{presents}} an approach on high-level semantic feature detec- tion in video sequences. <b>Keyframes</b> are selected {{to represent the}} visual content of the shots. Then, low-level feature extraction is performed on the <b>keyframes</b> and a feature vector including color and texture features is formed. A region thesaurus that contains all the high-level features is constructed using a subtractive clusterin...|$|R
50|$|Elements of {{a motion}} {{graphics}} project can be animated by various means, {{depending on the}} capabilities of the software. These elements may be in the form of art, text, photos, and video clips, to name a few. The most popular form of animation is <b>keyframing,</b> in which properties of an object can be specified at certain points in time by setting a series of <b>keyframes</b> so that the properties of the object can be automatically altered (or tweened) in the frames between <b>keyframes.</b> Another method involves a behavior system such as is found in Apple Motion that controls these changes by simulating natural forces without requiring the more rigid but precise <b>keyframing</b> method. Yet another method involves the use of formulas or scripts, such as the expressions function in Adobe After Effects or the creation of ActionScripts within Adobe Flash. Computers are capable of calculating and randomizing changes in imagery to create the illusion of motion and transformation. Computer animations can use less information space (computer memory) by automatically tweening, a process of rendering the key changes of an image at a specified or calculated time. These key poses or frames are commonly referred to as <b>keyframes</b> or low CP. Adobe Flash uses computer animation tweening as well as frame-by-frame animation and video.|$|R
40|$|Self-localization and mapping are {{important}} for indoor mobile robot. We report a robust algorithm for map building and subsequent localization especially suited for indoor floor-cleaning robots. Common methods, for example, SLAM, can easily be kidnapped by colliding or disturbed by similar objects. Therefore, <b>keyframes</b> global map establishing method for robot localization in multiple rooms and corridors is needed. Content-based image matching {{is the core of}} this method. It is designed for the situation, by establishing <b>keyframes</b> containing both floor and distorted wall images. Image distortion, caused by robot view angle and movement, is analyzed and deduced. And an image matching solution is presented, consisting of extraction of overlap regions of <b>keyframes</b> extraction and overlap region rebuild through subblocks matching. For improving accuracy, ceiling points detecting and mismatching subblocks checking methods are incorporated. This matching method can process environment video effectively. In experiments, less than 5 % frames are extracted as <b>keyframes</b> to build global map, which have large space distance and overlap each other. Through this method, robot can localize itself by matching its real-time vision frames with our <b>keyframes</b> map. Even with many similar objects/background in the environment or kidnapping robot, robot localization is achieved with position RMSE < 0. 5 [*]m...|$|R
