34|258|Public
50|$|Unlike a dope vector, a <b>data</b> <b>descriptor</b> {{does not}} contain address information.|$|E
50|$|In computing, a <b>data</b> <b>descriptor</b> is a {{structure}} containing information that describes data.|$|E
50|$|GrADS uses a 4-Dimensional data environment: longitude, latitude, {{vertical}} level, and time. Data {{sets are}} placed within the 4-D space {{by use of}} a <b>data</b> <b>descriptor</b> file. GrADS interprets station data as well as gridded data, and the grids may be regular, non-linearly spaced, Gaussian, or of variable resolution. Data from different data sets may be graphically overlaid, with correct spatial and time registration. It uses the ctl mechanism to join differing time group data sets. Operations are executed interactively by entering FORTRAN-like expressions at the command line. A rich set of built-in functions are provided, but users may also add their own functions as external routines written in any programming language.|$|E
40|$|My {{notes from}} the June 19 - 20 SRP working group include {{substantial}} changes requested to the description of <b>data</b> buffer <b>descriptors.</b> However I can find no mention of these in the official minutes of that meeting (01 - 195). Since the minutes do spell out much more trivial changes, I hesitate to include these changes without approval. Revision 0 of this document is the affected portions of SRP with change bars from SRP revision 07. Revision 1 of this document incorporates changes from the July 19 - 20 SRP working group, including the changes described by 01 - 227 r 1 (SRP Buffer Descriptor Formats). Revision 2 of this document incorporates changes from the August 21 SRP teleconference. 5. 4. 2 <b>Data</b> buffer <b>descriptors</b> 5. 4. 2. 1 Overview An SRP_CMD request (see 6. 8) may contain a data-out buffer descriptor, a data-in buffer descriptor, both or neither, depending upon the data transfer(s) requested by the SCSI command. The format of each <b>data</b> buffer <b>descriptor</b> is specified by a format code value. Some <b>data</b> buffer <b>descriptor</b> format code values use {{the contents of a}} count field to further specify the <b>data</b> buffer <b>descriptor</b> format. Table 2 defines <b>data</b> buffer <b>descriptor</b> format code values. Table 2 - <b>Data</b> buffer <b>descriptor</b> formats <b>Data</b> buffer <b>descriptor</b> format cod...|$|R
5000|$|<b>Data</b> <b>descriptors</b> {{may be used}} in compilers, as a {{software}} structure at run time in languages like Ada [...] or PL/I, or as a hardware structure in some computers such as Burroughs large systems.|$|R
5000|$|<b>Data</b> <b>descriptors</b> are {{typically}} used at run-time to pass argument information to called subroutines. HP OpenVMS and Multics [...] have system-wide language-independent standards for argument descriptors. Descriptors {{are also used}} to hold information about data that is only fully known at run-time, such as a dynamically allocated array.|$|R
50|$|The default calling {{convention}} for programs {{written in the}} PL/I language passes all arguments by reference, although other conventions may optionally be specified. The arguments are handled differently for different compilers and platforms, but typically the argument addresses are passed via an argument list in memory. A final, hidden, address may be passed pointing to an area to contain the return value. Because of {{the wide variety of}} data types supported by PL/I a <b>data</b> <b>descriptor</b> may also be passed to define, for example, the lengths of character or bit strings, the dimension and bounds of arrays (dope vectors), or the layout and contents of a data structure. Dummy arguments are created for arguments which are constants or which do not agree with the type of argument the called procedure expects.|$|E
40|$|Archaeologists {{continue}} {{to search for}} techniques {{that enable them to}} analyze archaeological data efficiently with artificial intelligence approaches increasingly employed to create new knowledge from archaeological data. The {{purpose of this paper is}} to investigate the application of Pattern Recognition methods in detection of buried archaeological sites of the semi-arid Khorramabad Plain located in west Iran. This environment has provided suitable conditions for human habitation for over 40, 000 years. However, environmental changes in the late Pleistocene and Holocene have caused erosion and sedimentation resulting in burial of some archaeological sites making archaeological landscape reconstructions more challenging. In this paper, the environmental variables that have influenced formation of archaeological sites of the Khorramabad Plain are identified through the application of Arc GIS. These variables are utilized to create an accurate predictive model based on the application of One-Class classification Pattern Recognition techniques. These techniques can be built using data from one class only, when the data from other classes are difficult to obtain, and are highly suitable in this context. The experimental results of this paper confirm one-class classifiers, including Auto-encoder Neural Network, k-means, principal component analysis <b>data</b> <b>descriptor,</b> minimum spanning tree <b>data</b> <b>descriptor,</b> k-nearest neighbour and Gaussian distribution as promising applications in creating an effective model for detecting buried archaeological sites. Among the investigated classifiers, minimum spanning tree <b>data</b> <b>descriptor</b> achieved the best performance on the Khorramabad Plain data set. © 2016 Elsevier Ltd...|$|E
40|$|Data {{need to be}} {{more than}} just available, they need to be discoverable and understandable. Iain Hrynaszkiewicz {{introduces}} Nature’s new published data paper format, a <b>Data</b> <b>Descriptor.</b> Peer-review and curation of these data papers will facilitate open access to knowledge and interdisciplinary research, pushing the boundaries of discovery. Some of the most tangible benefits of open data stem from social and interdisciplinary sciences as these fields require effective cross-disciplinary communication...|$|E
50|$|For {{interval}} and ratio level <b>data,</b> further <b>descriptors</b> include the variable's skewness and kurtosis.|$|R
40|$|This {{proposal}} describes {{changed to}} be made in SPC- 3 to support OSD quota enforcement. Also, per agreement of the March CAP working group, the formats for OSD sense <b>data</b> <b>descriptors</b> will move to the next OSD revision. Revision History r 1 r 2 Eliminate dynamic attributes quotes, restore partition capacity quota, add default value for the partition capacity quota as a root quota. Note: These changes eliminate the need for a Collection Quotas attributes page. Reduce this proposal to just SPC- 3 changes. Put OSD changes in a separate document (04 - 095) for consideration as part of the OSD Letter Ballot. Move those OSD specific sense <b>data</b> <b>descriptors</b> already defined in SPC- 3 out of SPC- 3 and in to OSD (also affects the 04 - 095 proposal). All changes agreed by March CAP working group. Changes that are new to this revision are marked with change bars. SPC- 3 r 17 Change...|$|R
40|$|Pooled {{analysis}} of secondary data increases {{the power of}} research and enables scientific discovery in nutritional epidemiology. Information on study characteristics that determine data quality is needed to enable correct reuse and interpretation of data. This study aims to define essential quality characteristics for data from observational studies in nutrition. First, a literature review was performed to get an insight on existing instruments that assess the quality of cohort, case-control, and cross-sectional studies and dietary measurement. Second, 2 face-to-face workshops were organized to determine the study characteristics that affect data quality. Third, consensus on the <b>data</b> <b>descriptors</b> and controlled vocabulary was obtained. From 4884 papers retrieved, 26 relevant instruments, containing 164 characteristics for study design and 93 characteristics for measurements, were selected. The workshop and consensus process resulted in 10 descriptors allocated to "study design" and 22 to "measurement" domains. <b>Data</b> <b>descriptors</b> were organized as an ordinal scale of items to facilitate the identification, storage, and querying of nutrition data. Further integration of an Ontology for Nutrition Studies will facilitate interoperability of data repositories. © 2017 American Society for Nutrition...|$|R
40|$|This <b>data</b> <b>descriptor</b> {{outlines}} {{a shared}} neuroimaging dataset from the UCLA Consortium for Neuropsychiatric Phenomics, {{which focused on}} understanding the dimensional structure of memory and cognitive control (response inhibition) functions in both healthy individuals (130 subjects) and individuals with neuropsychiatric disorders including schizophrenia (50 subjects), bipolar disorder (49 subjects), and attention deficit/hyperactivity disorder (43 subjects). The dataset includes an extensive set of task-based fMRI assessments, resting fMRI, structural MRI, and high angular resolution diffusion MRI. The dataset is shared through the OpenfMRI project, and is formatted according to the Brain Imaging Data Structure (BIDS) standard...|$|E
3000|$|In {{reviewing}} the preceding research, {{there have been}} no cases where a regression analysis could be performed to estimate the proper royalty rate and up-front payment using the formula derived from the regression of the dataset of historical licensing data (Lee et al. 2016). This study suggests the way to estimate the proper royalty rate and up-front payment using multiple <b>data</b> <b>descriptor</b> we can get easily as input and {{can be used as a}} simple tool to answer the basic question, “What would be the most determining factors if I perform regression analysis using several independent variables?” [...]...|$|E
40|$|This <b>data</b> <b>descriptor</b> {{introduces}} data on {{healthy food}} supplied by supermarkets {{in the city}} of Amsterdam, The Netherlands. In addition to two neighborhood variables (i. e., share of autochthons and average housing values), the data comprises three street network-based accessibility measures derived from analyses using a geographic information system. Data are provided on a spatial micro-scale utilizing grid cells with a spatial resolution of 100 m. We explain how the data were collected and pre-processed, and how alternative analyses can be set up. To illustrate the use of the data, an example is provided using the R programming language. View Full-Tex...|$|E
40|$|We {{present a}} {{framework}} for multi-camera video surveillance. The framework consists of three phases: detection, representation, and recognition. The detection phase handles multi-source spatio-temporal data fusion for efficiently and reliably extracting motion trajectories from video. The representation phase summarizes raw trajectory data to construct hierarchical, invariant, and content-rich descriptions of the motion events. Finally, the recognition phase deals with event classification and identification on the <b>data</b> <b>descriptors.</b> Through empirical study in a parking-lot surveillance setting, we show that our spatio-temporal fusion scheme and biased sequence-data learning method are highly effective in identifying suspicious events...|$|R
40|$|The {{ensemble}} communication library exploits overlapping {{of message}} aggregation (computation) and DMA transfers (communication) for embedded multi-processor systems. In contrast to traditional communication libraries, ensemble operates on n-dimensional <b>data</b> <b>descriptors</b> {{that can be}} used to specify often-occurring data access patterns in n-dimensional arrays. This allows ensemble to setup a three-stage pack-transfer-unpack pipeline, eectively overlapping message aggregation and DMA transfers. ensemble is used to support Spar/Java, a Java-based language with SPMD annotations. Measurements on a TriMedia-based multi-processor system show that ensemble increases performance up to 39 % for peer-to-peer communication, and up to 34 % for all-to-all communication. 1...|$|R
40|$|Current {{methods for}} the {{accurate}} recognition of instruments within music {{are based on}} discriminative <b>data</b> <b>descriptors.</b> These are features of the music fragment that capture {{the characteristics of the}} audio and suppress details that are redundant for the problem at hand. The extraction of such features from an audio signal requires the user to set certain parameters. We propose a method for optimizing the parameters for a particular task {{on the basis of the}} Simulated Annealing algorithm and Support Vector Machine classification. We show that using an optimized set of audio features improves the recognition accuracy of drum sounds in music fragments...|$|R
40|$|This <b>data</b> <b>descriptor</b> {{introduces}} the dataset of the transcriptome of low-phosphorus tolerant soybean (Glycine max) variety NN 94 - 156 under phosphorus-deficient and -sufficient conditions. This data {{is comprised of}} the transcriptome datasets (four libraries) acquired from roots and leaves of the soybean plants challenged with low-phosphorus, which allows further analysis whether systemic tolerance response to low phosphorus stress occurred. We describe the detailed procedure of how plants were prepared and treated and how the data were generated and pre-processed. Further analyses of this data {{would be helpful to}} improve our understanding of molecular mechanisms of low-phosphorus stress in soybean...|$|E
40|$|PAWS (Parallel Application WorkSpace) is a {{software}} infrastructure {{for use in}} connecting separate parallel applications within a component-like model. A central PAWS Controller coordinates the linking of serial or parallel applications across a network {{to allow them to}} share parallel data structures such as multidimensional arrays. Applications use the PAWS API to indicate which data structures are to be shared and at what points the data is ready to be sent or received. PAWS implements a general parallel <b>data</b> <b>descriptor,</b> and automatically carries out parallel layout remapping when necessary. Connections can be dynamically established and dropped, and can use multiple data transfer pathways between applications. PAWS uses the NEXUS communication library and is independent of the application’s parallel communication mechanism. 1...|$|E
40|$|Abstract — We {{introduce}} a framework {{to predict the}} landing behaviour of a Micro Air Vehicle (MAV) from {{the appearance of the}} landing surface. We approach this problem by learning a mapping from visual texture observed from an onboard camera to the landing behaviour on a set of sample materials. In this case we exemplify our framework by predicting the yaw angle of the MAV after landing. Our framework demonstrates the applicability of established texture classification methods usually tested on stationary camera setups for the more challenging case of textures observed from a MAV. Results for supervised training demonstrate good estimation of the landing behaviour and motivate future work to implement autonomous decision making strategies and other behaviour predictions based on imagery. motion traces energy accuracy motion summaries training flights visual data descriptors actual flight visual <b>data</b> <b>descriptor</b> I...|$|E
5000|$|... 0xFB: Additional {{white point}} <b>data.</b> 2× 5-byte <b>descriptors,</b> padded with [...]|$|R
50|$|Typical exact array access {{analysis}} include linearization and atom images. Summary {{methods can}} be further divided into array sections, bounded regular sections using triplet notation, linear-constraint methods such as <b>data</b> access <b>descriptors</b> and array region analysis.|$|R
40|$|A {{simulator}} for connectionist networks {{which uses}} gradient methods of nonlinear optimization for network learning is described. The simulator (GRADSIM) {{was designed for}} temporal flow model connectionist networks. The complete gradient is computed for networks of general connectivity, including recurrent links. The simulator is written in C, uses simple network and <b>data</b> <b>descriptors</b> for flexibility, and is easily modified for new applications. A version of the simulator which precompiles the network objective function and gradient computations for greatly increased processing speed is also described. Benchmark results for the simulator running on the DEC VAX 8650, SUN 3 / 260 and CYBER 205 are presented...|$|R
40|$|There is wording in {{the sense}} <b>data</b> <b>descriptor</b> format that {{requires}} certain validity bits to always be set to 1. This assumes that, whenever the associated descriptors are not needed, they are never returned. However this implementation requires more firmware effort than an implementation that simply returns all the descriptors (whether needed or not) and fills in the valid bits appropriately. This implementation is also easiest to translate to fixed descriptor format when required. My goal is to remove the restrictive wording on the validity bit descriptions {{so that they are}} not required to always be set to 1. SPC- 4 changes: 4. 5. 2 Descriptor format sense data 4. 5. 2. 1 Descriptor format sense data overview The descriptor format sense data for response codes 72 h (current errors) and 73 h (deferred errors) is defined in table 25...|$|E
40|$|The {{database}} comprises 10 excel files. The file "Nichols and Stolze_Vegetation 1972 - 73. xlsx" {{contains the}} vegetation observations from the 34 sampling sites which spanned ecosystems from the arctic tree-line {{to the high}} arctic in 1972 - 73. The additional nine xlsx files report two years of pollen data from Tauber (TAU) and Tuffy (TFY) static samplers and from living moss and lichen polsters (POL). Pollen numbers are given for standard counts. Some samples were only scanned for rare pollen types and their presence is noted as "SCAN" in the original xlsx workbooks and as "# 1 " in the uploaded datasets. The pollen data from the Tauber and Tuffy samplers (TAU&TFY) are presented in four xlsx files. All pollen data files include site and sample information and a letter code for the documented pollen types. "Nichols and Stolze_TAU&TFY_Count Data. xlsx" records the pollen counts at pollen sum 1, pollen sum 2, and pollen sum 3. "Nichols and Stolze_TAU&TFY_Percentage Data. xlsx" gives the percentage data for pollen sum 1, pollen sum 2, and pollen sum 3. The percentage calculation {{is based on a}} pollen sum excluding local palynomorphs which may be over-represented: Cyperaceae, Ericaceae, Filipendula, Lycopodium clavatum, Lycopodium selago, Polypodium, Pteridium, Selago, and Sphagnum. "Nichols and Stolze_TAU&TFY_Absolute Data. xlsx" provides the absolute pollen data as absolute numbers/pre-treated sample and absolute numbers/cm 2 for the final pollen sum. The datasheets include information for the calculation of the absolute values and the equations are given in the <b>data</b> <b>descriptor</b> prepared for Scientific Data. "Nichols and Stolze_TAU&TFY_SPIN&SPIC Data. xlsx" gives the absolute data per trap and per cm 2 for Pinus and Picea pollen. For this method, the Pinus and Picea pollen of an entire slide were recorded. Data for calculation of the absolute values are also given, with equations for the calculation of the absolute data and description of the method in the <b>data</b> <b>descriptor.</b> The moss and lichen pollen data are presented in five excel files. The count data obtained at pollen sum 1, pollen sum 2, and pollen sum 3 are given in "Nichols and Stolze_POL_Count Data. xlsx". "Nichols and Stolze_POL_Percentage Data. xlsx" gives the percentage data at pollen sum 1, pollen sum 2, and pollen sum 3. Once again, the percentage calculation is based on a pollen sum excluding Cyperaceae, Ericaceae, Filipendula, Lycopodium clavatum, Lycopodium selago, Polypodium, Pteridium, Selago, and Sphagnum. "Nichols and Stolze_POL_Absolute Data. xlsx" provides the absolute pollen data (abs/g dry weight) for the final pollen sum. The absolute data for Pinus and Picea pollen (abs/g dry weight) recorded by counting an entire pollen slide (see above) are given in "Nichols and Stolze_POL_SPIN&SPIC Data. xlsx". "Nichols and Stolze_POL_Sample Recovery Data. xlsx" records the degree (percentage) of sample recovery. Calculations are based on the numbers of exotic Lycopodium spores added to each sample and counted per microscope slide. All files reporting absolute data and the recovery factor for polster samples contain information required for the calculations; the equations are given in the <b>data</b> <b>descriptor</b> prepared for Scientific Data...|$|E
40|$|Abstract: In {{this paper}} we present {{preliminary}} results {{for a new}} framework in identification of predictor models for unknown systems, which builds on recent devel-opments of statistical learning theory. The three key elements of our approach are: the unknown mechanism that generates the observed data (referred to as the remote data generation mechanism – DGM), a selected family of models, with which we want to describe the observed data (the <b>data</b> <b>descriptor</b> model – DDM), and a consistency cri-terion, which serves to assess whether a given observation {{is compatible with the}} selected model. The identification procedure will then select a model within the assumed family, according to some given optimality objective (for instance, accurate prediction), and which is consistent with the observations. To the optimal model, we attach a certificate of reliability, that is a statement of probability that the computed model will be consistent with future unknown data...|$|E
40|$|International audienceProvenance traces {{captured}} by scientific workflows {{can be useful}} for designing, debugging and maintenance. However, our experience suggests that they are of limited use for reporting results, in part because traces do not comprise domain-specific annotations needed for explaining results, and the black-box nature of some workflow activities. We show that by basic mark-up of the data processing within activities and using a set of domain specific label generation functions, standard workflow provenance can be utilised as a platform for the labelling of data artefacts. These labels can in turn aid selection of data subsets and proxy for <b>data</b> <b>descriptors</b> for shared datasets...|$|R
40|$|Often, {{applications}} from biology and medical imaging lead to data on non-Euclidean spaces. On such spaces the Euclidean {{concept of a}} mean forks into several canonical generalizations of non- Euclidean means. More involved <b>data</b> <b>descriptors,</b> for instance principal components generalize into even more complicated concepts. (Semi) -intrinsic statistical analysis allows to study inference on descriptors that can be represented as elements of another non-Euclidean space. We give examples for geodesic principal components on shape spaces and concentric small circles on spheres. In particular, {{with respect to the}} statistical inference via central limit theorems, due to the geometry of the spaces, we find curious non-Euclidean phenomena...|$|R
40|$|Data {{narratives}} or data {{stories have}} emerged as {{a new form of}} the scholarly communication focused on data. In this paper, we explore the potential value of data narratives and the requirements for data stories to enhance scholarly communication. We examine three types of data stories that form a continuum from the less to the more structured: the DataONE data stories, the Data Curation Profiles, and the <b>Data</b> <b>Descriptors</b> from the journal Scientific Data. We take the position that these data stories will increase the value of scholarly communication if they are linked to the datasets and to the publications that describe results, and have instructional value...|$|R
40|$|Abstract—Using {{gradient}} {{information for}} a pixel-based cost function for stereo matching has lacked adequate {{attention in the}} literature. This paper provides experimental {{evidence to show that}} the gradient as a <b>data</b> <b>descriptor</b> outperforms other pixelbased functions such as absolute differences and the Birchfield and Tomasi cost functions. The cost functions are tested against stereo image datasets where ground truth data is available. Furthermore, analysing the effect of the cost functions when exposure and illumination settings are different between the left and right camera is analysed. Not only has the performance of the cost functions been analysed, but also analysis into “why ” one cost function is better than another. The analysis tests the global and spacial optimality of the cost function, showing that the gradient information returns stronger minima than the other two. These results are aimed at future research towards the design of a ne...|$|E
40|$|In {{this paper}} we present {{preliminary}} results {{for a new}} framework in identification of predictor models for unknown systems, which builds on recent developments of statistical learning theory. The three key elements of our approach are: the unknown mechanism that generates the observed data (referred to as the remote data generation mechanism - DGM), a selected family of models, with which we want to describe the observed data (the <b>data</b> <b>descriptor</b> model - DDM), and a consistency criterion, which serves to assess whether a given observation {{is compatible with the}} selected model. The identification procedure will then select a model within the assumed family, according to some given optimality objective (for instance, accurate prediction), and which is consistent with the observations. To the optimal model, we attach a certificate of reliability, that is a statement of probability that the computed model will be consistent with future unknown dat...|$|E
40|$|This dataset {{describes}} the public transport networks of 27 {{cities across the}} world in multiple easy-to-use data formats. These data formats include network edge lists, temporal network event lists, SQLite databases, GeoJSON files, and General Transit Feed Specification (GTFS) compatible ZIP-files. The original source data for creating these networks has been published by public transport agencies according to the GTFS data format. To produce the network data extracts for each city, the original data have been curated for errors, filtered spatially and temporally and augmented with walking distances between public transport stops using data from OpenStreetMap. Cities included in this data set: Adelaide, Antofagasta, Athens, Belfast, Berlin, Bordeaux, Brisbane, Canberra, Detroit, Dublin, Grenoble, Helsinki, Kuopio, Lisbon, Luxembourg, Melbourne, Nantes, Palermo, Paris, Prague, Rennes, Rome, Sydney, Toulouse, Turku, Venice, and Winnipeg. More detailed documentation of the data will be added once the accompanying <b>data</b> <b>descriptor</b> manuscript has been finalised...|$|E
40|$|In {{angiogenesis}} with concurrent inflammation, many pathways are activated, some {{linked to}} VEGF and others largely VEGF-independent. Pathways involving inflammatory mediators, chemokines, and micro-RNAs may play {{important roles in}} maintaining a pro-angiogenic environment or mediating angiogenic regression. Here, we describe a gene expression dataset to facilitate exploration of pro-angiogenic, pro-inflammatory, and remodelling/normalization-associated genes during both an active capillary sprouting phase, and in the restoration of an avascular phenotype. The dataset was generated by microarray analysis of the whole transcriptome in a rat model of suture-induced inflammatory corneal neovascularisation. Regions of active capillary sprout growth or regression in the cornea were harvested and total RNA extracted from four biological replicates per group. High quality RNA was obtained for gene expression analysis using microarrays. Fold change of selected genes was validated by qPCR, and protein expression was evaluated by immunohistochemistry. We provide a gene expression dataset that may be re-used to investigate corneal neovascularisation, and may also have implications in other contexts of inflammation-mediated angiogenesis. Funding Agencies|Swedish Research Council [2012 - 2472]; Bayer HealthCare AB, Solna, Sweden; Bioinformatics Infrastructure for Life Sciences (BILS) Sweden The publication is a peer-reviewed description of a research dataset. Aims {{and scope of the}} journal: Scientific Data  primarily publishes <b>Data</b> <b>Descriptors,</b> a new type of publication that provides detailed descriptions of research datasets, including the methods used to collect the data and technical analyses supporting the quality of the measurements. <b>Data</b> <b>Descriptors</b> focus on helping others reuse data, rather than testing hypotheses, or presenting new interpretations, methods or in-depth analyses. </p...|$|R
40|$|When {{there is}} a need to {{understand}} the data stored in a database, one of the main requirements is being able to extract knowledge in the form of rules. Classification strategies allow extracting rules almost naturally. In this paper, a new classification strategy is presented that uses hyper-rectangles as <b>data</b> <b>descriptors</b> to achieve a model that allows extracting knowledge in the form of classification rules. The participation of an expert for training the model is discussed. Finally, the results obtained using the databases from the UCI repository are presented and compared with other existing classification models, showing that the algorithm presented requires less computational resources and achieves the same accuracy level and number of extracted rules. </p...|$|R
5000|$|Be able {{to specify}} the <b>descriptors</b> <b>data</b> type, whether they are primary (integers, text, time, ...) or derived (enumerated, ...).|$|R
