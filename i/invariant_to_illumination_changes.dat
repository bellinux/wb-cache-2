20|10000|Public
50|$|Lowe's {{method for}} image feature {{generation}} transforms an image {{into a large}} collection of feature vectors, {{each of which is}} invariant to image translation, scaling, and rotation, partially <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes</b> and robust to local geometric distortion. These features share similar properties with neurons in primary Visual cortex that are encoding basic forms, color and movement for object detection in primate vision. Key locations are defined as maxima and minima of the result of difference of Gaussians function applied in scale space to a series of smoothed and resampled images. Low contrast candidate points and edge response points along an edge are discarded. Dominant orientations are assigned to localized keypoints. These steps ensure that the keypoints are more stable for matching and recognition. SIFT descriptors robust to local affine distortion are then obtained by considering pixels around a radius of the key location, blurring and resampling of local image orientation planes.|$|E
40|$|A novel {{approach}} to automatically detect vehicles in road tunnels {{is presented in}} this paper. Non-uniform and poor illumination conditions prevail in road tunnels making difficult to achieve robust vehicle detection. In order {{to cope with the}} illumination issues, we propose a local higher-order statistic filter to make the vehicle detection <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes,</b> whereas a morphological-based background subtraction is used to generate a convex hull segmentation of the vehicles. An evaluation test comparing our approach with a benchmark object detector shows that our approach outperforms in terms of false detection rate and overlap area detection...|$|E
40|$|In this paper, {{an object}} {{recognition}} system [7] {{has been developed}} that uses SURF(Speeded-Up Robust Features) and RANSAC(Random Sample Consensus) algorithms to identify a series of real-life objects in a given scene using their 2 -D images. SURF algorithm {{has been used for}} feature detection, extraction and matching. The features are invariant to image scaling, translation, and rotation, and partially <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes</b> and affine or 3 D projection[2]. RANSAC algorithm has been used to filter out the results obtained by the SURF algorithm and remove the outliers. Ten different objects have been successfully recognized using this system...|$|E
30|$|We firstly extract dense SIFT {{features}} {{as the local}} feature. SIFT features are <b>invariant</b> <b>to</b> scaling and rotation and partially <b>invariant</b> <b>to</b> <b>illumination</b> <b>change,</b> viewpoint change, and noise. These properties are advantageous in HEp- 2 cells classification as cell images are unaligned and have high within-class variabilities. In our experiments, SIFT features are extracted at single scale from densely located patches of gray-level images. The patches are centered at every 6 pixels and with a fixed size of 18 × 18 pixels.|$|R
40|$|Abstract. This paper {{presents}} {{a novel approach}} towards detecting intrinsically two-dimensional (i 2 D) image structures using local phase information. The local phase of the i 2 D structure {{can be derived from}} a curvature tensor and its conjugate part in a rotation-invariant manner. By employing damped 2 D spherical harmonics as basis functions, the local phase is unified with a scale concept. The i 2 D structures can be detected as points of stationary phases in this scale-space by means of the so call phase congruency. As a dimensionless quantity, phase congruency has the advantage of being <b>invariant</b> <b>to</b> <b>illumination</b> <b>change.</b> Experiments demonstrate that our approach outperforms Harris and Susan detectors under the <b>illumination</b> <b>change</b> and noise contamination. ...|$|R
40|$|A {{new method}} of {{translational}} image registration is presented: 'orientation correlation'. The method is fast, exhaustive, statistically robust, and illumination invariant. No existing method {{has all of}} these properties. A modification that is particularly well suited to matching images of differing modalities, 'squared orientation correlation', is also given. Orientation correlation works by correlating 'orientation images'. Each pixel in a orientation image is a complex number that represents the orientation of intensity gradient. This representation is <b>invariant</b> <b>to</b> <b>illumination</b> <b>change.</b> Angles of gradient orientation are matched. Andrews robust kernel function is applied to angle differences. Through the use of correlation the method is exhaustive. The method is fast as the correlation can be computed using Fast Fourier Transforms...|$|R
40|$|The {{use of a}} robust, {{low-level}} motion estimator {{based on}} a Robust Hough Transform (RHT) {{in a range of}} tasks, such as optical flow estimation, and motion estimation for video coding and retrieval from video sequences was discussed. RHT derived not only pixels displacements, but also provided direct motion segmentation and other motion-related clues. The RHT algorithm employed an affine region-to-region transformation model and was <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes,</b> in addition to being statistically robust. It was found that RHT did not base the correspondence analyses on any specific type of feature, but used textured regions in the image as non-localized features...|$|E
40|$|Abstract — In this paper, we {{used the}} Scale Invariant Feature Transform (SIFT) feature for image retrieval. SIFT {{descriptors}} are invariant to image scaling, transformation, rotation and partially <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes</b> and affine, gives the local features of an image. Therefore, feature from the images can be extracted more accurately by using SIFT than color, texture, shape and spatial relations. SIFT descriptor vectors for each image is indexed by making the use of vocabulary tree. Further, relevance feedback technique is used {{to bridge the gap}} between low level features and high level concepts. The proposed method is tested on mixed database of Corel and Caltech 3000 images which shows a significant improvement in precision and average recall rate...|$|E
40|$|Abstract. The {{success of}} any object {{recognition}} system, whether biological or artificial, lies in using appropriate representation schemes. The schemes should efficiently encode object concepts while being tolerant to appearance variations induced {{by changes in}} viewing geometry and illumination. Here, we present a biologically plausible representation scheme wherein objects are encoded as sets of qualitative image measurements. Our emphasis {{on the use of}} qualitative measurements renders the representations stable in the presence of sensor noise and significant changes in object appearance. We develop our ideas {{in the context of the}} task of face-detection under varying illumination. Our approach uses qualitative photometric measurements to construct a face signature (‘ratio-template’) that is largely <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes.</b> ...|$|E
40|$|A new, simple, {{fast and}} {{effective}} method for moving object detection in outdoor environments, <b>invariant</b> <b>to</b> extreme <b>illumination</b> <b>changes</b> {{is presented as}} an improvement to the shading model method described in [8]. It {{is based on an}} analytical parameter introduced in the shading model, background updating technique and window processing...|$|R
40|$|Most {{lighting}} can be accurately modeled using {{a simplified}} Planckian function. If we form logarithms of color ratios of camera sensor values, {{then in a}} Lambertian plus specular two-lobe model of reflection the temperature-dependent term is separate and {{is seen as a}} straight line: i. e., changing lighting amounts to changing each pixel value in a straight line, for a given camera. Here we use a 4 -sensor camera. In this case, forming color ratios reduces the dimensionality to 3. Applying logarithms and projecting onto the plane in the 3 D color space orthogonal to the light-change direction results in an image representation that is <b>invariant</b> <b>to</b> <b>illumination</b> <b>change.</b> For a given camera, the position of the specular point in the 2 D plane is always the same, independent of the lighting. Thus a camera calibration produces illumination invariance at a single pixel. In the plane, matte surfaces reduce to points and specularities are almost straight lines. Extending each pixel value back to the matte position, postulated to be the maximum radius from the fixed specular point, at any angle in the 2 D plane, removes specularity. Thus images are independent of shading (by forming ratios), independent of shadows (by making them independent of illumination temperature) and independent of specularities. The method is examined by forming 4 D images from hyperspectral images, using real camera sensors, with encouraging result...|$|R
40|$|Abstract. In this paper, {{we propose}} a novel method for {{recognizing}} objects in images {{in a way}} that is <b>invariant</b> <b>to</b> blur and affine trans-formation of the images. The method is based on a set of rotated and log-log sampled phase-only bispectrum slices of the images and their phase correlation. The method is <b>invariant</b> <b>to</b> centrally symmetric blur, such as linear motion or out of focus blur. Because of the normalization of the amplitude information, the method is also <b>invariant</b> <b>to</b> uniform <b>illumination</b> <b>changes.</b> The only known method having similar invariance properties is based on image moments. According to the experiments con-ducted, the proposed method outperforms the moment based method in the presence of various degradations. ...|$|R
40|$|This paper {{describes}} a spectral-spatial model (for colour object recognition) which exploits the shape, colour and position of regions {{on the surface}} of a rigid object in describing it. Given a model and test image (with colour constancy pre-processing) suitably segmented into colour regions, model and test regions with similar shape and colour are identified. If at least three model regions have matching test regions then the consistency of these matches are verified using distance/area affine invariant ratios. Subsequently, model regions are affine transformed into image space for matching, from which a match probability is determined. Experimental results demonstrate that this model is significantly <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes,</b> affine deformity and partial occlusion. ...|$|E
40|$|ABSTRACT: We {{present an}} object {{recognition}} approach using higher-order color invariant features with an entropy-based similarity measure. Entropic graphs offer an unparameterized alternative to common entropy estimation techniques, {{such as a}} histogram or assuming a probability distribution. An entropic graph estimates entropy from a spanning graph structure of sample data. We extract color invariant features from object images <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes</b> in intensity, viewpoint, and shading. The Henze–Penrose similarity measure is used to estimate the similarity of two images. Our method is evaluated on the ALOI collection, a large collection of object images. This object image collection consists of 1000 objects recorded under various imaging circumstances. The proposed method is shown to be effective under {{a wide variety of}} imaging conditions...|$|E
40|$|Abstract — In Video Retrieval system, each {{video that}} {{is stored in}} the {{database}} has its features extracted and compared to {{the features of the}} query image. The local invariant features are obtained for all frames in a sequence and tracked throughout the shot to extract stable features. Proposed work is to retrieve video from the database by giving query as an object. Video is firstly converted into frames, these frames are then segmented and an object is separated from the image. Then features are extracted from object image by using SIFT features. Features of the video database obtained by the segmentation and feature extraction using SIFT feature are matched by Nearest Neighbor Search (NNS). In this paper we have evaluated the proposed video retrieval system. The proposed method is better than previous video retrieval methods because it is <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes...</b>|$|E
40|$|International audienceImage {{retrieval}} from image databases {{is usually}} performed by using global image characteristics. However {{the use of}} local image information is highly desirable when {{only part of the}} image is of interest. An original solution was introduced in [9] using invariant local signal characteristics. This paper extends this contribution by extending the set of <b>invariants</b> considered <b>to</b> allow <b>illumination</b> <b>change.</b> Then it is shown that the invariant distribution is far from uniform and a probabilistic indexing scheme is proposed. Experimental results validate the approch and the different methods are discussed...|$|R
40|$|Abstract. In this paper, {{we propose}} a new {{descriptor}} for texture classification that is robust to image blurring. The descriptor utilizes phase information computed locally {{in a window}} for every image position. The phases of the four low-frequency coefficients are decorrelated and uniformly quantized in an eight-dimensional space. A histogram of the resulting code words is created and used as a feature in texture classification. Ideally, the low-frequency phase components are shown <b>to</b> be <b>invariant</b> <b>to</b> centrally symmetric blur. Although this ideal invariance is not completely achieved due to the finite window size, the method is still highly insensitive to blur. Because only phase information is used, the method is also <b>invariant</b> <b>to</b> uniform <b>illumination</b> <b>changes.</b> According <b>to</b> our experiments, the classification accuracy of blurred texture images is much higher with the new method than with the well-known LBP or Gabor filter bank methods. Interestingly, it is also slightly better for textures that are not blurred. ...|$|R
40|$|Abstract [...] -Traffic sign {{recognition}} usually {{consists of}} two parts: detection and classification. In this paper we describe the classification stage using ring partitioned method. In this method, first the RGB image is converted into gray scale image using color thresholding and histogram specification technique. This gray scale image, called as specified gray scale image is <b>invariant</b> <b>to</b> the <b>illumination</b> <b>changes.</b> Then the image is classified using ring partitioned method. The image is divided by several concentric areas like rings. In every ring the histogram is used as an image descriptor. The matching process is done by computing the histogram distances for all rings of the images by introducing the weights for every ring. The method doesn’t {{need a lot of}} samples of sign images for training process, alternatively only the standard sign images are used as the reference images. The experimental results show the effectiveness of the method in the matching of occluded, rotated, and illumination problems of traffic sign images. I...|$|R
40|$|Abstract. This {{paper is}} focused on camera calibration, image matching, {{both of which are}} the key issues in {{three-dimensional}} (3 D) reconstruction. In terms of camera calibration firstly, we adopt the method based on the method proposed by Zhengyou Zhang. In addition to this, it is selective for us to deal with tangential distortion. In respect of image matching, we use the SIFT algorithm, which is invariant to image translation, scaling, rotation, and partially <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes</b> and to affine or 3 D projections. It performs well in the follow-up matching the corresponding points. Lastly, we perform 3 D reconstruction of the surface of the target object. A Graphical User Interface is designed to help us to realize the key function of binocular stereo vision, with better visualization. Apparently, the entire GUI brings convenience to the follow-up work...|$|E
40|$|An object {{recognition}} {{system has been}} developed that uses {{a new class of}} local image features. The features are invariant to image scaling, translation, and rotation, and partially <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes</b> and affine or 3 D projection. These features share similar properties with neurons in inferior temporal cortex that are used for {{object recognition}} in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low-residual least-squares solution for the unknown model parameters. Experimental results show th [...] ...|$|E
40|$|In Video Retrieval system, each {{video that}} {{is stored in}} the {{database}} has its features extracted and compared to {{the features of the}} query image. The local invariant features are obtained for all frames in a sequence and tracked throughout the shot to extract stable features. Proposed work is to retrieve video from the database by giving query as an object. Video is firstly converted into frames, these frames are then segmented and an object is separated from the image. Then features are extracted from object image by using SIFT features. Features of the video database obtained by the segmentation and feature extraction using SIFT feature are matched by Nearest Neighbor Search (NNS). In this paper we have evaluated the proposed video retrieval system. The proposed method is better than previous video retrieval methods because it is <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes...</b>|$|E
40|$|Abstract—We {{present an}} {{efficient}} and noise robust template matching method based on asymmetric correlation (ASC). The ASC similarity function is <b>invariant</b> <b>to</b> affine <b>illumination</b> <b>changes</b> and robust <b>to</b> extreme noise. It correlates the given nonnormalized template with a normalized version of each image {{window in the}} frequency domain. We show that this asymmetric normalization is more robust to noise than other cross correlation variants such as the correlation coefficient. Direct computation of ASC is very slow, as a DFT needs to be calculated for each image window independently. To make the template matching efficient, we developed a much faster algorithm which carries out a prediction step in linear time and then computes DFTs {{for only a few}} promising candidate windows. We extend the proposed template matching scheme to deal with partial occlusion and spatially varying light change. Experimental results demonstrate the robustness of the proposed ASC similarity measure compared to state of the art template matching methods. I...|$|R
40|$|Gesture {{recognition}} {{plays an}} important role in human–computer interaction. However, most existing methods are complex and time-consuming, which limit the use of gesture recognition in real-time environments. In this paper, we propose a static gesture recognition system that combines depth information and skeleton data to classify gestures. Through feature fusion, hand digit gestures of 0 – 9 can be recognized accurately and efficiently. According to the experimental results, the proposed gesture recognition system is effective and robust, which is <b>invariant</b> <b>to</b> complex background, <b>illumination</b> <b>changes,</b> reversal, structural distortion, rotation, etc. We have tested the system both online and offline which proved that our system is satisfactory to real-time requirements, and therefore it can be applied to gesture recognition in real-world human–computer interaction systems...|$|R
40|$|Image {{retrieval}} from image databases {{is usually}} performed by using global image {{characteristics such as}} texture or colour. The use of local image information is highly desirable when {{only part of the}} image is of interest, but global approaches are not well suited to this. An original solution was introduced in [11] using invariant local signal characteristics. This paper extends this contribution by extending the set of <b>invariants</b> considered <b>to</b> allow <b>illumination</b> <b>change.</b> Then it is shown that the invariant distribution is far from uniform and a probabilistic indexing scheme is proposed. Experimental results validate the approch and the different method are discussed. The main result is that it is much more valuable to increase the discrimant power of the vector used to perform the indexing process; The Bayesian decision improves the standard method, but this improvement is much more limited than expected...|$|R
40|$|Lighting {{variation}} {{is a major}} challenge for an automatic face recognition system. In order to overcome this problem, many methods have been proposed. Most of them try to extract features <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes</b> or to reduce illumination changes in a pre-processing step and to extract features for recognition. In this paper, we present a procedure similar to the latter where the two steps are complementary. In the pre-processing step {{we deal with the}} illumination changes and in the features extraction step we use the BSIF (Binarized Statistical Image Features), a recently proposed textural algorithm. In our opinion, a method capable of reducing the lighting variations is ideal for an algorithm like the BSIF. The performance of our system has been tested on the FRGC dataset and the presented results show the validity of our approach...|$|E
40|$|We {{present a}} new {{approach}} to iteratively estimate both high-quality depth map and alpha matte from a single image or a video sequence. Scene depth, which is <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes,</b> color similarity and motion ambiguity, provides a natural and robust cue for foreground/background segmentation – a prerequisite for matting. The image mattes, on the other hand, encode rich information near boundaries where either passive or active sensing method performs poorly. We develop a method to combine the complementary nature of scene depth and alpha matte to mutually enhance their qualities. We formulate depth inference as a global optimization problem where information from passive stereo, active range sensor and matte is merged. The depth map is used in turn to enhance the matting. In addition, we extend this approach to video matting by incorporating temporal coherence, which reduces flickering in the composite video. We show that these techniques lead to improved accuracy and robustness for both static and dynamic scenes. 1...|$|E
40|$|This paper proposes an {{efficient}} computer-aided Plant Image Retrieval method based on plant leaf images using Shape, Color and Texture features {{intended mainly for}} medical industry, botanical gardening and cosmetic industry. Here, we use HSV color space to extract the various features of leaves. Log-Gabor wavelet {{is applied to the}} input image for texture feature extraction. The Scale Invariant Feature Transform (SIFT) is incorporated to extract the feature points of the leaf image. Scale Invariant Feature Transform transforms an image into a large collection of feature vectors, each of which is invariant to image translation, scaling, and rotation, partially <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes</b> and robust to local geometric distortion. SIFT has four modules namely detection of scale space extrema, local extrema detection, orientation assignment and key point descriptor. Results on a database of 500 plant images belonging to 45 different types of plants with different orientations scales, and translations show that proposed method outperforms the recently developed methods by giving 97. 9 % of retrieval efficiency for 20, 50, 80 and 100 retrievals...|$|E
40|$|International audienceIn this paper, {{we propose}} a micro-macro feature {{combination}} approach for texture classification. The two disparate yet complementary categories of features are combined. By this way, Local Binary Pattern (LBP) {{plays the role}} of micro-structure feature extractor while the scattering transform captures macro-structure information. In fact, for extracting the macro-type features, coefficients are aggregated from three different layers of the scattering network. It is a handcrafted convolution network which is implemented by computing consecutively wavelet transforms and modulus non-linear operators. By contrast, in order to extract micro-structure features which are rotation-invariant, relatively robust <b>to</b> noise and <b>illumination</b> <b>change,</b> the completed LBP is utilized alongside the biologically-inspired filtering (BF) preprocessing technique. Overall, since the proposed framework can exploit the advantages of both feature types, its texture representation is not only <b>invariant</b> <b>to</b> rotation, scaling, <b>illumination</b> <b>change</b> but also highly discriminative. Intensive experiments conducted on many texture benchmarks such as CUReT, UIUC, KTH-TIPS- 2 b, and OUTEX show that our framework has a competitive classification accuracy...|$|R
40|$|Abstract. With {{the rapid}} {{development}} of 3 D imaging technology, face recognition using 3 D range data has become another alternative {{in the field}} of biometrics. Unlike face recognition using 2 D intensity images, which has been studied intensively by many researchers since the 1960 ’s, 3 D range data records the exact geometry of a person and it is <b>invariant</b> with respect <b>to</b> <b>illumination</b> <b>changes</b> of the environment and orientation changes of the person. This paper proposes a new algorithm to register and identify 3 D range faces. Profiles and contours are extracted for the matching of a probe face with available gallery faces. Different combinations of profiles are tried for the purpose of face recognition using a set of 27 subjects. Our results show that the central vertical profile {{is one of the most}} powerful profiles to characterize individual faces and that the contour is also a potentially useful feature for face recognition...|$|R
40|$|International audienceThis paper {{presents}} {{a novel approach}} for detecting affine invariant interest points. Our method can deal with significant affine transformations including large scale changes. Such transformations introduce {{significant changes in the}} point location {{as well as in the}} scale and the shape of the neighbourhood of an interest point. Our approach allows to solve for these problems simultaneously. It is based on three key ideas: 1) The second moment matrix computed in a point can be used to normalize a region in an affine invariant way (skew and stretch). 2) The scale of the local structure is indicated by local extrema of normalized derivatives over scale. 3) An affine-adapted Harris detector determines the location of interest points. A multi-scale version of this detector is used for initialization. An iterative algorithm then modifies location, scale and neighbourhood of each point and converges <b>to</b> affine <b>invariant</b> points. For matching and recognition, the image is characterized by a set of affine invariant points; the affine transformation associated with each point allows the computation of an affine invariant descriptor which is also <b>invariant</b> <b>to</b> affine <b>illumination</b> <b>changes.</b> A quantitative comparison of our detector with existing ones shows a significant improvement in the presence of large affine deformations. Experimental results for wide baseline matching show an excellent performance in the presence of large perspective transformations including significant scale changes. Results for recognition are very good for a database with more than 5000 images...|$|R
40|$|A light {{distributed}} {{visual odometry}} method adapted to embedded hardware platform is proposed. The {{aim is to}} guide underwater surveys in real time. We rely on image stream captured using portable stereo rig attached to the embedded system. Taken images are analyzed on the fly to assess image quality in terms of sharpness and lightness, so that immediate actions can be taken accordingly. Images are then transferred over the network to another processing unit to compute the odometry. Relying on a standard ego-motion estimation approach, we speed up points matching between image quadruplets using a low level points matching scheme relying on fast Harris operator and template matching that is <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes.</b> We benefit from having the light source attached to the hardware platform to estimate a priori rough depth belief following light divergence over distance low. The rough depth is used to limit points correspondence search zone as it linearly depends on disparity. A stochastic relative bundle adjustment is applied to minimize re-projection errors. The evaluation of the proposed method demonstrates the gain in terms of computation time w. r. t. other approaches that use more sophisticated feature descriptors. The built system opens promising areas for further development and integration of embedded computer vision techniques...|$|E
40|$|Abstract. The Scale Invariant Feature Transform (SIFT) is an {{algorithm}} used {{to detect}} and describe scale-, translation- and rotation-invariant local features in images. The original SIFT algorithm has been successfully applied in general object detection and recognition tasks, panorama stitching and others. One of its more recent uses also includes face recognition, where it was shown to deliver encouraging results. SIFTbased face recognition techniques {{found in the literature}} rely heavily on the so-called keypoint detector, which locates interest points in the given image that are ultimately used to compute the SIFT descriptors. While these descriptors are known to be among others (partially) <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes,</b> the keypoint detector is not. Since varying illumination {{is one of the main}} issues affecting the performance of face recognition systems, the keypoint detector represents the main source of errors in face recognition systems relying on SIFT features. To overcome the presented shortcoming of SIFT-based methods, we present in this paper a novel face recognition technique that computes the SIFT descriptors at predefined (fixed) locations learned during the training stage. By doing so, it eliminates the need for keypoint detection on the test images and renders our approach more robust to illumination changes than related approaches from the literature. Experiments, performed on the Extended Yale B face database, show that the proposed technique compares favorably with several popular techniques from the literature in terms of performance...|$|E
40|$|Recently, {{remotely}} sensed {{multispectral data}} have been {{proved to be}} very useful for many applications {{in the field of}} Earth surveys. For certain applications, however, limits in the spatial resolution of satellite sensors and variation in ground surface restrict the usefulness of the available data, since the observed spectral signature of the pixels {{is the result of a}} number of surface materials found in the area of the pixel. Two mixed pixel classification techniques which have shown high correlation with vegetation coverage of single pixels are described in this thesis: the vegetation indices and the linear mixing model. The two approaches are adjusted in order to deal with sets of pixels and not individual pixels. The sets of pixels are treated as statistical distributions and moments can be estimated. The vegetation indices and the linear mixing model can then be expressed in terms of these statistics. The illumination direction is an important factor that should be taken into account in mixed pixel classification, since it modifies the statistics of the distributions of pixels, and has received no attention until now. The effect of illumination on the relation between the vegetation indices and the proportion of sets of mixed pixels is examined. It is demonstrated that some vegetation indices, which are defined from the ratio of statistics in two spectral bands, can be considered relatively <b>invariant</b> <b>to</b> <b>illumination</b> <b>changes.</b> Finally, a new illumination invariant mixing model is proposed which is expressed in terms of some photometric invariant statistics. It is shown to perform very well and it can be used to un-mix accurately sets of pixels under many illumination angles. The newly introduced mixing model can be considered a suitable choice in the mixed pixel classification field. Key words: Mixed pixels, sets of pixels, vegetation index, illumination invariants...|$|E
40|$|This paper {{presents}} a real-time keypoint matching algorithm using a local descriptor derived by Zernike moments. From an input image, {{we find a}} set of keypoints by using an existing corner detection algorithm. At each keypoint we extract a fixed size image patch and compute a local descriptor derived by Zernike moments. The proposed local descriptor is <b>invariant</b> <b>to</b> rotation and <b>illumination</b> <b>changes.</b> In order <b>to</b> speed up the computation of Zernike moments, we compute the Zernike basis functions in advance and store them {{in a set of}} lookup tables. The matching is performed with an Approximate Nearest Neighbor (ANN) method and refined by a RANSAC algorithm. In the experiments we confirmed that videos of frame size 320 × 240 with the scale, rotation, illumination and even 3 D viewpoint changes are processed at 25 ~ 30 Hz using the proposed method. Unlike existing keypoint matching algorithms, our approach also works in realtime for registering a reference image...|$|R
40|$|Traditional {{residential}} area detection methods are mainly based on image features, such as texture, spectrum, shape and etc. However, these features are not <b>invariant</b> <b>to</b> scale and <b>illumination</b> <b>changes,</b> which consequently reduce the robust {{of the existing}} algorithms. To solve this problem, the proposed method uses local feature for {{residential area}} detection from high-resolution remote-sensing imagery, which consists of three steps. Firstly, a large set of local feature points are extracted by Harris corner detector. In order to achieve a reliable extraction of corners from residential areas, two criterions are further proposed to validate and filter them. Afterwards, the extracted corners are incorporated into a likelihood function, and are {{used to measure the}} possibility of each pixel belonging to the residential area. Finally, residential areas are extracted by an adaptive binary segmentation method. Experimental results show that the proposed approach outperforms the existing algorithms in terms of detection accuracy. Department of Land Surveying and Geo-Informatic...|$|R
40|$|In {{this paper}} we apply {{boosting}} to learn complex non-linear local visual feature representations, drawing inspiration from its successful application to visual object detection. The main goal of local feature descriptors is to distinctively represent a salient image region while remaining <b>invariant</b> <b>to</b> viewpoint and <b>illumination</b> <b>changes.</b> This representation {{can be improved}} using machine learning, however, past approaches have been mostly limited to learning linear feature mappings in either the original input or a kernelized input feature space. While kernelized methods have proven somewhat effective for learning non-linear local feature descriptors, they rely heavily on the choice of an appropriate kernel function whose selection is often difficult and non-intuitive. We propose to use the boosting-trick to obtain a non-linear mapping of the input to a high-dimensional feature space. The non-linear feature mapping obtained with the boosting-trick is highly intuitive. We employ gradient-based weak learners resulting in a learned descriptor that closely resembles the well-known SIFT. As demonstrated in our experiments, the resulting descriptor can be learned directly from intensity patches achieving state-of-the-art performance. ...|$|R
