152|15|Public
25|$|Concern about {{possible}} carcinogenic properties of aspartame was originally raised and popularized {{in the mainstream}} media by John Olney in the 1970s and again in 1996 by suggesting that aspartame may be related to brain tumors. Reviews have found that these concerns were flawed, due to reliance on the <b>ecological</b> <b>fallacy</b> and the purported mechanism of causing tumors being unlikely to actually cause cancer. Independent agencies such as the FDA and National Cancer Institute have reanalyzed multiple studies based on these worries and found no association between aspartame and brain cancer.|$|E
500|$|This {{study has}} been {{extensively}} discussed by later scholars and several major criticisms have emerged. First, Durkheim took most of his data from earlier researchers, notably Adolph Wagner and Henry Morselli, who were much more careful in generalizing from their own data. Second, later researchers found that the Protestant–Catholic differences in suicide seemed {{to be limited to}} German-speaking Europe and thus may have always been the spurious reflection of other factors. Durkheim's study of suicide has been criticized {{as an example of the}} logical error termed the <b>ecological</b> <b>fallacy.</b> However, diverging views have contested whether Durkheim's work really contained an <b>ecological</b> <b>fallacy.</b> More recent authors such as Berk (2006) have also questioned the micro–macro relations underlying Durkheim's work. Some, such as Inkeles (1959), Johnson (1965) and Gibbs (1968), have claimed that Durkheim's only intent was to explain suicide sociologically within a holistic perspective, emphasizing that [...] "he intended his theory to explain variation among social environments in the incidence of suicide, not the suicides of particular individuals".|$|E
5000|$|... 2006. [...] "Ethnography, the <b>Ecological</b> <b>Fallacy,</b> and the 1995 Chicago Heat Wave," [...] American Sociological Review 71.|$|E
50|$|Many {{examples}} of <b>ecological</b> <b>fallacies</b> {{can be found}} in studies of social networks, which often combine analysis and implications from different levels. This has been illustrated in an academic paper on networks of farmers in Sumatra.|$|R
40|$|This paper {{critically}} {{examines the}} maps being produced to represent {{and promote the}} so called network society. Drawing on the deconstructionists approach pioneered by Brian Harley, we attempt to read and expose the “second text ” of the geographic maps of the Internet, Cyberspace and the network society. We examine, in detail, maps that display, {{with varying degrees of}} subtlety, the ideological agendas of Cyberboosterism of their creators. These maps are important because they are widely reproduced and consumed without critical comment. Many contain serious problems of <b>ecological</b> <b>fallacies</b> and commonly use choropleth cartographic methods. 15 th March 199...|$|R
30|$|In {{order to}} test the derived hypotheses, {{multilevel}} models were specified (see, inter alia, de Leeuw and Meijer 2010; Hox 2010; Raudenbush and Bryk 2002; Snijders and Bosker 2012). This class of models takes into account a nested data structure and allows the appropriate inclusion of independent variables at each analytical level to avoid individualistic or <b>ecological</b> <b>fallacies</b> (Subramanian et al. 2009). This {{makes it possible to}} explore a broad range of relationships between individual- as well as context-level characteristics and the dependent variable of interest, generally located at the lowest or most disaggregated (in most cases individual) level. Here, an example is the relationship between the federal state characteristic provision of upper secondary education and the NEET status as individual-level outcome measure. Besides the fixed effects element, the multilevel model provides extensive options to account for statistical uncertainty via the specification of random effects. These are included in the model equations by error terms.|$|R
5000|$|The <b>ecological</b> <b>fallacy</b> {{describes}} errors due to performing analyses on {{aggregate data}} {{when trying to}} reach conclusions on the individual units. [...] Errors occur in part from spatial aggregation. For example, a pixel represents the average surface temperatures within an area. <b>Ecological</b> <b>fallacy</b> would be to assume that all points within the area have the same temperature. This topic {{is closely related to}} the modifiable areal unit problem.|$|E
50|$|An {{example of}} <b>ecological</b> <b>fallacy</b> is the {{assumption}} that a population average has a simple interpretation when considering likelihoods for an individual.|$|E
50|$|An early {{example of}} the <b>ecological</b> <b>fallacy</b> was Émile Durkheim's 1897 study of suicide in France {{although}} this has been debated by some.|$|E
40|$|Maps {{have long}} been {{recognized}} as important and powerful modes of visual communication. In this paper we examine critically maps which are being produced to represent and promote {{information and communication technologies}} and the use of cyberspace. Drawing on the approach of map deconstruction we attempt to read and expose the 'second text' of maps of the Net. As such, we examine in detail a number of maps that display, with varying degrees of subtlety, the ideological agendas of cyberboosterism and techno-utopianism of their creators. A critical reading of these maps is important because they are widely reproduced and consumed on the Internet, in business and governmental reports, and in the popular press, all too often without a detailed consideration of the deliberate and intended messages being communicated. As we illustrate, many of these maps not only promote certain ideological messages but are often also poor in terms of cartographic design, with many containing serious <b>ecological</b> <b>fallacies.</b> We restrict our analyses to maps at the global scale...|$|R
5000|$|Researchers {{who argue}} for causal effects {{argue that the}} {{discrepancy}} of violent acts seen on TV compared {{to that in the}} real world are huge. One study looked at the frequency of crimes occurring in the real world compared with the frequency of crimes occurring in the following reality-based TV programs: America's Most Wanted, Cops, Top Cops, FBI, The Untold Story and American Detective, (Oliver, 1994). The types of crimes were divided into two categories, violent crimes and non-violent crimes. 87% of crimes occurring in the real world are non-violent crimes, whereas only 13% of crimes occurring on TV are considered non-violent crimes. [...] However, this discrepancy between media and real-life crimes may arguably dispute rather than support media effects theories. Some previous research linked boxing matches to homicides although other researchers consider such linkages to be reminiscent of <b>ecological</b> <b>fallacies</b> (e.g. Freedman, 2002). Much more research is required to actually establish any causal effects.|$|R
50|$|The use of data {{in modern}} society brings about {{new ways of}} {{understanding}} and measuring the world, but also brings with it certain concerns or issues. Data scholars attempt to bring {{some of these issues}} to light in their quest to be critical of data. Rob Kitchin identifies both technical and organizational issues of data, as well as some normative and ethical questions. Technical and organization issues concerning data range from the scope of datasets, access to the data, the quality of the data, the integration of the data, the application of analytics and <b>ecological</b> <b>fallacies,</b> as well as the skills and organizational capabilities of the research team. Some of the normative and ethical concerns addressed by Kitchin include surveillance through one's data (dataveillance), the privacy of one's data, the ownership of one's data, the security of one's data, anticipatory or corporate governance, and finally profiling individuals by their data. All of these concerns {{must be taken into account}} by scholars of data in their objective to be critical.|$|R
50|$|An <b>{{ecological}}</b> <b>fallacy</b> (or ecological inference fallacy) is {{a logical}} fallacy {{in the interpretation of}} statistical data where inferences about the nature of individuals are deduced from inference for the group to which those individuals belong. <b>Ecological</b> <b>fallacy</b> sometimes refers to the fallacy of division, which is not a statistical issue. The four common statistical ecological fallacies are: confusion between ecological correlations and individual correlations, confusion between group average and total average, Simpsons paradox, and confusion between higher average and higher likelihood.|$|E
50|$|<b>Ecological</b> <b>fallacy</b> {{can refer}} to the {{following}} statistical fallacy: the correlation between individual variables is deduced from the correlation of the variables collected for the group to which those individuals belong.|$|E
50|$|The <b>ecological</b> <b>fallacy</b> {{may occur}} when {{conclusions}} about individuals {{are drawn from}} analyses conducted on grouped data. The nature {{of this type of}} analysis tends to overestimate the degree of association between variables.|$|E
40|$|Socio-economic {{inequality}} indices, {{like the}} Gini coefficient or the Theil index, offer us {{a viable alternative}} to central tendency statistics when being used to aggregate software metrics data. The specific value of these inequality indices lies in their ability to capture changes in the distribution of metrics data more effectively than, say, average or median. Knowing whether the distribution of one metrics is more unequal than that of another one or whether its distribution becomes more or less unequal over time is the crucial element here. There are, however, challenges in the application of these indices that can result in <b>ecological</b> <b>fallacies.</b> The first issue relates to occurrences of zeros in metrics data, and not all inequality indices cope well with this event. The second problem arises from applying a macro-level inference to a micro-level analysis of a changing population. The Gini coefficient works for the former, whereas the decomposable Theil index serves the latter. Nevertheless, when used with care, and usually in combination, both indices can provide us with a powerful tool not only to analyze software, but also to assess its organizational health and maintainability over time...|$|R
40|$|The econometric {{study of}} civil war is {{increasing}} recognized to suffer from problems of 'over-aggregation'. As such, {{there is a high}} risk of estimation biases, <b>ecological</b> <b>fallacies,</b> and endogeneity problems. In this paper, I seek to contribute to the disaggregation of the study {{of civil war}} by focusing on the socio-economic dynamics of secessionist conflict as an identifiably distinct subset of 'civil wars', and by using a new subnational dataset compiled for this purpose. I test a series of hypotheses relating to the socio-economic conditions that encourage secessionism and political institutions that might mediate it. In contrast to the mainstream literature on civil war, I find a very strong predictive role for a measure of ethnic diversity in accounting for the incidence of secession. I also find a relatively straightforward set of socio-economic relationships. The relationship between relative socio-economic performance and conflict incidence is non-linear: regions that suffer from high 'horizontal inequalities'-whether relatively poor or relatively rich-in relation {{to the rest of the}} country are more prone to secessionism. The presence of hydrocarbon deposits also dramatically increases the likelihood of secessionism. But the institutional story is more complex and contingent upon interaction effect with the degree of ethnic diversity and the level of horizontal inequality...|$|R
40|$|This paper {{investigates the}} {{association}} between night-time lights and socio-economic metrics at the regional level. This regional level of understanding is critical as it underpins much economic monitoring and policy-making for sustainable development. Stable light data obtained from night time images of 2001, captured by Defense Meteorological Satellite Program - Operational Linescan System (DMSP-OLS) satellite, {{are used in the}} study. The data records artificial lights from human habitations from the earth surface and is a surrogate of the level of development of an area. Data on socio economic metrics at the sub-national level for the year 2001 for the state of Maharashtra in India have been sourced from Primary Census Abstract of India, 2001. However, most of the socio economic variables are not available at the village level. This paper describes the process of deriving maps of census metrics not collected by Indian census for small regions (such as villages) using DMSP-OLS images that are otherwise unavailable. Linear regression models with correlation coefficients ranging from 0. 75 to 0. 90 (p < 0. 05) at the district and the taluk level from the nighttime satellite images were used to predict these census metrics for villages. Maps are produced for villages. Errors associated with the Modifiable Areal Unit Problem (MAUP) and <b>ecological</b> <b>fallacies</b> are also discussed. The paper concludes with an overall assessment of the results at these various spatial scales...|$|R
50|$|Where {{real-world}} patterns may {{not conform}} to the regions discussed, {{issues such as the}} <b>ecological</b> <b>fallacy</b> and the modifiable areal unit problem (MAUP) can lead to major misinterpretations, and other techniques are preferable.|$|E
50|$|Because a {{correlation}} describes the measured {{strength of a}} relationship, correlations at the group level can be much higher than those at the individual level. Thinking both are equal {{is an example of}} <b>ecological</b> <b>fallacy.</b>|$|E
50|$|Durkheim stands {{accused of}} {{committing}} an <b>ecological</b> <b>fallacy.</b> Indeed, Durkheim's conclusions about individual behaviour (e.g. suicide) {{are based on}} aggregate statistics (the suicide rate among Protestants and Catholics). This type of inference, which explains micro events in terms of macro properties, is often misleading, as Simpson's paradox shows.|$|E
40|$|Human Factors and Ergonomics (HFE) {{researchers}} {{have a long}} tradition of focusing on the individual or micro-level. However, HFE {{researchers have}} started expanding their focus to include organizational or macro-level factors. That said, a gap still exists of theories or models that explain the link between micro and macro variables. Identifying these links and thus integrating macroergonomics and microergonomics is called mesoergonomics. Mesoergonomics considers the relationship between variables bounded across multiple levels in a work system. By bounding the system of interest across levels (or time, hierarchy, space, and process), the context surrounding the phenomenon of interest is identified. Understanding the context lessens the risk of missing contributing factors or explanations of the phenomenon, reducing the likelihood of contextual and <b>ecological</b> <b>fallacies.</b> This panel will discuss the challenges and benefits of conducting meso-or multi-level ergonomics research. Some of the panelists, specifically Carayon, will also discuss the difficulties of determining the proper system boundaries for researching particular phenomenon. SUMMARY The term “level ” refers to the unit of measurement, analysis, or inference, such as the individual, group or organizational level (Karsh, 2006). Human Factors and Ergonomics (HFE) researchers have {{a long tradition of}} focusing on the individual or micro- level. At the microergonomic level, the researcher is concerned with the human-machine, human-software interfaces, studying concepts such as perception, task switching, repetitive movements, and decision-making by conducting laboratory experiments. With the inception of th...|$|R
30|$|Transparency {{is crucial}} in how {{analytical}} spatial units are defined and how statistical results are derived from the collected data. This holds true for processes of data aggregation and standardization as well. Data has to be aggregated for privacy protection and easier data handling, but detail and precision are lost in this process (King 2001). Standardizing data is crucial for comparison. It allows the use of multivariate methods, but the variety within aggregations is no longer visible. As Klinenberg (2003) showed {{in his study of}} the 1995 Chicago heat wave, vulnerability and resilience are the result of an interplay of certain indicators: the most affected population {{in the case of the}} heat wave was old and poor and isolated and African American and living in areas with high violent crime rates. Aggregated and standardized data may not reveal this interaction of indicators; this can mislead decision-makers, but such data can also be misused by them for their own interests. Connected to the problems of data aggregation is the question of “ecological fallacy.” <b>Ecological</b> <b>fallacies</b> can occur when an inference is made about an individual from higher levels of aggregation (Meentemeyer 1989). This may lead to the implicit assumption that people in one region are “equally” vulnerable, as illustrated by the case of food insecurity in Ethiopia (Stephen 2004): early-warning decision-makers conceptualize the spatial dimension of food security as aggregated because this serves their own and international agendas. As a consequence, localized problems do not command the solutions or resources that they should.|$|R
40|$|This paper {{presents}} a methodology of mapping population-centric social, infrastructural, and environmental metrics at neighborhood scale. This methodology extends traditional survey analysis methods to create cartographic products useful in agent-based modeling and geographic information analysis. It utilizes and synthesizes survey microdata, sub-upazila attributes, land use information, and ground truth locations of attributes to create neighborhood scale multi-attribute maps. Monte Carlo methods are employed to combine {{any number of}} survey responses to stochastically weight survey cases and to simulate survey cases' locations in a study area. Through such Monte Carlo methods, known errors {{from each of the}} input sources can be retained. By keeping individual survey cases as the atomic unit of data representation, this methodology ensures that important covariates are retained and that <b>ecological</b> inference <b>fallacy</b> is eliminated. These techniques are demonstrated with a case study from the Chittagong Division in Bangladesh. The results provide a population-centric understanding of many social, infrastructural, and environmental metrics desired in humanitarian aid and disaster relief planning and operations wherever long term familiarity is lacking. Of critical importance is that the resulting products have easy to use explicit representation of the errors and uncertainties of each of the input sources via the automatically generated summary statistics created at the application's geographic scale...|$|R
50|$|At {{the close}} of the introduction, the authors warn the reader against {{committing}} the <b>ecological</b> <b>fallacy</b> of inferring things about individuals based on the aggregate data presented in the book. They also assert that intelligence {{is just one of many}} valuable human attributes and one whose importance among human virtues is overrated.|$|E
50|$|Examining {{trends in}} {{morbidity}} and mortality in the population is usually not difficult and may provide some indication {{of the effectiveness of}} injury prevention interventions. However, this approach suffers from the potential of <b>ecological</b> <b>fallacy,</b> where the data shows an association between an intervention and a change in the outcome, but there is actually no causal relationship.|$|E
5000|$|Statisticians have {{observed}} that the Court's approach is invalidated by the <b>ecological</b> <b>fallacy.</b> [...] Social scientists have found that federal judges vary widely when applying the Gingles preconditions. [...] Three judge courts made up of all Democrat appointees have {{ruled in favor of}} Section 2 liability in 41% of cases, contrasted with 11% under the all Republican appointed panels.|$|E
40|$|ResumenEl objetivo de este trabajo es contribuir al conocimiento del concepto de escala para la comprensión geográfica e {{integral}} de los problemas de socio-ambientales en México. Este objetivo se lleva a cabo mediante la comparación de las dimensiones escalares de las políticas territoriales de adaptación al cambio climático y del cambio de uso de suelo, dos procesos socio-ambientales que representan dos vertientes diferentes de análisis geográfico. El trabajo presenta los principales elementos que se han debatido en los últimos años en la literatura anglosajona sobre el concepto de escala, así como los diferentes elementos y dimensiones que lo componen: la extensión, la resolución, el nivel, la jerarquía, el problema de la unidad de área modificable y las falacias espaciales. Al aplicar dichos principios a la comparación entre dos problemas geográficos de naturaleza epistemológica diferente, se pone de manifiesto la importancia que tiene este concepto para el pensamiento geográfico y la necesidad de generar reflexiones sistemáticas {{en este sentido}} para la geografía que se produce en lengua española. Para la política de cambio climático, los resultados sugieren que la falta de integración conceptual y programática entre las políticas de los diferentes niveles, así como la relación concurrente entre ellas, genera un problema para producir resultados efectivos de adaptación. En relación con el cambio de uso del suelo, la visión escalar revela que las directas (próximas) e indirectas (subyacentes) operan en múltiples jerarquías; asimismo, sus consecuencias biofísicas, sociales y económicas se manifiestan en diferentes escalas de espacio y tiempo. AbstractThe aim of {{this paper}} is to contribute to the knowledge of the concept of scale for an integrated geographical understanding of socio-environmental problems in Mexico. This objective is accomplished by comparing the same scalar dimensions of two different problems: the first one is adaptation to climate change policies in Mexico, and the second is deforestation. This paper presents the main elements that have been discussed in recent years in the Anglo literature on the concept of scale, particularly the extent, resolution, level, hierarchy, the problem of modifiable area unit and spatial fallacies. The properties of geographical concepts emerge and can be observed according to the combination of scalar elements. The most fundamental scalar principles discussed in the field refer to identify the combination of elements in which it is possible to observe each geographical phenomena's variability, characteristics and properties. One of the most relevant problems of the scalar thinking refers to the Modifiable Areal Unit Problem (MAUP), which stems from the data aggregation; the relevant values represented for each spatial unit relate to each other in different ways according to how they organized within a hierarchical structure. Data aggregation affects how the variability and heterogeneity of a phenomenon can be observed, given that the change of scale may show or hide specific properties of the dataset in a change of resolution. This problem is relevant for the inferences that stem from the data, given that individualistic or <b>ecological</b> <b>fallacies</b> may emerge if the dataset characteristics are not correctly interpreted in terms of representation, similarity or heterogeneity. Applying these principles to the comparison between two different geographical problems of different epistemological nature, we show the importance of this concept for the geographical thought; the comparison highlights the need to generate systematic reflections in this regard for the Geography produced in Spanish language. Regarding the socio-environmental problems addressed, the climate change adaptation policies in Mexico show a lack of conceptual and programmatic integration in different levels; the lack of an adequate concurrent relationship between them creates a problem to generate effective results for adaptation. We identify three policy levels (global, national and local), in which we briefly examine the relevant policy instrument and actor(s) that negotiate, design and/ or implement it in each level. For the international level we briefly present the role of the Mexican government on the negotiation and adoption of the Kyoto Protocol principles and goals; for the national level, we examine the approach and jurisdiction of the Special Program of Climate Change and the related juridical field; for the local level, we discuss the Municipal Climate Action Programmes, their design and scope, as well as the lack density that has prevented these instrument to influence other policy levels. This section discusses the hierarchies between these levels, the extension (jurisdiction) under which each of them are relevant and the importance of each scalar level for visualizing the main characteristics of the different adaptation policies. Regarding deforestation and land use change, the scale analysis reveals that might be direct (proximate) and indirect (underlying) and operate on multiple levels; also, its consequences are manifested in different scales of space and time. The spatial heterogeneity of land use change reveals the combination of biophysical and social, economic and political conditions, so the deforestation rate and causes change with observation scale. The implication of scale is that important land use processes could remain undetected, thus not monitored by traditional tools and aggregated land use categories typically applied. On the other hand, the choice of the time scale could undetect economic or social processes that change from year to year, which obscure the underlying causes of deforestation. It is therefore necessary to see change in land use with a view hierarchies...|$|R
40|$| {{accounts}} {{even when}} case descriptions {{do not include}} clear links to food concerns. This categorization has been uncritically adopted by several researchers. The third issue highlighted in the paper consists of understudied and presupposed causal mechanisms. Much of the literature and commentaries on food riots argue that the link between food price rises and conflict runs through food insecurity, implying that the hungry poor are the ones participating in conflict. Such a claim requires more detailed, micro-level evidence, however, and cannot be verified by relating macro-level variables (e. g. food imports) to food riot occurrences. Furthermore, I argue that political and institutional factors play important roles as well in conflict processes, and cannot be replaced by purely economic accounts. Mobilization structures and political opportunities can be key in explaining protest mobilization. Chapter 3 ‘Staging a “Revolution”: The 2011 - 2012 Electoral Protests in Senegal’ engages further with the theoretical limitations of resource scarcity accounts of conflict. The case of Senegal is particularly interesting because the protests, directed at then-President Abdoulaye Wade’s attempts to change electoral rules in his favour, have been predominantly explained {{from the viewpoint of}} grievances at the population level, in public debate as well as the academic literature. Economic grievances, in particular caused by rising food prices and youth unemployment, were seen as major drivers of mobilization. Some scholars also emphasized political grievances, stemming from democratic aspirations. The case of the Senegalese electoral protests could be regarded as a ‘positive’ case of the scarcity-conflict thesis, in which economic downturn and inflation run together with mobilization. Nonetheless, in this paper I argue that a grievance perspective limits our understanding of the protests and their political implications. Theoretically, I use a resource mobilization perspective to trace how the protest movement organized, gained success, and eventually demised during the electoral process, which ended with the victory of Macky Sall in the second round. The approach reveals key insights on the role of opposition parties and political interests in the mobilization, and hence empirically substantiates the value of using different theoretical lenses to look at protest mobilization in. The study relies on qualitative interviews with key members of the main protest movement against President Wade, as well as internal movement documents. By tracing the evolution of the protest movement, the study reveals the importance of coalition-building between civil society organizations such as human rights and development NGOs, youth movements, and community-based organizations. These organizations also connected to political parties, which played major roles in the organization of the movement and mobilization processes. Opposition parties were major members of the protest movement, contributed the bulk of the resources (finances, vehicles etc.) for the organization of protests, and were often responsible for mobilizing individuals at the grassroots level. The analysis also reveals that protesters strategically made use of violence, in the form of vandalism or tyre-burning, as a resource to gain international media attention and interest for events ongoing in Senegal. This indeed led to support from major foreign powers for the movement. Whereas Chapters 2 and 3 focus more on the theoretical shortcomings in the literature on protest mobilization in Africa, Chapters 4, 5, and 6 focus predominantly on methodological shortcomings. Chapter 4 zooms in on the use of micro-level theory to underpin macro-level relations and makes use of available datasets on protest events and protest. Chapters 5 and 6 focus on the method of event analysis and the use of news reports to capture events. Both chapters focus {{on a wide range of}} events. Besides protests and riots, armed conflict events are also considered, for example. This allows for a discussion of the methodological implications of measuring different types of events occurring in Africa. Chapter 4 ‘Between fallacy and feasibility? Dealing with the risk of <b>ecological</b> <b>fallacies</b> in the quantitative study of protest mobilization and conflict’ reviews several recent quantitative scarcity-social disorder studies and demonstrates how these predominantly make use of micro-level grievance theory to substantiate empirical relations between macro-level variables, such as scarcity indicators and event counts. This form of empirical testing stems to an important extent from the econometric study of civil war onset for which a lack of reliable data on the individual level in violent settings led to the use of macro-level approaches. However, the reasons for individual mobilization in low-level conflicts, including protests and riots, can more easily be captured by means of opinion surveys. Hence it is possible to generate micro-level data, which could be used to investigate mobilization dynamics in social disorder events. Micro-data can also reveal possible <b>ecological</b> <b>fallacies</b> arising from macro-analyses. The paper shows empirically that macro-analyses risk leading to flawed inferences to the individual level. We do this by making use of macro data on protest and riot events (cfr. ACLED) and micro-data on protest participation (cfr. Afrobarometer surveys). We investigate two theoretical models. The first model states that poorer countries are likely to witness more protest events, because poorer, and hence more aggrieved, people are more likely to engage in protests. The second model holds that less democratic regimes are likely to experience more protests because people who rate their country as not being a democracy are more likely to engage in protests in order to voice their discontent about the lack of political participation, transparency and accountability. We test these relations both on the macro and on the micro-level. Results show how different relations can be found on both levels of analysis, and hence one has to be careful when inferring micro-level relations from macro-level data. The empirical approach in the resource scarcity literature can be compared to early motivational theories discussed in the social movement literature. These stated that structural strains or economic stressors engender certain psychological processes (i. e. frustration, grievance), which were argued to add up to a collective action event. The micro-level was empirically taken for granted, however. Further empirical engagement with the micro-level in the social movement literature engendered additional theoretical insights. The paper hence argues that a similar development could improve our understanding of mobilization in Africa and refers to several methodological approaches drawn from the social movement literature which can be used to this aim. Chapter 5 ‘How Events Enter (Or Not) Datasets: The Pitfalls of Using Newspapers in the Study of Conflict’ focuses on the errors that can arise from the use of news reports to study conflict events and introduces an analytical framework to understand and investigate these errors. The chapter discusses the increasing research interest in the establishment of conflict event datasets that focus on events in developing settings. Yet it is noted that much of the methodological debate with respect to the use of media reports to construct event data is to be found in Western-focused social movement research. While critical assessments of conflict event data have emerged in recent years, it is argued that this field of study stands to benefit from further engagement with the social movement, as well as communications literatures. We also devote attention to the implications of the focus on a wide range of events in the conflict literature, including protests and armed conflict, while the social movement literature is mainly focused on protests. In order to support cross-fertilization between both literatures with regard to the collection and use of event data, the chapter sets forth the Total Event Error framework. This framework draws on insights from the survey research literature and the Total Survey Error framework. It distinguishes between measurement errors and errors of representation and captures bias as well as unreliability. Several crucial errors arise from the media source. Selection bias refers to the systematic selection of some events into the news by media while neglecting others. Description bias refers to how media describe events and their systematic tendencies in doing so. These errors arise from the rationale of the media source, but errors also arise in the data collection process. We focus on coding rules, intercoder reliabilities, the accuracy of external sources such as police records and NGO reports, and data adjustments in analyses such as data weighting and missing data imputation. Some errors are addressed in the social movement and communication literatures, but are considered only to a lesser extent in conflict event studies. The chapter brings together insights from conflict studies, and the social movement and communication literatures. A such, it captures the current state of the art on the use of news reports to study conflict events. Drawing from these insights, we formulate guidelines for researchers developing and using event data. However, we also note that more methodological research is needed on events in developing settings, as well as on how errors can diverge for different types of violent and nonviolent events currently studied in the conflict literature. The Total Event Error Framework can be used to identify and situate errors and investigate them further. Chapter 6 ‘Fit For Purpose? Selection Bias in African-Focused Conflict Event Datasets’ is predominantly concerned with media selection effects and how datasets drawn from different news sources reveal differing patterns in the absolute and relative occurrence of conflict events, as well as their spread across Nigerian territory. The paper compares the coverage of conflict events of two existing cross-national datasets, SCAD and ACLED, with a self-composed dataset for the case of Nigeria. Whereas SCAD is based on Associated Press and Agence France Press news reports extracted from Lexis-Nexis, ACLED is based on online international and national news sources as well as NGO reports. The Nigerian dataset (for which the acronym GDN is used from the newspapers on which the dataset is based) draws from the hard copy versions of The Guardian, This Day, and The Nation, three Nigerian newspapers with national coverage. The period covered by the dataset is April 2014 to March 2015, corresponding to a full year before the presidential elections. Archival research for the project was carried out from July to September 2015 in Lagos, Nigeria. The case of Nigeria was chosen because the country witnesses a large range of conflict events: Boko Haram violence and terror attacks, pastoralist-herder conflicts, electoral violence, riots, as well as protests. The dataset covered all these types of events, which also allows us to compare news errors for different forms of conflicts. All datasets cover the same types of events, ranging from protests and riots, to armed conflict events and terrorist attacks. Firstly, the absolute counts of events indicate that SCAD, which is solely based on international sources, covers the least events, while GDN covers most events. The latter is interesting as ACLED draws from various Nigerian newspapers, but only online versions which could be the source for the underreporting. The findings hold for all conflict types. Secondly, the relative occurrence of events is different with more ar...|$|R
40|$|This {{document}} {{consists of}} three Parts, of which Part I provides a general introduction to the institutional and empirical framework around cantonal variations in costs and utilization of healthcare services in Switzerland (CH). Therefore, a more theoretical introduction is offered by presenting {{important aspects of the}} meaning of health care in the framework of health economic theory and a short overview about the research history of the explanation of different levels of healthcare costs and utilization and their temporal and spatial trends. This introduction is followed by a presentation of the main components comprising the construction, functioning and funding of the Swiss healthcare system. A short comparison of levels of and trends in healthcare costs to those of other Western countries reveals that the situation for Switzerland is not especially unique. More unique to Switzerland are the strong variations of regional healthcare costs per person observed within the country, as they can widely vary across cantons—even by factors of 2 or 3. Part I of the document continues with an overview on Switzerland’s health data situation and reveals that its most critical weaknesses exist in the areas of outpatient healthcare provision and of epidemiology. Part I terminates with a presentation of the literature overviewing international and national differences in regional healthcare costs. The review concludes that it is challenging for health economics to provide consistent answers to many of the important research questions pertaining to the field of regional healthcare cost and utilization differences. The most frequently cited causes of this difficulty are the complexity of the healthcare systems and the crucial dearth of broadly recognized theoretical models and available data. A recently presented model (Chandra and Skinner 2012; Skinner 2012) was considered to be a good starting point for a more systematic analysis of geographic variations in healthcare costs and utilization. One agreement in the literature about the methodological findings is obvious: when explaining regional health differences, it is advantageous to account for levels of and trends in healthcare costs (or utilization) simultaneously. However, one has to be aware that different sets of variables can influence each dimension. Thus, by being able to combine cross-section and time-series analyses, a panel econometric approach seems to be the most promising statistical-technical instrument for tackling these types of research questions. Moreover, because the prices paid in the health sector (e. g., the cantonal taxpoint values in Switzerland), the volumes of care (e. g., the number of per capita GP consultations), and the medical practices applied (e. g., the average number of taxpoints used per consultation) can—again—be influenced by different sets of factors, separate analyses of these three main components of healthcare costs is preferable. Moreover, the literature review identifies individual data (i. e., the individual patient, the individual insurance policyholder, or the individual healthcare provider) and geo-coded information 1 as the statistical and geographical level that offers the most possibilities for such research. Unfortunately, the Swiss health data normally do not allow in-depth analysis on such individual levels. In Part II of the document, three essays containing concrete analyses of regional differences in costs and in actual and future utilization of healthcare services in Switzerland are presented; none of these empirical investigations goes beyond the cantonal level. The first essay investigates the factors associated with cantonal differences in the utilization of mandatory health insurance (MHI) services between 2000 and 2007 by applying panel econometric (fixed effects) models. For variations in utilization for each of the six largest MHI service provider domains—viz., general practitioners, medical specialists, hospital inpatient care, hospital outpatient care, medication and nursing homes—significant factors that are correlated with utilization frequency over time and across cantons can be identified. In particular, a greater density of service providers tends to be significantly associated with the per-capita consumption of healthcare services. On the demand side, older, more urban and wealthier populations with more deprivation problems summarize the principal positively correlated factors. Financing characteristics in the form of high deductibles or managed care instruments can also {{play a role in the}} utilization level of healthcare services, although some large difficulties 2 were faced in confirming their effects empirically. Finally, the general time trends describing the accelerating utilization of outpatient drugs, nursing homes and outpatient hospitals are presented in contrast to the declining trends observed for inpatient hospitals, GPs, and specialist services in private practices. The main contribution of the first essay of the thesis is its being the first such work to analyze spatial and temporal differences in quantities instead of costs of healthcare service domains in Switzerland. Moreover, the testing of a constant set of 12 explanatory variables across the six healthcare service domains allows a bi-directional interpretation of the results. In addition to understanding how each of the six target variables is interrelated with the whole set of regressors, one can learn more about how each purportedly influential factor is individually associated with all six healthcare service domains. The second essay in Part II begins by presenting the large differences in average annual per-bed costs between individual nursing homes and between nursing homes grouped by cantons. The paper tries to identify empirically some explanations for these sizable per-bed cost variations. At the same time, the assumed existence of two-levels of explanatory factors (viz., individual and cantonal levels) is taken into consideration by modeling them with regression estimations in multilevel form. Moreover, besides the variation of total costs per bed and per year, the variations of the annual per-bed costs of accommodation and assistance and the annual per-bed costs covered by the MHI are calculated separately. Because the data from 2006 alone were available for the research presented in the second essay at the time of its conception, it was decided to approach this study with only a single year being analyzed in a cross-level setting. This approach clearly has its limitations, but it did not preclude employing more sophisticated panel data approaches at a later date. Such a limited model explains variations in the annual per-bed costs between cantons fairly well, but quite a share of variation within cantons remains unexplained. Because no ideal indicator was available for the data on the hotel service standards of Swiss nursing homes, this result was not surprising—especially regarding annual per-bed costs of accommodation and assistance. However, the operationalized variables—such as the number of days invoiced per bed and year (i. e., occupancy rate), the intensity of nursing time spent per patient and day, the qualification level of the personnel, the relative number of non-medical employees, and the cantonal wage index—were significantly correlated with all three cost indicators. The essay admits to the difficulty faced in deriving recommendations for policy-making authorities from these results. Cantons should at least monitor their nursing home costs and financing continuously—in particular, their costs for assistance and accommodation. Should increasingly large numbers of individuals and their families out of the growing number of people with chronic illnesses be unable to pay these costs out of their own pockets in the coming years, the cantons might be forced to intervene. The third essay of Part II analyzes regional differences in the utilization of somatic acute care beds in Swiss hospitals. A description of cantonal population age structures and trends and hospital utilization patterns in 2010 is followed by calculations of ranges of cantonal acute care hospital volumes through 2030. Originally developed by researchers at the Swiss Health Observatory and the Statistical Office of Canton Vaud (VD) for the statistical support of hospital planning processes in individual cantons, a projection model is applied for the first time in this study for all 26 cantons simultaneously and allows a direct comparison of the results across cantons and with the country as a whole as well as calculations of national-level results for medical branches. The projection model realizes a systematic link between Swiss medical statistics for hospitals and official cantonal population scenarios. Various hypotheses on future length-of-stay (LOS) trends in Swiss acute care hospitals are simulated with the model. The most important results of the study are the following: the national number of hospital days required through 2030 should increase no more (or only slightly more) than the population increases. While an increase of hospital days between 5 percent and 13 percent is expected in the two “middle” scenarios of the model, the population will grow 11 percent between 2010 and 2030 in the official “average” demographic scenario. This rather positive outcome on the national level is the result of major differences between cantons. Some cantons will have to deal with increases of hospital days of approximately 30 percent, whereas in other cantons hospital days will rise less than 5 percent. Moreover, treatments typical for older patients, such as cardiology and vascular medicine treatments, will clearly be more necessary in 2030 than medical branches with very young patients, such as neonatology or obstetrics. Part III provides some specific conclusions for the Swiss healthcare system with its characteristics of regulated competition and strong federalist structure. As a strategy of analyzing and comparing healthcare cost components defined as being low aggregated (e. g., individual health service domains or providers and individual cost components, such as quantities and prices) is favored and targeted in the three empirical essays of Part II, it seems important that such detailed analyses should afterwards be complemented by an attempt to draw an overall picture of the results. Accordingly, an applied synthesis of the results for two exemplary cantons—one canton with low (Obwalden OW) and one canton with high per capita healthcare costs (Geneva GE) —is presented in the Excursus of Part III. Without yet having executed the necessary empirical work, proposals are made in the Excursus about how cantons might be distinguished by some of their characteristics on the demand and supply sides of the healthcare system. On the demand side, cantons may have a more “integrated” or a more “globalized” population. Concrete characteristics that assign a cantonal population to one or the other type could be derived from their different economic conditions (e. g., income, assets, and their distributions), the importance of social-economic exclusion (e. g., unemployment, receipt of benefits), the average physical and mental health status, and the actual and future age structure (including future requirements of health care). On the supply side, two different types of cantons are proposed as well. First, there might be the “peripheral-type scheme” of a cantonal health provision system. Such a health system is focused on primary care and nursing homes, and it is characterized by a modest health provision infrastructure with only a few active specialists, with many health services being purchased in other cantons. Second, the “center-type scheme” of a cantonal health provision system is proposed. Such a system is characterized by a large (university or principal) hospital that is surrounded by many independent specialists and pharmacies. This system, in contrast, attracts patients from other cantons. The document concludes by offering a few suggestions for future research. Rather concrete propositions were made in the three empirical articles of the thesis. First, they were mainly about more sophisticated methodological approaches : the use of instrumental variables and two-stages least squares estimations in the first article, the use of panel data models with additional regressors in the second article, and the integration of additional variables such as sex, morbidity trends, and future changes of regional patient flows in the third article. Second, extensions concerning the data were required: the necessity of always working with statistical data on an aggregated level might be the most significant problem this thesis faces throughout. Moreover, more detailed domains of health service providers could be analyzed in the first article, a search for better variables concerning hospitality standards and cantonal regulations in nursing homes is proposed in the second article, and a more precise analysis of hospital cases with very long LOSs would be useful in the third article. As a more general recommendation, more research about possible trends in the future health status of aging populations in Western societies was proposed. Special models to simulate such quantitative calculations exist and could be translated to the case presented by Switzerland.. 1 Of course, in reality, the exact research question actually determines whether individual-level data are needed; however, most of the time, data aggregation (if needed) is possible, but desaggregation is not. 2 The main methodological challenges were endogeneity problems (omitted variables and variable selection biases) and <b>ecological</b> <b>fallacies</b> (see Chapter 4) ...|$|R
50|$|The <b>ecological</b> <b>fallacy</b> was {{discussed}} {{in a court}} challenge to the Washington gubernatorial election, 2004 in which a number of illegal voters were identified, after the election; their votes were unknown, because the vote was by secret ballot. The challengers argued that illegal votes cast in the election would have followed the voting patterns of the precincts {{in which they had}} been cast, and thus adjustments should be made accordingly. An expert witness said this approach was like trying to figure out Ichiro Suzuki's batting average by looking at the batting average of the entire Seattle Mariners team, since the illegal votes were cast by an unrepresentative sample of each precinct's voters, and might be as different from the average voter in the precinct as Ichiro was from the rest of his team. The judge determined that the challengers' argument was an <b>ecological</b> <b>fallacy</b> and rejected it.|$|E
50|$|In {{statistics}} an <b>ecological</b> <b>fallacy</b> is {{a logical}} fallacy {{in the interpretation of}} statistical data where inferences about the nature of individuals are deduced from inference for the group to which those individuals belong. The four common statistical ecological fallacies are: confusion between ecological correlations and individual correlations, confusion between group average and total average, Simpson's paradox, and other statistical methods.|$|E
5000|$|A {{striking}} <b>ecological</b> <b>fallacy</b> is Simpson's paradox. Simpson's {{paradox is}} the fact that when comparing two populations divided into groups, the average of some variable in the first population can be higher in every group and yet lower in the total population. Formally, when each value of Z refers to a different group and X refers to some treatment, it can happen that ...|$|E
5000|$|Lastly, {{ecological}} {{studies may}} be used (where the global environment variables and their global effect on two different populations are compared). Such studies are [...] "cheap and dirty": they can be easily conducted on very large populations (the whole USA, in Dr Cohen's study), but are prone {{to the existence of}} confounding factors, and exposed to the <b>ecological</b> <b>fallacy</b> problem.|$|E
5000|$|This {{study has}} been {{extensively}} discussed by later scholars and several major criticisms have emerged. First, Durkheim took most of his data from earlier researchers, notably Adolph Wagner and Henry Morselli, who were much more careful in generalizing from their own data. Second, later researchers found that the Protestant-Catholic differences in suicide seemed {{to be limited to}} German-speaking Europe and thus may have always been the spurious reflection of other factors. Durkheim's study of suicide has been criticized {{as an example of the}} logical error termed the <b>ecological</b> <b>fallacy.</b> However, diverging views have contested whether Durkheim's work really contained an <b>ecological</b> <b>fallacy.</b> More recent authors such as Berk (2006) have also questioned the micro-macro relations underlying Durkheim's work. Some, such as Inkeles (1959), Johnson (1965) and Gibbs (1968), have claimed that Durkheim's only intent was to explain suicide sociologically within a holistic perspective, emphasizing that [...] "he intended his theory to explain variation among social environments in the incidence of suicide, not the suicides of particular individuals." ...|$|E
5000|$|Researchers {{are said}} to commit the <b>ecological</b> <b>fallacy</b> when they make {{untested}} inferences about individual-level relationships from aggregate data. It is called a fallacy because {{it is based on}} the problematic assumption that relationships at one level of aggregation also hold at another level of aggregation. To illustrate, consider the fact that George Wallace, a four-term governor of Alabama and well-known segregationist who ran as a third-party candidate well in the 1968 US Presidential election, received a higher share of votes in regions with higher percentages of blacks. [...] From this one might erroneously conclude that blacks were disproportionately inclined to vote for Wallace (post-election surveys showed that, while one in eight whites voted for Wallace, virtually no blacks did).Firebaugh has contributed to this literature by delineating theoretical conditions or rules under which it is possible to infer individual-level relationships from aggregate data. These conditions are important because researchers are subject to the <b>ecological</b> <b>fallacy</b> in virtually all the social and behavioral sciences - from history to political science to epidemiology - since individual-level data often are unavailable.|$|E
