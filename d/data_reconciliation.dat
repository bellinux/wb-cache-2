285|62|Public
5000|$|Given [...] {{measurements}} , <b>data</b> <b>reconciliation</b> can mathematically {{be expressed}} as an optimization {{problem of the}} following form: ...|$|E
5000|$|Process operations: {{scheduling}} process networks, multiperiod {{planning and}} optimization, <b>data</b> <b>reconciliation,</b> real-time optimization, flexibility measures, fault diagnosis ...|$|E
5000|$|Advanced data {{validation}} and reconciliation (DVR) is an integrated approach of combining <b>data</b> <b>reconciliation</b> and {{data validation}} techniques, which {{is characterized by}} ...|$|E
5000|$|... #Subtitle level 2: Advanced <b>data</b> {{validation}} and <b>reconciliation</b> ...|$|R
5000|$|... #Caption: The {{workflow}} of {{an advanced}} <b>data</b> validation and <b>reconciliation</b> process.|$|R
30|$|This paper {{presents}} a simple mathematical tool to threat {{and enhance the}} quality of measured <b>data.</b> This <b>reconciliation</b> method is described and a global methodology including validation of measurement, elimination of irrelevant points and validation of the reconciliation method is proposed.|$|R
50|$|When the {{deployment}} has finished {{the system will}} need proper maintenance to stay alive. This includes <b>data</b> <b>reconciliation,</b> execution and monitoring and performance tuning.|$|E
5000|$|Redundancy {{can be used}} as {{a source}} of {{information}} to cross-check and correct the measurements [...] and increase their accuracy and precision: on the one hand they reconciled Further, the <b>data</b> <b>reconciliation</b> problem presented above also includes unmeasured variables [...] Based on information redundancy, estimates for these unmeasured variables can be calculated along with their accuracies. In industrial processes these unmeasured variables that <b>data</b> <b>reconciliation</b> provides are referred to as soft sensors or virtual sensors, where hardware sensors are not installed.|$|E
50|$|The {{initiative}} {{is focused on}} driving the creation of digital methodologies for data collection, <b>data</b> <b>reconciliation,</b> and file formats. Operational, strategic, and technical guidance for OMI will be provided by design and consulting firm IDEO and Context Labs.|$|E
50|$|Industrial process <b>data</b> {{validation}} and <b>reconciliation,</b> or more briefly, <b>data</b> validation and <b>reconciliation</b> (DVR), is {{a technology}} that uses process information and mathematical methods in order to automatically correct measurements in industrial processes. The use of DVR allows for extracting accurate and reliable information {{about the state of}} industry processes from raw measurement data and produces a single consistent set of data representing the most likely process operation.|$|R
2500|$|... – LSE’s {{business}} {{solution for}} Post-Trade Services, <b>Data</b> Solutions and <b>Reconciliations.</b> It offers customers a global hosted platform for integrating matching, validation and reconciliations.|$|R
40|$|The {{development}} of chemical processes {{is based on}} both experiments and process simulations. <b>Data</b> evaluation and <b>reconciliation,</b> model development and validation, and design of experiments are essential steps in this procedure. The different tools and approaches available are usually not supported in an integrated way in the process developer's workflow. Therefore, a framework for process design was created and integrated in a tool box which supports process design comprehensively. It contains methods for <b>data</b> selection and <b>reconciliation,</b> sensitivity analysis, model validation and model adjustment...|$|R
50|$|This use {{of average}} values, like a moving average, {{acts as a}} {{low-pass}} filter, so high frequency noise is mostly eliminated. The result is that, in practice, <b>data</b> <b>reconciliation</b> is mainly making adjustments to correct systematic errors like biases.|$|E
50|$|<b>Data</b> <b>reconciliation</b> is a {{technique}} that targets at correcting measurement errors that are due to measurement noise, i.e. random errors. From a statistical point of view the main assumption is that no systematic errors exist in the set of measurements, since they may bias the reconciliation results and reduce the robustness of the reconciliation.|$|E
50|$|It is {{important}} to note that the remediation of gross errors reduces the quality of the reconciliation, either the redundancy decreases (elimination) or the uncertainty of the measured data increases (relaxation). Therefore it can only be applied when the initial level of redundancy is high enough to ensure that the <b>data</b> <b>reconciliation</b> can still be done (see Section 2,).|$|E
5000|$|Tier 1 reduces {{delivery}} time, digitizes <b>data</b> {{and reduces}} <b>reconciliation</b> time (e.g. electronic files (xml, edi, flat,etc.) which match the customers purchase order, or web invoicing solutions).|$|R
50|$|Data input {{required}} intermediate processing via punched {{paper tape}} or punched card and separate input to a repetitive, labor-intensive task, removed from user control and error-prone. Invalid or incorrect data needed correction and resubmission with consequences for <b>data</b> and account <b>reconciliation.</b>|$|R
5000|$|For these reasons, a pricing model-based {{approach}} to attribution {{may not be}} the right one where <b>data</b> sourcing or <b>reconciliation</b> is an issue. An alternative solution is to perform a Taylor expansion on the price of a security [...] and remove higher-order terms, which gives ...|$|R
5000|$|<b>Data</b> <b>reconciliation</b> relies {{strongly}} on {{the concept}} of redundancy to correct the measurements as little as possible in order to satisfy the process constraints. Here, redundancy is defined differently from redundancy in information theory. Instead, redundancy arises from combining sensor data with the model (algebraic constraints), sometimes more specifically called [...] "spatial redundancy", [...] "analytical redundancy", or [...] "topological redundancy".|$|E
5000|$|Random errors {{means that}} the {{measurement}} [...] is a random variable with mean , where [...] is the true value that is typically not known. A systematic error {{on the other hand}} is characterized by a measurement [...] which is a random variable with mean , which is not equal to the true value [...] For ease in deriving and implementing an optimal estimation solution, and based on arguments that errors are the sum of many factors (so that the Central limit theorem has some effect), <b>data</b> <b>reconciliation</b> assumes these errors are normally distributed.|$|E
5000|$|Observability {{may also}} be {{characterized}} for steady state systems (systems typically {{defined in terms of}} algebraic equations and inequalities), or more generally, for sets in [...] ,. [...] Just as observability criteria are used to predict the behavior of Kalman filters or other observers in the dynamic system case, observability criteria for sets in [...] are used to predict the behavior of <b>data</b> <b>reconciliation</b> and other static estimators. In the nonlinear case, observability can be characterized for individual variables, and also for local estimator behavior rather than just global behavior.|$|E
50|$|Addepar, Inc. is an American {{investment}} management technology company that offers an integrated financial software platform to single and multi-family offices, wealth advisors, large financial institutions, endowments and foundations, funds and fund administrators for financial <b>data</b> aggregation & <b>reconciliation,</b> investment portfolio analysis, and client reporting needs.|$|R
40|$|As {{detailed}} {{in the original}} statement of work, the objective of phase two of this research effort {{was to develop a}} general framework for rocket engine performance prediction that integrates physical principles, a rigorous mathematical formalism, component level test data, system level test <b>data,</b> and theory-observation <b>reconciliation.</b> Specific phase two development tasks are defined...|$|R
40|$|The {{development}} of chemical processes is usually based on both experiments (often in pilot plants), and process simulation. Design of experiments, <b>data</b> evaluation and <b>reconciliation,</b> model development and validation are essential steps in this procedure. Different tools and approaches {{are available for}} each of these tasks but in the process developer's workflow, they are usually not supported in an integrated way. Therefore, in the project INES, on which this paper reports, a new interface between experiments and simulation for process design was created, and integrated in a tool box which comprehensively supports process design. It contains modules for <b>data</b> selection and <b>reconciliation,</b> sensitivity analysis, and model validation and -adjustment. Methods from the literature are suitably combined to support the overall goal. The chosen methods, their combination and implementation are described and examples are given which demonstrate the benefits of the new interactive tool in the process development workflow...|$|R
5000|$|DVR {{has become}} more and more {{important}} due to industrial processes that {{are becoming more and more}} complex. DVR started in the early 1960s with applications aiming at closing material balances in production processes where raw measurements were available for all variables. At the same time the problem of gross error identification and elimination has been presented. In the late 1960s and 1970s unmeasured variables were taken into account in the <b>data</b> <b>reconciliation</b> process., DVR also became more mature by considering general nonlinear equation systems coming from thermodynamic models.,, Quasi steady state dynamics for filtering and simultaneous parameter estimation over time were introduced in 1977 by Stanley and Mah. [...] Dynamic DVR was formulated as a nonlinear optimization problem by Liebman et al. in 1992.|$|E
5000|$|While {{individual}} solutions {{have helped}} to automate or streamline particular application areas, maintaining multiple systems containing overlapping data and [...] brought significant inefficiencies. The industry found that eliminating data discrepancies between systems has reduced <b>data</b> <b>reconciliation</b> activities and helped ensure that those responsible for a clinical trial always have accurate and up-to-date information. As the number of relevant applications increases with greater adoption of EDC and other technologies, the problems of duplication of data and redundancy in process have increased. As a consequence, the pursuit of an integrated technology suite to streamline workflows and improve usability has become a key characteristic of the industry’s latest [...] "eClinical" [...] approach. Furthermore, It improves productivity by reducing the need for internal staff to input data.|$|E
40|$|The use of <b>data</b> <b>reconciliation</b> {{techniques}} can considerably reduce the inaccuracy of process data due to measurement errors. This in turn results in improved control system performance and process knowledge. Dynamic <b>data</b> <b>reconciliation</b> techniques {{are applied to}} a model-based predictive control scheme. It is shown through simulations on a chemical reactor system that the overall performance of the model-based predictive controller is enhanced considerably when <b>data</b> <b>reconciliation</b> is applied. The dynamic <b>data</b> <b>reconciliation</b> techniques used include a combined strategy for the simultaneous identification of outliers and systematic bias...|$|E
40|$|The PCS sends and {{receives}} nibbles of <b>data</b> to the <b>Reconciliation</b> sublayer. It uses 16 signals to transfer and receive data, and to indicate collisions and carrier. • RX signals – RXD- 4 lines for received data nibbles – RX_ER – indicate a receive error – RX_DV – indicate {{the reception of}} valid data – RX_CLK – used as timing reference for transfer of RX signal...|$|R
40|$|Abstract. Ubiquitous and {{pervasive}} computing environments {{have the potential}} to provide rich sources of information about a user and their surroundings. Such context information may form a basis upon which Adaptive Information Services may be adapted. However, the nature of context information means that it is usually gathered in an ad-hoc and distributed manner with many devices and sensors storing potentially relevant <b>data.</b> The <b>reconciliation</b> and reasoning across this information presents a research challenge, but has the possibility of yielding valuable insights about users. In an ad-hoc pervasive computing environment, determining context information cannot rely on a fixed meta-data schema. This work shows how an ontology driven context service architecture will perform distributed open schema queries over heterogeneous context sources in order to provide information service adaptation. ...|$|R
50|$|Analytical {{techniques}} base {{the calculation}} of the measurement estimate on approximations of the physical laws that govern {{the relationship of the}} quantity of interest with other available measurements and parameters. For well-understood processes these approximations can be very accurate (e.g. using mass & energy balance equations) while for other processes precise physical models do not exist and the used approximations can be quite crude.Analytical virtual sensing is often implemented through <b>data</b> validation and <b>reconciliation</b> methods.|$|R
40|$|The {{operation}} of {{power plants and}} chemical processes requires process measurements for optimal operations. Process measurements are essential for plant performance optimization, process monitoring and process control. It is vital to have reliable and accurate process data to achieve process optimization. However, process measurements are inevitably subject to measurement errors. These measurement errors are classified as random and gross errors. <b>Data</b> <b>reconciliation</b> technique is an effective data treatment method that is used in chemical processes to {{enhance the quality of}} process data. The purpose of <b>data</b> <b>reconciliation</b> is to reduce random errors to achieve measurements which are as accurate and reliable as possible. <b>Data</b> <b>reconciliation</b> technique uses available process measurements to produce consistent and accurate estimates, so close to the true values that they satisfy model constraints. Further, <b>data</b> <b>reconciliation</b> technique depends on measurement redundancy to perform reconciliation and produce reliable estimates. In addition, <b>data</b> <b>reconciliation</b> can also provide estimates of unmeasured observable variables. Process <b>data</b> <b>reconciliation</b> is not complete without a gross error detection strategy that can effectively detect and eliminate gross errors in measurements. <b>Data</b> <b>reconciliation</b> is applied to linear and nonlinear steady state processes with measured and partially measured variables. Heat exchanger and steam generator models with nonlinear mass and energy constraints are used. The reconciliation process is applied in a feed water flow measurements model to illustrate the applicability of <b>data</b> <b>reconciliation...</b>|$|E
40|$|Abstract: The aim of {{this work}} is to compare Dynamic <b>Data</b> <b>Reconciliation</b> {{techniques}} by both theoretical tools and numerical simulations. Most nonlinear dynamic <b>data</b> <b>reconciliation</b> methods have studied cases where the input variables are constant {{over long periods of}} time separated by simple step changes. This scenario often occurs in process control; however, it imposes strong limitations on the method’s applicability. In this work the dynamic <b>data</b> <b>reconciliation</b> algorithms are compared by the application to the cases where the input variables are ramps or slow sinusoidal functions or any smooth signal. The model-based Kalman filter is compared with Liebman’s Dynamic <b>Data</b> <b>Reconciliation</b> Technique and a hybrid approach is proposed...|$|E
40|$|The Engineering Pilot {{plant at}} Murdoch University {{is one of}} the unique process {{facilities}} that currently exists in any universities in Australia. It provides an opportunity for Instrumentation and Control engineering students studying at Murdoch University to explore various control methods and strategies to extend their range of knowledge in this area with the aid of physical instruments and software provided in the pilot plant. The purpose of this thesis project is to devise a strategy to improve the process measurements in the pilot plant through investigation of theory and implementation of Steady State <b>Data</b> <b>Reconciliation</b> and Online Dynamic <b>Data</b> <b>reconciliation</b> and to observe and testify if <b>data</b> <b>reconciliation</b> can improve process control performance with a more accurate set of measurements This project has been divided into three different stages with the first stage being, the understanding of <b>data</b> <b>reconciliation,</b> understanding of DR using existing case studies, followed by the investigation of the physical implementation and foundation work of <b>data</b> <b>reconciliation</b> in the pilot plant, and finally the testing and conclusion {{as to whether or not}} <b>data</b> <b>reconciliation</b> can improve the process measurements and consequently the control results. Results of the implementation of <b>data</b> <b>reconciliation</b> indicated the improvement of process measurements. Non-Linear Tank and Needle Tank have shown considerable measurement improvement with the implementation of <b>data</b> <b>reconciliation</b> on the measuring variables of both tanks. Flow rate in, flow rate out, and levels of both tanks were monitored and measured except the flow going into Non-linear Tank were estimated as there were no measuring device available for that particular variable. Ball Mill Tank and Cyclone Underflow Tank were also investigated with a recycle stream, steady state <b>data</b> <b>reconciliation</b> have also been implemented at this process system and helps to improve a better understanding of <b>data</b> <b>reconciliation</b> applications. Dynamic <b>data</b> <b>reconciliation</b> were also implemented at the Needle Tank in the pilot plant for case study investigation, the results obtained is of what was expected and proved that DDR can provide significant contribution in terms of improving process control performance of the process system in the future...|$|E
50|$|When {{quantifying}} MFA systems {{either by}} measurements or from statistical data, mass and other process balances {{have to be}} checked to ensure the correctness of the quantification and to reveal possible data inconsistencies or even misconceptions in the system such as the omission of a flow or a process. Conflicting information can be reconciled using <b>data</b> validation and <b>reconciliation,</b> and the STAN-software offers basic reconciliation functionality that is suitable for many MFA application.|$|R
40|$|This paper {{presents}} an update to the Report on the Observance of Standards and Codes (ROSC) on Fiscal Transparency Module for Italy. Some {{progress has been}} made vis-Ã -vis the 2003 ROSC update, especially toward strengthening the integrity of <b>data.</b> The <b>reconciliation</b> of above- and below-the-line fiscal data has contributed to strengthening the quality of data and budget reporting. On the recommendations regarding public availability of information, progress is needed to allow a firmer assessment of Italy’s fiscal developments and prospects. A few issues remain outstanding. Fiscal transparency;Reports on the Observance of Standards and Codes;public finances, budget law, fiscal data, budget management, local public finances, contingent liabilities, fiscal balances, government policy, resource allocation, fiscal years, budgetary allocations, budgetary plans, open budget preparation, fiscal plans, budget documents, fiscal information, fiscal developments, fiscal balance, budget deficit, fiscal targets, budget preparation, budgetary limits, budgetary policies, budget documentation, budget execution...|$|R
40|$|We {{present a}} {{methodology}} for Data Warehouse design and its application within the TELECOM ITALIA information system. The methodology {{is based on}} a conceptual representation of the Enterprise, which is exploited both in the integration phase of the Warehouse information sources and during the knowledge discovery activity on the information stored in the Warehouse. The application of the methodology in the TELECOM ITALIA framework has been supported by prototype software tools both for conceptual modeling and for <b>data</b> integration and <b>reconciliation.</b> (c) 2004 Elsevier B. V. All rights reserved...|$|R
