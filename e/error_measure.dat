551|4253|Public
5000|$|If A is the {{identity}} matrix, then the Mahalanobis metric {{is exactly the}} same as the <b>error</b> <b>measure</b> used in k-means. If A is not {{the identity}} matrix, then [...] will favor certain directions as the k q-flat <b>error</b> <b>measure.</b>|$|E
50|$|The <b>error</b> <b>measure</b> {{is given}} by {{subtracting}} the above two results i.e.|$|E
5000|$|The {{maximum of}} this <b>error</b> <b>measure</b> is {{what needs to}} be minimized.There are {{different}} norms available for minimizing the error namely: ...|$|E
40|$|The {{focus of}} this study is to use Monte Carlo method in fuzzy linear regression. The purpose of the study is to figure out the {{appropriate}} <b>error</b> <b>measures</b> for the estimation of fuzzy linear regression model parameters with Monte Carlo method. Since model parameters are estimated without any mathematical programming or heavy fuzzy arithmetic operations in fuzzy linear regression with Monte Carlo method. In the literature, only two <b>error</b> <b>measures</b> (E 1 and E 2) are available for the estimation of fuzzy linear regression model parameters. Additionally, accuracy of available <b>error</b> <b>measures</b> under the Monte Carlo procedure has not been evaluated. In this article, mean square error, mean percentage error, mean absolute percentage error, and symmetric mean absolute percentage error are proposed for the estimation of fuzzy linear regression model parameters with Monte Carlo method. Moreover, estimation accuracies of existing and proposed <b>error</b> <b>measures</b> are explored. <b>Error</b> <b>measures</b> are compared to each other in terms of estimation accuracy; hence, this study demonstrates that the best <b>error</b> <b>measures</b> to estimate fuzzy linear regression model parameters with Monte Carlo method are proved to be E 1, E 2, and the mean square error. One the other hand, the worst one can be given as the mean percentage error. These results would be useful to enrich the studies that have already focused on fuzzy linear regression models...|$|R
40|$|A {{new set of}} <b>error</b> <b>measures,</b> {{called the}} Volume <b>Error</b> <b>Measures</b> (VEM) to {{quantify}} the quality of voxel representation of 3 D objects is presented. The VEM also serve as tools for comparing voxelization methods. Application of the VEM to some simple examples is presented. A 3 D Accumulation Volume Buffer (AVB) method for anti-aliased voxelization has been developed. This new method accumulates the voxel values sampled at multiple sub-voxel sample points located within a voxel. The VEM is {{used to evaluate the}} effectiveness of this anti-aliasing scheme. Details of the implementation and the results of application of the <b>error</b> <b>measures</b> are discusse...|$|R
30|$|First, the {{correctness}} of segments resulted {{after the}} application of the three algorithms described above is compared. Concerning the endoscopic database all the algorithms have the ability to produce segmentations that comply with the manual segmentation made by a medical expert. Then for evaluating the accuracy of the segmentation <b>error</b> <b>measures</b> are used. The proposed <b>error</b> <b>measures</b> quantify the consistency between segmentations of differing granularities. Because human segmentation is considered true segmentation the <b>error</b> <b>measures</b> are calculated in relation with manual segmentation. The GCE and LCE demonstrate that the image segmentation based on an HS produces a better segmentation than the back-projection method and the LV.|$|R
5000|$|The {{objective}} is to reweight the source labeled sample such that it [...] "looks like" [...] the target sample (in term of the <b>error</b> <b>measure</b> considered) ...|$|E
5000|$|In 1992, Dunbar {{used the}} {{correlation}} observed for non-human primates to predict a social group size for humans. Using a regression equation on data for 38 primate genera, Dunbar predicted a human [...] "mean group size" [...] of 148 (casually rounded to 150), a result he considered exploratory {{due to the}} large <b>error</b> <b>measure</b> (a 95% confidence interval of 100 to 230).|$|E
5000|$|This {{observation}} {{leads to}} the problem which is solved in triangulation. Which 3D point xest is the best estimate of x given [...] and [...] and the geometry of the cameras? The answer is often found by defining an <b>error</b> <b>measure</b> which depends on xest and then minimize this error. In the following some of the various methods for computing xest presented in the literature are briefly described.|$|E
2500|$|If the {{wavefront}} <b>error</b> is <b>measured</b> {{after it}} has been corrected by the wavefront corrector, then operation is said to be [...] "closed loop". In the latter case then the wavefront <b>errors</b> <b>measured</b> will be small, and errors ...|$|R
40|$|In {{this paper}} new <b>error</b> <b>measures</b> to {{evaluate}} image features in three-dimensional scenes are proposed and reviewed. The proposed <b>error</b> <b>measures</b> {{are designed to}} take into account feature shapes, and ground truth data can be easily estimated. As other approaches, they are not error-free and a quantitative evaluation is given according to the number of wrong matches and mismatches in order to assess their validity...|$|R
30|$|This section {{presents}} the experimental {{results for the}} evaluation of the three segmentation algorithms and <b>error</b> <b>measures</b> values.|$|R
50|$|The {{automatic}} mesh cleaning filters includes {{removal of}} duplicated, unreferenced vertices, non-manifold edges, vertices, and null faces. Remeshing tools support high quality simplification based on quadric <b>error</b> <b>measure,</b> {{various kinds of}} subdivision surfaces, and two surface reconstruction algorithms from point clouds based on the ball-pivoting technique and on the Poisson surface reconstruction approach. For the removal of noise, usually present in acquired surfaces, MeshLab supports various kinds of smoothing filters and tools for curvature analysis and visualisation.|$|E
5000|$|Least {{mean square}} (LMS) Adaptive Filters [...] use {{the most common}} <b>error</b> <b>measure</b> method, the mean square error. The 2D LMS Adaptive filters are derived from the 1D LMS adaptvie filters main method which {{minimizes}} the output mean square value by adjusting coefficients of the filter. The filter has the primary 2D input signal d and the reference input signal x. The primary input signal d consists of the ideal signal and noise component. The filter is an N by N causal FIR filter with impulse response [...] Then {{we can get the}} filter output given by ...|$|E
50|$|The CPU time {{to solve}} the system of {{equations}} differs substantially from method to method. Finite differences are usually the cheapest on a per grid point basis followed by the finite element method and spectral method. However, a per grid point basis comparison {{is a little like}} comparing apple and oranges. Spectral methods deliver more accuracy on a per grid point basis than either FEM or FDM. The comparison is more meaningful if the question is recast as ”what is the computational cost to achieve a given error tolerance?”. The problem becomes one of defining the <b>error</b> <b>measure</b> which is a complicated task in general situations.|$|E
40|$|Abstract Location {{problems}} {{occurring in}} urban or regional settings may involve many {{tens of thousands}} of “demand points, ” usually individual private residences. In modeling such problems it is common to aggregate demand points to obtain tractable models. We survey aggregation approaches to a large class of location models, consider and compare various aggregation <b>error</b> <b>measures,</b> identify some effective (and ineffective) aggregation <b>error</b> <b>measures,</b> and discuss some open research areas...|$|R
40|$|A new fast block {{matching}} algorithm is presented. The sum of absolute differences (SAD) {{and the mean}} square error (MSE) are used to find a suitable motion vector. A lower bound for both <b>error</b> <b>measures</b> is exploited {{to reduce the number}} of search positions and therefore the computational requirements. The <b>error</b> <b>measures</b> for the remaining search positions are calculated simultaneously so that the computational load for these calculations only slightly increases. The algorithm is compared to a fast full search block {{matching algorithm}} based on the same concept but only using the SAD or the MSE as the matching criterion. It is shown that the algorithm using both <b>error</b> <b>measures</b> combines the advantages of both algorithms using only the SAD or the MSE...|$|R
40|$|Histograms and Wavelet {{synopses}} {{provide useful}} tools in query optimization and approximate query answering. Traditional histogram construction algorithms, such as V-Optimal, optimize absolute <b>error</b> <b>measures</b> {{for which the}} error in estimating a true value of 10 by 20 has the same e#ect of estimating a true value of 1000 by 1010. However, several researchers have recently pointed out the drawbacks of such schemes and proposed wavelet based schemes to minimize relative <b>error</b> <b>measures...</b>|$|R
40|$|International audienceRemotely sensed {{images are}} often too complex to be {{analyzed}} by a single algorithm. In this work, the author considers the problem of merging results issued from different algorithms performing the same task on a satellite image so as {{to improve the quality}} of the result. Two types of error measures are computed for each (intermediate) result to estimate its reliability: the global <b>error</b> <b>measure</b> determines whether a result is good enough to be used in the fusion process; the local <b>error</b> <b>measure</b> of each site determines hour information given by this site will be taken into account in the fusion process: site with a smaller local <b>error</b> <b>measure</b> has a higher reliability, thus has more influence on decision making in the fusion process. Map knowledge is used for evaluating the <b>error</b> <b>measure...</b>|$|E
40|$|The dual {{analysis}} {{concept was}} introduced by Fraeijs de Veubeke [1] {{as a consequence of}} upper and lower bounds of the energy. As such bounds exist only in those cases where one type of condition is homogeneous, it is commonly admitted that a dual <b>error</b> <b>measure</b> does not exist with general boundary conditions. This paper presents a re-examination of the dual <b>error</b> <b>measure</b> by a way which avoids any use of upper and lower bounds of the energy. It is found that such an <b>error</b> <b>measure</b> holds whatever the boundary conditions be. Furthermore, {{it is not necessary to}} obtain the approximate solutions by a Rayleigh-Ritz process, so that the second analysis, which seemed necessary in the original dual analysis concept, may be replaced by any admissible approximation. This implies the possibility of a dual <b>error</b> <b>measure</b> at a simple post-processor level. Peer reviewe...|$|E
40|$|We {{introduce}} a new <b>error</b> <b>measure</b> for matrix-product states without requiring the relatively costly two-site density matrix renormalization group (2 DMRG). This <b>error</b> <b>measure</b> {{is based on an}} approximation of the full variance 〈ψ | (Ĥ - E) ^ 2 |ψ〉. When applied to a series of matrix-product states at different bond dimensions obtained from a single-site density matrix renormalization group (1 DMRG) calculation, it allows for the extrapolation of observables towards the zero-error case representing the exact ground state of the system. The calculation of the <b>error</b> <b>measure</b> is split into a sequential part of cost equivalent to two calculations of 〈ψ | Ĥ | ψ〉 and a trivially parallelized part scaling like a single operator application in 2 DMRG. The reliability of the new <b>error</b> <b>measure</b> is demonstrated at four examples: the L= 30, S= 1 / 2 Heisenberg chain, the L= 50 Hubbard chain, an electronic model with long-range Coulomb-like interactions and the Hubbard model on a cylinder of size 10 × 4. Extrapolation in the new <b>error</b> <b>measure</b> is shown to be on-par with extrapolation in the 2 DMRG truncation error or the full variance 〈ψ | (Ĥ - E) ^ 2 |ψ〉 {{at a fraction of the}} computational effort. Comment: 10 pages, 11 figure...|$|E
40|$|Histograms and Wavelet {{synopses}} {{provide useful}} tools in query optimization and approximate query answering. Traditional histogram construction algorithms, e. g., V-Optimal, use <b>error</b> <b>measures</b> {{which are the}} sums of a suitable function, e. g., square, of the error at each point. Although the best-known algorithms for solving these problems run in quadratic time, a sequence of results have given us a linear time approximation scheme for these algorithms. In recent years, {{there have been many}} emerging applications where we are interested in measuring the maximum (absolute or relative) error at a point. We show that this problem is fundamentally different from the other traditional nonl∞ <b>error</b> <b>measures</b> and provide an optimal algorithm that runs in linear time for a small number of buckets. We also present results which work for arbitrary weighted maximum <b>error</b> <b>measures...</b>|$|R
5000|$|If the {{wavefront}} <b>error</b> is <b>measured</b> {{before it}} has been corrected by the wavefront corrector, then operation {{is said to be}} [...] "open loop". If the wavefront <b>error</b> is <b>measured</b> after {{it has been}} corrected by the wavefront corrector, then operation is said to be [...] "closed loop". In the latter case then the wavefront <b>errors</b> <b>measured</b> will be small, and errorsin the measurement and correction {{are more likely to be}} removed. Closed loop correction is the norm.|$|R
40|$|Histograms and Wavelet {{synopses}} {{provide useful}} tools in query optimization and approximate query answering. Traditional histogram construction algorithms, such as V-Optimal, optimize absolute <b>error</b> <b>measures</b> {{for which the}} error in estimating a true value of 10 by 20 has the same effect of estimating a true value of 1000 by 1010. However, several researchers have recently pointed out the drawbacks of such schemes and proposed wavelet based schemes to minimize relative <b>error</b> <b>measures.</b> None of these schemes provide satisfactory guarantees – and we provide evidence that the difficulty {{may lie in the}} choice of wavelets as the representation scheme. In this paper, we consider histogram construction for the known relative <b>error</b> <b>measures.</b> We develop optimal as well as fast approximation algorithms. We provide a comprehensive theoretical analysis and demonstrate the effectiveness of these algorithms in providing significantly more accurate answers through synthetic and real life data sets...|$|R
30|$|Our {{proposed}} method {{consists of}} three main components that could be altered or improved while still keeping the same validation framework: the <b>error</b> <b>measure,</b> the similarity measure and the clustering algorithm. The <b>error</b> <b>measure</b> we presented was specifically developed for reproducing h-indices; we believe other goals could be accomplished as well. The similarity measure could be easily extended by further metadata. Furthermore, our clustering algorithm, while intuitive and computationally efficient, could potentially be replaced by some more sophisticated community detection.|$|E
3000|$|... as our <b>error</b> <b>measure.</b> We {{focus on}} {{reductions}} that preserve {{the behavior of}} the system (Eq. 4) relative to a given linear measurement functional [...]...|$|E
40|$|Perceptrons with graded {{input-output}} {{relations and}} a limited output precision are studied within the Gardner- Derrida canonical ensemble approach. Soft non-negative error measures are introduced allowing for extended retrieval properties. In particular, {{the performance of}} these systems for a linear (quadratic) <b>error</b> <b>measure,</b> corresponding to the perceptron (adaline) learning algorithm, is compared with the performance for a rigid <b>error</b> <b>measure,</b> simply {{counting the number of}} errors. Replica-symmetry-breaking effects are evaluated, and the analytic results are compared with numerical simulations. [S 1063 - 651 X(99) 04503 - 1...|$|E
40|$|The {{predictive}} {{uncertainty of}} hydrologic models has {{attracted the attention}} of hydrologists during the last decade. However, an often overlooked type of uncertainty is the uncertainty of hydrologic measurements. Hydrologic measurements are the reference against which simulated values are contrasted. This paper presents a methodology to account for the uncertainty of hydrologic measurements and to assess the model performance based on both predictive and measurement uncertainty. Accordingly, three <b>error</b> <b>measures,</b> namely the measurement uncertainty-based mean relative error (MU_MRE), measurement and predictive uncertainty-based credibility (MPU_CRE), and mean relative error (MPU_MRE) are defined to assess the performance of hydrologic models. The proposed approach and <b>error</b> <b>measures</b> are perceived to be more realistic in light of uncertain measurements. They can provide a different assessment from an assessment made based on traditional <b>error</b> <b>measures.</b> This approach could have further implications and impacts on the process of calibration and validation of hydrologic models. 1...|$|R
30|$|<b>Error</b> <b>measures</b> {{defined in}} Equation  19 account {{at the same}} time for the cross effects among the {{different}} fields and the ones between space and time discretizations.|$|R
40|$|<b>Error</b> <b>measures</b> for the {{evaluation}} of forecasts are usually based {{on the size of the}} forecast <b>errors.</b> Common <b>measures</b> are e. g. the Mean Squared Error (MSE), the Mean Absolute Deviation (MAD) or the Mean Absolute Percentage <b>Error</b> (MAPE). Alternative <b>measures</b> for the comparison of forecasts are turning points or hits-and-misses, where an indicator loss function is used to decide, if a forecast is of high quality or not. Here, we discuss the latter to obtain reliable combined forecasts...|$|R
40|$|Flow in the world's oceans {{occurs at}} {{a wide range}} of spatial scales, from a {{fraction}} of a metre up to many thousands of kilometers. In particular, regions of intense flow are often highly localised, for example, western boundary currents, equatorial jets, overflows and convective plumes. Conventional numerical ocean models generally use static meshes. The use of dynamically-adaptive meshes has many potential advantages but needs to be guided by an <b>error</b> <b>measure</b> reflecting the underlying physics. A method of defining an <b>error</b> <b>measure</b> to guide an adaptive meshing algorithm for unstructured tetrahedral finite elements, utilizing an adjoint or goal-based method, is described here. This method is based upon a functional, encompassing important features of the flow structure. The sensitivity of this functional, with respect to the solution variables, is used as the basis from which an <b>error</b> <b>measure</b> is derived. This <b>error</b> <b>measure</b> acts to predict those areas of the domain where resolution should be changed. A barotropic wind driven gyre problem is used to demonstrate the capabilities of the method. The overall objective of this work is to develop robust error measures for use in an oceanographic context which will ensure areas of fine mesh resolution are used only where and when they are required. (c) 2006 Elsevier Ltd. All rights reserved...|$|E
40|$|We {{propose a}} sinogram {{restoration}} method {{which consists of}} a patch-wise non-linear processing, based on a sparsity prior {{in terms of a}} learned dictionary. An off-line learning process uses a statistical model of the sinogram noise and minimizes an <b>error</b> <b>measure</b> in the image domain over the training set. The <b>error</b> <b>measure</b> is designed to preserve low-contrast edges for visibility of soft tissues. Our numerical study shows that the algorithm improves on the performance of the standard Filtered Back-Projection algorithm and effectively allows to halve the radiation dose for the same image quality...|$|E
40|$|The {{increase}} in available computational {{power and the}} higher quality of experimental recordings have turned the tuning of neuron model parameters into a problem that can be solved by automatic global optimization algorithms. Neurofitter is a software tool that interfaces existing neural simulation software and sophisticated optimization algorithms with {{a new way to}} compute the <b>error</b> <b>measure.</b> This <b>error</b> <b>measure</b> represents how well a given parameter set is able to reproduce the experimental data. It is based on the phase-plane trajectory density method, which is insensitive to small phase differences between model and data. Neurofitter enables the effortless combination of many different time-dependent data traces into the <b>error</b> <b>measure,</b> allowing the neuroscientist to focus on what are the seminal properties of the model. We show results obtained by applying Neurofitter to a simple single compartmental model and a complex multi-compartmental Purkinje cell (PC) model. These examples show that the method is able to solve a variety of tuning problems and demonstrate details of its practical application...|$|E
30|$|When {{there are}} more than one model developed, a {{comparative}} analysis can be made by calculating the accuracy of each model and comparing them. The accuracy of any predictive model can be determined only by choosing appropriate <b>error</b> <b>measures.</b> In this paper, different models are compared using two <b>error</b> <b>measuring</b> parameters, Mean Absolute Percentage Error (MAPE) and Symmetric Mean Absolute Percentage Error (SMAPE) which can be defined as shown below. MAPE is a relative <b>measure</b> which reflects <b>error</b> as the percentage of actual data. Hence, the accuracy of the model can be easily judged.|$|R
40|$|This paper {{presents}} a novel Bayesian method to track multiple targets in an image sequence without explicit detection. Our method is formulated based on finite set {{representation of the}} multi-target state and the recently developed multi-Bernoulli filter. Experimental results on sport player and cell tracking studies show that our method can automatically track numerous targets, and it outperforms the state-of-the-art in terms of false positive (false alarm) and false negative (missing) rates as detection <b>error</b> <b>measures,</b> {{and in terms of}} label switching rate and lost tracks ratio as tracking <b>error</b> <b>measures...</b>|$|R
50|$|Image-Based segmentation: Some methods {{initiate}} a template and refine its shape {{according to the}} image data while minimizing integral <b>error</b> <b>measures,</b> like the Active contour model and its variations.|$|R
