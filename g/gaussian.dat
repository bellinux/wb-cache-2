10000|1663|Public
5|$|Convolve the {{locations}} of the points with a <b>Gaussian</b> function with small standard deviation, in effect replacing each point's location with a weighted average of {{the locations}} of nearby points along the curve, with <b>Gaussian</b> weights. The standard deviation of the <b>Gaussian</b> should be chosen to be small enough that, after this step, the sample points still have nearly-uniform spacing.|$|E
5|$|Many of {{the other}} {{applications}} of the Euclidean algorithm carry over to <b>Gaussian</b> integers. For example, {{it can be used}} to solve linear Diophantine equations and Chinese remainder problems for <b>Gaussian</b> integers; continued fractions of <b>Gaussian</b> integers can also be defined.|$|E
5|$|If the {{function}} f {{corresponds to a}} norm function, such as that used to order the <b>Gaussian</b> integers above, then the domain is known as norm-Euclidean. The norm-Euclidean rings of quadratic integers are exactly those where D=−11, −7, −3, −2, −1, 2, 3, 5, 6, 7, 11, 13, 17, 19, 21, 29, 33, 37, 41, 57 or 73. The quadratic integers with D=−1 and −3 are known as the <b>Gaussian</b> integers and Eisenstein integers, respectively.|$|E
30|$|The same {{transform}} and bias vector can {{be shared}} between all the <b>Gaussians</b> of all the states (global adaptation). If enough adaptation data is available, multiple transforms can be estimated separately for sets of states or <b>Gaussians.</b> The states or <b>Gaussians</b> can be grouped by either their phonetic similarity or their acoustic similarity. The groups are called base classes.|$|R
30|$|It is {{important}} to point out at this juncture that the process by which only the average number of the most influencing <b>Gaussians</b> is kept is based on their percent contribution relative to the total contribution of all the <b>Gaussians.</b> These influencing <b>Gaussians</b> are selected whenever their relative percent contributions exceed a very small threshold chosen to be equal to 0.001 % in our simulation.|$|R
40|$|Given data {{drawn from}} a mixture of multivariate <b>Gaussians,</b> a basic problem is to {{accurately}} estimate the mixture parameters. We provide a polynomial-time algorithm for this problem for the case of two <b>Gaussians</b> in n dimensions (even if they overlap), with provably minimal assumptions on the <b>Gaussians,</b> and polynomial data requirements. In statistical terms, our estimator converges at an inverse polynomial rate, and no such estimator (even exponential time) was known for this problem (even in one dimension). Our algorithm reduces the n-dimensional problem to the one-dimensional problem, where the method of moments is applied. One technical challenge is proving that noisy estimates of the first six moments of a univariate mixture suffice to recover accurate estimates of the mixture parameters, as conjectured by Pearson (1894), and in fact these estimates converge at an inverse polynomial rate. As a corollary, we can efficiently perform near-optimal clustering: in the case where the overlap between the <b>Gaussians</b> is small, one can accurately cluster the data, and when the <b>Gaussians</b> have partial overlap, one can still accurately cluster those data points which {{are not in the}} overlap region. A second consequence is a polynomial-time density estimation algorithm for arbitrary mixtures of two <b>Gaussians,</b> generalizing previous work on axis-aligned <b>Gaussians</b> (Feldman et al, 2006) ...|$|R
5|$|David X. Li :Canadian {{qualified}} actuary who in {{the first}} decade of the 21st century pioneered the use of <b>Gaussian</b> copula models for the pricing of collateralized debt obligations (CDOs).|$|E
5|$|The <b>Gaussian</b> {{integers}} {{are complex}} {{numbers of the}} form α=u+vi, where u and v are ordinary integers and i is the square root of negative one. By defining an analog of the Euclidean algorithm, <b>Gaussian</b> integers can {{be shown to be}} uniquely factorizable, by the argument above. This unique factorization is helpful in many applications, such as deriving all Pythagorean triples or proving Fermat's theorem on sums of two squares. In general, the Euclidean algorithm is convenient in such applications, but not essential; for example, the theorems can often be proven by other arguments.|$|E
25|$|Mathematically, {{applying}} a <b>Gaussian</b> blur to an image {{is the same}} as convolving the image with a <b>Gaussian</b> function. This is also known as a two-dimensional Weierstrass transform. By contrast, convolving by a circle (i.e., a circular box blur) would more accurately reproduce the bokeh effect. Since the Fourier transform of a <b>Gaussian</b> is another <b>Gaussian,</b> {{applying a}} <b>Gaussian</b> blur has the effect of reducing the image's high-frequency components; a <b>Gaussian</b> blur is thus a low pass filter.|$|E
40|$|We {{propose a}} family of complex {{differential}} operators, symmetry derivatives, for pattern recognition in images. We present three theorems on their properties as applied to <b>Gaussians.</b> These show that all orders of symmetry derivatives of <b>Gaussians</b> yield compact expressions obtained by replacing the original differential polynomial with an ordinary polynomial. Just like <b>Gaussians,</b> the symmetry derivatives of <b>Gaussians</b> are (form) invariant to Fourier transform, that is they are rescaled versions of the original. As a result, the symmetry derivatives of <b>Gaussians</b> are closed under the convolution operator, i. e. they map on {{a member of the}} family when convolved with each other. Since <b>Gaussians</b> are utilized extensively in image processing, the revealed properties have practical consequences, e. g. when designing filters and filtering schemes that are unbiased w. r. t. orientation (isotropic). A use of these results is illustrated by an application: tracking the cross markers in long image sequences from vehicle crash tests. The implementation and the results of this application are discussed in terms of the theorems presented, along with conclusions. ...|$|R
40|$|Given data {{drawn from}} a mixture of multivariate <b>Gaussians,</b> a basic problem is to {{accurately}} estimate the mixture parameters. We give an algorithm for this problem that has a running time, and data requirement polynomial in the dimension and the inverse of the desired accuracy, with provably minimal assumptions on the <b>Gaussians.</b> As simple consequences of our learning algorithm, we can perform near-optimal clustering of the sample points and density estimation for mixtures of k <b>Gaussians,</b> efficiently. The building blocks of our algorithm {{are based on the}} work (Kalai et al, STOC 2010) [17] that gives an efficient algorithm for learning mixtures of two <b>Gaussians</b> by considering a series of projections down to one dimension, and applying the method of moments to each univariate projection. A major technical hurdle in [17] is showing that one can efficiently learn univariate mixtures of two <b>Gaussians.</b> In contrast, because pathological scenarios can arise when considering univariate projections of mixtures of more than two <b>Gaussians,</b> the bulk of the work in this paper concerns how to leverage an algorithm for learning univariate mixtures (of many <b>Gaussians)</b> to yield an efficient algorithm for learning in high dimensions. Our algorithm employs hierarchical clustering and rescaling, together with delicate methods for backtracking and recovering from failures that can occur in our univariate algorithm. Finally, while the running time and data requirements of our algorithm depend exponentially on the number of <b>Gaussians</b> in the mixture, we prove that such a dependence is necessary. ar X i...|$|R
40|$|There are {{efficient}} {{software programs}} for extracting from image sequences certain mixtures of distributions, such as multivariate <b>Gaussians,</b> {{to represent the}} important features needed for accurate document retrieval from databases. This note describes a method to use information geometric methods to measure distances between distributions in mixtures of multivariate <b>Gaussians.</b> There is no general analytic solution for the information geodesic distance between two k-variate <b>Gaussians,</b> but for many purposes the absolute information distance is not essential and comparative values suffice for proximity testing. For two mixtures of multivariate <b>Gaussians</b> we must resort to approximations to incorporate the weightings. In practice, the relation between a reasonable approximation and a true geodesic distance {{is likely to be}} monotonic, which is adequate for many applications. Here we compare several choices for the incorporation of weightings in distance estimation and provide illustrative results from simulations of differently weighted mixtures of multivariate <b>Gaussians...</b>|$|R
25|$|Most {{surfaces}} {{will contain}} regions of positive <b>Gaussian</b> curvature (elliptical points) and regions of negative <b>Gaussian</b> curvature {{separated by a}} curve of points with zero <b>Gaussian</b> curvature called a parabolic line.|$|E
25|$|In {{representing}} the wave {{function of a}} localized particle, the wave packet is often taken to have a <b>Gaussian</b> shape and is called a <b>Gaussian</b> wave packet. <b>Gaussian</b> wave packets also are used to analyze water waves.|$|E
25|$|<b>Gaussian</b> {{process is}} a {{powerful}} non-linear interpolation tool. Many popular interpolation tools are actually equivalent to particular <b>Gaussian</b> processes. <b>Gaussian</b> processes can be used not only for fitting an interpolant that passes exactly through the given data points but also for regression, i.e., for fitting a curve through noisy data. In the geostatistics community <b>Gaussian</b> process regression {{is also known as}} Kriging.|$|E
30|$|In this article, the {{backend process}} of the LDA and {{diagonal}} covariance <b>Gaussians</b> is used in language recognition system, because the backend {{process of the}} LDA and diagonal covariance <b>Gaussians</b> is superior to log-likelihood ratios normalization in our experiments.|$|R
40|$|It was proved {{independently}} by Foschi and Hundertmark, Zharnitsky that <b>Gaussians</b> extremize the adjoint Fourier restriction inequality for L^ 2 functions on the paraboloid {{in the two}} lowest-dimesional cases. Here we {{prove that}} <b>Gaussians</b> are critical points for the L^p to L^q adjoint Fourier restriction inequalities {{if and only if}} p= 2. Also, <b>Gaussians</b> are critial points for the L^ 2 to L^r_t L^q_x Strichartz inequalities for all admissible pairs (r,q) in (1,infinity) ^ 2. Comment: 9 page...|$|R
50|$|For {{reasons of}} convenience, many quantum {{chemistry}} programs {{work in a}} basis of Cartesian <b>Gaussians</b> even when spherical <b>Gaussians</b> are requested, as integral evaluation is much easier in the cartesian basis, and the spherical functions can be simply expressed using the cartesian functions.|$|R
25|$|<b>Gaussian</b> {{curvature}} is {{the product}} of the two principal curvatures. It is an intrinsic property that can be determined by measuring length and angles and is independent of how the surface is embedded in space. Hence, bending a surface will not alter the <b>Gaussian</b> curvature, and other surfaces with constant positive <b>Gaussian</b> curvature can be obtained by cutting a small slit in the sphere and bending it. All these other surfaces would have boundaries, and the sphere is the only surface that lacks a boundary with constant, positive <b>Gaussian</b> curvature. The pseudosphere {{is an example of a}} surface with constant negative <b>Gaussian</b> curvature.|$|E
25|$|EnKFs {{rely on the}} <b>Gaussian</b> assumption, {{although}} they in practice are used for nonlinear problems, where the <b>Gaussian</b> assumption may not be satisfied. Related filters attempting to relax the <b>Gaussian</b> assumption in EnKF while preserving its advantages include filters that fit the state pdf with multiple <b>Gaussian</b> kernels, filters that approximate the state pdf by <b>Gaussian</b> mixtures, {{a variant of the}} particle filter with computation of particle weights by density estimation, and a variant of the particle filter with thick tailed data pdf to alleviate particle filter degeneracy.|$|E
25|$|For example, {{a sphere}} of radius r has <b>Gaussian</b> {{curvature}} 1/r2 everywhere, and a flat plane and a cylinder have <b>Gaussian</b> curvature 0 everywhere. The <b>Gaussian</b> curvature {{can also be}} negative, {{as in the case}} of a hyperboloid or the inside of a torus.|$|E
30|$|Bhattacharyya {{distance}} [39] between single <b>Gaussians</b> (Bhattacharyya).|$|R
40|$|AbstractImproved entropy decay {{estimates}} for the heat equation are obtained by selecting well-parametrized <b>Gaussians.</b> Either by mass centering or by fixing the second moments or the covariance matrix of the solution, relative entropy toward these <b>Gaussians</b> is shown to decay with better constants than classical estimates...|$|R
50|$|These <b>Gaussians</b> are plotted in the {{accompanying}} figure.|$|R
25|$|The {{fundamental}} theorem of arithmetic {{continues to}} hold in unique factorization domains. An {{example of such a}} domain is the <b>Gaussian</b> integers Z, that is, the set of complex numbers of the form a+bi where i denotes the imaginary unit and a and b are arbitrary integers. Its prime elements are known as <b>Gaussian</b> primes. Not every prime (in Z) is a <b>Gaussian</b> prime: in the bigger ring Z, 2 factors into the product of the two <b>Gaussian</b> primes (1+i) and (1−i). Rational primes (i.e. prime elements in Z) of the form 4k+3 are <b>Gaussian</b> primes, whereas rational primes of the form 4k+1 are not.|$|E
25|$|A common {{example is}} a chirped <b>Gaussian</b> pulse, a wave whose field {{amplitude}} follows a <b>Gaussian</b> envelope and whose instantaneous phase has a frequency sweep.|$|E
25|$|If we {{consider}} the square of a <b>Gaussian</b> integer we get the following direct interpretation of Euclid's formulae as representing a perfect square <b>Gaussian</b> integers.|$|E
5000|$|... #Caption: After {{difference}} of <b>Gaussians</b> filtering {{in black and}} white ...|$|R
50|$|Treatment of the {{difference}} of <b>Gaussians</b> approach in blob detection.|$|R
40|$|A two-step {{optimization}} {{is proposed}} to represent an arbitrary quantum state {{to a desired}} accuracy with the least number of <b>gaussians</b> in phase space. The Husimi distribution of the quantum state provides the information to determine the modulus of the weight for the <b>gaussians.</b> Then, the phase information contained in the Wigner distribution is used to obtain the full complex weights by considering the relative phases for pairs of <b>gaussians,</b> the chords. The method is exemplified with several excited states n of the harmonic and the Morse oscillators. A semiclassical interpretation {{of the number of}} <b>gaussians</b> needed {{as a function of the}} quantum number n is given. The representation can also be used to characterize Wigner and Husimi distributions directly which do not originate in a quantum state. Comment: J. Phys. B:At. Mol. Opt. Phys. (accepted, March 2004...|$|R
25|$|Functions {{that are}} {{localized}} {{in the time}} domain have Fourier transforms that are spread out across the frequency domain and vice versa, a phenomenon known as the uncertainty principle. The critical case for this principle is the <b>Gaussian</b> function, of substantial importance in probability theory and statistics {{as well as in}} the study of physical phenomena exhibiting normal distribution (e.g., diffusion). The Fourier transform of a <b>Gaussian</b> function is another <b>Gaussian</b> function. Joseph Fourier introduced the transform in his study of heat transfer, where <b>Gaussian</b> functions appear as solutions of the heat equation.|$|E
25|$|Rayleigh {{distribution}}, for {{the distribution}} of vector magnitudes with <b>Gaussian</b> distributed orthogonal components. Rayleigh distributions are found in RF signals with <b>Gaussian</b> real and imaginary components.|$|E
25|$|This is G, {{since the}} Fourier {{transform}} of this integral is easy. Each fixed τ contribution is a <b>Gaussian</b> in x, whose Fourier transform is another <b>Gaussian</b> of reciprocal width in k.|$|E
40|$|The project {{consists}} of two distinct levels i. e. separation level and diagnostic level. At the separation level, statistical models of <b>gaussians</b> and color are separately used to classify each pixel as belonging to backgroung or foreground. Adopted method is mixture of <b>gaussians.</b> A mixture of <b>gaussians</b> model is suitable here because {{the results of the}} picture tests will not depend on the lens opening, but rather on the colors in the backgroung. A mixture of <b>gaussians</b> model for return data seems reasonable. The achieved results the used method on the real sequences are presented in the thesis. Diagnostic level is identified human body on the scene. Adopted method is ASM(Active Shape Models) with PCA(Principal Component Analysis). ASM are statistical models of the shape of human bodies which iteratively deform to fit to an example of the object in a new image...|$|R
50|$|The above {{result could}} also be {{generalized}} in mixture of <b>Gaussians.</b>|$|R
40|$|In this paper, we {{describe}} how a stochastic PERT network can be formulated as a Bayesian network. We approximate such PERT Bayesian network by mixtures of <b>Gaussians</b> hybrid Bayesian networks. Since there exists algorithms for solving mixtures of <b>Gaussians</b> hybrid Bayesian networks exactly, {{we can use}} these algorithms to make inferences in PERT Bayesian networks. ...|$|R
