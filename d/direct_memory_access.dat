511|9448|Public
5|$|Super Mario RPG: Legend of the Seven Stars {{is one of}} {{only seven}} SNES games {{released}} outside Japan to use the Nintendo SA-1 chip. Compared with standard SNES games, the additional microprocessor allows higher clock speeds; faster access to the random-access memory (RAM); greater memory mapping capabilities, data storage, and compression; new <b>direct</b> <b>memory</b> <b>access</b> (DMA) modes, such as bitmap to bit plane transfer; and built-in CIC lockout for piracy protection and regional marketing control.|$|E
5|$|These {{and other}} {{differences}} reflect the differing design {{goals of the}} two buses: USB was designed for simplicity and low cost, while FireWire was designed for high performance, particularly in time-sensitive applications such as audio and video. Although similar in theoretical maximum transfer rate, FireWire400 is faster than USB2.0 high-bandwidth in real-use, especially in high-bandwidth use such as external hard drives. The newer FireWire800 standard is {{twice as fast as}} FireWire400 and faster than USB2.0 high-bandwidth both theoretically and practically. However, FireWire's speed advantages rely on low-level techniques such as <b>direct</b> <b>memory</b> <b>access</b> (DMA), which in turn have created opportunities for security exploits such as the DMA attack.|$|E
25|$|Channel {{controllers}} {{are making}} a comeback {{in the form of}} bus mastering peripheral devices, such as PCI <b>direct</b> <b>memory</b> <b>access</b> (DMA) devices. The rationale for these devices is the same as for the original channel controllers, namely off-loading transfer, interrupts, and context switching from the main CPU.|$|E
5000|$|I/O Acceleration Technology (I/OAT) is a DMA engine (an {{embedded}} DMA controller) by Intel bundled with high-end server motherboards, that offloads memory copies {{from the}} main processor by performing <b>direct</b> <b>memory</b> <b>accesses</b> (DMA). It is typically used for accelerating network traffic, but supports any kind of copy.|$|R
5000|$|... #Caption: XDMA {{might be}} similar the AMD DirectGMA (<b>Direct</b> Graphics <b>Memory</b> <b>Access)</b> {{to be found}} on AMD FirePro-branded product line.|$|R
50|$|Video {{circuits}} or modules {{are usually}} controlled using VT100-compatible command set over UART, FPGA designs usually allow <b>direct</b> video <b>memory</b> <b>access.</b>|$|R
25|$|When I/O {{transfer}} is complete or an error is detected, the controller communicates with the CPU through the channel using an interrupt. Since the channel has {{direct access to}} the main memory, it is also often referred to as DMA controller (where DMA stands for <b>direct</b> <b>memory</b> <b>access),</b> although that term is looser in definition and is often applied to non-programmable devices as well.|$|E
25|$|The {{official}} Acorn RISC Machine project {{started in}} October 1983. They chose VLSI Technology as the silicon partner, {{as they were}} a source of ROMs and custom chips for Acorn. Wilson and Furber led the design. They implemented it with a similar efficiency ethos as the 6502. A key design goal was achieving low-latency input/output (interrupt) handling like the 6502. The 6502's memory access architecture had let developers produce fast machines without costly <b>direct</b> <b>memory</b> <b>access</b> (DMA) hardware.|$|E
25|$|EIDE was an {{unofficial}} update (by Western Digital) {{to the original}} IDE standard, with the key improvement being the use of <b>direct</b> <b>memory</b> <b>access</b> (DMA) to transfer data between the disk and the computer without {{the involvement of the}} CPU, an improvement later adopted by the official ATA standards. By directly transferring data between memory and disk, DMA eliminates the need for the CPU to copy byte per byte, therefore allowing it to process other tasks while the data transfer occurs.|$|E
50|$|An {{additional}} dedicated 20 GB/s {{bus that}} bypasses L1 and L2 GPU cache for <b>direct</b> system <b>memory</b> <b>access,</b> reducing synchronisation challenges when performing fine-grain GPGPU compute tasks.|$|R
40|$|This paper {{describes}} an enhancement to the Catamount lightweight kernel for <b>direct</b> shared <b>memory</b> <b>access</b> between processes {{running on a}} multi-core processor {{as part of a}} parallel application. Unlike traditional shared memory support for interprocess communication, which involves dynamic memory allocation and mapping, we leverage Catamount’s static contiguous memory mapping scheme to allows the processes on a multi-core process to directly <b>access</b> each other’s <b>memory</b> through simple virtual address manipulation. This paper describes our implementation of <b>direct</b> shared <b>memory</b> <b>access</b> in Catamount, contrasts it to other approaches, and discusses the potential benefits of this approach in supporting MPI applications. 1...|$|R
40|$|Abstract. We present Mapped Separation Logic, an {{instance}} of Separation Logic for reasoning about virtual memory. Our logic is formalised in the Isabelle/HOL theorem prover and it allows reasoning on properties about page tables, <b>direct</b> physical <b>memory</b> <b>access,</b> virtual <b>memory</b> <b>access,</b> and shared <b>memory.</b> Mapped Separation Logic fully supports all rules of abstract Separation Logic, including the frame rule. ...|$|R
2500|$|On 10 August 2017, Microsoft {{announced}} a [...] edition {{to be made}} available in September, along with the Fall Creators Update for Windows 10. This edition is designed for high-end hardware for intensive computing tasks and supports Intel Xeon or AMD Opteron processors, up to 4 CPUs, up to 6TB RAM, the ReFS file system, Non-Volatile Dual In-line Memory Module (NVDIMM) and remote <b>direct</b> <b>memory</b> <b>access</b> (RDMA). The announcement included no licensing details.|$|E
2500|$|It is {{possible}} to develop very complex channel programs, including testing of data and conditional branching within that channel program. [...] This flexibility frees the CPU from the overhead of starting, monitoring, and managing individual I/O operations. [...] The specialized channel hardware, in turn, is dedicated to I/O and can carry it out more efficiently than the CPU (and entirely in parallel with the CPU). Channel I/O is not unlike the <b>Direct</b> <b>Memory</b> <b>Access</b> (DMA) of microcomputers, only more complex and advanced.|$|E
2500|$|To {{achieve the}} high {{performance}} needed for mathematically intensive tasks, such as decoding/encoding MPEG streams, generating or transforming three-dimensional data, or undertaking Fourier analysis of data, the Cell processor marries the SPEs and the PPE via EIB to give access, via fully cache coherent DMA (<b>direct</b> <b>memory</b> <b>access),</b> to both main memory {{and to other}} external data storage. To {{make the best of}} EIB, and to overlap computation and data transfer, each of the nine processing elements (PPE and SPEs) is equipped with a DMA engine. [...] Since the SPE's load/store instructions can only access its own local scratchpad memory, each SPE entirely depends on DMAs to transfer data to and from the main memory and other SPEs' local memories. A DMA operation can transfer either a single block area of size up to 16KB, or a list of 2 to 2048 such blocks. One of the major design decisions in the architecture of Cell is the use of DMAs as a central means of intra-chip data transfer, with a view to enabling maximal asynchrony and concurrency in data processing inside a chip.|$|E
40|$|Partitioned global {{address space}} (PGAS) {{languages}} provide a unique programming model that can span shared-memory multiprocessor (SMP) architectures, distributed memory machines, or cluster ofSMPs. Users can program large scale machines with easy-to-use, shared memory paradigms. In order to exploit large scale machines efficiently, PGAS language implementations and their runtime {{system must be}} designed for scalability and performance. The IBM XLUPC compiler and runtime system provide a scalable design {{through the use of}} the shared variable directory (SVD). The SVD stores meta-information needed to access shared data. It is dereferenced, in the worst case, for every shared <b>memory</b> <b>access,</b> thus exposing a potential performance problem. In this paper we present a cache of remote addresses as an optimization that will reduce the SVD access overhead and allow the exploitation of native (remote) <b>direct</b> <b>memory</b> <b>accesses.</b> It results in a significant performance improvement while maintaining the run-time portability and scalability. Postprint (published version...|$|R
40|$|We {{present an}} {{extension}} to classical separation logic which allows reasoning about virtual memory. Our logic is formalised in the Isabelle/HOL theorem prover {{in a manner}} allowing classical separation logic notation to be used at an abstract level. We demonstrate that in the common cases, such as user applications, our logic reduces to classical separation logic. At {{the same time we}} can express properties about page tables, <b>direct</b> physical <b>memory</b> <b>access,</b> virtual <b>memory</b> <b>access,</b> and shared <b>memory</b> in detail...|$|R
40|$|Abstract. A {{method for}} model {{checking}} of microcontroller code is presented. The main {{objective is to}} check embedded C code including typical hardware specific ingredients like embedded assembly statements, <b>direct</b> <b>memory</b> <b>accesses,</b> <b>direct</b> register accesses, interrupts, and timers, without any further manual preprocessing. For this purpose, the state space is generated directly from the assembly code that is generated from C code for the specific microcontroller, in our case the ATMEL ATmega family. The properties to be checked can refer to the global C variables {{as well as to}} the microcontroller registers and the SRAM. By this approach we are able to find bugs which cannot be found if one looks at the C code or the assembly code alone. The paper explains the basic functionality of our tools using two illustrative examples. ...|$|R
2500|$|Languages {{that are}} {{strongly}} typed and don't allow <b>direct</b> <b>memory</b> <b>access,</b> such as COBOL, Java, Python, and others, prevent buffer overflow from occurring in most cases. Many programming languages other than C/C++ provide runtime checking {{and in some}} cases even compile-time checking which might send a warning or raise an exception when C or C++ would overwrite data and continue to execute further instructions until erroneous results are obtained which might or might not cause the program to crash. [...] Examples of such languages include Ada, Eiffel, Lisp, Modula-2, Smalltalk, OCaml and such C-derivatives as Cyclone, Rust and D. The Java and [...]NET Framework bytecode environments also require bounds checking on all arrays. Nearly every interpreted language will protect against buffer overflows, signaling a well-defined error condition. Often where a language provides enough type information to do bounds checking an option is provided to enable or disable it. Static code analysis can remove many dynamic bound and type checks, but poor implementations and awkward cases can significantly decrease performance. Software engineers must carefully consider the tradeoffs of safety versus performance costs when deciding which language and compiler setting to use.|$|E
50|$|In computing, remote <b>direct</b> <b>memory</b> <b>access</b> (RDMA) is a <b>direct</b> <b>memory</b> <b>access</b> {{from the}} memory of one {{computer}} into that of another without involving either one's operating system. This permits high-throughput, low-latency networking, which is especially useful in massively parallel computer clusters.|$|E
50|$|The {{address bus}} was 16-bits wide {{in the initial}} {{implementation}} and later extended to 24-bits wide. A bus control signal could put these lines in a tri-state condition to allow <b>direct</b> <b>memory</b> <b>access.</b> The Cromemco Dazzler, for example, was an early S-100 card that retrieved digital images from memory using <b>direct</b> <b>memory</b> <b>access.</b>|$|E
40|$|Multithreading {{is known}} be {{effective}} for tolerating communication latency in distributed-memory multiprocessors. Two {{types of support}} for multithreading {{have been used to}} date including software and hardware. This paper presents the impact of multithreading on performance through empirical studies. In particular, we explicate the performance difference between software support and hardware support for the 80 -processor EM-X distributed-memory multiprocessor which we have designed and implemented. The EMX provides three types of hardware supports for fine-grain multithreading including <b>direct</b> remote <b>memory</b> <b>access,</b> fast thread invocation, and dedicated instructions for generating fixed-sized communication packets. To demonstrate the effect of multithreading, we have performed various experiments using micro benchmark programs and MP 3 D, one of the SPLASH benchmarks. Three types of performance parameters have been measured including processor efficiency, remote memory latency, and network load. Experimental results indicate that the EM-X architecture is highly effective for supporting the multithreading principles of execution through dedicated hardware and software. keywords Multithreading, latency hiding, fine grain communication, <b>direct</b> remote <b>memory</b> <b>access,</b> shared <b>memory</b> benchmark, synthetic workload. ...|$|R
40|$|The {{soundness}} of device drivers generally cannot be verified in isolation, but {{has to take}} into account the reactions of the hardware devices. In critical embedded systems, interfaces often were simple “volatile ” variables, and the interface specification typically a list of bounds on these variables. Some newer systems use “intelligent ” controllers that handle dynamic worklists in shared <b>memory</b> and perform <b>direct</b> <b>memory</b> <b>accesses,</b> all asynchronously from the main processor. Thus, it is impossible to truly verify the device driver without taking the intelligent device into account, because incorrect programming of the device can lead to dire consequences, such as memory zones being erased. We have successfully verified a device driver extracted from a critical industrial system, asynchronously combined with a model for a USB OHCI controller. This paper studies this case, as well as introduces a model and analysis techniques for this asynchronous composition...|$|R
40|$|With {{the rapid}} {{improvement}} of processor speed, {{performance of the}} memory hierarchy has become the principal bottleneck for most applications. A number of compiler transformations {{have been developed to}} improve data reuse in cache and registers, thus reducing the total number of <b>direct</b> <b>memory</b> <b>accesses</b> in a program. Until now, however, most data reuse transformations have been static [...] -applied only at compile time. As a result, these transformations cannot be used to optimize irregular and dynamic applications, in which the data layout and data access patterns remain unknown until run time and may even change during the computation. In this paper, we explore ways to achieve better data reuse in irregular and dynamic applications by building on the inspector-executor method used by Saltz for run-time parallelization. In particular, we present and evaluate a dynamic approach for improving both computation and data locality in irregular programs. Our results demonstrate that run-time progra [...] ...|$|R
50|$|RDMA (Remote <b>Direct</b> <b>Memory</b> <b>Access)</b> {{protocols}} deeply rely on zero-copy techniques.|$|E
5000|$|MS-SMBD http://msdn.microsoft.com/en-us/library/hh536346.aspx SMB2 Remote <b>Direct</b> <b>Memory</b> <b>Access</b> (RDMA) Transport Protocol Specification ...|$|E
5000|$|... the SMB Direct Protocol (SMB over remote <b>direct</b> <b>memory</b> <b>access</b> (RDMA)) ...|$|E
40|$|Abstract. AProVE is {{a system}} for {{automatic}} termination and complex-ity proofs of C, Java, Haskell, Prolog, and term rewrite systems. The particular strength of AProVE when analyzing C is its capability to rea-son about pointer arithmetic combined with <b>direct</b> <b>memory</b> <b>accesses</b> (as, e. g., in standard implementations of string algorithms). As a prerequisite for termination, AProVE also proves memory safety of C programs. 1 Verification Approach and Software Architecture To analyze programs with explicit pointer arithmetic, one has to handle the interplay between addresses and the values they point to. AProVE uses an ap-proach based on symbolic execution and abstraction to transform the input program into a symbolic execution graph that over-approximates all possible program runs. Language-specific features (such as pointer arithmetic in C) are handled when generating this graph. The nodes of the symbolic execution graph are abstract states that represent sets of actual program states, and paths in the graph correspond to evaluations in the program. To keep the graph finite, w...|$|R
40|$|Abstract. Partitioned Global Address Space (PGAS) {{languages}} {{offer an}} attrac-tive, high-productivity programming model for programming large-scale paral-lel machines. PGAS languages, such as Unified Parallel C (UPC), combine {{the simplicity of}} shared-memory programming with {{the efficiency of the}} message-passing paradigm by allowing users control over the data layout. PGAS lan-guages distinguish between private, shared-local, and shared-remote <b>memory,</b> with shared-remote <b>accesses</b> typically much more expensive than shared-local and private accesses, especially on distributed memory machines where shared-remote access implies communication over a network. In this paper we present a simple extension to the UPC language that allows the programmer to block shared arrays in multiple dimensions. We claim that this extension allows for better control of locality, and therefore performance, in the language. We describe an analysis that allows the compiler to distinguish between local shared array accesses and remote shared array accesses. Local shared array ac-cesses are then transformed into <b>direct</b> <b>memory</b> <b>accesses</b> by the compiler, saving the overhead of a locality check at runtime. We present results to show that local-ity analysis is able to significantly reduce the number of shared accesses. ...|$|R
40|$|ACM, 2007. This is the author's {{version of}} the work. It is posted here by {{permission}} of ACM for your personal use. Not for redistribution. The definitive version was published in EMSOFT 2007. International audienceThe soundness of device drivers generally cannot be verified in isolation, but has {{to take into account}} the reactions of the hardware devices. In critical embedded systems, interfaces often were simple "volatile" variables, and the interface specification typically a list of bounds on these variables. Some newer systems use "intelligent" controllers that handle dynamic worklists in shared <b>memory</b> and perform <b>direct</b> <b>memory</b> <b>accesses,</b> all asynchronously from the main processor. Thus, it is impossible to truly verify the device driver without taking the intelligent device into account, because incorrect programming of the device can lead to dire consequences, such as memory zones being erased. We have successfully verified a device driver extracted from a critical industrial system, asynchronously combined with a model for a USB OHCI controller. This paper studies this case, as well as introduces a model and analysis techniques for this asynchronous composition...|$|R
50|$|The MC68340 is a {{high-performance}} 32-bit microprocessor with <b>direct</b> <b>memory</b> <b>access</b> (DMA).|$|E
5000|$|Data {{processing}} {{other than}} the CPU, such as <b>direct</b> <b>memory</b> <b>access</b> (DMA) ...|$|E
5000|$|<b>Direct</b> <b>memory</b> <b>access</b> (DMA) circuitry, {{interrupt}} controller, time-slot assigner and {{baud rate}} generators.|$|E
40|$|Partitioned Global Address Space (PGAS) {{languages}} {{offer an}} attractive, high-productivity programming model for programming large-scale parallel machines. PGAS languages, such as Unified Parallel C (UPC), combine {{the simplicity of}} shared-memory programming with {{the efficiency of the}} message-passing paradigm by allowing users control over the data layout. PGAS languages distinguish between private, shared-local, and shared-remote <b>memory,</b> with shared-remote <b>accesses</b> typically much more expensive than shared-local and private accesses, especially on distributed memory machines where shared-remote access implies communication over a network. In this paper we present a simple extension to the UPC language that allows the programmer to block shared arrays in multiple dimensions. We claim that this extension allows for better control of locality, and therefore performance, in the language. We describe an analysis that allows the compiler to distinguish between local shared array accesses and remote shared array accesses. Local shared array accesses are then transformed into <b>direct</b> <b>memory</b> <b>accesses</b> by the compiler, saving the overhead of a locality check at runtime. We present results to show that locality analysis is able to significantly reduce the number of shared accesses. Peer ReviewedPostprint (published version...|$|R
40|$|NestStep is a {{parallel}} programming language for the BSP (bulk-synchronous parallel) programming model. In this paper {{we describe the}} concept of distributed shared arrays in NestStep and its implementation on top of MPI. In particular, we describe a novel method for runtime scheduling of irregular, direct remote accesses to sections of distributed shared arrays. Our method, which is fully parallelized, uses conventional two-sided message passing and thus avoids the overhead of a standard implementation of <b>direct</b> remote <b>memory</b> <b>access</b> based on onesided communication. The main prerequisite is that the given program is structured in a BSP-compliant way...|$|R
5000|$|The {{computation}} {{and communication}} actions {{do not have}} to be ordered in time. Communication typically takes the form of the one-sided put and get <b>Direct</b> Remote <b>Memory</b> <b>Access</b> (DRMA) calls, rather than paired two-sided send and receive message passing calls.The barrier synchronization concludes the superstep: it ensures that all one-sided communications are properly concluded. Systems based on two-sided communication include this synchronisation cost implicitly for every message sent. The method for barrier synchronisation relies on the hardware facility of the BSP computer. In Valiant's original paper, this facility periodically checks if the end of the current superstep is reached globally. The period of this check is denoted by [...]|$|R
