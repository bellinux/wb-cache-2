386|6396|Public
50|$|Writing is the {{operation}} of storing a <b>data</b> <b>pattern</b> into the memory using a particularaddress pattern. During a write, the input to the memory consists of an address pattern and a <b>data</b> <b>pattern.</b> The address pattern is used to select hard memory locations whose hard addresses are within a certain cutoff distance from the address pattern. The <b>data</b> <b>pattern</b> is stored into each of the selected locations.|$|E
50|$|Reading is the {{operation}} of retrieving a <b>data</b> <b>pattern</b> from the memory using a particular address pattern. During a read, an address pattern is used to select {{a certain number of}} hard memory locations (just like during a write). The contents of the selected locations are bitwise summed and thresholded to derive an M-bit <b>data</b> <b>pattern.</b> This serves as the output read from the memory.|$|E
5000|$|IBM TrueNorth, a neuromorphic {{processor}} {{aimed at}} similar sensor <b>data</b> <b>pattern</b> recognition and intelligence tasks, including video/audio.|$|E
5000|$|... a {{contents}} portion that is M-bits {{wide and}} that can accumulate multiple M-bit <b>data</b> <b>patterns</b> written into the location. The contents' portion is not fixed; it is modified by <b>data</b> <b>patterns</b> written into the memory.|$|R
5000|$|Filter: defines packet <b>data</b> <b>patterns</b> of {{interest}} e.g. MAC address or TCP port ...|$|R
40|$|The {{aggressive}} {{scaling down}} of flash memories has threatened data reliability since the scaling down of cell sizes {{gives rise to}} more serious degradation mechanisms such as cell-to-cell interference and lateral charge spreading. The effect of these mechanisms has pattern dependency and some <b>data</b> <b>patterns</b> are more vulnerable than other ones. In this paper, we will categorize <b>data</b> <b>patterns</b> taking into account degradation mechanisms and pattern dependency. In addition, we propose several modulation coding schemes to improve the data reliability by transforming original vulnerable <b>data</b> <b>patterns</b> into more robust ones. Comment: 7 pages, 9 figures, Proc. IEEE International Conference on Computing, Networking and Communications (ICNC), Jan. 201...|$|R
50|$|XPath is the {{standard}} mechanism {{used to refer to}} nodes and data items within XML. It has similarities to standard techniques for navigating directory hierarchies used in operating systems user interfaces. To data and structure mine XML data of any form, at least two extensions are required to conventional data mining. These are the ability to associate an XPath statement with any <b>data</b> <b>pattern</b> and sub statements with each data node in the <b>data</b> <b>pattern,</b> and the ability to mine the presence and count of any node or set of nodes within the document.|$|E
5000|$|The Qt {{framework}} {{uses the}} private class <b>data</b> <b>pattern</b> in its shared libraries. The classes that implement the pattern include a [...] "d-pointer" [...] {{to the data}} class. Methods are provided for manipulating member variables in the data class, allowing changes without breaking binary compatibility.|$|E
5000|$|A {{logic signal}} {{generator}} or <b>data</b> <b>pattern</b> generator or digital pattern generator produces logic signals - that is logical 1s and 0s {{in the form}} of conventional voltage levels. The usual voltage standards are: LVTTL, LVCMOS.It is different from a [...] "pulse/pattern generator", which refers to signal generators able to generate logic pulses with different analog characteristics (such as pulse rise/fall time, high level length, ...).|$|E
50|$|<b>Data</b> <b>Patterns</b> Pvt Ltd, India {{has also}} {{established}} a comfortable lead with its counterpart internationally.|$|R
40|$|Abstract. <b>Data</b> {{modeling}} <b>patterns</b> is {{an emerging}} {{field of research}} in the data modeling area. Its aims are to create a body of knowledge to help understand data modeling problems better and to create better data models. Current <b>data</b> modeling <b>patterns</b> are generally discussed at the instance level (only applicable in a specific domain, e. g. a business situation) and with an Entity-Relationship Modeling (ERM) way of thinking. This paper discusses <b>data</b> modeling <b>patterns</b> using the expressive power of Fully Communication Oriented Information Modeling (FCO-IM), a Dutch fact oriented modeling (FOM) method. We also consider more abstract higher level <b>data</b> <b>patterns</b> – meta level patterns – and de-scribe a few basic meta level <b>data</b> modeling <b>patterns</b> in brief as well as a meta level pattern in content versioning. ...|$|R
30|$|Spectral {{methods are}} a great option for air quality data imputation, which should be {{considered}} especially when the missing <b>data</b> <b>patterns</b> are unknown.|$|R
5000|$|Fair is {{the first}} FinTech company to utilize Artificial Intelligence (A.I.) to {{generate}} predicative car values and depreciations. By harnessing the power of big <b>data,</b> <b>pattern</b> learnings and predictive analytics the current and future value of a vehicle shifts from an art to a science. A.I. {{is also used to}} inform how much a consumer can afford on a monthly basis. This enables Fair to underwrite the contract and manage the financial terms.|$|E
50|$|The 1571 uses {{a saddle}} {{canceler}} when reading the data stream. A correction signal is generated when the raw <b>data</b> <b>pattern</b> on the disk {{consists of two}} consecutive zeros. With the GCR recording format a problem occurs in the read signal waveform. The worst case pattern 1001 may cause a saddle condition where a false data bit may occur. The original 1541 drives uses a one-shot to correct the condition. The 1571 uses a gate array to corrected this digitally.|$|E
50|$|Non-random {{distribution}} of locations: Although the storage locations are initially distributed randomly in the binary N address space, the final {{distribution of}} locations {{depends upon the}} input patterns presented, and may be non-random thus allowing better flexibility and generalization. The <b>data</b> <b>pattern</b> is first stored at locations which lie closest to the input address. The signal (i.e. <b>data</b> <b>pattern)</b> then spreads throughout the memory, and {{a small percentage of}} the signal strength (e.g. 5%) is lost at each subsequent location encountered. Distributing the signal in this way removes the need for a select read/write radius, one of the problematic features of the original SDM. All locations selected in a write operation do not now receive a copy of the original binary pattern with equal strength. Instead they receive a copy of the pattern weighted with a real value from 1.0->0.05 to store in real valued counters (rather than binary counters in Kanerva's SDM). This rewards the nearest locations with a greater signal strength, and uses the natural architecture of the SDM to attenuate the signal strength. Similarly in reading from the memory, output from the nearest locations is given a greater weight than from more distant locations.The new signal method allows the total signal strength received by a location to be used as a measure of the fitness of a location and is flexible to varying input (as the loss factor does not have to be changed for input patterns of different lengths).|$|E
50|$|Industrial <b>data</b> <b>patterns</b> can {{be highly}} {{transient}} and interpreting them requires domain expertise, which can hardly be harnessed by merely mining numeric data.|$|R
40|$|International audiencePrivacy-preserving {{data mining}} often focuses on data {{alteration}} but may bias <b>data</b> <b>patterns</b> interpretation {{and does not}} offer different levels of access to patterns according to their use. This paper addresses data mining as a prediction tool and proposes to offer several levels of access to <b>data</b> <b>patterns</b> according to users' trustworthiness. The grounding intuition is that patterns' predictive value depends on their precision that should thus vary according to their use. The following problem is considered: a medical data holder wants to disclose <b>data</b> or <b>data</b> <b>patterns</b> and still control {{the meaning of the}} disclosed patterns or of the patterns that may be mined out of the released dataset. To tackle this issue, we propose a framework compliant with existing data mining techniques by modeling trust in terms of data precision and generalising data according to standard medical classifications...|$|R
40|$|Abstract. This paper {{addresses}} {{the problem of}} efficient information theoretic, non-parametric data clustering. We develop a procedure for adapting the cluster memberships of the <b>data</b> <b>patterns,</b> {{in order to maximize}} the recent Cauchy-Schwarz (CS) probability density function (pdf) distance measure. Each pdf corresponds to a cluster. The CS distance is estimated analytically and non-parametrically by means of the Parzen window technique for density estimation. The resulting form of the cost function makes it possible to develop an efficient adaption procedure based on constrained gradient descent, using stochastic approximation of the gradients. The computational complexity of the algorithm is O(MN), M ≪ N, where N is the total number of <b>data</b> <b>patterns</b> and M is the number of <b>data</b> <b>patterns</b> used in the stochastic approximation. We show that the new algorithm is capable of performing well on several odd-shaped and irregular data sets. ...|$|R
50|$|Laser {{scanners}} {{work the}} same way as pen type readers except that they use a laser beam as the light source and typically employ either a reciprocating mirror or a rotating prism to scan the laser beam {{back and forth across the}} bar code. As with the pen type reader, a photo-diode is used to measure the intensity of the light reflected back from the bar code. In both pen readers and laser scanners, the light emitted by the reader is rapidly varied in brightness with a <b>data</b> <b>pattern</b> and the photo-diode receive circuitry is designed to detect only signals with the same modulated pattern.|$|E
5000|$|Every {{load cell}} {{is subject to}} [...] "ringing" [...] when {{subjected}} to abrupt load changes. This stems from the spring-like behavior of load cells. In order to measure the loads, they have to deform. As such, a load cell of finite stiffness must have spring-like behavior, exhibiting vibrations at its natural frequency. An oscillating <b>data</b> <b>pattern</b> can {{be the result of}} ringing. Ringing can be suppressed in a limited fashion by passive means. Alternatively, a control system can use an actuator to actively damp out the ringing of a load cell. This method offers better performance at a cost of significant increase in complexity.|$|E
5000|$|By {{modeling}} the data-dependent noise as a finite-order Markov process, the optimum MLSE for channels with ISI has been derived. In particular, {{it when the}} data-dependent noise is conditionally Gauss-Markov, the branch metrics can be computed from the conditional second-order statistics of the noise process. In other words, the optimum MLSE can be implemented efficiently by using the Viterbi algorithm, in which the branch-metric computation involves data-dependent noise prediction. Because the predictor coefficients and prediction error both depend on the local <b>data</b> <b>pattern,</b> the resulting structure {{has been called a}} data-dependent NPML detector. [...] Reduced-state sequence detection schemes can be applied to data-dependent NPML, reducing implementation complexity.|$|E
30|$|Data mining {{techniques}} {{can be used}} to discover Frequent Workload Patterns (FWPs) according to the previous history of resource usages [110]. The resource allocations can be determined by using the Association Rules Technique (ART) according to the prediction of resource availability in a given time period. The idea of using ART is on the discovered <b>data</b> <b>patterns</b> is to find out the possibility that the same patterns will repeat in future. In other words, ART {{can be used to}} represent the correlation between <b>data</b> <b>patterns.</b>|$|R
40|$|This paper {{presents}} a novel circuit for parallel bit-level maximum/niinimum selection. The selection {{is based on}} a labelupdating scheme which sequentially scans a set of <b>data</b> <b>patterns</b> from MSB to LSB and generates corresponding labels. The complet? circuit realizing this scheme consists of a set of updating units and a global OR unit, where each updating unit is compased of only a few basic gat,es. Due to structure modularity, the developed circuit provides a very cost-effect,ivP hardware solution for comparing large volumes of <b>data</b> <b>patterns</b> as thosed required in digital and video signal processing. ...|$|R
40|$|JavaScript is {{breaking}} {{ground with the}} wave of new client-side frameworks. However, there are some key differences between some of them. One major distinction is the <b>data</b> flow <b>pattern</b> they applying. As of now, there are two predominant patterns used on client side frameworks, the Two-way <b>data</b> flow <b>pattern</b> and the Unidirectional <b>data</b> flow <b>pattern.</b> In this research, an empirical experiment was conducted to test the <b>data</b> flow <b>patterns</b> impact on maintainability. The scope of maintainability {{of this research is}} defined by a set of metrics: Amount of lines code, an amount of files and amount of dependencies. By analyzing the results, a conclusion could not be made to prove that the <b>data</b> flow <b>patterns</b> does affect maintainability, using this research method. ...|$|R
50|$|Most clock {{recovery}} circuits designed for SONET OC-192 and 64b/66b are specified to tolerate an 80-bit run length. Such a run cannot occur in 64b/66b because transitions are guaranteed at 66 bit intervals, {{and in fact}} long runs are very unlikely. Although it is theoretically possible for a random <b>data</b> <b>pattern</b> to align with the scrambler state and produce a long run of 65 zeroes or 65 ones, the probability of such an event is equal to flipping a fair coin and having it {{come up in the}} same state 64 times in a row. At 10 Gigabits per second, the expected event rate of a 66-bit block with a 65 bit run-length, assuming random data, is 66x264/(2x1010) seconds, or about once every 1900 years.|$|E
50|$|PBIST was {{originally}} adopted by large memory chips that have high pin counts and operate at high frequencies, thereby exceeding {{the capability of}} production testers.The purpose of PBIST is to avoid developing and buying more sophisticated and very expensive testers. The interface between PBIST, which is internal to the processor, and the external tester environment is through the standard JTAG TAP controller pins. Algorithms and controls are fed into the chip through the TAP controller’s Test Data Input (TDI) pin. The final result of the PBIST test is read out through the Test Data Output (TDO) pin. PBIST supports the entire algorithmic memory testing requirements imposed by the production testing methodology. In order to support all of the required test algorithms, PBIST must {{have the capability to}} store the required programs locally in the device. It must also be able to perform different address generation schemes, different test <b>data</b> <b>pattern</b> generation, looping schemes, and data comparisons.|$|E
5000|$|Analysis of {{the results}} and the panel {{statements}} by higher education policy thinktank Wonkhe noted that the University of Nottingham, which had a positive flag for highly-skilled employment and a negative flag for student satisfaction, was awarded gold, [...] "the presumption that a negative flag would rule out Gold having been overturned by the panel, perhaps because the TEF guidance also steered the panel away from over-reliance on NSS scores." [...] Similarly, the University of Bristol overcame two negative flags - both in NSS-related categories - to be awarded silver, but the University of Liverpool, with {{the same number of}} negative flags, received bronze, [...] "perhaps because one was not in an NSS-derived category". Wonkhe further noted that [...] "it seems perverse that an institution - in Bristol’s case - which was ‘notably’ below benchmark should receive a higher outcome than Liverpool for which the statement is softer" [...] and that [...] "for institutions with a similar <b>data</b> <b>pattern</b> to Bristol’s, such as Southampton (with two negative flags in the same categories, but which wasn’t upgraded to Silver) there could be some well-deserved anger. And if you look to Durham, with its one positive flag, and no negatives, it only has a Silver result when compared to Nottingham’s Gold." ...|$|E
40|$|Program disturb, read disturb, and {{retention}} time noise {{are identified as}} three major contributors to multilevel cell (MLC) NAND flash memory bit errors. With program/erase cycling and technology scaling, bit error rate (BER) of MLC NAND flash memory rapidly increases. Previous works revealed that BER is heavily dependent on <b>data</b> <b>patterns.</b> Based on this observation, we propose data-pattern-aware (DPA) error protection technique to extend the lifespan of NAND flash-based storage systems. DPA manipulates the ratios of 0 's and 1 's in the stored data to reduce the probability of the <b>data</b> <b>patterns,</b> which are susceptible to device noises. By minimizing the vulnerable <b>data</b> <b>patterns,</b> our scheme can effectively reduce the BER and improves the system endurance. Our DPA scheme also incorporates a data management scheme to minimize the redundancy-induced performance overhead. Simulation results show that our scheme can increase flash system life expectancy by up to 4 x. Department of Computing 2016 - 2017 > Academic research: refereed > Publication in refereed journalbcr...|$|R
30|$|The {{fundamental}} {{problem with the}} notion that data come before theory is simple: We think that patterns are unusual and therefore meaningful; in Big <b>Data,</b> <b>patterns</b> are inevitable and therefore meaningless.|$|R
30|$|The main {{operational}} instrument of large ensemble analysis of <b>data</b> <b>patterns</b> requires that their consistency is established. Finding consistent diabetes patterns helps to manage heterogeneity [21] and associated data complexities [22].|$|R
50|$|This {{algorithm}} {{was first}} proposed by Dimitri Bertsekas in 1979. Detailed analysis and extensions to more general network optimization problems (ε-relaxation in 1986, and network auction in 1992) {{are provided in}} his network optimization books Linear Network Optimization 1991, and Network Optimization: Continuous and Discrete Models 1998. The auction algorithm has excellent computational complexity, as given in these books, and is reputed {{to be among the}} fastest for solving single commodity network optimization problems. In addition, the original version of this algorithm is known to possess a distributed nature particularly suitable for distributed systems, since its basic computational primitives (bidding and auctioning) are localized rather than relying on queries of global information. However, the original version that is intrinsically distributable has a pseudo-polynomial time complexity, which means that the running time depends on the input <b>data</b> <b>pattern.</b> Later versions have improved the time complexity to the state-of-the-art level by using techniques such as ε-scaling (also discussed in the original 1979 paper) but at the sacrifice of undermining its distributed characteristics. In order to retain the distributed nature and also attain a polynomial time complexity, recently some researchers from the multi-agent community have been trying to improve the earlier version of the auction algorithm by switching to a different economic model, namely, from the selfish bidders' perspective to a merchant’s point of view, where the merchant of a market adjusts the article prices in order to quickly clear the inventory.|$|E
30|$|Case I: pre-declared {{embedded}} data pattern: The first case study uses a pre-declared embedded <b>data</b> <b>pattern</b> with 8 -bit data. The proposed controller tests the basic read/write operation for a Cannon 16 MB SD card using this <b>data</b> <b>pattern.</b> The same 8 -bit <b>data</b> <b>pattern</b> {{has been written}} repeatedly to the SD card for testing the single and multiblock data write operation. Later on, the same <b>data</b> <b>pattern</b> has been retrieved to validate the single as well as multiblock data read operation from the same SD card.|$|E
3000|$|The leakage {{of power}} {{spectrum}} density (PSD) for the TS-OFDM signal would occur due to the repetition of the same <b>data</b> <b>pattern</b> of TS signal over one frame and also due to the discontinuities {{at both ends of}} TS signal in the time domain. To reduce the leakage of PSD due to the repetition of the same <b>data</b> <b>pattern</b> of TS signal, this paper employs a different <b>data</b> <b>pattern</b> of TS signal for each data symbol over one frame as shown in Fig. 2 [...]...|$|E
50|$|A more {{elaborated}} curve {{that uses}} times series analysis can often reveal surprising historical and current <b>data</b> <b>patterns.</b> The qualitative trend analysis {{is one of}} the most demanding and creative methods in Futures Studies.|$|R
5000|$|Compression - Relies on <b>data</b> <b>patterns</b> {{that can}} be {{represented}} more efficiently. Essentially compression techniques similar to ZIP, RAR, ARJ etc. are applied on-the-fly to data passing through hardware (or virtual machine) based WAN acceleration appliances.|$|R
40|$|Modern {{manufacturing}} plants {{rely heavily on}} the use of automation. Automated facilities use sensors to measure material state and react to <b>data</b> <b>patterns,</b> which correspond to physical events. Many patterns can be predefined either by careful analysis or from domain experts. Instances of these patterns can then be discovered through techniques such as pattern recognition. However, this approach will fail to detect events that have not been predefined, potentially causing expensive production errors. A solution to this dilemma, novelty detection, allows for the identification of interesting <b>data</b> <b>patterns</b> embedded in otherwise normal data. This paper describes several novelty detection methods for time series data that have been proposed in the literature. ...|$|R
