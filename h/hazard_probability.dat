61|166|Public
50|$|The SSA {{includes}} quantitative FMEA, {{which is}} summarized into FMES. Normally FMES probabilities {{are used in}} quantitative FTA {{to demonstrate that the}} <b>hazard</b> <b>probability</b> limits are in fact met. Cutset analysis of the fault trees demonstrates that no single failure condition will result in a hazardous or catastrophic event. The SSA may include the results of all safety analysis and be one document or may be many documents. An FTA is only one method for performing the SSA. Other methods include dependence diagram or reliability block diagram and Markov Analysis.|$|E
5000|$|Risk, {{vulnerability}} and hazard {{are the three}} factors or elements which we are considering here in this pseudo equation. Another definition of risk given by Factor analysis of information risk which {{may be related to}} disaster is 'the probable frequency and probable magnitude of future losses. Again this definition focuses on the probability of future loss whereby degree of vulnerability to hazard represents the level of risk on a particular population, built environment or environment. The relationship between severity of environmental <b>hazard,</b> <b>probability</b> and risk. Hazard severity will obviously vary it is necessary to outline threats posed by hazard. These are: ...|$|E
50|$|The PSSA {{may contain}} {{qualitative}} FTA, {{which can be}} used to identify systems requiring redundancy so that catastrophic events do not result from a single failure (or dual failure where one is latent). A fault tree is prepared for each SFHA hazard rated hazardous or catastrophic. Fault trees may be performed for major hazards if warranted. DALs and specific safety design requirements are imposed on the subsystems. The safety design requirements are captured and traced. These may include preventive or mitigation strategies selected for particular subsystems. The PSSA and CCA generate separation requirements to identify and eliminate common mode failures. Subsystem failure rate budgets are assigned so that <b>hazard</b> <b>probability</b> limits can be met.|$|E
40|$|In this paper, we derive the [beta]-entropy for Pareto-type {{and related}} distributions. Further, the [beta]-entropy for some {{weighted}} versions of these distributions, such as order statistics, proportional hazards, proportional reversed <b>hazards,</b> <b>probability</b> weighted moments, upper record and lower record, is obtained. Renyi entropy Pareto distributions Weighted distributions Burr distributions Order statistics...|$|R
40|$|The {{objectives}} of the planetary quarantine program are presented {{and the history of}} early contamination prevention efforts is outlined. Contamination models which were previously established are given and include: determination of parameters; symbol nomenclature; and calculations of contamination and <b>hazard</b> <b>probabilities.</b> Planetary quarantine is discussed as an issue of national and international concern. Information on international treaty and meetings on spacecraft sterilization, quarantine standards, and policies is provided. The specific contamination probabilities of the U. S. S. R. Venus 3 flyby are included...|$|R
40|$|We are {{concerned}} with micro-generation, individual households generating electricity using a renewable energy technology. We focus on modelling the adoption probability of photo-voltaic solar panels by a household. Using data collected from an area of Canada where a generous feed-in tariff is available to households generating electricity from solar panels, we measure household level preferences for panels and use these preferences along with household characteristics to predict adoption time intentions. We use recent developments in measuring household level preferences for innovations via discrete choice experiments and establish a causal link between the attributes of the technology and adoption time intentions using discrete time survival mixture analysis. Significant preferences included lower cost, greater energy savings and lower fossil fuel inflation. Estimation of <b>hazard</b> <b>probabilities</b> showed that the significant preferences had intuitively reasonable effects. The <b>hazard</b> <b>probabilities</b> allow us to compute cumulative probability of adoption over a ten year period per household. Technology awareness has {{a significant effect on}} the adoption probability, reinforcing the need for effective education. Our approach indicates the level of heterogeneity in preferences, particularly high for investment criteria and CO 2 emissions. These findings suggest that education campaigns should explain more about investment criteria, feed-in tariffs and environmental effects...|$|R
50|$|Syangja {{district}} {{has been described}} {{as one of the most}} disaster prone areas in western Nepal. The geological and climatic condition have resulted in extremes of landslides, debris flow, flooding and wild fires. Most of the disasters have been natural and in the recent years, there is some evidence of increase of flooding, land sliding, deforestation and land degradation. The district ranked second highest, along with Makawanpur, with the total number of landslide events occurred during the period 1971-2000 being 46. It also ranked highest in terms of landslide density and loss of property compared to other districts. Landslides at Dhanubase (Dhanubaseko pahiro) is a major vulnerable hotspot in Waling area. Waling falls under the very high <b>hazard</b> <b>probability</b> area according to the Syangja district disaster management plan.|$|E
5000|$|... "Perception of Risk Posed by Extreme Events" [...] - In this {{publication}} Slovic discusses what research says about people’s perceived risk when associated with extreme events. The way people think action {{should take place}} is based on their perceptions. These perceptions can vary from people’s status, background, education, biology, etc. The different perceptions decide how risky a choice of action is in extreme events over another. Risk perceptions are connected between emotions and reason, which creates rational behavior. Slovic explains what risk actually is. He says it is a <b>hazard,</b> <b>probability,</b> it has consequences, and threat. Since it has so many subjective meanings tied to it, it often causes communication failure. Risk perceptions are studied in three major ways: axiomatic measurement paradigm, socio-cultural paradigm, and psychometric paradigm. The axiomatic measurement looks at how people view consequences of a risky choice and how it might impact their lives. The Socio-cultural looks at the “effect of group and culture level variable on risk perception”. Psychometric paradigm looks at how people react emotionally to a risky situation that “affects judgments of the riskiness of physical, environmental and material risk”. When public officials overreact to a new/unknown danger it is likely because they overestimate its true danger or for reassurance for the public.|$|E
40|$|To {{evaluate}} {{the frequency and}} distribution of landslides hazards over Japan, this study uses a probabilistic model based on multiple logistic regression analysis. Study particular concerns several important physical parameters such as hydraulic parameters, geographical parameters and the geological parameters which {{are considered to be}} influential in the occurrence of landslides. Sensitivity analysis confirmed that hydrological parameter (hydraulic gradient) is the most influential factor in the occurrence of landslides. Therefore, the hydraulic gradient is used as the main hydraulic parameter; dynamic factor which includes the effect of heavy rainfall and their return period. Using the constructed spatial data-sets, a multiple logistic regression model is applied and landslide <b>hazard</b> <b>probability</b> maps are produced showing the spatial-temporal distribution of landslide <b>hazard</b> <b>probability</b> over Japan. To represent the landslide hazard in different temporal scales, extreme precipitation in 5 years, 30 years, and 100 years return periods are used for the evaluation. The results show that the highest landslide <b>hazard</b> <b>probability</b> exists in the mountain ranges on the western side of Japan (Japan Sea side), including the Hida and Kiso, Iide and the Asahi mountainous range, the south side of Chugoku mountainous range, the south side of Kyusu mountainous and the Dewa mountainous range and the Hokuriku region. The developed landslide <b>hazard</b> <b>probability</b> maps in this study will assist authorities, policy makers and decision makers, who are responsible for infrastructural planning and development, as they can identify landslide-susceptible areas and thus decrease landslide damage through proper preparation...|$|E
40|$|Abstract. To deliver complex {{functionalities}} in a {{cost effective}} manner, embedded software should ideally be developed with standardized interoperable components. At the same time, {{most of these}} embedded systems must be demonstrably safe and reliable. This paper aims to extend SaveCCM, a modelling language for component-based embedded systems, with standard safety evaluation models. Based on this extension, failure and <b>hazard</b> <b>probabilities</b> can be estimated early in the development process {{and can be used}} to check if a system can fulfil its safety requirements. The procedure of the safety evaluation is demonstrated with the case study of a computer assisted braking system. ...|$|R
40|$|Abstract. The bridges are of much {{importance}} to the transportation network system, as they connect key highways at crucial nodes. This paper reviews and discussed risk-based methodologies to assess the risk acceptability and cost-effectiveness of protective measures for bridge under multiple manmade or natural hazards. Three risk acceptance criteria based on fatality risks, failure probabilities and net benefit assessment were described and discussed. The decision support framework accompanying these risk acceptance criteria considers <b>hazard</b> <b>probabilities,</b> value of human life, physical and indirect damages, risk reduction and protective measure costs were studied and an illustrative example was presented...|$|R
40|$|To deliver complex {{functionalities}} in a {{cost effective}} manner, embedded software should ideally be developed with standardized interoperable components. At the same time, {{most of these}} embedded systems must be demonstrably safe and reliable. This paper aims to extend SaveCCM, a modelling language for component-based embedded systems, with standard safety evaluation models. Based on this extension, failure and <b>hazard</b> <b>probabilities</b> can be estimated early in the development process {{and can be used}} to check if a system can fulfil its safety requirements. The procedure of the safety evaluation is demonstrated with the case study of a computer assisted braking system...|$|R
40|$|Being able to forcast extreme {{volatility}} is {{a central}} issue in financial risk management. We present a large volatility predicting method based {{on the distribution of}} recurrence intervals between volatilities exceeding a certain threshold Q for a fixed expected recurrence time τ_Q. We find that the recurrence intervals are well approximated by the q-exponential distribution for all stocks and all τ_Q values. Thus a analytical formula for determining the <b>hazard</b> <b>probability</b> W(Δ t |t) that a volatility above Q will occur within a short interval Δ t if the last volatility exceeding Q happened t periods ago can be directly derived from the q-exponential distribution, which is found to be in good agreement with the empirical <b>hazard</b> <b>probability</b> from real stock data. Using these results, we adopt a decision-making algorithm for triggering the alarm of the occurrence of the next volatility above Q based on the <b>hazard</b> <b>probability.</b> Using a "receiver operator characteristic" (ROC) analysis, we find that this predicting method efficiently forecasts the occurrance of large volatility events in real stock data. Our analysis may help us better understand reoccurring large volatilities and more accurately quantify financial risks in stock markets. Comment: 13 pages and 7 figure...|$|E
40|$|This {{study is}} {{pertaining}} to {{an evaluation of}} landslide occurrence on natural terrain due to snowmelt in Japan, using a probabilistic model based on multiple logistic regression analysis. The evaluation concerns several physical parameters such as hydraulic parameters, geographical parameters and geological parameters which {{are considered to be}} influential in the occurrence of landslides. A Snow Water Equivalent model (SWE) is utilized to estimate snowmelt and associated infiltration in light, heavy and normal snow years. Using the constructed spatial data-sets, we apply a multiple logistic regression model to produce landslide susceptibility maps showing the spatial–temporal distribution of landslide hazard probabilities throughout Japan using 1 km × 1 km resolution grid cells. The results have revealed that, over 95 % landslide <b>hazard</b> <b>probability</b> exists in the mountain ranges on the western side of Japan (the Japan Sea side). In particular, this study is dealing with the Aizu region of Fukushima prefecture in order to verifying the landslide <b>hazard</b> <b>probability.</b> Verification proved that, the areas identified as high risk areas (having over 90 % landslide <b>hazard</b> <b>probability</b> in numerical modeling) show 87 % agreement with observed landslides in the Aizu region. Also we evaluated the relationship between landslides and snow melting process giving special concern to change of temperature in the spring...|$|E
40|$|Abstract. According to the {{features}} of amusement rides and the definition, the quantitative model of risk assessment for amusement rides was build based on <b>hazard</b> <b>probability</b> and hazardous consequence calculated with economical loss. According to the structure features that the amusement rides are composed with mechanical, electric control, hydraulic and other systems, the failure probability of electric control system was calculated with unit failure probability and simplified bathtub curve; and the failure probability of mechanical system is calculated according to the fatigue damage accumulation theory. The <b>hazard</b> <b>probability</b> is modified by comprehensive correction factor to consider the influence on the risk performance of amusement rides caused by the safety measures, redundancy units, strengthening process and management methods. The loss rate, direct economic loss and indirect economic loss are {{used to calculate the}} quantitative hazardous consequence. The steps of risk assessment based on the quantitative model were introduced...|$|E
50|$|Scientists {{and governments}} try to {{identify}} areas with {{a high risk of}} lahars based on historical events and computer models. Volcano scientists {{play a critical role in}} effective hazard education by informing officials and the public about realistic <b>hazard</b> <b>probabilities</b> and scenarios (including potential magnitude, timing, and impacts); by helping evaluate the effectiveness of proposed risk-reduction strategies; by helping promote acceptance of (and confidence in) hazards information through participatory engagement with officials and vulnerable communities as partners in risk reduction efforts; and by communicating with emergency managers during extreme events. An example of such a model is TITAN2D. These models are directed towards future planning: identifying low-risk regions to place community buildings, discovering how to mitigate lahars with dams, and constructing evacuation plans.|$|R
40|$|Housing {{allowances}} in Norway {{are meant}} to enable low-income households to increase, or maintain, their housing consumption. In order to identify low-income households and to ensure a desirable vertical equity, the allowances decrease as income rises. The marginal withdrawal of housing allowances as income increases is quite high. Interacting with the tax system this might potentially produce a dependency culture and a depressing effect on the supply of labour. A dependency culture would yield long spells as recipients of housing allowances. This paper demonstrates {{that this is not}} the case for single parents and families with children. Around 30 per cent of the receivers from these groups drop out of the system every year. Furthermore the <b>hazards</b> (<b>probability</b> of dropping out) seem to be independent of the length of the claim. Housing allowances, dependency culture, Norway,...|$|R
40|$|Background. Whether routine {{antifungal}} prophylaxis decreases posttransplantation {{fungal infections}} in patients receiving orthotopic liver transplantation (OLT) remains unclear. This study aimed {{to determine the}} effectiveness of antifungal prophylaxis for patients receiving OLT. Patients and Methods. This is a retrospective analysis of a database at Chang Gung Memorial Hospital. We have been administering routine antibiotic and prophylactic antifungal regimens to recipients with high model for end-stage liver disease scores (> 20) since 2009. After propensity score matching, 402 patients were enrolled. We conducted a multistate model to analyze the cumulative <b>hazards,</b> <b>probability</b> of fungal infections, and risk factors. Results. The cumulative <b>hazards</b> and transition <b>probability</b> of “transplantation to fungal infection” were lower in the prophylaxis group. The incidence rate of fungal infection after OLT decreased from 18. 9 % to 11. 4 % (p= 0. 052); overall mortality improved from 40. 8 % to 23. 4 % (p< 0. 001). In the “transplantation to fungal infection” transition, prophylaxis {{was significantly associated with}} reduced hazards for fungal infection (hazard ratio: 0. 57, 95 % confidence interval: 0. 34 – 0. 96, p= 0. 033). Massive ascites, cadaver transplantation, and older age were significantly associated with higher risks for mortality. Conclusion. Prophylactic antifungal regimens in high-risk recipients might decrease the incidence of posttransplant fungal infections...|$|R
40|$|Long-term {{survivors}} in trials with survival endpoints are subjects {{who will not}} experience the event of interest. Membership {{in the class of}} long-term survivors is unobserved and should be inferred from the data by means of a mixture model. An important question is how large the sample size should be to come to accurate conclusions with respect to the effect of treatment. This question is studied for trials with survival endpoints in discrete time by means of a simulation study. Various combinations of sample size, <b>hazard</b> <b>probability,</b> and probability of being a long-term survivor are studied. The results show that <b>hazard</b> <b>probability</b> has a large effect on the accuracy of the treatment effect estimate. A sample size of 200 might be sufficient for large hazard probabilities, whereas large sample sizes of 5, 000 cases should be considered for small hazard probabilities. © 2014 Copyright Taylor and Francis Group, LLC...|$|E
40|$|In visual line transecting, {{whales are}} {{observed}} when surfacing to breath. Surfacings are short. Point process models for surfacing times are {{combined with a}} model for the <b>hazard</b> <b>probability</b> of initial sighting to obtain a likelihood for the observed encounters. Point processes are also useful for describing the clustering of encounters along the track line due to spatial clustering of whales. ...|$|E
40|$|Being able {{to predict}} the {{occurrence}} of extreme returns is important in financial risk management. Using the distribution of recurrence intervals [...] -the waiting time between consecutive extremes [...] -we show that these extreme returns are predictable on the short term. Examining a range {{of different types of}} returns and thresholds we find that recurrence intervals follow a q-exponential distribution, which we then use to theoretically derive the <b>hazard</b> <b>probability</b> W(Δ t |t). Maximizing the usefulness of extreme forecasts to define an optimized hazard threshold, we indicates a financial extreme occurring within the next day when the <b>hazard</b> <b>probability</b> is greater than the optimized threshold. Both in-sample tests and out-of-sample predictions indicate that these forecasts are more accurate than a benchmark that ignores the predictive signals. This recurrence interval finding deepens our understanding of reoccurring extreme returns and can be applied to forecast extremes in risk management. Comment: 18 pages, 5 figues, 3 table...|$|E
40|$|This paper {{presents}} a simplified model for calculating the <b>probabilities</b> of smoke <b>hazard</b> in multi-storey buildings. This model first establishes the fire {{development in a}} compartment on any floor, using the results from a one zone fire growth model, and then computes the smoke movement in the building, yielding the temperature and concentration of toxic gases at any location in the building. At a critical time, which {{is defined as the}} time when the conditions in the stairs leading to an exit are lethal, such that occupants are trapped and cannot evacuate, the smoke movement calculations are terminated and the <b>probability</b> of smoke <b>hazard</b> are computed based on the temperature and the concentration of toxic gases in the building. The procedure used for the calculation of smoke movement, the critical time and the smoke <b>hazard</b> <b>probabilities</b> is described in this paper. Results are presented, as an example, for a twenty-five storey apartment building. Reprinted in NRCC- 39021 Peer reviewed: YesNRC publication: Ye...|$|R
40|$|Survival {{analysis}} {{is becoming increasingly}} important in the prevention and social sciences literature. When using survival analysis methods in social science and prevention applications {{it is important to}} identify the type of right censoring occurring in the data, so that the appropriate risk set can be used in estimation of the hazard function. In this article we review the two most widely used approaches to estimating the hazard function and the type of right censoring these methods assume. A type of right censoring that occurs frequently in social science studies involving prospective longitudinal panel designs is retro-censoring which occurs when a subject's response is necessarily lost in the interval in which the subject is censored. When retro-censoring occurs, the naive use of popular methods from biostatistics and other fields to estimate <b>hazard</b> <b>probabilities</b> will lead to biased estimators and inflated Type I error rates. However, once identified, retro-censoring is easily ac [...] ...|$|R
40|$|The Sultanate of Oman {{is among}} the Indian Ocean {{countries}} that were subjected to at least two confirmed tsunamis during the twentieth and twenty-first centuries: the 1945 tsunami due to an earthquake in the Makran subduction zone in the Sea of Oman (near-regional field tsunami) and the Indian-Ocean tsunami in 2004, caused by an earthquake from the Andaman Sumatra subduction zone (far - field tsunami). In this paper, we present a probabilistic tsunami hazard assessment for the entire coast of Oman from tectonic sources generated along the Makran subduction zone. The tsunami hazard is assessed {{taking into account the}} contribution of small-and large-event magnitudes. Results of the earthquake recurrence rate studies and the tsunami numerical modeling for different magnitudes were used through a logic-tree to estimate the tsunami <b>hazard</b> <b>probabilities.</b> We derive <b>probability</b> <b>hazard</b> exceedance maps for the Omani coast considering the exposure times of 100, 250, 500, and 1000 years. The hazard maps consist of computing the likelihood that tsunami waves exceed a specific amplitude. We find that the probability that a maximum wave amplitude exceeds 1 m somewhere along the coast of Oman reaches, respectively, 0. 7 and 0. 85 for 100 and 250 exposure times, and it is up to 1 for 500 and 1000 years of exposure times. These probability values decrease significantly toward the southern coast of Oman where the tsunami impact, from the earthquakes generated at Makran subduction zone, is low. info:eu-repo/semantics/publishedVersio...|$|R
40|$|We {{present a}} simple method of {{probabilistic}} risk analysis for ecosystems. The only requirements are time series — modelled or measured — of environment and ecosystem variables. Risk {{is defined as}} the product of <b>hazard</b> <b>probability</b> and ecosystem vulnerability. Vulnerability is the expected difference in ecosystem performance between years with and without hazardous conditions. We show an application to drought risk for net primary productivity of coniferous forests across Europe, for both recent and future climatic conditions...|$|E
40|$|Abstract—The {{continuous}} {{growth of}} air traffic {{leads to an}} escalating number of accidents and incidents on surface movement, demanding additional safety mechanisms. The Advanced Surface Movement Guidance and Control Systems (A-SMGCS) allow continuous surveillance, monitoring and guidance of surface assets, greatly reducing the <b>hazard</b> <b>probability.</b> The development and demonstration of an EGNOS based A-SMGCS is {{the objective of the}} AIRNET project. The AIRNET project addresses all A-SMGCS components, from the human-machine interface to the underlying network. This paper focuses the underlying network architecture, which seamlessly integrates disparate wireless technologies like VDL- 4...|$|E
40|$|In {{the rural}} Tropics, the {{participatory}} risk assessment, based on local knowledge only, is very widespread. This practice {{is appropriate for}} hazard identification and for raising the awareness of local communities {{in relation to the}} importance of risk reduction, but it is still imprecise in determining risk level, ranking and treatment in a context of climate change, activities in which technical knowledge is unavoidable. Integration of local and technical-scientific knowledge within the framework of an encoded risk assessment method (ISO 31010), could favour more effective decision making with regard to risk reduction. The aim of this chapter is to verify the applicability of a multi-risk local assessment-MLA which combines local knowledge (participatory workshop, transect walk, hazard and resource mapping, disaster historical profile) and scientific knowledge (climate downscaling modelling, <b>hazard</b> <b>probability</b> and scenarios, potential damages, residual risk). The test is carried out in two villages of the Western Niger, particu-larly exposed to flooding and agricultural drought. The risk (<b>hazard</b> <b>probability</b> * potential damages) is identified, analysed (level of risk) and evaluated (residual risk, adaptation measures compared with potential damage costs). The MLA is feasible. The two villages, while bordering on one another, have a different risk ranking. Depending on the village, the risk treatment could reduce the risk level to 17 % and to 41 % of the current risk, with costs equating to 34 % and 28 % of the respective potential damages...|$|E
30|$|In {{order to}} perform the hazard impact simulation, we first divide the map of China into 20  ×  25 grids. Grids that are {{completely}} within the Chinese territory are assigned a size value of 1. Grids {{on the border of}} the Chinese territory, depending on how much of the grid is within China, take a size value between 0 and 1. The total area size of the Chinese territory is derived by summing up these size values. The <b>hazard</b> occurrence <b>probability</b> for each grid is determined based on the spatial distribution of random hazards, rainstorms, and sandstorms. The <b>hazard</b> occurrence <b>probability</b> of random <b>hazards</b> is the same for all grids. For any specific percentage of hazard covered area, we can calculate the number of grids needed to make up such a percentage, and then randomly choose the appropriate number of grids as the hazard covered areas. Finally, we calculate the percentage of effectively impacted nodes by hazards through counting the number of airports in the hazard covered grids.|$|R
40|$|The {{receiver}} {{operating characteristic}} (ROC) curve is a tool of particular use in disease status classification with a continuous medical test (marker). A variety of statistical regression models have been proposed for the comparison of ROC curves for different markers across covariate groups. A full parametric modeling of the marker distribution has been generally found to be overly reliant on the strong parametric assumptions. Pepe (2003) has instead developed parametric models for the ROC curve that induce a semi-parametric model for the marker distributions. The estimating equations proposed for use in these ROC-GLM models may differ from commonly used estimating equations in those same probability models. In this paper, we investigate {{the analysis of the}} power ROC curve when based on the parametric exponential model and the broader semi-parametric proportional <b>hazards</b> <b>probability</b> model. In the case of the latter, we consider estimating equations derived from the usual partial likelihood methods in time-to-event analyses and the ROC-GLM approach of Pepe, et al. In exploring the robustness of these ROC analysis approaches to violations of the distributional assumptions, we find that the ROC-GLM estimating equation provides an extra measure of robustness when compared to the Cox proportional hazards estimating equation...|$|R
40|$|Spatial {{econometrics}} {{and analytical}} spatial economic modeling advanced {{significantly in the}} recent years. Yet, methodologically {{they are designed to}} tackle marginal changes in the underlying dynamics of spatial urban systems. In the world with climate change, however, abrupt sudden non-marginal changes in economic system are expected. This is especially relevant for urban development in coastal and delta areas where the <b>probabilities</b> of natural <b>hazards</b> such as catastrophic floods and hurricanes increase dramatically with climate change. New information about risks and micro-level interactions among economic agents alters individual location choices and impacts urban land markets dynamics potentially leading to the emergence of critical transitions from the bottom-up. We address this gap by incorporating adaptive expectations about land market dynamics into a spatial agent-based model of a coastal city. We build upon the previous research on agent-based modeling of urban land markets, and make a step forward towards empirical modeling by using actual hedonic study and spatial data for a coastal town in North Carolina, USA. Decentralized urban market with adaptive expectations about property prices in the areas with increasing <b>hazard</b> <b>probabilities,</b> may experience abrupt changes that shift the trends of spatial development and pricing...|$|R
30|$|The Sendai Framework {{promotes}} a people-centered approach {{and the use}} of a participatory process in decision making that responds to the needs of users and is sensitive to social and cultural aspects, gender, and age. Working Group 2 (UNISDR 2016 c) addressed ways to promote a common understanding of exposure 25 and vulnerability 26 as key dimensions of risk alongside <b>hazard</b> <b>probability.</b> The severity of the impacts of a disaster depends strongly on the level of exposure and vulnerability (Terry and Goff 2012) in the affected area. Evidence indicates that overall risk has increased worldwide, largely due to increases in the exposure of persons and assets and possibly increases in inequality, which is a shaper of vulnerability, thus calling for greater attention to these dimensions of risk.|$|E
40|$|This paper {{describes}} a Weather Impact Model (WIM) capable of serving {{a variety of}} predictive applications ranging from real-time operation and day-ahead operation planning, to asset and outage management. The proposed model is capable of combining various weather parameters into different weather impact features of interest to a specific application. This work focuses {{on the development of}} a universal weather impacts model based on the logistic regression embedded in a Geographic Information System (GIS). It is capable of merging massive data sets from historical outage and weather data, to real-time weather forecast and network monitoring measurements, into a feature known as weather <b>hazard</b> <b>probability.</b> The examples of the outage and asset management applications are used to illustrate the model capabilities...|$|E
40|$|Artificial {{neural network}} (ANN) theory is {{emerging}} {{as an alternative to}} conventional statistical methods in modeling nonlinear functions. The popular Cox proportional hazard model falls short in modeling survival data with nonlinear behaviors. ANN is a good alternative to the Cox PH as the proportionality of the hazard assumption and model relaxations are not required. In addition, ANN possesses a powerful capability of handling complex nonlinear relations within the risk factors associated with survival time. In this study, we present a comprehensive comparison of two different approaches of utilizing ANN in modeling smooth conditional <b>hazard</b> <b>probability</b> function. We use real melanoma cancer data to illustrate the usefulness of the proposed ANN methods. We report some significant results in comparing the survival time of male and female melanoma patients...|$|E
5000|$|A {{main task}} of AC 25.1309 - 1 {{is to provide}} {{standard}} definitions of terms (including <b>hazard</b> and <b>probability</b> classifications) for consistent use throughout the framework {{set up for the}} accomplishment of functional airplane safety. Where regulations (FAR) and standards (ARP) may use such terms as failure condition, and extremely improbable, AC 25.1309 - 1 defines their specific meanings. [...] In this respect, AC 25.1309 - 1 is comparable to ISO 26262 - 1 Vocabulary, at least in regard to the relative dependent standards. Key definitions include: ...|$|R
40|$|In {{competing}} risks analysis, the primary interest of researchers is {{the estimation of}} the net survival probability (NSP) if a cause of failure could be eliminated from a population. The Kaplan-Meier product-limit estimator {{under the assumption that}} the eliminated risk is non-informative to the other remaining risks, has been widely used in the estimation of the NSP. The assumption implies that the hazard of the remaining risks before and after the elimination are equal and it could be biased. This paper addressed this possible bias by proposing a non-parametric multistate approach that accounts for an informative eliminated risk in the estimation procedure, whereby the <b>hazard</b> <b>probabilities</b> of the remaining risks before and after the elimination of a risk are not assumed to be equal. When a non-informative eliminated risk was assumed, it was shown that the proposed multistate estimator reduces to the Kaplan-Meier estimator. For illustration purposes, the proposed procedure was implemented on a published dataset and the change in hazard after elimination of a cause is investigated. Comparing the results to those obtained from using the Kaplan-Meier method, it was found that in the presence of (both constant and non-constant) informative eliminated risk, the proposed multistate approach was more sensitive and flexible...|$|R
40|$|This paper {{illustrates}} {{the use of}} statistical prediction model for early identification of low employability Malaysian graduates using the proportional hazard model. The relative predicted <b>hazard</b> rate or <b>probability</b> of exit from unemployment is used to proxy the graduate's employability. The out-of-sample evaluation shows that the statistical prediction model predicts correctly 83...|$|R
