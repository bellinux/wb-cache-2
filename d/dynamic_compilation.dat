191|37|Public
50|$|Runtime environments using <b>dynamic</b> <b>compilation</b> {{typically}} have programs run slowly {{for the first}} few minutes, and then after that, most of the compilation and recompilation is done and it runs quickly. Due to this initial performance lag, <b>dynamic</b> <b>compilation</b> is undesirable in certain cases. In most implementations of <b>dynamic</b> <b>compilation,</b> some optimizations that could be done at the initial compile time are delayed until further compilation at run-time, causing further unnecessary slowdowns. Just-in-time compilation is a form of <b>dynamic</b> <b>compilation.</b>|$|E
50|$|<b>Dynamic</b> <b>compilation</b> is {{a process}} used by some {{programming}} language implementations to gain performance during program execution. Although the technique originated in the Self programming language, the best-known language that uses this technique is Java. Since the machine code emitted by a dynamic compiler is constructed and optimized at program runtime, the use of <b>dynamic</b> <b>compilation</b> enables optimizations for efficiency not available to compiled programs except through code duplication or metaprogramming.|$|E
50|$|The {{combination}} of <b>dynamic</b> <b>compilation</b> and a rich system interface makes Racket a capable scripting language, similar to Perl or Python.|$|E
50|$|Objectively {{comparing}} {{the performance of}} a Java program and an equivalent one written in another language such as C++ needs a carefully and thoughtfully constructed benchmark which compares programs completing identical tasks. The target platform of Java's bytecode compiler is the Java platform, and the bytecode is either interpreted or compiled into machine code by the JVM. Other compilers almost always target a specific hardware and software platform, producing machine code that will stay virtually unchanged during execution. Very different and hard-to-compare scenarios arise from these two different approaches: static vs. <b>dynamic</b> <b>compilations</b> and recompilations, the availability of precise information about the runtime environment and others.|$|R
50|$|PyMC3 is a Python {{package for}} Bayesian {{statistical}} modeling and probabilistic machine learning {{which focuses on}} advanced Markov chain Monte Carlo and variational fitting algorithms. PyMC3 relies on Theano for automatic differentiation and also for computation optimization and <b>dynamic</b> C <b>compilation.</b> PyMC3, together with STAN, {{are the most popular}} probabilistic programming tools. PyMC3 is an open source project, developed by the community and fiscally sponsored by NumFocus.|$|R
50|$|Now-discontinued CHIP is the {{original}} board, mostly targeting hobbyists. The system is built around the SoC processor R8 from AllWinner as its core, which integrates an ARM CortexTM-A8 CPU (based on ARM architecture V7-A) and peripherals, such as Graphic Engine, UART, SPI, USB port, CIR, CMOS Sensor Interface and LCD controller. The CPU is also accompanied with NEON SIMD coprocessor and has RCT JAVA-Accelerations to optimize just-in-time (JIT) and <b>dynamic</b> adaptive <b>compilation</b> (DAC).|$|R
5000|$|Best paper {{award at}} MICRO-38 {{for the paper}} titled A <b>Dynamic</b> <b>Compilation</b> Framework for Controlling Microprocessor Energy and Performance in 2005 ...|$|E
5000|$|<b>Dynamic</b> <b>compilation</b> enables an {{additional}} layer of optimization at run-time (i.e. {{for a specific}} parameters set the application was provided with) ...|$|E
5000|$|Microsoft's Common Intermediate Language is an {{intermediate}} language {{designed to be}} shared by all compilers for the [...]NET Framework, before static or <b>dynamic</b> <b>compilation</b> to machine code.|$|E
25|$|The Burroughs/Unisys APLB {{interpreter}} (1982) was {{the first}} to use <b>dynamic</b> incremental <b>compilation</b> to produce code for an APL-specific virtual machine. It recompiled on-the-fly as identifiers changed their functional meanings. In addition to removing parsing and some error checking from the main execution path, such compilation also streamlines the repeated entry and exit of user-defined functional operands. This avoids the stack setup and take-down for function calls made by APL's built-in operators such as Reduce and Each.|$|R
40|$|This paper {{reports on}} work {{undertaken}} {{to create a}} 3 D image, which displays as many indices as possible (thematic attributes) on a map. A new method has been developed for displaying the information. It resembles the classical method of the block-diagrams (prism maps), but it has more advantages and the efficiency is much greater. In addiltion, this method can be used simultaneously with anamorphosing. The technique of 3 D anamorphoses (cartograms) compilation is outlined. Accordingly, algorithms for <b>dynamic</b> anamorphoses <b>compilation</b> for multi-dimensional images and multi-dynamic animations are discussed...|$|R
5000|$|Their {{most recent}} project [...] "Better Days" [...] is a <b>dynamic</b> reggae <b>compilation</b> the two created while {{traveling}} in Jamaica in 2004. Featuring {{artists such as}} Turbulence. Norris Man, and Milton Blake, [...] "Better Days" [...] is set to be independently released in May 2011. Sabbo- Sabbo is a DJ and producer from Tel Aviv, Israel. His unique style has {{brought him to the}} forefront of the DJ and blog world. Producing a sound that combines heavy dancehall influences with a mix of bass and global grooves, this DJ has a sound unlike any other.|$|R
50|$|The Java {{virtual machine}} (JVM) loads the class files and either interprets the {{bytecode}} or just-in-time compiles it to machine code and then possibly optimizes it using <b>dynamic</b> <b>compilation.</b>|$|E
5000|$|Infonomics.Today is an {{automatically}} curated, <b>dynamic</b> <b>compilation</b> and Twitter feed on {{the emerging}} field of Infonomics, big data, data science and related topics developed by Craig Johnson in 2015 ...|$|E
50|$|This {{allows the}} {{developer}} {{the freedom of}} working with compiled code {{without the need for}} the traditional compile-link-run cycle. This is like a specialized form of incremental or <b>dynamic</b> <b>compilation.</b>|$|E
40|$|The YNVM is a {{response}} to the rigidity inherent in current programming languages and environments, which present significant barriers to the construction of dynamically reconfigurable systems. It provides extreme late binding of system/language features, unconstrained application involvement in the management and use of meta-data, maximally open reflective features, and elevates concrete implementation (compiled code) to first-class status by providing <b>dynamic</b> (re) <b>compilation</b> of any part of a live system without loss of eciency. It is compatible with existing code bases, and applications built on top of (or next to, or under) it can be made highly reconfigurable with minumum effort and negligible overheads...|$|R
40|$|JavaSpaces {{provides}} a simple yet expressive mechanism for distributed computing with commodity technology. We discuss {{the suitability of}} JavaSpaces for implementing different classes of concurrent computations based on low-level metrics (null messaging and array I/O), and present performance results for several parametric algorithms. We found that although inefficient for communication intensive problems, JavaSpaces yields good speedups for parametric experiments, relative to both sequential Java and C. We also outline a <b>dynamic</b> native <b>compilation</b> technique, which for short, compute-intensive codes further boosts performance without compromising Java portability or extensive algorithm recoding. Discussion and empirical results are presented {{in the context of}} our public benchmark suite. ...|$|R
40|$|International audienceThe YNVM is a {{response}} to the rigidity inherent in current programming languages and environments, which present significant barriers to the construction of dynamically reconfigurable systems. It provides extreme late binding of system/language features, unconstrained application involvement in the management and use of meta-data, maximally open reflective features, and elevates concrete implementation (compiled code) to first-class status by providing <b>dynamic</b> (re) <b>compilation</b> of any part of a live system without loss of efficiency. It is compatible with existing code bases, and applications built on top of (or next to, or under) it can be made highly reconfigurable with minumum effort and negligible overheads...|$|R
50|$|Compilation error {{refers to}} a state when a {{compiler}} fails to compile a piece of computer program source code, either due to errors in the code, or, more unusually, due to errors in the compiler itself. A compilation error message often helps programmers debugging the source code for possible errors. Although the exact definitions of compilation and interpretation are a bit vague, generally compilation errors are only referring to static compilation and not <b>dynamic</b> <b>compilation.</b> However, {{it is important to}} note that <b>dynamic</b> <b>compilation</b> can still technically have compilation errors, although many programmers and sources may identify them as run-time errors. Most just-in-time compilers, such as the Javascript V8 engine, will ambiguously refer to them as Syntax Errors since they check for them at run-time.|$|E
50|$|Support for {{the latest}} ANSI/ISO C++ {{language}} specifications, including a host of compiler enhancements including <b>Dynamic</b> <b>Compilation</b> and Adaptive Compiler Technology (ACT), which radically speed compiler build processes; full ANSI/ISO template implementation; full ANSI/ISO STL (standard template library) support; and a high-performance 32-bit ANSI C++ native code compiler.|$|E
5000|$|The project notably {{includes}} self-hosting {{versions of}} the C# and VB.NET compilers - compilers written in the languages themselves. The compilers are available via the traditional command-line programs but also as APIs available natively from within [...]NET code. Roslyn exposes modules for syntactic (lexical) analysis of code, semantic analysis, <b>dynamic</b> <b>compilation</b> to CIL, and code emission.|$|E
5000|$|James George [...] "Jim" [...] Mitchell (born 25 April 1943) is a Canadian {{computer}} scientist. He {{has worked}} on programming language design and implementation (FORTRAN WATFOR, Mesa, Euclid, C++, Java), interactive programming systems, <b>dynamic</b> interpretation and <b>compilation,</b> document preparation systems, user interface design, distributed transactional file systems, and distributed, object-oriented operating systems. He has also worked {{on the design of}} hardware for computer graphics, high-level language execution, and audio input/output.|$|R
40|$|An {{integrated}} {{platform for}} fast genetic operators {{is presented to}} support intrinsic evolution on Xilinx Virtex II Pro Field Programmable Gate Arrays (FPGAs). <b>Dynamic</b> bitstream <b>compilation</b> is achieved by directly manipulating the bitstream using a layered design. Experimental results on a case study have shown that a full design {{as well as a}} full repair is achievable using this platform with an average time of 0. 4 microseconds to perform the genetic mutation, 0. 7 microseconds to perform the genetic crossover, and 5. 6 milliseconds for one input pattern intrinsic evaluation. This represents a performance advantage of three orders of magnitude over JBITS and more than seven orders of magnitude over the Xilinx design tool driven flow for realizing intrinsic genetic operators on a Virtex II Pro device. 1...|$|R
5000|$|In computing, {{just-in-time}} (JIT) compilation, {{also known}} as <b>dynamic</b> translation, is <b>compilation</b> done during execution of a program - at run time - rather than prior to execution. Most often this consists of translation to machine code, which is then executed directly, but can also refer to translation to another format. A system implementing a JIT compiler typically continuously analyses the code being executed and identifies parts of the code where the speedup gained from compilation would outweigh the overhead of compiling that code.|$|R
50|$|A common {{implementation}} of JIT compilation is to first have AOT compilation to bytecode (virtual machine code), known as bytecode compilation, {{and then have}} JIT compilation to machine code (<b>dynamic</b> <b>compilation),</b> rather than interpretation of the bytecode. This improves the runtime performance compared to interpretation, {{at the cost of}} lag due to compilation. JIT compilers translate continuously, as with interpreters, but caching of compiled code minimizes lag on future execution of the same code during a given run. Since only part of the program is compiled, there is significantly less lag than if the entire program were compiled prior to execution.|$|E
50|$|Graph {{coloring}} allocators produce efficient code, {{but their}} allocation time is high. In cases of static compilation, allocation {{time is not}} a significant concern. In cases of <b>dynamic</b> <b>compilation,</b> such as just-in-time (JIT) compilers, fast register allocation is important. An efficient technique proposed by Poletto and Sarkar is linear scan allocation. This technique requires only a single pass over the list of variable live ranges. Ranges with short lifetimes are assigned to registers, whereas those with long lifetimes tend to be spilled, or reside in memory. The results are on average only 12% less efficient than graph coloring allocators.|$|E
5000|$|POP-2, often {{referred}} to as POP2 is a discontinued programming language developed around 1970 from the earlier language POP-1 (developed by Robin Popplestone in 1968, originally named COWSEL) by Robin Popplestone and Rod Burstall at the University of Edinburgh. [...] It drew roots from many sources: the languages LISP and ALGOL 60, and theoretical ideas from Peter J. Landin. It used an incremental compiler, which gave it some of the flexibility of an interpreted language, including allowing new function definitions at run time and modification of function definitions while a program was running (both of which are features of <b>dynamic</b> <b>compilation),</b> without the overhead of an interpreted language.|$|E
40|$|Compilation of the {{programming}} language Id Nouveau into machine code for the MIT tagged-token dataflow architecture is thoroughly described. Id Nouveau is a higher-order functional language augmented {{with a novel}} data structure facility known as I-Structures. The tagged-token dataflow architecture is a dataflow computer of the <b>dynamic</b> variety. <b>Compilation</b> takes place in two steps. In the first step, the Id Nouveau program is converted into an abstract dataflow graph called a program graph. Program graphs embody no detailed knowledge of the target architecture, yet have a very precise operational semantics. At the same time, they represent data and control flow in a way very convenient for program transformation. Several common optimizing transformations are discussed. The second step of compilation converts the program graph into machine code for the tagged-token architecture, {{taking into account the}} machine's finite resources. Peephole optimizations for machine code are discussed, and a general-purpose optimization algorithm is given...|$|R
40|$|Language-integrated meta-programming and {{extensible}} compilation {{have been}} recurring themes of programming languages since {{the invention of}} LISP. A recent real-world application of these techniques {{is the use of}} small meta-programs to specify database queries, as used in the Microsoft LINQ extensions for. NET. It is important that. NET languages such as F # are able to leverage the functionality provided by LINQ and related components for heterogeneous execution, both for pragmatic reasons and as a first step toward applying more disciplined, formal approaches to these problems. This paper explores the use of a modest metaprogramming extension to F # to access and leverage the functionality of LINQ and other components. We do this by demonstrating an implementation of language integrated SQL queries using the LINQ/SQLMetal libraries. We also sketch two other applications: the execution of data-parallel quoted F # programs on a GPU via the Accelerator libraries, and <b>dynamic</b> native-code <b>compilation</b> via LINQ...|$|R
40|$|A {{prominent}} {{software security}} violation-buffer overflow attack has taken various forms and poses serious threats until today. One such vulnerability is return-oriented programming attack. An return-oriented programming attack circumvents the dynamic execution prevention, which is employed in modern operating systems to prevent execution of data segments, {{and attempts to}} execute unintended instructions by overwriting the stack exploiting the buffer overflow vulnerability. Numerous defense mechanisms have been proposed {{in the past few}} years to mitigate/prevent the attack â€“ compile time methods that add checking logic to the program code before <b>compilation,</b> <b>dynamic</b> methods that monitor the control-flow integrity during execution and randomization methods that aim at randomizing instruction locations. This paper discusses (i) these different static, dynamic, and randomization techniques proposed recently and (ii) compares the techniques based on their effectiveness and performances...|$|R
50|$|A {{completely}} COLA-based computer system, whilst {{capable of}} implementing the operating system, libraries, applications and other levels {{of a traditional}} computer system, allows these distinctions to blur or disappear if the end-user wishes. Every aspect of the computer system, since it is written in a COLA (including the COLA itself), can be overridden, mutated, bypassed, etc. just as the local datastructures and functions in a traditional program can. There is also flexibility in how code is run, since there is a choice of interpreting, static compilation, <b>dynamic</b> <b>compilation,</b> in fact if the COLA is given a suitable backend object then it can even reprogram FPGA's to run arbitrary sections of the system.|$|E
50|$|The LLVM project {{started in}} 2000 at the University of Illinois at Urbana-Champaign, under the {{direction}} of Vikram Adve and Chris Lattner. LLVM was originally developed as a research infrastructure to investigate <b>dynamic</b> <b>compilation</b> techniques for static and dynamic programming languages. LLVM was released under the University of Illinois/NCSA Open Source License, a permissive free software licence. In 2005, Apple Inc. hired Lattner and formed a team to work on the LLVM system for various uses within Apple's development systems. LLVM {{is an integral part of}} Apple's latest development tools for macOS and iOS. Since 2013, Sony has been using LLVM's primary front end Clang compiler in the software development kit (SDK) of its PS4 console.|$|E
50|$|JIT {{compilation}} is {{a combination}} of the two traditional approaches to translation to machine code - ahead-of-time compilation (AOT), and interpretation - and combines some advantages and drawbacks of both. Roughly, JIT compilation combines the speed of compiled code with the flexibility of interpretation, with the overhead of an interpreter and the additional overhead of compiling (not just interpreting). JIT compilation is a form of <b>dynamic</b> <b>compilation,</b> and allows adaptive optimization such as dynamic recompilation - thus in theory JIT compilation can yield faster execution than static compilation. Interpretation and JIT compilation are particularly suited for dynamic programming languages, as the runtime system can handle late-bound data types and enforce security guarantees.|$|E
40|$|A {{future is}} a {{parallel}} programming language construct that enables programmers to specify potentially asynchronous computations. We present and empirically evaluate a novel implementation of futures for Java. Our futures implementation is a JVM extension that couples estimates of future computational granularity with underlying resource availability to enable automatic and adaptive decisions of when to spawn futures in parallel or to execute them sequentially. Our system builds from, combines, and extends (i) lazy task creation and (ii) a JVM sampling infrastructure previously used solely for <b>dynamic</b> and adaptive <b>compilation.</b> We empirically evaluate our system using different benchmarks, triggers for automatic spawning of futures, processor availability, and JVM configurations. We show that our future implementation for Java is efficient and scalable for finegrained Java futures without requiring programmer intervention...|$|R
40|$|Achieving good {{performance}} in bytecoded language interpreters is difficult without sacrificing both simplicity and portability. This {{is due to}} the complexity of dynamic translation ("just-in-time compilation") of bytecodes into native code, which is the mechanism employed universally by highperformance interpreters. We demonstrate that a few simple techniques make it possible to create highly-portable dynamic translators that can attain as much as 70 % the performance of optimized C for certain numerical computations. Translators based on such techniques can offer respectable performance without sacrificing either the simplicity or portability of much slower "pure" bytecode interpreters. Keywords: bytecode interpretation, threaded code, inlining, <b>dynamic</b> translation, just-in-time <b>compilation.</b> 1 Introduction Bytecoded languages such as Smalltalk [Gol 83], Caml [Ler 97] and Java [Arn 96, Lin 97] offer significant engineering advantages over more conventional languages: higher levels of abst [...] ...|$|R
40|$|This paper {{presents}} a module {{system and a}} programming environment designed to support interactive program development in Scheme. The module system extends lexical scoping while maintaining its flavor and benefits and supports mutually recursive modules. The programming environment supports <b>dynamic</b> linking, separate <b>compilation,</b> production code compilation, and a window-based user interface with multiple read-evalprint contexts. 1. Introduction Interactive programming is an important technique for reducing program development time. An interactive programming system allows a programmer to enter a program or program fragment directly into the system and to receive the output from that program or fragment immediately, reducing the usual compile-linkexecute process conceptually to a single evaluate step. Interactive programming is also valuable for experimental programming, rapid prototyping, and debugging. Modular programming is an important programming paradigm for large-scale program de [...] ...|$|R
