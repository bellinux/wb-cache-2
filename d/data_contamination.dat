125|350|Public
50|$|In his lectures, Legates has {{acknowledged}} that humans have {{a direct impact on}} the environment. However he has disputed large scale climatological studies where he claims that researchers fail to incorporate sufficient data involving increased solar activity, water vapor as a greenhouse gas, <b>data</b> <b>contamination</b> through expansion of the urban heat island effect surrounding data collection points, and many other key variables in addition to the human chemical emissions that are the sole focus of many climatological studies.|$|E
40|$|We {{examine the}} {{sensitivity}} of poverty indices to <b>data</b> <b>contamination</b> using {{the concept of the}} influence function, and demonstrate that an important commonly used subclass of poverty measures will be robust under <b>data</b> <b>contamination.</b> This is illustrated using simulations. In this respect poverty and inequality indices have fundamentally different robustness properties. We investigate both the case where the poverty line is exogenously fixed and where it must be estimated from the data...|$|E
40|$|This work is {{motivated}} by the problem of image mis-registration in remote sensing and {{we are interested in}} determining the resulting loss in the accuracy of pattern classification. A statistical formulation is given where we propose to use <b>data</b> <b>contamination</b> to model and understand the phenomenon of image mis-registration. This model is widely applicable to many other types of errors as well, for example, measurement errors and gross errors etc. The impact of <b>data</b> <b>contamination</b> on classification is studied under a statistical learning theoretical framework. A closed-form asymptotic bound is established for the resulting loss in classification accuracy, which is less than ϵ/(1 -ϵ) for <b>data</b> <b>contamination</b> of an amount of ϵ. Our bound is sharper than similar bounds in the domain adaptation literature and, unlike such bounds, it applies to classifiers with an infinite Vapnik-Chervonekis (VC) dimension. Extensive simulations have been conducted on both synthetic and real datasets under various types of <b>data</b> <b>contamination,</b> including label flipping, feature swapping and the replacement of feature values with data generated from a random source such as a Gaussian or Cauchy distribution. Our simulation results show that the bound we derive is fairly tight. Comment: 23 pages, 10 figure...|$|E
40|$|The PDS-OSEM reconstructs PET {{images with}} iteratively {{compensating}} random and scatter corrections from prompt sinogram. The PDS-OSEM can reconstruct PET images with low count <b>data</b> and <b>data</b> <b>contaminations.</b> The PDS-OSEM provides less noise and higher quality of reconstructed images {{than those of}} OP-OSEM algorithm in statistical sense. a r t i c l e i n f o Article history...|$|R
5000|$|... #Caption: Maxey Flat {{ground water}} <b>contamination</b> <b>data</b> from 1988 ...|$|R
40|$|This paper {{presents}} some statistical methodologies {{to evaluate}} the food exposure to a contaminant and quantify {{the outcome of a}} new maximum limit on a food item. Our application deals with Ochratoxin A (OTA). We focus on the quantitative evaluation of the distribution of exposure based on both consumption <b>data</b> and <b>contamination</b> <b>data.</b> One specific aspect of <b>contamination</b> <b>data</b> is left censorship due to the limits of detection. Three calculation procedures are proposed: [P 1] a deterministic method using means of contamination; [P 2] a probabilistic method using a parametric adjustment of the distributions of contamination taking into account the left censorship; and [P 3] a non-parametric method which consists in randomly selecting the consumption <b>data</b> and the <b>contamination</b> values. Our main result shows that a non-parametric probabilistic approach is well adapted for the purpose of exposure assessment, when large samples are available. In the application to OTA, the probability to exceed a safe level is high, particularly for children. Simulations show that the impact of the existing standards on cereals and the currently proposed standards on wine generally do not significantly reduce the risk to be overexposed to OTA...|$|R
40|$|Response styles can distort survey findings. Culture-specific {{response}} styles (CSRS) {{are particularly}} problematic for researchers using multicultural samples because the resulting <b>data</b> <b>contamination</b> {{can lead to}} inaccurate conclusions about the research question under study. This article critically reviews past recommendations to correct for cultural biases in responses, and proposes a framework that enables the researcher to assess the robustness of empirical findings from CSRS. This approach also avoids the disadvantages of ignoring the problem and interpreting spurious results or choosing one single correction technique that potentially introduces new kinds of <b>data</b> <b>contamination...</b>|$|E
40|$|This {{paper is}} {{concerned}} with the robust endpoint detection and noisy speech recognition over wireless network. Firstly, the MLP-based and GMM-based endpoint detection incorporated with <b>data</b> <b>contamination</b> and continuous spectral subtraction techniques were investigated. Then, for noisy wireless speech recognition, a combined technique of <b>data</b> <b>contamination</b> and robust training was proposed to separately model the environmental characteristics and phonetic information. According to the results from an abbreviated stock name recognition task, we observe that the proposed techniques has the potential to improve robustness not only on diverse data contaminated training data, but also on the unmatched noisetype condition between training and testing environments. 1...|$|E
40|$|The paper {{discusses}} {{the effect of}} model deviations such as <b>data</b> <b>contamination</b> on the maximum likelihood estimator (MLE) for a general class of latent trait models (citeNP{MoKn: 00 }). This is done {{with the use of}} the influence function (Hampel 1968, 1974) a mathematical tool to assess the robustness properties of any statistic, such as an estimator. Simulation studies show that the MLE can be seriously biased by model deviations. Therefore, we then propose alternative robust estimators that are not less influenced by <b>data</b> <b>contamination.</b> The performance of the robust estimators in terms of bias and variance is compared to the MLE estimator both analytically and through simulation studies...|$|E
40|$|International audienceOver {{the past}} few decades, phycotoxins, {{secondary}} metabolites produced by toxic phytoplankton, have seen an increase in their frequency, concentrations, and geographic distribution. As shellfish accumulate phycotoxins making them unfit for human consumption, they are considered as an important food safety issue. Thus, a consumer exposure assessment on phycotoxins is necessary. Exposure assessment requires two types of information: <b>contamination</b> and consumption <b>data.</b> Shellfish <b>contamination</b> <b>data</b> on major toxins encountered by at-risk populations (Domoic Acid group, Okadaic Acid group, and Saxitoxin group) have been reviewed. Consumption data have been reviewed for both general and potential high-consumer populations. Then, we undertook acute and chronic exposure assessments, combining available French <b>contamination</b> <b>data</b> and our own consumption data. Studies including exposure assessment were then reviewed. Lastly, risk characterization was undertaken. It can be concluded that both acute and chronic exposure to phycotoxins via shellfish consumption {{is a matter of}} concern, mainly for high consumers identified in this review (specific populations and shellfish harvesters). However, the results for risk characterization must be improved. There is a need for (i) toxicological data to establish a Tolerable Daily Intake; (ii) an assessment of consumption and <b>contamination</b> <b>data,</b> undertaken at the same time, so as to assess exposure...|$|R
40|$|The {{family of}} {{weighted}} likelihood estimators largely overlaps with minimum divergence estimators. They are robust to <b>data</b> <b>contaminations</b> compared to MLE. We define {{the class of}} generalized weighted likelihood estimators (GWLE), provide its influence function and discuss the efficiency requirements. We introduce a new truncated cubic-inverse weight, which is both first and second order efficient and more robust than previously reported weights. We also discuss new ways of selecting the smoothing bandwidth and weighted starting values for the iterative algorithm. The advantage of the truncated cubic-inverse weight is illustrated in a simulation study of three-component normal mixtures model with large overlaps and heavy <b>contaminations.</b> A real <b>data</b> example is also provided. Finite normal mixture Generalized weighted likelihood estimator Influence function Smoothing bandwidth Truncated cubic-inverse weight Weighted starting value...|$|R
30|$|This {{analysis}} {{demonstrates that}} the HQRPLN method reproduces the underlying regression parameters for overdispersed count <b>data</b> subject to <b>contamination,</b> and also accurately identifies response outliers.|$|R
40|$|In {{conventional}} {{structural equation}} modeling (SEM), {{with the presence of}} even a tiny amount of <b>data</b> <b>contamination</b> due to outliers or influential observations, normal-theory maximum likelihood (ML-Normal) is not efficient and can be severely biased. The multivariate-t-based SEM, which recently got implemented in Mplus as an approach for mixture modeling, represents a robust estimation alternative to downweigh the impact of outliers and influential observations. To our knowledge, the use of maximum likelihood estimation with a multivariate-t model (ML-t) to handle outliers has not been shown in SEM literature. In this paper we demonstrate the use of ML-t using the classic Holzinger and Swineford (1939) data set with a few observations modified as outliers or influential observations. A simulation study is then conducted to examine the performance of fit indices and information criteria under ML-Normal and ML-t in the presence of outliers. Results showed that whereas all fit indices got worse for ML-Normal with increasing amount of outliers and influential observations, their values were relatively stable with ML-t, and the use of information criteria was effective in selecting ML-normal without <b>data</b> <b>contamination</b> and selecting ML-t with <b>data</b> <b>contamination,</b> especially when the sample size was at least 200...|$|E
40|$|Purpose – The {{purpose of}} this paper is to {{critically}} review past recommendations to correct for cultural biases in empirical survey data sets, and propose a framework that enables the researcher to assess the robustness of empirical findings from culture-specific response styles (CSRS). Design/methodology/approach – The paper proposes to analyze a set of derived data sets, including the original data as well as data corrected for response styles using theoretically plausible correction methods for the empirical data at hand. The level of agreement of results across correction methods indicates the robustness of findings to possible contamination of data by cross-cultural response styles. Findings – The proposed method can be used to inform researchers and data analysts about the extent to which the validity of their conclusions is threatened by <b>data</b> <b>contamination</b> and provides guidance regarding the results that can safely be reported. Practical implications – Response styles can distort survey findings. CSRS are particularly problematic for researchers using multicultural samples because the resulting <b>data</b> <b>contamination</b> can lead to inaccurate conclusions about the research question under study. Originality/value – The proposed approach avoids the disadvantages of ignoring the problem and interpreting spurious results or choosing one single correction technique that potentially introduces new kinds of <b>data</b> <b>contamination...</b>|$|E
40|$|Distributional {{dominance}} {{criteria are}} commonly applied to draw welfare inferences about comparisons, but conclusions drawn from empirical implementations of dominance criteria may be inßuenced by <b>data</b> <b>contamination.</b> We examine a non-parametric approach to reÞning Lorenz-type comparisons {{and apply the}} technique to two important examples from the LIS data-base...|$|E
40|$|This article proposes {{statistical}} {{tools for}} quantitative {{evaluation of the}} risk due {{to the presence of}} some particular contaminants in food. We focus on the estimation of the probability of the exposure to exceed the so-called provisional tolerable weekly intake (PTWI), when both consumption <b>data</b> and <b>contamination</b> <b>data</b> are independently available. A Monte Carlo approximation of the plug-in estimator, which may be seen as an incomplete generalized U-statistic, is investigated. We obtain the asymptotic properties of this estimator and propose several confidence intervals, based on two estimators of the asymptotic variance: (i) a bootstrap type estimator and (ii) an approximate jackknife estimator relying on the Hoeffding decomposition of the original U-statistics. As an illustration, we present an evaluation of the exposure to Ochratoxin A in France...|$|R
40|$|A {{number of}} studies on {{materials}} technologies for the Space Infrared Telescope Facility (SIRTF) intended to enhance SIRTF mission performance are reviewed. Particular attention is given to dewar support straps, dewar structural materials, infrared black coatings, optical mirror materials, cryogenic properties <b>data</b> base, <b>contamination</b> control, and thermal control materials/coatings. It is noted that the alternative to baseline materials offer potential for substantial improvements in reliability, weight, and lifetime...|$|R
30|$|In this study, we {{analyze the}} source {{location}} {{and timing of}} seismic energy release for the triggered event by employing the back-projection method (Spudich and Cranswick 1984). In the waveform <b>data,</b> <b>contaminations</b> of coda waves from the mainshock into the onset of body waves from the triggered event are found at most stations, {{making it difficult to}} identify the onset. The back-projection method determines the event location and timing by evaluating coherent signals in stacked waveform and does not use the onset data. It is anticipated that the method may provide an alternative approach to estimating the source location and timing for such event data. We also reproduce observed waveforms of the triggered event by using waveforms of smaller-sized events as Green’s functions and infer the source mechanism of the triggered event. We verify the estimated source location and mechanism by calculating synthetic waveforms. These source data investigated in this study would contribute to quantitatively studying the causes of observed strong motions, seismic activity around the triggered event, and stress transfer from the mainshock to the triggered event.|$|R
40|$|Response styles can distort survey findings. Culture-specific {{response}} styles (CSRS) {{are particularly}} problematic to cross-cultural and empirical tourism researchers using multi-cultural samples because the resulting <b>data</b> <b>contamination</b> {{can lead to}} inaccurate conclusions about the research question under study. This is particularly the case when constructs such as satisfaction are measured, which are difficult to operationalise. Nevertheless, possible culture-specific response style effects are typically ignored, thus jeopardizing the validity of reported findings. This chapter raises awareness of the problem, illustrates the problem empirically and presents a method that enables researchers to assess the robustness of empirical findings on cross-cultural differences in satisfaction to CSRS. This approach avoids the disadvantages of ignoring the problem and interpreting spurious results or choosing one single correction technique that potentially introduces new kinds of <b>data</b> <b>contamination...</b>|$|E
40|$|Distributional {{dominance}} {{criteria are}} commonly applied to draw welfare in- ferences about comparisons, but conclusions drawn from empirical imple- mentations of dominance criteria may be inßuenced by <b>data</b> <b>contamination.</b> We examine a non-parametric approach to reÞning Lorenz-type comparisons {{and apply the}} technique to two important examples from the LIS data-base...|$|E
40|$|Distributional {{dominance}} {{criteria are}} commonly applied to draw welfare inferences about comparisons, but conclusions drawn from empirical implementations of dominance criteria {{may be influenced}} by <b>data</b> <b>contamination.</b> We show the conditions under which this may occur and propose empirical methods to work round the proble using both non-parametric and parametric approaches...|$|E
40|$|In {{relation}} to the efforts to reconstruct the radiation dose in Dolon village, which was affected by the first USSR atomic bomb test in 1949 at the Semipalatinsk nuclear test site, the width and the center-axis location of the radioactive plume were investigated based on the soil <b>contamination</b> <b>data</b> around Dolon and the nearby villages. Assuming that the radioactive plume passed over along {{a straight line from}} the ground zero point to this area, the spatial distributions of soil contamination were plotted {{as a function of the}} perpendicular distance from the supposed center-axis of the plume. In total 83 and 52 soil <b>contamination</b> <b>data</b> were available for 137 Cs and 239, 240 Pu, respectively. The plotted distribution formed a peak-like shape both for 137 Cs and 239, 240 Pu. A Gaussian function drawn so as to envelop the points plotted for 239, 240 Pu indicated that the central part of the radioactive plume passed over the residential area of Dolon with a σ value of 1. 5 km. Additional soil <b>contamination</b> <b>data</b> around Dolon and other villages are necessary for more detailed discussion...|$|R
40|$|In {{statistical}} {{theory and}} practice, a certain distribution is usually assumed and then optimal solutions sought. Since deviations from an assumed distribution are very common, one cannot {{feel comfortable with}} assuming a particular distribution and believing it to be exactly correct. That brings the robustness issue in focus. In this book, we have given statistical procedures which are robust to plausible deviations from an assumed mode. The method of modified maximum likelihood estimation is used in formulating these procedures. The modified maximum likelihood estimators are explicit functions of sample observations and are easy to compute. They are asymptotically fully efficient and are as efficient as the maximum likelihood estimators for small sample sizes. The maximum likelihood estimators have computational problems and are, therefore, elusive. A broad range of topics are covered in this book. Solutions are given which are easy to implement and are efficient. The solutions are also robust to data anomalies: outliers, inliers, mixtures and <b>data</b> <b>contaminations.</b> Numerous real life applications of the methodology are given...|$|R
40|$|Semipalatinsk/Atomic bomb test/Radioactive plume/Fallout contamination/Dolon. In {{relation}} to the efforts to reconstruct the radiation dose in Dolon village, which was affected by the first USSR atomic bomb test in 1949 at the Semipalatinsk nuclear test site, the width and the center-axis location of the radioactive plume were investigated based on the soil <b>contamination</b> <b>data</b> around Dolon and the nearby villages. Assuming that the radioactive plume passed over along {{a straight line from}} the ground zero point to this area, the spatial distributions of soil contamination were plotted {{as a function of the}} per-pendicular distance from the supposed center-axis of the plume. In total 83 and 52 soil <b>contamination</b> <b>data</b> were available fo...|$|R
40|$|The paper {{deals with}} the {{property}} of asymptotic uniform linearity of residual empirical processes for AR(p) when observations contain outliers. We apply the result to construct robust GM–tests for linear hypotheses. The scheme of <b>data</b> <b>contamination</b> by additive single outliers with the intensity O(n− 1 / 2), n is data level, is considered...|$|E
40|$|<b>Data</b> <b>contamination</b> and {{excessive}} correlations between regressors (multicollinear-ity) constitute a standard and {{major problem in}} econometrics. Two techniques en-able solving these problems, in separate ways: the Gini regression for the former, and the PLS (partial least squares) regression for the latter. Gini-PLS regressions are proposed in order to treat extreme values and multicollinearity simultaneously...|$|E
40|$|We {{introduce}} {{and define}} the higher–order robustness for M–estimators. By refining the Von Mises expansion, we {{go beyond the}} first–order term (i. e. the Influence Function) and we provide the conditions needed to ensure higher–order stability of the asymptotic bias. We then define a class of admissible M–estimators, featuring second–order robustness. By construction, our estimators are re–descending and they allow for a better control of the bias, both in finite sample and/or in presence of contaminations. To derive the distribution of second–order robust M–estimators, we apply a second–order saddle–point density approximation. Under the reference model, this approximation is very precise, having a small relative error also for small to moderate sample size. In the talk, we investigate also {{the behavior of the}} saddle-point in presence of <b>data</b> <b>contamination.</b> Specifically, we decompose the error approximation due to the saddle–point in two parts: one part related to the sample size and another part related to small departures from the reference model (<b>data</b> <b>contamination).</b> We show that in presence of <b>data</b> <b>contamination,</b> second–order robust M– estimators imply a stronger stability of the saddle–point approximation w. r. t. classical and Huber– type M–estimators. Finally, we run preliminary Monte Carlo simulations, providing evidence that our estimators perform better (in terms of bias) than MLE and Huber–type M–estimators, even in moderate to small sample sizes and for a large amount of contaminations. References B. R. Clarke (1983). Uniqueness and Frechet differentiability of functionals solutions to maximum likelihood type equations. The Annals of Statistics, 11 : 1198 – 1205. B. R. Clarke (1986). Non smooth analysis and Frechet differentiability of M-functionals. Probabilit...|$|E
30|$|It {{has been}} {{generally}} agreed that effect-based identification of key toxicants as well as analysis, modelling and assessment of bioavailability and bioaccumulation are needed, along with better evaluation of monitoring <b>data</b> on <b>contamination,</b> toxicity, ecological quality and ecosystem services on a large scale. However, sound scientific concepts, models and decision support systems have {{to find their way}} to major stakeholders, environmental managers and even policy makers as their implementation would certainly contribute to the common European goal--higher level of ecosystem and human health protection.|$|R
40|$|Laser data relays {{potentially}} offer continuous 1 Gb/sec bandwidths, drastically increasing low-altitude {{satellite data}} collection capacity over present store-and-dump techniques. Availability {{of the laser}} link as a reliable alternative, operating within conventional low-altitude communication subsystem weight and power allocations, will create customer pressure for adoption. Major communication relay system impacts are discussed including reliability, mechanical design, attitude control, on-board <b>data</b> handling, <b>contamination</b> control, and traffic-net management. Interface parameters which drive the fundamental relay satellite design concepts are discussed, and conditions requiring early quantitative analysis are identified...|$|R
40|$|The {{interconnectedness}} between {{capital structure}} and firm performance {{is a topic}} of high interest among scholars and management alike. The scholars tend to unveil the why segment of the relationship, while the management looks into the how side to promote capital structure policy which can optimise the firm performance. While many studies have looked into this relationship across multiple industries and spanning across decades of data, the current study trains its lens on Malaysian public listed company companies which operate in the construction sector, and with data window between 2010 to 2014. This specific sector was chosen for their high gearing which renders firms to relatively high insolvency exposure emanating from interest rate fluctuations. The five-year timeframe was selected to isolate potential <b>data</b> <b>contaminations</b> streaming from global financial crisis which winds down in 2009. Financial data of the company were extracted from Bloomberg Terminal based on a pre-prepared list of Bloomberg tickers. A total of 225 observations were recorded in this study. Using Tobins Q {{as a proxy for}} firm performance, this study finds a mixed result where short term debts ratio indicates a significant negative effect, while long term debt ratio presents a non-significant influence. Explanations on this output are therefore discussed in this paper...|$|R
40|$|The aims of {{this study}} were (1) to {{understand}} the extent to which offering or not offering a Don’t know option has the potential of contaminating survey data, and (2) to investigate the interaction between offering a Don’t know option and the verbalisation of scale points. Results from an experimental study with 196 online panel members indicate that empirical data sets can be contaminated if Don’t know options are not offered to respondents who are unable to to assess an object under study. The maximum extent of <b>data</b> <b>contamination</b> could not be determined because only one product category was examined. But the contamination for the less known fast food restaurant under study amounted to almost 20 % of the data. Furthermore results show that using the typical Likert scale verbalisation of the middle point (“neither agree not disagree”) is often misinterpreted as a Don’t know option by respondents, thus increasing the risk of <b>data</b> <b>contamination</b> that cannot be corrected retrospectively. Practical recommendations for market researchers are derived from these results...|$|E
40|$|The {{economic}} analysis of income distribution and related topics makes {{extensive use of}} dominance criteria to draw inferences about welfare comparisons. However {{it is possible that}} - just as some inequality statistics can be very sensitive to extreme values - conclusions drawn from empirical implementations of dominance criteria may also be influenced by <b>data</b> <b>contamination.</b> We show the conditions under which this may occur and propose empirical methods to work round the problem...|$|E
40|$|High-breakdown-point {{regression}} estimators {{protect against}} large errors and <b>data</b> <b>contamination.</b> We generalize {{the concept of}} trimming used by many of these robust estimators, such as the least trimmed squares and maximum trimmed likelihood, and propose a general trimmed estimator, which renders robust estimators applicable far beyond the standard (non) linear regression models. We derive here the consistency and asymptotic distribution of the proposed general trimmed estimator under mild β-mixing conditions and demonstrate its applicability in nonlinear regression and limited dependent variable models. ...|$|E
40|$|Peer-reviewedThe {{bioinformatics}} Basic Local Alignment Search Tools (BLASTN and TBLASTN) {{were used}} to search public databases for nucleic acid (NA) and amino acid (AA) sequences identical or similar to HIV- 1 DNA and proteins. Several significant alignments were detected {{in a variety of}} non-HIV- 1 taxa and other sources deposited in public genomic and protein repositories. Homologies between a number of HIV- 1 proteins and those of Candida, Cryptococcus and Schistosoma mansoni are of uncertain significance and suggest the need for further analyses. The overwhelmingly likely cause for these <b>data</b> is <b>contamination...</b>|$|R
40|$|Contaminants {{and natural}} toxicants such as mycotoxins {{may be present}} in various food items that may be {{considered}} dangerous to human health if the cumulative intake remains above the toxicologic safe references. This intake or exposure can be estimated using both consumption surveys and analytical data that record the contamination levels of food. Analytical data often present some left censorship, that is, data below some limit of detection or quantification. This article proposes the integration of a nonparametric modeling of the left censorship of analytical data in a model aiming at giving a quantitative evaluation of the risk due {{to the presence of}} some particular contaminants in food. We focus on the estimation of the "risk," defined as the probability for exposure to exceed the so-called "provisional tolerable weekly intake" (PTWI), when both consumption <b>data</b> and <b>contamination</b> <b>data</b> are independently available. To account for the left censorship of the <b>contamination</b> <b>data</b> (due to the existence of detection/quantification limits), we propose using a Kaplan-Meier estimator instead of the empirical cumulative distribution function generally used in nonparametric procedures. We give the asymptotic behavior of our estimator and derive the asymptotic properties of the associated risk estimator. Several confidence intervals are obtained using a double-bootstrap procedure. A detailed algorithm is proposed. As an illustration, we present an evaluation of the risk exposure to ochratoxin A in France and use our risk estimator to show that children under age 10 years are a population at particular risk. Imposing some maximum limits on particular food items, namely cereals and wine, would not significantly reduce the risk...|$|R
40|$|Microbiology {{results are}} {{reported}} in semi-structured formats and have a high content of useful patient information. We developed and validated a hybrid regular expression and natural language processing solution for processing blood culture microbiology reports. Multi-center Veterans Affairs training and testing data sets were randomly extracted and manually reviewed to determine the culture and sensitivity as well as contamination results. The tool was iteratively developed for both outcomes using a training dataset, and then evaluated on the test dataset to determine antibiotic susceptibility <b>data</b> extraction and <b>contamination</b> detection performance. Our algorithm had a sensitivity of 84. 8 % and a positive predictive value of 96. 0 % for mapping the antibiotics and bacteria with appropriate sensitivity findings in the test <b>data.</b> The bacterial <b>contamination</b> detection algorithm had a sensitivity of 83. 3 % and a positive predictive value of 81. 8 %...|$|R
