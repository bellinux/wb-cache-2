10000|2062|Public
5|$|Improvements in {{structural}} alignment methods {{constitute an}} active area of research, and new or modified methods are often proposed that are claimed to offer advantages over the {{older and more}} widely distributed techniques. A recent example, TM-align, uses a novel method for weighting its distance matrix, to which standard <b>dynamic</b> <b>programming</b> is then applied. The weighting is proposed to accelerate the convergence of <b>dynamic</b> <b>programming</b> and correct for effects arising from alignment lengths. In a benchmarking study, TM-align {{has been reported to}} improve in both speed and accuracy over DALI and CE.|$|E
5|$|Many {{sequence}} alignment {{and protein}} structure alignment algorithms {{have been developed}} assuming linear data representations and as such {{are not able to}} detect circular permutations between proteins. Two examples of frequently used methods that have problems correctly aligning proteins related by circular permutation are <b>dynamic</b> <b>programming</b> and many hidden Markov models. As an alternative to these, a number of algorithms are built on top of non-linear approaches and are able to detect topology-independent similarities, or employ modifications allowing them to circumvent the limitations of <b>dynamic</b> <b>programming.</b> The table below is a collection of such methods.|$|E
5|$|Perl is {{a family}} of high-level, general-purpose, interpreted, <b>dynamic</b> <b>programming</b> {{languages}}. The languages in this family include Perl 5 and Perl 6.|$|E
40|$|International audienceDynamic {{programming}} {{is a powerful}} technique for solving optimization problems efficiently. We consider a <b>dynamic</b> <b>program</b> as simply a recursive program that is evaluated with memoization and lookup of answers. In this paper we examine how, given a function calculating a bound {{on the value of}} the <b>dynamic</b> <b>program,</b> we can optimize the compilation of the <b>dynamic</b> <b>program</b> function. We show how to automatically transform a <b>dynamic</b> <b>program</b> to a number of more efficient versions making use of the bounds function. We compare the different transformed versions on a number of example <b>dynamic</b> <b>programs,</b> and show the benefits in search space and time that can result...|$|R
40|$|We {{propose a}} <b>dynamic</b> <b>program</b> {{to find the}} {{shortest}} path in a network having gamma probability distributions as arc lengths. Two operators of sum and comparison need to be adapted for the proposed <b>dynamic</b> <b>program.</b> Convolution approach is used to sum two gamma probability distributions being employed in the <b>dynamic</b> <b>program...</b>|$|R
40|$|AbstractIn {{this paper}} we {{consider}} finite-stage stochastic <b>dynamic</b> <b>programs</b> with associative reward systems. An invariant imbedding technique yields a <b>dynamic</b> <b>program</b> with a terminal reward system on an augmented state space. According to the associativity, we clarify {{the validity of}} three possible recursive equations—one parametrized equation and two non-parametrized ones. Each of the additive, multiplicative, and multiplicative-additive <b>dynamic</b> <b>programs</b> admits the three recursive equations. Further, the corresponding composite relations among three optimum value functions hold. However, minimum, maximum, and fractional <b>dynamic</b> <b>programs</b> admit only the parametrized recursive equation. They do not admit the two non-parametrized equations. Further, the corresponding relations among the three optimum value functions do not hold...|$|R
5|$|The crucial {{property}} {{turns out}} to be treewidth, a measure of how tree-like the graph is. For a graph G of treewidth at most k and a graph H, the homomorphism problem can be solved in time |V(H)|O(k) with a standard <b>dynamic</b> <b>programming</b> approach. In fact, it is enough to assume that the core of G has treewidth at most k. This holds even if the core is not known.|$|E
5|$|A {{direct method}} for {{producing}} an MSA uses the <b>dynamic</b> <b>programming</b> technique {{to identify the}} globally optimal alignment solution. For proteins, this method usually involves two sets of parameters: a gap penalty and a substitution matrix assigning scores or probabilities to the alignment of each possible pair of amino acids based on the similarity of the amino acids' chemical properties and the evolutionary probability of the mutation. For nucleotide sequences a similar gap penalty is used, but a much simpler substitution matrix, wherein only identical matches and mismatches are considered, is typical. The scores in the substitution matrix may be either all positive or a mix {{of positive and negative}} {{in the case of a}} global alignment, but must be both positive and negative, in the case of a local alignment.|$|E
5|$|The {{most widely}} used {{approach}} to multiple sequence alignments uses a heuristic search known as progressive technique (also known as the hierarchical or tree method) developed by Paulien Hogeweg and Ben Hesper in 1984. Progressive alignment builds up a final MSA by combining pairwise alignments beginning with the most similar pair and progressing to the most distantly related. All progressive alignment methods require two stages: a first stage in which {{the relationships between the}} sequences are represented as a tree, called a guide tree, and a second step in which the MSA is built by adding the sequences sequentially to the growing MSA according to the guide tree. The initial guide tree is determined by an efficient clustering method such as neighbor-joining or UPGMA, and may use distances {{based on the number of}} identical two letter sub-sequences (as in FASTA rather than a <b>dynamic</b> <b>programming</b> alignment).|$|E
40|$|This {{annotated}} bibliography reviews current research in <b>dynamic</b> and interactive <b>program</b> steering. In particular, we review systems-related research addressing <b>dynamic</b> <b>program</b> steering, raising issues in operating and language systems, mechanisms and algorithms for <b>dynamic</b> <b>program</b> adaptation, program monitoring {{and the associated}} data storage techniques, and the design of dynamically steerable or adaptable programs. We define program steering as the capacity to control the execution of long-running, resource-intensive <b>programs.</b> <b>Dynamic</b> <b>program</b> steering consists of two separable tasks: monitoring program or system state (monitoring) and then enacting program changes made in response to observed state changes (steering) ...|$|R
40|$|The network revenue {{management}} (RM) problem arises in airline, hotel, media, and other industries where the sale products use multiple resources. It can be formulated as a stochastic <b>dynamic</b> <b>program,</b> but the <b>dynamic</b> <b>program</b> is computationally intractable {{because of an}} exponentially large state space, {{and a number of}} heuristics have been proposed to approximate its value function. In this paper we show that the piecewise-linear approximation to the network RM <b>dynamic</b> <b>program</b> is tractable; specifically we show that the separation problem of the approximation can be solved as a relatively compact linear program. Moreover, the resulting compact formulation of the approximate <b>dynamic</b> <b>program</b> turns out to be exactly equivalent to the Lagrangian relaxation of the <b>dynamic</b> <b>program,</b> an earlier heuristic method proposed for the same problem. We perform a numerical comparison of solving the problem by generating separating cuts or as our compact linear program. We discuss extensions to versions of the network RM problem with overbooking as well as the difficulties of extending it to the choice model of network revenue RM...|$|R
50|$|Most {{performance}} analysis tools use <b>dynamic</b> <b>program</b> analysis techniques.|$|R
25|$|When {{a problem}} shows optimal {{substructures}} – meaning the optimal {{solution to a}} problem can be constructed from optimal solutions to subproblems – and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called <b>dynamic</b> <b>programming</b> avoids recomputing solutions that have already been computed. For example, Floyd–Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. <b>Dynamic</b> <b>programming</b> and memoization go together. The main difference between <b>dynamic</b> <b>programming</b> and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in <b>dynamic</b> <b>programming.</b> The difference between <b>dynamic</b> <b>programming</b> and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence <b>dynamic</b> <b>programming</b> is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, <b>dynamic</b> <b>programming</b> reduces the exponential nature of many problems to polynomial complexity.|$|E
25|$|At this point, we {{have several}} choices, {{one of which is}} to design a <b>dynamic</b> <b>programming</b> {{algorithm}} that will split the problem into overlapping problems and calculate the optimal arrangement of parenthesis. The <b>dynamic</b> <b>programming</b> solution is presented below.|$|E
25|$|Many {{problems}} in graph algorithms may be solved efficiently on graphs of bounded pathwidth, by using <b>dynamic</b> <b>programming</b> on a path-decomposition of the graph. Path decomposition {{may also be}} used to measure the space complexity of <b>dynamic</b> <b>programming</b> algorithms on graphs of bounded treewidth.|$|E
5000|$|... #Subtitle level 3: Gambling {{game as a}} {{stochastic}} <b>dynamic</b> <b>program</b> ...|$|R
30|$|Starting from n[*]=[*]T/∆t, use LSM {{to solve}} the {{backward}} <b>dynamic</b> <b>program</b> Eq. (5).|$|R
2500|$|... {{return the}} solution, S', using the [...] {{values in the}} <b>dynamic</b> <b>program</b> {{outlined}} above ...|$|R
25|$|Richard Bellman {{developed}} <b>dynamic</b> <b>programming</b> {{since the}} 1940s.|$|E
25|$|There is a pseudo-polynomial time {{algorithm}} using <b>dynamic</b> <b>programming.</b>|$|E
25|$|There are {{at least}} three {{possible}} approaches: brute force, backtracking, and <b>dynamic</b> <b>programming.</b>|$|E
40|$|NABBIT is a work-stealing {{library for}} {{execution}} of task graphs with arbitrary dependencies which is implemented as a library for the multithreaded programming language Cilk++. We prove that Nabbit executes static task graphs in parallel in time which is asymptotically optimal for graphs whose nodes have constant in-degree and out-degree. To evaluate {{the performance of}} Nabbit, we implemented a <b>dynamic</b> <b>program</b> representing the Smith-Waterman algorithm, an irregular <b>dynamic</b> <b>program</b> on a two-dimensional grid. Our experiments indicate that when task-graph nodes are mapped to reasonably sized blocks, Nabbit exhibits low overhead and scales as well as or better than other scheduling strategies. The Nabbit implementation that solves the <b>dynamic</b> <b>program</b> using a task graph even manages {{in some cases to}} outperform a divide-and-conquer implementation for directly solving the same <b>dynamic</b> <b>program.</b> Finally, we extend both the Nabbit implementation and the completion-time bounds to handle dynamic task graphs, that is, graphs whose nodes and edges are created on the fly at runtime. National Science Foundation (U. S.) (Grant 0615215...|$|R
40|$|Concurrency {{programs}} {{are hard to}} test or debug due to their non-deterministic nature. Existing <b>dynamic</b> <b>program</b> analysis approaches tried to address this by carefully examine a recorded execution trace. However, developing such analysis tools is complicated, re-quiring {{to take care of}} many tedious implementation details, and comparing and evaluating different analysis approaches are also subject to various biases, due to lack of a common base platform. This motivates us to design DPAC, an infrastructure that support in building <b>dynamic</b> <b>program</b> analysis tools for concurrency Java programs. DPAC takes events and their various processing mecha-nisms as its underlying model to facilitate monitoring and manipu-lation of program executions as required by <b>dynamic</b> <b>program</b> anal-ysis. Various analysis tools can be implemented by customizing their required event types and processing mechanisms. We show two concrete case studies how our DPAC helps building existing <b>dynamic</b> <b>program</b> analysis approaches, as well as tuning subtle im-plementation details for supporting customized function implemen-tation and code transformation...|$|R
5000|$|In {{their most}} general form, {{stochastic}} <b>dynamic</b> <b>programs</b> deal with functional equations taking the following structure ...|$|R
25|$|Links to the MAPLE {{implementation}} of the <b>dynamic</b> <b>programming</b> approach may be found among the external links.|$|E
25|$|<b>Dynamic</b> <b>programming</b> {{is widely}} used in {{bioinformatics}} for the tasks such as sequence alignment, protein folding, RNA structure prediction and protein-DNA binding. The first <b>dynamic</b> <b>programming</b> algorithms for protein-DNA binding were developed in the 1970s independently by Charles DeLisi in USA and Georgii Gurskii and Alexander Zasedatelev in USSR. Recently these algorithms have become very popular in bioinformatics and computational biology, particularly in the studies of nucleosome positioning and transcription factor binding.|$|E
25|$|This <b>dynamic</b> <b>programming</b> {{approach}} {{is used in}} machine learning via the junction tree algorithm for belief propagation in graphs of bounded treewidth. It also {{plays a key role}} in algorithms for computing the treewidth and constructing tree decompositions: typically, such algorithms have a first step that approximates the treewidth, constructing a tree decomposition with this approximate width, and then a second step that performs <b>dynamic</b> <b>programming</b> in the approximate tree decomposition to compute the exact value of the treewidth.|$|E
5000|$|Bernd Bruegge, Tim Gottschalk, Bin Luo. A {{framework}} for <b>dynamic</b> <b>program</b> analyzers. In ACM SIGPLAN Notices, 28(10), 1993.|$|R
40|$|We {{introduce}} {{the concept of}} multimodularity into the class of stochastic <b>dynamic</b> <b>programs</b> in which state and decision variables are economic substitutes. We discuss its properties and its relationships with supermodularity, converxity, and L# convexity in real space. We show that multimodularity is preserved under minimization and multimodularity leads to monotone optimal policies with bounded sensitivity. Several examples from inventory management are used to illustrate its applications in stochastic <b>dynamic</b> <b>programs...</b>|$|R
40|$|Abstract—NABBIT is a work-stealing {{library for}} {{execution}} of task graphs with arbitrary dependencies which is implemented as a library for the multithreaded programming language Cilk++. We prove that NABBIT executes static task graphs in parallel in time which is asymptotically optimal for graphs whose nodes have constant in-degree and out-degree. To evaluate {{the performance of}} NABBIT, we implemented a <b>dynamic</b> <b>program</b> representing the Smith-Waterman algorithm, an irregular <b>dynamic</b> <b>program</b> on a two-dimensional grid. Our experiments indicate that when task-graph nodes are mapped to reasonably sized blocks, NABBIT exhibits low overhead and scales as well as or better than other scheduling strategies. The NABBIT implementation that solves the <b>dynamic</b> <b>program</b> using a task graph even manages {{in some cases to}} outperform a divide-and-conquer implementation for directly solving the same <b>dynamic</b> <b>program.</b> Finally, we extend both the NABBIT implementation and the completion-time bounds to handle dynamic task graphs, that is, graphs whose nodes and edges are created on the fly at runtime. Keywords-Cilk, dag, dynamic multithreading, parallel computing, work/span analysis...|$|R
25|$|Julia (programming {{language}}), a high-level, multi-paradigm, open-source, <b>dynamic</b> <b>programming</b> language primarily {{intended for}} numerical computations, {{although it is}} flexible enough for general-purpose programming.|$|E
25|$|Several {{algorithms}} {{are available}} to solve knapsack problems, based on <b>dynamic</b> <b>programming</b> approach, branch and bound approach or hybridizations of both approaches.|$|E
25|$|At the {{beginning}} of the 1970s, it was observed that a large class of combinatorial optimization problems defined on graphs could be efficiently solved by non-serial <b>dynamic</b> <b>programming</b> as long as the graph had a bounded dimension, a parameter related to treewidth. Later, several authors independently observed, {{at the end of the}} 1980s, that many algorithmic problems that are NP-complete for arbitrary graphs may be solved efficiently by <b>dynamic</b> <b>programming</b> for graphs of bounded treewidth, using the tree-decompositions of these graphs.|$|E
40|$|AbstractWe {{consider}} the standard <b>dynamic</b> <b>program</b> {{to solve the}} TSP. We then obtain exponentially large neighborhoods by selecting a polynomially bounded number of states, and restricting the <b>dynamic</b> <b>program</b> to those states only. We show how the Balas and Simonetti neighborhood and the insertion dynasearch neighborhood can be viewed in this manner. We also show {{that one of the}} dynasearch neighborhoods can be derived directly from the 2 -exchange neighborhood using this approach...|$|R
40|$|The {{valuation}} of Flexible Manufacturing Systems {{is one of}} the most frequently undertaken productivity improvement activities. In practice, the introduction of an FMS into industry must be done on the basis of cost justification. Recently developed techniques for the e{{valuation of}} the value of flexibility typically include the computation of stochastic <b>dynamic</b> <b>programs.</b> However, the computational effort of stochastic <b>dynamic</b> <b>programs</b> grows combinatorially and limits application to real world problems. In this contribution, we derive fast approximations to the stochastic <b>dynamic</b> <b>program</b> and compare their results to the exact solution. The proposed methods show an excellent worst case behavior (1 %) for a wide range of volatility of the underlying stochastic profit margins and costs for switching the production mode. The computational effort is reduced by a factor of more than 200...|$|R
5000|$|Gambling game can be {{formulated}} as a Stochastic <b>Dynamic</b> <b>Program</b> as follows: {{there are}} [...] games (i.e. stages) {{in the planning}} horizon ...|$|R
