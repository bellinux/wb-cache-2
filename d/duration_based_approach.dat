1|10000|Public
40|$|Asset {{variance}} and covariance {{are fundamental}} for {{financial risk management}} and many finance applications. With the advent of tick-by-tick high-frequency data, the estimation of univariate variances and multivariate covariance matrices has attracted more attention from econometricians. Many of the proposed high-frequency variance and covariance estimators are based on time-domain measurements. In this thesis, we investigate variance and covariance estimators constructed on the price domain: the price duration based variance and covariance estimators. A price event occurs when the absolute cumulative price change equals or exceeds a pre-specified threshold value. The time taken between two consecutive price events is a price duration. Intuitively, shorter durations are indicative of higher volatility. The duration-based approach provides a new angle {{to look at the}} high-frequency data, additionally, the duration based variance and covariance estimators are shown to be more efficient than competing time-domain high-frequency estimators. The information advantage of the <b>duration</b> <b>based</b> <b>approach</b> is demonstrated through two empirical applications, a volatility forecasting exercise and an out-of-sample globalminimum-variance portfolio allocation problem. The duration based estimators are shown to provide both better forecasting performance and better portfolio allocation results. The paper in Chapter 2 is under the first round Revise&Resubmit to the Journal of Business & Economic Statistics. In Chapter 2, we discuss the estimation of univariate variance using price durations. Variance estimation using high-frequency data needs {{to take into account the}} effect of market microstructure (MMS) noise, including discrete transaction times, discrete price levels, and bid/ask spreads, as well as price jumps. The price duration estimator has a built-in feature to be robust to large price jumps, while its robustness against the MMS noise is achieved through a careful selection of the threshold value that defines a price event. We discuss the selection of this optimal threshold value through both simulation and empirical evidence. We devise both a non-parametric and a parametric estimator. For the estimation of integrated variance at a daily frequency, the non-parametric duration based variance estimator suffices, while the parametric estimator additionally provides us with an instantaneous variance estimator. As an empirical application to 20 DJIA stocks, we compare the volatility forecasting performance of three classes of volatility estimators, including the realized volatility, the option implied volatility, and the price duration based volatility estimators, on one-day, one-week, and one-month horizons. Forecasting comparisons among individual estimators, as well as in a combination setup, are considered. The duration based estimators, especially the parametric price duration volatility estimator, are found to provide more accurate out-of-sample forecasts. In Chapter 3, we introduce a covariance matrix estimator using price durations. In the multivariate setting, there is the additional issue of nonsynchronous trade arrival times when estimating a high-dimensional variance-covariance matrix using tick-by-tick transaction data. Through simulation, we assess the effects of the lasttick time-synchronization method and MMS noise on the duration based covariance estimator, and compare its accuracy and efficiency with other candidate covariance estimators. Since the covariance matrix is estimated on a pairwise basis, it is not guaranteed to be positive semi-definite (psd). To reduce the number of negative eigenvalues produced by a non-psd matrix, we devise an averaging estimator which is the average of a wide range of duration based covariance matrix estimators. This estimator is applied to a portfolio of 19 DJIA stocks on an out-of-sample global minimum variance portfolio allocation problem where the objective is to minimize the one-day ahead portfolio variance. A simple shrinkage technique is used to improve non-psd and ill-conditioned matrices. The price duration covariance matrix estimator is shown to provide a comparably low portfolio variance while yielding considerably lower portfolio turnover rates than previous estimators...|$|E
40|$|Present {{electricity}} grids {{are predominantly}} thermal (coal, gas) and hydro based. Conventional power planning involves hydro-thermal scheduling and merit order dispatch. In the future, modern renewables (hydro, solar and biomass) {{are likely to}} have a significant share in the power sector. This paper presents a method to analyse the impacts of renewables in the electricity grid. A load <b>duration</b> curve <b>based</b> <b>approach</b> has been developed. Renewable energy sources have been treated as negative loads to obtain a modified load duration curve from which capacity savings in terms of base and peak load generation can be computed. The methodology is illustrated for solar, wind and biomass power for Tamil Nadu (a state in India). The trade-offs and interaction between renewable sources are analysed. The impacts on capacity savings by varying the wind regime have also been shown. Scenarios for 2021 - 22 have been constructed to illustrate the methodology proposed. This technique can be useful for power planners for an analysis of renewables in future electricity grids. Renewable energy Grid integration Load duration curve...|$|R
5000|$|Out of Doors {{contains}} {{the following five}} pieces with approximate <b>duration</b> <b>based</b> on metronome markings: ...|$|R
30|$|Profile 3 {{accumulated}} per day behavior {{representing the}} number of calls made along with their <b>duration</b> <b>based</b> {{on the type of}} calls (Loc, Nat, Int).|$|R
5000|$|The {{remainder}} of the slopes have a limited <b>duration</b> <b>based</b> on the selected base, so the remaining time of the conversion (in converter clock periods) is: ...|$|R
40|$|In {{the past}} few decades, {{mathematics}} <b>based</b> <b>approaches</b> have been widely adopted in various image restora-tion problems, among which the partial differential equation (PDE) <b>based</b> <b>approach</b> (e. g. the total variation model [56] and its generalizations, nonlinear diffusions [15, 52], etc.), and wavelet frame <b>based</b> <b>approach</b> are some of successful examples. These approaches were developed through different paths and generally provided understandings from different angles of the same problem. As shown in numerical simulations, implementa-tions of wavelet frame <b>based</b> <b>approach</b> and PDE <b>based</b> <b>approach</b> quite often end up with solving a similar numerical problem with similar numerical behaviors, even though different approaches have advantages in different applications. Since wavelet frame based and PDE <b>based</b> <b>approaches</b> have all been modeling {{the same type of}} problems with success, it is natural to ask whether wavelet frame <b>based</b> <b>approach</b> is fundamentally connected with PDE <b>based</b> <b>approach</b> when we trace {{all the way back to}} their roots. A fundamental connection of a wavelet frame <b>based</b> <b>approach</b> with total variation model and its generalizations were established in [8]. This connection gives wavelet frame <b>based</b> <b>approach</b> a geometric explanation and, at the same time, it equips a PDE <b>based</b> <b>approach</b> with a time frequency analysis. It was shown in [8] that a special type of wavelet frame model using generic wavelet frame systems can be regarded as an approximation of a generic variational mode...|$|R
30|$|Release duration—determine {{the range}} of release <b>duration</b> <b>based</b> on a field operator’s average site {{arriving}} time, 15  min for pressure-regulating station, and the mission fails if the training time exceeds the maximum time without a series of proper actions.|$|R
30|$|In our {{proposed}} mechanism, {{the exact}} TXOP estimation {{is not the}} goal. However, the mechanism provides a heuristic approach for allocating the TXOP <b>duration</b> <b>based</b> on the feedback queue size by implementing the finite state machine to dynamically adjust the TXOP duration for each SI.|$|R
40|$|Abstract- Text mining {{is nothing}} but the {{discovery}} of interesting knowledge in text documents. But {{there is a big}} challenging issue that how to guarantee the quality of discovered relevant features. And that are in the text documents for describing user preferences because of the large number of terms, patterns and noise. For text mining there are basically two types of approaches; one is term <b>based</b> <b>approach</b> and another is phrase <b>based</b> <b>approach.</b> But term <b>based</b> <b>approach</b> suffered with the problem of polysemy and synonymy. And phrase <b>based</b> <b>approach</b> suffered with low frequency occurrence. But phrase <b>based</b> <b>approachs</b> are better than the term <b>based</b> <b>approachs.</b> But pattern <b>based</b> <b>approach</b> is better than the term based and phrase <b>based</b> <b>approach.</b> The proposed method is an innovative and effective pattern discovery technique. This method includes two main processes pattern deploying and inner pattern evaluation. This paper presents an effective technique to improve the effectiveness of using and updating discovered patterns for finding relevant and interesting information. Using Baysian filtering algorithm and effective pattern Discovery technique we can detect the spam mails from the email dataset with good correctness of term. Index Terms — Text mining, information filtering, pattern mining, sequential pattern, closed sequential patterns...|$|R
40|$|Abstract. String based {{as well as}} tree based {{methods have}} been used to learn {{wrappers}} for extraction from semi-structured docu-ments (e. g., HTML documents). Previous work has shown that tree <b>based</b> <b>approaches</b> perform better while needing less examples than string <b>based</b> <b>approaches.</b> A disadvantage is that they can only extract complete text nodes, whereas string <b>based</b> <b>approaches</b> can extract within text nodes. This paper proposes a hybrid approach that com-bines the advantages of both systems and compares it experimentally with a string <b>based</b> <b>approach</b> on some sub node extraction tasks. ...|$|R
40|$|The {{present study}} on Activity <b>Based</b> <b>Approach</b> enhance {{achievement}} in sciences of class-VII students. Activity <b>Based</b> <b>Approach</b> consisted of different {{activities for the}} all around development of children at the elementary level. Activity should be prepared by low cost material which {{is available in the}} locality. Hence it is concluded that Activity <b>Based</b> <b>Approach</b> is significantly effective than the traditional approach of teaching...|$|R
30|$|In this section, some {{relevant}} approaches presenting similar concept are comprehensively elaborated. These {{approaches are}} broadly categorized into (1) MEC <b>based</b> <b>approaches</b> (2) Cloudlets <b>based</b> <b>approaches</b> and (3) Open Fog Consortium.|$|R
40|$|Software Maintenance Testing is {{essential}} during software testing phase. All defects found during testing must undergo a re-test process {{in order to}} eliminate the flaws. By doing so, test cases are absolutely needed to evolve and change accordingly. In this paper, several maintenance testing approaches namely regression test suite <b>approach,</b> heuristic <b>based</b> <b>approach,</b> keyword <b>based</b> <b>approach,</b> GUI <b>based</b> <b>approach</b> and model <b>based</b> <b>approach</b> are evaluated <b>based</b> on software evolution taxonomy framework. Some of the discussed approaches support changes of test cases. Out of the review study, a couple of results are postulated and highlighted including the limitation of the existing approaches...|$|R
40|$|Abstract. The {{approaches}} to learn wrappers for extraction from semi-structured documents (like HTML documents) {{are divided into}} string based ones, and tree based ones. In previous papers we have shown that tree <b>based</b> <b>approaches</b> perform much better and need less examples than string <b>based</b> <b>approaches,</b> but have the disadvantage that they can only extract complete text nodes, whereas string <b>based</b> <b>approaches</b> can extract within text nodes. In this {{paper we propose a}} hybrid approach that combines the advantages of both systems. We compare this approach experimentally with a string <b>based</b> <b>approach</b> on some sub node extraction tasks. ...|$|R
40|$|The {{present study}} was {{designed}} to investigate the influences of type of psychophysical task (two-alternative forced-choice [2 AFC] and reminder tasks), type of interval (filled vs. empty), sensory modality (auditory vs. visual), and <b>base</b> <b>duration</b> (ranging from 100 through 1, 000 ms) on performance on duration discrimination. All of these factors were systematically varied in an experiment comprising 192 participants. This approach allowed for obtaining information not only on the general (main) effect of each factor alone, but also on the functional interplay and mutual interactions of some or all of these factors combined. Temporal sensitivity was markedly higher for auditory than for visual intervals, {{as well as for the}} reminder relative to the 2 AFC task. With regard to <b>base</b> <b>duration,</b> discrimination performance deteriorated with decreasing <b>base</b> <b>durations</b> for intervals below 400 ms, whereas longer intervals were not affected. No indication emerged that overall performance on duration discrimination was influenced by the type of interval, and only two significant interactions were apparent: <b>Base</b> <b>Duration</b> × Type of Interval and <b>Base</b> <b>Duration</b> × Sensory Modality. With filled intervals, the deteriorating effect of <b>base</b> <b>duration</b> was limited to very brief <b>base</b> <b>durations,</b> not exceeding 100 ms, whereas with empty intervals, temporal discriminability was also affected for the 200 -ms <b>base</b> <b>duration.</b> Similarly, the performance decrement observed with visual relative to auditory intervals increased with decreasing <b>base</b> <b>durations.</b> These findings suggest that type of task, sensory modality, and <b>base</b> <b>duration</b> represent largely independent sources of variance for performance on duration discrimination that can be accounted for by distinct nontemporal mechanisms...|$|R
50|$|Medical {{professionals}} {{recommend a}} preventative <b>based</b> <b>approach</b> of stopping fungus before it occurs. Prevention is preferable over a reactive treatment <b>approach.</b> The preventative <b>based</b> <b>approach</b> involves removing heat and moisture to the groin area.|$|R
40|$|This paper {{introduces}} {{a new set}} of long duration captures of Internet traffic headers. The capture is being performed on a continuous on-going basis and is approaching a year in <b>duration.</b> <b>Based</b> on the current extent of the archive some typical analyses are presented, covering protocol mix, network trip times and TCP flag analysis. 1...|$|R
40|$|We {{propose a}} self-supervised word-segmentation {{technique}} for Chinese information retrieval. This method combines {{the advantages of}} traditional dictionary <b>based</b> <b>approaches</b> with character <b>based</b> <b>approaches,</b> while overcoming many of their shortcomings. Experiments on TREC data show comparable performance to both the dictionary based and the character <b>based</b> <b>approaches.</b> However, our method is completely language independent and unsupervised, which provides a promising avenue for constructing accurate multilingual or cross-lingual information retrieval systems that are exible and adaptive...|$|R
3000|$|... f, {{there is}} no problem of {{applicability}} of theory but of parameters’ stability and test costs. As the cost of a test increases with increasing test duration, there is a desire to have a prior estimate of the minimum test duration that yields valid results. Many authors proposed a recommended minimum <b>duration,</b> <b>based</b> on practical field experience (Gehlin 1998; Austin 1998). Other authors developed a method to estimate the minimum test <b>duration</b> <b>based</b> on the borehole and soil properties (Beier and Smith 2003). The probabilistic criterion proposed consists of finding a consistent value based on a forward regression (FR) procedure: when the slope value does not show a meaningful change by an increment of the test time, it can be assumed that the heat transfer in the underground follows the predicted evaluation model, and the evaluation time is large enough to provide for the statistical precision desired.|$|R
40|$|Abstract — Fingerprint {{technique}} {{is used for}} identification and verification purposes as it develops a low cost and fast computing system. There are many approaches for fingerprint recognition. The two basic approaches for fingerprint identification are minutiae <b>based</b> <b>approach</b> and Pattern recognition approach. Now a day’s wavelet <b>based</b> <b>approach</b> is most preferred due to its time-frequency. We are presenting here this new Wavelet <b>based</b> <b>approach</b> and compare the results by Traditional DFT, Traditional FFT, FRIRV techniques...|$|R
40|$|The aim of {{this paper}} is to measure {{security}} in requirement engineering using questionnaire <b>based</b> <b>approach.</b> The questionnaire is applied in the four stages of requirement engineering (Elicitation, analyses, validation, management). The questionnaire <b>based</b> <b>approach</b> is composing of three main parts. First the security questions part. Second the evaluation part which should be filled by the stakeholders. Third the assessment part. Finally A case study conducted to apply Questionnaire <b>based</b> <b>approach</b> and to measure security in requirement engineering. 1...|$|R
40|$|Abstract. Distributed Constraint Satisfaction Problems (DCSPs) {{provide a}} model to capture {{a broad range of}} {{cooperative}} multi-agent problem solving settings. Researchers have generally proposed two different sets of approaches for solving DCSPs, backtracking <b>based</b> <b>approaches,</b> such as Asynchronous Backtracking (ABT), and mediation <b>based</b> <b>approaches,</b> such as Asynchronous Partial Overlay (APO). These sets of approaches differ in the levels of coordination employed during conflict resolution. While the computational and communication complexity of the backtracking <b>based</b> <b>approaches</b> is well understood, the tradeoffs in complexity involved in moving toward mediation <b>based</b> <b>approaches</b> are not. In this paper we comprehensively reexamine the space of mediation <b>based</b> <b>approaches</b> for DCSP and fill gaps in existing frameworks with new strategies. We present different mediation session selection rules, including a rule that favors smaller mediation sessions, and different mediation strategies, including a decentralized hybrid strategy based on ABT. We present empirical results on solvable 3 -coloring and random binary DCSP problems, that accurately capture the computational and communication tradeoffs between ABT and various mediation <b>based</b> <b>approaches.</b> Our results confirm that under some circumstances the newly presented strategies dominate previously proposed techniques. ...|$|R
5000|$|... Africa Contact {{follows a}} rights <b>based</b> <b>approach</b> <b>based</b> on the UN Convention on Social and Economic Rights.|$|R
30|$|Evaluation {{showed that}} the {{proposed}} utility based solution outperformed the existing heuristic <b>based</b> <b>approach</b> in terms of energy savings and minimizing SLAVs in both lightly loaded and more heavily loaded cloud data centers. Perhaps the key factor that differentiates the approaches is that the heuristics <b>based</b> <b>approach</b> adapts whenever {{there is a problem}} (PM overload, or PM under-load). On the contrary, the utility <b>based</b> <b>approach</b> adapts only if it can identify an adaptation that is expected to improve on the current allocation.|$|R
40|$|Abstract – The {{present study}} on Activity <b>Based</b> <b>Approach</b> enhance {{achievement}} in sciences of class-VII students. Activity <b>Based</b> <b>Approach</b> consisted of different {{activities for the}} all around development of children at the elementary level. Activity should be prepared by low cost material which {{is available in the}} locality. Hence it is concluded that Activity <b>Based</b> <b>Approach</b> is significantly effective than the traditional approach of teaching. Key Words – Effect of activity based; approach on achievement in science; students at elementary stage...|$|R
40|$|Text mining {{is nothing}} but the {{discovery}} ofinteresting knowledge in text documents. But there is a bigchallenging issue that how to guarantee the quality of discoveredrelevant features. And {{that are in the}} text documents fordescribing user preferences because of the large number ofterms, patterns and noise. For text mining there are basically twotypes of approaches; one is term <b>based</b> <b>approach</b> and another isphrase <b>based</b> <b>approach.</b> But term <b>based</b> <b>approach</b> suffered withthe problem of polysemy and synonymy. And phrase basedapproach suffered with low frequency occurrence. But phrasebased approachs are better than the term <b>based</b> <b>approachs.</b> Butpattern <b>based</b> <b>approach</b> is better than the term <b>based</b> and phrasebased <b>approach.</b> The proposed method is an innovative andeffective pattern discovery technique. This method includes twomain processes pattern deploying and inner pattern evaluation. This paper presents an effective technique to improve theeffectiveness of using and updating discovered patterns forfinding relevant and interesting information. Using Baysianfiltering algorithm and effective pattern Discovery technique wecan detect the spam mails from the email dataset with goodcorrectness of term...|$|R
40|$|In {{this paper}} we present {{work on a}} {{scenario}} and persona <b>based</b> <b>approach</b> to exploring social software solutions for a globally distributed network of researchers, designers and artists. We discuss issues identified with scenario <b>based</b> <b>approaches</b> and a potential participatory solution adopted in this project...|$|R
40|$|Object {{detection}} {{is a part}} of {{our everyday}} lives, however, automatic object detection by computer is still an open question. In 30 years of research in computer vision, little progress has been made. This report is a survey on the most recent techniques in object detection research. First, we introduce the definition, challenges, applications and general components of the object detection system. This is followed by a review of various appearance <b>based</b> <b>approaches</b> and feature <b>based</b> <b>approaches.</b> Appearance <b>based</b> <b>approaches</b> are classified <b>based</b> on different classifiers into linear representation, distribution-based, support vector machines, sparse Winnow network. Meanwhile different feature <b>based</b> <b>approaches</b> are distinguished from each other by what features are being used- texture, shape, context and multiple features. Then a framework of an object detection system i...|$|R
40|$|In speaker {{verification}} {{the world}} model <b>based</b> <b>approach</b> and the cohort model <b>based</b> <b>approach</b> {{have been used}} for better HMM score measurements for verification comparison. From theoretical analysis these two approaches represent two different paradigms for verification decision-making strategy. Two techniques could be combined for a better solution. In the paper we present a hybrid score measurement which combines the world model based technique and the cohort model based technique together. The method is evaluated with the YOHO database. The results show that the combination can lead a better score measurement which improves speaker verification performance. An experimental comparison between the world model <b>based</b> <b>approach</b> and the cohort model <b>based</b> <b>approach</b> with the YOHO database can also be found in the paper. 1...|$|R
40|$|Texture {{segmentation}} is {{the process}} of partitioning an image into regions with different textures containing similar group of pixels. This paper presents a comparative study of four texture segmentation methods based on the following features: descriptors, heuristic function, fuzzy logic and Mask based features. Many types of textures are considered for analysis. The comparative results show that descriptor <b>based</b> <b>approach</b> is the most suitable for segmenting both natural and mosaic textures whereas heuristic function <b>based</b> <b>approach</b> is most suitable for random textures. Fuzzy features <b>based</b> <b>approach</b> is found to yield better segments for regular patterns while Mask feature <b>based</b> <b>approach</b> is the best for segmenting Natural images, but fails miserably on Mosaic textures. Fuzzy C-means classification is used for achieving texture segmentation. 1...|$|R
40|$|Remote sensing imagery {{needs to}} be {{converted}} into tangible information which can be utilized {{in conjunction with other}} data sets, often within widely used Geospatial Information Systems (GIS). Remote sensing data help in mapping land resources, especially in mountainous areas where accessibility is limited. Classification of remote sensing data in mountainous terrain is problematic because of variations in the sun illumination angle. Traditional approaches have many problems in these conditions. In object <b>based</b> <b>approach</b> can utilized GIS tools for improvement of classification results. In the present work we used pixel based and object <b>based</b> <b>approaches</b> that in both we imported GIS concepts and ancillary data for refining of classification results. The results showed that object <b>based</b> <b>approach</b> have higher accuracy than pixel <b>based</b> <b>approach.</b> 1...|$|R
40|$|While {{billions of}} dollars have been spent in {{development}} projects in least developed countries, poverty continues to increase. This study proposes human-rights <b>based</b> <b>approach</b> to poverty eradication. To this end, the study seeks to assess the key determinants of use of rights- <b>based</b> <b>approaches</b> to poverty reduction and it’s usefulness in Kenya with special reference to NGOs in Kibera. The study further high lights {{some of the basic}} skills of implementing the rights <b>based</b> <b>approach</b> to poverty reduction. The attempts to establish the proportion of NGOs applying rights <b>based</b> <b>approach</b> to poverty reduction in Kibera Division as well. The review of relevant literature has been undertaken and a field study done. The study is informed by a qualitative human rights framework. </em...|$|R
40|$|A 30 -year-old female patient {{presented}} {{to us with}} erythema over the face and raised hyperpigmented, scaly skin lesions, mainly over photo-exposed parts, of about 5 -month <b>duration.</b> <b>Based</b> on her clinical features and initial laboratory finding, we considered {{the possibility of a}} connective tissue disease. On further follow-up, she was found to be human immunodeficiency virus positive (confirmed by Western blot) ...|$|R
50|$|Bharati Vidyapeeth Institute of Technology (BVIT) is an polytechnic {{college in}} Navi Mumbai. It {{is located in}} CBD Belapur, but closer to Kharghar being {{situated}} opposite to the Kharghar railway station. The educational institution is affilitated to MSBTE. It offers Diploma courses of three years <b>duration</b> <b>based</b> on the semester pattern in various fields to students who have passed SSC or HSC.|$|R
50|$|Various fringe events {{also take}} place at {{separate}} venues in the region during the festival {{and a number of}} other film-based activities are run throughout the year.The festival includes a competition for film makers to make a film under 5 minutes in <b>duration</b> <b>based</b> upon a creative title. The winner of the competition receives an award and a cash prize.|$|R
40|$|This paper {{proposes a}} new {{duration-based}} backtesting procedure for VaR forecasts. The GMM test framework proposed by Bontemps (2006) {{to test for}} the distributional assumption (i. e. the geometric distribution) {{is applied to the}} case of the VaR forecasts validity. Using simple J-statistic based on the moments de…ned by the orthonormal polynomials associated with the geometric distribution, this new approach tackles most of the drawbacks usually associated to <b>duration</b> <b>based</b> backtesting procedures. First, its implementation is extremely easy. Second, it allows for a separate test for unconditional coverage, independence and conditional coverage hypothesis (Christo¤ersen, 1998). Third, Monte-Carlo simulations show that for realistic sample sizes, our GMM test outperforms traditional <b>duration</b> <b>based</b> test. Besides, we study the consequences of the estimation risk on the duration-based backtesting tests and propose a sub-sampling approach for robust inference derived from Escanciano and Olmo (2009). An empirical application for Nasdaq returns con…rms that using GMM test leads to major consequences for the ex-post evaluation of the risk by regulation authorities...|$|R
