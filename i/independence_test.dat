212|568|Public
25|$|Lee, S. and Honavar, V. (2017). Self-Discrepancy Conditional <b>Independence</b> <b>Test.</b> In: Conference on Uncertainty in Artificial Intelligence (UAI-17).|$|E
25|$|Lee, S. and Honavar, V. (2017). A Kernel <b>Independence</b> <b>Test</b> for Relational Data. In: Conference on Uncertainty in Artificial Intelligence (UAI-17).|$|E
5000|$|Lee, S. and Honavar, V. (2017). Self-Discrepancy Conditional <b>Independence</b> <b>Test.</b> In: Conference on Uncertainty in Artificial Intelligence (UAI-17).|$|E
30|$|If inhomogeneous Poisson {{processes}} can model nonstationary data, {{they are}} not appropriate to model dependencies between points. However, several studies have established potential dependence of spike occurrences for different neurons. This has been detected via descriptive statistics, via <b>independence</b> <b>tests</b> for a given fixed model or via model-free <b>independence</b> <b>tests</b> based on permutations (also called trials-shuffling) [5, 6, 13, 15, 22, 46].|$|R
40|$|Abstract This article {{reviews the}} nonparametric serial <b>independence</b> <b>tests</b> based on {{measures}} of divergence between densities. Among others, the well-known Kull-back-Leibler, Hellinger and Tsallis divergences are analyzed. Moreover, the copula-based version of the considered divergence functionals is defined and taken into account. In order to implement serial <b>independence</b> <b>tests</b> based on these divergence functionals, {{it is necessary to}} choose a density estimation technique, a way to com-pute p-values and other settings. Via a wide simulation study, the performance of the serial <b>independence</b> <b>tests</b> arising from the adoption of the divergence function-als with different implementation is compared. Both single-lag and multiple-lag test procedures are investigated in order to find the best solutions in terms of size and power...|$|R
40|$|This article reviews some nonparametric serial <b>independence</b> <b>tests</b> {{based on}} {{measures}} of divergence between densities. Among others, the well-known Kullback-Leibler, Hellinger, Tsallis, and Rosenblatt divergences are analyzed. Moreover, their copula-based version is taken into account. Via a wide simulation study, the performances of the considered serial <b>independence</b> <b>tests</b> are compared under different settings. Both single-lag and multiple-lag testing procedures are investigated {{to find out the}} best "omnibus" solution...|$|R
5000|$|Lee, S. and Honavar, V. (2017). A Kernel <b>Independence</b> <b>Test</b> for Relational Data. In: Conference on Uncertainty in Artificial Intelligence (UAI-17).|$|E
40|$|Identifying the {{statistical}} independence of random variables {{is one of}} the important tasks in statistical data analysis. In this paper, we propose a novel non-parametric <b>independence</b> <b>test</b> based on a least-squares density ratio estimator. Our method, called least-squares <b>independence</b> <b>test</b> (LSIT), is distribution-free, and thus it is more flexible than parametric approaches. Furthermore, it is equipped with a model selection procedure based on cross-validation. This is a significant advantage over existing non-parametric approaches which often require manual parameter tuning. The usefulness of the proposed method is shown through numerical experiments. Keywords <b>independence</b> <b>test,</b> density ratio estimation, unconstrained least-squares importance fitting, squared-loss mutual information. ...|$|E
40|$|In {{order to}} {{structure}} a gene network, a score-based approach is often used. A score-based approach, however, is problem-atic because by assuming a probability distribution, one is prevented from finding other dependent {{relationships with other}} genes. In this research, we structured a gene network from observed gene expression data using a multiresolution <b>independence</b> <b>test</b> and a conditional <b>independence</b> <b>test,</b> which is the non-parametric method proposed by Margaritis for learning the structure of Bayesian networks without making any probability distribution assumptions. The experimental results achieved an improvement in sensitivity of 0. 05, and an improvement in specificity of 0. 01. Index Terms — gene network, Bayesian network, condi-tional <b>independence</b> <b>test,</b> non-parametri...|$|E
40|$|This paper {{describes}} a parallel {{version of the}} PC algorithm for learning {{the structure of a}} Bayesian network from data. The PC algorithm is a constraint-based algorithm consisting of fi ve steps where {{the first step is to}} perform a set of (conditional) <b>independence</b> <b>tests</b> while the remaining four steps relate to identifying the structure of the Bayesian network using the results of the (conditional) <b>independence</b> <b>tests.</b> In this paper, we describe a new approach to parallelization of the (conditional) <b>independence</b> <b>testing</b> as experiments illustrate that this is by far the most time consuming step. The proposed parallel PC algorithm is evaluated on data sets generated at random from five different real- world Bayesian networks. The results demonstrate that signi cant time performance improvements are possible using the proposed algorithm...|$|R
25|$|Bromberg, F., Margaritis, D., and Honavar, V. (2009). Efficient Markov Network Structure Discovery from <b>Independence</b> <b>Tests.</b> Journal of Artificial Intelligence Research. Vol. 35. pp.449–485.|$|R
40|$|We study {{statistical}} sum-tests and <b>independence</b> <b>tests,</b> {{in particular}} for computably enumerable semimeasures on a discrete domain. Among other things, we prove that for universal semimeasures every Sigma 0 / 1 -sum-test is bounded, but unbounded Pi 0 / 1 -sum-tests exist, and we study {{to what extent}} the latter can be universal. For universal semimeasures, in the unary case of sum-test we leave open whether universal Pi 0 / 1 -sum-tests exist, whereas in the binary case of <b>independence</b> <b>tests</b> we prove that they do not exist...|$|R
40|$|An <b>independence</b> <b>test</b> {{based on}} the sum of squared sample {{correlation}} coefficients was introduced by Schott (2005) and its asymptotic normality was also proved. In this paper, a Berry-Esseen bound with an optimal rate of O(1 /√m) is obtained and a general Berry-Esseen bound via Stein's method of exchangeable pair is also established. Key Words: Berry-Esseen bound, <b>independence</b> <b>test,</b> Stein's method, exchangeable pair...|$|E
30|$|The general mesh {{qualities are}} not {{sufficient}} to investigate the computational mesh. Therefore, the mesh <b>independence</b> <b>test</b> is {{an important aspect of}} simulating the three-phase separators.|$|E
40|$|A {{method for}} {{identification}} and handling of deterministic relationships for constraint-based structure discovery is proposed. The key {{of the method}} is reducing the degree of freedom of the conditional <b>independence</b> <b>test.</b> If the reduced degree of freedom becomes zero, it is proved that the <b>independence</b> <b>test</b> always holds, and is unreliable. Then, the test is skipped and at the end, it is passed to a human expert for taking the decision. The proposed method is tested successfully against randomly generated Bayesian networks as well as against a diagnostic network with many deterministic relationships...|$|E
40|$|AbstractThis paper {{considers}} a parallel algorithm for Bayesian network structure learning from large data sets. The parallel algorithm is {{a variant of}} the well known PC algorithm. The PC algorithm is a constraint-based algorithm consisting of five steps where {{the first step is to}} perform a set of (conditional) <b>independence</b> <b>tests</b> while the remaining four steps relate to identifying the structure of the Bayesian network using the results of the (conditional) <b>independence</b> <b>tests.</b> In this paper, we describe a new approach to parallelization of the (conditional) <b>independence</b> <b>testing</b> as experiments illustrate that this is by far the most time consuming step. The proposed parallel PC algorithm is evaluated on data sets generated at random from five different real-world Bayesian networks. The algorithm is also compared empirically with a process-based approach where each process manages a subset of the data over all the variables on the Bayesian network. The results demonstrate that significant time performance improvements are possible using both approaches...|$|R
3000|$|As {{discussed}} in [...] "Constraint-Based search" [...] section, the constraint-based approach to causal discovery involves conditional <b>independence</b> <b>tests,</b> {{which would be}} a difficult task if the form of dependence is unknown. It has the advantage that it is generally applicable, but the disadvantages are that faithfulness is a strong assumption and that it may require very large sample sizes to get good conditional <b>independence</b> <b>tests.</b> Furthermore, the solution of this approach to causal discovery is usually nonunique, and in particular, it does not help in determining causal direction in the two-variable case, where no conditional independence relationship is available.|$|R
40|$|We {{investigate}} efficient algorithms {{for learning}} {{the structure of}} a Markov network from data using the independence-based approach. Such algorithms conduct a series of conditional <b>independence</b> <b>tests</b> on data, successively restricting the set of possible structures until there is only a single structure consistent with the outcomes of the conditional <b>independence</b> <b>tests</b> executed (if possible). As Pearl has shown, the instances of the conditional independence relation in any domain are theoretically interdependent, made explicit in his well-known conditional independence axioms. The first couple of algorithms we discuss, GSMN and GSIMN, exploit Pearl 2 ̆ 7 s independence axioms {{to reduce the number of}} tests required to learn a Markov network. This is useful in domains where <b>independence</b> <b>tests</b> are expensive, such as cases of very large data sets or distributed data. Subsequently, we explore how these axioms can be exploited to 2 ̆ 2 correct 2 ̆ 2 the outcome of unreliable statistical <b>independence</b> <b>tests,</b> such as in applications where little data is available. We show how the problem of incorrect tests can be mapped to inference in inconsistent knowledge bases, a problem studied extensively in the field of non-monotonic logic. We present an algorithm for inferring independence values based on a sub-class of non-monotonic logics: the argumentation framework. Our results show the advantage of using our approach in the learning of structures, with improvements in the accuracy of learned networks of up to 20...|$|R
30|$|Wang et al. [20] used {{simulated}} {{and real}} datasets (kidney cancer RNA-seqdataset) {{to compare the}} false positive rates and statistical power of CANOVA to six other methods (Distance correlation’s, Hoeffding’s <b>independence</b> <b>test,</b> CANOVA the Pearson correlation coefficient, the Spearman’s rank correlation coefficient, the Kendall’s rank correlation coefficient and the Maximal information coefficient), and showed that CANOVA, the Pearson correlation coefficient, the Spearman’s rank correlation coefficient, the Kendall’s rank correlation coefficient and the MIC gave the expected false positives. Hence, these methods can detect the true significant variables. However, the false positive rate {{is lower than the}} expected for distance correlation and higher than the expected for Hoeffding’s <b>independence</b> <b>test.</b> So the true significant variables may not be detected by distance correlation, and there may be false significant variables in Hoeffding’s <b>independence</b> <b>test</b> result. Hence, Pearson correlation were recommended when correlation between two continuous variable is linear, and CANOVA were recommended when the correlation between two continuous variable is non-linear or complicated.|$|E
40|$|A wild {{bootstrap}} method for nonparametric hypothesis tests based on kernel dis-tribution embeddings is proposed. This {{bootstrap method}} {{is used to}} construct provably consistent tests that apply to random processes, for which the naive permutation-based bootstrap fails. It applies to {{a large group of}} kernel tests based on V-statistics, which are degenerate under the null hypothesis, and non-degenerate elsewhere. To illustrate this approach, we construct a two-sample test, an instantaneous <b>independence</b> <b>test</b> and a multiple lag <b>independence</b> <b>test</b> for time series. In experiments, the wild bootstrap gives strong performance on synthetic examples, on audio data, and in performance benchmarking for the Gibbs sampler. ...|$|E
40|$|This paper proposes an <b>independence</b> <b>test</b> {{for a set}} of {{clustered}} binary observations, such {{as might}} be encountered in developmental toxicity studies. An exchangeable binary model is employed, under an assumption of exchangeability among cluster elements, to model probability of positive response. With a Weibull form assumed for response, the <b>independence</b> <b>test</b> is equivalent to testing whether a parameter value is unity. This Weibull form also allows for parametric tests for a covariate-response relationship and for covariate effects on correlation. The procedure is illustrated using data obtained from a developmental toxicity study. Exchangeability Developmental study Correlated data...|$|E
30|$|This paper’s {{contribution}} {{is in the}} integration of diverse techniques and disciplines of statistical <b>independence</b> <b>tests,</b> applied machine learning, and fuzzy inference systems {{in such a way}} that, as yet, has not been undertaken previously.|$|R
40|$|Abstract—Approaches {{based on}} {{conditional}} <b>independence</b> <b>tests</b> {{are among the}} most popular methods for learning graphical models from data. Due to the predominance of Bayesian networks in the field, they are usually developed for directed graphs. For possibilistic networks of a certain kind, however, undirected graphs are a more natural basis and thus algorithms for learning undirected graphs are desirable in this area. In this paper I present an algorithm for learning undirected graphical models, which is derived from the well-known Cheng–Bell–Liu algorithm. Its main advantage is the lower number of conditional <b>independence</b> <b>tests</b> that are needed, while it achieves results of comparable quality. I...|$|R
40|$|Conditional <b>independence</b> <b>tests</b> have {{received}} special attention lately in machine learning and computational intelligence related literature {{as an important}} indicator of the relationship among the variables used by their models. In the field of probabilistic graphical models, which includes Bayesian network models, conditional <b>independence</b> <b>tests</b> are especially important for the task of learning the probabilistic graphical model structure from data. In this paper, we propose the full Bayesian significance test for <b>tests</b> of conditional <b>independence</b> for discrete datasets. The full Bayesian significance test is a powerful Bayesian test for precise hypothesis, {{as an alternative to}} the frequentist’s significance tests (characterized by the calculation of the p-value) ...|$|R
40|$|We {{give the}} {{explicit}} Karhunen-Loeve expansion {{of a family}} of centered Gaussian processes including the Anderson-Darling process. An application is provided through the description of a Cramer-von Mises type test of independence. Anderson-Darling statistic <b>Independence</b> <b>test</b> Karhunen-Loeve expansion Legendre functions...|$|E
30|$|We {{recommend}} that the authors perform code validation by comparing with the well-known benchmark solution. We also suggest that the authors conduct a grid <b>independence</b> <b>test</b> on the most critical condition of their research case in order to comprehend the effect of grid size on the numerical solution.|$|E
30|$|For {{the element}} <b>independence</b> <b>test,</b> the element size was {{adjusted}} to be 3, 2, and 1  mm, and corresponding maximum velocity and pressure were recorded {{to show the}} independence. The result showed that 1 -mm element size allowed accurate longitudinal comparison. The mesh comprised more than 158 K nodes and 580 K elements.|$|E
30|$|When Markov {{equivalence}} {{fails to}} entail distribution equivalence, using conditional independence relations alone for causal inference is still correct, {{but it is}} not as informative as theoretically possible. For example, assuming linearity, causal sufficiency, and nonGaussian errors (Shimizu et al. 2006), conditional <b>independence</b> <b>tests</b> can at best reliably determine the correct Markov equivalence class, while using other features of the sample density can be used to reliably determine a unique graph (Shimizu et al. 2006) or find information about latent variables. For example, linear graphical models entail rank constraints on various submatrices of the covariance matrix, regardless of the particular parameter values (Sullivant et al. 2010; Spirtes 2013). These rank constraints, together with conditional <b>independence</b> <b>tests,</b> can be used to identify models with latent confounders (Kummerfeld et al. 2014).|$|R
40|$|AbstractIn {{the paper}} we {{describe}} a new independence-based approach for learning Belief Networks. The proposed algorithm avoids {{some of the}} drawbacks of this approach by making an intensive use of low order conditional <b>independence</b> <b>tests.</b> Particularly, the set of zero- and first-order independence statements are used {{in order to obtain}} a prior skeleton of the network, and also to fix and remove arrows from this skeleton. Then, a refinement procedure, based on minimum cardinality d-separating sets, which uses a small number of conditional <b>independence</b> <b>tests</b> of higher order, is carried out to produce the final graph. Our algorithm needs an ordering of the variables in the model as the input. An algorithm that partially overcomes this problem is also presented...|$|R
40|$|In {{this paper}} the serial <b>independence</b> <b>tests</b> known as SIS (Serial Independence Simultaneous) and SICS (Serial Independence Chi-Square) are considered. These tests are here contextualized {{in the model}} {{validation}} phase for nonlinear models in which the underlying assumption of serial <b>independence</b> is <b>tested</b> on the estimated residuals. Simulations are used to explore {{the performance of the}} tests, in terms of size and power, once a linear/nonlinear model is fitted on the raw data. Results underline that both the tests are powerful against various types of alternatives...|$|R
40|$|Constraint-based causal {{discovery}} (CCD) algorithms require {{fast and}} accurate conditional independence (CI) testing. The Kernel Conditional <b>Independence</b> <b>Test</b> (KCIT) is currently {{one of the}} most popular CI tests in the non-parametric setting, but many investigators cannot use KCIT with large datasets because the test scales cubicly with sample size. We therefore devise two relaxations called the Randomized Conditional <b>Independence</b> <b>Test</b> (RCIT) and the Randomized conditional Correlation Test (RCoT) which both approximate KCIT by utilizing random Fourier features. In practice, both of the proposed tests scale linearly with sample size and return accurate p-values much faster than KCIT in the large sample size context. CCD algorithms run with RCIT or RCoT also return graphs at least as accurate as the same algorithms run with KCIT but with large reductions in run time. Comment: R package: github. com/ericstrobl/RCI...|$|E
40|$|We {{address the}} problem of {{reliability}} of independence-based causal discovery algorithms that results from unreliable statistical independence tests. We model the problem as a knowledge base containing a set of independences that are related through the well-known Pearl 2 ̆ 7 s axioms. Statistical tests on finite data sets may result in errors in these tests and inconsistencies in the knowledge base. Our approach uses an instance of the class of defeasible logics called argumentation, augmented with a preference function that is used to reason and possibly correct errors in these tests, thereby resolving the corresponding inconsistencies. This results in a more robust conditional <b>independence</b> <b>test,</b> called argumentative <b>independence</b> <b>test.</b> We evaluated our approach on data sets sampled from randomly generated causal models as well as real-world data sets. Our experiments show a clear advantage of argumentative over purely statistical tests, with improvements in accuracy of up to 17...|$|E
40|$|Switching between {{different}} alternative polyadenylation (APA) sites {{plays an important}} role in the fine tuning of gene expression. New technologies for the execution of 3 ’-end enriched RNA-seq allow genome-wide detection of the genes that exhibit significant APA site switch-ing {{between different}} samples. Here, we show that the <b>independence</b> <b>test</b> gives better re-sults than the linear trend test in detecting APA site-switching events. Further examination suggests that the discrepancy between these two statistical methods arises from complex APA site-switching events that cannot be represented by a simple change of average 3 ’-UTR length. In theory, the linear trend test is only effective in detecting these simple changes. We classify the switching events into four switching patterns: two simple patterns (3 ’-UTR shortening and lengthening) and two complex patterns. By comparing the results of the two statistical methods, we show that complex patterns account for 1 / 4 of all observed switching events that happen between normal and cancerous human breast cell lines. Be-cause simple and complex switching patterns may convey different biological meanings, they merit separate study. We therefore propose to combine both the <b>independence</b> <b>test</b> and the linear trend test in practice. First, the <b>independence</b> <b>test</b> should be used to detect APA site switching; second, the linear trend test should be invoked to identify simple switch-ing events; and third, those complex switching events that pass independence testing but fail linear trend testing can be identified...|$|E
40|$|Description E-{{statistics}} (energy) {{tests and}} statistics for comparing distributions: multivariate normality, multivariate distance components and k-sample test for equal distributions,hierarchical clustering by e-distances, multivariate <b>independence</b> <b>tests,</b> distance correlation, goodness-of-fit tests. Energy-statistics concept {{based on a}} generalization of Newton's potential energy is due to Gabor J. Szekely...|$|R
40|$|In {{this paper}} we give {{necessary}} conditions and sufficient {{conditions for a}} test to be locally unbiased, we define local admissibility and we characterize local admissibility in hypothesis testing problems with simple null hypotheses. Applications are presented involving same-sign alternatives, ordered alternatives and <b>independence</b> <b>testing</b> of several variables...|$|R
40|$|This paper {{presents}} empirical {{results for}} classification using Bayesian networks constructed using the K 2 Bayesian metric, and compares these results {{with those of}} other researchers who have used Bayesian networks constructed using the MDL score and using conditional <b>independence</b> <b>tests.</b> There are significant disparities in these results, which is somewhat paradoxical as it is has been shown that the MDL score is asymptotically equivalent to the Bayesian metric, and that structure search based on maximising a score is equivalent to structure search based on conditional <b>independence</b> <b>tests.</b> To resolve this paradox, we analyse the differences in methods used by different researchers to identify the source of the disparities. We conclude that differences in performance are attributable to differences in parameter estimation and structure search heuristics, rather than to differences in the scores/tests used...|$|R
