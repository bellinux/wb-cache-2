16|425|Public
50|$|They {{are only}} used for static face sealing, not moving glands. The thin tube walls are {{sensitive}} to damage from rough <b>handling,</b> <b>point</b> tools and crimping during assembly. The pressure-actuated form {{is considered to be}} slightly more resistant to point damage, but more at risk of folding and crimping, as they are not supported by internal pressurisation during assembly.|$|E
50|$|Kamela is {{the highest}} {{railroad}} pass in the Blue Mountains, with an elevation of 4206 ft. Kamela had a roundhouse {{and served as a}} wood and timber <b>handling</b> <b>point</b> for the railroad. In 1904, the population of the community was 220. In 1940 Kamela had a population of 27. At one time the community had a school. One of the last Oregon train robberies began in Kamela in 1914.|$|E
40|$|The {{following}} {{most recent}} developments in computational fracture mechanics at Cardiff University are reviewed: hybrid crack element (HCE) which can give directly the stress intensity factor (SIF) {{as well as the}} coefficients of higher order terms in the plane linear elastic crack tip asymptotic field; extended finite element method (XFEM) which avoids using a mesh conforming with the crack {{as is the case with}} the traditional FEM and gives highly accurate crack tip fields; penalty function technique for <b>handling</b> <b>point</b> loads; and compressed sparse row (CSR) storage scheme for efficient implementation of the above techniques. Possible future improvements are also discussed...|$|E
5000|$|A set of <b>handles</b> <b>pointing</b> {{forward and}} back, are welded inside the hollow portion.|$|R
5000|$|... #Caption: A {{reconstruction}} {{of an early}} Chinese compass. A spoon made of lodestone, its <b>handle</b> <b>pointing</b> south, was mounted on a brass plate with astrological symbols.|$|R
5000|$|... #Caption: Fig. 1. Pulvermacher's chain. The <b>handles</b> <b>pointing</b> {{away from}} the holder are {{insulated}} so that the physician can safely place the device in contact with {{any part of the}} patient's body.|$|R
40|$|Spatial data is data {{related to}} space. In various {{application}} fields like GIS, multimedia information systems, etc., {{there is a}} need to store and manage these data. Some datastructures used for the spatial access methods are R tree and its extensions where objects could be approximated by their minimum bounding rectangles and Quad tree based structures where space is subdivided according to certain rules. Also another structure KD Tree is used for organizing points in a k dimensional space. This paper makes review on some of these Hiearchical datastructures used for <b>handling</b> <b>point</b> data. It focuses on PR Quad Tree and KD Tree. The insertion procedure of these structures is reviewed and analyzed and also a comparison between them is drawn. ...|$|E
40|$|Recently {{point set}} model is getting {{increasing}} research attention in many geometric modeling application areas including computer graphics and CAD/CAM. This paper presents {{a novel approach}} to directly slicing point set model with the focus on making rapid prototyping part out of point set model without making any mesh or surface. Main challenge in <b>handling</b> <b>point</b> set model lies in how to interpret inter-point empty space and implicit quadric surfel is used in this research. This paper also explains how to utilize the quadric surfel for slicing the point set, so as to obtain contour curves for RP. Also described in this paper is how to extract smooth curve(s) out of the 2 D point cloud obtained by slicing the 3 D point set model...|$|E
40|$|Abstract. Landmarks {{are crucial}} for human wayfinding. Their {{integration}} in wayfinding assistance systems {{is essential for}} generating cognitively ergonomic route directions. I present an approach to automatically determining references to different types of landmarks. This approach exploits the circular order of a decision point’s branches. It allows uniformly <b>handling</b> <b>point</b> landmarks as well as linear and areal landmarks; these may be functionally relevant for a single decision point or a sequence of decision points. The approach is simple, yet powerful and can handle different spatial situations. It {{is an integral part}} of Guard, a process generating context-specific route directions that adapts wayfinding instructions to a route’s properties and environmental characteristics. Guard accounts for cognitive principles of good route directions; the resulting route directions reflect people’s conceptualization of route information. ...|$|E
40|$|Linear Programming has {{numerous}} applications, e. g., operations research, relaxations {{in global}} optimization, computational geometry. Recently {{it has been}} shown that many real world problems exhibit numerical difficulties due to ill-conditioning. Lurupa is a software package for computing rigorous optimal value bounds. The package can <b>handle</b> <b>point</b> and interval problems. Numerical experience with the Netlib lp library is given...|$|R
50|$|In 1998, Mike Tucker and Rob Ellis {{conducted}} an experiment at the University of Plymouth which expanded {{the concept of}} S-R compatibility to higher-order cognition. In their experiment, subjects were given two buttons, one on their left and one on their right, and shown a series of pictures of familiar objects like frying pans and teacups. For each image, {{they were asked to}} press the left button if the object in the image was upright and the right button if the object was inverted. However, the objects also varied in their rotation, such that the handles faced either left or right. The experiment revealed that seeing the <b>handle</b> <b>pointing</b> in one direction primed subjects to reach with the corresponding hand, which caused discrepancies in S-R compatibility that affected reaction time; for example, a subject seeing an inverted teapot with a <b>handle</b> <b>pointing</b> left took longer to press the button on the right than a subject who saw the same teapot pointing right.|$|R
40|$|Abstract – Behavioural {{models for}} RF devices are {{often based on}} single-tone data. However, this type of {{excitation}} is not representative of the digital modulation to which many RF devices are subjected in telecommunication systems. In this work, we show {{that the use of}} multisine excitation renders the modelling more efficient from both the experiment design and the data <b>handling</b> <b>points</b> of view, without loss of accuracy...|$|R
40|$|There {{are several}} data {{structures}} which can calculate the prefix sums of an array efficiently, while <b>handling</b> <b>point</b> updates on the array, such as Segment Trees and Binary Indexed Trees (BIT). Both these data structures {{can handle the}} these two operations (query and update) in O(n) time. In this paper, we present a data structure similar to the BIT, but with an even smaller constant. To do this, we use Zeckendorf's Theorem, a property of the Fibonacci sequence of numbers. The new data structure achieves the same complexity of O(n), but requires about _ϕ^ 2 n computations for the Query Operation {{as opposed to the}} _ 2 n computations required for a BIT Query Operation in the worst case. Comment: 7 pages, 2 figures, 2 graphs, 4 algorithm...|$|E
40|$|Multiresolution transforms, {{including}} a wavelet transform, {{are applied to}} image visualization, image restoration, filtering and compression, and object detection. Variance stabilization is used, when appropriate, to cater for common astronomical image noise models. We discuss validation of such methods {{in the case of}} astronomical image processing. A range of examples illustrate the effectiveness of this approach in <b>handling</b> <b>point</b> source and extended astronomical objects. 1 Introduction We describe a general framework for image analysis based on the following methodological components: multiresolution; noise separation; object support image; and Minkowski operators. Image analysis applications, most of which will be touched on below, include: restoration; noise filtering; compression; artifact filtering; object detection; and object description. General characteristics of astronomical images include: the presence of noise, to be addressed in the next paragraph; point sources (stars or [...] ...|$|E
40|$|The paper {{presents}} a single-pass, view-dependent method {{to solve the}} general rendering equation, using a combined finite element and random walk approach. Applying finite element techniques, the surfaces are decomposed into planar patches on which the radiance {{is assumed to be}} combined from finite number of unknown directional radiance functions by predefined positional basis functions. The directional radiance functions are then computed by random walk or by stochastic iteration using bundles of parallel rays. To compute the radiance transfer in a single direction, several global visibility methods are considered, including the global versions of the painter's, z-buffer, Weiler-Atherton's and planar graph based algorithms. The method requires no preprocessing except for <b>handling</b> <b>point</b> lightsources, for which a first-shot technique is proposed. The proposed method is particularly efficient for scenes including not very specular materials illuminated by large area lightsources o [...] ...|$|E
50|$|As {{with other}} blades in the Malay Archipelago, traditionally-made badik are {{believed}} to be imbued with a supernatural force during the time of their forging. The pamor in particular is said to affect its owner, bringing either well-being and prosperity or misfortune and poverty. Aside from being used as a weapon and hunting tool, the badik is a symbol of cultural identity in Sulawesi. As recently as the 1960s, the badik was worn as part of daily attire and badik crimes were reported regularly. In the colonial era, it was considered a pity if a man died without his badik. The Bugis and Makassar people still carry badik on ceremonial occasions today. The badik is worn on the right side, butt end of the <b>handle</b> <b>pointing</b> to the rear; it may also be positioned at their left side providing the butt end of the <b>handle</b> <b>points</b> to the rear. When the weapon is shifted from the right to the left side, or when worn at the left, handle reversed facing forward, it is signatory of impending combat.|$|R
50|$|The {{following}} {{pseudo code}} shows one means of implementing a quadtree which <b>handles</b> only <b>points.</b> There are other approaches available.|$|R
40|$|The thesis {{deals with}} the {{reorganization}} of production injection pump casing, when the existing production solution, using cutting lines, {{did not meet the}} requirements for flexible manufacturing. There are <b>handled</b> <b>points</b> leading to the replacement of the line of machining centres after a brief introduction of the company. Among the main points of thesis are the design of operations, choice of tools and cutting conditions, proposal clamping device and technology, techno-economic evaluation of the newly introduced production...|$|R
40|$|The {{management}} and query processing of one dimensional intervals {{is a special}} case of extended object handling. One dimensional intervals {{play an important role}} in temporal databases and they can also be used for fuzzy matching, fuzzy logic and measuring quality classes, etc. Most existing multidimensional access methods for extended objects do not address this special problem and most of them are main memory access methods that do not support e#cient access to secondary storage. The research in the application of the UB-Tree to extended objects is part of my doctoral work. The contribution of this article is a specific solution for managing and querying one dimensional intervals with the UB-Tree, a multidimensional extension of the classical B-Tree. The combination of UB-Tree and transformation of extended objects to parameter space is an e#ective solution for this specific problem. Keywords: one dimensional intervals, extended object <b>handling,</b> <b>point</b> query, range query, spatial data [...] ...|$|E
40|$|Scientific {{processes}} are usually time constrained with overall deadlines and local milestones. In scientific workflow systems, {{due to the}} dynamic nature of the underlying computing infrastructures such as grid and cloud, execution delays often take place and result in {{a large number of}} temporal violations. Since temporal violation handling is expensive in terms of both monetary costs and time overheads, an essential question aroused is “do we need to handle every temporal violation in scientific workflow systems? ” The answer would be “true ” according to existing works on workflow temporal management which adopt the philosophy similar to the handling of functional exceptions, that is, every temporal violation should be handled whenever it is detected. However, based on our observation, the phenomenon of self-recovery where execution delays can be automatically compensated for by the saved execution time of subsequent workflow activities has been entirely overlooked. Therefore, considering the nonfunctional nature of temporal violations, our answer is “not necessarily true. ” To take advantage of self-recovery, this article proposes a novel adaptive temporal violation <b>handling</b> <b>point</b> selection strategywhere this phenomenon is effectively utilised to avoid unnecessary temporal violation handling. Based on simulations of both real-world scientific workflows and randomly generated test cases, the experimental results demonstrate that our strategy can significantly reduce the cost on temporal violation handling by over 96 % while maintaining extreme low violation rate under norma...|$|E
40|$|Abstract. Real-world data {{is never}} perfect and can often suffer from corruptions (noise) that may impact {{interpretations of the}} data, models created from the data and {{decisions}} made based on the data. Noise can reduce system performance in terms of classification accuracy, time in building a classifier {{and the size of}} the classifier. Accordingly, most existing learning algorithms have integrated various approaches to enhance their learning abilities from noisy environments, but the existence of noise can still introduce serious negative impacts. A more reasonable solution might be to employ some preprocessing mechanisms to handle noisy instances before a learner is formed. Unfortunately, rare research has been conducted to systematically explore the impact of noise, especially from the noise <b>handling</b> <b>point</b> of view. This has made various noise processing techniques less significant, specifically when dealing with noise that is introduced in attributes. In this paper, we present a systematic evaluation on the effect of noise in machine learning. Instead of taking any unified theory of noise to evaluate the noise impacts, we differentiate noise into two categories: class noise and attribute noise, and analyze their impacts on the system performance separately. Because class noise has been widely addressed in existing research efforts, we concentrate on attribute noise. We investigate the relationship between attribute noise and classification accuracy, the impact of noise at different attributes, and possible solutions in handling attribute noise. Our conclusions can be used to guide interested readers to enhance data quality by designing various noise handling mechanisms...|$|E
5000|$|... simple Bouguer anomaly: {{downward}} reduction {{just by the}} Bouguer gradient (0.1967). This anomaly <b>handles</b> the <b>point</b> as if it {{is located}} on a flat plain.|$|R
40|$|We discuss {{interpolation}} {{methods for}} {{the reconstruction of}} the radiosity function across a patch. Two groups of methods are compared: One group based on regular grids and one based on hierarchical subdivisions. We <b>handle</b> <b>points</b> on hierarchical subdivisions as scattered data points which opens the field of scattered data interpolation. These different methods were implemented and characteristic images that show the (dis) advantages are discussed and compared. Additionally, we calculated errors in standard error measures. It shows that some scattered data interpolation methods produce acceptable images...|$|R
50|$|This sketch glosses {{over some}} things, {{such as how}} to <b>handle</b> fixed <b>points.</b> It turns out that more {{mappings}} and more sets are necessary to work around this.|$|R
40|$|Many {{traditional}} methods of holding and handling fish in small boats persist throughout Newfoundland today. Not only are these methods inefficient from the materials <b>handling</b> <b>point</b> of view but the resulting poor quality causes great waste of the dwindling resource and contributes to consumer non-acceptance of fish as a regular source of protein food. [...] A number of experiments on filleting cod, the principal inshore species are described. These experiments demonstrate the clean economic advantage of processing good quality raw material, and an incidental advantage, to the processor, of filleting dressed cod as compared with round (gut-in) cod. Further, the experiments show, as expected, the more rapid deterioration of quality in round cod compared with dressed cod. [...] The experimental results together with statistical reports of Environment Canada on annual cod landings and production are extrapolated to estimate annual losses resulting from processing and marketing poor quality fish of all species. A system of handling, discharging and transporting fish from inshore boats {{based on the principle}} of containerization is proposed as a solution to many of the problems of this fishery. [...] Flexible containers of net or plastic covered cloth for use in open boats and rigid containers for use in decked boats would be hoisted by a suitable shore-based facility, discharged into an elevated hopper and, after any necessary processing, culling or grading, placed in an insulated, covered container with ice for transportation to a processing plant. [...] A province-wide network of some 200 such systems would handle most inshore and near-offshore landings, at reasonable cost and with a much higher average level of product quality. [...] A case is made for joint involvement of Federal and Provincial Governments with industry to implement the proposed Province-wide system over a five year period...|$|E
40|$|Reverse {{logistics}} {{is getting}} more and more attention both from companies and the public. It is an area with the possibility for companies to do good {{in the eyes of the}} public. This work focus on the return system of a one-way handling material called the loading ledge. The loading ledge is developed by IKEA as a substitute for the EUR-pallet in transports from suppliers with a long distance and even flow and in transports from suppliers with short distance and uneven flow. The loading ledge is made out of 100 % recyclable plastic. The purpose of this work is to find the most efficient set-up for recycling of loading ledges within IKEA on a European level from a transport and <b>handling</b> <b>point</b> of view considering aspects like cost, environment, time and, from an administrative point of view, user-friendliness. Who will the participants of the set-up be, where will they be located, which transports are needed and which income and costs does the set-up generate, are questions that will be answered. An optimisation model of the problem is modelled using the modelling language AMPL and solved by CPLEX, a computer program for solving linear integer optimisation problems. The result of this work is five suggestions for the set-up found using the optimisation model. The set-ups differ in number of participants, transports and costs. Common for all set-ups is the use of a Polish supplier. Otherwise the set-ups differ depending on where the sorting is performed. When performed at the distributions central the set-ups uses Sweden and Poland as locations for recycling stations and when sorting is performed at the recycling station the set-ups uses Germany and Poland as locations for recycling stations. The set-ups are presented graphically along with suggestions for implementation. Key words: logistics, reverse logistics and optimisation...|$|E
40|$|Cloud {{computing}} is a latest market-oriented computing paradigm {{which can}} provide virtually unlimited scalable high performance computing resources. As {{a type of}} high-level middleware services for cloud computing, cloud workflow systems are a research frontier for both cloud computing and workflow technologies. Cloud workflows often underlie many large scale data/computation intensive e-science applications such as earthquake modelling, weather forecast and Astrophysics. At build-time modelling stage, these sophisticated processes are modelled or redesigned as cloud workflow specifications which normally contain the functional requirements for {{a large number of}} workflow activities and their non-functional requirements such as Quality of Service (QoS) constraints. At runtime execution stage, cloud workflow instances are executed by employing the supercomputing and data sharing ability of the underlying cloud computing infrastructures. In this thesis, we focus on scientific cloud workflow systems. In the real world, many scientific applications need to be time constrained, i. e. they are required to be completed by satisfying a set of temporal constraints such as local temporal constraints (milestones) and global temporal constraints (deadlines). Meanwhile, task execution time (or activity duration), as one of the basic measurements for system performance, often needs to be monitored and controlled by specific system management mechanisms. Therefore, how to ensure satisfactory temporal correctness (high temporal QoS), i. e. how to guarantee on-time completion of most, if not all, workflow applications, is a critical issue for enhancing the overall performance and usability of scientific cloud workflow systems. At present, workflow temporal verification is a key research area which focuses on time-constrained large-scale complex workflow applications in distributed high performance computing environments. However, existing studies mainly emphasise on monitoring and detection of temporal violations (i. e. violations of temporal constraints) at workflow runtime, there is still no comprehensive framework which can support the whole lifecycle of time-constrained workflow applications in order to achieve high temporal QoS. Meanwhile, cloud computing adopts a marketoriented resource model, i. e. cloud resources such as computing, storage and network are charged by their usage. Hence, the cost for supporting temporal QoS (including both time overheads and monetary cost) should be managed effectively in scientific cloud workflow systems. This thesis proposes a novel probabilistic temporal framework and its strategies for cost-effective delivery of high QoS in scientific cloud workflow systems (or temporal framework for short in this thesis). By investigating the limitations of conventional temporal QoS related research, our temporal framework can provide a systematic and cost-effective support for time-constrained scientific cloud workflow applications over their whole lifecycles. With a probability based temporal consistency model, there are three major components in the temporal framework: Component 1 – temporal constraint setting; Component 2 – temporal consistency monitoring; Component 3 – temporal violation handling. Based on the investigation and analysis, we propose some new concepts and develop a set of innovative strategies and algorithms towards cost-effective delivery of high temporal QoS in scientific cloud workflow systems. Case study, comparisons, quantitative evaluations and/or mathematical proofs are presented for the evaluation of each component. These demonstrate that our new concepts, innovative strategies and algorithms for the temporal framework can significantly reduce the cost for the detection and handling of temporal violations while achieving high temporal QoS in scientific cloud workflow systems. Specifically, at scientific cloud workflow build time, in Component 1, a statistical time-series pattern based forecasting strategy is first conducted to predict accurate duration intervals of scientific cloud workflow activities. Afterwards, based on the weighted joint normal distribution of workflow activity durations, a probabilistic setting strategy is applied to assign coarse-grained temporal constraints through a negotiation process between service users and service providers, and then fine-grained temporal constraints can be propagated along scientific cloud workflows in an automatic fashion. At scientific cloud workflow runtime, in Component 2, the state of scientific cloud workflow execution towards specific temporal constraints, i. e. temporal consistency, is monitored constantly with the following two steps: first, a minimum probability time redundancy based temporal checkpoint selection strategy determines the workflow activities where potential temporal violations take place; second, according to the probability based temporal consistency model, temporal verification is conducted on the selected checkpoints to check the current temporal consistency states and the type of temporal violations. In Component 3, detected temporal violations are handled with the following two steps: first, an adaptive temporal violation <b>handling</b> <b>point</b> selection strategy decides whether a temporal checkpoint should be selected as a temporal violation <b>handling</b> <b>point</b> to trigger temporal violation handling strategies; Second, at temporal violation handling points, different temporal violation handling strategies are executed accordingly to tackle different types of temporal violations. In our temporal framework, we focus on metaheuristics based workflow rescheduling strategies for handling statistically recoverable temporal violations. The major contributions of this research are that we have proposed a novel comprehensive temporal framework which consists of a set of new concepts, innovative strategies and algorithms for supporting time-constrained scientific applications over their whole lifecycles in cloud workflow systems. With these, we can significantly reduce the cost for detection and handling of temporal violations whilst delivering high temporal QoS in scientific cloud workflow systems. This would eventually improve the overall performance and usability of cloud workflow systems because a temporal framework {{can be viewed as a}} software service for cloud workflow systems. Consequently, by deploying the new concepts, innovative strategies and algorithms, scientific cloud workflow systems would be able to better support large-scale sophisticated e-science applications in the context of cloud economy...|$|E
40|$|Abstract. We {{introduce}} a new surface deformation and modeling method in this paper. Referring to the swept volume generation, the surface is pulled or pushed along a trajectory curve. The key point is the sweeping function. Surface points are moved to {{where they should be}} during sweeping operations according to the global parameter, which is determined by topological distance. An index factor controls how much the surface deforms around the <b>handle</b> <b>point.</b> The proposed method is easy to extend to fit different applications such as various constraints, local deformation and animations. ...|$|R
5000|$|Although barycentric {{coordinates}} are {{most commonly used}} to <b>handle</b> <b>points</b> inside a triangle, {{they can also be}} used to describe a point outside the triangle. If the point is not inside the triangle, then we can still use the formulas above to compute the {{barycentric coordinates}}. However, since the point is outside the triangle, at least one of the coordinates will violate our original assumption that [...] In fact, given any point in cartesian coordinates, we can use this fact to determine where this point is with respect to a triangle.|$|R
5000|$|To prove formal {{properties}} on the code. Using specifications {{written in}} ANSI/ISO C Specification Language enables it to ensure {{properties of the}} code for any possible behavior. Frama-C <b>handles</b> floating <b>point</b> numbers.|$|R
40|$|Watershed {{development}} programs in India {{have played a}} significant role in improving the livelihoods of the rural communities living in rainfed areas. Current assessments are limited in assessing interrelated impacts as the watershed development is influenced by multi domain areas. Few studies have reported on the novel ICT techniques being used for watershed assessment with actual watershed data or examined the spatial or temporal variations in the watershed. The objective of the research was to study current novel geospatial and data mining methods used in hydrological assessments of watershed development and to apply the identified novel methods on a real-time watershed data. The following major research question has been addressed by the research study: “Can novel geospatial and data mining methods be effectively used to assess the hydrological impacts of watershed development? In order to answer this question, the research was carried out in a number of phases to examine existing ICT techniques utilised for impact assessment of the watershed area. The research methodology used in this study was a mixed method approach based on case study, diagnostic research and quantitative approach. Two contiguous watersheds in rainfed region of Andhra Pradesh, India was chosen as study area. Data sets were sourced from a number of Government and NGO agencies and field visits. Data representing sixteen parameters of hydrological, environmental and social factors which were known to influence watersheds were chosen for the study. The data consisted of both spatial and spatio-temporal data. A grid of 2880 cells covering the study area was developed. Data for the period 2006 to 2008 in two seasons (pre-monsoon and post-monsoon) were collected, compiled, classified and assigned to the grid network database. The study area was divided into three zones viz., upstream, midstream and downstream. The data underwent preprocessing in order to make it suitable for further data analysis. This included watershed delineation, creating grid network, <b>handling</b> <b>point</b> data, line data and polygon data, and formatting data into unified format. The data was converted into nominal classes to be utilised for data mining. The watershed data set was analysed using descriptive statistics, geospatial and data mining approach. The first analysis used descriptive statistics based on univariate analysis using pivot tables wizard. This analysis used all sixteen watershed parameters. A series of different scenarios for soils, ground water levels, landuse and checkdams were examined. The second approach was a geospatial analysis which used optimised hot spot analysis. The analysis used NDVI, ground water levels data as the input parameters. The data was examined in relation to landuse and location of checkdams. The third approach employed spatial data mining techniques by using DBSCAN clustering and Apriori rule based association rule mining techniques on watershed data. The analysis used fourteen spatio-temporal parameters. The output from the analysis was visualised using a GIS environment. A comparison of the results from the three approaches showed that all the three approaches provided some insight into the understanding of factors influencing the watershed development. The descriptive statistics provided a simple analysis of trends of the parameters. It was limited in its ability to show the interrelationships between parameters. The geospatial analysis of the watershed area was useful in understanding the spatial and temporal trends across the watershed area. This analysis can only be used for spatial data with numeric values. The data mining analysis of the watershed area was useful in understanding previously hidden relationships between the parameters influencing the watershed area. This analysis could be used for both spatial and spatio-temporal data analysis. The results obtained through each analysis approach require some expertise to interrogate the effects of changes in the watershed area. The relationships are complex and interrelationships are influencing the effects of parameters. Variation was found in the granularity of the outputs of each approach. It is evident that a combination of the approaches provided the capability to investigate these from general data trends to complex data analysis. Validation of the approaches was made with a similar study carried out by ACIAR funded project. Some validation could be made of the findings from this thesis with the ACIAR based studies. The importance of factors such as groundwater level, watershed zone and rainfall was noted in both studies. Although the ACIAR research was conducted in similar study area, it was limited in its analysis of the effects of upstream/downstream interactions and did not study on the integration of multiple parameters in a robust manner. The research was considered novel in the integration of three different approaches for watershed impact assessment utilising hydrological, socio and environmental parameters for a contiguous watershed data with a spatial and temporal analysis. It was also novel in that it proposed hybrid method of utilising Geospatial analysis and data mining methods together and visualising the output of data mining in a GIS environment. This research proposed a novel integrated technology based framework for impact assessment which comprises datasets, processing, analysis and results components. This framework could be used to develop it as a decision support tool to assess the impacts of watershed development to assist researchers and planners to provide unbiased assessment of the impact of the watershed development from a range of perspectives. The framework can be used at different spatial and temporal scales...|$|E
40|$|We {{present an}} anytime-capable fast {{deterministic}} greedy algorithm for smoothing piecewise linear paths consisting of connected linear segments. With this method, path points {{with only a}} small influence on path geometry (i. e. aligned or nearly aligned points) are successively removed. Due to the removal of less important path points, the computational and memory requirements of the paths are reduced and traversing the path is accelerated. Our algorithm {{can be used in}} many different applications, e. g. sweeping, path finding, programming-by-demonstration in a virtual environment, or 6 D CNC milling. The algorithm <b>handles</b> <b>points</b> with positional and orientational coordinates of arbitrary dimension...|$|R
50|$|As {{with other}} blades in the Malay Archipelago, traditionally-made badik are {{believed}} to be imbued with a supernatural force during the time of their forging. The pamor in particular is said to affect its owner, bringing either well-being and prosperity or misfortune and poverty. Aside from being used as a weapon and hunting tool, the badik is a symbol of cultural identity in Sulawesi. The Bugis and Makassar people still carry badik as part of their daily attire. The badik is worn on the right side, with the butt end of the <b>handle</b> <b>pointing</b> to the rear.|$|R
40|$|Skinning using <b>point</b> <b>handles</b> has {{experimentally}} shown {{its effectiveness}} for stretching, twisting, and supple deformations [Jacobson et al. 2011] which {{are difficult to}} achieve using rigid bones. However, <b>point</b> <b>handles</b> are much less effective for limbs bending since their influence weights vary over the skin mesh. This poster presents an efficient scheme, which expands the space of deformations possible to the <b>point</b> <b>handles</b> by supporting rigid bending using a rigidity energy minimization framework. Thus, 3 D points that are much easier to design while provide larger space of deformations than a skeleton {{can be used as}} alternative handles in character skinning...|$|R
40|$|In this paper, a blind space-time {{receiver}} {{is proposed}} to <b>handle</b> <b>point</b> and diffused sources for asynchronous multipath DS-CDMA systems. The receiver {{is based on}} a computationally efficient subspace-type algorithm for its joint space-time channel estimation which is insusceptible to near-far problems. The coherency between the sources is removed by a novel temporal smoothing technique operating in the transformed domain. Unlike many conventional DS-CDMA receivers, the proposed formulation and approach is applicable even with the presence of co-code interferers. Furthermore it is robust to channel estimation errors in the event of any unidentified (incomplete) or erroneous (incorrect) channel parameter...|$|R
50|$|The game Spoons can {{be played}} with more players, using one or more decks of 52 {{ordinary}} playing cards {{and a number of}} spoons totalling one fewer than the number of players. Even though the game is famous for the usage of spoons, virtually anything can be used. The spoons are placed {{in the center of the}} table in a circle with <b>handles</b> <b>pointing</b> outward so that they may be easily grabbed by any of the players. One person is designated first dealer and deals four cards to each player. The dealer will use the remaining cards to draw from.|$|R
