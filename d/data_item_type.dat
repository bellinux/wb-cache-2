0|5578|Public
40|$|This paper {{presents}} an extended update functionality in temporal databases, {{as a vehicle}} for supporting simultaneous values semantics. A situation of simultaneous values of a single <b>data</b> <b>item</b> occurs when multiple values that are valid at the same time were assigned to a <b>data</b> <b>item</b> over the database history. In conventional temporal database models there is a fixed semantics to handle such a case; if the <b>data</b> <b>item's</b> <b>type</b> is an atom, then only one value is assigned, usually {{on the basis of the}} update time; if the <b>data</b> <b>item's</b> <b>type</b> allows for multiple values, then all the values persist. We show that various types of applications require support for different forms of simultaneous values semantics. The support of simultaneous values is achieved using an extended set of operation types, that consists of the conventional update operation types (insert, modify and delete) as well as additional types. The semantics of the update operation types is given, as well as the correctness of the model [...] . ...|$|R
2500|$|... and A'i and t'i are {{the number}} of <b>data</b> <b>items</b> of <b>type</b> A and {{the total number of}} items in the ith sample.|$|R
40|$|Hierarchical {{data sets}} often include {{relations}} indicating natural or imposed dependencies between <b>data</b> <b>items.</b> Both <b>types</b> of relations, hierarchical and other, are often {{needed to understand}} the data. We present, ARCTREES, a novel way of visualizing hierarchical and non-hierarchical relations within one interactive visualization. An initial user study is described wherein ARCTREES compared favorably to the traditional Treemap visualization...|$|R
5000|$|The most {{important}} {{difference between a}} file system and WinFS is that WinFS knows the <b>type</b> of each <b>data</b> <b>item</b> that it stores. And the type specifies {{the properties of the}} <b>data</b> <b>item.</b> The WinFS <b>type</b> system is closely associated with the [...]NET framework’s concept of classes and inheritance. A new type can be created by extending and nesting any predefined types.|$|R
40|$|AbstractA {{probabilistic}} model and a software implementation {{have been developed}} to aid in finding missing persons and in related applications. The method can be applied generally to find most probable correspondences between two sets of imprecisely described objects. These can be descriptions of illnesses vs patients (diagnoses), job offerings vs job applicants, special tasks vs a personnel file (task assignment problem), etc. Every object of the two sets is described by a collection of data, a significant part of which can be erroneous, unreliable, imprecise or given in several contradicting versions. Among the parameters of the method is the following information about each of the <b>data</b> <b>item</b> <b>types</b> and of some of their possible combinations (the parametric information does not depend on the actual data) : its logical characteristics, its importance relatively to other <b>types</b> of <b>data</b> <b>items,</b> the meaning and the relative degrees of kinship between values of this <b>data</b> <b>item</b> for two objects to be compared (e. g. kinship of equal values: phonetic kinship; numeric kinship, whose degree is proportional to the inverse of arithmetic difference between the values; matrix of kinship degrees defined for possible pairs of values), interpretation of multiplicity of values for this <b>data</b> <b>item</b> for one object, the a priori probability of <b>data</b> <b>item's</b> correctness (in addition, the probability of any value for any object can provided in a set of objects' descriptions by an investigator who gathers the actual data), etc. A straightforward implementation of the method by software would result in unfeasible time complexity for large sets of objects. Therefore special algorithms have been designed to preprocess the sets of descriptions so that the time of matching-finding is reduced byan order of magnitude while the probabilistic output remains unaltered...|$|R
50|$|The first {{aspects of}} the second phase were {{deployed}} on 4 February 2013, introducing statements to Wikidata entries. The values were initially limited to two <b>data</b> <b>types</b> (<b>items</b> and images on Wikimedia Commons), with more data types (such as coordinates and dates) to follow later. The first new type, string, was deployed on 6 March.|$|R
40|$|Description Generates design matrix for {{analysing}} real {{paired comparison}}s and derived paired comparison <b>data</b> (Likert <b>type</b> <b>items</b> / ratings or rankings) using a loglinear approach. Fits loglinear Bradley-Terry model (LLBT) exploting an eliminate feature. Computes pattern models for paired comparisons, rankings, and ratings. Some treatment of missing values (MCAR and MNAR). Fits latent class (mixture) models for paired comparison, rating and ranking patterns using a nonparametric ML approach...|$|R
40|$|Novice Java programmers face great {{difficulties}} when {{learning to}} design unit tests for any nontrivial cases. Deciding whether {{the result of}} a method, or the effect the method produced represents the expected result one must understand the difference between equality based on the values an object represents versus the reference equality (identity) — and be able to define the correct equals method. We describe the tester library that supports test design, evaluation, and reporting of the results in the manner that supports a novice programmer. The library uses Java reflection and annotation to compare any two <b>data</b> <b>items</b> (primitive <b>types</b> or objects) by the value they represent, produces report where the expected and actual values are prettyprinted, and a failed test report includes a link to the failed test. The library has been used in classrooms and is used daily in our program design...|$|R
40|$|We {{discuss the}} aspect of {{synchronisation}} in the language design {{and implementation of the}} asynchronous data flow language S-Net. Synchronisation is a crucial aspect of any coordination approach. S-Net provides a particularly simple construct, the synchrocell. As a primitive S-Net language construct synchrocell implements a one-off synchronisation of two <b>data</b> <b>items</b> of different <b>type</b> on a stream of such <b>data</b> <b>items.</b> We believe this semantics captures the essence of synchronisation, and no simpler design is possible. While the exact built-in behaviour as such is typically not what is required by S-Net application programmers, we show that in conjunction with other language features S-Net synchrocells meet typical demands for synchronisation in streaming networks quite well. Moreover, we argue that their simplistic design, in fact, is a necessary prerequisite to implement an even more interesting scenario: modelling state in streaming networks of stateless components. We finish with the outline of an efficient implementation by the S-Net runtime system...|$|R
40|$|OBJECTIVE: To {{estimate}} the coverage provided by SNOMED CT for clinical research concepts {{represented by the}} items on case report forms (CRFs), {{as well as the}} semantic nature of those concepts relevant to post-coordination methods. DESIGN: Convenience samples from CRFs developed by rheumatologists conducting several longitudinal, observational studies of vasculitis were selected. A total of 17 CRFs were used as the basis of analysis for this study, from which a total set of 616 (unique) items were identified. Each unique <b>data</b> <b>item</b> was classified as either a clinical finding or procedure. The items were coded by the presence and nature of SNOMED CT coverage and classified into semantic types by 2 coders. MEASUREMENTS: Basic frequency analysis was conducted to determine levels of coverage provided by SNOMED CT. Estimates of coverage by various semantic characterizations were estimated. RESULTS: Most of the core clinical concepts (88 %) from these clinical research <b>data</b> <b>items</b> were covered by SNOMED CT; however, far fewer of the concepts were fully covered (that is, where all aspects of the CRF item could be represented completely without post-coordination; 23 %). In addition, a large majority of the concepts (83 %) required post-coordination, either to clarify context (e. g., time) or to better capture complex clinical concepts (e. g., disease-related findings). For just over one third of the sampled CRF <b>data</b> <b>items,</b> both <b>types</b> of post-coordination were necessary to fully represent the meaning of the item. CONCLUSION: SNOMED CT appears well-suited for representing a variety of clinical concepts, yet is less suited for representing the full amount of information collected on CRFs...|$|R
40|$|Medical {{forms are}} very heterogeneous: on a European scale there are {{thousands}} of <b>data</b> <b>items</b> in several hundred different systems. To enable data exchange for clinical care and research purposes {{there is a need to}} develop interoperable documentation systems with harmonized forms for data capture. A prerequisite in this harmonization process is comparison of forms. So far – to our knowledge – an automated method for comparison of medical forms is not available. A form contains a list of <b>data</b> <b>items</b> with corresponding medical concepts. An automatic comparison needs <b>data</b> <b>types,</b> <b>item</b> names and especially item with these unique concept codes from medical terminologies. The scope of the proposed method is a comparison of these items by comparing their concept codes (coded in UMLS). Each <b>data</b> <b>item</b> is represented by item name, concept code and value domain. Two items are called identical, if item name, concept code and value domain are the same. Two items are called matching, if only concept code and value domain are the same. Two items are called similar, if their concept codes are the same, but the value domains are different. Based on these definitions an open-source implementation for automated comparison of medical forms in ODM format with UMLS-based semantic annotations was developed. It is available as package compareODM fro...|$|R
40|$|The Abstract Task Graph (ATaG) is a {{data driven}} {{programming}} model for end-to-end application development on networked sensor systems. An ATaG {{program is a}} system-level, architecture-independent specification of the application functionality. The application is modeled {{as a set of}} abstract tasks that represent types of information processing functions in the system, and a set of abstract <b>data</b> <b>items</b> that represent <b>types</b> of information exchanged between abstract tasks. Input and output relationships between abstract tasks and <b>data</b> <b>items</b> are explicitly indicated as channels. Each abstract task is associated with user-provided code that implements the actual information processing functions in the system. Appropriate numbers and types of tasks can then be instantiated at compile-time or run-time to match the actual hardware and network configuration, with each node incorporating the user-provided code, automatically generated glue code, and a runtime engine that manages all coordination and communication in the network. This paper primarily deals with the key concepts of ATaG and the program syntax and semantics. The end-to-end application development methodology is discussed briefly. ...|$|R
40|$|Several query {{languages}} {{have been}} proposed for managing data streams in modern monitoring applications. Continuous queries expressed in these languages usually employ windowing constructs in order to extract finite portions of the potentially unbounded stream. Explicitly or not, window specifications rely on ordering. Usually, timestamps are attached to all tuples flowing into {{the system as a}} means to provide ordered access to <b>data</b> <b>items.</b> Several window <b>types</b> have been implemented in stream prototype systems, but a precise definition of their semantics is still lacking. In this paper, we describe a formal framework for expressing windows in continuous queries over data streams. After classifying windows according to their basic characteristics, we give algebraic expressions for the most significant window types commonly appearing in applications. As an essential step towards a stream algebra, we then propose formal definitions for the windowed analogs of typical relational operators, such as join, union or aggregation, and we identify several properties useful to query optimization. © IFIP International Federation for Information Processing 2006...|$|R
40|$|Abstract. Several query {{languages}} {{have been}} proposed for managing data streams in modern monitoring applications. Continuous queries ex-pressed in these languages usually employ windowing constructs in order to extract finite portions of the potentially unbounded stream. Explicitly or not, window specifications rely on ordering. Usually, timestamps are attached to all tuples flowing into {{the system as a}} means to provide or-dered access to <b>data</b> <b>items.</b> Several window <b>types</b> have been implemented in stream prototype systems, but a precise definition of their semantics is still lacking. In this paper, we describe a formal framework for express-ing windows in continuous queries over data streams. After classifying windows according to their basic characteristics, we give algebraic ex-pressions for the most significant window types commonly appearing in applications. As an essential step towards a stream algebra, we then pro-pose formal definitions for the windowed analogs of typical relational operators, such as join, union or aggregation, and we identify several properties useful to query optimization. ...|$|R
40|$|Interactive {{multimedia}} presentations are {{an essential}} issue in many advanced multimedia application tools. Before presenting multimedia <b>data,</b> media <b>items,</b> interaction <b>types</b> and synchronization constraints {{have to be}} specified in a multimedia document. This paper identifies and classifies the temporal interaction types in multimedia systems, and shows {{their impact on the}} specification process and the supporting system. Then, we describe how to specify the interaction types by using the standardized multimedia document language HyTime. The HyTime mechanisms are demonstrated by examples followed by a discussion of the advantages and limits of each technique. 1 Introduction With the emerging multimedia technologies, more and more application tools are developed to process and present multimedia data. Generally, multimedia data are stored as multimedia or hypermedia documents using a proprietary format [Appl 91], [BuZe 93], [BHL 91], [LiGh 90]. The result is that tools of different vendors or [...] ...|$|R
40|$|Pearson Test of English Academic (PTE Academic) has six <b>item</b> <b>types</b> that assess {{academic}} writing either independently or integratively. This research {{focuses on}} evaluating the construct validity and effectiveness of the six writing <b>item</b> <b>types.</b> Exploratory Factor Analysis was performed to examine the underlying writing constructs {{as measured by the}} six <b>item</b> <b>types.</b> <b>Item</b> scores for different writing skills were subjected to Rasch IRT analysis. The difficulty of the <b>item</b> <b>types</b> was estimated and the effectiveness of each <b>item</b> <b>type</b> was evaluated by calculating the information function of each one. The results identified two writing constructs: an Analytical/Local Writing construct and a Synthetic/Global Writing construct. The study has implications for test developers on the use of multiple <b>item</b> <b>types</b> and their effectiveness, and for test users on how they can improve their writing skills...|$|R
40|$|A {{method of}} {{representing}} {{a group of}} <b>data</b> <b>items</b> comprises, for each of a plurality of <b>data</b> <b>items</b> in the group, determining the similarity between said <b>data</b> <b>item</b> and each of a plurality of other <b>data</b> <b>items</b> in the group, assigning a rank to each pair {{on the basis of}} similarity, wherein the ranked similarity values for each of said plurality of <b>data</b> <b>items</b> are associated to reflect the overall relative similarities of <b>data</b> <b>items</b> in the group...|$|R
50|$|In a list, {{the order}} of <b>data</b> <b>items</b> is significant. Duplicate <b>data</b> <b>items</b> are permitted. Examples of {{operations}} on lists are searching for a <b>data</b> <b>item</b> in the list and determining its location (if it is present), removing a <b>data</b> <b>item</b> from the list, adding a <b>data</b> <b>item</b> to the list at a specific location, etc. If the principal operations on the list are to be the addition of <b>data</b> <b>items</b> {{at one end and}} the removal of <b>data</b> <b>items</b> at the other, it will generally be called a queue or FIFO. If the principal operations are the addition and removal of <b>data</b> <b>items</b> at just one end, it will be called a stack or LIFO. In both cases, <b>data</b> <b>items</b> are maintained within the collection in the same order (unless they are removed and re-inserted somewhere else) and so these are special cases of the list collection. Other specialized operations on lists include sorting, where, again, {{the order of}} <b>data</b> <b>items</b> is of great importance.|$|R
50|$|In COBOL, union <b>data</b> <b>items</b> {{are defined}} in two ways. The first uses the RENAMES (66 level) keyword, which {{effectively}} maps a second alphanumeric <b>data</b> <b>item</b> {{on top of}} the same memory location as a preceding <b>data</b> <b>item.</b> In the example code below, <b>data</b> <b>item</b> PERSON-REC is defined as a group containing another group and a numeric <b>data</b> <b>item.</b> PERSON-DATA is defined as an alphanumeric <b>data</b> <b>item</b> that renames PERSON-REC, treating the data bytes continued within it as character data.|$|R
40|$|Three new computer-administered <b>item</b> <b>types</b> for the {{analytical}} {{scale of the}} Graduate Record Examination (GRE) General Test were developed and evaluated. One <b>item</b> <b>type</b> was a free-response version of the current analytical reasoning <b>item</b> <b>type.</b> The second <b>item</b> <b>type</b> was a somewhat constrained free-response version of the pattern identification (or number series) <b>item</b> <b>type</b> in which the student had to state the rule that generated the series. The third <b>item</b> <b>type</b> used the computer to administer yes/no analysis of explanation questions with a limited branching strategy. The computer tests were administered at four Educational Testing Service regional offices {{to a sample of}} students who had previously taken the GRE General Test. Scores from the regular GRE administration and the special computer administration were matched for a sample of 349 students. A number of test administration design issues wer...|$|R
5000|$|The {{technical}} specification for Gopher, RFC 1436, defines 14 <b>item</b> <b>types.</b> A one-character code indicates {{what kind of}} content the client should expect. <b>Item</b> <b>type</b> [...] is an error code for exception handling. Gopher client authors improvized <b>item</b> <b>types</b> [...] (HTML), [...] (informational message), and [...] (sound file) {{after the publication of}} RFC 1436.|$|R
40|$|This study {{compares the}} {{contribution}} to measurement {{accuracy of the}} verbal score of each of four verbal <b>item</b> <b>types</b> included in the Graduate Record Examinations (GRE) General Test. Comparisons are based on item response theory, a methodology that allows the researcher {{to look at the}} accuracy of individual points on the score scale. This methodology {{is based on the assumption}} that the four verbal <b>item</b> <b>types</b> measure the same verbal ability. Since the results of the study do indicate that the reading comprehension <b>item</b> <b>type</b> measures something slightly different from what is measured by sentence completion, analogy, or antonym <b>item</b> <b>types,</b> only tentative conclusions may be drawn. The antonym item typo contributes the most accuracy of the four <b>item</b> <b>types</b> for scores above about 550. Analogy items contribute to the measurement accuracy of verbal ability throughout the score range. This is especially true when <b>item</b> <b>type...</b>|$|R
40|$|The use of item-ability {{regressions}} (the {{comparison of}} the regression of the observed proportion of people answering an item correctly on estimated &thetas; with the estimated item response function) to investigate {{the psychometric properties of}} particular <b>item</b> <b>types</b> in a given population was explored using data from four administrations of 10 <b>item</b> <b>types</b> (a total of 806 items) from the Graduate Record Examinations General Test. Although the method does not allow an absolute determination of fit for a latent trait model (in this case, for the three-parameter logistic model), it does show that certain <b>item</b> <b>types</b> consistently fit the model worse than other <b>item</b> <b>types,</b> and it led to and supported a specific hypothesis as to why the model probably did not fit these <b>item</b> <b>types...</b>|$|R
40|$|To improve data {{accessibility}} in ad hoc networks, {{in our previous}} work we proposed three methods of replicating <b>data</b> <b>items</b> by considering the data access frequencies from mobile nodes to each <b>data</b> <b>item</b> and the network topology. In this paper, we extend our previously proposed methods to consider the correlation among <b>data</b> <b>items.</b> Under these extended methods, the data priority of each <b>data</b> <b>item</b> is defined based on the correlation among <b>data</b> <b>items,</b> and <b>data</b> <b>items</b> are replicated at mobile nodes with the data priority. We employ simulations {{to show that the}} extended methods are more efficient than the original ones. ...|$|R
50|$|In a tree, {{which is}} {{a special kind of}} graph, a root <b>data</b> <b>item</b> has {{associated}} with it some number of <b>data</b> <b>items</b> which in turn have associated with them some number of other <b>data</b> <b>items</b> in what is frequently viewed as a parent-child relationship. Every <b>data</b> <b>item</b> (other than the root) has a single parent (the root has no parent) and some number of children, possibly zero. Examples of operations on trees are the addition of <b>data</b> <b>items</b> so as to maintain a specific property of the tree to perform sorting, etc. and traversals to visit <b>data</b> <b>items</b> in a specific sequence.|$|R
40|$|Railway {{distributed}} system integration needs to realize information exchange, resources sharing and coordination process across fields, departments and application systems. And railway data integration {{is essential to}} implement this integration. In order to resolve the problem of heterogeneity of data models among data sources of different railway operation systems, this paper has brought forth the concept of metadata dictionary that abstracts related information and information content of each data source. Based on the metadata dictionary, this paper presents a novel integration data model of spatial structure, a XML-oriented 3 -dimension common data model. The proposed model accommodates both the flexibility of level relationship and syntax expression in data integration. In this model, a spatial data pattern is used to describe and express the characteristic relationship of <b>data</b> <b>items</b> among all <b>types</b> of data. Based on the data model with rooted directed graph {{and the organization of}} level as well as the flexibility of the expression, the model can represent the mapping between different data models, including relationship model and object-oriented model. A consistent concept and algebraic description of the data set is given to function as the metadata in data integration, so that the algebraic manipulation of data integration is standardized to support the data integration of {{distributed system}}. </p...|$|R
40|$|Many exam {{programs}} {{have begun to}} include innovative <b>item</b> <b>types</b> in their operational assessments. While innovative <b>item</b> <b>types</b> appear to have great promise for expanding measurement, there can also be genuine challenges to their successful implementation. In this paper we present a set of four activities that can be beneficially incorporated into the design and development of innovative <b>item</b> <b>types.</b> These tasks are: template design, item writing guidelines, item writer training, and usability studies. When these four tasks are fully incorporated in the test development process then the potential for improved measurement through innovative <b>item</b> <b>types</b> is much greater...|$|R
40|$|Abstract. 4 -ary vector {{expression}} {{was defined by}} 3 -ary vector to describe subject and <b>data</b> <b>item</b> in dataspace. Association method between subject and <b>data</b> <b>item</b> was represented. Correlation of <b>data</b> <b>item</b> was defined by 4 -ary vector. Correlation and association way between <b>data</b> <b>items</b> were represented by 4 -ary vector. The validity of these methods was verified in technical document library...|$|R
40|$|The {{aim of this}} {{research}} was to identify, develop, and evaluate empirically new reasoning <b>item</b> <b>types</b> that might be used to broaden the analytical measure of the Graduate Record Examinations (GRE) General Test and to strengthen its construct validity. Six <b>item</b> <b>types</b> were selected for empirical evaluation, including the two currently used in the GRE analytical measure. Two experimental batteries, one using a three-option format and the other, a multiple yes-no format, were administered to 2 samples of approximately 370 examinees each. Item analyses and analyses of sex differences, criterion- related validity, and relationships of the experimental <b>item</b> <b>types</b> to the current GRE measures were conducted. All but one of the experimental <b>item</b> <b>types</b> exhibited promise for strengthening the GRE analytical measure, and even the one exception appeared to be a possible <b>item</b> <b>type</b> for the GRE verbal measure. Different combinations of the <b>item</b> <b>types</b> were evaluated in a series of confirmatory factor analyses, supplemented by correlational analyses and an exploratory factor analysis. The study also provided evidence that the reasoning domain consists of two major subdomains: informal reasoning and formal-deductive reasoning. Nineteen tables present analysis results. Three appendixes give examples of the experimental <b>item</b> <b>types,</b> list participating test centers, and present correlation matrices. Appendix C contains eight tables. (Contains 41 references. ...|$|R
5000|$|The [...] "Point" [...] message defines two {{mandatory}} <b>data</b> <b>items,</b> x and y. The <b>data</b> <b>item</b> {{label is}} optional. Each <b>data</b> <b>item</b> has a tag. The tag is defined after the equal sign. For example, x has the tag 1.|$|R
50|$|In a multiset (or bag), {{like in a}} set, {{the order}} of <b>data</b> <b>items</b> does not matter, {{but in this case}} {{duplicate}} <b>data</b> <b>items</b> are permitted. Examples of operations on multisets are the addition and removal of <b>data</b> <b>items</b> and determining how many duplicates of a particular <b>data</b> <b>item</b> are present in the multiset. Multisets can be transformed into lists by the action of sorting.|$|R
30|$|We {{found that}} the {{proposed}} algorithm is presently in the broadcast structure. The wireless broadcast scheduling has been considered the <b>data</b> <b>item</b> frequency of the fixed and it has an unreasonable supposition. The <b>data</b> <b>item</b> frequency would be {{the request of the}} client for a change under the factual dynamic environments. Each of the <b>data</b> <b>item</b> has a frequency value itself and the each frequency of <b>data</b> <b>item</b> should been computed for its weight value and adjusted for dynamic broadcast adaptive so the frequency of <b>data</b> <b>item</b> has no fixed probability value.|$|R
50|$|In a set, {{the order}} of <b>data</b> <b>items</b> does not matter (or is undefined) but {{duplicate}} <b>data</b> <b>items</b> are not permitted. Examples of operations on sets are the addition and removal of <b>data</b> <b>items</b> and searching for a <b>data</b> <b>item</b> in the set. Some languages support sets directly. In others, sets can be implemented by a hash table with dummy values; only the keys are used in representing the set.|$|R
30|$|Focus {{has been}} put on {{maintaining}} flexibility in terms of applied testing strategies and <b>item</b> <b>types.</b> The ability to administer arbitrary <b>item</b> <b>types</b> in particular is a feature that sets apart our approach from available related work. In the following section, we thus focus on demonstrating the flexibility of the platform in terms of administering <b>item</b> <b>types</b> that differ in content presentation as well as form and amount of user interaction required when providing answers.|$|R
500|$|<b>Data</b> <b>items</b> in COBOL are {{declared}} hierarchically {{through the}} use of level-numbers which indicate if a <b>data</b> <b>item</b> is part of another. An item with a higher level-number is subordinate to an item with a lower one. Top-level <b>data</b> <b>items,</b> with a level-number of 1, are called [...] Items that have subordinate aggregate data are called those that do not are called [...] Level-numbers used to describe standard <b>data</b> <b>items</b> are between 1 and 49.|$|R
40|$|This thesis proposes new methods, {{based on}} dynamic programming, for solving certain {{single-stage}} and multi-stage integer stochastic knapsack problems. These problems model stochastic portfolio optimization problems (SPOPs) which assume deterministic unit weight, and normally distributed unit return with known mean and variance for each <b>item</b> <b>type.</b> Given an initial wealth, {{the objective is}} to select a portfolio that maximizes the probability of achieving or exceeding a specified final return threshold; the multi-stage problem allows revisions of the portfolio at regular time intervals. An exact method is developed to solve a single-stage SPOP with independence of returns among <b>item</b> <b>types.</b> For a problem from the literature with 11 <b>item</b> <b>types,</b> this method obtains an optimal solution in {{a fraction of a second}} on a laptop computer. An approximation method, based on discretization of possible wealth values, is developed to solve a multi-stage SPOP with inter- and intra-stage independence of returns among <b>item</b> <b>types.</b> Running on a desktop computer, this approximation method solves a 3 -stage problem with 6 <b>item</b> <b>types</b> in under 12 minutes. With finer discretization in a 3 -stage problem with 8 <b>item</b> <b>types,</b> the solution time is about 46 minutes. Singapore Ministry of Defense author (civilian) ...|$|R
