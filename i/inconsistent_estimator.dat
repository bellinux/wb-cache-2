45|88|Public
5000|$|For any {{non-linear}} model (for instance Logit and Probit models), however, heteroscedasticity has more severe consequences: the maximum likelihood {{estimates of the}} parameters will be biased (in an unknown direction), as well as inconsistent (unless the likelihood function is modified to correctly {{take into account the}} precise form of heteroscedasticity). As pointed out by Greene, “simply computing a robust covariance matrix for an otherwise <b>inconsistent</b> <b>estimator</b> does not give it redemption.” ...|$|E
5000|$|For any {{non-linear}} model (for instance Logit and Probit models), however, heteroscedasticity has more severe consequences: the maximum likelihood {{estimates of the}} parameters will be biased (in an unknown direction), as well as inconsistent (unless the likelihood function is modified to correctly {{take into account the}} precise form of heteroskedasticity). As pointed out by Greene, “simply computing a robust covariance matrix for an otherwise <b>inconsistent</b> <b>estimator</b> does not give it redemption. Consequently, the virtue of a robust covariance matrix in this setting is unclear.” ...|$|E
30|$|The {{problem in}} using OLS on {{fractional}} dependent variable {{is that it}} is not asymptotically efficient estimator. It is an unbiased but <b>inconsistent</b> <b>estimator.</b>|$|E
40|$|Spectral {{extraction}} in {{data from}} ground-based gamma-ray observations is usually performed using the on-off likelihood statistic {{which is based}} on the profile likelihood technique. The latter is known to lead to <b>inconsistent</b> <b>estimators</b> in some situations. We present here a systematic Monte Carlo study of the distribution of fitted spectral parameters for typical observations with VHE (very high energy; > 100 GeV) observatories and show that in some conditions spectral extraction yields <b>inconsistent</b> <b>estimators.</b> We discuss some techniques to alleviate this effect and the impact on the search for spectral cutoffs in the very low statistic regime...|$|R
40|$|For the {{estimation}} of coefficients in a measurement error model, the least squares method utilizing original observations and averaged observations over replications provides <b>inconsistent</b> <b>estimators.</b> Based on these, consistent estimators are formulated and asymptotic properties are analyzed. Ultrastructural model Measurement errors Replicated observations...|$|R
30|$|It {{is known}} that in {{situations}} like mixtures of distributions and distributions with a parameter-dependent lower bound, where the ML <b>estimator</b> leads to <b>inconsistent</b> <b>estimators,</b> the MSP estimator is consistent; see [13]. Motivated by this fact it is natural to consider the MSP estimator in parameter estimation for the Kum-Pareto and Kum-Power distributions.|$|R
40|$|Sample {{periodogram}} {{is widely}} {{known as an}} <b>inconsistent</b> <b>estimator</b> for true spectral density. We show that it becomes consistent when the true spectrum at the zero frequency (often known as long-run variance) equals zero. Asymptotic results for consistency of the periodogram {{as well as the}} rate of convergence are formally derived...|$|E
40|$|This paper {{investigates the}} effects of {{consistent}} and inconsistent long-run variance estimation on a unit root test based on the generalization of the von Neumann ratio. The results from the Monte Carlo experiments suggest that the tests based on an <b>inconsistent</b> <b>estimator</b> have less size distortion and more stability of size across different autocorrelation specifications {{as compared to the}} tests based on a consistent estimator. This improvement in size property, however, comes at the cost of a loss in power. The finite sample power, as well as the local asymptotic power, of the tests with an <b>inconsistent</b> <b>estimator</b> is shown to be much lower than that of conventional tests. This finding resembles the case of the autocorrelation robust test in the standard regression context. The paper also points out that combining consistent and inconsistent estimators in the long-run variance ratio test for a unit root is one possibility of balancing the size and power. Bandwidth, local asymptotic power, von Neumann ratio...|$|E
40|$|For any {{nonlinear}} regression function, it is {{shown that the}} orthogonal regression procedure deliv-ers an <b>inconsistent</b> <b>estimator.</b> A new technical approach to the proof of inconsistency based on the implicit-function theorem is presented. For small measurement errors, the leading term of the asymptotic expansion of the estimator is derived. We construct a corrected estimator, which has a smaller asymptotic deviation for small measurement errors...|$|E
30|$|Excluding those {{variables}} {{may increase}} the risk of endogeneity and produce biased and <b>inconsistent</b> OLS <b>estimator.</b>|$|R
30|$|All {{empirical}} studies presented face {{the problem of}} selection effects. The question of why some women choose occupations in STEM, whereas many do not, remains unanswered. Therefore, unobserved heterogeneity and <b>inconsistent</b> <b>estimators</b> cannot be denied. This study takes these issues into account by implementing a special type of grouping estimator proposed by Blundell et al. (1998). The details of this estimation strategy are explained in the following.|$|R
40|$|In {{this paper}} we {{consider}} a weighted harmonic mean of two <b>inconsistent</b> <b>estimators</b> {{to propose a}} new estimator of the coefficient of a linear regression model with measurement errors. The proposed estimator is simple {{and it does not}} depend on any unknown quantity. The approximate bias and MSE of the estimator are derived. Further, an empirical application is also presented. c ○ 2001 Peking University Pres...|$|R
3000|$|... the {{presence}} of a lagged dependent variable on the right-hand side makes LSDV an <b>inconsistent</b> <b>estimator,</b> when asymptotic are considered in the direction of N tends to infinity. However, the asymptotic properties of panel data estimators can be considered in the direction of T, and Amemiya (1967) has shown that when considered in that direction, LSDV proves to be consistent and asymptotically equivalent to the Maximum Likelihood Estimator (MLE).|$|E
40|$|Ordinary {{least squares}} (OLS) {{is well known}} to produce an <b>inconsistent</b> <b>estimator</b> of the spatial {{parameter}} in pure spatial autoregression (SAR). This paper Ordinary least squares (OLS) is well known to produce an <b>inconsistent</b> <b>estimator</b> of the spatial parameter in pure spatial autoregression (SAR). This paper explores the potential of indirect inference to correct the inconsistency of OLS. Under broad conditions, it is shown that indirect inference (II) based on OLS produces consistent and asymptotically normal estimates in pure SAR regression. The II estimator used here is robust to departures from normal disturbances and is computationally straightforward compared with quasi maximum likelihood (QML). Monte Carlo experiments based on various specifications of the weight matrix show that: (i) the indirect inference estimator displays little bias even in very small samples and gives overall performance that {{is comparable to the}} QML while raising variance in some cases; (ii) indirect inference applied to QML also enjoys good finite sample properties; and (iii) indirect inference shows robust performance in the presence of heavy tailed error distributions...|$|E
40|$|The Akaike {{information}} criterion, AIC, {{is widely}} used for model selection. Using the AIC as the estimator of asymptotic unbias for the second term Kullbake-Leibler risk considers the divergence between the true model and offered models. However, it is an <b>inconsistent</b> <b>estimator.</b> A proposed approach {{the problem is the}} use of A 2 ̆ 7 IC, a consistently offered information criterion. Model selection of classic and linear models are considered by a Monte Carlo simulation...|$|E
40|$|A simple {{model for}} avian {{survival}} {{often leads to}} boundary estimates when the model is fitted to data by maximum likelihood. Data may result from ring recovery experiments alone, or in combination with a radio tracking investigation. A study of profile likelihoods enables {{the conditions under which}} boundary estimates occur to be completely specified. Fitting an incorrect model is shown to result in <b>inconsistent</b> <b>estimators</b> of all parameters, and an asymptotic certainty of boundary estimates...|$|R
40|$|The Neyman and Scott (1948) {{model is}} widely used to {{demonstrate}} a serious weakness of the Maximum Likelihood (ML) method: it can give rise to <b>inconsistent</b> <b>estimators.</b> The primary objective {{of this paper is}} to revisit this example with a view to demonstrate that the culprit for the inconsistent estimation is not the ML method but an ill-defined statistical model. It is also shown that a simple recasting of this model renders it well-defined and the ML method gives rise to consistent and asymptotically efficient estimators...|$|R
40|$|International audiencePseudo panels {{constituted}} {{with repeated}} cross-sections are good substitutes to true panel data. But individuals grouped in a cohort {{are not the}} same for successive periods, and it results in a measurement error and <b>inconsistent</b> <b>estimators.</b> The solution is to constitute cohorts of large numbers of individuals but as homogeneous as possible. This paper explains a new way to do this: by using a self-organizing map, whose properties are well suited to achieve these objectives. It is applied to a set of Canadian surveys, in order to estimate income elasticities for 18 consumption functions. ...|$|R
30|$|A {{regressor}} of {{a regression}} {{model can be}} endogenous due to its correlation with error term, which arises from measurement errors. For example, missing observations in the bilateral trade flows that are wrongly recorded as zero (i.e., measurement errors) may also lead to <b>inconsistent</b> <b>estimator</b> of regressors as this measurement error {{is more likely to}} be correlated with small countries (small economy of a country), i.e., the measurement error will depend on the covariates (Santos Silva and Tenreyro 2006).|$|E
40|$|In {{practice}} structural equations {{are often}} estimated by least-squares, thus ne- glecting any simultaneity. This paper reveals why this may often be justifiable and when. Assuming data stationarity and {{existence of the}} first four moments of the disturbances we study the limiting distribution of the ordinary least-squares (OLS) estimator in a linear simultaneous equations model. In simple static mod- els we compare the asymptotic e¢ ciency of this <b>inconsistent</b> <b>estimator</b> with that of consistent simple instrumental variable (IV) estimators and depict cases where - due to relative weakness of the instruments or mildness of the simultaneity - the <b>inconsistent</b> <b>estimator</b> is more precise. In addition, we examine by simulation to what extent these first-order asymptotic findings are reflected in finite sam- ples, taking into account non-existence of moments of the IV estimator. In all comparisons we distinguish between conditional and unconditional (asymptotic) distributions. By dynamic visualization techniques we enable to appreciate any di¤erences in e¢ ciency over a parameter space of a much higher dimension than just two, viz. in colored animated image sequences (which are not very e¤ective in print, but much more so in live-on-screen projection) ...|$|E
40|$|The {{method of}} {{instrument}} variables for {{the identification of}} the parameters of structural systems excited by white noise in the presence of white measurement noise is applied. The requisite equations and the resulting consistent estimator are derived and simulation results for least squares and instrumental variables methods, for light and heavy damping, are compared. The present method gave better results than least squares when measurement noise was presented, and verified the fact that least squares yields an <b>inconsistent</b> <b>estimator</b> unless the measurement noise is zero...|$|E
40|$|AbstractThis paper {{considers}} the linear model with endogenous regressors and multiple {{changes in the}} parameters at unknown times. It is shown that minimization of a Generalized Method of Moments criterion yields <b>inconsistent</b> <b>estimators</b> of the break fractions, but minimization of the Two Stage Least Squares (2 SLS) criterion yields consistent estimators of these parameters. We develop a methodology for estimation and inference of {{the parameters of the}} model based on 2 SLS. The analysis covers the cases where the reduced form is either stable or unstable. The methodology is illustrated via an application to the New Keynesian Phillips Curve for the US...|$|R
40|$|Abstract [...] - Pseudo panels {{constituted}} {{with repeated}} cross-sections are good substitutes to true panel data. But individuals grouped in a cohort {{are not the}} same for successive periods, and it results in a measurement error and <b>inconsistent</b> <b>estimators.</b> The solution is to constitute cohorts of large numbers of individuals but as homogeneous as possible. This paper explains a new way to do this: by using a self-organizing map, whose properties are well suited to achieve these objectives. It is applied to a set of Canadian surveys, in order to estimate income elasticities for 18 consumption functions. ...|$|R
40|$|This paper {{considers}} the linear model with endogenous regressors and multiple {{changes in the}} parameters at unknown times. It is shown that minimization of a Generalized Method of Moments criterion yields <b>inconsistent</b> <b>estimators</b> of the break fractions, but minimization of the Two Stage Least Squares (2 SLS) criterion yields consistent estimators of these parameters. We develop a methodology for estimation and inference of {{the parameters of the}} model based on 2 SLS. The analysis covers the cases where the reduced form is either stable or unstable. The methodology is illustrated via an application to the New Keynesian Phillips Curve for the US...|$|R
40|$|This {{discussion}} paper {{led to a}} publication in 'Computational Statistics & Data Analysis' 51 (7) 3296 - 318. In practice structural equations are often estimated by least-squares, thus neglecting any simultaneity. This paper reveals why this may often be justifiable and when. Assuming data stationarity and existence of the first four moments of the disturbances we find the limiting distribution of the ordinary least-squares (OLS) estimator in a linear simultaneous equations model. In simple static and dynamic models we compare the asymptotic efficiency of this <b>inconsistent</b> <b>estimator</b> with that of consistent simple instrumental variable (IV) estimators and depict cases where [...] due to relative weakness of the instruments or mildness of the simultaneity [...] the <b>inconsistent</b> <b>estimator</b> is more precise. In addition, we examine by simulation to what extent these first-order asymptotic findings are reflected in finite sample, taking into account non-existence of moments of the IV estimator. By dynamic visualization techniques we enable to appreciate any differences in efficiency over a parameter space of a much higher dimension than just two, viz. in colored animated image sequences (which are not very effective in print, but much more so in live-on-screen projection). efficiency of an inconsistent estimator; invalid instruments; simultaneity bias; weak instruments; 4 D diagrams...|$|E
40|$|Ordinary {{least squares}} {{estimation}} of an impulse-indicator coefficient is inconsistent, but its variance can be consistently estimated. Although {{the ratio of}} the <b>inconsistent</b> <b>estimator</b> to its standard error has a "t"-distribution, that test is inconsistent: one solution is to form an index of indicators. We provide Monte Carlo evidence that including a plethora of indicators need not distort model selection, permitting the use of many dummies in a general-to-specific framework. Although White's (1980) heteroskedasticity test is incorrectly sized in that context, we suggest an easy alteration. Finally, a possible modification to impulse 'intercept corrections' is considered. Copyright 2005 Blackwell Publishing Ltd. ...|$|E
40|$|This paper {{studies the}} effect of time–inhomogeneous jumps and {{leverage}} type effects on realised variance calculations when the logarithmic asset price is given by a Lévy–driven stochastic volatility model. In such a model, the realised variance is an <b>inconsistent</b> <b>estimator</b> of the integrated variance. Nevertheless {{it can be used}} within a quasi–maximumlikelihood setup to draw inference on the model parameters. In order to do that, this paper introduces a new methodology for deriving all cumulants of the returns and realised variance in explicit form by solving a recursive system of inhomogeneous ordinary differential equations. Lévy processes, stochastic volatility, leverage effect, superposition, realised variance...|$|E
40|$|The sample {{selection}} {{model has}} been studied {{in the context of}} semi-parametric methods. With the deficiencies of the parametric model, such as <b>inconsistent</b> <b>estimators,</b> semi-parametric estimation methods provide better alternatives. This article focuses on the context of fuzzy concepts as a hybrid to the semiparametric sample selection model. The better approach when confronted with uncertainty and ambiguity is to use the tools provided by the theory of fuzzy sets, which are appropriate for modeling vague concepts. A fuzzy membership function for solving uncertainty data of a semi-parametric sample selection model is introduced as a solution to the problem...|$|R
40|$|This article {{investigates the}} effects of {{measurement}} error on the estimation of nonparametric variance functions. We show that either ignoring measurement error or direct application of the simulation extrapolation, SIMEX, method leads to <b>inconsistent</b> <b>estimators.</b> Nevertheless, the direct SIMEX method can reduce bias relative to a naive estimator. We further propose a permutation SIMEX method which leads to consistent estimators in theory. The performance of both SIMEX methods depends on approximations to the exact extrapolants. Simulations show that both SIMEX methods perform better than ignoring measurement error. The methodology is illustrated using microarray data from colon cancer patients...|$|R
40|$|We {{investigate}} {{the effects of}} measurement error on the estimation of nonparametric variance functions. We show that either ignoring measurement error or direct application of the simulation extrapolation, SIMEX, method leads to <b>inconsistent</b> <b>estimators.</b> Nevertheless, the direct SIMEX method can reduce bias relative to a naive estimator. We further propose a permutation SIMEX method that leads to consistent estimators in theory. The performance of both the SIMEX methods depends on approximations to the exact extrapolants. Simulations show that both the SIMEX methods perform better than ignoring measurement error. The methodology is illustrated using microarray data from colon cancer patients. Copyright 2008, Oxford University Press. ...|$|R
30|$|Since the {{periodogram}} is an unbiased but <b>inconsistent</b> <b>estimator</b> of the spectrum, {{a consistent}} estimator {{can be achieved}} by smoothing it (use of lag windows or averaging). One such consistent estimator is the modified (boxed) periodogram. Actually, Robinson (1994) proved that the averaged periodogram estimator was consistent under very mild conditions. It involves dividing the log of the periodogram into equally spaced boxes and then averaging the values inside each of the boxes leaving out very low frequencies. Further, to address the scattered nature of the periodogram, a robustified least squares (least-trimmed squares of regression) which minimises approximately T /  2 smallest squared residuals can be employed.|$|E
40|$|This is the author's peer-reviewed final manuscript, as {{accepted}} by the publisher. The published article is copyrighted by Taylor & Francis {{and can be found}} at: [URL] paper shows the first autocorrelation of basketball shot results is a highly biased and <b>inconsistent</b> <b>estimator</b> of the first autocorrelation of the ex ante probabilities the shots are made. Shot result autocorrelation is close to zero even when shot probability autocorrelation is close to one. The bias is caused by what is equivalent to a severe measurement error problem. The results imply that the widespread belief among players and fans in the hot hand is not necessarily a cognitive fallacy...|$|E
40|$|In a {{positive}} context, asset valuation models may {{be judged on}} the basis of their predictive ability rather than on the number and elegance of the underlying assumptions. The Average Daily Rate (ADR) rule-of-thumb has been used for decades as a quick way of estimating hotel and motel room rates and, more recently, as a simple gross-income multiplier model for predicting values of lodging properties. This study examines how well the ADR rule-of-thumb model predicts property values. The results of our comparative analysis of estimates from the ADR model with those from a hedonic valuation model indicate that the ADR model performs well in the aggregate, but is an <b>inconsistent</b> <b>estimator</b> at various levels of disaggregation, such as when property subsamples were organized by number of rooms, age, occupancy rate and number of restaurants...|$|E
40|$|Empirical {{likelihood}} (EL) {{is appropriate}} to estimate moment condition models when a random sample from the target population is available. However, many economic surveys are subject {{to some form of}} stratification, in which case direct application of EL will produce <b>inconsistent</b> <b>estimators.</b> In this paper we propose a two-step EL (TSEL) estimator to deal with stratified samples in models defined by unconditional moment restrictions in presence of some aggregate information, which may consist, for example, of the mean and the variance of the variable of interest and/or the explanatory variables. A Monte Carlo simulation study reveals promising results for many versions of the TSEL estimator. Stratified Sampling, Empirical Likelihood, Weighted Estimation, Auxiliary Information...|$|R
40|$|Empirical {{likelihood}} {{is appropriate}} to estimate moment condition models when a random sample from the target population is available. However, many economic surveys are subject {{to some form of}} stratification, in which case direct application of empirical likelihood will produce <b>inconsistent</b> <b>estimators.</b> In this paper we propose a two-step empirical likelihood estimator to deal with stratified samples in models defined by unconditional moment restrictions in the presence of some aggregate information such as the mean and the variance of the variable of interest. A Monte Carlo simulation study reveals promising results for many versions of the two-step empirical likelihood estimator. Copyright � 2006 The Authors; Journal compilation � 2006 Blackwell Publishing Ltd and The University of Manchester. ...|$|R
40|$|This paper {{explores the}} {{question}} of the choice of whether to use a joint posterior or a marginal posterior as basis for analysis with respect to various parameters of interest. It turns out that in the one-way analysis of variance situation there is an optimal choice. Connections with Empirical Bayes estimation is stressed as well as the use of the EM-algorithm. We demonstrate that for the latter, care must be taken with the choice of parameters to be declared "missing", for a wrong choice could lead to <b>inconsistent</b> <b>estimators</b> or estimators with poor mean-square behaviour. The discussion is in the context of one-way analysis of variance. Analysis of variance Posterior analysis Modal estimation Empirical Bayes estimates EM-algorithm...|$|R
