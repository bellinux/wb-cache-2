74|5694|Public
25|$|By August 1944, 92.2 {{percent of}} all Richmond {{shipyard}} employees had joined the plan, the first voluntary group plan {{in the country to}} <b>feature</b> <b>group</b> medical practice, prepayment and substantial medical facilities on such a large scale.By 1990, Kaiser Permanente was still the country's largest nonprofit HMO.|$|E
5000|$|<b>Feature</b> <b>Group</b> B: Associated with 950-XXXX calling; {{instead of}} a local {{telephone}} number the user enters 950 and 4 additional digits which identify the long distance carrier. Operation {{is similar to the}} local access numbers (<b>feature</b> <b>group</b> A) except that the 950-XXXX access number is the same in every community, NANP-wide. Some exchanges send the caller's number automatically; where this service is not provided or not desired (calling card applications), the 950-XXXX number must be followed by a calling card number and the long-distance destination number. If ANI is provided, calls from the one subscribed line may be made as 950-XXXX and the long-distance destination. Largely deprecated by <b>feature</b> <b>group</b> D, but the 950 prefix and a list of carrier codes remain reserved in all North American area codes, even in Canada where most providers went from <b>feature</b> <b>group</b> A directly to 1+ default carrier dialling and <b>feature</b> <b>group</b> D (101xxxx + destination) calling without ever using 950-XXXX as a primary means to access alternate long distance carriers from home land lines.|$|E
50|$|These <b>Feature</b> <b>Group</b> {{alternatives}} {{allowed the}} LEC's end users to make long distance calls using the interexchange carrier's network, when non-stored program-controlled exchanges {{could not be}} modified to provide equal access. By the mid 1990s, Equal Access features in exchange software had rendered <b>Feature</b> <b>Group</b> D universally available in modern landline exchanges; the others are either used for calling card applications or are obsolete.|$|E
5000|$|While {{there are}} other <b>feature</b> <b>groups</b> for local access, the four common <b>feature</b> <b>groups</b> exist for access from the local {{subscriber}} to competitive long-distance carriers: ...|$|R
40|$|Building {{effective}} prediction {{models from}} high-dimensional data {{is an important}} problem in several domains such as in bioinformatics, healthcare analytics and general regression analysis. Extracting <b>feature</b> <b>groups</b> automatically from such data with several correlated features is necessary, {{in order to use}} regularizers such as the group lasso which can exploit this deciphered grouping structure to build effective prediction models. Elastic net, fused-lasso and Octagonal Shrinkage Clustering Algorithm for Regression (oscar) are some of the popular <b>feature</b> <b>grouping</b> methods proposed in the literature which recover both sparsity and <b>feature</b> <b>groups</b> from the data. However, their predictive ability is affected adversely when the regression coefficients of adjacent <b>feature</b> <b>groups</b> are similar, but not exactly equal. This happens as these methods merge such adjacent <b>feature</b> <b>groups</b> erroneously, which is widely known as the misfusion problem. In order to solve this problem, in this thesis, we propose a weighted L 1 norm-based approach which is effective at recovering <b>feature</b> <b>groups,</b> despite the proximity of the coefficients of adjacent <b>feature</b> <b>groups,</b> building extremely accurate prediction models. This convex optimization problem is solved using the fast iterative soft-thresholding algorithm (FISTA). We depict how our approach is more successful than competing <b>feature</b> <b>grouping</b> methods such as the elastic net, fused-lasso and oscar at solving the misfusion problem on synthetic datasets. We also compare the goodness of prediction of our algorithm against state-of-the-art non-convex <b>feature</b> <b>grouping</b> methods when applied on a real-world breast cancer dataset, the 20 -Newsgroups dataset and synthetic datasets...|$|R
40|$|This paper {{investigates the}} {{classification}} of different emotional states using speech features from differ-ent <b>feature</b> <b>groups.</b> We use both suprasegmental fea-ture groups like pitch, energy, and duration and seg-mental <b>feature</b> <b>groups</b> like voice quality, zero cross-ing rate, and articulation. We want to exploit {{the selection of the}} most relevant features from these different <b>feature</b> <b>groups</b> to get a better understand-ing of the speaker independent emotion recognition. We study how these different <b>feature</b> <b>groups</b> overlap or complement each other. By using the sequential floating forward selection algorithm (SFFS), feature subsets maximizing the classification rate will be generated. For this purpose, we use a Bayesian clas-sifier and a speaker independent cross validation. A detailed study is also done on the relevance of the <b>feature</b> <b>groups</b> for classifying different emotion di-mensions known from the psychological emotion re-searc...|$|R
50|$|Here, the {{compiler}} {{will allow}} only the classes listed between the curly braces {{to access the}} features within the <b>feature</b> <b>group</b> (e.g. DECIMAL, DCM_MA_DECIMAL_PARSER, DCM_MA_DECIMAL_HANDLER).|$|E
5000|$|A <b>Feature</b> <b>Group,</b> in North American {{telephone}} industry jargon, is {{most commonly used}} to designate various standard means of access by callers to competitive long distance services.|$|E
50|$|By August 1944, 92.2 {{percent of}} all Richmond {{shipyard}} employees had joined the plan, the first voluntary group plan {{in the country to}} <b>feature</b> <b>group</b> medical practice, prepayment and substantial medical facilities on a large scale.|$|E
40|$|Automatic {{generation}} of significant <b>feature</b> <b>groups</b> from a given set of basic features, i. e. creation of abstract characteristics, {{is a fundamental}} problem to be solved in pattern recognition. With the presented method {{it is possible to}} generate automatically local and important <b>feature</b> <b>groups</b> of contours of 2 D objects resulting in reasonable class membership. The method was designed for the development of general classifiers, relying on automatically generated, local and important <b>feature</b> <b>groups,</b> derived from all kinds of basic features, for example contour points, colours, and infra-red spectra. ...|$|R
5000|$|<b>Features</b> <b>group</b> projects, solo projects, {{production}} {{credits and}} features ...|$|R
40|$|Automatic {{generation}} of significant <b>feature</b> <b>groups</b> {{out of a}} given set of basic features, i. e. creation of abstract characteristics, is a fundamental problem to be solved in pattern recognition. With the presented method it's possible to detect automatically local and significant <b>feature</b> <b>groups</b> of 2 D objects resulting in meaningful class memberships...|$|R
50|$|It is {{the only}} Morning Musume single to date to <b>feature</b> <b>group</b> members {{contributing}} to the instrumental backing track, as several of the members can be heard blowing police whistles during the song's choruses and closing moments.|$|E
5000|$|... 950-xxxx - <b>Feature</b> <b>group</b> {{code for}} access to a carrier from a non-subscriber location. The feature {{requires}} the customer dial a 950-xxxx number and enter a calling card number and destination telephone number. It was originally used for locations where 101-xxxx dialing was not possible.|$|E
50|$|By August 1944, 92.2 {{percent of}} all Richmond {{shipyard}} employees had joined the plan, the first voluntary group plan {{in the country to}} <b>feature</b> <b>group</b> medical practice, prepayment and substantial medical facilities on such a large scale.By 1990, Kaiser Permanente was still the country's largest nonprofit HMO.|$|E
30|$|National Male/Female Average Score Average scores {{inclusive}} of all cities and <b>features</b> <b>grouped</b> by gender.|$|R
50|$|Soundstage (1983). The Roches {{were the}} <b>featured</b> <b>group</b> in one episode of this {{televised}} music series.|$|R
5000|$|The {{released}} mix <b>features</b> <b>group</b> vocals, but {{an early}} mix featured only a double-tracked vocal by Nesmith.|$|R
5000|$|<b>Feature</b> <b>Group</b> D: The current standard, {{requires}} the local switch support equal access by competing carriers at the trunk level; highest quality connection, and allows pre-selection of the interexchange carrier by the end-user. This <b>feature</b> <b>group</b> permits {{two types of}} calls. If a user dials 1 + area code + seven-digit number, the long distance call is handled by a default carrier chosen by the user. Alternatively, a user dials 101 + four-digit carrier code + area code + seven-digit number, and the call is handled by the carrier specified by the carrier code. The original batch of carrier codes began with 0, so this type of [...] "dial around" [...] service was typically marketed as dial-around 101-xxxx service. NANPA maintains separate lists of carrier codes for feature groups 'B' and 'D' as not all long distance providers support both standards.|$|E
50|$|Some 911 Public Safety Answering Points (PSAPs) {{still use}} the MF format to {{identify}} the calling party to the PSAP when processing calls from Mobile Telephone Switching Offices (MTSOs) and land telephone offices. This {{is based on an}} earlier system which used MF {{to identify the}} calling party to a <b>feature</b> <b>group</b> 'D' (101xxxx) alternate long distance provider.|$|E
50|$|The second {{category}} of cars is the Group B class, {{all of which}} are 5-speed monsters that accelerate quickly and are very difficult for novices to control. These cars all took part in the original Group B races. However these overpowered supercars of rally were retired in 1986. Most superrally stages <b>feature</b> <b>Group</b> B cars exclusively.|$|E
50|$|Monster Madhouse {{debuted in}} 2006 as a <b>featured</b> <b>group</b> of monster hunters or Monsterminators led by Borloff (Moore).|$|R
40|$|In this paper, {{we propose}} a Correlation based Feature Analysis (CFA) and Multi-Modality Fusion (CFA-MMF) {{framework}} for multimedia semantic concept retrieval. The CFA method {{is able to}} reduce the feature space and capture the correlation between features, separating the feature set into different <b>feature</b> <b>groups,</b> called Hidden Coherent <b>Feature</b> <b>Groups</b> (HCFGs), based on Maximum Spanning Tree (MaxST) algorithm. A correlation matrix is built upon feature pair correlations, and then a MaxST is constructed based on the correlation matrix. By performing a graph cut procedure on the MaxST, a set of <b>feature</b> <b>groups</b> are obtained, where the intra-group correlation is maximized and the inter-group correlation is minimized. Finally, one classifier is trained {{for each of the}} <b>feature</b> <b>groups,</b> and the generated scores from different classifiers are fused for the final retrieval. The proposed framework is effective because it reduces the dimensionality of the feature space. The experimental results on the NUSWIDE-Lite data set demonstrate the effectiveness of the proposed CFA-MMF framework. Index Terms — multimedia semantic retrieval, feature correlation, maximum spanning tree, multi-modality, fusion 1...|$|R
50|$|<b>Feature</b> <b>groups</b> {{consist of}} <b>feature</b> toggles that work together. This allows the {{developer}} to easily manage {{a set of}} related toggles.|$|R
50|$|AKB48's {{producers}} have developed several television shows {{to promote the}} group. AKBingo!, AKB48 Show!, AKB to XX, and Nemōsu TV are variety shows. The Majisuka Gakuen series and Sakura Karano Tegami <b>feature</b> <b>group</b> members in dramatic roles. Outside the self-developed house shows, members of AKB48 have appeared in high-profile programs such as Mecha-Mecha Iketeru!, Waratte Iitomo! or SMAPxSMAP. Members have modeled in gravure magazines.|$|E
50|$|The live shows {{began on}} August 4, 2012 in PAGCOR Grand Theater in Parañaque, Metro Manila. Each week, the top acts {{performances}} {{took place on}} Saturday nights, while the results are announced during Sunday nights. Each week has {{a different set of}} theme. Often during results nights, guests are invited live to perform; while the remaining acts during results nights also <b>feature</b> <b>group</b> performances.|$|E
5000|$|<b>Feature</b> <b>Group</b> C: Rare, {{originally}} used by AT&T for operator-assisted coin phones {{since they}} allow the operator {{to keep control}} of the caller's telephone line until the transaction is completed. As coin-handling for trunk calls is now automated within the phone (like a COCOT, {{the current generation of}} coin phones operates self-contained without the central exchange providing coin-call support functions), group 'C' is largely obsolete.|$|E
50|$|A {{downloadable}} car pack, <b>featuring</b> <b>Group</b> B rally {{cars from}} the 1980s, {{was released on}} the day of the game's release.|$|R
40|$|Domain {{knowledge}} {{can help us}} build more accurate prediction models. Molecular biology {{is one of the}} fields where induction of prediction models is relatively hard due to few learning instances in a typical data set, but there exists vast domain knowledge. Basic entities of the field [...] -genes, proteins, and metabolic products [...] -are described and categorized in various freely accessible databases. This thesis focuses on methods that transform data from the space of features into the space of <b>feature</b> <b>groups,</b> which can be assembled from existing data bases and represent prior knowledge. Features in data sets from the field of molecular biology that we used in the thesis represent genes. Methods working with gene groups assume that gene expression profiles belonging to the same group are similar. We show that gene expressions of gene pairs from groups in databases KEGG and BioGRID are more similar than gene expression of random gene pairs, but the differences are small. The differences do not change with the database version. We propose a technique for transformation of data into a space of <b>feature</b> <b>groups</b> with collective matrix factorization, which simultaneously factorizes matrices representing data and <b>feature</b> <b>groups</b> into a product of latent factors with ranks smaller than ranks of original matrices. The models induced from the transformed data can be as accurate as models on the non-transformed data. In contrast to existing approaches, the proposed approach can also use features that are not in predefined <b>groups</b> of <b>features</b> but are similar to <b>features</b> in a <b>group.</b> Transformation techniques that transform data into a space of <b>feature</b> <b>groups</b> require estimation of transformation parameters such as, for example, feature weights. Techniques that use values of the target variable for parameter estimation, produce values for the <b>feature</b> <b>groups</b> that are at least partially fitted to the target variable. The induced models could therefore overestimate the importance of class-overfitted features, which can decrease their accuracy on novel data. We propose a solution that uses stacking. The proposed solution can work with any transformation technique and, for some data sets, boosts accuracy substantially. In the thesis we throughly study transformation of data into predefined <b>feature</b> <b>groups.</b> We show, in the largest study so far, that, on average, models induced from data sets transformed with <b>feature</b> <b>groups</b> do not obtain better prediction accuracies than models induced on non-transformed data sets. As the accuracies on transformed and non-transformed data sets are similar, the transformed data may still be preferred as models on <b>feature</b> <b>groups</b> are easier to interpret. ...|$|R
50|$|Also included, the Closed Instant Messaging Client (ComAgent) <b>features</b> <b>group</b> chat, buddy list, file sharing, email notifications, and {{automatic}} address book synchronization.|$|R
5000|$|Brownstein {{has acted}} (what {{she calls a}} [...] "mere hobby") in the short film Fan Mail, the {{experimental}} <b>feature</b> <b>Group,</b> and the Miranda July film Getting Stronger Everyday. Brownstein and Fred Armisen published several video skits {{as part of a}} comedy duo called [...] "ThunderAnt". She also starred opposite James Mercer of The Shins in the 2010 independent film Some Days Are Better Than Others. The film had its world premiere at SXSW on March 13, 2010.|$|E
50|$|The Hong Kong Baby Products Fair is {{an annual}} trade fair {{organized}} by the Hong Kong Trade Development Council (HKTDC), with its first edition scheduled for January 2010. It is a spin-off event from the HKTDC Hong Kong Toys & Games Fair, the second largest fair {{of its kind in}} the world and the biggest in Asia. The new Baby Products Fair will presents nearly 300 exhibitors and will <b>feature</b> <b>group</b> pavilions representing the Chinese mainland, Taiwan and Thailand.|$|E
50|$|In August 2014, Blackburn {{was named}} by CBS as its lead play-by-play {{announcer}} for its sports network. Blackburn will call college basketball, football, and baseball games. On December 27,2014 Blackburn filled in for Verne Lundquist during the 2014 Sun Bowl. Blackburn {{was a part}} of the CBS Sports team doing <b>feature</b> <b>group</b> coverage for the 2015 Masters Golf Tournament. He also calls NFL games for CBS on occasion, partnering with Chris Simms for 2015 and 2016.|$|E
50|$|The {{video was}} filmed in Hollywood and <b>featured</b> <b>group</b> 3OH!3 {{is in the}} video. The video was {{released}} on August 26, 2010.|$|R
40|$|Heterogeneous feature {{representations}} {{are widely}} used in machine learning and pattern recognition, especially for multimedia analysis. The multi-modal, often also highdimensional, features may contain redundant and irrelevant information that can deteriorate the performance of modeling in classification. It is a challenging problem to select the informative features for a given task from the redundant and heterogeneous <b>feature</b> <b>groups.</b> In this paper, we propose a novel framework to address this problem. This framework is composed of two modules, namely, multi-modal deep neural networks and feature selection with sparse group LASSO. Given diverse <b>groups</b> of discriminative <b>features,</b> the proposed technique first converts the multi-modal data into a unified representation with different branches of the multi-modal deep neural networks. Then, through solving a sparse group LASSO problem, the feature selection component is used to derive a weight vector to indicate {{the importance of the}} <b>feature</b> <b>groups.</b> Finally, the <b>feature</b> <b>groups</b> with large weights are considered more relevant and hence are selected. We evaluate our framework on three image classification datasets. Experimental results show that the proposed approach is effective in selecting the relevant <b>feature</b> <b>groups</b> and achieves competitive classification performance as compared with several recent baseline methods...|$|R
50|$|In 2015, Longhorn Network {{simulcast}} the <b>featured</b> <b>groups</b> {{coverage of}} ESPN's {{broadcast of the}} Open Championship, as it featured Longhorns alumni Jordan Spieth.|$|R
