6060|3023|Public
5|$|A {{common theme}} {{throughout}} Zobel's {{work is the}} issue of impedance matching. The obvious approach to filter design is to design directly for the attenuation characteristics desired. With modern computing power, a brute force approach is possible and easy, simply incrementally adjusting each component while recalculating in an <b>iterative</b> <b>process</b> until the desired response is achieved. However, Zobel developed a more indirect line of attack. He realized very early on that mismatched impedances inevitably meant reflections, and reflections meant a loss of signal. Improving the impedance match, conversely, would automatically improve a filter's pass-band response.|$|E
25|$|NLLSQ {{is usually}} an <b>iterative</b> <b>process.</b> The <b>iterative</b> <b>process</b> {{has to be}} {{terminated}} when a convergence criterion is satisfied. LLSQ solutions can be computed using direct methods, although problems {{with large numbers of}} parameters are typically solved with iterative methods, such as the Gauss–Seidel method.|$|E
25|$|Brockmann, R.J., & Sinatra, S. (1995). How the <b>iterative</b> <b>process</b> {{helped the}} Allies win the Persian Gulf War. STC Intercom, 42 (9), 1, 44.|$|E
30|$|It {{is clear}} that the <b>iterative</b> <b>processes</b> (2.1) include many <b>iterative</b> <b>processes</b> as special cases.|$|R
40|$|The convergency of <b>iterative</b> <b>processes</b> {{can present}} {{unexpected}} behaviours. In this paper {{an analysis of}} <b>iterative</b> <b>processes</b> convergency is made, and illustrated by its application to Newton method. This leads {{to the definition of}} a dynamic relaxation scheme that enables to master <b>iterative</b> <b>processes</b> convergency in limit cases...|$|R
30|$|Further, Lan [3] {{remarked that}} the above <b>iterative</b> <b>processes</b> include many <b>iterative</b> <b>processes</b> as special cases {{and he gave}} a {{sufficient}} and necessary condition for the iterative sequence to converge to the common fixed points for two generalized asymptotically quasi-nonexpansive mappings.|$|R
25|$|Palindromic {{numbers are}} numbers that {{read the same}} when their digits are reversed. A Lychrel number is a {{positive}} integer that never yields a palindromic number when subjected to the <b>iterative</b> <b>process</b> of being added to itself with digits reversed. The question of whether there are any Lychrel numbers in base10 is an open problem in recreational mathematics; the smallest candidate is 196.|$|E
25|$|There are {{two things}} in this {{definition}} that may need some clarification. First, the process of risk management is an ongoing, <b>iterative</b> <b>process.</b> It must be repeated indefinitely. The business environment is constantly changing and new threats and vulnerabilities emerge every day. Second, the choice of countermeasures (controls) used to manage risks must {{strike a balance between}} productivity, cost, effectiveness of the countermeasure, and the value of the informational asset being protected.|$|E
25|$|Affine shape {{adaptation}} is {{a methodology}} for iteratively adapting {{the shape of}} the smoothing kernels in an affine group of smoothing kernels to the local image structure in neighbourhood region of a specific image point. Equivalently, affine shape adaptation can be accomplished by iteratively warping a local image patch with affine transformations while applying a rotationally symmetric filter to the warped image patches. Provided that this <b>iterative</b> <b>process</b> converges, the resulting fixed point will be affine invariant. In the area of computer vision, this idea has been used for defining affine invariant interest point operators as well as affine invariant texture analysis methods.|$|E
30|$|The {{theory of}} multivalued nonexpansive {{mappings}} is {{harder than the}} corresponding theory of single valued nonexpansive mappings. Different <b>iterative</b> <b>processes</b> {{have been used to}} approximate the fixed points of multivalued nonexpansive mappings. Among these <b>iterative</b> <b>processes,</b> Sastry and Babu [5] considered the following.|$|R
40|$|The convergency of <b>iterative</b> <b>processes</b> can exhibit {{unexpected}} behaviours. In {{this paper}} {{an analysis of}} the convergency of the <b>iterative</b> <b>processes</b> is made through the application of the Newton method to the polynomial equations resolution. This leads to the definition of a dynamic relaxation scheme applied to computer graphics...|$|R
40|$|In {{this paper}} {{we focus on}} the {{application}} of automatic differentiation (AD) technique on <b>iterative</b> <b>processes.</b> We review some of the results on theconvergence of general <b>iterative</b> <b>processes</b> and the convergence of thederivative code of such <b>iterative</b> <b>processes.</b> We are especially interestedin a class of fixed point iteration problems and we extended some of thoseresults to discuss this class of problems. Finally we apply an AD packageADIC to a network performance evaluation problem for numerical experimentsto get sensitivities of network blocking probabilities w. r. t. networkoffered traffic load...|$|R
25|$|Given {{the short}} reads {{produced}} by {{the current generation of}} sequencing technologies, de novo assembly is a major computational problem. It is normally approached by an <b>iterative</b> <b>process</b> of finding and connecting sequence reads with sensible overlaps. Various computational and statistical techniques, such as de bruijn graphs and overlap layout consensus graphs, have been leveraged to solve this problem. Nonetheless, due to the highly repetitive nature of eukaryotic genomes, accurate and complete reconstruction of genome sequences in de novo assembly remains challenging. Pair end reads have been posed as a possible solution, though exact fragment lengths are often unknown and must be approximated.|$|E
25|$|Some early {{examples}} of computer markup languages available outside {{the publishing industry}} {{can be found in}} typesetting tools on Unix systems such as troff and nroff. In these systems, formatting commands were inserted into the document text so that typesetting software could format the text according to the editor's specifications. It was a trial and error <b>iterative</b> <b>process</b> to get a document printed correctly. Availability of WYSIWYG ("what {{you see is what you}} get") publishing software supplanted much use of these languages among casual users, though serious publishing work still uses markup to specify the non-visual structure of texts, and WYSIWYG editors now usually save documents in a markup-language-based format.|$|E
25|$|Generally {{heuristic}} algorithm {{relies on}} the iterative strategy, scilicet based on a comparison method, optimizing the results of multiple sequence alignment by the <b>iterative</b> <b>process.</b> Davie M proposed using particle swarm optimization algorithm to solve the multiple sequence alignment problem; Ikeda T proposed a heuristic algorithm {{which is based on}} A* search algorithm; Bimey E first proposed using hidden Markov model to solve the multiple sequence alignment problem; and many other biologists use genetic algorithm to solve it. All these algorithms generally are robust and insensitive to the number of sequences, but they also have shortcoming, for example, the result got from particle swarm optimization algorithm is unstable and its merits depend on the selection of random numbers, the runtime of A * search algorithm is too long and the genetic algorithm is easy to fall into local excellent.|$|E
30|$|It is {{our purpose}} {{in this paper}} that we will extend the above <b>iterative</b> <b>processes</b> to the more general <b>iterative</b> <b>processes</b> and give a {{sufficient}} and necessary condition for two asymptotically quasi-nonexpasnive mapping in the intermediate sense. Our result extends the corresponding results of Lan [3], Zhou et al. [30], and many others.|$|R
50|$|Computational Complexity of <b>Iterative</b> <b>Processes,</b> SIAM Journal on Computing 1, 1972, 167-179.|$|R
5000|$|... co-organizer of the {{workshop}} <b>Iterative</b> <b>Processes</b> for Solving Equations, Kiel (Germany, 1998) ...|$|R
25|$|Based on this work, Dayhoff and her {{coworkers}} {{developed a}} set of substitution matrices called the PAM (Percent Accepted Mutation), MDM (Mutation Data Matrix), or Dayhoff. They are derived from global alignments of closely related protein sequences. The identification number included with the matrix (ex. PAM40, PAM100) refers to the evolutionary distance; greater numbers correspond to greater distances. Matrices using greater evolutionary distances are extrapolated from those used for lesser ones. To produce a Dayhoff matrix, pairs of aligned amino acids in verified alignments are used to build a count matrix, which is then used to estimate at mutation matrix at 1 PAM (considered an evolutionary unit). From this mutation matrix, a Dayhoff scoring matrix may be constructed. Along with a model of indel events, alignments generated by these methods {{can be used in}} an <b>iterative</b> <b>process</b> to construct new count matrices until convergence.|$|E
25|$|Kaplan credits artist Arnold Tsang {{from coming}} up with the {{preliminary}} designs of all the heroes in the game. The narrative and characters themselves were then developed through an <b>iterative</b> <b>process</b> between the gameplay developers, artists, and promotional media as they worked to bring the narrative together. In this regards, Torbjörn was the defining character for the game, as while he was not created first, his art style was originally created by Tsang to help bridge the gap between the Warcraft universe to Overwatch. Blizzard's art director Sam Didier reviewed Tsang's original design, prompting several questions to help tighten the art design that led to Torbjörn's gameplay mechanics, and subsequently, Torbjörn's appearance was used as a baseline in all other character and map designs to make sure that these assets would appear to fit into the same universe as Torbjörn. Another example is Doomfist, a character introduced into one of the game's promotional videos where his gauntlet was on display. This {{led to the creation of}} one of the maps that expanded upon the Doomfist concept, making that a title passed down among others in the past, and seeding some of the existing heroes' backstory has had connections to the Doomfist title. This process gave them a sufficient starting point to work from in introducing Doomfist as a playable hero in Overwatch. Other examples of similar iterative expansion to the characters given by Metzen and Chu include the heroes Genji and Hanzo who were characters borne out of an initial single character concept and leading to them being rivals of each other, and the introduction of Lúcio as a means to expand upon the loosely connected Vishkar Corporation concept that was part of Symmetra's backstory.|$|E
500|$|Image filters {{continued}} {{to be used by}} designers long after the superior network synthesis techniques were available. Part of the reason for this may have been simply inertia, but it was largely due to the greater computation required for network synthesis filters, often needing a mathematical <b>iterative</b> <b>process.</b> Image filters, in their simplest form, consist of a chain of repeated, identical sections. The design can be improved simply by adding more sections and the computation required to produce the initial section is on the level of [...] "back of an envelope" [...] designing. In the case of network synthesis filters, on the other hand, the filter is designed as a whole, single entity and to add more sections (i.e., increase the order) the designer would have no option but {{to go back to the}} beginning and start over. The advantages of synthesised designs are real, but they are not overwhelming compared to what a skilled image designer could achieve, and in many cases it was more cost effective to dispense with time-consuming calculations. This is simply not an issue with the modern availability of computing power, but in the 1950s it was non-existent, in the 1960s and 1970s available only at cost, and not finally becoming widely available to all designers until the 1980s with the advent of the desktop personal computer. Image filters {{continued to}} be designed up to that point and many remained in service into the 21st century.|$|E
40|$|AbstractIn {{this paper}} the {{existence}} and uniqueness of solutions to quite general classes of integro-algebraic systems and differential–algebraic systems are investigated. The convergence of different <b>iterative</b> <b>processes</b> of solving such systems including waveform relaxation methods is also investigated. There are given constructive sufficient conditions under which the solutions exist and are unique and the considered <b>iterative</b> <b>processes</b> are convergent...|$|R
30|$|Next, we prove strong {{convergence}} of <b>iterative</b> <b>processes</b> with errors for m-accretive operators.|$|R
30|$|In [3], Liu {{introduced}} {{the concepts of}} Ishikawa and Mann <b>iterative</b> <b>processes</b> with errors as follows.|$|R
2500|$|By repeating {{cycles of}} this process, DNA {{polymerase}} δ and Fen1 can coordinate {{the removal of}} RNA primers and leave a DNA nick at the lagging strand. It has been proposed that this <b>iterative</b> <b>process</b> is preferable to the cell because it is tightly ...|$|E
2500|$|The STM numerically solves {{equation}} 3 {{through an}} <b>iterative</b> <b>process.</b> [...] This {{can be done}} using the bisection or Newton-Raphson Method, and is essentially solving for total head at a specified location using equations 4 and 5 by varying depth at the specified location.|$|E
2500|$|For {{comparison}} purposes, {{the following}} are the results generated using a matrix method. [...] Note that in the analysis above, the <b>iterative</b> <b>process</b> was carried to >0.01 precision. The fact that the matrix analysis results and the moment distribution analysis results match to 0.001 precision is mere coincidence.|$|E
30|$|Next, we modify Ishikawa <b>iterative</b> <b>processes</b> {{to obtain}} a strong {{convergence}} theorem without any compact assumption.|$|R
3000|$|... [...]. We {{first recall}} the {{following}} two <b>iterative</b> <b>processes</b> due to Ishikawa [1] and Mann [2], respectively.|$|R
3000|$|... {{are defined}} by the <b>iterative</b> <b>processes</b> (1.3) and (1.4), respectively. Then, the {{following}} two assertions are equivalent: [...]...|$|R
2500|$|The {{utility of}} Cauchy {{sequences}} {{lies in the}} fact that in a complete metric space (one where all such sequences are known to converge to a limit), the criterion for convergence depends only on the terms of the sequence itself, as opposed to the definition of convergence, which uses the limit value as well as the terms. This is often exploited in algorithms, both theoretical and applied, [...] where an <b>iterative</b> <b>process</b> can be shown relatively easily to produce a Cauchy sequence, consisting of the iterates, thus fulfilling a logical condition, such as termination.|$|E
2500|$|A more {{quantitative}} analysis of LEED experimental {{data can be}} achieved by analysis of so-called I-V curves, which are measurements of the intensity versus incident electron energy. The I-V curves can be recorded by using a camera connected to computer controlled data handling or by direct measurement with a movable Faraday cup. The experimental curves are then compared to computer calculations based on the assumption of a particular model system. The model is changed in an <b>iterative</b> <b>process</b> until a satisfactory agreement between experimental and theoretical curves is achieved. A quantitative measure for this agreement is the so-called reliability- or R-factor. A commonly used reliability factor is the one proposed by Pendry. It is expressed in terms of the logarithmic derivative of the intensity: ...|$|E
2500|$|Gödel's {{completeness}} theorem establishes an equivalence in first-order logic {{between the}} formal provability of a formula and its truth in all possible models. Precisely, for any consistent first-order theory it gives an [...] "explicit construction" [...] {{of a model}} described by the theory; this model will be countable if {{the language of the}} theory is countable. However this [...] "explicit construction" [...] is not algorithmic. It is based on an <b>iterative</b> <b>process</b> of completion of the theory, where each step of the iteration consists in adding a formula to the axioms if it keeps the theory consistent; but this consistency question is only semi-decidable (an algorithm is available to find any contradiction but if there is none this consistency fact can remain unprovable).|$|E
3000|$|... 2. However, {{it leads}} high {{computational}} burden for practical implementation due to <b>iterative</b> <b>processes</b> for determining proper α [...]...|$|R
30|$|The {{decreasing}} {{order of}} {{rate of convergence}} for <b>iterative</b> <b>processes</b> is Agarwal et al., Picard, Noor, Ishikawa and Mann processes.|$|R
3000|$|... [...]. This type of {{assumptions}} is typical for controlling {{the convergence of}} <b>iterative</b> <b>processes</b> for asymptotically nonexpansive mappings, see, e.g., [25].|$|R
