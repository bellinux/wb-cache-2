4307|3088|Public
5|$|Concorde's {{technical}} {{leap forward}} boosted the public's understanding of conflicts between {{technology and the}} environment as well as awareness of the complex <b>decision</b> <b>analysis</b> processes that surround such conflicts. In France, the use of acoustic fencing alongside TGV tracks {{might not have been}} achieved without the 1970s controversy over aircraft noise. In the UK, the CPRE has issued tranquillity maps since 1990.|$|E
5|$|Since {{antiviral}} {{drugs are}} effective in treating influenza if given early (see treatment section, below), it can be important to identify cases early. Of the symptoms listed above, the combinations of fever with cough, sore throat and/or nasal congestion can improve diagnostic accuracy. Two <b>decision</b> <b>analysis</b> studies suggest that during local outbreaks of influenza, the prevalence will be over 70%, and thus patients {{with any of these}} combinations of symptoms may be treated with neuraminidase inhibitors without testing. Even {{in the absence of a}} local outbreak, treatment may be justified in the elderly during the influenza season as long as the prevalence is over 15%.|$|E
25|$|The <b>Decision</b> <b>Analysis</b> Society {{annually}} awards the Frank P. Ramsey Medal to recognise substantial {{contributions to}} decision theory and {{its application to}} important classes of real decision problems.|$|E
40|$|The {{purpose of}} the current study was to {{evaluate}} the relative advantages and disadvantages of a novel statistical technique within the context of personnel decision-making. Specifically, the use of <b>decision</b> tree <b>analysis</b> was examined regarding its potential benefits over binary logistic regression. Using Monte Carlo simulation, a series of data sets were generated based on meta-analytic correlation matrices representing the topics of (1) employee performance and (2) employee health. Each data set was analyzed via both <b>decision</b> tree <b>analysis</b> and binary logistic regression with subsequent comparisons being made concerning model validity, adverse impact, and interpretability. Overall, <b>decision</b> tree <b>analysis</b> demonstrated a variety of benefits over the more traditional method. In general, <b>decision</b> tree <b>analysis</b> produced predictive models that possessed nearly equivalent levels of validity as models produced by logistic regression. Of greater importance, the majority of <b>decision</b> tree <b>analysis</b> models produced no adverse impact, whereas logistic regression models were largely associated with discriminatory results. Lastly, <b>decision</b> tree <b>analysis</b> models were generally more parsimonious and interpretable than the competing logistic regression models. The practical implications of these results are discussed and suggest that the use <b>decision</b> tree <b>analysis</b> holds the potential to greatly improve the way in which organizations make decisions regarding the productivity and health of employees...|$|R
40|$|The {{purpose of}} this paper is to {{investigate}} the use of <b>decision</b> tree <b>analysis</b> in the identification of stakeholders who participate in and who do not participate in tourism and political activities in a community. <b>Decision</b> tree <b>analysis</b> is a tool for partitioning a data set based on the relationships between a set of independent variables and a dependent variable. The research reported here tests the application of, <b>decision</b> tree <b>analysis,</b> an analytical technique that is not traditionally used to segment stakeholders in tourism. Based on the results of the <b>decision</b> tree <b>analysis</b> four groups were identified: high participants, high-moderate participants, lowmoderate participants, and low participants...|$|R
30|$|<b>Decision</b> matrix <b>analysis</b> is {{a useful}} {{technique}} for making a decision. It is particularly powerful where {{there are a number}} of good alternatives to choose from and many different factors to take into account. <b>Decision</b> matrix <b>analysis</b> helps in deciding between several options where many different criteria are involved.|$|R
25|$|The Center for Technology and Systems Management (CTSM) aims {{to advance}} the state of {{technology}} and systems analysis {{for the benefit of}} people and the environment. The focus is on enhancing safety, efficiency and effectiveness by performing reliability, risk, uncertainty or <b>decision</b> <b>analysis</b> studies.|$|E
25|$|The {{addictive}} alkaloid nicotine is a stimulant, and {{popularly known}} as the most characteristic constituent of tobacco. Nicotine is known to produce conditioned place preference, a sign of enforcement value. Nicotine scores almost as highly as opioids on drug effect questionnaire liking scales, which are a rough indicator of addictive potential. Users may develop tolerance and dependence. Thousands of different substances in cigarette smoke, including polycyclic aromatic hydrocarbons (such as benzopyrene), formaldehyde, cadmium, nickel, arsenic, tobacco-specific nitrosamines, and phenols contribute to {{the harmful effects of}} smoking. Tobacco's overall harm to user and self score as determined by a multi-criteria <b>decision</b> <b>analysis</b> was determined at 3 percent below cocaine, and 13 percent above amphetamines, ranking 6th most harmful of the 20 drugs assessed.|$|E
25|$|Bayesian <b>decision</b> <b>analysis</b> {{can also}} {{be applied to the}} channel {{selection}} process. In order to help provide further information the method can be used that produces results in a profit or loss aspect. Prior information can include costs, expected profit, training expenses and any other costs relevant to the decision as well as managerial experience which can be displayed in a normal distribution. Bayesian decision making under uncertainty lets a marketing manager assess his/her options for channel logistics by computing the most profitable method choice. A number of different costs can be entered into the model that helps to assess the ramifications of change in distribution method. Identifying and quantifying all of the relevant information for this process can be very time consuming and costly if the analysis delays possible future earnings.|$|E
40|$|Abstract Background <b>Decision</b> curve <b>analysis</b> {{has been}} {{introduced}} as a method to evaluate prediction models {{in terms of their}} clinical consequences if used for a binary classification of subjects into a group who should and into a group who should not be treated. The key concept for this type of evaluation is the "net benefit", a concept borrowed from utility theory. Methods We recall the foundations of <b>decision</b> curve <b>analysis</b> and discuss some new aspects. First, we stress the formal distinction between the net benefit for the treated and for the untreated and define the concept of the "overall net benefit". Next, we revisit the important distinction between the concept of accuracy, as typically assessed using the Youden index and a receiver operating characteristic (ROC) analysis, and the concept of utility of a prediction model, as assessed using <b>decision</b> curve <b>analysis.</b> Finally, we provide an explicit implementation of <b>decision</b> curve <b>analysis</b> to be applied in the context of case-control studies. Results We show that the overall net benefit, which combines the net benefit for the treated and the untreated, is a natural alternative to the benefit achieved by a model, being invariant with respect to the coding of the outcome, and conveying a more comprehensive picture of the situation. Further, within the framework of <b>decision</b> curve <b>analysis,</b> we illustrate the important difference between the accuracy and the utility of a model, demonstrating how poor an accurate model may be in terms of its net benefit. Eventually, we expose that the application of <b>decision</b> curve <b>analysis</b> to case-control studies, where an accurate estimate of the true prevalence of a disease cannot be obtained from the data, is achieved with a few modifications to the original calculation procedure. Conclusions We present several interrelated extensions to <b>decision</b> curve <b>analysis</b> that will both facilitate its interpretation and broaden its potential area of application. </p...|$|R
40|$|In {{industries}} such as biosciences, {{the range}} of uncertainty and managerial flexibility make traditional financial analysis such as discounted cash flow and <b>decision</b> tree <b>analysis</b> largely irrelevant for strategic decision-making. In place of such traditional methods, this article proposes the use of <b>decision</b> options <b>analysis,</b> a real-options-based methodology that holds out the promise of better investment and portfolio decisions with "fewer" data requirements. Moreover, <b>decision</b> options <b>analysis</b> can be implemented with software tools that allow systematic application of these techniques. This article presents case studies that outline the actual application of real options analysis in licensing transactions and portfolio management in biosciences companies. 2005 Morgan Stanley. ...|$|R
3000|$|... eFor more {{detailed}} information about <b>Decision</b> Point <b>Analysis</b> supported in ProM framework please check the link: [URL] [...]...|$|R
25|$|The {{choice of}} {{selection}} criterion for <b>decision</b> <b>analysis</b> is subjective. The choice of criterion is made {{outside of the}} analysis (it is exogenous). One of the influences on this choice on this is attitude to risk. Risk aversion describes how willing or unwilling someone is to take risks. Evidence indicates that most, but not all, individuals prefer certain outcomes to uncertain ones. Risk-averse individuals prefer decision criteria that reduce the chance of the worst possible outcome, while risk-seeking individuals prefer decision criteria that maximize the chance of the best possible outcome. In terms of returns on investment, if {{society as a whole}} is risk-averse, we might be willing to accept some investments with negative expected returns, e.g., in mitigation. Such investments may help to reduce the possibility of future climate damages or the costs of adaptation.|$|E
2500|$|The optimal {{result of}} <b>decision</b> <b>analysis</b> {{depends on how}} [...] "optimal" [...] is defined (Arrow et al., 1996. See also the section on trade offs). <b>Decision</b> <b>analysis</b> {{requires}} a selection criterion to be specified. In a <b>decision</b> <b>analysis</b> based on monetized cost–benefit analysis (CBA), the optimal policy is evaluated in economic terms. The optimal result of monetized CBA maximizes net benefits. Another type of <b>decision</b> <b>analysis</b> is cost-effectiveness analysis. Cost-effectiveness analysis aims to minimize net costs.|$|E
2500|$|Skinner, David, Introduction to <b>Decision</b> <b>Analysis,</b> 2nd Edition (1999). Probabilistic.|$|E
40|$|This paper {{presents}} {{data mining}} techniques {{that can be}} used to study voting patterns in the United States House of Representatives and shows how the results can be interpreted. We processed the raw data available at [URL] performed t-weight calculations, an attribute relevance study, association rule mining, and <b>decision</b> tree <b>analysis</b> and present and interpret interesting results. WEKA and SQL Server 2005 were used for mining association rules and <b>decision</b> tree <b>analysis...</b>|$|R
2500|$|The {{two most}} common tools are <b>Decision</b> Tree <b>Analysis</b> (DTA) and Real options {{valuation}} (ROV); they may often be used interchangeably: ...|$|R
40|$|Abstract Background <b>Decision</b> curve <b>analysis</b> {{is a novel}} {{method for}} {{evaluating}} diagnostic tests, prediction models and molecular markers. It combines the mathematical simplicity of accuracy measures, such as sensitivity and specificity, with the clinical applicability of decision analytic approaches. Most critically, <b>decision</b> curve <b>analysis</b> can be applied directly to a data set, and {{does not require the}} sort of external data on costs, benefits and preferences typically required by traditional decision analytic techniques. Methods In this paper we present several extensions to <b>decision</b> curve <b>analysis</b> including correction for overfit, confidence intervals, application to censored data (including competing risk) and calculation of decision curves directly from predicted probabilities. All of these extensions are based on straightforward methods that have previously been described in the literature for application to analogous statistical techniques. Results Simulation studies showed that repeated 10 -fold crossvalidation provided the best method for correcting a decision curve for overfit. The method for applying decision curves to censored data had little bias and coverage was excellent; for competing risk, decision curves were appropriately affected by the incidence of the competing risk and the association between the competing risk and the predictor of interest. Calculation of decision curves directly from predicted probabilities led to a smoothing of the decision curve. Conclusion <b>Decision</b> curve <b>analysis</b> can be easily extended to many of the applications common to performance measures for prediction models. Software to implement <b>decision</b> curve <b>analysis</b> is provided. </p...|$|R
2500|$|At New England, Anderson {{specialized}} in risk and uncertainty, in particular Bayesian statistics, stochastic processes and efficiency analysis, and modern decision theory {{as applied to}} agricultural economics. [...] Along with Dillon and J. Brian Hardaker, Anderson co-authored a 1977 book, Agricultural <b>Decision</b> <b>Analysis,</b> which served as an influential source on risk and <b>decision</b> <b>analysis</b> for agricultural economics researchers and the agricultural industry. [...] The American Journal of Agricultural Economics described the expository text as [...] "Something of a classic in the field, especially amongst graduate students of U.S. colleges." ...|$|E
2500|$|<b>Decision</b> <b>Analysis</b> in Aerospace Medicine: Costs and Benefits of a Hyperbaric Facility in Space, John-Baptiste, A; Cook, T; Straus, S; Naglie, G; et al. Aviation, Space, and Environmental Medicine, Volume 77, Number 4, April 2006, pp.434–443(10) ...|$|E
2500|$|Jock Robert Anderson (born 23 January 1941) is an Australian {{agricultural}} economist, specialising {{in agricultural}} development economics, risk and decision theory, and international rural development policy. [...] Born in Monto, Queensland, he {{studied at the}} University of Queensland, attaining bachelor's and master's degrees in agricultural science. [...] After graduation, Anderson joined the Faculty of Agricultural Economics at the University of New England. [...] At New England, he focused on research in farm management, risk, and uncertainty and received a doctor of philosophy in economics in 1970. [...] In 1977, Anderson co-authored a book, Agricultural <b>Decision</b> <b>Analysis,</b> which has served as an influential source on risk and <b>decision</b> <b>analysis</b> for agricultural economics researchers and the agricultural industry.|$|E
50|$|Competitor {{analysis}} typically {{starts with}} an {{identification of the}} market structure and proceed with complex strategic <b>decision</b> making <b>analysis</b> of Game Theory.|$|R
50|$|Once the inputs are {{received}} a <b>decision</b> tree <b>analysis</b> tool uses the inputs; builds a step-by-step decision tree {{and makes a}} decision recommendation.|$|R
5000|$|KnowledgeSEEKER is a {{data mining}} product. Its {{features}} include data profiling, data visualization and <b>decision</b> tree <b>analysis.</b> It was first released in 1990.|$|R
2500|$|In 2004 {{the second}} edition of Coping with Risk in Agriculture, co-authored by Anderson, Hardaker, Ruud B. M. Huirne, G. Lien, was published. [...] The book was {{intended}} as a replacement text for Agricultural <b>Decision</b> <b>Analysis</b> and was targeted at senior undergraduate or graduate students of farm management, agricultural researchers, farm advisors, veterinarians, farmers, and policy makers.|$|E
2500|$|Dominance-based {{rough set}} {{approach}} (DRSA) {{is an extension}} of rough set theory for multi-criteria <b>decision</b> <b>analysis</b> (MCDA), introduced by Greco, Matarazzo and Słowiński (2001). The main change in this extension of classical rough sets is the substitution of the indiscernibility relation by a dominance relation, which permits the formalism to deal with inconsistencies typical in consideration of criteria [...] and preference-ordered decision classes.|$|E
2500|$|A key concept {{from the}} science of {{economics}} is [...] "utility": {{a measure of how}} valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, <b>decision</b> <b>analysis,</b> and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.|$|E
40|$|A {{recently}} developed model of Internet affinity {{was used for}} survey design and data collection on variables that have potential influence on affinity for Internet use. A total of 600 Croatian students {{with access to the}} Internet at their college participated in this survey. The collected data were used for investigation of the relation between <b>decision</b> tree <b>analysis</b> and regression analysis of predictor variables of Internet affinity. Different predictors were found to influence two distinct criteria of Internet affinity (1) frequency of Internet use and (2) desire to use the Internet. <b>Decision</b> tree <b>analysis</b> and regression analysis manifested much similarity in the uncovered lists of variables that affected the criteria. However, the results of <b>decision</b> tree <b>analysis</b> were more informative since they provided greater insight into the structural relations of important predictor variables and also into specific subgroups of subjects in the survey...|$|R
5000|$|Capital budgeting: Risk {{representation}} {{ranges from}} flat adjustments to cash flows and duration via risk adjusted discount rates to <b>decision</b> tree <b>analysis,</b> stochastic simulation and real options.|$|R
40|$|Abstract: DSPSR is a high-performance, open-source, object-oriented, {{digital signal}} {{processing}} software library and application suite for use in radio pulsar astronomy. Written primarily in Cþþ, the library implements an extensive range of modular algorithms that can optionally exploit both multiple-core processors and general-purpose graphics processing units. After over a decade of research and development, DSPSR is now stable and in widespread use in the community. This paper presents {{a detailed description of}} its functionality, justification of major design <b>decisions,</b> <b>analysis</b> of phase-coherent dispersion removal algorithms, and demonstration of performance on some contemporary microprocessor architectures...|$|R
2500|$|Furthermore, {{some of the}} {{simplest}} elements of Modern Portfolio Theory are applicable to virtually any kind of portfolio. [...] The concept of capturing the risk tolerance of an investor by documenting how much risk is acceptable for a given return may be applied {{to a variety of}} <b>decision</b> <b>analysis</b> problems. [...] MPT uses historical variance as a measure of risk, but portfolios of assets like major projects don't have a well-defined [...] "historical variance". [...] In this case, the MPT investment boundary can be expressed in more general terms like [...] "chance of an ROI less than cost of capital" [...] or [...] "chance of losing {{more than half of the}} investment". [...] When risk is put in terms of uncertainty about forecasts and possible losses then the concept is transferable to various types of investment.|$|E
50|$|In 1992 the <b>Decision</b> <b>Analysis</b> Society awarded Schlaifer the Frank P. Ramsey Medal for {{distinguished}} {{contributions to}} the field of <b>decision</b> <b>analysis.</b> In making the award Bob Winkler noted Schlaifer's pioneering efforts in <b>decision</b> <b>analysis.</b>|$|E
5000|$|<b>Decision</b> <b>Analysis</b> Publication Award for the year, <b>Decision</b> <b>Analysis</b> Society of the Institute for Operations Research and the Management Sciences (1994) ...|$|E
40|$|The study aims {{to gather}} and {{organize}} information for decision making against the problems arising in Power Engineering Consultant 2 ̆ 7 s survey and soil investigation product due to new policy in production cost efficiency that is implemented in 2012. The study conducted using Kepner and Tragoe 2 ̆ 7 s analytical process that consisted of four stages analytical process such as situation <b>analysis,</b> problem <b>analysis,</b> <b>decision</b> making <b>analysis</b> and potential problem analysis. As for the <b>decision</b> making <b>analysis,</b> the analysis will be conducted using Mix Method research. Starting with the qualitative method that involving detailed exploration from interview session with selected person and followed by a quantitative method, in which SMART analysis method is implemented. As the input of <b>decision</b> making <b>analysis,</b> there are five alternatives solution for Power Engineering Consultant 2 ̆ 7 s problem, such as: fix the problem and continue to cooperate with third party institutions, establish cooperation with external surveyor team, establish internal surveyor team, acquisition of one selected surveyor company, and establishing “umbrella contract” with selected surveyor company. After the <b>decision</b> making <b>analysis</b> is conducted, the alternative to establishing “umbrella contract” with selected surveyor company is selected as the best alternative solution for Power Engineering Consultant 2 ̆ 7 s problem. The result is considered as the best solution since the decision makers {{are involved in the}} entire decision making process. Based on this result, the implementation plan then made to be in line with Power Engineering Consultant 2 ̆ 7 s current conditio...|$|R
40|$|In {{this paper}} we present our new {{paradigm}} {{for dealing with}} the inference problem which arises from downgrading. Our new paradigm has two main parts: the application of <b>decision</b> tree <b>analysis</b> to the inference problem, and the concept of parsimonious downgrading. We also include a new thermodynamically motivated way of dealing with the deduction of inference rules from partial data. Keywords Data mining, inference, downgrading, rules. 1. A NEW PARADIGM Our new paradigm is a combination of <b>decision</b> tree <b>analysis</b> and parsimonious downgrading. <b>Decision</b> tree <b>analysis</b> has existed in the field of AI since the 1980 's [3]. In brief, decision trees are graphs associated to data, with the goal of deducing rules from the data. Our new paradigm applies decision trees to the inference problem. In this paper we introduce the new concept of parsimonious downgrading. When High wishes to downgrade a set of data to Low, it may be necessary, because of inference channels, to trim the set. Parsimonious [...] ...|$|R
5000|$|In other words, for {{any given}} <b>decision,</b> info-gap's <b>analysis</b> yields the same results for all total regions of {{uncertainty}} that contain [...] This applies to both the robustness and opportuneness models.|$|R
