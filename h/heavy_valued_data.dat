0|10000|Public
5000|$|The {{theory was}} deeply ethnocentric—it makes <b>heavy</b> <b>value</b> judgements on {{different}} societies; with Western civilization {{seen as the}} most valuable.|$|R
40|$|Abstract. Definition of {{conversion}} {{from a single}} <b>value</b> <b>data</b> to the Vague <b>value</b> <b>data</b> is given; two conversion formulas from a single <b>value</b> <b>data</b> to the Vague <b>value</b> <b>data</b> are given; a similarity measure formula between Vague sets are given; Vague pattern recognition algorithm is given. The algorithm is applied to irrigation system design, application examples show that theVague pattern recognition algorithms and formulas are all useful...|$|R
50|$|Feelings {{of shame}} are also not uncommon. Many {{cultures}} place a <b>heavy</b> <b>value</b> on cleanliness, {{and refusing to}} bathe can make someone the target of mockery or teasing, which can increase {{the severity of the}} phobia. It may also cause the sufferer to not seek treatment.|$|R
40|$|The {{missing values}} are not {{uncommon}} in real data sets. The algorithms and methods used for the data analysis of complete data sets cannot always be applied to missing <b>value</b> <b>data.</b> In order to use the existing methods for complete <b>data,</b> the missing <b>value</b> <b>data</b> sets are preprocessed. The other {{solution to this problem}} is creation of new algorithms dedicated to missing <b>value</b> <b>data</b> sets...|$|R
40|$|In this article, {{we discuss}} methods for {{analysis}} of distribution <b>valued</b> <b>data</b> {{as a kind of}} symbolic data. Most methods for data analysis assume that the data are sets of numbers with structure. For example, typical multivariate data are identified as a set of n vectors of real numbers and dissimilarity data on pairs of n objects are as matrix. However, requests {{for analysis of}} data with new models become higher, as the kind and quantity of the data is increased. In order to overcome this problem, Symbolic Data Analysis (SDA) supplies various <b>data</b> descriptions; interval <b>valued</b> <b>data,</b> modal interval data, categorical <b>data,</b> distribution <b>valued</b> <b>data,</b> etc. Distribution <b>valued</b> <b>data</b> is fruitful because of its ability of expression. An approach to analyze distribution <b>valued</b> <b>data</b> is the use of functions (density function, cumulative distribution function, quantile function etc.) We focus on quantile functions to describe the objects or data. Another great approach to deal with such complex data is Functional Data Analysis (FDA.) In FDA, the objects or data are described by functions. Then we can use techniques of FDA to analyze the quantile functions. We introduce methods of clustering and multidimensional scaling (MDS) for distribution <b>valued</b> <b>data</b> and related topics...|$|R
40|$|An {{electrochemical}} method of separating <b>heavy</b> metal <b>values</b> from a radioactive molten salt including Li halide at temperatures of about 500 {degree}C. The method comprises positioning a solid Li-Cd alloy anode in the molten salt containing the <b>heavy</b> metal <b>values,</b> positioning a Cd-containing cathode or a solid cathode positioned above a catch crucible in the molten salt {{to recover the}} <b>heavy</b> metal <b>values,</b> establishing a voltage drop between the anode and the cathode to deposit material at the cathode to reduce the concentration of heavy metals in the salt, and controlling the deposition rate at the cathode by controlling the current between the anode and cathode...|$|R
40|$|Disregard {{restriction}} header and footer - The Director, The Army Basing Study (TABS) {{asked that}} we evaluate the Army’s process for collecting certified installation and leased facility military <b>value</b> <b>data</b> {{as part of}} our audit support for TABS 2005. We focused our effort on determining whether: The 2005 Army Basing Study Group had a sound process in place to collect certified military <b>value</b> <b>data</b> that was adequately supported with appropriate evidentiary matter and was accurate. Management controls were in place and operating for the military <b>value</b> <b>data</b> call...|$|R
40|$|In {{this paper}} a robust fuzzy k-means {{clustering}} model for interval <b>valued</b> <b>data</b> is introduced. The peculiarity {{of the proposed}} model is the capability to manage anomalous interval <b>valued</b> <b>data</b> by reducing the effects of such outliers in the clustering model. In the interval case, the concept of anomalous data involves both {{the center and the}} width (the radius) of an interval. In order to show how our model works the results of a simulation experiment and an application to real interval <b>valued</b> <b>data</b> are discussed. © Springer-Verlag 2006...|$|R
50|$|The {{notion of}} fed speak {{originated}} {{from the fact}} that financial markets placed a <b>heavy</b> <b>value</b> on the statements made by Federal Reserve governors, which could in turn lead to a self-fulfilling prophecy. To prevent this, the governors developed a language, termed fedspeak, in which ambiguous and cautious statements were made to purposefully obscure and detract meaning from the statement.|$|R
5000|$|Finding {{particular}} strings in key names, value {{names and}} <b>value</b> <b>data</b> ...|$|R
50|$|Result /analyze: Any {{specific}} {{format of}} result of capture <b>value</b> <b>data</b> presentation.|$|R
5000|$|Creating, manipulating, {{renaming}} and deleting Registry keys, subkeys, <b>values</b> and <b>value</b> <b>data</b> ...|$|R
40|$|In {{this paper}} we address {{problems}} arising {{from the use of}} categorical <b>valued</b> <b>data</b> in rule induction. By naively using categorical values in rule induction, we risk reducing the chances of finding a good rule in terms both of confidence (accuracy) and of support or coverage. In this paper we introduce a technique called arcsin transformation where categorical <b>valued</b> <b>data</b> is replaced with numeric values. Our results show that on relatively large databases, containing many unordered categorical attributes, larger databases incorporating both unordered and numeric data, and especially those databases that are small containing rare cases, this technique is highly effective when dealing with categorical <b>valued</b> <b>data...</b>|$|R
40|$|This paper calls {{attention}} to the implications of using <b>value</b> <b>data</b> in efficiency measurement through Data Envelopment Analysis (DEA). The main contributions are twofold: (i) it provides a reconciliation of the previous literature on analysing issues of quantity and <b>value</b> <b>data</b> in efficiency measurement, (ii) it provides some guidelines on what to do, when these issues arise in a data set...|$|R
40|$|Project (M. B. A., Business Administration (Finance)) [...] California State University, Sacramento, 2013. This project {{lays the}} groundwork for {{simplified}} performance measurement while incorporating price/book <b>value</b> <b>data</b> for further analysis Portfolio holding data are obtained from Fidelity monthly account statements. Book <b>value</b> <b>data</b> are obtained from Mergent Online. The conclusions were generally inconclusive, and require further study as the Student Investment Fund matures. Business Administration (Finance...|$|R
40|$|Many machine {{learning}} and neurally inspired algorithms are limited, {{at least in}} their pure form, to working with nominal data. However, for many real-world problems, some provision {{must be made to}} support processing of continuously <b>valued</b> <b>data.</b> This paper presents empirical results obtained by using six different discretization methods as preprocessors to three different supervised learners on several real-world problems. No discretization technique clearly outperforms the others. Also, discretization as a preprocessing step is in many cases found to be inferior to direct handling of continuously <b>valued</b> <b>data.</b> These results suggest that {{machine learning}} algorithms should be designed to directly handle continuously <b>valued</b> <b>data</b> rather than relying on preprocessing or ad hoc techniques. 1...|$|R
2500|$|... i.e. reformulated gasoline. Has lower heating <b>value</b> <b>data,</b> actual {{energy content}} is higher see {{higher heating value}} ...|$|R
30|$|The planktonic δ 13 C {{is usually}} {{used as a}} paleoproductivity proxy in surface waters (Berger et al. 1978). The {{comparison}} between the planktonic isotopic values reveals perfect correlation between the δ 18 O and the δ 13 C records (Figure  2). During the late Holocene, the planktonic δ 13 C values exhibit a decreasing trend, the YD and the HE 1 show two main enrichments and the last glacial is marked generally by <b>heavier</b> <b>values.</b>|$|R
40|$|The process e^- γ→ e^+ W_R^- W_R^- is {{studied in}} the {{framework}} of the Left-Right symmetric model. It is shown that this reaction and e^- γ→ l^+ W_R^- W_R^- for the arbitrary final lepton are likely to be discovered for CLIC collider option. For relatively light doubly charged Higgs boson its mass does not have much influence on the discovery potential, while for <b>heavier</b> <b>values</b> the probability of the reaction increases. Comment: 18 pages, 7 figures, LaTe...|$|R
40|$|An {{integrated}} {{study of}} the geochemistry, sedimentology and biostratigraphy of two outcrop sections of the La Luna Formation in Western Venezuela was carried out to determine the eustatic changes that influenced the deposition and preservation of anoxic sediments in the Late Cretaceous. Five (5) depositional sequences within the La Luna Formation were identified in the studied outcrop sections (Las Hernández and San Miguel) by shifts in 13 C stable isotopes, {{as well as in}} CaCO 3, Si, and Ti elemental content (normalized to Al). Transgressive system tracts (TST) are characterized by <b>heavier</b> <b>values</b> in 13 C isotopes, lower CaCO 3, and higher content of TOC. During these periods the values of Si/Al and Ti/Al show an increasing trend and reach their highest peaks at the maximum flooding surfaces (MFS). Highstand system tracts (HST) are characterized by high contents of CaCO 3, and a shift towards lighter values of 13 C isotopes. The sequence boundaries (SB) are evidenced by a rapid shift in the 13 C isotopes (from lighter to <b>heavier</b> <b>values)</b> and by the geochemical contrast between the HST and the TST. The identified systems tracts (Turonian-Campanian), clearly correspond with the Haq et al (1988) global sea-level change curv...|$|R
50|$|TSearch: a {{powerful}} tool that reads process memory and writes process memory. Like Cheat Engine, it can change client-side <b>values</b> <b>data.</b>|$|R
40|$|This paper {{presents}} empirical {{results obtained}} by using six different discretization methods as preprocessors to three different supervised learners on several real-world problems. No discretization technique clearly outperforms the others. Also, discretization as a preprocessing step {{is in many}} cases found to be inferior to direct handling of continuously <b>valued</b> <b>data.</b> These results suggest that machine learning algorithms should be designed to directly handle continuously <b>valued</b> <b>data</b> {{rather than relying on}} preprocessing or ad hoc techniques...|$|R
50|$|Founded in November 2001, Vormetric marketed {{software}} and hardware for encryption and key management to protect high <b>value</b> <b>data</b> from data loss and theft.|$|R
50|$|Primitive LPC types (int, string, status, float, etc.) {{are passed}} by <b>value.</b> <b>Data</b> {{structure}} types (object, array, mapping, class, struct) are passed by reference.|$|R
50|$|In {{addition}} to typical <b>data</b> <b>values,</b> the <b>data</b> <b>value</b> store contains a special {{type of data}} for storing relationships between tables. This functions similarly to foreign keys in RDBMS structures, but with a CDBMS, the relationship is known by the dictionary and stored as a <b>data</b> <b>value,</b> making navigation between tables completely automatic.|$|R
40|$|Much of the {{empirical}} work in urban economics focuses on Chicago {{because of the}} land <b>value</b> <b>data</b> gathered by Hoyt and Olcott. This article presents new land <b>value</b> <b>data</b> for Cleveland, Ohio and I uses it to show that while the urban form of Cleveland evolved much as Chicago's did, {{the timing of the}} changes was quite different. These results suggest that urban economists must be careful about inferring too much about the evolution of urban form from the work that has been done on Chicago. ...|$|R
40|$|The aim of {{this paper}} is to cluster units (objects) {{described}} by interval-valued information by adopting an unsupervised neural network approach. By considering a suitable distance measure for interval data, Self-Organizing Maps to deal with interval <b>valued</b> <b>data</b> are suggested. The technique, called Midpoint Radius Self-Organizing Maps (MR-SOMs), recovers the underlying structure of interval <b>valued</b> <b>data</b> by using both the midpoints (or centers) and the radii (a measure of the interval width) information. In order to show how the method MR-SOMs works a suggestive application on telecommunication market segmentation is described...|$|R
3000|$|..., where a, b, and c {{are real}} numbers. We obtain the local well-posedness for the Cauchy problem with low {{regularity}} initial <b>value</b> <b>data</b> by the Fourier restriction norm method.|$|R
40|$|A {{watermark}} generator {{for providing}} a watermark signal as {{a sequence of}} subsequent watermark coefficients based on a stream of subsequent stream values representing discrete <b>valued</b> <b>data</b> comprises a differential encoder. The differential encoder is configured to apply a phase rotation to a current stream value of the stream values representing the discrete <b>valued</b> <b>data</b> or to a current watermark symbol, the current watermark symbol corresponding to a current stream value of the stream values representing the discrete <b>valued</b> <b>data,</b> to obtain a current watermark coefficient of the watermark signal. The differential encoder is configured to derive a phase of a previous spectral coefficient of a watermarked signal which {{is a combination of}} the host signal and the watermark signal. The differential encoder is further configured to provide the watermark signal such that a phase angle of the phase rotation applied to the current stream value or the current watermark symbol is dependent on the phase of the previous spectral coefficient of the watermarked signal...|$|R
40|$|There is a {{fundamental}} upper limit on the extent that any classifier [...] - neural network or otherwise [...] - can discriminate between two classes of discrete <b>valued</b> <b>data.</b> When classes overlap in feature space, discrimination will be less than perfect. In a two alternative forced-choice decision problem, a classifier's discrimination ability is often {{measured in terms of}} the area under its receiver operating characteristic (ROC) curve. In this paper, we show how to calculate the maximum possible area under the ROC curve obtained with a particular representation of a given set of discrete <b>valued</b> <b>data.</b> This result corresponds to the fundamental upper limit of discrimination. We extend this result to show how to estimate both the maximum and the average discrimination we can expect to achieve on unseen test data, again, for a particular representation of a discrete <b>valued</b> <b>data</b> set. These results have practical engineering application in the selection of discriminative features for neural netwo [...] ...|$|R
50|$|There is an {{extension}} to the restricted Boltzmann machine that affords using real <b>valued</b> <b>data</b> rather than binary data. Along with higher order Boltzmann machines, it is outlined here https://www.youtube.com/watch?v=VdIURAu1-aU.|$|R
40|$|A {{computer}} program written by author in Delphi for statistical {{control of production}} processes is presented. The JM-SPC program {{can be used for}} both the variable valued and the attribute <b>valued</b> <b>data</b> type. The data are introduced manually into table (up to 20 columns; up to 10000 rows). The option of data introducing is with some function equipped, which make this process very easy. The data analysis option offer many of analytical techniques. For variable <b>valued</b> <b>data</b> sets following control tools are available: individual values chart, average values chart, moving average values chart, exponentially moving average values chart, median values chart, range chart, standard deviation chart. Also two-signal control charts are available, for example: X-R, X-S, Me-R, Me-S. For variable <b>valued</b> <b>data</b> the performance of processes can be evaluated by using of two commonly used indices: potential process capability index (Cp) and process capability index (Cpk). For the attribute <b>valued</b> <b>data</b> type the user can select: p- and np-chart, c- and u-chart, Pareto diagram. Application of JM-SPCprogram for statistical analysis of real processes has been presented on the example of Al-Si high-pressure die casting production. For example of production of two different frame castings following analyses have been conducted: 1) the analysis of defect fraction in sample (by using the p-chart), 2) the analysis of relative differences in importance of each defects categories (by using of the Pareto diagram) ...|$|R
30|$|The {{calculated}} {{number of}} positive cells {{was confirmed by}} manual counting in three fields of view, and the calculated value correlated very well with the manually counted <b>values</b> (<b>data</b> not shown).|$|R
3000|$|... [...]. a, b, and c {{are real}} numbers. We are {{interested}} in obtaining the well-posedness for the Cauchy problem of (1.1) with initial <b>value</b> <b>data</b> under low regularity (which means [...]...|$|R
40|$|In {{this paper}} we show {{how to use}} <b>value</b> <b>data</b> (price times quantity) to {{construct}} Fisher price and quantity indexes. In particular, we think of revenue and expenditure data. This model extends the work of Cross and Färe, who showed how to recover relative prices from <b>value</b> <b>data</b> with no expli-cit price or quantity information. We examine the accuracy of our model over a range of price changes, firm sample sizes, and response variation, in a Monte Carlo experiment in which firms respond to price changes with error. The model outperforms it component indexes with accuracy levels that increase with response variation...|$|R
40|$|This {{contribution}} {{presents a}} variation of the Wiener filter criterion, i. e. minimizing the mean squared error, by combining it with the main principle of normalized convolution, i. e. the introduction of prior information in the filter process via the certainty map. Thus, we are able to optimize a filter according to the signal and noise characteristics while preserving edges in images. In spite of its low computational costs the proposed filter scheme outperforms state of the art filter methods working also in the spatial domain. Furthermore, the Wiener filter paradigm is extended from scalar <b>valued</b> <b>data</b> to tensor <b>valued</b> <b>data...</b>|$|R
