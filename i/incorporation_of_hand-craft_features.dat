0|10000|Public
30|$|Other {{approaches}} include [10], {{in which}} the tools of social network analysis are applied to graphs constructed from netflow data to detect {{a wide range of}} intrusive behaviors, including scans. In [11], the RIPPER data mining tool is applied to a set <b>of</b> <b>hand-crafted</b> <b>feature</b> vectors in order to learn novel rules to classify network scans.|$|R
40|$|The paper {{presents}} a novel concept for collaborative descriptors between deeply learned and <b>hand-crafted</b> <b>features.</b> To achieve this concept, we apply convolutional maps for pre-processing, namely the convovlutional maps {{are used as}} input <b>of</b> <b>hand-crafted</b> <b>features.</b> We recorded {{an increase in the}} performance rate of + 17. 06 % (multi-class object recognition) and + 24. 71 % (car detection) from grayscale input to convolutional maps. Although the framework is straight-forward, the concept should be inherited for an improved representation. Comment: CVPR 2017 Workshop Submissio...|$|R
40|$|Abstract. Most {{existing}} {{machine vision}} systems perform recognition {{based on a}} xed set <b>of</b> <b>hand-crafted</b> <b>features,</b> geometric models, or eigen-subspace decomposition. Drawing from psychology, neuroscience and in-tuition, we show that certain aspects of human performance in visual discrimination cannot be explained by any of these techniques. We ar-gue that many practical recognition tasks for articial vision systems operating under uncontrolled conditions critically depend on incremen-tal learning. Loosely motivated by visuocortical processing, we present feature representations and learning methods that perform biologically plausible functions. The paper concludes with experimental results gen...|$|R
40|$|Most phoneme {{recognition}} state-of-the-art systems rely on {{a classical}} neural network classifiers, fed with highly tuned features, such as MFCC or PLP features. Recent advances in ``deep learning'' approaches questioned such systems, but while some attempts were made with simpler features such as spectrograms, state-of-the-art systems still rely on MFCCs. This might {{be viewed as a}} kind of failure from deep learning approaches, which are often claimed {{to have the ability to}} train with raw signals, alleviating the need <b>of</b> <b>hand-crafted</b> <b>features.</b> In this paper, we investigate a convolutional neural network approach for raw speech signals. While convolutional architectures got tremendous success in computer vision or text processing, they seem to have been let down in the past recent years in the speech processing field. We show that it is possible to learn an end-to-end phoneme sequence classifier system directly from raw signal, with similar performance on the TIMIT and WSJ datasets than existing systems based on MFCC, questioning the need <b>of</b> complex <b>hand-crafted</b> <b>features</b> on large datasets. Comment: NIPS Deep Learning Workshop, 201...|$|R
5000|$|Jacqueline [...] "Jackie" [...] Winsor (born October 20, 1941, in St. John's, Newfoundland) is a Canadian-American sculptor. Her style, which {{developed}} in the early 1970s {{as a reaction to}} the work of minimal artists, has been characterized as post-minimal, anti-form, and process art. Informed by her own personal history, Winsor's sculptures from this period sit at the intersection of Minimalism and feminism, maintaining an attention to elementary geometry and symmetrical form while eschewing Minimalism's reliance on industrial materials and methods through the <b>incorporation</b> <b>of</b> <b>hand-crafted,</b> organic materials such as wood and hemp.|$|R
40|$|In {{supervised}} {{approaches for}} keyphrase extraction, a candidate phrase is encoded {{with a set}} <b>of</b> <b>hand-crafted</b> <b>features</b> and machine learning algorithms are trained to discriminate keyphrases from non-keyphrases. Although the manually-designed features have shown to work well in practice, feature engineering is a difficult process that requires expert knowledge and normally does not generalize well. In this paper, we present SurfKE, a feature learning framework that exploits the text itself to automatically discover patterns that keyphrases exhibit. Our model represents the document as a graph and automatically learns feature representation of phrases. The proposed model obtains remarkable improvements in performance over strong baselines. Comment: To appear in AAAI 2018 Student Abstract and Poster Progra...|$|R
40|$|Hashing-based methods seek {{compact and}} {{efficient}} binary codes that preserve the neighborhood {{structure in the}} original data space. For most existing hashing methods, an image is first encoded as a vector <b>of</b> <b>hand-crafted</b> visual <b>feature,</b> followed by a hash projection and quantization step to get the compact binary vector. Most <b>of</b> the <b>hand-crafted</b> <b>features</b> just encode the low-level information of the input, the feature may not preserve the semantic similarities of images pairs. Meanwhile, the hashing function learning process is independent with the feature representation, so the feature may not be optimal for the hashing projection. In this paper, we propose a supervised hashing method based on a well designed deep convolutional neural network, which tries to learn hashing code and compact representations of data simultaneously. The proposed model learn the binary codes by adding a compact sigmoid layer before the loss layer. Experiments on several image data sets show that the proposed model outperforms other state-of-the-art methods. Comment: 7 pages, 5 figure...|$|R
40|$|Motivated by {{the large}} number of {{languages}} (seven) and the short development time (two months) of the 2009 CoNLL shared task, we exploited latent variables to avoid the costly process <b>of</b> <b>hand-crafted</b> <b>feature</b> engineering, allowing the latent variables to induce features from the data. We took a pre-existing generative latent variable model of joint syntacticsemantic dependency parsing, developed for English, and applied it to six new languages with minimal adjustments. The parser’s robustness across languages indicates that this parser has a very general feature set. The parser’s high performance indicates that its latent variables succeeded in inducing effective features. This system was ranked third overall with a macro averaged F 1 score of 82. 14 %, only 0. 5 % worse than the best system. ...|$|R
30|$|Gradients (HOG) [17], SIFT [18], and Harr-like. Before {{the success}} <b>of</b> CNN-based {{approaches}}, <b>hand-crafted</b> <b>feature</b> approaches such as deformable part-based model (DPM) [19] have achieved the state-of-art performance. DPM explores improved HOG feature to describe {{each part of}} vehicle and followed by classifiers like SVM and Adaboost. However, <b>hand-crafted</b> <b>feature</b> approaches have low feature representation.|$|R
40|$|Most {{existing}} {{machine vision}} systems perform recognition {{based on a}} xed set <b>of</b> <b>hand-crafted</b> <b>features,</b> geometric models, or eigen-subspace decomposition. Drawing from psychology, neuroscience and intuition, we show that certain aspects of human performance in visual discrimination cannot be explained by any of these techniques. We argue that many practical recognition tasks for articial vision systems operating under uncontrolled conditions critically depend on incremental learning. Loosely motivated by visuocortical processing, we present feature representations and learning methods that perform biologically plausible functions. The paper concludes with experimental results generated by our method. 1 Introduction How exible are the representations for visual recognition, encoded by the neurons of the human visual cortex? Are they predetermined by a xed developmental schedule, or does their development depend on their stimulation? Does their development cease at some poin [...] ...|$|R
40|$|Compared {{to other}} {{applications}} in computer vision, convolutional neural networks have under-performed on pedestrian detection. A breakthrough was made very recently by using sophisticated deep CNN models, {{with a number}} <b>of</b> <b>hand-crafted</b> <b>features,</b> or explicit occlusion handling mechanism. In this work, we show that by re-using the convolutional feature maps (CFMs) of a {{deep convolutional neural network}} (DCNN) model as image features to train an ensemble of boosted decision models, we are able to achieve the best reported accuracy without using specially designed learning algorithms. We empirically identify and disclose important implementation details. We also show that pixel labelling may be simply combined with a detector to boost the detection performance. By adding complementary <b>hand-crafted</b> <b>features</b> such as optical flow, the DCNN based detector can be further improved. We set a new record on the Caltech pedestrian dataset, lowering the log-average miss rate from 11. 7 % to 8. 9 %, a relative improvement of 24 %. We also achieve a comparable result to the state-of-the-art approaches on the KITTI dataset. Comment: Fixed some typo...|$|R
40|$|State-of-the-art {{sequence}} labeling systems traditionally require {{large amounts}} of task-specific knowledge in the form <b>of</b> <b>hand-crafted</b> <b>features</b> and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable {{to a wide range}} of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks [...] - Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data [...] - 97. 55 % accuracy for POS tagging and 91. 21 % F 1 for NER. Comment: 10 pages, 3 figures. To appear on ACL 201...|$|R
40|$|Emotion {{recognition}} from speech {{may play a}} crucial role in many applications related to human–computer interaction or understanding the affective state of users in certain tasks, where other modalities such as video or physiological parameters are unavailable. In general, a human’s emotions may be recognized using several modalities such as analyzing facial expressions, speech, physiological parameters (e. g., electroencephalograms, electrocardiograms) etc. However, measuring of these modalities may be difficult, obtrusive or require expensive hardware. In that context, speech may be the best alternative modality in many practical applications. In this work we present an approach that uses a Convolutional Neural Network (CNN) functioning as a visual feature extractor and trained using raw speech information. In contrast to traditional machine learning approaches, CNNs are responsible for identifying the important features of the input thus, making the need <b>of</b> <b>hand-crafted</b> <b>feature</b> engineering optional in many tasks. In this paper no extra features are required other than the spectrogram representations and <b>hand-crafted</b> <b>features</b> were only extracted for validation purposes of our method. Moreover, it does not require any linguistic model and is not specific to any particular language. We compare the proposed approach using cross-language datasets and demonstrate that it is able to provide superior results vs. traditional ones that use <b>hand-crafted</b> <b>features...</b>|$|R
40|$|We propose DeepBreath, a deep {{learning}} model which automatically recognises people's psychological stress level (mental overload) from their breathing patterns. Using a low cost thermal camera, we track a person's breathing patterns as temperature changes around his/her nostril. The paper's technical contribution is threefold. First of all, instead <b>of</b> creating <b>hand-crafted</b> <b>features</b> to capture aspects of the breathing patterns, we transform the uni-dimensional breathing signals into two dimensional respiration variability spectrogram (RVS) sequences. The spectrograms easily capture {{the complexity of the}} breathing dynamics. Second, a spatial pattern analysis based on a deep Convolutional Neural Network (CNN) is directly applied to the spectrogram sequences without the need <b>of</b> <b>hand-crafting</b> <b>features.</b> Finally, a data augmentation technique, inspired from solutions for over-fitting problems in {{deep learning}}, is applied to allow the CNN to learn with a small-scale dataset from short-term measurements (e. g., up to a few hours). The model is trained and tested with data collected from people exposed to two types of cognitive tasks (Stroop Colour Word Test, Mental Computation test) with sessions of different difficulty levels. Using normalised self-report as ground truth, the CNN reaches 84. 59 % accuracy in discriminating between two levels of stress and 56. 52 % in discriminating between three levels. In addition, the CNN outperformed powerful shallow learning methods based on a single layer neural network. Finally, the dataset of labelled thermal images will be open to the community. Comment: Submitted to " 2017 7 th International Conference on Affective Computing and Intelligent Interaction (ACII) " - ACII 201...|$|R
30|$|Despite the {{importance}} of motion information, they are less exploited with current state-of-the-art pedestrian detection methods. According to the benchmark Caltech Pedestrian [8], incorporated motion features still rely only on <b>hand-crafted</b> <b>features</b> such as histograms of oriented gradients of optical flow (HOF [3, 6]), or temporal differences of weakly stabilized frames (SDt [7]), which is relatively more popular as it ignores non-informative motions. The key insights <b>of</b> these <b>hand-crafted</b> <b>features</b> are that (1) contours of moving objects in fine scale is important for detection, and (2) informative motions have to be extracted by factoring out unnecessary motions (such as camera-centric motions).|$|R
40|$|International audienceText zone {{classification}} {{is a vital}} step in the dig-itization process, {{without which}} OCR systems perform poorly. Prior methods to document zone classification have relied on largesets <b>of</b> <b>hand-crafted</b> <b>features</b> for training zone classifiers. Suchfeatures are usually database-dependent, and their computationis time consuming. In this work we propose a novel method fortext zone classification that relies on the approach of unsupervisedfeature learning. Within our method, feature vectors of documentzones are automatically learned by patches extraction, encodingand pooling, where feature encoding {{is based on a}} codebookof visual words. The training phase of the text classifier takesinto consideration the unbalance between text zones and non-text zones of all types. The proposed method has been tested onpublicly available standard databases, and achieved competitiveor better results compared to state-of-the-art methods. Theresults show that our approach matches well the task of textclassification, and is robust to zone shapes, orientations and size...|$|R
40|$|Video reviews are {{the natural}} {{evolution}} of written product reviews. In this paper we target this phenomenon and introduce the first dataset created from closed captions of YouTube product review videos {{as well as}} a new attention-RNN model for aspect extraction and joint aspect extraction and sentiment classification. Our model provides state-of-the-art performance on aspect extraction without requiring the usage <b>of</b> <b>hand-crafted</b> <b>features</b> on the SemEval ABSA corpus, while it outperforms the baseline on the joint task. In our dataset, the attention-RNN model outperforms the baseline for both tasks, but we observe important performance drops for all models in comparison to SemEval. These results, as well as further experiments on domain adaptation for aspect extraction, suggest that differences between speech and written text, which have been discussed extensively in the literature, also extend to the domain of product reviews, where they are relevant for fine-grained opinion mining. Comment: 8 th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA...|$|R
40|$|International audienceSeq 2 seq models {{based on}} Recurrent Neural Networks (RNNs) have {{recently}} {{received a lot}} of attention in the domain of Semantic Parsing. While in principle they can be trained directly on pairs (natural language utterances, logical forms), their performance is limited by the amount of available data. To alleviate this problem, we propose to exploit various sources of prior knowledge: the well-formedness of the logical forms is modeled by a weighted context-free grammar; the likelihood that certain entities present in the input utterance are also present in the logical form is modeled by weighted finite-state automata. The grammar and automata are combined together through an efficient intersection algorithm to form a soft guide (" background ") to the RNN. We test our method on an extension of the Overnight dataset and show that it not only strongly improves over an RNN base-line, but also outperforms non-RNN models based on rich sets <b>of</b> <b>hand-crafted</b> <b>features...</b>|$|R
40|$|Many {{prediction}} {{problems can}} be phrased as inferences over local neighborhoods of graphs. The graph represents the interaction between entities, and the neighborhood of each entity contains information that allows the inferences or predictions. We present an approach for applying machine learning directly to such graph neighborhoods, yielding predicitons for graph nodes {{on the basis of}} the structure of their local neighborhood and the features of the nodes in it. Our approach allows predictions to be learned directly from examples, bypassing the step of creating and tuning an inference model or summarizing the neighborhoods via a fixed set <b>of</b> <b>hand-crafted</b> <b>features.</b> The approach is based on a multi-level architecture built from Long Short-Term Memory neural nets (LSTMs); the LSTMs learn how to summarize the neighborhood from data. We demonstrate the effectiveness of the proposed technique on a synthetic example and on real-world data related to crowdsourced grading, Bitcoin transactions, and Wikipedia edit reversions...|$|R
40|$|The task {{of image}} auto-annotation, namely {{assigning}} {{a set of}} relevant tags to an image, is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and fix an often large number <b>of</b> <b>hand-crafted</b> <b>features</b> to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of standard sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection for annotation. We benchmark our model on the STL- 10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we compete with or outperform existing annotation approaches that use over a dozen distinct handcrafted image descriptors. Furthermore, using 256 -bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efficient storage and fast comparisons. Self-taught learning is used {{in all of our}} experiments and deeper architectures always outperform shallow ones. ...|$|R
40|$|TD-Gammon is a {{neural network}} that {{is able to}} teach itself to play backgammon solely by playing against itself and {{learning}} from the results, based on the TD(X) reinforcement learning algorithm (Sutton 1988). Despite starting from random initial weights (and hence random initial strategy), TD-Gammon achieves a surprisingly strong level of play. With zero knowledge built in {{at the start of}} learning (i. e., given only a ”raw ” description of the board state), the network learns to play at a strong intermediate level. Furthermore, when a set <b>of</b> <b>hand-crafted</b> <b>features</b> is added to the network’s input representation, the result is a truly staggering level of performance: the latest version of TD-Gammon is now estimated to play at a strong master level that is extremely close to the world’s best human players. Reinforcement learning is a fascinating and challenging alternative to the more standard approach to training neural networks by supervised learning. Instead of training on a ”teacher signal ” indicating the correc...|$|R
40|$|The {{classification}} of breast masses from mammograms into benign or malignant has been commonly addressed with machine learning classifiers that use as input a large set <b>of</b> <b>hand-crafted</b> <b>features,</b> usually based on general geometrical and texture information. In this paper, we propose a novel deep learning method that automatically learns features based {{directly on the}} optmisation of breast mass classification from mammograms, where we target an improved classification performance compared to the approach described above. The novelty of our approach lies in the two-step training process that involves a pre-training based on the learning of a regressor that estimates the values of a large set of handcrafted features, followed by a fine-tuning stage that learns the breast mass classifier. Using the publicly available INbreast dataset, we show that the proposed method produces better classification results, compared with the machine learning model using <b>hand-crafted</b> <b>features</b> and with deep learning method trained directly for the classification stage without the pre-training stage. We also show that the proposed method produces the current state-of-the-art breast mass classification results for the INbreast dataset. Finally, we integrate the proposed classifier into a fully automated breast mass detection and segmentation, which shows promising results. Neeraj Dhungel, Gustavo Carneiro, and Andrew P. Bradle...|$|R
40|$|Hashing aims at {{generating}} highly compact similarity preserving {{code words}} which are {{well suited for}} large-scale image retrieval tasks. Most existing hashing methods first encode the images as a vector <b>of</b> <b>hand-crafted</b> <b>features</b> followed by a separate binarization step to generate hash codes. This two-stage process may produce sub-optimal encoding. In this paper, for the first time, we propose a deep architecture for supervised hashing through residual learning, termed Deep Residual Hashing (DRH), for an end-to-end simultaneous representation learning and hash coding. The DRH model constitutes four key elements: (1) a sub-network with multiple stacked residual blocks; (2) hashing layer for binarization; (3) supervised retrieval loss function based on neighbourhood component analysis for similarity preserving embedding; and (4) hashing related losses and regularisation to control the quantization error and {{improve the quality of}} hash coding. We present results of extensive experiments on a large public chest x-ray image database with co-morbidities and discuss the outcome showing substantial improvements over the latest state-of-the art methods. Comment: Submitted to Information Processing in Medical Imaging, 2017 (Under review...|$|R
40|$|With {{the rapid}} growth of web images, hashing has {{received}} increasing interests in large scale image retrieval. Research efforts have been devoted to learning compact binary codes that preserve semantic similarity based on labels. However, most of these hashing methods are designed to handle simple binary similarity. The complex multilevel semantic structure of images associated with multiple labels have not yet been well explored. Here we propose a deep semantic ranking based method for learning hash functions that preserve multilevel semantic similarity between multi-label images. In our approach, deep convolutional neural network is incorporated into hash functions to jointly learn feature representations and mappings from them to hash codes, which avoids the limitation of semantic representation power <b>of</b> <b>hand-crafted</b> <b>features.</b> Meanwhile, a ranking list that encodes the multilevel similarity information is employed to guide the learning of such deep hash functions. An effective scheme based on surrogate loss is used to solve the intractable optimization problem of nonsmooth and multivariate ranking measures involved in the learning procedure. Experimental results show the superiority of our proposed approach over several state-of-the-art hashing methods in term of ranking evaluation metrics when tested on multi-label image datasets. Comment: CVPR 201...|$|R
40|$|International audienceMost of {{the time}} it is nearly {{impossible}} to differentiate between particular type of sound events from a waveform only. Therefore, frequency domain and time-frequency domain representations have been used for years providing representations of the sound signals that are more inline with the human perception. However, these representations are usually too generic and often fail to describe specific content that is present in a sound recording. A lot of work have been devoted to design features that could allow extracting such specific information leading to a wide variety <b>of</b> <b>hand-crafted</b> <b>features.</b> During the past years, owing to the increasing availability of medium scale and large scale sound datasets, an alternative approach to feature extraction has become popular, the so-called feature learning. Finally, processing the amount of data that is at hand nowadays can quickly become overwhelming. It is therefore of paramount importance to be able {{to reduce the size of}} the dataset in the feature space. The general processing chain to convert an sound signal to a feature vector that can be efficiently exploited by a classifier and the relation to features used for speech and music processing are described is this chapter...|$|R
40|$|Comparing {{images to}} {{recommend}} items from an image-inventory {{is a subject}} of continued interest. Added with the scalability of deep-learning architectures the once `manual' job <b>of</b> <b>hand-crafting</b> <b>features</b> have been largely alleviated, and images can be compared according to features generated from a deep convolutional neural network. In this paper, we compare distance metrics (and divergences) to rank features generated from a neural network, for content-based image retrieval. Specifically, after modelling individual images using approximations of mixture models or sparse covariance estimators, we resort to their information-theoretic and Riemann geometric comparisons. We show that using approximations of mixture models enable us to compute a distance measure based on the Wasserstein metric that requires less effort than other computationally intensive optimal transport plans; finally, an affine invariant metric is {{used to compare the}} optimal transport metric to its Riemann geometric counterpart [...] we conclude that although expensive, retrieval metric based on Wasserstein geometry is more suitable than information theoretic comparison of images. In short, we combine GPU scalability in learning deep feature vectors with statistically efficient metrics that we foresee being utilised in a commercial setting. Comment: 5 th ICDM Workshop on High Dimensional Data Mining (HDM 2017...|$|R
40|$|In the Internet, {{ubiquitous}} {{presence of}} redundant, unedited, raw videos has made video summarization an important problem. Traditional methods of video summarization employ a heuristic set <b>of</b> <b>hand-crafted</b> <b>features,</b> which {{in many cases}} fail to capture subtle abstraction of a scene. This paper presents a deep learning method that maps {{the context of a}} video to the importance of a scene similar to that is perceived by humans. In particular, a convolutional neural network (CNN) -based architecture is proposed to mimic the frame-level shot importance for user-oriented video summarization. The weights and biases of the CNN are trained extensively through off-line processing, so that it can provide the importance of a frame of an unseen video almost instantaneously. Experiments on estimating the shot importance is carried out using the publicly available database TVSum 50. It is shown that the performance of the proposed network is substantially better than that of commonly referred feature-based methods for estimating the shot importance in terms of mean absolute error, absolute error variance, and relative F-measure. Comment: Accepted in International Conference on new Trends in Computer Sciences (ICTCS), Amman-Jordan, 201...|$|R
40|$|Information cascades, {{effectively}} {{facilitated by}} most social network platforms, {{are recognized as}} {{a major factor in}} almost every social success and disaster in these networks. Can cascades be predicted? While many believe that they are inherently unpredictable, recent work has shown that some key properties of information cascades, such as size, growth, and shape, can be predicted by a machine learning algorithm that combines many features. These predictors all depend on a bag <b>of</b> <b>hand-crafting</b> <b>features</b> to represent the cascade network and the global network structure. Such features, always carefully and sometimes mysteriously designed, are not easy to extend or to generalize to a different platform or domain. Inspired by the recent successes of deep learning in multiple data mining tasks, we investigate whether an end-to-end deep learning approach could effectively predict the future size of cascades. Such a method automatically learns the representation of individual cascade graphs {{in the context of the}} global network structure, without <b>hand-crafted</b> <b>features</b> and heuristics. We find that node embeddings fall short of predictive power, and it is critical to learn the representation of a cascade graph as a whole. We present algorithms that learn the representation of cascade graphs in an end-to-end manner, which significantly improve the performance of cascade prediction over strong baselines that include feature based methods, node embedding methods, and graph kernel methods. Our results also provide interesting implications for cascade prediction in general...|$|R
40|$|Action {{recognition}} in videos is a challenging task {{due to the}} complexity of the spatio-temporal patterns to model and the difficulty to acquire and learn on large quantities of video data. Deep learning, although a breakthrough for image classification and showing promise for videos, has still not clearly superseded action recognition methods using <b>hand-crafted</b> <b>features,</b> even when training on massive datasets. In this paper, we introduce hybrid video classification architectures based on carefully designed unsupervised representations <b>of</b> <b>hand-crafted</b> spatio-temporal <b>features</b> classified by supervised deep networks. As we show in our experiments on five popular benchmarks for action recognition, our hybrid model combines the best of both worlds: it is data efficient (trained on 150 to 10000 short clips) and yet improves significantly {{on the state of the}} art, including recent deep models trained on millions of manually labelled images and videos. Comment: Accepted for publication in the 14 th European Conference on Computer Vision (ECCV), Amsterdam, 2016, plus supplementary materia...|$|R
40|$|This paper {{addresses}} {{the problem of}} online tracking and classification of multiple objects in an image sequence. Our proposed solution is to first track all objects in the scene without relying on object-specific prior knowledge, which in other systems can take the form <b>of</b> <b>hand-crafted</b> <b>features</b> or user-based track initialization. We then classify the tracked objects with a fast-learning image classifier {{that is based on}} a shallow convolutional neural network architecture and demonstrate that object recognition improves when this is combined with object state information from the tracking algorithm. We argue that by transferring the use of prior knowledge from the detection and tracking stages to the classification stage we can design a robust, general purpose object recognition system with the ability to detect and track a variety of object types. We describe our biologically inspired implementation, which adaptively learns the shape and motion of tracked objects, and apply it to the Neovision 2 Tower benchmark data set, which contains multiple object types. An experimental evaluation demonstrates that our approach is competitive with state-of-the-art video object recognition systems that do make use of object-specific prior knowledge in detection and tracking, while providing additional practical advantages by virtue of its generality. Comment: 15 page...|$|R
40|$|Hierarchies allow feature sharing between objects at {{multiple}} levels of representation, can code exponential variability {{in a very}} compact way and enable fast inference. This makes them potentially suitable for learning and recognizing a higher number of object classes. However, {{the success of the}} hierarchical approaches so far has been hindered by the use <b>of</b> <b>hand-crafted</b> <b>features</b> or predetermined grouping rules. This paper presents a novel framework for learning a hierarchical compositional shape vocabulary for representing multiple object classes. The approach takes simple contour fragments and learns their frequent spatial configurations. These are recursively combined into increasingly more complex and class-specific shape compositions, each exerting a high degree of shape variability. At the top-level of the vocabulary, the compositions are sufficiently large and complex to represent the whole shapes of the objects. We learn the vocabulary layer after layer, by gradually increasing the size of the window of analysis and reducing the spatial resolution at which the shape configurations are learned. The lower layers are learned jointly on images of all classes, whereas the higher layers of the vocabulary are learned incrementally, by presenting the algorithm with one object class after another. The experimental results show that the learned multi-class object representation scales favorably with the number of object classes and achieves a state-of-the-art detection performance at both, faster inference as well as shorter training times...|$|R
40|$|Comunicació presentada al Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE 2017), celebrat el dia 16 de novembre de 2017 a Munic, Alemanya. This work {{describes}} {{our contribution}} to the acoustic scene classifi- cation task of the DCASE 2017 challenge. We propose a system that consists of the ensemble of two methods of different nature: a feature engineering approach, where a collection <b>of</b> <b>hand-crafted</b> <b>features</b> is input to a Gradient Boosting Machine, and another approach based on learning representations from data, where log-scaled melspectrograms are input to a Convolutional Neural Network. This CNN is designed with multiple filter shapes in the first layer. We use a simple late fusion strategy to combine both methods. We report classification accuracy of each method alone and the ensemble system on the provided cross-validation setup of TUT Acoustic Scenes 2017 dataset. The proposed system outperforms each of its component methods and improves the provided baseline system by 8. 2 %. This work is partially supported by the European Union’s Horizon 2020 research and innovation programme under grant agreement No 688382 “AudioCommons”, and the European Research Council under the European Union’s Seventh Framework Program, {{as part of the}} CompMusic project (ERC grant agreement 267583), and the Spanish Ministry of Economy and Competitiveness under the Maria de Maeztu Units of Excellence Programme (MDM- 2015 - 0502) ...|$|R
40|$|Visual {{features}} are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits <b>of</b> both <b>hand-crafted</b> <b>features</b> [31] and deep-learned features [24]. Specifically, we utilize deep architectures to learn discrimi-native convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional fea-tures into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to trans-form convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity {{compared with those}} hand-crafted features; (ii) TDDs {{take account of the}} intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMD-B 51 and UCF 101. Experimental results show that TDDs outperform previous <b>hand-crafted</b> <b>features</b> [31] and deep-learned features [24]. Our method also achieves superior performance to {{the state of the art}} on these datasets 1...|$|R
40|$|Convolutional Networks (ConvNets) have {{recently}} improved image recognition performance thanks to end-to-end learning of deep feed-forward models from raw pixels. Deep {{learning is a}} marked departure from the previous state of the art, the Fisher Vector (FV), which relied on gradient-based encoding <b>of</b> local <b>hand-crafted</b> <b>features.</b> In this paper, we discuss a novel connection between these two approaches. First, we show that one can derive gradient representations from ConvNets {{in a similar fashion}} to the FV. Second, we show that this gradient representation actually corresponds to a structured matrix that allows for efficient similarity computation. We experimentally study the benefits of transferring this representation over the outputs of ConvNet layers, and find consistent improvements on the Pascal VOC 2007 and 2012 datasets. Comment: To appear at BMVC 201...|$|R
40|$|Medical {{images are}} {{valuable}} for clinical diagnosis and decision making. Image modality {{is an important}} primary step, as {{it is capable of}} aiding clinicians to access required medical image in retrieval systems. Traditional methods of modality classification are dependent on the choice <b>of</b> <b>hand-crafted</b> <b>features</b> and demand a clear awareness of prior domain knowledge. The feature learning approach may detect efficiently visual characteristics of different modalities, but it is limited to the number of training datasets. To overcome the absence of labeled data, on the one hand, we take deep convolutional neural networks (VGGNet, ResNet) with different depths pre-trained on ImageNet, fix most of the earlier layers to reserve generic features of natural images, and only train their higher-level portion on ImageCLEF to learn domain-specific features of medical figures. Then, we train from scratch deep CNNs with only six weight layers to capture more domain-specific features. On the other hand, we employ two data augmentation methods to help CNNs to give the full scope to their potential characterizing image modality features. The final prediction is given by our voting system based on the outputs of three CNNs. After evaluating our proposed model on the subfigure classification task in ImageCLEF 2015 and ImageCLEF 2016, we obtain new, state-of-the-art results— 76. 87 % in ImageCLEF 2015 and 87. 37 % in ImageCLEF 2016 —which imply that CNNs, based on our proposed transfer learning methods and data augmentation skills, can identify more efficiently modalities of medical images...|$|R
40|$|As humans {{we possess}} an {{intuitive}} ability for navigation which we master {{through years of}} practice; however existing approaches to model this trait for diverse tasks including monitoring pedestrian flow and detecting abnormal events have been limited by using a variety <b>of</b> <b>hand-crafted</b> <b>features.</b> Recent {{research in the area}} of deep-learning has demonstrated the power of learning features directly from the data; and related research in recurrent neural networks has shown exemplary results in sequence-to-sequence problems such as neural machine translation and neural image caption generation. Motivated by these approaches, we propose a novel method to predict the future motion of a pedestrian given a short history of their, and their neighbours, past behaviour. The novelty of the proposed method is the combined attention model which utilises both "soft attention" as well as "hard-wired" attention in order to map the trajectory information from the local neighbourhood to the future positions of the pedestrian of interest. We illustrate how a simple approximation of attention weights (i. e hard-wired) can be merged together with soft attention weights in order to make our model applicable for challenging real world scenarios with hundreds of neighbours. The navigational capability of the proposed method is tested on two challenging publicly available surveillance databases where our model outperforms the current-state-of-the-art methods. Additionally, we illustrate how the proposed architecture can be directly applied for the task of abnormal event detection without handcrafting the features...|$|R
