473|775|Public
25|$|For {{univariate}} distributions {{that are}} symmetric about one median, the Hodges–Lehmann estimator is a robust and highly <b>efficient</b> <b>estimator</b> {{of the population}} median.|$|E
25|$|However, {{with clean}} data or in {{theoretical}} settings, they can sometimes prove very good estimators, particularly for platykurtic distributions, where for small data sets the mid-range {{is the most}} <b>efficient</b> <b>estimator.</b>|$|E
25|$|For {{estimating}} {{the standard deviation}} of a normal distribution, the scaled interdecile range gives a reasonably <b>efficient</b> <b>estimator,</b> though instead taking the 7% trimmed range (the difference between the 7th and 93rd percentiles) and dividing by 3 (corresponding to 86% of the data of a normal distribution falling within 1.5 standard deviations of the mean) yields an estimate of about 65% efficiency.|$|E
40|$|In {{this paper}} we {{consider}} partially linear varying coefficient models. We provide semiparametric <b>efficient</b> <b>estimators</b> of the parametric part {{as well as}} rate-optimal estimators of the nonparametric part. In our model, different nonparametric coefficients have different smoothing variables. This requires employing a projection technique to get proper estimators of the nonparametric coefficients, and thus conventional kernel smoothing cannot give semiparametric <b>efficient</b> <b>estimators</b> of the parametric components. We take the smooth backfitting approach {{in conjunction with the}} profiling technique to get semiparametric <b>efficient</b> <b>estimators</b> of the parametric part. We also show that our estimators of the nonparametric part achieve the univariate rate of convergence, regardless of the covariate's dimension. We report the finite sample properties of the semiparametric <b>efficient</b> <b>estimators</b> and compare them with those of other estimators. © 2014 Elsevier Inc...|$|R
40|$|Abstract. Suppose {{we observe}} a time series that {{alternates}} between different au-toregressive processes. We give {{conditions under which}} it has a stationary version, derive a characterization of <b>efficient</b> <b>estimators</b> for differentiable functionals of the model, {{and use it to}} construct <b>efficient</b> <b>estimators</b> for the autoregression parameters and the innovation distributions. We also study the cases of equal autoregression parameters and of equal innovation densities...|$|R
40|$|AbstractSuppose {{we observe}} a time series that {{alternates}} between different nonlinear autoregressive processes. We give {{conditions under which}} the model is locally asymptotically normal, derive a characterization of <b>efficient</b> <b>estimators</b> for differentiable functionals of the model, {{and use it to}} construct <b>efficient</b> <b>estimators</b> for the autoregression parameters and the innovation distributions. Surprisingly, the estimators for the autoregression parameters can be improved if we know that the innovation densities are equal...|$|R
25|$|Since each {{observation}} has expectation λ so {{does this}} sample mean. Therefore, the maximum likelihood estimate is an unbiased estimator of λ. It {{is also an}} <b>efficient</b> <b>estimator,</b> i.e. its estimation variance achieves the Cramér–Rao lower bound (CRLB). Hence it is minimum-variance unbiased. Also it can be proved that the sum (and hence the sample mean {{as it is a}} one-to-one function of the sum) is a complete and sufficient statistic for λ.|$|E
25|$|Homoscedasticity: , {{which means}} that the error term has the same {{variance}} σ2 in each observation. When this requirement is violated this is called heteroscedasticity, in such case a more <b>efficient</b> <b>estimator</b> would be weighted least squares. If the errors have infinite variance then the OLS estimates will also have infinite variance (although by the law of large numbers they will nonetheless tend toward the true values so long as the errors have zero mean). In this case, robust estimation techniques are recommended.|$|E
2500|$|The {{first of}} these {{expressions}} shows that the variance of s2 is equal to , which is slightly greater than the σσ-element of the inverse Fisher information matrix [...] Thus, s2 is not an <b>efficient</b> <b>estimator</b> for σ2, and moreover, since s2 is UMVU, we can conclude that the finite-sample <b>efficient</b> <b>estimator</b> for σ2 does not exist.|$|E
30|$|Obtaining more <b>efficient</b> <b>estimators</b> in the {{presence}} of contemporaneous correlated errors across units and heteroskedasticity in panel data models.|$|R
30|$|CRBs {{provide a}} useful and optimistic insight about the {{estimation}} problem {{that may help}} to achieve practical and <b>efficient</b> <b>estimators.</b>|$|R
40|$|Suppose {{we observe}} a {{stationary}} Markov chain with unknown transition distribution. The empirical estimator for {{the expectation of}} a function of two successive observations {{is known to be}} efficient. For reversible Markov chains, an appropriate symmetrization is efficient. For functions of more than two arguments, these estimators cease to be efficient. We determine the influence function of <b>efficient</b> <b>estimators</b> of expectations of functions of several observations, both for completely unknown and for reversible Markov chains. We construct simple <b>efficient</b> <b>estimators</b> in both cases...|$|R
50|$|Thus an <b>efficient</b> <b>estimator</b> {{need not}} exist, {{but if it}} does, it is the MVUE.|$|E
50|$|For {{univariate}} distributions {{that are}} symmetric about one median, the Hodges-Lehmann estimator is a robust and highly <b>efficient</b> <b>estimator</b> {{of the population}} median; for non-symmetric distributions, the Hodges-Lehmann estimator is a robust and highly <b>efficient</b> <b>estimator</b> of the population pseudo-median, which is the median of a symmetrized distribution and which {{is close to the}} population median. The Hodges-Lehmann estimator has been generalized to multivariate distributions.|$|E
5000|$|An <b>efficient</b> <b>estimator</b> is {{also the}} minimum {{variance}} unbiased estimator (MVUE).This is because an <b>efficient</b> <b>estimator</b> maintains equality on the Cramér-Rao inequality for all parameter values, which means it attains the minimum variance for all parameters (the definition of the MVUE). The MVUE estimator, even if it exists, is not necessarily efficient, because [...] "minimum" [...] does not mean equality holds on the Cramér-Rao inequality.|$|E
25|$|Mean squared {{error is}} used for obtaining <b>efficient</b> <b>estimators,</b> a widely used class of estimators. Root mean square error is simply the square root of mean squared error.|$|R
40|$|We {{consider}} semiparametric {{models of}} semi-Markov processes with arbitrary state space. Assuming {{that the process}} is geometrically ergodic, we characterize <b>efficient</b> <b>estimators,</b> {{in the sense of}} Hájek and Le Cam, for arbitrary real-valued smooth functionals of the distribution of the embedded Markov renewal process. We construct <b>efficient</b> <b>estimators</b> of the parameter and of linear functionals of the distribution. In particular we treat the two cases in which we have a paramet-ric model for the transition distribution of the embedded Markov chain and an arbitrary conditional distribution of the inter-jump times, and vice versa. ...|$|R
40|$|Suppose {{we observe}} a time series that {{alternates}} between different nonlinear autoregressive processes. We give {{conditions under which}} the model is locally asymptotically normal, derive a characterization of <b>efficient</b> <b>estimators</b> for differentiable functionals of the model, {{and use it to}} construct <b>efficient</b> <b>estimators</b> for the autoregression parameters and the innovation distributions. Surprisingly, the estimators for the autoregression parameters can be improved if we know that the innovation densities are equal. 62 G 20 62 M 05 Convolution theorem Regular estimator Asymptotically linear estimator Newton-Raphson procedure Weighted least squares estimator Linear autoregression...|$|R
50|$|The spatial median is {{a robust}} and highly <b>efficient</b> <b>estimator</b> {{of a central}} {{tendency}} of a population.|$|E
50|$|Hausman {{also showed}} that the {{covariance}} between an <b>efficient</b> <b>estimator</b> and the difference of an efficient and inefficient estimator is zero.|$|E
50|$|For {{univariate}} distributions {{that are}} symmetric about one median, the Hodges-Lehmann estimator is a robust and highly <b>efficient</b> <b>estimator</b> {{of the population}} median.|$|E
5000|$|Finite-sample <b>efficient</b> <b>estimators</b> are {{extremely}} rare. In fact, it was proved that efficient estimation {{is possible only}} in an exponential family, and only for the natural parameters of that family.|$|R
40|$|An {{invertible}} causal {{linear process}} is a process which has infinite order moving average and autoregressive representations. We assume that the coefficients in these representations depend on a Euclidean parameter, while the corresponding innovations have an unknown centered distribution with some moment restrictions. We discuss efficient estimation of differentiable functionals in such a semiparametric model. For this we first obtain a suitable semiparametric version of local asymptotic normality and then use Hajek's convolution theorem to characterize <b>efficient</b> <b>estimators.</b> Then we apply this result to construct <b>efficient</b> <b>estimators</b> of the Euclidean parameter and of linear functionals of the innovation distribution...|$|R
40|$|The main {{objective}} of the thesis {{is the development of}} efficient multipath mitigation techniques for navigation systems. By efficient mitigation we refer to the use of asympto-tic <b>efficient</b> <b>estimators,</b> and also to the minimisation of their computational burden. In this thesis, the <b>efficient</b> <b>estimators</b> are derived from the Maximum Likelihood Principle in several important scenarios. The computational burden is reduced in two ways. One is through data compression techniques that yield receiver implementations of small complexity and small data sizes. The other consists of the efficient implementation of Newton-type methods for the computation of the Maximum Likelihood estimators. The first part of the thesis is dedicated to present the fundamentals of synchronisation in a navigation receiver, and to {{the state of the art}} in multipath mitigation. Afterwards, several results concerning the interpolation of a band limited signal in a finite interval are introduced, that will later make possible to design <b>efficient</b> <b>estimators.</b> After this introductory part, several data compression methods are presented, well-suited for a DS-CDMA navigation signal that has been corrupted by multipath. Th...|$|R
5000|$|Using the {{commonly}} used result, showed by Hausman, that the covariance of an <b>efficient</b> <b>estimator</b> with its difference from an inefficient estimator is zero yields ...|$|E
5000|$|An <b>efficient</b> <b>{{estimator}}</b> {{need not}} exist, {{but if it}} does and if it is unbiased,it is the MVUE. Since the mean squared error (MSE) of an estimator δ is ...|$|E
50|$|However, {{with clean}} data or in {{theoretical}} settings, they can sometimes prove very good estimators, particularly for platykurtic distributions, where for small data sets the mid-range {{is the most}} <b>efficient</b> <b>estimator.</b>|$|E
50|$|Among {{the models}} {{encountered}} in practice, <b>efficient</b> <b>estimators</b> exist for: the mean μ {{of the normal}} distribution (but not the variance σ2), parameter λ of the Poisson distribution, the probability p in the binomial or multinomial distribution.|$|R
30|$|As {{discussed}} in[13] (see Section 2), <b>efficient</b> <b>estimators</b> {{like the}} spectral reassignment or the derivative method[4] {{are suitable for}} informed spectral analysis. In fact, these estimators almost reach the theoretical bounds and minimize the bitrate required to code the extra information.|$|R
40|$|In this paper, we {{analytically}} investigate three <b>efficient</b> <b>estimators</b> for cointegrating regression models: Phillips and Hansen’s (1990) fully modified OLS estimator, Park’s (1992) canonical cointegrating regression estimator, and Saikkonen’s (1991) dynamic OLS estimator. First, by the Monte Carlo simulations, we {{demonstrate that}} these effi-cient methods {{do not work}} well when the regression errors are strongly serially correlated. In order to explain this result, {{we assume that the}} regression errors are generated from a nearly integrated autoregressive (AR) process with the AR coefficient approaching 1 at a rate of 1 /T, where T is the sample size. We derive the limiting distributions of the three <b>efficient</b> <b>estimators</b> as well as the OLS estimator and show that they have the same lim-iting distribution under this assumption. This implies that the three efficient methods no longer work well when the regression errors are strongly serially correlated. Further, we consider the case where the AR coefficient in the regression errors approaches 1 at a rate slower than 1 /T. In this case, the limiting distributions of the <b>efficient</b> <b>estimators</b> depend on the approaching rate. If the rate is slow enough, the efficiency is establishe...|$|R
50|$|Despite its drawbacks, in {{some cases}} it is useful: the {{midrange}} is a highly <b>efficient</b> <b>estimator</b> of μ, given a small sample of a sufficiently platykurtic distribution, but it is inefficient for mesokurtic distributions, such as the normal.|$|E
50|$|This {{estimator}} has mean θ and {{variance of}} σ2&thinsp;/&thinsp;n, which {{is equal to}} the reciprocal of the Fisher information from the sample. Thus, the sample mean is a finite-sample <b>efficient</b> <b>estimator</b> for the mean of the normal distribution.|$|E
5000|$|For the {{exponential}} family [...] one has [...]Applying [...] on {{both sides}} yields [...]The other potential [...] ( [...] is entropy, [...] and [...] was used). is the covariance of , the Cramér-Rao bound,i.e. an <b>efficient</b> <b>estimator</b> must be exponential.|$|E
40|$|AbstractIn a multiparameter {{estimation}} problem, for first-order <b>efficient</b> <b>estimators,</b> second-order Pitman admissibility, and Pitman closeness {{properties are}} studied. Bearing {{in mind the}} dominant role of Stein-rule estimators in multiparameter estimation theory, such second-order properties are also studied for shrinkage maximum likelihood estimators...|$|R
40|$|This paper {{concerns}} {{linear regression}} with interval censored data. M-estimators for the regression coefficients are derived. Asymptotic consistency and normality of the M-estimators are obtained via an exponential inequality for U-statistics. Asymptotically <b>efficient</b> <b>estimators</b> are provided under mild conditions. 1. Introduction. Conside...|$|R
40|$|This paper investigates how {{bandwidth}} choice {{rules in}} long-run variance estimation affect finite-sample performance of <b>efficient</b> <b>estimators</b> for cointegrating regression models. Monte Carlo {{results indicate that}} Hirukawa's (2010) bandwidth choice rule contributes bias reduction in the estimators. Bandwidth Cointegration Kernel Long-run variance Simulation...|$|R
