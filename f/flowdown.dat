32|1|Public
5000|$|Ensure {{that all}} system {{requirements}} have been validated, allocated, the requirements are complete, and the <b>flowdown</b> is adequate to verify system performance ...|$|E
5000|$|Allocation: {{identifies}} the allowable amount of parameter variation {{that will improve}} response variation to a desired level. (Often referred to as statistical tolerance analysis or requirements <b>flowdown.)</b> ...|$|E
50|$|Requirements {{traceability}} is a sub-discipline of requirements management within {{software development}} and systems engineering. Traceability {{as a general}} term {{is defined by the}} IEEE Systems and Software Engineering Vocabulary as (1) {{the degree to which a}} relationship can be established between two or more products of the development process, especially products having a predecessor-successor or master-subordinate relationship to one another; (2) the identification and documentation of derivation paths (upward) and allocation or <b>flowdown</b> paths (downward) of work products in the work product hierarchy; (3) the degree to which each element in a software development product establishes its reason for existing; and (4) discernable association among two or more logical entities, such as requirements, system elements, verifications, or tasks.|$|E
40|$|Purpose - The {{purpose of}} this paper is to create {{actionable}} knowledge, thereby supporting and stimulating practitioners to improve processes in the financial services sector. Design/methodology/approach - This paper is based on a case base of improvement projects in financial service organizations. The data consist of 181 improvement projects of processes in 14 financial service organizations executed between 2004 and 2010. Following the case-based reasoning approach, based on retrospective analysis of the documentation of these improvement projects, this paper aims to structure this knowledge in a way that supports practitioners in defining improvement projects in their own organizations. Findings - Identification of eight generic project definition templates, along with their critical to quality <b>flowdowns</b> and operational definitions. An overview of the distribution of improvement projects of each generic template over different departments and the average benefit per project for each department. The generic templates give people with knowledge about the process under improvement the ability to use their knowledge effectively in the form of an improvement project. Originality/value - Due to increasing international competition, financial service organizations must continuously improve in order to secure a competitive advantage. This paper turns continuous improvement from an abstract concept into something tangible and achievable, by giving practitioners with local knowledge tried and tested templates to identify promising themes for process improvement, and to make effective project definitions...|$|R
40|$|Abstract. This paper {{describes}} a use case driven approach for functional analysis/allocation and requirements <b>flowdown.</b> The approach utilizes use cases and use case realizations for functional architecture modeling, {{which in turn}} {{form the basis for}} design synthesis and requirements <b>flowdown.</b> We refer to this approach as the FAR (Functional Architecture by use case Realizations) approach. The FAR approach is currently applied in several large-scale defense projects within BAE Systems HÃ¤gglunds AB and the experience so far is quite positive. The approach is illustrated throughout the paper using the well known Automatic Teller Machine (ATM) example...|$|E
40|$|The {{purpose of}} this article is to {{describe}} and clarify a tool that is at the core of the definition phase of most quality improvement projects. This tool is called the critical to quality (CTQ) <b>flowdown.</b> It relates high-level strategic focal points to project objectives. In their turn project objectives are linked to and decomposed into CTQs, which are made operational in the form of measurements. In this article the nature of the connections between strategic focal point, project objectives, CTQs, and measurements is elaborated. The CTQ <b>flowdown</b> serves several purposes. It provides clear project definitions, clarifies the business rationale of an improvement project, makes explicit business assumptions behind project definitions, helps to focus on the vital few real business drivers, and facilitates optimally solving trade-off problems. This article provides a theoretical grounding of the CTQ <b>flowdown,</b> and also provides practitioners with a prescriptive template. Key words: balanced scorecard, key performance indicators, measurement, project definition, Six Sigma, tree diagra...|$|E
40|$|Abstract. The FAR {{approach}} (Functional Architecture by {{use case}} Realizations) is a use case driven methodology for functional analysis/allocation, and requirements <b>flowdown.</b> The FAR approach is tailored towards {{the development of}} long lived software intensive defense systems, for example vehicles. In this paper, we present an empirical study where FAR is applied and evaluated in two large-scale defense projects. Our {{results indicate that the}} FAR approach performs better than the previously used approach in the organization...|$|E
40|$|The WFIRST Project is a {{joint effort}} between GSFC and JPL. The project {{scientists}} and engineers {{are working with the}} community Science Definition Team to define the requirements and initial design of the mission. The objective is to design an observatory that meets the WFIRST science goals of the Astr 02010 Decadal Survey for minimum cost. This talk will be a report of recent project activities including requirements <b>flowdown,</b> detector array development, science simulations, mission costing and science outreach. Details of the interim mission design relevant to scientific capabilities will be presented...|$|E
40|$|We {{describe}} the Optical Pulse Generation (OPG) testbed, {{which is the}} integration of the MOR and Preamplifier Development Laboratories. We use this OPG testbed to develop and demonstrate the overall capabilites of the NIF laser system front end. We will present the measured energy and power output, temporal and spatial pulse shaping capability, FM bandwidth and dispersion for beam smoothing, and measurements of the pulse-to-pulse power variation of the OPG system and compare these results with the required system performance specifications. We will discuss the models that are used to predict the system performance and how the OPG output requirements <b>flowdown</b> to the subordinate subsystems within the OPG system...|$|E
40|$|The {{guide is}} {{intended}} as a resource to aid engineers and systems contracts in the design, implementation, and operation of metrology, calibration, and measurement systems, and to assist NASA personnel in the uniform evaluation of such systems supplied or operated by contractors. Methodologies and techniques acceptable in fulfilling metrology quality requirements for NASA programs are outlined. The measurement process is covered from a high level through more detailed discussions of key elements within the process, Emphasis {{is given to the}} <b>flowdown</b> of project requirements to measurement system requirements, then through the activities that will provide measurements with defined quality. In addition, innovations and techniques for error analysis, development of statistical measurement process control, optimization of calibration recall systems, and evaluation of measurement uncertainty are presented...|$|E
40|$|Increasing {{complexity}} in modern systems {{as well as}} cost and schedule constraints require a new paradigm of system engineering to fulfill stakeholder needs. Challenges facing efficient trade studies include poor tool interoperability, lack of simulation coordination (design parameters) and requirements <b>flowdown.</b> A recent trend toward Model Based System Engineering (MBSE) includes flexible architecture definition, program documentation, requirements traceability and system engineering reuse. As a new domain MBSE still lacks governing standards and commonly accepted frameworks. This paper proposes a framework for efficient architecture definition using MBSE in conjunction with Domain Specific simulation to evaluate trade studies. A general framework is provided followed with a specific example including a method for designing a trade study, defining candidate architectures, planning simulations to fulfill requirements and finally a weighted decision analysis to optimize system objectives...|$|E
40|$|TMT {{has defined}} the {{accuracy}} {{to be achieved}} for both absolute and differential astrometry in its top-level requirements documents. Because of the complexities {{of different types of}} astrometric observations, these requirements cannot be used to specify system design parameters directly. The TMT astrometry working group therefore developed detailed astrometry error budgets for a variety of science cases. These error budgets detail how astrometric errors propagate through the calibration, observing and data reduction processes. The budgets need to be condensed into sets of specific requirements that can be used by each subsystem team for design purposes. We show how this <b>flowdown</b> from error budgets to design requirements is achieved for the case of TMT's first-light Infrared Imaging Spectrometer (IRIS) instrument. Comment: 8 pages, 4 figures. Proceeding of SPIE, Astronomical Telescopes and Instrumentation 201...|$|E
40|$|This paper {{describes}} {{an approach to}} developing an operational concept for the US National Airspace System. This approach carefully ties the system mission, the functions that the system must perform and the available resources, and presents a logical functional structure for the overall system that ties together functions, resources and subsystems. A key aspect {{of this approach is}} a careful <b>flowdown</b> of performance requirements or allocations from toplevel system performance goals through the concept layers down to the technology level to ensure that design decisions will lead to a system that delivers the desired performance. The papers illustrates this approach by defining an operational concept that is driven by system capacity as the primary performance goal, including a logical system transition path from the current system to a higher throughput system for 2015...|$|E
40|$|Life cycle {{approaches}} for software devel-opment {{have become more}} and more flexible. For general system development, the situation is less clear. Most of the related methods involve strict requirements <b>flowdown.</b> This can be problematic like the waterfall model especially for software-intensive systems. The recently established CMMI proposes concur-rent development in systems engineering but provides little guidance on how to define and execute specific processes for a specific pro-ject or organization. Anchor point milestones may serve for synchronization of system and software development efforts. Apart from this kind of synchronization, however, a key re-maining question is, how exactly the life cycle models of software development and of more general system development depend on each other. Especially for developing highly inno-vative software-intensive systems, we found that architects of software and other sub-systems should communicate and interact during their concurrent work. Therefore, we propose system and software co-architecting...|$|E
40|$|The Thirty Meter Telescope {{primary mirror}} is {{composed}} of 492 segments that are controlled to high precision {{in the presence of}} wind and vibration disturbances, despite the interaction with structural dynamics. The higher bandwidth and larger number of segments compared with the Keck telescopes requires greater attention to modeling to ensure success. We focus here on the development and validation of a suite of quasi-static and dynamic modeling tools required to support the design process, including robustness verification, performance estimation, and requirements <b>flowdown.</b> Models are used to predict the dynamic response due to wind and vibration disturbances, estimate achievable bandwidth in the presence of control-structure-interaction (CSI) and uncertainty in the interaction matrix, and simulate and analyze control algorithms and strategies, e. g. for control of focus-mode, and sensor calibration. Representative results illustrate TMT performance scaling with parameters, but the emphasis is on the modeling framework itself...|$|E
40|$|To design todayâs complex, multi-disciplinary systems, {{designers}} need {{a design}} method {{that allows them}} to systematically decompose a complex design problem into simpler sub-problems. Systems engineering provides such a framework. In an iterative, hierarchical fashion systems are decomposed into subsystems and requirements are allocated to these subsystems based on estimates of their attributes. In this paper, we investigate the role and limitations of modeling and simulation in this process of system decomposition and requirements <b>flowdown.</b> We first identify different levels of complexity in the estimation of system attributes, ranging from simple aggregation to complex emergent behavior. We also identify the main obstacles to the systems engineering decomposition approach: identifying coupling at the appropriate level of abstraction and characterizing and processing uncertainty. The main contributions of this paper are to identify these short-comings, present the role of modeling and simulation in overcoming these shortcomings, and discuss research directions for addressing these issues and expanding the role of modeling and simulation in the future...|$|E
40|$|The {{purpose of}} this system {{description}} document (SDD) is to establish requirements that drive {{the design of the}} surface industrial heating, ventilation, and air-conditioning (HVAC) system and its bases to allow the design effort to proceed to license application. This SDD will be revised at strategic points as the design matures. This SDD identifies the requirements and describes the system design, as it currently exists, with emphasis on attributes of the design provided to meet the requirements. This SDD is an engineering tool for design control; accordingly, the primary audience and users are design engineers. This SDD is part of an iterative design process. It leads the design process with regard to the <b>flowdown</b> of upper tier requirements onto the system. Knowledge of these requirements is essential to performing the design process. The SDD follows the design with regard to the description of the system. The description that provided in this SDD reflects the current results of the design process...|$|E
40|$|The Energetic Transient Array (ETA) is a mission, {{proposed}} by the Center for Space Research (CSR) at MIT, {{to set up an}} interplanetary constellation of six microsatellites for Gamma Ray Burst (GRB) astrometry. The microsatellites will be deployed into distinct heliocentric orbits by a carrier spacecraft which will be propelled by a stationary plasma (SPT- 70) electric thruster. This thesis documents the analysis carried out for the ETA constellation design. It addresses ETA system design from the systems engineering standpoint, with emphasis on constellation and propulsion system issues. Beginning with scientific requirements, the most important of which is GRB localization accuracy, systems engineering tools are applied to define trade options, system requirements, architecture and interfaces. Require-ments <b>flowdown</b> to the constellation level enables an assessment of propulsion options through a top level trade study. This has resulted in the selection of the SPT- 70 propulsion system for the carrier spacecraft. Constellation analysis, consisting of trade studies, includes consideration of design aspects such as number of microsatellites, SPT- 70 thrust...|$|E
40|$|Key {{characteristics}} (KCs) play {{a significant}} role in product lifecycle management (PLM) and in collaborative and global product development. Over the last decade, KCs methodologies and tools have been studied and practiced in several domains of the product lifecycle, and many world-class companies have introduced KCs considerations into their product development practices. However, there has been no systematic survey of KCs techniques, methodologies, and practices in this respect. This paper aims to give a comprehensive survey of KCs methodologies, and practices from the perspective of enterprise integration and PLM. The paper firstly presents a holistic framework of KCs methodologies and practices through the product lifecycle, and summarizes the fundamentals of KCs including their definition and classification, KC <b>flowdown,</b> and the identification and selection of KCs. A review of the KCs methods and practices in the product lifecycle is then presented, particularly in engineering design, manufacturing planning, production and testing as well as information and knowledge management respectively. Finally, the problems and challenges for future research on KCs techniques are discussed...|$|E
40|$|Review Item Discrepancy (RID) number REQ- 086, {{filed against}} the S&OC Requirements Document (JWST-RQMT- 002032) at the S&OC System Requirements Review {{requires}} the provision of supplemental information to effect its closure. The RID states: REQ- 086 : Traceability to GSRD & MRD, Document {{does not have a}} table showing all [Ground Segment requirements] have been traced. Also several times there is a liberal interpretation of GS (example SOC 1569 with GS- 145, soc- 1623 & gs 181). The RID closure action includes the provision of a table containing all current Ground Segment Requirements Document requirements, showing how the S&OC requirements trace to them. This report provides that information. The appendix to this report provides a DOORS report that shows the <b>flowdown</b> from each requirement in section 3. 7 of the GSRD to the S&OC requirements that have been linked to it. The report demonstrates that every requirement in 3. 7. 1 has been flowed to at least one S&OC requirement. Requirements currently traced to Institutional Systems or Common Systems requirements are also shown. 4 JWST-STScI-CI- 0099 SM- 1...|$|E
40|$|One of {{the most}} {{challenging}} yet poorly defined aspects of engineering a complex aerospace system is behavior engineering, including definition, specification, design, implementation, and verification and validation of the system's behaviors. This is especially true for behaviors of highly autonomous and intelligent systems. Behavior engineering is more of an art than a science. As a process it is generally ad-hoc, poorly specified, and inconsistently applied from one project to the next. It uses largely informal representations, and results in system behavior being documented {{in a wide variety of}} disparate documents. To address this problem, JPL has undertaken a pilot project to apply its institutional capabilities in Model-Based Systems Engineering to the challenge of specifying complex spacecraft system behavior. This paper describes the results of the work in progress on this project. In particular, we discuss our approach to modeling spacecraft behavior including 1) requirements and design <b>flowdown</b> from system-level to subsystem-level, 2) patterns for behavior decomposition, 3) allocation of behaviors to physical elements in the system, and 4) patterns for capturing V&V activities associated with behavioral requirements. We provide examples of interesting behavior specification patterns, and discuss findings from the pilot project...|$|E
40|$|Purpose - Many {{companies}} in the publishing industry are facing the task of developing new business models and becoming more efficient and effective in execution. Lean Six Sigma (LSS) is a unified framework for systematically developing efficiency and quality improvements; it can help realize significant results and breakthrough improvements in the publishing industry, as demonstrated with many projects from a Dutch multinational publishing company. The {{purpose of this paper}} is to facilitate the process of defining LSS projects in publishing, because lack of a clear definition is an important cause for project failure. Design/methodology/approach - The paper discusses and categorizes 49 project definitions based on two elements: the critical to quality <b>flowdown</b> and the corresponding set of operational definitions and shows how this simple categorization and subsequent standardization of approaches can help LSS teams simplify the definition phase. Findings - The strategy presented in this paper provides seven standard LSS project definitions ("generic templates"). Originality/value - Project leaders can use the templates presented in this paper as an example and as a guide in the project definition phase. This helps them to formulate crystal-clear project definitions, which have explicitly stated goals and a solid business rationale...|$|E
40|$|Third in {{the series}} of NASA great observatories, the Advanced X-Ray Astrophysics Facility (AXAF) is {{scheduled}} for launch from the Space Shuttle in November of 1998. Following {{in the path of}} the Hubble Space Telescope and the Compton Gamma Ray Observatory, this observatory will image light at X-ray wavelengths, facilitating the detailed study of such phenomena as supernovae and quasars. The AXAF project is sponsored by the Marshall Space Flight Center in Huntsville, Alabama. Because of exacting requirements on the performance of the AXAF optical system, it was necessary to reduce the transmission of reaction wheel jitter disturbances to the observatory. This reduction was accomplished via use of a passive mechanical isolation system to interface the reaction wheels with the spacecraft central structure. In addition to presenting a description of the spacecraft, the isolation system, and the key image quality requirement <b>flowdown,</b> this paper details the analyses performed in support of system-level imaging performance requirement verification. These analyses include the identification of system-level requirement suballocations, quantification of imaging and pointing performance, and formulation of unit-level isolation system transmissibility requirements. Given in comparison to the non-isolated system imaging performance, the results of these analyses clearly illustrate the effectiveness of an innovative reaction wheel passive isolation system...|$|E
40|$|National Security Technologies, LLC (NSTec), has {{established}} a work management program and corresponding electronic Facilities and Operations Management Information System (e-FOM) to implement Integrated Safety Management (ISM). The management of work scopes, the identification of hazards, {{and the establishment of}} implementing controls are reviewed and approved through electronic signatures. Through the execution of the program and the implementation of the electronic system, NSTec staff work within controls and utilize feedback and improvement process. The Integrated Work Control Manual further implements the five functions of ISM at the Activity level. By adding the Risk and Work Configuration Management program, NSTec establishes risk acceptance (business and physical) for liabilities within the performance direction and work management processes. Requirements, roles, and responsibilities are specifically identified in the program while e-FOM provides the interface and establishes the <b>flowdown</b> from the Safety Chain to work and facilities management processes to company work-related directives, and finally to Subject Matter Expert concurrence. The Program establishes, within the defined management structure, management levels for risk identification, risk mitigation (controls), and risk acceptance (business and physical) within the Safety Chain of Responsibility. The Program also implements Integrated Safeguards and Security Management within the NSTec Safety Chain of Responsibility. Once all information has been entered into e-FOM, approved, and captured as data, the information becomes searchable and sortable by hazard, location, organization, mitigating controls, etc...|$|E
40|$|The Department of Defense {{continues}} to work to improve the acquisition of military equipment and capability to assist the warfighter in protecting the U. S. and its allies, and help oppressed nations around the world, amidst continuously changing conditions and threats. The DoD seeks to improve the acquisition process and overall program execution of military systems, to provide greater, more effective and reliable warfighting capability, at affordable cost and within reasonable schedules. One of the primary and critically important areas of program acquisition and execution lies in the umbrella discipline of Systems Engineering, which is the overall integrating function in defense programs, from proper requirements definition & <b>flowdown,</b> effective and affordable design that integrates reliability, availability and maintainability considerations into the overall balance of design that emphasizes supportability and usage aspects along with overall performance, cost and schedule. Systems Engineering principles embody strong technical and risk management aspects, for both the acquiring program office {{as well as the}} executing defense prime and subcontractors. Strong emphasis on Systems Engineering throughout the life cycle of the program, from concept development through sustainment, is a key enabler of successful programs. The annual Systems Engineering Conference explores the role of Systems Engineering in defense programs from all aspects and perspectives, including the pragmatic, practical and academic viewpoints, and brings key practitioners together to work on effective solutions to achieving a successful warfighting force...|$|E
40|$|Prognostics and Health Management (PHM) {{principles}} {{have considerable}} promise {{to change the}} game of lifecycle cost of engineering systems at high safety levels by providing a reliable estimate of future system states. This estimate is a key for planning and decision making in an operational setting. While technology solutions have made considerable advances, the tie-in into the systems engineering process is lagging behind, which delays fielding of PHM-enabled systems. The derivation of specifications from high level requirements for algorithm performance to ensure quality predictions is not well developed. From an engineering perspective some key parameters driving the requirements for prognostics performance include: (1) maximum allowable Probability of Failure (PoF) of the prognostic system to bound {{the risk of losing}} an asset, (2) tolerable limits on proactive maintenance to minimize missed opportunity of asset usage, (3) lead time to specify the amount of advanced warning needed for actionable decisions, and (4) required confidence to specify when prognosis is sufficiently good to be used. This paper takes a systems engineering view towards the requirements specification process and presents a method for the <b>flowdown</b> process. A case study based on an electric Unmanned Aerial Vehicle (e-UAV) scenario demonstrates how top level requirements for performance, cost, and safety flow down to the health management level and specify quantitative requirements for prognostic algorithm performance...|$|E
40|$|Projected {{for launch}} {{in the latter}} part of 1998, the Advanced X-ray Astrophysics Facility (AXAF), the third {{satellite}} in the Great Observatory series, promises to dramatically open the x-ray sky as the Hubble and Compton observatories have done in their respective realms. Unlike its companions, however, AXAF will be placed in a high altitude, highly elliptical orbit (10, 000 x 100, 000 km), and will therefore be subject to its own unique environment, spacecraft and science instrument constraints and communication network interactions. In support of this mission, ground operations personnel have embarked on the development of the AXAF Offline System (OFLS), a body of software divided into four basic functional elements: (1) Mission Planning and Scheduling, (2) Command Management, (3) Altitude Determination and Sensor Calibration and (4) Spacecraft Support and Engineering Analysis. This paper presents an overview concept for one of these major elements, the Mission Planning and Scheduling subsystem (MPS). The derivation of this concept is described in terms of requirements driven by spacecraft and science instrument characteristics, orbital environment and ground system capabilities. The <b>flowdown</b> of these requirements through the systems analysis process and the definition of MPS interfaces has resulted in the modular grouping of functional subelements depicted in the design implementation approach. The rationale for this design solution is explained and capabilities for the initial prototype system are proposed from the user perspective...|$|E
40|$|This report {{contains}} a specification of requirements for a video-on-demand (VoD) application developed at Belgacom, {{used as a}} trial application in the 2 RARE project. The specification contains three parts: an informal specification in natural language; a semiformal specification consisting {{of a number of}} diagrams intended to illustrate the informal specification; and a formal specification that makes the requiremants on the desired software system precise. The informal specification is structured {{in such a way that}} it resembles official specification documents conforming to standards such as that of IEEE or ESA. The semiformal specification uses some of the tools in from a requirements engineering toolkit called TRADE (Toolkit for Requirements And Design Engineering). The purpose of TRADE is to combine the best ideas in current structured and object-oriented analysis and design methods within a traditional systems engineering framework. In the case of the VoD system, the systems engineering framework is useful because it provides techniques for allocation and <b>flowdown</b> of system functions to components. TRADE consists of semiformal techniques taken from structured and object-oriented analysis as well as a formal specification langyage, which provides constructs that correspond to the semiformal constructs. The formal specification used in TRADE is LCM (Language for Conceptual Modeling), which is a syntactically sugared version of order-sorted dynamic logic with equality. The purpose of this report is to illustrate and validate the TRADE/LCM approach in the specification of distributed, communication-intensive systems...|$|E
40|$|The LISA mission observes {{gravitational}} waves {{by measuring the}} separations between freely floating proof masses located 5 million kilometers apart with an accuracy of - 10 picometers. The separations are measured interferometrically. The telescope is an afocal Cassegrain style design with a magnification of 80 x. The entrance pupil has a 40 cm diameter and will either be centered on-axis or de-centered off-axis to avoid obscurations. Its two main purposes are to transform the small diameter beam used on the optical bench to a diffraction limited collimated beam to efficiently transfer the metrology laser between spacecraft, and to receive the incoming light from the far spacecraft. It transmits and receives simultaneously. The basic optical design and requirements are well understood for a conventional telescope design for imaging applications, but the LISA design {{is complicated by the}} additional requirement that the total optical path through the telescope must remain stable at the picometer level over the measurement band during the mission to meet the measurement accuracy. We describe the mechanical requirements for the telescope and the preliminary work that has been done to understand the materials and mechanical issues associated with the design of a passive metering structure to support the telescope and to maintain the spacing between the primary and secondary mirrors in the LISA on-orbit environment. This includes the requirements <b>flowdown</b> from the science goals, thermal modeling of the spacecraft and telescope to determine the expected temperature distribution, layout options for the telescope including an on- and off-axis design. Plans for fabrication and testing will be outlined...|$|E
40|$|Air {{traffic demand}} is growing. New methods of {{airspace}} design are required that can enable new designs, do {{not depend on}} current operations, and can also support quantifiable performance goals. The main goal of this thesis is to develop methods to model inherent safety and control cost so that these can be included as principal objectives of airspace design, in support of prior work which examines capacity. The first contribution of the thesis is to demonstrate two applications of airspace analysis and design: assessing the inherent safety and control cost of the airspace. Two results are shown, a model which estimates control cost depending on autonomy allocation and traffic volume, and the characterization of inherent safety conditions which prevent unsafe trajectories. The effects of autonomy ratio and traffic volume on control cost emerge from a Monte Carlo simulation of air traffic in an airspace sector. A maximum likelihood estimation identifies the Poisson process {{to be the best}} stochastic model for control cost. Recommendations are made to support control-cost-centered airspace design. A novel method to reliably generate collision avoidance advisories, in piloted simulations, by the widely-used Traffic Alert and Collision Avoidance System (TCAS) is used to construct unsafe trajectory clusters. Results show that the inherent safety of routes can be characterized, determined, and predicted by relatively simple convex polyhedra (albeit multi-dimensional and involving spatial and kinematic information). Results also provide direct trade-off relations between spatial and kinematic constraints on route geometries that preserve safety. Accounting for these clusters thus supports safety-centered airspace design. The second contribution of the thesis is a general methodology that generalizes unifying principles from these two demonstrations. The proposed methodology has three steps: aggregate data, synthesize lean model, and guide design. The use of lean models is a result of a natural <b>flowdown</b> from the airspace view to the requirements. The scope of the lean model is situated at a level of granularity that identifies the macroscopic effects of operational changes on the strategic level. The lean model technique maps low-level changes to high-level properties and provides predictive results. The use of lean models allows the mapping of design variables (route geometry, autonomy allocation) to design evaluation metrics (inherent safety, control cost). Ph. D...|$|E
40|$|The New Worlds, New Horizons (NWNH) in Astronomy and Astrophysics 2010 Decadal Survey {{prioritized}} {{the community}} consensus for ground-based and space-based observatories. Recognizing {{that many of}} the community s key questions could be answered with a wide-field infrared survey telescope in space, and that the decade would be one of budget austerity, WFIRST was top ranked in the large space mission category. In addition to the powerful new science that could be accomplished with a wide-field infrared telescope, the WFIRST mission was determined to be both technologically ready and {{only a small fraction of}} the cost of previous flagship missions, such as HST or JWST. In response to the top ranking by the community, NASA formed the WFIRST Science Definition Team (SDT) and Project Office. The SDT was charged with fleshing out the NWNH scientific requirements to a greater level of detail. NWNH evaluated the risk and cost of the JDEM-Omega mission design, as submitted by NASA, and stated that it should serve as the basis for the WFIRST mission. The SDT and Project Office were charged with developing a mission optimized for achieving the science goals laid out by the NWNH re-port. The SDT and Project Office opted to use the JDEM-Omega hardware configuration as an initial start-ing point for the hardware implementation. JDEM-Omega and WFIRST both have an infrared imager with a filter wheel, as well as counter-dispersed moderate resolution spectrometers. The primary advantage of space observations is being above the Earth's atmosphere, which absorbs, scatters, warps and emits light. Observing from above the atmosphere enables WFIRST to obtain precision infrared measurements of the shapes of galaxies for weak lensing, infrared light-curves of supernovae and exoplanet microlensing events with low systematic errors, and infrared measurements of the H hydrogen line to be cleanly detected in the 1 <z< 2 redshift range important for baryon acoustic oscillation (BAO) dark energy measurements. The Infrared Astronomical Satellite (IRAS), the Cosmic Background Explorer (COBE), Herschel, Spitzer, and Wide-field Infrared Sur-vey Explorer (WISE) are all space missions that have produced stunning new scientific advances by going to space to observe in the infrared. This interim report describes progress as of June 2011 on developing a requirements <b>flowdown</b> and an evaluation of scientific performance. An Interim Design Reference Mission (IDRM) configuration is presented that is based on the specifications of NWNH with some refinements to optimize the design in accordance with the new scientific requirements. Analysis of this WFIRST IDRM concept is in progress to ensure the capability of the observatory is compatible with the science requirements. The SDT and Project will continue to refine the mission concept over the coming year as design, analysis and simulation work are completed, resulting in the SDT s WFIRST Design Reference Mission (DRM) by the end of 2012...|$|E

