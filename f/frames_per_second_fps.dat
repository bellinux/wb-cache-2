261|10000|Public
5|$|Two {{weeks after}} the expansion's {{official}} release, the PlayStation 4 version was re-released to fix performance issues. In a performance analysis by Eurogamers Digital Foundry, {{it was discovered that}} although Fallout 4 typically ran at 30 <b>frames</b> <b>per</b> <b>second</b> (<b>fps),</b> when the player was outside and, in Far Harbors foggy biomes, the frame rate could drop to 15 fps, and could drop even lower during action-oriented events such as firefights. In the same analysis, the Xbox One version was found to run at 20–30 fps but experienced various issues, such as stuttering and software lock-ups. The update toned down the level of fog, and was successful in making the game more stable.|$|E
5|$|There were consequences, as well, {{for other}} {{technological}} {{aspects of the}} cinema. Proper recording and playback of sound required exact standardization of camera and projector speed. Before sound, 16 <b>frames</b> <b>per</b> <b>second</b> (<b>fps)</b> was the supposed norm, but practice varied widely. Cameras were often undercranked or overcranked to improve exposures or for dramatic effect. Projectors were commonly run too fast to shorten running time and squeeze in extra shows. Variable frame rate, however, made sound unlistenable, and a new, strict standard of 24 fps was soon established. Sound also forced the abandonment of the noisy arc lights used for filming in studio interiors. The switch to quiet incandescent illumination in turn required a switch to more expensive film stock. The sensitivity of the new panchromatic film delivered superior image tonal quality and gave directors the freedom to shoot scenes at lower light levels than was previously practical.|$|E
25|$|Dolphin {{was first}} {{released}} in September 2003 by programmers Henrik Rydgård (ector) and F|RES as an experimental Nintendo GameCube emulator that could boot {{up and run}} commercial games. Audio was not yet emulated, and there were performance issues. Many games crashed on start up or barely ran at all; average speed was from 2 to 20 <b>frames</b> <b>per</b> <b>second</b> (<b>FPS).</b> Its name refers to the development code name for the GameCube.|$|E
40|$|The paper proposes the {{hardware}} {{implementation of the}} Gaussian Mixture Model (GMM) algorithm included in the OpenCV library. The OpenCV GMM algorithm is adapted to allow the FPGA implementation while providing a minimal impact {{on the quality of}} the processed videos. The circuit performs 30 <b>frame</b> <b>per</b> <b>second</b> (<b>fps)</b> background (Bg) identification on High Definition (HD) video sequences when implemented on commercial FPGA and outperforms previously proposed implementations. When implemented on Virtex 5 lx 50 FPGA using one level of pipeline, runs at 95. 3 MHz, uses 5. 3 % of FPGA resources with a power dissipation of 1. 47 mW/MHz...|$|R
50|$|The total <b>frames</b> <b>per</b> <b>second</b> is 57.4 <b>fps</b> whilst {{enhanced}} with Butter gets 58.5 fps.|$|R
40|$|The paper proposes an {{improved}} hardware {{implementation of the}} OpenCV version of the Gaussian Mixture Model (GMM) algorithm. Truncated binary multipliers and a ROM compression technique are employed to reduce hardware complexity while increasing circuit processing capability. The OpenCV GMM algorithm is adapted to allow the FPGA implementation while providing a minimal impact {{on the quality of}} the processed videos. When implemented on Virtex 5 FPGA the proposed circuit is able to process High Deﬁnition (HD) video sequences at 30 <b>frame</b> <b>per</b> <b>second</b> (<b>fps)</b> improving the performances with respect to previously proposed implementations (- 7. 6 % in area and + 12. 6 % in speed) ...|$|R
25|$|The {{basic problem}} facing {{designers}} of color televisions was this: Sending each {{frame of the}} moving image meant sending three signals, red, green and blue. The sequential systems, like Baird's earlier efforts, sent the three images one after another. In order for motion to appear smooth, images must change at least 16 times a second. To reduce flicker, over 40 <b>frames</b> <b>per</b> <b>second</b> (<b>fps)</b> is mandatory. In sequential systems, each color requires a separate field. For this reason, very high refresh (field) rates were necessary. CBS' system refreshed at 144fps. (Peter Goldmark's CBS team tried several field rates. Within the 6 MHz allowable channel bandwidth, the most acceptable rate was 144 fps. This rate made pictures incompatible with existing systems working at 50 or 60Hz.|$|E
500|$|One {{of the new}} {{firms to}} enter the field was the Kinetoscope Exhibition Company; the firm's partners, {{brothers}} Otway and Grey Latham, Otway's friend Enoch Rector, and their employer, Samuel J. Tilden Jr., sought to combine {{the popularity of the}} Kinetoscope with that of prizefighting. This led to a series of significant developments in the motion picture field: The Kinetograph was then capable of shooting only a 50-foot-long negative (evidence suggests [...] feet was the longest length actually used). At 16 frames per foot, this meant a maximum running time of 20 seconds at 40 <b>frames</b> <b>per</b> <b>second</b> (<b>fps),</b> the speed most frequently employed with the camera. At the rate of 30 fps that had been used as far back as 1891, a film could run for almost 27 seconds. Hendricks identifies Sandow as having been shot at 16 fps, as does the Library of Congress in its online catalog, where its duration is listed as 40 seconds.|$|E
500|$|The episode, {{most notably}} the scenes {{featuring}} the teenagers moving at high speed, relied heavily on special effects. However, instead of creating them on a computer, director Robert Lieberman was able to create many of the necessary scenes using different camera speeds. In {{order to get the}} proper feel for the rush effects, test footage of assistant director Xochi Blymyer was filmed at 24 <b>frames</b> <b>per</b> <b>second</b> (<b>FPS),</b> 12 FPS, 6 FPS and 3 FPS. After filming, a digital [...] "blur effect" [...] was added to make the shot look unfocused. During the scene where the teenagers stumble into the light and receive the speed power, the special effects crew shot two separate shots: one of the teens' bodies and one of the teens' heads rapidly flailing. The special effects crew then [...] "pull the head off" [...] of the shots featuring the teens moving rapidly, and used them to replace the heads on the shots of the teens' still bodies. This method kept their bodies in focus but allowed their heads to rapidly move.|$|E
40|$|This work {{presents}} a development system, based on Field Programmable Gate Array (FPGA), that was {{specifically designed for}} testing the entire electronics to be integrated in an endoscopic capsule, such as a camera, an image compression engine, a high-speed telemetric system, illumination and inertial sensors. Thanks to its high flexibility, several features were tested and evaluated, thus allowing to find the optimal configuration, in terms of power consumption, performances and size, to be fit in a capsule. As final result, an average frame rate of 19 <b>frame</b> <b>per</b> <b>second</b> (<b>fps)</b> over a transmission channel of 1. 5 Mbit/s {{was chosen as the}} best choice {{for the development of a}} miniaturized endoscopic capsule prototype...|$|R
50|$|High-speed dual {{multi-core}} image-processing engines {{with world}} record (Nikon claim) 600 megapixels <b>per</b> <b>second</b> speed, enhanced H.264 HD video engine {{and controlled by}} a dual-core ARM microcontroller are the main improvements. Its high speed allows the world's fastest speed (Nikon claim) of 60 <b>frames</b> <b>per</b> <b>second</b> (10 <b>fps</b> with full autofocus).|$|R
40|$|We {{describe}} a complete lab-on-a-chip type ion-sensitive chemicalsensorarraysystem for imagingproton concentration and flow. The {{core of the}} imagingsystem relies on a 715. 8 μm × 715. 8 μm large 64 × 64 -pixel ISFET sensorarray that is designed and fabricated in a conventional 4 -metal 0. 35 μm CMOS process along with readout circuitry and additional electronics. The high-speed and real-time data acquisition of ∼ 100 <b>frame</b> <b>per</b> <b>second</b> (<b>fps),</b> 40 μs <b>per</b> pixel, results in {{a large amount of}} data that is streamed to the hard disk. Image reconstruction methods allow visualization of the dynamic behavior of chemical substances and ionic spreads over the sensorarray at aresolution of the size of a pixel. The functionalities of the on-chip ion-camera device are demonstrated using appropriate acid and alkaline solutions. A pH sensitivity of 20 mV/pH was achieved without modification to the foundry processed chip...|$|R
500|$|Scale explosions {{were created}} using {{substances}} such as fuller's earth, petrol gel, magnesium strips and Cordtex explosive. Originally filmed {{at up to}} 120 <b>frames</b> <b>per</b> <b>second</b> (<b>f.p.s.),</b> they were slowed down to 24 f.p.s. during post-production to increase their apparent magnitude and length. Gunpowder canisters were ignited to create rocket jets. The wires that electronically fired the rockets also allowed {{a member of the}} crew, holding a cruciform and positioned on an overhead gantry, to [...] "fly" [...] the model over the set. By far the most unwieldy model was Thunderbird 2, which Meddings remembered as being [...] "awful" [...] to fly. A combination of unreliable rockets and weak wiring frequently caused problems: should the former be slow to ignite, the current quickly caused the latter to overheat and snap, potentially damaging the model and even setting fire to the set. Conditions above the studio floor were often dangerous due to the heat and smoke. Although many of the exhaust sound effects used in the series were drawn from an audio library, some were specially recorded during a Red Arrows display at RAF Little Rissington in Gloucestershire.|$|E
500|$|The CD-I {{version was}} coded by Ashley Hogg. He stated {{that there were}} {{problems}} that only occurred on this console, and CD-Rs were expensive. The Game Gear team were originally against the [...] "on-rails" [...] handling of the Mega Drive version, and wanted to revert to the NES drifting. The Game Gear version was created by Ashley Routledge and Dave Saunders, who adjusted the Mega Drive graphics to the Game Gear's smaller screen. It was produced alongside the Mega Drive version, and programmed from scratch to run at 60 <b>frames</b> <b>per</b> <b>second</b> (<b>FPS).</b> The main problem was keeping as much detail as possible on the Game Gear's inferior storage and memory capabilities. Much of the art had to be redrawn with fewer colours at lower resolutions. The biggest problem was the screen size, which {{made it difficult to}} take corners at high speeds. The team tweaked the camera movement so the vehicle was positioned further back, so players were able to see the track ahead according to how fast the vehicle is travelling. Another thing considered important was the lack of options screens, as it was felt players should be able to [...] simply play the game.|$|E
500|$|Supersonic's {{focus was}} on the {{graphics}} and game modes for smaller vehicles. As the Mega Drive was considered the leading platform, an 8-bit look was impossible. David Darling was not happy with early efforts, and had Supersonic redraw the graphics. He said the perspective was wrong and there was slowdown. He also said that getting the game running at 50 <b>frames</b> <b>per</b> <b>second</b> (<b>FPS)</b> in eight-player mode was difficult. Supersonic asked Big Red Software {{to assist with the}} graphics. Most of the background graphics were produced by Mark Neesam, using an Amiga 500. He stated that some graphics were hard, despite having access to the originals, and believed that additional colours enabled him to [...] "muddy up" [...] graphics, but also used the increased colour palette to clean some up. Richard Darling encouraged tweaking, giving the game a personality. Journalists frequently visited to check on progress. Violet Berlin, co-presenter of the television programme Bad Influence!, made a cameo appearance as a character: she made one such visit and Richard Eddy of Codemasters asked to take her picture to put in the game. She agreed on the condition that she was made the fastest character, although it was already decided that another character would be. She was instead made the second fastest character.|$|E
40|$|International audienceA new contour-tracking {{algorithm}} is presented for ultrasound tongue image sequences, which {{can follow the}} motion of tongue contours over long durations with good robustness. To cope with missing segments caused by noise, or by the tongue midsagittal surface being parallel to the direction of ultrasound wave propagation, active contours with a contour-similarity constraint are introduced, {{which can be used}} to provide 'prior' shape information. Also, in order to address accumulation of tracking errors over long sequences, we present an automatic re-initialization technique, based on the complex wavelet image similarity index. Experiments on synthetic data and on real 60 <b>frame</b> <b>per</b> <b>second</b> (<b>fps)</b> data from different subjects demonstrate that the proposed method gives good contour tracking for ultrasound image sequences even over durations of minutes, which can be useful in applications such as speech recognition where very long sequences must be analyzed in their entirety...|$|R
40|$|Image {{thinning}} algorithms {{are widely}} used in image processing to simplify elaboration preserving geometrical features. Standard approaches are based on iterative methods and on distance transforms. Both techniques are well known to be computationally intensive. In this work we propose a parallel, fast and flexible hardware architecture for image thinning to achieve real-time performance. The test case is the 720 × 576 PAL standard video at 25 <b>frame</b> <b>per</b> <b>second</b> (<b>fps).</b> Synthesis was performed for a Stratix II FPGA EP 2 S 30 and for a standard cell 65 nm CMOS technology. The former showed a usage of 4 %slices and 1 % registers, the latter gave an occupation of 5 kgates for the processing core. The execution time for one frame was 0. 03 s on the FPGA and 0. 009 s on the 65 nm, resulting in a maximum throughput of 33 fps and 111 fps, respectively...|$|R
50|$|The Expeed 3A, a {{successor}} to the Expeed 3 EI-160 used in the Nikon 1 series, was first released in the Nikon 1 V2 and mainly features an increased world record image-processing speed of up to 850 megapixels <b>per</b> <b>second.</b> This enables 60 <b>frames</b> <b>per</b> <b>second</b> (15 <b>fps</b> with full autofocus) speed even with the new 14 megapixel image sensor. It is developed exclusively for Nikon 1 cameras.|$|R
2500|$|Techland {{originally}} {{aimed to}} deliver the game at 60 <b>frames</b> <b>per</b> <b>second</b> (<b>FPS)</b> on both PlayStation 4 and Xbox One. However, according to senior game producer Adrian Ciszewski, Dying Lights frame rate will be locked at 30 FPS on consoles {{in order to be}} able to deliver native 1080p graphics, reduce input lag to minimum, as well as to provide a smoother and more gameplay-tailored performance. He considered 1080p/30 FPS [...] "the optimal solution for Dying Light and all its gameplay features on consoles".|$|E
2500|$|... 343 Industries {{general manager}} Bonnie Ross {{recalled}} that after Halo 4, the team {{spent a lot}} of time discussing where they wanted to take the Halo series, reflecting on feedback from fans about what they did and did not like. In late 2012, game leads Tim Longo, David Berger, and Chris Lee laid out the studio's vision for the next game from a creative and technical standpoint, setting key goals to focus on: utilizing the Xbox One's hardware and Microsoft's cloud infrastructure for larger campaign and multiplayer spaces, deeper player investment systems, and a frame rate of 60 <b>frames</b> <b>per</b> <b>second</b> (<b>FPS).</b>|$|E
2500|$|One {{factor that}} complicates {{discussion}} of the Patterson film is that Patterson said he normally filmed at 24 frames per second, but in his haste to capture the Bigfoot on film, he did not note the camera's setting. His Cine-Kodak K-100 camera had markings on its continuously variable dial at 16, 24, 32, 48, and 64 frames per second, but no click-stops, and was capable of filming at any frame speed within this range. Grover Krantz wrote, [...] "Patterson clearly told John Green that he found, after the filming, that the camera was set on 18 <b>frames</b> <b>per</b> <b>second</b> (<b>fps)</b> [...] [...] [...] [...]" [...] It {{has been suggested that}} Patterson simply misread [...] "16" [...] as [...] "18".|$|E
40|$|Abstract—For an {{intelligent}} multi-camera multi-object surveillance system, object correspondence {{across time and}} space is important to many smart visual applications. In this paper, we propose a temporal and spatial consistent labeling algorithm for this demand. In the algorithm, an object corresponding database records the temporal and spatial consistency information for each segmented mask. With the database, the object-mask correlations are propagated through the propagation rules by analyzing mask splitting/merging conditions. In the spatial consistent labeling method, the homography warping and the earth mover’s distance are adopted to match same objects across different views. The earth mover’s distance solves the double matching problem, allows the algorithm to work normally under a small deviation of detected object locations, and makes pairing results have minimum global matching distances. The concept trusting-formerpairs-more is also adopted to avoid frequent pair switching if two objects are too close. The correct spatial labeling rate is about 89. 25 % in average. For online processing applications, the algorithm need not trace back to the past frames. The overall processing speed is about 10. 24 <b>frame</b> <b>per</b> <b>second</b> (<b>fps)</b> with CIF size video running on a 2. 8 GHz general purpose CPU. I...|$|R
40|$|This paper {{describes}} a generalized parallelization methodology for mapping video coding algorithms onto a multiprocessing architecture, through systematic task decomposition, scheduling and performance analysis. It exploits data parallelism {{inherent in the}} coding process and performs task scheduling base on task data size and access locality with the aim to hide as much communication overhead as possible. Utilizing Petri-nets and task graphs for representation and analysis, the method enables parallel video frame capturing, buffering and encoding without extra communication overhead. The theoretical speedup analysis indicates that this method offers excellent communication hiding, resulting in system efficiency well above 90 %. A H. 261 video encoder has been implemented on a TMS 32 OC 8 O system using this method, and its performance was measured. The theoretical and measured performances are similar in that the measured speedup of the H. 261 is 3. 67 and 3. 76 on four PP for QCIF and 352 x 240 video, respectively. They correspond to frame rates of 30. 7 <b>frame</b> <b>per</b> <b>second</b> (<b>fps)</b> and 9. 25 fps, and system efficiency of 91. 8 % and 94 % respectively. As it is, this method is particularly efficient for platforms with small number of parallel processors...|$|R
40|$|The {{formation}} of a free-vortex has been captured by using a high-speed camera (Y 3, IDTVision, Inc.). The experiment is conducted using a rectangular tank, which is filled with tap water. The water free surface is open to atmospheric pressure and is at room temperature, 25. Water occupies a volume of 25 × 25 × 10 cm^ 3. By using a stirring-spoon, the stagnant water is forced to rotate {{at a rate of}} 2 π/sec. Once all the points in the water is rotating, it will be drained from a ball valve, with a diameter of 5 mm, {{from the bottom of the}} tank and the acquisition starts. The {{formation of}} the vortex is captured with a resolution of 352 × 824 pixels at 200 <b>frames</b> <b>per</b> <b>seconds</b> (<b>fps)</b> and is exported at 5 fps and with a resolution of 1280 × 720 in a "fluid dynamics video". The duration of the video in real time is 3. 9 seconds. The slow motion video is 160 seconds. The height of the water remains almost unchanged while acquiring the images. Comment: There are videos include...|$|R
2500|$|The {{original}} Wii U {{version of}} the game received [...] "generally favorable reviews", according to review aggregator Metacritic. Considered by Eurogamer to be [...] "the most vibrant home console racing game in years", the game was praised for its [...] "exquisite details", vast sense of scale, orchestrated soundtrack, and gameplay. Digital Foundry deemed it to be [...] "near perfection" [...] with [...] "phenomenal attention to detail", featuring a [...] "magnificent visual package" [...] and [...] "magical playability". Their technical analysis attributes the game's smoothness of motion and overall gameplay, to its typically high performance of 60 <b>frames</b> <b>per</b> <b>second</b> (<b>FPS),</b> with the split-screen mode's effective 30 FPS nonetheless comparing favorably with industry standard. GameSpot generally praised the game, but criticized the game's Battle Mode for reusing the game's main race courses instead of presenting uniquely created battle arenas as prior Mario Kart games had done. The Nintendo Switch version received [...] "universal acclaim" [...] from Metacritic.|$|E
50|$|Number of <b>frames</b> <b>per</b> <b>second</b> (<b>FPS)</b> {{commonly}} used for compression/decompression speed measurement.|$|E
5000|$|The maximum {{frame rate}} {{supported}} by HEVC is 300 <b>frames</b> <b>per</b> <b>second</b> (<b>fps).</b>|$|E
50|$|In July 2012, Plastic Logic {{demonstrated}} a flexible display that was 130 µm thick, {{as well as}} the first flexible plastic display that can play colour video animation content at 12 <b>frames</b> <b>per</b> <b>second</b> (14 <b>fps</b> in black and white), driven by OTFTs (organic thin film transistors).Plastic Logic also demonstrated several product concepts including an ultra-thin e-paper companion device for a smartphone. The 10.7” touchscreen pane for viewing of webpages and documents was designed for easier reading of content than on the screen of a smartphone.|$|R
40|$|The Retinex is {{an image}} {{enhancement}} algorithm that improves the brightness, contrast and sharpness of an image. It performs a non-linear spatial/spectral transform that provides simultaneous dynamic range compression and color constancy. It {{has been used for}} a wide variety of applications ranging from aviation safety to general purpose photography. Many potential applications require the use of Retinex processing at video frame rates. This is difficult to achieve with general purpose processors because the algorithm contains a large number of complex computations and data transfers. In addition, many of these applications also constrain the potential architectures to embedded processors to save power, weight and cost. Thus we have focused on digital signal processors (DSPs) and field programmable gate arrays (FPGAs) as potential solutions for real-time Retinex processing. In previous efforts we attained a 21 (full) <b>frame</b> <b>per</b> <b>second</b> (<b>fps)</b> processing rate for the single-scale monochromatic Retinex with a TMS 320 C 6711 DSP operating at 150 MHz. This was achieved after several significant code improvements and optimizations. Since then we have migrated our design to the slightly more powerful TMS 320 C 6713 DSP and the fixed point TMS 320 DM 642 DSP. In this paper we briefly discuss the Retinex algorithm, the performance of the algorithm executing on the TMS 320 C 6713 and the TMS 320 DM 642, and compare the results with the TMS 320 C 6711...|$|R
40|$|The main {{objective}} {{of this paper is}} to discuss on the effectiveness of visualising terrain draped with Unmanned Aerial Vehicle (UAV) images generated from different contour intervals using Unity 3 D game engine in online environment. The study area that was tested in this project was oil palm plantation at Sintok, Kedah. The contour data used for this study are divided into three different intervals which are 1 m, 3 m and 5 m. ArcGIS software were used to clip the contour data and also UAV images data to be similar size for the overlaying process. The Unity 3 D game engine was used as the main platform for developing the system due to its capabilities which can be launch in different platform. The clipped contour data and UAV images data were process and exported into the web format using Unity 3 D. Then process continue by publishing it into the web server for comparing the effectiveness of different 3 D terrain data (contour data) draped with UAV images. The effectiveness is compared based on the data size, loading time (office and out-of-office hours), response time, visualisation quality, and <b>frame</b> <b>per</b> <b>second</b> (<b>fps).</b> The results were suggest which contour interval is better for developing an effective online 3 D terrain visualisation draped with UAV images using Unity 3 D game engine. It therefore benefits decision maker and planner related to this field decide on which contour is applicable for their task...|$|R
50|$|<b>Frames</b> <b>per</b> <b>second</b> (<b>FPS)</b> {{tells the}} rate at which a camera is taking photos. Burst rate tells how many frames can be taken in quick succession, before the frame rate slows down.|$|E
5000|$|HD {{video mode}} with autofocus. Up to 1080p at 24, 25 and 50i, 30 and 60i, 720p at 50 or 60 <b>frames</b> <b>per</b> <b>second</b> (<b>fps).</b> H.264/MPEG-4 AVC Expeed video processor. HDMI out with support of {{uncompressed}} video (clean HDMI) ...|$|E
50|$|Featuring a 14 {{megapixel}} {{image sensor}} and further increased autofocus (hybrid autofocus with phase detection/contrast-detect AF and AF-assist illuminator) speed to 15 <b>frames</b> <b>per</b> <b>second</b> (<b>fps),</b> the maximum continuous shooting speed stays at 60 fps {{for up to}} 40 frames.|$|E
40|$|Abstract—Complex scenarios, {{including}} miss detections, oc-clusions, false detections, and trajectory terminations, {{make the}} data association challenging. In this paper, we propose an online tracking-by-detection method to track multiple targets with unified handling of aforementioned complex scenarios, where current detection responses {{are linked to}} previous trajectories. We introduce a dummy node to each trajectory {{to allow it to}} temporally disappear. If a trajectory fails to find its matching detection, it will be linked to its corresponding dummy node until the emergence of its matching detection. Source nodes are also incorporated to account for the entrance of new targets. The standard Hungarian algorithm, extended by the dummy nodes, can be exploited to solve the online data association implicitly in a global manner, although it is formulated between two consecutive frames. Moreover, as dummy nodes tend to accumulate in a fake or disappeared trajectory while they only occasionally appear in a real trajectory, we can deal with false detections and trajectory terminations by simply checking the number of consecutive dummy nodes. Our approach works on a single, uncalibrated camera, and requires neither scene prior knowledge nor explicit occlusion reasoning, running at 132 <b>frame</b> <b>per</b> <b>second</b> (<b>fps)</b> on the PETS 09 -S 2 L 1 benchmark sequence. Experimental results validate the effectiveness of the dummy nodes in complex scenarios and show that our proposed approach is robust against false detections and miss detections. Quantitative comparisons with other methods on five benchmark sequences demonstrate that we can achieve comparable results with most existing offline methods and better results than other online algorithms. Index Terms—Multi-target tracking, complex scenarios I...|$|R
40|$|In general, steel {{processing}} covered {{heat treatment}} process to improve mechanical properties after machining process. The parameter of machining {{process can be}} set {{to improve the quality}} of the material surface according to standard quality. When material quenched in cooling media, there are three different stage during cooling process. The first stage is vapour blanket stage, the second is boiling stage and the last is cooling stage. The purpose of the research is to visualize the cooling process due to effect of surface roughness of medium carbon steel after machining in the water quenching. The speciment has diameter 20 mm and 90 mm in height make by turning process. In order to make different surface roughness on the specimen surface, two different parameter in turning process was applied. For coarse surface roughness the feed parameter was setup in 0. 4 mm/revolution and for the fine surface roughness the feed parameter was set up in 0, 2 mm/revolution with rotational speed of the workpiece is 250 rpm. The specimen was heated with heating temperatur 800 oC, 850 oC and 900 oC and homogenized for 3 hours in the furnace and than quenched in water cooling medium. By using transparant medium, the cooling process stage was recorded using camera Nikon D 7000 with shooting speed 8 <b>frame</b> <b>per</b> <b>second</b> (<b>fps).</b> By comparing <b>frame</b> by frame of the image taken from camera during cooling process, we can analyze the cooling process. The result show that the roughness surface of the specimen give a different effect of the cooling process stage. The coarse surface roughness show that the cooling process need more time than the fine surface roughness and the different of the cooling process very significant in the specimen with heating temperature 900 o...|$|R
40|$|Nowadays, {{surveillance}} cameras are widely installed {{in public places}} for security and law enforcement, but the video quality may be low because of the limited transmission bandwidth and storage capacity. In this study, the authors proposed a gait recognition method for extremely low-quality videos, which have a frame-rate at one <b>frame</b> <b>per</b> <b>second</b> (1 <b>fps)</b> and resolution of 32 × 22 pixels. Different from popular temporal reconstruction-based methods, the proposed method uses the average gait image (AGI) over the whole sequence as the appearance-based feature description. Based on the AGI description, the authors employed {{a large number of}} weak classifiers to reduce the generalisation errors. The performance can be further improved by incorporating the model-based information into the classifier ensemble. The authors found that the performance improvement is directly proportional to the average disagreement level of weak classifiers (i. e. diversity), which can be increased by using the model-based information. The authors evaluated the proposed method on both indoor and outdoor databases (i. e. the low-quality versions of OU-ISIR-D and USF databases), and the results suggest that our method is more general and effective than other state-of-the-art algorithms...|$|R
