31|12|Public
5000|$|In {{terms of}} the total mass M, the nuclear mass m, the density ρ, and a <b>fudge</b> <b>factor</b> f which takes into account {{geometrical}} and other effects, criticality corresponds to ...|$|E
50|$|A <b>fudge</b> <b>factor</b> is {{an ad hoc}} {{quantity}} {{or element}} introduced into a calculation, formula or model {{in order to make}} it fit observations or expectations. Examples include Einstein's Cosmological Constant, dark energy, dark matter and inflation.|$|E
50|$|It is to {{be noted}} that not all the years {{correspond}} to the original year of release, e.g. 1968, 1987, 1990, 1991, 1992, 1997 and 1999. The songs were, however, remixed and/or re-released in those years so the <b>fudge</b> <b>factor</b> fits.|$|E
5000|$|... #Subtitle level 2: <b>Fudge</b> <b>factors</b> and Hilbert's fifteenth problem ...|$|R
40|$|New public {{numerical}} code for fast calculations of the cosmological recombination of primordial hydrogen-helium plasma is presented. The code {{is based on}} the three-level approximation (TLA) model of recombination and allows us to take into account some fine physical effects of cosmological recombination simultaneously with using <b>fudge</b> <b>factors.</b> The code can be found at [URL] 10 pages, 7 figures, 1 table, to be submitted to MNRA...|$|R
50|$|It {{was much}} more {{difficult}} to understand how it worked or why it failed. Designers gathered as much data as possible during the explosion, before the device destroyed itself, and used the data to calibrate their models, often by inserting <b>fudge</b> <b>factors</b> into equations to make the simulations match experimental results. They also analyzed the weapon debris in fallout to see how much of a potential nuclear reaction had taken place.|$|R
5000|$|... where β is a {{dimensionless}} {{number of}} order 1. This is typically {{regarded as a}} purely empirical correction or <b>fudge</b> <b>factor</b> to make the model fit the data, but can have theoretical meaning, for example showing {{the presence of a}} range of activation energies or in special cases like the Mott variable range hopping.|$|E
50|$|John McElroy {{was one of}} {{the first}} to attempt this in 1941, using a {{speculative}} magnetic chart for the year 1500 and currents from pilot charts. His vast overrun in distance was corrected by a <b>fudge</b> <b>factor,</b> leaving his endpoint in the vicinity of Watlings Island. This result was substantially confirmed by Doug Peck's sailing voyage of 1991.|$|E
5000|$|Although the {{calculation}} {{starts with a}} sum of the market capitalisation of the constituent stocks, it's intended to reflect changes in share price, NOT market capitalisation. Therefore, a <b>fudge</b> <b>factor</b> called the [...] "Divisor" [...] is used {{to ensure that the}} index value only changes when stock prices change, not whenever market capitalisation changes. For example, if a company increases its market capitalisation by issuing new shares, the Divisor is adjusted so that the ASX 200 index value does not change.|$|E
50|$|Instead {{of passing}} through points, a {{different}} condition on a curve is being tangent {{to a given}} line. Being tangent to five given lines also determines a conic, by projective duality, but from the algebraic point of view tangency to a line is a quadratic constraint, so naive dimension counting yields 25 = 32 conics tangent to five given lines, of which 31 must be ascribed to degenerate conics, as described in <b>fudge</b> <b>factors</b> in enumerative geometry; formalizing this intuition requires significant further development to justify.|$|R
40|$|The authors present three methods (with {{increasing}} complexity) {{of incorporating}} the gap capacitance of the probe basis into the input impedance calculation of a probe-fed antenna. They are the variational formula, the mode-matching method, and the mode-matching method with a diaphragm. It is {{noted that in}} the previous approach the gap capacitance of the probe base that shunts the input impedance of the microstrip antenna {{is not included in}} the input impedance. Under the single-mode approximation, the mode-matching methods are identical to the variational formula. However, when a diaphragm is present, the single-mode approximation is insufficient. Therefore, the variational formula is inaccurate when a diaphragm exists. The theory shows that the effect of the gap capacitance at the coaxial aperture on the input impedance of the microstrip antenna is important when the dielectric substrate is thick. The theories agree well with experiment data with no <b>fudge</b> <b>factors.</b> link_to_subscribed_fulltex...|$|R
40|$|The general {{equation}} {{from previous}} work is specialized to a linear potential $V(r) =-a+F r$ {{acting in the}} space of spherically symmetric S wave functions. The fine and hyperfine interaction creates then a $\frac 1 r$-dependence in the effective potential energy equation and a position dependent mass $\widetilde m(r) $ in the effective kinetic energy of the associated Schr\"odinger equation. The results are compared with the available experimental and theoretical spectral data on the $\pi$ and $\rho$. Solving the eigenvalue problem within the analytically tractable Airy-function approach induces a certain amount of arbitrariness (<b>fudge</b> <b>factors).</b> Despite of this, the agreement with experimental data is good and partially better than other calculations, including Godfrey and Isgur \cite{GodIsg 85 } and Baldicchi and Prosperi \cite{BalPro 02 }. The short comings of the present model can be removed easily in more elaborate work. Comment: LaTeX 2 e, 5 pages, 11 references. Paper 4 of...|$|R
50|$|The first Cruel Quiz {{was held}} on Monday, 2006-06-26 and had become a regular segment, which ran most nights. This quiz {{contained}} five general knowledge questions. At the beginning, 10 callers are lined up ready to play. Each caller was allowed five seconds to answer each question (known as the five-second <b>fudge</b> <b>factor).</b> The short time limit was designed to reduce {{the ability of a}} caller to quickly Google the answer. In one instance, a caller by the name of Edwin attempted to Google the Austrian capital when Spoonman heard his light typing and cut him off the line.|$|E
5000|$|However, in project {{management}} it's common {{to build a}} certain error margin into the predicted [...] "resource cost" [...] of a project to make predictions more realistic: there are many unforeseen factors that may delay a project or make it more costly, but very few factors {{that could result in}} it being delivered before time or under the calculated budget ... so to some degree, [...] "unexpected" [...] overruns are to be expected, even if their precise nature can't be predicted in advance. Experienced planners may know that a certain type of project will tend to overrun by a certain percentage of its calculated resource requirements, and may multiply the [...] "ideal" [...] calculations by a safety margin to produce a more realistic estimate, and this margin may sometimes be referred to as a <b>fudge</b> <b>factor.</b> However, when planning ahead for expected unpredictabilities, these [...] "error margins" [...] are usually assigned other, more specific names : for instance in warehouse stock control, where a certain amount of stock is expected to disappear naturally through damage, pilfering or other unexplained problems, the discrepancy is referred to as shrinkage.|$|E
40|$|International audienceThe Significance Analysis of Microarrays (Tusher et al. 2001) {{is widely}} used in {{analyzing}} microarray data. It introduces a <b>fudge</b> <b>factor</b> to adjust test statistic by deflating the large value of test statistics due to the small variances. Lin et al. (2008) {{pointed out that the}} <b>fudge</b> <b>factor</b> does not effectively improve the power and control the FDR in the presence of small variance genes. Motivated by those results, we extend our study to compare several methods for choosing the <b>fudge</b> <b>factor</b> and use simulation studies to investigate the power and the control of the FDR of the considered methods...|$|E
40|$|New {{experiment}} studying radioactive ion before K-capture weak decay provides {{new method}} for investigating creation of coherent final state producing ν oscillations. Single radioactive ion passes through known electromagnetic interactions in storage ring before decaying by first order weak interaction conserving momentum described by Fermi’s golden rule. Initial state must contain coherent linear combinations of two states with same momentum difference producing final state oscillations. Following passage of ion through storage ring provides information about ν masses and mixing without detecting ν. Observing every weak decay avoids suppression in conventional oscillation experiments by low ν absorption cross sections. Normally unobservable long wave lengths made observable by long distance circulation around storage ring. No oscillations {{can be observed}} in “missing mass ” experiment where both energy and momentum are conserved. Energy-time uncertainty allows decay into same final state of two initial state components with same momentum difference and slightly different energies. Relative phase of these two components changes with time and can produce oscillations between Dicke superradiant and subradiant states. Analysis of oscillations in simple model with no <b>fudge</b> <b>factors</b> gives value for the squared neutrino mass difference in the same ball park as the KAMLAND value. Treatment consistent with quantum mechanics and causality...|$|R
40|$|The global-beta {{correction}} {{is one of}} {{the optics}} correc-tions such as dispersion and xy-coupling corrections to re-alize a model lattice [1]. In the KEKB B-Factory, it is performed to regularize the ring optics for achieving high luminosity. The global-beta function is measured by re-constructing responses of orbits generated by 6 different steering magnets per plane. The correction of the beta function and the phase advance is performed by using the <b>fudge</b> <b>factors</b> of power supplies of quadrupole magnets. This correction scheme has been successfully. In the typ-ical case, the rms of the beta function beat and the beta-tron tune difference are corrected within 5 % and 0. 0005, respectively. In the luminosity run, we can operate the low energy ring(LER) with the horizontal betatron tune very close to half-integer(45. 5050). In this paper, we will report in detail the measurement and correction techniques and its performance in the KEKB operation. GLOBAL-BETA MEASUREMENT In our measurement method, we reconstruct both beta function β(s) and betatron phase advance φ(s) from the set of the closed orbit distortions(CODs) generated by the single steering kick. From the first order perturbation the-ory of the ring, the displacement of the closed orbit is given as χ(s) = βχ(s) 2 sinπν...|$|R
40|$|For {{more than}} 90 years, solar extreme {{ultraviolet}} (EUV) irradiance modeling has progressed from empirical blackbody radiation formulations, through <b>fudge</b> <b>factors,</b> to typically measured irradiances and reference spectra was well as time-dependent empirical models representing continua and line emissions. A summary of recent EUV measurements by five rockets and three satellites during the 1980 s is presented {{along with the}} major modeling efforts. The most significant reference spectra are reviewed and threee independently derived empirical models are described. These include Hinteregger's 1981 SERF 1, Nusinov's 1984 two-component, and Tobiska's 1990 / 1991 /SERF 2 /EUV 91 flux models. They each provide daily full-disk broad spectrum flux values from 2 to 105 nm at 1 AU. All the models depend to {{one degree or another}} on the long time series of the Atmosphere Explorer E (AE-E) EUV database. Each model uses ground- and/or space-based proxies to create emissions from solar atmospheric regions. Future challenges in EUV modeling are summarized including the basic requirements of models, the task of incorporating new observations and theory into the models, the task of comparing models with solar-terrestrial data sets, and long-term goals and modeling objectives. By the late 1990 s, empirical models will potentially be improved through the use of proposed solar EUV irradiance measurements and images at selected wavelengths that will greatly enhance modeling and predictive capabilities...|$|R
40|$|In this paper, a novel {{method for}} edge {{detection}} of microcalcification clusters in mammogram images is presented using {{the concept of}} Fractal Dimension and Hurst co-efficient that enables to locate the microcalcifications in the mammograms. This technique detects the edges accurately than the ones obtained by the conventional Sobel method. Generally, Sobel method detects {{the edges of the}} regions/objects in an image using the <b>Fudge</b> <b>factor</b> that assumes its value as 0. 5, by default. In this proposed technique, the <b>Fudge</b> <b>factor</b> is suitably replaced with Hurst Co-efficient, which is computed as the difference of Fractal dimension and the topological dimension of a given input image. These two dimensions are image-dependent, and hence the respective Hurst co-efficient too varies with respect to images. Hence, the image-dependent Hurst co-efficient based Sobel method is proved to produce better results than the <b>Fudge</b> <b>factor</b> based Sobel method. The results of the proposed method substantiate the merit of the proposed technique. Comment: Appeared in ICECIT- 201...|$|E
3000|$|R 0 = 7994 m is {{the scale}} height for the Rayleigh scattering, c(T) is the {{concentration}} factor {{that depends on}} the atmospheric turbidity, ν= 4 is the Junge’s exponent, K(λ) is the wavelength-dependent <b>fudge</b> <b>factor,</b> and H [...]...|$|E
40|$|International audienceWe obtain {{quantified}} {{versions of}} Ingham's classical Taube-rian theorem {{and some of}} its variants by means of a natural modification of Ingham's own simple proof. As corollaries of the main general results, we obtain quantified decay estimates for C 0 -semigroups. The results reproduce those known in the literature but are both more general and, in one case, sharper. They also lead {{to a better understanding of}} the so-called " <b>fudge</b> <b>factor</b> " appearing in proofs based on estimating contour integrals...|$|E
40|$|In {{the last}} two years, effort was {{concentrated}} on: (1) surface modeling; (2) surface grid generation; and (3) 3 -D flow space grid generation. The surface modeling shares the same objectives as the surface modeling in computer aided design (CAD), so software available in CAD can in principle be used for solid modeling. Unfortunately, however, the CAD software cannot be easily used in practice for grid generation purposes, {{because they are not}} designed to provide appropriate data base for grid generation. Therefore, we started developing a generalized surface modeling software from scratch, that provides the data base for the surface grid generation. Generating surface grid is an important step in generating a 3 -D space for flow space. To generate a surface grid on a given surface representation, we developed a unique algorithm that works on any non-smooth surfaces. Once the surface grid is generated, a 3 -D space can be generated. For this purpose, we also developed a new algorithm, which is a hybrid of the hyperbolic and the elliptic grid generation methods. With this hybrid method, orthogonality of the grid near the solid boundary can be easily achieved without introducing empirical <b>fudge</b> <b>factors.</b> Work to develop 2 -D and 3 -D grids for turbomachinery blade geometries was performed, and as an extension of this research we are planning to develop an adaptive grid procedure with an interactive grid environment...|$|R
40|$|How fast can we {{compute the}} value of an L-function {{at the center of the}} {{critical}} strip? We will divide this question into two separate questions while also making it more precise. Fix an elliptic curve E defined over Q and let L(E, s) be its L-series. For each fundamental discriminant D let L(E, D, s) be the L-series of the twist ED of E by the corresponding quadratic character; note that L(E, 1, s) = L(E, s). A. How fast can we compute the central value L(E, 1) ? B. How fast can we compute L(E, D, 1) for D in some interval say a ≤ D ≤ b? These questions are obviously related but, as we will argue below, are not identical. We should perhaps clarify what to compute means. First of all, we know, thanks to the work of Wiles and others, that L(E, s) = L(f, s) for some modular form f of weight 2; hence, L(E, s), first defined on the half-plane ℜ(s) > 3 / 2, extends to an analytic function on the whole s-plane which satisfies a functional equation as s goes to 2 − s. In particular, it makes sense to talk about the value L(E, 1) of our L-function at the center of symmetry s = 1. The same reasoning applies to L(E, D, s). As a first approximation to our question we may simply want to know the real number L(E, D, 1) to some precision given in advance; but we can expect something better. The Birch–Swinnerton-Dyer conjectures predict a formula of type (1) L(E, D, 1) = κD m 2 D, for some integer mD and κD an explicit easily computable positive constant. (Up to the usual <b>fudge</b> <b>factors</b> the conjectures predict that m 2 D, if non-zero, should be the order of the Tate–Shafarevich group of ED.) To compute L(E, D, 1) would then mean to calculate mD exactly. In fact, formulas à la Waldspurger have the form (1) with mD the |D|-th coefficient of a modular form g of weight 3 / 2 which is in Shimura correspondance with f. The main point of this note is to discuss informally how explicit versions of such formulas can be used for problem B above. Let us also note the interesting fact that mD, being related to the coefficient of a modular form, typically does not have a constant sign. The significance of the extra information provided by sgn(mD) remains a tantalizing mystery. 2. There is a standard analytic method to compute L(E, 1), which we now recall. If E has conductor N then the associated modular form f has level N and f|wN = −εf...|$|R
40|$|We want to {{accurately}} know the energy spread and bunch length dependence on current in the ATF damping ring. One reason {{is to know}} {{the strength of the}} impedance: From the energy spread measurements we know whether or not we are above the threshold to the microwave instability, and from the energy spread and bunch length measurements we find out the extent of potential-well bunch lengthening (PWBL). Another reason for these measurements is to help in our understanding of the intra-beam scattering (IBS) effect in the ATF. The ATF as it is now, running below design energy and with the wigglers turned off, is strongly affected by IBS. To check for consistency with IBS theory of, for example, the measured vertical beam size, we need to know all dimensions of the beam, including the longitudinal one. But beyond this practical reason for studying IBS, IBS is currently a hot research topic at many accelerators around the world (see e. g. Ref. [1]), and the effect in actual machines is not well understood. Typically, when comparing theory with measurements <b>fudge</b> <b>factors</b> are needed to get agreement (see e. g. Ref. [1]). With its strong IBS effect, the ATF is an ideal machine for studying IBS, and an indispensable ingredient for this study is a knowledge of the longitudinal phase space of the beam. The results of earlier bunch lengthening measurements in the ATF can be found in Refs. [2]-[4]. Measurements of current dependent effects, especially bunch length measurements using a streak camera, can be difficult to perform accurately. For example, space charge in the camera itself can lead to systematic errors in the measurement results. It is important the results be accurate and reproducible. In the measurements of both December 1998 [3] and December 1999 [4], by using light filters, the authors first checked that space charge in the streak camera was not significant. And then the Dec 99 authors show that their results agree with those Dec 98, i. e. on the dates of the two measurements the results were reproducible. Since IBS is so strong in the ATF, in the Dec 99 measurements an attempt was made to estimate the impedance effect using the following method: First, from the form of the energy spread vs. current measurements it was concluded that the threshold to the microwave instability was beyond 2 mA. Then, by dividing the bunch length vs. current curve by the energy spread vs. current curve the effect of IBS was divided out, and PWBL was approximated. The assumption is that PWBL can be treated as a perturbation on top of IBS. The result was that this component of bunch lengthening was found to grow by 7 - 15 % (depending on the rf voltage) between the currents of. 5 mA and 2 mA, about a factor of 3 less than the total bunch length growth. The conclusion was that the inductive component of the impedance was small, in fact much smaller than had been concluded earlier in Ref. [2]. Electron machines generally run in a parameter regime where IBS is an insignificant effect, and impedance measurements and calculations have also normally been performed for machines where IBS is unimportant. To simplify the interpretation of the impedance from bunch length measurements, in April 2000 the energy spread and bunch length measurements of Dec 99 were repeated, but now with the beam on a linear (difference) coupling resonance, where the horizontal and vertical emittances were approximately equal. For this case the effect of IBS was expected to be very small. An energy spread vs. current measurement under such conditions will also allow us to more clearly see whether we reach the threshold to the microwave instability. As part of the April data taking we, in addition, repeated the earlier off-coupling measurements, in order to check the reproducibility of the earlier results. In this report we present and analyze this recent set of data, and compare it with the results of the earlier measurements, particularly those of Dec 99. The measurements and analysis of data in this report follow essentially the same procedure as was used in Ref. [4]. In the present report we will try to be relatively brief. The comparison of our results with IBS theory will be given in a following report. For more details about the measurement and analysis techniques presented in this report, the reader should consult Ref. [4]...|$|R
40|$|In recent years, the {{business}} world has been riddled with scandals and fraud that consistently make international headlines. These transgressions do not just occur overnight. Rather, they begin {{with the kinds of}} moral slip-ups that take place in offices regularly. They are based in poor leadership, misguided systems, loose protocols, and, most of all, innate human behavior. This paper examines six ethical dilemmas that are the result of psychological behavior: the <b>fudge</b> <b>factor,</b> slippery slope, conflict of interest, ego depletion, revenge, and social group cheating. I explore solutions to these issues that come from a series of academic studies as well as from real-life cases in business environments to minimize or eliminate the associated moral transgressions in the workplace...|$|E
40|$|The major {{theoretical}} limitation for extracting cosmological parameters {{from the}} CMB sky {{lies in the}} precision {{with which we can}} calculate the cosmological recombination process. Uncertainty in the details of hydrogen and helium recombination could effectively increase the errors or bias the values of the cosmological parameters derived from the Planck satellite, for example. Here we modify the cosmological recombination code RECFAST by introducing one more parameter to reproduce the recent numerical results for the speed-up of the helium recombination. Together with the existing hydrogen <b>fudge</b> <b>factor,</b> we vary these two parameters to account for the remaining dominant uncertainties in cosmological recombination. By using the CosmoMC code with Planck forecast data, we find that we need to determine the parameters to better than ten per cent for He I and one per cent for H, in order to obtain negligible effects on the cosmological parameters. For helium recombination, if the existing studies have calculated the ionization fraction to the 0. 1 per cent level by properly including the relevant physical processes, then we already have numerical calculations which are accurate enough for Planck. For hydrogen, setting the <b>fudge</b> <b>factor</b> to speed up low redshift recombination by 14 per cent appears to be sufficient for Planck. However, more work still {{needs to be done to}} carry out comprehensive numerical calculations of all the relevant effects for hydrogen, as well as to check for effects which couple hydrogen and helium recombinaton through the radiation field. Comment: 6 pages, 6 figures; Paper revised according to the reviewer's suggestions. An updated RECFAST (version 1. 4) available at [URL]...|$|E
40|$|In {{this article}} we {{investigate}} the time evolution of the adiabatic (curvature) and isocurvature (entropy) spectral indices after end of inflation for all cosmological scales and two different initial conditions. For this purpose, we first extract an explicit equation for the time evolutin of the comoving curvature perturbation (which may {{be known as the}} generalized Mukhanov-Sasaki equation). It shall be manifested that the evolution of adiabatic spectral index severely depends on the initial conditions and just for the super-Hubble scales and adiabatic initial conditions is constant as be expected. Moreover, it shall be clear that the adiabatic spectral index after recombination approach to a constant value for the isocurvature perturbations. Finally, we re-investigate the Sachs-Wolfe effect and show that the <b>fudge</b> <b>factor</b> 13 in the adiabatic ordinary Sachs-Wolfe formula must be replaced by 0. 4. ...|$|E
40|$|The aim of {{this note}} is to {{describe}} how the “fudge factors ” in the Birch and Swinnerton-Dyer conjecture vary {{in a family of}} quadratic twists (see Proposi-tion 5, which follows directly from Tate’s algorithm [T]). We illustrate with two examples. Definition 1. If E is an elliptic curve over Q and p is a prime, the <b>fudge</b> <b>factor</b> (or Tamagawa factor) cp(E) is defined by cp(E) = [E(Qp) : E 0 (Qp) ] where E 0 (Qp) is the subgroup of E(Qp) consisting of those points whose re-duction modulo p (on a minimal model of E) is nonsingular. The fundamental method for computing the fudge factors is Tate’s al-gorithm. This algorithm, originally described in a 1965 letter to Cassels, was published in [T] and essentially reproduced in §IV. 9 of [S]. Stan-dard number theoretic computer packages, such as PARI/GP (available a...|$|E
40|$|Analysis of {{the data}} from Golub et al. Consider the {{microarray}} experiment in Golub et al. (1999) where ALL and AML subtypes of leukemia are compared. The data are available within package multtest. We can analyse those data in SAGx with the function samrocNboot. The ideas behind it are presented in Broberg (2003). Briefly, the method relies on a penalised t-test statistica d = (¯x 1 − ¯x 2) /(S + a) with <b>fudge</b> <b>factor</b> a Efron et al. (2001). In this case the effect estimated consists of a difference in group means. In general the method can estimate and test one such effect {{in the presence of}} explanatory variables such as AGE or GENDER using a linear model. In such a case the function samrocN provides a solution. Example code now follows. > library("SAGx") > library("multtest") > data(golub) > set. seed(849867) > samroc. res show(samroc. res) Samroc result...|$|E
30|$|Considering {{these many}} factors, Prismatic uses the {{following}} heuristic {{to choose a}} good launch configuration. At runtime, {{the properties of the}} available devices are queried, which includes the maximum number of threads per threadblock, the total amount of shared memory, and the total number of streaming multiprocessors. BlockSize_x is chosen to be either the largest power of two smaller than the number of plane waves or the maximum number of threads per block, whichever is smaller. The total number of threadblocks that can run concurrently on a single streaming multiprocessor is then estimated using BlockSize_x, the limiting number of threads per block, and the limiting number of threadblocks per streaming multiprocessor. The total number of threadblocks across the entire device is then estimated as this number times the total number of streaming multiprocessors, and then the grid dimensions of the launch configuration are set to create three times this many blocks, where the factor of three is a <b>fudge</b> <b>factor</b> that we found produces better results.|$|E
40|$|There {{is a well}} {{developed}} framework, the Black-Scholes theory, for the pricing of contracts based on the future prices of certain assets, called options. This theory assumes that the probability distribution of the returns of the underlying asset is a gaussian distribution. However, it is observed in the market that this hypothesis is flawed, leading {{to the introduction of}} a <b>fudge</b> <b>factor,</b> the so-called volatility smile. Therefore, {{it would be interesting to}} explore extensions of the Black-Scholes theory to non-gaussian distributions. In this contribution we provide an explicit formula for the price of an option when the distributions of the returns of the underlying asset is parametrized by an Edgeworth expansion, which allows for the introduction of higher independent moments of the probability distribution, namely skewness and kurtosis. We test our formula with options in the brazilian and american markets, showing that the volatility smile can be reduced. We also check whether our approach leads to more efficient hedging strategies of these instruments. ...|$|E
40|$|We discuss {{two aspects}} of planet {{formation}} in this thesis. At first we present a model of type I migration which includes new results of outward migration of low mass planets. The second part contains SADFACE, a 1 -dimensional vertically integrated disk model for planet population synthesis calculations. Results of the first part is that with the inclusion of thus modes of outward migration no artificial <b>fudge</b> <b>factor</b> to reduce the speed of type I migration is needed. We find {{two to three times}} as many ”cold” planets when we include outward migration than without. Results of the second part is that in the scope of the model we can find initial conditions that fit observed disks quite well. We also present likelihood of initial conditions which can be further used for planet population synthesis calculations. We also find that rotation and temperature of a cloud are correlated and can not be independently chosen...|$|E
40|$|Topological defects must respect causality, a {{statement}} leading to restrictive {{constraints on the}} power spectrum of the total cosmological perturbations they induce. Causality constraints have for long been known to require {{the presence of an}} under-density in the surrounding matter compensating the defect network on large scales. This so-called compensation can never be neglected and significantly complicates calculations in defect scenarios, eg. computing cosmic microwave background fluctuations. A quick and dirty way to implement the compensation are the so-called compensation fudge factors. Here we derive the complete photon-baryon-CDM backreaction effects in defect scenarios. The <b>fudge</b> <b>factor</b> comes out as an algebraic identity and so we drop the negative qualifier ``fudge''. The compensation scale is computed and physically interpreted. Secondary backreaction effects exist, and neglecting them constitutes the well-defined approximation scheme within which one should consider compensation factor calculations. We quantitatively assess the accuracy of this approximation, and conclude that the considerable pains associated with improving on it are often a waste of effort...|$|E
40|$|This paper {{demonstrates}} how the principal-agent problem between venture capitalists and their investors (limited partners) causes limited partner returns {{to depend on}} diversifiable risk. Our theory shows why the need for investors to motivate VCs alters the negotiations between VCs and entrepreneurs and changes how new firms are priced. The three-way interaction rationalizes the use of high discount rates by VCs and predicts a correlation between total risk and net of fee investor returns. We take our theory to a unique data set and find empirical support for {{the effect of the}} principal-agent problem on equilibrium private equity asset prices. (JEL G 24, D 82, G 31) 2 Venture capitalists (often called VCs) are known to use high discount rates in assessing potential investments. This strategy may be just a <b>fudge</b> <b>factor</b> that offsets optimistic entrepreneurial projections, but VCs claim to use high discount rates even in internal projections. Furthermore, Cochrane (2005) looks at individual VC projects and shows that they earn large positive alphas, which suggests the use of high discount rates in pricing. In general, VCs see...|$|E
40|$|By {{combining}} a renormalization group argument {{relating the}} charge e and mass m of the proton by e^ 2 ln m ~ 0. 1 pi (in Planck units) with the Carter-Carr-Rees anthropic argument that gives an independent approximate relation m ~ e^ 20 {{between these two}} constants, both can be crudely estimated. These equations have the factor of 0. 1 pi and the exponent of 20 which depend upon known discrete parameters (e. g., the number of generations of quarks and leptons, {{and the number of}} spatial dimensions), but they contain NO continuous observed parameters. Their solution gives the charge of the proton correct to within about 8 %, though the mass estimate is off by a factor of about 1000 (16 % error on a logarithmic scale). When one adds a <b>fudge</b> <b>factor</b> of 10 previously given by Carr and Rees, the agreement for the charge is within about 2 %, and the mass is off by a factor of about 3 (2. 4 % error on a logarithmic scale). If this 10 were replaced by 15, the charge agrees within 1. 1 % and the mass itself agrees within 0. 7 %. Comment: 12 pages, LaTe...|$|E
40|$|AbstractBy {{combining}} a renormalization group argument {{relating the}} charge e and mass mp of the proton by e 2 lnmp≈− 0. 1 π (in Planck units) with the Carter–Carr–Rees anthropic argument that gives an independent approximate relation mp∼e 20 {{between these two}} constants, both can be crudely estimated. These equations have the factor of 0. 1 π and the exponent of 20 which depend upon known discrete parameters (e. g., the number of generations of quarks and leptons, {{and the number of}} spatial dimensions), but they contain no continuous observed parameters. Their solution gives the charge of the proton correct to within about 8 %, though the mass estimate is off by a factor of about 1000 (16 % error on a logarithmic scale). When one adds a <b>fudge</b> <b>factor</b> of 10 previously given by Carr and Rees, the agreement for the charge is within about 2 %, and the mass is off by a factor of about 3 (2. 4 % error on a logarithmic scale). If this 10 were replaced by 15, the charge agrees within 1. 1 % and the mass itself agrees within 0. 7 %...|$|E
