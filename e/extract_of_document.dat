0|10000|Public
50|$|For the LFs, the EFS {{provides}} an electronic case file showing hearing dates and documents file by them, served on them or {{received from the}} Courts. It also {{provides an}} electronic platform for the service <b>of</b> <b>documents</b> on other LFs. The EFS also allows for faster response as well as accurate and up-to-date information. Hence, other benefits of the EFS include the speedy inspection <b>of</b> <b>documents</b> electronically {{and the ability to}} request for and receive electronic <b>extracts</b> <b>of</b> <b>documents</b> via the Internet. Electronic cause book searches and legal research are also available through LawNet. Lawyers can even obtain details of hearing fixtures via Short Messaging System (SMS) using their mobile phones.|$|R
40|$|Perception needs {{comparison}} but comparing {{different parts}} <b>of</b> a temporal <b>document</b> {{is not an}} easy task though. We have build up a set of methods to split up temporal documents (such as digital videos or musical recordings) into discrete objects. Ergonomic interfaces are then developped to exploit these results. In these interfaces, browsing fonctionnalities and extracts selection are simple and fast. Those kind of representations help contents perception as well as documents analysis. In order to do so, we propose an interface that enables to program the rendering of the discrete objects. This "un-sequencing " <b>of</b> the <b>document</b> is brought out various comparing functionalities that can be used with <b>extracts</b> <b>of</b> <b>documents</b> <b>of</b> the same or different media...|$|R
50|$|Whereas Dharampal’s {{published}} oeuvre, in dispelling colonial {{myths about}} India’s recent past, {{serves as a}} seminal and powerful inspiration for engaging in crucial reinterpretations {{about the nature of}} Indian society, the enormous portent of his research (much of which in the form of extensive notes and typed <b>extracts</b> <b>of</b> <b>documents</b> from British and Indian archives still remains in manuscript form) has yet to impact more extensively on radically transforming conventional historiography of modern India. Copies of Dharampal’s extensive archival collection are lodged in the library of the Gandhi Seva Sangh, Sevagram, Wardha and at the Centre for Policy Studies, Chennai.|$|R
40|$|Metadata {{extraction}} from texts is import ant {{since it}} enables search for documents {{based on the}} metadata identified. It is impossible to retrieve journal articles without the metadata <b>of</b> <b>documents,</b> which includes information such as author, title, and journal publication details. This paper proposes a new technique to <b>extract</b> metadata <b>of</b> <b>documents,</b> called Metadata Extraction with Cue Model, which uses combinations of a few features to extract metadata automatically from documents. Automatic metadata extraction is increasingly important in today’s influx of published material...|$|R
40|$|This work is copyright. The Copyright Act 1968 permits {{fair dealing}} for study, research, news reporting, {{criticism}} or review. Selected passages, tables or diagrams may be reproduced {{for such purposes}} provided acknowledgment of the source is included. Major <b>extracts</b> <b>of</b> the entire <b>document</b> may not be reproduced by any process withou...|$|R
40|$|A {{sentence}} <b>extract</b> summary <b>of</b> a <b>document</b> is {{a subset}} <b>of</b> the <b>document</b> 's sentences {{that contains the}} main ideas in the document. We present two approaches to generating such summaries. The first uses a pivoted QR decomposition of the term-sentence matrix {{in order to identify}} sentences that have ideas that are distinct from those in other sentences...|$|R
6000|$|... "At any rate, {{it is an}} ill {{that has}} agreed uncommonly well with you," [...] growled George, as, rising from the table, {{he went to a}} solid iron safe that stood {{in the corner of the}} room, and, {{unlocking}} it with a small key that he took from his pocket, <b>extracted</b> a bundle <b>of</b> <b>documents.</b>|$|R
25|$|Le Caron {{compiled}} {{the first}} dictionary of the Huron language, and also dictionaries of the Algonkin and Montagnais languages. None are extant today. In June 1624, {{he sent to}} France {{a study of the}} Indians, their customs, and the difficulties involved in their conversion. Large <b>extracts</b> <b>of</b> the <b>document</b> were preserved by Le Clercq. The introduction refers to a second memoir, the manuscript of which is now lost. He wrote two indictments to the king of the Compagnie des Marchands de Rouen et de Saint-Malo, which the Recollects believed were hindering the evangelization of the Indians.|$|R
40|$|This work is copyright. The Copyright Act 1968 permits {{fair dealing}} for study, research, news reporting, {{criticism}} or review. Selected passages, tables or diagrams may be reproduced {{for such purposes}} provided acknowledgement of the source is included. Major <b>extracts</b> <b>of</b> the entire <b>document</b> may not be reproduced by any process without written permission of the publisher. This publication is also available online at: www. science. org. a...|$|R
40|$|Abstract—Digital library users {{might not}} enter a digital library through {{homepage}} menus. As a result, digital library owners {{should consider the}} visibility to search engines <b>of</b> stored PDF <b>documents.</b> The aim <b>of</b> this research project was to determine {{to what extent the}} visibility <b>of</b> these PDF <b>documents</b> can be improved. In a series of empirical experiments, 100 PDF documents stored on digital libraries were identified an inspected. Searches were done for them and rankings on search engine result pages recorded. The current visibility <b>of</b> these <b>documents</b> was then calculated. After submission to Google, a waiting period was allowed for crawler visitation and the searches repeated. The results of these experiments proved that the visibility <b>of</b> these <b>documents</b> could be improved only marginally. It is therefore concluded that the designers of university digital libraries should consider other alternatives, such as providing text <b>extracts</b> <b>of</b> PDF <b>documents,</b> to enhance the overall visibility of content. Keywords-digital library; PDF document; search engine crawler I...|$|R
40|$|International audienceIn {{this chapter}} {{we present a}} {{flexible}} and configurable application for mining large XML document collections. This work is centered on the process <b>of</b> <b>extracting</b> <b>document</b> features related to structure and content. From this process, an attribute frequency matrix is generated and, depending on the cluster algorithm, it is transformed and/or used to obtain similarity measures...|$|R
50|$|Between September 1923 and February 1973 (nine {{days before}} his death), Camilleri held a diary which was never {{intended}} to be published or even read by others. Its Italian publishers call it Diario Intimo (Intimate Journal). Unfortunately, only short <b>extracts</b> <b>of</b> this remarkable <b>document</b> have ever been published. It runs over twenty-eight registers containing in all some four thousand pages. They are held at {{the archives of the}} Salesian Pontifical University in Rome.|$|R
40|$|This paper uses Systemic Functional Linguistic (SFL) {{theory as}} a basis for <b>extracting</b> {{semantic}} features <b>of</b> <b>documents.</b> We focus on the pronominal and determination system and the role it plays in constructing interpersonal distance. By using a hierarchical system model that represents the author’s language choices, it is possible to construct a rich and informative feature representation. Using these systemic features, we report clear separation between registers with different interpersonal distance. 4 page(s...|$|R
40|$|URL] audienceThis work {{presents}} a methodology for XML mining {{centered on the}} process <b>of</b> <b>extracting</b> <b>document</b> features related to structure and content. This information is used to obtain similarity measures and to cluster XML documents. A conceptual framework is proposed to design an application with {{the primary goal of}} implementing a modular and easily configurable tool for mining large XML document collections...|$|R
40|$|Summary {{evaluation}} measures {{produce a}} ranking <b>of</b> all possible <b>extract</b> summaries <b>of</b> a <b>document.</b> Recall-based evaluation measures, which depend on costly human-generated ground truth summaries, produce uncorrelated rankings when ground truth is varied. This paper proposes usir/g sentence-rankbased and content-based measures for evaluating extract summaries, and compares these with recall- based evaluation measures. Content-based measures increase {{the correlation of}} rankings induced by synonymous ground truths, and exhibit other desirable properties...|$|R
50|$|The first {{scholar who}} dealt with its study was Aureliano Fernández-Guerra in 1865, {{in the very}} {{important}} article «Speech about the Carther of Aviles». He used <b>extracts</b> <b>of</b> 19 <b>documents</b> from 1256 to 1316. In 1868, Martín Sangrador y Vitores included in his work about the Asturias administration a copy in Galician of the royal prerrogative given away by Fernando II to the Abbey. The next edition <b>of</b> the <b>documents</b> about monastery {{had to wait until}} the mid-20th century, when the Royal Institute of Asturian Studies (RIDEA) edited the article «El convento benedictino de Villanueva de Ozcos» by Marcos G. Martínez, a rather poor edition. Only in 1981, Pedro Floriano Llorente publishes in RIDEA «Colección dipolomática de Villanueva de Ozcos», which implies an important improvement concerning the previous, both by perfection technical, as by the personal and toponymic references.|$|R
40|$|Our {{purpose is}} to design and develop a {{workflow}} based on RDF (Resource Description Framework) that generates Semantic Graph for a set <b>of</b> technical terms <b>extracted</b> from <b>documents</b> <b>of</b> various formats, such as PDF, HTML, and plain text. Our attempt is to generate Semantics Graph {{as a result of}} text mining including morphological analysis and syntax analysis. |$|R
40|$|Abstract—This work {{presents}} a methodology for XML mining {{centered on the}} process <b>of</b> <b>extracting</b> <b>document</b> features related to structure and content. This information is used to obtain similarity measures and to cluster XML documents. A conceptual framework is proposed to design an application with {{the primary goal of}} implementing a modular and easily configurable tool for mining large XML document collections. Index Terms—conceptual framework, clustering, modularity, XML mining, UML...|$|R
40|$|Abstract. The {{number of}} company data {{deposited}} in {{hierarchical storage management}} systems heavily increases. Thus, new approaches are necessary {{to keep track of}} a data pool. This paper introduces a semantic storage extension (SSE) for existing hierarchical storage management systems that allows them to exploit semantic relations between files and use them for a more efficient and more intelligent data management. Our approach enhances traditional hierarchical storage management systems regarding migration, deletion, and retrieval operations by making use of semantic relations between files and contextual knowledge. Thereby a predictive file management is possible, which contributes to an increasing system performance and a better user experience. To this end, the SSE uses <b>extracted</b> features <b>of</b> <b>documents</b> to define relations between them and also offers the possibility to specify additional knowledge by a domain expert...|$|R
30|$|This {{work has}} applied a {{multiscale}} graph partitioning algorithm (Markov Stability) to <b>extract</b> content-based clusters <b>of</b> <b>documents</b> from a textual dataset of healthcare safety incident reports in an unsupervised manner {{at different levels}} of resolution. The method uses paragraph vectors to represent the records and obtains an ensuing similarity graph <b>of</b> <b>documents</b> constructed from their content. The framework brings the advantage of multi-resolution algorithms capable of capturing clusters without imposing a priori their number or structure. Since different levels of resolution of the clustering can be found to be relevant, the practitioner can choose the level of description and detail to suit the requirements of a specific task.|$|R
40|$|In {{this paper}} I explore the discursive {{use of public}} space, {{understood}} as a rhetorical resource for localized social action. Analysing several <b>extracts</b> <b>of</b> written <b>documents</b> and in-depth interviews, I focus on the rhetorical use of space-formulations and constructions of people-in-place relations by social and institutional agents confronted by the physical definition of a space in conflict. The discursive work includes the rhetorical management of culturally and ideologically organized constructions of urban territoriality, argumentative uses of localized social categories and behaviour-scripts {{for the performance of}} normative patterns of coexistence in the public space, and rhetorical work on spatially rooted symbolic processes. The purpose is to contribute to a critical examination of conflictive sociospatial phenomena from a discursive approach, seeking to make visible the social tensions involved in the deliberate attempt to control and organize urban space. </p...|$|R
40|$|The {{collection}} contains a photocopy of 22 handwritten pages of notes {{pertaining to the}} Schutzjude Samuel Abraham of Osterode. The notes are <b>extracts</b> from <b>documents</b> <b>of</b> the Royal State Archives in Königsberg, Prussia (R. K. C. Spec. XXXii, Tit. 6, No. 1 I). They were taken by Saly S. Samulon, a lawyer from Graudenz (today Grudziądz), when he visited the archives in October and November 1910. Processed for digitizatio...|$|R
40|$|Most <b>of</b> web <b>documents</b> {{coverage}} multiple topic. Identifying sentiment <b>of</b> multi-topic <b>documents</b> is {{a challenge}} task. In this paper, we proposed a new method to solve this problem. The method firstly reveals the latent topical facets in documents by Parametric Mixture Model. By focusing on modeling the generation process <b>of</b> a <b>document</b> with multiple topics, we can <b>extract</b> specific properties <b>of</b> <b>documents</b> with multiple topics. PMM models documents with multiple topics by mixing model parameters of each single topic. In order to analyze sentiment of each topic, conditional random fields techniques is used to identify sentiment. Empirical experiments on test datasets show that this approach is effective for extracting subtopics and revealing sentiments of each topic. Moreover, this method is quite general and {{can be applied to}} any kinds of text collections...|$|R
40|$|Neste trabalho foi feito um estudo de tendências tecnológicas em nanotecnologia aplicado ao setor de materiais poliméricos, com base em informações extraídas de documentos de patentes. Foi usada como fonte de dados o banco de patentes da USPTO (United States Patent Trademark Office). Os dados foram obtidos via web, utilizando-se diversas palavras-chaves Foram mapeados os principais países depositantes, tipo de depositante e ano de aplicação, setores de aplicação, tipos de polímeros utilizados e principais aditivos e cargas incorporados às matrizes poliméricas. In {{this work}} {{a study of}} {{technological}} tendencies in nanotechnology applied to polymeric materials sector was carried out, based on information <b>extracted</b> <b>of</b> paten <b>documents.</b> The patent office of USPTO (United States Patent Trademark Office) {{was used as a}} data source. The data were supplied via web, using several keywords. A mapping was made of the major countries contributing, types and year of patent deposition, application sectors, polymer types used, main additives and fillers incorporated to the polymeric matrices...|$|R
40|$|This {{case study}} {{examined}} the utility of regular expressions to identify clinical data relevant to the epidemiology of treatment of hypertension. We designed a software tool that employed regular expressions to identify and <b>extract</b> instances <b>of</b> <b>documented</b> blood pressure values and anti-hypertensive treatment intensification from the text of physician notes. We determined sensitivity, specificity and precision of identification of blood pressure values and anti-hypertensive treatment intensification using a gold standard of manual abstraction of 600 notes by two independent reviewers. The software processed 370 Mb of text per hour, and identified elevated blood pressure documented in free text physician notes with sensitivity and specificity of 98 %, and precision of 93. 2 %. Anti-hypertensive treatment intensification was identified with sensitivity 83. 8 %, specificity of 95. 0 %, and precision of 85. 9 %. Regular expressions {{can be an effective}} method for focused information extraction tasks related to high-priority disease areas such as hypertension...|$|R
40|$|Note: (ISCII- 91 or IS 13194 : 1991 is {{the current}} National Standard used for Indian Languages) 1. This {{document}} is a soft copy <b>of</b> some <b>extracts</b> <b>of</b> the original <b>document</b> published by Bureau of Indian Standards. The tabulation and Roman transliteration is not {{identical to the original}} document due to differences in the typesetting software used earlier. The purpose is to convey the information only and no reference should be made to this soft copy as a standard document. Annex G and H are not included in this soft copy. 2. The original document is available from the following address (Group LTD- 37...|$|R
40|$|A {{sentence}} <b>extract</b> summary <b>of</b> a <b>document</b> is {{a subset}} of the doc-ument's sentences that contains the main ideas in the document. We present two approaches to generating such summaries. The rst uses a pivoted QR decomposition of the term-sentence matrix in order to identify sentences that have ideas that are distinct from those in other sentences. The second is based on a hidden Markov model that judges the likelihood that each sentence should be contained in the summary. We compare the results of these methods with summaries generated by humans, showing that we obtain higher agreement than do earlier methods...|$|R
40|$|AbstractTo address {{multilingual}} document classification in an effcient {{and effective}} manner, we {{claim that a}} synergy between classical IR techniques such as vector model and some advanced data mining methods, especially Formal Concept Analysis, is particularly appropriate. We propose in this paper, a new statistical approach for extracting inter-language clusters from multilingual documents based on Closed Concepts Mining and vector model. Formal Concept Analysis techniques are applied to extract Closed Concepts from comparable corpora; and, then, exploit these Closed Concepts and vector models in the clustering and alignment <b>of</b> multilin- gual <b>documents.</b> An experimental evaluation is conducted on the collection <b>of</b> bilingual <b>documents</b> French-English <b>of</b> CLEF’ 2003. The results confirmed that the synergy between Formal Concept Analysis and vector model is fruitful to <b>extract</b> bilingual classes <b>of</b> <b>documents,</b> with an interesting comparability score...|$|R
40|$|We propose {{an agent}} for {{exploring}} and categorizing documents on the World Wide Web based on a user pro#le. The heart of the agent is an automatic categorization of a set <b>of</b> <b>documents,</b> combined with a process for generating new queries used to search for new related documents and #ltering the resulting documents to <b>extract</b> the set <b>of</b> <b>documents</b> most {{closely related to the}} starting set. The document categories are not given a-priori. The resulting document set could also be used to update the initial set <b>of</b> <b>documents.</b> We present the overall architecture and describe two novel algorithms which provide signi#cant improvementover traditional clustering algorithms and form the basis for the query generation and search component of the agent. 1 Introduction The World Wide Web is a vast resource of information and services that continues to grow rapidly. Powerful search engines have been developed to aid in locating unfamiliar documents by category, contents, or subject. Relying on la [...] ...|$|R
40|$|Using a mixed methods {{approach}} this thesis explores {{the construction and}} dissemination of a cross-cultural play within the Australian context. The experience of developing and performing this play confirms, I believe, the valuable contribution performance could make to the contentious domain of competing epistemologies within decolonizing research methodology. Performance is able to interrogate the encrypted language buried within the emotionally complex terrain of decolonizing intent. By celebrating through story a shared humanity, performance can demonstrate both the on-going pain and shame of shared history {{and the possibility of}} moving beyond the negative towards a more positive collective future. Although this study did not find a reconciliation narrative, it did locate the beginning of one. Through an exploration of the Myall Creek massacre of 1838 and the Memorial erected to commemorate it 162 years after the event, this study found a narrative about the power of acknowledgement. This study also suggests that the cultural impediments to reconciliation lie within five persistent narratives intrinsic to the Australian colonization process. Predominantly reliant on performance ethnography as the principle research methodology, the play {{at the heart of this}} research endeavour, Today We’re Alive, is verbatim theatre, where only the words that were spoken in the field and <b>extracts</b> <b>of</b> <b>documents</b> in the public domain contribute to the performance text. The voices that tell this story include Aboriginal and non-Aboriginal members of the Myall Creek Memorial Committee, descendants of massacre survivors, descendants of the massacre perpetrators and informed others recommended to me in the field. The first draft of the play was taken back to the primary research field to a community hall near the massacre site over 600 kilometres from Sydney for a performed reading. The six actor/co-researchers, three Aboriginal and three non-Aboriginal performers, delivered a performance that exceeded all expectations. Several factors, I believe, influenced this outcome. Therefore this draft of the play is embedded in the body of this thesis, as the script and the performance of it are critical to the analysis of the research findings...|$|R
30|$|Information of fuel, {{heat value}} (HV) and heating rate (HR) for every power plant is <b>extracted</b> from <b>document</b> <b>of</b> “Detailed {{statistics}} of power generation in Iran” (Tavanir Expert Holding Company 2013). As the plants use several fuels (gasoline, fuel {{oil and natural}} gas), heating value and fuel consumptions of generation units are formulated in as the weighted average. Energy balance document is used to specify fuel price of plants (Ministry of Energy 2013). Variable maintenance cost of plants is also provided by Iran Grid Management Company.|$|R
40|$|International audienceDealing with non {{annotated}} {{documents for}} the design <b>of</b> a <b>document</b> recognition system {{is not an easy}} task. In general, statistical methods cannot learn without an annotated ground truth, unlike syntactical methods. However their ability to deal with non annotated data {{comes from the fact that}} the description is manually made by a user. The adaptation to a new kind <b>of</b> <b>document</b> is then tedious as the whole manual process of extraction of knowledge has to be redone. In this paper, we propose a method to extract knowledge and generate rules without any ground truth. Using large volume <b>of</b> non annotated <b>documents,</b> it is possible to study redundancies <b>of</b> some <b>extracted</b> elements in the document images. The redundancy is exploited through an automatic clustering algorithm. An interaction with the user brings semantic to the detected clusters. In this work, the extracted elements are some keywords extracted with word spotting. This approach has been applied to old marriage record field detection on the FamilySearch HIP 2013 competition database. The results demonstrate that we successfully automatically infer rules from non annotated documents using the redundancy <b>of</b> <b>extracted</b> elements <b>of</b> the <b>documents...</b>|$|R
50|$|In 2008, Carole Mallory, {{a former}} mistress, sold seven boxes <b>of</b> <b>documents</b> and {{photographs}} to Harvard University, Norman Mailer's alma mater. They contain <b>extracts</b> <b>of</b> her letters, books and journals.|$|R
40|$|Probabilistic topic {{modeling}} is {{a powerful}} tool to uncover hidden thematic structure <b>of</b> <b>documents.</b> These hidden structures are useful for <b>extracting</b> concepts <b>of</b> <b>documents</b> and other data mining tasks, such as information retrieval. Latent Dirichlet allocation (LDA), is a generative probabilistic topic model for collections of discrete data such as text corpora. LDA represents documents as a bag-of-words, where the important structure <b>of</b> <b>documents</b> is neglected. In this work, we proposed three extended LDA models that incorporates syntactic and semantic structures <b>of</b> text <b>documents</b> into probabilistic topic models. Our first proposed topic model enriches text documents with collapsed typed dependency relations to effectively acquire syntactic and semantic dependencies between consecutive and nonconsecutive words <b>of</b> text <b>documents.</b> This representation has several benefits. It captures relations between consecutive and nonconsecutive words <b>of</b> text <b>documents.</b> In addition, the labels of the collapsed typed dependency relations help to eliminate less important relations, i. e., relations involving prepositions. Moreover, in this thesis, we introduced a method to enforce topic similarity to conceptually similar words. As a result, this algorithm leads to more coherent topic distribution over words. Our second and third proposed generative topic models incorporate term importance into latent topic variables by boosting the probability of important terms and consequently decreasing the probability of less important terms to better reflect the themes <b>of</b> <b>documents.</b> In essence, we assign weights to terms by employing corpus-level and document-level approaches. We incorporate term importance using a nonuniform base measure for an asymmetric prior over topic term distributions in the LDA framework. This leads to better estimates for important terms that occur less frequently in documents. Experimental {{studies have been conducted}} to show the effectiveness of our work across a variety of text mining applications. Furthermore, we employ our topic models to build a personalized content-based news recommender system. Our proposed recommender system eases reading and navigation through online newspapers. In essence, the recommender system acts as filters, delivering only news articles that can be considered relevant to a user. This recommender system has been used by The Globe and Mail, a company that offers most authoritative news in Canada, featuring national and international news...|$|R
40|$|Traditionally, text {{classifiers}} {{are built}} from labeled training examples. Labeling is usually done manually by human experts (or the users), {{which is a}} labor intensive and time consuming process. In the past few years, researchers investigated various forms of semi-supervised learning to reduce the burden of manual labeling. In this paper, we propose a different approach. Instead of labeling a set <b>of</b> <b>documents,</b> the proposed method labels a set of representative words for each class. It then uses these words to <b>extract</b> a set <b>of</b> <b>documents</b> for each class from a set <b>of</b> unlabeled <b>documents</b> to form the initial training set. The EM algorithm is then applied to build the classifier. The key issue of the approach is how to obtain a set of representative words for each class. One way is to ask the user to provide them, which is difficult because the user usually can only give a few words (which are insufficient for accurate learning). We propose a method to solve the problem. It combines clustering and feature selection. The technique can effectively rank {{the words in the}} unlabeled set according to their importance. The user then selects/labels some words from the ranked list for each class. This process requires less effort than providing words with no help or manual labeling <b>of</b> <b>documents.</b> Our results show that the new method is highly effective and promising...|$|R
40|$|Grammars are a {{powerful}} technique for modeling and <b>extracting</b> the structure <b>of</b> <b>documents.</b> One large challenge, however, is computational complexity. The computational cost of grammatical parsing {{is related to}} both {{the complexity of the}} input and the ambiguity of the grammar. For programming languages, where the terminals appear in a linear sequence and the grammar is unambiguous, parsing is O(N). For natural languages, which are linear yet have an ambiguous grammar, parsing is O(N 3). For documents, where the terminals are arranged in two dimensions and the grammar is ambiguous, parsing time can be exponential in the number of terminals. In this paper we introduce (and unify) several types of geometrical data structures which can be used to significantly accelerate parsing time. Each data structure embodies a different geometrical constraint on the set of possible valid parses. These data structures are very general, in that they can be used by any type of grammatical model, and a wide variety <b>of</b> <b>document</b> understanding tasks, to limit the set of hypotheses examined and tested. Assuming a clean design for the parsing software, the same parsing framework can be tested with various geometric constraints to determine the most effective combination...|$|R
