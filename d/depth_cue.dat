237|724|Public
5|$|Thayer's {{original}} argument, restated by Cott, {{was that}} nature did {{the exact opposite}} with countershading that an artist did with paint when creating the illusion of solid three-dimensionality, namely counteracting the effect of shade to flatten out form. Shading is a powerful cue used by animals in different phyla to identify the shapes of objects. Research with chicks showed that they preferred to peck at grains with shadows falling below them (as if illuminated from above), so both humans and birds may make use of shading as a <b>depth</b> <b>cue.</b>|$|E
50|$|Stereopsis {{was first}} {{explained}} by Charles Wheatstone in 1838: “… the mind perceives {{an object of}} three dimensions {{by means of the}} two dissimilar pictures projected by it on the two retinæ …”. He recognized that because each eye views the visual world from slightly different horizontal positions, each eye's image differs from the other. Objects at different distances from the eyes project images in the two eyes that differ in their horizontal positions, giving the <b>depth</b> <b>cue</b> of horizontal disparity, also known as retinal disparity and as binocular disparity. Wheatstone showed that this was an effective <b>depth</b> <b>cue</b> by creating the illusion of depth from flat pictures that differed only in horizontal disparity. To display his pictures separately to the two eyes, Wheatstone invented the stereoscope.|$|E
5000|$|Gender: Women {{are more}} {{susceptible}} than men to virtual reality sickness. This {{may be due to}} hormonal differences, it may be because women have a wider field of view than men, or gender differences in <b>depth</b> <b>cue</b> recognition. [...] Women are most susceptible to virtual reality sickness during ovulation and a wider field of view is also associated with an increase in virtual reality sickness.|$|E
40|$|International audienceThe {{perception}} of depth in images and video sequences {{is based on}} diﬀerent <b>depth</b> <b>cues.</b> Studies have considered depth perception threshold {{as a function of}} viewing distance (Cutting & Vishton, 1995), the combination of diﬀerent monocular <b>depth</b> <b>cues</b> and their quantitative relation with binocular <b>depth</b> <b>cues</b> and their diﬀerent possible type of interactions (Landy, 1995). But these studies only consider artiﬁcial stimuli and none of them attempts to provide a quantitative contribution of monocular and binocular <b>depth</b> <b>cues</b> compared {{to each other in the}} speciﬁc context of natural images. This study targets this particular application case. The evaluation of the strength of diﬀerent <b>depth</b> <b>cues</b> compared to each other using a carefully designed image database to cover as much as possible diﬀerent combinations of monocular (linear perspective, texture gradient, relative size and defocus blur) and binocular <b>depth</b> <b>cues.</b> The 200 images were evaluated in two distinct subjective experiments to evaluate separately perceived depth and diﬀerent monocular <b>depth</b> <b>cues.</b> The methodology and the description of the deﬁnition of the diﬀerent scales will be detailed. The image database is also released for the scientiﬁc community...|$|R
40|$|When full <b>depth</b> <b>cues</b> are available, size judgements are {{dominated}} by physical size. However, with reduced <b>depth</b> <b>cues,</b> size judgements are less influenced by physical size and more influenced by projected size. This study reduces <b>depth</b> <b>cues</b> further than previous size judgement studies, by manipulating monocularly presented pictorial <b>depth</b> <b>cues</b> only. Participants were monocularly presented with two shapes {{against a background of}} zero (control), one, two or three pictorial <b>depth</b> <b>cues.</b> Each cue was added progressively in the following order: height in the visual field, linear perspective, and texture gradient. Participants made a „same-different? judgement regarding the projected size of the two shapes, i. e. ignoring any <b>depth</b> <b>cues.</b> As expected, accuracy increased and response times decreased as the ratio between the projected size of the two shapes increased (range of projected size ratios, 1 : 1 to 1 : 5). In addition, {{with the exception of the}} larger size ratios (1 : 4 and 1 : 5), detection of projected size difference was poorer as <b>depth</b> <b>cues</b> were added. One-cue and two-cue conditions had the most weighting in this performance decrement, with little weighting from the three-cue condition. We conclude that even minimal depth information is difficult to inhibit. This indicates that depth perception requires little focussed attention...|$|R
40|$|The current {{eye-tracking}} study {{explored the}} relative impact of object size and <b>depth</b> <b>cues</b> on 8 -month-old infants' visual attention processes. A series of slides containing 3 objects of either different or same size were displayed on backgrounds with varying <b>depth</b> <b>cues.</b> The distribution of infants' first looks (a measure of initial attention switch) and infants' looking durations (a measure of sustained attention) at the objects were analyzed. Results {{revealed that the}} large objects captured infants' attention first, that is, {{most of the times}} infants directed their visual attention first to the largest object in the scene regardless of <b>depth</b> <b>cues.</b> For sustained attention, infants preferred maintaining their attention to the largest object also, but this occurred only when <b>depth</b> <b>cues</b> were present. These findings suggest that infants' initial attention response is driven mainly by object size, while infants' sustained attention is more the product of combined figure and background processing, where object sizes are perceived as a function of <b>depth</b> <b>cues...</b>|$|R
5000|$|This {{allows the}} {{observer}} {{to view the}} 3D subject from different angles as they move their head, simulating the real-world <b>depth</b> <b>cue</b> of shifting parallax. It also reduces or eliminates the complication of pseudoscopic viewing zones typical of [...] "no glasses" [...] 3D displays that use only two images, {{making it possible for}} several randomly located observers to all see the subject in correct 3D at the same time.|$|E
50|$|In this approach, two cameras with a known {{physical}} relationship (i.e. {{a common}} {{field of view}} the cameras can see, and how far apart their focal points sit in physical space) are correlated via software. By finding mappings of common pixel values, and calculating how far apart these common areas reside in pixel space, a rough depth map can be created. This {{is very similar to}} how the human brain uses stereoscopic information from the eyes to gain <b>depth</b> <b>cue</b> information, i.e. how far apart any given object in the scene is from the viewer.|$|E
50|$|The {{principle}} of integral photography, {{which uses a}} two-dimensional (X-Y) array of many small lenses to capture a 3-D scene, was introduced by Gabriel Lippmann in 1908. Integral photography is capable of creating window-like autostereoscopic displays that reproduce objects and scenes life-size, with full parallax and perspective shift and even the <b>depth</b> <b>cue</b> of accommodation, but the full realization of this potential requires {{a very large number}} of very small high-quality optical systems and very high bandwidth. Only relatively crude photographic and video implementations have yet been produced.|$|E
40|$|This study investigates how {{frames of}} {{reference}} are chosen in a dynamic navigational task. Participants issued verbal instructions to an animated robot and were provided with one of three views for navigating the animated robot around a virtual world. The different views included a flat two-dimensional (2 D) North-up map, a three-dimensional (3 D) robot’s eye view of the world, and a 3 D view from behind the robot (3 D-Camera) in which <b>depth</b> <b>cues</b> were manipulated. Our results show people adopt an egocentric frame of reference when <b>depth</b> <b>cues</b> are salient and an exocentric reference frame when <b>depth</b> <b>cues</b> are absent. The results suggest the absence or presence of <b>depth</b> <b>cues</b> is a critical component in choosing a reference frame. We discuss the extension of Bryant and Tversky’s (1999) theoretical framework to a dynamic environment, such as navigation...|$|R
30|$|Comparing {{the results}} in Figure 8 shows that both {{techniques}} like other motion-based depth estimation approaches fail to estimate the depth maps of static objects (e.g., the table in Figure 8 (b)). According to the human visual system, which integrates different <b>depth</b> <b>cues</b> to perceive the depth, one can expect the integration of <b>depth</b> <b>cues</b> to provide more sufficient means for depth map estimation techniques [5, 6]. Improvement on retrieving depth information of static objects may require integration of our approach with other <b>depth</b> <b>cues,</b> such as sharpness, and it is recommended path for future research.|$|R
40|$|Binocular unmasking {{refers to}} the {{improved}} detection performance of noise-masked simple visual patterns (e. g. grating) in the presence compared {{to the absence of}} 3 -D <b>cues</b> (stereoscopic <b>depth</b> <b>cues).</b> Here we investigated whether binocular unmasking is also important when perceiving noise-masked real-life objects. Specifically, we measured the detection, categorization, and identification thresholds of real-life objects in the presence and absence of stereoscopic <b>depth</b> <b>cues.</b> We found that the detection, categorization, and identification of objects was significantly better in the presence than in the absence of stereoscopic <b>depth</b> <b>cues</b> using hypothesis tests. Hence binocular unmasking seems important for object perception...|$|R
50|$|Thayer's {{original}} argument, restated by Cott, {{was that}} nature did {{the exact opposite}} with countershading that an artist did with paint when creating the illusion of solid three-dimensionality, namely counteracting the effect of shade to flatten out form. Shading is a powerful cue used by animals in different phyla to identify the shapes of objects. Research with chicks showed that they preferred to peck at grains with shadows falling below them (as if illuminated from above), so both humans and birds may make use of shading as a <b>depth</b> <b>cue.</b>|$|E
50|$|Convergence {{micropsia}} {{implies that}} the state of convergence of the eyes contributes to determining the distance of an object from the eyes, that it acts as a <b>depth</b> <b>cue.</b> At six meters, to view an object without double vision the optic axes of the eyes are essentially parallel (no convergence). At closer distances, to view an object without double vision the optic axes need to approach each other at an angle (increasing convergence). Normally, the convergence angle gives accurate information about the distance of objects. Under the conditions that yield convergence micropsia, the overconverged vergence angle specifies a shorter distance than the actual distance of the object; this affects apparent size.|$|E
5000|$|... 2D-to-3D {{conversion}} {{adds the}} binocular disparity <b>depth</b> <b>cue</b> to digital images {{perceived by the}} brain, thus, if done properly, greatly improving the immersive effect while viewing stereo video in comparison to 2D video. However, {{in order to be}} successful, the conversion should be done with sufficient accuracy and correctness: the quality of the original 2D images should not deteriorate, and the introduced disparity cue should not contradict to other cues used by the brain for depth perception. If done properly and thoroughly, the conversion produces stereo video of similar quality to [...] "native" [...] stereo video which is shot in stereo and accurately adjusted and aligned in post-production.|$|E
40|$|In two {{experiments}} with 162 participants, {{we examined the}} role of stereoscopic <b>depth</b> <b>cues</b> in the memory for objects and scenes. Stereoscopic <b>depth</b> <b>cues</b> are formed by disparities in the locations of objects in the images formed on the two eyes, and are naturally used by most observers to recover the distances to objects in the world. Using naturalistic pictures, we determined that stereo <b>depth</b> <b>cues</b> do persist in memory, {{as long as the}} observer is given several seconds to view the image and extract the depth relations from the scene. However, we did not find direct evidence that stereoscopic <b>depth</b> <b>cues</b> increase the rate of information extraction from scenes. One implication of the present work is that objects that will be recognized in the real world should be initially presented using stereoscopic <b>depth</b> <b>cues.</b> STEREO PROJECT REPORT PAGE 2 The world around us has three dimensions, although we often represent scenes using flat, 2 D representations such as photographs or movies. However, by virtue of the geometry of space and the separation of our two eyes by several inches, we perceive two slightl...|$|R
5000|$|... #Caption: The Necker cube: a {{wire frame}} cube with no <b>depth</b> <b>cues.</b>|$|R
30|$|With {{the need}} for <b>depth</b> <b>cues,</b> shape understanding, and {{avoidance}} of projective ambiguity, coordinating the locations of mechanical, electrical, and plumbing pipes is a demanding task. On one hand, <b>depth</b> <b>cues</b> and shape understanding require a 3 D display, while a standard 3 D display presents issues of projective ambiguity. The issue is averted in a physical model where subjects benefit from <b>depth</b> <b>cues</b> and shape understanding of a 3 D display and avoiding projective ambiguity from a true three dimensional, haptic output. However, 77 % of practitioners preferred a 3 D computer model, while 15 % and 8 % chose 2 D drawings and a physical model respectively.|$|R
50|$|The peering {{behavior}} {{that can be}} observed in M. religiosa is believed to be essential for the measurement of distances and depth perception: a site-to-site pendulum like movement of the head or the whole body in a horizontal plane is used to scan the environment. Motion parallax is a <b>depth</b> <b>cue</b> that describes the fact that the closer and object is, the faster it seems to move when turning your head while looking at it compared to slower moving objects which are more distant. Animals that were blinded on one eye did not strike for prey proving that binocular vision is essential since the disparity between the information of each eye is used to estimate distances as well.|$|E
5000|$|The {{film was}} shot entirely on location, in Denali National Park and {{in and around}} Talkeetna, Alaska. Due to extreme {{crevasse}} danger on the glacier the climbers/guides were always roped together in teams {{of three or four}} during travel. Higher up the mountain, the steepness and sheer exposure also required roping-up of not only the climbers to each other but also to the mountain. That required snow protection in the form of snow pickets such that the ropes, and hence climbers, were tied to the mountain to prevent catastrophe should a climber fall. This aspect, though adding to filming difficulty, also gave the production an [...] "up close and personal" [...] feel. Some of the scenes showed the ropes, adding an interesting additional 3D <b>depth</b> <b>cue</b> ...|$|E
50|$|There {{are several}} depth cues {{contained}} in the observed stripe patterns. The displacement of any single stripe can directly be converted into 3D coordinates. For this purpose, the individual stripe has to be identified, which can for example be accomplished by tracing or counting stripes (pattern recognition method). Another common method projects alternating stripe patterns, resulting in binary Gray code sequences identifying the number of each individual stripe hitting the object.An important <b>depth</b> <b>cue</b> also results from the varying stripe widths along the object surface. Stripe width {{is a function of}} the steepness of a surface part, i.e. the first derivative of the elevation. Stripe frequency and phase deliver similar cues and can be analyzed by a Fourier transform. Finally, the wavelet transform has recently been discussed for the same purpose.|$|E
40|$|The aim of {{this work}} is to {{identify}} the <b>depth</b> <b>cues</b> that provide intuitive depth-ordering when used to visualize abstract data. In particular {{we focus on the}} <b>depth</b> <b>cues</b> that are effective on a high-dynamic-range (HDR) display: contrast and brightness. In an experiment participants were shown a visualization of the volume layers at different depths with a single isolated monocular cue as the only indication of depth. The observers were asked to identify which slice of the volume appears to be closer. The results show that brightness, contrast and relative size are the most effective monocular <b>depth</b> <b>cues</b> for providing an intuitive depth ordering...|$|R
5000|$|Features: Texture mapping, Gouraud shading, {{transparency}} effects, <b>depth</b> <b>cueing,</b> 16.7 million colors, 240,000 polygons/second ...|$|R
40|$|In autism {{spectrum}} disorder (ASD), atypical {{integration of}} visual <b>depth</b> <b>cues</b> {{may be due}} to flattened perceptual priors or selective fusion. The current study attempts to disentangle these explanations by psychophysically assessing within-modality integration of ordinal (occlusion) and metric (disparity) <b>depth</b> <b>cues</b> while accounting for sensitivity to stereoscopic information. Participants included 22 individuals with ASD and 23 typically developing matched controls. Although adults with ASD were found to have significantly poorer stereoacuity, they were still able to automatically integrate conflicting <b>depth</b> <b>cues,</b> lending support to the idea that priors are intact in ASD. However, dissimilarities in response speed variability between the ASD and TD groups suggests that there may be differences in the perceptual decision-making aspect of the task...|$|R
5000|$|If a {{stationary}} three-dimensional figure (for example, a wireform cube) is illuminated from behind {{so that its}} shadow falls on a translucent screen, an observer {{in front of the}} screen will see a two-dimensional pattern of lines. But if the same object is rotated, the observer will (accurately) see it as a turning three-dimensional cube, even though only two-dimensional information is presented. This is the kinetic depth effect (KDE), a potent <b>depth</b> <b>cue.</b> It occurs spontaneously, it can be seen with monocular vision, it occurs with solid figures as well as wireforms, and the figures need not be regular geometric objects nor need they have familiar shapes. Wallach and O’Connell found only two essential conditions for obtaining the effect. The object must be composed of straight lines with definite endpoints or corners, and the projected shadows of those lines must change in both length and orientation as the object rotates (otherwise a flat, deforming figure is seen.) ...|$|E
40|$|AbstractVarious {{visual cues}} provide {{information}} about depth and shape in a scene. When several of these cues are simultaneously available in a single location in the scene, the visual system attempts to combine them. In this paper, we discuss three key issues relevant to the experimental analysis of <b>depth</b> <b>cue</b> combination in human vision: cue promotion, dynamic weighting of cues, and robustness of cue combination. We review recent psychophysical studies of human <b>depth</b> <b>cue</b> combination {{in light of these}} issues. We organize the discussion and review as the development of a model of the <b>depth</b> <b>cue</b> combination process termed modified weak fusion (MWF). We relate the MWF framework to Bayesian theories of cue combination. We argue that the MWF model is consistent with previous experimental results and is a parsimonious summary of these results. While the MWF model is motivated by normative considerations, it is primarily intended to guide experimental analysis of <b>depth</b> <b>cue</b> combination in human vision. We describe experimental methods, analogous to perturbation analysis, that permit us to analyze <b>depth</b> <b>cue</b> combination in novel ways. In particular these methods allow us to investigate the key issues we have raised. We summarize recent experimental tests of the MWF framework that use these methods...|$|E
40|$|Various {{visual cues}} provide {{information}} about depth and shape in a scene. When several of these cues are simultaneously available in a single location in the scene, the visual system attempts to combine them. In this paper, we discuss three key issues relevant to the experimental analysis of <b>depth</b> <b>cue</b> combination in human vision: cue promotion, dynamic weighting of cues, and robustness of cue combination. We review recent psychophysical studies of human <b>depth</b> <b>cue</b> combination {{in light of these}} issues. We organize the discussion and review as the development of a model of the <b>depth</b> <b>cue</b> combination process termed modified weak fusion (MWF). We relate the MWF framework to Bayesian theories of cue combination. We argue that the MWF model is consistent with previous experimental results and is a parsimonious summary of these results. While the MWF model is motivated by normative considerations, it is primarily intended to guide experimental analysis of <b>depth</b> <b>cue</b> combination in human vision. We describe experimental methods, analogous to perturbation analysis, that permit us to analyze <b>depth</b> <b>cue</b> combination in novel ways. In particular these methods allow us to investigate the key issues we have raised. We summarize recent experimental tests of the MWF framework that use these methods. Depth Multiple cues Sensor fusio...|$|E
40|$|In recent years, 3 D {{graphics}} {{has become}} more available for web development with low-level access to graphics hardware and increased power of web browsers. With core browsing tasks for users being to quickly scan a website and find what they are looking for, can 3 D graphics – or <b>depth</b> <b>cues</b> – be used to facilitate these tasks? Therefore, {{the main focus of}} this work was to examine user performance on websites in terms of visual attention. Previous research on the use of 3 D graphics in web design and other graphical interfaces has yielded mixed results, but some suggest <b>depth</b> <b>cues</b> might be used to segment a visual scene and improve visual attention. In this work, the main question asked was:  How do <b>depth</b> <b>cues</b> affect visual search in a web-based environment? To examine the question, a user study was conducted where participants performed a visual search task on four different web-based prototypes with varying <b>depth</b> <b>cues.</b> The findings suggest <b>depth</b> <b>cues</b> might have a negative effect by increasing reaction time, but certain cues can improve task completion (hit rate) in text-rich web environments. It is further elaborated that it might be useful to look at the problem from a more holistic perspective, also emphasizing other factors such as visual complexity and prototypicality of websites...|$|R
40|$|Abstract— 2 D-to- 3 D {{conversion}} is {{an important}} step for obtaining 3 D videos, where a variety of monocular <b>depth</b> <b>cues</b> have been explored to generate 3 D videos from 2 D videos. As in a human brain, a fusion of these monocular <b>depth</b> <b>cues</b> can re-generate 3 D data from 2 D data. By mimicking how our brains generate depth perception, we propose a reliability-based fusion of multiple <b>depth</b> <b>cues</b> for an automatic 2 D-to- 3 D video conversion. A series of comparisons between the proposed framework and the previous methods is also presented. It shows that significant improvement is achieved in both subjective and objective experimental results. From the subjective viewpoint, the brain-inspired framework outperforms earlier conversion methods by preserving more reliable <b>depth</b> <b>cues.</b> Moreover, an enhancement of 0. 70 - 3. 14 dB and 0. 0059 - 0. 1517 in the perceptual quality of the videos is realized in terms of the objective-modified peak signal-to-noise ratio and disparity distortion model, respectively. Index Terms— 2 D-to- 3 D conversion, brain-inspired fusion...|$|R
40|$|International audienceThis article {{describes}} a general depth indicator for stereoscopic 3 D video sequences. This indicator targets {{the characterization of}} the depth in 3 D video sequences based on both monocular and binocular <b>depth</b> <b>cues.</b> Evaluating monocular <b>depth</b> <b>cues</b> is no easy task. Due to this high complexity, only {{a subset of the}} monocular <b>depth</b> <b>cues</b> are currently considered since it is believed that they play a major impact in depth perception. The proposed algorithm will consider the following different depth cues: binocular depth, linear perspective, blur from defocus, motion parallax and texture gradient. It will be detailed how all these all metrics have been designed. The second main contribution is the definition of a specific application scope of each metrics. This is motivated by the need {{to take into account the}} reliability of each individual metrics during the pooling. Therefore, it will be considered in the paper to identify the cases where each individual metric may fail and integrate these aspects in the general combination of the <b>depth</b> <b>cues...</b>|$|R
40|$|This study {{investigated}} the effect of different monocular depth cues on the understanding of a tactical situation in 3 D visual displays. The <b>depth</b> <b>cue</b> conditions were target symbol, target symbol with drop-shadow, and target symbol with drop-line. The tactical understanding {{was defined as the}} accuracy in perception of 3 D bearing (measured through Azimuth and Elevation) between own-ship and target symbols. Perhaps the most important result from these experiments with respect to design issues is that the differences between the <b>depth</b> <b>cue</b> conditions are minimal. That means that the need for drop-shadows or drop-lines is limited, which is good from a cluttering perspective...|$|E
40|$|The {{motivation}} {{of this research}} is to enhance the user experience by offering better depth interpretations in augmented reality systems. The thesis focuses on how to use blur effect to enhance the depth interpretations since the blur effect has been proved to be an independent <b>depth</b> <b>cue</b> to human visual system and the proper blur in the virtual scene could increase the consistency in augmented reality. In this thesis, the perceptual problems in video see-through augmented reality applications such as how to generate a consistent perception between virtual and real parts, how the blur effect act as a <b>depth</b> <b>cue</b> are addressed. The whole thesis is structured to solve three main problems: 1. how to determine the degree of blur which should be rendered on the virtual objects; 2. how to render the certain degree of blur onto the virtual objects; 3. how the users would perceive the blurred virtual contents as a <b>depth</b> <b>cue.</b> // To solve the first problem, a new <b>depth</b> <b>cue</b> method based on blur effect is proposed. Different from the previous approaches, the proposed method offers an algorithm which estimates the blur effect in the whole scene based on the spatial information in the real world and the intrinsic parameters of the camera. To solve the second problem, a prototype AR system was designed and implemented. The prototype system realized the proposed <b>depth</b> <b>cue</b> method and the system design and implementation details are introduced in this thesis. The algorithm of the blur shader is discussed. // Three user tests were conducted to solve the problem how the users would perceive the blur effect rendered by different blurring methods. The user tests results are discussed in this thesis. It could be confirmed from the user test results that the proposed <b>depth</b> <b>cue</b> method could help the user to gain a better depth interpretation in AR system. In the user tests, a comparison between the proposed method (which estimates the blur effect in the whole scene based on the measured blur radius of some known position) and previous method (which only renders blur effect on the position whose blur radius is measured) is addressed. However, the test results could not confirm which method is better. // In the future work, three aspects are considered: 1. for the proposed <b>depth</b> <b>cue</b> method, besides the blur effect caused by defocusing of the camera lens, the motion blur should also be investigated; 2. for the prototype system implementation, a moving camera should be introduced and the real-time detecting and rendering should also be discussed. The camera view of the prototype system will no longer be limited to a desk but expanded to various situations; 3. for the user tests, more researches should be addressed to investigate on how the blur effect would match depth interpretation in quantity. 報告番号:; 学位授与日: 2013 - 09 - 27; 学位の種別: 修士; 学位の種類: 修士(工学); 学位記番号:; 研究科・専攻: 工学系研究科・電気系工学専...|$|E
40|$|Enabling {{users to}} {{accurately}} perceive the correct depth of occluded objects {{is one of}} the major challenges in user interfaces for Mixed Reality (MR). Therefore, several visualization techniques and user evaluations for this area have been published. Our research is focused on photorealistic X-ray type visualizations in outdoor environments. In this paper, we present an evaluation of depth perception in far-field distances through two photorealistic visualizations of occluded objects (X-ray and Melt) in the presence and absence of a <b>depth</b> <b>cue.</b> Our results show that the distance to occluded objects was underestimated in all tested conditions. This finding is curious, as it contradicts previously published results of other researchers. The Melt visualization coupled with a <b>depth</b> <b>cue</b> was the most accurate among all the experimental conditions. ...|$|E
40|$|Abstract. The common {{approach}} {{to estimate the}} distance of an object in computer vision and robotics is to use stereo vision. Stereopsis, how-ever, provides good estimates only within near space and thus is more suitable for reaching actions. In order to successfully plan and execute an action in far space, other <b>depth</b> <b>cues</b> {{must be taken into}} account. Self-body movements, such as head and eye movements or locomotion can provide rich information of depth. This paper proposes a model for inte-gration of static and self-motion-based <b>depth</b> <b>cues</b> for a humanoid robot. Our results show that self-motion-based visual cues improve the accu-racy of distance perception and combined with other <b>depth</b> <b>cues</b> provide the robot with a robust distance estimator suitable for both reaching and walking actions...|$|R
40|$|Light-field cameras have {{recently}} become {{available to the}} consumer market. An array of micro-lenses captures enough information that one can refocus images after acquisition, as well as shift one’s viewpoint within the subapertures of the main lens, effectively obtaining multiple views. Thus, <b>depth</b> <b>cues</b> from both defocus and correspondence are available simultaneously in a single capture. Previously, defocus could be achieved only through multiple image exposures focused at different <b>depths,</b> while correspondence <b>cues</b> needed multiple exposures at different viewpoints or multiple cameras; moreover, both cues could not easily be obtained together. In this paper, we present a novel simple and principled algorithm that computes dense depth estimation by combining both defocus and correspondence <b>depth</b> <b>cues.</b> We analyze the x-u 2 D epipolar image (EPI), where by convention we assume the spatial x coordinate is horizontal and the angular u coordinate is vertical (our final algorithm uses the full 4 D EPI). We show that defocus <b>depth</b> <b>cues</b> are obtained by computing the horizontal (spatial) variance after vertical (angular) integration, and correspondence <b>depth</b> <b>cues</b> by computing the vertical (angular) variance. We then show how to combine the two cues into a high quality depth map, suitable for computer vision applications such as matting, full control of depth-of-field, and surface reconstruction. 1...|$|R
40|$|Visual <b>depth</b> <b>cues</b> are {{combined}} {{to produce the}} essential depth and dimensionality of Desktop Virtual Environments (DVEs). This study discusses DVEs {{in terms of the}} visual <b>depth</b> <b>cues</b> that create and support perception of frames of references and accomplishment of visual search tasks. This paper presents the results of an investigation that identifies the effects of the experimental stimuli positions and visual depth cues: luminance, texture, relative height and motion parallax on precise depth judgements made within a DVE. Results indicate that the experimental stimuli positions significantly affect precise depth judgements, texture is only significantly effective for certain conditions, and motion parallax, in line with previous results, is inconclusive to determine depth judgement accuracy for egocentrically viewed DVEs. Results also show that exocentric views, incorporating relative height and motion parallax visual cues, are effective for precise depth judgements made in DVEs. The results help us to understand the effects of certain visual <b>depth</b> <b>cues</b> to support the perception of frames of references and precise depth judgements, suggesting that the visual <b>depth</b> <b>cues</b> employed to create frames of references in DVEs may influence how effectively precise depth judgements are undertaken...|$|R
