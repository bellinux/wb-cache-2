358|308|Public
5000|$|One {{concern is}} the {{provisions}} regarding retention of business records. David Cay Johnston said, [...] "It is ... {{hard to make the}} case that the cost of keeping a <b>duplicate</b> <b>record</b> at the home office in a different country is a burden." [...] He noted that business records requirements are sufficiently important that they were codified in law even before the Code of Hammurabi.|$|E
40|$|Abstract: <b>Duplicate</b> <b>record</b> {{detection}} {{is a key}} step in Deep Web data integration, but {{the existing}} approaches do not adapt to its large-scale nature. In this paper, a three-step automatic approach is proposed for <b>duplicate</b> <b>record</b> detection in Deep Web. It firstly uses cluster ensemble to select initial training instance. Then it utilizes tri-training classification to con-struct classification model. Finally, it uses evidence theory to combine the results of multiple classification models to con-struct the domain-level <b>duplicate</b> <b>record</b> detection model {{which can be used}} for large-scale <b>duplicate</b> <b>record</b> detection in the same domain. Experimental results show that the proposed approach is better than previous work and and the domain-level <b>duplicate</b> <b>record</b> detection model can get high performance...|$|E
40|$|In {{real time}} applications, {{identification}} of records {{that represent the}} same real-world entity is a major challenge to be solved. Such records are termed to be duplicate records. This study presented a thorough analysis {{of the literature on}} <b>duplicate</b> <b>record</b> detection. The <b>duplicate</b> <b>record</b> detection is an important step for data integration. An overview of data deduplication issue is discussed in detail. This paper covered almost all the metrics that are commonly used to detect similar entries and a set of duplicate detection algorithms...|$|E
40|$|As {{research}} {{becomes more}} and more interdisciplinary, literature search from CD-ROM databases is often carried out on more than one CD-ROM database. This results in retrieving <b>duplicate</b> <b>records</b> due to same literature being covered (indexed) in more than one database. The retrieval software does not identify such <b>duplicate</b> <b>records.</b> Three different programs have been written to accomplish the task of identifying the <b>duplicate</b> <b>records.</b> These programs are executed from a shell script to minimize manual intervention. The various fields that have been used (extracted) to identify the <b>duplicate</b> <b>records</b> include the article title, year, volume number, issue number and pagination. The shell script when executed prompts for input file that may contain <b>duplicate</b> <b>records.</b> The programs identify the <b>duplicate</b> <b>records</b> and write them to a new file...|$|R
40|$|The {{importance}} of supporting keyword searches on relations {{has been widely}} recognized. Different from the existing keyword search techniques on relations, this paper focuses on nearly <b>duplicate</b> <b>records</b> in relational databases due to abbreviation and typos. As a result, processing keyword searches with <b>duplicate</b> <b>records</b> involves many unique challenges. In this paper we discuss the motivation and present a system, RSEARCH, to show challenges in supporting keyword search using nearly <b>duplicate</b> <b>records</b> and key techniques including identifying nearly <b>duplicate</b> <b>records</b> and generating results efficiently. ...|$|R
40|$|Recent studies {{documented}} that survey data contain <b>duplicate</b> <b>records.</b> We assess how <b>duplicate</b> <b>records</b> affect regression estimates, {{and we evaluate}} the effectiveness of solutions to deal with <b>duplicate</b> <b>records.</b> Results show that the chances of obtaining unbiased estimates when data contain 40 doublets (about 5 % of the sample) range between 3. 5 % and 11. 5 % depending on the distribution of duplicates. If 7 quintuplets are present in the data (2 % of the sample), then the probability of obtaining biased estimates ranges between 11 % and 20 %. Weighting the <b>duplicate</b> <b>records</b> by the inverse of their multiplicity, or dropping superfluous duplicates outperform other solutions in all considered scenarios. Our results illustrate the risk of using data in presence of <b>duplicate</b> <b>records</b> and call for further research on strategies to analyze affected data. " (author's abstract...|$|R
40|$|Though {{data quality}} issues arise with ever-zooming {{quantity}} of data, it {{is a welcome}} sign that of late, significant improvement {{has been made in}} data engineering. Consequently, there have been significant investments from private and government organizations in developing methods for removing replicas from the data repositories. This phenomenon has caused a significant interest among researchers in developing efficient and effective duplicate detection strategy using modern and emerging techniques. In this paper, we have proposed accordingly. In the previous work <b>duplicate</b> <b>record</b> detection was done using Q-gram concept and the fuzzy classifier. Here, different set of features from the data is found out using the Q-gram concept that leads to computational complex environment. In order to reduce the computational task, a set of important Q-gram-based feature subsets is selected. With this intention, the overall steps of the proposed technique are carried out using three different steps, such as, 1) feature computation, 2) feature selection, and 3) detection. Initially, the features are computed using Q-gram concept and then, the subset of optimal feature sets is identified using particle swarm algorithm (PSO) {{which is one of the}} most effective optimization algorithms. Once we select the optimal features sets, the Naïve Bayes Classifier is utilized to detect the duplication records. There are two processes which characterize the proposed <b>Duplicate</b> <b>Record</b> Detection technique such as the training phase and the testing phase. The experimental results showed that the proposed <b>Duplicate</b> <b>Record</b> Detection technique has higher accuracy than that of the existing method. The accuracy obtained for the proposed <b>Duplicate</b> <b>Record</b> Detection is found to be 89 %...|$|E
40|$|In this paper, we {{developed}} a robust data cleaning technique, called PC-Filter+ (PC stands for partition comparison) based on its predecessor, for effective and efficient <b>duplicate</b> <b>record</b> detection in large databases. PC-Filter+ provides more flexible algorithmic options for constructing the Partition Comparison Graph (PCG). In addition, PC-Filter+ is {{able to deal with}} duplicate detection under different memory constraints...|$|E
40|$|Abstract — The {{problem of}} {{identifying}} approximately <b>duplicate</b> <b>record</b> in database {{is an essential}} step for data cleaning & data integration process. A dynamic web page is displayed to show the results {{as well as other}} relevant advertisements that seem relevant to the query. The real world entities have two or more representation in databases. When dealing with large amount of data it is important that there be a well defined and tested mechanism to filter out duplicate result. This keeps the result relevant to the queries. <b>Duplicate</b> <b>record</b> exists in the query result of many web databases especially when the duplicates are defined based on only some of the fields in a record. Using exact matching technique Records that are exactly same can be detected. The system that helps user to integrate and compares the query results returned from multiple web databases matches the different sources records that referred to the same real world entity. In this paper, we analyze the literature on <b>duplicate</b> <b>record</b> detection. We cover similarity metrics which are commonly used to detect similar field entries, and present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database also the techniques for improving the efficiency and scalability of approximate duplicate detection algorithms are covered. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area...|$|E
40|$|Deduplication {{is the key}} {{operation}} in dataintegration from multiple data sources. To achieve higherquality information and more simplified data representation,data preprocessing is required. Data cleaning is one among thedata preprocessing steps. Data cleaning includes the process ofparsing, data transformation, duplicate elimination andstatistical methods. If two records represent the same realworld entity then it is called <b>duplicated</b> <b>records.</b> The problem ofdetecting and eliminating <b>duplicate</b> <b>records</b> is called recorddeduplication. This paper presents an analysis of recorddeduplication techniques and algorithms that detect andremove the <b>duplicate</b> <b>records...</b>|$|R
40|$|In recent years, the Web of Science Core Collection and Scopus {{databases}} {{have become}} primary sources for conducting studies that evaluate scientific investigations. Such studies require that <b>duplicate</b> <b>records</b> be excluded to avoid errors of overrepresentation. In this line, we identify <b>duplicate</b> <b>records</b> in Scopus and examine their origins. Identifying journals with <b>duplicate</b> <b>records</b> in Scopus, selecting and downloading bibliographic journal records, and identifying and analyzing the <b>duplicate</b> <b>records</b> is the methodology adopted. <b>Duplicate</b> <b>records</b> are found when {{articles published in}} a journal are incorrectly mapped by Scopus to this journal and to a different journal from the same publisher and when there are journal title changes, orthographic differences in the presentation of a journal name, and journal name variants. In these last three cases, one bibliographic <b>record</b> of each <b>duplicate</b> is mapped to Medline coverage of Scopus. Consequently, the identified duplicates and the {{significant differences in the}} number of citations received in duplicate articles may influence bibliometric studies. Thus, {{there is a need for}} rigorous quality control guidelines to govern database managers and editors to prevent the creation of duplicates. Peer Reviewe...|$|R
40|$|Detecting {{database}} records that are approximate duplicates, but not exact duplicates, {{is an important}} task. Databases may contain <b>duplicate</b> <b>records</b> concerning the same realworld entity because of data entry errors, because of unstandardized abbreviations, or because of differences in the detailed schemas of records from multiple databases, among other reasons. In this paper, we present an efficient algorithm for recognizing clusters of approximately <b>duplicate</b> <b>records.</b> Three key ideas distinguish the algorithm presented. First, {{a version of the}} Smith-Waterman algorithm for computing minimum edit-distance is used as a domainindependent method to recognize pairs of approximately <b>duplicate</b> <b>records.</b> Second, the union/find algorithm is used to keep track of clusters of <b>duplicate</b> <b>records</b> incrementally, as pairwise duplicate relationships are discovered. Third, the algorithm uses a priority queue of cluster subsets to respond adaptively to the size and homogeneity of the clusters discovered as [...] ...|$|R
40|$|Merged with <b>duplicate</b> <b>record</b> 10026. 1 / 2138 on 03. 04. 2017 by CS (TIS) A high {{performance}} liquid chromatography-inductively coupled plasma-mass spectrometry (HPLC-ICP-MS) method {{has been developed}} for the separation and quantification of g kg- 1 levels of arsenobetaine, monomethylarsonic acid (MMAA), dimethylarsinic acid (DMAA), arsenite and arsenate. Using this coupling, arsenic species in fruit and vegetables grown on soils containing up to 1. 4...|$|E
40|$|Merged with <b>duplicate</b> <b>record</b> 10026. 1 / 739 on 27. 02. 2017 by CS (TIS) This study {{investigates the}} effect of the late Cenomanian Oceanic Anoxic Event (OAE) on the planktonic and benthonic foraminifera. On the former, the OAE was the cause of major extinctions within the population, the return to pre-OAE oxygen levels {{permitting}} recolonization of the vacated niches. On the latter, the OAE caused extinctions but resulted in a low oxygen tolerant fauna which slowly evolved into the vacated niches on the post-OAE recovery of oxygen levels. The changes in the foraminiferal populations have been integrated with changes in other marine organisms through the late Cenomanian...|$|E
40|$|High-density {{digital data}} storage system {{designed}} for cost-effective storage of {{large amounts of}} information acquired during experiments. System accepts up to 20 channels of 16 -bit digital data with overall transfer rates of 500 kilobytes per second. Data recorded on 8 -millimeter magnetic tape in cartridges, each capable of holding up to five gigabytes of data. Each cartridge mounted on one of two tape drives. Operator chooses to use either or both of drives. One drive used for primary storage of data while other {{can be used to}} make a <b>duplicate</b> <b>record</b> of data. Alternatively, other drive serves as backup data-storage drive when primary one fails...|$|E
40|$|Abstract. To {{solve the}} problem of {{attribute}} weight determination in the approximately <b>duplicate</b> <b>records,</b> we put forward a method based on fuzzy comprehensive evaluation to get attribute weight in data set. We first perform an analysis of the composition factors of attribute. Then we carry out an evaluation of their rank. Finally, we make a determination of the attribute weight using the fuzzy comprehensive evaluation method, on the basis of which the approximately <b>duplicate</b> <b>records</b> are detected. Theoretical analysis and experimental results show that the method can objectively determine all attributes weight, and effectively detect the approximately <b>duplicate</b> <b>records</b> in massive data set...|$|R
5000|$|Virus - The Agent That Shapes The Desert (<b>Duplicate</b> <b>Records,</b> 2010) ...|$|R
5000|$|... #Subtitle level 3: Incentives to make <b>duplicate</b> <b>recorded</b> {{versions}} of a song ...|$|R
40|$|Duplicate records do {{not share}} a common key and/or they contain errors that make {{duplicate}} matching a difficult task. Errors are introduced {{as the result of}} transcription errors, incomplete information, lack of standard formats, or any combination of these factors. In this paper, we present a thorough analysis of the literature on <b>duplicate</b> <b>record</b> detection. We cover similarity metrics that are commonly used to detect similar field entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the efficiency and scalability of approximate duplicate detection algorithms. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area...|$|E
40|$|Merged with <b>duplicate</b> <b>record</b> 10026. 1 / 649 on 20. 12. 2016 by CS (TIS). Merged with <b>duplicate</b> <b>record</b> 10026. 1 / 2083 on 07. 02. 2017 by CS (TIS) This is a {{digitised}} {{version of}} a thesis that was deposited in the University Library. If you are the author please contact PEARL Admin (pearladmin@plymouth. ac. uk) to discuss options. Remote sensing is an efficient tool to monitor the aquatic ecology. The optical signature in coastal marine environment {{is a reflection of}} the complex distribution of optically active marine components. It is essential to understand the relationship between the remote sensing signal and marine constituent material to take advantage of high resolution remote sensing data available from spaceborne and airborne platforms. The objective of this research was to develop a semi-analytical forward model to predict the remote sensing optical signature in coastal waters dominated by non-planktonic material. Laboratory and in situ measurements collected over a 5 year period (1998 - 2003) were used to compile a biogeooptical database for coastal waters. The database is exploited to realise various biogeophysical relationships. A major advancement proposed in the thesis towards the modelling of backscattering probability was the synthesis of knowledge from Mie theory and particulate composition from geochemical analysis. This approach was used to derive particulate backscattering from in situ absorption and attenuation measurements. Results show that this model can produce backscattering values in a realistic way than with a constant value as proposed by Petzold. Absorption and backscattering values derived from ac- 9 measurements were used to calculate radiance reflectance and remote sensing reflectance. The biogeophysical relationships developed were incorporated into the forward optics model to successfully simulate the inherent optical property ratio. Further development of the model and applications through inversion were discussed and outlined. Plymouth Marine Laborator...|$|E
40|$|Abstract—To {{improve the}} {{software}} quality {{the number of}} errors from the software must be removed. The research paper presents a study towards machine learning and software quality prediction as an expert system. The {{purpose of this paper}} is to apply the machine learning approaches, such as case-based reasoning, to predict software quality. The main objective of this research is to minimize software costs. Predict the error in software module correctly and use the results in future estimation. The novel idea behind this system is that Knowledge base (KBS) building is an important task in CBR and the knowledge base can be built based on world new problems along with world new solutions. Second, reducing the maintenance cost by removing the <b>duplicate</b> <b>record</b> set from the KBS. Third, error prediction with the help o...|$|E
40|$|Studies {{using the}} newly created Ontario Cancer Registry (OCR) {{identified}} a number of <b>duplicate</b> <b>records.</b> These could be traced to incorrect spelling of surnames which did not permit incoming records to be correctly compared during the routine computerized record linkage process. The method of elimination of the majority of these <b>duplicate</b> <b>records</b> and the results will be presented...|$|R
5000|$|Zhenya ( [...] P-Town Remix) ( [...] Compilation {{contribution}} to Kaffee Burger [...] (<b>Duplicate</b> <b>Records)</b> 2006) ...|$|R
5000|$|Überthrash II - 7" [...] Split, <b>Duplicate</b> <b>Records</b> (Split w. Audiopain / Aura Noir / Infernö) (2005) ...|$|R
40|$|Abstract: <b>Duplicate</b> <b>record</b> {{detection}} {{is important}} for data preprocessing and cleaning. Artificial Bee Colony (ABC) {{is one of the}} most recently introduced algorithms based on the intelligent foraging behavior of a honey bee swarm. Our approach to duplicate detection is the use of ABC algorithm for generating the optimal similarity measure to decide whether the data is duplicate or not. In the training phase, ABC algorithm is used to generate the optimal similarity measure. Once the optimal similarity measure obtained, the deduplication of remaining datasets is done with the help of optimal similarity measure generated from the ABC algorithm. We have used Restaurant and Cora datasets to analyze the proposed algorithm and the performance of the proposed algorithm is compared against the genetic programming technique with the help of evaluation metrics...|$|E
40|$|Often, in {{the real}} world, {{entities}} have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a dif cult task. Errors are introduced {{as the result of}} transcription errors, incomplete information, lack of standard formats or any combination of these factors. In this article, we present a thorough analysis of the literature on <b>duplicate</b> <b>record</b> detection. We cover similarity metrics that are commonly used to detect similar eld entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the ef ciency and scalability of approximate duplicate detection algorithms. We conclude with a coverage of existing tools and with a brief discussion of the big open problems in the area...|$|E
40|$|Merged with <b>duplicate</b> <b>record</b> 10026. 1 / 2688 on 28. 02. 2017 by CS (TIS) The {{relevance}} of ship based routeing is discussed. Data collected at sea are analysed to produce vessel response characteristics Meteorological data are analysed {{in a conventional}} manner to establish effective steering criteria with respect to 500 mb flow. For {{the first time a}} routeing model is formulated which recognises the three spatial dimensions of a middle latitude storm. A theoretical analysis of relative flow in a growing baroclinic wave is undertaken. Reference to displacement of the wave trough affords a measure of both storm development and steering effectiveness. Short, medium and long term planning elements are combined in a model. The effectiveness of this approach is demonstrated by actually "weather routeing" a vessel, whilst comparing progress of a sister ship navigated conventionally. Sources of error and limitations of the model are discussed...|$|E
5000|$|Überthrash - 7" [...] Split, <b>Duplicate</b> <b>Records</b> (Split w. Audiopain / Aura Noir / Infernö, Ltd. 500) (2004) ...|$|R
40|$|There is an {{increasing}} demand for systems that can provide secure data storage in a cost-effective manner. Having <b>duplicate</b> <b>records</b> occupies more space and even increases the access time. Thus {{there is a need}} to eliminate <b>duplicate</b> <b>records.</b> This sounds to be simple but requires an tedious work since <b>duplicate</b> <b>records</b> do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are also introduced as the result of transcription errors, incomplete information, lack of standard formats, or any combination of these factors. Several approaches areproposed to eliminate duplicate data first at the file level and then at the chunk level to reduce the duplicatelookup complexity. In this paper,few of the methods are discussed with its advantages and disadvantages. And also a better solution is proposed...|$|R
5000|$|Oblivion Clock is an EP by the Norwegian avant-garde metal band Virus. It was {{released}} on 1 December 2012 by <b>Duplicate</b> <b>Records.</b>|$|R
40|$|Merged with <b>duplicate</b> <b>record</b> 10026. 1 / 2618 on 07. 20. 2017 by CS (TIS) This thesis surveys linearisation {{techniques}} for implementing monolithic MOS active resistors and transconductors, and investigates {{the design of}} linear tunable resistors and transconductors. Improving linearity and tunability {{in the presence of}} non-ideal factors such as bulk modulation, mobility-degradation effects and mismatch of transistors is a principal objective. A family of new non-saturation-mode resistors and two novel saturation-mode transconductors are developed. Where possible, approximate analytical expressions are derived to explain the principles of operation. Performance comparisons of the new structures are made with other well-known circuits and their relative advantages and disadvantages evaluated. Experimental and simulation results are presented which validate the proposed linearisation techniques. It is shown that the proposed family of resistors offers improved linearity whilst the transconductors combine extended tunability with low distortion. Continuous-time filter examples are given to demonstrate the potential of these circuits for application in analogue signal-processing tasks. GEC Plessey Semiconductors, Plymout...|$|E
40|$|Abstract—Often, in {{the real}} world, {{entities}} have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are introduced {{as the result of}} transcription errors, incomplete information, lack of standard formats, or any combination of these factors. In this paper, we present a thorough analysis of the literature on <b>duplicate</b> <b>record</b> detection. We cover similarity metrics that are commonly used to detect similar field entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the efficiency and scalability of approximate duplicate detection algorithms. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area. Index Terms—Duplicate detection, data cleaning, data integration, record linkage, data deduplication, instance identification, database hardening, name matching, identity uncertainty, entity resolution, fuzzy duplicate detection, entity matching. ...|$|E
40|$|In this paper, {{a robust}} {{filtering}} technique, called PC-Filter (PC stands for partition comparison), is proposed for {{effective and efficient}} <b>duplicate</b> <b>record</b> detection in large databases. PC-Filter distinguishes itself from all of existing methods by using record partitions in duplicate detection. PC-Filter operates in three steps. It first sorts the whole database and splits the sorted database {{into a number of}} record partitions. The Partition Comparison Graph (PCG) is then generated by performing fast partition pruning. Finally, duplicate records are effectively detected through internal and external partition comparison based on PCG. Four closure properties, used as heuristics, have been devised to achieve a remarkable efficiency of the filter based on triangle inequity of record similarity. The partition size is well specified such that the time complexity of PC-Filter can be optimized. By equipping existing detection methods with PC-Filter, we are able to well solve the major problems that the existing methods suffer. ...|$|E
40|$|<b>Duplicate</b> <b>records</b> {{detection}} is {{the process}} of identifying multiple records that refer to one unique real-world entity or object. However, <b>duplicate</b> <b>records</b> may do not share a common key and contain errors that make <b>duplicate</b> <b>records</b> detection a difficult task. By analyzing the MPN algorithm, it is clear that transitive closure in the merge step will cause higher false-positive rate. Our improved method treats a similar dataset as a complete sub-graph, and therefore the problem of <b>duplicate</b> <b>records</b> detection is converted to finding complete sub-graphs from an association graph where the vertexes represent data records and the edges reflect the similarity between records. In our proposed method, {{the first step is to}} build the association graph. Afterwards, it searches complete sub-graphs for each sliding window. It regards the corresponding vertex of the first record of current window as the first potential vertex of a complete sub-graph, and adds new vertexes into that sub-graph when new vertexes are adjacent to all the vertexes already in the sub-graph. These steps are repeated until all the records are checked. At the same time, our algorithm effectively avoids the redetection of some parts of an already detected sub-graph. Finally, the experimental results illustrate that the improved algorithm solves the problem of false cluster caused by transitive closure effectively...|$|R
50|$|The Agent That Shapes the Desert is {{the third}} studio album by Norwegian avant-garde metal band Virus. It was {{released}} on 14 February 2011 via <b>Duplicate</b> <b>Records.</b>|$|R
30|$|It can be {{seen from}} Fig.  4 that the {{accuracy}} of the pre-processed <b>duplicated</b> <b>recording</b> detection method is higher than that of the unpreprocessed <b>duplicated</b> <b>recording</b> detection method, but since the experimental data used is not large, the pre-processing cannot be fully displayed. In the two cases of ω= 16 and ω= 8, ωchooses 16 to be more accurate than 8 because the largest cluster in the experimental data contains 15 records. If ω= 8, it will result in some <b>duplicate</b> <b>records,</b> which cannot be detected, although ω= 16 when it is needed to do many unnecessary comparisons, increasing the amount of calculation, but it can guarantee a higher accuracy rate. In this experiment, the pre-processed ω= 16 ranking neighbor method {{is the same as the}} Canopy cluster’s replication record detection method, but the Canopy method has a higher recall rate, indicating that it can obtain more replication records. The algorithm is more efficient.|$|R
