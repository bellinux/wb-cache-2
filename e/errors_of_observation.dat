30|10000|Public
25|$|The {{theory of}} errors may {{be traced back to}} Roger Cotes's Opera Miscellanea (posthumous, 1722), but a memoir {{prepared}} by Thomas Simpson in 1755 (printed 1756) first applied the theory to the discussion of <b>errors</b> <b>of</b> <b>observation.</b> The reprint (1757) of this memoir lays down the axioms that positive and negative errors are equally probable, and that certain assignable limits define the range of all errors. Simpson also discusses continuous errors and describes a probability curve.|$|E
25|$|The fourth {{chapter of}} this {{treatise}} includes an exposition {{of the method}} of least squares, a remarkable testimony to Laplace's command over the processes of analysis. In 1805 Legendre had published the method of least squares, making no attempt to tie it {{to the theory of}} probability. In 1809 Gauss had derived the normal distribution from the principle that the arithmetic mean of observations gives the most probable value for the quantity measured; then, turning this argument back upon itself, he showed that, if the <b>errors</b> <b>of</b> <b>observation</b> are normally distributed, the least squares estimates give the most probable values for the coefficients in regression situations. These two works seem to have spurred Laplace to complete work toward a treatise on probability he had contemplated as early as 1783.|$|E
60|$|Redwood {{said that}} in working so much upon needlessly small animals he was {{convinced}} experimental physiologists made a great mistake. It is exactly like making experiments in chemistry with an insufficient quantity of material; <b>errors</b> <b>of</b> <b>observation</b> and manipulation become disproportionately large. It was of extreme importance just at present that scientific men should assert their right to have their material big. That was why he was doing his present series of experiments at the Bond Street College upon Bull Calves, {{in spite of a}} certain amount of inconvenience to the students and professors of other subjects caused by their incidental levity in the corridors. But the curves he was getting were quite exceptionally interesting, and would, when published, amply justify his choice. For his own part, {{were it not for the}} inadequate endowment of science in this country, he would never, if he could avoid it, work on anything smaller than a whale. But a Public Vivarium on a sufficient scale to render this possible was, he feared, at present, in this country at any rate, a Utopian demand. In Germany--Etc.|$|E
2500|$|... (1861) On the Algebraic and Numerical Theory <b>of</b> <b>Errors</b> <b>of</b> <b>Observations</b> and the Combination <b>of</b> <b>Observations.</b>|$|R
5000|$|Statistical {{evaluation}} of regression models with dependent <b>errors</b> <b>of</b> <b>observations.</b> Teor. application. (1989), t. 34, No. 4, s. 764-768.|$|R
6000|$|... 45. The {{indicating}} galvanometer, in {{all experiments}} made with this magnet, was about eight feet from it, not {{directly in front}} of the poles, but about 16° or 17° on one side. It was found that on making or breaking the connexion of the poles by soft iron, the instrument was slightly affected; but all <b>error</b> <b>of</b> <b>observation</b> arising from this cause was easily and carefully avoided.|$|R
60|$|Geometry, like Arithmetic, {{has been}} subsumed, in recent times, under the general study of order. It was {{formerly}} supposed that Geometry was {{the study of}} the nature of the space in which we live, and accordingly it was urged, by those who held that what exists can only be known empirically, that Geometry should really be regarded as belonging to applied mathematics. But it has gradually appeared, by the increase of non-Euclidean systems, that Geometry throws no more light upon the nature of space than Arithmetic throws upon the population of the United States. Geometry is a whole collection of deductive sciences based on a corresponding collection of sets of axioms. One set of axioms is Euclid's; other equally good sets of axioms lead to other results. Whether Euclid's axioms are true, is a question as to which the pure mathematician is indifferent; and, what is more, it is a question which it is theoretically impossible to answer with certainty in the affirmative. It might possibly be shown, by very careful measurements, that Euclid's axioms are false; but no measurements could ever assure us (owing to the <b>errors</b> <b>of</b> <b>observation)</b> that they are exactly true. Thus the geometer leaves to the man of science to decide, as best he may, what axioms are most nearly true in the actual world. The geometer takes any set of axioms that seem interesting, and deduces their consequences. What defines Geometry, in this sense, is that the axioms must give rise to a series of more than one dimension. And it is thus that Geometry becomes a department in the study of order.|$|E
5000|$|... "Observations and Statistics: An {{essay on}} the theory of <b>errors</b> <b>of</b> <b>observation</b> and the first {{principles}} of statistics", 1887, Transactions of Cambridge Society.|$|E
50|$|He {{published}} {{two small}} collections of verse {{in the mid}} to late 1950s: This and That (Fantasy Press 1955) and <b>Errors</b> <b>of</b> <b>Observation</b> (The School of Art, University of Reading 1957).|$|E
3000|$|... is {{a random}} error, which {{is assumed to}} be {{independent}} <b>of</b> the <b>errors</b> <b>of</b> other <b>observations</b> and normally distributed with a mean of zero and variance of σ [...]...|$|R
40|$|Abstract: It is {{desirable}} {{to account for}} misclassification <b>error</b> <b>of</b> meteorological <b>observations</b> so that the true skill of the forecast can be assessed. Errors in observations can occur in, among other places, pilot reports of icing, and tornado spotting. Not accounting for misclassification error gives a misleading picture of the forecast’s true performance. We present an extension to the climate skill score test developed in Briggs and Ruppert (2005) to account for possible misclassification <b>error</b> <b>of</b> the meteorological <b>observation.</b> This extension supposes a statistical misclassification error model where “gold ” standard data, or expert opinion, is available to characterize the misclassification <b>error</b> characteristics <b>of</b> the <b>observation.</b> These model parameters are then inserted into the BR skill score for which a statistical test of significance can be performed...|$|R
40|$|Abstract. Based on {{the basic}} theory of two-way {{satellite}} time and frequency transfer (TWSTFT), the mathematical model on real <b>errors</b> <b>of</b> <b>observations</b> is established in this paper. However, the model is rank-deficient. In order {{to solve this problem}} effectively, the paper introduces the algorithm of combining parameters, and applies Quasi-Accurate Detection <b>of</b> Gross <b>Errors</b> (QUAD) proposed in reference [1] to data pre-processing. The method programs opportune algorithms and resolves the problem <b>of</b> detecting gross <b>errors.</b> In the end, the method has been verified to be successful by calculating and analysing simulated data and practical measured data...|$|R
50|$|The {{theory of}} errors may {{be traced back to}} Roger Cotes's Opera Miscellanea (posthumous, 1722), but a memoir {{prepared}} by Thomas Simpson in 1755 (printed 1756) first applied the theory to the discussion of <b>errors</b> <b>of</b> <b>observation.</b> The reprint (1757) of this memoir lays down the axioms that positive and negative errors are equally probable, and that certain assignable limits define the range of all errors. Simpson also discusses continuous errors and describes a probability curve.|$|E
50|$|The fourth {{chapter of}} this {{treatise}} includes an exposition {{of the method}} of least squares, a remarkable testimony to Laplace's command over the processes of analysis. In 1805 Legendre had published the method of least squares, making no attempt to tie it {{to the theory of}} probability. In 1809 Gauss had derived the normal distribution from the principle that the arithmetic mean of observations gives the most probable value for the quantity measured; then, turning this argument back upon itself, he showed that, if the <b>errors</b> <b>of</b> <b>observation</b> are normally distributed, the least squares estimates give the most probable values for the coefficients in regression situations. These two works seem to have spurred Laplace to complete work toward a treatise on probability he had contemplated as early as 1783.|$|E
50|$|The formal {{study of}} {{theory of errors}} may {{be traced back to}} Roger Cotes' Opera Miscellanea (posthumous, 1722), but a memoir {{prepared}} by Thomas Simpson in 1755 (printed 1756) first applied the theory to the discussion of <b>errors</b> <b>of</b> <b>observation.</b> The reprint (1757) of this memoir lays down the axioms that positive and negative errors are equally probable, and that there are certain assignable limits within which all errors may be supposed to fall; continuous errors are discussed and a probability curve is given. Simpson discussed several possible distributions of error. He first considered the uniform distribution and then the discrete symmetric triangular distribution followed by the continuous symmetric triangle distribution. Tobias Mayer, in his study of the libration of the moon (Kosmographische Nachrichten, Nuremberg, 1750), invented the first formal method for estimating the unknown quantities by generalized the averaging of observations under identical circumstances to the averaging of groups of similar equations.|$|E
40|$|Constrains of {{dark energy}} (DE) at high {{redshift}} from current and mock future observational data are obtained. It is found that present data give poor constraints of DE even beyond redshift z= 0. 4, and mock future 2298 type Ia supernove data only {{give a little}} improvement of the constraints. We analyze in detail why constraints of DE decrease rapidly with the increasing of redshift. Then we try to improve the constraints of DE at high redshift. It is shown that {{the most efficient way}} is to improve the <b>error</b> <b>of</b> <b>observations.</b> Comment: 3 figures, 6 tables, data are update...|$|R
40|$|Some serious faults in <b>error</b> {{analysis}} <b>of</b> <b>observations</b> for SNIa {{have been}} found. Redoing the same <b>error</b> analysis <b>of</b> SNIa, by our idea, {{it is found}} that the average total observational <b>error</b> <b>of</b> SNIa is obviously greater than $ 0. 55 ^m$, so we can't decide whether the universe is accelerating expansion or not. Comment: 3 pages, 2 figures, submitted to OMEG 11 Proceeding (Tokyo, Japan. Nov. 14 - 18, 2011...|$|R
40|$|The Kalman filter {{requires}} kinematic {{and observation}} models not contain any systematic error. Otherwise, the resultant navigation solution will be biased or even divergent. In order {{to overcome this}} limitation, this paper presents a new random weighting method to estimate the systematic <b>error</b> <b>of</b> <b>observation</b> model in dynamic vehicle navigation. This method randomly weights the covariance matrices <b>of</b> <b>observation</b> residual vector, predicted residual vector and estimated state vector to control their magnitudes, thus governing the random weighting estimation for the covariance matrix <b>of</b> <b>observation</b> vector. Random weighting theories are established for estimations <b>of</b> the <b>observation</b> model&# 039;s systematic error and the covariance matrices <b>of</b> <b>observation</b> residual vector, predicted residual vector, observation vector and estimated state vector. Experiments and comparison analysis with the existing methods demonstrate that the proposed random weighting method can effectively resist the disturbance <b>of</b> the <b>observation</b> model&# 039;s systematic error on the state parameter estimation, leading to the improved accuracy for dynamic vehicle navigation...|$|R
40|$|This paper studies nearly optimal income {{taxation}} when individuals are uncertain about their wages {{and when the}} government observes individual income with errors. Given the distribution of observed wage rates, it asks how the proportion of inequality known ex ante to individuals, or the proportion of observed inequality due to <b>errors</b> <b>of</b> <b>observation,</b> should affect optimal taxes. The taxes compared are nearly optimal for small inequality. First approximations to optimal taxes are found for these problems and also for the many-good optimal commodity tax problems. Among other results, {{it is found that}} wage uncertainty usually decreases the optimal earnings-tax rate and that <b>errors</b> <b>of</b> <b>observation</b> always do. Copyright 1990 by Royal Economic Society. ...|$|E
40|$|Titles of 408 papers, {{books and}} parts of books, {{relating}} to the {{method of least squares}} and the theory of accidental <b>errors</b> <b>of</b> <b>observation,</b> chronologically arranged according to their dates of publication [...] . 1722 - 1876. "From the Transactions of the Connecticut Academy, v. 4, 1877. Mode of access: Internet...|$|E
40|$|The {{following}} note, {{prepared for}} the NACA, contains several remarks on the possible improvement of the experimental determination of the aerodynamic properties of wing sections. It shows how <b>errors</b> <b>of</b> <b>observation</b> can subsequently be partially eliminated, and how the computation of the maxima or minima of aerodynamic characteristics can be much improved...|$|E
40|$|I. The law of causality. [...] II. Laws <b>of</b> <b>errors.</b> [...] III. Tabular arrangements. [...] IV. Curves <b>of</b> <b>errors.</b> [...] V. Functional laws <b>of</b> <b>errors.</b> [...] VI. Laws <b>of</b> <b>errors</b> {{expressed}} by symmetrical functions. [...] VII. Relations between functional laws <b>of</b> <b>errors</b> and half-invariants. [...] VIII. Laws <b>of</b> <b>errors</b> <b>of</b> functions <b>of</b> <b>observations.</b> [...] IX. Free functions. [...] X. Adjustment. [...] XI. Adjustment by correlates. [...] XII. Adjustment by elements. [...] XIII. Special auxiliary methods. [...] XIV. The theory of probability. [...] XV. The formal theory of probability. [...] XVI. The determination of probabilities a priori and a posteriori. [...] XVII. Mathematical expectation and its mean <b>error.</b> Mode <b>of</b> access: Internet...|$|R
40|$|Abstract The adjoint {{assimilation}} {{technique is}} used to invert the prescribed initial field in the Bohai Sea. Based on this technique, the practical performances of the limited-memory BFGS (L-BFGS) method, the Regularization method, and the Gradient Descent (GD) method are investigated computationally through a series experiments. Experimental results demonstrate that the prescribed initial field can be successfully estimated by these three methods. Inversion result with the Regularization method is better than that with the L-BFGS method, although <b>errors</b> <b>of</b> <b>observations</b> are higher. Though higher simulation errors than L-BFGS and Regularization method, {{the difference between the}} prescribed distribution and inversion result is the lowest, indicating that inversion result with the traditional GD method is the best...|$|R
40|$|Suppose that we {{have the}} freedom to adapt the {{observational}} network by choosing the times and locations <b>of</b> <b>observations.</b> Which choices would yield the best analysis of the atmospheric state or the best subsequent forecast? Here, this problem of "adaptive observations" is formulated as a problem in statistical design. The statistical framework provides a rigorous mathematical statement <b>of</b> the adaptive <b>observations</b> problem and indicates where the uncertainty of the current analysis, the dynamics <b>of</b> <b>error</b> evolution, the form and <b>errors</b> <b>of</b> <b>observations,</b> and data assimilation each enter the calculation. The statistical formulation of the problem also makes clear the importance of the optimality criteria (for instance, one might choose to minimize the total error variance in a given forecast) and identifies approximations that make calculation of optimal solutions feasible in principle. Optimal solutions are discussed and interpreted for a variety of cases. Selected approaches to the adaptiv [...] ...|$|R
40|$|The {{method of}} doublet ratios, used in obtaining {{interstellar}} Na I and Ca II abundances, is generalized to include realistic multiple-cloud cases. Entirely apart from any <b>errors</b> <b>of</b> <b>observation,</b> the simplified velocity distribution {{used in the}} method leads to errors in the inferred column densities which are systematic and which can be {{as large as a}} factor of ten or more in some practical cases. The D lines of Na I toward zeta Oph illustrate such order-of-magnitude underestimates...|$|E
40|$|Household surveys play {{a pivotal}} role in {{empirical}} economics. Cross-section and longitudinal surveys are regularly conducted throughout the globe. A description of survey design and sampling methods provides the foundation for discussing survey errors. These include errors associated with sampling, survey coverage and non response (which includes attrition from panel surveys) as well as <b>errors</b> <b>of</b> <b>observation</b> or measurement. In recent years, surveys have tended to become more complex and broader in scope with many reaching beyond measuring economic choices, constraints and outcomes. This trend will likely continue and exciting technological innovations in survey methods and implementation promise to revolutionize the field...|$|E
40|$|The {{relationship}} between readings on two instruments may {{be represented by}} a linear functional relationship with <b>errors</b> <b>of</b> <b>observation</b> in both variables. This study describes a method of fitting the {{relationship between}} the two circular variables and the replicate observations are available. Maximum likelihood estimates are used and it is shown that the closed-form expression for the estimators are not available and the estimates may be obtained iteratively by choosing a suitable initial value. The model is illustrated with an application to the analysis of the wave and wind direction data recorded by two different instruments and assuming the pseudo-replicates based on time have been obtained from this unreplicated circular data...|$|E
40|$|Models that {{describe}} symmetries {{present in the}} <b>error</b> structure <b>of</b> <b>observations</b> {{have been widely used}} in dierent applications, with early examples from psychometric and medical research. The aim {{of this article is to}} study a multilevel model with a covariance structure that is block circular symmetric. Useful results are obtained for the spectra of these structured matrices...|$|R
3000|$|The dump {{coefficient}} of Gaussian σ_z and σ_ϕ is, respectively, 500  m and [...] 1 ϕ, and [...] B_ 0 is 10  g/m 3. It {{is assumed that}} the observation error covariance is diagonal. <b>Error</b> <b>of</b> ash fall <b>observation</b> is 10 [...]...|$|R
40|$|A {{comparison}} between satellite and in situ sea surface temperature (SST) {{data in the}} Western Mediterranean Sea in 1999 is realised. The {{aim of this study}} is to better understand the differences between these two data sets, in order to realise merged maps of SST using satellite and in situ data. When merging temperature from different platforms, it is crucial to take the expected RMS <b>error</b> <b>of</b> the <b>observations</b> into account and to correct for possible biases. Advanced Very High Resolution Radiometer (AVHRR) SST day-time and night-time satellite data are used, and the in situ data have been obtained from various databases (World Ocean Database’ 05, Coriolis, Medar/Medatlas and ICES). Statistics about the differences due to the hour of the day, the month of the year, the type of sensor/platform used (CTD, XBT, drifter, etc) and the spatial distribution are made using a combination <b>of</b> <b>error</b> measures, diagrams and statistical hypothesis testing. In addition to quantify the errors between different platforms, several assumptions often made when creating gridded analyses will be critically reviewed: unbiased data sets, non-correlated <b>errors</b> <b>of</b> the <b>observations,</b> spatially uniform variance, and Gaussian-distributed data. Peer reviewe...|$|R
40|$|This paper {{presents}} a new robust adaptive filtering method for transfer alignment by {{taking into account}} the systematic <b>errors</b> <b>of</b> <b>observation</b> and kinematic models in the filtering process. The proposed method overcomes the limitation of the traditional Kalman filter, that is, the requirement of precise kinematic and observation models for transfer alignment. It adaptively adjusts and updates the prior information through the equivalent weighting matrix and adaptive factor to resist the disturbances of systematic model errors on system state estimation, thus improving the accuracy of state parameter estimation. Experimental results and comparison analysis demonstrate that the proposed robust adaptive filtering method can effectively improve the performance of transfer alignment, and the achieved performance is much higher than those of the Kalman and traditional robust adaptive Kalman filters...|$|E
40|$|The quadratic, cubic, and quartic {{force field}} of HCN has been {{calculated}} by a least squares refinement {{to fit the}} most recent observed data on the vibration-rotation constants of HCN, DCN and H 13 CN. All of the observed parameters are fitted within their standard <b>errors</b> <b>of</b> <b>observation.</b> The corresponding parameters for other isotopic species are calculated. For HCP and DCP the more limited data available have been fitted to an anharmonic force field using constraints based on comparison with HCN. Using this force field the zero-point rotational constants B 0 have been corrected to obtain the equilibrium constants Be, and hence the equilibrium structure has been determined to be re(CH) = 1 • 0692 (7) A, and re(CP) = 1 • 5398 (2) A...|$|E
40|$|The Pearsonian {{coefficient}} of correlation {{as a measure of}} association between two variates is highly prone to the deleterious effects of outlier observations (in data). Statisticians have proposed a number of formulas to obtain robust measures of correlation that are considered to be less affected by <b>errors</b> <b>of</b> <b>observation,</b> perturbation or presence of outliers. Spearman’s rho, Blomqvist’s signum, Bradley’s absolute r and Shevlyakov’s median correlation are some of such robust measures of correlation. However, in many applications, correlation matrices that satisfy the criterion of positive semi-definiteness are required. Our investigation finds that while Spearman’s rho, Blomqvist’s signum and Bradley’s absolute r make positive semi-definite correlation matrices, Shevlyakov’s median correlation very often fails to do that. The use of correlation matrices based on Shevlyakov’s formula, therefore, is problematic. ...|$|E
40|$|The Kalman filter is a {{commonly}} used computational method for dynamic vehicle navigation and positioning. However, it requires kinematic and observation models not contain any systematic error; otherwise, the resultant navigation solution will be biased or even divergent. In order {{to overcome this}} limitation, this paper presents a new windowing-based random weighting method to fit the systematic <b>errors</b> <b>of</b> kinematic and <b>observation</b> models within a moving time window for dynamic vehicle navigation. This method compensates the systematic model errors by correcting observation residual vector and state noise vector during the filtering process. Random weighting theories are established to fit the systematic model errors and the covariance matrices <b>of</b> <b>observation</b> vector and predicted state vector within a moving time window. Experiments and comparison analysis with the existing methods demonstrate that the proposed method can effectively resist the disturbances on system state estimation due to the systematic <b>errors</b> <b>of</b> kinematic and <b>observation</b> models, thus significantly improving the accuracy of dynamic vehicle navigatio...|$|R
30|$|Results {{are given}} as mean ± Standard <b>error</b> (SE) <b>of</b> n <b>observations</b> taken in three {{replicates}} (n =  3). Data obtained were statistically analyzed with {{one-way analysis of}} variance (ANOVA) and the observations were considered significant when P value was less than 0.05 (P <  0.05).|$|R
40|$|The {{variation}} of the coefficient of discharge with the length-diameter ratio of the orifice was determined for nozzles having single orifice 0. 008 and 0. 020 inch in diameter. Ratios from 0. 5 to 10 were investigated at injection pressures from 500 to 5, 000 pounds per square inch. The tests showed that, within the <b>error</b> <b>of</b> the <b>observation,</b> the coefficients were the same whether the nozzles were assembled {{at the end of}} a constant tube or in an automatic injection valve having a plain stem...|$|R
