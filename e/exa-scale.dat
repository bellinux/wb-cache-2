43|0|Public
50|$|As of 2015, {{the largest}} {{reported}} simulations involved {{a hundred million}} atoms. Schulten's team modeled the structure and function of a Purple bacteria’s chromatophore, one of the simplest living examples of photosynthesis. Modeling the processes involved in converting sunlight into chemical energy meant representing 100 million atoms, 16,000 lipids, and 101 proteins, {{the contents of a}} tiny sphere-shaped organelle occupying just one percent of the cell’s total volume. The team used the Titan supercomputer at the Oak Ridge National Laboratory in Tennessee. At his death Schulten was already planning simulations for the <b>exa-scale</b> Summit computer, expected to be built by 2018.|$|E
50|$|Schulten {{identified}} {{the goal of}} the life sciences as being to characterize biological systems from the atomic to the cellular level. He used petascale computers, and planned to use <b>exa-scale</b> computers, to model atomic-scale bio-chemical processes. His work made possible the dynamic simulation of the activities of thousands of proteins working together at the macromolecular level. His research group developed and distributed software for computational structural biology, which Schulten used to make a number of significant discoveries. The molecular dynamics package NAMD and the visualization software VMD are estimated to be used by at least 300,000 researchers worldwide. Schulten died in 2016 following an illness.|$|E
40|$|The {{aim of the}} EXA-DUNE {{project is}} to combine the {{expertise}} in hardware-oriented numerics acquired in the FEAST project with the flexibility and abstraction realized in the DUNE project to provide finite element components capable of efficiently exploiting <b>exa-scale</b> hardware and applying it to porous media applications. Simulation of flow and transport processes in porous media pro-vides a formidable challenge and application field for <b>exa-scale</b> computing. Relevant continuum-scale models include partial differential equations of elliptic, parabolic and hyperbolic type which are cou-pled through highly nonlinear coefficient functions. The multi-scale character and uncertainties in the parameters constitute an additional level of complexity but provide also opportunities for <b>exa-scale</b> computing. This talk will give a short overview of the EXA-DUNE project before discussing the numerical methods that we think are able to be highly efficient on modern hardware. For incompressible two-phase flow we developed a new, fully-coupled discontinuous Galerkin discretization [1] that is comparable in efficiency (measured in accuracy per computation time) to simple cell-centered schemes but offers the opportunity to increase arithmetic intensity substantially in the assembly stage {{as well as the}} solve phase. For the efficient solution of the arising linear systems a hybrid preconditioner based on subspac...|$|E
40|$|Abstract. I {{will discuss}} {{highlights}} in {{the progress that}} is being made toward calculating processes of importance in nuclear physics from QCD using high performance computing. As <b>exa-scale</b> computing resources are expected to become available around 2017, I present current estimates of the computational resources required to accomplish central goals of nuclear physics...|$|E
40|$|During {{the last}} 15 years, the {{supercomputing}} {{industry has been}} using mass-produced, off-the-shelf components to build cluster computers. Such components are not perfect for HPC purposes, but are cheap due to effect of scale in their production. The coming <b>exa-scale</b> era changes the landscape: <b>exa-scale</b> computers will contain components in quantities large enough to justify their custom development and production. We propose a new heterogeneous processor, equipped with a network controller and designed specifically for HPC. We then show {{how it can be}} used for enterprise computing market, guaranteeing its widespread adoption and therefore low production costs. Comment: 5 pages, 4 figures. The work was presented at one of the workshops at the International Conference on Computational Science (ICCS 2013) in Barcelona, Spain. However, due to prolonged deadlines, papers from this workshop didn't get into conference proceedings; we rectify this by submitting the paper to arXiv. or...|$|E
40|$|Features like resilience, power consumption, and {{availability}} of large scale computing system strongly depend on 1 - the complexity of individual components (e. g. the gate count of each chip) and 2 - the number of components in the system. <b>Exa-scale</b> computing systems and networks of 3 G devices are examples of distributed systems composed of {{a huge number of}} high complexity individual devices. Indeed, the FI...|$|E
40|$|As {{the speed}} and number of {{interconnects}} in data centers continues to increase exponentially, architectures for data centers have to be reevaluated to enable the move to <b>Exa-scale</b> computing. A major challenge here is {{to come up with}} low cost and low power solutions for both compute networking and interconnects. In this talk we will highlight some of the recent work carried out in the TU/e on achieving these goals...|$|E
40|$|I discuss {{highlights}} in {{the progress}} that is being made toward calculating processes of importance in nuclear physics from QCD using high performance computing. As <b>exa-scale</b> computing resources are expected to become available around 2017, I present current estimates of the computational resources required to accomplish central goals of nuclear physics. Comment: Talk presented at "Quark Confinement and the Hadron Spectrum IX", August 30 - September 3, 2010, Madrid, Spain. 6 pages, 13 figure...|$|E
40|$|Supercomputer {{building}} is a many sceene, many authors game, comprising {{a lot of different}} technologies, manufacturers and ideas. Checking data available in the public database in a systematic way, some general tendencies and limitations can be concluded, both for the past and the future. The feasibility of building <b>exa-scale</b> computers as well as their limitations and utilization are also discussed. The statistical considerations provide a strong support for the conclusions. Comment: 10 figure...|$|E
40|$|Abstract. The growing {{power of}} {{parallel}} supercomputers gives scientists {{the ability to}} simulate more complex problems at higher fidelity, leading to many high-impact scientific advances. To maximize the utilization of the vast amount of data generated by these simulations, scientists also need scalable solutions for studying their data to different extents and at different abstraction levels. As we move into peta- and <b>exa-scale</b> computing, simply dumping as much raw simulation data as the storage capacity allows for post-processing analysis and visualization {{is no longer a}} viable approach. A common practice is to use a separate parallel computer to prepare data for subsequent analysis and visualization. A naive realization of this strategy not only limits the amount of data that can be saved, but also turns I/O into a performance bottleneck when using a large parallel system. We conjecture that the most plausible solution for the peta- and <b>exa-scale</b> data problem is to reduce or transform the data in-situ as it is being generated, so the amount of data that must be transferred over the network is kept to a minimum. In this paper, we discuss different approaches to in-situ processing and visualization as well as the results of our preliminary study using large-scale simulation codes on massively parallel supercomputers. 1...|$|E
40|$|In {{this white}} paper we {{describe}} work {{done on the}} development of an efficient iterative solver for lattice QCD based on the Algebraic Multi-Grid approach (AMG) within the tmLQCD software suite. This development is aimed at modern computer architectures that will be relevant for the <b>Exa-scale</b> regime, namely multicore processors together with the Intel Xeon Phi coprocessor. Because of the complexity of this solver, implementation turned out to take a considerable effort. Fine tuning and optimization will require more work and will be the subject of further investigation. However, the work presented here provides a necessary initial step in this direction...|$|E
40|$|With the {{anticipated}} slow-down of Moore's Law {{in the near}} future, three-dimensional (3 D) packaging of microelectronic structures would enable to further increase the integration density required to meet the forecasted demands of future <b>exa-scale</b> computing, cloud computing, big data systems, cognitive computing, mobile communicatoin and other emerging technologies. Through-silicon vias (TSVs) are a pathway to provide electrical connections for signaling and power-delivery through 3 D-stacked silicon (Si) microstructures. TSVs and related structures such as, e. g., interconnects and redistribution lines, however, induce stress in their proximity, namely upon electrochemical deposition and subsequent annealing, the latter due to the large mismatch in the {{coefficient of thermal expansion}} between Si and the TSV-filling materials used...|$|E
40|$|Many tile systems require {{techniques}} {{to be applied}} to increase components resilience and control the FIT (Failures In Time) rate. When scaling to peta- <b>exa-scale</b> systems the FIT rate may become unacceptable due to component numerosity, requiring more systemic countermeasures. Thus, the ability to be fault aware, i. e. to detect and collect information about fault and critical events, is a necessary feature that large scale distributed architectures must provide in order to apply systemic fault tolerance techniques. In this context, the LO|FA|MO approach is a way to obtain systemic fault awareness, by implementing a mutual watchdog mechanism and guaranteeing fault detection in a no-single-point-of-failure fashion. This document contains specification and implementation details about this approach, {{in the shape of a}} technical report. Comment: Technical Report, Preprin...|$|E
40|$|Toward the {{next-generation}} <b>exa-scale</b> short-reach optical interconnects (OIs) supporting large-capacity data transmission, {{a compact}} computer-compatible 8 -core heterogeneous trench-assisted multicore fiber (TA-MCF) is proposed, in which cores {{are arranged in}} a rectangular array. To analyze the crosstalk (XT) between adjacent cores of TA-MCF OI, a rigorous full-vectorial H- field finite element method (FEM) and coupled power theory are applied. The impact of various trench design parameters on the mode-coupling coefficient Cmn and the coupling length Lc is discussed in detail. An accurate explicit condition for the achievement of low XT in an 8 -core heterogeneous TA-MCF OI is obtained through numerical simulations. A rigorous modal solution approach based on the computationally efficient FEM and the least squares boundary residual method is employed to analyze the coupling loss caused by the misalignment to a butt-coupled TA-MCF OI...|$|E
40|$|Gao, Guang R. The {{upcoming}} <b>exa-scale</b> era {{requires a}} parallel program execution model capable of achieving scalability, productivity, energy efficiency, and resiliency. The codelet {{model is a}} fine-grained dataflow-inspired execution model which {{is the focus of}} several tera-scale and <b>exa-scale</b> studies such as DARPA's UHPC, DOE's X-Stack, and the European TERAFLUX projects. Current codelet implementations aim to making fully use of computation resources by balancing their workload in the multi-core and many-core systems. The performance is improved by this method. However, by making use of the features of the codelet model the memory optimization can be also implemented to improve the performance as well as energy efficiency. In this thesis, we focus on the memory optimization on memory workload balance and locality exploitation in the codelet model. As a case study, various versions of FFT algorithms are implemented on IBM Cyclops- 64 - a many-core system to demonstrate that the fine-grain codelet execution model is able to execute the codelets that involve different workload on the memory bandwidth in an appropriate order to reduce memory contention and thus improve performance. The experiment result shows that our fine-grain guided algorithm achieves up to 46 % performance improvement comparing to a coarse-grain implementation on Cyclops- 64. To automatically exploit locality in codelet execution, we provide three optimal or nearly optimal scheduling algorithms based on static information of codelet graph and locality. They have different trade-offs in algorithmic complexity, locality exploitation, program execution time, and energy efficiency. We test and analyze the three algorithms on various applications on an emulation platform of Cyclops- 64. The experiment result shows that our algorithms reduce up to 59. 7 % of global memory access by using local memory to buffer intermediate data between two adjacent codelets on the same core and thus improve up to 68. 1 % performance improvement and 40. 7 % energy saving comparing to the dynamic codelet scheduling approach. University of Delaware, Department of Electrical and Computer EngineeringM. E. E...|$|E
40|$|With the {{increasing}} {{power of the}} HPC hardware systems, numerical simulations are heading towards <b>exa-scale</b> computing. Early inspection and analysis of on-going large simulations enables domain experts to obtain first insight into their running simulation process and intermediate results. Compared to conventional postprocessing, such in-situ processing {{has the advantage of}} keeping data in memory, avoiding to store the large amount of raw data to disk, providing on-the-fly analysis, and preventing early failures in the simulation process. In this poster we present a distributed and scalable software infrastructure, which provides distributed insitu data processing, feature extraction and interactive exploration at user’s front-end. We have integrated and extended our system to multiple simulation applications, ranging from Lattice-Boltzmann blood flow simulation to grid based simulation for propulsion systems. A user-interactive front-end is integrated to our system, allowing to directly interact with the visualization of running simulations, gain insight, and make decisions...|$|E
40|$|Two-point Correlation Function (TPCF) {{is widely}} used in {{astronomy}} to characterize the distribution of matter/energy in the Universe, and help derive the physics that can trace back {{to the creation of}} the universe. However, it is prohibitively slow for current sized datasets, and would continue to be a critical bottleneck with the trend of increasing dataset sizes to billions of particles and more, which makes TPCF a compelling benchmark application for future <b>exa-scale</b> architectures. State-of-the-art TPCF implementations do not map well to the underlying SIMD hardware, and also suffer from load-imbalance for large core counts. In this paper, we present a novel SIMD-friendly histogram update algorithm that exploits the spatial locality of histogram updates to achieve near-linear SIMD scaling. We also present a load-balancing scheme that combines domain-specific initial static division of work and dynamic task migration across nodes to effectively balance computation across nodes...|$|E
40|$|Abstract. To {{prepare for}} future peta- or <b>exa-scale</b> computing, it is {{important}} to gain a good understanding on what impacts a hierarchical storage system would have on the performance of data-intensive applications, and accordingly, how to leverage its strengths and mitigate possible risks. To this aim, this paper adopts a user-level perspective to empirically reveal the implications of storage organization to parallel programs running on Jaguar at the Oak Ridge National Laboratory. We first describe the hierarchical configuration of Jaguar’s storage system. Then we evaluate the performance of individual storage components. In addition, we examine the scalability of metadata- and data-intensive bench-marks over Jaguar. We have discovered that the file distribution pattern can impact the aggregated I/O bandwidth. Based on our analysis, we have demon-strated {{that it is possible to}} improve the scalability of a representative applica-tion S 3 D by as much as 15 %. ...|$|E
40|$|Uncertainty {{quantification}} (UQ) for porous {{media flow}} {{is of great}} importance for many societal, environmental and industrial problems. An obstacle for progress {{in this area is}} the extreme computational effort needed for solving realistic problems. It is expected that <b>exa-scale</b> computers will open the door for a significant progress in this area. We demonstrate how new features of the Distributed and Unified Numerics Environment DUNE [1] address these challenges. In the frame of the DFG funded project EXA-DUNE the software has been extended by multiscale finite element methods (MsFEM) and by a parallel framework for the multilevel Monte Carlo (MLMC) approach. This is a general concept for computing expected values of simulation results depending on random fields, e. g. the permeability of porous media. It belongs to the class of variance reduction methods and overcomes the slow convergence of classical Monte Carlo by combining cheap/inexact and expensive/accurate solutions in an optimal ratio...|$|E
40|$|Optimally hybrid {{numerical}} solvers {{were constructed}} for massively parallel generalized eigenvalue problem (GEP). The strong scaling benchmark {{was carried out}} on the K computer and other supercomputers for electronic structure calculation problems in the matrix sizes of M = 10 ^ 4 - 10 ^ 6 with upto 105 cores. The procedure of GEP is decomposed into the two subprocedures of the reducer to the standard eigenvalue problem (SEP) and the solver of SEP. A hybrid solver is constructed, when a routine is chosen for each subprocedure from the three parallel solver libraries of ScaLAPACK, ELPA and EigenExa. The hybrid solvers with the two newer libraries, ELPA and EigenExa, give better benchmark results than the conventional ScaLAPACK library. The detailed analysis on the results implies that the reducer can be a bottleneck in next-generation (<b>exa-scale)</b> supercomputers, which indicates the guidance for future research. The code was developed as a middleware and a mini-application and will appear online. Comment: 9 pages, 8 figure...|$|E
40|$|As we {{move towards}} <b>exa-scale</b> computing, energy is {{becoming}} increasingly important, even in the high performance computing arena. However, the simple equation, Energy = Power × Time, suggests that optimizing for speed already optimizes for energy, {{under the assumption that}} Power is constant. When power is not constant, a strategy that achieves energy savings at the cost of slower execution is Dynamic Voltage and Frequency Scaling (DVFS). However, DVFS is currently applicable only to the processor, and the entire system has many other sources of power dissipation. We show that there is little to gain in compilers by trying to trade off speed for energy using DVFS. It is best to produce code that runs full-throttle, completing as quickly as possible, an approach called “race to sleep. ” Our result is based on analyses of a high-level energy model that characterizes energy consumption, related to survey of power consumption trends of recent processors for both desktop and server, as well as Cray supercomputers. ...|$|E
40|$|Abstract In the {{prospect}} of the upcoming <b>exa-scale</b> era with millions of execution units, {{the question of how to}} deal with this level of parallelism efficiently is of time-critical relevance. State-of-the-Art parallelization techniques such as OpenMP and MPI are not guaran-teed to solve the expected problems of starvation, grow-ing latencies, overheads, and contention. On the other hand, new parallelization paradigms promise to effi-ciently hide latencies and contain starvation and con-tention. In this paper we analyze the performance of one novel parallelization strategy for shared and distributed memory machines. We will focus on shared memory ar-chitectures and compare the performance of the Par-alleX execution model against the quasi-standard Open-MP for a standard stencil-based problem. We compare in detail the OpenMP implementation of two applica-tions of Jacobi solvers (one based on regular grid and one based on an irregular grid structure) with the cor-responding implementation of these applications using HPX (High Performance ParalleX), the first feature-complete, open-source implementation of ParalleX, an...|$|E
40|$|Continental-scale hyper-resolution {{simulations}} {{constitute a}} grand challenge in characterizing non-linear feedbacks {{of states and}} fluxes of the coupled water, energy, and biogeochemical cycles of terrestrial systems. Tackling this challenge requires advanced coupling and supercomputing technologies for earth system models that are discussed in this study, utilizing {{the example of the}} implementation of the newly developed Terrestrial Systems Modeling Platform (TerrSysMP) on JUQUEEN (IBM Blue Gene/Q) of the Jülich Supercomputing Centre, Germany. The applied coupling strategies rely on the Multiple Program Multiple Data (MPMD) paradigm and require memory and load balancing considerations in the exchange of the coupling fields between different component models and allocation of computational resources, respectively. These considerations can be reached with advanced profiling and tracing tools leading to the efficient use of massively parallel computing environments, which is then mainly determined by the parallel performance of individual component models. However, the problem of model I/O and initialization in the peta-scale range requires major attention, because this constitutes a true big data challenge in the perspective of future <b>exa-scale</b> capabilities, which is unsolved...|$|E
40|$|Fault {{tolerant}} algorithms for {{the numerical}} approximation of elliptic partial differential equations on modern supercomputers {{play a more}} and more {{important role in the}} future design of <b>exa-scale</b> enabled iterative solvers. Here, we combine domain partitioning with highly scalable geometric multigrid schemes to obtain fast and fault-robust solvers in three dimensions. The recovery strategy is based on a hierarchical hybrid concept where the values on lower dimensional primitives such as faces are stored redundantly and thus can be recovered easily in case of a failure. The lost volume unknowns in the faulty region are re-computed approximately with multigrid cycles by solving a local Dirichlet problem on the faulty subdomain. Different strategies are compared and evaluated with respect to performance, computational cost, and speed up. Especially effective are strategies in which the local recovery in the faulty region is executed in parallel with global solves and when the local recovery is additionally accelerated. This results in an asynchronous multigrid iteration that can fully compensate faults. Excellent parallel performance on a current peta-scale system is demonstrated...|$|E
40|$|Three-dimensional (3 D) {{electronic}} systems enable higher integration densities {{compared to their}} 2 D counterparts, a gain required {{to meet the demands}} of future <b>exa-scale</b> computing, cloud computing, big data systems, cognitive computing, mobile devices and other emerging technologies. Through-silicon vias (TSVs) open a pathway to integrate electrical connections for signaling and power delivery through the silicon (Si) carrier used in 3 D-stacked microstructures. As a limitation, TSVs induce locally thermomechanical stress in the Si lattice due to a mismatch in the coefficients of thermal expansion between Si and the TSV-filling metals and therefore enforce temperature related expansion and shrinkage during the annealing cycle. This temperature-induced crowding and relaxation of the Si lattice in proximity of the TSV (called 'keep-out-zone' forbidden for active device positioning) can cause a variety of issues ranging from stress-induced device performance degradation, interfacial delamination or interconnect failures due to cracking of the bond or even of the entire Si microstructures at stress hotspots upon assembly or operation. Additionally also the interconnect structures induce stress that will overlap with the TSV induced stress...|$|E
40|$|The present paper gives {{a review}} of our recent {{progress}} and latest results for novel linear-algebraic algorithms and its application to large-scale quantum material simulations or electronic structure calculations. The algorithms are Krylov-subspace (iterative) solvers for generalized shifted linear equations, {{in the form of}} (zS-H) x=b,in stead of conventional generalized eigen-value equation. The method was implemented in our order-$N$ calculation code ELSES ([URL] with modelled systems based on ab initio calculations. The code realized one-hundred-million-atom, or 100 -nm-scale, quantum material simulations on the K computer in a high parallel efficiency with up to all the built-in processor cores. The present paper also explains several methodological aspects, such as use of XML files and 'novice' mode for general users. A sparse matrix data library in our real problems ([URL] was prepared. Internal eigen-value problem is discussed as a general need from the quantum material simulation. The present study is a interdisciplinary one and is sometimes called 'Application-Algorithm-Architecture co-design'. The co-design will {{play a crucial role in}} <b>exa-scale</b> scientific computations. Comment: 13 pages, 6 figure...|$|E
40|$|This work {{investigates the}} {{potential}} of an in-time parallelization of atmospheric chemical ki- netics. Its numerical calculation is one time-consuming step within the numerical prediction of the air quality. The widely used parallelization strategies only allow a limited potential level of parallelism. A higher level of parallelism within the codes {{will be necessary to}} enable benefits from future <b>exa-scale</b> computing architectures. In air quality prediction codes, chem- ical kinetics is typically considered to react in isolated boxes over short splitting intervals. This allows their trivial parallelization in space, which however is limited by the number of grid entities. This work pursues a parallelization beyond this trivial potential and investigates a parallelization across time using the so called “parareal algorithm”. The latter is an iterative prediction-correction scheme, whose efficiency strongly depends on the choice of the predictor. For that purpose, different options are being investigate and compared: Time-stepping schemes with fixed step size, adaptive time-stepping schemes and repro-models, functional representations, that map a given state to a later state in time. Only the choice of repromodels leads to a speed-up through parallelism, compared to the sequential reference for the scenarios considered here...|$|E
40|$|Spherical centroidal Voronoi tessellations (SCVT) {{are used}} in many {{applications}} {{in a variety of}} fields, one being climate modeling. They are a natural choice for spatial discretizations on the Earth, or any spherical surface. The climate modeling community, which has started to make use of SCVTs, is beginning to focus on <b>exa-scale</b> computing for large scale climate simulations. As the data size increases, the efficiency of the grid generator becomes extremely important. Current high resolution simulations on the earth call for a spatial resolution of about 15 km. In terms of an SCVT this corresponds to a quasi-uniform SCVT with roughly 2 million Voronoi cells. Computing this grid serially is very expensive and can take on the order of weeks to converge sufficiently for the needs of climate modelers. This paper outlines a new algorithm that utilizes existing computational geometry tools such as conformal mapping techniques, planar triangulation algorithms, and basic domain decomposition, to compute SCVTs in parallel, thus reducing the overall time to convergence. This new algorithm shows speedup on the order of 4000 when using 42 processors over STRIPACK in computing a triangulation used for generating an SCVT...|$|E
40|$|A {{century of}} {{coherent}} experimental and theoretical investigations have uncovered {{the laws of}} nature that underly nuclear physics. The standard model of strong and electroweak interactions, with its modest number of input parameters, dictates the dynamics of the quarks and gluons - the underlying building blocks of protons, neutrons, and nuclei. While the analytic techniques of quantum field theory have {{played a key role in}} understanding the dynamics of matter in high energy processes, they encounter difficulties when applied to low-energy nuclear structure and reactions, and dense systems. Expected increases in computational resources into the <b>exa-scale</b> during the next decade will provide the ability to numerically compute a range of important strong interaction processes directly from QCD with quantifiable uncertainties using the technique of Lattice QCD. These calculations will refine the chiral nuclear forces that are used as input into nuclear many-body calculations, including the three- and four-nucleon interactions. I discuss the state-of-the-art Lattice QCD calculations of quantities of interest in nuclear physics, progress that is expected in the near future, and the impact upon nuclear physics. Comment: Presentation at the International Conference on Nuclear Theory in the Supercomputing Era - 2013, Iowa State University, May 13 - 17, 2013, Ames, Iowa. 16 pages, 13 figure...|$|E
40|$|Much {{attention}} has been given to the challenges of handling massive data volumes in modern data-intensive science. This paper examines an equally daunting challenge – the diversity of interdisciplinary data, notably research data, and the need to interrelate these data to understand complex systemic problems such as environmental change and its impact. We use the experience of the International Polar Year 2007 – 8 (IPY) as a case study to examine data management approaches seeking to address issues around complex interdisciplinary science. We find that, while technology is a critical factor in addressing the interdisciplinary dimension of the data intensive science, the technologies developing for <b>exa-scale</b> data volumes differ from those that are needed for extremely distributed andheterogeneous data. Research data will continue to be highly heterogeneous and distributed and will require technologies to be much simpler and more flexible. More importantly, {{there is a need for}} both technical and cultural adaptation. We describe a vision of discoverable, open, linked, useful, and safe collections of data, organized and curated using the best principles and practices of information and library science. This vision provides a framework for our discussion and leads us to suggest several short- and long-term strategies to facilitate a socio-technical evolution in the overall science data ecosystem...|$|E
40|$|As {{computing}} {{has moved}} relentlessly through giga-, tera-, and peta-scale systems, <b>exa-scale</b> (a million trillion opera-tions/sec.) computing is currently under active research. DARPA has recently sponsored the “UHPC ” [1] — ubiqui-tous high-performance computing — program, encouraging partnership with academia and industry to explore such sys-tems. Among the requirements are {{the development of}} novel techniques in “self-awareness” 1 in support of performance, energy-efficiency, and resiliency. Trends in processor and system architecture, driven by power and complexity, point us toward very high-core-count designs and extreme software parallelism to solve exascale-class problems. Our research is exploring a fine-grain, event-driven model in support of adaptive operation of these ma-chines. We are developing a Codelet Program Execution Model which breaks applications into codelets (small bits of functionality) and dependencies (control and data) between these objects. It then uses this decomposition to accom-plish advanced scheduling, to accommodate code and data motion within the system, and to permit flexible exploita-tion of parallelism in support of goals for performance and power. Categories and Subject Descriptors CR-number [subcategory]: third-level ∗This research was, in part, funded by the U. S. Government. The views and conclusions contained in this document {{are those of the}} authors and should not be interpreted as rep-resenting the official policies, either expressed or implied, of the U. S. Government...|$|E
40|$|As {{traditional}} hardware {{scaling laws}} {{have started to}} break down, Co-Design of hardware and software {{has become the most}} promising avenue towards <b>exa-scale</b> computing. We present a bottom-up approach {{as part of a larger}} project that develops an optimization framework for computational codesign for molecular dynamics applications. Our approach finds optimum circuit designs for arithmetic functions, such as square root or multiplication, which are the basic building blocks of the domain-specific arithmetic calculations in molecular dynamics simulations. Our design approach employs the Boolean satisfiability problem (SAT) as a vehicle for circuit design, using state-of-the-art SAT solvers that show their algorithmic power on mid-range performance computing platforms to rein in the inevitable combinatorial explosion of possible circuit designs as we increase the bit-length of our operations. While the main emphasis is on the modeling methodology, we show initial results of automated designs for a 4 -bit square root circuit and a mini-calculator. 1 MODELING HARDWARE/SOFTWARE CODESIGN AS COMBINATORIAL OPTIMIZATION When exploring the hardware and software design spaces for a given application domain, such as molecular dynamics simulations, co-design requires performance prediction of hardware-software pairs, where one or the other component may not exist in real life because it simply has not been built yet, thus makin...|$|E
40|$|In {{this thesis}} we propose a new {{algorithm}} for solving PDEs on massively parallel computers. The Nested Iteration Adaptive Mesh Refinement Range Decomposition (NI-AMRRD) algorithm uses nested iteration and adaptive mesh refinement locally before performing a global communication step. Only a few such steps are observed {{to be necessary}} before reaching a solution {{that is on the}} order of discretization error. The target application is peta- and <b>exa-scale</b> machines, where traditional parallel numerical PDE communication patterns stifle scalability. The RD algorithm uses a partition of unity to equally distribute the error and thus the work. The computational advantages of this approach are that the decomposed problems can be solved using nested iteration and any multigrid cycle type, with communication needed only a few times when the partitioned solutions are summed. This offers potential advantages in the paradigm of expensive communication but very cheap computation. This thesis introduces the method and explains the details of the communication step. Two performance models are developed, showing that the communication cost associated with a traditional parallel implementation of nested iteration is proportional to log(P) 2, whereas the NI-AMR-RD method reduces the communication time to log(P). Numerical results for the Laplace problem with dirichlet boundary conditions demonstrate this enhanced performance...|$|E
40|$|How {{will the}} United States satisfy energy demand in a {{tightening}} global energy marketplace while, {{at the same}} time, reducing greenhouse gas emissions? Exascale computing [...] expected to be available within the next eight to ten years ? may {{play a crucial role}} in answering that question by enabling a paradigm shift from test-based to science-based design and engineering. Computational modeling of complete power generation systems and engines, based on scientific first principles, will accelerate the improvement of existing energy technologies and the development of new transformational technologies by pre-selecting the designs most likely to be successful for experimental validation, rather than relying on trial and error. The predictive understanding of complex engineered systems made possible by computational modeling will also reduce the construction and operations costs, optimize performance, and improve safety. Exascale computing will make possible fundamentally new approaches to quantifying the uncertainty of safety and performance engineering. This report discusses potential contributions of <b>exa-scale</b> modeling in four areas of energy production and distribution: nuclear power, combustion, the electrical grid, and renewable sources of energy, which include hydrogen fuel, bioenergy conversion, photovoltaic solar energy, and wind turbines. Examples of current research are taken from projects funded by the U. S. Department of Energy (DOE) Office of Science at universities and national laboratories, with a special focus on research conducted at Lawrence Berkeley National Laboratory...|$|E
40|$|Abstract—As multi-petascale and <b>exa-scale</b> {{high-performance}} computing (HPC) systems inevitably {{have to deal}} with a number of resilience challenges, such as a significant growth in component count and smaller circuit sizes with lower circuit voltages, redundancy may offer an acceptable level of resilience that traditional fault tolerance techniques, such as checkpoint/restart, do not. Although redundancy in HPC is quite controversial due to the associated cost for redundant components, the constantly increasing number of cores-per-processor is tilting this cost calculation toward a system design where computation, such as for redundancy, is much cheaper and communication, needed for checkpoint/restart, is much more expensive. Recent research and development activities in redundancy for Message Passing Interface (MPI) applications focused on availability/reliability models and replication algorithms. This paper takes a first step toward solving an open research problem associated with running a parallel application redundantly, which is file I/O under redundancy. The approach intercepts file I/O calls made by a redundant application to employ coordination protocols that execute file I/O operations in a redundancy-oblivious fashion when accessing a node-local file system, or in a redundancy-aware fashion when accessing a shared networked file system. A proof-of concept prototype is presented and a number of coordination protocols are described and evaluated. The results show the performance impact for redundantly accessing a shared networked file system, but also demonstrate the capability to regain performance by utilizing MPI communication between replicas and parallel file I/O. Keywords-{{high-performance computing}}; fault tolerance; re-silience; redundancy; Message Passing Interface; I...|$|E
40|$|The SKA will {{be capable}} of {{producing}} a stream of science data products that are <b>Exa-scale</b> {{in terms of their}} storage and processing requirements. This Google-scale enterprise is attracting considerable international interest and excitement from within the industrial and academic communities. In this chapter we examine the data flow, storage and processing requirements of a number of key SKA survey science projects to be executed on the baseline SKA 1 configuration. Based on a set of conservative assumptions about trends for HPC and storage costs, and the data flow process within the SKA Observatory, it is apparent that survey projects of the scale proposed will potentially drive construction and operations costs beyond the current anticipated SKA 1 budget. This implies a sharing of the resources and costs to deliver SKA science between the community and what is contained within the SKA Observatory. A similar situation was apparent to the designers of the LHC more than 10 years ago. We propose that {{it is time for the}} SKA project and community to consider the effort and process needed to design and implement a distributed SKA science data system that leans on the lessons of other projects and looks to recent developments in Cloud technologies to ensure an affordable, effective and global achievement of SKA science goals. Comment: 27 pages, 14 figures, Conference: Advancing Astrophysics with the Square Kilometre Array June 8 - 13, 2014 Giardini Naxos, Ital...|$|E
