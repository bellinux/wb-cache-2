459|1873|Public
5000|$|One can {{simulate}} sample-paths of an fBm using {{methods for}} generating stationary Gaussian processes with known covariance function. The simplest methodrelies on the Cholesky decomposition method of the covariance matrix (explained below), which on {{a grid of}} size has complexity of order [...] A more complex, but computationally faster method is the circulant <b>embedding</b> <b>method</b> of [...]|$|E
5000|$|The closest point method (CPM) is an <b>embedding</b> <b>method</b> {{for solving}} partial {{differential}} equations on surfaces. The closest point method uses standard numerical approaches such as finite differences, finite element or spectral methods {{in order to}} solve the embedding partial differential equation (PDE) which {{is equal to the}} original PDE on the surface. The solution is computed in a band surrounding the surface in order to be computationally efficient. In order to extend the data off the surface, the closest point method uses a closest point representation. This representation extends function values to be constant along directions normal to the surface.|$|E
40|$|Background : This study {{focuses on}} a new {{acupuncture}} method of <b>embedding</b> <b>method</b> which inserts a substance on the acupuncture points for continuous stimulation. Clinical applications and cautions were examined through literary investigations. Results : Based on the literary consideration of <b>embedding</b> <b>method,</b> the following results were obtained: 1. <b>Embedding</b> <b>method</b> {{is a combination of}} traditional and embedding technique to provide longer duration of stimulation on the acupuncture points. 2. To administer the <b>embedding</b> <b>method,</b> one needs to utilize a embedding thread besides acupuncture apparatus. Sheep gut is commonly used in China and the surgical thread is the choice in Korea. 3. <b>Embedding</b> <b>method</b> may vary from the patient to patient, depending on the nature and location of the illness. Piercing, embedding, and tying are some of the possibilities. 4. <b>Embedding</b> <b>method</b> may have different arrangement of threads based on the choice of usage. 5. <b>Embedding</b> <b>method</b> is effective for various chronic illnesses such as aches, functional diseases, and the diseases of internal organs. 6. When using the embedding methods, cautions against infection and side effects due to strong stimulation are mandatory...|$|E
40|$|<b>Embedded</b> <b>methods</b> are a {{relatively}} new approach to feature selection. Unlike filter methods, which do not incorporate learning, and wrapper approaches, {{which can be used}} with arbitrary classifiers, in <b>embedded</b> <b>methods</b> the features selection part {{can not be separated from}} the learning part. Existing <b>embedded</b> <b>methods</b> are reviewed based on a unifying mathematical framework...|$|R
40|$|Network {{embedding}} assigns nodes in {{a network}} to low-dimensional representations and effectively preserves the network structure. Recently, {{a significant amount of}} progresses have been made toward this emerging network analysis paradigm. In this survey, we focus on categorizing and then reviewing the current development on network <b>embedding</b> <b>methods,</b> and point out its future research directions. We first summarize the motivation of network embedding. We discuss the classical graph embedding algorithms and their relationship with network embedding. Afterwards and primarily, we provide a comprehensive overview {{of a large number of}} network <b>embedding</b> <b>methods</b> in a systematic manner, covering the structure- and property-preserving network <b>embedding</b> <b>methods,</b> the network <b>embedding</b> <b>methods</b> with side information and the advanced information preserving network <b>embedding</b> <b>methods.</b> Moreover, several evaluation approaches for network embedding and some useful online resources, including the network data sets and softwares, are reviewed, too. Finally, we discuss the framework of exploiting these network <b>embedding</b> <b>methods</b> to build an effective system and point out some potential future directions...|$|R
40|$|In {{this article}} I {{introduce}} a new communication theory for complex information represented as a direct graph of nodes. In addition, I introduce an application for the theory, a new radical <b>method,</b> <b>embed,</b> {{that can be used}} to update object databases declaratively. The <b>embed</b> <b>method</b> revolutionizes updating of object databases. One <b>embed</b> <b>method</b> call can replace dozens of lines of complicated updating code in a traditional client program of an object database, which is a huge improvement. As a declarative <b>method</b> the <b>embed</b> <b>method</b> takes only one natural parameter, the root object of a modified object structure in the run-time memory, which makes it extremely easy to use. The communication theory behind the <b>embed</b> <b>method</b> states that modified complex information represented as a directed graph of nodes can always be transferred back to its original system in an exact and meaningful way. The theory can also have additional applications since today information often has a network or directed graph like structure and it often evolves or it is maintained. The <b>embed</b> <b>method</b> applies the communication theory by modeling the object database and a modified object structure in the run-time memory as directed graphs of nodes without any artificial limitations. For example, different kinds of circular substructures and their modifications are allowed. Persistence in the object database is defined in a well known and accepted way, by reachability from persistent root objects. The <b>embed</b> <b>method</b> also removes structures of garbage objects from the object database if any appear during the update operation, leaving the database in a consistent state. The <b>embed</b> <b>method</b> applies reference counting techniques. It understands local topology of the object database, avoiding examining unrelated objects in the database. For these reasons, the <b>embed</b> <b>method</b> is efficient and it scales for databases of different sizes...|$|R
40|$|This paper proposes an {{efficient}} <b>embedding</b> <b>method</b> for scaling kernel k-means on cloud infrastructures. The <b>embedding</b> <b>method</b> allows for approximating the com-putation {{of the nearest}} centroid to each data instance and, accordingly, it elim-inates the quadratic space and time complexities of the cluster assignment step in the kernel k-means algorithm. We show that the proposed <b>embedding</b> <b>method</b> is effective under memory and computing power constraints, and that it achieves better clustering performance compared to other approximations of the kernel k-means algorithm. ...|$|E
40|$|Abstract—The bag-of-words (BoW) {{model has}} been known as an {{effective}} method for large-scale image search and indexing. Recent work shows that {{the performance of the}} model can be further improved by using the <b>embedding</b> <b>method.</b> While different variants of the BoW model and <b>embedding</b> <b>method</b> have been developed, less effort has been made to discover their underlying working mechanism. In this paper, we systematically investigate the image search performance variation with respect to a few factors of the BoW model, and study how to employ the <b>embedding</b> <b>method</b> to further improve the image search performance. Subsequently, we summarize several observations based on the experiments on descriptor matching. To validate these observations in a real image search, we propose an effective and efficient image search scheme, in which the BoW model and <b>embedding</b> <b>method</b> are jointly optimized in terms of effectiveness and efficiency by following these observations. Our comprehensive experiments demonstrate that it is beneficial to employ these observations to develop an image search algorithm, and the proposed image search scheme outperforms state-of-the-art methods in both effectiveness and efficiency. Index Terms—Bag-of-words (BoW), <b>embedding</b> <b>method,</b> high effectiveness, high efficiency, large scale image search. I...|$|E
40|$|We {{apply the}} <b>embedding</b> <b>method</b> of Batalin-Tyutin for {{revealing}} noncommutative {{structures in the}} generalized Landau problem. Different types of noncommutativity follow from different gauge choices. This establishes a duality among the distinct algebras. An alternative approach is discussed which yields equivalent results as the <b>embedding</b> <b>method.</b> We also discuss the consequences in the Landau problem for a non constant magnetic field. Comment: To appear in Modern Physics Letters...|$|E
40|$|Although many <b>embedded</b> feature {{selection}} <b>methods</b> {{have been introduced}} {{during the last few}} years, a unifying theoretical framework has not been developed to date. We start this chapter by defining such a framework which we think is general enough to cover many <b>embedded</b> <b>methods.</b> We will then discuss <b>embedded</b> <b>methods</b> based on how they solve the {{feature selection}} problem...|$|R
40|$|<b>Embedded</b> WENO <b>methods</b> utilize all {{adjacent}} smooth substencils {{to construct}} a desirable interpolation. Conventional WENO schemes underuse this possibility close to large gradients or discontinuities. <b>Embedded</b> <b>methods</b> based on the WENO schemes of Jiang and Shu [1] and on the WENO-Z scheme of Borges et al. [2] are explicitly constructed. Several possible choices are presented that result in either better spectral properties or a higher order of convergence. The <b>embedded</b> <b>methods</b> are demonstrated to be improvements over their standard counterparts by several numerical examples. All the <b>embedded</b> <b>methods</b> presented have virtually no added computational effort compared to their standard counterparts. Keywords: Essentially non-oscillatory, WENO, high-resolution scheme, hyperbolic conservation laws, nonlinear interpolation, spectral analysis...|$|R
40|$|AbstractA {{generator}} of new <b>embedded</b> P-stable <b>methods</b> {{of order}} 2 n+ 2, where n {{is the number}} of layers used by the <b>embedded</b> <b>methods,</b> for the approximate numerical integration of the one-dimensional Schrödinger equation is developed in this paper. These new <b>methods</b> are called <b>embedded</b> <b>methods</b> because of a simple natural error control mechanism. Numerical results obtained for one-dimensional differential equations of the Schrödinger type show the validity of the developed theory...|$|R
40|$|Feature {{extraction}} and dimension reduction for networks {{is critical}} {{in a wide variety}} of domains. Efficiently and accurately learning features for multiple graphs has important applications in statistical inference on graphs. We propose a method to jointly embed multiple undirected graphs. Given a set of graphs, the joint <b>embedding</b> <b>method</b> identifies a linear subspace spanned by rank one symmetric matrices and projects adjacency matrices of graphs into this subspace. The projection coefficients can be treated as features of the graphs. We also propose a random graph model which generalizes classical random graph model and can be used to model multiple graphs. We show through theory and numerical experiments that under the model, the joint <b>embedding</b> <b>method</b> produces estimates of parameters with small errors. Via simulation experiments, we demonstrate that the joint <b>embedding</b> <b>method</b> produces features which lead to state of the art performance in classifying graphs. Applying the joint <b>embedding</b> <b>method</b> to human brain graphs, we find it extract interpretable features that can be used to predict individual composite creativity index. Comment: 14 page...|$|E
40|$|The bag-of-words (BoW) {{model has}} been known as an {{effective}} method for large-scale image search and indexing. Recent work shows that {{the performance of the}} model can be further improved by using the <b>embedding</b> <b>method.</b> While different variants of the BoW model and <b>embedding</b> <b>method</b> have been developed, less effort has been made to discover their underlying working mechanism. In this paper, we systematically investigate the image search performance variation with respect to a few factors of the BoW model, and study how to employ the <b>embedding</b> <b>method</b> to further improve the image search performance. Subsequently, we summarize several observations based on the experiments on descriptor matching. To validate these observations in a real image search, we propose an effective and efficient image search scheme, in which the BoW model and <b>embedding</b> <b>method</b> are jointly optimized in terms of effectiveness and efficiency by following these observations. Our comprehensive experiments demonstrate that it is beneficial to employ these observations to develop an image search algorithm, and the proposed image search scheme outperforms state-of-the-art methods in both effectiveness and efficiency...|$|E
40|$|We {{propose a}} {{subspace}} <b>embedding</b> <b>method</b> via Fast Cauchy Transform (FCT) in L 2 norm. It {{is motivated by}} and complements {{the work of the}} subspace <b>embedding</b> <b>method</b> in Lp norm, for all p∈[1,∞] except p = 2, by K. L. Clarkson (ACM-SIAM, 2013). Unlike the traditionally used orthogonal basis in Johnson-Lindenstrauss (JL) embedding, we employ the well-conditioned basis in L 2 norm to obtain concentration property of FCT in L 2 norm...|$|E
30|$|For this research, {{the filter}} feature {{selection}} method is chosen. Filter methods are typically much less computationally intensive than wrapper <b>methods</b> [40]. While <b>embedded</b> <b>methods</b> may yield good results {{with a low}} amount of computational complexity, using <b>embedded</b> <b>methods</b> would {{limit the number of}} classification algorithms for comparison. The two algorithms chosen for feature selection are gain ratio (GR) and Chi Squared (CS).|$|R
40|$|Knowledge graph {{embedding}} aims to embed entities and relations of knowledge graphs into low-dimensional vector spaces. Translating <b>embedding</b> <b>methods</b> regard relations as the translation from head entities to tail entities, which achieve the state-of-the-art results among knowledge graph <b>embedding</b> <b>methods.</b> However, a major limitation {{of these methods}} is the time consuming training process, which may take several days or even weeks for large knowledge graphs, and result in great difficulty in practical applications. In this paper, we propose an efficient parallel framework for translating <b>embedding</b> <b>methods,</b> called ParTrans-X, which enables the methods to be paralleled without locks by utilizing the distinguished structures of knowledge graphs. Experiments on two datasets with three typical translating <b>embedding</b> <b>methods,</b> i. e., TransE [3], TransH [17], and a more efficient variant TransE- AdaGrad [10] validate that ParTrans-X can speed up the training process by more than an order of magnitude. Comment: WI 2017 : 460 - 46...|$|R
5000|$|... t-distributed {{stochastic}} neighbor embedding (t-SNE) [...] {{is widely}} used. It {{is one of}} a family of stochastic neighbor <b>embedding</b> <b>methods.</b>|$|R
40|$|The <b>embedding</b> <b>method</b> (Inglesfield J E 1981 J. Phys. C: Solid State Phys. 14 3795) is {{nowadays}} {{widely used}} for determining properties of bound and continuum eigenstates of the Schrödinger equation. Recently, Crampin (2004 J. Phys. : Condens. Matter 16 8875) extended Inglesfield’s formalism to systems {{described by the}} Dirac equation. In this paper, we reformulate the <b>embedding</b> <b>method</b> for bound states of Dirac particles {{in the language of}} the theory of surface integral operators, which are natural analogues of the Dirichlet-to-Neumann (DtN) and Neumann-to-Dirichlet (NtD) operators used in the nonrelativistic theory. A method of constructing kernels of these Dirac counterparts of the DtN and NtD maps from solutions to an analogue of the Steklov (Stekloff) eigenproblem for an exterior Dirac problem is presented. A numerical example illustrating the utility of the DtN/NtD <b>embedding</b> <b>method</b> is provided. PACS numbers: 03. 65. Pm, 03. 65. Ge, 02. 30. Xx 1...|$|E
30|$|In our construction, {{we rely on}} {{this idea}} for Hamming {{distance}} approximation combined with the <b>embedding</b> <b>method</b> from [10, 11] of edit distance into the Hamming space.|$|E
30|$|The multi-bit <b>embedding</b> <b>method</b> {{designed}} by the authors in [6] is further explored in the following, and a more extensive set of experimental results is here presented.|$|E
50|$|Using an <b>embedded</b> <b>method</b> {{brings the}} power that the method is {{designed}} to implement the software product that the method comes with. This suggests a less complicated usage of the method and more support possibilities.The negative aspect of an <b>embedded</b> <b>method</b> obviously {{is that it can}} only be used for specific product software. Engineers and consultants, operating with several software products, could have more use of a general method, to have just one way of working.|$|R
30|$|The <b>embedding</b> <b>methods</b> {{that use}} the {{additive}} approximation of UNIWARD for the spatial, JPEG, and side-informed JPEG domain will be called S-UNIWARD, J-UNIWARD, and SI-UNIWARD, respectively.|$|R
40|$|AbstractAn <b>embedded</b> Runge—Kutta—Fehlberg <b>method</b> is developed. It {{should be}} noted that this <b>embedded</b> <b>method</b> is procuced using the Runge—Kutta—Fehlberg method with {{algebraic}} order four to estimate a truncation phase-lag error of algebraic order three. The numerical results indicate that this new method is efficient for the numerical solution of differential equations with periodic solution, using variable stepsize...|$|R
40|$|The {{embedding}} of one interconnection network {{into another}} {{is a very}} important issue in the design and analysis of parallel algorithms. Through such embeddings, the algorithms originally developed for one architecture can be directly mapped to another architecture. This paper describes a new <b>embedding</b> <b>method,</b> based on matrix transformations, for optimally embedding hierarchical hypercube networks (HHNs) into the hypercube (binary n-cube). Thus, this <b>embedding</b> <b>method</b> has practical importance in enhancing the capabilities and extending the usefulness of the hypercube, since hierarchical hypercube networks have proven to be very cost-effective {{for a wide range of}} applications...|$|E
40|$|The {{main goal}} of this work is to {{incorporate}} security in an existing ultra wideband (UWB) network. We present an <b>embedding</b> <b>method</b> where a tag is added at the physical layer and superimposed to the UWB-impulse radio signal. The tag should be added in a transparent way so that guaranteeing compatibility with existing receivers ignoring {{the presence of the}} tag. We discuss technical details of the new <b>embedding</b> <b>method.</b> In addition, we discuss embedding strength and we analyze robustness performance. We demonstrate that the proposed embedding technique meets all the system design constraints...|$|E
3000|$|The {{presented}} two-scheme <b>embedding</b> <b>method</b> {{improves the}} performance of data hiding by using the proper distribution of the available DCT coefficients among two different modified BCH schemes. First scheme uses [...]...|$|E
5000|$|For convenience, the {{coefficients}} of the explicit exponential Rosenbrock methods {{together with their}} <b>embedded</b> <b>methods</b> can be represented by using the so-called reduced Butcher tableau as follows: ...|$|R
40|$|In this letter, we show {{a direct}} {{relation}} between spectral <b>embedding</b> <b>methods</b> and kernel {{principal components analysis}} and how both are special cases of a more general learning problem: learning the principal eigenfunctions of an operator defined from a kernel and the unknown data-generating density. Whereas spectral <b>embedding</b> <b>methods</b> provided only coordinates for the training points, the analysis justifies a simple extension to out-of-sample examples (the Nyström formula) for multidimensional scaling (MDS), spectral clustering, Laplacian eigenmaps, locally linear embedding (LLE), and Isomap. The analysis provides, for all such spectral <b>embedding</b> <b>methods,</b> {{the definition of a}} loss function, whose empirical average is minimized by the traditional algorithms. The asymptotic expected value of that loss defines a generalization performance and clarifies what these algorithms are trying to learn. Experiments with LLE, Isomap, spectral clustering, and MDS show that this out-of-sample embedding formula generalizes well, with a level of error comparable to the effect of small perturbations of the training set on the embedding...|$|R
30|$|The tissues {{were fixed}} in Bouin’s fluid {{for less than}} 24 [*]h. The tissues were then {{processed}} via paraffin wax <b>embed</b> <b>method</b> of Drury and Wallington (1980) [10] and Scheehan and Brapchak (1980) [11].|$|R
40|$|We {{propose a}} new <b>embedding</b> <b>method</b> {{for a single}} vector and {{for a pair of}} vectors. This <b>embedding</b> <b>method</b> enables: a) {{efficient}} classification and regression of functions of single vectors; b) efficient approximation of distance functions; and c) non-Euclidean, semimetric learning. To the best of our knowledge, this is the first work that enables learning any general, non-Euclidean, semimetrics. That is, our method is a universal semimetric learning and approximation method that can approximate any distance function with as high accuracy as needed with or without semimetric constraints. The project homepage including code is at: [URL]...|$|E
40|$|We {{propose a}} new method for {{sampling}} from stationary Gaussian random field on a grid {{which is not}} regular but has a regular block structure which {{is often the case}} in applications. The introduced block circulant <b>embedding</b> <b>method</b> (BCEM) can outperform the classical circulant <b>embedding</b> <b>method</b> (CEM) which requires a regularization of the irregular grid before its application. Comparison of BCEM vs CEM is performed on some typical model problems. Comment: [17 pages, 8 figures] We added Remarks 2. 1, 3. 1, 3. 2, and Example 1. 3 and removed the Appendix which is now summarized in Remark 2. ...|$|E
40|$|International audienceAbstract [...] The {{main goal}} of this work is to {{incorporate}} security in an existing ultra wideband (UWB) network. We present an <b>embedding</b> <b>method</b> where a tag is added at the physical layer and superimposed to the UWB-impulse radio signal. The tag should be added in a transparent way so that guaranteeing compatibility with existing receivers ignoring {{the presence of the}} tag. We discuss technical details of the new <b>embedding</b> <b>method.</b> In addition, we discuss embedding strength and we analyze robustness performance. We demonstrate that the proposed embedding technique meets all the system design constraints...|$|E
50|$|<b>Embedded</b> <b>methods</b> {{have been}} {{recently}} proposed {{that try to}} combine the advantages of both previous methods. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously.|$|R
50|$|One way to {{implement}} product software is through usage of an <b>embedded</b> <b>method</b> or model. <b>Embedded</b> models {{are part of}} the auxiliary materials (see: definition of product software) that come with the software package.|$|R
30|$|Conventional feature {{selection}} approaches {{can be divided}} into three main directions: filter, wrapper, and <b>embedded</b> <b>methods</b> [57]. Filter methods compute a discrimination score of each feature independently of the other features based on the correlation between the feature and the label, e.g., information gain, Gini index, Relief [58, 59]. Wrapper methods measure the usefulness of feature subsets according to their predictive power, optimizing the subsequent induction procedure that uses the respective subset for classification [51, 60 – 63]. <b>Embedded</b> <b>methods</b> perform {{feature selection}} in the process of model training based on sparsity regularization [64 – 67]. For example, Miranda et al. add a regularization term that penalizes the size of the selected feature subset to the standard cost function of SVM, thereby optimizing the new objective function to conduct feature selection [68]. Essentially, the process of feature selection and learning algorithm interact in <b>embedded</b> <b>methods</b> which means the learning part and the feature selection part cannot be separated, while wrapper methods utilize the learning algorithm as a black box.|$|R
