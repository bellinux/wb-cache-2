158|217|Public
50|$|CST {{has been}} used to acquire skills from <b>human</b> <b>demonstration</b> in the PinBall domain. It has been also used to acquire skills from <b>human</b> <b>demonstration</b> on a mobile manipulator.|$|E
5000|$|Maya Cakmak, {{assistant}} professor of computer science and engineering at the University of Washington, is {{trying to create a}} robot that learns by imitating - a technique called [...] "programming by demonstration". A researcher shows it a cleaning technique for the robot's vision system and it generalizes the cleaning motion from the <b>human</b> <b>demonstration</b> as well as identifying the [...] "state of dirt" [...] before and after cleaning.|$|E
50|$|White Head Island, Gull Rock, Machias Seal Island, and North Rock {{are part}} of the larger Grand Manan archipelago. The latter two are about 15 {{kilometres}} southwest of Grand Manan Island. Both Machias Seal Island and North Rock are claimed by both Canada and the United States. For administrative purposes, they form the most southerly part of New Brunswick. Machias Seal Island also houses the province's last remaining staffed lighthouse. The light has been in continuous operation since 1832. The post of lighthouse-keeper was withdrawn for cost-saving reasons in the 1990s, but soon reinstated to provide a <b>human</b> <b>demonstration</b> of national sovereignty. Research is conducted on the island by the Atlantic Cooperative Wildlife Ecology Research Network at the University of New Brunswick in Fredericton.|$|E
40|$|Both Learning from Demonstration (LfD) and Reinforcement Learning (RL) {{are popular}} {{approaches}} for building decision-makingagents. LfDappliessupervisedlearningtoa set of <b>human</b> <b>demonstrations</b> to infer and imitate the human policy, while RL uses only a reward signal and exploration {{to find an}} optimal policy. For complex tasks both of these techniques may be ineffective. LfD may require many more demonstrationsthan it is feasible to obtain, and RL can take an inadmissible {{amount of time to}} converge. We present Automatic Decomposition and Abstraction from demonstration (ADA), an algorithm that uses mutual information measures over a set of <b>human</b> <b>demonstrations</b> to decompose a sequential decision process into several subtasks, finding state abstractions for each one of these subtasks. ADA then projects the <b>human</b> <b>demonstrations</b> into the abstracted state space to build a policy. This policy can later be improved using RL algorithms to surpass the performance of the human teacher. We find empirically that ADA can find satisficing policies for problems that are too complex to be solved with traditional LfD and RL algorithms. In particular, we show that we can use mutual information across state features to leverage <b>human</b> <b>demonstrations</b> to reduce the effects of the curse of dimensionality by finding subtasks and abstractions in sequential decision processes...|$|R
40|$|Presented at the 11 th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2012) 4 - 8 June 2012, Valencia, Spain. © 2012, International Foundation for Autonomous Agents and Multiagent Systems (www. ifaamas. org). Both Learning from Demonstration (LfD) and Reinforcement Learning (RL) {{are popular}} {{approaches}} for building decision-making agents. LfD applies supervised learning {{to a set}} of <b>human</b> <b>demonstrations</b> to infer and imitate the human policy, while RL uses only a reward signal and exploration to find an optimal policy. For complex tasks both of these techniques may be ineffective. LfD may require many more demonstrations than it is feasible to obtain, and RL can take an inadmissible amount of time to converge. We present Automatic Decomposition and Abstraction from demonstration (ADA), an algorithm that uses mutual information measures over a set of <b>human</b> <b>demonstrations</b> to decompose a sequential decision process into several sub- tasks, finding state abstractions for each one of these sub- tasks. ADA then projects the <b>human</b> <b>demonstrations</b> into the abstracted state space to build a policy. This policy can later be improved using RL algorithms to surpass the performance of the human teacher. We find empirically that ADA can find satisficing policies for problems that are too complex to be solved with traditional LfD and RL algorithms. In particular, we show that we can use mutual information across state features to leverage <b>human</b> <b>demonstrations</b> to reduce the effects of the curse of dimensionality by finding subtasks and abstractions in sequential decision processes...|$|R
30|$|Here we {{show some}} related works. Study on vision-based robot {{programming}} from <b>human</b> <b>demonstrations</b> {{has a long}} history (e.g., [2 - 4]). Recent efforts on programming by demonstration can be found at [5, 6]. However, most of them use not view-based, but target-specific model-based techniques for image processing.|$|R
40|$|Programming robots by <b>human</b> <b>demonstration</b> is an {{effective}} method for task-level robot programming. Robot programming for contact tasks such as assembly, where both motion and forces need to be controlled, requires {{an understanding of the}} interaction with the environment and cannot effectively be accomplished by playing back recorded trajectories. Furthermore, since <b>human</b> <b>demonstration</b> is used to provide the motion and force trajectories, {{it is also important to}} include aspects of human movement control in the model of interaction with the environment. In this paper, we present a method of understanding and modeling contact tasks based on end-point stiffness using data obtained by <b>human</b> <b>demonstration.</b> The concept of end-point stiffness has been used both for explaining human movement control and for the control of robots. Experimental results demonstrate the effectiveness of the method in understanding and interpreting the data obtained by <b>human</b> <b>demonstration</b> of contact tasks. 1 Introd [...] ...|$|E
40|$|The {{motivation}} {{behind this}} work is to transfer force-based assembly skills to robots by using <b>human</b> <b>demonstration.</b> For this purpose, we model the skills as a sequence of contact formations (which describe how a workpiece touches its environment) and desired transitions between contact formations. In this paper, we present a method of identifying single-ended contact formations from force sensor patterns. Instead of using geometric models of the workpieces, fuzzy logic is used to learn and model the patterns in the force signals. Membership functions are generated automatically from training data and then used by the fuzzy classifier. This classification scheme is used to learn desired sequences of contact formations which comprise a force-based skill. Experimental results are presented which use the technique to extract skill information from <b>human</b> <b>demonstration</b> data. 1 Introduction Robot programming by using <b>human</b> <b>demonstration</b> provides a natural approach to human-robot interaction a [...] ...|$|E
40|$|Abstract — This paper {{presents}} {{a contribution to}} programming by <b>human</b> <b>demonstration,</b> {{in the context of}} compliant motion task specification for sensor-controlled robot systems that physically interact with the environment. One wants to learn about the geometric parameters of the task and segment the total motion executed by the human into subtasks for the robot that can each be executed with simple compliant motion task specifications. The motion of the <b>human</b> <b>demonstration</b> tool is sensed with a 3 D camera, and the interaction with the environment is sensed with a force sensor in the <b>human</b> <b>demonstration</b> tool. Both measurements are uncertain, and do not give direct information about the geometric parameters of the contacting surfaces, or about the contact formations encountered during the <b>human</b> <b>demonstration.</b> The paper uses a Bayesian Sequential Monte Carlo method (also known as a particle filter) to do the simultaneous estimation of the contact formation (discrete information) and the geometric parameters (continuous information). The simultaneous contact formation segmentation and the geometric parameter estimation are helped by the availability of a contact state graph of all possible contact formations. The presented approach applies to all compliant motion tasks involving polyhedral objects with a known geometry, where the uncertain geometric parameters are the poses of the objects. This work improves {{the state of the art}} by scaling the contact estimation to all possible contacts, by presenting a prediction step based on the topological information of a contact state graph, and by presenting efficient algorithms that allow the estimation to operate in realtime. In real world experiments it is shown that the approach is able to discriminate in realtime between some 250 different contact formations in the graph. Index Terms — compliant motion, Bayesian estimation, particle filter, <b>human</b> <b>demonstration,</b> task segmentatio...|$|E
40|$|Abstract — Multi-fingered robot {{grasping}} is {{a challenging}} {{problem that is}} difficult to tackle using hand-coded programs. In this paper we present an imitation learning approach for learning and generalizing grasping skills based on <b>human</b> <b>demonstrations.</b> To this end, we split the task of synthesizing a grasping motion into three parts: (1) learning efficient grasp representations from <b>human</b> <b>demonstrations,</b> (2) warping contact points onto new objects, and (3) optimizing and executing the reach-and-grasp movements. We learn low-dimensional latent grasp spaces for different grasp types, which form the basis for a novel extension to dynamic motor primitives. These latent-space dynamic motor primitives are used to synthesize entire reach-and-grasp movements. We evaluated our method on a real humanoid robot. The results of the experiment demonstrate the robustness and versatility of our approach. I...|$|R
40|$|Abstract — This paper {{presents}} {{a method to}} teach a robot to play Ping Pong from failed demonstrations in a highly noisy and uncertain setting. To infer useful information from failed demonstrations, we use a MultiDonut Algorithm [7] that minimises the probability of repeating a failed demonstration and generates new attempts similar but {{not quite the same}} as the <b>demonstration.</b> We compare <b>human</b> <b>demonstrations</b> against a random strategy and show that <b>human</b> <b>demonstrations</b> provide useful information and hence yield faster learning, especially in higher dimensions. We show that learning from observing failed attempts allows the robot to perform the task more reliably than any individual demonstrator did. We also show how this algorithm adapts to gradual deterioration in the system and increases the chances of success when interacting with an unreliable system. I...|$|R
40|$|Multi-fingered robot {{grasping}} is {{a challenging}} {{problem that is}} difficult to tackle using hand-coded programs. In this paper we present an imitation learning approach for learning and generalizing grasping skills based on <b>human</b> <b>demonstrations.</b> To this end, we split the task of synthesizing a grasping motion into three parts: (1) learning efficient grasp representations from <b>human</b> <b>demonstrations,</b> (2) warping contact points onto new objects, and (3) optimizing and executing the reach-and-grasp movements. We learn low-dimensional latent grasp spaces for different grasp types, which form the basis for a novel extension to dynamic motor primitives. These latent-space dynamic motor primitives are used to synthesize entire reach-and-grasp movements. We evaluated our method on a real humanoid robot. The results of the experiment demonstrate the robustness and versatility of our approach...|$|R
40|$|This paper {{presents}} {{a contribution to}} Bayesian based sensor fusing {{in the context of}} <b>human</b> <b>demonstration</b> for compliant motion task specification, where sensor-controlled robot systems physically interact with the environment. One wants to learn about the geometric parameters of a task and segment the total motion executed during the <b>human</b> <b>demonstration</b> into subtasks for the robot. The motion of the <b>human</b> <b>demonstration</b> tool is sensed by measuring the position of multiple LED markers with a 3 D camera, and the interaction with the environment is sensed with a force/torque sensor inside the demonstration tool. All measurements are uncertain, and do not give direct information about the geometric parameters of the contacting surfaces, or about the contact formations encountered during the <b>human</b> <b>demonstration.</b> The paper uses a Bayesian Sequential Monte Carlo method (also known as a particle filter) to simultaneously estimate the contact formation (discrete information) and the geometric parameters (continuous information), where different measurement models link the information from heterogeneous sensors to the hybrid unknown parameters. The simultaneous contact formation segmentation and the geometric parameter estimation are helped by the availability of a contact state graph of all possible contact formations. The presented approach applies to all compliant motion tasks involving polyhedral objects with a known geometry, where the uncertain geometric parameters are the poses of the objects. The approach has been verified in real world experiments, in which it is able to discriminate in realtime between some 250 different contact formations in the graph...|$|E
40|$|This paper {{presents}} {{a contribution to}} programming by <b>human</b> <b>demonstration,</b> {{in the context of}} compliant motion task specification for sensor-controlled robot systems that physically interact with the environment. One wants to learn about the geometric parameters of the task and segment the total motion executed by the human into subtasks for the robot that can each be executed with simple compliant motion task specifications. The motion of the <b>human</b> <b>demonstration</b> tool is sensed with a 3 D camera, and the interaction with the environment is sensed with a force sensor in the <b>human</b> <b>demonstration</b> tool. Both measurements are uncertain, and do not give direct information about the geometric parameters of the contacting surfaces, or about the contact formations encountered during the <b>human</b> <b>demonstration.</b> The paper uses a Bayesian Sequential Monte Carlo method (also known as a particle filter) to do the simultaneous estimation of the contact formation (discrete information) and the geometric parameters (continuous information). The simultaneous contact formation segmentation and the geometric parameter estimation are helped by the availability of a contact state graph of all possible contact formations. The presented approach applies to all compliant motion tasks involving polyhedral objects with a known geometry, where the uncertain geometric parameters are the poses of the objects. The approach has been verified in real world experiments, in which it is able to discriminate in realtime between 245 different contact formations of the contact state graph...|$|E
40|$|Easy {{programming}} methods {{following the}} Programming by Demonstration (PbD) paradigm were developed {{within the last}} years. The main goal of these systems is to allow the unexperienced human user to easily integrate motion and perception skills or complex problem solving strategies. Learning form <b>human</b> <b>demonstration</b> assume very vast sensory employment. Due {{to the fact that}} missing extracted information from demonstration mostly can be compensated by graphical, speech or gesture interaction, sensorial input simplifies the programming process. Describing unconsciously performed actions or morotic coordinations is very complex and in general not possible. This paper describes how tactile sensors are integrated in a PbD system which learns form <b>human</b> <b>demonstration.</b> Therefore a analysis of the used tactile sensor and its characteristics is performed. Further on the integration of tactile information in the systems cognitive functions is pointed out. Finally it can be concluded that the enhancement of a data glove with tactile sensors improves the analysis of <b>human</b> <b>demonstration.</b> Moreover, the supplied information increases the subsymbolic and symbolic task knowledge which leads to a more reliable recognition of the user's actions...|$|E
40|$|Robots {{capable of}} growing {{knowledge}} and learning new tasks is of demanding interest. We formalize knowledge transfer in human-robot interactions, {{and establish a}} testing framework for it. As a proof of concept, we implement a robot system that not only learns in real-time from <b>human</b> <b>demonstrations,</b> but also transfers this knowledge...|$|R
40|$|Deep {{reinforcement}} learning (deep RL) has achieved superior performance in complex sequential tasks {{by using a}} deep neural network as its function approximator and by learning directly from raw images. A drawback of using raw images is that deep RL must learn the state feature representation from the raw images in addition to learning a policy. As a result, deep RL can require a prohibitively large amount of training time and data to reach reasonable performance, {{making it difficult to}} use deep RL in real-world applications, especially when data is expensive. In this work, we speed up training by addressing half of what deep RL is trying to solve [...] - learning features. Our approach is to learn some of the important features by pre-training deep RL network's hidden layers via supervised learning using a small set of <b>human</b> <b>demonstrations.</b> We empirically evaluate our approach using deep Q-network (DQN) and asynchronous advantage actor-critic (A 3 C) algorithms on the Atari 2600 games of Pong, Freeway, and Beamrider. Our results show that: 1) pre-training with <b>human</b> <b>demonstrations</b> in a supervised learning manner is better at discovering features relative to pre-training naively in DQN, and 2) initializing a deep RL network with a pre-trained model provides a significant improvement in training time even when pre-training from a small number of <b>human</b> <b>demonstrations...</b>|$|R
30|$|Multiple <b>human</b> <b>demonstrations</b> must be “consistent”. For example, let us {{consider}} pushing of an object to a goal avoiding an obstacle. If the object is carried {{on the right side}} of the obstacle in one demonstration and on the left side in another demonstration, a neural network would generate an interpolational robot motion to make the object hit the obstacle.|$|R
40|$|Captured human motion {{data can}} provide {{a rich source of}} {{examples}} of successful manipulation strategies. General techniques for adapting these examples for use in robotics are not yet available, however, in part because the problem to be solved by the robot will rarely be the same as that in the <b>human</b> <b>demonstration.</b> This paper considers the problem of adapting a <b>human</b> <b>demonstration</b> of a quasistatic manipulation task to new objects and friction conditions (Figure 1). We argue that a manipulation plan is similar to a demonstration if it involves the identical number of contacts and if the applied contact wrenches follow similar trajectories. Based on this notion of similarity, we present an algorithm that uses the <b>human</b> <b>demonstration</b> to constrain the solution space to a set of manipulation plans similar to the demonstration. Our algorithm provides guarantees on maximum task forces and flexibility in contact placement. Results for the task of tumbling large, heavy objects show that manipulation plans similar to a demonstration can be synthesized for a variety of object sizes, shapes, and coe#cients of friction. Experimental results with a humanoid robot show that the approach produces natural-looking motion in addition to e#ective manipulation plans...|$|E
40|$|This paper {{presents}} a pattern recognition approach to identifying contact formations from force sensor signals. The approach is sensor-based {{and does not}} use geometric models of the workpieces. The design of a fuzzy classifier is described, where membership functions are generated automatically from training data. The technique is demonstrated using supervised learning. Test results are included for experiments using both rigid and non-rigid workpieces. The technique is discussed {{in the context of}} robot programming by <b>human</b> <b>demonstration.</b> 1 Introduction Robot programming by using <b>human</b> <b>demonstration</b> has become an active research area. The method provides a natural approach to human-robot interaction and at the same time, avoids much of the computational complexity found in model-based program generation. Vision-based systems have been proposed and developed, which strive to extract an assembly plan from human demonstrators [10, 5]. While useful for observing high-level task informati [...] ...|$|E
40|$|Abstract. This paper {{presents}} {{our research}} group’s latest results in autonomous force-controlled manipulation tasks: (i) advanced non-linear estimators for simultaneous parameter estimation and contact formation “map building ” for 6 D contact tasks (with active sensing {{integrated into the}} task planner), and (ii) the application of these results to programming by <b>human</b> <b>demonstration,</b> for tasks involving contacts. ...|$|E
40|$|This paper {{describes}} {{our current}} research on learning task level representations by a robot through observation of <b>human</b> <b>demonstrations.</b> We focus on human hand actions and represent such hand actions in symbolic task models. We propose {{a framework of}} such models by efficiently integrating multiple observations based on attention points; we then evaluate the produced model by using a humanform robot...|$|R
40|$|To perform many common {{industrial}} robotic tasks, e. g. deburring a work-piece, {{in small}} and medium size companies where {{a model of}} the work-piece may not be available, building a geometrical model of how to perform the task from a data set of <b>human</b> <b>demonstrations</b> is highly demanded. In many cases, however, the <b>human</b> <b>demonstrations</b> may be sub-optimal and noisy solutions to the problem of performing a task. For example, an expert may not completely remove the burrs that result in deburring residuals on the work-piece. Hence, we present an iterative algorithm to estimate a noise-free geometrical model of a work-piece from a given dataset of profiles with deburring residuals. In a case study, we compare the profiles obtained with the proposed method, nonlinear principal component analysis and Gaussian mixture model/Gaussian mixture regression. The comparison illustrates the effectiveness of the proposed method, in terms of accuracy, to compute a noise-free profile model of a task...|$|R
40|$|Robot {{learning}} from demonstration {{is a method}} which enables robots to learn {{in a similar way}} as humans. In this paper, a framework that enables robots to learn from multiple <b>human</b> <b>demonstrations</b> via kinesthetic teaching is presented. The subject of learning is a high-level sequence of actions, as well as the low-level trajectories necessary to be followed by the robot to perform the object manipulation task. The multiple <b>human</b> <b>demonstrations</b> are recorded and only the most similar demonstrations are selected for robot learning. The high-level learning module identifies the sequence of actions of the demonstrated task. Using Dynamic Time Warping (DTW) and Gaussian Mixture Model (GMM), the model of demonstrated trajectories is learned. The learned trajectory is generated by Gaussian mixture regression (GMR) from the learned Gaussian mixture model.   In online working phase, the sequence of actions is identified and experimental results show that the robot performs the learned task successfully...|$|R
40|$|Gesture-Based Programming is a {{paradigm}} for the evolutionary programming of rapidly deployable manipulation systems by <b>human</b> <b>demonstration.</b> The {{goal is to}} provide a more natural environment for the user and to generate more complete and successful programs by focusing on task experts rather than programming experts. What makes it unique from other programming by <b>human</b> <b>demonstration</b> approaches are the same things that make it evolutionary: a composable knowledge base of expertise agents and a facility for supervised practice after initial training. Prior knowledge of previously acquired skills (sensorimotor expertise) facilitates the interpretation of "gestures" during training and then provides closed-loop control of execution during run-time. This paper, part one of two, presents a high-level description of the system as well as descriptions of capabilities we've demonstrated on a PUMA robot and Utah/MIT hand. The companion paper provides a detailed account of one method of acquiri [...] ...|$|E
40|$|This paper {{presents}} a general {{approach to the}} acquisition of sensor-based robot skills from human demonstrations. Since human-generated examples cannot be assumed to be optimal {{with respect to the}} robot, adaptation of the initially acquired skill is explicitly considered. Results for acquiring and refining manipulation skills for a Puma 260 manipulator are given. 1 Introduction Since humans can carry out motions with no apparent difficulty, one would expect the generation of elementary skills to be a relatively simple problem. However, it turns out that it is extremely difficult to duplicate this elementary operative intelligence, which is used by humans unconsciously, in a computercontrolled robot [10]. This observation motivates research in the field of Robot Skill Acquisition via <b>Human</b> <b>Demonstration</b> [1, 11, 7], which is an extension of Robot Programming by <b>Human</b> <b>Demonstration</b> [9] that deals with the aquisition of sensor-based robot skills from human demonstrations (Fig. 1). Figure [...] ...|$|E
40|$|We {{describe}} several {{algorithms used}} for the inference of linguistic robot policies from <b>human</b> <b>demonstration.</b> First, tracking and match objects using the Hungarian Algorithm. Then, we convert Regular Expressions to Nondeterministic Finite Automata (NFA) using the McNaughton-Yamada-Thompson Algorithm. Next, we use Subset Construction to convert to a Deterministic Finite Automaton. Finally, we minimize finite automata using either Hopcroft's Algorithm or Brzozowski's Algorithm...|$|E
40|$|We {{present the}} design of a modular tactile sensor and {{actuator}} system for observing <b>human</b> <b>demonstrations</b> of contact tasks. The system consists of three interchangeable parts: an intrinsic tactile sensor for measuring net force/ torque, an extrinsic tactile sensor for measuring contact distributions, and a tactile actuator for displaying tactile distributions. The novel components are the extrinsic sensor and tactile actuator which are “inside-out symmetric” to each other and employ an electrorheological gel for actuation...|$|R
40|$|This paper {{describes}} {{our current}} research on learning tasks (what to do) and behaviors (how to do it) by a robot through observation of <b>human</b> <b>demonstrations.</b> We focus on human hand actions and represent such hand actions in symbolic behavior and task models. We propose frameworks of such models, derived {{a method to}} acquire those models through observation, evaluated them by using a human-form robot, and considered a method to improve such models automatically...|$|R
40|$|In this {{deliverable}} {{we present}} three novel learning approaches and movement representations which {{can benefit from}} kinesthetic support and imitation learning. These different approaches were applied to motor skill learning tasks that can benefit from <b>human</b> <b>demonstrations.</b> In particular EPFL-B has investigated how incremental learning {{can be used to}} adapt a learned forward model of a real robot arm to a new task. The model was initially learned via kinesthetic teaching using a dynamical system based approach. Furthermore EPFL-B has demonstrated which learning mechanisms are involved in teaching a robot how to catch fast inflight objects. Also this challenging task benefits from <b>human</b> <b>demonstrations.</b> UniBi demonstrated how kinesthetic teaching facilitates motor skill learning in three different scenarios which emphasize that kinesthetic teaching transfers knowledge about tasks but also task constraints: first a inverse kinematics model of a KUKA light-weight robot is learned using recurrent neuronal networks. In a second experiment the iCub robot was taught via kinesthetic teaching to learn pointing movements without depth calculation of camera calibration. Finally, UniBi analysed the trade-offs in a learning and control architecture between representation and generalization on iCub which was used to learn and generalize multiple movement primitives that in turn were acquired via kinesthetic teaching. At TUG an alternative movement primitive representation based on probabilistic inference in learned graphical models was developed. This approach has interesting and new features, i. e. it allows for an intuitive representation of complex motor skills based on learned cost functions. The cost function is in the most simple case specified by learned via-points. They allow for an easy integration of task specific prior knowledge. This prior knowledge might be observed from <b>human</b> <b>demonstrations,</b> which is currently investigated on a real robot in cooperation with UGent. 1 1 Moto...|$|R
40|$|Manipulation of {{deformable}} objects, such as {{ropes and}} cloth, {{is an important}} but challenging problem in robotics. We present a learning-based system where a robot takes as input a sequence of images of a human manipulating a rope from an initial to goal configuration, and outputs a sequence of actions that can reproduce the <b>human</b> <b>demonstration,</b> using only monocular images as input. To perform this task, the robot learns a pixel-level inverse dynamics model of rope manipulation directly from images in a self-supervised manner, using about 60 K interactions with the rope collected autonomously by the robot. The <b>human</b> <b>demonstration</b> provides a high-level plan {{of what to do}} and the low-level inverse model is used to execute the plan. We show that by combining the high and low-level plans, the robot can successfully manipulate a rope into a variety of target shapes using only a sequence of human-provided images for direction. Comment: 8 pages, accepted to International Conference on Robotics and Automation (ICRA) 201...|$|E
40|$|Abstract When {{presented}} with an object to be manipulated, a robot must identify the available forms of interaction. How might an agent acquire this mapping from object representation to action? In this paper, we describe {{an approach that}} learns a mapping from objects to grasps from <b>human</b> <b>demonstration.</b> For a given object, the teacher demonstrates a set of feasible grasps. We cluster these grasps {{in terms of the}} position and orientation of the hand relative to the object. Individual clusters in this pose space are represented using probability density functions, and thus correspond to variations around canonical grasp approaches. Multiple clusters are captured through a mixture distribution-based representation. Experimental results demonstrate the feasibility of extracting a compact set of canonical grasps from the <b>human</b> <b>demonstration.</b> Each of these canonical grasps can then be used to parameterize a reach controller that brings the robot hand into a specific spatial relationship with the object...|$|E
40|$|Programming by <b>human</b> <b>demonstration</b> {{is a new}} {{paradigm}} {{for the development of}} robotic applications that focuses on the needs of task experts rather than programming experts. The traditional text-based programming paradigm demands the user be an expert in a particular programming language and further demands that the user can translate the task into this foreign language. This level of programming expertise generally precludes the user from having detailed task expertise because his/her time is devoted to the practice of programming, not the practice of the task. The goal of programming by demonstration is to eliminate both the programming language expertise and, more importantly, the expertise required to translate the task into the language. Gesture-Based Programming is a new form of programming by <b>human</b> <b>demonstration</b> that views the demonstration as a series of inexact "gestures" that convey the "intention " of the task strategy, not the details of the strategy itself. This is analogous [...] ...|$|E
40|$|Abstract: We {{present the}} design of a modular tactile sensor and {{actuator}} system for observing <b>human</b> <b>demonstrations</b> of contact tasks. The system consists of three interchangeable parts: an intrinsic tactile sensor for measuring net force/ torque, an extrinsic tactile sensor for measuring contact distributions, and a tactile actuator for displaying tactile distributions. The novel components are the extrinsic sensor and tactile actuator which are “inside-out symmetric ” to each other and employ an electrorheological gel for actuation. ...|$|R
40|$|Abstract: Grasp {{planning}} {{is one of}} key issues for robotic dexterous hands to accomplish the desired tasks. A possible technical solution is master-slave operation. Human hand and robot hand operate as the master and the slave, respectively. The motion mapping from human hand to robot hand is presented and implemented in a virtual environment so that the robot hand can accomplish the grasp tasks by <b>human</b> <b>demonstrations.</b> The simulation results illustrate {{the validity of the}} method...|$|R
40|$|In this work, {{we first}} discuss details about a novel motion {{planning}} approach for robot {{point to point}} reaching tasks called stable estimator of dynamical systems (SEDS). A human operator first demonstrates reaching movements several times, and Gaussian Mixture Model and Gaussian Mixture Regression are used to roughly encode <b>human</b> <b>demonstrations</b> through a first order ordinary differential equation. Then based on Lyapunov Stability Theorem, a constrained nonlinear optimization problem is formulated to iteratively refine the previously learned differential model and SEDS is derived. Since during <b>human</b> <b>demonstrations,</b> the velocity is usually quite low which heavily restricts the kinetic capability of the robot, and sometimes we expect the robot to move more fast, such as to catch flying objects and to avoid fast moving obstacles. Therefore, it is extremely significant to develop a method to control the velocity and duration of the robot movement. In this paper, we define a nonlinear function based on {{the distance between the}} robot and the target to adjust the velocity of the robot. Experiments have been conducted in simulation environments to verify three properties of the proposed method, namely global asymptotical stability, adaptation to spatial perturbations and velocity controllability...|$|R
