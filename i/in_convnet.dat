2|50|Public
40|$|The rapid {{progress}} in visual recognition capabilities {{over the past}} several years can be attributed largely to improvements in generic and transferrable feature representations, particularly learned representations based on convolutional networks (convnets) trained “end-to-end” to predict visual semantics given raw pixel intensity values. In this thesis, we analyze the structure of these convnet representations and their generality and transferrability to other tasks and settings. We begin in Chapter 2 by examining the hierarchical semantic structure that naturally emerges <b>in</b> <b>convnet</b> representations from large-scale supervised training, even when this structure is unobserved in the training set. Empirically, the resulting representations generalize surprisingly well to classification in related yet distinct settings. Chapters 3 and 4 showcase the flexibility of convnet-based representations for prediction tasks where the inputs or targets have more complex structure. Chapter 3 focuses on representation transfer to the object detection and semantic segmentation tasks in which objects must be localized within an image, as well as labeled. Chapter 4 augments convnets with recurrent structure to handle recognition problems with sequential inputs (e. g., video activity recognition) or outputs (e. g., image captioning). Across each of these domains, end-to-end fine-tuning of the representation for the target task provides a substantial additional performance benefit. Finally, we address the necessity of label supervision for representation learning. In Chapter 5 we propose an unsupervised learning approach based on generative models, demonstrating that some of the transferrable semantic structure learned by supervised convnets can be learned from images alone...|$|E
40|$|Image {{representation}} {{is a key}} component in visual recognition systems. In visual recognition problem, the solution or the model {{should be able to}} learn and infer the quality of certain visual semantics in the image. Therefore, {{it is important for the}} model to represent the input image in a way that the semantics of interest can be inferred easily and reliably. This thesis is written in the form of a compilation of publications and tries to look into the Convolutional Networks (CovnNets) representation in visual recognition problems from an empirical perspective. Convolutional Network is a special class of Neural Networks with a hierarchical structure where every layer’s output (except for the last layer) will be the input of another one. It was shown that ConvNets are powerful tools to learn a generic representation of an image. In this body of work, we first showed that this is indeed the case and ConvNet representation with a simple classifier can outperform highly-tuned pipelines based on hand-crafted features. To be precise, we first trained a ConvNet on a large dataset, then for every image in another task with a small dataset, we feedforward the image to the ConvNet and take the ConvNets activation on a certain layer as the image representation. Transferring the knowledge from the large dataset (source task) to the small dataset (target task) proved to be effective and outperformed baselines on a variety of tasks in visual recognition. We also evaluated the presence of spatial visual semantics <b>in</b> <b>ConvNet</b> representation and observed that ConvNet retains significant spatial information despite the fact that it has never been explicitly trained to preserve low-level semantics. We then tried to investigate the factors that affect the transferability of these representations. We studied various factors on a diverse set of visual recognition tasks and found a consistent correlation between the effect of those factors and the similarity of the target task to the source task. This intuition alongside the experimental results provides a guideline to improve the performance of visual recognition tasks using ConvNet features. Finally, we addressed the task of visual instance retrieval specifically as an example of how these simple intuitions can increase the performance of the target task massively. QC 20161209 </p...|$|E
40|$|We {{classify}} digits of real-world house numbers using convolutional {{neural networks}} (ConvNets). ConvNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are handdesigned, ConvNets can automatically learn a {{unique set of}} features optimized for a given task. We augmentedthetraditionalConvNetarchitecturebylearning multi-stagefeaturesandbyusingLppoolingandestablish a new state-of-the-art of 95. 10 % accuracy on the SVHNdataset (48 %error improvement). Furthermore, weanalyzethebenefitsofdifferentpoolingmethodsand multi-stage features <b>in</b> <b>ConvNets.</b> The source code and atutorialare availableat eblearn. sf. net. 1...|$|R
30|$|Classifiers Classifiers are {{the other}} main {{component}} of pedestrian detectors. Beyond basic linear support vector machines (linear SVMs) [43] and AdaBoost [44], many types of classifiers have been developed and introduced into detectors. Boosted forests, which {{can be interpreted as}} both a forest version of AdaBoost decision stumps and a boosting version of random forests [45], have recently been preferred in pedestrian detection [16, 19]. Structured classifiers such as partial area-under-curve optimization [46 – 48] and deformable part models (DPMs) with latent SVMs [49, 50] are also useful. Since DPMs can be interpreted as a variant of ConvNets [51], their advantage is inherent <b>in</b> <b>ConvNets.</b> Faster computation by cascading [52, 53] is also possible over boosted forest classifiers.|$|R
40|$|We {{classify}} digits of real-world house numbers using convolutional {{neural networks}} (ConvNets). ConvNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are hand-designed, ConvNets can automatically learn a {{unique set of}} features optimized for a given task. We augmented the traditional ConvNet architecture by learning multi-stage features and by using Lp pooling and establish a new state-of-the-art of 94. 85 % accuracy on the SVHN dataset (45. 2 % error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features <b>in</b> <b>ConvNets.</b> The source code and a tutorial are available at eblearn. sf. net. Comment: 4 pages, 6 figures, 2 table...|$|R
40|$|In {{this paper}} we propose the first {{bio-inspired}} six layer convolutional network (ConvNet) non-frame based that can be implemented with already physically available spike-based electronic devices. The system was designed to recognize people in three different positions: standing, lying or up-side down. The inputs were spikes obtained with a motion retina chip. We provide simulation results showing recognition delays of 16 milliseconds from stimulus onset (time-to-first spike) with a recognition rate of 94 %. The weight sharing property <b>in</b> <b>ConvNets</b> {{and the use of}} AER protocol allow a great {{reduction in the number of}} both trainable parameters and connections (only 748 trainable parameters and 123 connections in our AER system (out of 506998 connections that would be required in a frame-based implementation). Peer Reviewe...|$|R
40|$|Multi-task {{learning}} in Convolutional Networks has displayed remarkable {{success in the}} field of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations <b>in</b> <b>ConvNets</b> using multi-task learning. Specifically, we propose a new sharing unit: "cross-stitch" unit. These units combine the activations from multiple networks and can be trained end-to-end. A network with cross-stitch units can learn an optimal combination of shared and task-specific representations. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples. Comment: To appear in CVPR 2016 (Spotlight...|$|R
40|$|Convolutional Neural Networks (Convnets) have {{achieved}} good {{results in a}} range of computer vision tasks the recent years. Though given a lot of attention, visualizing the learned representations to interpret Convnets, still remains a challenging task. The high dimensionality of internal representations and the high abstractions of deep layers are the main challenges when visualizing Convnet functionality. We present in this paper a technique based on clustering internal Convnet representations with a Dirichlet Process Gaussian Mixture Model, for visualization of learned representations <b>in</b> <b>Convnets.</b> Our method copes with the high dimensionality of a Convnet by clustering representations across all nodes of each layer. We will discuss how this application is useful when considering transfer learning, i. e. transferring a model trained on one dataset to solve a task on a different one. Comment: Presented at NIPS 2016 Workshop: Practical Bayesian Nonparametric...|$|R
40|$|We {{present a}} deep layered {{architecture}} that generalizes classical convolutional neural networks (ConvNets). The architecture, called SimNets, {{is driven by}} two operators, one being a similarity function whose family contains the convolution operator used <b>in</b> <b>ConvNets,</b> {{and the other is}} a new soft max-min-mean operator called MEX that realizes classical operators like ReLU and max pooling, but has additional capabilities that make SimNets a powerful generalization of ConvNets. Three interesting properties emerge from the architecture: (i) the basic input to hidden layer to output machinery contains as special cases kernel machines with the Exponential and Generalized Gaussian kernels, the output units being "neurons in feature space" (ii) in its general form, the basic machinery has a higher abstraction level than kernel machines, and (iii) initializing networks using unsupervised learning is natural. Experiments demonstrate the capability of achieving state of the art accuracy with networks that are an order of magnitude smaller than comparable ConvNets...|$|R
30|$|There {{are more}} hand-crafted and deep motion {{features}} in other video-based tasks, such as video segmentation, activity detection, motion analysis, and event detection. Recent {{perspectives of the}} evolution of deep motion features with respect to hand-crafted features can be found, e.g., [56, 57]. In short, rather than using ConvNets over spatio-temporal space, as <b>in</b> multi-frame <b>ConvNets</b> [12, 14], it is more successful to use separate ConvNets for spatial and temporal (optical flow) features [15]. Similar two-stream architectures have been introduced in more recent studies [56 – 59].|$|R
40|$|We {{present an}} {{analysis}} of three possible strategies for exploiting the power of existing convolutional neural networks (<b>ConvNets)</b> <b>in</b> different scenarios from the ones they were trained: full training, fine tuning, and using ConvNets as feature extractors. In many applications, especially including remote sensing, it is not feasible to fully design and train a new ConvNet, as this usually requires {{a considerable amount of}} labeled data and demands high computational costs. Therefore, {{it is important to understand}} how to obtain the best profit from existing ConvNets. We perform experiments with six popular ConvNets using three remote sensing datasets. We also compare <b>ConvNets</b> <b>in</b> each strategy with existing descriptors and with state-of-the-art baselines. Results point that fine tuning tends to be the best performing strategy. In fact, using the features from the fine-tuned ConvNet with linear SVM obtains the best results. We also achieved state-of-the-art results for the three datasets used...|$|R
30|$|Deep motion features, on {{the other}} hand, have been {{actively}} explored in activity recognition [9 – 14]. Though deep convolutional neural networks (ConvNets) over spatio-temporal space had not been competitive against hand-crafted features with sophisticated encodings and classifiers, recently proposed two-stream ConvNets [15] have achieved remarkable progress. Two-stream ConvNets are inspired by two (ventral and dorsal) streams in the human brain, and capture spatial and optical flow features <b>in</b> separate <b>ConvNets.</b> Spatial features from ConvNets have been extensively used in pedestrian detection, and deep methods [16 – 18] have produced state-of-the-art results. Thus, a two-stream framework to combine spatial and temporal features should enable significant and natural improvements over these methods.|$|R
40|$|This paper {{presents}} Pinterest Related Pins, an item-to-item {{recommendation system}} that combines collaborative filtering with content-based ranking. We demonstrate that signals derived from user curation, {{the activity of}} users organizing content, are highly effective when {{used in conjunction with}} content-based ranking. This paper also demonstrates the effectiveness of visual features, such as image or object representations learned from <b>convnets,</b> <b>in</b> improving the user engagement rate of our item-to-item recommendation system...|$|R
40|$|This paper aims {{to improve}} the feature {{learning}} <b>in</b> Convolutional Networks (<b>Convnet)</b> by capturing the structure of objects. A new sparsity function is imposed on the extracted featuremap to capture the structure {{and shape of the}} learned object, extracting interpretable features {{to improve the}} prediction performance. The proposed algorithm is based on organizing the activation within and across featuremap by constraining the node activities through ℓ_ 2 and ℓ_ 1 normalization in a structured form. Comment: The paper need some improvement...|$|R
40|$|Recently ConvNets or {{convolutional}} {{neural networks}} (CNN) {{have come up}} as state-of-the-art classification and detection algorithms, achieving near-human performance in visual detection. However, ConvNet algorithms are typically very computation and memory intensive. In {{order to be able}} to embed ConvNet-based classification into wearable platforms and embedded systems such as smartphones or ubiquitous electronics for the internet-of-things, their energy consumption should be reduced drastically. This paper proposes methods based on approximate computing to reduce energy consumption <b>in</b> state-of-the-art <b>ConvNet</b> accelerators. By combining techniques both at the system- and circuit level, we can gain energy in the systems arithmetic: up to 30 x without losing classification accuracy and more than 100 x at 99 % classification accuracy, compared to the commonly used 16 -bit fixed point number format. Comment: Published in IEEE Winter Conference on Applications of Computer Vision (WACV 2016...|$|R
40|$|Abstract — Intelligent tasks, such as visual perception, {{auditory}} perception, {{and language}} understanding require {{the construction of}} good internal representations of the world (or ”features”), which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologicallyinspired trainable architecture that can learn invariant features. Each stage <b>in</b> a <b>ConvNets</b> is composed of a filter bank, some non-linearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described...|$|R
40|$|This thesis explores good {{practices}} {{for improving the}} performance of an existing convnet trained with a dataset of clean data when an additional dataset of noisy data is available. We develop techniques to clean the noisy data {{with the help of}} the clean one, a family of solutions that we will refer to as denoising, and then we explore the best sorting of the clean and noisy datasets during the fine-tuning of a convnet. Then we study strategies to select the subset of images of the clean data that will improve the classification performance, a practice we will efer to as fracking. Next, we determine how many layers are actually better to fine-tune <b>in</b> our <b>convnet,</b> given our amount of data. And finally, we compare the classic convnet architecture where a single network is fine-tuned to solve a multi-class problem with the case of fine-tuning a convnet for binary classification for each considered class...|$|R
40|$|In this paper, we {{evaluate}} the generalization power of deep features (<b>ConvNets)</b> <b>in</b> two new scenarios: aerial and {{remote sensing image}} classification. We evaluate experi-mentally ConvNets trained for recognizing everyday objects for the classification of aerial and remote sensing images. ConvNets obtained the best results for aerial images, while for remote sensing, they performed well but were outper-formed by low-level color descriptors, such as BIC. We also present a correlation analysis, showing the potential for combining/fusing different ConvNets with other descriptors or even for combining multiple ConvNets. A preliminary set of experiments fusing ConvNets obtains state-of-the-art results for the well-known UCMerced dataset. 1...|$|R
40|$|We {{show that}} a {{generative}} random field model, which we call generative ConvNet, {{can be derived from}} the commonly used discriminative ConvNet, by assuming a ConvNet for multi-category classification and assuming one of the categories is a base category generated by a reference distribution. If we further assume that the non-linearity <b>in</b> the <b>ConvNet</b> is Rectified Linear Unit (ReLU) and the reference distribution is Gaussian white noise, then we obtain a generative ConvNet model that is unique among energy-based models: The model is piecewise Gaussian, and the means of the Gaussian pieces are defined by an auto-encoder, where the filters in the bottom-up encoding become the basis functions in the top-down decoding, and the binary activation variables detected by the filters in the bottom-up convolution process become the coefficients of the basis functions in the top-down deconvolution process. The Langevin dynamics for sampling the generative ConvNet is driven by the reconstruction error of this auto-encoder. The contrastive divergence learning of the generative ConvNet reconstructs the training images by the auto-encoder. The maximum likelihood learning algorithm can synthesize realistic natural image patterns...|$|R
40|$|International audienceIn this paper, {{we propose}} diﬀerent {{strategies}} for simplifying ﬁlters, used as feature extractors, to be learnt in convolutional neural networks (<b>ConvNets)</b> <b>in</b> order {{to modify the}} hypothesis space, and to speed-up learning and processing times. We study two kinds of ﬁlters that {{are known to be}} computationally eﬃcient in feed-forward processing: fused convolution/sub-sampling ﬁlters, and separable ﬁlters. We compare the complexity of the back-propagation algorithm on ConvNets based on these diﬀerent kinds of ﬁlters. We show that using these ﬁlters allows to reach the same level of recognition performance as with classical ConvNets for handwritten digit recognition, up to 3. 3 times faster...|$|R
30|$|Despite {{the great}} successes of deep {{learning}} convolutional networks, researchers {{are not yet}} clear about its feature learning mechanism and optimal network configuration [5]. Bruna [5] proposed a scattering convolutional network (ScatNet) based on scattering theory, using a fixed filter. ScatNet showed better performance than <b>ConvNet</b> <b>in</b> hand-to-hand recognition and texture discrimination, giving researchers {{a certain degree of}} depth for learning the “black box problem” for better understanding. Chan et al. [6] proposed the principle component analysis (PCA) network (PCANet), a shallow and unsupervised learning-intensive deep learning network. PCANet [7 – 9] leverages the level of PCA convolution filtering to deal with the input image, two hashes, and a block histogram operation to produce the final eigenvector.|$|R
40|$|Recent {{enhancements}} of deep convolutional {{neural networks}} (ConvNets) empowered by {{enormous amounts of}} labeled data have closed the gap with human performance for many object recognition tasks. These impressive results have generated interest in understanding and visualization of <b>ConvNets.</b> <b>In</b> this work, we study the effect of background in the task of image classification. Our results show that changing the backgrounds of the training datasets can have drastic effects on testing accuracies. Furthermore, we enhance existing augmentation techniques with the foreground segmented objects. The findings of this work are important in increasing the accuracies when only a small dataset is available, in creating datasets, and creating synthetic images. Comment: 8 pages, 7 figure...|$|R
40|$|Deep {{convolutional}} {{networks have}} achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods {{is not so}} evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. {{which is based on}} the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices <b>in</b> learning <b>ConvNets</b> on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB 51 (69. 4 %) and UCF 101 (94. 2 %). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices. Comment: Accepted by ECCV 2016. Based on this method, we won the ActivityNet challenge 2016 in untrimmed video classificatio...|$|R
30|$|Deep {{learning}} Deep learnings are data-driven {{methods for}} learning feature hierarchies. They are now important in pedestrian detection {{as well as}} generic object classification [31 – 33], detection [34], and semantic segmentation [35]. Historically, researchers have applied simple ConvNets [36], unsupervised ConvNets [37], or specially designed networks [38 – 41] to pedestrian detection. However, they did not significantly outperform the state-of-the-art detectors with hand-crafted features. Hosang et al. [42] enlightened the strength of ConvNets by introducing transfer learning; namely, the networks become more powerful when they are pre-trained on the large ImageNet rather than trained only on pedestrian datasets. Several recent studies made remarkable progress by using <b>ConvNets</b> <b>in</b> combination with decision forests [16], transfer and multi-task learning in other tasks [17], and part mining [18].|$|R
40|$|Convolutional Neural Network {{is known}} as ConvNet have been {{extensively}} used in many complex machine learning tasks. However, hyperparameters optimization {{is one of a}} crucial step <b>in</b> developing <b>ConvNet</b> architectures, since the accuracy and performance are reliant on the hyperparameters. This multilayered architecture parameterized by a set of hyperparameters such as the number of convolutional layers, number of fully connected dense layers & neurons, the probability of dropout implementation, learning rate. Hence the searching the hyperparameter over the hyperparameter space are highly difficult to build such complex hierarchical architecture. Many methods have been proposed over the decade to explore the hyperparameter space and find the optimum set of hyperparameter values. Reportedly, Gird search and Random search are said to be inefficient and extremely expensive, due to a large number of hyperparameters of the architecture. Hence, Sequential model-based Bayesian Optimization is a promising alternative technique to address the extreme of the unknown cost function. The recent study on Bayesian Optimization by Snoek in nine convolutional network parameters is achieved the lowerest error report in the CIFAR- 10 benchmark. This article is intended to provide the overview of the mathematical concept behind the Bayesian Optimization over a Gaussian prior. Comment: 10 Page...|$|R
40|$|International audienceIndoor {{environment}} classification, {{also known}} as indoor environment recognition, is a highly appreciated perceptualability in mobile robots. In this paper, we present a novel approach which is centered on biologically inspired methodsfor recognition and representation of indoor environments. First, global visual features are extracted by using the GIST descriptor, and then we use the subsequent features for training the discriminative deep belief network (DDBN) classifier. DDBN employs a new deep architecture {{which is based on}} restricted Boltzmann machines (RBMs) and the joint density model. The back-propagation technique is used over the entire classifier to fine-tune the weights for an optimum classification. The acquired experimental results validate our approach as it performs well both in the real-world and insynthetic datasets and outperforms the Convolution Neural Networks (<b>ConvNets)</b> <b>in</b> terms of computational efficienc...|$|R
40|$|Convolutional Networks (ConvNets) have {{recently}} improved image recognition performance thanks to end-to-end learning of deep feed-forward models from raw pixels. Deep {{learning is a}} marked departure from the previous state of the art, the Fisher Vector (FV), which relied on gradient-based encoding of local hand-crafted features. In this paper, we discuss a novel connection between these two approaches. First, we show that one can derive gradient representations from <b>ConvNets</b> <b>in</b> a similar fashion to the FV. Second, we show that this gradient representation actually corresponds to a structured matrix that allows for efficient similarity computation. We experimentally study the benefits of transferring this representation over the outputs of ConvNet layers, and find consistent improvements on the Pascal VOC 2007 and 2012 datasets. Comment: To appear at BMVC 201...|$|R
30|$|The deep {{learning}} method was the {{convolutional neural network}} [33], which is a deep neural network dedicated for image classification, it is also named the <b>ConvNets</b> <b>in</b> some literatures. CNN has been proved to significantly outperform the classical machine learning methods for natural image classification. Unlike the classical methods which take the feature vectors as input, CNN takes an image patch of n[*]×[*]n pixels as input. CNN performs classification according to {{the appearance of the}} image patch; it learns the patterns of patch appearance from a large amount of training patches. The outputs of CNN are the scores for different classes, and the class with the highest score is deemed as the classification result. For our application, the input of CNN is a patch around the lymph node, the outputs are two scores of being benign and malignant.|$|R
40|$|We {{recorded}} high-density EEG in a flanker task experiment (31 subjects) and {{an online}} BCI control paradigm (4 subjects). On these datasets, we evaluated {{the use of}} transfer learning for error decoding with deep convolutional neural networks (deep <b>ConvNets).</b> <b>In</b> comparison with a regularized linear discriminant analysis (rLDA) classifier, ConvNets were significantly better in both intra- and inter-subject decoding, achieving an average accuracy of 84. 1 % within subject and 81. 7 % on unknown subjects (flanker task). Neither method was, however, able to generalize reliably between paradigms. Visualization of features the ConvNets learned from the data showed plausible patterns of brain activity, revealing both {{similarities and differences between}} the different kinds of errors. Our findings indicate that deep learning techniques are useful to infer information about the correctness of action in BCI applications, particularly for the transfer of pre-trained classifiers to new recording sessions or subjects. Comment: 6 pages, 9 figures, The 6 th International Winter Conference on Brain-Computer Interface 201...|$|R
40|$|In this paper, we {{introduce}} a new {{deep convolutional neural network}} (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module <b>in</b> typical deep <b>ConvNets</b> produces classification results that are either better than or comparable to {{the state of the art}} on the following benchmark datasets: MNIST, CIFAR- 10, CIFAR- 100 and SVHN...|$|R
40|$|In this paper, {{we propose}} to adopt ConvNets to {{recognize}} human actions from depth maps on relatively small datasets based on Depth Motion Maps (DMMs). In particular, three strategies are developed to effectively leverage {{the capability of}} <b>ConvNets</b> <b>in</b> mining discriminative features for recognition. Firstly, different viewpoints are mimicked by rotating virtual cameras around subject represented by the 3 D points of the captured depth maps. This not only synthesizes more data from the captured ones, but also makes the trained ConvNets view-Tolerant. Secondly, DMMs are constructed and further enhanced for recognition by encoding them into Pseudo-RGB images, turning the spatial-Temporal motion patterns into textures and edges. Lastly, through transferring learning the models originally trained over ImageNet for image classification, the three ConvNets are trained independently on the colorcoded DMMs constructed in three orthogonal planes. The proposed algorithm was extensively evaluated on MSRAction 3 D, MSRAction 3 DExt and UTKinect-Action datasets and achieved the stateof-the-Art results on these datasets...|$|R
40|$|This paper {{proposes a}} hybrid {{convolutional}} network (ConvNet) -Restricted Boltzmann Machine (RBM) model for face verification in wild conditions. A key contribution {{of this work}} is to directly learn relational visual features, which indicate identity similarities, from raw pixels of face pairs with a hybrid deep network. The deep <b>ConvNets</b> <b>in</b> our model mimic the primary visual cortex to jointly extract local relational visual features from two face images compared with the learned filter pairs. These relational features are further processed through multiple layers to extract high-level and global features. Multiple groups of <b>ConvNets</b> are constructed <b>in</b> order to achieve robustness and characterize face similarities from different aspects. The top-layer RBM performs inference from complementary high-level features extracted from different ConvNet groups with a two-level average pooling hierarchy. The entire hybrid deep network is jointly fine-tuned to optimize for the task of face verification. Our model achieves competitive face verification performance on the LFW dataset...|$|R
40|$|Understanding the {{internal}} process of ConvNets is commonly done using visualization techniques. However, these techniques do not usually provide {{a tool for}} estimating the stability of a <b>ConvNet</b> against noise. <b>In</b> this paper, we show how to analyze a <b>ConvNet</b> <b>in</b> the frequency domain using a 4 -dimensional visualization technique. Using the frequency domain analysis, we show the reason that a ConvNet might be sensitive to a very low magnitude additive noise. Our experiments on a few ConvNets trained on different datasets revealed that convolution kernels of a trained ConvNet usually pass most of the frequencies {{and they are not}} able to effectively eliminate the effect of high frequencies. Our next experiments shows that a convolution kernel which has a more concentrated frequency response could be more stable. Finally, we show that fine-tuning a ConvNet using a training set augmented with noisy images can produce more stable ConvNets. Comment: Under review as a conference paper at ICLR 2016, minor changes in the tex...|$|R
30|$|Feature map {{generation}} We use convolutional layers as feature transformation per {{frame to}} enable sliding-window detection over the feature maps, differently from per-window classification approaches [18, 34]. In classical neural networks, the input size (of the image) {{has to be}} fixed; however, ConvNets have been recently used as flexible filters, which are applicable to images of arbitrary size (over {{the size of the}} convolutional kernels) and fixed channel numbers [16, 76, 77]. This becomes possible when ConvNets are without fully connected layers. We use <b>ConvNets</b> <b>in</b> this manner; namely, they are applied to clipped windows in training but to full-size input images during the test. During the test, the forest classifier looks up pixels in the feature maps, the pre-computed convolutional features of the whole image, to execute sliding-window detection. We also adopt pyramid patchworking [78] during the test. That is, we stitch the spatial pyramid of an input image into a larger single image, which shortens test runtime. This makes the input image size 932 × 932 pixels while the original size of frames is 640 × 480 pixels.|$|R
40|$|Convolution Neural Networks, {{known as}} ConvNets {{exceptionally}} perform well in many complex machine learning tasks. The architecture of ConvNets demands the huge and rich {{amount of data}} and involves with {{a vast number of}} parameters that leads the learning takes to be computationally expensive, slow convergence towards the global minima, trap in local minima with poor predictions. In some cases, architecture overfits the data and make the architecture difficult to generalise for new samples that were not in the training set samples. To address these limitations, many regularization and optimization strategies are developed for the past few years. Also, studies suggested that these techniques significantly increase the performance of the networks as well as reducing the computational cost. In implementing these techniques, one must thoroughly understand the theoretical concept of how this technique works in increasing the expressive power of the networks. This article is intended to provide the theoretical concepts and mathematical formulation of the most commonly used strategies <b>in</b> developing a <b>ConvNet</b> architecture. Comment: 15 page...|$|R
40|$|Convolutional Neural Networks (ConvNets) {{have shown}} {{excellent}} results on many visual classification tasks. With {{the exception of}} ImageNet, these datasets are carefully crafted such that objects are well-aligned at similar scales. Naturally, the feature learning problem gets more challenging {{as the amount of}} variation in the data increases, as the models have to learn to be invariant to certain changes in appearance. Recent results on the ImageNet dataset show that given enough data, ConvNets can learn such invariances producing very discriminative features [1]. But could we do more: use less parameters, less data, learn more discriminative features, if certain invariances were built into the learning process? In this paper we present a simple model that allows ConvNets to learn features in a locally scale-invariant manner without increasing the number of model parameters. We show on a modified MNIST dataset that when faced with scale variation, building <b>in</b> scale-invariance allows <b>ConvNets</b> to learn more discriminative features with reduced chances of over-fitting. Comment: Deep Learning and Representation Learning Workshop: NIPS 201...|$|R
