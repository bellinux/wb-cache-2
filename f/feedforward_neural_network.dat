865|10000|Public
5000|$|<b>Feedforward</b> <b>Neural</b> <b>Network</b> Methodology, Series on Statistics for Engineering and Information Science, Springer-Verlag, 1999.|$|E
50|$|A <b>feedforward</b> <b>neural</b> <b>network</b> is an {{artificial}} neural network wherein connections between the units do not form a cycle. As such, it is different from recurrent neural networks.|$|E
5000|$|Model: {{contains}} {{the definition of}} the data mining model. E.g., A multi-layered <b>feedforward</b> <b>neural</b> <b>network</b> is represented in PMML by a [...] "NeuralNetwork" [...] element which contains attributes such as: ...|$|E
40|$|AbstractMathematical essence and {{structures}} of the <b>feedforward</b> <b>neural</b> <b>networks</b> are investigated in this paper. The interpolation mechanisms of the <b>feedforward</b> <b>neural</b> <b>networks</b> are explored. For example, the well-known result, namely, that a <b>neural</b> <b>network</b> is an universal approximator, can be concluded naturally from the interpolative representations. Finally, the learning algorithms of the <b>feedforward</b> <b>neural</b> <b>networks</b> are discussed...|$|R
5000|$|... #Subtitle level 3: Quantum Generalisations of <b>Feedforward</b> <b>Neural</b> <b>Networks</b> ...|$|R
40|$|In this paper, {{a general}} {{backpropagation}} learning {{framework for the}} training of <b>feedforward</b> <b>neural</b> <b>networks</b> is proposed. The convergence to global minimum under the framework is investigated using the Lyapunov stability theory. It is shown the existing <b>feedforward</b> <b>neural</b> <b>networks</b> training algorithms are special cases of the proposed framework. 1...|$|R
5000|$|The SDM may be {{regarded}} {{either as a}} content-addressable extension of a classical random-access memory (RAM) or as a special type of three layer <b>feedforward</b> <b>neural</b> <b>network.</b> The main SDM alterations to the RAM are: ...|$|E
5000|$|In 2006, Hinton and Salakhutdinov {{showed how}} a many-layered <b>feedforward</b> <b>neural</b> <b>network</b> could be {{effectively}} pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation.|$|E
50|$|The <b>feedforward</b> <b>neural</b> <b>network</b> was {{the first}} and {{simplest}} type of artificial neural network devised. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network.|$|E
40|$|Abstract — In this paper, a {{geometrical}} {{representation of}} McCulloch-Pitts neural model is presented. From the represen-tation, a clear visual picture {{and interpretation of}} the model can be seen. Two interesting applications based on the interpretation are discussed. They are 1) a new design principle of <b>feedforward</b> <b>neural</b> <b>networks</b> and 2) a new proof of mapping abilities of three-layer <b>feedforward</b> <b>neural</b> <b>networks.</b> Index Terms—Feedforward <b>neural</b> <b>networks,</b> measurable func-tions, neighborhood covering. I...|$|R
30|$|Sigmoidal {{multi-layer}} <b>feedforward</b> <b>neural</b> <b>networks</b> can realize arbitrary continuous nonlinear {{function at}} arbitrary precision, which {{is applied to}} <b>neural</b> <b>network</b> filtering.|$|R
40|$|This paper aims {{to place}} <b>neural</b> <b>networks</b> {{in the context}} of boolean circuit complexity. We define {{appropriate}} classes of <b>feedforward</b> <b>neural</b> <b>networks</b> with specified fan-in, accuracy of computation and depth and using techniques of communication complexity proceed to show that the classes fit into a well-studied hierarchy of boolean circuits. Results cover both classes of sigmoid activation function networks and linear threshold networks. This provides a much needed theoretical basis {{for the study of the}} computational power of <b>feedforward</b> <b>neural</b> <b>networks...</b>|$|R
50|$|He is also {{exploring}} {{the use of}} high-performance computing and big data to develop better techniques for deep learning. One of the tasks being studied, not surprisingly, is face recognition. In another project, his team is working with astronomer Robert Brunner to train a <b>feedforward</b> <b>neural</b> <b>network</b> to identify images of galaxies.|$|E
5000|$|Cap depth - for a <b>feedforward</b> <b>neural</b> <b>network,</b> {{the depth}} of the CAPs (thus of the network) is the number of hidden layers plus one (as the output layer is also parameterized), but for {{recurrent}} neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.|$|E
50|$|Two {{datasets}} are linearly separable {{if their}} convex hulls do not intersect. The method may be formulated as a <b>feedforward</b> <b>neural</b> <b>network</b> with weights that are trained via linear programming. Comparisons between neural networks trained with the MSM versus backpropagation show MSM is {{better able to}} classify data. The decision problem associated linear program for the MSM is NP-Complete.|$|E
50|$|This task is {{much more}} {{difficult}} for <b>neural</b> <b>networks.</b> For simple <b>feedforward</b> <b>neural</b> <b>networks,</b> this task is not solveable because feedforward networks don't have any working memory.|$|R
50|$|The {{universal}} approximation theorem {{concerns the}} capacity of <b>feedforward</b> <b>neural</b> <b>networks</b> with a single hidden layer of finite size to approximate continuous functions.|$|R
25|$|Ciresan {{and colleagues}} (2010) in Jurgen Schmidhuber's group showed {{that despite the}} {{vanishing}} gradient problem, GPUs makes back-propagation feasible for many-layered <b>feedforward</b> <b>neural</b> <b>networks.</b>|$|R
50|$|In {{the context}} of neural networks, a {{perceptron}} is an artificial neuron using the Heaviside step function as the activation function. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from a multilayer perceptron, which is a misnomer for a more complicated neural network. As a linear classifier, the single-layer perceptron is the simplest <b>feedforward</b> <b>neural</b> <b>network.</b>|$|E
50|$|A deep <b>feedforward</b> <b>neural</b> <b>network</b> (DNN) is an {{artificial}} neural network with multiple hidden layers of units between the input and output layers. Similar to shallow neural networks, DNNs can model complex non-linear relationships. DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data.|$|E
50|$|The <b>feedforward</b> <b>neural</b> <b>network</b> was {{the first}} and {{simplest}} type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops. Feedforward networks can be constructed with various types of units, such as binary McCulloch-Pitts neurons, the simplest of which is the perceptron. Continuous neurons, frequently with sigmoidal activation, are used in the context of backpropagation.|$|E
40|$|Several recent {{publications}} have exhibited {{relationships between}} the theories of logic programming and of <b>neural</b> <b>networks.</b> We consider a general approach to representing normal logic programs via <b>feedforward</b> <b>neural</b> <b>networks.</b> We show that the immediate consequence operator associated with each logic program, which {{can be understood as}} implicitly determining its declarative semantics, can be approximated by 3 -layer <b>feedforward</b> <b>neural</b> <b>networks</b> arbitrarily well in a certain measure-theoretic sense. If this operator is continuous in a topology known as the atomic topology, then the approximation is uniform in all points. ...|$|R
40|$|Abstract:- In {{this paper}} the use o <b>feedforward</b> f <b>neural</b> <b>networks</b> in the {{characterization}} of images by texture content is investigated. An in depth experimental study is conducted comparing several well known textural feature extraction techniques along with a novel discrete wavelet transform based methodology. It is demonstrated that the new technique leads to the design and selection o <b>feedforward</b> f <b>neural</b> <b>networks</b> architectures with the best texture classification accuracy. Key-words:- Feature extraction, texture classification, wavelet-based texture descriptors, cooccurrence matrices, gray level run length moments, fractal dimension, statistical texture analysis <b>feedforward</b> <b>neural,</b> <b>networks...</b>|$|R
40|$|<b>Feedforward</b> <b>neural</b> <b>networks</b> {{have been}} used to perform classifications and to learn {{functional}} mappings. This paper compares human performance to <b>feedforward</b> <b>neural</b> <b>networks</b> using back propagation in generating functional relationships from limited data. Many business judgments are made in situations where decision makers are required to infer relationships porn partial, incomplete, and conflicting information. Experiments conducted to compare the interpolations of humans to the behavior of the <b>neural</b> <b>network</b> models are presented. The experiments are described and the results and their implications to designing humancomputer interfaces in Decision Support Systems are discussed...|$|R
50|$|The basic {{network is}} a <b>feedforward</b> <b>neural</b> <b>network</b> with {{continuous}} activation. This {{can be extended}} to include spiking units and hypercolumns, representing mutually exclusive or interval coded features. This network {{has been used for}} classification tasks and data mining, for example for discovery of adverse drug reactions. The units can also be connected as a recurrent neural network (losing the strict interpretation of their activations as probabilities) but becoming a possible abstract model of biological neural networks and memory.|$|E
50|$|The Time Delay Neural Network, {{like other}} neural networks, {{operates}} with multiple interconnected layers composed of clusters. These clusters {{are meant to}} represent neurons in a brain and, like the brain, each cluster need only focus on small regions of the input. A proto-typical TDNN has three layers of clusters, one for input, one for output, and the middle layer which handles manipulation of the input through filters. Due to their sequential nature, TDNN’s are implemented as a <b>feedforward</b> <b>neural</b> <b>network</b> instead of a recurrent neural network.|$|E
50|$|A {{probabilistic}} {{neural network}} (PNN) is a four-layer <b>feedforward</b> <b>neural</b> <b>network.</b> The layers are Input, hidden, pattern/summation and output. In the PNN algorithm, the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then, using PDF of each class, the class probability of a new input is estimated and Bayes’ rule is employed to allocate it to the class with the highest posterior probability. It {{was derived from the}} Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It is used for classification and pattern recognition.|$|E
40|$|It {{is clear}} that the {{learning}} speed of <b>feedforward</b> <b>neural</b> <b>networks</b> is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: (1) the slow gradient-based learning algorithms are extensively used to train <b>neural</b> <b>networks,</b> and (2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these conventional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single-hidden layer <b>feedforward</b> <b>neural</b> <b>networks</b> (SLFNs) which randomly chooses hidden nodes and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide good generalization performance at extremely fast learning speed. The experimental results based on a few artificial and real benchmark function approximation and classification problems including very large complex applications show that the new algorithm can produce good generalization performance in most cases and can learn thousands of times faster than conventional popular learning algorithms for <b>feedforward</b> <b>neural</b> <b>networks...</b>|$|R
50|$|Due to the {{inability}} of <b>feedforward</b> <b>Neural</b> <b>Networks</b> to model temporal dependencies, an alternative approach is to use <b>neural</b> <b>networks</b> as a pre-processing e.g. feature transformation, dimensionality reduction, for the HMM based recognition.|$|R
25|$|Deep <b>feedforward</b> <b>neural</b> <b>networks</b> {{were used}} in {{conjunction}} with reinforcement learning by AlphaGo, Google Deepmind's program that was the first to beat a professional human Go player.|$|R
50|$|The Generalized Hebbian Algorithm (GHA), {{also known}} in the {{literature}} as Sanger's rule, is a linear <b>feedforward</b> <b>neural</b> <b>network</b> model for unsupervised learning with applications primarily in principal components analysis. First defined in 1989, it is similar to Oja's rule in its formulation and stability, except it {{can be applied to}} networks with multiple outputs. The name originates because of the similarity between the algorithm and a hypothesis made by Donald Hebb about the way in which synaptic strengths in the brain are modified in response to experience, i.e., that changes are proportional to the correlation between the firing of pre- and post-synaptic neurons.|$|E
5000|$|Extreme {{learning}} {{machines are}} <b>feedforward</b> <b>neural</b> <b>network</b> for classification, regression, clustering, sparse approximation, compression or feature learning {{with a single}} layer or multi layers of hidden nodes, where the parameters of hidden nodes (not just the weights connecting inputs to hidden nodes) need not be tuned. These hidden nodes can be randomly assigned and never updated (i.e. they are random projection but with nonlinear transforms), or can be inherited from their ancestors without being changed. In most cases, the output weights of hidden nodes are usually learned in a single step, which essentially amounts to learning a linear model. The name [...] "extreme learning machine" [...] (ELM) was given to such models by its main inventor Guang-Bin Huang.|$|E
5000|$|A {{probabilistic}} {{neural network}} (PNN) is a <b>feedforward</b> <b>neural</b> <b>network,</b> which is widely used in classification and pattern recognition problems. In the PNN algorithm, the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then, using PDF of each class, the class probability of a new input data is estimated and Bayes’ rule is then employed to allocate the class with highest posterior probability to new input data. By this method, the probability of mis-classification is minimized. This type of ANN {{was derived from the}} Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It was introduced by D.F. Specht in the 1966. In a PNN, the operations are organized into a multilayered feedforward network with four layers: ...|$|E
40|$|Abs t rac t: W e propose an {{architecture}} of a multilayer quadratic perceptron (MLQP) that combines advantages of multilayer perceptrons(MLPs) and higher-order <b>feedforward</b> <b>neural</b> <b>networks.</b> The fea-tures of MLQP are in its simple structure, practical number of adjustable connection weights and powerful learning ability. I n this paper, {{the architecture of}} MLQP is described, a backpropagation learning algorithm for MLQP is derived, and the learning speed of MLQP is compared expen'men-tally with M L P and other two kinds of the second-order <b>feedforward</b> <b>neural</b> <b>networks</b> on pattern classification and function approxamation problems. 1...|$|R
40|$|We use deep <b>feedforward</b> {{artificial}} <b>neural</b> <b>networks</b> {{to approximate}} solutions of partial differential equations of advection and diffusion type in complex geometries. We derive analytical {{expressions of the}} gradients of the cost function {{with respect to the}} network parameters, as well as the gradient of the network itself with respect to the input, for arbitrarily deep networks. The method is based on an ansatz for the solution, which requires nothing but <b>feedforward</b> <b>neural</b> <b>networks,</b> and an unconstrained gradient based optimization method such as gradient descent or quasi-Newton methods. We provide detailed examples on how to use deep <b>feedforward</b> <b>neural</b> <b>networks</b> as a basis for further work on deep <b>neural</b> <b>network</b> approximations to partial differential equations. We highlight the benefits of deep compared to shallow <b>neural</b> <b>networks</b> and other convergence enhancing techniques. Comment: 35 pages, 12 figure...|$|R
40|$|Learning in <b>feedforward</b> <b>neural</b> <b>networks</b> {{may fail}} due to several reasons. We give {{a model for}} the failure caused by {{premature}} saturation of hidden neurons. In order to avoid this type of failure, we suggest evolution strategies for the training of the networks. The properties of the chosen algorithm are demonstrated by some applications. Keywords: <b>feedforward</b> <b>neural</b> <b>networks,</b> gradient based learning, evolutionary algorithms 1 Introduction <b>Feedforward</b> <b>neural</b> <b>networks</b> trained by error backpropagation are a well established tool for engineering and research purposes today. Networks of this kind are known to exhibit universal approximator properties, i. e. they can in principle approximate any continuous function on a compact set using one hidden layer. However, the training method of error backpropagation may fail to adjust the weights properly, even if the chosen structure of the network is well suited to solve the given task. In section 2 we give {{a model for the}} failure caused by pr [...] ...|$|R
