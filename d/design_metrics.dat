411|473|Public
5000|$|This {{method of}} energy {{modeling}} {{has been criticized}} for inaccurately predicting actually energy usage of LEED-certified buildings. The USGBC admits that [...] "current information indicates that most buildings do not perform as well as <b>design</b> <b>metrics</b> indicate. As a result, building owners might not obtain the benefits promised." ...|$|E
50|$|In 1964, Rising {{obtained}} a bachelor's {{degree in chemistry}} at the University of Kansas, in 1984 a Master of Science degree in computer science at Southern Illinois University and in 1987 a M. A. in mathematics at the Southwest Missouri State University. In 1992, Rising obtained her PhD degree in computer science from the Arizona State University, with her thesis entitled Information hiding metrics for modular programming languages relating to object-based <b>design</b> <b>metrics.</b>|$|E
50|$|Founded in 2000 {{with her}} partner Charles Pelly, The Design Academy, Inc. A {{multidisciplinary}} consulting group, dedicated to applying product design and creative knowledge to corporations and organizations through a “Power of Collective Thought”. She is a design strategist helping companies leverage {{the value of}} design and develops guidelines around <b>design</b> <b>metrics.</b> She has postgraduate studies in Marketing at Northeastern University, BS consumer studies at the University of New Hampshire and Associate degree in accounting at Bentley College.|$|E
50|$|In education, Miller {{worked with}} the Los Angeles Unified School District from 1997 to 2000, {{developing}} student achievement goals and strategies, budgets and operating plans, and <b>designing</b> <b>metrics</b> and processes for monitoring school performances.|$|R
40|$|Abstract {{to obtain}} {{cost-effective}} QoS. In this paper, we {{address the issues}} in <b>designing</b> <b>metrics</b> {{that are important in}} evaluating the Quality of Service(QoS) In this paper, we address the issues in <b>designing</b> <b>metrics</b> that are important in evaluating the QoS of video trans-mission. There has been little work in det ermining effec-tive metrics of QoS for video transmission that characterize both cost (revenue generated or service demand) and guar-anteed service. The metrics of analysis and comparison for video transmission must be determined as an end-to-end measure of QoS from video server to end-user(s). By devel-oping these metrics, we hope to enhance the client, server and networking components of a system with monitoring ca-pabilities to measure and evaluate video characterizations. This paper is organized as follows. In Section 2, we dis...|$|R
40|$|The {{topic of}} this master thesis are the Systems for {{managing}} enterprise IT. This paper {{is divided into}} three parts, where the first and second part gives a comprehensive view and concentrates existing knowledge from the examined area. The third part consists of my own conclusions based on available resources. For the overall IT Management, it is necessary to manage and measure IT Performance. Therefore, the first part deals with IT Performance Management (ITPM). Performance Management should penetrate the entire organization and ITPM {{should be part of the}} overall Performance Management of the organization. It is necessary to manage and measure performance at all levels of the organization (strategic, tactical and operational level). To measure the organizational IT performance, it is necessary to <b>design</b> relevant <b>metrics.</b> To <b>design</b> these <b>metrics</b> it is recommendable to follow a framework, whether ITIL or COBIT. The second part of this paper deals with IT Management frameworks and their support for <b>designing</b> <b>metrics.</b> It deals with the new wave in IT [...] ITSM (IT Service Management). The de facto standard for ITSM is ITIL. Furthermore, the second part discusses the de facto standard for IT Governance [...] COBIT and IT Balanced Scorecard for clarifying and translating the vision and strategy into specific objectives. The third part describes a specific procedure for <b>designing</b> <b>metrics</b> according to the ITIL framework. In addition, this part describes the software Metricus which enables a centralized management of ITPM and uses a holistic view of the overall ITPM...|$|R
50|$|The 2014 edition {{made some}} {{improvements}} to the methodology, including adjustments to the corridor definition, infrequent-service penalties, and increased emphasis on basics. In order to allow BRT corridors in downtown areas to qualify as BRT, {{the definition of a}} BRT corridor has been reduced to 3 km in length. The peak and off-peak frequency <b>design</b> <b>metrics</b> have been removed, and penalties for low peak and off-peak frequencies have been added. An additional point was added to each of the BRT basic elements, to put greater emphasis on the basic elements of a BRT corridor.|$|E
5000|$|The Clang parser {{handles the}} {{situation}} {{a completely different}} way, namely by using a non-reference lexical grammar. Clang's lexer does not attempt to differentiate between type names and variable names: it simply reports the current token as an identifier. The parser then uses Clang's semantic analysis library to determine {{the nature of the}} identifier. This allows a much cleaner separation of concerns and encapsulation of the lexer and parser, and is therefore considered [...] a much more elegant solution than The Lexer Hack by most modern software <b>design</b> <b>metrics.</b> This is also the approach used in most other modern languages, which do not distinguish different classes of identifiers in the lexical grammar, but instead defer them to the parsing or semantic analysis phase, when sufficient information is available.|$|E
40|$|The <b>Design</b> <b>Metrics</b> {{research}} {{team has been}} evaluating design documentation and calculating a number of <b>design</b> <b>metrics.</b> <b>Design</b> <b>metrics</b> are obtained from the artifacts available in the design stage of the software life cycle. Until recently the <b>Design</b> <b>Metrics</b> {{research team}} has hand calculated and recorded these metrics. A Software Design Analyzer (SDA) has been written to automate some of these calculations. The SDA recognizes the design elements of a software system (i. e., structure charts or DFDs), searches for the relevant information in the design in order to calculate the metrics, and deposits the information into a repository for later reference. Automatically retrieving the design information from a CASE (Computer Aided Software Engineering) tool storehouse allows the research team {{to concentrate on the}} formulation, calibration and analysis of the <b>design</b> <b>metrics.</b> The incorporation of CASE technology provides an existing platform for the transfer of the <b>design</b> <b>metrics</b> into industry [...] . ...|$|E
50|$|His {{research}} interests are object-oriented analysis and <b>design,</b> object-oriented <b>metrics,</b> agent-oriented methodologies, and {{the migration of}} organizations to object technology.|$|R
40|$|Metrics {{in academia}} are often an opaque mess, filled with biases and ill-judged {{assumptions}} {{that are used}} in overly deterministic ways. By getting involved with their design, academics can productively push metrics in a more transparent direction. Chris Elsden, Sebastian Mellor and Rob Comber introduce an example of <b>designing</b> <b>metrics</b> within their own institution. Using the metric of grant income, their tool ResViz shows a chord diagram of academic collaboration and aims to encourage a multiplicity of interpretations...|$|R
40|$|Master {{thesis is}} {{concerned}} with assessing the state of information security and its use {{as an indicator of}} corporate performance in energy company. Chapter analysis of the problem and current situation presents findings on the state of information security and implementation stage of ISMS. The practical part is focused on risk analysis and assessment the maturity level of processes, which are submitted {{as the basis for the}} proposed security measures and recommendations. There are also <b>designed</b> <b>metrics</b> to measure level of information security...|$|R
40|$|The <b>design</b> <b>metrics</b> {{developed}} by the <b>Design</b> <b>Metrics</b> team at Ball State University are a suite of metrics {{that can be applied}} during the design phase of software development. The benefit of the metrics {{lies in the fact that}} the metrics can be applied early in the software development cycle. The suite includes the external design metric De,the internal design metric D 27 D(G), the design balance metric DB, and the design connectivity metric DC. The suite of <b>design</b> <b>metrics</b> have been applied to large-scale industrial software as well as student projects. Bell Communications Research of New Jersey has made available a software system that can be used to apply <b>design</b> <b>metrics</b> to large-scale telecommunications software. This thesis presents the suite of <b>design</b> <b>metrics</b> and attempts to determine if the characteristics of telecommunications software are accurately reflected in the conventions used to compute the metrics. Department of Computer ScienceThesis (M. S. ...|$|E
40|$|In {{the last}} decade, {{empirical}} studies on object-oriented <b>design</b> <b>metrics</b> have shown {{some of them}} to be useful for predicting the fault-proneness of classes in object-oriented software systems. This research did not, however, distinguish among faults according to the severity of impact. It would be valuable to know how object-oriented <b>design</b> <b>metrics</b> and class fault-proneness are related when fault severity is taken into account. In this paper, we use logistic regression and machine learning methods to empirically investigate the usefulness of object-oriented <b>design</b> <b>metrics,</b> specifically, a subset of the Chidamber and Kemerer suite, in predicting fault-proneness when taking fault severity into account. Our results, based on a public domain NASA data set, indicate that 1) most of these <b>design</b> <b>metrics</b> are statistically related to fault-proneness of classes across fault severity, and 2) the prediction capabilities of the investigated metrics greatly depend on the severity of faults. More specifically, these <b>design</b> <b>metrics</b> are able to predict low severity faults in fault-prone classes better than high severity faults in fault-prone classes. Department of Computin...|$|E
40|$|In this paper, {{we report}} on a machine {{learning}} approach to condensing class diagrams. The goal of the algorithm is to learn to identify what classes are most relevant {{to include in the}} diagram, as opposed to full reverse engineering of all classes. This paper focuses on building a classifier that is based on the names of classes in addition to <b>design</b> <b>metrics,</b> and we compare to earlier work that is based on <b>design</b> <b>metrics</b> only. We assess our condensation method by comparing our condensed class diagrams to class diagrams that were made during the original forward design. Our results show that combining text metrics with <b>design</b> <b>metrics</b> leads to modest improvements over using <b>design</b> <b>metrics</b> only. On average, the improvement reaches 5. 3 %. 7 out of 10 evaluated case studies show improvement ranges from 1 % to 22 %...|$|E
40|$|Abstract — <b>Designing</b> routing <b>metrics</b> is {{critical}} for performance in wireless mesh networks. The unique characteristics of mesh networks, such as static nodes and the shared nature of the wireless medium, invalidate existing solutions from both wired and wireless networks and impose unique requirements on <b>designing</b> routing <b>metrics</b> for mesh networks. In this paper, we focus on identifying these requirements. We first analyze the possible types of routing protocols {{that can be used}} and show that proactive hop-by-hop routing protocols are the most appropriate for mesh networks. Then, we examine the requirements for <b>designing</b> routing <b>metrics</b> according to the characteristics of mesh networks and the type of routing protocols used. Finally, we study several existing routing metrics, including hop count, ETX, ETT, WCETT and MIC in terms of their ability to satisfy these requirements. Our simulation results of the performance of these metrics confirm our analysis of these metrics. I. ...|$|R
40|$|Aspect-Orientation (AO) aims at modularizing {{crosscutting}} {{concerns in}} software systems. It builds on current technologies, like object-orientation. Software metrics are means of qualifying software <b>designs.</b> <b>Metrics</b> for AO {{are crucial to}} determine {{the effectiveness of this}} emerging paradigm. Some OO metrics can be used to measure AO systems, since AO is an extension to OO. In this paper, we describe the Chidamber and Kemerer (C&K) metrics suite, a suite of metrics for evaluating objectoriented designs. We discuss the effect of AO on these metrics, and suggest some future work for defining AO metrics...|$|R
40|$|Metrics are an {{important}} technique in quantifying desirable software and software development characteristics of aspect- oriented software development (AOSD). Over the last few years, {{a growing number of}} studies have explored how Aspect-Oriented Programming (AOP) might impact software maintainability. In this paper, we present a systematic review of recent AO programs and <b>designs</b> <b>metrics</b> studies. We look at attributes most frequently used as indicators of maintainability in current aspect-oriented (AO) programs. In this review work consolidates data from recent research results, highlights circumstances when the applied metrics suitable to AO programs and draws attention to deficiencies where AO metrics need to be improved...|$|R
40|$|The <b>Design</b> <b>Metrics</b> Research Team in the Computer Science Department at Ball State University {{has been}} engaged in {{developing}} and validating quality <b>design</b> <b>metrics</b> since 1987. Since then a number of <b>design</b> <b>metrics</b> have been developed and validated. One of the <b>design</b> <b>metrics</b> developed by the research team is design balance (DB). This thesis {{is an attempt to}} validate the metric DB. In this thesis, results of the analysis of five systems are presented. The main objective of this research is to examine if DB can be used to evaluate the complexity of a software design and hence the quality of the resulting software. Two of the five systems analyzed were student projects and the remaining three were from industry. The five systems analyzed were written in different languages, had different sizes and exhibited different error rates. Department of Computer ScienceThesis (M. S. ...|$|E
40|$|The {{size and}} {{complexity}} of web applications is increasing at an extremely rapid rate. Many web applications have evolved from simple HTML pages to complex serviceoriented applications that have high maintenance costs. UML web <b>design</b> <b>metrics</b> are used to gauge whether the maintainability cost of the system can be controlled by correlating the UML <b>design</b> <b>metrics</b> to different measures of maintainability. This research empirically explores the relationships between existing UML <b>design</b> <b>metrics</b> based on Conallen’s extension for web applications and maintenance effort. This research is evaluated, through an empirical case study of an industrial web application from the telecommunications domain. ...|$|E
40|$|This paper {{discusses}} a metrics {{approach for}} analyzing software designs that helps designers engineer quality {{into the design}} product. These metrics gauge project quality as well as design complexity at all times during the design phase. The metrics are developed from primitive <b>design</b> <b>metrics</b> which are predictive, objective and automatable. The architectural <b>design</b> <b>metrics</b> used are comprised of terms related {{to the amount of}} data flowing through the module and the number of invocation sequences through the module. A detailed <b>design</b> <b>metrics</b> component takes into account the structure and complexity of a module. This paper presents empirical results to illustrate the metrics' success in identifying stress points in a software design and demonstrate their relationship {{to the quality of the}} resulting software. To automate the calculation of the <b>design</b> <b>metrics</b> in the Rational environment, DIANA (Descriptive Intermediate Attributed Notation for Ada) was utilized. Provided in the environment a [...] ...|$|E
40|$|AbstractThis paper {{discusses}} metrics for {{the effectiveness}} of learning of serious games in corporate training. Existing evaluation models are examined in order to verify their applicability to modern organizations in the knowledge economy. <b>Designing</b> <b>metrics</b> for learning requires taking into account different stakeholders, such as the employees, the employers and the management for the financial side. Game builders can also benefit from metrics that relate known game features, such as immersion, to learning effectiveness. Such metrics would allow an early assessment of the suitability of a game for training, thereby reducing {{the consequences of a}} wrong design and the development costs...|$|R
40|$|Abstract. In this paper, we {{introduce}} a benchmark to test efficiency of RDF data model for data storage and querying {{in relation to}} a concrete dataset. We created Czech DBpedia- a freely available dataset composed of data extracted from Czech Wikipedia. But during creation and querying of this dataset, we faced problems caused by a lack of performance of used RDF storage. We <b>designed</b> <b>metrics</b> to measure efficiency of data storage approaches. Our metric quantifies the impact of data decomposition in RDF triples. Results of our benchmark applied to the dataset of Czech DBpedia are presented...|$|R
5000|$|Sociology and Economy {{of music}} — music {{industry}} {{and use of}} MIR in the production, distribution, consumption chain, user profiling, validation, user needs and expectations, evaluation of music IR systems, building test collections, experimental <b>design</b> and <b>metrics</b> ...|$|R
40|$|The {{development}} {{of high quality}} software the first time, greatly depends upon the ability to judge the potential quality of the software early in the life cycle. The Software Engineering Research Center <b>design</b> <b>metrics</b> research team at Ball State University has developed a metrics approach for analyzing software designs. Given a design, these metrics highlight stress points and determine overall design quality. The {{purpose of this study}} is to analyze multiple software releases of the Advanced Field Artillery Tactical Data System (AFATDS) using <b>design</b> <b>metrics.</b> The focus is on examining the transformations of <b>design</b> <b>metrics</b> at each of three releases of AFATDS to determine the relationship of <b>design</b> <b>metrics</b> to the complexity and quality of a maturing system. The software selected as a test case for this research is the Human Interface code from Concept Evaluation Phase releases 2, 3, and 4 of AFATDS. To automate the metric collection process, a metric tool called the Design Metric Analyzer was developed. Further analysis of <b>design</b> <b>metrics</b> data indicated that the standard deviation and mean for the metric was higher for release 2, relatively lower for release 3, and again higher for release 4. Interpreting this means that there was a decrease in complexity and an improvement in the quality of the software from release 2 to release 3 and an increase in complexity in release 4. Dialog with project personnel regarding <b>design</b> <b>metrics</b> confirmed most of these observations. Department of Computer ScienceThesis (M. S. ...|$|E
30|$|The key <b>design</b> <b>metrics</b> under {{different}} system parameters are numerically illustrated as follows.|$|E
40|$|The {{goal of this}} {{research}} is to provide a tool to unobtrusively introduce the <b>design</b> <b>metrics</b> D,, and Di into existing engineering practices. To that goal, different methods for visualizing the UML <b>design</b> <b>metrics</b> were explored. A small survey was conducted to determine which of the different visualizations is most attractive. The results of this survey were used to guide the design of the <b>Design</b> <b>Metrics</b> Visualization Plugin (Design MVP), which was developed as a plugin for the open-source UML tool Dia. This plugin allows for the calculation, and more importantly, visualization of the UML metrics during UML class diagram modeling. Department of Computer ScienceThesis (M. S. ...|$|E
40|$|Although the {{original}} OMG Model-Driven Architecture Approach is {{not concerned with}} software evolution, model-driven techniques may be good candidates to ease software evolution. However, a systematic evaluation of the bene-fits and drawback of model-driven approaches compared to other approaches are lacking. Besides maintainability other quality attributes of the software are of interest, in partic-ular performance metrics. One specific area where model driven approaches are established {{in the area of}} software evolution are the generation of adapters to persist modern object oriented business models with legacy software and databases. This paper presents a testbed and an evalu-ation process with specifically <b>designed</b> <b>metrics</b> to evalu-ate model-driven techniques regarding their maintainabil-ity and performance against established persistency frame-works. ...|$|R
40|$|Hewlett-Packard’s Imaging & Printing Group (IPG) is {{charting}} {{a course}} towards environmental leadership in its markets. To do this, IPG must look beyond just satisfying the regulations and identify opportunities for groundbreaking improvement. Carefully <b>designed</b> <b>metrics</b> {{are necessary to}} guide design, chart progress and set goals in this effort. IPG’s Environmental Strategy Team is leading an initiative to establish these metrics internally. This paper describes the development process the authors followed to construct the initial metrics, which {{are focused on the}} “carbon footprint ” of products under development. The paper also discusses the lessons learned developing the initial metrics, the results achieved thus far, implementation details, challenges, and future opportunities for improvement...|$|R
50|$|HEART is a {{framework}} for mobile <b>design</b> and <b>metrics.</b> It is an acronym that stands for happiness, engagement, adoption, retention and task success. Included by Gartner as a design approach that accommodates mobile interface issues such as partial user attention and interruption.|$|R
40|$|The U. S. Army Information Systems Engineering Command {{maintains}} over 100 standard Army {{management information}} systems. The need {{has been identified}} to modernize these systems. One way is to upgrade these systems from a COBOL, flat file, batch processing mode to systems written in Ada {{in order to increase}} functionality, maintainability and reusability [HOBB 90]. Through the SERC <b>Design</b> <b>Metrics</b> Research Project, we have developed a metrics approach for analyzing software designs which helps designers engineer quality into the design product. This paper discusses the calculation and analysis of our <b>design</b> <b>metrics</b> on the COBOL and Ada systems received from AIRMICS. This analysis was automated through the use of support tools that our research team developed for COBOL and through our Design Metric Analyzer (DMA) for Ada. The analyses of our metric results shed light on the design quality and design balance of the COBOL and Ada systems. <b>Design</b> <b>Metrics</b> The <b>Design</b> <b>Metrics</b> Research Team at Ba [...] ...|$|E
40|$|The {{distributed}} {{minority and}} majority voting based redundancy (DMMR) scheme was recently proposed as an efficient {{alternative to the}} conventional N-modular redundancy (NMR) scheme for the physical design of mission/safety-critical circuits and systems. The DMMR scheme enables significant improvements in fault tolerance and <b>design</b> <b>metrics</b> compared to the NMR scheme albeit {{at the expense of}} a slight decrease in the system reliability. In this context, this paper studies the system reliability, fault tolerance and <b>design</b> <b>metrics</b> tradeoffs in the DMMR scheme compared to the NMR scheme when the majority logic group of the DMMR scheme is increased in size relative to the minority logic group. Some example DMMR and NMR systems were realized using a 32 / 28 nm CMOS process and compared. The results show that 5 -of-M DMMR systems have a similar or better fault tolerance whilst requiring similar or fewer function modules than their counterpart NMR systems and simultaneously achieve optimizations in <b>design</b> <b>metrics.</b> Nevertheless, 3 -of-M DMMR systems have the upper hand with respect to fault tolerance and <b>design</b> <b>metrics</b> optimizations than the comparable NMR and 5 -of-M DMMR systems. With regard to system reliability, NMR systems are closely followed by 5 -of-M DMMR systems which are closely followed by 3 -of-M DMMR systems. The verdict is 3 -of-M DMMR systems are preferable to implement higher levels of redundancy from a combined system reliability, fault tolerance and <b>design</b> <b>metrics</b> perspective to realize mission/safety-critical circuits and systems...|$|E
40|$|A metrics {{approach}} for analyzing software designs that helps designers engineer quality {{into the design}} product has been developed in previous research by Zage and Zage[1]. Three of the <b>design</b> <b>metrics</b> developed are an external design metric D e, which focuses on a module's external relationships to other modules in the software system, and an internal design metric D i, which incorporates factors related to a module's internal structure, and a composite design metric D(G) that is the linear combination of D e and D i. Over a sevenyear metrics evaluation and validation period, on study data consisting of university-based projects and large-scale industrial software, these <b>design</b> <b>metrics</b> consistently proved to be excellent predictors of error-prone modules. In this paper, we {{address the problem of}} identifying error-prone modules during the design phase of a telecommunications software system in order to detect and resolve design problems at an early stage. This paper validates the three metrics examined as effective predictors of error proneness. 1. Introduction to the <b>Design</b> <b>Metrics</b> D e, D i and D(G) In the <b>design</b> <b>metrics</b> project, the research team at Ball State University began analyzin...|$|E
5000|$|SPaM pioneered {{and leads}} {{innovation}} in supply chain and procurement practices. [...] They created dramatic improvements in manufacturing, distribution, procurement, product <b>design,</b> forecasting, <b>metrics,</b> and inventory controlefficiencies; {{leading to the}} publication and adoption of their methods outside HP. [...] Notable contributions include: ...|$|R
40|$|Research is {{dedicated}} to development of artificial neural net's method of software quality evaluation and prediction, which provides the realization of comparative analysis of different project versions and selection {{of the best of}} them accordance its characteristics on the basis of <b>design</b> stage <b>metrics</b> analysis...|$|R
40|$|Considering the {{software}} evaluation methods analysis results conclusion was drawn, that the perspective research direction is development of intelligent systems, {{which will be}} analyzed and processed the <b>design</b> stage <b>metrics</b> analysis results and will be provided the project evaluation and the designed software characteristics prediction. Intelligence System of Software Complexity and Quality Evaluation and Prediction (ISCQEP) is designed to the evaluation of design stage results and prediction of software complexity and quality characteristics {{on the basis of}} processing of <b>design</b> stage <b>metrics</b> with exact and predicted values. Quantitative exact and predicted values of <b>design</b> stage <b>metrics</b> is given to the ISCQEP input, and conclusions about the project and designed software complexity and quality are the results of the system functioning. Today the main parameters in the selection of software project version are the design cost and time and designing company reputation, but a decisions {{on the basis of these}} parameters are not always guarantee the proper software quality. ISCQEP conclusions allow to compare the different project versions, when the cost and time is approximately equal. The proposed intelligence system of software complexity and quality evaluation and prediction provides the motivated and grounded decision about selection of project on the basis not only cost and time, but also considering project and designed software complexity and quality...|$|R
