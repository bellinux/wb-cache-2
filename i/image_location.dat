247|1095|Public
25|$|In {{some cases}} S2 is negative, {{indicating}} that the image is formed {{on the opposite side}} of the lens from where those rays are being considered. Since the diverging light rays emanating from the lens never come into focus, and those rays are not physically present at the point where they appear to form an image, this is called a virtual image. Unlike real images, a virtual image cannot be projected on a screen, but appears to an observer looking through the lens as if it were a real object at the location of that virtual image. Likewise, it appears to a subsequent lens as if it were an object at that location, so that second lens could again focus that light into a real image, S1 then being measured from the virtual <b>image</b> <b>location</b> behind the first lens to the second lens. This is exactly what the eye does when looking through a magnifying glass. The magnifying glass creates a (magnified) virtual image behind the magnifying glass, but those rays are then re-imaged by the lens of the eye to create a real image on the retina.|$|E
50|$|Servio {{carries out}} at-scale {{optimization}} of existing content, including rewriting, title tag creation, keyword research, interlinking, and <b>image</b> <b>location.</b>|$|E
5000|$|Laser aim - Produce a {{red light}} beam for aiming where the <b>image</b> <b>location</b> is to be taken from ...|$|E
5000|$|... #Caption: CTX context <b>image</b> showing <b>location</b> of {{next two}} HiRISE <b>images.</b> <b>Location</b> is Ismenius Lacus quadrangle.|$|R
5000|$|... which {{minimizes}} {{the sum of}} the squares of the distances from the projected model locations to the corresponding <b>image</b> <b>locations.</b>|$|R
5000|$|... #Caption: Gullies {{and massive}} flow of material, {{as seen by}} HiRISE under HiWish program. Gullies are {{enlarged}} in next two <b>images.</b> <b>Location</b> is Bamberg crater.|$|R
5000|$|... #Caption: Gullies {{and massive}} flow of material, {{as seen by}} HiRISE under HiWish program. Gullies are {{enlarged}} in next <b>image.</b> <b>Location</b> is Bamberg crater ...|$|E
5000|$|... #Caption: Wide view {{of group}} of gullies, {{as seen by}} HiRISE under HiWish program. Note that part of this image is {{enlarged}} in the following <b>image.</b> <b>Location</b> is Diacria quadrangle.|$|E
5000|$|... #Caption: Depressions with {{straight}} southern walls, {{as seen by}} HiRISE under HiWish program. Box indicates part enlarged in images below. <b>Image</b> <b>location</b> is in Milankovič crater in Diacria quadrangle. These depressions may {{be an example of}} scalloped topography.|$|E
40|$|In {{order to}} obtain the best {{possible}} match between two diffusion tensor (DT) images, {{it is important to}} use an appropriate similarity measure to drive the image registration. A numerical measure of similarity is obtained by comparing the data values at corresponding <b>image</b> <b>locations.</b> For scalar <b>images</b> the simplest approach is to use the difference in scalar intensity at corresponding <b>image</b> <b>locations</b> but many others have been proposed that generally produce better results [1]. In the case of DT image matching, a comparative measure of similarity between diffusion tensors is required to drive the registration. Many diffusion tensor similarity measures hav...|$|R
40|$|This paper {{addresses}} {{the challenge of}} 3 D full-body human pose estimation from a monocular image sequence. Here, two cases are considered: (i) the <b>image</b> <b>locations</b> of the human joints are provided and (ii) the <b>image</b> <b>locations</b> of joints are unknown. In the former case, a novel approach is introduced that integrates a sparsity-driven 3 D geometric prior and temporal smoothness. In the latter case, the former case is extended by treating the <b>image</b> <b>locations</b> of the joints as latent variables. A deep fully convolutional network is trained to predict the uncertainty maps of the 2 D joint locations. The 3 D pose estimates are realized via an Expectation-Maximization algorithm over the entire sequence, where it is shown that the 2 D joint location uncertainties can be conveniently marginalized out during inference. Empirical evaluation on the Human 3. 6 M dataset shows that the proposed approaches achieve greater 3 D pose estimation accuracy over state-of-the-art baselines. Further, the proposed approach outperforms a publicly available 2 D pose estimation baseline on the challenging PennAction dataset. Comment: Published in CVPR 201...|$|R
40|$|A new {{hybrid of}} feature-based and intensity-based {{registration}} is presented. The algorithm reflects {{a new understanding}} of the role of alignment error in the generation of registration constraints. This leads to an iterative process where distinctive <b>image</b> <b>locations</b> from the moving image are matched against the intensity structure of the fixed image. The search range of this matching process is controlled by both the uncertainty in the current transformation estimate and the properties of the <b>image</b> <b>locations</b> to be matched. The resulting hybrid algorithm is applied to retinal image registration by incorporating it as the main estimation engine within our recently published Dual-Bootstrap ICP algorithm...|$|R
50|$|Besides, {{directly}} {{around the}} label a buffer zone is present that enhances the seek {{performance for the}} adjacent user data zones. Finally, a sufficient DPD tracking signal is required at the <b>image</b> <b>location.</b> For that reason, the bright and dark pixels are created by recording marks and spaces within the EFM+-runlength range.|$|E
50|$|Binocular {{disparity}} {{refers to}} the difference in <b>image</b> <b>location</b> of an object seen by {{the left and right}} eyes, resulting from the eyes’ horizontal separation (parallax). The brain uses binocular disparity to extract depth information from the two-dimensional retinal images in stereopsis. In computer vision, binocular disparity {{refers to the}} difference in coordinates of similar features within two stereo images.|$|E
50|$|The <b>image</b> <b>location</b> {{and size}} {{can also be}} found by {{graphical}} ray tracing, as illustrated in the figures above. A ray drawn {{from the top of the}} object to the surface vertex (where the optical axis meets the mirror) will form an angle with that axis. The reflected ray has the same angle to the axis, but is below it (See Specular reflection).|$|E
30|$|Finally, spatial {{filtering}} step requires (4 + (2 K + 1) 2)L additions, 6 L subtractions, 3 L divisions and 4 L multiplications per <b>image,</b> <b>locations,</b> where K is the window size and L {{is the number}} of image pixels.|$|R
40|$|Determining {{the pose}} of a mobile device based on visual in-formation is a {{promising}} approach {{to solve the}} indoor local-ization problem. We present an approach that transforms lo-calized images along a mapping trajectory into virtual view-points that cover a set of densely sampled camera positions and orientations in a confined environment. The viewpoints are represented by their respective bag-of-features vectors and image retrieval techniques are applied to determine the most likely pose of query images at very low computational complexity. As virtual <b>image</b> <b>locations</b> and orientations are decoupled from actual <b>image</b> <b>locations,</b> the system is {{able to work with}} sparse reference imagery and copes well with per-spective distortion. Experiments confirm that pose retrieval performance is significantly improved...|$|R
40|$|Deep {{convolutional}} {{neural networks}} (CNN) have revolutionized various {{fields of vision}} research and have seen unprecedented adoption for multiple tasks such as classification, detection, captioning, etc. However, they offer little transparency into their inner workings and are often treated as black boxes that deliver excellent performance. In this work, we aim at alleviating this opaqueness of CNNs by providing visual explanations for the network's predictions. Our approach can analyze variety of CNN based models trained for vision applications such as object recognition and caption generation. Unlike existing methods, we achieve this via unraveling the forward pass operation. Proposed method exploits feature dependencies across the layer hierarchy and uncovers the discriminative <b>image</b> <b>locations</b> that guide the network's predictions. We name these locations CNN-Fixations, loosely analogous to human eye fixations. Our approach is a generic method that requires no architectural changes, additional training or gradient computation and computes the important <b>image</b> <b>locations</b> (CNN Fixations). We demonstrate {{through a variety of}} applications that our approach is able to localize the discriminative <b>image</b> <b>locations</b> across different network architectures, diverse vision tasks and data modalities. Comment: Codes are available at [URL]...|$|R
50|$|Previous steps found keypoint {{locations}} {{at particular}} scales and assigned orientations to them. This ensured invariance to <b>image</b> <b>location,</b> scale and rotation. Now {{we want to}} compute a descriptor vector for each keypoint such that the descriptor is highly distinctive and partially invariant to the remaining variations such as illumination, 3D viewpoint, etc. This step is performed on the image closest in scale to the keypoint's scale.|$|E
5000|$|Navigation is {{primarily}} between still images of street scenes and building fronts. Moving the mouse raised a small [...] "GO" [...] icon in directions you may progress, bringing up another still <b>image</b> <b>location.</b> Some of these icons appear on entrances to public locations; a movie theatre, restaurants, clubs, etc. Clicking on these {{takes you to}} an interior scene including people. Rolling over {{some of these people}} raised a [...] "TALK" [...] icon, indicating that clicking on them will instigate a [...] "conversation".|$|E
5000|$|The LabelMe dataset {{has some}} {{problems}} {{that should be}} noted. Some are inherent in the data, such as the objects in the images not being uniformly distributed with respect to size and <b>image</b> <b>location.</b> This {{is due to the}} images being primarily taken by humans who tend to focus the camera on interesting objects in a scene. However, cropping and rescaling the images randomly can simulate a uniform distribution. Other problems are caused by the amount of freedom given to the users of the annotation tool. Some problems that arise are: ...|$|E
30|$|Feature points (also called {{interest}} points) {{are characteristic}} points in an image. They are stable and distinctive <b>image</b> <b>locations,</b> and have high information content. Feature points {{are useful in}} many research areas such as content-based image retrieval [15]. Numerous approaches for feature point detection exist in the literature. Corners are highly informative <b>image</b> <b>locations,</b> and they are considered as good candidates for feature points. The early work of using corners for image matching is the study by Moravec [16] on stereo matching. Among the most popular corner detectors, the Harris corner detector [17] {{is known to be}} robust against camera noise, image rotation, and illumination changes [18]. Using Harris corners as feature points has been proved to be effective for image matching applications [5, 19, 20].|$|R
40|$|In {{this work}} we use Branch-and-Bound (BB) to {{efficiently}} detect objects with deformable part models. Instead of evaluating the classifier score exhaustively over <b>image</b> <b>locations</b> and scales, we use BB {{to focus on}} promising <b>image</b> <b>locations.</b> The core problem is to compute bounds that accommodate part deformations; for this we adapt the Dual Trees data structure [7] to our problem. We evaluate our approach using Mixture-of-Deformable Part Models [4]. We obtain exactly the same results but are 10 - 20 times faster on average. We also develop a multiple-object detection variation of the system, where hypotheses for 20 categories are inserted in a common priority queue. For the problem of finding the strongest category in an image {{this results in a}} 100 -fold speedup. ...|$|R
3000|$|The second {{synthetic}} SV filter (BLUR 2) is a Gaussian variant filter. Its height grows as {{a factor}} of two from the left to right of the image, while its width keeps constant for all <b>image</b> <b>locations.</b> At the <b>image</b> center we have an isotropic blur with σ [...]...|$|R
5000|$|... where [...] is {{the rate}} of fire, [...] is a {{function}} of one type of information input and [...] is another. For example, neural activity for the interaction between gaze direction and retinal <b>image</b> <b>location</b> is almost exactly multiplicative, where [...] represents the location of a stimulus in retinal coordinates and [...] represents gaze angle. The primary process by which this interaction can take place is speculated to be recurrent neural networks where neural connections form a directed cycle. Recurrent circuitry is abundant in cortical networks and reportedly plays a role in sustaining signals, signal amplification, and response selectivity.|$|E
50|$|A second ray can {{be drawn}} {{from the top of the}} object passing through the focal point and {{reflecting}} off the mirror at a point somewhere below the optical axis. Such a ray will be reflected from the mirror as a ray parallel to the optical axis. The point at which the two rays described above meet is the image point corresponding to the top of the object. Its distance from the axis defines the height of the image, and its location along the axis is the <b>image</b> <b>location.</b> The mirror equation and magnification equation can be derived geometrically by considering these two rays.|$|E
5000|$|The {{principal}} ray or chief ray (sometimes {{known as the}} b ray) in an optical system is the meridional ray that starts {{at the edge of}} the object, and passes through the center of the aperture stop. This ray crosses the optical axis at the locations of the pupils. As such chief rays are equivalent to the rays in a pinhole camera. The distance between the chief ray and the optical axis at an <b>image</b> <b>location</b> defines the size of the image. The marginal and chief rays together define the Lagrange invariant, which characterizes the throughput or etendue of the optical system. Some authors define a [...] "{{principal ray}}" [...] for each object point. The principal ray starting at a point on the edge of the object may then be called the marginal principal ray.|$|E
40|$|A {{technique}} for {{the determination of}} 3 -dimensional coordinates of spherical metallic markers implanted in bone has been developed. The measurements of marker <b>image</b> <b>locations</b> in X-ray pictures made by two sources simultaneously, are processed by computer to give coordinates accurate to within {{a fraction of a}} millimeter...|$|R
5000|$|... #Caption: <b>Image</b> and <b>location</b> of Greenwald Island Memorial Rock ...|$|R
40|$|We {{present a}} system for {{semi-automatic}} medical image segmentation based on the livewire paradigm. Livewire is an image-feature driven method that finds the optimal path between user-selected <b>image</b> <b>locations,</b> thus reducing the need to manually define the complete boundary. Standard features used by the wire to find boundaries include gray values and gradients. We introduc...|$|R
5000|$|In third-order astigmatism, the {{sagittal}} and transverse rays form foci {{at different}} distances along the optic axis. These foci {{are called the}} sagittal focus and the transverse focus, respectively. In the presence of astigmatism, an off-axis point on the object is not sharply imaged by the optical system. Instead, sharp lines are formed at the sagittal and transverse foci. The image at the transverse focus is a short line, oriented {{in the direction of}} the sagittal plane; images of circles centered on the optic axis, or lines tangential to such circles, will be sharp in this plane. The image at the sagittal focus is a short line, oriented in the tangential direction; images of spokes radiating from the center are sharp at this focus. In between these two foci, a round but [...] "blurry" [...] image is formed. This is called the medial focus or circle of least confusion. This plane often represents the best compromise <b>image</b> <b>location</b> in a system with astigmatism.|$|E
5000|$|The George B. Wren Supernova Search Telescope (SNST) at McDonald Observatory {{and the new}} Wren-Marcario Wheelchair Access Telescope (WAT) at the McDonald Observatory Visitor Center (to be {{operational}} early 2007) {{are both}} based on the Pfund configuration. WAT is unique in that it will employ two 18" [...] f/8 mirrors arranged on a north-south line and facing each other, with the steering flat halfway between. The north 18" [...] mirror covers the northern half-hemisphere of the sky, and the south 18" [...] mirror covers the south sky, thus providing full sky coverage, which is not possible with a single-mirror Pfund. The 24" [...] steering flat and viewing port assembly rotate in azimuth to either mirror. Each half-hemisphere has its own fixed <b>image</b> <b>location.</b> WAT is fully compliant with all Americans with Disabilities Act (ADA) requirements and will provide both fully and differently abled visitors to McDonald Observatory with superb and comfortable viewing. A detailed Wikipedia article on the WAT will be included {{as soon as the}} instrument is operational, expected in early 2009.|$|E
50|$|In {{some cases}} S2 is negative, {{indicating}} that the image is formed {{on the opposite side}} of the lens from where those rays are being considered. Since the diverging light rays emanating from the lens never come into focus, and those rays are not physically present at the point where they appear to form an image, this is called a virtual image. Unlike real images, a virtual image cannot be projected on a screen, but appears to an observer looking through the lens as if it were a real object at the location of that virtual image. Likewise, it appears to a subsequent lens as if it were an object at that location, so that second lens could again focus that light into a real image, S1 then being measured from the virtual <b>image</b> <b>location</b> behind the first lens to the second lens. This is exactly what the eye does when looking through a magnifying glass. The magnifying glass creates a (magnified) virtual image behind the magnifying glass, but those rays are then re-imaged by the lens of the eye to create a real image on the retina.|$|E
40|$|It is {{important}} to protect digital pictures and detect tampered <b>image</b> <b>locations</b> in digital Cyberspace. In this study, we propose an image authentication scheme based on digital signature. The proposed scheme is capable of detecting if certain blocks of an image have been altered. The block can be as small as 86 image pixels...|$|R
40|$|We {{present a}} new, fast and {{efficient}} method for image registration. Our approach applies a novel similarity function on the <b>image</b> <b>locations</b> which have high gradient magnitidues. The similarity function secures a fast convergence. We apply our technique {{to the task}} of affine registration of 2 D images which undergo large rigid motion and show good results...|$|R
5000|$|... #Caption: CTX context <b>image</b> showing <b>location</b> of next HiRISE image (letter A box).|$|R
