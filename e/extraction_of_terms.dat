33|10000|Public
40|$|International audienceThe {{automatic}} {{development of}} termino-logical databases, {{especially in a}} standardized format, has a crucial aspect for multiple applications related to technical and scientific knowledge that requires semantic and terminological descriptions covering multiple domains. In this context, we have two challenges: {{the first is the}} automatic <b>extraction</b> <b>of</b> <b>terms</b> in order to build a terminological database, and the second challenge is their normalization into a standardized format. To deal with these challenges, we propose an approach based on a cascade of transducers performed using CasSys tool of Unitex platform that benefits from both: the success of the rule-based approach for the <b>extraction</b> <b>of</b> <b>terms,</b> and the performance of the TMF standard for the representation of terms. We have tested and evaluated our approach on an Arabic scientific and technical documents for the Elevator domain and the results are very encouraging...|$|E
40|$|An {{approach}} to extracting essential terms and mining intrinsic contexts {{for a large}} amount of documents is studied. An application based on the apriori algorithm is proposed for the <b>extraction</b> <b>of</b> <b>terms</b> and indexing methods for abstraction of a set of documents by the use of latent semantic indexing on a conceptualized document space are discussed...|$|E
40|$|A {{method is}} {{presented}} for deriving the nonrelativistic quantum hamiltonian {{of a free}} massive fermion from the relativistic lagrangian of the Lorentz-violating standard-model extension. It permits the <b>extraction</b> <b>of</b> <b>terms</b> at arbitrary order in a Foldy-Wouthuysen expansion in inverse powers of the mass. The quantum particle hamiltonian is obtained and its nonrelativistic limit is given explicitly to third order. Comment: Accepted in J. Math. Phys., scheduled for December 1999 issu...|$|E
40|$|We {{present the}} work done by Elhuyar Foundation in the field <b>of</b> bilingual {{terminology}} <b>extraction.</b> The aim <b>of</b> this work is to develop some techniques for the automatic <b>extraction</b> <b>of</b> pairs <b>of</b> equivalent <b>terms</b> from Spanish-Basque translation memories, and to implement those techniques in a prototype. Our approach {{is based on a}} previous monolingual <b>extraction</b> <b>of</b> <b>term</b> candidates in each language, the creation of candidate bigrams from both segments of the same translation unit, and, finally, the selection of the most likely pair of candidates, based mainly on statistical information (association measures) and cognates. In the first step, we use linguistic techniques for the <b>extraction</b> <b>of</b> <b>term</b> candidates. The result of our work is ELexBI, a prototype tool that can extract equivalent terms from Spanish-Basque translation memories. This work wants to be a contribution to corpusbased bilingual lexicography and terminology in Basque...|$|R
40|$|The goal of {{this paper}} is to compile a method for multi-word term extraction, taking into account both the {{linguistic}} properties <b>of</b> Bulgarian <b>terms</b> and their statistical rates. The method relies on the <b>extraction</b> <b>of</b> <b>term</b> candidates matching given syntactic patterns followed by statistical (by means of Log-likelihood ratio) and linguistically (by means of inflectional clustering) based filtering aimed at improving the coverage and the precision <b>of</b> multi-word <b>term</b> <b>extraction.</b> ...|$|R
40|$|The paper {{describes}} the method <b>of</b> <b>extraction</b> <b>of</b> two-word domain <b>terms</b> combining their features. The features are computed from three sources: the occurrence statistics in a domain-specific text collection, the statistics of global search engines, and a domainspecific thesaurus. The {{evaluation of the}} approach is based on manually created thesauri. We show {{that the use of}} multiple features considerably improves the automatic <b>extraction</b> <b>of</b> domain-specific <b>terms.</b> We compare the quality of the proposed method in two different domains. ...|$|R
40|$|The {{extraction}} of relevant information in texts constitutes a fundamental process of text mining. We realize this process with a linguistic platform (ILC) for recognition and <b>extraction</b> <b>of</b> <b>terms</b> and their linguistic variants from corpora. We present a methodology {{to enhance the}} recognition of syntactic term variation in English using syntactic and morphosyntactic features. Those modifications contribute to improve filtering of the variants and to assist the expert in validating indexatio...|$|E
40|$|The {{information}} {{used for}} the <b>extraction</b> <b>of</b> <b>terms</b> {{can be considered as}} rather 'inter- nal', i. e. coming from the candidate string itself. This paper presents the incorpora- tion of 'external' information derived from the context of the candidate string. It is embedded to the C-value approach for automatic term recognition (ATR), in the form of weights constructed from statisti- cal characteristics of the context words of the candidate string...|$|E
40|$|In {{the field}} of Text Mining, a key phase in data {{preparation}} {{is concerned with the}} <b>extraction</b> <b>of</b> <b>terms,</b> i. e. collocation of words attached to specific concepts (e. g. Philosophy-Dissertation). In this paper, Term Extraction is formalized as a supervised learning task, extracting a ranking hypothesis from a set of terms labeled as relevant/irrelevant by the expert. This task is tackled using the evolutionary algorithm ROGER, optimizing the area under the ROC curve attached to a ranking hypothesis. Empirical validation on [...] ...|$|E
30|$|After {{detailing}} {{some ways}} for identifying terms, showing {{some examples of}} the difficulty in performing that, we describe and discuss {{in the next section}} the existing approaches for the <b>extraction</b> <b>of</b> candidate <b>terms.</b>|$|R
30|$|The first {{system of}} this nature was TERMINO, {{developed}} for the French language [2]. In Brazil, the beginning of investigations on automatic <b>extraction</b> <b>of</b> (candidate) <b>terms</b> occurred {{at the end of}} the 1990 s [3].|$|R
40|$|KB-N is a web-accessible {{searchable}} Knowledge Bank comprising A) {{a parallel}} corpus of quality assured and calibrated English and Norwegian text drawn from economic-administrative knowledge domains, and B) a domain-focused database representing that knowledge universe in <b>terms</b> <b>of</b> defined concepts {{and their respective}} bilingual terminological entries. A central mechanism in connecting A and B is an algorithm for the automatic <b>extraction</b> <b>of</b> <b>term</b> candidates from aligned translation pairs {{on the basis of}} linguistic, lexical and statistical filtering (first ever for Norwegian). The system is designed and programmed by Paul Meurer at Aksis (UiB). An important pilot application <b>of</b> the <b>term</b> base is subdomain and collocations based word-sense disambiguation for LOGON, a system for Norwegian-to-English MT currently being developed...|$|R
40|$|This paper {{presents}} {{a method for}} incorporating natural language processing into existing text categorization procedures. Three aspects are considered in the investigation: (i) a method for weighting terms based {{on the concept of}} a probability weighted amount of information, (ii) estimation of term occurrence probabilities using a probabilistic language model, and (iii) automatic <b>extraction</b> <b>of</b> <b>terms</b> based on POS tags automatically generated by a morphological analyzer. The e#ects of these considerations are examined in the experiments using Reuters 21578 and NTCIR-J 1 standard test collections. ...|$|E
40|$|Morphologically complex terms {{composed}} from Greek or Latin {{elements are}} frequent in {{scientific and technical}} texts. Word forming units are thus relevant cues for the identification of terms in domainspecific texts. This article describes a method for the automatic <b>extraction</b> <b>of</b> <b>terms</b> relying on the detection of classical prefixes and word-initial combining forms. Word-forming units are identified using a regular expression. The system then extracts terms by selecting words which either begin or coalesce with these elements. Next, terms are grouped in families which are displayed as a weighted list in HTML format. ...|$|E
40|$|In {{this paper}} {{we present a}} {{graph-based}} approach aimed at learning a lexical taxonomy automatically starting from a domain corpus and the Web. Unlike many taxonomy learning approaches in the literature, our novel algorithm learns both concepts and relations entirely from scratch via the automated <b>extraction</b> <b>of</b> <b>terms,</b> definitions and hypernyms. This results in a very dense, cyclic and possibly disconnected hypernym graph. The algorithm then induces a taxonomy from the graph. Our experiments show that we obtain high-quality results, both when building brand-new taxonomies and when reconstructing WordNet sub-hierarchies. ...|$|E
40|$|One of {{the main}} {{challenges}} in biomedical text mining is the identification of terminology, which is a key factor for accessing and integrating the information stored in literature. Manual creation of biomedical terminologies cannot {{keep pace with the}} data that becomes available. Still, many of them have been used in attempts to recognise terms in literature, but their suitability for text mining has been questioned as substantial re-engineering is needed to tailor the resources for automatic processing. Several approaches have been suggested to automatically integrate and map between resources, but the problems of extensive variability of lexical representations and ambiguity have been revealed. In this paper we present a methodology to automatically maintain a biomedical terminological database, which contains automatically extracted terms, their mutual relationships, features and possible annotations that can be useful in text processing. In addition to TermDB, a database used for terminology management and storage, we present the following modules that are used to populate the database: TerMine (recognition, <b>extraction</b> and normalisation <b>of</b> <b>terms</b> from literature), AcroTerMine (<b>extraction</b> and clustering <b>of</b> acronyms and their long forms), AnnoTerm (annotation and classification <b>of</b> <b>terms),</b> and ClusTerm (<b>extraction</b> <b>of</b> <b>term</b> associations and clustering <b>of</b> <b>terms).</b> 1...|$|R
40|$|Studies <b>of</b> {{different}} <b>term</b> extractors on a {{corpus of}} the biomedical domain revealed decreasing performances {{when applied to}} highly technical texts. The difficulty or impossibility of customising them to new domains is an additional limitation. In this paper, we propose to use external terminologies to influence generic linguistic data in order to augment the quality <b>of</b> the <b>extraction.</b> The tool we implemented exploits testified terms at different steps of the process: chunking, parsing and <b>extraction</b> <b>of</b> <b>term</b> candidates. Experiments reported here show that, using this method, more term candidates can be acquired with {{a higher level of}} reliability. We further describe the extraction process involving endogenous disambiguation implemented in the term extractor YaTeA...|$|R
40|$|In {{this paper}} we present an {{approach}} for the <b>extraction</b> <b>of</b> multi-word <b>terms</b> from special language corpora. the new element is {{the incorporation of}} context information for the evaluation <b>of</b> candidate <b>terms.</b> This information is embedded to the C-value method {{in the form of}} statistical weights. 1 Introduction Automatic term recognition (ATR) is the <b>extraction</b> <b>of</b> technical <b>terms</b> from special language corpora with the use of computers. Its applications include specialised dictionary construction and maintenance, human and machine translation, indexing in books and digital libraries, hypertext linking, text categorization etc. ATR also gives the potential to work with large amounts of real data, {{that it would not be}} able to handle manually. We should note that by ATR we neither mean dictionary string matching, nor term interpretation (which deals with the relations between terms and conceptsI). When ATR is concerned with single-word term extraction, domain-dependent linguistic information i [...] ...|$|R
40|$|FROM THE INTRODUCTION (Chapter 27) : This chapter has a twofold objective: (i) {{to present}} {{knowledge}} discovery in text (KDT) as a technique from knowledge engineering for analysis {{and decision making}} in interdisciplinary graduate studies and (ii) to explain ​​how the word clouds that illustrate this book were composed, using the software Wordle (FEINBERG, 2009). The composition of those clouds requires the <b>extraction</b> <b>of</b> <b>terms</b> in the text (including their identification, counting, and organization), which is the initial stage of KDT, besides {{the application of a}} graphic layout algorithm...|$|E
40|$|Automating {{the process}} of term {{recognition}} and classification is important for digital libraries. Automatic Term Recognition (ATR) has many applications in areas related to dig- ital libraries, e. g. information retrieval and extraction from the web, summarisation, machine translation, dictionary construction etc. Automatic term classification attracts the interest of researchers following the steps of ATR. By automatic term classification we mean the grouping of terms into sets whose elements share conceptual properties. In this paper we present C-value/NC-value a method for the automatic <b>extraction</b> <b>of</b> <b>terms,</b> and its further use for the recognition of similarities between them, {{to be used for}} them classification...|$|E
40|$|The {{research}} on {{word sense disambiguation}} (WSD) has great theoretical and practical significance in many fields of natural language processing (NLP). This paper presents an unsupervised approach to Chinese word sense disambiguation based on Hownet (an electronic Chinese lexical resource). In our approach, contexts that include ambiguous words are converted into vectors {{by means of a}} second-order context method, and these context vectors are then clustered by the k-means clustering algorithm. Lastly, the ambiguous words can be disambiguated after a similarity calculation process is completed. Our experiments involved <b>extraction</b> <b>of</b> <b>terms,</b> and an 82. 62 % average accuracy rate was achieved...|$|E
40|$|We {{will present}} a method <b>of</b> <b>extraction</b> <b>of</b> weak signals based on the {{evolutionary}} and structural analysis of the semantic fields. This method employs the following tools: •	Matrix of crossovers involving semantic terms which evolve with time, •	<b>Extraction</b> <b>of</b> emergent <b>terms</b> (for posterior normalization, this are last column selections), •	Matrix of concurrence crossing the emergent terms among them, •	Selection of diagonal bars / blocks, on this matrix, •	<b>Extraction</b> <b>of</b> the blocks that represent emergent and coherent concepts...|$|R
40|$|Windowing {{techniques}} play a {{key role}} in information retrieval. Previous works have suggested that the quality of access to information relies heavily on the characteristics of the windows. This study provides a linguistic approach to text windowing through an <b>extraction</b> <b>of</b> <b>term</b> variants with the help of a partial parser. The syntactic grounding of the method ensures that words observed within restricted spans are lexically related and that spurious word co-occurrences are ruled out with a good level of confidence. The system is computationally tractable on large corpora and large lists <b>of</b> <b>terms.</b> Illustrative examples <b>of</b> <b>term</b> variations from a large medical corpus are given. An experimental evaluation of the method shows that only a small proportion of co-occurring words are lexically related and motivates the call for natural language parsing techniques in text windowing. 1. INTRODUCTION The notion of text window [...] a span of contiguous words within a document [...] is crucial for severa [...] ...|$|R
40|$|In {{this paper}} we discuss morpho-syntactic clues {{that can be used}} to {{facilitate}} terminological processing in Serbian. A method (called SRCE) for automatic <b>extraction</b> <b>of</b> multiword <b>terms</b> is presented. The approach incorporates a set of generic morpho-syntactic filters for recognition <b>of</b> <b>term</b> candidates, a method for conflation of morphological variants and a module for foreign word recognition. Morpho-syntactic filters describe general term formation patterns, and are implemented as generic regular expressions. The inner structure together with th...|$|R
40|$|Technical terms (henceforth called terms), are {{important}} elements for digital libraries. In this paper {{we present a}} domain-independent method for the automatic extraction of multi-word terms, from machinereadable special language corpora. The method, (C-value/NC-value), combines linguistic and statistical information. The rst part, C-value enhances the common statistical measure of frequency of occurrence for term extraction, making it sensitive to a particular typeofmulti-word terms, the nested terms. The second part, NC-value, gives: 1) a method for the extraction of term context words (words that tend to appear with terms), 2) the incorporation of information from term context words to the <b>extraction</b> <b>of</b> <b>terms...</b>|$|E
40|$|The {{identification}} and <b>extraction</b> <b>of</b> <b>terms</b> {{play an important}} role in many areas of knowledge - based applications, such as automatic indexing, knowledge discovery and management, as well as in computational approaches to terminology and lexicography. In this paper, we present EXTra, a tool designed to extract and calculate the degree of termhood of multiword expressions as a function of the statistical distribution of their parts and of the presence of other sub - terms. This work describes EXTra‘s algorithm, and provides the results of its evaluation on a task of term extraction from an Italian corpus of documents belonging to the domain of Public Administration...|$|E
40|$|International audienceWe {{developed}} a system, TermWatch ([URL] which combines a linguistic <b>extraction</b> <b>of</b> <b>terms,</b> their structuring into a terminological network with a clustering algorithm. In this {{paper we explore}} its ability in integrating the most promising aspects of the studies on query refinement: choice of meaningful text units to cluster (domain terms), choice of tight semantic relations with which to cluster terms, structuring of terms in a network enabling abetter perception of domain concepts. We have run this experiment on the 367 645 English abstracts of PASCAL 2005 - 2006 bibliographic database ([URL] and compared the structured terminological resource automatically build by TermWarch to the English segment of TermScience resource ([URL] containing 88 211 terms...|$|E
40|$|When using {{dimensional}} regularization/reduction the epsilon-dimensional numerator of the 1 -loop Feynman diagrams {{gives rise}} to rational contributions. I list the set of fundamental rules that allow the <b>extraction</b> <b>of</b> such <b>terms</b> at the integrand level in any theory containing scalars, vectors and fermions, such as the electroweak standard model, QCD and SUSY. Comment: 19 pages, 14 figures, uses axodraw. sty. Version accepted for publication in JHE...|$|R
40|$|The problem {{addressed}} in this paper concerns the automatic identification and <b>extraction</b> <b>of</b> medical <b>terms</b> along with their definitions and modifiers from full text consumer-oriented medical articles. The system, DEFINDER (Definition Finder), uses rule-based techniques. The output of our system {{can be used in}} several applications: creation and/or enhancement of on-line terminological resources, summarization and text categorization according to level of expertise, e. g. lay vs. technical...|$|R
40|$|This {{paper is}} {{concerned}} about one aspect in the <b>extraction</b> <b>of</b> key <b>terms</b> that describe various types of information about a given gene. Our method for key term extraction {{is based on a}} comparison <b>of</b> <b>term</b> occurrences in documents associated with the gene ver-sus a broader set of documents. We in-vestigate the influence on the type <b>of</b> key <b>terms</b> extracted by the type of documents retrieved for the given gene. We provide analysis on five genes to draw our conclu-sions and hypotheses for future investiga-tions. ...|$|R
30|$|Possibility {{to include}} {{quantitative}} biomarker information. In this complex context, RSNA {{has been working}} on a template report library since 2008 [15]. The structured reports are stored at the website radreport.org, 1 which contains templates developed with the collaboration of subspecialty societies and coded with RadLex terms. The RadLex vocabulary and codes allow <b>extraction</b> <b>of</b> <b>terms</b> and also linkage between different languages. This initiative is now common between RSNA and ESR, with a joint committee (the “Template Library Advisory Panel, TLAP) that acts as an editorial board and reviews the templates’ content and structure. The open.radreport.org platform offers the possibility to all ESR and RSNA members to propose their own structured reports and to rate and comment on templates proposed by others.|$|E
30|$|Some of the {{classification}} errors were {{a result of}} the annotators receiving instructions to classify tweets containing any of the emotions fear, anger, or positive as other if the tweets relate to a “historical” state or if the expressed emotion related to someone else than the author of the tweet. Such a distinction can be important if the used classifications should be part of a social media analysis system (since {{we do not want to}} take action on emotions that are not present anymore), but no features have been used to explicitly take care of spatio-temporal constraints in the current experiments. If such features were added (e.g., using part-of-speech tags and <b>extraction</b> <b>of</b> <b>terms</b> that contain temporal information), some of {{the classification}} errors could probably have been avoided.|$|E
40|$|Abstract. In {{this paper}} we propose the model of a prototypical NLP {{architecture}} of an information access system to support a team of experts in a scientific design task, in a shared and heterogeneous framework. Specifically, we believe AI/NLP {{can be helpful in}} several tasks, such as the extraction of implicit information needs enclosed in meeting minutes or other documents, analysis of explicit information needs expressed through Natural Language, processing and indexing of document collections, extraction of required information from documents, modeling of a common knowledge base, and, finally, identification of important concepts through the automatic <b>extraction</b> <b>of</b> <b>terms.</b> In particular, we envisioned this architecture in the specific and practical scenario of the Concurrent Design Facility (CDF) of the European Space Agency (ESA), in the framework of the SHUMI project (Support To HUman Machine Interaction) developed in collaboration with the ESA/ESTEC- ACT (Advanced Concept Team). ...|$|E
30|$|In this article, we {{tackle the}} <b>extractions</b> <b>of</b> entity {{mentions}} and their relations from biomedical texts, specifically from MEDLINE abstracts 3, using a recent human-into-the-loop automation strategy {{that has not}} been applied in the medical domain before. Unlike named entity recognition (NER) systems on e.g. the news domain, entity recognition on medical domains comprises <b>of</b> <b>extractions</b> <b>of</b> technical <b>terms</b> in the broader medical and biological arena such as name of diseases, proteins, substances and so on, see e.g. [10, 11].|$|R
40|$|In this paper, we {{describe}} the compilation and structure of two linguistic resources, a corpus and a dictionary <b>of</b> <b>terms</b> {{in the field of}} economy, developed for Galician. In addition to this, {{we describe}} the use of these resources for the automatic <b>extraction</b> <b>of</b> multi-word <b>terms</b> by means <b>of</b> a combination of linguistic and statistical techniques. While doing this, special attention will be paid to the problems posed by minority languages such as Galician for the achievement of these tasks. 1...|$|R
50|$|Mining setts were a legal {{arrangement}} used historically in {{the counties of}} Devon and Cornwall in South West England to manage the exploitation of land for the <b>extraction</b> <b>of</b> tin. The <b>term</b> was also used on the Isle of Man.|$|R
