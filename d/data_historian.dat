22|54|Public
5000|$|... data {{acquisition}} from <b>data</b> <b>historian,</b> data base or manual inputs ...|$|E
5000|$|Nimbits is an {{open source}} <b>data</b> <b>historian</b> server built on cloud {{computing}} architecture that provides connectivity between devices using data points.|$|E
5000|$|Collection of {{production}} data. This includes collection, storage and exchange of process data, equipment status, material lot information and production logs {{in either a}} <b>data</b> <b>historian</b> or relational database.|$|E
40|$|Abstract. At EDF, {{a leading}} energy company, process data {{produced}} in power stations are archived both {{to comply with}} legal archiving require-ments and to perform various analysis applications. Such data consist of timestamped measurements, retrieved {{for the most part}} from pro-cess data acquisition systems. After archival, past and current values are used for various applications, including device monitoring, maintenance assistance, decision support, statistics publication, etc. Large amounts of data are generated in these power stations, and aggre-gated in soft real-time – without operational deadlines – at the plant level by local servers. For this long-term data archiving, EDF relies on <b>data</b> <b>historians</b> – like InfoPlus. 21, PI or Wonderware Historian – for years. This is also true for other energy companies worldwide and, in general, industry based on automated processes. In this paper, we aim at answering a simple, yet not so easy, question: how can <b>data</b> <b>historians</b> be placed in the data management landscape, from classical RDBMSs to NoSQL systems? To answer this question, we first give an overview of <b>data</b> <b>historians,</b> then discuss benchmarking these particular systems. Although many benchmarks are defined for conven-tional database management systems, none of them are appropriate for <b>data</b> <b>historians.</b> To establish a first objective basis for comparison, we therefore propose a simple benchmark inspired by EDF use cases, and give experimental results for <b>data</b> <b>historians</b> and DBMSs. ...|$|R
40|$|International audienceAt EDF, {{a leading}} energy company, process data {{produced}} in power stations are archived both {{to comply with}} legal archiving requirements and to perform various analysis applications. Such data consist of timestamped measurements, retrieved {{for the most part}} from process data acquisition systems. After archival, past and current values are used for various applications, including device monitoring, maintenance assistance, decision support, statistics publication, etc. Large amounts of data are generated in these power stations, and aggregated in soft real-time (without operational deadlines) at the plant level by local servers. For this long-term data archiving, EDF relies on <b>data</b> <b>historians</b> (like InfoPlus. 21, PI or Wonderware Historian) for years. This is also true for other energy companies worldwide and, in general, industry based on automated processes. In this paper, we aim at answering a simple, yet not so easy, question: how can <b>data</b> <b>historians</b> be placed in the data management landscape, from classical RDBMSs to NoSQL systems? To answer this question, we first give an overview of <b>data</b> <b>historians,</b> then discuss benchmarking these particular systems. Although many benchmarks are defined for conventional database management systems, none of them are appropriate for <b>data</b> <b>historians.</b> To establish a first objective basis for comparison, we therefore propose a simple benchmark inspired by EDF use cases, and give experimental results for <b>data</b> <b>historians</b> and DBMSs...|$|R
50|$|Level 3 — Manufacturing {{operations}} systems — Managing {{production work}} flow {{to produce the}} desired products. Batch management; manufacturing execution/operations management systems (MES/MOMS); laboratory, maintenance and plant performance management systems; <b>data</b> <b>historians</b> and related middleware. Time frame: shifts, hours, minutes, seconds.|$|R
50|$|Founded in 1997, Automsoft {{sought to}} address {{a gap in the}} market process <b>data</b> <b>historian</b> market by writing a product using object {{database}} technology OODBMS.Automsoft joins the OPC Foundation as a charter member.|$|E
50|$|Operational {{historian}} {{refers to}} a database software application that logs or historizes time-based process <b>data.</b> <b>Historian</b> software is used to record trends and historical information about industrial processes for future reference. It captures plant management information about production status, performance monitoring, quality assurance, tracking and genealogy, and product delivery with enhanced data capture, data compression, and data presentation capabilities.|$|E
50|$|A {{networked}} server {{making use}} of industry standard communications networking to the SCADA host may provide performance approaching that of an embedded ICCP application. On the application interface side the ICCP is not restricted to the SCADA environment but is open to other systems such as a separate <b>data</b> <b>historian</b> or other databases. Security may be easier to manage with the ICCP server segregated from the operational real time systems. The gateway processor approach {{is similar to the}} networked server except it is intended for legacy systems with minimal communications networking capability and so has the lowest performance. In the most minimal situation the ICCP gateway may communicate with the SCADA host via a serial port in a similar manner to the SCADA RTUs.|$|E
5000|$|PRODML Standards support {{automated}} {{production data}} acquisition, operations monitoring, optimization, reporting, and configuration management business processes. PRODML-based interactions {{are used by}} production software components, including field <b>data</b> <b>historians,</b> surveillance applications, model analysis and management applications, optimization applications, simulation applications, etc.|$|R
50|$|With these <b>data,</b> the <b>historian</b> {{is able to}} {{quantify}} fully {{the construction of a}} building in Paris at that time.|$|R
5000|$|His wife's name {{is listed}} as Adalarda in sources not mentioned. On the basis of {{onomastic}} <b>data</b> the <b>historian</b> Edward Glavichka believes that his wife Adalarda {{was the daughter of}} Matfrida II, count of Ayfelgau. Their children were: ...|$|R
40|$|A Profibus USB Interface is {{a device}} {{used to control}} several {{industrial}} measurement devices using the Profibus-protocol, which is a serial bus protocol based on RS 485. On the one hand it will {{be connected to the}} USB port of a PC on the other hand it will be connected to the Profibus-RS 485 line as the master device. It reads the data from the measurement devices and passes them to a database suitable for time-dependent data (<b>data</b> <b>historian)</b> using the OPC-standard (OPC is a standardised software interface allowing the exchange of different manufacturers data). This article describes in detail the installation procedure of an Ifak-system isPro USBx 12 Profibus USB Interface. For this installation procedure the example of connecting several BRONKHORST EL-FLOW mass flow meters/controllers as measurement devices is used. Several problems and software bugs are described and appropriate workarounds are given. This includes the necessity of a new driver software, which had to be developed by the vendor of Ifak-system, {{in order to make the}} isPro USBx 12 Profibus USB Interface collaborate with the BRONKHORST EL-FLOW mass flow meter/controller. The exact steps how to carry out the installation of both hardware and software as well as the proper configuration under Windows-XP are given, including ways, how to test the installed devices for their proper function. The composition of the data-record for one BRONKHORST EL-FLOW device within the <b>data</b> <b>historian</b> is described in detail. Appropriate references are given as well. JRC. G. 8 -Nuclear safeguard...|$|E
40|$|This study {{involves}} the experimental and computational {{study of the}} mixing and displacement phenomena that take place during hydrate inhibition of jumper type configurations using monoethylene-glycol (MEG) and methanol. All experiments were conducted in a three-inch jumper system at the University of Tulsa Hydrate Formation Project. This thesis presents the experimental results with respect to effect of inhibitor type, injection rate, brine salinity and liquid loading. Different dispersion and partitioning mechanisms were observed for methanol and MEG, especially in the vertical sections and low spots. Repeated MEG tests were conducted with Labview ® <b>data</b> <b>historian</b> system installed after improving the jumper facility. Methanol displacement tests were conducted following the tests matrix, and conclusions were made based on the experimental results. Methanol overriding the water phase at bot...|$|E
40|$|We are {{witnessing}} {{the trend of}} increasing data production in various domains including industrial automation. This trend requires means for data capturing, storing, and analyzing. Furthermore, a versatile data model is needed to enable easy knowledge representation as well as change management. In this paper, we utilize Semantic Big <b>Data</b> <b>Historian,</b> which can cope with previously mentioned requirements, for a demonstration of promising analytic approach combining Big Data methods and a user-friendly modular platform. The ap-proach is demonstrated on data from a hydroelectric power station. The station has been dealing with the interesting problem of prediction when to momentari-ly stop their turbine to increase generated power after the restart. In this contri-bution, we discuss several approaches how to process and analyze data from power station sensors for achieving the best results...|$|E
40|$|Multilayer neural {{networks}} {{have been successfully}} applied as intelligent sensors for process modeling and control. In this paper, a few practical issues are discussed and some solutions are presented. Several biased regression approaches, including ridge regression, PCA, and PLS, are integrated with neural net training to reduce the prediction variance. 1 Introduction The availability of process control computers and associated <b>data</b> <b>historians</b> {{make it easy to}} generate neural network solutions for process modeling and control. Numerous applications of {{neural networks}} in the field of process engineering have been reported in recent annual meetings and technical journals. Neural network solutions are well accepted in process industries since they are cost-effective, easy-to-understand, nonlinear, and data-driven. This chapter addresses several practical issues and some solutions regarding to use of neural networks for intelligent sensors and control. The chapter begins with an introducti [...] ...|$|R
40|$|Pulp {{and paper}} mills are {{increasingly}} implementing process information systems {{with the goal}} of better tracking, troubleshooting and optimizing their processes. In the field of process engineering {{as well as in the}} field of business process engineering, these information systems are playing an increasingly important role. However, their potential is far from being fully exploited, in large part due to a lack of dedicated resources for applications development at mills. There is also a rapid growth in applications capability being driven by system vendors, expanding the potential for finding value in process data. Some applications in the pulp and paper industry discussed in this paper include: 1. <b>data</b> <b>historians</b> for storing process data and using this data for process trouble-shooting; 2. enterprise asset management systems for streamlining maintenance tasks; 3. manufacturing execution systems for streamlining manufacturing, distribution operations and business processes...|$|R
5000|$|Solzhenitsyn {{published}} this two-volume {{work on the}} history of Russian-Jewish relations in 2001 and 2002. The book stirred controversy, and many historians reported it as unreliable in factual <b>data.</b> Some <b>historians</b> classified it as antisemitic. The book was published in French and German in 2002-2003. A partial English translation is found in [...] "The Solzhenitsyn Reader".|$|R
40|$|Abstract Within Électricité de France (EDF) {{hydroelectric}} power stations, IGCBoxes are industrial mini PCs dedicated to industrial process data archiv-ing. These equipments expose distinctive features, mainly on their storage sys-tem based exclusively on flash memory due to environmental constraints. This type of memory had notable consequences on data acquisition performance, with a substantial drop compared with hard disk drives. In this setting, we have designed Chronos, an open-source NoSQL system for sensor data manage-ment on flash memories. Chronos includes an efficient quasi-sequential write pattern {{along with an}} index management technique adapted for process data management. As a result, Chronos supports a higher velocity for inserted data, with acquisition rates improved {{by a factor of}} 20 to 54 over different solutions, therefore solving a practical bottleneck for EDF. Keywords database · flash memory · NoSQL system · <b>data</b> <b>historian...</b>|$|E
40|$|The {{performance}} monitoring has the fundamental aim {{to analyze the}} operative efficiency of single unit operations or overall process sections, especially for lowering costs, increasing profitability and improving the supply chain management in terms of production sites planning and maintenance scheduling. In any case, the industrial {{state of the art}} is based on approximated techniques, such as the extrapolation of future behaviour by using plant <b>data</b> <b>historian</b> and/or adopting a (non) linear regression of measurements for predicting the trend of future efficiencies. Instead, as reported in recent literature, a more careful and accurate approach concerns the utilization of detailed first-principles model: the present paper discusses and applies the projective {{performance monitoring}} approach, under stochastic disturbances and based on mathematical models. It shows, as preliminary results, comparisons between the expected behaviour obtained by a conventional approximated technique and the prediction of rigorous model based performances...|$|E
40|$|Owensboro Municipal Utilities (OMU) is a municipally {{owned and}} {{operated}} electric utility based in Owensboro, Kentucky. OMU’s Elmer Smith Station is a coal-burning facility consisting of two units (one cyclone and one T-fired) with a combined capacity of 425 MW. It is under continuous pressure to reliably and cost-effectively deliver electricity to its ratepayer owners. OMU believes that to improve the plant’s profitability, it needs to improve its ability to understand the financial impacts of its operational and maintenance actions. The plant had invested in instrumentation, control and data systems (e. g. Metso DCS, Black and Veatch’s Online Performance Monitoring (OPM) system, OSIsoft PI <b>data</b> <b>historian,</b> and Invensys AvantisPro CMMS), but it was having difficulty extracting the knowledge necessary to make financially-motivated decisions. To that end, OMU invested in NeuCo, Inc. ’s integrated performance, maintenance and profit optimization systems, to enable the plant to perform comprehensive, real-time cost and revenue analyses and determine opportunities for improving profitability. The system brings together process, fuels, and maintenance data so plant personnel can better understand the impacts of their actions and decisions, and prioritize their wor...|$|E
40|$|In {{engineering}} systems, {{early detection}} of the occurrence of faults is critical in avoiding product defects. This problematic is here discussed {{in the framework of}} an industrial process, namely, an injection moulding plastic machine. The relationships between the process state and the product quality are achieved through Principal Component Analysis. After having identified the main variables, two neural network architectures were investigated, TDNN and Elman networks, with respect to one-step ahead prediction. The results show that TDNN exhibited lower training times with respect to a desired performance criteria. However, for time series in which temporal dependency is large, the recurrent networks with time delayed inputs could lead to better results. 1 Introduction Neural networks have achieved, in recent years, a high degree of importance. The availability of process control computers as well as <b>data</b> <b>historians</b> made it easy to develop neural network solutions for process modeling [...] ...|$|R
40|$|In {{the last}} quarter century, many studies have {{demonstrated}} how the archaeological record reflects known socioeconomic distinctions on plantation settlements in the American South and Caribbean. Despite the lack of empirical <b>data,</b> <b>historians</b> have assumed that the physical structures of Latin American hacienda sites reflect similar social and economic distinctions. This study examines whether socioeconomic patterns are observable in the material remains of hacienda sites. Recent archaeological investigations at Hacienda Tabi, a nineteenth- and early twentieth-century sugar estate in Yucatan, Mexico, reveal that class distinctions based on occupational status are, indeed, manifest in the site's physical remains. Patterns relating to the documented hierarchy of labor are evident at the community, intra-community, and household levels of analysis. By employing empirical and statistically supported data, this study bolsters historians' previous assumptions. The study also demonstrates a method of reconstructing hacienda settlements in Yucatan, estimating their populations, and identifying the former dwelling locations {{of high and low}} ranking laborers...|$|R
50|$|The Florentine Catasto of 1427 {{provided}} {{an important source}} of raw historical <b>data</b> for <b>historians</b> of the Renaissance. The extensive surveys conducted by Florentine officials reveal changing forms of social organization over the period that records were collected. David Herlihy and Christiane Klapisch-Zuber's work on these records, Tuscans and Their Families {{is one of the first}} historical works to make use of computer-assisted statistical analysis.|$|R
40|$|In {{the context}} of smart grid development, this paper {{considers}} the problem of interoperability of micro-grid platforms, particularly among research institutions. Various levels of interoperability are introduced with the respective requirements. The primary aim of the paper is to propose a suitable private hybrid cloud based SCADA architecture satisfying various necessities {{in the framework of}} interoperability of micro-grid platforms while maintaining security restriction conditions. Due to the limited time restriction of critical SCADA functions in the electrical grid (protection, real time control, etc.), only selected non-critical SCADA functions (back-up, <b>data</b> <b>historian,</b> etc.) are accessible to partners from the private cloud. The critical SCADA tasks functionality remains under control of local server, thus, a hybrid cloud architecture. Common Information Model (IEC 61970 and IEC 61968, CIM/XML/RDF) is proposed to be used as model for information exchange. The communication model is based on PaaS delivery model and OPC Unified Architecture (OPC UA) specifications are considered. OPC gateway is proposed as conversion between the old OPC Distributed Common Object Model (DCOM) protocol and the Simple Object Access Protocol (SOAP) for cloud...|$|E
40|$|To {{enhance and}} modernise the Nuclear Material Accountancy and Control (NMAC) at Mayak RT- 1 {{reprocessing}} plant {{the concept of}} near real time accountancy (NRTA) is applied. A defence in depth concept is proposed with the superposition of the following barriers: (i) 	continuous survey of the functional status, (ii) 	the follow-up of the nuclear material (NM) flow and inventory, (iii) 	Near real time control of declared NM in processed solutions, (iv) 	Periodical Physical Inventory Takings and Verifications. The combination of an operational network architecture, allowing automatic data acquisition and an efficient plant-specific data analysis and interpretation software enables to follow in near real-time the NM flow and inventory through the plant. The JRC data analysis and interpretation kernel DAI integrated with the commercial <b>data</b> <b>historian</b> Wizcon is proposed to establish the RT- 1 specific NM monitoring software tool. The data analysis focuses firstly {{on a combination of}} the pressure signals (for level and density) and the temperature signal, determining the total volume. This is then combined with the volume concentration measurement performed by the hybrid K-edge to derive the total mass of nuclear material. Once the complete system is validated, it can be applied also to other Russian reprocessing plants at Seversk and Zheleznogorsk. JRC. G. 8 -Nuclear safeguard...|$|E
40|$|The current {{security}} {{challenges and}} hazards associated {{with oil and}} gas operations especially in the Nigerian Niger Delta calls for an innovative approach to managing the operations to reduce the exposure of staff to these hazards and risks without compromising the asset integrity and operations philosophy. Remote operation {{is defined as the}} remote monitoring and control of the field based production systems from an offsite location with the aim of optimizing the entire production process. It involves the continuous collation of operational data using smart instruments, the transmission of these data using a robust and secure communication link and the integration of these data to the company’s IT infrastructure comprised mainly of a <b>data</b> <b>historian</b> and production optimization tools. It provides the relevant personnel with information on the field performance and also provides an avenue for intervention while minimizing the exposure of the staff to the Health Safety and Environment (HSE) hazards associated with the fields. Remote operation has been shown to also reduce the Operational Expenses (OPEX) by reducing the number of field visits and the associated logistics and security costs and enhancing the field performance in terms of faster and more accurate interventions thereby enabling a better HS...|$|E
50|$|Guests have {{included}} Alice Meadows of ORCID, Hilda Bastian of NCBI, Konrad Förstner and Matthias Fromm of Open Science Radio, Tracy Teal of <b>Data</b> Carpentry, and <b>historian</b> Aileen Fyfe of University of St Andrews.|$|R
40|$|In this {{proof-of-concept}} study, {{a methodology}} is proposed to systematically analyze large <b>data</b> <b>historians</b> of secondary pharmaceutical manufacturing systems using data mining techniques. The {{objective is to}} develop an approach enabling to automatically retrieve operation-relevant information that can assist the management in the periodic review of a manufactory system. The proposed methodology allows one to automatically perform three tasks: the identification of single batches within the entire data-sequence of the historical dataset, the identification of distinct operating phases within each batch, and the characterization of a batch with respect to an assigned multivariate set of operating characteristics. The approach is tested on a six-month dataset of a commercial-scale granulation/drying system, where several millions of data entries are recorded. The quality of results and the generality of the approach indicate {{that there is a}} strong potential for extending the method to even larger historical datasets and to different operations, thus making it an advanced PAT tool that can assist the implementation of continual improvement paradigms within a quality-by-design framework...|$|R
5000|$|We Neopagans {{now face}} a crisis. As new <b>data</b> appeared, <b>historians</b> altered their {{theories}} {{to account for}} it. We have not. Therefore an enormous gap has opened between the academic and the 'average' Pagan view of witchcraft. We continue to use of out-dated and poor writers, like Margaret Murray, Montague Summers, Gerald Gardner, and Jules Michelet. We avoid the somewhat dull academic texts that present solid research, preferring sensational writers who play to our emotions. Jenny Gibbons (1998) ...|$|R
40|$|Modern {{industrial}} plants contain enormous numbers of sensors which, in turn, generate {{enormous amounts of}} process and diagnostic variable measurements. All this generated data is stored in a <b>Data</b> <b>Historian</b> database and then left untouched. This report evaluates whether there is useful information amongst this unused data, and if so, how this information can best be used to increase the reaction time of plant operators. This is done by examining the application of regression methods to make early faults detection possible. The simulations are performed using historic process data from a crude distiller unit at the Shell Pernis Refinery. Datasets representing both normal and faulty operations are taken from two different subsystems of the crude distiller unit. The output datasets have irregular sampling times that are larger than the input variable datasets so this potential problem is solved by using a linear interpolation to estimate the missing values in the output datasets. The processes in the subsystems are modelled using finite impulse response (FIR) models. Five different regression methods are used to identify these models. This report concludes firstly that the ordinary least squares and ridge regression methods {{can be used to}} construct accurate out-of-sample prediction models of key process variables; and secondly, that this can be done without prior process knowledge or extensive process specific analysis. Mechanical, Maritime and Materials EngineeringDelft Center for Systems and Control (DCSC...|$|E
40|$|AbstractThe {{industrial}} scale production of hydrogen gas through steam methane reforming (SMR) process requires an optimum furnace temperature distribution to not only maximize the hydrogen yield but also increase the longevity of the furnace infrastructure which usually operates around 1300 degree Kelvin (K). Kepler workflows are used in temperature homogenization, termed as balancing of this furnace through Reduced Order Model (ROM) based Matlab calculations using the dynamic temperature inputs from an array of infrared sensors. The outputs of the computation are used to regulate the flow rate of fuel gases which in turn optimizes the temperature distribution across the furnace. The input and output values are stored in a <b>data</b> <b>Historian</b> which is a database for real-time data and events. Computations are carried out on an OpenStack based cloud environment running Windows and Linux virtual machines. Additionally, ab initio computational fluid dynamics (CFD) calculation using Ansys Fluent software is performed to update the ROM periodically. ROM calculations complete in few minutes whereas CFD calculations usually take {{a few hours to}} complete. The Workflow uses an appropriate combination of the ROM and CFD models. The ROM only workflow currently runs every 30 minutes to process the real-time data from the furnace, while the ROM CFD workflow runs on demand. ROM only workflow can also be triggered by an operator of the furnace on demand...|$|E
40|$|Industrial {{systems have}} been {{developing}} into more and more complex systems during last decades. They have changed from centralized solutions to distributed, more robust, and more exible eco-systems comprising {{a high number of}} embedded systems. In recent years, we are witnessing the research trend in the area of embedded systems which concerns the very close integration of physical and computing systems. This dissertation thesis deals with the problem of the semantic integration of components (sensors and actuators) of cyber-physical systems within industrial automation domain and presents resulting bene ts. Cyber-physical systems were created based on the aforementioned trend of the close integration of computing systems and physical systems. This tight integration involves infrastructures responsible for control, computation, communication, and sensing. These systems are composed of many subsystems produced by various manufacturers, and the subsystems produce an enormous volume of data. Furthermore, data generated from all of the system parts has di erent dimensions, sampling rates, levels of details, etc. Next, cyber-physical systems form systems which represent building blocks of the fourth industrial revolution (Industry 4. 0) for example (Industrial) Internet of Things, Smart Cities, Smart Factories. Thus, the right understanding of data (data meanings, given context, subsystems purposes, and possible ways of subsystems integration) belong to essential requirements for enabling Industry 4. 0 visions. In this thesis, the utilization of ontologies was proposed to deal with the semantic heterogeneity for enabling easier cyber-physical system components integration. Moreover, the current widespread e ort to create exible highly customized manufacturing requires novel methods for data handling together with subsequent data utilization. Storing knowledge and data in an ontology o ers a needed solution. For example, an ontology employment brings easy system data model management, increase an e ciency of cyber-physical system components interoperability, advanced data processing, reusability of sensors and actuators, and utilization of ontology matching methods for an integration of other data models. This work concerns the problem, how to describe cyber-physical system components using ontologies to enable e ective integration. Next, the ontology matching system suitable for integration of heterogeneous data models in industrial automation domain is described. The proposed solution of the semantic interoperability is demonstrated on the Plug&Play cyber-physical system components. On the other hand, storing data in an ontology and mainly processing of RDF statements brings one signi cant bottleneck | performance issue. Thus, Big Data technologies are employed for overcoming this issue together with a proposal of suitable storage data models. The overall approach is demonstrated on the proposed and developed prototype named Semantic Big <b>Data</b> <b>Historian.</b> In particular, the main contributions of the dissertation thesis are as follows: 1. The proposal of the solution for CPS low-level semantic integration based on Semantic web Technologies together with a veri cation of a feasibility of proposed approach using Semantic Big <b>Data</b> <b>Historian.</b> 2. The overcoming performance issues of processing shop floor data represented as RDF-triples with the help of Big Data technologies and suitable storage data models | vertical partitioning and hybrid SBDH model. 3. The proposal and implementation of a suitable way how to integrate heterogeneous data models from industrial automation domain where the highest precision and recall are required. The approach is based on similarity measures aggregation using self-organizing maps and user involvement with the help of active learning and visualization of self-organizing map output layer. 4. Enabling reusability of cyber-physical system components together with effortless configuration based on utilization of Semantic Web technologies. This approach was named as Plug&Play cyber-physical system components. Katedra kybernetik...|$|E
40|$|Permission is hereby {{granted to}} the University of Alberta Libraries to {{reproduce}} single copies of this thesis and to lend or sell such copies for private, scholarly or scientific research purposes only. Where the thesis is converted to, or otherwise made available in digital form, the University of Alberta will advise potential users of the thesis of these terms. The author reserves all other publication and other rights {{in association with the}} copyright in the thesis and, except as herein before provided, neither the thesis nor any substantial portion thereof may be printed or otherwise reproduced in any material form whatsoever without the author's prior written permission. —To my family In many chemical plants, <b>data</b> <b>historians</b> store thousands of variables at fast sampling rates. Much of this collected data is routine operating data that could easily be used for system identification and forecasting, especially in the design of soft sensors. Currently, there is no framework for assessing the quality of these data sets. Therefore, this dissertation proposes a two-step method, consisting of data segmentation and data quality assessment, with application to soft sensor development...|$|R
40|$|This {{paper will}} propose that, rather than sitting on silos of <b>data,</b> <b>historians</b> that utilise {{quantitative}} methods should endeavour {{to make their}} data accessible through databases, and treat this as {{a new form of}} bibliographic entry. Of course in many instances historical data does not lend itself easily to the creation of such data sets. With this in mind some of the issues regarding normalising raw historical data will be looked at with reference to current work on nineteenth century Irish trade. These issues encompass (but are not limited to) measurement systems, geographic locations, and potential problems that may arise in attempting to unify disaggregated sources. It will discuss the need for a concerted effort by historians to define what is required from digital resources for them to be considered accurate, and to what extent the normalisation requirements for database systems may conflict with the desire for accuracy. Many of the issues that the historian may encounter engaging with databases will be common to all historians, and there would be merit in having defined standards for referencing items, such as people, places, locations, and measurements...|$|R
40|$|Genet Jean-Philippe. Gerhard Jaritz, Images. A Primer of Computer-Supported Analysis with kleio IAS; Matthew Woollard et Peter Denley, Source-Oriented <b>Data</b> Processing for <b>Historians.</b> A Tutorial for kleio; Manfred Thaller, Kleio. A Database system. In: Histoire & Mesure, 1996 volume 11 - n° 1 - 2. Varia. pp. 186 - 187...|$|R
