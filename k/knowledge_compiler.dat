6|51|Public
40|$|Transformation {{technique}} enables inefficient {{expert systems}} {{to run in}} real time. Paper suggests use of <b>knowledge</b> <b>compiler</b> to transform knowledge base and inference mechanism of expert-system computer program into conventional computer program. Main benefit, faster execution and reduced processing demands. In avionic systems, transformation reduces need for special-purpose computers...|$|E
40|$|The {{concept of}} using a <b>knowledge</b> <b>compiler</b> to {{transform}} the knowledge base and inference mechanism of an expert system into a conventional program is presented. The need to accommodate real-time systems requirements in applications such as embedded avionics is outlined. Expert systems and a brief comparison of expert systems and conventional programs are reviewed. Avionics applications of expert systems are discussed before the discussions of applying the proposed concept to example systems using forward and backward chaining...|$|E
40|$|Harpy {{is one of}} {{the first}} systems to {{demonstrate}} that high performance, large vocabulary connected speech recognition systems can in fact be realized economically for task- oriented (restricted) languages. In this chapter the authors present, using simple examples, the principles of organization of the Harpy system. They illustrate how knowledge sources (KSs) are specified, how the <b>knowledge</b> <b>compiler</b> integrates the KSs into a unified directional graph representation, and how this knowledge is utilized. In conclusion, they discuss many of the limitations of the present system and how these can be eliminated or reduced in future system...|$|E
5000|$|Every Boy's Book of <b>Knowledge</b> (Prion, 2007, <b>compiler</b> and editor) ...|$|R
40|$|Design of the {{software}} architecture for a knowledge driven HIS is presented. In our design the frame {{has been used as}} the basic unit of knowledge representation. The structure of the frame is being designed to be sufficiently universal to contain knowledge required to implement not only expert systems, but almost all traditional HIS functions including ADT, order entry and results review. The design incorporates a two level format for the knowledge. The first level as ASCII records is used to maintain the knowledge base while the second level converted by special <b>knowledge</b> <b>compilers</b> to standard computer languages is used for efficient implementation of the knowledge applications...|$|R
50|$|Muchnick {{eventually}} {{departed from}} his teaching profession. He {{then went on}} to apply his <b>knowledge</b> of <b>compilers</b> as a vital member of the teams that developed two computer architectures — PA-RISC at Hewlett-Packard and SPARC at Sun Microsystems.|$|R
40|$|Abstract. Students in {{introductory}} programming classes often {{articulate their}} questions and information needs incompletely. Consequently, the automatic classification of student questions to provide automated tutorial responses is a challenging problem. This paper analyzes 411 questions from an introductory Java programming course {{by reducing the}} natural language of the questions to a vector space, and then utilizing cosine similarity to identify similar previous questions. We report classification accuracies between 23 % and 55 %, obtaining substantial improvements by exploiting domain <b>knowledge</b> (<b>compiler</b> error messages) and educational context (assignment name). Our mean reciprocal rank scores are comparable to and arguably better than most scores reported in a major information retrieval competition, even though our dataset consists of questions asked by students {{that are difficult to}} classify. Our results are especially timely and relevant for online courses where students are completing the same set of assignments asynchronously and access to staff is limited. ...|$|E
40|$|In {{contrast}} to {{other kinds of}} libraries, software libraries need to be conceptually organized. When looking for a component, the main concern of users is the functionality of the desired component; implementation details are secondary. Software reuse would be enhanced with conceptually organized large libraries of software components. In this paper, we present GURU, a tool that allows automatical building of such large software libraries from documented software components. We focus here on GURU's indexing component which extracts conceptual attributes from natural language documentation. This indexing method is based on words' co-occurrences. It first uses EXTRACT, a co-occurrence <b>knowledge</b> <b>compiler</b> for extracting potential attributes from textual documents. Conceptually relevant collocations are then selected according to their resolving power, which scales down the noise due to context words. This fully automated indexing tool thus goes further than keyword-based tools {{in the understanding of}} a document without the brittleness of knowledge based tools. The indexing component of GURU is fully implemented, and some results are given in the paper...|$|E
40|$|KBSDE is a <b>knowledge</b> <b>compiler</b> {{that uses}} a classification-based {{approach}} to map solution constraints in a task specification onto particular search algorithm components that {{will be responsible for}} satisfying those constraints (e. g., local constraints are incorporated in generators; global constraints are incorporated in either testers or hillclimbing patchers). Associated with each type of search algorithm component is a subcompiler that specializes in mapping constraints into components of that type. Each of these subcompilers in turn uses a classification-based approach, matching a constraint passed to it against one of several schemas, and applying a compilation technique associated with that schema. While much progress has occurred in our research since we first laid out our classification-based approach [Ton 91], we focus in this paper on our reformulation research. Two important reformulation issues that arise out of the choice of a schema-based approach are: (1) compilability [...] Can a constraint that does not directly match any of a particular subcompiler's schemas be reformulated into one that does? and (2) Efficiency [...] If the efficiency of the compiled search algorithm depends on the compiler's performance, and the compiler's performance depends on the form in which the constraint was expressed, can we find forms for constraints which compile better, or reformulate constraints whose forms can be recognized as ones that compile poorly? In this paper, we describe a set of techniques we are developing for partially addressing these issues...|$|E
40|$|State-of-the-art <b>knowledge</b> <b>compilers</b> {{generate}} deterministic subsets of DNNF, {{which have}} been recently shown to be exponentially less succinct than DNNF. In this paper, we propose a new method to compile DNNFs without enforcing determinism necessarily. Our approach is based on compiling deterministic DNNFs {{with the addition of}} auxiliary variables to the input formula. These variables are then existentially quantified from the deterministic structure in linear time, which would lead to a DNNF that is equivalent to the input formula and not necessarily deterministic. On the theoretical side, we show that the new method could generate exponentially smaller DNNFs than deterministic ones, even by adding a single auxiliary variable. Further, we show that various existing techniques that introduce auxiliary variables to the input formulas can be employed in our framework. On the practical side, we empirically demonstrate that our new method can significantly advance DNNF compilation on certain benchmarks...|$|R
5000|$|IceChat {{source code}} is {{available}} on GitHub, where it was moved to, from CodePlex. It has a several CMD files, in the format of [...] which allow all users to build the [...]exe with no coding <b>knowledge</b> or any <b>compiler</b> installed.|$|R
40|$|Dynamic linking lets {{programs}} use {{the most}} recent versions of classes without re-compilation. In Java and. NET, bytecode specifies which classes should be dynamically linked. This information represents the <b>compiler's</b> <b>knowledge</b> of the compilation environment, but the execution environment might be di#erent...|$|R
40|$|This paper {{describes}} {{a course in}} compiler design {{that focuses on the}} Scheme implementation of a Scheme compiler that generates native assembly code for a real architecture. The course is suitable for advanced undergraduate and beginning graduate students. It is intended both to provide a general <b>knowledge</b> about <b>compiler</b> design and implementation and to serve as a springboard to more advanced courses. Although this paper concentrates on the implementation of a compiler, an outline for an advanced topics course that builds upon the compiler is also presented...|$|R
40|$|Effective {{strategies}} for performance analysis and tuning will be {{essential for the}} success of data parallel languages such as High-Performance Fortran (HPF) and Fortran D. Since compilers for these languages insert all communication, they have considerable knowledge about a program's dynamic structure and the relationship between its parallelism and communication. This paper explores how this <b>compiler</b> <b>knowledge</b> can be exploited to support performance evaluation and tuning. First, the compiler itself can use parameterized models to tune the performance of individual program phases; this approach can be effective provided that the compiler can test and handle violations of the model assumptions. Second, by exploiting <b>compiler</b> <b>knowledge</b> and introducing code transformations to improve monitorability, we can collect dynamic performance information that is far more compact than full communication traces, but well suited to the needs of tuning specific communication patterns. Third, we discus [...] ...|$|R
40|$|Given the {{widespread}} use of memory-safe languages, students must understand garbage collection well. Following a constructivist philosophy, an effective approach would be to have them implement garbage collectors. Unfortunately, a full implementation depends on substantial <b>knowledge</b> of <b>compilers</b> and runtime systems, which many courses do not cover or cannot assume. This paper presents an instructive approach to teaching gc, where students implement it atop a simplified stack and heap. Our approach eliminates enormous curricular dependencies while preserving the essence of gc algorithms. We take pains to enable testability, comprehensibility, and facilitates debugging. Our approach has been successfully classroom-tested for several years at several institutions...|$|R
40|$|Knowledge {{compilation}} {{is concerned}} with compiling problems encoded in some input language into some tractable, output language, {{with the goal of}} allowing one to solve such problems efficiently if the compilation is successful. This paradigm was originally motivated by the need to push much of the computational overhead into an offline compilation phase, which can then be amortized over a large number of queries in an online computation phase. In this dissertation, we study various new approaches to enhance the offline compilation phase, both theoretically and practically. We also study knowledge compilation from a perspective where it is employed as a general methodology for computation instead of just a paradigm that {{is concerned with}} the offline/online divide. In particular, we introduce a hierarchy of complexity parameters to bound the sizes of compiled representations. These new parameters are based on incorporating the logical content of the input representations, as opposed to existing parameters (e. g., treewidth) that are only based on the structure of the input. Our results improve some of the best known upper bounds on the compilation of influential representations, such as DNNFs, SDDs, and OBDDs. Moreover, we develop two new practical compilation algorithms for the DNNF and SDD languages, leading to orders of magnitude faster compilations. Finally, we study solving Beyond-NP problems using knowledge compilation, while particularly extending the reach of knowledge compilation to tackling problems in the highly intractable complexity class PP^PP. Our results show the applicability of <b>knowledge</b> <b>compilers</b> as black-box tools for solving Beyond-NP problems, similar to the use of SAT solvers for solving NP-complete problems...|$|R
5000|$|The {{subscript}} notation [...] (where [...] designates a pointer) is syntactic sugar for [...] Taking {{advantage of}} the <b>compiler's</b> <b>knowledge</b> of the pointer type, the address that [...] points to is not the base address (pointed to by [...] ) incremented by [...] bytes, but rather is defined to be the base address incremented by [...] multiplied {{by the size of}} an element that [...] points to. Thus, [...] designates the th element of the array.|$|R
40|$|Parallelizing compilers are {{essential}} tools for parallelizing old but sophisticated sequential programs. Program restructuring techniques, like loop transformations together with dependence analysis {{are applied to}} transform automatically sequential programs into parallel code. User interaction with the parallelizing process is very useful because, on massive parallel systems, small mistakes may cause large degradation on performance. In this paper we propose an interactive compiling environment, named Graphic Parallelizing Environment (GPE) 1, equipped with visualization tools to join user <b>knowledge</b> and <b>compiler</b> techniques to efficiently tune the program parallelization process. GPE is oriented to the advanced users of parallel computers. The Parafrase- 2 parallelizing compiler represents {{the core of the}} environment. Tcl/Tk is used as middleware integration language, mainly used to implement the environment components dispatcher, and the graphical components. Keywords Compilers, G [...] ...|$|R
40|$|The book is for {{compiler}} programmers who {{are familiar}} with concepts of compilers and want to indulge in understanding, exploring, and using LLVM infrastructure in a meaningful way in their work. This book is also for programmers who are not directly involved in compiler projects but are often involved in development phases where they write thousands of lines of code. With <b>knowledge</b> of how <b>compilers</b> work, {{they will be able to}} code in an optimal way and improve performance with clean code...|$|R
5000|$|The TypeScript {{compiler}} {{makes use}} of type inference to infer types when types are not given. For example, the [...] method in the code above would be inferred as returning a [...] even if no return type annotation had been provided. This {{is based on the}} static types of [...] and [...] being , and the <b>compiler's</b> <b>knowledge</b> that the result of adding two [...] is always a [...] However, explicitly declaring the return type allows the compiler to verify correctness.|$|R
40|$|User {{feedback}} {{in program}} parallelization {{is very important}} since often blind compilation may bring to mistakes in the generated code causing performance degradation. This {{is why it is}} necessary to provide the user with clear information about how the compiler parallelizes the program and, at the same time, an interface to control and assist program parallelization according to user feedback. To cope with this aspect we developed the HTG Visualization Tool 1 (HTGviz), a graphic user interface for program parallelization based on a joint work of user <b>knowledge</b> and <b>compiler</b> techniques. HTGviz is implemented on top of Parafrase- 2 parallelizing compiler, and Tcl/Tk is used as middleware integration language to implement graphical components. The interaction between the user and the compiler is carried out through the use of the Hierarchical Task Graph (HTG) program representation where task parallelism is represented by precedence relations (arcs) among task nodes. HTGviz [...] ...|$|R
40|$|A {{step-by-step}} {{guide that}} {{enables you to}} quickly implement a DSL with Xtext and Xtend in a test-driven way {{with the aid of}} simplified examples. This book is for programmers who want to learn about Xtext and how to use it to implement a DSL (or a programming language) together with Eclipse IDE tooling. It assumes that the user is familiar with Eclipse and its functionality. Existing basic <b>knowledge</b> of a <b>compiler</b> implementation would be useful, though not strictly required, since the book will explain all the stages of the development of a DSL...|$|R
40|$|With {{the arrival}} of {{multicore}} systems, parallel programming is becoming increasingly mainstream. Writing correct parallel programs, however, {{has turned out to}} be difficult and prone to errors without proper support from the employed programming languages, compilers, and runtime systems. Over the last years, researchers and engineers have developed numerous abstractions and programming models that make developing parallel programs easier, safer, and more efficient. Despite the advances made in parallel programming models, libraries, and runtime systems, the corresponding compilers still remain largely ignorant of the parallelism exhibited by the program execution. In particular, current compilers do not have any knowledge about what tasks are scheduled when in the program and how they are ordered—even though many higherlevel parallel programming models and libraries contain a wealth of task-order information that can be exploited by the <b>compiler.</b> Without task-scheduling <b>knowledge,</b> however, <b>compilers</b> are missing important optimization and verification opportunities. This dissertation explores how compilers for parallel programs with shared memory ca...|$|R
40|$|Abstract- Compiler {{construction}} {{is a widely}} used software engineering exercise, but because most students will not be compiler writers, {{care must be taken}} to make it relevant in a core curriculum. The course is suitable for advanced undergraduate and beginning graduate students. Auxiliary tools, such as generators and interpreters, often hinder the learning: students have to fight tool idiosyncrasies, mysterious errors, and other poorly educative issues. We introduce a set of tools especially designed or improved for compiler construction educative projects in C. We also provide suggestions about new approaches to compiler Construction. We draw guidelines from our experience to make tools suitable for education purposes. The final result {{of this paper is to}} provide a general <b>knowledge</b> about <b>compiler</b> design and implementation and to serve as a springboard to more advanced courses. Although this paper concentrates on the implementation of a compiler, an outline for an advanced topics course that builds upon the compiler is also presented by us...|$|R
40|$|This paper {{presents}} efficient {{mechanisms for}} activation, execution and rating that {{are suitable for}} use in BB 1 -style blackboard architectures. We describe a <b>knowledge</b> source <b>compiler</b> that produces match networks and demons for efficient activation and rating while compiling the entire system for increased execution speed. Experiments using the enhancements in a general-purpose blackboard shell illustrate approximately a doubling of run time speed, including an increase in activation speed {{by a factor of}} 7. 6 on the average. We have also resolved a subclass of blackboard systems that can be compiled down to the machine level by using a condensed representation where low-level blackboard accesses are replaced by vector references. Our analysis shows that the time complexity of the execution cycle of a condensed system is faster than the conventional approach by the ratio of the time required for blackboard retrievals to the time required for vector element retrievals. In practice, this ra [...] ...|$|R
40|$|This paper {{describes}} {{a course in}} compiler design {{that focuses on the}} Scheme implementation of a Scheme compiler that generates native assembly code for a real architecture. The course is suitable for advanced undergraduate and beginning graduate students. It is intended both to provide a general <b>knowledge</b> about <b>compiler</b> design and implementation and to serve as a springboard to more advanced courses. Although this paper concentrates on the implementation of a compiler, an outline for an advanced topics course that builds upon the compiler is also presented. 1 Introduction A good course in compiler construction is hard to design. The main problem is time. Many courses assume C or some similarly low-level language as both the source and implementation language. This assumption leads in one of two directions. Either a rich source language is defined and the compiler is not completed, or the source and target languages are drastically simplified in order to finish the compiler. Neither so [...] ...|$|R
40|$|In this paper, we {{investigate}} the communication {{characteristics of the}} Message Passing Interface (MPI) implementation of the NAS parallel benchmarks and study the effectiveness of compiled communication for MPI programs. Compiled communication is a technique that utilizes the <b>compiler</b> <b>knowledge</b> of both the application communication requirement and the underlying network architecture to significantly optimize the performance of communications whose information can be determined at compile time (static communications). The results indicate that compiled communication {{can be applied to}} {{a large portion of the}} communications in the benchmarks. In particular, the majority of collective communications are static...|$|R
40|$|We {{present the}} {{top-down}} {{design of a}} new system which performs automatic parallelization of numerical Fortran 77 or C source programs for execution on distributed-memory message-passing multiprocessors such as e. g. the INTEL iPSC 860 or the TMC CM- 5. The key idea is a high-level pattern-matching approach which in some useful way permits partial restructuring of a wide class of numerical programs. With {{only a few hundred}} patterns, {{we will be able to}} completely match many important numerical algorithms. Together with mathematical background <b>knowledge</b> and parallel <b>compiler</b> engineering experience, this opens access to a new potential for automatic parallelization that has never been exploited before...|$|R
40|$|P³T is an {{interactive}} performance estimator that assists users in performance tuning of scientific Fortran programs. It detects performance bottlenecks in the program, identifies {{the cause of}} performance problems, and advises the user on how to gain performance. Four {{of the most critical}} performance aspects of parallel programs are estimated: load balance, cache locality, communication and computation overhead. P³T is an integrated tool of the Vienna Fortran Compilation System, which enables the estimator to aggressively exploit considerable <b>knowledge</b> about the <b>compiler's</b> analysis information and code restructuring strategies. We evaluate existing features and describe substantial enhancements in three key areas: graphical user interface, performance parameters and input programs. P³T's graphica...|$|R
40|$|Abstract. We {{present the}} {{top-down}} {{design of a}} new system which per-forms automatic parallelization of numerical Fortran 77 or C source pro-grams for execution on distributed-memory message- passing multi-processors such as e. g. the INTEL iPSC 860 or the TMC CM- 5. The key idea is a high-level pattern-matching approach which in some useful way permits partial restructuring ofa wide class of numerical pro-grams. With {{only a few hundred}} patterns, {{we will be able to}} completely match many important numerical algorithms. Together with mathemat-ical background <b>knowledge</b> and parallel <b>compiler</b> engineering experience, this opens access to a new potential for automatic paxallelization that has never been exploited before. ...|$|R
40|$|Veristic {{computing}} {{is defined}} to mean computing with words. It necessarily entails {{the use of}} informed search in the solution of qualitatively constrained equations. Its use does not preclude computing with numbers. Versitic computing allows for the specification of higher-level programming languages, which can evolve domain-specific knowledge bases. The knowledge is evolved on a high-end computer for subsequent porting to a PC. The application of that knowledge to the translation of a higher-level program is termed, expert compilation. This paper serves to make clear the ubiquitous role assumed by randomization {{in all aspects of}} software engineering [...] from programming language design to program design to program testing to knowledge transference. Keywords Data Mining, Expert <b>Compilers,</b> <b>Knowledge</b> Discovery, Randomization, Transference. ...|$|R
40|$|In this research, we {{investigate}} the communication {{characteristics of the}} Message Passing Interface (MPI) implementation of the NAS parallel benchmarks and study the feasibility and effectiveness of compiled communication on MPI programs. Com-piled communication is a technique that utilizes the <b>compiler</b> <b>knowledge</b> of both the application communication requirement and the underlying network architecture to significantly optimize the performance of communications whose information can be determined at compile time (static communications). For static communications, constant propagation supports the compiled communication optimization approach. The analysis of NPB communication characteristics indicates that compiled commu-nication {{can be applied to}} {{a large portion of the}} communications in the benchmarks. In particular, the majority of collective communications are static. We conclude that the compiled communication technique is worth proceeding...|$|R
5000|$|The GOAL {{compiler}} {{is implemented}} in Allegro Common Lisp. It supports {{a long term}} compiling listener session which gives the <b>compiler</b> <b>knowledge</b> {{about the state of}} the compiled and therefore running program, including the symbol table. This, in addition to dynamic linking, allows a function to be edited, recompiled, uploaded, and inserted into a running game without having to restart. The process is similar to the [...] "edit and continue" [...] feature offered by some C++ compilers, but allows the programmer to replace arbitrary amounts of code (even up to entire object files), and does not interrupt the running game with the debugger. This feature was used to implement code as well as level streaming in the Jak and Daxter games.|$|R
40|$|P 3 T is an {{interactive}} performance estimator that assists users in performance tuning of scientific Fortran programs. It detects performance bottlenecks in the program, identifies {{the causes of}} performance problems, and guides the user in selecting effective program transformations {{in order to gain}} performance. Four of the most critical performance aspects of parallel programs are estimated: load balance, cache locality, communication and computation overhead. P 3 T is an integrated tool of the Vienna Fortran Compilation System, which enables the estimator to aggressively exploit considerable <b>knowledge</b> about the <b>compiler's</b> analysis information and code restructuring strategies. We evaluate existing features and describe substantial enhancements in three key areas: graphical user interface, performance parameters and input programs. P 3 T 0 s graphical user interface directs the user to bottlenecks in a computation that prevent the program from performing well. In addition, it [...] ...|$|R
40|$|Finding an {{optimized}} {{combination of}} compiler options that benefits the most a given embedded application {{is a challenge}} for most application developers. It is only with a deep understanding of the application at hand and a fairly good <b>knowledge</b> of the <b>compiler</b> features that a program-mer can achieve the desired results in terms of performance, power consumption and code size out of an application. We have developed a practical methodology for auto-matic exploring compiler options (UMECO) {{to solve the problem}} mentioned above. This paper reports a case study of this methodology on the Intel XScale microarchitecture. Based on practical experimentation, we enhance KCC, our research compiler infrastructure, with an extended user in-terface that the users can provide advice to. All are con-trolled by a reduced set of compiler flags. We also demon-strate a compiler trade-off strategy based on experimental results for a set of well known embedded benchmarks...|$|R
40|$|In {{this paper}} we {{describe}} a performance parameter which models the work {{contained in a}} parallel program and the corresponding work distribution. The work distribution is modeled at the program level which carefully distinguishes between useful and redundant work. We achieve high accuracy due to aggressive exploitation of <b>compiler</b> <b>knowledge</b> such as loop iteration spaces, array access patterns and data distributions. The underlying algorithm {{is based on the}} intersection and volume computation of n-dimensional linear and convex polytopes. The performance parameter has been implemented under the P 3 T, which is a static parameter based performance prediction tool under the Vienna Fortran Compilation System (VFCS). 1 Introduction In order to parallelize scientific applications for distributed memory systems such as the iPSC/ 860 hypercube, Meiko CS- 2, Intel Paragon, CM- 5 and the Delta Touchstone the programmer commonly decomposes the physical domain of the application [...] represented by a [...] ...|$|R
