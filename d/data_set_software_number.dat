0|10000|Public
50|$|Based {{on their}} <b>data</b> <b>set,</b> <b>software</b> company Searchmetrics {{found that the}} average loss of rankings for the non-mobile {{friendly}} sites measured was 0.21 positions on average. Content marketing company BrightEdge has tracked over 20,000 URLs since the update, and is reporting a 21% decrease in non mobile-friendly URLs on the first 3 pages of search results. According to Peter J. Meyers it was “nothing to write home about”.|$|R
40|$|<b>Data</b> <b>sets</b> {{provided}} herein {{are a part}} of a freely available {{database of}} annotated compound <b>data</b> <b>sets</b> and <b>software</b> tools developed in our laboratory for chemoinformatics and computational medicinal chemistry. The original release is described in the following article: Hu Y, Bajorath J: Freely available compound <b>data</b> <b>sets</b> and <b>software</b> tools for chemoinformatics and computational medicinal chemistry applications [v 1; ref status: indexed, [URL] F 1000 Research 2012; 1 : 11 (doi: 10. 12688 /f 1000 research. 1 - 11. v 1). The updated version containing the data provided herein will be described in a forthcoming data note in F 1000 Research...|$|R
40|$|A usable data base, the Pilot climate Data System (PCDS) is described. The PCDS is {{designed}} to be an interactive, easy-to-use, on-line generalized scientific information system. It efficiently provides uniform data catalogs; inventories, and access method, as well as manipulation and display tools for a large assortment of Earth, ocean and atmospheric data for the climate-related research community. Researchers can employ the PCDS to scan, manipulate, compare, display, and study climate parameters from diverse <b>data</b> <b>sets.</b> <b>Software</b> features, and applications of the PCDS are highlighted...|$|R
50|$|In addition, the CIS {{have the}} Standards Committee, dealing with standards, <b>data</b> <b>sets,</b> and <b>software</b> of {{interest}} to people working in CI, and the Technology Transfer Committee, promoting transfers of CI technologies to the industry.|$|R
50|$|Software mining {{is closely}} related to data mining, since {{existing}} software artifacts contain enormous business value, key for the evolution of software systems. Knowledge discovery from software systems addresses structure, behavior as well as the data processed by the software system. Instead of mining individual <b>data</b> <b>sets,</b> <b>software</b> mining focuses on metadata, such as database schemas. OMG Knowledge Discovery Metamodel provides an integrated representation to capturing application metadata as part of a holistic existing system metamodel. Another OMG specification, the Common Warehouse Metamodel focuses entirely on mining enterprise metadata.|$|R
40|$|Abstract: This paper {{describes}} {{a new technique}} for source-source transformation of sequential programs. We show that the transformed programs so generated provide significant speedups over the original program on vector-processors and vector-multiprocessors [...] We exploit the parallelism that arises when multiple instances of a program are executed on simultaneously available <b>data</b> <b>sets.</b> This {{is in contrast to}} the existing approaches that aim. at detecting parallelism within a program. Analytic and simulation models of our technique clearly indicate the speedups that could be achieved when several <b>data</b> <b>sets</b> are available simultaneously, {{as is the case in}} many fields of interest Index terms: vector multiprocessors, program unification, multiple <b>data</b> <b>sets,</b> <b>software</b> testing, urn model, order statistic, simulation. I...|$|R
40|$|We {{introduce}} Locally Linear Embedding (LLE) to {{the astronomical}} {{community as a}} new classification technique, using SDSS spectra as an example <b>data</b> <b>set.</b> LLE is a nonlinear dimensionality reduction technique which has been studied {{in the context of}} computer perception. We compare the performance of LLE to well-known spectral classification techniques, e. g. principal component analysis and line-ratio diagnostics. We find that LLE combines the strengths of both methods in a single, coherent technique, and leads to improved classification of emission-line spectra at a relatively small computational cost. We also present a data subsampling technique that preserves local information content, and proves effective for creating small, efficient training samples from a large, high-dimensional <b>data</b> <b>sets.</b> <b>Software</b> used in this LLE-based classification is made available. Comment: 51 pages, 12 figures, submitted to the Astronomical Journal. For associated code, see [URL]...|$|R
40|$|Feature subset {{selection}} {{is the process}} of choosing a subset of good features with respect to the target concept. A clustering based feature subset selection algorithm has been applied over <b>software</b> defect prediction <b>data</b> <b>sets.</b> <b>Software</b> defect prediction domain has been chosen due to the growing importance of maintaining high reliability and high quality for any software being developed. A software quality prediction model is built using software metrics and defect data collected from a previously developed system release or similar software projects. Upon validation of such a model, it could be used for predicting the fault-proneness of program modules that are currently under development. The proposed clustering based algorithm for feature selection uses minimum spanning tree based method to cluster features. And then the algorithm is applied over four different <b>data</b> <b>sets</b> and its impact is analyzed...|$|R
5000|$|In value theory, {{there is}} also the problem of {{so-called}} [...] "non-material goods and services", such as intellectual property (all kinds of texts, <b>data</b> <b>sets,</b> <b>software,</b> designs, techniques, knowledge, inventions, information services etc.). Obviously intellectual property existed already in Marx's time, but its scope and volume was fairly small. In modern times, in which science and education have become large-scale businesses, there is general tendency to attach a property right and a price-tag to more and more ideas, which are given precise boundaries. However, it remains unclear what regulates the value of intellectual property in an economic sense. How is the value of intellectual property correctly defined and calculated? Often the prices paid for intellectual assets are not proportional to real production costs.|$|R
40|$|Software {{design and}} {{sustainable}} software engineering {{are essential for}} the long-term development of bioinformatics software. Typical challenges in an academic environment are short-term contracts, island solutions, pragmatic approaches and loose documentation. Upcoming new challenges are big <b>data,</b> complex <b>data</b> <b>sets,</b> <b>software</b> compatibility and rapid changes in data representation. Our approach to cope with these challenges consists of iterative intertwined cycles of development (“Butterfly” paradigm) for key steps in scientific software engineering. User feedback is valued as well as software planning in a sustainable and interoperable way. Tool usage should be easy and intuitive. A middleware supports a user-friendly Graphical User Interface (GUI) {{as well as a}} database/tool development independently. We validated the approach of our own software development and compared the different design paradigms in various software solutions...|$|R
40|$|Abstract. A {{semantic}} wiki provides {{visualization of}} social media analysis applicable to military Information Operations {{and law enforcement}} counterterrorism efforts. Using inputs from disparate <b>data</b> <b>sets,</b> semantic <b>software</b> exports <b>data</b> to link analysis, geospatial displays, and temporal representation. Challenges encountered in software development include the balance between automated and human assisted entity extraction, interoperability with existing visualization systems and ontology management. ...|$|R
40|$|We present OpenML and mldata, {{open science}} {{platforms}} that provides {{easy access to}} machine learning data, software and results to encourage further study and application. They go beyond the more traditional repositories for <b>data</b> <b>sets</b> and <b>software</b> packages in that they allow researchers to also easily share the results they obtained in experiments and to compare their solutions with those of others...|$|R
40|$|Significant {{progress}} has been made in the past few years in the development of recommendations, policies, and procedures for creating and promoting citations to <b>data</b> <b>sets,</b> <b>software,</b> and other research infrastructures like computing facilities. Open questions remain, however, about the extent to which referencing practices of authors of scholarly publications are changing in ways desired by these initiatives. This paper uses four focused case studies to evaluate whether research infrastructures are being increasingly identified and referenced in the research literature via persistent citable identifiers. The findings of the case studies show that references to such resources are increasing, but that the patterns of these increases are variable. In addition, the study suggests that citation practices for <b>data</b> <b>sets</b> may change more slowly than citation practices for software and research facilities, due to the inertia of existing practices for referencing the use of data. Similarly, existing practices for acknowledging computing support may slow the adoption of formal citations for computing resources...|$|R
40|$|Software {{reliability}} is {{an important}} attribute of software engineering to ensure the success of software. Software reliability is the probability that there will no failure for a specified time. The reliability of the software depends on various attributes of software such as Size of <b>software,</b> <b>Number</b> of failures and Total time. These <b>data</b> <b>sets</b> of known <b>software</b> follow a specific trend which needs to be studied. The present work collects and analyzes these <b>data</b> <b>sets.</b> The training of these <b>data</b> <b>sets</b> is done through ANFIS. The relative error at definite epochs is noted. The software to be tested is then passed to same network that will give the desired result...|$|R
40|$|Engineers at Sandia National Laboratories are {{combining}} {{entertainment industry}} software with traditional data collection techniques {{to create an}} interactive visualization tool. By replacing the usual flight simulator joystick with a telemetry data stream, experimental data is combined with existing three-dimensional (3 D) engineering models. Users are immersed in their experiment, allowing interaction with and comprehension of complex <b>data</b> <b>sets.</b> <b>Software</b> tools are currently under development for post flight data visualization, and their usefulness and reusability have been demonstrated on numerous spaced-based programs within Sandia. However, data from remote sensors are subject to transmission errors that yield nonphysical behavior in real-time data visualization applications. We propose to investigate the applicability of real-time processing algorithms and estimation theories, such as Kalman filters, that have been successfully applied in other fields. Results will be integrated into existing postflight visualization tools for Proof-of-Concept validation and for potential integration of real-time applications...|$|R
40|$|The NASA Short-term Prediction Research and Transition Center (SPoRT) 's new "Weather in a Box" {{resources}} will provide weather research and forecast modeling capabilities for real-time application. Model output will provide additional forecast guidance and {{research into the}} impacts of new NASA satellite <b>data</b> <b>sets</b> and <b>software</b> capabilities. By combining several research tools and satellite products, SPoRT can generate model guidance that is strongly influenced by unique NASA contributions...|$|R
50|$|Paul Wessel and Walter H. F. Smith created GMT in 1988 at Lamont-Doherty Earth Observatory, officially {{releasing}} it on October 7, 1991 {{under the}} GNU General Public License. The letters GMT originally stood for Gravity, Magnetism and Topography, the three basic types of geophysical data. Besides its {{strong support for}} the visualisation of geographic <b>data</b> <b>sets,</b> the <b>software</b> includes tools for processing and manipulating multi-dimensional datasets. Most GMT users are geoscientists.|$|R
40|$|AbstractReproducibility of {{experiments}} is {{considered as one}} of the main principles of the scientiﬁc method. Recent developments in data and computation intensive science, i. e. e-Science, and state of the art in Cloud computing provide the necessary components to preserve <b>data</b> <b>sets</b> and re-run code and software that create research data. The Executable Paper (EP) concept uses state of the art technology to include <b>data</b> <b>sets,</b> code, and <b>software</b> in the electronic publication such that readers can validate the presented results. In this paper we present how to advance current state of the art to preserve, <b>data</b> <b>sets,</b> code, and <b>software</b> that create research data, the basic components of an execution platform to preserve long term compatibility of EP, and we identify a number of issues and challenges in the realization of EP...|$|R
40|$|To {{meet the}} {{intelligence}} community’s need for link analysis tools that work together, researchers are currently investigating ways of building workflows of these tools using an intelligent system architecture. A key challenge {{in building a}} dynamic link analysis workflow environment is representing {{the behavior of the}} individual link analysis algorithms being composed. In this paper, we outline techniques for modeling algorithms that allow a system architecture to reason about their behavior and performance, individually and in combination. The algorithm characterization model we propose is based on a layered approach, where the layers range from high-level qualitative descriptions of algorithms to detailed statistical descriptions of their effect on the data. Recent research and development in technology for intelligence analysis has produced a large number of tools, each of which addresses some aspect of the link analysis problem—the challenge of finding events, entities, and connections of interest in large relational <b>data</b> <b>sets.</b> <b>Software</b> developed in recent projects perform many diverse functions within link analysis, including detecting pre-define...|$|R
30|$|In {{environmental}} {{life cycle}} assessment (ELCA) the results are put in environmental impact categories, (e.g. global warming, acidification and human toxicity), {{which can be used}} for exploring and evaluating the trade-offs between alternatives (Stranddorf et al. 2005). A systematic overview inherit in ELCA enables identification of environmental burdens shifting between the life-cycle stages. However, large <b>data</b> <b>sets</b> may make it difficult to apply (Finkbeiner et al. 2006). <b>Data</b> <b>sets</b> and <b>software</b> are available for ELCA, although often there are licenses to be paid for their access and use.|$|R
40|$|Given {{a network}} {{structured}} <b>data</b> <b>set</b> (N spatially embedded nodes (xi,yj) and M edges, each edge Ek mapped to C edge costs) several software options are available allowing one to compute shortest paths between nodes in this <b>data</b> <b>set.</b> <b>Software</b> options include powerful GIS software engines such as ArcView and MapInfo, high level programming language implementations such as C++, Java or LISP and mathematical software such as Matlab and Mathematica. All software options {{find the same}} shortest, optimal, paths given the same input criteria. However one may differentiate between each software option based on {{the relative importance of}} a small number of criterion. Evaluation criterion can be summarised as software license cost, implementation time (software development hours), implementation cost, underlying data structures, shortest path query execution time (and hence query response time), etc. Current research work into establishing a performance efficiency hierarchy between Java, C++ and ArcView is described. Experimentation will be performed in order to statistically compare shortest path query execution time, response time and implementation issues of ArcView, C++ and Java performing shortest path computations on identical <b>data</b> <b>sets</b> and identical input parameters. This research will provide useful experimental results to GIS researchers uncertain of which implementation best matches their particular shortest path computation needs...|$|R
40|$|A PIV data {{validation}} and post-processing software package {{was developed to}} provide semi-automated {{data validation}} and data reduction capabilities for Particle Image Velocimetry <b>data</b> <b>sets.</b> The <b>software</b> provides three primary capabilities including (1) removal of spurious vector data, (2) filtering, smoothing, and interpolating of PIV data, and (3) calculations of out-of-plane vorticity, ensemble statistics, and turbulence statistics information. The software runs on an IBM PC/AT host computer working either under Microsoft Windows 3. 1 or Windows 95 operating systems...|$|R
40|$|Reproducibility of {{experiments}} is {{considered as one}} of the main principles of the scientific method. Recent developments in data and computation intensive science, i. e. e-Science, and state of the art in Cloud computing provide the necessary components to preserve <b>data</b> <b>sets</b> and re-run code and software that create research data. The Executable Paper (EP) concept uses state of the art technology to include <b>data</b> <b>sets,</b> code, and <b>software</b> in the electronic publication such that readers can validate the presented results. In this paper we present how to advance current state of the art to preserve, <b>data</b> <b>sets,</b> code, and <b>software</b> that create research data, the basic components of an execution platform to preserve long term compatibility of EP, and we identify a number of issues and challenges in the realization of EP. © 2011 Published by Elsevier Ltd...|$|R
40|$|The {{purpose of}} this {{document}} is to describe the Cassini/Huygens science data archive system which includes policy, roles and responsibilities, description of science and supplementary data products or <b>data</b> <b>sets,</b> metadata, documentation, <b>software,</b> and archive schedule and methods for archive transfer to the NASA Planetary Data System (PDS) ...|$|R
40|$|The fossil {{software}} package {{is a collection}} of analytical tools to synthetically anal-yse ecological and geographical <b>data</b> <b>sets.</b> The <b>software</b> is designed to be used with the R Statistical Language and is under an Open Source license, making it free to download, use or modify. The package includes functions for estimating species rich-ness, shared species/beta diversity, species area curves and geographic distances and areas. The package also contains extensive documentation and examples of how to use all of the functions...|$|R
50|$|Another {{promising}} {{application of}} knowledge discovery {{is in the}} area of software modernization, weakness discovery and compliance which involves understanding existing software artifacts. This process is related to a concept of reverse engineering. Usually the knowledge obtained from existing software is presented in the form of models to which specific queries can be made when necessary. An entity relationship is a frequent format of representing knowledge obtained from existing software. Object Management Group (OMG) developed specification Knowledge Discovery Metamodel (KDM) which defines an ontology for the software assets and their relationships for the purpose of performing knowledge discovery of existing code. Knowledge discovery from existing software systems, also known as software mining is closely related to data mining, since existing software artifacts contain enormous value for risk management and business value, key for the evaluation and evolution of software systems. Instead of mining individual <b>data</b> <b>sets,</b> <b>software</b> mining focuses on metadata, such as process flows (e.g. data flows, control flows, & call maps), architecture, database schemas, and business rules/terms/process.|$|R
40|$|Approved {{for public}} release, {{distribution}} unlimitedThe use of Airborne LiDAR Systems (ALS) to obtain topographical information of the earth's surface and generate Digital Elevation Models (DEMs) has grown {{extensively in the}} field of Remote Sensing. Selected areas of point cloud LiDAR data collected from Honduras in 2008 was used to produce DEMs with varying densities to show the effects of lower resolution LiDAR data. An IDL code was utilized to reduce the selected LiDAR point cloud data to 90 %, 66 %, 50 %, 30 %, 10 %, 5 %, 3 %, 1 %, 0. 5 %, 0. 3 %, 0. 1 %, 0. 05 %, 0. 03 %, and 0. 01 % of its original density to obtain lower resolution <b>data</b> <b>sets.</b> The <b>software</b> Quick Terrain Modeler (QTM) and its ILAP Bare Earth Extractor Plug-in was used to generate DEMs from the varying point cloud density <b>data</b> <b>sets</b> and the <b>software</b> ENVI was used to perform DEM analysis. It was found that LiDAR point cloud density <b>data</b> <b>set</b> of at least 0. 6 points per square meter is necessary to generate an accurate Digital Elevation Model for the test environment. US Navy (USN) author...|$|R
40|$|Abstract. Current digital {{libraries}} for scholar publications are facing new challenges to facilitate discovery {{and access to}} digital objects distinct from the traditional full-text documents, e. g. figures, <b>data</b> <b>sets</b> or <b>software</b> related to scientific developments. This work presents {{an extension of the}} data storage model of Invenio, a digital library platform developed at CERN. We concentrate on fulfilling requirements arising while extending INSPIRE, the information resource in High Energy Physics, with storage of figures and preservation of data files on which publications are based...|$|R
40|$|This article {{describes}} the first CHIL evaluation campaign in which 12 technologies were evaluated. The major outcomes of the first evaluation campaign are the so-called Evaluation Packages. An evaluation package is the full documentation (definition and description of the evaluation methodologies, protocols and metrics) alongside the <b>data</b> <b>sets</b> and <b>software</b> scoring tools, which an organisation needs in order to perform the evaluation {{of one or more}} systems for a given technology. These evaluation packages will be made available to the community through ELDA General Catalogue...|$|R
30|$|First, we verify our VLSI {{algorithm}} {{using the}} Middlebury <b>data</b> <b>set</b> with a <b>software</b> simulation. In the previous sections, we presented a new architecture which {{is equivalent to}} HBP in terms of input-output relationship and which is a systolic array with a small memory space. Hence, it is suitable for VLSI implementation.|$|R
40|$|Understanding and {{interpreting}} large <b>data</b> <b>sets</b> {{is an important}} but challenging operation in many technical disciplines. Computer visualization has become a valuable tool to help portray characteristics of large <b>data</b> <b>sets.</b> In <b>software</b> visualization, illustrating the operation of very large programs or programs working on very large <b>data</b> <b>sets</b> has remained {{one of the key}} open problems. Here, we introduce an approach that usesemantic zooming to depict largeprogram executions. Our method utilizes abstract, clustered graphics to portray program operations on the entire <b>data</b> <b>set.</b> Then, by interacting with the presentation, a viewer can zoom in to examine details and individual values. At this “magnijed ” level, the presentation adjusts to reflect displays common in existing algorithm animation and program visualization systems, 1...|$|R
40|$|We have {{generated}} a number of compound <b>data</b> <b>sets</b> and programs for {{different types of}} applications in pharmaceutical research. These <b>data</b> <b>sets</b> and programs were originally designed for our research projects and are made publicly available. Without consulting original literature sources, {{it is difficult to}} understand specific features of <b>data</b> <b>sets</b> and <b>software</b> tools, basic ideas underlying their design, and applicability domains. Currently, 30 different entries are available for download from our website. In this data article, we provide an overview of the data and tools we make available and designate the areas of research for which they should be useful. For selected <b>data</b> <b>sets</b> and methods/programs, detailed descriptions are given. This article should help interested readers to select data and tools for specific computational investigations...|$|R
50|$|In 2002, as {{a project}} for the British Columbia Ministry of Sustainable Resource Management, Vivid Solutions Inc. created a {{software}} program to do automated matching ("conflation") of roads and rivers from different digital maps into an integrated single geospatial <b>data</b> <b>set.</b> The <b>software</b> team wisely made the program flexible enough to be used not just for roads and rivers, but almost any kind of spatial data: provincial boundaries, power-station locations, satellite images, and so on. The program was named JUMP (JAVA Unified Mapping Platform), {{and it has become}} a popular, free Geographic Information System (GIS).|$|R
40|$|Large public <b>data</b> <b>sets</b> on <b>software</b> {{evolution}} promise {{great value}} to both researchers and practitioners, in particular for software (development) analytics. To realise this value, the data quality of such <b>data</b> <b>sets</b> {{needs to be}} studied and improved. Despite these <b>data</b> <b>sets</b> being of a secondary nature, i. e., they were not collected by the people using them, data quality is often taken for granted, casting doubt on conclusions drawn from those data. This paper reports on an intial investigation {{of the quality of}} the software evolution data available on Ohloh, and further describes steps taken to cleanse the <b>data</b> <b>set.</b> Our goal is that other researchers, practitioners, and parties responsible for <b>data</b> <b>sets</b> such as Ohloh, use the outcomes of the validation and cleansing steps to improve quality of <b>data</b> <b>sets</b> in the public domain...|$|R
40|$|Understanding and {{interpreting}} a large data source {{is an important}} but challenging operation in many technical disciplines. Computer visualization has become a valuable tool to help capture and portray characteristics of large <b>data</b> <b>sets.</b> In <b>software</b> visualization, illustrating the operation of very large programs or programs working on very large <b>data</b> <b>sets</b> has remained {{one of the key}} open problems. Here, we introduce an approach that uses semantic zooming to depict large program executions. Our method utilizes abstract, clustered graphics to portray program operations on the entire <b>data</b> <b>set.</b> Then, by interacting with the presentation, a viewer can zoom in to examine details and individual values. At this "magnified" level, the presentation adjusts to reflect displays common in existing algorithm animation and program visualization systems...|$|R
40|$|The 3 D {{multimedia}} applications {{have been}} experiencing recently a tremendous growth {{in number and}} complexity. Such applications mainly consist of complex algorithms that process extensive amounts of data to create 3 D images and results. For quick access, data need to be stored in small and expensive memories near the processor. Due to the increasing memory-processor gap in speed and the characteristics of multimedia applications (with highly power- and spaceconsuming <b>data</b> <b>sets),</b> <b>software</b> transformations are required to decrease memory requirements. In this paper, we propose a method to reduce the indirections of data types in real 3 D multimedia applications. It is based on software transformations of the original algorithm to minimize the intermediate assignments and, as such, the required data types. To assess the performance of our method, we apply it to a relatively new 3 D image reconstruction application. As a result, for this multimedia application, our method reduces 50 × the amount of memory accesses, 30 × the normalized memory footprint and 67 × the energy consumption compared to a manually well-optimized version of the algorithm. Finally, compared to the original application, the overall performance improves by 40 % on a PC. 1...|$|R
