0|34|Public
40|$|The {{problem of}} medium to {{long-term}} sales <b>forecasting</b> <b>raises</b> {{a number of}} requirements that must be suitably addressed {{in the design of}} the employed forecasting methods. These include long forecasting horizons (up to 52 periods ahead), a high number of quantities to be forecasted, which limits the possibility of human intervention, frequent introduction of new articles (for which no past sales are available for parameter calibration) and withdrawal of running articles. The problem has been tackled by use of a damped-trend Holt&# 8211;Winters method as well as feedforward multilayer neural networks (FMNNs) applied to sales data from two German companies...|$|R
40|$|Abstract: The {{problem of}} medium to long term sales <b>forecasting</b> <b>raises</b> {{a number of}} re-quirements that must be {{suitably}} addressed {{in the design of}} the employed forecasting methods. These include long forecasting horizons (up to 52 periods ahead), a high num-ber of quantities to be forecasted, which limits the possibility of human intervention, as well as frequent introduction of new articles (for which no past sales are available for pa-rameter calibration) and withdrawal of running articles. The problem has been tackled by use of a modified Holt-Winters method as well as Feedforward Multilayer Neural Net-works (FMNN) applied to sales data from two German companies. Copyright © 2005 IFA...|$|R
5000|$|After {{an inquiry}} from August 2008 - March 2009, the UK Competition Commission {{announced}} that BAA {{would be required}} to sell three of the seven UK airports it owned at the time. These were Gatwick, Stansted and either Glasgow or Edinburgh airports within two years over fears the monopoly position held by BAA over London and Scotland's airports could have [...] "adverse effects for both passengers and airlines". The sales were <b>forecast</b> to <b>raise</b> between £3.5bn and £4bn.|$|R
40|$|Under {{the auspices}} of the World Meteorological Organization, {{there are a number of}} {{international}} initiatives to promote the development and use of so-called ensemble prediction systems (EPS) for flood forecasting. The campaign to apply these meteorological techniques to flood <b>forecasting</b> <b>raises</b> important questions how the probabilistic information these systems provide can be applied for what in operational terms is typically a binary decision of whether or not to issue a flood warning. To explore these issues, we report on the results of a series of focus group discussions conducted with operational flood forecasters from across Europe on behalf of the European Flood Alert System. Working in small groups to simulate operational conditions, forecasters engaged in a series of carefully designed forecasting exercises. using various different combinations of actual data from real events. Supplemented by results from a follow-up questionnaire survey, these exercises suggest that flood forecasters may understand risk and uncertainty differently than promoters of EPS intend. The paper concludes by exploring the implications of these divergent ‘epistemic cultures’ for efforts to apply ensemble prediction techniques developed in the context of weather forecasting to the rather different one of flood forecasting. JRC. H. 7 -Land management and natural hazard...|$|R
40|$|Based on an {{assembly}} supply chain consisting of multiple complementary component suppliers and one manufac-turer, we study what effects does ex-change rate {{have on the}} decision-making and profit of the enterprises in the assem-bly supply chain through constructing a game model. The {{results show that the}} manufacture’s optimal order quantity and every enterprise’s profit will increase with the increase of expected exchange rate. This illustrates the manufacturer should increase order quantity if success-fully <b>forecast</b> the <b>raise</b> of exchange rate, and all the enterprises in the supply chain will benefit from the behavior...|$|R
40|$|In this paper, {{we explore}} the {{managerial}} decision-making process {{with a particular}} eye on why managers are timing the interest rate market. We ask whether the documented sensitivity of interest rate swap usage to the term structure {{is a function of}} managers trying to meet earnings forecasts, attempting to boost near-term results prior to raising external capital, or simply to increase their compensation? Using a very large, hand-collected dataset of swap activity, our empirical findings suggest that the choice of interest rate exposure is primarily driven by a concern to meet consensus earnings <b>forecasts</b> and <b>raise</b> managerial pay...|$|R
30|$|Time series {{models are}} good at {{analyzing}} and forecasting long-term data, which has clear trend and regular fluctuation. Therefore, this paper uses weekly price in time series models, by calculating the average daily prices in 1  week [27]. The purpose is to <b>raise</b> <b>forecasting</b> accuracy, by avoiding the influence of fluctuation and abnormal amplitude.|$|R
50|$|On March 12, 2014, the Servicio Meteorológico Nacional (SMN) {{issued its}} first {{outlook for the}} Pacific {{hurricane}} season, expecting a total of fifteen named storms, seven hurricanes, and three major hurricanes. A month later, the agency revised their outlook to fourteen named storms, seven hurricanes, and five major hurricanes, citing the anticipated development of El Niño for above-average activity, compared to the 1949-2013 average of 13.2, On May 22, the Climate Prediction Center (CPC) announced its prediction of 14 to 20 named storms, seven to eleven hurricanes, three to six major hurricanes, and an Accumulated Cyclone Energy (ACE) within 95-160% of the median. It also called for a 50% chance of an above-normal season, a 40% chance of a near-normal season, and a 10% chance of a below-normal season. Similar to the SMN outlook, {{the basis for the}} forecast was the expectation of below average wind shear and above average sea surface temperatures, both factors associated with El Niño conditions. The CPC also noted that the Eastern Pacific was in a lull that first began in 1995; however, they expected that this would be offset by the aforementioned favorable conditions. Within the Central Pacific Hurricane Center (CPHC)'s jurisdiction, four to seven tropical cyclones were expected to form, slightly above the average of four to five tropical cyclones. On July 31, the SMN released their final <b>forecast,</b> <b>raising</b> the numbers to 17 named storms, 8 hurricanes and 5 major hurricanes.|$|R
40|$|Recent {{evidence}} suggests that all asset returns are predictable to some extent with excess returns on real estate relatively easier to <b>forecast.</b> This <b>raises</b> {{the issue of whether}} we can successfully exploit this level of predictability using various market timing strategies to realize superior performance over a buy-and-hold strategy. We find that the level of predictability associated with real estate leads to moderate success in market timing, although this is not necessarily the case for the other asset classes examined in general. Besides this, real estate stocks typically have higher trading profits and higher mean risk-adjusted excess returns when compared to small stocks as well as large stocks and bonds even though most real estate stocks are small stocks...|$|R
50|$|On July 6, TSR {{released}} {{their second}} forecast for the season. They predicted mostly the same numbers {{as the previous}} <b>forecast,</b> but <b>raised</b> the number of intense typhoons to 7. PAGASA issued their second and final forecast for the year on July 15, within its seasonal climate outlook for the period July - December. The outlook noted that between five and eleven tropical cyclones were expected between July and September, while four to nine were expected to develop or enter the Philippine Area of Responsibility between October and December. TSR issued their final forecast for the season on August 8, sustaining the tropical cyclone numbers, however its ACE was slightly lowered than the previous forecast.|$|R
40|$|This paper {{examines}} the forecasting implications for Product-Service Systems (PSS) applications in manufacturing firms. The approach taken {{is to identify}} the scope of operations for PSS applications by identifying all the activities associated with {{the total cost of}} ownership (TCO). The paper then develops a revenue model for manufacturing firms providing PSS applications. The revenue model identifies three generic revenue streams that provide the basis for discussion on the differences in forecasting approaches between product firms and Product-Service Systems (PSS) in manufacturing firms. The forecasting approaches are different {{due to the nature of}} customer involvement in the service aspect of PSS applications. This necessitates an understanding of the customer service experience and the factors affecting this such as the service profit chain which links profitability, customer loyalty and service value to employee satisfaction, capability and productivity. The <b>forecasting</b> approaches identified <b>raises</b> <b>forecasting</b> challenges for each of the three generic revenue sources. These challenges vary from the difficulty in obtaining the service user’s viewpoint through to difficulties in determining market acceptance of PSS applications...|$|R
5000|$|The first {{forecast}} {{for the year}} was issued by TSR on December 13, 2016. They anticipated that the 2017 season would be a near-average season, with a prediction of 14 named storms, 6 hurricanes, and 3 major hurricanes. They also predicted an ACE index of around 101 units. On December 14, CSU released a qualitative discussion detailing five possible scenarios for the 2017 season, {{taking into account the}} state of the Atlantic Multidecadal Oscillation and the possibility of El Niño developing during the season. TSR lowered their forecast numbers on April 5, 2017 to 11 named storms, 4 hurricanes, and 2 major hurricanes, based on recent trends favoring the development of El Niño. The next day, CSU released their prediction, also predicting a total of 11 named storms, 4 hurricanes, and 2 major hurricanes. On April 17, The Weather Company released their forecasts, calling for 2017 to be a near-average season, with a total of 12 named storms, 6 hurricanes, and 2 major hurricanes. The next day, on April 18, North Carolina State University released their prediction, also predicting a near-average season, with a total of 11-15 named storms, 4-6 hurricanes, and 1-3 major hurricanes. On May 20, The Weather Company issued an updated <b>forecast,</b> <b>raising</b> their numbers to 14 named storms, 7 hurricanes, and 3 major hurricanes to account for Tropical Storm Arlene as well as the decreasing chance of El Niño forming during the season. On May 25, NOAA released their prediction, citing a 70% chance of an above average season due to [...] "a weak or nonexistent El Niño", calling for 11-17 named storms, 5-9 hurricanes, and 2-4 major hurricanes. On May 26, TSR updated its prediction to around the same numbers as its December 2016 prediction, with only a minor change in the expected ACE index amount to 98 units.|$|R
40|$|In 1983 and 1984, the United States economy {{staged a}} {{vigorous}} economic recovery {{at a time}} when the value of the dollar was high and rising, leading to a steady deterioration of the trade balance. The strength of both the economy and the dollar exceeded most <b>forecasts.</b> This <b>raises</b> a question: are there expansionary effects from a currency appreciation that are overlooked when we focus solely on the trade balance?; The purpose of this paper is to try and resolve the issue of whether or not an appreciation of the dollar is expansionary. To do so, I evaluate the simulation properties of the MPS and MCM models. As part of this evaluation, I develop a "back-of-the-envelope" model to serve as a third alternative against which to compare the two large econometric models. ...|$|R
30|$|Prediction {{of ozone}} levels using a {{theoretical}} method (i.e. detailed atmospheric diffusion model) {{is difficult and}} empirical analysis is required to develop a forecasting system. A well-evaluated ozone <b>forecast</b> model can <b>raise</b> the possibility of successful ozone control strategy. In addition, forecasting the daily maximum ozone concentrations can help avoid and reduce ozone-related injuries and damages. This research is significant, {{as it is the}} first study comparing the ozone forecasting systems across different locations in Kuwait.|$|R
40|$|Real house {{prices are}} {{forecast}} to rise {{over the next}} two decades by about the same amount hey rose over the past two decades. The longer-run positive trends in real incomes and population size and the advance of the baby boom into ages of greater effective demand for houses are <b>forecast</b> to <b>raise</b> real house prices 10 %. Simulation indicates that the demand for houses, and thus the prices of houses, may fall when baby boomers reach adulthood, because the effects of the larger population may be more than offset by the effects of the lower individual real incomes of young workers. As boomers age, their changing preferences and rising incomes raise simulated house prices above their initial levels. Simulated house prices remain above their initial levels until the baby boom generation has died off. © 1991 Academic Press, Inc. I...|$|R
50|$|Tony Peck (born September 5, 1983 in Louisiana, Missouri) is {{the drummer}} for Peoria, Illinois based band, The <b>Forecast.</b> He was <b>raised</b> in Pleasant Hill, Illinois {{and is a}} 2002 {{graduate}} of Pleasant Hill High School where he {{was active in the}} school's music programs and was a 2002 recipient of the Arion Award. After graduating from high school he began touring and recording with several bands, including The Junior Varsity, for two years before being asked to join The Forecast in July 2004. Tony currently resides in Quincy, Illinois.|$|R
40|$|First {{chapter of}} my {{dissertation}} uses an EGARCH method and a Stochastic Volatility (SV) method which relies upon Markov Chain Monte Carlo (MCMC) framework based on Efficient Importance Sampling (EIS) to model inflation volatility of Turkey. The strength of SV model {{lies in its}} success in explaining time varying and persistence volatility. This chapter uses the CPI index of Turkey as the inflation measure. The inflation series suffer from four exchange rate crisis in Turkey during this period. Therefore two different models are estimated for both EGARCH and SV models; with crisis dummies and without dummies. Comparison of different model results for EGARCH and SV models indicate the robustness problem for EGARCH and that SV model is far more robust than EGARCH. Stochastic Volatility (SV) models typically exhibit short-term dynamics with high persistence. It follows that volatility is conceptually predictable. Since, however, it is not observable; the validation of SV <b>forecasts</b> <b>raises</b> non-trivial issues. In second chapter I propose a new test statistics to evaluate the validity of one-step-ahead forecasts of returns unconditionally on volatility. Specifically, I construct a Kolmogorov-Smirnov test statistic for the null hypothesis that the predicted cumulative distribution of return evaluated at observed values is uniform. Estimation of the SV model is based upon an Efficient Importance Sampling procedure. Applications of this test statistic to quarterly data for inflation in the U. S. and Turkey fully support the validity of one-step-ahead SV forecasts of inflation. The basic SV model assumes that volatility is just explained by its first order lag. In the last chapter of my dissertation (coauthored with Jean-Francois Richard) we show {{that the difference between}} return and monthly moving average do granger-cause volatility. 35 S&P 500 stock return applications from six different industries show that the difference parameter is both significant and addition of this variable to volatility equation affects both the persistence parameter and the standard deviation of volatility. Persistence increases with the inclusion of difference variable. Furthermore standard deviation of volatility decreases which is the indication of Granger-Causality. Likelihood-ratio (LR) test results also prove that the model improves when the difference variable is added...|$|R
40|$|The {{enhanced}} {{availability of}} many different hydro-meteorological modelling and <b>forecasting</b> systems <b>raises</b> {{the issue of how}} to optimally combine this great deal of information. Especially the usage of deterministic and probabilistic forecasts with sometimes widely divergent predicted future streamflow values makes it even more complicated for decision makers to sift out the relevant information. In this study multiple streamflow forecast information will be aggregated based on several different predictive distributions, and quantile forecasts. For this combination the Bayesian model averaging (BMA) approach, the non-homogeneous Gaussian regression (NGR), also known as the ensemble model output statistic (EMOS) techniques, and a novel method called Beta-transformed linear pooling (BLP) will be applied. By the help of the quantile score (QS) and the continuous ranked probability score (CRPS), the combination results for the Sihl River in Switzerland with about 5 years of forecast data will be compared and the differences between the raw and optimally combined forecasts will be highlighted. The results demonstrate the importance of applying proper forecast combination methods for decision makers in the field of flood and water resource management...|$|R
40|$|Short-term {{traffic flow}} data is {{characterized}} by rapid and dramatic fluctuations. It reflects {{the nature of the}} frequent congestion in the lane, which shows a strong nonlinear feature. Traffic state estimation based on the data gained by electronic sensors is critical for much intelligent traffic management and the traffic control. In this paper, a solution to freeway traffic estimation in Beijing is proposed using a particle filter, based on macroscopic traffic flow model, which estimates both traffic density and speed. Particle filter is a nonlinear prediction method, which has obvious advantages for traffic flows prediction. However, with the increase of sampling period, the volatility of the traffic state curve will be much dramatic. Therefore, the prediction accuracy will be affected and difficulty of <b>forecasting</b> is <b>raised.</b> In this paper, particle filter model is applied to estimate the short-term traffic flow. Numerical study is conducted based on the Beijing freeway data with the sampling period of 2 min. The relatively high accuracy of the results indicates the superiority of the proposed model. Department of Electrical EngineeringRefereed conference pape...|$|R
5000|$|Immediately {{following}} Kevin Rudd's replacement as Prime Minister by Julia Gillard, the Government did a {{deal with}} the largest mining companies to replace the RSPT with a new tax - the Minerals Resource Rent Tax (MRRT). The Government claimed the new tax would raise $10.6 billion in its first two years, just $1.5 billion less than the $12 billion that RSPT had been <b>forecast</b> to <b>raise.</b> It was quickly realised that this was a wildly optimistic estimate. Professor John Quiggin said, [...] "All the changes that were made to the package between the original tax and the agreement they reached in the end were too generous." [...] Prior to the introduction of the MRRT in the May 2012 budget, the government revised down its forecasts, suggesting that the tax would only bring in $3 billion for the financial year. In October 2012, the figure was reduced to $2 billion, while on 14 May 2013, it was announced that the receipts were expected to be less than $200 million.|$|R
40|$|Social {{behaviour}} depends crucially {{on the way}} {{events are}} linked over time, and on how these linkages are perceived. From a given event, people {{may be able to}} infer what followed, or what preceded it. However these two tasks are not as similar as they may seem. Two experiments are reported in which participants had to infer subsequent events given earlier ones, or else the reverse. Performance was consistently more accurate when working ‘backwards’. We call this the ‘inverse <b>forecast</b> effect’. It <b>raises</b> issues about the strategies people use to predict and understand everyday events, and about just how the future is formed from the past...|$|R
40|$|One of {{the most}} {{important}} objectives of channel relationship management concerns how to effectively monitor the changes in each competitor’s performance in different types of channel structures in order to grasp the dynamic state of competition. Unfortunately, competitor performance data is usually unavailable from a company’s management information system. Under the condition of incomplete information, firms combine market share-related data from both internal and external sources and make predictions based on statistical models to ensure that they produce accurate market share information. In this paper, we propose a Bayesian model to improve the accuracy of market <b>forecasts</b> and thereby <b>raise</b> the quality of channel relationship management and help managers formulate more appropriate marketing strategies...|$|R
40|$|Collective Adaptive Systems (CAS) {{consist of}} a large number of {{spatially}} distributed heterogeneous entities with decentralised control and varying degrees of complex autonomous behaviour that may be competing for shared resources even when collaborating to reach common goals. It is important to carry out thorough quantitative modelling and analysis and verification of their design to investigate all aspects of their behaviour before they are put into operation. This requires combinations of formal methods and applied mathematics which moreover scale to large-scale CAS. The primary goal of <b>FORECAST</b> is to <b>raise</b> awareness in the software engineering and formal methods communities of the particularities of CAS and the design and control problems which they bring...|$|R
30|$|The Working Tax Credit {{provides}} {{incentives to}} married women who increase their participation rate up to 3 percentage points. Contrary to the Italian system, the working tax credit {{has all of}} the characteristics of an individual taxation system. In fact, tax credits or transfers (and hence, second earner tax rates) do not depend on the spouse’s income and hence do not vary with the marital status. This is shown in Figure 9, Additional file 1 : Appendix D, panels b) and d), where the SET is constant at about 20 percent for married women. Similarly, panels a) and c) show that the SET changes only with women’s income. Another interesting feature of this system is that it provides incentives to undertake low earnings jobs. As we can see in Figure 9 (panels a) and c)), the SET is particularly low (and even negative) at low levels of earnings. This prediction can also be seen from Figure 7, bottom panel, where the raise in labor supply mostly comes from low educated mothers. The model <b>forecasts</b> a <b>raise</b> in part-time employment rates of married men and women of about 2 percentage points. Unmarried individuals do not benefit from the simulated tax system.|$|R
30|$|This {{paper is}} little more than a {{preliminary}} draft that needs to be developed in different directions. Three issues at least deserve further work. First, the paper considers foresight methods only. Extending the proposed classification to <b>forecasting</b> will <b>raise</b> the problem of the interaction between risk and uncertainty [7, 8] and, more generally, the problem of whether statistical methods either define or complement the professional identity of the futurist. My position is that statistical methods primarily complement the professional identity of the futurist. The decision about megatrends requires sufficiently robust criteria for distinguishing trends from megatrends. Finally, the connection with anticipation presents its difficulties as well, essentially when dealing with the tangled issue of future generating methods [10]. Second, the interaction between futures methods and the methods of other fields, such as those of critical and integral studies, has generated more nuanced versions of futures methods. Analysis of the methods that primarily define the work of a futurist does not deny interaction and mutual development. The old saying “distinguish to unite” remains valid. Third, the paper has not addressed the issue of combining methods; therefore, I have not discussed neither Voros’ generic framework [34] nor Inayatullah’s six pillars [17] or the six steps models of Bishop and Hines [3]. All this issues require further analysis that I must postpone to subsequent papers.|$|R
40|$|This paper simulates out-of-sample {{inflation}} forecasting for Germany, the UK, and the US. In {{contrast to}} other studies, we use output gaps estimated with unrevised real-time GDP data. This exercise assumes an information set {{similar to that}} available to a policymaker at a given point in time since GDP data is subject to sometimes substantial revisions. In addition to using real-time datasets for the UK and the US, we employ a dataset for real-time German GDP data not used before. We find that Phillips curves based on ex post output gaps generally improve the accuracy of inflation forecasts compared to an AR(1) forecast but that real-time output gaps often do not help <b>forecasting</b> inflation. This <b>raises</b> the question how operationally useful certain output gap estimates are for forecasting inflation. Economic forecasting;Economic growth;Gross domestic product;inflation, monetary policy, forecasting inflation, inflation forecasts, monetary policy rules, monetary fund, inflation data, inflationary pressures, actual inflation, monetary indicators...|$|R
40|$|Abstract. We {{approached}} {{problems of}} paleobathymetry from two different, but interconnected directions: from a paleoecological {{point of view}} and from a sedimentological one. To approximate from paleoecology, in our preliminary study (Kovács and Arnaud-Vanneau, 2004), we defined 6 paleoecological assemblages focusing solely on the Pleşca Valley 2 outcrop. After analyzing correlative sections, as we had actually <b>forecast,</b> it <b>raised</b> the demand of restructuring previous working assemblages into new ones, built up from sub-assemblages reflecting more restrictive ecological conditions. The reasons for this complementation are that several depositional profiles replace each other during basin morphology evolution, and secondly, substrate and trophic conditions also might change from one paleogeographic location to other. Initial strandplain carbonate ramp (Pleşca Valley 1) evolves to a beach barrier carbonate ramp (Cheile Baciului), locally to a reef-mound beach barrier ramp (Pleşca Valley 2, Turea) and finally to a siliciclastic delta profile (Mera). Each profile from the above is holding its own particular facies zones, reflected by particular paleoecological assemblages and typical sedimentary structures. Considering a shallowing depth gradient, depositional environments and their dominant IDRA assemblages will align one after other as follows: I. offshore basin; IIa. Proximal outer ramp-outer middle ramp – muddy substrate, oligophotic, oligotrophic; IIb. Outer middle ramp shoals – mezophotic; IIc. Outer middle ramp – probably eutrophic; IIIa. Middle ramp – reef mounds; IIIb. Middle ramp – sandy substrate; IVa Inner middle ramp bioherms; IVb Inner middle ramp – sandy substrate; IVc. Inner middle ramp – probably eutrophic; Va. Outer inner ramp shoals – euphotic; Vb. Inner ramp – lagoon; VI. Proximal inner ramp; VII Nearshore...|$|R
40|$|The {{ability to}} {{forecast}} seasonal climate {{is of great}} practical interest. One of the most obvious benefits would be agriculture, for which various preparations (planting, machinery, irrigation, manpower) would be enabled. The expectation {{of being able to}} make such forecasts far enough in advance (on the order of 9 months) hinges on components of the system with the longest persistence or predictability. The mixed results of El Nino <b>forecasts</b> has <b>raised</b> the hope that tropical Pacific sea surface temperatures (SST) fall into this category. For agriculturally-relevant forecasts to be made, and utilized, requires several conditions. The SST in the regions that affect agricultural areas must be forecast successfully, many months in advance. The climate response to such sea surface temperatures must then be ascertained, either through the use of historical empirical studies or models (e. g., GCMS). For practical applications, the agricultural production must be strongly influenced by climate, and farmers on either the local level or through commercial concerns must be able to adjust to using such forecasts. In a continuing series of papers, we will explore each of these components. This article concerns the question of utilizing SST to forecast the climate in several regions of agricultural production. We optimize the possibility of doing so successfully by using observed SST in a hindcast mode (i. e., a perfect forecast), and we also use the globally observed values (rather than just those from the tropical Pacific, for which predictability has been shown). This then is the ideal situation; in subsequent papers we will explore degrading the results by using only tropical Pacific SSTs, and then using onl...|$|R
40|$|The {{official}} {{estimate of}} real GDP {{growth for the}} first three months of 2015 was shockingly weak. However, such estimates in the past appear to have understated first-quarter growth fairly consistently, even though they are adjusted to try to account for seasonal patterns. Applying a second round of seasonal adjustment corrects this residual seasonality. After this correction, aggregate output grew much faster in the first quarter than reported. In late April, the Bureau of Economic Analysis (BEA) released its initial estimate of U. S. economic growth {{for the first three}} months of 2015. The report was very disappointing, as inflation-adjusted, or real, gross domestic product (GDP) edged up a mere 0. 2 % at an annual rate in the first quarter. This estimate was far weaker than many economists had <b>forecast,</b> and it <b>raised</b> concerns that the underlying economic recovery may have stalled. Such anemic growth is of particular concern to Federal Reserve policymakers considering when to begin normalizing monetary policy. However, a number of analysts have suggested that the reported weakness in first-quarter growth may have been exaggerated by a statistical anomaly (see, for example, Liesman 2015 and Wolfers 2015). Indeed, an unusual pattern has prevailed for some time in which first-quarter real GDP growth is generally lower than growth later in the year. This regular, calendar-based statistical pattern is a puzzle because th...|$|R
40|$|The aim of {{the study}} is to {{investigate}} possibilities to manage forecast errors at the Nordic power market based {{on the size of the}} actor. This is part of a larger question at hand, whether the Nordic power market structure is suitable to support large wind power installations. An increased amount of wind power will unavoidably generate an increased amount of <b>forecast</b> errors and <b>raise</b> the demand for adjustment and regulating power. The investigation is carried out in three steps. · First a scenario is created containing eight actors that is balance responsible for varying size of wind power production. Forecast error volumes are modeled associated with each actor in the scenario. · Secondly, conditions at the intraday market and the regulating market during 2006 are investigated and the result is used as input for the next step. · Last, price models are developed and used to calculate future imbalance costs associated to each actor, and the cost saving potential in different options. Because of uncertainties about the future intraday/regulating market situation, several calculations are carried out with different perspectives for the model calibration, different distributions of the forecast error volumes between the intraday market and the regulating market, and different options for managing the forecasts error. The results indicate that it is a major difference in the cost saving potential if the forecast error is “sold” or if the adjustment is “bought”. The cost saving potential differs significantly between the smaller and the larger actors...|$|R
40|$|Treball de Fi de Grau en Psicologia. Codi: PS 1048. Curs acadèmic 2015 - 2016 This paper poses a twofold {{objective}} a) {{examine whether}} there is relationship between contextual vari-ables, intention to learn (expectations) and learning strategies. and b) analyzing the motivational pro-file 3 ºESO students in the subject of biology The sample consists of 40 students / as 3 ºESO, who collaborated on the research anonymously and voluntarily. The instruments used for data collection were three reports self-Likert two students and one teacher of the subject. For analysis of the results the SPSS program that calculating descriptive statistics {{mean and standard deviation}} for each factor and the coefficient of internal consistency relia-bility Cronbach alpha was performed was used bivariate correlations Pearson and diagrams were cal-culated bar of the study variables. As for the scales you use, according to the results, we can say that they have good reliability and con-struct validity. The results for the first objective, to examine {{whether there is}} relationship between contextual variables, intention to learn (expectations) and learning strategies, can <b>forecast</b> as hypothe-ses <b>raised</b> in a significant association between the variables studied. According to the second to ana-lyze the motivational profile of students, we can say that the motivational profile showing subjects is high. Regarding the educational implications for education, bearing in mind, that if we are able to activate student motivation have guaranteed success in their learning. This research service teacher analysis of the main variables that influence learning and offers effective tools to guide them in their teaching...|$|R
40|$|Climatic {{variability}} exerts tremendous {{influence on}} the livelihoods and well-being of pastoralists who inhabit the arid and semi-arid lands of the Horn of Africa. Recent advances in climate <b>forecasting</b> technologies have <b>raised</b> the intriguing prospect of reasonably accurate forecasts of coming seasons' rainfall patterns. Several donors and governments {{in the region are}} keenly interested in these technologies and in developing forecast delivery channels on the assumption that this information will prove valuable to the vulnerable populations it is meant to help not only indirectly, as an input into top-down early warning systems, but also directly, as a basis for improving choice under uncertainty. We explore the value of such external climate forecast information to pastoralists in a large study area spanning southern Ethiopia and northern Kenya using original data collected using both open-ended qualitative methods to identify and understand indigenous climate forecasting methods and quantitative data collected using survey instruments fielded in two rounds, one before and one after the long rains of 2001. The data show that pastoralists rely heavily on indigenous forecasting methods — in terms of having both access to and confidence in these methods — while external forecasts are less commonly received or believed. We elicited pastoralists' subjective, probabilistic expectations of the coming season's rainfall and find that neither use of nor belief in external forecasts causes any appreciable change in respondents' seasonal rainfall expectations. Moreover, relatively few pastoralists act on their own climate expectations, no matter how formed. In sum, climate forecast information does not seem a limiting factor at present in pastoralist communities in the Horn of Africa, not least of which because of the existence of a vibrant and still-relevant tradition of indigenous forecasting...|$|R
40|$|The stylized {{fact of the}} {{inflationary process}} {{is that it makes}} life more uncertain. If prices are {{increasing}} rapidly it is argued that there is more noise in the system so that prices convey less information. This implies that people will make wrong decisions if they do not make substantial investments in collecting additional price information. It seems likely that initially people will not catch on to what’s happening, not collect the required additional information, and consequently make wrong decisions. For example, in times of inflation a shopper at a supermarket would have to make literally thousands of pairwise price comparisons in order to have full information about the new price structure. Such a large information requirement clearly does not make life any easier for shoppers and it is likely that they would respond by learning about the new price structure not immediately, but over time. These general notions about the uncertainty effects of inflation {{have a great deal of}} intuitive appeal. But very little formal research has been devoted to understanding these issues. In this paper we analyse one aspect of the area, the distortive effects of inflation on relative prices. Using quarterly Australian data for consumer prices, we find that inflation does systematically change relative prices; i. e. some prices consistently rise by more than the overall rate of inflation and some consistently rise by less. This means that inflation (i) causes costly resource reallocations and changes in consumption patterns; and (ii) makes it more difficult to <b>forecast</b> prices, which <b>raises</b> the degree of uncertainty in the economy. To illustrate the results, in Table 1 we give the percentage of the variability of each consumer price due to (i) inflation and (ii) real changes in the economy (changes in income, tastes, technology, etc.). As can be seen, 19 per cent of the variability of the price of food is due to inflation and the remaining 81 per cent is due to real factors. The last row of the table gives the inflation and real percentages for all relative price changes. These totals are weighted averages of the corresponding individual commodity percentages. Inflation accounts for 24 per cent of the total relative price variability. Our results indicate that the distortion of relative prices is substantial and should be included in any evaluation of the cost of inflation. ...|$|R
40|$|The {{instability}} {{of a global}} market which is already besieged by constant crises {{over the past several}} years coupled with enterprises competitiveness and cost cutting pressures are small number of challenges businesses encounter. In addition, businesses are required to be agile and rapidly adapt to changes as they occur while creating new business values as well as increase efficiency. Furthermore, globalization not only introduced international competitors but also opportunities for collaboration. Consequently, faster information transfer mechanisms which facilitate prompt and swift decision making are essential for establishing such global, agile and adaptive organization. To deal with these challenges as well as guarantee the survival and profitability of the business, organizations turned to IT. Investing heavily in IS and harnessing IT to manage different aspects of the business as well as meet those challenges (KO, Lee & Lee, 2008). IT solutions promise great flexibility, more automation and new products/business opportunities introduced as a result of new technologies. Over the past a few years, Business Process Management Systems (BPMs) and Services Oriented Architecture (SOA) are examples of such IT solutions which have been proposed as evolutionary initiatives that enable organizations become more agile through better flexibility and better reusability that lower costs and increase efficiency (Kamoun, 2007). According to Gartner (2006) BPM is a management discipline that requires shift to process-centric thinking and reduce organizations reliance on traditional territorial and functional structures. BPM is supported by methods, techniques and software to design, enact, control and analyze operational processes (Aalst et al. 2003). This process-centric view supported by advanced tools and functionalities enable process automation as well as continuous analysis and optimization of the process. Therefore, offering solutions to mitigate a number of challenges organizations face. At the end of 2006 the Business Process Management Systems (BPMS) market reached US$ 1. 7 billion in total revenue (Hill et al. 2007) and it was <b>forecasted</b> it would <b>raise</b> to an estimated to $ 6. 3 billion by 2011 (Forster, 2007). This is a clear indication to the optimism surrounding BPM and how organizations received it. However, to achieve the process flexibility that BPM promises, processes themselves must be independent of any specific information resource or task automation application. In addition, the integration technology should loosely couple resources and application that make up the process (Noel, 2005). In general, the underlying IT infrastructure is seldom flexible and contains high level coupling (Gopala, 2006). This rigid infrastructure and high coupling will result in the logic of a process being hard coded into a certain technology which completely defeats the purpose of BPM. This where SOA comes in, providing the technical ability and the platform to establish process independency. SOA is an architectural style for developing distributed systems which advocates loose coupling of components hence allowing greater flexibility. SOA itself is not a specific technology, but it is applicable to many technologies. Services are created and modified without impacting the users of those services. In addition, services have stable and clear interface definition which eradicates the need to know the implementation details. These characteristics of services under the SOA paradigm eliminate all issues on the technical level which hinder a successful BPM implementation. BPM and SOA are developed as radical and evolutionary initiatives that will enable enterprises become more agile. Although undoubtedly both can be pursued and implemented independently for instance, BPM can be successfully deployed relying on proprietary infrastructure and similarly SOA can utilize Business Process Execution Language (BPEL) standards (Kamoun, 2007). The combination of SOA and BPM is more powerful than either one alone (Gopala, 2006; Rosen, 2009 a). In addition, this synergy between BPM and SOA is rather obvious and many believe both are natural complement for one another (Brunswick, 2008; Kamoun, 2007). However, leveraging these two concepts presents a challenge in itself mainly because of the different nature of both concepts. For example, BPM is business driven and uses top-down process approach on the other hand; SOA is IT driven and uses bottom-up architectural approach. There are several obstacles and challenges organizations must overcome to bring the two concepts together and obtain maximum advantage offered by this synergy. For instance, assessing the maturity and alignment of both is essential and a prerequisite for bringing them together. Moreover, the attainment of the required alignment to leverage both concepts necessitates major organizational transformation (Gopala, 2006; Kamoun, 2007; Woodley, 2005). Even with the identification and attainment of the required alignment, combining the two concepts is far from a straightforward process. The gap between the business approach of BPM and the IT view of SOA should be narrowed which also means one or both concepts have to be tailored to create the sought after successful and beneficial union. There is obvious gap between the business needs and what the IT can actually deliver. Furthermore, this gap includes other aspects such the reliability of IT’s ability to fully understand and translate the business needs in order to deliver an optimal solution which meets these needs. This research project focuses on examining the BPM/SOA integration holistically in order to identify key aspect for a successful integration. Furthermore, utilizing these aspects to asses organizations’ BPM/SAO integration initiative not only to determine where these organizations stand in the integration journey but also help organization devise their own development roadmap. The research is based on extensive literature study combined with experts interviews analyzing various aspect of the BPM/SOA integration. Main areas which were examined and analyzed were BPM/SOA differences, challenges between both concepts, challenges hindering integration, maturity models and different approaches proposed to achieve a successful integration. The key results and contributions of this research project are 1 -	Critical Success Factors For BPM/SOA integration 2 -	Conceptual BPM/SOA integration approach 3 -	An assessment tool to gauge the level of BPM/SOA integration The Critical Success Factors are derived from comprehensive analysis of best practices as well as common pitfall in the field of practice pitfalls. They are operationlized in the assessment method to scan organizations and determine their level of readiness for a successful integration initiative...|$|R

