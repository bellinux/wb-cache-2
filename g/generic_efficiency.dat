6|18|Public
40|$|Private state-verification is {{introduced}} in a two-period economy with spot markets in both periods and complete futures markets for contingent delivery {{in the second}} period. Existence of equilibrium is established, under standard assumptions. The equilibrium allocation is shown to be generically efficient {{if the number of}} states is not greater than the number of goods. General equilibrium, Differential information, Private state-verification, Two-period economies, Existence of equilibrium, <b>Generic</b> <b>efficiency...</b>|$|E
40|$|We study a {{group of}} new methods to solve an open problem that is the {{shortest}} paths problem on a given fix-weighted instance. It is the real significance at a considerable altitude to reach our aim to meet these qualities of <b>generic,</b> <b>efficiency,</b> precision which we generally require to a methodology. Besides our proof to guarantee our measures might work normally, we pay more interest to root out the vital theory about calculation and logic in favor of our extension to range over a wide field about decision, operator, economy, management, robot, AI and etc. Comment: 26 pages, 11, 100 words, 2 picture...|$|E
40|$|A model, {{introduced}} earlier for {{the dynamics}} of a <b>generic</b> <b>efficiency</b> measure in a population of agents by Majumdar and Krapivsky (Phys. Rev. E 63, 054101 (2001)), is investigated on scale-free networks whose degree distribution follows a power law with the tunable exponent γ. The model shows a delocalization transition from a stagnant phase to a growing one when decreasing the degree exponent γ of scale-free networks. By {{taking into account the}} specific dynamical properties of the model and the geometrical properties of scale-free networks, we predict the appearance of this critical transition. This work is useful for understanding these kinds of transitions occurring in many dynamical processes on scale-free networks. Copyright EDP Sciences/Società Italiana di Fisica/Springer-Verlag 2005...|$|E
40|$|The paper {{discusses}} community {{enforcement in}} infinitely repeated two-action games with local monitoring. Each player interacts with and observes only a fixed set of partners, {{of whom he}} is privately informed. The main result shows that for <b>generic</b> beliefs <b>efficiency</b> can be sustained in a sequential equilibrium in which strategies are independent of the players’ beliefs about the monitoring structure. Stronger results are obtained when players are arbitrarily patient and payoffs are evaluated according to Banach-Mazur limits, and when players are impatient and only acyclic monitoring structures are allowed...|$|R
40|$|International audienceThis work {{is devoted}} to {{interval}} observer design for Linear Parameter-Varying (LPV) systems under assumption that the vector of scheduling parameters is not available for measurements. Stability conditions are {{expressed in terms of}} matrix inequalities, which can be solved using standard numerical solvers. Robustness and estimation accuracy with respect to model uncertainty is analyzed using L_inf/L_ 1 framework. Two solutions are proposed for nonnegative systems and for a <b>generic</b> case. The <b>efficiency</b> of the proposed approach is demonstrated through computer simulations...|$|R
40|$|The {{bootstrap}} {{provides a}} simple and powerful means of assessing the quality of estimators. However, in settings involving large datasets, the computation of bootstrap-based quantities can be prohibitively demanding. As an alternative, we present the Bag of Little Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to obtain a robust, computationally efficient means of assessing estimator quality. BLB is well suited to modern parallel and distributed computing architectures and retains the <b>generic</b> applicability, statistical <b>efficiency,</b> and favorable theoretical properties of the bootstrap. We provide {{the results of an}} extensive empirical and theoretical investigation of BLB’s behavior, including a study of its statistical correctness, its largescale implementation and performance, selection of hyperparameters, and performance on real data. 1...|$|R
40|$|The {{purpose of}} this study is to {{investigate}} the efficiency of business evaluation methods in the Australian energy industry during the periods from 1989 to 2007. The six commonly used business evaluation methods (CAPM, WACC, EVA, P/E ratio, DCF and MetaCapitalism) were selected and compared with the share price in the whole market, listed market and delisted market, to explore which valuation methods were better for evaluating business performance in the Australian energy sector over the long-term. An empirical analysis using linear regression, we find evidence that CAPM is a much better method for listed companies to measure the rate of return of an asset in a well-diversified portfolio in the Australian energy industry, while DCF was a better method for listed and delisted companies, when making capital budgeting decisions for public companies in the Australian energy sector. We also find that the <b>generic</b> <b>efficiency</b> prescriptions of the radical corporate strategy: MetaCapitalism, was based on fallible assumptions, and tended to transfer negative signals to the market as reflected in the share price, as well as threatening the long-term sustainability of business and social stability...|$|E
40|$|Updated {{constraints}} on dark matter cross section and mass are presented combining CMB power spectrum measurements from Planck, WMAP 9, ACT, and SPT {{as well as}} several low-redshift datasets (BAO, HST, supernovae). For the CMB datasets, we combine WMAP 9 temperature and polarization data for l <= 431 with Planck temperature data for 432 2500, and Planck CMB four-point lensing measurements. We allow for redshift-dependent energy deposition from dark matter annihilation by using a `universal' energy absorption curve. We also include an updated treatment of the excitation, heating, and ionization energy fractions, and provide updated deposition efficiency factors (f_eff) for 41 different dark matter models. Assuming perfect energy deposition (f_eff = 1) and a thermal cross section, dark matter masses below 26 GeV are excluded at the 2 -sigma level. Assuming a more <b>generic</b> <b>efficiency</b> of f_eff = 0. 2, thermal dark matter masses below 5 GeV are disfavored at the 2 -sigma level. These limits are a factor of ~ 2 improvement over those from WMAP 9 data alone. These current constraints probe, but do not exclude, dark matter as an explanation for reported anomalous indirect detection observations from AMS- 02 /PAMELA and the Fermi Gamma-ray Inner Galaxy data. They also probe relevant models that would explain anomalous direct detection events from CDMS, CRESST, CoGeNT, and DAMA, as originating from a generic thermal WIMP. Projected constraints from the full Planck release should improve the current limits by another factor of ~ 2, but will not definitely probe these signals. The proposed CMB Stage IV experiment will more decisively explore the relevant regions and improve upon the Planck constraints by another factor of ~ 2. Comment: 11 pages, 6 figures, Appeared in PR...|$|E
40|$|The {{landmark}} Waxman-Hatch Act of 1984 {{represented a}} “grand compromise” legislation {{that sought to}} balance incentives for innovation by establishing finite periods of market exclusivity yet simultaneously providing access to lower cost generics expeditiously following patent expiration. Here we examine trends {{in the first quarter}} century since passage of the legislation, building on earlier work by Grabowski and Vernon [1992, 1996] and Cook [1998]. The generic share of retail prescriptions in the U. S. has grown from 18. 6 % in 1984 to 74. 5 % in 2009, with a notable acceleration in recent years. This increase reflects increases in both the share of the total market potentially accessible by generics, and the <b>generic</b> <b>efficiency</b> rate – the latter frequently approaching 100 %. Whereas in 1994, the generic price index fell from 100 to 80 in the 12 months following initial generic entry and by 24 months to 65, in 2009 the comparable generic price indexes are 68 and 27, respectively. Recent studies sponsored by the American Association of Retired Persons focus only on brand prices and ignore substitution to lower priced options following loss of patent protection. For the prescription drugs most commonly used by beneficiaries in Medicare Part D, the average price per prescription declined by 21. 3 % from 2006 to 2009, rather than increasing by 25 - 28 % as reported by the AARP. Finally, we quantify changes over time in the average daily cost of pharmaceutical treatment in nine major therapy areas, encompassing the entire set of molecules within each therapy class, not simply the molecule whose patent has expired. Across all nine therapeutic areas, at 24 months post-generic entry, the weighted mean reduction in pharmaceutical treatment cost per patient is 35. 1 %. ...|$|E
40|$|Self-assembled Ge {{quantum dots}} were formed by in-situ thermal {{annealing}} of a thin amorphous Ge layer deposited by {{molecular beam epitaxy}} either on a thin porous TiO 2 layer grown on SiO 2 on Si(001) or directly on the SiO 2 layer itself. For samples with dot diameters ranging from 10 to 35 nm, the dot photoluminescence (PL) appeared primarily as a wide near-infrared band peaked near 800 meV. The peak energy of the PL band reflects the average dot size and its shape depends on the dot size distribution. Using tight binding and effective mass theoretical models, we have analyzed the PL spectrum {{in terms of the}} dot size distribution. The observed size distribution determined from transmission electron and atomic force microscopy allowed the determination of the nonlinear increase in the PL efficiency with decreasing dot diameter. Although the absolute intensities of the PL from the samples vary, the calculated efficiency curves are all well fitted by straight lines on a log-log plot, with essentially the same slope for all samples, thereby demonstrating that under the weak confinement regime investigated here there is a universal power-law increase in PL efficiency with decreasing dot size. Knowing this <b>generic</b> PL <b>efficiency,</b> we show {{that it is possible to}} evaluate the size distribution of Ge dots from their PL energy dependence. Peer reviewed: YesNRC publication: Ye...|$|R
40|$|Organisations lack clear {{guidance}} {{on how they}} can become more innovative at the operational level. The operations strategy literature shows that organisations compete on four <b>generic</b> capabilities: cost <b>efficiency,</b> quality of products or services, speed of delivery, and flexibility of operations. Should organisations choose between these capabilities, i. e., engage in trading-off these capabilities and focussing on one capability ("trade-off" model), or combine them, thereby competing on multiple capabilities simultaneously ("cumulative capabilities" model), remains an unresolved issue. Our paper addresses this by empirically testing {{the relationship between the}} four operations capabilities and innovation performance through a large-scale global study of manufacturing plants. Our results show support for the cumulative capabilities model and not the trade-off model. Furthermore, both delivery and flexibility capabilities are comparatively stronger predictors of innovativeness than cost efficiency and quality capabilities. This study provides interesting insights for practitioners and managers in generating clearer guidelines as to what organisations need to do with their key operational capabilities, in order to become more innovative...|$|R
40|$|In {{this paper}} we propose a novel {{procedure}} for obtaining low-dimensional models of large-scale multi-phase, non-linear, reactive fluid flow systems. Our approach is based on the combination of methods of proper orthogonal decompositions, black-box system identification techniques and non-linear spline based blending of local linear black-box models to create a reduced order linear parameter-varying model. The proposed method, which is of empirical nature, gives computationally very efficient low-order process models for large-scale processes. The proposed method does not need Galerkin type of projections on equation residuals to obtain the reduced order models and the proposed method is of <b>generic</b> nature. The <b>efficiency</b> of the proposed approach is illustrated on a benchmark problem of an industrial glass manufacturing process where the process non-linearity and non-linearity arising due to the corrosion of refractory materials is approximated using a linear parameter varying model. The results show good performance of the proposed framework. © 2009 Elsevier Ltd. All rights reserved...|$|R
40|$|The {{symbolic}} model checker NuSMV {{has been}} used to check safety properties for railway interlockings. When the size of the models increased, the model checking efficiency decreased dramatically to a point at which the verification failed due to lack of memory. At that point the models we could check were still small in the real world of railway interlockings. Various standard options to the NuSMV model checker were tried, mostly without significant improvement. However, the analysis of our model provided information on how to optimise the variable orderings and also the ordering and clustering of the partitioned transition relation. The NuSMV code was adapted to enable user control for ordering and clustering of transitions. This replacement of the tool 2 ̆ 7 s <b>generic</b> algorithm improved <b>efficiency</b> enormously, enabling the checking of safety properties for very large models. This paper discusses how the characteristics of our model are used to find the optimised parameters...|$|R
40|$|We {{present a}} testing theory for Markovian {{processes}} {{in order to}} formalize a notion of efficiency which may be useful {{for the analysis of}} soft real time systems. Our Markovian testing theory is shown to enjoy close connections with the classical testing theory of De Nicola-Hennessy and the probabilistic testing theory of Cleaveland-Smolka et al. The Markovian testing equivalence is also shown to be coarser than the Markovian bisimulation equivalence. A fully abstract characterization is developed to ease the task of establishing testing related relationships between Markovian processes. It is then demonstrated that our Markovian testing equivalence, which is based on the (easier to work with) probability of executing a successful computation whose average duration is not greater than a given amount of time, coincides with the Markovian testing equivalence based on the (more intuitive) probability of reaching success within a given amount of time. Finally, it is shown that {{it is not possible to}} define a Markovian preorder which is consistent with reward based performance measures, thus justifying why a <b>generic</b> notion of <b>efficiency</b> has been considered...|$|R
40|$|Abstract:-Congestion in {{wireless}} sensor networks {{occurs when}} traffic load allocated to any sensor node is beyond its capacity. To support traditional quality of service like packet loss ratio, packet delay, wasting energy and throughput, congestion has to be controlled in the {{wireless sensor networks}}. To do this, a distributed congestion control algorithm is used for tree based communications. For each node, fair and efficient transmission rate is assigned. In this algorithm, aggregate output and input traffic rate is monitored in each node. Based on difference of the two, a node decides to increase or decrease the bandwidth allocable to a flow originating from itself and the flow is being routed through it. The design allows {{for the development of}} a <b>generic</b> utility and <b>efficiency</b> controlling module. Separation of efficiency and fairness controlling modules enable each one to use a separate control law, thereby providing a more flexible design. The working of congestion control is independent of the routing algorithm and is designed to adapt to changes in the underlying routing topology...|$|R
40|$|Abstract. We {{present a}} testing theory for Markovian {{processes}} {{in order to}} formalize a notion of efficiency which may be useful {{for the analysis of}} soft real time systems. Our Markovian testing theory is shown to enjoy close connections with the classical testing theory of De Nicola-Hennessy and the probabilistic testing theory of Cleaveland-Smolka et al. The Markovian testing equivalence is also shown to be coarser than the Markovian bisimulation equivalence. A fully abstract characteriza-tion is developed to ease the task of establishing testing related relation-ships between Markovian processes. It is then demonstrated that our Markovian testing equivalence, which is based on the (easier to work with) probability of executing a successful computation whose average duration is not greater than a given amount of time, coincides with the Markovian testing equivalence based on the (more intuitive) probability of reaching success within a given amount of time. Finally, it is shown that {{it is not possible to}} define a Markovian preorder which is consistent with reward based performance measures, thus justifying why a <b>generic</b> notion of <b>efficiency</b> has been considered. ...|$|R
40|$|Generic {{programming}} {{has recently}} {{emerged as a}} paradigm for developing highly reusable software libraries, most notably in C++. We have designed and implemented a constrained generics extension for C++ to support modular type checking of generic algorithms and to address other issues associated with unconstrained generics. To be as broadly applicable as possible, generic algorithms are defined with minimal requirements on their inputs. At the same time, to achieve {{a high degree of}} <b>efficiency,</b> <b>generic</b> algorithms may have multiple implementations that exploit features of specific classes of inputs. This process of algorithm specialization relies on non-local type information and conflicts directly with the local nature of modular type checking. In this paper, we review the design and implementation of our extensions for generic programming in C++, describe the issues of algorithm specialization and modular type checking in detail, and discuss the important design tradeoffs in trying to accomplish both. We present the particular design that we chose for our implementation, with the goal of hitting the sweet spot in this interesting design space...|$|R
40|$|The paper {{seeks to}} {{reformulate}} the ‘colourful and fluid’ early {{debate on the}} effects of foreign direct investment (FDI) in two ways. Firstly, the wide range of separate specific concerns of the early debate are subsumed within four <b>generic</b> issues, (i) <b>efficiency,</b> (ii) distribution, (iii) sovereignty, (iv) growth and development. Secondly, the analysis is now structured around modes of analysis of multinational enterprises (MNEs), as the agents that carry out FDI. MNEs are seen as using the freedoms of international transfers central to globalisation in order to leverage competitively the differences of national (or other coherently-defined) economic units. Crucially this response to difference is analysed as reflecting three potential MNE strategic motivations, (i) market seeking, (ii) efficiency seeking, (iii) knowledge seeking. The core of the paper investigates how adoption of different motivations by MNEs would affect performance in terms of the different generic issues. The synergies of this mode of analysis with trade policy (the implicit, or often very explicit, move to outward-oriented industrialisation in the era of globalisation) and new growth theory are also discussed. ...|$|R
40|$|In {{light of}} alleged un-tapped {{potentials}} for cost effective energy savings {{in the industrial}} sector various policies have been implemented to overcome barriers {{to the adoption of}} energy efficiency measures by companies. Especially, small and medium-sized companies (SMEs) are supported in their adoption decision by informational and financial instruments. To adequately design these instruments a thorough understanding about their mechanism is crucial. Thus, this paper investigates the impact of two financial instruments (funding for cross-cutting (CC) technologies, low-interest loan) in addition to an informational instrument (energy audit) on the adoption of four <b>generic</b> energy <b>efficiency</b> measures in SMEs in Germany. Based on 766 observations, we apply t-tests and propensity score matching techniques to estimate the effects of these instruments. Findings suggest that the financial instruments in addition to an energy audit accelerate the adoption, but effectiveness varies by technologies. Based on a t-test, the adoption rate for insulation is 15 % and for heating 19 % higher for companies which used a low-interest loan (and an energy audit) compared to companies which only had an energy audit. For lighting, the adoption rate is nearly 12 % higher for companies which used the CC technologies programme (and an energy audit) compared to the control group. The propensity score matching results differ: To adopt measures for insulation the propensity is 12 %, for heating optimization 12 % and for heating about 25 % higher for companies which used a low-interest loan (and energy audit) compared to the control group. Regarding lighting we do not find a significant effect of the CC technologies programme on the adoption. Findings indicate that estimates of policy effectiveness by t-tests might be misleading, i. e. overestimating the effectiveness regarding the adoption of measures for lighting, insulation and heating optimization, and underestimating the effectiveness for heating...|$|R
40|$|In this paper, {{we review}} the key {{figures of merit}} to assess the {{performance}} of advanced random access (RA) schemes exploiting physical layer coding, repetitions and collision resolution techniques. We then investigate RA modeling aspects {{and their impact on}} the figures of merit for the exemplary advanced RA schemes: Contention Resolution Diversity Slotted ALOHA (CRDSA), Irregular Repetition Slotted ALOHA (IRSA), Coded Slotted ALOHA (CSA) and Enhanced Spread-Spectrum ALOHA (E-SSA). We show that typical simplifications of the reception model when used to optimize RA schemes lead to inaccurate findings, both in terms of parameter optimization and figures of merit, such as the packet loss ratio (PLR) and throughput. We also derive a <b>generic</b> RA energy <b>efficiency</b> model able to compare the schemes in terms of the energy required to transmit a packet. The combination of achievable RA throughput at the target PLR and energy efficiency, for the same average user power investment per frame and occupied bandwidth, shows that E-SSA, which is an unslotted scheme, provides the best overall performance, while, in terms of the slotted schemes, CRDSA outperforms the more elaborated IRSA and CSA. This surprising results {{is due to the fact}} that the IRSA and CSA optimization has so far been performed using RA channel models that are not accurately reflecting the physical layer receiver behavior. We conclude by providing insights on how to include more accurate reception models in the IRSA and CSA design and optimization. Comment: To appear in IEEE Journal on Selected Areas in Communications - Special Issue on Advances in Satellite Communication...|$|R
40|$|To {{quantify}} {{the impact that}} planting indigenous trees and shrubs in mixed communities (environmental plantings) have on net sequestration of carbon and other environmental or commercial benefits, precise and non-biased estimates of biomass are required. Because these plantings consist of several species, estimation of their biomass through allometric relationships is a challenging task. We explored methods to accurately estimate biomass through harvesting 3139 trees and shrubs from 22 plantings, and collating similar datasets from earlier studies, in non-arid (> 300 mm rainfallyear- 1) regions of southern and eastern Australia. Site-and-species specific allometric equations were developed, as were three types of generalised, multi-site, allometric equations based on categories of species and growth-habits: (i) species-specific, (ii) genus and growth-habit, and (iii) universal growth-habit irrespective of genus. Biomass was measured at plot level at eight contrasting sites to test the accuracy of prediction of tonnes dry matter of above-ground biomass per hectare using different classes of allometric equations. A finer-scale analysis tested performance of these at an individual-tree level across {{a wider range of}} sites. Although the percentage error in prediction could be high at a given site (up to 45 %), it was relatively low (< 11 %) when generalised allometry-predictions of biomass was used to make regional- or estate-level estimates across a range of sites. Precision, and thus accuracy, increased slightly with the level of specificity of allometry. Inclusion of site-specific factors in <b>generic</b> equations increased <b>efficiency</b> of prediction of above-ground biomass by as much as 8 %. Site-and-species-specific equations are the most accurate for site-based predictions. Generic allometric equations developed here, particularly the generic species-specific equations, can be confidently applied to provide regional- or estate-level estimates of above-ground biomass and carbon. © 2013 Elsevier B. V...|$|R
40|$|International audienceFunctional {{electrical}} stimulation (FES) is a palliative solution to improve severe motor and sensory deficiencies; {{it has been}} successfully used in numerous applications such as pacemakers, deep brain stimulation, pain control, hearing restoration or vagus nerve stimulation for epilepsy or cardiac neuromodulation. Implanted {{electrical stimulation}} is also studied for sensory feedback in amputees or vision for blind people. Implanted FES for movement rehabilitation such as foot droop for hemiplegic’s patients [1] or even more complex movements like ambulation [2] and grasping [3] are used but not so disseminated. One {{of the reasons is}} that these applications need for multisite and selective stimulation. When not limited to one site of stimulation, solutions for implanted FES are actually mainly based on centralized architectures, except the Bion implant [4]. They lead to complex surgery, high risk of failure during and after surgery, and global infection problems involving the whole explantation of the device [5]. Moreover, they are not really adequate to face selectivity issues that require multi-polar electrodes and thus an important number of wires or the generation of complex waveforms. Selectivity imposes to use multipolar electrodes {{in order to be able}} to control the nerve fiber recruitment [6]. Nerve fiber recruitment impacts muscle fibers recruitment and thus the functional response [7 - 8]. Moreover, such neural-based stimulation selectivity may need complex multi-phasic stimulation patterns; these patterns must be very accurate from a time point of view (anodal blocking requires for example few micro-seconds synchronization). It won't be possible to manage such real-time constraints simultaneously on many sites from a central implant. Besides, such complex technologies further increase the difficulty of their use in a clinical context. Tuning for adaptation to individuals is often suboptimal and in the worst case, not efficient at all. Very complex hardware designs based on Application Specific Integrated Circuits (ASIC), advanced and proved FPGA based digital architectures and networked global architecture are then proposed by laboratories. The challenge remains to transfer to wide spread clinical use through efficient industrial transfer. But safety, low power consumption, complexity of the design and thus the maintenance should then be taken into account. Software engineering is the last issue to be considered as most of these systems include a huge number of parameters that cannot be manually tuned by a practitioner for each patient. A tradeoff should be found between a high level of flexibility, <b>generic</b> approach, <b>efficiency</b> and usability. We will first present the context of FES, in particular issues to be faced to reach a selective and efficient neural-based stimulation in terms of functional rehabilitation. Then, the use in a clinical context will be discussed in particular in the context of selective sensory feedback stimulation and multipolar stimulation for selective efferent stimulation...|$|R
40|$|Thesis by publication. Includes bibliographical references. 1. Introduction [...] 2. Precipitation scaling with {{temperature}} in warm and cold climates: {{an analysis of}} CMIP 5 simulations [...] 3. Evaluation of modern and mid-Holocene seasonal precipitation of the Mediterranean and northern Africa in the CMIP 5 simulations [...] 4. Evaluation of CMIP 5 palaeosimulations to improve climate projections [...] 5. Simulation of tree-ring widths with a model for primary production, carbon allocation, and growth [...] 6. A model analysis of climate and CO₂ controls on tree growth in a semi-arid woodland [...] 7. Allocation changes buffer CO₂ effect on tree growth since the last ice age. Past climates {{provide an opportunity to}} examine the response of the Earth System to large changes in external forcing. The changes in forcing over the last 21, 000 years since the Last Glacial Maximum have been as large as those projected to occur over the 21 st century as a result of anthropogenic changes in greenhouse gas concentrations and land-use changes. The fact that there have been equally large changes in forcing in the past as expected in the future, coupled with the availability of climat reconstructions of past climates, provides the motivation for using evaluations of simulations of past climates to evaluate how well the models that are used to project future climate changes perform. Terrestrial vegetation is highly sensitive to changes in climate, and records of past vegetation changes are widely used to reconstruct past climate states. Statistical or model-inversion techniques have been used to reconstruct changes in seasonal temperature and water balance from pollen records from lakes and bogs. Most of these pollen records are at comparatively low resolution, and thus provide reconstructions of the long-term changes in mean climate state. Tree-ring series are the most abundant source of information used to reconstruct changes in short-term (interannual to decadal) climate variability. However, most of the available reconstructions focus on relationships with temperature and these relationships appear to break down in recent decades at many sites. An alternative approach is to use forward modelling to translate simulated climate variability into tree growth, which can then be directly compared to observations of tree-ring series. In {{the first part of this}} thesis, I have used a global synthesis of pollen-based palaeoclimate reconstructions to evaluate how well state-of-the-art climate models from the fifth phase of the Coupled Model Intercomparison Project (CMIP 5) capture large scale (sub-continental to hemispheric) patterns of climate change. I initially examine the scaling of changes in precipitation with temperature at a hemispheric scale in warm and cold climate states (Chapter 2). I then examine how well models simulate large-scale changes in monsoon precipitation in response to changes in orbital forcing during the mid-Holocene, 6000 years ago, focusing on the northern Africa monsoon (Chapter 3). Both of these analyses, and an evaluation of simulated changes in precipitation and {{temperature in}} mid-continental Eurasia in the mid-Holocene, are included in Chapter 4 which provides a summary of all of the evaluations that have been done of the CMIP 5 palaeoclimate simulations. All of these papers focus on evaluation of changes in the long-term mean climate, but it is also important to evaluate how well the models simulate short-term (annual to decadal) climate variability. To do this, I developed a model to simulate tree growth driven by climate-driven changes in net primary production (Chapter 5). I tested this model using tree-ring data from the historical period in two contrasting climate settings, specifically in a cool climate in the Changbai Mountains, northeastern China (Paper 4) and in the semiarid environment of the Great Western Woodlands, Western Australia (Chapter 6). The final paper (Chapter 7) uses the model to simulate tree growth during the Last 2 Glacial Maximum (ca 21, 000 years ago) in California, USA. Changes in the hydrological cycle are expected to scale with temperature changes. However, both the observed changes in precipitation in recent decades and model simulations of precipitation changes during the historic period and the 21 st century are smaller than would be predicted from the Clausius-Clapeyron relationship which describes the change in atmospheric water vapour content with temperature (~ 7 %/°C). It has been argued that this reflects energetic constraints on evaporation. To test this hypothesis, I analyzed the scaling of precipitation with temperature in warm (increased CO₂) and cold (Last Glacial Maximum, LGM) climates using six CMIP 5 models that have simulated the response to both. Globally, precipitation increases in warm climates and decreases in cold climates. The estimate of the scaling across all the climate states and all models indicates a 2. 06 %± 0. 09 % change per degree temperature change at the global scale. The simulated scaling of precipitation to temperature is controlled by energetic constraints on evaporation rather than the atmospheric water-holding capacity, and is also affected by water availability. These constraints lead to a lower sensitivity of precipitation to temperature change over the land than that over the ocean, and a lower sensitivity over tropical land than over extratropical land. The simulated changes in precipitation per degree temperature change are comparable to the observed changes in both the historical period and the LGM, showing the models correctly predict the constraints on precipitation scaling. Temperature-controlled precipitation change is a global large-scale phenomenon. However, regional precipitation change can also be influenced by changes in the large-scale circulation. Monsoon precipitation is one of the most typical circulation controlled evaluated the spatial expression of seasonal climates of the Mediterranean and northern Africa in pre-industrial (piControl) and mid-Holocene (midHolocene, 6 yr BP) CMIP 5 simulations using the observed regional pattern and amount of seasonal precipitation. Most of the piControl simulations reproduce the observed modern precipitation patterns in the Mediterranean and equatorial zone, but they overestimate the area influenced by the monsoon and underestimate the extent of desert. The models also fail to capture the observed amount of precipitation. The models simulated a stronger monsoon in response to orbital changes in seasonal insolation receipts inthe mid-Holocene, including a northward expansion of the monsoon and an increase in summer and autumn rainfall. However, the mid-Holocene simulations underestimate the observed changes in annual precipitation, except in equatorial zone. The underestimation of precipitation in the latitude band from 15 – 30 °N is at least 50 %. The failure to capture the observed monsoon expansion is unrelated to biases in the piControl simulations. The failure to capture the observed changes in rainfall over northern Africa is an example of a long-standing modelling problem: current state-of-the-art models do not produce a better match to observations than previous generations of models. The mid-continent of Eurasia provides another example of a persistent problem in the simulation of regional climates during the mid-Holocene. The CMIP 5 mid-Holocene simulations produce conditions drier than today in mid-continental Eurasia, particularly between 45 ° and 60 ° N, whereas observations systematically show that this region was wetter than today. In the models, dry conditions reduce evapotranspiration and result in an increase in surface temperature compared to today. However, the observations show that the mid-continent was cooler than today. These three analyses form a major part of the evaluation of the CMIP 5 palaeoclimate simulations described in Chapter 4. The main conclusion of this summary paper is that while models are able to reproduce the large-scale features (such as precipitation scaling with temperature) of past climates accurately they are poor at reproducing regional changes such as monsoon expansion or the water-balance of the midcontinental regions. This suggests that while we can have confidence in projections of large-scale features of projected future climates, such as the greater warming at high latitudes than in the tropics or enhanced land-sea contrast, predictions of regional climates are very uncertain. Statistical reconstructions of past climate provide one source of information for model evaluation. An alternative approach is to use climate model outputs to drive simple forward models to predict the actual observations of vegetation changes. I have developed a tree growth model (the T model) that predicts carbon allocation to leaves,stem and roots, and thus can simulate tree-ring series. The tree-growth model is driven by a <b>generic</b> light-use <b>efficiency</b> model (the P model). The P model provides values for gross primary production (GPP) per unit of absorbed photosynthetically active radiation (PAR), which is estimated from leaf area. In the tree-growth model,GPP is allocated to foliage, transport tissue, and fine-root production and respiration in such a way as to satisfy well-understood dimensional and functional relationships. The T model represents both ontogenetic effects (the impact of ageing) and the effects of environmental variations and trends (climate and atmospheric CO₂ concentration [CO₂]) on growth. I have tested the T model under modern climate conditions for three species in three different climate settings, including Pinus koraiensis in the cool and mild climate ofthe Changbai Mountains, northeastern China, Callitris columellaris in the semi-arid climate of the Great Western Woodlands, Western Australia, and Juniperus occidentalis in the montane climate of California, USA. In all three regions, when driven by the local climate and [I have tested the T model under modern climate conditions for three species in threedifferent climate settings, including Pinus koraiensis in the cool and mild climate ofthe Changbai Mountains, northeastern China, Callitris columellaris in the semi-aridclimate of the Great Western Woodlands, Western Australia, and Juniperus occidentalisin the montane climate of California, USA. In all three regions, when driven bythe local climate and [CO₂], the T model produces realistic simulations of the interannual variability in ring width, and captures the effect of ontogenetic ageing on tree growth. The model correctly reproduces the effects of individual climate variables on tree growth, including the positive response of tree-ring width to growing season total photosynthetically active radiation (PAR 0), the positive response of ring-width to the ratio of actual to potential evapotranspiration (α), and the negative response of ring width to annual mean vapour pressure deficit (VPD). Thus, the T model responds toclimate in a realistic way in different regions and for different species. I have also analysed the response of tree growth to changing [CO₂]. The impact of the recent increase in [CO₂] on tree growth in the Changbai Mountains is small compared to the influence of climate variability. Callitris trees in the Great Western Woodlands show no change in radial growth in response to increasing [CO₂]. By using a time-dependent calibration of the T model against increasing [CO₂], I have shown that this results from changes in carbon allocation strategy. As [CO₂] increases, Callitris allocates more carbon to below-ground growth such that the ratio of fine-roo tmass to foliage area increases by 14 % with a 40 ppm increase in [CO₂]. Studies from many parts of the world have shown that tree ring-widths are apparently insensitive to recent changes in [CO₂]; my analyses suggest that the absence of increased radial growth could be a consequence of a shift towards increased below-ground carbon allocation. Soil nutrient status and moisture availability may influence the sensitivity of carbon partitioning to changes in [CO₂]; these effects are not included in the currentversion of the T model. In my final analysis, I examine the impact of changes in climate and [CO₂] on tree growth during the Last Glacial Maximum. An earlier study comparing fossil Juniperus wood from La Brea tar pits in California with modern Juniperus suggested that there was almost no change in ring width or the ratio of internal to external [CO₂] at theLGM compared to present, despite the fact that the trees were growing in [CO₂] levels of ca 180 ppm. My model analyses show that: the ci/ca ratio was stable because both vapour pressure deficit and temperature were decreased with compensating effects; reduced photorespiration at lower temperatures partly mitigated the effect of low ci on gross primary production, but maintenance of present-day radial growth required a ~ 25 % reduction in below-ground carbon allocation. Mode of access: World wide web 1 online resource (185 leaves) illustrations, map...|$|R

