0|10000|Public
50|$|Charlotte A. Adams (later Charlotte A. Cunningham) (born 1859) was an Australian {{mountain}} climber. She {{became the}} first woman <b>of</b> European <b>descent</b> to <b>climb</b> to the peak of Mount Kosciuszko in February 1881, aged 21.|$|R
5000|$|... #Caption: Illustration <b>of</b> <b>gradient</b> <b>descent</b> on {{a series}} of level sets.|$|R
5000|$|... {{and hence}} the {{algorithm}} is a special case <b>of</b> <b>gradient</b> <b>descent.</b>|$|R
5000|$|A step <b>of</b> <b>gradient</b> <b>descent</b> iswhich is {{equivalent}} to the Richardson iteration by making [...]|$|R
5000|$|A {{variometer}} - {{also known}} as a rate <b>of</b> <b>climb</b> and <b>descent</b> indicator (RCDI), rate-of-climb indicator, vertical speed indicator (VSI), or vertical velocity indicator (VVI) - is one of the flight instruments in an aircraft used to inform the pilot of the rate <b>of</b> <b>descent</b> or <b>climb.</b> [...] It can be calibrated in feet per minute, knots (1 kn ≈ 100 ft/min) or metres per second (1 m/s ≈ 200 ft/min), depending on country and type of aircraft.|$|R
5000|$|Boltzmann machine - like a Hopfield net but uses {{annealed}} Gibbs sampling instead <b>of</b> <b>gradient</b> <b>descent</b> ...|$|R
50|$|The {{associated}} flow {{is called}} the gradient flow, and {{is used in the}} method <b>of</b> <b>gradient</b> <b>descent.</b>|$|R
5000|$|The {{algorithm}} {{above is}} equivalent to restricting the number <b>of</b> <b>gradient</b> <b>descent</b> iterations for the empirical risk ...|$|R
50|$|The Falls River is a 5.1 mi {{stream that}} drains Baraga County in the Upper Peninsula of Michigan in the United States, and flows into Lake Superior in the {{community}} of L'Anse. Its name derives {{from the fact that it}} flows over slate bedrock with a <b>gradient</b> (rate <b>of</b> <b>descent)</b> that increases as it nears its mouth, cascading over numerous slate waterfalls.|$|R
5000|$|... #Caption: An {{animation}} {{showing the}} first 83 iterations <b>of</b> <b>gradient</b> <b>descent</b> applied to this example. Surfaces are isosurfaces of [...] at current guess , and arrows show the direction <b>of</b> <b>descent.</b> Due {{to a small}} and constant step size, the convergence is slow.|$|R
50|$|In {{attempts}} {{to improve the}} <b>gradient</b> <b>of</b> the <b>descent</b> from the Blue Mountains plateau {{to the floor of}} the Hartley Valley, Lawson's Long Alley was opened in 1824. This still did not prove satisfactory and construction of a second deviation, known as Lockyer's Pass, was commenced. However this route was not completed, as its construction was abandoned in favour of construction of Mitchell's route via Victoria Pass.|$|R
5000|$|... #Caption: A {{comparison}} <b>of</b> <b>gradient</b> <b>descent</b> (green) and Newton's method (red) for minimizing {{a function}} (with small step sizes). Newton's method uses curvature information {{to take a}} more direct route.|$|R
40|$|This paper {{demonstrates}} how a multi-layer feed-forward network may be trained, {{using the method}} <b>of</b> <b>gradient</b> <b>descent,</b> by feeding gradients forward rather than by feeding errors backwards as is usual in the case <b>of</b> back-propagation. The <b>gradient</b> <b>of</b> steepest <b>descent</b> requires that the <b>gradient</b> <b>of</b> {{the output of the}} network with respect to each connection matrix be calculated and that the output of the final layer be calculated. The work in this paper shows how the <b>gradients</b> <b>of</b> the final output are determined by feeding the <b>gradients</b> <b>of</b> the intermediate outputs forward {{at the same time that}} the outputs of the intermediate layers are fed forward in order to determine the output of the final layer. This method turns out to be equivalent to back propagation for a 2 -layer network but is much more readily extended to several layers. The method makes obvious a great potential for concurrency and the algorithm may be directly implemented using an array processing language. It may be used, without modification, for a network with arbitrary number of layers. The logic complexity of the algorithm is independent of the number of layers in the network...|$|R
50|$|The road {{passes through}} the plains of the Black Sea north of the Caucasus Mountains. No large rivers {{intersect}} this road, with very few small bridges. Several sections of the road requires more attention <b>of</b> drivers. <b>Descents</b> and <b>climbs</b> await at the following sites - 270 km, 280 km and 300 km, 352 km and 353 km, 356 km. Lack of visibility of 250 km, {{in the region of}} 264 km and 352 km.|$|R
50|$|Since this {{is special}} type <b>of</b> <b>gradient</b> <b>descent,</b> there {{currently}} {{is not much}} benefit to analyzing it on its own as the nonlinear Landweber, but such analysis was performed historically by many communities not aware of unifying frameworks.|$|R
25|$|Most employ {{some form}} <b>of</b> <b>gradient</b> <b>descent,</b> using {{backpropagation}} {{to compute the}} actual gradients. This is done by simply taking the derivative of the cost function {{with respect to the}} network parameters and then changing those parameters in a gradient-related direction.|$|R
50|$|Nesterov is {{most famous}} {{for his work in}} convex optimization, {{including}} his 2004 book, considered a canonical reference on the subject. His main novel contribution is an accelerated version <b>of</b> <b>gradient</b> <b>descent</b> that converges considerably faster than ordinary gradient descent.|$|R
50|$|Perceptrons can {{be trained}} by a simple {{learning}} algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form <b>of</b> <b>gradient</b> <b>descent.</b>|$|R
30|$|These back {{propagated}} error values {{are used to}} compute the <b>gradient</b> <b>descent</b> [27] <b>of</b> the loss function. We apply some weight update rule based on the <b>gradient</b> <b>descent</b> <b>of</b> loss function to optimize {{the weight of the}} network and hence, train the network to learn the function required to describe the input examples.|$|R
5000|$|Most employ {{some form}} <b>of</b> <b>gradient</b> <b>descent,</b> using {{backpropagation}} {{to compute the}} actual gradients. This is done by simply taking the derivative of the cost function {{with respect to the}} network parameters and then changing those parameters in a gradient-related direction. Backpropagation training algorithms fall into three categories: ...|$|R
5000|$|... where [...] {{is called}} the shape gradient. This gives a natural idea <b>of</b> <b>gradient</b> <b>descent,</b> where the {{boundary}} [...] is evolved in the direction <b>of</b> negative shape <b>gradient</b> {{in order to reduce}} the value of the cost functional. Higher order derivatives can be similarly defined, leading to Newtonlike methods.|$|R
40|$|This paper {{studies the}} {{projected}} saddle-point dynamics for a twice differentiable convex-concave function, which we term saddle function. The dynamics consists <b>of</b> <b>gradient</b> <b>descent</b> <b>of</b> the saddle function in variables corresponding to convexity and (projected) gradient ascent in variables corresponding to concavity. We provide a novel {{characterization of the}} omega-limit set of the trajectories of these dynamics {{in terms of the}} diagonal Hessian blocks of the saddle function. Using this characterization, we establish global asymptotic convergence of the dynamics under local strong convexity-concavity of the saddle function. If this property is global, and for the case when the saddle function takes the form of the Lagrangian of an equality constrained optimization problem, we establish the input-to-state stability of the saddle-point dynamics by providing an ISS Lyapunov function. Various examples illustrate our results...|$|R
50|$|The 2011 Tour {{climbed the}} Col du Galibier twice to {{celebrate}} the 100th anniversary of the first appearance of the pass in the Tour de France, including the first ever summit finish, won by Andy Schleck after a 60 km solo breakaway. This was the highest ever stage finish in the Tour de France. It {{was scheduled to be}} used again in stage 20 of the 2015 Tour, but was left out nine days before the race start due to landslides in the Chambon Tunnel, situated towards the bottom <b>of</b> the <b>descent</b> <b>of</b> the <b>climb.</b>|$|R
5000|$|... #Caption: A {{comparison}} of the convergence <b>of</b> <b>gradient</b> <b>descent</b> with optimal step size (in green) and conjugate vector (in red) for minimizing a quadratic function associated with a given linear system. Conjugate gradient, assuming exact arithmetic, converges in at most n steps where n {{is the size of}} the matrix of the system (here n=2).|$|R
40|$|Matlab and Computational Assignments Please {{provide a}} {{printout}} of the Matlab code you wrote {{to generate the}} {{solutions to the problems}} below. 1. Consider the non-quadratic problem given in Eq. (9. 20) in B & V. Implement five flavors <b>of</b> <b>gradient</b> <b>descent</b> algorithms, and provide the convergence plots for all five. (a) Standard gradient descent with backtracking. (b) Two kinds <b>of</b> Steepest <b>Descent,</b> using the two matrices suggested in the book...|$|R
40|$|In {{this work}} we {{introduce}} a conditional accelerated lazy stochastic gradient descent algorithm with optimal {{number of calls}} to a stochastic first-order oracle and convergence rate O(1 /ε^ 2) improving over the projection-free, Online Frank-Wolfe based stochastic <b>gradient</b> <b>descent</b> <b>of</b> Hazan and Kale [2012] with convergence rate O(1 /ε^ 4). Comment: 33 pages, 9 figure...|$|R
40|$|Abstract: This paper {{describes}} a new Evolutionary Programming algorithm based on Self-Organised Criticality. When tested {{on a range}} of problems drawn from real-world applications in science and engineering, it performed better than a variety <b>of</b> <b>gradient</b> <b>descent,</b> direct search and genetic algorithms. It proved capable of delivering high quality results faster, and is simple, robust and highly parallel...|$|R
40|$|This paper {{shows that}} a perturbed form <b>of</b> <b>gradient</b> <b>descent</b> converges to a second-order {{stationary}} point in a number iterations which depends only poly-logarithmically on dimension (i. e., it is almost "dimension-free"). The convergence rate of this procedure matches the well-known convergence rate <b>of</b> <b>gradient</b> <b>descent</b> to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community...|$|R
40|$|The {{processing}} {{performed by}} a feed-forward neural network is often interpreted through use of decision hyperplanes at each layer. The adaptation process, however, is normally explained using the picture <b>of</b> <b>gradient</b> <b>descent</b> <b>of</b> an error landscape. In this paper {{the dynamics of the}} decision hyperplanes is used as the model of the adaptation process. A electro-mechanical analogy is drawn where the dynamics of hyperplanes is determined by interaction forces between hyperplanes and the particles which represent the patterns. Relaxation of the system is determined by increasing hyperplane inertia (mass). This picture is used to clarify the dynamics of learning, and to go some way to explaining learning deadlocks and escaping from certain local minima. Furthermore network plasticity is introduced as a dynamic property of the system, and its reduction as a necessary consequence of information storage. Hyperplane inertia is used to explain and avoid destructive relearning in trained networks...|$|R
30|$|By means <b>of</b> <b>gradient</b> <b>descent</b> to {{optimize}} weighted connections feeding into gates {{as well as}} cells, an LSTM network can learn to control information flow. Error is back-propagated through the network {{in such a way}} that exponential decay is avoided. LSTM's learning algorithm is local in space and time with computational complexity per time-step and weights of O(1) for standard topologies.|$|R
5000|$|Let [...] and [...] be the t-th iterates <b>of</b> <b>gradient</b> <b>descent</b> {{applied to}} the {{expected}} and empirical risks, respectively, where both iterations are initialized at the origin, and both use the step size [...] The [...] form the population iteration, which converges to , but cannot be used in computation, while the [...] form the sample iteration which usually converges to an overfitting solution.|$|R
40|$|Abstract. In this paper, {{we study}} a family <b>of</b> <b>gradient</b> <b>descent</b> {{algorithms}} to approximate the regression function from reproducing kernel Hilbert spaces. Here early stopping {{plays a role}} of regularization, where given a finite sample and some regularity condition on the regression function, a stopping rule is given and some probabilistic upper bounds are obtained for {{the distance between the}} function iterated at the stopping time and the regression function. A crucial advantage over other regularized least square algorithms studied recently lies in that it breaks through the saturation phenomenon where the convergence rate no longer improves after a certain level of regularity achieved by the regression function. These upper bounds show that in some situations we can achieve optimal convergence rates. We also discuss the implication of these results in the context of classification. Some connections are addressed with the Landweber iteration for regularization in inverse problems and the online learning algorithms as stochastic approximations <b>of</b> <b>gradient</b> <b>descent</b> method. 1...|$|R
40|$|This paper {{describes}} a new Evolutionary Programming algorithm based on Self-Organised Criticality. When tested {{on a range}} of problems drawn from real-world applications in science and engineering, it performed better than a variety <b>of</b> <b>gradient</b> <b>descent,</b> direct search and genetic algorithms. It proved capable of delivering high quality results faster, and is simple, robust and highly parallel. Griffith Sciences, School of Information and Communication TechnologyFull Tex...|$|R
40|$|The {{estimation}} {{or training}} methods in the neural network literature are usually some simple form <b>of</b> <b>gradient</b> <b>descent</b> algorithm suitable for implementation in hardware using massively parallel computations. For ordinary computers {{that are not}} massively parallel, optimization algorithms {{such as those in}} several SAS procedures are usually far more efficient. This talk shows how to fit neural networks using SAS/OR R, SAS/ETS R, and SAS/STAT R software...|$|R
40|$|Aim of {{this work}} is to {{experimentally}} investigate the potential of a novel technique for CR image restoration which make use <b>of</b> <b>gradient</b> <b>descent</b> algorithm to minimize a local error function derived from the conventional global constrained error measure adopted within regularization approaches. Results of preliminary experiments show that the proposed restoration algorithm is promising for medical imaging restoration and could be useful in limiting x-ray dose absorbed by patients...|$|R
50|$|For solving linear equations, {{gradient}} descent is rarely used, with the conjugate gradient method {{being one of}} the most popular alternatives. The speed <b>of</b> convergence <b>of</b> <b>gradient</b> <b>descent</b> depends on the ratio of the maximum to minimum eigenvalues of , while the speed of convergence <b>of</b> conjugate <b>gradients</b> has a more complex dependence on the eigenvalues, and can benefit from preconditioning. Gradient descent also benefits from preconditioning, but this is not done as commonly.|$|R
