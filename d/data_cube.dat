938|694|Public
2500|$|Some MOLAP tools {{require the}} pre-computation and storage of derived data, such as consolidations – the {{operation}} known as processing. Such MOLAP tools generally utilize a pre-calculated data set {{referred to as}} a <b>data</b> <b>cube.</b> The <b>data</b> <b>cube</b> contains all the possible answers to a given range of questions. As a result, they [...] have a very fast response to queries. On the other hand, updating can take a long time depending on the degree of pre-computation. Pre-computation can also lead to what is known as data explosion.|$|E
5000|$|NNEW {{will provide}} fast access to weather {{information}} to all National Airspace System users by {{the provision of}} the 4-D Wx <b>Data</b> <b>Cube.</b> The 4-D Wx <b>Data</b> <b>Cube</b> will consist of: ...|$|E
50|$|Next Generation (NextGen) Network Enabled Weather (NNEW) is {{a project}} to develop a 4-dimension (all points, lateral, {{vertical}} and time dimensions) weather <b>data</b> <b>cube</b> (4-D Wx <b>Data</b> <b>Cube)</b> from disparate contributors and locations.|$|E
40|$|Abstract. The {{adoption}} of the Linked Data principles and technologies has promised to enhance the analysis of statistics at a Web scale. Statistical data, however, is typically organized in <b>data</b> <b>cubes</b> where a numeric fact (aka measure) is categorized by dimensions. Both <b>data</b> <b>cubes</b> and linked <b>data</b> introduce complexity that raises the barrier for reusing the data. The majority of linked data tools {{are not able to}} support or do not facilitate the reuse of linked <b>data</b> <b>cubes.</b> In this demo we present the OpenCube Toolkit that enables the easy publishing and exploitation of linked <b>data</b> <b>cubes</b> using visualizations and data analytics...|$|R
50|$|CALIFA {{data are}} made public through regular {{releases}} (DR). DR1, containing 200 <b>data</b> <b>cubes</b> of 100 galaxies, was released 1 November 2012. DR2, containing 400 <b>data</b> <b>cubes</b> of 200 galaxies, {{was released on}} 1 October 2014.|$|R
40|$|Applications like Online Analytical Processing depend {{heavily on}} the ability to quickly {{summarize}} large amounts of information. Techniques were proposed recently that speed up aggregate range queries on MOLAP <b>data</b> <b>cubes</b> by storing pre-computed aggregates. These approaches try to handle <b>data</b> <b>cubes</b> of any dimensionality by dealing with all dimensions {{at the same time and}} treat the different dimensions uniformly. The algorithms are typically complex, and it is difficult to prove their correctness and to analyze their performance. We present a new technique to generate Iterative <b>Data</b> <b>Cubes</b> (IDC) that addresses these problems. The proposed approach provides a modular framework for combining one-dimensional aggregation techniques to create space-optimal high-dimensional <b>data</b> <b>cubes.</b> A large variety of cost tradeoffs for high-dimensional IDC can be generated, making it easy to find the right configuration based on the application requirements. 1 Introduction <b>Data</b> <b>cubes</b> are used i [...] ...|$|R
50|$|Spectral {{images are}} often {{represented}} {{as an image}} cube, a type of <b>data</b> <b>cube.</b>|$|E
50|$|In {{computer}} programming contexts, a <b>data</b> <b>cube</b> (or datacube) is a multi-dimensional array of values, {{commonly used to}} describe a time series of image data. The <b>data</b> <b>cube</b> is used to represent data along some measure of interest. Even though it is called a 'cube', it can be 1-dimensional, 2-dimensional, 3-dimensional, or higher-dimensional.Every dimension represents a new attribute in the database and the cells in the cube represent the measure of interest.|$|E
50|$|A <b>data</b> <b>cube</b> is {{also used}} {{in the field of}} imaging spectroscopy, since a spectrally-resolved image is {{represented}} as a three-dimensional volume.|$|E
30|$|Data integration: Integration of {{multiple}} databases, <b>data</b> <b>cubes,</b> or files.|$|R
40|$|Abstract. Applications like Online Analytical Processing depend {{heavily on}} the ability to quickly {{summarize}} large amounts of information. Techniques were proposed recently that speed up aggregate range queries on MOLAP <b>data</b> <b>cubes</b> by storing pre-computed aggregates. These approaches try to handle <b>data</b> <b>cubes</b> of any dimensionality by dealing with all dimensions {{at the same time and}} treat the different dimensions uniformly. The algorithms are typically complex, and it is difficult to prove their correctness and to analyze their performance. We present a new technique to generate Iterative <b>Data</b> <b>Cubes</b> (IDC) that addresses these problems. The proposed approach provides a modular framework for combining one-dimensional aggregation techniques to create spaceoptimal high-dimensional <b>data</b> <b>cubes.</b> A large variety of cost tradeoffs for high-dimensional IDC can be generated, making it easy to find the right configuration based on the application requirements. ...|$|R
40|$|In the multi-dimensional {{model of}} data warehouses, data drawn from {{multiple}} sources is organized into fact tables and dimension tables. <b>Data</b> <b>cubes</b> pre-compute the aggregations of measurements along combinations of dimensions {{to speed up}} the processing of Online Analytical Processing (OLAP) queries. When the underlying fact tables or dimension tables change over time, <b>data</b> <b>cubes</b> need to be incrementally maintained to reflect these changes. In this thesis, we study the problem of incrementally maintaining <b>data</b> <b>cubes</b> under slowly changing dimensions. We argue that temporal features are essential in a data warehouse to avoid loss of historical information, and to help maintain <b>data</b> <b>cubes</b> to be consistent with the state of the underlying tables. We first characterize view maintenance problems based on changing dimensions. We then introduce our temporal multi-dimensional model that can describe changing dimension data, and we discuss strategies for incrementally maintaining <b>data</b> <b>cubes</b> based on that temporal model. A maintenance algorithm is developed, and its performance is evaluated experimentally. The results show that our approach is highly effective and is more efficient than re-materialization in some change cases. The results also show that our approach significantly outperforms existing incremental maintenance approaches...|$|R
50|$|The {{technique}} {{was designed to}} put into practice the concept of 'tilted sampling' of the hyperspectral <b>data</b> <b>cube,</b> which had been deemed difficult to achieve. Spatio-spectral scanning yields a series of thin, diagonal slices of the <b>data</b> <b>cube.</b> Figuratively speaking, each acquired image is a 'rainbow-colored' spatial map of the scene. More precisely, each image represents two spatial dimensions, {{one of which is}} wavelength-coded. To acquire the spectrum of a given object point, scanning is needed.|$|E
5000|$|The Integral Field Spectrograph (IFS) {{covers a}} 1.73" [...] x 1.73" [...] field of view, {{translating}} the spectral data into a three-dimensional (x,y,λ) <b>data</b> <b>cube.</b>|$|E
50|$|In March 2010, it {{introduced}} the Taobao <b>Data</b> <b>Cube</b> platform, which gives small businesses {{access to its}} aggregate consumer transactions data for insight into industry trends.|$|E
40|$|SpectralCube is a toolkit for {{efficiently}} {{handling and}} performing simple analysis of spectral <b>data</b> <b>cubes.</b> It {{was designed for}} use with ALMA and JVLA data, but is readily and easily applicable to other <b>data</b> <b>cubes</b> including optical and infrared IFUs. This 5 -minute "lightning talk" gives a brief overview and update of spectral_cube & the radio-astro-tools packages...|$|R
40|$|Overview: Contains data on {{the numbers}} of schools, {{students}} and staff in Australia. This product, released in two stages, includes derived measures of student engagement and ratios of full-time equivalent (FTE) students to FTE teaching staff. The initial release, in February, contains time series <b>data</b> <b>cubes</b> with student <b>data,</b> including counts and FTE values, for full-time and part-time students. The <b>data</b> <b>cubes</b> display the <b>data</b> by various combinations of state/territory, school affiliation, Aboriginal and Torres Strait Islander status, sex, grade and age. The <b>data</b> <b>cubes</b> are supported with summary pages and explanatory notes. The subsequent release, in March, contains summary tables and additional data on schools, staff and derived measures...|$|R
50|$|For {{business}} intelligence software, <b>data</b> <b>cubes</b> are built from pre-computed aggregates from sales/customer data.|$|R
50|$|The AGDC code base is {{situated}} in GitHub as an open repository. The core code base moved to the Open <b>Data</b> <b>Cube</b> in early 2017 {{as part of an}} international collaboration. Whilst the code base is the Open <b>Data</b> <b>Cube,</b> individual cubes exist as their own right such as the AGDC on the National Computational Infrastructure National Facility (Australia) (NCI) using the High-Performance Computing Cluster HPCC. The core code can be installed on personal computers or public computers (using git) and has many unit tests.|$|E
50|$|Some MOLAP tools {{require the}} pre-computation and storage of derived data, such as consolidations - the {{operation}} known as processing. Such MOLAP tools generally utilize a pre-calculated data set {{referred to as}} a <b>data</b> <b>cube.</b> The <b>data</b> <b>cube</b> contains all the possible answers to a given range of questions. As a result, they have a very fast response to queries. On the other hand, updating can take a long time depending on the degree of pre-computation. Pre-computation can also lead to what is known as data explosion.|$|E
50|$|In mathematics, a {{one-dimensional}} array {{corresponds to}} a sequence, a two-dimensional array resembles a matrix; more generally, a tensor may be represented as an n-dimensional <b>data</b> <b>cube.</b>|$|E
40|$|Navigating Multidimensional Data Using Sk-Association Rules Navigating through multidimensional <b>data</b> <b>cubes</b> is a non-trivial task. Although On-Line Analytical Processing (OLAP) {{provides}} {{the capability to}} view multidimensional data in different perspectives through roll-up, drill-down, and slicing-dicing, it offers only minimal guidance to end users in the actual knowledge discovery process. It is impractical to navigate through the enormous numbers of cuboids that usually make up <b>data</b> <b>cubes,</b> and consequently users are likely to miss out valuable information in their cube navigation. In this paper, we address this problem by proposing a DIscovery of Sk-Association Rules (DISAR) algorithm to drive the knowledge discovery process. First, we develop {{a new set of}} rules known as sk-association rules using a powerful test of skewness on the pairs of lattice nodes. Second, we capture the navigation paths in the <b>data</b> <b>cubes</b> by using sk-association rules. Third, we use the sk-association rules to enhance the navigation capabilities in the <b>data</b> <b>cubes.</b> Experimental results demonstrate detailed evaluation of sk-association rules...|$|R
40|$|Abstract—Data cube is a {{key element}} in {{supporting}} fast OLAP. Traditionally, an aggregate function is used to compute the values in <b>data</b> <b>cubes.</b> In this paper, we extend the notion of <b>data</b> <b>cubes</b> with a new perspective. Instead of using an aggregate function, we propose to build <b>data</b> <b>cubes</b> using the skyline operation as the “aggregate function”. <b>Data</b> <b>cubes</b> built in this way are called “group-by skyline cubes ” and can support a variety of analytical tasks. Nevertheless, there are several challenges in implementing group-by skyline <b>cubes</b> in <b>data</b> warehouses: (i) the skyline operation is computational intensive, (ii) the skyline operation is holistic, and (iii) a group-by skyline cube contains both grouping and skyline dimensions, rendering it infeasible to pre-compute all cuboids in advance. This paper gives details on how to store, materialize, and query such cubes. Index Terms—H. 2. 4. h Query processing; H. 2. 7. b Data warehouse and repository...|$|R
5000|$|A {{data mining}} toolkit for {{exploring}} large <b>data</b> <b>cubes</b> in radioastronomy from facilities like ALMA or CARMA.|$|R
50|$|The Australian Geoscience <b>Data</b> <b>Cube</b> (AGDC) is an {{approach}} to storing, processing and analyzing large collections of earth observation data. The technology is designed to meet challenges of national interest by being agile and flexible with vast amounts of layered grid data.|$|E
5000|$|Note: program cubes are {{inspired}} by tensors and data cubes in databases. The primary difference is that <b>data</b> <b>cube</b> elements are numerical values that are added during cube contraction; program cube elements are transformations that are composed. Both use tensor notations and terminology.|$|E
50|$|A {{subset of}} the data {{published}} to the 4-D Wx <b>Data</b> <b>Cube</b> will be designated the Single Authoritative Source (SAS). The SAS is that data that must be consistent (only one answer) to support collaborative (more than one decision maker) air traffic management decisions.|$|E
5000|$|... "A 2-D x/y slice {{from all}} 4-D climate {{simulation}} <b>data</b> <b>cubes,</b> each one encoded in PNG format": ...|$|R
40|$|Focusing on novel {{database}} application scenarios, where {{data sets}} arise {{more and more}} in uncertain and imprecise formats, in this paper we propose a novel decomposition framework for efficiently computing and querying multidimensional OLAP <b>data</b> <b>cubes</b> over probabilistic <b>data,</b> which well-capture previous kind of data. Several models and algorithms supported in our proposed framework are formally presented and described in details, based on well-understood theoretical statistical/probabilistic tools, which converge to the definition of the so-called probabilistic OLAP <b>data</b> <b>cubes,</b> the most prominent result of our research. Finally, we complete our analytical contribution by introducing an innovative Probability Distribution Function (PDF) -based approach, which makes use of well-known probabilistic estimators theory, for efficiently querying probabilistic OLAP <b>data</b> <b>cubes,</b> along with a comprehensive experimental assessment and analysis over synthetic probabilistic databases...|$|R
5000|$|Cubes {{provides}} to {{an analyst}} or any application end-user [...] "understandable and natural way of reporting using concept of <b>data</b> <b>Cubes</b> - multidimensional <b>data</b> objects".|$|R
5000|$|Insofar as {{two-dimensional}} {{output devices}} cannot readily characterize three dimensions, {{it is more}} practical to project [...] "slices" [...] of the <b>data</b> <b>cube</b> (we say project in the classic vector analytic sense of dimensional reduction, not in the SQL sense, although the two are conceptually similar), ...|$|E
5000|$|For {{modelling}} temporal databases, numerous ER extensions {{have been}} considered. Similarly, the ER model was found unsuitable for multidimensional databases (used in OLAP applications); no dominant conceptual model {{has emerged in}} this field yet, although they generally revolve around the concept of OLAP cube (also known as <b>data</b> <b>cube</b> within the field).|$|E
5000|$|... #Caption: A {{simulation}} of how MUSE {{will see the}} globular cluster NGC 2808. This colour image has been created by first creating a simulated MUSE observation of the globular cluster and then extract three spectral regions from this <b>data</b> <b>cube.</b> Thus for each source in this image there is in truth an entire spectrum.|$|E
5000|$|... {{inventories}} of information assets (including legacy and relational <b>data</b> sources, <b>cubes,</b> <b>data</b> warehouses, and data marts), ...|$|R
40|$|It {{has been}} {{demonstrated}} that malicious users can infer sensitive knowledge from online corporate databases and <b>data</b> <b>cubes</b> that do not adopt effective privacy preserving countermeasures. From this breaking evidence, a plethora of Privacy Preserving Data Mining (PPDM) techniques has been proposed during the last years. Each of these techniques focuses on supporting the privacy preservation of a specialized KDD/DM task such as frequent item set mining, clustering etc. Privacy Preserving OLAP (PPOLAP) is a specific PPDM technique dealing with the privacy preservation of <b>data</b> <b>cubes...</b>|$|R
40|$|The dynamicity of {{sensor data}} sources and {{publishing}} realtime sensor data over a generalised infrastructure like the Web pose {{a new set}} of integration challenges. Semantic Sensor Networks demand excessive expressivity for efficient formal analysis of sensor data. This article specifically addresses the problem of adapting data model specific or contextspecific properties in automatic generation of multidimensional <b>data</b> <b>cubes.</b> The idea is to generate <b>data</b> <b>cubes</b> onthe-fly from syntactic sensor data to sustain decision making, event processing and to publish this data as Linke...|$|R
