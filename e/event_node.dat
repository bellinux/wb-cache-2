12|324|Public
40|$|Abstract. Sensor {{networks}} are a sensing, computing and communication infrastructure that {{are able to}} observe and respond to phenomena in the natural environment and in our physical and cyber infrastructure. The sensors themselves can range from small passive microsensors to larger scale, controllable weather-sensing platforms. Presently, there are many research works for sensor networks. In our previous work, we built a simulation system for simulation the sensor networks. But, we considered that the <b>event</b> <b>node</b> is stationary in the observation field. However, in many applications the <b>event</b> <b>node</b> may move. For example, in an ecology environment the animals can move randomly. In this work, we investigate how the sensor network performs in the case when the <b>event</b> <b>node</b> moves. We carried out the simulations for lattice topology and TwoRayGround radio model considering AODV and DSR protocols. For the performance evaluation, we considered two metrics: routing efficiency and goodput and we compare the simulation results for two cases: when the <b>event</b> <b>node</b> is mobile and stationary. The simulation results have shown that the routing efficiency for the case of mobile <b>event</b> <b>node</b> is better than the stationary <b>event</b> <b>node</b> using AODV protocol. Also, the goodput for the mobile <b>event</b> <b>node</b> case does not change too much compared with the stationary event case using AODV, but the goodput is not good when the number of nodes is increased. Key words: sensor network, mobile event, goodput, routing efficienc...|$|E
40|$|Wireless Sensor Networks (WSNs) demand {{reliable}} and energy efficient paths for critical information delivery to sink node from an event occurrence node. Multipath routing facilitates reliable data delivery {{in case of}} critical information. This paper proposes an event triggered multipath routing in WSNs by employing a set of static and mobile agents. Every sensor node is assumed to know the location information of the sink node and itself. The proposed scheme works as follows: (1) <b>Event</b> <b>node</b> computes the arbitrary midpoint between an <b>event</b> <b>node</b> and the sink node by using location information. (2) <b>Event</b> <b>node</b> establishes a shortest path from itself to the sink node through the reference axis by using a mobile agent {{with the help of}} location information; the mobile agent collects the connectivity information and other parameters of all the nodes on the way and provides the information to the sink node. (3) <b>Event</b> <b>node</b> finds the arbitrary location of the special (middle) intermediate nodes (above/below reference axis) by using the midpoint location information given in step 1. (4) Mobile agent clones from the <b>event</b> <b>node</b> and the clones carry the event type and discover the path passing through special intermediate nodes; the path above/below reference axis looks like an arc. While migrating from one sensor node to another along the traversed path, each mobile agent gathers the node information (such as node id, location information, residual energy, available bandwidth, and neighbors connectivity) and delivers to the sink node. (5) The sink node constructs a partial topology, connecting event and sink node by using the connectivity information delivered by the mobile agents. Using the partial topology information, sink node finds the multipath and path weight factor by using link efficiency, energy ratio, and hop distance. (6) The sink node selects the number of paths among the available paths based upon the criticalness of an event, and (7) if the event is non-critical, then single path with highest path weight factor is selected, else multiple paths are selected for the reliable communication. The performance of the proposed scheme is tested in terms of performance parameters such as packet delivery ratio, energy consumption, latency, and overhead...|$|E
30|$|This paper {{described}} a reinforcement learning based message transfer model for transferring news report messages through a selected path in a trusted provenance network {{with the objective}} of maximizing the reward values based on trust or importance based and network congestion or utility based cost measures. The reward values have been calculated along a dynamically defined policy path connecting start topic or <b>event</b> <b>node</b> to a goal topic or event or issue nodes for incrementally defined time windows for a given network congestion situation. A hierarchy of agents of trusted roles has been used to accomplish the sub-goals associated with sub-story or subtopic in the provenance structure where an agent role has assumed the semantic role of the associated sub-topic. The twitted news story thread or plan of events has been defined in this work from the starting topic or <b>event</b> <b>node</b> to the goal topic or <b>event</b> <b>node</b> for incrementally defined intervals of time. The graphs have been clustered into subtopic and these sub-goals or sub topic nodes of a topic node at every level of granularity are associated with cluster of news reports which describe activities associated with sub-goal or sub-topic events. The policy path in a topic or story graph model has been defined by applying reinforcement learning principles [26] on dynamically defined event models associated with evolution of topic definition observed from incrementally acquired samples of input training data spanning multiple time windows. We have provided a methodology for unifying similar provenance graph models for adapting and averaging the policy path classifiers associated with individual models to produce a reduced set of unified models derived during training. A minimum set cover of classifiers has been identified for the models and a clustering procedure of the models has been suggested based on these classifiers. The methodology described in this work has been detailed for news reports modelling application. We aim at developing this methodology for application to econometrics in our future work.|$|E
30|$|We use control {{messages}} to make nodes aware of upcoming events. The key point is, instead of reporting {{the time of}} each <b>event,</b> <b>nodes</b> report the remaining time slots until the <b>event.</b> Neighboring <b>nodes</b> upon receiving the message add the time slots reported in the received control message to their local time to acquire the time of each event.|$|R
50|$|STX only allows queries {{immediately}} {{surrounding the}} current node {{so it can}} quickly start transforming and outputting SAX <b>event</b> <b>nodes</b> as they arrive. As it can discard nodes immediately after processing the memory use is significantly {{lower than that of}} XSLT. Having a limited query scope is a defining characteristic of STX.|$|R
5000|$|DOM Standard - {{defines a}} platform-neutral model for <b>events</b> and <b>node</b> trees.|$|R
30|$|This paper {{proposes a}} {{reinforcement}} learning based message transfer model for transferring news report messages through a selected path in a trusted provenance network {{with the objective}} of maximizing the reward values based on trust or importance based and network congestion or utility based cost measures. The reward values are calculated along a dynamically defined policy path connecting start topic or <b>event</b> <b>node</b> to a goal topic or event or issue nodes for incrementally defined time windows for a given network congestion situation. A hierarchy of agents of trusted roles is used to accomplish the sub-goals associated with sub-story or subtopic in the provenance structure where an agent role may assume the semantic role of the associated sub-topic. The twitted news story thread or plan of events is defined in this work from the starting topic or <b>event</b> <b>node</b> to the goal topic or <b>event</b> <b>node</b> for incrementally defined intervals of time. The graphs are clustered into subtopic and these sub-goals or sub topic nodes of a topic node at every level of granularity are associated with cluster of news reports which describe activities associated with sub-goal or sub-topic events. Such cluster of nodes may also represent drilled down sequence of sub-events describing a sub-topic or sub-goal node. The policy path in a topic or story graph model is defined by applying reinforcement learning principles on dynamically defined event models associated with evolution of topic definition observed from incrementally acquired samples of input training data spanning multiple time windows. We provide a methodology for unifying similar provenance graph models for adapting and averaging the policy path classifiers associated with individual models to produce a reduced set of unified models derived during training. A minimum set cover of classifiers is identified for the models and a clustering procedure of the models is suggested based on these classifiers. Other database clustering methods have also been suggested as alternatives for clustering these models. A collection of unified models are identified from the models identified within a cluster and the policy path classifiers associated with these models provide the story or topic descriptions destined to goal topic or event nodes characterizing these models within a cluster.|$|E
40|$|An {{approach}} {{focused on}} inferring probabilistic narratives from personal artifacts (including photographs) {{is presented in}} this work using personal photos metadata (timestamp, location, and camera parameters), formal event models, mobile device connectivity, external data sources and web services. We introduce plausibility measure — the occurrence-likelihood of an <b>event</b> <b>node</b> in the output graph. This measure is used {{to find the best}} event among the merely possible candidates. In addition, we propose a new clustering method that uses timestamp, location, and camera parameters in the EXIF header of the input photos to create event boundaries used to detect events. ...|$|E
40|$|Abstract. Multi-task {{scheduling}} networkis extended {{based on}} the complex network theory, from new angles and perspectives to study the multi-task scheduling. The description of method and related mathematical models of complex and multi-task directed network aregiven. Combined with topology of the product development process and their own characteristics,the evaluation model and algorithmto describe {{the importance of the}} <b>event</b> <b>node</b> are presented. It can assess the nature ofdifferent nodes effectively, including the key nodes, at the same time,the scheduling algorithm {{based on the}} importance of active nodes hasbeen proposed. The study has a certain significance toarrange the execution of tasksin complex product development process reasonably and realize optimal allocation of manufacturing resources...|$|E
30|$|We {{performed}} simulations using a self-developed discrete event simulator. The simulator {{is written}} in C++ and implements the Random Waypoint Mobility Model. The <b>events</b> (<b>nodes</b> meeting, node arrival at its selected destination, and alarms time-out) are pushed to and pulled from an ideal time-line. Initially, nodes {{are assumed to be}} randomly deployed over a network area. Then, until the simulation ends, for each node, a random speed and destination location are randomly chosen (within the bounds set by the user): this implies to analyze and to order all the meeting <b>events</b> and the <b>node</b> arrival <b>events</b> with reference to the time-line. While the time goes by, the events on the time-line are processed. The <b>events</b> corresponding to <b>node</b> arrival are processed as previously described (choosing a destination, a node speed, and analyzing the new generated <b>events).</b> The <b>node</b> meeting <b>events</b> are processed as the core part of our detection protocol, for example, updating the time-out or sharing information with the met nodes. The alarms time-out expiring event generates the network flooding.|$|R
30|$|A {{realistic}} {{model for}} information propagation {{may be a}} state dependent variant of the model where propagation events (attempts of influence) occur only for new information (or probability for new information is higher). In other words, information is mediated to neighbouring nodes only in cases when the node is unaware of the information before the propagation <b>event.</b> <b>Nodes</b> are less willing to propagate known information than new information.|$|R
40|$|Some events, like moods or {{qualitative}} evaluations, are not {{grounded in}} any particular observations or actions but can emerge {{as the result of}} internal or external interactions. “Opinion ” rules extracted from diverse sources such as blogs, newspaper articles, chat rooms, etc. can be synthesized to-gether into a fuzzy cognitive map linking <b>event</b> <b>nodes</b> to-gether. We explore in this paper how to learn fuzzy cognitive maps with dynamic programming techniques...|$|R
40|$|Understanding and {{managing}} the response time of web services is of key importance as {{dependence on the}} World Wide Web continues to grow. We present Remote Latency-based Management (RLM), a novel server-side approach for managing pageview response times as perceived by remote clients, in real-time. RLM passively monitors server-side network traffic, accurately tracks the progress of page downloads and their response times in real-time, and dynamically adapts connection setup behavior and web page content as needed to meet response time goals. To manage client perceived pageview response times, RLM builds a novel <b>event</b> <b>node</b> model to guide the use of several techniques for manipulating the packet traffic {{in and out of}} a web server complex, including fast SYN and SYN/ACK retransmission, and embedded object removal and rewrite. RLM operates as a stand-alone appliance that simply sits in front of a web server complex, without any changes to existing web clients, servers, or applications. We have implemented RLM on an inexpensive, commodity, Linux-based PC and present experimental results that demonstrate its effectiveness in managing client perceived pageview response times on transactional e-commerce web workloads...|$|E
30|$|Alternatively, {{previously}} calculated weights {{associated with}} features describing the nearest subtopic or sub-story or goal or topic node obtained using method {{described in this}} work can indicate a change of policy where previously suboptimal action can become optimal and vice versa. A Q Learning procedure {{is applied to the}} revised provenance graph model derived after merging of the significant models. If a newly calculated policy path is not destined to the same goal topic or <b>event</b> <b>node</b> then it is removed from farther consideration. Alternatively, the expected gain from executing action at a state is the difference between the expected Q value as reward value calculated from Eq.  16 from executing the changed action and earlier Q value associated with taking optimal action at the state. The Value of Perfect Information (VPI) associated with taking action at a state is the weighted sum of expected gain measure calculated for all discrete probabilities associating the state, action pair which separates the best classifier policy value and the considered classifier policy value. Here a strategy is selected that maximizes the sum of expected Q value for a state action pair and Value of Perfect Information associated with state action pair The alternatives to the best classifiers which when considered are re-ranked from this expected gain measure. The candidate set of classifiers which form the minimum cover set of classifiers or is the only member of a set of classifiers for a unified provenance graph model are considered for specifying recognition paths for relevant goal topics where these paths may be both general or discerning.|$|E
30|$|The {{cluster node}} {{expansion}} and cluster edge expansion measures are considered for growing a cluster using a derived cluster node expansion capability measure. This method considers the nodes {{in the cluster}} and the complement of the nodes in the cluster remaining in the graph for defining the expansion measure. Also here if the link trust probabilities are fluctuating, a measure based on relative dependence is used for identifying the stability of clustering. The distance measure between any two nodes in the cluster is defined using a Joint Entropy distance value as specified later in Eq.  17. Pair of nodes are only considered {{to be members of}} the same cluster if the activity or task steps or units corresponding to these node are within a predefined threshold distance measure defined using Joint Entropy distance measure between node pair. Also the similarity or distance measure is refined using similarities of roles of Agents associated with these nodes. This distance measure is defined based on length of category or role classification tree path separating the nearest ancestor nodes of agent node pair situated in this topic-subtopic role hierarchy tree. The agent similarity or distance measure is provided higher weight-age when compared to trust link measure in calculating the similarity or distance measure between <b>event</b> <b>node</b> pair. Here an agent role may be represented as an attribute in the event record associated with a network node. This leverages the nodes associated with the same agent role for membership in the same cluster. The entity words and the topic words are important in calculating similarity or distance measure for documents or news reports for these nodes to be positioned in the same topic or sub topic cluster. This similarity measure is represented using the probabilistic trust weight link connecting the nodes positioned in the provenance network. The distance measure calculation from this weight measure has been described later in this section.|$|E
40|$|Fairness in {{wireless}} sensor networks demands <b>event</b> <b>nodes</b> to have {{an equal}} share in the overall throughput of the system. Fairness is difficult to achieve in sensor networks due to multiple hop packet forwarding to a single destination that results in congestion. Moreover, greater the density of event region greater {{is the level of}} interference and greater is the number of packet drops. In this paper, we present a mechanism for fair event reporting that is implemented at the transport layer. The solution encompasses congestion control and fair rate adjustment schemes based on hop-by-hop packet delivery and buffer size. Moreover, a schedule based packet forwarding policy is used at the transport layer for ordered delivery of packets to the underlying layer; instead of commonly used jittered forwarding. The simulation results of our scheme shows high per node fair throughput from multiple <b>event</b> <b>nodes</b> to a single sink. Also, the use of our schedule-based packet forwarding policy helps to avoid packet collisions and increases the packet delivery ratio even from densely populated event regions...|$|R
3000|$|... are <b>events</b> that <b>node</b> w is {{determined}} that a packet was transmitted well to <b>node</b> u. Negative <b>events</b> represented by n [...]...|$|R
3000|$|... wait of the <b>event</b> {{occurrence}} <b>node</b> for {{the arrival}} of next scheduled slot determined by self timer before sending the sensed event data [...]...|$|R
40|$|Fault {{tree and}} digraph models are {{frequently}} used for system failure analysis. Both type of models represent a failure space {{view of the}} system using AND and OR nodes in a directed graph structure. Fault trees must have a tree structure and do not allow cycles or loops in the graph. Digraphs allow any pattern of interconnection between loops in the graphs. A common operation performed on digraph and fault tree models is the calculation of minimal cut sets. A cut set {{is a set of}} basic failures that could cause a given target failure event to occur. A minimal cut set for a target <b>event</b> <b>node</b> in a fault tree or digraph is any cut set for the node with the property that if any one of the failures in the set is removed, the occurrence of the other failures in the set will not cause the target failure event. CUTSETS will identify all the minimal cut sets for a given node. The CUTSETS package contains programs that solve for minimal cut sets of fault trees and digraphs using object-oriented programming techniques. These cut set codes can be used to solve graph models for reliability analysis and identify potential single point failures in a modeled system. The fault tree minimal cut set code reads in a fault tree model input file with each node listed in a text format. In the input file the user specifies a top node of the fault tree and a maximum cut set size to be calculated. CUTSETS will find minimal sets of basic events which would cause the failure at the output of a given fault tree gate. The program can find all the minimal cut sets of a node, or minimal cut sets up to a specified size. The algorithm performs a recursive top down parse of the fault tree, starting at the specified top node, and combines the cut sets of each child node into sets of basic event failures that would cause the failure event at the output of that gate. Minimal cut set solutions can be found for all nodes in the fault tree or just for the top node. The digraph cut set code uses the same techniques as the fault tree cut set code, except it includes all upstream digraph nodes in the cut sets for a given node and checks for cycles in the digraph during the solution process. CUTSETS solves for specified nodes and will not automatically solve for all upstream digraph nodes. The cut sets will be output as a text file. CUTSETS includes a utility program that will convert the popular COD format digraph model description files into text input files suitable for use with the CUTSETS programs. FEAT (MSC- 21873) and FIRM (MSC- 21860) available from COSMIC are examples of programs that produce COD format digraph model description files that may be converted for use with the CUTSETS programs. CUTSETS is written in C-language to be machine independent. It has been successfully implemented on a Sun running SunOS, a DECstation running ULTRIX, a Macintosh running System 7, and a DEC VAX running VMS. The RAM requirement varies with the size of the models. CUTSETS is available in UNIX tar format on a. 25 inch streaming magnetic tape cartridge (standard distribution) or on a 3. 5 inch diskette. It is also available on a 3. 5 inch Macintosh format diskette or on a 9 -track 1600 BPI magnetic tape in DEC VAX FILES- 11 format. Sample input and sample output are provided on the distribution medium. An electronic copy of the documentation in Macintosh Microsoft Word format is included on the distribution medium. Sun and SunOS are trademarks of Sun Microsystems, Inc. DEC, DeCstation, ULTRIX, VAX, and VMS are trademarks of Digital Equipment Corporation. UNIX is a registered trademark of AT&T Bell Laboratories. Macintosh is a registered trademark of Apple Computer, Inc...|$|E
40|$|Fault {{tree and}} digraph models are {{frequently}} used for system failure analysis. Both types of models represent a failure space {{view of the}} system using AND and OR nodes in a directed graph structure. Each model has its advantages. While digraphs can be derived in a fairly straightforward manner from system schematics and knowledge about component failure modes and system design, fault tree structure allows for fast processing using efficient techniques developed for tree data structures. The similarities between digraphs and fault trees permits the information encoded in the digraph to be translated into a logically equivalent fault tree. The DG TO FT translation tool will automatically translate digraph models, including those with loops or cycles, into fault tree models that have the same minimum cut set solutions as the input digraph. This tool could be useful, for example, if some parts of a system have been modeled using digraphs and others using fault trees. The digraphs could be translated and incorporated into the fault trees, allowing them to be analyzed using a number of powerful fault tree processing codes, such as cut set and quantitative solution codes. A cut set for a given node {{is a group of}} failure events that will cause the failure of the node. A minimum cut set for a node is any cut set that, if any of the failures in the set were to be removed, the occurrence of the other failures in the set will not cause the failure of the event represented by the node. Cut sets calculations can be used to find dependencies, weak links, and vital system components whose failures would cause serious systems failure. The DG TO FT translation system reads in a digraph with each node listed as a separate object in the input file. The user specifies a terminal node for the digraph that will be used as the top node of the resulting fault tree. A fault tree basic <b>event</b> <b>node</b> representing the failure of that digraph node is created and becomes a child of the terminal root node. A subtree is created for each of the inputs to the digraph terminal node and the root of those subtrees are added as children of the top node of the fault tree. Every node in the digraph upstream of the terminal node will be visited and converted. During the conversion process, the algorithm keeps track of the path from the digraph terminal node to the current digraph node. If a node is visited twice, then the program has found a cycle in the digraph. This cycle is broken by finding the minimal cut sets of the twice visited digraph node and forming those cut sets into subtrees. Another implementation of the algorithm resolves loops by building a subtree based on the digraph minimal cut sets calculation. It does not reduce the subtree to minimal cut set form. This second implementation produces larger fault trees, but runs much faster than the version using minimal cut sets since it does not spend time reducing the subtrees to minimal cut sets. The fault trees produced by DG TO FT will contain OR gates, AND gates, Basic Event nodes, and NOP gates. The results of a translation can be output as a text object description of the fault tree similar to the text digraph input format. The translator can also output a LISP language formatted file and an augmented LISP file which can be used by the FTDS (ARC- 13019) diagnosis system, available from COSMIC, which performs diagnostic reasoning using the fault tree as a knowledge base. DG TO FT is written in C-language to be machine independent. It has been successfully implemented on a Sun running SunOS, a DECstation running ULTRIX, a Macintosh running System 7, and a DEC VAX running VMS. The RAM requirement varies with the size of the models. DG TO FT is available in UNIX tar format on a. 25 inch streaming magnetic tape cartridge (standard distribution) or on a 3. 5 inch diskette. It is also available on a 3. 5 inch Macintosh format diskette or on a 9 -track 1600 BPI magnetic tape in DEC VAX FILES- 11 format. Sample input and sample output are provided on the distribution medium. An electronic copy of the documentation in Macintosh Microsoft Word format is provided on the distribution medium. DG TO FT was developed in 1992. Sun, and SunOS are trademarks of Sun Microsystems, Inc. DECstation, ULTRIX, VAX, and VMS are trademarks of Digital Equipment Corporation. UNIX is a registered trademark of AT&T Bell Laboratories. Macintosh is a registered trademark of Apple Computer, Inc. System 7 is a trademark of Apple Computers Inc. Microsoft Word is a trademark of Microsoft Corporation...|$|E
40|$|Abstract. For random {{distribution}} of large-scale wireless sensor network(WSN), the hexagonal distributed cluster-based multi-hop routing protocol based on event triggering (HDCMET) is proposed. Triggered by <b>events,</b> <b>nodes</b> optimize a cluster head, {{according to the}} rest energy. A cluster head chooses other nodes for a hexagon, with distance to the cluster, energy and so on. Simulation results show that this protocol prolongs the network life in total, improving the network performance in large-scale WSN, compared with the low energy adaptive cluster hierarchy (LEACH) ...|$|R
40|$|We {{propose a}} {{solution}} for detecting and summarizing hu-man activities from {{a large amount of}} video. A video is divided into short segments(called stories), and a set of simple motion/image features are computed for each image frame. Prototypical elementary action patterns(called ac-tion events) is constructed by vector quantization on these features. A bipartite graph is constructed by taking stories and action <b>events</b> as <b>nodes,</b> and the co-occurrence rela-tionship between them as the graph edges. We compute an optimal reordering of the video story and the action <b>event</b> <b>nodes</b> so that they are in maximum correlation with each other. The optimal mapping between the stories and action events leads to a grouping and ordering of the video sto-ries into activity patterns. The corresponding ordering on the action events reveals their characteristic features. Ex-perimental results are shown a hospital surveillance video. ...|$|R
40|$|This paper {{presents}} an event model which describes the data structures that are realised in mobile computing. The model {{is divided into}} two different types, namely atomic events and composite events, and provides the means for the classification and package of events. From the perspective of complex networks, a kind of event choreography network(ECN) is constructed, a formal definition is proposed for event ontology, and ECN construction and its features are analyzed. The results show that ECN simplifies the analysis of choreography relationship between multiple <b>event</b> <b>nodes...</b>|$|R
3000|$|... {{at the end}} of {{the block}} of length n is defined by E_i,j:={ W_i,j≠W_i,j (.)}, and the error <b>event</b> at <b>node</b> j in which node j wants to find w [...]...|$|R
3000|$|... set wake {{interval}} {{of the first}} hop neighbors of <b>event</b> occurrence <b>node</b> to 2 × WInormal and send the message about the changed wake interval to the first hop neighbors [...]...|$|R
30|$|The {{complexity}} of our scheme per distance estimation sample for K nodes {{is comprised of}} K signal sampling, storing, and communication <b>events</b> (<b>nodes</b> transmitting the recorded signals to the central entity), and K(K - 1)/ 2 computations of the signal cross correlations and peak searches. Note that the computation of cross correlations can be done computationally efficiently using the fast Fourier transform. In contrast, conventional TDOA-based schemes need K(K - 1)/ 2 ranging actions (i.e., ranging between all pairs of nodes). Subsequently, the ranging information must be communicated by at least K - 1 nodes. Thus, the main difference in complexity lies in the computation of the cross correlations.|$|R
3000|$|... are {{respectively}} {{the total number}} of positive events and {{the total number of}} negative events. If there are no <b>events</b> between <b>node</b> w and node u, the LTO is null. The sum of positive and negative events (p [...]...|$|R
40|$|In this paper, the {{multi-mode}} resource constrained {{project scheduling}} problem with discounted cash flows (RCPSPDCF) is considered. The objective is the maximization {{of the net}} present value (NPV) of all cash flows. The cash in- and out-flows are associated with activities and/or events. A genetic algorithm (GA) approach is developed exploiting the multi-component nature of the problem. Renewable, non-renewable, and doubly constrained resources are considered. Three different payment programs are considered: A lump sum payment at the terminal event, payments at pre-specified <b>event</b> <b>nodes,</b> and payments at pre-specified time points. A set of 93 problems are solved under three different payment programs. GA developed proves to be a robust algorithm resulting in satisfactory computational times...|$|R
5000|$|Program Evaluation Review Technique (PERT) {{offers a}} {{management}} tool, which relies [...] "on arrow and node diagrams {{of activities and}} events: arrows represent the activities or work necessary to reach the <b>events</b> or <b>nodes</b> that indicate each completed phase of the total project." ...|$|R
40|$|In {{the paper}} {{we present a}} method for {{determining}} testing scenarios for parallel and distributed software. It is based on simulation of software behavior and combines static and dynamic analysis to provide a convenient framework for structural program testing. The underlying testing model consists of parallel program control flow structures including data processing nodes, decision <b>nodes,</b> communication <b>event</b> <b>nodes</b> and independent control flow tokens. Simulation of token movements between node objects enables interactive design of testing scenarios for complex application programs whose operational behavior is difficult to predict before their actual execution. A test scenario can then be reproduced in the target system by providing the recorded data for relevant actions involved in interprocess communication. Keywords: parallel program testing, test design, testing scenario, operational behavior simulation. ...|$|R
40|$|Abstract — Based on the holonic C 2 {{organizational}} control architecture (OCA) that models a C 2 {{organization as}} an inte-gration of multi-level, de-centralized decision making networks, {{we present a}} holonic multi-objective evolutionary algorithm (MOEA) that produces robust and flexible distributed schedules within a dynamic ESG mission environment, such as asset break down, appearance of new <b>events,</b> <b>node</b> failures, etc. The lower level units generate multiple local schedules based on local resources, constraints, and interests (objectives). These local schedules correspond to a schedule pool, from which the Operational Unit can assemble a set of ranked L−Neighboring global schedules according to global objectives, and the actual schedule can shift among different stages of alternative schedules in order to adapt to environmental changes. Global feasibility is ensured at the upper level operational unit, while loca...|$|R
5000|$|Event: Element/Text <b>nodes</b> <b>events</b> may {{flow through}} {{different}} paths. A document may be concurrently flowing through many components {{at the same}} time.|$|R
30|$|To summarize, {{external}} shock <b>events</b> affect <b>nodes</b> in a resource dependency model, where complex interdependencies {{lead to a}} “spread” of impacts until even business entities modeled by a mission dependency model become impacted. This assessment {{is based on a}} well-defined probabilistic inference problem, which is discussed in the following section.|$|R
50|$|Windows NT Load Balancing Service (WLBS) is {{a feature}} of Windows NT that {{provides}} load balancing and clustering for applications. WLBS dynamically distributes IP traffic across multiple cluster nodes, and provides automatic failover in the <b>event</b> of <b>node</b> failure. WLBS was replaced by Network Load Balancing Services in Windows 2000.|$|R
40|$|Abstract. University {{timetabling}} (UTT) is {{a complex}} problem due to its combinatorial nature but also the type of constraints involved. The holy grail of (constraint) programming: ”the user states the problem the program solves it ” remains a challenge since solution quality is tightly coupled with deriving ”effective models”, best handled by technology experts. In this paper, focusing {{on the field of}} university timetabling, we introduce a visual graphic communication tool that lets the user specify her problem in an abstract manner, using a visual entity-relationship model. The entities are nodes of mainly two types: resource nodes (lecturers, assistants, student groups) and <b>events</b> <b>nodes</b> (lectures, lab sessions, tutorials). The links between the nodes signify a desired relationship between them. The visual modeling abstraction focuses {{on the nature of the}} entities and their relationships and abstracts from an actual constraint model. ...|$|R
40|$|Approved {{for public}} release; {{distribution}} unlimited. The {{purpose of this}} thesis is {{to determine if the}} Queuing Graphical Evaluation Review Technique (Q-GERT) can accurately model tactical military communications in a manner which can be used and understood by managers who have little computer expertise. Q-GERT is a FORTRAN-based analysis package which models networks consisting of <b>events</b> (<b>nodes)</b> and activities (arcs). The communications of a mechanized brigade covering force is easily modeled using Q-GERT symbolism and user written FORTRAN inserts; however, a manager must be proficient in FORTRAN and have access to the 1000 node/ 1000 arc Q-GERT package to accurately implement this model. The conclusion of this thesis is that Q-GERT is a flexible modeling technique which should prove to be a valuable managerial tool for the military manager when the larger Q-GERT package is used. [URL] United States Arm...|$|R
