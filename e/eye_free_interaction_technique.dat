0|4748|Public
40|$|Traversing large {{open spaces}} is a {{challenging}} task for blind cane users, as such spaces are often devoid of tactile fea-tures {{that can be}} followed. Consequently, in such spaces cane users may veer from their intended paths. Wearable de-vices have great potential for assistive applications for users who are blind as they typically feature a camera and sup-port hands and <b>eye</b> <b>free</b> <b>interaction.</b> We present HEADLOCK, a navigation aid for an optical head-mounted display that helps blind users traverse large open spaces by letting them lock onto a salient landmark across the space, such as a door, and then providing audio feedback to guide the user towards the landmark. A user study with 8 blind users eval-uated the usability and effectiveness of two types of audio feedback (sonification and text-to-speech) for guiding a user across an open space to a doorway. Qualitative results are reported, which may inform the design of assistive wearable technology for users who are blind...|$|R
40|$|Current {{interaction}} options with smartphones are limited. In this paper, {{we present}} four experiments that we conducted and our initial findings. Our experimenting process was analyzed together with Dourish’s work on tangible and embodied interaction and we gained insights {{to better understand}} the meaning behind smartphone interactions. This sets a foundation for discovering new ways to interact with smartphones in the future such as touch-less, <b>eyes</b> <b>free</b> or haptic <b>interaction...</b>|$|R
40|$|This paper {{presents}} {{a study that}} explores the issues of mobile multimodal interactions {{while on the move}} in the real world. Because multimodal interfaces allow new kinds of <b>eyes</b> and hands <b>free</b> <b>interactions,</b> usability issues while moving through different public spaces becomes an important issue in user experience and acceptance of multimodal interaction. This study focuses on these issues by deploying an RSS reader that participants used during their daily commute every day for one week. The system allows participants on the move to access news feeds <b>eyes</b> <b>free</b> through headphones playing audio and speech and hands free through wearable sensors attached to the wrists. The results showed participants were able to interact with the system on the move and became more comfortable performing these interactions as the study progressed. However, participants were far more comfortable gesturing on the street than on public transport, which was reflected in the number of interactions and the perceived social acceptability of the gestures in different contexts...|$|R
50|$|Michael {{returns to}} {{space for a}} {{showdown}} with Brother <b>Eye,</b> <b>freeing</b> Superman to assist him.|$|R
40|$|Abstract. Distant {{displays}} such as interactive Public Displays (IPD) or Interactive Television (ITV) require new <b>interaction</b> <b>techniques</b> {{as traditional}} input devices {{may be limited}} or missing in these contexts. <b>Free</b> hand <b>interaction,</b> as sensed with computer vision techniques, presents a promising <b>interaction</b> <b>technique.</b> This paper presents the adaptation of three menu techniques for free hand interaction: Linear menu, Marking menu and Finger-Count menu. The first study based on a Wizard-of-OZ protocol focuses on Finger-Counting postures in front of interactive television and public displays. It reveals that participants do choose the most efficient gestures neither before nor after the experiment. Results are used to develop a Finger-Count recognizer. The second experiment shows that all techniques achieve satisfactory accuracy. It also shows that Finger-Count requires more mental demand than other techniques...|$|R
40|$|This chapter {{describes}} {{the area of}} human-computer <b>interaction</b> <b>technique</b> research in general and then describes research in several new types of <b>interaction</b> <b>techniques</b> under way at the Human-Computer Interaction Laboratory of the U. S. Naval Research Laboratory: eye movement-based <b>interaction</b> <b>techniques,</b> three-dimensional pointing, and, finally, using dialogue properties in <b>interaction</b> <b>techniques...</b>|$|R
60|$|And {{here the}} worthy soul broke down, {{and began to}} cry, nor were Angela's <b>eyes</b> <b>free</b> from tears.|$|R
40|$|Part 1 : Long and Short PapersInternational {{audience}} Distant displays such as interactive Public Displays (IPD) or Interactive Television (ITV) require new <b>interaction</b> <b>techniques</b> {{as traditional}} input devices {{may be limited}} or missing in these contexts. <b>Free</b> hand <b>interaction,</b> as sensed with computer vision techniques, presents a promising <b>interaction</b> <b>technique.</b> This paper presents the adaptation of three menu techniques for free hand interaction: Linear menu, Marking menu and Finger-Count menu. The first study based on a Wizard-of-OZ protocol focuses on Finger-Counting postures in front of interactive television and public displays. It reveals that participants do choose the most efficient gestures neither before nor after the experiment. Results are used to develop a Finger-Count recognizer. The second experiment shows that all techniques achieve satisfactory accuracy. It also shows that Finger-Count requires more mental demand than other techniques. </p...|$|R
40|$|The present paper {{describes}} a multimodal approach to authoring in immersive virtual environments. The technique for 3 D curve input presented here {{is based on}} Taping, which is widely used in automotive design to create and modify characteristic lines of a model on a white board. Taping is used both at design review sessions or at model creation, in which designers fasten tape to back-projections of 3 D models or on clay models. Currently, this approach requires designers to make photographs of their intended changes so they can later {{be transferred to the}} 3 D CAD representation of the model. This process is time-consuming and can lead to miscommunications between designers and product engineers on how certain curves are to be interpreted. SketchAR is a multimodal immersive authoring system for Virtual Environments which uses sketching and calligraphic input as its main organizing paradigms. Within SketchAR virtual tape drawing has been integrated as a two-handed multi-modal <b>interaction</b> <b>technique.</b> Using optical tracking and multimodal interactions we have achieved a gadget-free <b>interaction</b> <b>technique</b> to input three-dimensional curves in 3 D. Our approach uses passive optical tracking devices for high-precision and low latency input. Menu <b>free</b> <b>interaction</b> is accomplished through a synergistic combination of hand motion tracking and speech. ...|$|R
5000|$|S: Advanced Driver Assist Display, [...] "D-Shaped" [...] Sports Steering Wheel, Siri <b>Eyes</b> <b>Free</b> Technology, Bluetooth Streaming Audio, and Rear-View Backup Camera System ...|$|R
50|$|The 2016 model {{comes with}} {{technology}} such as NissanConnect with Navigation, Siri <b>Eyes</b> <b>Free</b> {{and many other}} new features. Eight exterior colors are offered.|$|R
40|$|This paper {{focuses on}} the {{evaluation}} of virtual reality (VR) <b>interaction</b> <b>techniques</b> for exploration of data warehouse (DW). The experimental DW involves hierarchical levels and contains information about customers profiles and related purchase items. A user study {{has been carried out}} to compare two navigation and selection techniques. Sixteen volunteers were instructed to explore the DW and look for information using the <b>interaction</b> <b>techniques,</b> involving either a single WiimoteTM (monomanual) or both WiimoteTM and NunchuckTM (bimanual). Results indicated that the bimanual <b>interaction</b> <b>technique</b> is more efficient in terms of speed and error rate. Moreover, most of the participants preferred the bimanual <b>interaction</b> <b>technique</b> and found it more appropriate for the exploration task. We also observed that males were faster and made less errors than females for both <b>interaction</b> <b>techniques...</b>|$|R
40|$|This chapter {{describes}} {{the area of}} human-computer <b>interaction</b> <b>technique</b> research in general and then describes research in several new types of <b>interaction</b> <b>techniques</b> under way at the Human-Computer Interaction Laboratory of the U. S. Naval Research Laboratory: eye movement-based <b>interaction</b> <b>techniques,</b> three-dimensional pointing, and, finally, using dialogue properties in <b>interaction</b> <b>techniques.</b> Keywords. human-computer <b>interaction,</b> <b>interaction</b> <b>techniques,</b> eye movements, gesture, pointing, dialogue 1 Introduction Tufte [9] has described human-computer interaction as two powerful information processors (human and computer) attempting {{to communicate with each}} other via a narrow-bandwidth, highly constrained interface. A fundamental goal of research in human-computer interaction is, therefore, to increase the useful bandwidth across that interface. A significant bottleneck in the effectiveness of educational systems as well as other interactive systems is this communication path betw [...] ...|$|R
40|$|Abstract—Even though {{interaction}} {{is an important}} part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis <b>interaction</b> <b>techniques</b> exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of <b>interaction</b> <b>techniques</b> widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user’s intent while interacting with a system rather than the low-level <b>interaction</b> <b>techniques</b> provided by a system. The categories can act as a framework to help discuss and evaluate <b>interaction</b> <b>techniques</b> and hopefully lay an initial foundation toward a deeper understanding and a science of interaction. Index Terms—Information visualization, <b>interaction,</b> <b>interaction</b> <b>techniques,</b> taxonomy, visual analytics...|$|R
40|$|Abstract. This paper {{presents}} an analysis, implementation {{and evaluation of}} the physical mobile <b>interaction</b> <b>techniques</b> touching, pointing and scanning. Based on this we have formulated guidelines that show in which context which <b>interaction</b> <b>technique</b> is preferred by the user. Our main goal was to identify typical situations and scenarios in which the different techniques might be useful or not. In support of these aims we have developed and evaluated, within a user study, a low-fidelity and a high-fidelity prototype to assess scanning, pointing and touching <b>interaction</b> <b>techniques</b> within different contexts. Other work has shown that mobile devices can act as universal remote controls for interaction with smart objects but, to date, {{there has been no}} research which has analyzed when a given mobile <b>interaction</b> <b>technique</b> should be used. In this research we analyze the appropriateness of three <b>interaction</b> <b>techniques</b> as selection techniques in smart environments. ...|$|R
40|$|There are two typical {{approaches}} {{to the design of}} three-dimensional (3 D) <b>interaction</b> <b>techniques</b> for immersive virtual environments (VEs). Many 3 D <b>interaction</b> <b>techniques</b> are designed for generic user tasks such as navigation, selection, manipulation, and system control, without consideration of the domain-of-use. Despite a long history in HCI theory and practice of using domain knowledge for system-level design, it has not often been used for the design of <b>interaction</b> <b>techniques.</b> Other 3 D <b>interaction</b> <b>techniques</b> are designed for specific applications. While these techniques may be quite usable and useful in that single application, their reuse is limited. In this paper, we propose a middle ground: domain-specific design (DSD) of 3 D <b>interaction</b> <b>techniques.</b> The goal of DSD is to improve upon current practice so that 3 D <b>interaction</b> <b>techniques</b> are designed to address real-world tasks, while still allowing for reuse of the techniques within a particular domain. A three-level design framework provides a theoretical basis for DSD, and we show how this framework can be used to illustrate multiple paths for the design of domain-specific <b>interaction</b> <b>techniques.</b> We also provide a comprehensive, practical case study of an actual VE system, Virtual-SAP (structure analysis program), to illustrate how the approach is applied. Experimental results demonstrate that the use of the DSD approach increases the usefulness of this application, without sacrificing usability. Keywords: 3 D interaction, domain-specific interaction, design theory, virtual environment...|$|R
50|$|<b>Free</b> <b>interaction</b> time: the swimmer has the {{opportunity}} to establish free and open contact with dolphins.|$|R
40|$|Previous {{observations}} that fin-generated interactions are quasi-conical in nature were further confirmed by surface pressure measurements spanning Mach 2. 5 - 3. 5, which encompassed unseparated through strongly separated interactions. For strongly separated interactions {{in which the}} shock wave is bifurcated into a lambda-foot structure, the conical <b>free</b> <b>interaction</b> hypothesis was validated through an appropriate scaling of the far-field surface pressure distribution. The behavior of the lambda-foot structure, such as the decrease of {{the slope of the}} separation shock with interaction strength, was explained by invoking the conical <b>free</b> <b>interaction</b> hypothesis. Through the conical <b>free</b> <b>interaction</b> hypothesis, it was further shown that the triple-shock intersection behaves in a complicated manner with changes in interaction strength...|$|R
50|$|<b>Interaction</b> <b>techniques</b> are {{the glue}} between {{physical}} I/O devices and interaction tasks or domain objects. Different types of <b>interaction</b> <b>techniques</b> {{can be used}} to map a specific device to a specific domain object. For example, different gesture alphabets exist for pen-based text input.|$|R
40|$|This paper {{introduces}} a novel kinesthetic <b>interaction</b> <b>technique</b> for interactive floors. The <b>interaction</b> <b>techniques</b> utilize vision-based limb tracking on an interactive floor – a 12 m² glass surface with bottom projection. The kinesthetic <b>interaction</b> <b>technique</b> {{has been developed}} for an interactive floor implemented in a school square. The paper discusses the kinesthetic <b>interaction</b> <b>technique</b> and its potentials {{in the domain of}} learning applications: Kinesthetic interaction supports body-kinesthetic learning as argued in the learning literature. Kinesthetic interaction is fun and motivating thus encourages children to explore and learn. Kinesthetic interaction on large display surfaces supports collaborative, co-located play and learning through communication and negotiation among the participants. Finally, the paper discusses prospects and challenges in development of kinesthetic interaction for interactive floors...|$|R
50|$|A {{large part}} of {{research}} in human-computer interaction involves exploring easier-to-learn or more efficient <b>interaction</b> <b>techniques</b> for common computing tasks. This includes inventing new (post-WIMP) <b>interaction</b> <b>techniques,</b> possibly relying on methods from user interface design, and assessing their efficiency with respect to existing techniques using methods from experimental psychology. Examples of scientific venues in these topics are the UIST and the CHI conferences. Other research focuses on the specification of <b>interaction</b> <b>techniques,</b> sometimes using formalisms such as Petri nets {{for the purposes of}} formal verification.|$|R
5000|$|From the computer's perspective, an <b>interaction</b> <b>technique</b> involves: ...|$|R
5000|$|... #Subtitle level 2: Comparison {{with other}} <b>interaction</b> <b>techniques</b> ...|$|R
60|$|Then {{with two}} or three leather belts she was {{securely}} tied to the flagstaff, whilst a thick woollen scarf was wound round her face and neck, leaving only the <b>eyes</b> <b>free</b> to roam wildly on the awful scene around.|$|R
40|$|The <b>interaction</b> <b>techniques</b> {{that are}} used in {{tabletop}} groupware systems (such as pick-and-drop or pantograph) can affect the way that people collaborate. However, little is known about these effects, making it difficult for designers to choose appropriate techniques when building tabletop groupware. We carried out an exploratory study to determine how several different types of <b>interaction</b> <b>techniques</b> (pantograph, telepointers, radar views, drag-and-drop, and laser beam) affected coordination and awareness in two tabletop tasks (a game and a storyboarding activity). We found that the choice of <b>interaction</b> <b>technique</b> significantly affected coordination measures, performance measures, and preference – but that the effects were different for the two different tasks. Our study shows that the choice of tabletop <b>interaction</b> <b>technique</b> does indeed matter, and provides insight into how tabletop systems can better support group work...|$|R
40|$|In this {{position}} {{paper we discuss}} the usage of various interaction technologies with focus on the presentations of 3 D visualizations involving a presenter and an audience. While an <b>interaction</b> <b>technique</b> is commonly evaluated from a user perspective, we want to shift the focus from a sole analysis of the naturalness and the ease-of-use for the user, to focus on how expressive and understandable the <b>interaction</b> <b>technique</b> is when witnessed by the audience. The interaction process itself can {{be considered to be}} a communication channel and a more expressive <b>interaction</b> <b>technique</b> might {{make it easier for the}} audience to comprehend the presentation. Thus, while some natural <b>interaction</b> <b>techniques</b> for interactive visualization are easy to perform by the presenter, they may be less beneficial when interacting with the visualization in front of (and for) an audience. Our observations indicate that the suitability of an <b>interaction</b> <b>technique</b> as a communication channel is highly dependent on the setting in which the interaction takes place. Therefore, we analyze different presentation scenarios in an exemplary fashion and discuss how beneficial and comprehensive the involved techniques are for the audience. We argue that <b>interaction</b> <b>techniques</b> complement the visualization in an interactive presentation scenario as they also serve as an important communication channel, and should therefore also be observed from an audience perspective rather than exclusively a user perspective...|$|R
30|$|In the {{following}} subsections, we firstly describe the related work concerning the chord <b>interaction</b> <b>technique</b> {{and the personal}} windows interface and then, we demonstrate {{the need for a}} toolkit that can handle these multi-user multi-touch techniques and describe what experimental task is needed in order to evaluate this multi-user multi-touch <b>interaction</b> <b>techniques.</b>|$|R
40|$|Researchers have {{developed}} interaction concepts based on mobile projectors. Yet pursuing {{work in this}} area— particularly in building projector-based <b>interactions</b> <b>techniques</b> within an application—is cumbersome and timeconsuming. To mitigate this problem, we contribute ProjectorKit, a flexible open-source toolkit that eases rapid prototyping mobile projector <b>interaction</b> <b>techniques.</b> Author Keywords Mobile projectors, toolkit, rapid prototyping...|$|R
40|$|We {{present a}} set of <b>interaction</b> <b>techniques</b> for {{electronic}} musical performance using a tabletop tangible interface. Our system, the Audiopad, tracks the positions of objects on a tabletop surface and translates their motions into commands for a musical synthesizer. We developed and refi ned these <b>interaction</b> <b>techniques</b> through an iterative design process, in which new <b>interaction</b> <b>techniques</b> were periodically evaluated through performances and gallery installations. Based on our experience refi ning the design of this system, we conclude that tabletop interfaces intended for collaborative use should use <b>interaction</b> <b>techniques</b> designed to be legible to onlookers. We also conclude that these interfaces should allow users to spatially reconfi gure the objects in the interface {{in ways that are}} personally meaningful. Categories and Subject Descriptors H. 5. 2 [User Interfaces]: interaction styles, input devices and strategies J. 5 : [Arts and Humanities]: performing art...|$|R
40|$|<b>Interaction</b> <b>Techniques</b> in Virtual Environments Doug A. Bowman, Chadwick A. Wingrave, Joshua M. Campbell, and Vinh Q. Ly Department of Computer Science (0106) Virginia Tech Blacksburg, VA 24061 USA {bowman, cwingrav, jocampbe, vly}@vt. edu Abstract Usable {{three-dimensional}} (3 D) <b>interaction</b> <b>techniques</b> {{are difficult}} to design, implement, and evaluate. One {{reason for this is}} a poor understanding of {{the advantages and disadvantages of}} the wide range of 3 D input devices, and of the mapping between input devices and <b>interaction</b> <b>techniques.</b> We present an analysis of Pinch Gloves^TM and their use as input devices for virtual environments (VEs). We have developed a number of novel and usable <b>interaction</b> <b>techniques</b> for VEs using the gloves, including a menu system, a technique for text input, and a two-handed navigation technique. User studies have indicated the usability and utility of these techniques. ...|$|R
40|$|International audienceThis paper {{introduces}} a new device model {{and a new}} <b>interaction</b> <b>technique</b> model to deal with plasticity issues for Virtual Reality (VR) and Augmented Reality (AR). We aim to provide developers with solutions to use and create <b>interaction</b> <b>techniques</b> that will fit to the needed tasks of a 3 D application and to the input and output devices available. The device model {{introduces a}} new description of inputs and outputs devices that includes capabilities, limitations and representations in the real world. We also propose {{a new way to}} develop <b>interaction</b> <b>techniques</b> with an approach based on PAC and ARCH models. These techniques are implemented independently of the concrete devices used thanks to the proposed device model. Moreover, our approach aims to facilitate the portability of <b>interaction</b> <b>techniques</b> over different target OS and 3 D framework...|$|R
50|$|No {{cosmetic}} changes. Only two new features, {{a revised}} 18-inch wheel appearance and Siri <b>Eyes</b> <b>Free,</b> were added. Siren Red Tintcoat, Sable Metallic, and Blue Velvet Metallic became available as exterior colors, while Ebony/Saddle Up became available as an interior color option.|$|R
40|$|We {{present a}} study which {{describes}} the power consumption {{characteristics of a}} number of different <b>interaction</b> <b>techniques</b> on a desktop and laptop computer. In total, 8 interactions {{that can be used to}} carry out a single task (navigating a PDF document) were compared for power consumption across both a desktop and a laptop computer and across two different power saver settings. The results suggest that the power consumption of different <b>interaction</b> <b>techniques</b> for a single task vary significantly. Furthermore, the results suggest that a key factor in the power consumption of the <b>interaction</b> <b>technique</b> is the number of screen updates involved...|$|R
40|$|We present 3 dml, a markup {{language}} for 3 D <b>interaction</b> <b>techniques</b> and virtual environment applications that involve non-traditional devices. 3 dml has two main purposes: readability and rapid development. Designers can read 3 dml-based representations of 3 D <b>interaction</b> <b>techniques,</b> compare them, and understand them. 3 dml {{can also be}} used as a front end for any VR toolkit, so designers without programming skills can create VR applications as 3 dml documents that plug together <b>interaction</b> <b>techniques,</b> VR objects, and devices. This paper focuses on the language features and presentation scheme designed in our websit...|$|R
40|$|Mixed-reality {{games have}} the {{potential}} to let users play in the world surrounding them. However, to exploit this new approaches to game content creation, content presentation <b>techniques</b> and <b>interaction</b> <b>techniques</b> are required. In this paper we explore the potential of computer-vision on mobile devices with a camera as an interaction modality. Based on a theoretical review of the available design space potential <b>interaction</b> <b>techniques</b> are discussed. Some of these were implemented in an experimental game to enable practical evaluation. We provide an overview of the game and present intial experiences with the vision-based <b>interaction</b> <b>techniques</b> employed...|$|R
40|$|This paper {{presents}} {{a description of}} the <b>interaction</b> <b>techniques</b> used in the Chapel Hill Immersive Modeling Program (CHIMP). CHIMP is intended for the preliminary stages of architectural design. It is an immersive system; users work directly within a virtual world. The main goal has been to develop <b>interaction</b> <b>techniques</b> that exploit the benefits of working immersed while compensating for its limitations. <b>Interaction</b> <b>techniques</b> described and discussed in this paper include: • Action at a distance • Look-at menus • Remote controls (hand-held widgets) • Constrained object manipulation using twohands • Two-handed control panel interaction • Worlds in miniature • Interactive number...|$|R
