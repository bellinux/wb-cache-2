37|10000|Public
40|$|In {{this paper}} {{we examine the}} {{relationship}} of term co-occurrence analysis and a user interface for digital libraries. We describe a current working implementation of a <b>dynamic</b> <b>visual</b> <b>information</b> retrieval system based on co-cited author maps that assists in browsing and retrieving records from a large-scale database, 10 years of the Arts & Humanities Citation Index, in real time...|$|E
40|$|In the Morris water maze (MWM) task, {{proprioceptive}} {{information is}} likely to have a poor accuracy due to movement inertia. Hence, in this condition, <b>dynamic</b> <b>visual</b> <b>information</b> providing information on linear and angular acceleration would {{play a critical role in}} spatial navigation. To investigate this assumption we compared rat's spatial performance in the MWM and in the homing hole board (HB) tasks using a 1. 5 Hz stroboscopic illumination. In the MWM, rats trained in the stroboscopic condition needed more time than those trained in a continuous light condition to reach the hidden platform. They expressed also little accuracy during the probe trial. In the HB task, in contrast, place learning remained unaffected by the stroboscopic light condition. The deficit in the MWM was thus complete, affecting both escape latency and discrimination of the reinforced area, and was thus task specific. This dissociation confirms that <b>dynamic</b> <b>visual</b> <b>information</b> is crucial to spatial navigation in the MWM whereas spatial navigation on solid ground is mediated by a multisensory integration, and thus less dependent on visual information...|$|E
40|$|Abstract. We {{examine the}} {{relationship}} between term co-occurrence analysis and a user interface for digital libraries. We describe a current working implementation of a <b>dynamic</b> <b>visual</b> <b>information</b> retrieval system based on co-cited author maps that assists in browsing and retrieving records from a large-scale database, ten years of the Arts & Humanities Citation Index, in real time. Any figure in the arts or humanities, including scholars and critics, can be mapped, and the maps are live interfaces for retrieving co-citing documents. ...|$|E
40|$|We used {{eye-tracking}} {{to measure}} the <b>dynamic</b> patterns of <b>visual</b> <b>information</b> acquisition in twoplayers normal form games. Participants played one-shot games in which either, neither, or only oneof the players had a dominant strategy. First, we performed a mixture models cluster analysis to groupparticipants into types according to the pattern of <b>visual</b> <b>information</b> acquisition observed in a singleclass of games. Then, we predicted agents’ choices in different classes of games, and observed thatpatterns of <b>visual</b> <b>information</b> acquisition were game invariant. Our method allowed us to predictwhether the decision process would lead to equilibrium choices or not, and to attribute out-ofequilibriumresponses to limited cognitive capacities or social motives. Our results suggest theexistence of individually heterogeneous-but stable-patterns of <b>visual</b> <b>information</b> acquisition basedon subjective levels of strategic sophistication and social preferences. info:eu-repo/semantics/publishe...|$|R
40|$|AbstractWe used {{eye-tracking}} {{to measure}} the <b>dynamic</b> patterns of <b>visual</b> <b>information</b> acquisition in two-player normal-form games. Participants played one-shot games in which either, neither, or {{only one of the}} players had a dominant strategy. First, we performed a mixture models cluster analysis to group participants into types according to the pattern of <b>visual</b> <b>information</b> acquisition observed in a single class of games. Then, we predicted agents' choices in different classes of games and observed that patterns of <b>visual</b> <b>information</b> acquisition were game invariant. Our method allowed us to predict whether the decision process would lead to equilibrium choices or not, and to attribute out-of-equilibrium responses to limited cognitive capacities or social motives. Our results suggest the existence of individually heterogeneous-but-stable patterns of <b>visual</b> <b>information</b> acquisition based on subjective levels of strategic sophistication and social preferences...|$|R
40|$|Recent {{results suggest}} {{significant}} cross-correlation between the spike trains of the suprageniculate nucleus (SG) of the posterior thalamus and the caudate nucleus (CN) during visual stimulation. In {{the present study}} visually evoked local field potentials (LFPs) were recorded simultaneously in the CN and the SG in order to investigate the coupling between these structures at a population level. The effect of static and <b>dynamic</b> <b>visual</b> stimulation was analyzed in 55 SG-CN LFP pairs in the frequency range 5 - 57 Hz. Statistical analysis revealed significant correlation of the relative powers of each investigated frequency band (5 - 8 Hz, 8 - 12 Hz, 12 - 35 Hz and 35 - 57 Hz) during both static and <b>dynamic</b> <b>visual</b> stimulation. The temporal evolution of cross-correlation showed that {{in the majority of}} the cases the SG was activated first, and in approximately one third of the cases, the CN was activated earlier. These observations suggest a bidirectional information flow. The most interesting finding {{of this study is that}} different frequency bands exhibited significant cross-correlation in a stimulation paradigm-dependent manner. That is, static stimulation usually increased the cross-correlation of the higher frequency components (12 - 57 Hz) of the LFP, while dynamic stimulation induced changes in the lowest frequency band (5 - 8 Hz). This suggests a parallel processing of <b>dynamic</b> and static <b>visual</b> <b>information</b> in the SG and the CN. To our knowledge we are the first to provide evidence on the co-oscillation and synchronization of the CN and the SG at a population level upon visual stimulation, which suggests a significant cooperation between these structures in <b>visual</b> <b>information</b> processing...|$|R
40|$|Normally, {{people have}} {{difficulties}} recognizing objects from novel {{as compared to}} learned views, resulting in increased reaction times and errors. Recent studies showed, however, that this “view-dependency” can be reduced or even completely eliminated when novel views result from observer's movements instead of object movements. This observer movement benefit was previously attributed to extra-retinal (physical motion) cues. In two experiments, we demonstrate that <b>dynamic</b> <b>visual</b> <b>information</b> (that would normally accompany observer's movements) can provide a similar benefit and thus a potential alternative explanation. Participants performed sequential matching tasks for Shepard–Metzler-like objects presented via head-mounted display. As predicted by the literature, object recognition performance improved when view changes (45 ° or 90 °) resulted from active observer movements around the object instead of object movements. Unexpectedly, however, merely providing <b>dynamic</b> <b>visual</b> <b>information</b> depicting the viewpoint change showed an equal benefit, {{despite the lack of}} any extra-retinal/physical self-motion cues. Moreover, visually simulated rotations of the table and hidden target object (table movement condition) yielded similar performance benefits as simulated viewpoint changes (scene movement condition). These findings challenge the prevailing notion that extra-retinal (physical motion) cues are required for facilitating object recognition from novel viewpoints, and highlight the importance of dynamic visual cues, which have previously received little attention...|$|E
40|$|The {{literature}} on the modes of presentation of information show studies with non-conclusive results about the advantages of using dynamic images in the cognitive processing of certain musical parameters. This paper aims {{to know if there}} are effects of bimodal presentation of musical information–dynamic images associated to the sounds of instrumental parts) in the discrimination of musical texture. A quasi-experimental pre-post study was carried out with middle high school students (N= 39; 14 - 16 years old) studying from a high school from Valencia, Spain. The intervention sessions for the experimental group (GE; N= 18; 4 boys and 14 girls) included activities and materials for the perception and discrimination of musical texture by means of <b>dynamic</b> <b>visual</b> <b>information,</b> sound and speech. Control group (CG; n= 21; 14 boys and 7 girls) received the same content with sound and speech but without support of <b>dynamic</b> <b>visual</b> <b>information.</b> In order to control the effect of some covariates, there were controlled: musical aptitude, age, sex, selfperception of musical abilities and preferred modes of presentation of information. Data obtained do not show significant statistical intergroup differences. Although, there were significant statistical intragroup differences in favor of GE. This finding could be interpreted as secondary evidence that the teaching intervention based in multimodal presentation had a positive effect on the pupils’ ability of discriminating musical texture. Also, the absence of intergroup effects could be explained by the insufficient exposition time to the teaching intervention...|$|E
40|$|International audienceThe authors {{investigated}} {{whether the}} salience of <b>dynamic</b> <b>visual</b> <b>information</b> in a video-aiming task mediates the specificity of practice. Thirty participants practiced video-aiming movements in a full-vision, a weak-vision, or a target-only condition before being transferred to the target-only condition without knowledge of results. The full- and weak-vision conditions resulted in less endpoint bias and variability in acquisition than did the target-only condition. Going from acquisition to transfer resulted in a large increase in endpoint variability for the full-vision group {{but not for the}} weak-vision or target-only groups. Kinematic analysis revealed that weak dynamic visual cues do not mask the processing of other sources of afferent information; unlike strong visual cues, weak visual cues help individuals calibrate less salient sources of afferent information, such as proprioception...|$|E
40|$|State-of-the-art robotic {{explosive}} ordnance disposal robotics have not, in general, adopted {{recent advances in}} control technology and man-machine interfaces and lag many years behind academia. This paper describes the Haptics-based Immersive Telerobotic System project investigating an immersive telepresence envrionment incorporating advanced vehicle control systems, Augmented immersive sensory feedback, <b>dynamic</b> 3 D <b>visual</b> <b>information,</b> and haptic feedback for {{explosive ordnance}} disposal operators. The project aim is to provide operatiors a more sophisticated interface and expand sensory input to perform complex tasks to defeat improvised explosive devices successfully. The introduction of haptics and immersive teleprescence {{has the potential to}} shift the way teleprescence systems work for explosive ordnance disposal tasks or more widely for first responders scenarios involving remote unmanned ground vehicles. Peer reviewed: YesNRC publication: Ye...|$|R
40|$|Visual {{cryptography}} is a cryptographic technique {{which allows}} <b>visual</b> <b>information</b> to be encrypted {{in such a}} way that the decryption can be performed by the human visual system, without any cryptographic computation. <b>Dynamic</b> <b>visual</b> cryptography is an alternative image hiding method that is based not on the static superposition of shares, but on time-averaging geometric moiré. This method generates only one picture, and the secret image can be interpreted by human visual system only when the original encoded image is harmonically oscillated in a predefined direction at strictly defined amplitude of oscillation. Experimental implementations of <b>dynamic</b> <b>visual</b> cryptography require generation of harmonic oscillations. Unfortunately, a nonlinear system excited by harmonic oscillations could result into a chaotic response. Therefore, the concept of chaotic <b>dynamic</b> <b>visual</b> cryptography is an important problem both from the theoretical and practical points of view. The feasibility of chaotic <b>dynamic</b> <b>visual</b> cryptography is one of the main topics in this dissertation – theoretical relationships and computational experiments are derived and discussed in details. <b>Dynamic</b> <b>visual</b> cryptography scheme based on the deformations of the cover image and improved <b>dynamic</b> <b>visual</b> cryptography schemes with enhanced security are provided as well...|$|R
40|$|A cortical-like {{model for}} the {{binocular}} perception of motion-in-depth, based on phase-based measurements of dynamic disparity, is presented. The analysis evidenced that information hold in the interocular velocity dierence is the same of that derived by {{the evaluation of the}} total derivative of the binocular disparity. The resulting architecture resorts to spatiotemporal operators organized as motion energy detectors that can be directly implemented in analog VLSI lattice networks. Experimental simulations on stereo sequences validated the approach. 1 Introduction In many real-world visual application domains it is important to " <b>dynamic</b> 3 -D <b>visual</b> <b>information</b> from 2 -D images impinging the retinas. In particular, the perception of motion-in-depth (MID), i. e. the capability of discriminating between forward and backward movements of objects from an observer, has important implications for autonomous robot navigation and surveillance in dynamic environments. In general, the so [...] ...|$|R
40|$|Abstract. Face {{recognition}} {{is among the}} most challenging techniques for personal identity verification. Even though it is so natural for humans, there are still many hidden mechanisms which are still to be discovered. According to the most recent neurophysiological studies, the use of dynamic information is extremely important for humans in visual perception of biological forms and motion. Moreover, motion processing is also involved in the selection of the most informative areas of the face and consequently directing the attention. This paper provides an overview and some new insights on the use of <b>dynamic</b> <b>visual</b> <b>information</b> for face recognition, both for exploiting the temporal information and to define the most relevant areas to be analyzed on the face. In this context, both physical and behavioral features emerge in the face representation. ...|$|E
40|$|Abstract. As {{one of the}} {{techniques}} for robust speech recognition un-der noisy environment, audio-visual speech recognition using lip <b>dynamic</b> <b>visual</b> <b>information</b> together with audio information is attracting atten-tion and the research is advanced in recent years. Since visual informa-tion plays a great role in audio-visual speech recognition, what to select as the visual feature becomes a significant point. This paper proposes, for spoken word recognition, to utilize c combined parameter(combined parameter) as the visual feature extracted by Active Appearance Model applied to a face image including the lip area. Combined parameter con-tains information of the coordinate value and the intensity value as the visual feature. The recognition rate was improved by the proposed fea-ture compared to the conventional features such as DCT and the prin-cipal component score. Finally, we integrated the phoneme score from audio information and the viseme score from visual information with high accuracy. ...|$|E
40|$|Functional {{magnetic}} resonance imaging (fMRI) {{was used to examine}} the neural correlates of perceptual causality. Participants were imaged while viewing alternating blocks of causal events in which a ball collides with, and causes movement of another ball, versus non-causal events in which a spatial or a temporal gap precedes the movement of a second ball. There were significantly higher levels of relative activation in the right middle frontal gyrus and the right inferior parietal lobule for causal relative to non-causal events. Furthermore, when the differential effects of spatial and temporal incontiguities were subtracted from the contiguous stimuli, we observed both common (right prefrontal) and unique (right parietal and right temporal) regions of activation as a function of spatial and temporal processing of contiguity, respectively. Taken together, these data provide a means to help determine how the visual system extracts causality from <b>dynamic</b> <b>visual</b> <b>information</b> in the environment using spatial and temporal cues...|$|E
40|$|Recent {{findings}} suggest that the visuo-spatial sketchpad (VSSP) may be divided into two sub-components processing <b>dynamic</b> or static <b>visual</b> <b>information.</b> This model may be useful to elucidate the confusion of data concerning the functioning of the VSSP in schizophrenia. The present study examined patients with schizophrenia and matched controls in a new working memory paradigm involving dynamic (the Ball Flight Task - BFT) or static (the Static Pattern Task - SPT) visual stimuli. In the BFT, the responses of the patients were apparently based on the retention of the last set of segments of the perceived trajectory, whereas control subjects relied on a more global strategy. We assume that the patients' performances are the result of a reduced capacity in chunking <b>visual</b> <b>information</b> since they relied mainly on the retention of the last set of segments. This assumption is confirmed by the poor performance of the patients in the static task (SPT), which requires a combination of stimulus components into object representations. We assume that the static/dynamic distinction may help us to understand the VSSP deficits in schizophrenia. This distinction also raises questions about the hypothesis that visuo-spatial working memory can simply be dissociated into visual and spatial sub-components...|$|R
40|$|Introduction Industrial {{inspection}} robots, {{which are}} {{to move around}} {{a nuclear power plant}} and carry out inspection tasks, must follow a given route while observing static and dynamic features of the environment. The vision system for such a robot needs to execute efficiently and mutually the tasks that are necessary for carrying out inspection reliably and flexibly while safely moving, and the following attributes are desirable: (1) for inspection, high-resolution images, (2) for obstacle-avoidance during motion a very wide field of view, and (3) to enable reaction to <b>dynamic</b> situations, real-time <b>visual</b> <b>information</b> feedback. A practical vision system which can provide these capabilities with current technology is a stereo active vision system equipped with foveated wide-field image acquisition, as implemented in the ESCHeR active head shown in Fig. 1 (Kuniyoshi 1995). The key feature of ESCHeR is the special lenses that project Figure: 1 ESCHeR: ETL Stereo Compact Head for...|$|R
40|$|When {{observers}} perceive several {{objects in}} a space, {{at the same}} time, they should effectively perceive their own position as a viewpoint. However, {{little is known about}} observers' percepts of their own spatial location based on the <b>visual</b> scene <b>information</b> viewed from them. Previous studies indicate that two distinct visual spatial processes exist in the locomotion situation: the egocentric position perception and egocentric direction perception. Those studies examined such perceptions in <b>information</b> rich <b>visual</b> environments where much <b>dynamic</b> and static <b>visual</b> <b>information</b> was available. This study examined these two perceptions in information of impoverished environments, including only static lane edge information (i. e., limited information). We investigated the visual factors associated with static lane edge information that may affect these perceptions. Especially, we examined the effects of the two factors on egocentric direction and position perceptions. One is the "uprightness factor" that "far" <b>visual</b> <b>information</b> is seen at upper location than "near" <b>visual</b> <b>information.</b> The other is the "central vision factor" that observers usually look at "far" <b>visual</b> <b>information</b> using central vision (i. e., foveal vision) whereas 'near' <b>visual</b> <b>information</b> using peripheral vision. Experiment 1 examined the effect of the "uprightness factor" using normal and inverted road images. Experiment 2 examined the effect of the "central vision factor" using normal and transposed road images where the upper half of the normal image was presented under the lower half. Experiment 3 aimed to replicate the results of Experiments 1 and 2. Results showed that egocentric direction perception is interfered with image inversion or image transposition, whereas egocentric position perception is robust against these image transformations. That is, both "uprightness" and "central vision" factors are important for egocentric direction perception, but not for egocentric position perception. Therefore, the two visual spatial perceptions about observers' own viewpoints are fundamentally dissociable...|$|R
40|$|<b>Dynamic</b> <b>visual</b> <b>information</b> {{from the}} lip {{movement}} can significantly improve the accuracy and robustness of an {{automatic speech recognition}} system in a noisy environment. Useful geometric information about lip movement, such as the temporal variation of mouth width and height, can be obtained easily from a segmented lip. However, there is a difficulty in lip segmentation due to the weak color contrast between lip and face regions. As the result, successful segmentation of lip images cannot be obtained. Spatial Fuzzy Clustering algorithm (SFCM) adapts conventional FCM. This algorithm is able {{to take into account}} both the distributions of data in feature space and the spatial interactions between neighboring pixels during clustering. In SFCM color components are not treated independently. By appropriate preprocessing and postprocessing utilizing the color and shape properties of lip region, successful segmentation of lip images can be reached. Experiment of SFCM is done by using COLORFERET database. Experiment using different size of neighborhood shows average accuracy 90. 9 %...|$|E
40|$|This paper {{describes}} {{a method for}} robotic manipulation that uses direct image-space calculation of optical flow information for continuous real-time control of manipulative actions. State variables derived from optical flow measurements are described. The resulting approach is advantageous since it robustifies the system to changes in optical parameters and also simplifies the implementation needed {{to succeed in the}} task execution. Two reference tasks and their corresponding experiments are described: the insertion of a pen into a "cap" (the capping experiment) and the rotational point-contact pushing of an object of unknown shape, mass and friction to a specified goal point in the image-space. I. INTRODUCTION The visual system of an agent, either natural or artificial, has to cope with motion in at least two ways: it should be able to detect, measure and interpret the motion of external objects, and it must be able to use <b>dynamic</b> <b>visual</b> <b>information</b> to control, plan and coordinate its [...] ...|$|E
40|$|Motion {{processing}} {{represents a}} perceptual domain in which <b>dynamic</b> <b>visual</b> <b>information</b> is encoded {{to support the}} perception of movement. Research {{over the last decade}} has found a variety of abnormalities in the processing of motion information in schizophrenia. The abnormalities span from discrimination of basic motion features (such as speed) to integration of spatially distributed motion signals (such as coherent motion). Motion processing involves visual signals across space and time and thus presents a special opportunity to examine how spatial and temporal information is integrated in the visual system. This article surveys the behavioral and neuroimaging studies that probe into the spatial integration of motion information in schizophrenia. An emerging theme from these studies points to an imbalanced regulation of spatial interaction processes as a potential mechanism mediating different levels of abnormal motion processing in schizophrenia. The synthesis of these mechanism-driven studies suggests that further investigation of the neural basis and functional consequences of this abnormal motion processing are needed in order to render a basic biomarker for assessment and intervention of cognitive dysfunction in this mental disorder...|$|E
40|$|This study {{examined}} effects of hand movement on visual perception of 3 -D movement. I used an apparatus {{in which a}} cursor position in a simulated 3 -D space and {{the position of a}} stylus on a haptic device could coincide using a mirror. In three experiments, participants touched the center of a rectangle in the visual display with the stylus of the force-feedback device. Then the rectangle’s surface stereoscopically either protruded toward a participant or indented away from the participant. Simultaneously, the stylus either pushed back participant’s hand, pulled away, or remained static. <b>Visual</b> and haptic <b>information</b> were independently manipulated. Participants judged whether the rectangle visually protruded or dented. Results showed that when the hand was pulled away, subjects were biased to perceive rectangles indented; however, when the hand was pushed back, no effect of haptic information was observed (Experiment 1). This effect persisted even when the cursor position was spatially separated from the hand position (Experiment 2). But, when participants touched an object different from the visual stimulus, this effect disappeared (Experiment 3). These results suggest that the visual system tried to integrate the <b>dynamic</b> <b>visual</b> and haptic <b>information</b> when they coincided cognitively, and the effect of haptic information on visually perceived depth was direction-dependent...|$|R
40|$|It is {{expected}} nowadays that robots {{are able to}} work in real-life environments, possibly also sharing the same space with humans. These environments are generally considered as being cluttered and hard to train for. The work presented in this thesis focuses on developing an online and real-time biologically inspired model for teams of robots to collectively learn and memorise their visual environment in a very concise and compact manner, whilst sharing their experience to their peers (robots and possibly also humans). This work forms {{part of a larger}} project to develop a multi-robot platform capable of performing security patrol checks whilst also assisting people with physical and cognitive impairments to be used in public places such as museums and airports. The main contribution of this thesis is the development of a model which makes robots capable of handling <b>visual</b> <b>information,</b> retain information that is relevant to whatever task is at hand and eliminate superfluous information, trying to mimic human performance. This leads towards the great milestone of having a fully autonomous team of robots capable of collectively surveying, learning and sharing salient <b>visual</b> <b>information</b> of the environment even without any prior information. Solutions to endow a distributed team of robots with object detection and environment understanding capabilities are also provided. The way in which humans process, interpret and store <b>visual</b> <b>information</b> are studied and their visual processes are emulated by a team of robots. In an ideal scenario, robots are deployed in a totally unknown environment and incrementally learn and adapt to operate within that environment. Each robot is an expert of its area however, they possess enough knowledge about other areas to be able to guide users sufficiently till another more knowledgeable robot takes over. Although not limited, it is assumed that, once deployed, each robot operates in its own environment for most of its lifetime and the longer the robots remains in the area the more refined their memory will become. Robots should to be able to automatically recognize previously learnt features, such as faces and known objects, whilst also learning other new information. Salient information extracted from the incoming video streams can be used to select keyframes to be fed into a visual memory thus allowing the robot to learn new interesting areas within its environment. The cooperating robots are to successfully operate within their environment, automatically gather <b>visual</b> <b>information</b> and store it in a compact yet meaningful representation. The storage has to be <b>dynamic,</b> as <b>visual</b> <b>information</b> extracted by the robot team might change. Due to the initial lack of knowledge, small sets of visual memory classes need to evolve as the robots acquire <b>visual</b> <b>information.</b> Keeping memory size within limits whilst at the same time maximising the information content {{is one of the main}} factors to consider...|$|R
40|$|Especially in noisy environments like in human-robot interaction, <b>visual</b> <b>information</b> {{provides}} a strong cue facilitating a robust understanding of speech. In this paper, {{we consider the}} <b>dynamic</b> <b>visual</b> context of actions perceived by a camera. Based on an annotated multi-modal corpus of people who verbally explain tasks while they perform them, we present an automatic strategy for learning action-specific language models. The approach explicitly deals with the asynchrony of actions and verbal descriptions and includes an automatic parameter optimization based on a perplexity measure. Results show that a significant improvement of the word accuracy can be achieved using a dynamic switching of action-specific language models. ...|$|R
40|$|Conscious visual {{perception}} of the constantly changing environment {{is one of the}} brain’s most critical functions. In virtually every moment of every daily activity, the visual system is confronted with the task of accurately representing and interpreting scenes that change rapidly over time. Adults can judge the identity and order of changing images presented at a rate of up to 10 Hz (~ 50 ms per image); this limit reflects a finite temporal resolution of attention. In the research reported here, although 6 - to 15 -month-old infants could detect the presence of rapid flicker without difficulty, their ability to segment individual alternating states within the flicker was severely limited: Fifteen-month-old infants had a temporal resolution of attention approximately one order of magnitude lower than that of adults (~ 1 Hz). Coarse temporal resolution constrains how infants perceive and utilize <b>dynamic</b> <b>visual</b> <b>information</b> and {{may play a role in}} the visual processing deficits found in individuals with neurodevelopmental disorders. Keywords temporal individuation; Gestalt flicker fusion; contrast sensitivity Given the highly dynamic nature of the visual world, the ability to derive a temporall...|$|E
40|$|Figure 1. (a) The Mnemonic Desktop with pixel {{persistence}} and pixel flashback techniques: hidden parts of windows are being revealed showing motion trails and a dusty appearance that will fade out as changes are being replayed. (b) The Mnemonic Wall: visual changes outside the user’s central vision leave motion trails that will exhibit similar behavior as the user glances at them. Managing {{large amounts of}} <b>dynamic</b> <b>visual</b> <b>information</b> involves understanding changes happening out of the user’s sight. In this paper, we show how current software does not adequately support users in this task, and motivate {{the need for a}} more general approach. We propose an image-based storage, visualization, and implicit interaction paradigm called mnemonic rendering that provides better support for handling visual changes. Once implemented on a system, mnemonic rendering techniques can benefit all applications. We explore its rich design space and discuss its expected benefits as well as limitations based on feedback from users of a small-screen and a wall-size prototype. ACM Classification: H 5. 2 [Information interfaces an...|$|E
40|$|Abstract — As {{confirmed}} by recent neurophysiological studies, {{the use of}} dynamic information is extremely important for humans in visual perception of biological forms and motion. Apart from the mere computation of the visual motion of the viewed objects, the motion itself conveys far more information, which helps understanding the scene. This paper provides an overview and some new insights {{on the use of}} <b>dynamic</b> <b>visual</b> <b>information</b> for face recognition. In this context, not only physical features emerge in the face representation, but also behavioral features should be accounted. While physical features are obtained from the subject’s face appearance, behavioral features are obtained from the individual motion and articulation of the face. In order to capture both the face appearance and the face dynamics, a dynamical face model based on a combination of Hidden Markov Models is presented. The number of states (or facial expressions) are automatically determined from the data by unsupervised clustering of expressions of faces in the video. The underlying architecture closely recalls the neural patterns activated in the perception of moving faces. Preliminary results on real video image data show the feasibility of the proposed approach. Keywords—Face recognition, Biometrics, Human visual system, Human perceptio...|$|E
40|$|Bio-inspired {{asynchronous}} event-based vision sensors {{are currently}} introducing {{a paradigm shift}} in <b>visual</b> <b>information</b> processing. These new sensors rely on a stimulus-driven principle of light acquisition similar to biological retinas. They are event-driven and fully asynchronous, thereby reducing redundancy and encoding exact times of input signal changes, leading to a very precise temporal resolution. Approaches for higher-level computer vision often rely on the realiable detection of features in visual frames, but similar definitions of features for the novel <b>dynamic</b> and event-based <b>visual</b> input representation of silicon retinas have so far been lacking. This article addresses the problem of learning and recognizing features for event-based vision sensors, which capture properties of truly spatiotemporal volumes of sparse <b>visual</b> event <b>information.</b> A novel computational architecture for learning and encoding spatiotemporal features is introduced based {{on a set of}} predictive recurrent reservoir networks, competing via winner-take-all selection. Features are learned in an unsupervised manner from real-world input recorded with event-based vision sensors. It is shown that the networks in the architecture learn distinct and task-specific <b>dynamic</b> <b>visual</b> features, and can predict their trajectories over time...|$|R
40|$|This paper {{deals with}} a {{stabilizing}} receding horizon control for the three dimensional(3 D) <b>dynamic</b> <b>visual</b> feedback system with fixed camera configuration. Firstly, the brief summary of the <b>dynamic</b> <b>visual</b> feedback system with fixed camera configuration is given. Next, we discuss that the energy function of the the <b>dynamic</b> <b>visual</b> feedback system is a control Lyapunov function. Then, the stabilizing receding horizon control for the <b>dynamic</b> <b>visual</b> feedback system using a control Lyapunov function is proposed. The use of the terminal cost derived from a control Lyapunov function of the <b>dynamic</b> <b>visual</b> feedback system, which is obtained from the inverse optimal control approach, is a key point. ...|$|R
40|$|Catching a ball {{involves}} a <b>dynamic</b> transformation of <b>visual</b> <b>information</b> about ball motion into motor commands for moving the {{hand to the}} right place at the right time. We previously formulated a neural model for this transformation to account for the consistent leftward movement biases observed in our catching experiments. According to the model, these biases arise within the representation of target motion as well as within the transformation from a gaze-centered to a body-centered movement command. Here, we examine the validity of the latter aspect of our model in a catching task involving gaze fixation. Gaze fixation should systematically influence biases in catching movements, because in the model movement commands are only generated in the direction perpendicular to the gaze direction. Twelve participants caught balls while gazing at a fixation point positioned either straight ahead or 14 ° to the right. Four participants were excluded because they could not adequately maintain fixation. We again observed a consistent leftward movement bias, but the catching movements were unaffected by fixation direction. This result refutes our proposal that the leftward bias partly arises within the visuomotor transformation, and suggests instead that the bias predominantly arises within the early representation of target motion, specifically through an imbalance in the represented radial and azimuthal target motion. © 2009 Springer-Verlag...|$|R
40|$|When {{moving through}} our environment, {{the human brain}} must {{integrate}} information from our muscles and joints (proprioception), the acceleration detectors in our inner ear (vestibular cues) and <b>dynamic</b> <b>visual</b> <b>information</b> (optic flow). While past {{research has focused on}} understanding how each of these modalities can be used to perceive different aspects of self-motion independently, very little is understood about how these cues are integrated and the relative influences of each when they are combined. In recent years Virtual Reality (VR) technology and sophisticated self-motion simulators have begun to provide researchers with the opportunity to provide natural, yet tightly controlled stimulus conditions, while also maintaining the capacity to create unique experimental scenarios that could not occur in the real world. The impact of these technologies has been particularly evident in the context of multisensory self-motion perception and spatial navigation. This chapter begins with {{a brief description of the}} various simulation tools and techniques that are being used to study self-motion perception. Subsequently, human behavioral work investigating multisensory self-motion perception using these technologies will be summarized, focusing mainly on visual, proprioceptive and vestibular influences during full...|$|E
40|$|As {{confirmed}} by recent neurophysiological studies, {{the use of}} dynamic information is extremely important for humans in visual perception of biological forms and motion. Apart from the mere computation of the visual motion of the viewed objects, the motion itself conveys far more information, which helps understanding the scene. This paper provides an overview and some new insights {{on the use of}} <b>dynamic</b> <b>visual</b> <b>information</b> for face recognition. In this context, not only physical features emerge in the face representation, but also behavioral features should be accounted. While physical features are obtained from the subject’s face appearance, behavioral features are obtained from the individual motion and articulation of the face. In order to capture both the face appearance and the face dynamics, a dynamical face model based on a combination of Hidden Markov Models is presented. The number of states (or facial expressions) are automatically determined from the data by unsupervised clustering of expressions of faces in the video. The underlying architecture closely recalls the neural patterns activated in the perception of moving faces. Experimental results obtained from real video image data show the feasibility of the proposed approach...|$|E
40|$|When {{walking through}} space, both, <b>dynamic</b> <b>visual</b> <b>{{information}}</b> (i. e. optic flow), and body-based information (i. e., proprioceptive/efference copy and vestibular) jointly specify {{the magnitude of}} a distance travelled. While recent evidence has demonstrated {{the extent to which}} each of these cues can be used independently, relatively little is known about how they are integrated when simultaneously present. In this series of experiments, participants first travelled along a predefined distance and subsequently matched this distance by adjusting an egocentric, in-depth target. Visual information was presented via a head-mounted display and consisted of a long, richly textured, virtual hallway. Body-based cues were provided either by walking in a fully-tracked, free-walking space or by walking on a large, linear treadmill. Travelled distances were provided either through optic flow alone, body-based cues alone (i. e. blindfolded walking), or through both cues combined. In the combined condition, visually-specified distances were either congruent (1. 0 x) or incongruent (0. 7 x or 1. 4 x) with distances specified by body-based cues. The incongruencies were introduced either by changing the visual gain during natural walking or the proprioceptive gain during treadmill walking. Responses reflect a combined effect of both visual and body-based information, with an overall higher influence of body-based cues...|$|E
40|$|In {{this paper}} we discuss {{expressive}} visualization techniques that smartly uncover the most important information {{in order to maximize}} the <b>visual</b> <b>information</b> in the resulting images. This is achieved through <b>dynamic</b> changes in <b>visual</b> representations, through deformations, or through spatial modifications of parts of the data. Such techniques originate from technical illustration and are called cut-away views, ghosted views, and exploded views. These illustrative techniques unveil the most important <b>visual</b> <b>information</b> by employing high levels of abstraction. The change in visual representation or spatial position is done easily perceivable and the overall visual harmony is preserved...|$|R
40|$|Motion detection/estimation plays {{a crucial}} role in <b>dynamic</b> <b>visual</b> {{tracking}}. Whether a <b>dynamic</b> <b>visual</b> tracking system can successfully track a moving target closely depends on the quality of motion detection/estimation results. In <b>dynamic</b> <b>visual</b> tracking, the camera used to capture images is not stationary, so any slight vibration of the camera motion or the target motion can lead to a blurry image causing the visual tracking performance to be deteriorated. To cope with this difficulty, a motion detection/estimation approach consisting of a region-based spatial distribution of Gaussians (SDG) -like matching algorithm, a template-update-with-memory algorithm, and a template mask is developed in this study. Moreover, linear interpolation on vision commands is performed to improve the tracking performance. A <b>dynamic</b> <b>visual</b> tracking system designed for locking the target’s image {{in the center of the}} image plane is used as the test platform. Experimental results demonstrate the effectiveness of the proposed approach. Keywords: <b>dynamic</b> <b>visual</b> tracking, region-based matching, spatial distribution of Gaussians, motion detection/estimation, template mask 1...|$|R
40|$|Constructive Visualization: A token-based {{paradigm}} allowing {{to assemble}} <b>dynamic</b> <b>visual</b> representation for non-experts Samuel Huron To cite this version: Samuel Huron. Constructive Visualization: A token-based paradigm allowing to assemble <b>dynamic</b> <b>visual</b> representation for non-experts. Other [cs. OH]. Universite ́ Paris Sud- Paris XI, 2014. English. . HAL Id: tel- 0112689...|$|R
