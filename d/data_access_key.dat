0|10000|Public
40|$|<b>Data</b> <b>access</b> is <b>key</b> {{to science}} driven by {{distributed}} high-throughput computing (DHTC), an essential technology for many major research {{projects such as}} High Energy Physics (HEP) experiments. However, achieving efficient <b>data</b> <b>access</b> becomes quite difficult when many independent storage sites are involved because users are burdened with learning the intricacies of accessing each system and keeping careful track of data location. We present an alternate approach: the Any Data, Any Time, Anywhere infrastructure. Combining several existing software products, AAA presents a global, unified view of storage systems - a "data federation," a global filesystem for software delivery, and a workflow management system. We present how one HEP experiment, the Compact Muon Solenoid (CMS), is utilizing the AAA infrastructure and some simple performance metrics. Comment: 9 pages, 6 figures, submitted to 2 nd IEEE/ACM International Symposium on Big Data Computing (BDC) 201...|$|R
40|$|Performance, {{reliability}} and scalability in <b>data</b> <b>access</b> are <b>key</b> {{issues in the}} context of HEP data processing and analysis applications. In this paper we present the results of a large scale performance measurement performed at the INFN-CNAF Tier- 1, employing some storage solutions presently available for HEP computing, namely CASTOR, GPFS, Scalla/Xrootd and dCache. The storage infrastructure was based on Fibre Channel systems organized in a Storage Area Network, providing 260 TB of total disk space, and 24 disk servers connected to the computing farm (280 worker nodes) via Gigabit LAN. We also describe the deployment of a StoRM SRM instance at CNAF, configured to manage a GPFS file system, presenting and discussing its performances...|$|R
40|$|In {{this paper}} we explain our {{strategy}} for parallelizing a wavelet based compression routine. The compression routine {{is designed to}} work within a earthquake simulation model where huge amount of data is written and read from disks. As the compression routine makes up for {{a significant part of}} the computation it is very important that it parallelize well. We have implemented the parallel version in OpenMP. This makes the implementation easy, but it hides a fundamental problem. The optimal data layout on a distributed memory system changes for the di#erent parts of the algorithm. Thus either data has to be redistributed or the processors have to do remote <b>data</b> <b>access.</b> Our experiments show that the remote <b>data</b> <b>access</b> work pretty well on the Origin system thus for the number of CPU less or equal to 32, no significant slow down, due to remote <b>data</b> <b>access</b> is seen. <b>Key</b> words. parallel computing, data compression, wavelet transform, OpenMP AMS subject classifications. 68 P 30, 65 Y 05 1. Int [...] ...|$|R
40|$|Large-scale shared-memory multiprocessors {{typically}} have long latencies for remote <b>data</b> <b>accesses.</b> A <b>key</b> issue for execution performance of many common applications is the synchronization cost. The communication scalability of synchronization has been improved by {{the introduction of}} queue-based spin-locks instead of Test & (Test & Set). For architectures with long access latencies for global data, attention should also be paid to the number of global accesses that are involved in synchronization. We present a method to characterize the performance of proposed queue lock algorithms, and apply it to previously published algorithms. We also present two new queue locks, the LH lock and the M lock. We compare the locks in terms of performance, memory requirements, code size, and required hardware support. The LH lock is the simplest of all the locks, yet requires only an atomic swap operation. The M lock is superior in terms of global accesses needed to perform synchronization and still competitive in all other criteria. We conclude that the M lock is the best overall queue lock for the class of architectures studied. ...|$|R
40|$|Abstract- In {{automated}} {{energy management}} systems, to make instantaneous {{decisions based on}} the appliance status information, continuous <b>data</b> <b>access</b> is a <b>key</b> requirement. With the advances in sensor and communication technologies, {{it is now possible}} to remotely monitor the power consumption data. However, before an appliance is actively monitored, it must be identified using the obtained power consumption data. Appropriate methods are required to analyse power consumption patterns for proper appliance recognition. The focus of this work is to provide the model structure for storing and distinguishing the recurring footprints of the household appliances. Hidden Markov model based method is proposed to recognize the individual appliances from combined load. It is found that the proposed method can efficiently differentiate the power consumption patterns of appliances from their combined profiles. I...|$|R
40|$|Data {{storage and}} <b>data</b> <b>access</b> {{represent}} the <b>key</b> of CPU-intensive and data-intensive high performance Grid computing. Hadoop is an open-source data processing framework that includes fault-tolerant and scalable {{distributed data processing}} model and execution environment, named MapReduce, and distributed File System, named Hadoop distributed File System (HDFS). HDFS was deployed and tested within the Open Science Grid (OSG) middleware stack. Efforts {{have been taken to}} integrate HDFS with gLite middleware. We have tested the File System thoroughly in order to understand its scalability and fault-tolerance while dealing with small/medium site environment constraints. To benefit entirely from this File System, we made it working in conjunction with Hadoop Job scheduler to optimize the executions of the local physics analysis workflows. The performance of the analysis jobs which used such architecture seems to be promising, making it useful to follow up in the future...|$|R
50|$|Gabriel Vaughn (Josh Holloway) is a {{high-tech}} intelligence operative enhanced with a super-computer microchip in his brain. With this implant, Gabriel {{is the first}} human ever to be connected directly into the globalized information grid. He can get into any of its <b>data</b> centers and <b>access</b> <b>key</b> intel files in the fight to protect the United States from its enemies. Lillian Strand (Marg Helgenberger), {{the director of the}} United States Cyber Command who supports Gabriel and oversees the unit's missions, assigns Riley Neal (Meghan Ory), a Secret Service agent, to protect Gabriel from outside threats, as well as from his appetite for reckless, unpredictable behaviors and disregard for protocols. Meanwhile, Gabriel takes advantage of his chip to search for his wife who disappeared years ago after being sent by the C.I.A. to infiltrate and prevent the Lashkar-e-Taiba from carrying out a terrorist attack in Mumbai, India.|$|R
40|$|Abstract Caches {{are widely}} used to reduce the speed gap between {{processors}} and memories. However, the spatial locality of sequential <b>data</b> <b>accesses</b> existing in many popular applications is not well exploited by conventional data cache. In response to these problems, the Split Sequential Data Cache (SSDC) is proposed, in which the sequential access detector can predict whether <b>data</b> <b>accesses</b> are sequential, and direct them to the right sub cache. Experiments show that the SSDC outperforms the conventional data cache and other schemes. It reduces the miss rate of applications with intensive sequential <b>data</b> <b>accesses</b> with only a little increment of bandwidth requirement. Meanwhile, the experimental results on SPEC 2000 Int show that SSDC does not hurt the performance of applications without large sequential <b>accesses.</b> <b>Key</b> words 　computer architecture; cache design; split data cache; sequential data acces...|$|R
40|$|The {{nature of}} dark matter, dark energy and {{large-scale}} gravity pose {{some of the}} most pressing questions in cosmology today. These fundamental questions require highly precise measurements, and a number of wide-field spectroscopic survey instruments are being designed to meet this requirement. A key component in these experiments is the development of a simulation tool to forecast science performance, define requirement flow-downs, optimize implementation, demonstrate feasibility, and prepare for exploitation. We present SPOKES (SPectrOscopic KEn Simulation), an end-to-end simulation facility for spectroscopic cosmological surveys designed to address this challenge. SPOKES is based on an integrated infrastructure, modular function organization, coherent data handling and fast <b>data</b> <b>access.</b> These <b>key</b> features allow reproducibility of pipeline runs, enable ease of use and provide flexibility to update functions within the pipeline. The cyclic nature of the pipeline offers the possibility to make the science output an efficient measure for design optimization and feasibility testing. We present the architecture, first science, and computational performance results of the simulation pipeline. The framework is general, but for the benchmark tests, we use the Dark Energy Spectrometer (DESpec), one of the early concepts for the upcoming project, the Dark Energy Spectroscopic Instrument (DESI). We discuss how the SPOKES framework enables a rigorous process to optimize and exploit spectroscopic survey experiments in order to derive high-precision cosmological measurements optimally. Comment: 24 pages, 8 figures, 2 table...|$|R
40|$|Cloud Computing {{is the sum}} of SaaS and Utility Computing. This {{paradigm}} {{also brings}} forth many new challenges for <b>data</b> security and <b>access</b> control mechanisms, when users outsource sensitive data for sharing on Cloud systems, which are not within the same trusted domain as data owners. Storing data on untrusted storage makes secure data sharing a challenge issue. To keep sensitive user data confidential against untrusted Cloud systems, on one hand, <b>data</b> <b>access</b> policies should be enforced on these storage servers; on the other hand, confidentiality of sensitive data should be well protected against them. The existing solutions usually apply cryptographic methods by disclosing data decryption keys only to authorized users. However, in doing so, these solutions inevitably introduce a heavy computation overhead on the data owner for key distribution and data management when fine-grained <b>data</b> <b>access</b> control is desired, and thus do not scale well. The main challenges for cryptographic methods include simultaneously achieving system scalability and fine-grained <b>data</b> <b>access</b> control, efficient <b>key</b> or user management, user accountability, data security, computational overhead and etc. To address these challenge issues, in this paper we defined and enforcing access policies based on data attributes and enabling the data owner to delegate most computation-intensive tasks pertained to user revocation to untrusted servers without disclosing data content to them. We achieve this goal by exploiting and uniquely combining techniques of Ciphertext policy attribute based encryption system and proxy re-encryption and re-encryption. Our proposed scheme also has salient features of user access privilege confidentiality and user secret key accountability...|$|R
40|$|A formal {{specification}} {{of a security}} protocol cannot be limited to listing the messages exchanged. In MSR, each construct is associated with typing and <b>data</b> <b>access</b> specification (DAS) rules, which describe under which circumstances a principal can <b>access</b> <b>keys</b> and other information. A protocol specification is completed {{with a description of}} the intruder in the style of Dolev and Yao, the wolf in the protocol world. In this paper, we show that the protocol determines the intruder: the wolf is deep within. More precisely we show that the Dolev-Yao intruder rules can be automatically reconstructed from the DAS rules, and that the DAS rules can themselves be inferred from annotated typing declarations for the various message constructors. ...|$|R
50|$|Amazon SQS {{provides}} authentication {{procedures to}} allow for secure handling of data. Amazon uses its Amazon Web Services (AWS) identification to do this, requiring users to have an AWS enabled account with Amazon.com; this can be created at http://aws.amazon.com. AWS assigns a pair of related identifiers, your AWS <b>access</b> <b>keys,</b> to an AWS enabled account to perform identification. The first identifier is a public 20-character <b>Access</b> <b>Key.</b> This key is included in an AWS service request to identify the user. If the user is not using SOAP (protocol) with WS-Security, a digital signature is calculated using the Secret <b>Access</b> <b>Key.</b> The Secret <b>Access</b> <b>Key</b> is a 40-character private identifier. AWS uses the <b>Access</b> <b>Key</b> ID provided in a service request to look up an account's Secret <b>Access</b> <b>Key.</b> Amazon.com then calculates a digital signature with the key. If they match then the user is considered authentic, if not then the authentication fails and the request is not processed.|$|R
40|$|Computer Aided Identification (CAI) systems provide {{users with}} the {{resources}} to relate morpho-anatomic observations with names of taxa, and to subsequently access other knowledge about the organisms. Xper² version 2. 1 {{is one of the}} most user-friendly software in its category. It provides a complete environment dedicated to taxonomic descriptions management. While assisting taxonomists with knowledge acquisition for identification keys, it also helps with the publication of descriptive data by providing a large panel of tools, including analysis facilities (comparison of taxa or descriptors used), full traceability of information and knowledge (by adding references or external links). Xper² provides excellent support for automatic online publication of descriptive <b>data</b> and free <b>access</b> <b>keys,</b> as well as for exporting of datasets for phylogenetic and systematic research. It focuses on interoperability between systems and can import and export into structured descriptive data format (TDWG-SDD), and export to HTML and Nexus formats. Written in Java, it is available on Windows™ Mac™ or Linux in French, English, and Spanish, and a new Chinese version can also be downloaded. With its intuitive interface, Xper² is aimed at professional taxonomists as well as naturalists who merely want to identify specimens using a readymade application. Xper² is free of charge. It can be downloaded at: [URL]...|$|R
40|$|Abstract — This article {{emphasizes}} a {{new layer}} to information rights management {{as it applies}} to security, privacy and confidentiality of field level data elements. In the interconnected economy, consumers and corporations develop an electronic relationship where commercial and online applications provide convenient <b>access</b> to account <b>data.</b> As consumer use of digital information systems become widespread, the greater the need to protect their proprietary information with more secure authentication protocols. Human Digitization (“HD”) involves creating a customer profile {{at the onset of the}} data collection and encoding each data element with an <b>access</b> <b>key.</b> The use of authentication profiles in digital information rights management systems mitigates unauthorized <b>data</b> <b>access</b> and provides protection on three levels: 1) internal-employee access, 2) external-customer access, and 3) computer-program access...|$|R
40|$|To {{facilitate}} {{the identification of}} specimens, biodiversity informatics has developed numerous new computer-aided tools which compete with the old, printed single <b>access</b> <b>keys.</b> Free <b>access</b> <b>keys</b> are accessible for many different taxonomic groups, but key-generating software are also helpful to construct single <b>access</b> <b>keys.</b> This paper presents a midfield solution to create customized decision trees (single <b>access</b> <b>keys)</b> through an interactive webbased interface. This solution offers an online service to create keys according to the parameters and context chosen by the final users themselves. It is also useful for the administrator of this online system, because of low-maintenance needs, limited to configuration files when new knowledge bases are added on the server side. Presently, the software is available with a French interface at the following URL: [URL]...|$|R
40|$|Abstract. According to {{different}} node <b>data</b> <b>access</b> objects, {{and based on}} the idea of behavior-driven development, an interface-oriented node <b>data</b> <b>access</b> tool is designed and implemented. The tool mainly implements an executable node <b>data</b> <b>access</b> tool that is based on MFC, and writes a set of Win 32 console application, which improves the node <b>data</b> automation <b>access</b> scripts and encapsulates the module interfaces, thus improving the distributed network <b>data</b> <b>access</b> speed...|$|R
5000|$|... <b>access</b> <b>key</b> {{pieces of}} {{evidence}} such as records and documents; and ...|$|R
5000|$|Exception {{handling}} - translating <b>data</b> <b>access</b> related {{exception to}} a Spring <b>data</b> <b>access</b> hierarchy ...|$|R
50|$|Together with Spring's {{transaction}} management, its <b>data</b> <b>access</b> framework {{offers a}} flexible abstraction {{for working with}} <b>data</b> <b>access</b> frameworks. The Spring Framework doesn't offer a common <b>data</b> <b>access</b> API; instead, the full power of the supported APIs is kept intact. The Spring Framework is the only framework available in Java that offers managed <b>data</b> <b>access</b> environments outside of an application server or container.|$|R
40|$|Wireless Application Protocol (WAP) was {{developed}} to support efficient wireless <b>data</b> <b>access.</b> Since <b>data</b> transmission capacity of wireless link is limited, a caching model was proposed for WAP to reduce the <b>data</b> <b>access</b> time. Since many WAP applications exhibit temporal locality for <b>data</b> <b>access,</b> the cache in a WAP handset can effectively reduce the <b>data</b> <b>access</b> time by exploiting this property. This paper investigates the cache performance for WAP handsets. We consider the least recently used replacement policy and two strongly consistent <b>data</b> <b>access</b> algorithms called poll-each-read and callback. An analytic model is proposed to derive the effective hit ratio of <b>data</b> <b>access.</b> Our study indicates that callback may significantly outperform poll-each-read. We also report how the <b>data</b> <b>access</b> rate and the data update distribution affect the cache performance in a WAP handset. Keywords: cache, strong consistency, temporal locality, Wireless Application Protocol (WAP), wireless data 1 Introduct [...] ...|$|R
5000|$|Virtualized <b>Data</b> <b>Access</b> - Connect to {{different}} data sources {{and make them}} accessible from a common logical <b>data</b> <b>access</b> point.|$|R
30|$|The {{procedure}} for obtaining <b>data</b> <b>access</b> via remote <b>data</b> <b>access</b> {{is comparable to}} scientific use files. Externals researchers have to send a form that includes information on the applicant, {{the aim of the}} analyses and their content to the FDZ which decides on the application. Compared to scientific use files, remote <b>data</b> <b>access</b> has the disadvantage that an external researcher is not able to directly control the procession of his syntax files with the data and to receive his outputs immediately. But remote <b>data</b> <b>access</b> provides access to less anonymized data, which may offer a higher research potential than scientific use files. Currently, the FDZ charges no fee for this form of <b>data</b> <b>access.</b> Remote <b>data</b> <b>access</b> is available for IAB Establishment Panel (IABB), the Establishment History Panel (BHP) and the IAB Employment Panel (BAP). For the IAB Employment Sample (IABS, weakly anonymized version), the Integrated Employment Biographies (IEBS, weakly anonymized version) and the Linked Employer-Employee Data (LIAB), remote <b>data</b> <b>access</b> is only available after on-site use.|$|R
40|$|Much of the {{programming}} done today requires <b>data</b> <b>access.</b> With {{large amounts of}} data being maintained in databases, access mechanisms that allow manipulation of this data are vital. Visual Basic is a popular programming package because of its user-friendly interface and <b>data</b> <b>access</b> features. Two popular <b>data</b> <b>access</b> mechanisms are <b>data</b> bound controls with <b>data</b> <b>access</b> objects, and remote data controls with remote data objects. The main difference between these methods are that data bound controls ar...|$|R
50|$|Launched in 2008, SFLVault {{simplifies}} {{the management of}} <b>access</b> <b>keys</b> and passwords to large portfolio of services.|$|R
30|$|We hereby {{show that}} the {{attacker}} cannot bypass the data integrity checking that DTrace provides. First of all, DTrace injects <b>data</b> <b>access</b> recording operations right alongside each <b>data</b> <b>access</b> event by programmer annotation or program analysis. DTrace will not miss any <b>data</b> <b>access</b> operation {{as long as the}} recording coverage is complete.|$|R
40|$|Abstract—The lasting memory-wall problem {{combined}} with the newly emerged big-data problem makes <b>data</b> <b>access</b> delay the first citizen of performance optimizations of cluster computing. Reduction of <b>data</b> <b>access</b> delay, however, is application dependent. It depends on the <b>data</b> <b>access</b> behaviors of the underlying applications. Therefore, leaning and understanding <b>data</b> <b>access</b> behaviors is a must for effective <b>data</b> <b>access</b> optimizations. Modern microprocessors are equipped with hardware data prefetchers, which predict <b>data</b> <b>access</b> patterns and prefetch data for CPU. However, memory systems in design {{do not have the}} capability to understand <b>data</b> <b>access</b> behaviors for performance optimizations. In this study, we propose a novel approach, named KNOWAC, to collect I/O information automatically through highlevel I/O libraries. KNOWAC accumulates I/O knowledge and reveals data usage patterns by exploring the collected highlevel I/O characteristics. The discovered data usage patterns can be used for different I/O optimizations. We apply KNOWAC to I/O prefetch under the framework of PnetCDF in this study. Experimental results on a real-world application show that KNOWAC is promising and has a true practical value in mitigating the I/O bottleneck. I...|$|R
50|$|Gmail {{makes use}} of Ajax, {{employing}} browser features such as JavaScript, keyboard <b>access</b> <b>keys</b> and Web feed integration.|$|R
40|$|In {{wireless}} data transmission, {{the capacity}} of wireless link is typically limited. Since many applications exhibit temporal locality for <b>data</b> <b>access,</b> the cache mechanism can be built in a wireless terminal to effectively reduce the <b>data</b> <b>access</b> time. This paper studies the cache performance of the wireless terminal by considering a business card application. We investigate the least recently used replacement policy and two strongly consistent <b>data</b> <b>access</b> algorithms called poll-each-read and callback. An analytic model is proposed to derive the effective hit ratio of <b>data</b> <b>access,</b> {{which is used to}} validate against simulation experiments. Our study reports how the <b>data</b> <b>access</b> rate and the data update distribution affect the cache performance in a wireless terminal...|$|R
40|$|Abstract In process-driven, service-oriented {{architectures}} (SOAs), process {{activities can}} perform service operations, data transformations, or human tasks. Unfortunately, the process activities are usually tightly coupled. Thus, {{when the number}} of activities in the process grows, focusing on particular activities of the flow such as the service operations reading or writing persistent data is a time-consuming task. In particular, in order to solve structural problems concerning persistent <b>data</b> <b>access</b> such as deadlocks in data-intensive business processes, stakeholders need to understand the underlying persistent <b>data</b> <b>access</b> details of the activities i. e. physical storage schemes, and database connections. With our view-based model-driven approach, we provide a solution to generate flows of persistent <b>data</b> <b>access</b> activities (which we refer to as persistent <b>data</b> <b>access</b> flows). To {{the best of our knowledge}} these persistent <b>data</b> <b>access</b> flows are not used to solve structural problems in process-driven SOAs, yet. Moreover, our persistent <b>data</b> <b>access</b> flows can be flattened by diverse filer criteria e. g. by filtering all activities reading or writing from a specific database or table. Using our approach, we can enhance traceability and documentation of persistent <b>data</b> <b>access</b> in business processes. In a series of motivating scenarios from an industrial case study we present how our persistent <b>data</b> <b>access</b> flow concept can contribute to enhance productivity in service...|$|R
5000|$|Large File Send: {{send and}} receive large files from Outlook, with encryption, {{optional}} <b>access</b> <b>key</b> and custom expiration dates.|$|R
5000|$|Obtain {{conditional}} <b>access</b> <b>keys</b> (ECMs) to decrypt channel: {{wait for}} ECMs - part of PMT - 100ms to 500ms ...|$|R
5000|$|Universal <b>Data</b> <b>Access</b> Drivers - High-performance <b>data</b> <b>access</b> {{drivers for}} ODBC, JDBC, ADO.NET, and OLE DB that provide {{transparent}} access to enterprise databases across multiple platforms and databases.|$|R
40|$|This {{paper is}} {{describing}} <b>data</b> <b>access</b> architecture {{in a modern}} object-oriented application. Complex application solutions have multiple, parallel data sources. Each data source has specific properties and ways to <b>access</b> <b>data.</b> This architecture, by using already tried solutions, ensures a simple and flexible way to <b>access</b> different <b>data</b> sources. It’s also describing singleton, <b>data</b> <b>access</b> object and abstract factory patterns and their interaction in achieving flexible and scalable <b>data</b> <b>access</b> architecture...|$|R
40|$|Abstract—In {{wireless}} data transmission, {{the capacity}} of wireless links is typically limited. Since many applications exhibit temporal locality for <b>data</b> <b>access,</b> the cache mechanism can be built in a wire-less terminal to effectively reduce the <b>data</b> <b>access</b> time. This paper studies the cache performance of the wireless terminal by consid-ering a business-card application. We investigate the least-recently used replacement policy and two strongly consistent <b>data</b> <b>access</b> algorithms called poll-each-read and callback. An analytic model is proposed to derive the effective hit ratio of <b>data</b> <b>access,</b> {{which is used to}} validate against simulation experiments. Our study reports how the <b>data</b> <b>access</b> rate and the data update distribution affect the cache performance in a wireless terminal. Index Terms—Cache, strong consistency, temporal locality, wireless data. I...|$|R
40|$|Data prefetching, where data is fetched before CPU {{demands for}} it, has been {{considered}} as an effective solution to mask <b>data</b> <b>access</b> latency. However, the current client-initiated prefetching strategies do not work well for applications with complex, non-contiguous <b>data</b> <b>access</b> patterns. While technology advances continue to enlarge the gap between computing and <b>data</b> <b>access</b> performance, trading computing power for <b>data</b> <b>access</b> delay has become a natural choice. We propose a serverbased data-push approach. In this server-push architecture, a dedicated server named Data Push Server (DPS) initiates and proactively pushes data closer to the client in time. We present the DPS architecture and study the issues such as what data to fetch, when to fetch, how to push, and <b>data</b> <b>access</b> modeling. 1...|$|R
5000|$|<b>Data</b> <b>access</b> {{typically}} {{refers to}} software and {{activities related to}} storing, retrieving, or acting on data housed in a database or other repository. Two fundamental types of <b>data</b> <b>access</b> exist: ...|$|R
