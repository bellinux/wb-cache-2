0|36|Public
40|$|Smartphones are {{now more}} {{affordable}} than ever before, making them ubiquitous amongst some groups, such as students in Higher Education. Their sensing, processing, and <b>interconnection</b> <b>features</b> offer many opportunities for learning and leisure. But do they help or hinder student success?Publisher PDFPeer reviewe...|$|R
40|$|Abstract: This paper {{reviews the}} status in {{industry}} and academia regarding configurations, topologies, controls, and grid connections in grid-tied and micro-grid PV inverter applications. The paper {{will discuss the}} major technical needs to address problems in bringing cost down, increasing efficiency and improving reliability/availability. The paper foresees that new grid <b>interconnection</b> <b>features</b> {{will have to be}} integrated more into the inverters, along with the wide-spreading use of distributed generations...|$|R
40|$|The black-box reuse of library {{classes in}} the {{construction}} of an object-oriented (OO) application is difficult: the principle of information hiding may be violated if classes must know their partners, and code must be typically rewritten. One of the possible ways to increase class decoupling and thus reuse is to employ interconnection languages to describe OO architectures. In this paper we present a decoupling paradigm for individual classes or class collections that facilitates reuse, introducing <b>interconnection</b> <b>features</b> at the design and programming levels. We give examples in a new, second generation OO development system, using asynchronous messages sent to generically identified receivers. < 3 2000 Elsevier Science Inc. All rights reserved...|$|R
40|$|This {{paper will}} {{introduce}} {{the reader to}} the design functionality of the Cisco 12000 GSR product line, {{with a focus on}} the Cisco 12016 GSR. With more than 400 customer backbones utilizing the Cisco 12000 GSR today, the full suite of NEBS, 1 :N clocking fully resilient switch fabric, online insertion and removal (OIR) of modules, and distributed forwarding information across multiple line cards ensure that the Cisco 12000 GSR delivers IP carrier-class operations. In addition, Layer 3 protection switching can be designed to achieve much higher reliability than the individual network elements. This system approach includes the design principles of high availability, topology, peering, logical server placement, and <b>interconnection</b> <b>features</b> such as SONET/SDH automatic protection switching/MSP (APS/MSP), IP load-sharing across parallel networks paths, and fast restoration features in MPLS network...|$|R
40|$|Various {{types of}} <b>interconnection</b> may <b>feature</b> in local or global {{computer}} networks, telephone systems, and networks for parallel computers. Qualitative und quantitative classification {{of these is}} important for the design, testing, and operation of networks. A distinction has to be made between point-to-point, multicast, inverse multicast, broadcast and conference links an...|$|R
50|$|While a {{small amount}} of data could be {{represented}} in a circular diagram using straight lines to show the <b>interconnections,</b> a diagram <b>featuring</b> numerous lines would quickly become illegible. To reduce the visual complexity, chord diagrams employ a technique called hierarchical edge bundling.|$|R
40|$|The {{effectiveness}} of finite-element refinement criteria for achieving optimal meshes based on error equidistribution principles are investigated with benchmark {{systems for the}} electromagnetic simulation of microelectronic system <b>interconnection</b> (MSI) <b>features.</b> The usefulness of the criteria are evaluated for adaptive finite-element electromagnetic analysis of principal device characteristics present in practical MSI structures, which are known to pose challenging problems in numerical modeling. The criteria with, potentially, the most significant implications for MSI electromagnetic simulation, are examined with finite-element solutions for the fundamental benchmark systems computed from both optimal and adaptively refined discretizations...|$|R
40|$|Nanoparticles have {{attracted}} tremendous interest for their thermodynamic size effect which is particularly valuable {{to reduce their}} sintering temperature. This asset is exploited for the fabrication of flexible electronic devices using printing technologies, especially <b>interconnection</b> <b>features.</b> Those printed <b>interconnections</b> require both a low electrical resistivity and high mechanical properties which are largely correlated with microstructure. In this paper, the combined effects of the substrate crystalline orientation and the sintering condition have been demonstrated {{to have a significant}} impact on microstructures. Silver nanoparticles with a mean diameter of 30 nm were inkjet-printed on { 100 } silicon substrates coated with either amorphous Si 3 N 4 or with strongly textured { 111 } Ti/Au. The crystallographic texture and grain size of those printed films have been investigated after being sintered up to 500 °C at 50 °C/s using Electron Back-Scattered Diffraction. The results shows that a { 111 } fiber texture is developed above 300 °C only on oriented films through out-of-plane diffusion. Electrical resistivity shows no dependency on texture since it is mainly affected by grain size. An optimal value of 3. 4 μΩ. cm is achieved at 300 °C using a fast sintering ramp of 10 °C/s. Regarding the Young's modulus of nanofilms, nanoindentation measurements exhibited an oscillation between 38 GPa for smaller crystallites to 81 GPa for the biggest, which is compatible with wire bonding process...|$|R
30|$|The Grid Component Model (GCM) [2], is an {{extension}} of Fractal targeting specifically distributed systems. A strong point of GCM is the separation of concerns [3]. In GCM, the membrane, i.e. the management part of the component, can be defined precisely with all necessary <b>interconnections</b> between management <b>features</b> and {{with the rest of the}} component hierarchy. GCM/ProActive is the reference implementation of the GCM component model. It is based on the ProActive Java library [4].|$|R
40|$|The Batcher-Banyan {{network is}} a special class of {{multistage}} interconnection network mostly used in multiprocessor interconnections and in ATM switches. The switches designed for these applications need to be fully scalable. The main challenge was to design a switching element for a Batcher-Banyan network, which has the same property. The switching element designed {{could be used to}} implement networks of any size by only varying the input control signal and changing the external <b>interconnections.</b> The main <b>features</b> of this design ar...|$|R
40|$|This paper {{presents}} a component-based programming model whose {{goal is to}} make construction and management of distributed cooperative applications easier. The approach taken aims at combining Module <b>Interconnection</b> Language <b>features</b> with the concept of class derived from object-oriented models. Using our model, an application is seen as a hierarchy of interacting components, where a component represents the building unit for construction and management. Low level components contain objects or piece of software written in any programming language. The proposed model is being implemented on top of a distributed object-based platform that provides object distribution, object sharing and clustering. 1 Introduction This paper claims that the construction and the management of distributed cooperative applications would be facilitated by introducing the concept of class, as defined in object-oriented models, into Module Interconnection Languages. It proposes a component-based programming [...] ...|$|R
40|$|This paper {{presents}} a component-basedprogramming model whose {{goal is to}} make construction and management of distributedcooperative applications easier. The approach taken aims at combining Module <b>Interconnection</b> Language <b>features</b> with the concept of class derivedfrom object-orientedmodels. Using our model, an application is seen as a hierarchy of interacting components, whereacomponent represents the building unit for construction and management. Low level components contain objects or piece of software written in any programming language. The proposed model is being implemented on top of a distributed object-based platform that provides object distribution, object sharing and clustering. We argue that adequacy between application requirements and system mechanisms achieved with this model will allow e#cient use of OS functionalities as well as provide an easy way of managing and maintaining applications on any given architecture. 1 Introduction This paper claims that the constructio [...] ...|$|R
40|$|A new silicon based, planar hybrid {{technology}} {{is being developed}} to address limitations associated with packaging and interconnections. The approach combines the advantages of both hybrid and monolithic technologies. Microwave transistor chips (GaAs FETs) are integrated in high resistivity silicon substrates with a vertical precision of better than 2 /spl mu/m and lateral tolerances less than 10 /spl mu/m. Air bridge technology and thin film techniques are then used to provide the necessary <b>interconnections.</b> The basic <b>features</b> of the proposed technology are presented here...|$|R
40|$|Sparse code {{formation}} in the primary visual cortex (V 1) has been inspiration for many state-of-the-art visual recognition systems. To stimulate this behavior, networks are trained networks under mathematical constraint of sparsity or selectivity. In this paper, the authors exploit another approach which uses lateral <b>interconnections</b> in <b>feature</b> learning networks. However, instead of adding direct lateral interconnections among neurons, we introduce an inhibitory layer placed right after normal encoding layer. This idea overcomes the challenge of computational cost and complexity on lateral networks while preserving crucial objective of sparse code formation. To demonstrate this idea, we use sparse autoencoder as normal encoding layer and apply inhibitory layer. Early experiments in visual recognition show relative improvements over traditional approach on CIFAR- 10 dataset. Moreover, simple installment and training process using Hebbian rule allow inhibitory layer to be integrated into existing networks, which enables further analysis in the future. Comment: Technical report, 4 page...|$|R
40|$|ZigZag is a {{paradigm}} of hypermedia {{that consists of}} a multidimensional system of principled <b>interconnections.</b> Its basic <b>features</b> and specifications are now well known, but despite this, very few practical applications have been described or discussed. This paper examines two projects as case studies. These projects both use the unique properties of ZigZag in order to solve real-world problems. One of these case studies is a personal information management system for mobile phones, {{and the other is}} a bioinformatics visualization system. Although superficially extremely different, these areas both make use of information that is loosely structured and deeply interconnected...|$|R
30|$|The {{regional}} energy interconnection of the GCC involves six member states—Kuwait, Saudi Arabia, Bahrain, Qatar, the UAE and Oman. It {{was established}} in May 1981.The energy interconnection project {{is divided into three}} phases from 2009 to 2011. The regional energy <b>interconnection</b> now is <b>featured</b> with high voltage direct current (HVDC) transmission technology and high transfer capacities. The final goal of the energy interconnection is achieving the complete interlinking if the infrastructure network among the GCC states, especially in the fields of electricity, transportation, communication, and information. The GCC Interconnection Authority (GCCIA) {{was established in}} 2001 to meet the objective [35]. The GCCIA is a joint-stock commercially registered entity in acts independently of any country and organization. It has eventually become a regional player in the electricity-trade market.|$|R
40|$|AbstractThe {{study of}} sludge, {{collected}} after processing different makes of steel, is of big {{interest for the}} research of chip formation while grinding. Together with hard- and software it helps to establish qualitative and quantitative <b>interconnections</b> between geometry <b>features</b> of sludge and technological factors. The article deals with studying the influence of chemical composition of steel grinding makes {{on the results of}} photometrical analysis of chips after grinding. For this reason, the photo material obtained with the electronic scanning microscope JSM 6460 LV (JEOL, USA) was studied visually and some chips were geometrically measured. As the most apparent (explicit) form of chips is the globular ones which allow fulfilling the exact geometric measurements, the analysis of chips of this particular form is given in the article...|$|R
40|$|In this paper, we {{consider}} memristors, meminductors, and memcapacitors and their properties as port-Hamiltonian systems. The port-Hamiltonian formalism naturally arises from network modeling of physical systems {{in a variety}} of domains. Exposing the relation between the energy storage, dissipation, and interconnection structure,this framework underscores the physics of the system. One of the strong aspects of the port-Hamiltonian formalism is that a power-preserving interconnection between port-Hamiltonian systems results in another port-Hamiltonian system with composite energy, dissipation, and <b>interconnection</b> structure. This <b>feature</b> can advantageously be used to model, analyze, and simulate networks consisting of complex interconnections of both conventional and memory circuit elements. Furthermore, the port-Hamiltonian formalism naturally extends the fundamental properties of the memory elements beyond the realm of electrical circuits. Postprint (published version...|$|R
50|$|The masks used {{to process}} and {{manufacture}} SSI, MSI and early LSI and VLSI devices (such as the microprocessors {{of the early}} 1970s) were mostly created by hand, often using Rubylith-tape or similar. For large or complex ICs (such as memories or processors), this was often done by specially hired layout people under supervision {{of a team of}} engineers, who would also, along with the circuit designers, inspect and verify the correctness and completeness of each mask. However, modern VLSI devices contain so many transistors, layers, <b>interconnections,</b> and other <b>features</b> that it is no longer feasible to check the masks or do the original design by hand. The engineer depends on computer programs and other hardware aids to do most of this work.|$|R
40|$|Abstract—The {{introduction}} of new propulsion technologies such as electric or hybrid drives imposes fundamental changes to the overall structure of the vehicle’s electric and electronic system architecture. It also increases the need for cross-domain functionality, such as centralized energy management or the orchestration of mechanical braking and electric energy recu-peration during deceleration. This leads to new challenges with respect to architecture development as <b>interconnections</b> between <b>features</b> are introduced that are not yet fully understood. The vehicle’s system architecture evolves from towards a distributed multi-functional control system. Component oriented, model based approaches with multiple viewpoints have already proven being suitable in other domains to manage the dependencies between functionality by decomposing a system into a network of functional entities encapsulated in components. In this paper, we present a domain-specific component model to describe functional interdependencies as well as non-functional requirements needed to enable safe integration of software com-ponents in a centralized automotive ICT architecture. The model enables the composition of high-level functions and the definition of compatibility constraints. The approach is then applied to unveil feature interaction in a component architecture. This forms {{the foundation of a}} sound development and integration process for heavily interconnected functions. It also enables on-line product validation mechanisms to ensure functional integrity and safety as well as meeting of deployment constraints and timing requirements. I...|$|R
40|$|Since the {{relative}} importance of <b>interconnections</b> increases as <b>feature</b> size decreases, standard-cell based synthesis becomes less effective when deep-submicron technologies become available. Intra-cell connectivity can be decreased by the use of macro-cells. In this work we present methods for the automatic generation of macro-cells using pass transistors and domino logic. The synthesis of these cells is based on BDD and ZBDD representations of the logic functions. We address specific problems associated with the BDD approach (level degradation, long paths) and the ZBDD approach (sneak paths, charge sharing, long paths). We compare performance of the macrocells approach versus the conventional standard-cell approach based on accurate electrical simulation. This shows that the macro-cells perform well up to a certain complexity of the logic function. Functions of high complexity must be decomposed into smaller logic blocks that can directly be mapped to macro-cells. 1. Introduction an...|$|R
40|$|Abstract. Human {{communication}} {{is a highly}} complex phenomenon that can be approached from numerous theoretical perspectives of varying nature. It {{is a good example}} of how traditional metatheoretical, epistemological and methodological controversies can be channeled through a body of knowledge whose aim is the rational and systematic search for different approaches, <b>interconnections</b> and complementary <b>features.</b> The extraordinarily wide range of aspects to be considered and the experiential richness that goes hand in hand with every communicative episode make it necessary to choose observational methodology, capable of being both flexible and objective. In the first stage of the process involving observational methodology, qualitative methodology is preferred for the study of communication given the wide range of options it provides in terms of data collection, but it’s high relevant the characterization and application of a quantitative approach in the second stage of an observation of communicative behaviour. Current advances involving the use o...|$|R
40|$|Precise surface {{geometrical}} morphologies {{have been}} shown to improve cellular proliferation, adhesion, and functionality. It has been found that cells respond strongly to feature dimensions a fraction of their size. In this paper, soft lithography techniques were applied to microfabricate polydimethylsiloxane molds with precisely controlled micro-scale patterns. Three-dimensional polycaprolactone (PCL) scaffolds were fabricated using a multilayer micromolding (MMM) method. Proper heating and stamping parameters were developed for micromolding PCL. This process allowed control of the size, shape, and spacing of support structures within the scaffold. The micromolding of multiple layers with independent features allowed for alignment between layers. The high porosity, abundant <b>interconnections,</b> and sharp <b>features</b> were inherent advantages of the scaffolds. Human osteosarcoma cells were seeded in the 3 -D scaffolds for cell growth testing. Fluorescent microscopy and scanning electron micrographs showed that cells responded well to the 3 -D scaffolds and the scaffolds regulated cell morphology and adhesion. © 2007 Elsevier B. V. All rights reserved...|$|R
40|$|Abstract: Mobile Ad-hoc Networks (MANETs) are infrastructure-less {{networks}} comprising of nodes {{that are}} mobile {{and do not}} have fixed <b>interconnections.</b> These distinctive <b>features</b> render conventional routing algorithms unsuitable for these networks and designing special routing protocols for MANETs is an important area for ongoing research. Routing protocols that have been proposed till now can be categorized into three types, reactive, proactive and hybrid. This paper explicates and compares three significant routing protocols for MANETs, one from each category. We analyze these protocols in terms of end to end delay through simulations performed in Network Simulator (ns 2) for varying network conditions like network load, mobility and network size. Our results show that none of the proposed protocols clearly outperforms others under all conditions. Although AODV and DSDV perform better when network congestion is less and TORA performs better for congested networks, {{there is a need to}} design a routing protocol that maintains its performance under all network and mobility conditions...|$|R
40|$|This article {{describes}} the conceptual foundation of a Master’s programme in qualitative methods aimed at training professional qualitative researchers {{in the field of}} social and marketing psychology. Two principles underpin the Master’s project: anchorage to research questions generated by the real social context as the driver of the entire development of the research project (i. e., ‘issue-based research’); and the adoption of what is called ‘process methodology’ as the methodological <b>interconnection</b> between the <b>features</b> of the social field (i. e., the context of the research) and of the research field (i. e., the study’s scope). In practice, process methodology requires the learning of three sets of competences (content, contextual and flow) related to qualitative research. Those competences are devoted to the implementation and management of applied qualitative research able to produce situated knowledge and to enhance the transferability and usability of that knowledge. The article gives details of both the didactic structure of the programme as well as the teaching devices adopted...|$|R
40|$|We {{present a}} rapid, {{reproducible}} and sensitive neurotoxicity testing platform that combines {{the benefits of}} neurite outgrowth analysis with cell patterning. This approach involves patterning neuronal cells within a hexagonal array to standardize the distance between neighbouring cellular nodes, and thereby standardize {{the length of the}} neurite <b>interconnections.</b> This <b>feature</b> coupled with defined assay coordinates provides a streamlined display for rapid and sensitive analysis. We have termed this the network formation assay (NFA). To demonstrate the assay we have used a novel cell patterning technique involving thin film poly(dimethylsiloxane) (PDMS) microcontact printing. Differentiated human SH-SY 5 Y neuroblastoma cells colonized the array with high efficiency, reliably producing pattern occupancies above 70 %. The neuronal array surface supported neurite outgrowth, resulting in the formation of an interconnected neuronal network. Exposure to acrylamide, a neurotoxic reference compound, inhibited network formation. A dose-response curve from the NFA was used to determine a 20 % network inhibition (NI(20)) value of 260 mu M. This concentration was approximately 10 -fold lower than the value produced by a routine cell viability assay, and demonstrates that the NFA can distinguish network formation inhibitory effects from gross cytotoxic effects. Inhibition of the mitogen-activated protein kinase (MAPK) ERK 1 / 2 and phosphoinositide- 3 -kinase (PI- 3 K) signaling pathways also produced a dose-dependent reduction in network formation at non-cytotoxic concentrations. To further refine the assay a simulation was developed to manage the impact of pattern occupancy variations on network formation probability. Together these developments and demonstrations highlight the potential of the NFA {{to meet the demands of}} high-throughput applications in neurotoxicology and neurodevelopmental biology...|$|R
40|$|A {{number of}} {{interconnection}} networks {{have been proposed}} for multiprocessor systems from view points of theoretical and industrial interests. In order to construct massively parallel computers in VLSI implementation, interconnection between processing elements {{is one of the}} critical problems. The number of links between processing elements are limited to reduce wiring area. On the other side, Scale Integration (WSI) has been attracted as a technology to implement a number of PEs on a silicon wafer. So defect and fault-tolerance schemes are also very important for massively parallel computers. In this paper, we propose Shifted Recursive Torus (SRT) interconnection network based on shifting torus network to build multi-level interconnection recursively. The SRT interconnection network is designed by taking account of the limited number of links, easy implementation in VLSI and hierarchical structure for expandability. The SRT is defined as one dimensional interconnection network and is extended into two dimensional <b>interconnection</b> network. Network <b>features</b> such as diameter and average distance of SRT are discussed in detail. A reconfiguration strategy of the SRT proposed as a defect-tolerance scheme. リサーチレポート（北陸先端科学技術大学院大学情報科学研究科...|$|R
40|$|The {{first section}} of the report {{describes}} the AbNET system, a hardware and software communications system designed for distribution automation (it can also find application in substation monitoring and control). The topology of the power system fixes the topology of the communications network, which can therefore be expected to include {{a larger number of}} branch points, tap points, and <b>interconnections.</b> These <b>features</b> make this communications network unlike any other. The network operating software has {{to solve the problem of}} communicating to all the nodes of a very complex network in as reliable a way as possible even if the network is damaged, and it has to do so with minimum transmission delays and at minimum cost. The design of the operating protocols is described within the framework of the seven-layer Open System Interconnection hierarchy of the International Standards Organization. Section 2 of the report describes the development and testing of a high voltage sensor based on an electro-optic polymer. The theory of operation is reviewed. Bulk fabrication of the polymer is discussed, as well as results of testing of the electro-optic coefficient of the material. Fabrication of a complete prototype sensor suitable for use in the range 1 - 20 kV is described. The electro-optic polymer is shown to be an important material for fiber optic sensing applications. Appendix A is theoretical support for this work. The third section of the report presents the application of an artificial neural network, Kohonen's self-organizing feature map, for the classification of power system states. This classifier maps vectors of an N-dimensional space to a 2 -dimensional neural net in a nonlinear way preserving the topological order of the input vectors. These mappings are studied using a nonlinear power system model...|$|R
40|$|The {{understanding}} of karst systems is {{of paramount importance}} for the protection and valorisation of these environments. A multidisciplinary study is presented to investigate the possible <b>interconnection</b> between karst <b>features</b> of a karst area located in the south-western part of the Martani chain (Cesi Mountain, Central Italy). This hydrogeological structure contributes to recharge a deep regional aquifer. The latter feeds the high discharge and salinity Stifone springs. In the southwestern part of Martani chain, seven caves have been mapped, five of which are hosted in the Calcare Massiccio Formation. The analysis of thermo-hygrometric data collected since Autumn 2014 into the caves and those from external meteorological stations, showed the timing of the airflow inversion occurring on late winter/early spring and summer/ early autumn. Despite the complexity of the morphology of caves and of conceptual models of airflow pattern, these data seem to indicate that the monitored small caves could be interconnected to a considerably wider cave system. Data here presented coupled with the knowledge on hydrogeological and geological-structural setting of the limestone massif are useful to drive future speleological explorations, aiming to discover new large cavities and to better understand the water recharge process...|$|R
40|$|This work aims to {{understand}} whether the territory {{can still be}} considered a driver of firm development and competitiveness, bearing {{in mind that the}} current competitive context increasingly pushes firms towards collaborations and relationships that are on different spatial scales. In particular the paper focuses on how a firm can combine relationships that involve different physical and relational spaces referred to territory. The first part of the paper analyses the main contributions on relations and territory. The second part presents the analysis of four Italian firms that have embraced new production and market approaches based on the development of networks of relations in local and extra-local contexts. We verify that the firm and the entrepreneur influence the development and connotation of a territory, and that the territory influences the development of businesses located in it. A more competitive advantage of firm can be so reached developing a portfolio of relationships that overcomes physical distances, involving several organizations characterized by the peculiarities of territory in which they are located. At the same time, the networking generated by organization’s <b>interconnections</b> influence the <b>features</b> of the territory and its competitiveness...|$|R
40|$|ABSTRACT. Hydrological and hydrochemical {{monitoring}} of paired watersheds in the High Arctic {{was conducted in}} 2003 – 04 to investigate the influence of seasonal runoff on lake water chemistry and productivity. Despite similar limnological conditions overall between the two lakes, marked differences in aquatic productivity were attributed to watershed and basin morphology and the resultant influences on lake ice deterioration and growing season length. A switch from allochthonous to autoch-thonous sources of carbon {{late in the season}} reflected the simultaneous decline in river runoff and increase in aquatic produc-tivity as the growing season progressed. However, low air temperatures and protracted snowmelt and ponding in the deeply incised channel of one river in 2003 led to greater solute accumulation in runoff that was discernable in hydrochemical profiles of that lake, even though runoff was greater in 2004. Notwithstanding, calculated nutrient fluxes were greater during the higher-flow year (2004), but mixing was impeded by underflow conditions in the lakes. Despite these differences, connections between river and lake water chemistry appeared weak even with marked seasonal changes in the volume of runoff. Our results highlight the <b>interconnection</b> between site-specific <b>features</b> and hydroclimatic factors like snowmelt and lake ice conditions in influencing limnological conditions and suggest that similar systems may respond differently to the same hydroclimatic conditions...|$|R
40|$|International audienceTo {{investigate}} {{the effects of}} decadal solar variability on ozone and temperature in the tropical stratosphere, along with <b>interconnections</b> to other <b>features</b> of the middle atmosphere, simultaneous data obtained from the Halogen Occultation Experiment (HALOE) aboard the Upper Atmospheric Research Satellite (UARS) and the Stratospheric Aerosol and Gas Experiment II (SAGE II) aboard the Earth Radiation Budget Satellite (ERBS) during the period 1992 ? 2004 have been analyzed using a multifunctional regression model. In general, responses of solar signal on temperature and ozone profiles show good agreement for HALOE and SAGE~II measurements. The inferred annual-mean solar effect on temperature {{is found to be}} positive in the lower stratosphere (max 1. 2 ± 0. 5 K / 100 sfu) and near stratopause, while negative in the middle stratosphere. The inferred solar effect on ozone is found to be significant in most of the stratosphere (2 ± 1. 1 ? 4 ± 1. 6 % / 100 sfu). These observed results are in reasonable agreement with model simulations. Solar signals in ozone and temperature are in phase in the lower stratosphere and they are out of phase in the upper stratosphere. These inferred solar effects on ozone and temperature are found to vary dramatically during some months, at least in some altitude regions. Solar effects on temperature are found to be negative from August to March between 9 mb? 3 mb pressure levels while solar effects on ozone are maximum during January?March near 10 mb in the Northern Hemisphere and 5 mb? 7 mb in the Southern Hemisphere...|$|R
30|$|We {{believe that}} several {{additional}} directions {{should also be}} pursued for future work. First, {{this study did not}} consider learning from neural networks and deep learning, as these algorithms were not previously considered [11]. This is due to the relatively small size of several of the datasets within this study, which made it infeasible to obtain accurate deep learning models using this approach. We are currently considering additional datasets, particularly those with larger amounts of training data to allow us to better understand how deep learning can be augmented by discretized features. In a similar vein, we believe that interconnections likely exist between some of the generated discretized features. Multivariate feature selection and/or deep learning could potentially be used to help stress these <b>interconnections</b> and remove <b>features</b> which are redundant. Second, we propose using metacognition, or the process of learning about learning [26] to allow us to learn which discretized features should be added to a given dataset. We are also studying how one could find an optimal value, or set of values, for the conf and supp thresholds within D-MIAT. While this paper demonstrates that multiple D-MIAT thresholds can be used in combination, and each threshold does typically improve performance, we do not claim that the thresholds used in this study represent an optimal value, or set of values, for all datasets. One potential solution to this would be to develop a metacognition mechanism for learning these thresholds for a given dataset. Similarly, it is possible that a form of metacognition could be added to machine learning algorithms as has been generally suggested within neural networks [27] to help achieve this goal.|$|R
40|$|The Ion Cyclotron RF Transmitter System (ICRF) at Alcator C-Mod {{comprises}} {{four separate}} transmitters each capable of driving 2 MW of power into plasma loads. Four separate transmission lines guide RF power into three antennas, each mounted {{in a separate}} horizontal port, in the C-Mod Tokamak. Protection for the antennas, matching elements and transmission line is accomplished by two unique but interdependent subsystems encompassed by the ICRF Fault System. The Antenna Protection System evaluates antenna phasing and voltage, sets fault thresholds, generates fault signals, and passes fault information to the Master Fault Processor. During operation, the Master Fault Processor is responsible for detecting hazards along the transmission line, generating faults, processing faults from the Antenna Protection System, terminating RF drive and extinguishing faults within 10 mus. In addition, the system controls various delays and sets the boundaries for RF retries. The ICRF Control System provides amplitude regulation for all antennas and phase control for a four-strap antenna. We are modifying some of the fault processing components and control elements of these systems {{in an effort to}} improve reliability and serviceability, and increase flexibility. This upgrade will reduce wired <b>interconnections,</b> add remote <b>features</b> to improve access to key operating parameters, improve RF isolation with new switching components, simplify phase control, and expand the RF regulation system to an active control regime whereby plasma parameters may become direct feedback elements for RF regulation. Details of the proposed upgrade to the system will be presented, and implementation of any new technological tools will be discussed. United States. Dept of Energy (Cooperative Grant No. DE-FC 02 - 99 ER 54512 -C-Mod...|$|R
40|$|For {{the issue}} of {{planning}} construction areas in order of the prevention and mitigation of seismic risk, in addition to emergency management, the scientific community has for decades studies the identification of tools and methodologies for seismic vulnerability of buildings and infrastructure. In this context it’s very difficult the seismic vulnerability evaluation for existing structures, and especially those distributed. The complexity of the infrastructural vulnerability analysis {{is represented by the}} deep <b>interconnection</b> between different <b>features.</b> Several aspects are connected to this problem, from the correct choose of seismic input, structural and geological knowledge, correct modeling and analysis. The process of assessment of structural security conditions therefore must necessarily be dynamic and iterative, depending on the time scale and spatial analysis in the report. The type and quantity of data needed are dependent on the complexity and heterogeneity of both the geological and geotechnical characteristics of the natural system and of the characteristics of structures. So is very important the need for interdisciplinary approach that can integrate organically different methods of investigation related to the different disciplines involved (geology, geotechnical engineering, structural engineering). Structural knowledge refers not only to the geometry structural details, and materials, but also to the state of maintenance of the structure. A careful analysis of conservation state of and degradation of the structure, allows two goals. The first is to ensure that they are not workings special phenomena that may compromise the structural safety of the work, and for which the seismic capacity of the structure can be greatly cut down, the second is related to the observation of the conservation status structure, and quantification of degradation/corrosion due to natural structure working. The attainment of reliable information to study fragility analysis is a complicated and hard work. Still, to obtain hazard and vulnerability structure, it’s need knowledge of site features and geometrical and mechanical structure’s property. The literature on this subject is relatively large but, given the heterogeneity, complexity and scope of works is to be tested is the territory on which they insist, significant progress can be achieved by directing research towards the implementation of strategies and integrated methodologies multi-scale science-engineering. In this context the evaluation of seismic performance of some really existing viaducts are analyzed. The result of the work, is the implementation of global Bridge Management System, useful for global assessment at single structure level, and sufficiently detailed for administration of medium-sized populations, such for example a regional scale. The different sections of BMS, allows the collection and cataloguing of background data and knowledge data, and constitute, the base for performance evaluations in structural perspective, linked to the road network exercise, and in seismic perspective, for seismic vulnerability assessment. A guide for the maintenance program and seismic retrofit is provided, useful for by responsible agencies...|$|R

