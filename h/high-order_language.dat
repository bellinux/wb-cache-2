10|13|Public
40|$|A {{microprocessor}} designated AVM- 1 {{was designed}} to demonstrate the use of generic interpreters in verifying hierarchically decomposed microprocessor specifications. This report is intended to document the <b>high-order</b> <b>language</b> (HOL) code verifying AVM- 1. The organization of the proof is discussed and some technical details concerning {{the execution of the}} proof scripts in HOL are presented. The proof scripts used to verify AVM- 1 are also presented...|$|E
40|$|On-board {{computers}} for control and sequencing from Apollo to Voyager are described along with future trends and recent design examples. Consideration {{is given to}} a <b>high-order</b> <b>language</b> for the Space Shuttle program. Emphasis {{is placed on the}} usage of modern LSI and new distributed architectural approaches. The distributed computer of the Galileo spacecraft and the data processing system for the Shuttle Orbiter are outlined...|$|E
40|$|Recognizing {{and dealing}} with storage and timing {{channels}} when performing the security analysis of a computer system is an elusive task. Methods for discovering {{and dealing with}} these channels have mostly been informal, and formal methods have been restricted to a particular specification language. A methodology for discovering storage and timing channels {{that can be used}} through all phases of the software life cycle to increase confidence that all channels have been identified ispresented. The methodology is presented and applied to an example system having three different descriptions: English, formal specification, and <b>high-order</b> <b>language</b> implementation...|$|E
40|$|The {{selection}} of a <b>high-order</b> programming <b>language</b> for a real-time distributed network simulation is described. The additional problem of implementing a language on a possibly changing network is addressed. The recently designed language ADA (trademarked by DoD) was chosen since it provides the best model of the underlying application to be simulated...|$|R
40|$|Deterministic, {{parallel}} set-term unification algorithms for <b>high-order</b> logic-based database <b>languages,</b> {{of which}} set terms have the commutative and idempotent properties, are lacking. As a result, an efficient inference mechanism {{that can be}} used to determine answers to queries of these database languages deterministically is nonexistent. To overcome these shortcomings, we propose a set-term unification algorithm for LDL=NR, a <b>high-order</b> logic database <b>language.</b> The proposed algorithm not only computes all generalized ground unifiers (ggu s) of a given pair of set terms in LDL=NR without duplicates but also takes the advantage of existing multiple processors for computing all these unifiers in parallel. The proofs of correctness and computational complexity of the proposed algorithm are also included in this paper. Keywords: Logic database language, combinatorics, set term, logic programming, unification. 1 Introduction <b>High-order</b> logic-based database <b>languages,</b> such as HILOG [CC 90] [...] ...|$|R
40|$|SAV-D {{system is}} {{composed}} of display description language (DDL) and its language processor. Language provides for the description of displays in terms of static and variable references for text, special symbols, lines, and circles. SAV-D permits use of <b>high-order</b> English-like <b>language</b> to describe complete displays with increased speed and ease of coding, debugging, and modification. It also allows one to specify static and variable attribute, such as location, flash, dash, character size, and intensity...|$|R
40|$|International Telemetering Conference Proceedings / October 22 - 25, 2001 / Riviera Hotel and Convention Center, Las Vegas, NevadaSoftware and {{techniques}} are described for testing the Advanced Data Acquisition and Processing System (ADAPS), the primary flight test telemetry system used at Edwards AFB, California. The software described {{acts as an}} additional simulation capability and moves the simulation definition process into a realm where data is formed {{by means of a}} <b>high-order</b> <b>language.</b> The potential for creation of more sophisticated simulated test data is thus enabled. Extension of the techniques described in this paper to applications other than testing is discussed...|$|E
40|$|The {{development}} of a test engineer oriented language has been under way at the Kennedy Space Center for several years. The result of this effort is the Ground Operations Aerospace Language, GOAL, a self-documenting, <b>high-order</b> <b>language</b> suitable for coding automatic test, checkout and launch procedures. GOAL is a highly readable, writable, retainable language that is easily learned by nonprogramming oriented engineers. It is sufficiently powerful for use {{at all levels of}} Space Shuttle ground processing, from line replaceable unit checkout to integrated launch day operations. This paper will relate the language development, and describe GOAL and its applications...|$|E
40|$|Language model plays {{a pivotal}} role in large {{vocabulary}} speech recognition systems. Providing more syntactic and semantic information, high-level language models hold stronger ability in guiding the search process and hence optimizing the final result. But on the other hand, complex language models, compared with simple ones, usually introduce proportional computing workload that jeopardizes the system efficiency significantly. In this paper, a new approach of applying <b>high-order</b> <b>language</b> models to decoding process will be proposed, and experimental result in our Gallina system will be provided, which demonstrate that, with high-level language models, the system performance is steadily increased while no much more efficacy is being lost as expense...|$|E
40|$|Abstract Extraction of phrasal knowledge, such as proper names, domain-specific keyphrases and lexical {{templates}} from a domain-specific text collection {{are significant}} for developing effective information retrieval {{systems for the}} Internet. In this paper, {{we are going to}} introduce our ongoing research on automatic phrasal knowledge acquisition for English-Chinese bilingual texts. The underlying techniques consist of adaptive keyphrase extraction, lexical template extraction, phrase translation extraction and <b>high-order</b> Markov <b>language</b> model construction. In addition to the increase of retrieval effectiveness, IR systems based on these techniques are expected able to perform much better in many aspects, such as automatic term suggestion, information filtering, text classification and cross-language information retrieval, etc. ...|$|R
40|$|The Tartan {{language}} {{was designed as}} an experiment {{to see whether the}} Ironman requirement for a common <b>high-order</b> programming <b>language</b> could be satisfied by an extremely simple language The result, Tartan substantially meets the Ironman requirement We believe it is substantially simpler than the four designs that were done in {{the first phase of the}} 0 OO- 1 effort The language definition appears in a companion report; this report provides a more expository discussion of some of the language 2 ̆ 7 s features, some examples of its use, and a discussion of some facilities that could enhance the basic design at relatively little cos...|$|R
40|$|This paper studies {{security}} engineering of distributed systems when following the chemical-programming paradigm, represented {{here by the}} <b>High-Order</b> Chemical <b>Language</b> (HOCL). We have analysed how to model secure systems using HOCL. Emphasis is on modularity, hence we advocate {{for the use of}} aspect-oriented techniques, where security is seen as a cross-cutting concern impacting the whole system. We show how HOCL can be used to model Virtual Organisations (VOs), exemplified by a VO system for the generation of digital products. We also develop security patterns for HOCL, including patterns for security properties such as authorisation, integrity and secure logs. The patterns are applied to HOCL programs following an aspectoriented approach, where aspects are modelled as transformation functions that add to a program a cross-cutting concern. ...|$|R
40|$|HML (Hardware ML) is an {{innovative}} hardware description language {{based on the}} functional programming language SML (Standard ML). HML is a <b>high-order</b> <b>language</b> with polymorphic types. It uses advanced type checking and type inference techniques to verify hardware design rules. We have implemented an HML type checker and a translator to VHDL. We generate a synthesizable subset of VHDL and automatically infer types and interfaces. The paper gives an overview of HML and discusses its typechecking techniques and the translation from HML to VHDL. We use a non-restoring integer square-root example to illustrate the HML system. I. Introduction HML is a functional hardware description language based on Standard ML. SML is a high-level programming language which is strongly typed and polymorphic. HML inherits many programming features of SML and takes advantage of many advanced techniques that are adopted in SML, including the rich type system, advanced typechecking and type inference. It also i [...] ...|$|E
40|$|This thesis {{develops}} a robust inventory of large-scale lattice rescoring methods that {{improve the quality}} of statistical machine translation. These rescoring methods include (i) sentence-specific, <b>high-order</b> <b>language</b> models estimated over multi-billion word corpora, (ii) stochastic segmentation transducers that model the phrasal segmentation process in phrase-based SMT, (iii) efficient large-scale lattice minimum Bayes-risk decoding procedures based on weighted path counting transducers, (iv) multi-input and multi-source lattice combination techniques that synthesise multiple sources of translation knowledge, and (v) a novel decoding framework based on segmentation of a word lattice into regions of high and low confidence that supports targeted application of modelling techniques intended to address particular deficiencies in translation. Efficient realisations of these lattice rescoring methods are described in terms of general purpose weighted finite state transducer operations. A second theme of this thesis concerns the exploitation of monolingual corpora. Although monolingual data is much more widely available than parallel data, in SMT it is typically only used for building word-based language models. However, there are other complementary ways in which this data can be used to improve translation quality. Two novel lattice rescoring methods for exploiting monolingual corpora – phrasal segmentation models that learn the segmentation of sequences of words into sequences of translatable phrases, and monolingual coverage constraints that address the often overlooked issue of machine translation fluency – are proposed in this thesis. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|A thesis {{submitted}} in partial ful lment of {{the requirements}} of the University of Wolverhampton for the degree of Doctor of PhilosophyIn Statistical Machine Translation (SMT), inference needs to be performed over a high-complexity discrete distribution de ned by the intersection between a translation hypergraph and a target language model. This distribution is too complex to be represented exactly and one typically resorts to approximation techniques either to perform optimisation the task of searching for the optimum translation or sampling the task of nding a subset of translations that is statistically representative of the goal distribution. Beam-search is an example of an approximate optimisation technique, where maximisation is performed over a heuristically pruned representation of the goal distribution. For inference tasks other than optimisation, rather than nding a single optimum, one is really interested in obtaining a set of probabilistic samples from the distribution. This is the case in training where one wishes to obtain unbiased estimates of expectations in order to t the parameters of a model. Samples are also necessary in consensus decoding where one chooses from a sample of likely translations the one that minimises a loss function. Due to the additional computational challenges posed by sampling, n-best lists, a by-product of optimisation, are typically used as a biased approximation to true probabilistic samples. A more direct procedure is to attempt to directly draw samples from the underlying distribution rather than rely on n-best list approximations. Markov Chain Monte Carlo (MCMC) methods, such as Gibbs sampling, o er a way to overcome the tractability issues in sampling, however their convergence properties are hard to assess. That is, it is di cult to know when, if ever, an MCMC sampler is producing samples that are compatible iii with the goal distribution. Rejection sampling, a Monte Carlo (MC) method, is more fundamental and natural, it o ers strong guarantees, such as unbiased samples, but is typically hard to design for distributions of the kind addressed in SMT, rendering an intractable method. A recent technique that stresses a uni ed view between the two types of inference tasks discussed here | optimisation and sampling | is the OS approach. OS {{can be seen as a}} cross between Adaptive Rejection Sampling (an MC method) and A optimisation. In this view the intractable goal distribution is upperbounded by a simpler (thus tractable) proxy distribution, which is then incrementally re ned to be closer to the goal until the maximum is found, or until the sampling performance exceeds a certain level. This thesis introduces an approach to exact optimisation and exact sampling in SMT by addressing the tractability issues associated with the intersection between the translation hypergraph and the language model. The two forms of inference are handled in a uni ed framework based on the OS approach. In short, an intractable goal distribution, over which one wishes to perform inference, is upperbounded by tractable proposal distributions. A proposal represents a relaxed version of the complete space of weighted translation derivations, where relaxation happens with respect to the incorporation of the language model. These proposals give an optimistic view on the true model and allow for easier and faster search using standard dynamic programming techniques. In the OS approach, such proposals are used to perform a form of adaptive rejection sampling. In rejection sampling, samples are drawn from a proposal distribution and accepted or rejected as a function of the mismatch between the proposal and the goal. The technique is adaptive in that rejected samples are used to motivate a re nement of the upperbound proposal that brings it closer to the goal, improving the rate of acceptance. Optimisation can be connected to an extreme form of sampling, thus the framework introduced here suits both exact optimisation and exact iv sampling. Exact optimisation means that the global maximum is found with a certi cate of optimality. Exact sampling means that unbiased samples are independently drawn from the goal distribution. We show that by using this approach exact inference is feasible using only a fraction of the time and space that would be required by a full intersection, without recourse to pruning techniques that only provide approximate solutions. We also show that the vast majority of the entries (n-grams) in a language model can be summarised by shorter and optimistic entries. This means that the computational complexity of our approach is less sensitive to the order of the language model distribution than a full intersection would be. Particularly in the case of sampling, we show that it is possible to draw exact samples compatible with distributions which incorporate a <b>high-order</b> <b>language</b> model component from proxy distributions that are much simpler. In this thesis, exact inference is performed in the context of both hierarchical and phrase-based models of translation, the latter characterising a problem that is NP-complete in nature...|$|E
40|$|Efficient {{methods for}} storing and {{querying}} {{are critical for}} scaling <b>high-order</b> n-gram <b>language</b> models to large corpora. We propose a language model based on compressed suffix trees, a representation that is highly compact and can be easily held in memory, while supporting queries needed in computing language model probabilities on-the-fly. We present several optimisations which improve query runtimes up to 2500 x, despite only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). Comment: 14 pages in Transactions of the Association for Computational Linguistics (TACL) 201...|$|R
40|$|We {{describe}} the implementation steps required to scale <b>high-order</b> character <b>language</b> models to gigabytes of training data without pruning. Our online models build character-level PAT trie structures {{on the fly}} using heavily data-unfolded implementations of an mutable daughter maps with a long integer count interface. Terminal nodes are shared. Character 8 -gram training runs at 200, 000 characters per second and allows online tuning of hyperparameters. Our compiled models precompute all probability estimates for observed n-grams and all interpolation parameters, along with suffix pointers to speedup context computations from proportional to n-gram length to a constant. The result is compiled models that are larger than the training models, but execute at 2 million characters per second on a desktop PC. Cross-entropy on held-out data shows these models to be {{state of the art}} in terms of performance. ...|$|R
40|$|International audienceService-based {{infrastructures}} are shaping tomorrow's {{distributed computing}} systems {{by allowing the}} design of loosely-coupled distributed applications based on the composition of services spread over a set of resources available on the Internet. Compared to previous approaches such as remote procedure call, distributed objects or components, this new paradigm makes feasible the loose coupling of software modules, encapsulated into services, by allowing a late binding to them at runtime. In this context, an important issue is how to express the composition of services while keeping this loosely-coupled property. Different approaches have been proposed to express services composition, mostly using specialized languages. This article presents and explore an unconventional new approach for service composition based on a programming language, inspired by a chemical metaphor, called the <b>High-Order</b> Chemical <b>Language</b> (HOCL). The proposed approach provides a very abstract and generic way of programming service composition thanks to the high-order property of HOCL. We illustrated this approach by applying it to a simple example that aims at providing a travel organizer service based on the composi- tion of several basic and smaller services...|$|R
40|$|In Statistical Machine Translation (SMT), {{inference}} {{needs to}} be performed over a high-complexity discrete distribution de ned by the intersection between a translation hypergraph and a target language model. This distribution is too complex to be represented exactly and one typically resorts to approximation techniques either to perform optimisation { the task of searching for the optimum translation { or sampling { the task of nding a subset of translations that is statistically representative of the goal distribution. Beam-search {{is an example of}} an approximate optimisation technique, where maximisation is performed over a heuristically pruned representation of the goal distribution. For inference tasks other than optimisation, rather than nding a single optimum, one is really interested in obtaining a set of probabilistic samples from the distribution. This is the case in training where one wishes to obtain unbiased estimates of expectations in order to t the parameters of a model. Samples are also necessary in consensus decoding where one chooses from a sample of likely translations the one that minimises a loss function. Due to the additional computational challenges posed by sampling, n-best lists, a by-product of optimisation, are typically used as a biased approximation to true probabilistic samples. A more direct procedure is to attempt to directly draw samples from the underlying distribution rather than rely on n-best list approximations. Markov Chain Monte Carlo (MCMC) methods, such as Gibbs sampling, o er a way to overcome the tractability issues in sampling, however their convergence properties are hard to assess. That is, it is di cult to know when, if ever, an MCMC sampler is producing samples that are compatible iii with the goal distribution. Rejection sampling, a Monte Carlo (MC) method, is more fundamental and natural, it o ers strong guarantees, such as unbiased samples, but is typically hard to design for distributions of the kind addressed in SMT, rendering an intractable method. A recent technique that stresses a uni ed view between the two types of inference tasks discussed here | optimisation and sampling | is the OS approach. OS {{can be seen as a}} cross between Adaptive Rejection Sampling (an MC method) and A optimisation. In this view the intractable goal distribution is upperbounded by a simpler (thus tractable) proxy distribution, which is then incrementally re ned to be closer to the goal until the maximum is found, or until the sampling performance exceeds a certain level. This thesis introduces an approach to exact optimisation and exact sampling in SMT by addressing the tractability issues associated with the intersection between the translation hypergraph and the language model. The two forms of inference are handled in a uni ed framework based on the OS approach. In short, an intractable goal distribution, over which one wishes to perform inference, is upperbounded by tractable proposal distributions. A proposal represents a relaxed version of the complete space of weighted translation derivations, where relaxation happens with respect to the incorporation of the language model. These proposals give an optimistic view on the true model and allow for easier and faster search using standard dynamic programming techniques. In the OS approach, such proposals are used to perform a form of adaptive rejection sampling. In rejection sampling, samples are drawn from a proposal distribution and accepted or rejected as a function of the mismatch between the proposal and the goal. The technique is adaptive in that rejected samples are used to motivate a re nement of the upperbound proposal that brings it closer to the goal, improving the rate of acceptance. Optimisation can be connected to an extreme form of sampling, thus the framework introduced here suits both exact optimisation and exact iv sampling. Exact optimisation means that the global maximum is found with a certi cate of optimality. Exact sampling means that unbiased samples are independently drawn from the goal distribution. We show that by using this approach exact inference is feasible using only a fraction of the time and space that would be required by a full intersection, without recourse to pruning techniques that only provide approximate solutions. We also show that the vast majority of the entries (n-grams) in a language model can be summarised by shorter and optimistic entries. This means that the computational complexity of our approach is less sensitive to the order of the language model distribution than a full intersection would be. Particularly in the case of sampling, we show that it is possible to draw exact samples compatible with distributions which incorporate a <b>high-order</b> <b>language</b> model component from proxy distributions that are much simpler. In this thesis, exact inference is performed in the context of both hierarchical and phrase-based models of translation, the latter characterising a problem that is NP-complete in nature. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|Abstract—Service-based {{infrastructures}} are shaping tomor-row’s {{distributed computing}} systems {{by allowing the}} design of loosely-coupled distributed applications based on the com-position of services spread over a set of resources available on the Internet. Compared to previous approaches such as remote procedure call, distributed objects or components, this new paradigm makes feasible the loose coupling of software modules, encapsulated into services, by allowing a late binding to them at runtime. In this context, an important issue is how to express the composition of services while keeping this loosely-coupled property. Different approaches have been proposed to express services composition, mostly using specialized languages. This article presents and explore an unconventional new approach for service composition based on a programming language, inspired by a chemi-cal metaphor, called the <b>High-Order</b> Chemical <b>Language</b> and generic way of programming service composition thanks to the high-order property of HOCL. We illustrated this approach by applying it to a simple example that aims at providing a travel organizer service based on the composi-tion of several basic and smaller services. I...|$|R
40|$|Service-based {{infrastructures}} are shaping tomorrow’s {{distributed computing}} systems {{by allowing the}} design of loosely-coupled distributed applications based on the composition of services spread over a set of resources available on the Internet. Compared to previous approaches such as remote procedure call, distributed objects or components, this new paradigm makes feasible the loose coupling of software modules, encapsulated into services, by allowing a late binding to them at runtime. In this context, an important issue is how to express the composition of services while keeping this loosely-coupled property. Different approaches have been proposed to express services composition, mostly using specialized languages. This article presents and explore an unconventional new approach for service composition based on a programming language, inspired by a chemical metaphor, called the <b>High-Order</b> Chemical <b>Language</b> (HOCL). The proposed approach provides a very abstract and generic way of programming service composition thanks to the high-order property of HOCL. We illustrated this approach by applying it to a simple example that aims at providing a travel organizer service based on the composition of several basic and smaller services.   </p...|$|R
40|$|AbstractSpacecraft {{automatic}} test system, a comprehensive spacecraft test information {{system based on}} the various spacecraft test specifications formalized as spacecraft test language, is an important means to improve test efficiency. With the new requirements of the multi-spacecraft test in China, {{the study of the}} spacecraft test language becomes a new challenge for spacecraft test field. In this article, a <b>high-order</b> spacecraft test <b>language,</b> China aerospace test and operation language (CATOL), is given associated with the current test requirements; meanwhile, the structure of the language is presented. Then, for characterizing and formalizing the spacecraft processes, the syntax and operational semantics of one of the sub-languages, CATOL-PR, are defined. Finally, the prototype system of this proposed language is presented. This language will improve the specification of spacecraft test work in China and the efficiency of spacecraft testers, and promote the development in spacecraft {{automatic test}}...|$|R
40|$|The initial {{goals of}} the Space Shuttle Program {{required}} that the avionics and software systems blaze new trails in advancing avionics system technology. Many of the requirements placed on avionics and software were accomplished {{for the first time}} on this program. Examples include comprehensive digital fly-by-wire technology, use of a digital databus for flight critical functions, fail operational/fail safe requirements, complex automated redundancy management, and the use of a <b>high-order</b> software <b>language</b> for flight software development. In order to meet the operational and safety {{goals of the}} program, the Space Shuttle software had to be extremely high quality, reliable, robust, reconfigurable and maintainable. To achieve this, the software development team evolved a software process focused on continuous process improvement and defect elimination that consistently produced highly predictable and top quality results, providing software managers the confidence needed to sign each Certificate of Flight Readiness (COFR). This process, which has been appraised at Capability Maturity Model (CMM) /Capability Maturity Model Integration (CMMI) Level 5, has resulted in one of the lowest software defect rates in the industry. This paper will present an overview of the evolution of the Primary Avionics Software System (PASS) project and processes over thirty years, an argument for strong statistical control of software processes with examples, an overview of the success story for identifying and driving out errors before flight, a case study of the few significant software issues and how they were either identified before flight or slipped through the process onto a flight vehicle, and identification of the valuable lessons learned over the life of the project...|$|R

