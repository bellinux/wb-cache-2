15|178|Public
25|$|A typeI error {{occurs when}} the null {{hypothesis}} (H0) is true, but is rejected. It is asserting something that is absent, a <b>false</b> <b>hit.</b> A typeI error may be likened to a so-called false positive (a result that indicates that a given condition is present when it actually is not present).|$|E
50|$|A {{transition}} that {{is detected}} correctly {{is called a}} hit, a cut that is there but was not detected is called a missed hit and a position in that the software assumes a cut, but where actually no cut is present, is called a <b>false</b> <b>hit.</b>|$|E
5000|$|The {{two basic}} {{categories}} of recognition memory errors are false hits (or false alarms) and misses. [...] A <b>false</b> <b>hit</b> is {{the identification of}} an occurrence as old when {{it is in fact}} new. A miss is the failure to identify a previous occurrence as old.|$|E
40|$|Abstract — “Information Extraction ” systems, face {{a crucial}} problem of <b>false</b> <b>hits</b> {{because of a}} number of reasons, {{observations}} show {{that a large number of}} these <b>false</b> <b>hits</b> are due to the presence of subjectivity in the text. This observation motivates the exploration of the notion of using subjectivity analysis to decrease the number of <b>false</b> <b>hits</b> by the Information Extraction systems. “Subjectivity Analysis ” system is the one which identifies and extracts information related to opinions, attitudes and sentiments from the text. This paper presents a review on the method of subjectivity analysis, and then using its results to improve precision of information extraction system...|$|R
5000|$|Beider, A. & Morse, S. P. 2008. Beider-Morse Phonetic Matching: An Alternative to Soundex with Fewer <b>False</b> <b>Hits.</b> Avotaynu: The International Review of Jewish Genealogy 24/2: 12-18.|$|R
40|$|Information {{extraction}} (IE) {{systems are}} prone to <b>false</b> <b>hits</b> {{for a variety of}} reasons and we observed that many of these <b>false</b> <b>hits</b> occur in sentences that contain subjective language (e. g., opinions, emotions, and sentiments). Motivated by these observations, we explore the idea of using subjectivity analysis to improve the precision of information extraction systems. In this paper, we describe an IE system that uses a subjective sentence classifier to filter its extractions. We experimented with several different strategies for using the subjectivity classifications, including an aggressive strategy that discards all extractions found in subjective sentences and more complex strategies that selectively discard extractions. We evaluated the performance of these different approaches on the MUC- 4 terrorism data set. We found that indiscriminately filtering extractions from subjective sentences was overly aggressive, but more selective filtering strategies improved IE precision with minimal recall loss...|$|R
50|$|A type I error {{occurs when}} the null {{hypothesis}} (H0) is true, but is rejected. It is asserting something that is absent, a <b>false</b> <b>hit.</b> A type I error may be likened to a so-called false positive (a result that indicates that a given condition is present when it actually is not present).|$|E
5000|$|A third <b>false</b> <b>hit</b> error can {{be induced}} {{through the use}} of the Deese-Roediger-McDermott paradigm. If all items studied are highly related to one word that does not appear on the list, the subject is highly likely to {{recognize}} that word as old in the test. [...] An example of this would be a list containing the following words: nap, drowsy, bed, duvet, night, relax. The lure in this case is the word 'sleep'. It is highly likely that 'sleep' would be falsely recognized as appearing on that list due to the level of activation received from the list words. This phenomenon is so pervasive that the rate of false generated in this manner can even surpass the rate of correct responses ...|$|E
40|$|Abstract [...] The {{performance}} of a character-based Chinese text retrieval scheme (the combined scheme) is investigated. In the scheme both the monogram keys (singleton characters) and bigram keys (consecutive character pairs) are encoded into document signatures such {{that half of the}} bits in every signature are set. For disyllabic queries, an analytical expression of the <b>false</b> <b>hit</b> rate that accounts for both random false hits and adjacency false hits is proposed. Then optimal monogram and bigram weight assignments together with the corresponding minimal <b>false</b> <b>hit</b> rate are derived in terms of signature length, storage overhead of the combined scheme, and the occurrence frequency and the association value of a disyllabic query. The theoretical predictions of the optimal weight assignments and the minimal <b>false</b> <b>hit</b> rate are tested and verified in experiments u ing a real Chinese cot'pus for disyllabic queries of different association values. Satisfactory agreement between the experimental results and theoretical predictions i found. I...|$|E
40|$|This study {{evaluated}} {{methods for}} automated classification of rain events into groups of “high” and “low” {{spatial and temporal}} variability in offline and online situations. The applied classification techniques are fast and based on rainfall data only, and can thus be applied by, e. g., water system operators to change modes of control of their facilities. A k-means clustering technique was applied to group events retrospectively {{and was able to}} distinguish events with clearly different temporal and spatial correlation properties. For online applications, techniques based on k-means clustering and quadratic discriminant analysis both provided a fast and reliable identification of rain events of “high” variability, while the k-means provided the smallest number of rain events falsely identified as being of “high” variability (<b>false</b> <b>hits).</b> A simple classification method based on a threshold for the observed rainfall intensity yielded a large number of <b>false</b> <b>hits</b> and was thus outperformed by the other two methods...|$|R
25|$|If the {{detected}} structures {{have reached}} a certain threshold level, they are highlighted in the image for the radiologist. Depending on the CAD system these markings can be permanently or temporary saved. The latter's advantage is that only the markings which are approved by the radiologist are saved. <b>False</b> <b>hits</b> should not be saved, because an examination {{at a later date}} becomes more difficult then.|$|R
40|$|Abstract. The {{basic concept}} for {{processing}} spatial joins {{consists of two}} steps: First, the spatial join is performed on the minimum bounding rectangles of the ob-jects by using a spatial access method. This step provides a set of candidates which consists of answers (<b>hits)</b> and non-answers (<b>false</b> <b>hits).</b> In the second step, the exact geometry of the candidates is transferred from secondary storage into main memory and is tested against the join predicate. This step is called refine-ment step. It causes the main cost for computing a spatial join. In this paper, we introduce an additional filter step {{in order to reduce}} the cost of the refinement step. In this filter step more sophisticated approximations are used to identify hits as well as to filter out <b>false</b> <b>hits</b> from the set of candidates. For this purpose, we investigate various types of conservative and progressive approximations. The performance of the approximation approach is evaluated with data sets from real cartographic applications. The results show that this approach considerably re-duces the total execution time of the spatial join. ...|$|R
40|$|Each tuple in a valid-time {{relation}} {{includes an}} interval attribute T {{that represents the}} tuple's valid time. The overlap join between two valid-time relations determines all pairs of tuples with overlapping intervals. Although overlap joins are common, existing partitioning and indexing schemes are inefficient if the data includes long-lived tuples or if intervals intersect partition boundaries. We propose Overlap Interval Partitioning (OIP), a new partitioning approach for data with an interval. OIP divides the time range of a relation into k base granules and defines overlapping partitions for sequences of contiguous granules. OIP is the first partitioning method for interval data that gives a constant clustering guarantee: the difference in duration between the interval of a tuple and the interval of its partition is independent of {{the duration of the}} tuple's interval. We offer a detailed analysis of the average <b>false</b> <b>hit</b> ratio and the average number of partition accesses for queries with overlap predicates, and we prove that the average <b>false</b> <b>hit</b> ratio is independent of the number of short- and long-lived tuples. To compute the overlap join, we propose the Overlap Interval Partition Join (OIPJoin), which uses OIP to partition the input relations on-the-fly. Only the tuples from overlapping partitions have to be joined to compute the result. We analytically derive the optimal number of granules, k, for partitioning the two input relations, from the size of the data, the cost of CPU operations, and the cost of main memory or disk IOs. Our experiments confirm the analytical results and show that the OIPJoin outperforms state-of-the-art techniques for the overlap join...|$|E
40|$|In this paper, {{we propose}} a new {{framework}} of removing {{salt and pepper}} impulse noise. In our proposed framework, the most {{important point is that}} the number of noise-free white and black pixels in a noisy image can be determined by using the noise rates estimated by Fuzzy Impulse Noise Detection and Reduc-tion Method (FINDRM) and Efficient Detail-Preserving Approach (EDPA). For the noisy image includes many noise-free white and black pixels, the detected noisy pixel from the FINDRM is re-checked by using the alpha-trimmed mean. Finally, the impulse noise filtering phase of the FINDRM is used to restore the image. Simulation results show that for the noisy image including many noise-free white and black pixels, the proposed framework can decrease the <b>False</b> <b>Hit</b> Rate (FHR) efficiently compared with the FINDRM. Therefore, the proposed framework can be used more widely than the FINDRM...|$|E
40|$|Some {{emerging}} {{applications in}} mobile ad hoc networks (MANETs) and mobile sensor networks (MSNs) have varying mobility patterns that entails different routing strategies {{at different times}} to maintain high performance. This requires the routing protocol to adapt to different situations for better overall routing performance. We propose a model-based approach to enable such situation-awareness for the Dynamic Source Routing (DSR) protocol in such challenging environments. Our model captures the access behavior of route cache (hit, miss, and <b>false</b> <b>hit)</b> and is simple enough {{to be used in}} real-world settings. We also present a feedback-based architecture that uses the model outputs to cope with mobility changes by adjusting caching strategy on-the-fly. We validate our model against ns- 2 simulations for a variety of scenarios, including a real-world mobility. Our results show that the model can aptly drive adaptive routing that leads to consistent performance improvement over DSR for the scenarios considered. 1...|$|E
40|$|Subjectivity lexicons {{have been}} {{invaluable}} resources in subjectivity analysis and their creation {{has been an}} important topic. Many systems rely on these lexicons. For any subjectivity analysis system, which relies on a subjectivity lexicon, subjectivity sense ambiguity is a serious problem. Such systems will be misled by the presence of subjectivity clues used with objective senses called <b>false</b> <b>hits.</b> We believe that any type of subjectivity analysis system relying on lexicons will benefit from a sense-aware approach. We think sense-aware subjectivity analysis has been neglected mostly because of the concerns related to word sense disambiguation (WSD), the problem of automatically determining which sense of a word is activated by {{the use of the word}} in a particular context according to a sense-inventory. Although WSD is the perfect tool for sense-aware classification, trust in traditional fine-grained WSD as an enabling technology is not high due to previous mostly unsuccessful results. In this thesis, we investigate feasible and practical methods to avoid these <b>false</b> <b>hits</b> via sense-aware analysis. We define a new coarse-grained WSD task capturing the right semantic granularity specific to subjectivity analysis...|$|R
30|$|The table also {{demonstrates}} a rather annoying problem {{when dealing with}} literature studies of chemicals. The widespread use of abbreviations in chemical names may lead to <b>false</b> <b>hits.</b> An example {{is the use of}} DES for the synthetic estrogen diethylstillbestrol. This term has many alternative meanings in a similar context, such as DES-gene or Dysequlibrium Syndrome. Of course, this problem is most pronounced in title- or free text-searches but may be less prevalent in databases with a practice of chemical indexing such as SciFinder.|$|R
6000|$|... 'I entreat your Majesty's pardon,' said Berenger, {{anxious to}} retract his <b>false</b> step. <b>'It</b> was your {{goodness}} and the gracious Queen's {{that made me}} hope for your sanction.' ...|$|R
40|$|Abstract — Multi-step {{processing}} {{is commonly}} used for nearest neighbor (NN) and similarity search in applications involving highdimensional data and/or costly distance computations. Today, many such applications require a proof of result correctness. In this setting, clients issue NN queries to a server that maintains a database signed by a trusted authority. The server returns the NN set along with supplementary information that permits result verification using the dataset signature. An adaptation of the multi-step NN algorithm incurs prohibitive network overhead due to the transmission of false hits, i. e., records {{that are not in}} the NN set, but are nevertheless necessary for its verification. In order to alleviate this problem, we present a novel technique that reduces the size of each <b>false</b> <b>hit.</b> Moreover, we generalize our solution for a distributed setting, where the database is horizontally partitioned over several servers. Finally, we demonstrate the effectiveness of the proposed solutions with real datasets of various dimensionalities...|$|E
40|$|Business surveys {{often use}} complex sets of edit rules (edits, for short) to check {{returned}} questionnaires (records), locate suspicious or unacceptable responses, and support data cleaning operations prior {{to using the}} survey responses for estimation of the required target parameters. These sets of edits are complex because they may involve large numbers of survey questionnaires and variables, they may contain {{a large number of}} edits, and the edits may depend on a large number of tolerance parameters. When such sets of edits are used, they may cause large numbers of record failures and generate substantial costs of revision, especially if edit failures are dealt with by means of clerical operations, like reviewing original paper questionnaires or digital images of these, and re-contacting businesses for clarification and/or correction of the responses provided. Costs can be high both in terms of the resources required, as well as in terms of timeliness of survey processing, by delaying availability of the survey data for estimation and publication. In this paper we describe a generic tool, developed {{as a result of the}} collaboration between the University of Southampton and the ONS. This tool can help to assess the potential impact of changing the edits in a specified business survey. It is a SAS macro using the IML language which enables calculation of a number of edit performance and data quality indicators. Changes to the set of edits aiming to ‘relax’ the existing edits so that failure rates decrease and efficiency savings are achieved are assessed by means of several edit-related performance indicators, like failure and hit rates, <b>false</b> <b>hit</b> rates, etc [...] Data quality indicators include proportion of errors missed and estimates of the bias resulting from missing errors for a specified revision of the set of edits. Edit designers and managers can then aim to fine tune their edits so that failure rates, <b>false</b> <b>hit</b> rates and editing costs are reduced, while data quality is preserved. An illustration is provided by the application of the tool to revise the edits used for the UK Annual Business Inquiry Part 2 to the reference year 2007...|$|E
40|$|A {{fast and}} robust {{framework}} for incrementally detecting text on road signs from video {{is presented in}} this paper. This new framework makes two main contributions. 1) The framework applies a divide-and-conquer strategy to decompose the original task into two subtasks, that is, the localization of road signs and the detection of text on the signs. The algorithms for the two subtasks are naturally incorporated into a unified framework through a feature-based tracking algorithm. 2) The framework provides a novel way to detect text from video by integrating two-dimensional (2 -D) image features in each video frame (e. g., color, edges, texture) with the three-dimensional (3 -D) geometric structure information of objects extracted from video sequence (such as the vertical plane property of road signs). The feasibility of the proposed framework has been evaluated using 22 video sequences captured from a moving vehicle. This new framework gives an overall text detection rate of 88. 9 % and a <b>false</b> <b>hit</b> rate of 9. 2 %. It can easily be applied to other tasks of text detection from video and potentially be embedded in a driver assistance system...|$|E
40|$|This paper {{proposes a}} category- and selection-enabled nearest {{neighbor}} join (NNJ) between relation r and relation s, with similarity on T {{and support for}} category attributes C and selection predicate θ. Our solution does not suffer from redundant fetches and index <b>false</b> <b>hits,</b> which are the main performance bottlenecks of current nearest neighbor join techniques. A category-enabled NNJ leverages the category attributes C for query evaluation. For example, the categories of relation r {{can be used to}} limit relation s accessed at most once. Solutions that are not category-enabled must process each category independently and end up fetching, either from disk or memory, the blocks of the input relations multiple times. A selection-enabled NNJ performs well independent of whether the DBMS optimizer pushes the selection down or evaluates it on the fly. In contrast, index-based solutions suffer from many index <b>false</b> <b>hits</b> or end up in an expensive nested loop. Our solution does not constrain the physical design, and is efficient for row- as well as column-stores. Current solutions for column-stores use late materialization, which is only efficient if the data is clustered on the category attributes C. Our evaluation algorithm finds, for each outer tuple r, the inner tuples that satisfy the equality on the category and have the smallest distance to r with only one scan of both inputs. We experimentally evaluate our solution using a data warehouse that manages analyses of animal feeds...|$|R
40|$|The {{migratory route}} of the Wyoming Range mule deer (Odocoileus hemionus) herd is {{bisected}} by U. S. Highway 30 between Kemmerer and Cokeville, Wyoming, resulting in hundreds of deer-vehicle collisions at this site each year. We tested {{the effectiveness of the}} FLASH system, designed to detect deer presence on the highway and warn motorists by triggering flashing lights associated with a sign. We collected data on deer activity, system reliability, and vehicle speed in response to the warning signs. We also conducted a series of experimental manipulations to determine motorist response to the system with the lights flashing or not flashing and with {{the presence or absence of}} a realistic deer decoy in the road. It was found that more than 50 percent of the hits registered by the FLASH system were <b>false</b> <b>hits</b> not caused by deer, though a backup deer detection system worked well throughout the study period, with no <b>false</b> <b>hits</b> detected. Vehicles were not found to slow down significantly for the warning signs, although they did slow down in response to deer presence. During the experimental manipulations, vehicles only significantly reduced their speed (11. 6 and 6. 3 mph on average for passenger vehicles and tractor trailers respectively) when the deer decoy was in the crossing. Vehicles responded to the other treatments by reducing their speed by an average of less than 5 mph. The system tested may be effective in preventing deer-vehicle collisions under different traffic conditions, but is not suitable for this particular site...|$|R
40|$|We {{propose a}} new object {{decomposition}} method, called DMBRs, {{to improve the}} performance of spatial query processing. This method is suitable for complex spatial objects in real-world geographic applications. The basic idea is that a polygon is recursively divided into two subpolygons by splitting its MBR until a given constraint is satisfied. To increase {{the efficiency of the}} DMBRs method, an extension of an existing spatial indexing structure is presented. Since this new structure can prune a number of <b>false</b> <b>hits</b> quickly, the performance of spatial query processing can be improved. The proposed method is compared with traditional decomposition methods by an analytical study. This comparison shows that our decomposition method outperforms the traditional decomposition methods. ...|$|R
40|$|Various {{methods of}} {{automatic}} shot boundary detection {{have been proposed}} and claimed to perform reliably. Although the detection of edits is fundamental {{to any kind of}} video analysis since it segments a video into its basic components, the shots, only few comparative investigations on early shot boundary detection algorithms have been published. These investigations mainly concentrate on measuring the edit detection performance, however, do not consider the algorithms ’ ability to classify the types and to locate the boundaries of the edits correctly. This paper extends these comparative investigations. More recent algorithms designed explicitly to detect specific complex editing operations such as fades and dissolves are taken into account, and their ability to classify the types and locate the boundaries of such edits are examined. The algorithms ’ performance is measured in terms of hit rate, number of false hits, and miss rate for hard cuts, fades, and dissolves over a large and diverse set of video sequences. The experiments show that while hard cuts and fades can be detected reliably, dissolves are still an open research issue. The <b>false</b> <b>hit</b> rate for dissolves is usually unacceptably high, ranging from 50 % up to over 400 %. Moreover, all algorithms seem to fail under roughly the same conditions...|$|E
40|$|Abstract Background Src plays various {{roles in}} tumour progression, invasion, metastasis, {{angiogenesis}} and survival. It {{is one of}} the multiple targets of multi-target kinase inhibitors in clinical uses and trials for the treatment of leukemia and other cancers. These successes and appearances of drug resistance in some patients have raised significant interest and efforts in discovering new Src inhibitors. Various in-silico methods have been used in some of these efforts. It is desirable to explore additional in-silico methods, particularly those capable of searching large compound libraries at high yields and reduced false-hit rates. Results We evaluated support vector machines (SVM) as virtual screening tools for searching Src inhibitors from large compound libraries. SVM trained and tested by 1, 703 inhibitors and 63, 318 putative non-inhibitors correctly identified 93. 53 %~ 95. 01 % inhibitors and 99. 81 %~ 99. 90 % non-inhibitors in 5 -fold cross validation studies. SVM trained by 1, 703 inhibitors reported before 2011 and 63, 318 putative non-inhibitors correctly identified 70. 45 % of the 44 inhibitors reported since 2011, and predicted as inhibitors 44, 843 (0. 33 %) of 13. 56 M PubChem, 1, 496 (0. 89 %) of 168 K MDDR, and 719 (7. 73 %) of 9, 305 MDDR compounds similar to the known inhibitors. Conclusions SVM showed comparable yield and reduced <b>false</b> <b>hit</b> rates in searching large compound libraries compared to the similarity-based and other machine-learning VS methods developed from the same set of training compounds and molecular descriptors. We tested three virtual hits of the same novel scaffold from in-house chemical libraries not reported as Src inhibitor, one of which showed moderate activity. SVM may be potentially explored for searching Src inhibitors from large compound libraries at low false-hit rates. </p...|$|E
40|$|We propose and {{demonstrate}} {{the use of}} phase images for content-addressable holographic data storage. Use of binary phase-based data pages with 0 and p phase changes, produces uniform spectral distribution at the Fourier plane. The absence of strong DC component at the Fourier plane and more intensity of higher order spatial frequencies facilitate better recording of higher spatial frequencies, and improves the discrimination capability of the contentaddressable memory. This improves {{the results of the}} associative recall in a holographic memory system, and can give low number of <b>false</b> <b>hits</b> even for small search arguments. The phase-modulated pixels also provide an opportunity of subtraction among data pixels leading to better discrimination between similar data pages...|$|R
40|$|Abstract: Seven {{programs}} {{for analysis of}} PIXE spectra were compared using the 2000 IAEA test spectra, i. e. Geopixe, Gupix, Pixan, Pixeklm, Sapix, Winaxil and Witshex. A systematic statistical study of the analysis results was performed based on z-scores. The results indicate {{that most of the}} programs perform reasonably well with respect to peak areas. Except for a very rare exception, the statistical analysis shows that the participants generally reported smaller uncertainties than would have been expected from the reference uncertainty values. The results show that all the participants reported a number of statistically significant <b>false</b> <b>hits</b> and misses in their reports. All this indicates that programs in general still leave room for further improvements...|$|R
5000|$|Signal {{detection}} {{theory has}} been applied to recognition memory as a method of estimating the effect of the application of these internal criteria, referred to as bias. Critical to the dual process model is the assumption that recognition memory reflects a signal detection process in which old and new items each have a distinct distribution along a dimension, such as familiarity.The application of Signal Detection Theory (SDT) to memory depends on conceiving of a memory trace as a signal that the subject must detect in order to perform in a retention task. Given this conception of memory performance, {{it is reasonable to assume}} that percentage correct scores may be biased indicators of retention—just as thresholds may be biased indicators of sensory performance—and, in addition, that SDT techniques should be used where possible to separate the truly retention-based aspects of memory performance from the decision aspects. In particular, we assume that the subject compares the trace strength of the test item with a criterion, responding [...] "yes" [...] if the strength exceeds the criterion and [...] "no"otherwise. There are two types of test items, [...] "old" [...] (a test item that appeared in the list for that trial) and new" [...] (one that did not appear in the list). Strength theory assumes that there may be noise in the value of the trace strength, the location of the criterion, or both. We assume that this noise is normally distributed. The reporting criterion can shift along the continuum in the direction of more <b>false</b> <b>hits,</b> or more misses. The momentary memory strength of a test item is compared with the decision criteria and if the strength of the item falls within the judgment category, Jt, defined by the placement of the criteria, S makes judgment. The strength of an item is assumed to decline monotonically (with some error variance) as a continuous function of time or number of intervening items. <b>False</b> <b>hits</b> are 'new' words incorrectly recognized as old, and a greater proportion of these represents a liberal bias. [...] Misses are 'old' words mistakenly not recognized as old, and a greater proportion of these represents a conservative bias. [...] The relative distributions of <b>false</b> <b>hits</b> and misses can be used to interpret recognition task performance and correct for guessing. Only target items can generate an above-threshold recognition response because only they appeared on the list. The lures, along with any targets that are forgotten, fall below threshold, which means that they generate no memory signal whatsoever. False alarms in this model reflect memory-free guesses that are made to some of the lures.|$|R
40|$|We {{present a}} method to assess the {{reliability}} of local structure prediction from sequence. We introduce a greedy algorithm for filtering and enrichment of dynamic fragment libraries, compiled with remote-homology detection methods such as HHfrag. After filtering <b>false</b> <b>hits</b> at each target position, we reduce the fragment library to a minimal set of representative fragments, which are guaranteed to have correct local structure in regions of detectable conservation. We demonstrate that the location of conserved motifs in a protein sequence can be predicted by examining the recurrence and structural homogeneity of detected fragments. The resulting confidence score correlates with the local RMSD of the representative fragments and allows us to predict torsion angles from sequence with better accuracy compared to existing machine learning methods...|$|R
50|$|However, Shift JIS has {{the unfortunate}} {{property}} that it often breaks any parser (software that reads the coded text) {{that is not}} specifically designed to handle it. For example, a text search method can get <b>false</b> <b>hits</b> {{if it is not}} designed for Shift JIS. EUC, on the other hand, is handled much better by parsers that have been written for 7-bit ASCII (and thus EUC encodings are used on UNIX, where much of the file-handling code was historically only written for English encodings). But EUC is not backwards compatible with JIS X 0201, the first main Japanese encoding. Further complications arise because the original Internet e-mail standards only support 7-bit transfer protocols. Thus JIS encoding was developed for sending and receiving e-mails.|$|R
5000|$|Histogram {{differences}} (HD). Histogram {{differences is}} very similar to Sum of absolute differences. The difference is that HD computes the difference between the histograms of two consecutive frames; a histogram is a table that contains for each color within a frame the number of pixels that are shaded in that color. HD is not as sensitive to minor changes within a scene as SAD and thus produces less <b>false</b> <b>hits.</b> One major problem of HD is that two images can have exactly the same histograms while the shown content differs extremely, e. g. a picture of the sea and a beach can have the same histogram as one of a corn field and the sky. HD offers no guarantee that it recognizes hard cuts.|$|R
5000|$|Sum of {{absolute}} differences (SAD). This {{is both the}} most obvious and most simple algorithm of all: The two consecutive frames are compared pixel by pixel, summing up the absolute values of the differences of each two corresponding pixels. The result is a positive number that is used as the score. SAD reacts very sensitively to even minor changes within a scene: fast movements of the camera, explosions or the simple switching on of a light in a previously dark scene result in <b>false</b> <b>hits.</b> On the other hand, SAD hardly reacts to soft cuts at all. Yet, SAD is used often to produce a basic set of [...] "possible hits" [...] as it detects all visible hard cuts with utmost probability.|$|R
40|$|AbstractPhage display {{screenings}} {{are frequently}} employed to identify high-affinity peptides or antibodies. Although successful, phage display is a laborious technology and is notorious for identification of <b>false</b> positive <b>hits.</b> To accelerate {{and improve the}} selection process, we have employed Illumina next generation sequencing to deeply characterize the Ph. D. - 7 M 13 peptide phage display library before and after several rounds of biopanning on KS 483 osteoblast cells. Sequencing of the naive library after one round of amplification in bacteria identifies propagation advantage as {{an important source of}} <b>false</b> positive <b>hits.</b> Most important, our data show that deep sequencing of the phage pool after a first round of biopanning is already sufficient to identify positive phages. Whereas traditional sequencing of a limited number of clones after one or two rounds of selection is uninformative, the required additional rounds of biopanning are associated with the risk of losing promising clones propagating slower than nonbinding phages. Confocal and live cell imaging confirms that our screen successfully selected a peptide with very high binding and uptake in osteoblasts. We conclude that next generation sequencing can significantly empower phage display screenings by accelerating the finding of specific binders and restraining the number of <b>false</b> positive <b>hits...</b>|$|R
40|$|Abstract—In {{screening}} X-ray mammography {{two different}} views are captured of both breasts. First the individual images are analysed independently {{looking for signs}} of cancer, but due to the high <b>false</b> positive <b>hits</b> good result can be achieved only by joint analysis of the images. One such approach is based upon the experience that masses and calcifications emerge on both views; so if no matching pair is found, the given object is a <b>false</b> positive <b>hit.</b> Since 3 D correspondence is theoretically impossible, a “ 2. 5 D ” reference system (based on the line parallel to the pectoral muscle and placed in the nipple) is evolved to find corresponding regions on the two images – matching a stripe on one view of a breast to any segment on another view of the same breast. Since masses have a distinctive texture, further examination is possible within this stripe. Using intensity, co-occurrence and GLD (“gray level differences”) based texture features our algorithm performed 23 % loss of <b>false</b> negative <b>hits</b> keeping 93 % of true positive ones. These features are characteristic for usual masses but not for spiculated (“stellar”) ones therefore adding such features are needed. Kegelmayer et al. suggested using the ALOE (“analysis of local oriented edges”) texture feature that is {{based on the fact that}} linear structures have the same direction in normal tissues but significantly vary in speculated ones (due to their stellar shape) hence variance of the orientation histogram is distinctive for them. Since classical orientation estimation methods based on differential filtering (eg. Prewitt, Sobel or filtering with Gauss derivatives) result in noise for flat areas and do not provide information far from edges, the use of EdgeFlow for ALOE is more than a rational choice. Adding this feature the loss of <b>false</b> positive <b>hits</b> increased to 31 % while still keeping 92 % of true positive ones. I...|$|R
40|$|We {{describe}} {{a novel approach}} to precise searching in the full content of digital libraries. The Searchbench (for search workbench) is based on sentence-wise syntactic and semantic natural language processing (NLP) of both born-digital and scanned publications in PDF format. The term born-digital means natively digital, i. e. prepared electronically using typesetting systems such as LaTeX, OpenOffice, and the like. In the Searchbench, queries can be formulated as (possibly underspecified) statements, consisting of simple subject-predicate-object constructs such as ‘algorithm improves word alignment’. This reduces the number of <b>false</b> <b>hits</b> in large document collections when the search words happen to appear close to each other, but are not semantically related. The method also abstracts from passive voice and predicate syn-onyms. Moreover, negated statements can be excluded from the search results, and negated antonym predicates again count as synonyms (e. g. not include = exclude) ...|$|R
