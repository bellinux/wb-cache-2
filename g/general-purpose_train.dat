1|15|Public
5000|$|Hitachi Rail is {{the rolling}} stock {{manufacturing}} division of Hitachi. It and Mitsubishi Heavy Industries agreed to cooperate {{in the field of}} international intra-city railway systems in 2010. Hitachi markets a <b>general-purpose</b> <b>train</b> known as the [...] "A-train", which utilises double-skin, friction-stir-welded aluminium body construction. The A-train concept can be customised to form different types of trains, ranging from high-capacity commuter and metro trains (as in the automated 3000 series train for the Nanakuma Line of the Fukuoka City Subway) to limited express (as in the E257 series jointly produced with Tokyu Corporation) and high-speed trains (as in the Class 395 trains for Southeastern in the UK). They have made such trains for domestic and international operators alike. Among its most significant orders was the winning tender for British Rail's Intercity Express Programme in June 2008.|$|E
40|$|We {{present a}} system for 3 D {{planning}} and pre-operative rehearsal of mandibular distraction osteogenesis procedures. We describe two primary architectural components: a planning system that allows geometric bone manipulation to rapidly explore various modifications and configurations, and a visuohaptic simulator that allows both <b>general-purpose</b> <b>training</b> and preoperative, patient-specific procedure rehearsal. We provide relevant clinical background, then we describe the underlying simulation algorithms and their application to craniofacial procedures. Key Words: surgical simulation, surgical planning, craniofacial surgery, distraction osteogenesi...|$|R
50|$|The Curtiss-Wright CW-22 was a 1940s American <b>general-purpose</b> {{advanced}} <b>training</b> monoplane aircraft {{built by}} the Curtiss-Wright Corporation. It was operated by the United States Navy as a scout trainer with the designation SNC-1 Falcon.|$|R
50|$|It {{was decided}} to opt for a large first batch, totalling 136 aircraft, as this allowed for the {{implementation}} of more economical flow-line production at Airspeed's Portsmouth factory. On 19 June 1937, the first prototype Oxford, L4534, conducted its first flight at Portsmouth. Initially, two variants were planned; the Mark I, which was viewed as a <b>general-purpose</b> <b>training</b> aircraft equipped with a dorsal gun turret, and the Mark II, which lacked any turret but was instead fitted with dual controls. As further large contracts for the aircraft were placed with Airspeed, (100 Mk Is and 100 Mk IIs) it was arranged that de Havilland Aircraft would build them at Hatfield later, to meet the demands for Oxfords for training. Other companies also manufactured the aircraft.|$|R
2500|$|In {{the end it}} {{was decided}} to abandon the 440 in favour of the cheaper Latécoère 290, another militarised version of the Latécoère 280 transport. [...] The fate of the first 440 is not known, but the second machine {{remained}} in use at St-Raphael until at least 1934, serving as a <b>general-purpose</b> and <b>training</b> aircraft.|$|R
40|$|An under-explored {{question}} in cross-language information retrieval (CLIR) is {{to what degree}} the performance of CLIR methods depends {{on the availability of}} high-quality translation resources for particular domains. To address this issue, we evaluate several competitive CLIR methods - with different training corpora - on test documents in the medical domain. Our results show severe performance degradation when using a <b>general-purpose</b> <b>training</b> corpus or a commercial machine translation system (SYSTRAN), versus a domain-specific training corpus. A related unexplored question is whether we can improve CLIR performance by systematically analyzing training resources and optimally matching them to target collections. We start exploring this problem by suggesting a simple criterion for automatically matching training resources to target corpora. By using cosine similarity between training and target corpora as resource weights we obtained an average of 5. 6 % improvement over using all resources with no weights. The same metric yields 99. 4 % of the performance obtained when an oracle chooses the optimal resource every time...|$|R
40|$|Are {{temporary}} jobs a port {{of entry}} into permanent employment? In this paper we argue that the answer crucially depends {{on the type of}} temporary contracts being considered, as the different contracts observed in practice are typically characterized by varying combinations of training, tax-incentives and EPL provisions. We base our empirical evidence on a longitudinal sample of labour market entrants in Italy, a country where a large number of temporary contracts coexist with a relatively high employment protection for standard employees. We estimate dynamic multinomial logit models with fixed effects, to allow for non-random sorting of workers into the different types of contracts. We show that the transition to permanent employment is more likely for individuals holding any type of temporary contracts than for the unemployed, thus broadly confirming the existence of port-of-entry effects. Yet, not all temporary contracts are the same: training contracts are the best {{port of entry}}, while freelance contracts are the worst. We also show that temporary contracts are generally a port-of-entry into a permanent position within the same employer, but not across firms, implying that little <b>general-purpose</b> <b>training</b> is gained while on temporary jobs. Moreover, the time needed for an internal transformation from a temporary to a permanent position appears rather long, suggesting that firms are likely to use (a sequence of) temporary contracts as a cost-reduction strategy, rather than as a screening device for newly hired workers. temporary jobs, port of entry, matched employer-employee data, dynamic multinomial logit models, state dependence, fixed effects...|$|R
5000|$|Like {{his other}} designs, Edgardo Ciani's Urendo {{was built by}} Sezioni Sperimentale di Volo a Vela (SSVV) of Milan, an {{offshoot}} of the Aeroclub Volovelistio Milanese. It was intended as a <b>general-purpose</b> (including <b>training),</b> low-cost aircraft, and the necessary simplicity of its construction was apparent in its flat-sided fuselage. It is said that when Ciani saw it in the air for the first time, he exclaimed [...] "Ma l'é propri Urend!" [...] ("By God, it's really horrible!" [...] in the Milanese dialect) and named it Urendo (horrendous). Nonetheless, it has an efficient laminar flow wing, and, though its thermalling is limited with two aboard by a high wing loading, it flies well cross-country as a single-seater.|$|R
40|$|We {{present a}} <b>general-purpose</b> method to <b>train</b> Markov chain Monte Carlo kernels, {{parameterized}} by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, {{a proxy for}} mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106 x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. We release an open source TensorFlow implementation of the algorithm...|$|R
40|$|Unsupervised Hebbian {{learning}} {{produces a}} scale invariant associative phenomena with limited computational power (the linear separation of states). With {{the inclusion of}} a Liquid State Machine (LSM), simple associative methods are transformed to provide truly general purpose and computationally powerful ontogenetic learning. This radical expansion of the representational space of a problem enables solutions by computationally primitive means. This not only supercharges unsupervised techniques but also leads to cognitive penetrability and successful psychological explanation. Demonstrated on an embodied robot the approach displays cumulative <b>general-purpose</b> learning and <b>training</b> through human interaction leading to multiple psychological phenomena, sequence learning, navigation, and a strong resilience to catastrophic forgetting...|$|R
40|$|Machine {{learning}} and statistics with very large datasets {{is now a}} topic of widespread interest, both in academia and industry. Many such tasks can be posed as convex optimization problems, so algorithms for distributed convex optimization serve as a powerful, <b>general-purpose</b> mechanism for <b>training</b> a wide class of models on datasets too large to process on a single machine. In previous work, {{it has been shown}} how to solve such problems {{in such a way that}} each machine only looks at either a subset of training examples or a subset of features. In this paper, we extend these algorithms by showing how to split problems by both examples and features simultaneously, which is necessary to deal with datasets that are very large in both dimensions. We present some experiments with these algorithms run on Amazon’s Elastic Compute Cloud. ...|$|R
40|$|We {{demonstrate}} a hybrid neuromorphic learning paradigm that learns complex sensorimotor mappings {{based on a}} small set of hard-coded reflex behaviours. A mobile robot is first controlled by a basic set of reflexive hand-designed behaviours. All sensor data is provided via a spike-based silicon retina camera (eDVS), and all control is implemented via spiking neurons simulated on neuromorphic hardware (SpiNNaker). Given this control system, the robot is capable of simple obstacle avoidance and random exploration. To train the robot to perform more complex tasks, we observe the robot and find instances where he robot accidentally performs the desired action. Data recorded from the robot during these times is then used to update the neural control system, increasing {{the likelihood of the}} robot performing that task in the future, given a similar sensor state. As an example application of this <b>general-purpose</b> method of <b>training,</b> we demonstrate the robot learning to respond to novel sensory stimuli (a mirror) by turning right if it is present at an intersection, and otherwise turning left. In general, this system can learn arbitrary relations between sensory input and motor behaviour...|$|R
40|$|Reinforcement {{learning}} {{holds the}} promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise {{the autonomy of the}} learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by <b>training</b> <b>general-purpose</b> neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3 D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3 D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations...|$|R
40|$|Autonomous {{learning}} of robotic skills can allow general-purpose robots to learn wide behavioral repertoires without requiring extensive manual engineering. However, robotic skill learning methods typically {{make one of}} several trade-offs to enable practical real-world learning, such as requiring manually designed policy or value function representations, initialization from human-provided demonstrations, instrumentation of the training environment, or extremely long training times. In this paper, we propose a new reinforcement learning algorithm for learning manipulation skills that can <b>train</b> <b>general-purpose</b> neural network policies with minimal human engineering, while still allowing for fast, efficient learning in stochastic environments. Our approach builds on the guided policy search (GPS) algorithm, which transforms the reinforcement learning problem into supervised learning from a computational teacher (without human demonstrations). In contrast to prior GPS methods, which require a consistent set of initial states to which the system must be reset after each episode, our approach can handle randomized initial states, allowing it {{to be used in}} environments where deterministic resets are impossible. We compare our method to existing policy search techniques in simulation, showing that it can train high-dimensional neural network policies with the same sample efficiency as prior GPS methods, and present real-world results on a PR 2 robotic manipulator...|$|R
40|$|Machine {{translation}} underwent huge improvements {{since the}} groundbreaking introduction of statistical methods {{in the early}} 2000 s, going from very domain-specific systems that still performed relatively poorly despite the painstakingly crafting of thousands of ad-hoc rules, to <b>general-purpose</b> systems automatically <b>trained</b> on large collections of bilingual texts which manage to deliver understandable translations that convey the general meaning of the original input. These approaches however still perform quite {{below the level of}} human translators, typically failing to convey detailed meaning and register, and producing translations that, while readable, are often ungrammatical and unidiomatic. This quality gap, which is considerably large compared to most other natural language processing tasks, {{has been the focus of}} the research in recent years, with the development of increasingly sophisticated models that attempt to exploit the syntactical structure of human languages, leveraging the technology of statistical parsers, as well as advanced machine learning methods such as marging-based structured prediction algorithms and neural networks. The translation software itself became more complex in order to accommodate for the sophistication of these advanced models: the main translation engine (the decoder) is now often combined with a pre-processor which reorders the words of the source sentences to a target language word order, or with a post-processor that ranks and selects a translation according according to fine model from a list of candidate translations generated by a coarse model. In this thesis we investigate the statistical machine translation problem from various angles, focusing on translation from non-analytic languages whose syntax is best described by fluid non-projective dependency grammars rather than the relatively strict phrase-structure grammars or projectivedependency grammars which are most commonly used in the literature. We propose a framework for modeling word reordering phenomena between language pairs as transitions on non-projective source dependency parse graphs. We quantitatively characterize reordering phenomena for the German-to-English language pair as captured by this framework, specifically investigating the incidence and effects of the non-projectivity of source syntax and the non-locality of word movement w. r. t. the graph structure. We evaluated several variants of hand-coded pre-ordering rules in order to assess the impact of these phenomena on translation quality. We propose a class of dependency-based source pre-ordering approaches that reorder sentences based on a flexible models trained by SVMs and and several recurrent neural network architectures. We also propose a class of translation reranking models, both syntax-free and source dependency-based, which make use of a type of neural networks known as graph echo state networks which is highly flexible and requires extremely little training resources, overcoming one of the main limitations of neural network models for natural language processing tasks...|$|R

