19|24|Public
5000|$|His {{aggressive}} baseline game is {{coupled with}} an extremely powerful serve, often reaching the upper 130's and low 140's in mph; however, Gulbis has struggled with his second serve, and often has high double <b>fault</b> <b>statistics</b> across the ATP leaderboard. Other weaknesses are his consistency and his nerves; despite a strong serve and return, Gulbis has poor break point conversion, break point save and tie-break statistics. On numerous occasions he has had strings of uncharacteristic unforced errors when serving or returning for sets or matches, as well as hitting numerous [...] "easy" [...] slams well out in tough situations, something he has become known for and often jokes about in interviews. However, to his credit, {{he has a good}} record in ATP finals, winning all 6 of the 6 finals he has been in for ATP events.|$|E
40|$|The paper {{gives an}} {{overview}} over a LCC risk management approach suitable for power system asset management. The approach integrates relevant management aspects into a consistent decision support platform combining power system simulations with LCC analysis. Paper gives some {{examples from the}} use of the approach on power transformer management. A practical problem in the transformer examples in the paper was that the link between state indicators and <b>fault</b> <b>statistics</b> is yet not very well substantiated. The experience of the methodology is though encouraging and should inspire for more research in this field...|$|E
40|$|Bachelor {{thesis is}} focused on the {{evaluation}} of the number and severity of defects in the sewer network in small towns in the South Moravian region. At the beginning of the work I got to know appropriate legislation and clarification of basic terms. <b>Fault</b> <b>statistics</b> and other indicators of the technical condition of the sewer network were processed from conducted camera exploration. These data are processed by several factors, such as material, size profiles, sewer age and length of the sewer network. Data were processed in Microsoft Office Excel. We found, there is mostly concrete pipe in small towns, which is old. Based on the data will be necessary keep wrecking objects and sections cleaned or repaired...|$|E
5000|$|Object-oriented {{hierarchy}} of monitoring information types such as events, <b>fault,</b> and <b>statistics</b> ...|$|R
40|$|Emission {{of carbon}} dioxide from fossil-fueled power {{generation}} stations contributes to global climate change. Storage of this carbon dioxide within the pores of geologic strata (geologic carbon storage) is one approach to mitigating the climate change that would otherwise occur. The large storage volume needed for this mitigation requires injection into brine-filled pore space in reservoir strata overlain by cap rocks. One of the main concerns of storage in such rocks is leakage via faults. In {{the early stages of}} site selection, site-specific fault coverages are often not available. This necessitates a method for using available fault data to develop an estimate of the likelihood of injected carbon dioxide encountering and migrating up a fault, primarily due to buoyancy. <b>Fault</b> population <b>statistics</b> provide one of the main inputs to calculate the encounter probability. Previous <b>fault</b> population <b>statistics</b> work is shown to be applicable to areal <b>fault</b> density <b>statistics.</b> This result is applied to a case study in the southern portion of the San Joaquin Basin with the result that the probability of a carbon dioxide plume from a previously planned injection had a 3 % chance of encountering a fully seal offsetting fault...|$|R
40|$|Abstract-Detectability of {{the sensor}} fault {{detection}} {{system is the}} basic criteria for selecting of different <b>fault</b> detection <b>statistics.</b> and in MPCA are compared with. The complementary among them is analyzed qualitatively. Results of power plant data simulation show that and in MPCA can improve sensor fault detection, but is still better for some sensors. Combination of, and can improve sensor fault detectability of PCA-based fault detection system. Keywords-process monitoring, fault detection, Principal component analysis I...|$|R
40|$|The S-band linear {{accelerator}} (linac), {{which was built}} to be the electron source and {{the front end of}} the Advanced Photon Source (APS) injector, is now also being used to support a low-energy undulator test line (LEUTL) to drive a free-electron laser (FEL). The APS linac system employs five units of pulsed high-power klystrons (35 -MW class) as the main rf sources. The matching pulse modulators provide high-voltage pulses running at 280 kV and 300 A with 3. 5 -µs pulse width and a nominal pulse repetition rate of 30 Hz. The system availability of the entire APS linac during the last run period of calendar year 2000 was estimated to be slightly over 95 %. We present a discussion of the reliability and various <b>fault</b> <b>statistics</b> of the klystron-modulator system together with the key features of the system hardware. ...|$|E
40|$|Can C be safe? A {{survey and}} {{introduction}} to safer C Is {{it possible to}} achieve a safer software in C by limiting its use? As the C programming language still is very much alive, especially within the embedded and real-time community- much effort has been put into the development of such standards. There are many well known problems in the ANSI/ISO C language specification with or without measurements of <b>fault</b> <b>statistics.</b> And there also exists several controversy's about some constructs – {{such as the use}} of the goto statement. Another important topic is the security of systems. Buffer overflow exploits is one of the most common security vulnerabilities for software written in C. There are several approaches with varying success to find these faults; for an example on-line memory management (e. g. garbage collection) or off-line static analysis. This paper focuses on the field in general and will take a closer look on a few specifications that are software safety related. ...|$|E
40|$|Object-based {{classification}} is {{a promising}} technique for image classification. Unlike pixel-based methods, which only use the measured radiometric values, the object-based techniques {{can also use}} shape and context information of scene textures. These extra degrees of freedom provided by the objects allow the automatic identification of geological structures. In this article, we present an evaluation of object-based classification {{in the context of}} extraction of geological faults. Digital elevation models and radar data of an area near Lake Magadi (Kenya) have been processed. We then determine the statistics of the fault populations. The fractal dimensions of fault dimensions are similar to fractal dimensions directly measured on remote sensing images of the study area using power spectra (PSD) and variograms. These methods allow unbiased statistics of faults and help us to understand the evolution of the fault systems in extensional domains. Furthermore, the direct analysis of image texture is a good indicator of the <b>fault</b> <b>statistics</b> and allows us to classify the intensity and type of deformation. We propose that extensional fault networks can be modeled by iterative function system (IFS) ...|$|E
40|$|Erroneous {{measurements}} in multisensor navigation systems must {{be detected}} and isolated. A recursive estimator can find fast growing errors; a least squares batch estimator can find slow growing errors. This process is called fault detection. A protection radius {{can be calculated}} {{as a function of}} time for a given location. This protection radius can be used to guarantee the integrity of the navigation data. Fault isolation can be accomplished using either a snapshot method or by examining the history of the <b>fault</b> detection <b>statistics...</b>|$|R
30|$|Because most of {{the subway}} {{vehicles}} system life distribution data is censored data, we use the survival analysis in the system time between failures to process censored data. In the <b>fault</b> data <b>statistics,</b> censored data mainly includes two categories. One kind is interval-censored data, if the maintenance work is reliable and the failure occurs between the overhaul and the last overhaul, so fault time is an interval, uncertain value, and fault specific time unknown. One kind is the right censored data, statistical period of {{the beginning and the}} end will have censored data, and fault time is greater than a certain value of tracked. We use common failure distribution function on censored data for maximum likelihood method of parameter estimation to calculate A-D statistics to select fitting of better distribution function.|$|R
40|$|Leakage of CO{sub 2 } {{out of the}} {{designated}} storage region via faults is a widely recognized concern for geologic carbon sequestration. The probability of such leakage can be separated into {{the probability of a}} plume encountering a fault and the probability of flow along such a fault. In the absence of deterministic fault location information, the first probability can be calculated from regional <b>fault</b> population <b>statistics</b> and modeling of the plume shape and size. In this study, fault statistical parameters were measured or estimated for WESTCARB's Phase III pilot test injection in the San Joaquin Valley, California. Combining CO{sub 2 } plume model predictions with estimated fault characteristics resulted in a 3 % probability that the CO{sub 2 } plume will encounter a fault fully offsetting the 180 m (590 ft) thick seal. The probability of leakage is lower, likely much lower, as faults with this offset are probably low-permeability features in this area...|$|R
40|$|Fault-tolerant {{control of}} {{networked}} control systems (NCSs) with random actuator failure is studied. An innovative model is presented for this problem. It includes three sources of uncertainties, namely uncertainties {{in the plant}} model, uncertainties in networked communications and uncertainties in possible actuator failure/malfunction. Other main features are: (i) the <b>fault</b> <b>statistics</b> of each actuator is individually quantified, and (ii) a united framework is proposed to have logic zero-order-holders embedded in the NCS. The latter enables actuators - when in normal operation - to use the latest actuating signals available to them. Based on the Lyapunov-Krasovskii functional, three theorems are proved in the study for the system stability and controller design. Theorem 1 gives a matrix inequality for the system asymptotical stability in the mean-square and {{is the foundation of}} the other two theorems. Theorem 2 shows a stability condition regarding the design of a robust state-feedback control for the system under study. Finally, Theorem 3 gives a modified stability condition that can be employed for actual design. A numerical example is presented to show how such a robust controller can be designed...|$|E
40|$|In {{the paper}} the latest {{statistics}} of middle voltage network power line failures and causes of these failures are analyzed. The paper {{is based on}} the analysis of the Latvian statistical data, but these statistical data are also compared with other countries' statistics, which is quite similar in most countries. It means that information included in the paper can be widely used in any middle voltage electricity distribution network reliability calculations and analysis all over the world. In the paper possibilities to reduce number of failures using different methods are presented and also compared quality of energy distributed through the network for two cases – when network is formed by overhead or cable lines is also compared. This paper presents possibility to improve reliability of distribution network using underground cables instead of OHL and also demonstrates that the perception that cable lines are more expensive than overhead lines is wrong. Conclusions {{made on the basis of}} OHL and cable line <b>fault</b> <b>statistics</b> analysis are of high importance for planning of reliability centred maintenance...|$|E
40|$|Steam-induced {{vibration}} {{is a kind}} of {{self-excited vibration}} {{which is one of the}} most important issues affecting the operation reliability of steam turbines. A lot of actual cases of steam-induced vibration of steam turbine were collected in this work. And steam-induced vibration fault was statistical and classified by fault reason, happened time, occurrence load and fault severity. The statistical results show that steam-induced vibration usually occurs after turbine runs one year later and turbine is with a high load. Oil temperature change is the main running parameters which affect steam-induced vibration. Improper opening and order of adjustment valve is the most important factor causing steam-induced vibration fault. Failure Mode and Effect Analysis(FMEA) was an analysis method faced to system specific physical unit. Based on the FMEA method and the <b>fault</b> <b>statistics</b> results, fault mode of steam-induced vibration was analyzed in detail. And the fault tree analysis of steam-induced vibration was constructed. Some valuable conclusions were achieved, including the fault symptoms, fault consequence and effect, fault causes, preventive actions and other information of steam-induced vibration...|$|E
40|$|Abstract [...] This paper {{summarizes}} {{the application of}} statistical algorithms for fault location on power transmission lines. The proposed fault location algorithms utilize statistical information about the undefined parameters such as equivalent impedances of the system at the unmonitored end of the transmission line, or those containing random errors. Knowledge about the distribution of these values results in more accurate fault location for lines with grounded neutral, especially in case of distant short circuits through a large transient resistance. The proposed algorithms are based on modelling of the faulted line and the Monte-Carlo method. The algorithms calculate not only the expected value of {{the distance to the}} fault, but also another important additional characteristic for the fault location, namely, the length of the line segment, where the short circuit could have occurred. The algorithm retains its applicability in cases of all the simple fault types and needs minor modifications for conditions when one breaker is opened and a single-phase fault is sustained. T Index Terms [...] <b>Fault</b> Location, <b>Statistics,</b> Transmission Line...|$|R
40|$|One of {{the main}} {{concerns}} of storage in saline aquifers is leakage via faults. In {{the early stages of}} site selection, site-specific fault coverages are often not available for these aquifers. This necessitates a method using available fault data to estimate the probability of injected carbon dioxide encountering and migrating up a fault. The probability of encounter can be calculated from areal <b>fault</b> density <b>statistics</b> from available data, and carbon dioxide plume dimensions from numerical simulation. Given a number of assumptions, the dimension of the plume perpendicular to a fault times the areal density of faults with offsets greater than some threshold of interest provides probability of the plume encountering such a fault. Application of this result to a previously planned large-scale pilot injection in the southern portion of the San Joaquin Basin yielded a 3 % and 7 % chance of the plume encountering a fully and half seal offsetting fault, respectively. Subsequently available data indicated a half seal-offsetting fault at a distance from the injection well that implied a 20 % probability of encounter for a plume sufficiently large to reach it...|$|R
40|$|Global Satellite Navigation Systems (GNSS) {{have been}} widely used for positioning, {{navigation}} and timing (PNT). Therefore, the integrity of the satellite based navigation systems has been a major concern for many liability critical applications, such as civil aviation, and location-based services (LBS). Over the past two decades, GNSS Receiver Autonomous Integrity Monitoring (RAIM) procedures have been developed, but the efficiency of such procedures is highly dependent on measurement redundancy and geometric strength within the GNSS positioning solutions. Reliability of a PNT system can be measured by, not only the wellknown Minimal Detectable Biases (MDBs), but also the recently derived Minimal Separable Biases (MSBs) for the measurements. While the previous research has shown that the MSBs are directly related to the correlations between the faulty measurement detection statistics, a comprehensive analysis for such correlations between fault (or outlier) detection statistics is still lacking, even for commonly used GNSS/INS integration scenarios. In this research, we have demonstrated that with the aid of inertial sensors, even with low-cost MEMS sensors, the MDBs and correlation coefficients between the measurement <b>fault</b> detection <b>statistics</b> can be significantly reduced, thus improving the separability of faults in GNSS measurements...|$|R
40|$|Power system {{security}} {{is defined as}} the ability of a power system to maintain supply without unduly allowing network variables to stray from prescribed ranges. Traditionally, security has been assessed using deterministic criteria e. g. `N- 1 ' or `N- 2 ' under prescribed severe system loading levels. However, such worst-case deterministic approach does not provide explicitly an assessment of the probability of failure of the system, and the likelihood of the outages is treated equally. This approach may result in either over or under estimation of system planning reinforcement requirements and, hence, a corresponding excessive or insufficient {{system security}}. On the other hand, probabilistic security assessment may offer advantages by considering (i) a statistical description of the performance of the system over an annual cycle together with (ii) the application of historical <b>fault</b> <b>statistics</b> that provide a measure of the probability of faults leading to systems outages. This paper reviews approaches to probabilistic security assessment. Such approaches include (i) identifying various security indices, (ii) reviewing different techniques, (iv) utilizing Monte Carlo simulation, and (iv) integrating deterministic & probabilistic techniques. The application of probabilistic security assessment in real-time operation and system planning is further considered in detail. Preliminary application of the security assessment is presented to the Dubai Electricity and Water Authority (DEWA) transmission network...|$|E
40|$|Rainier Mesa (RM) is a tuffaceous, high-elevation plateau on the Nevada Test Site (NTS) {{that has}} been {{subjected}} to numerous nuclear tests between 1957 and 1992. Unlike other tests on the NTS located within or just above the saturated zone, tests at the RM T-tunnel complex were conducted within a variably saturated sequence of bedded and non-welded vitric and zeolitized tuff units, located approximately 500 m above the regional groundwater flow system. The low permeability and high porosity of the underlying zeolitized tuff units suggest the downward transport of radionuclides released from these tests are minimal through the tuff matrix. However, numerous faults observed to discharge water into tunnel drifts may serve as preferential pathways for radionuclide migration. Data collected from tunnel drifts indicate that faulting within the zeolitized tuff units is sparse with fractal clustering, and that connectivity between adjacent fault clusters is often weak to non-existent. The sparse fault density at RM, in conjunction with the extreme variability in the spatial distribution of faults, poses challenges not readily addressed by existing upscaling methods that upscale fracture properties as equivalent grid tensors. The unique <b>fault</b> <b>statistics</b> at RM has {{led to the development of}} a fracture continuum method designed to faithfully preserve flow and transport properties of the sparse fault networks. This method is based on selective mapping and upscaling of fault hydraulic and transport properties onto a continuum grid in support of dual-permeability simulations. Comparisons of global flow and random walk particle breakthrough between two-dimensional discrete fracture network and fracture continuum simulations demonstrate the utility of this method...|$|E
40|$|The Strelley- 1 well {{encountered}} a minimum 27 m of hydrocarbon column in fractured Dinantian Limestone in the East Midlands, UK. Fault population statistics and a first-pass elastic dislocation forward {{model of the}} limestone reservoir in the Strelley up-dip structure has been created. These indicate that the optimum trajectory of a planned appraisal / development well to intersect tensional fracture should penetrate the reservoir {{near the top of}} the structure and then track sub-horizontally down the flank of the structure in a southeasterly direction towards the discovery well. Seismic interpretation of 2 D lines along sometimes very winding English country roads was aided by using a sophisticated seismic interpretation package that displays seismic panels in true perspective from any view point. The seismic image is often poor, but the use of <b>fault</b> <b>statistics</b> and mapping of wall-rock shear and longitudinal strains helped constrain the 3 D structural model. The top reservoir horizon in this 3 D model was then forward modelled for displacement on fault surfaces within the up-dip Strelley structure. Using elastic dislocation theory, prediction of fracture type and orientation were then modelled by assuming that fracturing within the limestone was predominantly caused by Variscan rifting/extension. This model predicts that high palaeo-shear stresses and resulting shear plane failure are limited to within 400 m of the footwalls of the main bounding faults of the structure. Minor faults nearby to the well have very little influence over fracture type and orientations. Low paleo-shear stresses and associated tensile fractures occur in an orthogonal orientation to a projected sub-horizontal trajectory of a well, planned over a 2 km interval, between the crest of th...|$|E
40|$|Abstract—The {{transducer}} {{process of}} a sensor is interference-prone to environmental conditions or external disturbances depending on sensor type, measurement procedure etc. Dependable sensors {{are characterized by}} a broad independence of those factors or/and they can both detect situations that make a correct measurement impossible and validate the measurement result. In this paper we describe a statistical approach {{for the detection of}} faulty measurements caused by external disturbances. Our fault detection algorithm is based on a comparison of faultless reference measurements with current sensing values. Using this enhancement, a sensor becomes a real smart sensing device and supplies an additional validity estimation of each measurement. The approach was implemented and validated in a demonstration setup that integrates an infrared sensor array disturbed by a strong extraneous light. Index Terms—disturbed sensor measurements, <b>fault</b> detection, non-parametric <b>statistics,</b> smart sensor I...|$|R
40|$|An {{improved}} {{approach of}} fault detection for chiller sensors is presented {{based on the}} sensitivity analysis for the original data set used to train the Principal Component Analysis (PCA) model. Sensor faults are inevitable due to the aging, environment, location and so on. Meanwhile, because of {{the wide range of}} operational conditions, the fault of a certain sensor is very difficult to be directly detected by its own historical data. PCA is a multivariate data-based statistical analysis method and it is very useful for the sensor fault detection in HVAC 2 ̆ 6 R. The undetectable zone of a certain sensor by Q-statistic is derived from the definition of Q-statistic which is usually employed as a boundary to detect the sensor fault situation. Due to the similar style between Q-statistic and Hawkinsâ€™ TH 2, the undetectable zone by Hawkinsâ€™ TH 2 is also obtained. Undetectable zone is a predictive index to indicate the detectability of different sensors by different statistics. Since undetectable zone is the character of the original training data set, it can indicate the quality for the selected training data. One field data set is employed to validate the presented approach. Results show that the undetectable zone of a certain sensor by Q-statistic is quite different from that by Hawkinsâ€™ TH 2. Therefore, the undetectable zone can be used to improving the performance of PCA-based chiller sensor fault detection by choosing different <b>fault</b> detection <b>statistics</b> with less undetectable zone for different sensor...|$|R
30|$|The {{spread in}} {{results could be}} traced both to a large {{variability}} in event frequencies used, as well as consequence modelling. A large number of assumptions {{must be made to}} narrow down the infinite amount of scenarios to a manageable and understandable set that can be modelled. A multidisciplinary and collective procedure is recommended for the hazard identification phase to yield a more complete picture as this is a critical step in the analysis. Comparing the frequencies obtained from <b>fault</b> trees and <b>statistics</b> suggests that the technique of using fault trees to obtain failure frequencies is neither robust nor accurate. Even though the same model is used, the result could widely differ because the models were used differently. The authors conclude that transparency in terms of all the assumptions that are introduced in all steps of the risk analysis must be explained together with the result as they are strongly dependent (Contini et al. 1991).|$|R
40|$|This thesis {{examines}} how the unavailability for OKG {{will be affected}} by the planned replacement of the 400 kV substation at Simpevarp. The evaluation has been based on calculation of unavailability due to both faults and maintenance, fault and maintenance frequencies and estimated costs for the different substation designs. Four different variations of two-breaker arrangement designs have been simulated and been compared to simulations of the existing substation. To perform the calculations a program has been developed in java that simulates the different substation designs. The fault probabilities used in this study has primarily been taken from <b>fault</b> <b>statistics</b> for the Swedish grid but has also been compared to the assumptions used in other substation reliability studies. The results of this thesis show that the unavailability is likely to be higher for the proposed twobreaker arrangement design without separate disconnectors compared to the existing substation. When the two-breaker arrangement simulation instead included separate disconnectors the unavailability was found to be lower for the two-breaker arrangement design compared to the existing substation. The study also showed that the two-breaker arrangement designs had considerably lower fault frequencies compared to the existing substation. The thesis found that the unavailability that will be caused by maintenance can be significant and are likely to be higher than the unavailability caused by faults. The amount of unavailability caused by maintenance was, however, found to be uncertain because a large part of it can be performed during planned outage. Furthermore, {{it was found that the}} two-breaker arrangement design with and without disconnectors had similar expected present value of costs if the maintenance costs were excluded. This indicates that the best substation option from an economic point of view is determined by the maintenance costs...|$|E
40|$|A {{top-down}} {{risk assessment}} {{in the early}} phases of space exploration architecture development can provide understanding and intuition of the potential risks associated with new designs and technologies. In this approach, risk analysts draw from their past experience and the heritage of similar existing systems {{as a source for}} reliability data. This top-down approach captures the complex interactions of the risk driving parts of the integrated system without requiring detailed knowledge of the parts themselves, which is often unavailable in the early design stages. Traditional probabilistic risk analysis (PRA) technologies, however, suffer several drawbacks that limit their timely application to complex technology development programs. The most restrictive of these is a dependence on static planning scenarios, expressed through fault and event trees. Fault trees incorporating comprehensive mission scenarios are routinely constructed for complex space systems, and several commercial software products are available for evaluating <b>fault</b> <b>statistics.</b> These static representations cannot capture the dynamic behavior of system failures without substantial modification of the initial tree. Consequently, the development of dynamic models using fault tree analysis has been an active area of research in recent years. This paper discusses the implementation and demonstration of dynamic, modular scenario modeling for integration of subsystem fault evaluation modules using the Space Architecture Failure Evaluation (SAFE) tool. SAFE is a C++ code that was originally developed to support NASA s Space Launch Initiative. It provides a flexible framework for system architecture definition and trade studies. SAFE supports extensible modeling of dynamic, time-dependent risk drivers of the system and functions at the level of fidelity for which design and failure data exists. The approach is scalable, allowing inclusion of additional information as detailed data becomes available. The tool performs a Monte Carlo analysis to provide statistical estimates. Example results of an architecture system reliability study are summarized for an exploration system concept using heritage data from liquid-fueled expendable Saturn V/Apollo launch vehicles...|$|E
40|$|Avoidable rework {{constitutes}} {{a large part}} of development projects, i. e. 20 - 80 percent depending on the maturity of the organization and the complexity of the products. High amounts of avoidable rework commonly occur when having many faults left to correct in late stages of a project. In fact, research studies indicate that the cost of rework could be decreased by up to 30 - 50 percent by finding more faults earlier. However, since larger software systems have an almost infinite number of usage scenarios, trying to find most faults early through for example formal specifications and extensive inspections is very time-consuming. Therefore, such an approach is not cost-effective in products that do not have extremely high quality requirements. For example, in market-driven development, time-to-market is at least as important as quality. Further, some areas such as hardware dependent aspects of a product might not be possible to verify early through for example code reviews or unit tests. Therefore, in such environments, rework reduction is primarily about finding faults earlier to the extent it is cost-effective, i. e. find the right faults in the right phase. Through a set of case studies at a department at Ericsson AB, this thesis investigates how to achieve early and cost-effective fault detection through improvements in the test process. The case studies include investigations on how to identify which improvements that are most beneficial to implement, possible solutions to the identified improvement areas, and approaches for how to follow-up implemented improvements. The contributions of the thesis include a framework for component-level test automation and test-driven development. Additionally, the thesis provides methods for how to use <b>fault</b> <b>statistics</b> for identifying and monitoring test process improvements. In particular, we present results from applying methods that can quantify unnecessary fault costs and pinpointing which phases and activities to focus improvements on in order to achieve earlier and more cost-effective fault detection. The goal of the methods is to make organizations strive towards finding the right fault in the right test phase, which commonly is in early test phases. The developed methods were also used for evaluating the results of implementing the above-mentioned test framework at Ericsson AB. Finally, the thesis demonstrates how the implementation of such improvements can be continuously monitored to obtain rapid feedback on the status of defined goals. This was achieved through enhancements of previously applied fault analysis methods. Avhandlingen handlar om hur en mjukvaruutvecklingsorganisation kan hitta fel tidigare i utvecklingsprocessen. Fokus ligger på att hitta rätt fel i rätt fas, d. v. s. när det är som mest kostnadseffektivt. Avhandlingen presenterar en samling fallstudier utförda inom detta området på Ericsson AB. Nyckelord: processförbättring, felanalys, tidig feldetekterin...|$|E
40|$|Abstract: This paper {{presents}} an {{experimental study of}} the fault sensitivity of four programs included in the MiBench test suit. We investigate their fault sensitivity with respect to hardware faults that manifest as single bit flips in main memory locations and instruction set architecture registers. To this end, we have conducted extensive fault injection experiments with two versions of each program, one basic version and one where the program is equipped with software-implemented hardware fault tolerance (SIHFT) through triple time redundant execution, majority voting and forward recovery (TTR-FR). The results show that TTR-FR achieves an error coverage between 94. 6 % and 99. 2 %, while the non-fault-tolerant versions achieve an error coverage between 55. 8 % and 81. 1 %. To gain understanding {{of the origin of}} the non-covered <b>faults,</b> we provide <b>statistics</b> on the <b>fault</b> sensitivity of different source code blocks, physical fault locations (instruction set architecture registers and main memory words) and different categories of machine instructions...|$|R
40|$|Geological structures, such as faults, {{are major}} {{concern to the}} {{longwall}} design, planning and management. The uncertainty of fault systems could cause substantial monetary losses if its risk is not assessed and managed adequately. A methodology for quantifying and assessing fault uncertainty and integrating quantified information in longwall design and planning has been developed. The methodology {{is based on the}} stochastic simulation to generate multiple equal-probable realisations of fault populations which reproduce observed <b>faults,</b> honour the <b>statistics</b> of the <b>fault</b> attributes, and respect the constraints from soft information, providing the means to thereby model and assess the related fault uncertainty. Integrating fault uncertainty into longwall design and planning is a very important new concept and has many advantages. The fault uncertainty information can be used in longwall layout, longwall scheduling, and operation management. This paper briefly summaries the method of stochastic fault simulation and then discuss the application of quantified information in longwall design and planning...|$|R
40|$|Abstract – Fault {{detection}} and isolation (FDI) algorithms provide fault monitoring methods in GPS measurement to isolate abnormal {{signals from the}} GPS satellites or the acquired signal in receiver. In order to monitor the occurred faults, FDI generates test statistics and decides the case that is beyond a designed threshold as a fault. For such problem of fault {{detection and}} isolation, this paper presents and evaluates position domain integrity monitoring methods by formulating various pseudorange prediction methods and investigating the resulting test statistics. In particular, precise measurements like carrier phase and Doppler rate are employed under the assumption of fault free carrier signal. The presented position domain algorithm contains the following process; first a common pseudorange prediction formula is defined with the proposed variations in pseudorange differential update. Next, a threshold computation is proposed with the test statistics distribution considering the elevation angle. Then, by examining the test <b>statistics,</b> <b>fault</b> detection and isolation is done for each satellite channel. To verify the performance, simulations using the presented fault detection methods are done for an ideal and real fault case, respectively...|$|R
40|$|Power system {{reliability}} {{is defined as}} the ability of a power system to perform its function of maintaining supply without allowing network variables (e. g. voltage, component loading and frequency) to stray too far from the standard ranges. Traditionally over many decades, reliability has been assessed using deterministic criteria, e. g., ‘N- 1 ’ or ‘N- 2 ’ standards under prescribed severe system demand levels. However, using the so-called worst-case deterministic approach does not provide explicitly an assessment of the probability of failure of the component or system, and the likelihood of the outages is treated equally. On the other hand, a probabilistic security assessment may offer advantages by considering (i) a statistical description of the performance of the system together with (ii) the application of historical <b>fault</b> <b>statistics</b> that provide a measure of the probability of faults leading to component or system outages. The electrical transmission system, like other systems, is concerned with reducing different risks and costs to within acceptable limits. Therefore, a more precise algorithm of a probabilistic reliability assessment of electrical transmission systems offers an opportunity to achieve such efficiency. This research work introduces the concept of applying the Line Overloading Risk Index (LORI) to assess one of the risks to transmission systems, namely, line overloading. Line failure or outage due to line overloading is catastrophic; they may lead to either load interruptions or system blackout. Some recent studies have focused on the assessment of the LORI; however, such research has been restricted to the analysis of system with very few intermediate demand levels and an assumed constant line thermal rating. This research work aims to extend the evaluation of the LORI through a comprehensive evaluation of transmission system performance under hour-by-hour system demand levels over a oneyear period, for intact systems, as well as ‘N- 1 ’, ‘N- 2 ’. In addition, probable hourly line thermal ratings have also been evaluated and considered over an annual cycle based on detailed meteorological data. In order to accomplish a detailed analysis of the {{system reliability}}, engineering data and historical line fault and maintenance data in real transmission systems were employed. The proposed improved probabilistic reliability assessment method was evaluated using a software package, namely, NEPLAN, thus making it possible to simulate different probable load flow cases instead of assuming a single ‘worst case scenario’. An automated process function in NEPLAN was developed using an extensive programming code in order to expedite the load flow modelling, simulation and result reporting. The successful use of the automation process to create multiple models and apply different contingencies, has made possible this probabilistic study which {{would not have been possible}} using a ‘manual’ simulation process. When calculating the LORI, the development of a Probabilistic Distribution Function (PDF) for line loading, line thermal rating and system demand was essential and useful. The developed algorithm takes into consideration the likelihood of events occurring in addition to severity, which offers opportunity for more efficient planning and operation of transmission systems. Study cases performed on real electric transmission systems in Dubai and the GB have demonstrated that the developed algorithm has potential as a useful tool in system planning and operation. The research presented in this thesis offers an improved algorithm of probabilistic reliability assessment for transmission systems. The selected index, along with the developed algorithm, can be used to rank the transmission lines based on the probabilistic line overloading risk. It provides valuable information on the degree of line overloading vulnerability for different uncertainties...|$|E
40|$|Normal faults play an {{important}} role in hydrocarbon exploration. A normal fault can act as a migration path for fluids or seal fluids compartments. Sealing is enhanced when the fault zones are filled with impermeable materials like clay. “Clay smear is a loosely defined term born in hydrocarbon geology; its usage differs between publications, and the definition of processes operating is often unclear. In most general meaning, the term includes all processes, which somehow transform clay in the wall rock into clay that is part of the fault zone. Processes included are clay abrasion, shear in releasing fault links, preferred smear and lateral clay injection” (van der Zee et al., 2003). This study focuses on experimental investigations on the generation of sealed faults. The sealing quality of the fault is set in context to the rheological properties of the host rock. Structural and mechanical features such as shear strength of the materials involved, clay smear, the evolution of releasing bends and fault lenses play {{an important}} role and are investigated using analogue sandbox experiments. The experimental results are compared intensively with scientific theories and previous studies dealing with similar topics in the following chapters. The aim of this study is to find out which parameters have major effect on the final gouge geometry and composition and to investigate the kinematical evolution. Finally, the experimental results should give us information on the validity of in hydrocarbon geology frequently used fault sealing prediction theories and methods. The application of analogue models is a useful method to investigate the effect of different boundary conditions and material properties on the evolution of a fault zone. Dry sand or wet clay have been used before, but a combination of the two was not possible, and instead model materials such as silicon were used. But, the closer the model approaches to scaled analogues of natural conditions, the more informative are the experimental results. So, why not build up a model that includes real sand and clay instead of analogue model materials? For this purpose an analogue sandbox-model was built. Experiments are run under completely water-saturated conditions to simulate the behavior of ‘wet’ clay and to enable the investigation of fault development in layered sand-clay sequences. The observations focus on the behavior of the clay layers, as they have strong influence on the permeability, which is important in terms of hydrogeology and for the petroleum industry. This sandbox makes it possible to carry out experiments in the dimension of 20 x 40 x 20 cm. The rigid basement fault dips with 70 ° and the velocity can be varied from 2 up to 12 cm/hr with a maximum offset of 60 mm. In nine experiment series parameters such as the clay’s rigidity or the number of clay layers were varied systematically. These variations lead to the evolution of different gouge structures that strongly deviate in their structure and transport properties. The experimental results are discussed in reference to their natural effect on the fault zone structure. 2 D-images of the sandbox experiments were additionally processed with PIV-software (Particle Image Velocimetry) to quantify the displacement field. The software calculates the motion at the scale of a sand grain. The results give useful information on fault zone evolution. Results are investigated in terms of <b>fault</b> <b>statistics</b> and fault geometry. To infer the mechanisms of fault zone evolution in both nature and analogue models I focus on the mechanical background and kinematics of the initial deformation phase...|$|E
40|$|Summary	Distribution {{power systems}} {{experience}} {{a lot of}} faults due to high network utilisation and reduced investment in the infrastructure. Single phase faults are the most common. The standard distribution systems are radial operated. Failure of any one component in the series path would result into service disruption to all customers located downstream. Current methods used to localise faults are manual and lengthy. For permanent faults, {{it is a common}} practice to sectionalise a feeder to reduce the fault investigation area. The exact location of the fault is obtained by visual inspection. However, with the introduction of the new marketing policy resulting from deregulation and liberalisation of the power industry, utility companies are required to compensate consumers for non delivery of service. Distribution networks also records high growth rates due to new customer connections arising from increased demand for electric energy. An increase in consumption introduces transmission constraints on the distribution network, demanding for reinforcement on the main circuits. The penetration of distributed production facilities intends to curtail such investment costs and provide continuity of supply to the affected customers while the fault is being resolved. The objective of the thesis was to investigate the impacts caused by different power system configurations and parameters on automatic single phase fault localisation in power systems with distributed production using power frequency quantities. It was also required to reveal the present ground fault protection schemes and recommend requirements on a system with distributed generation. The approach used in the thesis included informal conversation, field research, literature review on similar topics, modelling and simulations using PSCAD/EMTDC software. A field research was conducted in Zambia on the pretext of understanding the problem from a practical perspective. The main focus was placed on power system configuration and layout, current methods used to localise faults and <b>fault</b> <b>statistics.</b> Secondary research on automated fault localisation methods for distribution systems based on fundamental power frequency quantities was done. Much of the literature available focuses on typical radial distribution feeders. In the thesis, two different methods were used in the simulation of distances to the fault location so that the findings could be consolidated. One of the methods is a standard approach used in numerical distance protection relays and the other, was derived, taking into account the presence of a distributed generator. The distribution system can be solidly grounded, ungrounded, grounded through an arc suppression coil or through a resistor. Each type of grounding configuration has its own impact on fault localisation. Different system grounding possibilities were used in the model when investigating the effect of parameter change, on fault localisation. Simulation results revealed that the distances to the fault locations are affected by all parameters selected for investigation. An increase in load with the same load angle was seen to have an impact of reducing the distance to the fault. An increase in the fault resistance had an effect of increasing the distance to the fault location. Moving the distributed generator away from the substation was seen to have had an effect of reducing the distance to the fault location. However, investigations were carried out without compensation for any of the parameters. This could have contributed to inaccuracies in the results obtained. While automatic localisation of ground faults is at the premium, it is required to minimise damages at the fault position and neighbouring equipment so that restoration of supply is prompt. This could be achieved with effective operation of the protection system. Integration of distributed production facilities can affect the dynamic behaviour of the distribution network and change the power flow. This affects operation of the conversional protection schemes deployed on the radial distribution systems. Therefore, protection schemes with a directional feature are recommended in order to improve selectivity. It can be deduced that precise ground fault localisation based on the use of power frequency quantities is a challenge due to many influencing parameters. However, automated fault localisation in power systems with distributed production is the future solution, especially from the operational perspective...|$|E
40|$|Includes bibliographical references. A country's {{ability to}} sustain and grow its {{industrial}} and commercial activities is {{highly dependent on}} a reliable electricity supply. Electrical faults on transmission lines are a cause of both interruptions to supply and voltage dips. These {{are the most common}} events impacting electricity users and also have the largest financial impact on them. This research focuses on understanding the causes of transmission line faults and developing methods to automatically identify these causes. Records of faults occurring on the South African power transmission system over a 16 -year period have been collected and analysed to find statistical relationships between local climate, key design parameters of the overhead lines and the main causes of power system faults. The results characterize the performance of the South African transmission system on a probabilistic basis and illustrate differences in <b>fault</b> cause <b>statistics</b> for the summer and winter rainfall areas of South Africa and for different times of the year and day. This analysis lays a foundation for reliability analysis and fault pattern recognition taking environmental features such as local geography, climate and power system parameters into account. A key aspect of using pattern recognition techniques is selecting appropriate classifying features. Transmission line fault waveforms are characterised by instantaneous symmetrical component analysis to describe the transient and steady state fault conditions. The waveform and environmental features are used to develop single nearest neighbour classifiers to identify the underlying cause of transmission line faults. A classification accuracy of 86 % is achieved using a single nearest neighbour classifier. This classification performance is found to be superior to that of decision tree, artificial neural network and naïve Bayes classifiers. The results achieved demonstrate that transmission line faults can be automatically classified according to cause...|$|R
40|$|Effective, {{statistically}} robust {{sampling and}} surveillance strategies form an integral component of large agricultural {{industries such as}} the grains industry. Intensive in-storage sampling is essential for pest detection, Integrated Pest Management (IPM), to determine grain quality and to satisfy importing nation’s biosecurity concerns, while surveillance over broad geographic regions ensures that biosecurity risks can be excluded, monitored, eradicated or contained within an area. In the grains industry, a number of qualitative and quantitative methodologies for surveillance and in-storage sampling have been considered. Primarily, research has focussed on developing statistical methodologies for in storage sampling strategies concentrating on detection of pest insects within a grain bulk, however, the need for effective and statistically defensible surveillance strategies has also been recognised. Interestingly, although surveillance and in storage sampling have typically been considered independently, many techniques and concepts are common between the two fields of research. This review aims to consider the development of statistically based in storage sampling and surveillance strategies and to identify methods that may be useful for both surveillance and in storage sampling. We discuss the utility of new quantitative and qualitative approaches, such as Bayesian <b>statistics,</b> <b>fault</b> trees and more traditional probabilistic methods and show how these methods {{may be used in}} both surveillance and in storage sampling systems...|$|R
40|$|International audienceA {{large-scale}} {{normal fault}} may {{be composed of}} several overlapping fault segments separated by relay zones at a finer scale. Fault segmentation may be critical to the understanding and the forecast of physical phenomena at the reservoir or basin scale (e. g. fluid flow, seismic rupture propagation). In this paper, we propose an automatic and stochastic method to sub-divide (downscale) below the data resolution a segmented normal fault into enechelon segments that may be linked by connecting faults, based on variations in {{the orientation of the}} fault. The downscaling algorithm is composed of three main steps. (1) The first step involves detecting the segments using geometrical criteria. (2) Then the overlapping segments are modeled using isolated <b>fault</b> descriptions and <b>statistics.</b> (3) Lastly, the maturity of the simulated relay zones is evaluated based on relay geometry. If a given maturity threshold is reached, the relay ramp is breached. These three downscaling steps depend on seven parameters that can be defined as constant or randomly chosen from probability distributions to sample uncertainties. The method was applied to a large normal fault that laterally limits a hydrocarbon reservoir and is poorly imaged from seismic data. This resulted in one hundred possible 3 D fault array models. The analysis of the emerging parameters shows a strong variability of the number and length of segments, and of the overlap and spacing between segments...|$|R
