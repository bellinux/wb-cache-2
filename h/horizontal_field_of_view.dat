53|10000|Public
25|$|Moreover, {{the larger}} the format size, the longer a lens {{will need to be}} to capture the same framing as a smaller format. In motion pictures, for example, a frame with a 12 degree <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> will require a 50mm lens on 16mm film, a 100mm lens on 35mm film, and a 250mm lens on 65mm film. Conversely, using the same focal length lens with each of these formats will yield a {{progressively}} wider image as the film format gets larger: a 50mm lens has a <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> of 12degrees on 16mm film, 23.6degrees on 35mm film, and 55.6degrees on 65mm film. Therefore, because the larger formats require longer lenses than the smaller ones, they will accordingly have a smaller depth of field. Compensations in exposure, framing, or subject distance need to be made in order to make one format look like it was filmed in another format.|$|E
500|$|Robert Gottschalk founded Panavision in late 1953, in {{partnership}} with Richard Moore, Meredith Nicholson, Harry Eller, Walter Wallin, and William Mann; the company was formally incorporated in 1954. Panavision was established principally for the manufacture of anamorphic projection lenses to meet the growing demands of theaters showing CinemaScope films. At the time of Panavision's formation, Gottschalk owned a camera shop in Westwood Village, California, where many of his customers were cinematographers. [...] A few years earlier, he and Moore—who worked {{with him in the}} camera shop—were experimenting with underwater photography; Gottschalk became interested in the technology of anamorphic lenses, which allowed him to get a wider field of view from his underwater camera housing. The technology was created during World War I to increase the field of view on tank periscopes; the periscope image was horizontally [...] "squeezed" [...] by the anamorphic lens. After it was unsqueezed by a complementary anamorphic optical element, the tank operator could see double the <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> without significant distortion. Gottschalk and Moore bought some of these lenses from C. P. Goerz, a New York optics company, for use in their underwater photography. As widescreen filmmaking became popular, Gottschalk saw an opportunity to provide anamorphic lenses to the film industry—first for projectors, and then for cameras. Nicholson, a friend of Moore, started working as a cameraman on early tests of anamorphic photography.|$|E
2500|$|Motion {{sickness}} due to {{virtual reality}} {{is very similar}} to simulation sickness and motion sickness due to films. [...] In virtual reality, however, the effect is made more acute as all external reference points are blocked from vision, the simulated images are three-dimensional and in some cases stereo sound that may also give a sense of motion. The NADS-1, a simulator located at the National Advanced Driving Simulator, is capable of accurately stimulating the vestibular system with a 360-degree <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> and 13 degrees of freedom motion base. Studies have shown that exposure to rotational motions in a virtual environment can cause significant increases in nausea and other symptoms of motion sickness.|$|E
40|$|In the {{following}} {{we will discuss}} a cost effective immersive gaming environment and the implementation in Blender [1], an open source game engine. This extends traditional approaches to immersive gaming which tend to concentrate on multiple flat screens, sometimes surrounding the player, or cylindrical [2] displays. In the former there are unnatural gaps between each display due to screen framing, in both cases they rarely cover the 180 <b>horizontal</b> degree <b>field</b> <b>of</b> <b>view</b> and are even less likely to cover the vertical <b>field</b> <b>of</b> <b>view</b> required to fully engage the <b>field</b> <b>of</b> <b>view</b> <b>of</b> the human visual system. The solution introduced here concentrates on seamless hemispherical displays, planetariums {{in general and the}} iDome [3] as a specific case study. The methodology discussed is equally appropriate to other realtime 3 D environments that are available in source code form or have a suitably powerful means of modifying the rendering pipeline...|$|R
40|$|The {{purpose of}} the project, which led to this thesis, was to {{investigate}} the possible effects different <b>horizontal</b> <b>Fields</b> <b>of</b> <b>View</b> (FoV) have on driving performance when driving at night with a Vision Enhancement System (VES). The FoV chosen to be examined were 12 degree and 24 degree FoV, both displayed on a screen with the horizontal size of 12 degree FoV. This meant that the different conditions of FoV also had different display ratios 1 : 1 and 1 : 2. No effort was made to separate these parameters. A simulator study was performed at the simulator at IKP, Linköping University. Sixteen participants in a within-group design participated in the study. The participants drove two road sections; one with a 12 degree FoV and the other with a 24 degree FoV. During each section four scenarios were presented in which the participants passed one of three types of objects; a moose, a deer or a man. In each section, two of the objects stood right next to the road and two were standing seventeen meters {{to the right of the}} road. As the drivers approached the objects standing seventeen meters to the right of the road, the objects moved out of the VES when the vehicle was 200 meters in front of the object with a 12 degree FoV. The objects could be seen with the naked eye when the vehicle was 100 meters in front of the object. When the drivers approached the objects with a 24 degree FoV the objects moved out of the VES display when it was possible to see them unaided. Results show that a 24 degree FoV displayed with a 1 : 2 ratio gives the drivers improved anticipatory control, compared to a 12 degree FoV displayed with a 1 : 1 ratio. The participants with a broader FoV were able to make informed decisions whereas with a narrow FoV some participants started to reaccelerate when they could not see an object. Results also show that any difference in recognition distance that may exist between a 12 degree and a 24 degree camera angle displayed in a 12 degree FoV display do not seem to have any adverse effect on the quality of driving...|$|R
30|$|<b>Field</b> <b>of</b> <b>view</b> Humans have {{a limited}} <b>field</b> <b>of</b> <b>view</b> and an age diminished “useful <b>field</b> <b>of</b> view” (UFOV) [90], which needs to be considered. Excluding head rotation, the typical <b>field</b> <b>of</b> <b>view</b> for a human has a {{difference}} between the <b>horizontal</b> and vertical <b>field</b> <b>of</b> <b>view,</b> an area <b>of</b> binocular overlap and areas of monocular far peripheral vision. “For many of our interaction tasks the UFOV varies between younger and older people. A 36 degree <b>field</b> <b>of</b> <b>view</b> will be practical in many situations” [90]. Within each person’s <b>field</b> <b>of</b> <b>view</b> we can also distinguish regions of central (ie. foveal, central, paracentral and macular) and peripheral (near, mid and far) vision. The useful <b>field</b> <b>of</b> <b>view,</b> typically includes both central vision, measured through visual acuity (ability to distinguish details and shapes of objects), and largely near peripheral parts of vision (part of vision that occurs outside the very center of gaze).|$|R
50|$|FAA FFS Level C - Requires {{a motion}} {{platform}} with all {{six degrees of}} freedom. Also lower transport delay (latency) over levels A & B. The visual system must have an outside-world <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> of at least 75 degrees for each pilot.|$|E
50|$|With {{a typical}} 2 {{megapixel}} CCD, a 1600×1200 pixels image is generated. The {{resolution of the}} image depends {{on the field of}} view of the lens used with the camera. The approximate pixel resolution can be determined by dividing the <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> (FOV) by 1600.|$|E
5000|$|Pixel-based scaling {{is almost}} {{exclusively}} used in games with two-dimensional graphics. With pixel-based scaling, {{the amount of}} content displayed on screen is directly tied to the rendering resolution. A larger horizontal resolution directly increases the <b>horizontal</b> <b>field</b> <b>of</b> <b>view,</b> and a larger vertical resolution increases the vertical field of view.|$|E
40|$|Although actions {{often appear}} in the visual periphery, {{little is known about}} action {{recognition}} away from fixation. We showed in previous studies that action recognition of moving stick-figures is surprisingly good in peripheral vision even at 75 ° eccentricity. Furthermore, there was no decline of performance up to 45 ° eccentricity. This finding could be explained by action sensitive units in the fovea sampling also action information from the periphery. To investigate this possibility, we assessed the horizontal extent of the spatial sampling area (SSA) of action sensitive units in the fovea by using an action adaptation paradigm. Fifteen participants adapted to an action (handshake, punch) at the fovea were tested with an ambiguous action stimulus at 0 °, 20 °, 40 ° and 60 ° eccentricity left and right of fixation. We used a large screen display to cover the whole <b>horizontal</b> visual <b>field</b> <b>of</b> <b>view.</b> An adaptation effect was present in the periphery up to 20 ° eccentricity (p< 0. 001), suggesting a large SSA of action sensitive units representing foveal space. Hence, action recognition in the visual periphery might benefit from a large SSA of foveal units...|$|R
40|$|The {{research}} {{contained in}} this thesis is an investigation into mosaic construction. Mosaic techniques are used to obtain images with a large <b>field</b> <b>of</b> <b>view</b> by assembling a sequence of smaller individual overlapping images. In existing methods of mosaic construction only successive images are aligned. Accumulation of small alignment errors occur, {{and in the case}} of the image path returning to a previous position in the mosaic, a significant mismatch between nonconsecutive images will result (looping path problem). A new method for consistently aligning all the images in a mosaic is proposed {{in this thesis}}. This is achieved by distribution of the small alignment errors. Each image is allowed to modify its position relative to its neighbour images in the mosaic by a small amount with respect to the computed registration. Two images recorded by a rotating ideal camera are related by the same transformation that relates the camera's sensor plane at the time the images were captured. When two images overlap, the intensity values in both images coincide through the intersection line of the sensor planes. This intersection line has the property that the images can be seamlessly joined through that line. An analogy between the images and the physical world is proposed to solve the looping path problem. The images correspond to rigid objects, and these are linked with forces which pull them towards the right positions with respect to their neighbours. That is, every pair of overlapping images are "hinged" through their corresponding intersection line. Aided by another constraint named the spherical constraint, this network of selforganising images has the ability of distributing itself on the surface of a sphere. As a direct result of the new concepts developed in this research work, spherical mosaics (i. e. mosaics with unlimited <b>horizontal</b> and vertical <b>field</b> <b>of</b> <b>view)</b> can be created...|$|R
40|$|The data contain ten <b>horizontal</b> <b>fields</b> <b>of</b> cloud optical thickness, {{retrieved}} from airborne measured reflected solar spectral radiance at 550 nm wavelength. The {{data were obtained}} during the airbone VERtical Distribution of Ice in Arctic clouds (VERDI) campaign based at Inuvik, North West Territories, Canada between 17 April and 17 May 2012. The applied <b>fields</b> <b>of</b> reflected solar spectral radiance were measured with the imaging spectrometer AisaEAGLE (36. 3 ° <b>field</b> <b>of</b> <b>view,</b> 467 spatial pixels, 400 - 970 nm wavelength range) mounted on the Polar 5 research aircraft of the Alfred-Wegener-Institute, Helmholtz Centre for Polar and Marine Research, Bremerhaven, Germany...|$|R
50|$|It gives a wider {{field of}} view. For example, humans have a maximum <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> of {{approximately}} 190 degrees with two eyes, approximately 120 degrees of which {{makes up the}} binocular field of view (seen by both eyes) flanked by two uniocular fields (seen by only one eye) of approximately 40 degrees.|$|E
50|$|Moreover, {{the larger}} the format size, the longer a lens {{will need to be}} to capture the same framing as a smaller format. In motion pictures, for example, a frame with a 12 degree <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> will require a 50 mm lens on 16 mm film, a 100 mm lens on 35 mm film, and a 250 mm lens on 65 mm film. Conversely, using the same focal length lens with each of these formats will yield a {{progressively}} wider image as the film format gets larger: a 50 mm lens has a <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> of 12 degrees on 16 mm film, 23.6 degrees on 35 mm film, and 55.6 degrees on 65 mm film. Therefore, because the larger formats require longer lenses than the smaller ones, they will accordingly have a smaller depth of field. Compensations in exposure, framing, or subject distance need to be made in order to make one format look like it was filmed in another format.|$|E
50|$|FAA FFS Level D - The {{highest level}} of FFS {{qualification}} currently available. Requirements are for Level C with additions. The motion platform must have all six degrees of freedom, and the visual system must have an outside-world <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> of at least 150 degrees, with a Collimated (distant focus) display. Realistic sounds in the cockpit are required, {{as well as a}} number of special motion and visual effects.|$|E
50|$|<b>Horizontal</b> <b>field</b> <b>of</b> fire: 445mil to the left, 675mil to the right.|$|R
5000|$|Many optical instruments, {{particularly}} binoculars or spotting scopes, are advertised {{with their}} <b>field</b> <b>of</b> <b>view</b> specified {{in one of}} two ways: angular <b>field</b> <b>of</b> <b>view,</b> and linear <b>field</b> <b>of</b> <b>view.</b> Angular <b>field</b> <b>of</b> <b>view</b> is typically specified in degrees, while linear <b>field</b> <b>of</b> <b>view</b> is a ratio of lengths. For example, binoculars with a 5.8 degree (angular) <b>field</b> <b>of</b> <b>view</b> might be advertised as having a (linear) <b>field</b> <b>of</b> <b>view</b> <b>of</b> 102 mm per meter. As long as the FOV is less than about 10 degrees or so, the following approximation formulas allow one to convert between linear and angular <b>field</b> <b>of</b> <b>view.</b> Let [...] be the angular <b>field</b> <b>of</b> <b>view</b> in degrees. Let [...] be the linear <b>field</b> <b>of</b> <b>view</b> in millimeters per meter. Then, using the small-angle approximation: ...|$|R
5000|$|Viewfinder: Eye-level fixed {{pentaprism}} showing 92% {{of vertical}} and 94% <b>of</b> <b>horizontal</b> <b>field</b> <b>of</b> view; magnification: 0.75× with 50 mm lens at infinity; transparent LCD screen and Acute-Matte screen; diopter: −2.5 to +0.5 adjustable; long eye-relief.|$|R
50|$|Cylindrical projection, {{where the}} {{stitched}} image shows a 360° <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> and a limited vertical field of view. Panoramas in this projection {{are meant to}} be viewed as though the image is wrapped into a cylinder and viewed from within. When viewed on a 2D plane, horizontal lines appear curved while vertical lines remain straight. Vertical distortion increases rapidly when nearing the top of the panosphere. There are various other cylindrical formats, such as Mercator and Miller cylindrical which have less distortion near the poles of the panosphere.|$|E
50|$|Motion {{sickness}} due to {{virtual reality}} {{is very similar}} to simulation sickness and motion sickness due to films. In virtual reality, however, the effect is made more acute as all external reference points are blocked from vision, the simulated images are three-dimensional and in some cases stereo sound that may also give a sense of motion. The NADS-1, a simulator located at the National Advanced Driving Simulator, is capable of accurately stimulating the vestibular system with a 360-degree <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> and 13 degrees of freedom motion base. Studies have shown that exposure to rotational motions in a virtual environment can cause significant increases in nausea and other symptoms of motion sickness.|$|E
50|$|Field of {{view and}} depth of field: Depth of field is {{tangentially}} related {{to the size of}} the image plane, however, it is a popular misconception that the image plane is directly related to DOF. Smaller image planes (whether film or sensor) require a proportionally smaller lens to achieve a similar field of view. This means that a frame with a 12 degree <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> will require a 50 mm lens on 16 mm film, a 100 mm lens on 35 mm film, and a 250 mm lens on 65 mm film. And a 250 mm lens delivers much shallower DOF than a 50 mm lens does. It follows that standard lenses on most consumer video cameras with small sensors provide much larger depth of field than 35 mm film. Digital cinema cameras like the Red One or Panavision Genesis, as well as some digital SLR cameras with video capabilities, (such as the Canon EOS 5D Mark II), have sensors roughly equal in size to 35 mm film frames and thus show the same field of view characteristics.|$|E
40|$|Abstract. This paper {{presents}} the quantification <b>of</b> <b>Field</b> <b>of</b> <b>View</b> <b>of</b> external Rear <b>View</b> Mirror in automobiles. The {{decision based on}} available <b>Field</b> <b>of</b> <b>View</b> and the blind zone for lane changing {{is one of the}} major factors in road accident. This study presents a preliminary study <b>of</b> quantifying <b>field</b> <b>of</b> <b>view</b> <b>of</b> rear <b>view</b> mirror in automotive vehicles. A simple method <b>of</b> estimating the <b>Field</b> <b>of</b> <b>View</b> is shown and also a passive method <b>of</b> increasing the <b>Field</b> <b>of</b> <b>View</b> is discussed. The proposed method helps the designers and road users to choose the mirror of right size to provide extra <b>field</b> <b>of</b> <b>view</b> beyond the regulatory requirements...|$|R
50|$|In astronomy, the <b>field</b> <b>of</b> <b>view</b> {{is usually}} {{expressed}} as an angular area {{viewed by the}} instrument, in square degrees, or for higher magnification instruments, in square arc-minutes. For reference the Wide Field Channel on the Advanced Camera for Surveys on the Hubble Space Telescope has a <b>field</b> <b>of</b> <b>view</b> <b>of</b> 10 sq. arc-minutes, and the High Resolution Channel of the same instrument has a <b>field</b> <b>of</b> <b>view</b> <b>of</b> 0.15 sq. arc-minutes. Ground based survey telescopes have much wider <b>fields</b> <b>of</b> <b>view.</b> The photographic plates used by the UK Schmidt Telescope had a <b>field</b> <b>of</b> <b>view</b> <b>of</b> 30 sq. degrees. The 1.8 m (71 in) Pan-STARRS telescope, with the most advanced digital camera to date has a <b>field</b> <b>of</b> <b>view</b> <b>of</b> 7 sq. degrees. In the near infra-red WFCAM on UKIRT has a <b>field</b> <b>of</b> <b>view</b> <b>of</b> 0.2 sq. degrees and the VISTA telescope has a <b>field</b> <b>of</b> <b>view</b> <b>of</b> 0.6 sq. degrees. Until recently digital cameras could only cover a small <b>field</b> <b>of</b> <b>view</b> compared to photographic plates, although they beat photographic plates in quantum efficiency, linearity and dynamic range, {{as well as being}} much easier to process.|$|R
40|$|<b>Field</b> <b>of</b> <b>view</b> {{has always}} been a design feature {{paramount}} to helmet design, and in particular spacesuit design, where the helmet must provide an adequate <b>field</b> <b>of</b> <b>view</b> for a large range of activities, environments, and body positions. Historically, suited <b>field</b> <b>of</b> <b>view</b> has been evaluated either qualitatively in parallel with design or quantitatively using various test methods and protocols. As such, oftentimes legacy suit <b>field</b> <b>of</b> <b>view</b> information is either ambiguous for lack of supporting data or contradictory to other <b>field</b> <b>of</b> <b>view</b> tests performed with different subjects and test methods. This paper serves to document a new <b>field</b> <b>of</b> <b>view</b> testing method that is more reliable and repeatable than its predecessors. It borrows heavily from standard <b>field</b> <b>of</b> vision tests such as the Goldmann kinetic perimetry test, but is designed specifically for evaluating <b>field</b> <b>of</b> <b>view</b> <b>of</b> a spacesuit helmet. In this test, three suits utilizing three different helmet designs were tested for <b>field</b> <b>of</b> <b>view.</b> Not only do these tests provide more reliable <b>field</b> <b>of</b> <b>view</b> data for legacy and prototype helmet designs, they also provide insight into how helmet design impacts <b>field</b> <b>of</b> <b>view</b> and what this means for the Constellation Project spacesuit helmet, which must meet stringent <b>field</b> <b>of</b> <b>view</b> requirements that are more generous to the crewmember than legacy designs...|$|R
5000|$|Robert Gottschalk founded Panavision in late 1953, in {{partnership}} with Richard Moore, Meredith Nicholson, Harry Eller, Walter Wallin, and William Mann; the company was formally incorporated in 1954. Panavision was established principally for the manufacture of anamorphic projection lenses to meet the growing demands of theaters showing CinemaScope films. At the time of Panavision's formation, Gottschalk owned a camera shop in Westwood Village, California, where many of his customers were cinematographers. [...] A few years earlier, he and Moore—who worked {{with him in the}} camera shop—were experimenting with underwater photography; Gottschalk became interested in the technology of anamorphic lenses, which allowed him to get a wider field of view from his underwater camera housing. The technology was created during World War I to increase the field of view on tank periscopes; the periscope image was horizontally [...] "squeezed" [...] by the anamorphic lens. After it was unsqueezed by a complementary anamorphic optical element, the tank operator could see double the <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> without significant distortion. Gottschalk and Moore bought some of these lenses from C. P. Goerz, a New York optics company, for use in their underwater photography. As widescreen filmmaking became popular, Gottschalk saw an opportunity to provide anamorphic lenses to the film industry—first for projectors, and then for cameras. Nicholson, a friend of Moore, started working as a cameraman on early tests of anamorphic photography.|$|E
40|$|In {{the present}} study we {{investigated}} steering accuracy {{in terms of our}} ability to keep to the middle of a lane in a fixed-base driving simulator. In particular, we studied the dependence of steering accuracy on the visibility of different road sections, on the assumption that performance reflects the importance of different road sections in guiding steering. Other influences on steering accuracy - including the presence of textural cues, {{in the form of a}} textured road surface, and the <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> - were also investigated. We found that textural cues can improve accuracy in lateral lane control, presumably by providing strong optical flow, and that driving accuracy is little affected by increasing the <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> from 40 ° to a full field of 180 °...|$|E
40|$|A {{catadioptric}} {{vision system}} using diverse mirrors {{has been a}} popular means to get panoramic images(K. Nayar, 1997) which contains a full <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> (FOV). This wide view is ideal for three-dimensional vision tasks such as motion estimation, localization, obstacle detection and mobile robots navigation. Omnidirectional stereo is...|$|E
40|$|<b>Field</b> <b>of</b> <b>view</b> {{has always}} been a design feature {{paramount}} to helmet design, and in particular space suit design, where the helmet must provide an adequate <b>field</b> <b>of</b> <b>view</b> for a large range of activities, environments, and body positions. For Project Constellation, a slightly different approach to helmet requirement maturation was utilized; one that was less a direct function of body position and suit pressure and more a function of the mission segment in which the <b>field</b> <b>of</b> <b>view</b> is required. Through taxonimization of various parameters that affect suited FOV, as well as consideration for possible nominal and contingency operations during that mission segment, a reduction process was able to condense the large number of possible outcomes to only six unique <b>field</b> <b>of</b> <b>view</b> angle requirements that still captured all necessary variables without sacrificing fidelity. The specific <b>field</b> <b>of</b> <b>view</b> angles were defined by considering mission segment activities, historical performance of other suits, comparison between similar requirements (pressure visor up versus down, etc.), estimated requirements from other teams for <b>field</b> <b>of</b> <b>view</b> (Orion, Altair, EVA), previous <b>field</b> <b>of</b> <b>view</b> tests, medical data for shirtsleeve <b>field</b> <b>of</b> <b>view</b> performance, and mapping <b>of</b> visual <b>field</b> data to generate 45 degree off-axis <b>field</b> <b>of</b> <b>view</b> requirements. Full resolution <b>of</b> several specific <b>field</b> <b>of</b> <b>view</b> angle requirements warranted further work, which consisted of low and medium fidelity <b>field</b> <b>of</b> <b>view</b> testing in the rear entry ISuit and DO 27 helmet prototype. This paper serves to document this reduction progress and followup testing employed to write the Constellation requirements for helmet <b>field</b> <b>of</b> <b>view...</b>|$|R
5000|$|If the {{apparent}} <b>field</b> <b>of</b> <b>view</b> is unknown, the actual <b>field</b> <b>of</b> <b>view</b> can be approximately found using:where: ...|$|R
5000|$|Develop a {{low power}} {{millimeter}} wave radar system for wide <b>field</b> <b>of</b> <b>view</b> detection and narrow <b>field</b> <b>of</b> <b>view</b> gait classification.|$|R
40|$|This paper {{presents}} a prototype system for pedestrian recognition and tracking of vehicles in urban environment {{that has been}} tested in a series passenger car. For environment sensing, a high resolution Multilayer Laserscanner with a <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> of up to 180 degree was used, that was mounted at the front bumper of the test vehicl...|$|E
40|$|This study {{investigates the}} effect of both {{horizontal}} and vertical field of view restriction on walking through apertures. Speed, angle of passage, the range of shoulder rotation and clearance of the aperture were used as measures. The hypothesis was that field of view restriction would increase the rotational angle by which participants passed through apertures because the perception affordance {{between them and the}} aperture would change. No effects were found on shoulder rotation nor an effect of <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> restriction was found. However, when the visible visual angle in the vertical plane decreased, walking speed decreased. Meaning that confidence is lost when the vertical Field of View is restricted, but not when the <b>horizontal</b> <b>Field</b> <b>of</b> <b>View</b> is restricted. This leads to speculation when and how people judge aperture passability. Furthermore, shows this study more evidence for affordances and an understanding as to how people pass through apertures...|$|E
40|$|The {{current state}} of SLAM radar is quite advanced, {{featuring}} various methods of data retrieval. One of the methods used is that of video telemetry to locate “common spots” in the surrounding environment which provide positional information during motion. Another method is that of using high-speed high-resolution laser measurement tools which provide a 360 ° <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> and a 90 ° vertical field of view. These systems create vast amounts of point cloud data and are expensive, ranging from £ 1, 000 upwards. These systems are often unsuitable for small competition robots due to these reasons. The developments discussed in this paper describes various alternative measurement technologies, such as ultrasonic and infra-red and how these can be adapted {{with the addition of}} a mechanical drive to provide an almost real-time 360 ° <b>horizontal</b> <b>field</b> <b>of</b> <b>view</b> and an adjustable vertical field of vie...|$|E
5000|$|If the {{apparent}} <b>field</b> <b>of</b> <b>view</b> is known, the actual <b>field</b> <b>of</b> <b>view</b> {{can be calculated}} from the following approximate formula: ...|$|R
40|$|Insolation radiometers (normal {{incidence}} pyrheliometers) {{are used}} to measure the solar radiation incident on solar concentrators for calibrating thermal power generation measurements. The measured insolation value is dependent on the atmospheric transparency, solar elevation angle, circumsolar radiation, and radiometer <b>field</b> <b>of</b> <b>view.</b> The radiant energy entering the thermal receiver is dependent on the same factors. The insolation value and the receiver input will be proportional if the concentrator and the radiometer have similar <b>fields</b> <b>of</b> <b>view.</b> This report describes one practical method for matching the <b>field</b> <b>of</b> <b>view</b> <b>of</b> a radiometer to that of a solar concentrator. The concentrator <b>field</b> <b>of</b> <b>view</b> can be calculated by optical ray tracing methods and the <b>field</b> <b>of</b> <b>view</b> <b>of</b> a radiometer with a simple shroud can be calculated by using geometric equations. The parameters for the shroud can be adjusted to provide an acceptable match between the respective <b>fields</b> <b>of</b> <b>view.</b> Concentrator <b>fields</b> <b>of</b> <b>view</b> have been calculated for a family of paraboloidal concentrators and receiver apertures. The corresponding shroud parameters have also been determined...|$|R
40|$|This study {{investigated}} the relationship between Useful <b>Field</b> <b>of</b> <b>View</b> and simulator-driving performance measures. Ninety professional drivers, aged 22 - 65 years from several government organizations voluntarily participated at this study. Useful <b>Field</b> <b>of</b> <b>View</b> was measured by a computerized task was developed at the present study. The participants then performed a driving simulator task and experienced a scenario {{that could lead to}} an accident. Reaction time and speed were measured and recorded by simulator and general driving performance and collision events were recorded by examiner. The reduction <b>of</b> Useful <b>Field</b> <b>of</b> <b>View</b> based on subject’s error score on Useful <b>Field</b> <b>of</b> <b>View</b> subtests between young and old group statistically was analyzed. Correlation analyses used to examine the relationship among the Useful <b>Field</b> <b>of</b> <b>View</b> as an independent variable and driving performance measures as a dependent variables. A univariate logistic regression analysis was used {{to determine the extent to}} which reduction <b>of</b> Useful <b>Field</b> <b>of</b> <b>View</b> predicts risk <b>of</b> accident in simulated car driving. There was a significant and negative correlation between Useful <b>Field</b> <b>of</b> <b>View</b> and simulator performance, on the divided peripheral subtest (Correlation Coefficient=- 0. 28). Student’s t-tests revealed significant differences in peripheral scores <b>of</b> Useful <b>Field</b> <b>of</b> <b>View</b> subtests between accident involved and non-involved groups. The result of logistic regression indicated that 40 % reduction <b>of</b> Useful <b>Field</b> <b>of</b> <b>View,</b> regardless <b>of</b> age, increased risk of accident involvement. Useful <b>Field</b> <b>of</b> <b>View</b> could be used to predict driving performance and risk of accident. The obtained result can help to identify a high risk driver which is useful to licensing authorities...|$|R
