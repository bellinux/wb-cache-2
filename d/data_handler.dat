24|33|Public
5000|$|The On Board <b>Data</b> <b>Handler</b> (OBDH) was {{the core}} for data {{management}} and system {{control on the}} satellite and it also managed the communication interfaces between the satellite and the ground station. Its computer supervised all subsystem processor activities, {{such as those of}} each instrument, and the communication busses.|$|E
50|$|The CFTP-1 board {{utilizes}} Xilinx Virtex I {{parts for}} both the control and experiment FPGAs. The complete CFTP payload, as delivered to MidSTAR-1, consists of the CFTP board itself (shown in the spacecraft image above), an ARM processor board that communicates with the control FPGA on the CFTP board through a PC/104 bus, and a power supply board. The ARM processor board communicates with the spacecraft's Command and <b>Data</b> <b>Handler</b> (C&DH) computer through a serial PPP link.|$|E
40|$|Abstract. This short {{paper is}} aimed {{to discuss the}} {{challenges}} for location-based services and proposes our framework, which {{makes it possible to}} obtain information from heterogeneous sources, and further set up the collaboration between Data Repositories and derived Top Hits Repository to improve the request-response efficiency. In our framework, the <b>Data</b> <b>Handler,</b> Profile Manager, Data Repository and TOP Hits Repository are key components. Through analyzing user profiles and location, <b>Data</b> <b>Handler</b> can locate suitable data sources and keep frequent queries and their answers in TOP Hits Repository for later requests. ...|$|E
5000|$|Within QuickTime, {{there were}} image codecs, media <b>handlers,</b> media <b>data</b> <b>handlers,</b> video {{digitizer}} drivers, file format importers and exporters, and many others.|$|R
50|$|In September 30, 2011, Secure Data Recovery Services {{became the}} first company from the data {{recovery}} sector to be awarded the Statement on Standards for Attestation Engagements (SSAE) 16 Type II Certification. The SSAE 16 Type II is given by the American Institute of Certified Public Accountants. SSAE / SSA certifications became important for <b>data</b> <b>handlers</b> {{after the passage of}} Sarbanes-Oxley Act of 2002.|$|R
5000|$|Powerful <b>data</b> triggers, event <b>handlers,</b> {{and push}} notification.|$|R
40|$|The {{real-time}} {{operational requirements}} for SARCOM translation into a high speed image <b>data</b> <b>handler</b> and processor {{to achieve the}} desired compression ratios and {{the selection of a}} suitable image data compression technique with as low as possible fidelity (information) losses and which can be implemented in an algorithm placing a relatively low arithmetic load on the system are described...|$|E
40|$|Part 1 : Networks and System ArchitectureInternational audienceDigital {{television}} enables IP data delivery using various protocols. Hybrid television HbbTV enhances {{digital television}} with applications delivery. HBB-Next is an architecture which enhances HbbTV with additional features. However {{it does not}} specify IP data delivery despite it has access to both broadcast and broadband channel. This paper proposes architecture and protocols for IP data delivery in HBBNext. To achieve this goal we designed new node (Application <b>Data</b> <b>Handler</b> - ADH) in HBB-Next architecture and new communication protocols (Application <b>Data</b> <b>Handler</b> Control Protocol - ADHCP, and Hybrid Encapsulation Protocol - HEP) for data transmission. We created Stochastic Petri Net (SPN) model of designed protocols and implemented them in ns 2 network simulator to verify our solution. Results of SPN model simulation and ns 2 network simulation are discussed and HEP protocol is compared to existing encapsulation protocols used in DVB systems...|$|E
40|$|Geographic Information Systems (GIS) {{industry}} sources {{quote the}} ratio of power users to casual users at 1000 : 1, within New Zealand this figure {{has been found to}} be 30 : 1. The casual user is often under-supported, with slow and cumbersome viewing tools. This project implements a full data download system in Java for use with Genasys (New Zealand) GIS software. Three components were developed; a vector <b>data</b> <b>handler,</b> an image download system, and a database client. These components were integrated to form a powerful client that offered a significant performance increase over the "server based" client. The image download system outperformed the "server based" client by over 400 %. The vector <b>data</b> <b>handler</b> outperformed the "server based" client by over 50 %, while the database client was over 250 % quicker. GIS users rated all components to be of significant benefit, offering improved performance over their current GIS viewing tools. The work completed in this thesis provides Genasys (New Zealand) a useful tool to enable powerful, fast and stable Java based GIS viewing clients. Keywords: GIS, Java, computer graphics, image pyramid...|$|E
40|$|Her Majestys Revenue & Customs (HMRC) {{was born}} out of the need to create a UK tax {{authority}} by merging both the Inland Revenue and HM Customs & Excise into one department. HMRC encounters spreadsheets in tax-payers systems on a very regular basis as well as being a heavy user of spreadsheets internally. The approach to spreadsheet risk assessment and spreadsheet audit is by the use of trained computer auditors and <b>data</b> <b>handlers.</b> This, by definition, limits the use of our specialist spreadsheet audit tool to such trained staff. In order to tackle the growing use of spreadsheets, a new way of approaching the problem has been piloted. The aim is to issue all staff who come across spreadsheets with a simple to use analysis and risk assessment tool, based on the departmental software SpACE (Spreadsheet Audit & Compliance Examination). Comment: 5 Page...|$|R
40|$|Software {{development}} remains complex, {{expensive and}} risky. Thus, {{the need for}} reusability has become obvious. Through reusability web developers/engineers can save countless hours {{and millions of dollars}} in development cost. Object-oriented (OO) framework mechanism provides a new vehicle for reuse and enables web engineers to customize or reuse various aspects in web engineering such as customizing one or more elements of user interface, automatically generate event <b>handlers,</b> <b>data</b> <b>handlers</b> or related <b>data</b> elements based on changes made by them. The OO framework mechanism includes tags and scripts that predefine some generic web application activities and a common programming interface for generating a framework customization environment. This study provides a novel solution to produce high quality web applications within a shortest development timeframe through the means of customization, reusability, extensibility and flexibility. At the end, this study will conduct a comprehensive evaluation on the proposed OO framework. Building on previous works, this study emphasized the reuse of design, code and testing as a tool to uncover strengths and weaknesses of the OO framework for dynamic web engineering...|$|R
50|$|Vela Trading Technologies is an {{independent}} provider of high performance trading and market data technology. The company provides market <b>data</b> feed <b>handlers,</b> consolidated market <b>data</b> feeds and low latency market access tools for trading venues across North America, Asia and Europe. Its clients include investment banks, hedge funds, trading technology software providers and proprietary trading firms. The company was known as SR Labs until June 20, 2016, when it was renamed Vela.|$|R
40|$|In the Phase I project we {{concentrated}} on three technical objectives {{to demonstrate the}} feasibility of the Phase II project: (1) {{the development of a}} parallel MDSplus <b>data</b> <b>handler,</b> (2) the parallelization of existing fusion data analysis packages, and (3) the development of techniques to automatically generate parallelized code using pre-compiler directives. We summarize the results of the Phase I research for each of these objectives below. We also describe below additional accomplishments related {{to the development of the}} TaskDL and mpiDL parallelization packages...|$|E
40|$|The {{objective}} of this project {{is to improve the}} accuracy of crop estimation about brazilian main agricuItural products. The data obtained by LANDSAT, SPOT and NOAA satellite series are combined in a integrated view, complemented with data provided by meteorological stations and field surveying. The project required the development of software tools, Iike the NOAA/AVHRR image <b>data</b> <b>handler</b> and the data base application that affords integration of different data sources. These tools provide support to crop estimation and diagnosis over the observed areas. Pages: 128 - 13...|$|E
40|$|The INTEGRAL Radiation Environment Monitor (IREM) is a payload {{supporting}} instrument {{on board}} the INTEGRAL satellite. The monitor continually measures electron and proton fluxes along the orbit and provides this information to the spacecraft on board <b>data</b> <b>handler.</b> The mission alert system broadcasts it to the payload instruments enabling them to react accordingly to the current radiation level. Additionally, the IREM conducts its autonomous research mapping the Earth radiation environment for the space weather program. Its scientific data are available for further analysis almost without delay. Comment: 5 pages, 7 figures, accepted for publication in A+A letter...|$|E
50|$|Since {{events are}} stateless, a {{mechanism}} {{is required to}} share <b>data</b> between event <b>handlers.</b> This mechanism is the Global Context. The Global context functions basically as a large map of data divided up into various zones with different lifetimes, properties and persistence.|$|R
30|$|Social {{benefits}} are {{the third most}} cited benefits. Social {{benefits are}} a group of benefits that improves the interaction between <b>data</b> <b>handlers</b> and the <b>data</b> itself, through practices and data attributes. Social benefits are data sharing, promoting openness, enhanced collaboration [W 9], increased visibility, greater visibility [W 18], and greater transparency [W 13]. These benefits improve the way data consumers, publishers, and intermediaries handle {{and deal with the}} data and their products and services. The scientific community, funding agencies, governments, and society are being benefited by these social benefits and increasingly cite them, thus adding value to products and services, roles in science, reproduce research, make data accessible, and advance and accelerate research studies and innovation [W 13]. Society’s trust increases when governments are more transparent [W 10]. When the data become available, success in using them depends on the social and professional context of its community of users [W 9] and the audit tools available for the community [W 16] in order to help their decision-making [W 42]. Participation and collaboration from the community help to promote Open Data, governmental transparency and create innovation through public usages [W 39]. Study [W 6] shows that the effectiveness of aid programs can be improved by providing transparent insight into aid activities.|$|R
40|$|Ultracam {{is a high}} speed, three channel CCD camera {{designed}} to provide imaging photometry at high temporal resolution, allowing the study of rapidly changing astronomical phenomena such as eclipses, rapidly flickering light curves and occultation events. It is {{designed to}} provide frame rates up to 500 Hz with minimum inter-frame dead time and to time-tag each frame to within 1 millisecond of UT. The high data rates that this instrument produces, together with its use as a visitor instrument at a number of observatories, have lead to a highly modular design. Each major service (camera, control, sequencing, <b>data</b> <b>handlers,</b> etc.) is a separate process that communicates using XML documents via HTTP transport, allowing the services to be redeployed or reconfigured with minimal effort. The use of XML and HTTP also allows a web browser to act as a front end for any of the services, as well as providing easy access to services from other control systems. The overall design allows for simple re-engineering for a variety of imaging systems, and is already expected to provide control of IR arrays for the UKIRT Wide-Field Camera project. The instrument has been successfully commissioned on the William Herschel Telescope, and several more observing runs are expected in the near future. Keywords...|$|R
40|$|A {{framework}} {{concept for}} {{design and implementation}} of medical workstations is described by (a) its underlying principles, (b) the handlers provided by the concept, (c) the available data structures and (d) the graphical user interface (GUI). The design principle takes care of a modular approach both for the framework and for the applications. The GUI provides a coherent look and feel for applications based on toolkits for displaying data objects and application control. The <b>data</b> <b>handler</b> allows management of n-dimensional data matrices in a multi-vendor environment, whereas the parameter handler {{takes care of the}} data object description. An implementation of a medical workstation exploiting the framework concept is presented...|$|E
40|$|We {{discuss the}} {{security}} design issues in providing secure updates to the write-shared object among users across different administering domains. In general, {{it is difficult}} to assume a dedicated central server for serializing updates and authenticating collaborators in a write-write sharing across administering domains. Hence, we have proposed a decentralized inter-domain data management method, which has been exemplified in Self-administering <b>Data</b> <b>Handler</b> Model [1]. This paper discusses security issues of the SDH: the authentication (identity), the authorization (access control), the data integrity, content confidentiality, and the traffic confidentiality. We also discuss that the SDH model is not more vulnerable to the computer virus attack than other inter-domain applications. 1...|$|E
40|$|The {{concepts}} {{of an integrated}} design data base system (DBMS) as it might apply to an electronic design company are discussed. Data elements of documentation, project specifications, project tracking, firmware, software, electronic and mechanical design can be integrated and managed through a single DBMS. Combining the attributes of a DBMS <b>data</b> <b>handler</b> with specialized systems and functional data can provide users with maximum flexibility, reduced redundancy, and increased overall systems performance. Although some system overhead is lost due to redundancy in transitory data, it is believed {{the combination of the}} two data types is advisable rather than trying to do all data handling through a single DBMS...|$|E
40|$|Purpose - The {{purpose of}} this paper is to discuss the impact of an Enterprise Resources Planning (ERP) system on the role of accountants, to provide job {{qualifications}} for their reference. Design/methodology/approach - This research adopts the case study method, using on-the-spot interviews and a questionnaire to find out the effects of an ERP system on the role of accountants. Findings - The role of accountants is mainly to be transaction <b>data</b> <b>handlers</b> and financial report providers. Clearly, accountants must have certain degree of knowledge in the realm of traditional finance accounting. In addition, accounting supervisors think implementing an ERP system changes the role of accountants. Research limitations/implications - The data collected by the authors are mainly from the Shanghai Financial Center and regional businesses in Shanghai, Beijing and Taiwan. The authors are only able to study the impact of ERP systems on the role of accountants in the short-term rather than in the long-term. Originality/value - It is widely accepted that an ERP system is more than just an accounting information system, so implementing an ERP system will not necessarily promote the positions of the accounting department and accountants on the subjective cognition of accountants. Accountants need to have knowledge of financial accounting, IT and management after ERP implementation...|$|R
40|$|The Mark 3 {{very long}} {{baseline}} interferometry (VLBI) system, comprising a complete {{end to end}} VLBI system optimized for both high accuracy geodesy and radio astronomy, is described. The data flow, the <b>data</b> base <b>handler</b> system, and the field station component and configurations are briefly discussed. The use of mobile and transportable stations allows measurements to be taken from {{a large number of}} sites with relatively few sets of equipment. Fixed stations form a long term reference network for tying together the measurements with the mobile and transportable stations...|$|R
40|$|This paper {{proposes a}} system {{software}} structure which encapsulates {{the processing of}} continuous-media <b>data</b> into stream <b>handlers</b> of a real-time environment. This environment is controlled by traditional non-real-time functions for resource, buffer and continuous-media data stream management. On top of this system, distributed multimedia applications can be built. 1 Introductio...|$|R
40|$|Major {{system design}} {{features}} of a distributed data management system for the NASA Deep Space Network (DSN) designed for continuous two-way deep space communications are described. The reasons for which the distributed data base utilizing third-generation minicomputers is selected as the optimum approach for the DSN are threefold: (1) with a distributed master data base, valid data is available in real-time to support DSN management activities at each location; (2) data base integrity {{is the responsibility of}} local management; and (3) the data acquisition/distribution and processing power of a third-generation computer enables the computer to function successfully as a <b>data</b> <b>handler</b> or as an on-line process controller. The concept of the distributed data base is discussed along with the software, data base integrity, and hardware used. The data analysis/update constraint is examined...|$|E
40|$|Reactive {{magnetron}} sputtering {{was used to}} deposit multilayer optical coatings as well as Rugate filters. For the combination of larger areas, high throughput and high precision of the films, a rotary platter coater was developed which uses reactive pulsed {{magnetron sputtering}}. Four substrates of 40 cm diameter can be processed simultaneously. For an optimized homogeneity and film quality, linear double magnetrons were implemented. Plasma process control is made by a lambda probe oxygen partial pressure measurement {{as well as by}} optical plasma emission spectroscopy. For in-situ process control of the growing films, a fast single wavelength in-situ ellipsometer and a spectral reflectometer is adapted and implemented into the process control softwase. All sensors are integrated via a <b>data</b> <b>handler</b> into the SPS process control. First results for the deposition of a multilayer filter as well as of Rugate filters are presented and are compared to results of RF magnetron sputtered filters...|$|E
40|$|In {{the era of}} {{information}} processing and use {{of information}} technology, speech recognition and processing plays {{a very important role}} for many human computing tasks. Speech pattern and Speech recognition is fundamental for speech analysis in speech data handling system. No doubt it allows all users to communicate with computers and along with electronic devices using any natural language. Today the potential power of the PCs enables the implementation of the novel method proposed in this paper. The prime concern is that none of the software is freely available, user friendly and cross platform. Therefore it necessitates using open-source programming language to an Open Ended System for Speech <b>Data</b> <b>Handler</b> using self created Hexapod: a six legged robot. This system facilitates the way any electronic device with an UART port or USB port can be controlled using speech commands. Keywords speech pattern, speech data processing, speech recognizer...|$|E
40|$|Building up new {{collections}} {{for digital}} libraries is a demanding task. Available data sets {{have to be}} extracted which is usually done {{with the help of}} software developers as it involves custom <b>data</b> <b>handlers</b> or conversion scripts. In cases where the desired data is only available on the data provider's website custom web scrapers are needed. This may be the case for small to medium-size publishers, research institutes or funding agencies. As data curation is a typical task that is done by people with a library and information science background, these people are usually proficient with XML technologies but are not full-stack programmers. Therefore we would like to present a web scraping tool that does not demand the digital library curators to program custom web scrapers from scratch. We present the open-source tool OXPath, an extension of XPath, that allows the user to define data to be extracted from websites in a declarative way. By taking one of our own use cases as an example, we guide you in more detail through the process of creating an OXPath wrapper for metadata harvesting. We also point out some practical things to consider when creating a web scraper (with OXPath). On top of that, we also present a syntax highlighting plugin for the popular text editor Atom that we developed to further support OXPath users and to simplify the authoring process...|$|R
40|$|Big data is {{the serious}} problem for <b>data</b> <b>handlers</b> the {{large amounts of}} data are created in day to day life. Online Social Networks (OSN’s) {{are one of the}} reasons for big data. Where anyone fires a query for they get a result from one {{particular}} database and it should be limited one. But if data come from multiple web databases, then it contains more results as compared to single database. The advantage of using multiple web databases is that it gets more relevant data. To address the problem of record matching in the Web database scenario, it present an unendorsed, online record matching method, UDD, which, for a given query, can efficiently identify duplicates from the query result records of multiple Web databases. After elimination of the same-source duplicates, the “presumed” non duplicate records from the same source can be used as training examples alleviating the burden of users having to manually label it. Starting from the non duplicate set, it use two cooperating classifiers, a weighted element similarity summing classifier and an SVM classifier, to iteratively recognize duplicates in the query results from multiple Web databases. Experimental results demonstrate that UDD works well for the Web database scenario where existing supervised methods do not apply. For this it used two databases Google and Faroo. With the initiation of information technology, a user is able to access relevant information from the World Wide Web, which contains a vast amount of information, simply and quickly by inflowing search queries. In response to information and deliver it directly to the user...|$|R
40|$|A {{procedure}} to habituate Highland Cattle calves to humans evolved with four years' experimentation on one farm by one male human <b>handler.</b> <b>Data</b> includes results of 18 bull and 20 heifer calves, from 14 dams and 4 sires. In 24 cases, they were fearless {{and easy to}} handle all the time. Fearful 8 heifer and 6 bull calves got little or unpleasant handling during first weeks...|$|R
40|$|We {{demonstrate}} Sensor Andrew, {{an infrastructure}} for Internet-scale sensing and actuation {{across a wide}} range of heterogeneous devices. The goal of Sensor Andrew is to enable a variety of ubiquitous large-scale monitor-ing and control applications {{in a way that is}} extensi-ble, easy to use, and secure while maintaining privacy. The core architecture is built around the eXtensible Mes-saging and Presence Protocol (XMPP) where transduc-ers are modeled as event nodes in a push-based publish-subscribe architecture. Sensor Andrew allows for easy integration of new sensors as well as support for legacy systems. A <b>data</b> <b>handler</b> provides registration, discovery and data logging facilities for each device. The major elements of this architecture have been deployed in five buildings at Carnegie Mellon University, and are com-prised of over 1000 sensing points reporting data from multiple communication interfaces. Our demonstration will showcase already active sensor systems on campus using Sensor Andrew through a web interface as well as the ability to locally configure sensor and actuator inter-actions as an example of how larger-scale applications could be built...|$|E
40|$|In {{this paper}} we present the Quonto Inconsistent <b>Data</b> <b>handler</b> (QuID). QuID is a reasoner for OWL 2 QL {{that is based}} on the system Quonto and is able to deal with {{inconsistent}} ontologies. The central aspect of QuID is that it implements two different, orthogonal strategies for dealing with inconsistency: ABox repairing techniques, based on data manipulation, and consistent query answering techniques, based on query rewriting. Moreover, by exploiting the ability of Quonto to delegate the management of the ABox to a relational database system (DBMS), such techniques are potentially able to handle very large inconsistent ABoxes. For the above reasons, QuID allows for experimentally comparing the above two different strategies for inconsistency handling in the context of OWL 2 QL. We thus report on the experimental evaluation that we have conducted using QuID. Our results clearly point out that inconsistency-tolerance in OWL 2 QL ontologies is feasible in practical cases. Moreover, our evaluation singles out the different sources of complexity for the data manipulation technique and the query rewriting technique, and allows for identifying the conditions under which one method is more efficient than the other. © 2012 Springer-Verlag Berlin Heidelberg...|$|E
40|$|This {{project was}} devoted to the {{development}} of a software package, called the Orbiter Flying Qualities (OFQ) Workstation, for working with the OFQ Archives which are specially selected sets of space shuttle entry flight data relevant to flight control and flying qualities. The basic approach to creation of the workstation software was to federate and extend commercial software products to create a low cost package that operates on personal computers. Provision was made to link the workstation to large computers, but the OFQ Archive files were also converted to personal computer diskettes and can be stored on workstation hard disk drives. The primary element of the workstation developed in the project is the Interactive <b>Data</b> <b>Handler</b> (IDH) which allows the user to select data subsets from the archives and pass them to specialized analysis programs. The IDH was developed as an application in a relational database management system product. The specialized analysis programs linked to the workstation include a spreadsheet program, FREDA for spectral analysis, MFP for frequency domain system identification, and NIPIP for pilot-vehicle system parameter identification. The workstation also includes capability for ensemble analysis over groups of missions...|$|E
40|$|OBJECTIVES. This study {{evaluated}} {{the production of}} dry fermented salami associated with an outbreak of Escherichia coli O 157. H 7 infection in Washington State and California. METHODS. Facility inspections, review of plant monitoring <b>data,</b> food <b>handler</b> interviews, and microbiological testing of salami products were conducted. RESULTS. Production methods complied with federal requirements and industry-developed good manufacturing practices. No evidence suggested that postprocessing contamination occurred. Calculations suggested that the infectious dose was smaller than 50 E. coli O 157 :H 7 bacteria. CONCLUSIONS. Dry fermented salami {{can serve as a}} vehicle of transmission for O 157 :H 7 strains. Our investigation and prior laboratory studies suggest that E. coli O 157 :H 7 can survive currently accepted processing methods...|$|R
40|$|Background Retained {{placenta}} {{is associated}} with post-partum haemorrhage. Meta-analysis has suggested that umbilical injection of oxytocin could increase placental expulsion {{without the need for}} a surgeon or anaesthetic. We assessed the effect of high-dose umbilical vein oxytocin as a treatment for retained placenta. Methods In this double-blind, placebo-controlled trial, haemodynamically stable women with a retained placenta for more than 30 min were recruited from 13 sites in the UK, Uganda, and Pakistan. 577 women were randomly assigned by a computer-generated randomisation list stratified by centre to 30 mL saline containing either 50 IU oxytocin (n= 292) or 5 mL water (n= 285), which was injected into the placenta through an umbilical vein catheter. All trial participants, study workers, and <b>data</b> <b>handlers</b> were masked to individual allocations. The primary outcome was the need for manual removal of the placenta. Analysis was by intention to treat. This study is registered, number ISRCTN 13204258. Findings The primary outcome was recorded for all participants. We detected no difference between the groups in the need for manual removal of placenta (oxytocin 179 / 292 [61 · 3 %] vs placebo 177 / 285 [62 · 1 %]; relative risk 0 · 98, 95 % CI 0 · 87 — 1 · 12; p= 0 · 84). The need for manual removal was higher in the UK (overall 250 / 361 [69 %]) than in Uganda (90 / 190 [47 %]) or Pakistan (16 / 26 [62 %]). Adverse events did not differ between the two groups. Interpretation Umbilical oxytocin has no clinically significant effect on the need for manual removal for women with retained placenta. Funding WHO, WellBeing of Women, Pakistan Higher Education Commission...|$|R
40|$|Irregular {{scientific}} {{applications are}} {{often difficult to}} parallelize due to elaborate dynamic data structures with complicated communication patterns. We describe flexible data orchestration abstractions that enable the programmer to express customized communication patterns arising in an important class of irregular computations [...] -adaptive finite difference methods for partial differential equations. These abstractions are supported by KeLP, a C++ run-time library. KeLP enables the programmer to manage spatial data dependence patterns and express <b>data</b> motion <b>handlers</b> as first-class mutable objects. Using two finite difference applications, we show that KeLP's flexible communication model effectively manages elaborate data motion arising in semi-structured adaptive methods. 1 Introduction Many scientific numerical methods employ structured irregular representations to improve accuracy. Irregular representations can resolve fine physical structures in complicated geometri [...] ...|$|R
