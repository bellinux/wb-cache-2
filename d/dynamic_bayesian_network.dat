727|10000|Public
25|$|The hidden markov {{model can}} be {{represented}} as the simplest <b>dynamic</b> <b>Bayesian</b> <b>network.</b> The mathematics behind the HMM were developed by L. E. Baum and coworkers.|$|E
25|$|When {{activity}} {{recognition is}} performed indoors and in cities using the widely available Wi-Fi signals and 802.11 access points, {{there is much}} noise and uncertainty. These uncertainties are modeled using a <b>dynamic</b> <b>Bayesian</b> <b>network</b> model by Yin et al. A multiple goal model that can reason about user's interleaving goals is presented by Chai and Yang, where a deterministic state transition model is applied. A better model that models the concurrent and interleaving activities in a probabilistic approach is proposed by Hu and Yang. A user action discovery model is presented by Yin et al., where the Wi-Fi signals are segmented to produce possible actions.|$|E
2500|$|In {{order to}} fully specify the Bayesian network and thus fully {{represent}} the joint probability distribution, {{it is necessary}} to specify for each node X the probability distribution for X conditional upon Xs parents. The distribution of X conditional upon its parents may have any form. It is common to work with discrete or Gaussian distributions since that simplifies calculations. Sometimes only constraints on a distribution are known; one can then use the principle of maximum entropy to determine a single distribution, the one with the greatest entropy given the constraints. (Analogously, in the specific context of a <b>dynamic</b> <b>Bayesian</b> <b>network,</b> one commonly specifies the conditional distribution for the hidden state's temporal evolution to maximize the entropy rate of the implied stochastic process.) ...|$|E
40|$|This paper {{considers}} the computational power of constant size, <b>dynamic</b> <b>Bayesian</b> <b>networks.</b> Although discrete <b>dynamic</b> <b>Bayesian</b> <b>networks</b> {{are no more}} powerful than hidden Markov models, <b>dynamic</b> <b>Bayesian</b> <b>networks</b> with continuous random variables and discrete children of continuous parents are capable of performing Turing-complete computation. With modified versions of existing algorithms for belief propagation, such a simulation {{can be carried out}} in real time. This result suggests that <b>dynamic</b> <b>Bayesian</b> <b>networks</b> may be more powerful than previously considered. Relationships to causal models and recurrent neural networks are also discussed...|$|R
40|$|Supplementary {{material}} for the paper: ’Bayesian regularization of non-homogeneous <b>dynamic</b> <b>Bayesian</b> <b>networks</b> by globally coupling interaction parameters’ (AISTATS 2012) This paper is a supplement to the main paper ’Bayesian regularization of nonhomogeneous <b>dynamic</b> <b>Bayesian</b> <b>networks</b> by globally coupling interaction parameters’...|$|R
30|$|Most Interaction-aware motion {{models are}} based on <b>Dynamic</b> <b>Bayesian</b> <b>Networks</b> (DBN).|$|R
5000|$|The Graphical Models Toolkit (GMTK), a <b>dynamic</b> <b>Bayesian</b> <b>network</b> {{prototyping}} system ...|$|E
5000|$|... : Modeling gene {{regulatory}} network via global optimization of <b>dynamic</b> <b>bayesian</b> <b>network</b> (released under a GPL license) ...|$|E
5000|$|... #Caption: Simplified <b>Dynamic</b> <b>Bayesian</b> <b>Network.</b> All the {{variables}} {{do not need}} to be duplicated in the graphical model, but they are dynamic, too.|$|E
5000|$|DBmcmc : Inferring <b>Dynamic</b> <b>Bayesian</b> <b>Networks</b> with MCMC, for Matlab (free software) ...|$|R
40|$|We {{address the}} problem of {{pronunciation}} variation in conversational speech with a context-dependent articulatory featurebased model. The model is an extension of previous work using <b>dynamic</b> <b>Bayesian</b> <b>networks,</b> which allow for easy factorization of a state into multiple variables representing the articulatory features. We build context-dependent decision trees for the articulatory feature distributions, which are incorporated into the <b>dynamic</b> <b>Bayesian</b> <b>networks,</b> and experiment with different sets of context variables. We evaluate our models on a lexical access task using a phonetically transcribed subset of the Switchboard corpus. We find that our models outperform a context-dependent phonetic baseline. Index Terms — Lexical access, articulatory features, <b>dynamic</b> <b>Bayesian</b> <b>networks</b> 1...|$|R
5000|$|... aGrUM: C++ library (with Python bindings) for {{different}} types of PGMs including <b>Bayesian</b> <b>Networks</b> and <b>Dynamic</b> <b>Bayesian</b> <b>Networks</b> (released under the GPLv3) ...|$|R
50|$|The hidden markov {{model can}} be {{represented}} as the simplest <b>dynamic</b> <b>Bayesian</b> <b>network.</b> The mathematics behind the HMM were developed by L. E. Baum and coworkers.|$|E
5000|$|The mutual {{information}} {{is used to}} learn the structure of Bayesian networks/dynamic Bayesian networks, which is thought to explain the causal relationship between random variables, as exemplified by the GlobalMIT toolkit http://code.google.com/p/globalmit/: learning the globally optimal <b>dynamic</b> <b>Bayesian</b> <b>network</b> with the Mutual Information Test criterion.|$|E
50|$|A Hidden Markov model (HMM) is a {{statistical}} Markov {{model in which}} the system being modeled {{is assumed to be}} a Markov process with unobserved (hidden) states. An HMM can be considered as the simplest <b>dynamic</b> <b>Bayesian</b> <b>network.</b> HMM models are widely used in speech recognition, for translating a time series of spoken words into text.|$|E
40|$|Although n-gram {{models are}} still the de facto {{standard}} in language modeling for speech recognition, more sophisticated models achieve better accuracy by taking additional information, such as syntactic rules, semantic relations or domain knowledge into account. Unfortunately, most of the effort in developing such models goes into the implementation of handcrafted inference routines. A generic mechanism to introduce background knowledge into a language model is lacking. We propose using <b>dynamic</b> <b>Bayesian</b> <b>networks.</b> <b>Dynamic</b> <b>Bayesian</b> <b>networks</b> are a generalization of the n-gram models and HMMs traditionally used in language modeling and speech recognition. Whereas those models use a single random variable to represent state, <b>Bayesian</b> <b>networks</b> can have any number of variables. As such they are particularly well-suited {{for the construction of}} models that take additional information into account. This paper discusses language modeling with <b>Bayesian</b> <b>networks.</b> Examples of <b>Bayesian</b> <b>network</b> implementations of wellknown language models are given and a novel topic-based language model is presented. Index Terms: language modeling, <b>dynamic</b> <b>Bayesian</b> <b>networks.</b> 1...|$|R
5000|$|For simplicity, we {{describe}} the algorithm on hidden Markov models. It can be easily generalized to <b>dynamic</b> <b>Bayesian</b> <b>networks</b> by using a junction tree.|$|R
40|$|A principled {{mechanism}} for identifying conditional dependencies in time-series data is provided through structure learning of <b>dynamic</b> <b>Bayesian</b> <b>networks</b> (DBNs). An important assumption of DBN structure learning {{is that the}} data are generated by a stationary process—an assumption {{that is not true}} in many important settings. In this paper, we introduce a new class of graphical models called non-stationary <b>dynamic</b> <b>Bayesian</b> <b>networks,</b> in which the conditional dependence structure of the underlying data-generation process is permitted to change over time. Non-stationary <b>dynamic</b> <b>Bayesian</b> <b>networks</b> represent a new framework for studying problems in which the structure of a network is evolving over time. We define the non-stationary DBN model, present an MCMC sampling algorithm for learning the structure of the model from time-series data under different assumptions, and demonstrate the effectiveness of the algorithm on both simulated and biological data. ...|$|R
50|$|When {{activity}} {{recognition is}} performed indoors and in cities using the widely available Wi-Fi signals and 802.11 access points, {{there is much}} noise and uncertainty. These uncertainties are modeled using a <b>dynamic</b> <b>Bayesian</b> <b>network</b> model by Yin et al. A multiple goal model that can reason about user's interleaving goals is presented by Chai and Yang, where a deterministic state transition model is applied. A better model that models the concurrent and interleaving activities in a probabilistic approach is proposed by Hu and Yang. A user action discovery model is presented by Yin et al., where the Wi-Fi signals are segmented to produce possible actions.|$|E
50|$|Especially in intelligence, both {{governmental}} and business, analysts {{must always be}} aware that the opponent(s) is intelligent and may be generating information intended to deceive. Since deception often {{is the result of a}} cognitive trap, Elsaesser and Stech use state-based hierarchical plan recognition (see abductive reasoning) to generate causal explanations of observations. The resulting hypotheses are converted to a <b>dynamic</b> <b>Bayesian</b> <b>network</b> and value of information analysis is employed to isolate assumptions implicit in the evaluation of paths in, or conclusions of, particular hypotheses. As evidence in the form of observations of states or assumptions is observed, they can become the subject of separate validation. Should an assumption or necessary state be negated, hypotheses depending on it are rejected. This is a form of root cause analysis.|$|E
5000|$|In {{order to}} fully specify the Bayesian network and thus fully {{represent}} the joint probability distribution, {{it is necessary}} to specify for each node X the probability distribution for X conditional upon Xs parents. The distribution of X conditional upon its parents may have any form. It is common to work with discrete or Gaussian distributions since that simplifies calculations. Sometimes only constraints on a distribution are known; one can then use the principle of maximum entropy to determine a single distribution, the one with the greatest entropy given the constraints. (Analogously, in the specific context of a <b>dynamic</b> <b>Bayesian</b> <b>network,</b> one commonly specifies the conditional distribution for the hidden states temporal evolution to maximize the entropy rate of the implied stochastic process.) ...|$|E
40|$|<b>Dynamic</b> <b>Bayesian</b> <b>networks</b> are {{a special}} type of <b>Bayesian</b> <b>networks,</b> which {{explicitly}} {{deal with the}} dimension of time. They are distinguished into repetitive and non-repetitive networks. Repetitive networks have {{the same set of}} random (statistical) variables and independence relations at each time step, whereas in non-repetitive networks the set of random variables and the independence relations between these random variables may vary in time. Due to their structural symmetry, repetitive networks are easier to use and are, therefore, often taken as a standard. However, repetitiveness is a very strong assumption, which normally does not hold, since particular dependences and independences may only hold at certain time steps. In this paper, we propose a new framework for the modularisation of non-repetitive <b>dynamic</b> <b>Bayesian</b> <b>networks,</b> which offers a practical approach to coping with the computational and structural difficulties associated with <b>dynamic</b> <b>Bayesian</b> <b>networks.</b> This framework is based on separating temporal and atemporal independence relations. We investigate properties of the modularisation and show the separation to be compositive. ...|$|R
40|$|Abstract—This paper {{compares the}} {{performance}} of inference in static and <b>dynamic</b> <b>Bayesian</b> <b>Networks.</b> For the comparison both kinds of <b>Bayesian</b> <b>networks</b> are created for the exemplary application activity recognition. Probability {{and structure of the}} <b>Bayesian</b> <b>Networks</b> have been learnt automatically from a recorded data set consisting of acceleration data observed from an inertial measurement unit. Whereas dynamic networks incorporate temporal dependencies which affect the quality of the activity recognition, inference is less complex for dynamic networks. As performance indicators recall, precision and processing time of the activity recognition are studied in detail. The results show that <b>dynamic</b> <b>Bayesian</b> <b>Networks</b> provide considerably higher quality in the recognition but entail longer processing times...|$|R
30|$|A {{number of}} more complex methods {{have also been}} {{explored}} in the literature, including non-linear time series [27], Principal Component Analysis [28], Gaussian Mixtures [29] and <b>Dynamic</b> <b>Bayesian</b> <b>Networks</b> [30].|$|R
50|$|A <b>Dynamic</b> <b>Bayesian</b> <b>Network</b> (DBN) is a Bayesian network which relates {{variables}} {{to each other}} over adjacent time steps. This is often called a Two-Timeslice BN (2TBN) because it says that {{at any point in}} time T, the value of a variable can be calculated from the internal regressors and the immediate prior value (time T-1). DBNs were developed by Paul Dagum in the early 1990s when he led research funded by two National Science Foundation grants at Stanford University's Section on Medical Informatics. Dagum developed DBNs to unify and extend traditional linear state-space models such as Kalman filters, linear and normal forecasting models such as ARMA and simple dependency models such as hidden Markov models into a general probabilistic representation and inference mechanism for arbitrary nonlinear and non-normal time-dependent domains.|$|E
40|$|This paper {{presents}} a methodology for real-time estimation of water distribution system state parameters using a <b>dynamic</b> <b>Bayesian</b> <b>network</b> to combine current observations {{with knowledge of}} past system behavior. The <b>dynamic</b> <b>Bayesian</b> <b>network</b> presented here allows the flexibility to model both discrete and continuous variables and represent causal relationships that exist within the distribution system. The posterior belief state can be inferred using a compact approximation algorithm that {{has been shown to}} contain inference errors. Simulations over stochastic variables are proposed to define the transition and observation models for the <b>dynamic</b> <b>Bayesian</b> <b>network...</b>|$|E
40|$|International audience—In this work, {{we propose}} a novel {{system for the}} {{recognition}} of handwritten Arabic words. It is evolved based on horizontal-vertical Hidden Markov Model and <b>Dynamic</b> <b>Bayesian</b> <b>Network</b> Model. Our strategy consists of looking for various HMM architectures and selecting those which provide the best recognition performance. Experiments on handwritten Arabic words from IFN/ENIT strongly support the feasibility of the proposed approach. The recognition rates achieve 92. 19 % with horizontal-vertical Hidden Markov Model and 88. 82 % with a <b>Dynamic</b> <b>Bayesian</b> <b>Network...</b>|$|E
25|$|Efficient {{algorithms}} {{exist that}} perform inference {{and learning in}} <b>Bayesian</b> <b>networks.</b> <b>Bayesian</b> <b>networks</b> that model sequences of variables (e.g. speech signals or protein sequences) are called <b>dynamic</b> <b>Bayesian</b> <b>networks.</b> Generalizations of <b>Bayesian</b> <b>networks</b> that can represent and solve decision problems under uncertainty are called influence diagrams.|$|R
40|$|<b>Dynamic</b> <b>Bayesian</b> <b>networks</b> (DBNs) are {{becoming}} widely used to learn gene regulatory networks from time series microarray data. Careful experimental design {{is required for}} data generation, {{because of the high}} cost of running each microarray experiment. This paper presents a theoretical analysis of learning DBNs without hidden variables from time series data. The analysis reveals, among other lessons, that under a reasonable set of assumptions a fixed budget is better spent on many short time series than on a few long time series. Keywords: <b>dynamic</b> <b>Bayesian</b> <b>networks,</b> gene expression microarrays, gene regulatory networks, PAC-learnability, time series dat...|$|R
40|$|Abstract. <b>Bayesian</b> <b>networks</b> for {{the static}} {{as well as}} for the dynamic case have gained an {{enormous}} interest in the research community of machine learning and pattern recognition. Although the parallels between <b>dynamic</b> <b>Bayesian</b> <b>networks</b> and Kalman filters are well-known since many years, <b>Bayesian</b> <b>networks</b> have not been applied to problems in the area of adaptive control of dynamic systems. In our work we exploit the well-known similarities between <b>Bayesian</b> <b>networks</b> and Kalman filters to model and control linear dynamic systems using <b>dynamic</b> <b>Bayesian</b> <b>networks.</b> The analytical models are compared with models being trained with step and impulse response. The experiments show that the analytical model as well as the trained model are suitable for control purposes, which leads to the idea of self adaptive controllers...|$|R
40|$|In this paper, air combat {{simulation}} data is reconstructed into a <b>dynamic</b> <b>Bayesian</b> <b>network.</b> It gives {{a compact}} probabilistic model {{that describes the}} progress of air combat and allows efficient computing for study of different courses of the combat. This capability is used in what-if type analysis that investigates the effect of different air combat situations on the air combat evolution and outcome. The utilization of the <b>dynamic</b> <b>Bayesian</b> <b>network</b> is illustrated by analyzing simulation results produced with a discrete event air combat simulation model called X-Brawler. ...|$|E
40|$|Abstract Background In {{computational}} biology, one often {{faces the}} problem of deriving the causal relationship among different elements such as genes, proteins, metabolites, neurons and so on, based upon multi-dimensional temporal data. Currently, there are two common approaches used to explore the network structure among elements. One is the Granger causality approach, {{and the other is}} the <b>dynamic</b> <b>Bayesian</b> <b>network</b> inference approach. Both have at least a few thousand publications reported in the literature. A key issue is to choose which approach is used to tackle the data, in particular when they give rise to contradictory results. Results In this paper, we provide an answer by focusing on a systematic and computationally intensive comparison between the two approaches on both synthesized and experimental data. For synthesized data, a critical point of the data length is found: the <b>dynamic</b> <b>Bayesian</b> <b>network</b> outperforms the Granger causality approach when the data length is short, and vice versa. We then test our results in experimental data of short length which is a common scenario in current biological experiments: it is again confirmed that the <b>dynamic</b> <b>Bayesian</b> <b>network</b> works better. Conclusion When the data size is short, the <b>dynamic</b> <b>Bayesian</b> <b>network</b> inference performs better than the Granger causality approach; otherwise the Granger causality approach is better. </p...|$|E
40|$|We {{present a}} generic {{formulation}} of self- and cross-correcting Bayesian trackers using a <b>Dynamic</b> <b>Bayesian</b> <b>Network.</b> Correction operations in a tracker such as parameter tuning, model updates and re-initialization are represented using hidden variables {{together with the}} target state and measurement variables in the <b>Dynamic</b> <b>Bayesian</b> <b>network</b> model. The representation allows one to model different self- and cross-correcting tracking frameworks under the same formulation and facilitates comparison and the design of new trackers. The proposed model is demonstrated with three state-of-the-art trackers {{that are based on}} different principles to implement online correction of target tracking...|$|E
50|$|Bayesian {{programming}} {{may also}} be seen as an algebraic formalism to specify graphical models such as, for instance, <b>Bayesian</b> <b>networks,</b> <b>dynamic</b> <b>Bayesian</b> <b>networks,</b> Kalman filters or hidden Markov models. Indeed, Bayesian Programming is more general than <b>Bayesian</b> <b>networks</b> and has a power of expression equivalent to probabilistic factor graphs.|$|R
40|$|Due to shorter {{life cycles}} {{and more complex}} {{production}} processes the automatic generation of models for control purposes is of great importance. Even though <b>Bayesian</b> <b>networks</b> have proven their usefulness in machine learning and pattern recognition and the close relationship between <b>Dynamic</b> <b>Bayesian</b> <b>networks</b> and Kalman Filters respectively difference equations {{they have not been}} applied to problems in the area of automatic control. In our work we deduce the structure of a <b>Dynamic</b> <b>Bayesian</b> <b>networks</b> using the state space description and difference equations. Both models are trained by the EM algorithm and used for control purposes. The experiments show that both models performs well, but the training process of the model based on difference equations is much more stable...|$|R
40|$|Abstract—This letter {{investigates the}} problem of {{incorporating}} auxiliary information, e. g., pitch, zero crossing rate (ZCR), and rate-of-speech (ROS), for speech recognition using <b>dynamic</b> <b>Bayesian</b> <b>networks.</b> In this letter, we propose switching auxiliary chains for exploiting different auxiliary information tailored to different phonetic states. The switching function can be specified by a priori knowledge or, more flexibly, be learned from data with information-theoretic dependency selection. Experiments on the OGI Numbers database show that the new model achieves 7 % word-error-rate relative reduction by jointly exploiting pitch, ZCR, and ROS, while keeping almost the same parameter size as the standard HMM. Index Terms—Auxiliary features, <b>dynamic</b> <b>Bayesian</b> <b>networks</b> (DBNs), speech recognition...|$|R
