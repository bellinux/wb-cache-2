59|8|Public
50|$|The University of Alberta {{developed}} a Student Oriented Batch Facility in 1971 to provide quick <b>job</b> <b>turnaround</b> for undergrad students learning to program in FORTRAN, ALGOL, PL/C, and 360 Assembler. It was a dedicated punch card input, printer output system that provided 5 minute {{turn around and}} ran several thousands of jobs {{a week at a}} fixed cost per job (15 cents).|$|E
50|$|Turnaround Managers {{are also}} called Turnaround Practitioners , and often are interim {{managers}} who only {{stay as long}} as it takes to achieve the turnaround. Assignments can take anything from 3 to 24 months depending on the size of the organization and the complexity of the <b>job.</b> <b>Turnaround</b> management does not only apply to distressed companies, it in fact can help in any situation where direction, strategy or a general change of the ways of working needs to be implemented. Therefore turnaround management is closely related to change management, transformation management and post-merger-integration management. High growth situation for example are one typical scenario where turnaround experts also help. More and more turnaround managers are becoming a one-stop-shop and provide help with corporate funding (working closely with banks and the Private Equity community) and with professional services firms (such as lawyers and insolvency practitioners) to have access to a full range of services that are typically needed in a turnaround process. Most turnaround managers are freelancers and work on day rates. The job often involves frequent travel. Others work for large corporations and have permanent positions.|$|E
3000|$|... • Hard disk bytes read is {{the factor}} {{that has the}} third {{greatest}} influence (16.431 % of the contribution) on the <b>job</b> <b>turnaround.</b>|$|E
30|$|The worst {{performance}} was obtained with VMR- 30 R 2, when using more reduce work units, {{with a low}} replication factor. This {{is due to the}} increased probability of a slow or faulty node being assigned a reduce task. With 30 reduce work units, and a replication factor of 2, there are 60 tasks that may negatively impact the MapReduce <b>job’s</b> <b>turnaround</b> time. On VMR- 15 R 2 there are only 15 reduce work units, which translates to half the total number of tasks.|$|R
50|$|Riihilahti was {{appointed}} {{and has done}} a good <b>turnaround</b> <b>job</b> for HJK's home stadium as the CEO of Sonera Stadium, as well as coach for HJK's reserves, Klubi-04. In December 2013 he {{was appointed}} as the CEO of HJK.|$|R
40|$|Moldable jobs, {{which allow}} {{the number of}} {{allocated}} proces-sors to be adjusted before running in clusters, have attracted increasing concern in parallel job scheduling research. Com-pared with traditional rigid jobs where the number of allo-cated processors is fixed, moldable jobs are more flexible and therefore have more potential for improving their average turnaround time (a crucial metric to describe performance of jobs in a cluster). Average turnaround time of mold-able jobs depends greatly on resource allocation schemes. Unfortunately, existing schemes do not perform well in re-ducing average turnaround time, either because they only consider a single <b>job’s</b> <b>turnaround</b> time instead of the aver-age turnaround time of all jobs, or because they just aim at fairness between short and long jobs instead of their av-erage turnaround time. In this paper, we investigate how resource allocation affects the average turnaround time of moldable jobs in clusters, and propose a scheme named HRF (highest revenue first), which allocates processors according to the highest revenue of shortening runtime. In our sim-ulations, experimental results show that HRF can reduce average turnaround time up to 71 % when compared with state-of-the-art schemes...|$|R
3000|$|... • Load map task {{capacity}} is the factor {{that has the}} second greatest influence (almost 21 % of the contribution) on the <b>job</b> <b>turnaround.</b>|$|E
3000|$|... • Load reduce task {{capacity}} is the factor {{that has the}} most influence (almost 50 % of the contribution) on the <b>job</b> <b>turnaround</b> in this experiment.|$|E
3000|$|The Relief {{results show}} that the {{performance}} measures job processing time and <b>job</b> <b>turnaround,</b> have the highest quality scores (W [...]) and also have the potential to be distinguishing features between the two classes. In this case the performance measure ‘hard disk bytes written’ is also selected by means of the same approach as in the means and variance analysis: in other words, this has in terms of their use to stand out {{from the rest of the}} measures and give more certainty to the analysis of relationships. Thus, the measures job processing time, <b>job</b> <b>turnaround</b> and hard disk bytes written are also selected as candidates to represent the performance of BDA in the Hadoop system.|$|E
40|$|Computer job {{scheduling}} is often performed with little {{understanding of the}} formal properties of the jobs being scheduled. One {{reason for this is}} that optimal solutions for {{job scheduling}} on computers are difficult to obtain if the job stream has mixed objectives, i. e., it consists of some <b>jobs</b> whose <b>turnaround</b> time has to be minimized and others whose deadlines must be met. A practical algorithm for scheduling mixed job streams on monoprogrammed computers, with potential application to a multiprogramming environment is presented. The algorithm takes into account variable cost rates for each job. Experimental results illustrate the efficiency of the algorithm in terms of both its proximity to optimal solutions and its low computational complexity. 1...|$|R
50|$|Earlier, {{the economy}} of Jonaicha Khurd was based mainly on {{agriculture}} and government jobs predominately in teaching, army and police services. Since {{the last few years}} this trend is undergoing a change because of the less remunerative nature of agriculture and more awareness about the importance of education. Now major contributing factors of {{the economy of}} this village are government and industrial jobs, business, and agriculture. The proximity to various industrial hubs like Gurgaon, Shahajahanpur, Neemrana, Behror, Bhiwadi, Khushkhera, and Alwar provides many <b>jobs.</b> The <b>turnaround</b> in the infrastructure of the village, especially telephone and road connectivity, coupled with the availability of electricity, have changed the very image of an Indian village. This is in turn resulting in a higher family income and living standards.|$|R
40|$|AbstractSingapore {{schools have}} a {{reputation}} of high achieving in the various international surveys, such as the TIMSS, PIRLS and PISA, {{in the past ten}} years. The latest PISA 2012 results ranked Singapore 2 nd in Maths, 3 rd in Reading and 3 rd in Science. However, in the midst of such high achieving schools are schools which have been saddled with poor discipline, low morale among staff and students and under-performance in academic standards. A few of these schools have achieved remarkable turnaround. How do such schools managed to turnaround?PurposeThis paper presents the context, challenges and processes involved in turning around an under-performing school in Singapore through the experiences of a ‘turnaround’ principal. Considerations of important factors such as context, personality, leadership and five essential questions for the turnaround leader are discussed with references to effective schools, school improvement as well as corporate turnaround literature. ConclusionThe process of turning around schools is never an easy one. The principal as the chief driver of the turnaround needs the moral courage, the belief and competencies to do the <b>job</b> of <b>turnaround.</b> In the end he must make the seemingly impossible task possible through working with all stakeholders in the school...|$|R
40|$|In the Data Grid environment, {{the primary}} goal of data {{replication}} is to shorten the data access time experienced by the job and consequently reduce the <b>job</b> <b>turnaround</b> time. After introducing a Data Grid architecture that supports efficient data access for the Grid job, the dynamic data replication algorithms are put forward. Combined with different Grid scheduling heuristics, the performances of the data replication algorithms are evaluated with various simulations. The simulation results demonstrate that the dynamic replication algorithms can reduce the <b>job</b> <b>turnaround</b> time remarkably. In particular, the combination of shortest turnaround time scheduling heuristic (STT) and centralized dynamic replication with response-time oriented replica placement (CDR RTPlace) exhibits remarkable performance in diverse system environments and job workloads. © 2005 Elsevier B. V. All rights reserved...|$|E
3000|$|... • The {{results of}} this case study show, based on both the {{graphical}} and statistical data analysis of the SNR, that the Load reduce task capacity into which {{is used by the}} Job in a MapReduce application in our cluster has the most influence in its <b>job</b> <b>turnaround</b> measure.|$|E
30|$|The {{analysis}} shows that measures job processing time and <b>job</b> <b>turnaround</b> {{have the potential}} to be distinguishing features between the two classes because their means are far apart and interest in such measures increases, this means their test values are greater than 0.9. In addition, it is important to mention that although between the second and third result (hard disk bytes written) there is a considerable difference; the latter is also selected in order to analyze its relationship with the rest of measures because it also has the potential, in terms of their use, to stand out {{from the rest of the}} measures and give more certainty to the analysis of relationships. Thus, the measures job processing time, <b>job</b> <b>turnaround</b> and hard disk bytes written are selected as candidates to represent the performance of the BDA in the Hadoop system.|$|E
40|$|Abstract- For {{more than}} a decade, {{scheduling}} of jobs has been an attractive research subject for researchers. There are several different ways to schedule jobs, and the threads which make them up. As well, the job scheduling {{is one of the}} active research fields, where the researchers work to enhance the efficiency of the job scheduling process in a scheduling environment. In existing hybrid techniques, some efficient factors related to <b>jobs</b> like <b>turnaround</b> time, <b>job</b> execution time and more have not been considered in the job scheduling process. The main drawback is lack of factors in the scheduling process which reduces the performance. To overcome such drawback in the existing methods, an adaptive ABC technique is proposed. In this proposed adaptive ABC technique, the term adaptiveness is achieved by using mutation, crossover and velocity in the employed bee phase for finding the new food sources. The adaptive ABC algorithm optimally allocates the jobs to the accurate processors or resources. Moreover, these existing techniques mostly concentrate on two major factors such as the minimization of the makespan and the completion time. The adaptiveness improves the efficiency of scheduling process when compared to the two conventional hybrid job scheduling techniques. The experimental result shows the performance of the proposed job scheduling process...|$|R
40|$|Abstract. Effective {{resource}} management remains {{a challenge for}} large scale cluster computing systems, {{as well as for}} clusters of clusters. Re-source management involves the ability to monitor resource usage and en-force polices to manage available resources and provide the desired level of service. One of the difficulties in {{resource management}} is that users are notoriously inaccurate in predicting the resource requirements of their jobs. In this research, a novel concept called ‘profile guided scheduling’ is proposed. This work examines if the resource requirements of a given job in a cluster system can be predicted based on the past behavior of the user’s submitted jobs. The scheduler can use this predicted value to get an estimate of the job’s performance metrics prior to schedul-ing and thus make better scheduling decisions based on the predicted value. In particular, this approach is applied in a multi-cluster setting, where the scheduler must account for limited network bandwidth avail-able between clusters. By having prior knowledge of a job’s bandwidth requirements, the scheduler can make intelligent co-allocation decisions and avoid co-allocating jobs that consume high network bandwidth. This will mitigate the impact of limited network performance on co-allocated <b>jobs,</b> decreasing <b>turnaround</b> time and increasing system throughput. ...|$|R
30|$|Increasing task {{replication}} {{does not}} improve the system’s performance {{with a smaller}} number of nodes. However, with 200 nodes, we were able to conclude that having a larger pool of clients does correspond directly to a performance boost. This was attributed to the lower impact of slow or faulty nodes on the <b>job</b> <b>turnaround</b> time.|$|E
40|$|Problem statement: Meta-scheduling {{has become}} very {{important}} due to the increased number of submitted jobs for execution. Approach: We considered the job type in the scheduling decision that was not considered previously. Each job can be categorized into two types namely, data-intensive and computational-intensive in a specific ratio. Job ratio reflected the exact level of the job type in two specific numbers {{in the form of}} ratio and was computed to match the appropriate sites for the jobs in order to decrease the <b>job</b> <b>turnaround</b> time. Moreover, the number of jobs in the queue was considered in the batch decision to ensure server-load balancing. Results: The new factor that we considered namely, the job ratio can reduce the <b>job</b> <b>turnaround</b> time by submitting jobs in batches rather than submitting the jobs one by one. Conclusion: Our proposed system can be implemented in any middleware to provide job scheduling service...|$|E
40|$|Abstract: Problem statement: Meta-scheduling {{has become}} very {{important}} due to the increased number of submitted jobs for execution. Approach: We considered the job type in the scheduling decision that was not considered previously. Each job can be categorized into two types namely, dataintensive and computational-intensive in a specific ratio. Job ratio reflected the exact level of the job type in two specific numbers {{in the form of}} ratio and was computed to match the appropriate sites for the jobs in order to decrease the <b>job</b> <b>turnaround</b> time. Moreover, the number of jobs in the queue was considered in the batch decision to ensure server-load balancing. Results: The new factor that we considered namely, the job ratio can reduce the <b>job</b> <b>turnaround</b> time by submitting jobs in batches rather than submitting the jobs one by one. Conclusion: Our proposed system can be implemented in any middleware to provide job scheduling service. Key words: Meta-scheduling, job scheduling, job ratio, computational jobs, data-intensive job...|$|E
40|$|Symbiotic job {{scheduling}} boosts simultaneous multithreading (SMT) processor performance by co-scheduling jobs that have ‘compatible’ {{demands on the}} processor’s shared resources. Existing approaches however require a sampling phase, evaluate {{a limited number of}} possible co-schedules, use heuristics to gauge symbiosis, are rigid in their optimization target, and do not preserve systemlevel priorities/shares. This paper proposes probabilistic job symbiosis modeling, which predicts whether jobs will create positive or negative symbiosis when co-scheduled without requiring the co-schedule to be evaluated. The model, which uses per-thread cycle stacks computed through a previously proposed cycle accounting architecture, is simple enough to be used in system software. Probabilistic job symbiosis modeling provides six key innovations over prior work in symbiotic {{job scheduling}}: (i) it does not require a sampling phase, (ii) it readjusts the job co-schedule continuously, (iii) it evaluates a large number of possible co-schedules at very low overhead, (iv) it is not driven by heuristics, (v) it can optimize a performance target of interest (e. g., system throughput or <b>job</b> <b>turnaround</b> time), and (vi) it preserves system-level priorities/shares. These innovations make symbiotic job scheduling both practical and effective. Our experimental evaluation, which assumes a realistic scenario in which jobs come and go, reports an average 16 % (and up to 35 %) reduction in <b>job</b> <b>turnaround</b> time compared to the previously proposed SOS (sample, optimize, symbios) approach for a two-thread SMT processor, and an average 19 % (and up to 45 %) reduction in <b>job</b> <b>turnaround</b> time for a four-thread SMT processor...|$|E
3000|$|Note that O is a {{value that}} is {{measured}} using Java’s System.nanoTime (...) [32] method, whereas P and T are values produced as outputs of the simulation. The O-by-T ratio (denoted O/T) {{is used as}} an indicator of the processing overhead of RM-DCWF. This is an appropriate indication of the processing overhead because it puts the measured values of the algorithm runtimes (O) into context by considering the value of O relative to the mean <b>job</b> <b>turnaround</b> time (T).|$|E
40|$|In {{connection}} with a study regarding the ground support software development for the Space Shuttle, an investigation was conducted concerning the most suitable software development techniques to be employed. A time-sharing 'trial period' {{was used to determine}} whether or not time-sharing would be a cost-effective software development technique for the Ground Based Shuttle system. It was found that time-sharing substantially improved <b>job</b> <b>turnaround</b> and programmer access to the computer for the representative group of ground support programmers. Moreover, this improvement resulted in an estimated saving of over fifty programmer days during the trial period...|$|E
40|$|The major GRID infastructures are {{designed}} mainly for batch-oriented computing with coarse-grained jobs and relatively high <b>job</b> <b>turnaround</b> time. However many practical applications in natural and physical sciences may be easily parallelized and {{run as a}} set of smaller tasks which require little or no synchronization and which may be scheduled in a more efficient way. The Distributed Analysis Environment Framework (DIANE), is a Master-Worker execution skeleton for applications, which complements the GRID middleware stack. Automatic failure recovery and task dispatching policies enable an easy customization of the behaviour of the framework in a dynamic and non-reliable computing environment. We demonstrate the experience of using the framework with several diverse real-life applications, including Monte Carlo Simulation, Physics Data Analysis and Biotechnology. The interfacing of existing sequential applications {{from the point of view}} of non-expert user is made easy, also for legacy applications. We analyze the runtime efficiency and load balancing of the parallel tasks in various configurations and diverse computing environments: GRIDs (LCG, Crossgrid), batch farms and dedicated clusters. In practice, the usage of ther Master/Worker layer allows to dramatically reduce the <b>job</b> <b>turnaround</b> time, a scenario suitable for short deadline jobs and interactive data analysis. Finally it is also possible to easily introduce more complex synchronization patterns, beyond trivial parallelism, such as arbitrary dependency graphs (including cycles, in contrast to DAGs) which may be suitable for bio-informatics applications...|$|E
30|$|Figure  10 {{shows that}} the {{performance}} on this experiment is determined by two sub concepts; Time behavior and Resource utilization. The results of the performance analysis show that the main performance measures involved in these sub concepts are: Processing time, <b>Job</b> <b>turnaround</b> and Hard disk bytes written. In addition, there are two sub concepts which have greater influence in the performance sub concepts; Capacity and Availability. These concepts contribute with the performance by means of their specific performance measures which have contribution {{in the behavior of}} the performance measures, they are respectively: Memory utilization, Load reduce task, and Time system up.|$|E
40|$|This thesis {{describes}} the design, development, implementation, and output {{results of a}} software monitor program which measures <b>job</b> <b>turnaround</b> time on an IBM 360 system under OS/MFT and HASP. This {{program is designed to}} be used in conjunction with other monitors and accounting data to measure the performance of the System/ 360. In this thesis, relevant RASP logic is summarized, followed by design specifications of the monitor, solutions to design problems, and a full description of the monitor's program logic. Actual results obtained by the monitor are included. Ball State UniversityMuncie, IN 47306 Thesis (M. S.) [...] Ball State University, 1977...|$|E
40|$|Abstract—Cloud {{computing}} {{offers many}} possibilities for prospective users; there are however many different storage and compute services {{to choose from}} between all the cloud providers and their multiple datacenters. In this paper {{we focus on the}} problem of selecting the best storage services according to the application's requirements and the user's priorities. In previous work we described a capability based matching process that filters out any service that does not meet the requirements specified by the user. In this paper we introduce a mathematical model that takes this output lists of compatible storage services and constructs an integer linear programming problem. This ILP problem takes into account storage and compute cost as well as performance characteristics like latency, bandwidth, and <b>job</b> <b>turnaround</b> time; {{a solution to the problem}} yields an optimal assignment of datasets to storage services and of application runs to compute services. We show that with modern ILP solvers a reasonably sized problem can be solved in one second; even with an order of magnitude increase in cloud providers, number of datacenters, or storage services the problem instances can be solved under a minute. We finish our paper with two use cases, BLAST and MODIS. For MODIS our recommended data allocation leverages both cloud and local resources; it incurs in half the cost of a pure cloud solution and the <b>job</b> <b>turnaround</b> time is 52 % faster compared to a pure local solution...|$|E
40|$|An {{automated}} {{technique for}} extracting mesoscale winds from sequences of GOES visible infrared spin scan radiatiometer (VISSR) image pairs has been developed, tested extensively, and configured for quasi-real time research applications on the Atmospheric Sciences Division's research computing sytem. The entire system of computer codes was successfully vectorized for execution on an array processor resulting in <b>job</b> <b>turnaround</b> {{in less than}} 1 hour. An objective quality control system provides much greater than 99 percent accuracy in eliminating questionable wind estimates. Dynamical analysis of cloud wind divergence has revealed temporally consistent convergence centers on the meso-beta scale that are highly correlated with ongoing and future developing convective storms...|$|E
40|$|AbstractQuality of Service (QoS) {{support in}} private clouds is a {{challenging}} process {{because of the}} limitations of available resources and the high rate of received jobs, which leads to an NP hard scheduling problem. In private clouds, resource owners are usually interested in maximizing their resource utilization and completion rates while minimizing the turnaround time of their jobs, which complicates the scheduling problem even more. Haizea is an eminent cloud scheduler that offers high performance in terms of <b>job</b> <b>turnaround</b> time and completion rate. However, Haizea, and cloud schedulers in general, suffer from low resource utlization. Additionally, cloud schedulers usually consider only end users’ demands, while providers’ demands are entirely neglected. This is because an infinite pool of resources is assumed, which is difficult to achieve and simply not true in private clouds. Conversely, Condor, the eminent High Throughput Computing (HTP) scheuler, is known for addressing these shortcomings by formulating owner's and user's requirements as a logical expression evaluated based on the context which result is high resource utilization. Unfortunatly, this comes with the price of long execution time. As each of Haizea and Condor has its own advantages and limitations, in this paper, we propose a hybrid Haizea and Condor approach (HHCS) which utilizes techniques from both schedulers in a way that maximizes their advantages and overcomes their limitations. The proposed approach has been tested thoroughly in a simulated private cloud environment under various numbers of nodes and jobs. Experimental results illustrated an enhanced performance in terms of resources utilization without compromising the <b>job</b> <b>turnaround</b> time or the job completion rate...|$|E
30|$|Experiments {{were carried}} out to analyze the {{relationships}} between the performance measures of several MapReduce applications and performance concepts that best represent the performance of CCP and BDA, as for example CPU processing time and time behavior. We found that when an application is developed in the MapReduce programming model to be executed in the experimental CCP, the performance on the experiment is determined by two main performance concepts; Time behavior and Resource utilization. The results of performance analysis show that the main performance measures involved in these concepts are: Processing time, <b>Job</b> <b>turnaround</b> and Hard disk bytes written. Thus, these measures {{must be taken into account}} in order to improve the performance of the application.|$|E
40|$|In this paper, {{we present}} a bandwidth-centric {{parallel}} job communication model {{that takes into account}} intercluster network utilization as a means by which to capture the interaction and impact of simultaneously co-allocated jobs in a mini-grid. Our model captures the time-varying utilization of shared inter-cluster network resources in the grid. We compare our dynamic model with previous research that utilizes a fixed execution time penalty for coallocated jobs. We have found that the fixed penalty model is more generous in its prediction of <b>job</b> <b>turnaround</b> time than our dynamic communication model. Additionally, we see that the penalty co-allocated jobs may experience without causing a severe performance degradation decreases as the number of clusters increases. ...|$|E
30|$|RM-DCWF {{configuration}} {{using the}} Proportional Distribution of Job Laxity Algorithm (PD): Overall, {{it is observed}} that using Task Scheduling Policy 1 (TSP 1) generates lower or similar values for the proportion of late jobs, P, average <b>job</b> <b>turnaround</b> time, T, and average job matchmaking and scheduling time, O, than those achieved with Task Scheduling Policy 2 (TSP 2). Furthermore, the two approaches used to calculate the laxity of the job (Sample Laxity, SL and True Laxity, TL) achieve similar performance with the SL approach achieving a slightly smaller P in most cases. When using PD, the results of the experiments showed that the highest performing RM-DCWF configuration (in terms of P) for all three workloads experimented with is PD-SL-TSP 1.|$|E
40|$|Abstract — Although {{theoretical}} {{results have}} been established regarding the utility of preemptive scheduling in reducing average <b>job</b> <b>turnaround</b> time, job suspension/restart is not much used in practice at supercomputer centers for parallel job scheduling. A number of questions remain unanswered regarding the practical utility of preemptive scheduling. We explore this issue through a simulation-based study, using real job logs from supercomputer centers. We develop a tunable selective-suspension strategy and demonstrate its effectiveness. We also present {{new insights into the}} effect of preemptive scheduling on different job classes and deal with the impact of suspensions on worstcase response time. Further, we analyze the performance of the proposed schemes under different load conditions. Index Terms — Preemptive scheduling, Parallel job scheduling, Backfilling...|$|E
40|$|Ever {{since its}} {{introduction}} in 1973, the Virtual Storage Access Method (VSAM) {{has been a}} popular data storage construct on MVS systems. VSAM {{is the cornerstone of}} online applications such as IMS and CICS, and is widely used in vendor packages and inhouse-written batch applications. Its flexibility and automatic internal data management routines have long made it an access method of choice. This paper focuses on how SAS programmers can optimize the processing of VSAM data sets by manipulating VSAM buffers. It presents specific techniques and methodologies for determining what kind, and how many VSAM buffers to allocate to a SAS program. If implemented correctly, these buffering methodologies will greatly reduce disk I/O’s, reduce CPU time, and lead to better <b>job</b> <b>turnaround</b> time...|$|E
40|$|Distributed Systems” {{is used to}} {{describe}} when ever there are several computers interconnected in some fashion so that program or procedure running on the system with multiple processors. However, it has different meanings to different systems because processors can beinterconnected in many ways for various reasons. The task allocation in a Distributed Processing System finds extensive applications in the facilities, where large amount of data is to be processed in {{relatively short period of}} time, or where real-time computations are required. Also the purpose of task assignment in distributed computing systems it to reduce the <b>job</b> <b>turnaround</b> time and increase the through put. The main objective {{of this paper is to}} minimize the total program execution period by allocating the tasks optimally...|$|E
40|$|In {{this paper}} we study {{distributed}} job scheduling in grid environments when each {{job is a}} DL application. The scheduling goal is to minimize the average steady-state <b>job</b> <b>turnaround</b> time. In this context, we identify in which regimes classes of scheduling strategies are efficient, namely for which platforms and which communication to computation ratios. We also quantify what level of global information about the platform is required for efficient scheduling. All our findings are obtained via simulation of wide ranges of application and platform scenarios. Our most significant findings are {{that the use of}} grid information is only necessary at high workload, and that at high workload using dynamic information improves performance by around 10 % when compared to using static information. I...|$|E
