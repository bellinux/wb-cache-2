1|35|Public
40|$|Two {{different}} {{porous asphalt}} pavement systems (PAPS) {{were designed to}} enhance the removal of VOCs (dichloromethane and toluene) and heavy metals (cadmium, copper and lead) from roadway runoff. These two PAPS utilized granular activated carbon (GAC) additions to the conventional PAPS. One PAPS, the CPP system (carbon in porous pavement), had GAC added directly to the top porous asphalt mix, and the other PAPS, the CCA system (carbon in coarse aggregates), had GAC added to the sub-base <b>filter</b> <b>course</b> below the porous asphalt layer. The removal of selected VOCs and heavy metals through the CPP and the CCA systems was measured and compared to a conventional PAPS. ^ The {{results show that the}} addition of GAC into the top porous asphalt layer and the sub-base <b>filter</b> <b>course</b> layer enhanced the overall porous asphalt pavement contaminants removal capability. The addition of GAC to the sub-base <b>filter</b> <b>course</b> layer resulted in higher removal efficiencies for all of the constituents tested in this study. The CPP system is capable of removing on average 83 % of dichloromethane, 95 % of toluene, 71 % of cadmium, 66 % of copper, and 73 % of lead. The CCA system resulted in the average removal of 99 % of dichloromethane, 100 % of toluene, 95 % of cadmium, 76 % of copper, and 75 % of lead from a synthetic roadway runoff. ^ The CCA system was able to control the effluent concentrations of all of the contaminants (dichloromethane, toluene, cadmium, copper and lead) to meet the US EPA MCLs. Even though the same amount of GAC was added into the two systems some of the GAC in the CPP system could have been coated by the asphalt binder during the construction process which would reduce the effective GAC surface area available for adsorption and thus reduce the effectiveness of contaminant removal. ...|$|E
40|$|This paper {{describes}} how to engage actively students in web information retrieval and <b>filtering</b> <b>course</b> using open source programming. To teach this course, I utilized hands-on lab projects from various open source projects including the Galago search engine. Projects included, {{but were not}} limited to, implementing information retrieval (IR) algorithms, collaborative filtering (CF) algorithms, web-based interfaces, and adding features into an open-source search engine. By practicing with real-world open source programming, students found that they better understood how to connect background knowledge to real-world applications in preparation for industry jobs...|$|R
40|$|Moodle is an E-Learning {{platform}} {{that is in}} use by many educational institutions across the world. This paper outlines {{the development of a}} <b>Course</b> page Content <b>Filter</b> created in PHP which can be applied to a Moodle <b>course</b> page to <b>filter</b> the <b>course’s</b> learning objects, displaying only those documents matching the <b>filter</b> selections. The <b>Course</b> Content <b>Filter</b> has been developed as a code patch to be installed to the Moodle installation and activated by the Moodle administrator. It comprises of additional settings to both the Moodle course page and the learning object upload pages. Filter controls appear {{at the top of the}} student’s course page allowing a student to actively <b>filter</b> the <b>course</b> page content. The <b>Course</b> Content <b>Filter</b> assists students in navigating to the learning objects relevant to their needs. An example, as described in this paper, is the <b>filtering</b> of <b>course</b> content by Learning Channel. Learning channels are the modes in which a student can take in information. Providing a student with course material in a form relevant to their learning style makes course content more accessible, optimising the learning experience. For example, a student with a preference for their visual learning channel will benefit from course material presented in a visual form. The content filter described here will allow them to quickly access content which is optimised for their learning preference. This paper also outlines advantages of such a content filter in a Virtual Learning Environment (VLE), reviews current availability of such filters, and describes our approach to developing such a filter...|$|R
3000|$|... d In {{order to}} {{correctly}} estimate the error, {{we need to}} correct the phase shift of the odd samples. This is done by once more filtering the odd samples with the same <b>filter.</b> Of <b>course,</b> for subjective listening tests, {{we do not need}} to correct the phase.|$|R
40|$|Analog filters play a {{significant}} role in every electronic system. There are two different ways to implement analog filters. The first one uses cascade connection of second-order circuits. These types of filters are easy to design, but they are relatively sensitive to the tolerance of elements. The second method uses passive ladder prototypes and practical VLSI implementation needs a series of integrators. This type of design is much less sensitive to the tolerance of elements, but also it is more difficult to design. The second method is rarely taught in basic analog <b>filter</b> <b>courses.</b> Both cascade as well as ladder-based filter implementation designs are complex and computationally intensive. Therefore, computer software could be very helpful to assist students and engineers in their design effort. This paper describes an educational MATLAB tool which simplifies the process of analog filter design. The software allows the use of four approximation methods: Butterworth, Chebyshew, Inverse Chebyshev and Cauer. In addition, several types of filters can be designed such as lowpass, highpass, bandpass and bandstop. In comparison to other tools this software also generates actual circuit implementation...|$|R
30|$|In this scenario, the {{bar code}} area {{needs to be}} cropped from the image that {{contains}} the bar code to identify the bar code from the area. The obtained bar code will be the input element of the Bloom <b>filter.</b> Of <b>course,</b> the Bloom <b>filter</b> adopted here is the improved Bloom filter introduced in this paper. This solution {{takes advantage of the}} high-speed feature of the Bloom filter to automatically recognize the bar code and quickly retrieve it for further processing.|$|R
50|$|They {{demonstrated}} that adding hammy words - {{words that are}} more likely to appear in ham (non-spam email content) than spam - was effective against a naïve Bayesian filter, and enabled spam to slip through. They went on to detail two active attacks (attacks that require feedback to the spammer) that were very effective against the spam <b>filters.</b> Of <b>course,</b> preventing any feedback to spammers (such as non-delivery reports, SMTP level errors or web bugs) defeats an active attack trivially.|$|R
30|$|While {{the ability}} to use a tree to {{calculate}} a sum efficiently is well known, {{the ability to}} use a closely related approach to calculate a cumulative sum 15 efficiently appears to be less well known by researchers working on particle <b>filters.</b> Of <b>course,</b> a naïve implementation involves computing the cumulative sum by simply adding each element of the input to the previous element of the output. Such an approach would have a runtime of N. However, a more efficient approach has existed since the development of APL if not for longer 16.|$|R
40|$|Course Contents Discrete-time {{signals and}} systems, Discrete-Time Fourier Transform (DTFT), z-Transform, digital filter structures, Discrete Fourier Transform (DFT) and Fast Fourier Transform (FFT), Sampling and quantization, finite word length effects, Frequency {{response}} of LTI systems, and <b>filter</b> design techniques <b>Course</b> Objectives To understand the basic {{digital signal processing}} concepts and techniques and t...|$|R
50|$|The SkyMapper {{telescope}} {{was built to}} carry out the Southern Sky Survey, which will image the entire southern sky several times over in SkyMapper's six spectral <b>filters</b> over the <b>course</b> of five years. This survey will be analogous to the Sloan Digital Sky Survey of the Northern hemisphere sky. It has several enhancements, including temporal coverage, more precise measurements of stellar properties and coverage of large parts of the plane of the Galaxy.|$|R
40|$|In this paper, {{the design}} of {{universal}} compressive sensing filter based on normal filters including the lowpass, highpass, bandpass, and bandstop filters with different cutoff frequencies (or bandwidth) has been developed to enable signal acquisition with sub-Nyquist sampling. Moreover, to control flexibly the size and the coherence of the compressive sensing filter, as an example, the microstrip filter based on defected ground structure (DGS) has been employed to realize the compressive sensing <b>filter.</b> Of <b>course,</b> the compressive sensing filter also can be constructed along the identical idea by many other structures, for example, the man-made electromagnetic materials, the plasma with different electron density, and so on. By the proposed architecture, the n-dimensional signals of S-sparse in arbitrary orthogonal frame can be exactly reconstructed with measurements {{on the order of}} Slog(n) with overwhelming probability, which is consistent with the bonds estimated by theoretical analysis...|$|R
40|$|Resting-state fMRI (RS-fMRI) {{has been}} drawing {{more and more}} {{attention}} in recent years. However, a publicly available, systematically integrated and easy-to-use tool for RS-fMRI data processing is still lacking. We developed a toolkit {{for the analysis of}} RS-fMRI data, namely the RESting-state fMRI data analysis Toolkit (REST). REST was developed in MATLAB with graphical user interface (GUI). After data preprocessing with SPM or AFNI, a few analytic methods can be performed in REST, including functional connectivity analysis based on linear correlation, regional homogeneity, amplitude of low frequency fluctuation (ALFF), and fractional ALFF. A few additional functions were implemented in REST, including a DICOM sorter, linear trend removal, bandpass <b>filtering,</b> time <b>course</b> extraction, regression of covariates, image calculator, statistical analysis, and slice viewer (for result visualization, multiple comparison correction, etc.). REST is an open-source package and is freel...|$|R
40|$|Abstract ⎯ FilterD is a {{software}} for analog filter design and investigation, which {{has proved to}} be effective for both educational and practical design purposes. It is used in undergraduate engineering courses, in graduate study and general research. The programme is easily extendable, i. e. it is possible to use computed filter data for non-standard computations as the system is implemented in Mathcad math-programming language. FilterD can compute lowpass, high-pass, band-pass and band-reject active and passive filters. It is possible to use Butterworth, Chebyshev, Bessel and Cauer approximations of ideal filter. FilterD has been in use for four years. It is in the everyday use for undergraduate and graduate <b>filter</b> design <b>course,</b> as a design tool for many dissertation projects and have been downloaded by about 80 k professional users by WWW net. Index Terms ⎯ filter design, Mathcad application, analog filters, filter realisations...|$|R
40|$|Abstract. The air <b>filter</b> in the <b>course</b> {{of their}} work will produce large {{vibration}} by the excitation source from the engine and other incentives will cause severe vibration response when the excitation frequency and its natural frequency close to shorten {{the life of the}} filter. Therefore, during the structural design must be modal analysis, its natural frequency to avoid the excitation frequency. This paper describes the application the HyperWorks right air filter of a particular model the steps of the modal analysis, its natural frequency, and the analysis and evaluation...|$|R
40|$|Distribution grids are {{increasingly}} populated {{by a variety}} of renewable energy sources, which create new opportunities in terms of efficient use of energy and can also help to improve power quality. In fact, they are connected to the grid by means of power switching interfaces (PSI) which are capable to control power flow and waveform of absorbed currents as well. They can therefore contribute to power balance and compensation of asymmetry and distortion together with other compensation units, e. g., passive filters, SVC (static VAR compensators) and APF (active power <b>filters).</b> Of <b>course,</b> effective use of such distributed compensation capability requires cooperative operation of compensators and proper sharing of compensation duties. This paper shows that distributed compensation of load unbalance, reactive power and harmonic distortion can be achieved by a cooperative control approach. Energy efficiency and power quality can be managed as a whole at a system level, by taking full advantage of the distributed compensation capability...|$|R
40|$|Electric filters have a {{relevant}} importance in electronic systems {{because they are}} present in almost any electronic system. For example, communication systems, as many other electric systems, make intensive use of filtering to separate unwanted noise from the desired signal. Unfortunately, filter design is an intensive numerical calculations to obtain either the parameters of a filter transfer function or the element values for a filter circuit realization. This paper describes a software package whose purpose {{is to provide a}} tool {{to be used as a}} teaching aid in analog and digital <b>filter</b> design <b>courses.</b> The feature of this package is that it uses MATLAB [2] for the numerical computations. The main advantage of the filter design software package described in this paper is that makes uses of one of the MATLAB toolboxes, the signals toolbox (which is used for analog and digital filter design), but used with an interface that makes possible even to the novice user to readily design filters, either MATLAB or the signals toolbox...|$|R
5|$|Maxwell {{was also}} {{interested}} in applying his theory of colour perception, namely in colour photography. Stemming directly from his psychological work on colour perception: if a sum of any three lights could reproduce any perceivable colour, then colour photographs could be produced {{with a set of}} three coloured <b>filters.</b> In the <b>course</b> of his 1855 paper, Maxwell proposed that, if three black-and-white photographs of a scene were taken through red, green and blue filters and transparent prints of the images were projected onto a screen using three projectors equipped with similar filters, when superimposed on the screen the result would be perceived by the human eye as a complete reproduction of all the colours in the scene.|$|R
40|$|The thesis {{presents}} {{development of}} a web portal for students to communicate about courses and studies. Hereinafter is described a made survey and its results, used development tools, required functionality derived from survey, development process and developed application. Big focus was put on a responsive web design, so that web portal {{can be used on}} devices with smaller screen, such as smart mobile phones. Django was chosen for the server side development and AngularJS with Bootstrap for the client side. Communication between server and client is done wholly through API. Data is stored in a PostgreSQL database. On a developed web portal a user can register, log in, follow courses and write posts, comments and sticky notes. Furthermore user can <b>filter</b> posts by <b>course,</b> context, content, author and bookmark them. Enabled is also administrative control over content of database through web browser...|$|R
40|$|Root-Nyquist {{filters are}} crucial {{components}} in data communications and recently {{the ability to}} attach fractional sample delay attributes to them has become very important. Here we explore the application of Chebyshev approximation to design of such filters. Special {{attention is paid to}} the use of the Karam-McClellan algorithm, a fully complex extension of the well-established ParksMcClellan algorithm, which delivers peak error performance improvement in the region of one dB or better. 1. BACKGROUND The Alternation Theorem underlying Chebyshev FIR filter approximation has, until very recently, been restricted to real-only transfer function targets. The attraction of the Parks-McClellan program implementing the Remez Exchange Algorithm has been primarily due to the nature of its quality guarantees and its wellcharacterized reliable capabilities in delivering linearphase FIR <b>filter</b> designs. Of <b>course,</b> these are usually cast as single-component (real-only or else imaginary-only) transf [...] ...|$|R
40|$|Usually, {{performance}} {{is the primary}} objective in systems that make use of user modeling (Um) techniques. But since machine learning (Ml) in user modeling addresses several issues {{in the context of}} human computer interaction (Hci), the requirements on "performance" are manifold. Thus, machine learning for user modeling (Ml 4 Um) has to meet several demands in order to satisfy the aims of involved disciplines. In this article we describe the application of a rather unusual Ml method to Um, namely inductive logic programming (Ilp). Though not primarily associated with efficient learning methods, we motivate the use of Ilp by showing translucency of derived user models and the explanatory potential of such models during a user adapted <b>filtering</b> process. In <b>course</b> of the OySTER project our goal is to induce conceptual user models that allow for a transparent query refinement and information filtering in the domain of Www meta-search...|$|R
40|$|OBJECTIVE: We {{investigated}} an algorithm {{to detect}} grey level transitions with multiple scales of resolution to improve edge detection and localisation in ultrasound {{images of the}} prostate. INTRODUCTION: We had developed a non-analytical operator for prostate contour determination implemented with minimum and maximum filters and locate edges. We implemented a technique for improved determination of boundary parts in prostatic ultrasound images by adjusting the edge detection parameter to signal information. METHODS: First the influence of prefilter settings and edge detection parameters is investigated in a test image and a real ultrasound image. Then, local standard deviation is used to identify or fewer homogeneous regions that are <b>filtered</b> with <b>course</b> resolution, while areas with larger deviation that grey level transitions occur, which should be preserved using smaller filter sizes to improve edge localisation. RESULTS: Analysis of images with different filter sizes indicated that areas are merged for increasing filter sizes: less pronounced edges disappear or displace for larger filters. Two scales of resolution lead to an improved localisation of edges when smaller filter sizes are used in areas with an increased local standard deviation. CONCLUSIONS: This paper illustrates an edge detection method suitable as pre-processing step in interpretation of medical images. By adapting input parameters to signal information, object recognition can be applied in images from different imaging modalities. Also, disadvantages are discussed, resulting in a new application combining a localisation algorithm to find the initial contour and a delineation algorithm to improve the outlining of the resulting contou...|$|R
40|$|An {{opportunity}} for design improvement through material substitution was identified by experimental assessment {{of an existing}} linear actuator. Material selection procedures were used to screen a broad database of materials to identify feasible candidates and quantify their relative merit. Further assessment by numerical methods identified additional opportunities for topological optimisation. Aluminum provides a compromise between the objectives of mass and cost and may provide an {{opportunity for}} enhanced material selection. A novel process allows heat treatment of cast high pressure diecast (HPDC) aluminum alloys. The increased yield strength provides significant mass reduction opportunities. Traditional material selection tools engage with a large number of candidate materials, but provide little design guidance for the optimisation of a specific scenario. Conversely, numerical optimization provides insight into specific scenarios, but has significant computational cost. The techniques applied in this paper provide an example of the integration of both methods; i. e. a <b>course</b> <b>filter</b> rapidly screens a large database, resulting in a subset that can be processed by the numerical methods within available design time...|$|R
40|$|THE {{polycyclic}} hydrocarbon content of air may {{be determined by}} analysis of smoke which is collected usually by filtration. When ambient atmospheres are sampled it is tacitly assumed that the sought hydrocarbons are present only in the solid phase and are not appreciably volatile at {{the temperature of the}} <b>filter.</b> In the <b>course</b> of recent work on air pollution by vehicle exhausts (Commins, Waller and Lawther, 1957), filters used to collect smoke in track and bench tests became hot and this observation indicated the urgent need to examine the validity of this method of sampling. In the investigation reported here 3, 4 -benzpyrene was the only hydrocarbon considered. EXPERIMENTAL The presence in town air of 3, 4 -benzpyrene in the vapour phase was sought by drawing filtered air through a drechsel bottle containing pure liquid paraffin. This solvent, in which 3, 4 -benzpyrene fluoresces very strongly, was used because the volatility of the more commonly used solvents made them unsuitable for use in a long experiment. The liquid paraffin was not fluorescent at the beginnin...|$|R
40|$|To an organisation, {{centralized}} Internet filtering {{and blocking}} {{is very important}} for a couple of reasons. With the flooding of pornographic materials on the Web, educators and parents would like to block these offensive materials from their children. Companies also want {{to reduce the amount of}} work time that its employees spend on non-productive Web surfing. Current blocking and filtering mechanisms can roughly be classified into two approaches: URL based and content filtering. In the URL based approach, a requested URL address will be blocked if a match is found in the blocked list. However, keeping the list up-to-date is very difficult. New sites are kept uploading onto the Internet daily; many blocked sites try to use multiple IPS and domain names; the sites might also be moved regularly. In the content filtering approach, keyword matching is often used. Its main problem is the mis-blocking. Many desirable Web sites are blocked because some predefined keywords appear in their Web pages, though in different meaning or context. There are even suggestions for image, audio, and video understanding in real-time content <b>filtering.</b> Of <b>course,</b> the delay time as well as the mis-match between the HTTP streaming protocol and the complexity of the filtering algorithm will be of great concern. In this paper, we investigate how far the multimedia content analysis should go for Internet filtering and blocking. A set of guidelines for defining the heuristics used in the real-time Web content analysis is also given. These heuristics not only have higher filtering accuracy than most multimedia retrieval techniques do, but they also have comparable runtime overhead as the keyword matching does. Our one-year experience of deploying a pornographic filtering system in high schools will also be described. Experience from the system implementation and deployment is found to give a very good direction on the centralized filtering and blocking of Web content. 1...|$|R
40|$|This {{dissertation}} {{presents the}} derivation and demonstration of novel, Adaptive Bathymetric Estimation algorithms (ABE) {{for use with}} forward looking sonar arrays in shallow waters. In addition to providing high-resolution localization of the targets on the ocean bottom {{in front of the}} host vehicle, it automatically estimates a local sound speed profile (SSP) model, and corrects the sonar localization for ray bending arising from refraction. The implementation of this algorithm arises from the combination of three advanced technologies: (1) high frequency, high resolution forward looking sonar, (2) advanced satellite based navigation and positioning systems (GPS), and (3) model-based parameter estimation techniques based on the Extended Kalman <b>Filter.</b> During the <b>course</b> of this work, the developed algorithms are analyzed through computer simulations using statistical measures of their performance and consistency. The presentation of the development is staged, with each chapter increasing the internal complexity of the model and adding capability. We then present an in-water experiment which tested the validity of the ABE in real-world operation. We evaluate the algorithm 2 ̆ 7 s performance in a shallow water environment, and show results of the success and limitations of the technique. ...|$|R
40|$|The AEMU Thermal Loop Pump Development Specification {{establishes}} {{the requirements for}} design, performance, and testing of the Water Pump {{as part of the}} Thermal System of the Advanced Portable Life Support System (PLSS). It is envisioned that the Thermal Loop Pump is a positive displacement pump that provides a repeatable volume of flow against a given range of back-pressures provided by the various applications. The intention is to operate the pump at a fixed speed for the given application. The primary system is made up of two identical and redundant pumps of which only one is in operation at given time. The Auxiliary Loop Pump is an identical pump design to the primary pumps but is operated at half the flow rate. Inlet positive pressure to the pumps is provided by the upstream Flexible Supply Assembly (FSA- 431 and FSA- 531) which are physically located inside the suit volume and pressurized by suit pressure. An integrated relief valve, placed in parallel to the pump's inlet and outlet protects the pump and loop from over-pressurization. An integrated <b>course</b> <b>filter</b> is placed upstream of the pump's inlet to provide filtration and prevent potential debris from damaging the pump...|$|R
40|$|Granular media {{filtration}} {{is one of}} {{the most}} complicated water treatment processes to operate and design. There are currently no bench scale tests which can accurately predict the performance of <b>filters</b> over the <b>course</b> of a <b>filter</b> run and therefore pilot plant trials should be carried out whenever filters are to be designed or modified in any way. However, the costs of pilot plant trials are often prohibitive. This paper describes the design and operation of a pilot filtration plant for an investigation into the performance of various filter designs at higher filtration rates. In this study, maintaining a constant filtration rate over the <b>course</b> of the <b>filter</b> run, even in the absence of an operator, was particularly important. All the filter designs evaluated in this study were supposed to maintain a constant filtration rate while the water level above the filter media also remained constant. In practice, this required a valve on the filter effluent to open gradually over the course of a run as the filter bed clogged up. Commercially available control valves are generally too large for pilot plant applications. Furthermore, including any kind of electronic or hydraulic system to operate the valve substantially increases the cost of the plant. In this study, the authors were able to develop a cheap and simple rate control system to maintain the required filtration rates and filter level. The design and operation of this device is described along with other aspects of the plant hydraulics...|$|R
40|$|As {{part of the}} astrometric Hubble Space Telescope (HST) large program GO- 12911, we {{conduct an}} {{in-depth}} study to characterize the point spread function (PSF) of the Uv-VISual channel (UVIS) of the Wide Field Camera 3 (WFC 3), as a necessary step to achieve the astrometric goals of the program. We extracted a PSF {{from each of the}} 589 deep exposures taken through the F 467 M <b>filter</b> over the <b>course</b> of a year and find {{that the vast majority of}} the PSFs lie along a one-dimensional locus that stretches continuously from one side of focus, through optimal focus, to the other side of focus. We constructed a focus-diverse set of PSFs and find that with only five medium-bright stars in an exposure it is possible to pin down the focus level of that exposure. We show that the focus-optimized PSF does a considerably better job fitting stars than the average "library" PSF, especially when the PSF is out of focus. The fluxes and positions are significantly improved over the "library" PSF treatment. These results are beneficial for a much broader range of scientific applications than simply the program at hand, but the immediate use of these PSFs will enable us to search for astrometric wobble in the bright stars in the core of the globular cluster M 4, which would indicate a dark, high-mass companion, such as a white dwarf, neutron star, or black hole. Comment: 18 pages, 20 figures. Accepted for publication in MNRAS on 2017 May 1...|$|R
40|$|The {{dynamic range}} of the XMM-Newton Optical Monitor (XMM-OM) is limited at the bright end by {{coincidence}} loss, the superposition of multiple photons in the individual frames recorded from its micro-channel-plate (MCP) intensified charge-coupled device (CCD) detector. One way to overcome this limitation is to use photons that arrive during the frame transfer of the CCD, forming vertical read-out streaks for bright sources. We calibrate these read-out streaks for photometry of bright sources observed with XMM-OM. The bright source limit for read-out streak photometry is set by the recharge time of the MCPs. For XMM-OM {{we find that the}} MCP recharge time is 0. 55 ms. We determine that the effective bright limits for read-out streak photometry with XMM-OM are approximately 1. 5 magnitudes brighter than the bright source limits for normal aperture photometry in full-frame images. This translates into bright-source limits in Vega magnitudes of UVW 2 = 7. 1, UVM 2 = 8. 0, UVW 1 = 9. 4, U= 10. 5, B= 11. 5, V= 10. 2 and White= 12. 5 for data taken early in the mission. The limits brighten by up to 0. 2 magnitudes, depending on <b>filter,</b> over the <b>course</b> of the mission as the detector ages. The method is demonstrated by deriving UVW 1 photometry for the symbiotic nova RR Telescopii, and the new photometry is used to constrain the e-folding time of its decaying UV emission. Using the read-out streak method, we obtain photometry for 50 per cent of the missing UV source measurements in version 2. 1 of the XMM-Newton Serendipitous UV Source Survey (XMM-SUSS 2. 1) catalogue. Comment: Accepted for publication in MNRA...|$|R
30|$|Beyond {{contributing}} to theories of human vision, understanding web page gist is relevant for design and usability. We {{can learn from}} web pages {{that are easy to}} comprehend at a glance in order to improve easy access to relevant information. For this reason, researchers in the HCI (Human-Computer Interaction) community have begun to study perception of web pages at a glance. However, to our knowledge all of these studies involved subjective judgments, e.g., “is this web page aesthetically pleasing,” or “does this web page appear to have high or low usability?” Researchers have found that participants form subjective impressions of the appeal of a web page in the first 50  ms of viewing, and respond consistently when shown the same stimulus later (Lindgaard, Fernandes, Dudek, & Brown, 2006). Furthermore, first impressions of visual appeal based on short (50  ms) exposures correlate well with judgments based upon longer viewing times (500  ms and further up to 10  s) (Tractinsky, Cokhavi, Kirschenbaum, & Sharfi, 2006). Users also make consistent subjective ratings about the trustworthiness and perceived usability of web pages after only 50  ms of viewing (Lindgaard, Dudek, Sen, Sumegi, & Noonan, 2011). Inspired by human vision research (see Oliva & Torralba, 2006) that suggests that low spatial frequencies are sufficient to communicate the layout of a natural scene, Thielsch & Hirschfeld (2010, 2012) found high correlation between judgments of aesthetics made on low-pass filtered web page screenshots and the original web page screenshots. Perceived usability, on the other hand, correlated better with judgments made based on high-pass <b>filtered</b> stimuli. Of <b>course,</b> just because observers can consistently make certain subjective judgments at a glance does not imply {{that they will be able}} to perform the tasks of interest in this paper. Instead of studying subjective judgments, we ask observers to perform objective tasks with web pages at a glance.|$|R
5000|$|In August 1970, James McClellan entered {{graduate}} school at Rice University with a concentration in mathematical models of analog filter design. James McClellan enrolled in a new course called [...] "Digital Filters" [...] due to his interest in <b>filter</b> design. The <b>course</b> was taught jointly by Thomas Parks and Sid Burrus. At that time, DSP was an emerging field and, as a result, lectures often involved recently published research papers. The following semester, the spring of 1971, Thomas Parks offered a course called [...] "Signal Theory," [...] which James McClellan took as well. During spring break of the semester, Thomas drove from Houston to Princeton in order to attend a conference. At the conference, he heard Ed Hofstetter's presentation about a new FIR filter design algorithm (Maximal Ripple algorithm). He brought the paper by Hofstetter, Oppenheim, and Siegel, back to Houston, thinking {{about the possibility of}} using the Chebyshev approximation theory to design FIR filters. He heard that the method implemented in Hofstetter's algorithm was similar to the Remez exchange algorithm and decided to pursue the path of using the Remez exchange algorithm. The students in the [...] "Signal Theory" [...] course were required to do a project and since Chebyshev approximation was a major topic in the course, the implementation of this new algorithm became James McClellan's course project. This ultimately led to the Parks-McClellan algorithm, which involved the theory of optimal Chebyshev approximation and an efficient implementation. By the end of the spring semester, James McClellan and Thomas Parks were attempting to write a variation of the Remez exchange algorithm for FIR filters. It took about six weeks to develop {{and by the end of}} May, some optimal filters had been designed successfully.|$|R
30|$|Evaluation of the {{performance}} of bilateral filtering in a group of patients from clinical routine implies that our investigation does suffer from the absence of a true gold standard: we only compared SUVmax values after filtering (with MAF and BF, respectively) with those obtained in the original data, which in turn are not necessarily guaranteed to be correct. Regarding MAF filtering, our results confirm the well-known fact that image smoothing with MAF filters reduces spatial resolution, therefore increases partial volume effects and consequently compromises quantification accuracy, especially in small lesions (and ultimately prevents detection of lesions near the resolution limit). Quantification errors in small lesions (relative to the values derived in the original data) easily approached 30 - 40 % in our investigation when using MAF, see Figure 6. After BF filtering, however, we observed in 32 out of 34 lesions only a slight reduction of SUVmax (mean ± std. dev.: (4.6 ± 3.7)%) but this small reduction of the SUVmax is probably explainable by the achieved noise reduction in the interior of the lesion which necessarily reduces the value of the maximum voxel. This effect decreases the known bias of SUVmax values [15] caused by selecting the single hottest voxel of the lesion. In this sense it is very well possible that the BF filtered SUVmax value is a better estimate of the true SUV than the value derived from the original data, but verification of this conjecture would of course require further investigation. This contrasts to the consequences of MAF filtering where the indiscriminate reduction of spatial resolution massively decreases signal recovery in small lesions. This is the reason for the strong size dependence of the SUVmax reduction after MAF filtering demonstrated in Figure 6. The effect can also be appreciated in Figure 5 where the blurring and concomitant signal reduction in the small lesions is obvious (quantitatively it amounts to 35 % (top) and 30 % (bottom) in these examples). Moreover, reduction of spatial resolution and blurring of the object boundaries by MAF <b>filtering</b> does of <b>course</b> also affect quantitation of larger lesions where SUVmax alone often is not a suitable measure and parameters like lesion volume and SUVmean are more relevant.|$|R
40|$|New {{regulations}} in the United States and Europe, {{designed to address}} climate change concerns by reducing greenhouse gas emissions, are causing increased use of gasoline direct-injection (GDI) engines in light-duty vehicles (LDV). Separate new regulations that aim to reduce particulate emissions to address air pollution concerns are taking effect concurrent with greenhouse gas limitations in both jurisdictions. GDI engines are proven to create more particulate emissions than previously utilized port-injection technology. Increasing particulate emissions rates combined with falling regulatory particulate emissions limits requires new strategies to reduce these emissions from gasoline powered LDVs. Particulate filters have been successfully implemented to reduce particulate emissions from diesel engine exhaust for over a decade. Diesel particulate filters have a demonstrated filtration efficiency of 95 % or greater and have reduced diesel particulate mass (PM) emissions by one to two orders of magnitude. GDI engines require {{no more than one}} order of magnitude reduction in particulate emissions to meet new regulations. Existing particulate filter technology in use in diesel vehicles is capable of reducing GDI engine emissions to new regulatory levels; however, it is proposed that these reduction may be achievable through means other than gasoline particulate filters (GPF). A GPF will create an additional back-pressure in the engine exhaust system that will reduce engine power and efficiency. This backpressure will increase as PM is trapped in the filter and decrease as combustible PM removed. A buildup of incombustible ash present in engine-out PM will increase the baseline backpressure of the <b>filter</b> during the <b>course</b> of its service life. It is important to understand the impact of ash on the filter pressure drop performance before implementing GPF to meet new emissions regulations. This study builds on existing diesel particulate filter technology and demonstrates through experimental results the mechanisms by which ash increases GPF pressure drop. Ash deposits are also shown to increase the light-off temperature of three-way catalyst coatings in GPF. by Nicholas C. Custer. Thesis: S. M. in Mechanical Engineering, and S. M in Naval Architecture and Marine Engineering, Massachusetts Institute of Technology, Department of Mechanical Engineering, 2015. Cataloged from PDF version of thesis. Includes bibliographical references (pages 71 - 74) ...|$|R
40|$|Gift of Water (GOW, est. 1995) is an Indiana based nonprofit {{organization}} dedicated to providing clean drinking water for communities in Haiti. Their system involves prechlorination, followed by a string filter, granular activated carbon (GAC), and then post-chlorination. The initial design of the GOW system included a polypropylene string filter with a nominal pore size opening of 5 μm, which has been changed to one with a nominal pore size opening of 1 μm. Experiments were conducted to compare the original system with the modified system and to quantify {{the effectiveness of the}} systems to produce clean drinking water, including measurements of volumetric flow rate, E. coli removal efficiency, turbidity, free, total and combined residual chlorine concentrations, volatile disinfection by-products (DBPs), and UV absorbance at 254 nm. The clogging rates for the 1 μm and the 5 μm filters were measured to quantify the sustainability of the two filter types. Inactivation assays for bacteriophage as surrogates for human viruses were also performed on the GOW system with a pure and a natural water source. Finally, source water quality data, including turbidity and viable E. coli concentrations, from communities where GOW systems are used were collected during a trip to Haiti. Little difference was noted in the volumetric flow rates between the two string <b>filters</b> throughout the <b>course</b> of the experiment. Bacterial inactivation was consistently effective; both filter types successfully removed the E. coli from the source water. Both filters were able to remove a large fraction of the colloidal particles from the water. The effluent turbidity values for both filter types fell below 5 NTU when the influent water was {{less than or equal to}} 12 NTU. ^ Free and total chlorine concentrations decreased in the final water samples for both filter types. The activated carbon filters were effective in removing most of the free and total chlorine from the first chlorine dose. The combined chlorine concentration measurements were substantially less than the free chlorine concentration measurements. Chloroform (CHCl 3) was the only DBP observed to be present above the detection limit in the chlorinated water samples. The chloroform concentrations measured in the effluent water samples from the 1 μm and the 5 μm filters were all well below the Maximum Contaminant Level of 80 μg/L for Total Trihalomethanes, as established by the US EPA. The absorbance at 254 nm generally decreased from intermediate to final water samples. The UV-absorbing compounds are successfully removed by the filtration or adsorption in the GAC, or by the reactions in the secondary chlorine dose. ^ The clogging rates between the two filters were nearly identical. The 1 μm filter allowed an average of 4037 liters of water through the filter, and the 5 μm filter allowed an average of 3828 liters of water through the filter before clogging occurred. This corresponds to approximately 202 runs for the 1 μm filters and 191 runs for the 5 μm filters. The GOW system achieved over seven logs of inactivation for the öS 1 phage suspension and over four logs of inactivation for the T 4 phage suspension when used with Milli-Q water. When used with Wabash River water, the GOW system achieved approximately three logs of inactivation for the öS 1 phage suspension and less than one log of inactivation for the T 4 phage suspension. The GOW system was not effective at inactivating human virus surrogates in this natural water source. ^ Water quality data from six water sources were collected in Haiti. The E. colicoli concentrations in the samples ranged from 1. 5 most probable number (MPN) of E. coli per 100 mL of the sample to 48. 3 MPN of E. coli per 100 mL of sample. The turbidity measurements ranged from 0. 19 NTU to 6. 64 NTU. ...|$|R
40|$| imaging. **Magnetic Twisting Cytometry (MTC) ** 1. On {{the day of}} the {{experiment}} (day 4 in t in the previous section), take the dish out of the incubator and remove most of the culture medium, such that only the cells in the center well (glass region) is slightly covered in medium. - Add 20 μl of RGD-coated magnetic beads (~ 20 μg of beads) to the center well of the dish by scattering them all over. - Carefully place the beads back into the incubator and leave for 10 minutes to allow for integrin clustering and formation of focal adhesions surrounding the beads. - Remove cells from incubator and rinse it once with PBS. Avoid disturbing cells in the center well. Add and remove PBS gently {{by the side of the}} dish. - Add CO 2 -independent medium to the dish. This is to maintain the pH of the cell culture when it is exposed to the open while under the microscope. - Place the dish in the MTC stage where coils are located. Then place it on the inverted microscope. - Find a single cell that is well transfected with both CFP and YFP plasmids. The cell also needs to have a single bead attached to it. Exclude all cells that are not well transfected, have more than one bead attached, or are in contact with neighboring cells. - After the good cell is found, magnetize the magnetic beads by applying a strong magnetic pulse (~ 1000 G, < 0. 5 ms). - Now that the beads are polarized and magnetized, apply a magnetic field in the direction perpendicular to that of the magnetizing pulse. This will cause the bead to rotate. Input the parameter for MTC. Parameters of stress peak magnitude for FRET analysis are typically 17. 5 Pa (50 G step load) or other magnitudes, where for phase lag analysis is 24. 5 Pa (70 G oscillatory load). - While force is being applied, capture the necessary brightfield or fluorescence images. **FRET imaging and analysis** 1. For FRET imaging, the Dual-View imaging system was used to split the image into two (1344 × 512 pixels each). The top view filters for YFP, while the bottom view filters for CFP. Each image is 1344 × 1024 pixels and simultaneously captures both CFP and YFP activity. - While force is being applied by the MTC, FRET dual-view time course images are captured to monitor the protein-protein interaction within the nucleus before and after force. - After experiments are done and images obtained. A customized Matlab program is used to analyze the data. The program first divides the top (YFP) and bottom (CFP) image in to two separate files. - The region of interest (an individual CB in our case) is then selected. The program crops this region from the CFP and YFP images, then aligns them by cross-correlation. - A binary mask is then created for CFP and YFP images by using Matlab’s “graythresh” function. The binary mask is then multiplied with the fluorescent images generating images that have only the fluorescing region and a black background. - The CFP/YFP ratio value is then calculated for each individual pixel that has been aligned and cross-correlated. An average of the region is obtained and reported. Each image or time point will generate one CFP/YFP value. - Note: More details on the Matlab program has been described by Na S et. al. (14) **Polyacrylamide gels for traction force measurement** 1. Polyacrylamide (PA) gels with 0. 2 μm fluorescent beads embedded within are used to measure the traction force each cell generates. By varying the concentration of bis and acrylamide, different gel stiffness can be obtained. - To prepare PA gels, first smear 3 -aminopropyltrimethoxysilane over the glass surface of a 35 mm glass-bottom-dish using a cotton-tipped swab and let it sit there for 6 min. - Wash it thoroughly with water before applying 100 μl/ dish of 0. 5 % gluteraldehyde for 30 min. - Wash again thoroughly and let them dry. Avoid touching the glass surface throughout the whole gel making procedure. - Determine the bis:acrylamide solution proportions to get the desired substrate stiffness. 0. 6, 2, and 8 kP, corresponds to 0. 06 % bisacrylamide and 3 % acrylamide, 0. 05 % bisacrylamide and 5 % acrylamide, 0. 3 % bisacrylamide and 5 % acrylamide respectively. Prepare 1 ml of each desired mixture in a small 2 ml vial. - Add 10 μl of 0. 2 μm fluorescent beads to the bis-acrylamide mixture. Before adding fluorescent beads, be sure to vortex or sonicate. - Add polymerizing activator/initiator to the beads-bis-acrylamide mixture. 10 % APS at 1 : 200 volume ratio (5 μl in this case). TEMED at 1 : 2000 volume ratio (0. 5 μl in this case). Mix everything together thoroughly. - Add 15 μl of the mixture to the glass surface of the treated dish. (15 μl would give 75 μm thick substrates) - Flatten droplet with a 12 mm circular cover glasses. - Turn the glass bottom dish upside down. This ensures the fluorescent beads to be closer to the top surface. - Place the upside down dishes in a 37 oC incubator for 30 - 45 minutes. Elevated temperature helps in the polymerization. - After the gels are fully polymerized, flood the dish with 100 mM HEPES. Then carefully remove the circular cover glass with a single edge razor. - Make 1 mM solution of SANPAH with DMSO and 100 mM HEPES. Add DMSO to SANPAH first to dissolve the solid powder, and then add it to HEPES. For example, 5 mg SANPAH+ 50 μl DMSO+ 10 ml (100 mM) HEPES. - Take out HEPES from the glass bottom dishes, dab excess HEPES with Kim wipes from around gel edge - Apply 200 μl of SANPAH solution the gel (center well of dish). - Expose surface to UV for 6 min (6 ″ away from the lamp) to photo activate the gel surface. SANPAH color will turn dark. Without SANPAH treatment, collagen will not bind to gel surface. - Rinse off SANPAH with 100 mM HEPES. - Repeat photo activation procedure once more and rinse it off with 100 mM HEPES. - Coat the gel surface with the desired concentration of collagen and incubate at 4 ° C overnight. - Before seeding cells onto the gel surface, sterilize it under UV light for 10 - 15 minutes. - PA gels can be stored in PBS at 4 ° C for three weeks. - Note: To determine the ratio of bis to acrylamide for desired substrate stiffness, refer to references (23 - 25). **Traction Force Microscopy (TFM) ** 1. Cells are cultured on the PA gels. Depending on the PA gel stiffness, the cell will generate different traction forces, and hence different magnitude of deformation. - Three images need to be captured. First is the brightfield or phase contrast image of the cell which will be used to identify the cell boundary. Second is the fluorescent beads marker image while the cell is still on the substrate. Third is the reference fluorescent beads marker image after the cell has been removed or trypsinized from the gel surface. - A customize Matlab program was used to analyze the traction force generated. The displacement field induced by each individual cell’s tractional forces was determined by comparing the fluorescent bead positions before and after trypsinization (cell-free and thus force-free). - An image correlation method where the flourecent images are divided into small window areas is used to determine the displacement vectors (26). - The root-mean-square (RMS) traction field was then calculated from the displacement field using Fourier Transform Traction Cytometry (FTTC) based on the Boussinesq solution (27). **Cell stiffness measurement** 1. The stress applied to the cell (in Pa) can be calculated from the applied twisting magnetic field (in G) by multiplying the bead constant (in Pa/G) with the applied twisting field (in G). The bead constant reflects the magnetic property of the bead and may differ from batch-to-batch. The beads are calibrated by immersing them in a known viscous fluid, and applying a constant magnetic field while measuring the remnant magnetic field (21). For example, a 50 G applies 17. 5 Pa of stress to the cell if the bead constant is 0. 35 Pa/G(9). - When a cell with a single bead bound to its apical surface is found under the microscope, an oscillatory stress of 0. 3 Hz is applied using the MTC. - The MTC software tracks the displacement coordinates of the magnetic bead and saves them in a text file. - By quantifying the magnetic bead displacement, and the bead embedded area, the cell complex modulus can be estimated. A custom Matlab program is then used to calculate the cell stiffness. - The beads whose displacement waves are synchronized to the input sinusoidal signals were selected. This is to filter out spontaneous movements of the beads or microscope stage shifts. - Beads with displacements less than 5 nm (detectable resolution) and loosely bound beads were not selected for analysis. To increase the signal to noise ratio, the peak amplitude of the displacement was averaged over 5 consecutive cycles for each cell. - The complex stiffness is calculated using the equation G*=T/d. For each bead, the elastic stiffness G’ (the real part of G*) and the dissipative stiffness G” (the imaginary part of G*) was calculated based on the phase lag. The measured stiffness has the units of torque per unit bead volume per unit bead displacement (Pa/nm). - A finite element model is then used to convert the cell stiffness (Pa/nm) to modulus (Pa) based on the bead to cell surface contact (28) (Figure. 1). ![Fig 1]([URL] "Fig 1 ") *Figure 1. Quantification of magnetic bead embedment in HeLa cells. An RGD-coated bead was bound to the apical surface of the cell for ~ 15 minutes before it was fixed and stained with phalloidin. Integrin-mediated focal adhesions form around the bead-cell contact area, giving rise to an actin ring. The bead embedment was estimated by measuring the actin ring diameter from the fluorescent image and comparing it to the bead diameter from the brightfield image (double arrows). Bead embedment in HeLa cells is 20 - 30 %. Scale bar = 10 μm*. 9. More details on how to calculate cell stiffness have been described by Fabry B et. al. (29). **Phase lag quantification** 1. An oscillatory stress (0. 3 Hz or 0. 83 Hz) is applied to a cell that is well transfected, similar to the stress used to measure cell stiffness - Time course images of the bead, CFP labeled protein, and YFP labeled protein are captured while the cyclic force is being applied. - A custom Matlab program is used to analyze the images and the displacement of bead, CFP and YFP labeled proteins are determined. The phase lag of fluorescent proteins to the bead is then calculated. - One complete cycle of stress corresponds to 360 o. For example, at 0. 3 Hz, the period for one complete cycle is 3. 33 s. If CFP lags behind the bead displacement by 0. 3 s, that will correspond to a phase lag of ~ 32 o. **Mean Square Displacement (MSD) ** 1. For CB dynamics, an oscillatory stress (0. 3 Hz) needs to be applied. Fluorescent images used for MSD analysis were obtained using single-view fluorescence <b>filter.</b> - Time <b>course</b> images of the bead, CFP labeled protein, and YFP labeled protein are captured before, during and after the cyclic force is being applied. - Binary images of the bead, CFP and YFP are obtained by using the “graythresh” Matlab function. The centroid coordinates of the bead and each fluorescing protein are then obtained. - The coordinates of each fluorescence particle obtained was then used to calculate the mean square displacement (MSD) of Coilin and SMN. The MSD before, during, and after mechanical loading were calculated using a customized Matlab program based (15). The same procedure is performed on bright-field images to obtain the bead MSD. ### Timing Preparation for experiments takes up to 3 days. Dishes need to be coated with collagen or other matrix proteins. Beads need to be coated with RGD or ligands to integrins. Cells need to be transfected. Depending on the transfection efficiency and how magnetic beads bind to the cell surface, locating an appropriate cell for data collection may take up sometime. The whole process of making PA gels may take a day (excluding incubation time...|$|R

