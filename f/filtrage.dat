37|0|Public
5000|$|Sur l'effet d'un repos prolongé et sur celui d'un <b>filtrage</b> par la porcelaine sur la pureté de l'eau; Genf, 1885 (mit Pierre Louis Dunant) ...|$|E
40|$|This paper {{tries to}} give some insight about {{relationships}} between Viterbi and Forwardbackward algorithm (used {{in the context of}} Hidden Markov Models) on one hand and Kalman filtering and Rauch-Tung-Striebel smoothing on the other. We give an unifying view which shows how those algorithms are related and give an example of an hybrid system which can be filtered through a mixed algorithm. Key-words: filtering, smoothing, Viterbi, Kalman (R'esum'e : tsvp) delyon@irisa. fr Unit e de recherche INRIA Rennes IRISA, Campus universitaire de Beaulieu, 35042 RENNES Cedex (France) T el ephone : (33) 99 84 71 00 [...] T el ecopie : (33) 99 38 38 32 Remarques sur le <b>filtrage</b> de donn'ees semi-markoviennes R'esum'e : Le but de cette note est la mise au clair des relations existant entre d'une part les algorithmes de Viterbi et Baum-Welch (utilis'es dans le cadre des mod`eles `a source de markovienne cach'ee) et d'autre part ceux de Kalman (<b>filtrage)</b> et de Rauch-Tung-Striebel (lissage). On donne `a [...] ...|$|E
40|$|Le <b>filtrage</b> de la terre salée et la peau étendue. " ("The {{filtering}} of {{the salt}} earth and the stretched skin. ") Photograph of African men, women and children sitting and {{standing next to the}} stretched skin of an animal. In the background, trees.; Photographie d'hommes, de femmes et d'enfants africains assis et debout près d'une peau étendue sur le sol. À l'arrière-plan, des arbres...|$|E
40|$|International audienceWe {{consider}} {{here the}} problem of statistical ltering and smoothing in nonlinear non-Gaussian systems. The noveltyconsists in approximating the nonlinear system by a recent switching system, in which exact fast optimal ltering and smoothingare workable. Our methods are applied to an asymetric stochastic volatility model and some experiments show their eciency. Nous nous intéressons a la question du <b>filtrage</b> et du lissage statistique dans les syst emes non linéaires et non gaussiens. La nouveauté réside dans l'approximation du syst eme non-linéaire par un mod elè a sauts dans lequel un calcul rapide et optimal du <b>filtrage</b> et du lissage est possible. Ces méthodes montrent leur intérêt, en particulier dans les récents mod eles de volatilité stochastique asymétrique. Abstract – We consider here {{the problem of}} statistical filtering and smoothing in nonlinear non-Gaussian systems. The novelty consists in approximating the nonlinear system by a recent switching system, in which exact fast optimal filtering and smoothing are workable. Our methods are applied to an asymetric stochastic volatility model and some experiments show their efficiency...|$|E
40|$|An {{approach}} to linear-quadratic Wiener filtering, {{in the case}} of a real valued discrete time stochastic process, with finite moments up to the fourth order is given and then extended to the prediction problem. The crucial yole played by the third order moments is pointed out through ail the derivations ranging fi •o m the representation of lq filtering, or the calculation of the optimal frlter to the foret of the variance of the estimate. It is shown that the performance of a linear filter is always improved by the use of a quadratic term, whenever the observation and the unknown processes are not,jointly gaussian. The amount by which the variance decreases is explicitely determined, whatever the statistical properties of the underlying process are. In order to display the opportunity of a quadratic filtering, a particular fantily of stochastic processes is introduced. A case of null linear estimation and, simultaneously, singular quadratic estimation is raised up. Finally, it is argued that when the third order moments are vanishing, the quadratic part of a predictor vasnishes also. Une approche du <b>filtrage</b> de Wiener, dans le cas d'un processus réel de moments finis est proposée puis appliquée à la prédiction de même nature. Le rôle crucial des moments de troisième ordre est mis en avant tant dans la représentation du <b>filtrage</b> que dans celle du filtre optimal ou dans l'évaluation de la variance d'estimatio...|$|E
40|$|Nous nous intéressons dans cet article à l’énumération de toutes les {{solutions}} d’un puzzle de type edge-matching. Nous montrons qu’une modélisation adaptée du problème combinée à l’utilisation de structures algorithmiques efficaces permet d’obtenir un <b>filtrage</b> efficace et global, de complexité O(1). Nous vérifions expérimentalement la pertinence du compromis filtrage/complexité proposé par comparaison avec un des meilleurs algorithmes arborescents disponibles. In {{this paper}} {{we consider the}} enumeration of all solutions of an edge-matching puzzle. We show that a judicious modeling of the problem, combined {{with the use of}} appropriate data structures allows obtaining an effective filtering algorithm with complexity O(1). Our experiments show the relevancy of the proposed power/complexity trade-off, compared to the results of one of the best available tree search algorithms. 1...|$|E
40|$|The {{original}} publication {{is available}} at www. springerlink. comInternational audienceKnown protocol analysis techniques consider protocols where some piece of information expected in a protocol message is located at a fixed position. However this is too restrictive to model web-services where messages are XML semi-structured documents and where significant information (such as name, signature, [...] .) has to be extracted from nodes occurring at flexible positions. Therefore we have extended the Dolev Yao model by a subterm predicate that allows one to express data extraction by subterm matching. This also allows one to detect so-called rewriting attacks that are specific to web-services. Nous étudions les protocoles utilisant une opération d'extraction de sous-message par <b>filtrage,</b> utilisée par exemple dans les web-services et proposons une procédure de détection des attaques par réécriture pour ces protocoles...|$|E
40|$|We {{consider}} the nonlinear filtering problem for systems with noise [...] free state equation. First, we study a particle approximation of the {{a posteriori probability}} distribution, and we give {{an estimate of the}} approximation error. Then we show, and we illustrate with numerical examples, that this approximation can produce a non consistent estimation {{of the state of the}} system when the measurement noise tends to zero. Hence, we propose a histogram [...] like modification of the particle approximation, which is always consistent. Finally, we present an application to target motion analysis. Key-words: Nonlinear filtering, particle approximation, cell approximation, target motion analysis. (R'esum'e : tsvp) Unite de recherche INRIA Sophia-Antipolis 2004 route des Lucioles, BP 93, 06902 SOPHIA-ANTIPOLIS Cedex (France) Telephone : (33) 93 65 77 77 [...] Telecopie : (33) 93 65 77 Approximations particulaire et cellulaire pour le <b>filtrage</b> non lin'eaire R'esum'e : Nous consid'erons le probl`eme de [...] ...|$|E
40|$|Dans les systèmes de {{transmission}} multiporteuses et impliquant plusieurs utilisateurs, deux phénomènes viennent perturber la réception et la détection de symboles : le canal de propagation et le décalage des fréquences porteuses (DFP). Cette thèse traite de techniques d égalisation et de synchronisation en fréquence reposant sur des techniques de type Kalman telles que le <b>filtrage</b> de Kalman étendu (EKF) du 1 er ou du 2 nd ordre, le <b>filtrage</b> de Kalman étendu itératif ou le <b>filtrage</b> de Kalman par sigma point (SPKF). Pour relaxer les hypothèses de Gaussianité sur les bruits de mesure et de modèle dans la représentation dans l espace d état, des approches de type H[infini] sont aussi étudiées. Ces méthodes sont ensuite exploitées dans des systèmes de type OFDMA ou OFDM-IDMA et sont combinées avec d autres approches (MMSE-SD, tests statistiques, etc.) pour mettre en œuvre des récepteurs pouvant être notamment robustes à des interférences large bande, comme c est le cas dans des applications de radio intelligence. Multicarrier modulation is {{the common}} feature of high-data rate mobile wirelesssystems. In that case, two phenomena disturb the symbol detection. Firstly,due to the relative transmitter-receiver motion and {{a difference between the}} localoscillator (LO) frequency at the transmitter and the receiver, a carrier frequencyoffset (CFO) affects the received signal. This leads to an intercarrier interference(ICI). Secondly, several versions of the transmitted signal are received due to thewireless propagation channel. These unwanted phenomena must be taken intoaccount when designing a receiver. As estimating the multipath channel and theCFO is essential, this PhD deals with several CFO and channel estimation methodsbased on optimal filtering. Firstly, as the estimation issue is nonlinear, we suggest using the extended Kalmanfilter (EKF). It is based on a local linearization of the equations around the laststate estimate. However, this approach requires a linearization based on calculationsof Jacobians and Hessians matrices and may not be a sufficient descriptionof the nonlinearity. For these reasons, we can consider the sigma-point Kalmanfilter (SPKF), namely the unscented Kalman Filter (UKF) and the central differenceKalman filter (CDKF). The UKF is based on the unscented transformationwhereas the CDKF is based on the second order Sterling polynomial interpolationformula. Nevertheless, the above methods require an exact and accurate apriori system model as well as perfect knowledge of the additive measurementnoisestatistics. Therefore, we propose to use the H filtering, which is known tobe more robust to uncertainties than Kalman filtering. As the state-space representationof the system is non-linear, we first evaluate the extended H filter,which is based on a linearization of the state-space equations like the EKF. As analternative, the unscented H filter, which has been recently proposed in theliterature, is implemented by embedding the unscented transformation into the extended H filter and carrying out the filtering by using the statistical linearerror propagation approach. The above techniques have been implemented in different multicarrier contexts:Firstly, we address the estimation of the multiple CFOs and channels by meansof a control data in an uplink orthogonal frequency division multiple access(OFDMA) system. To reduce the amount of control data, the optimal filteringtechniques are combined in an iterative way with the so-called minimum meansquare error successive detector (MMSE-SD) to obtain an estimator that doesnot require pilot subcarriers. BORDEAUX 1 -Bib. electronique (335229901) / SudocSudocFranceF...|$|E
40|$|Article dans revue scientifique avec comité de lecture. internationale. International audienceWe give {{algorithms}} for linear and {{for general}} context matching and discuss how the {{performance in the}} general case can be improved {{through the use of}} information derived from approximations that can be computed in polynomial time. We investigate the complexity of context matching with respect to the stratification of variable occurrences, where our main results are that stratified context matching is NP-complete, but that stratified simultaneous monadic context matching is in P. SSMCM is equivalent to stratified simultaneous word matching. We also show that the linear and the shared-linear case are in P and of time complexity O(n³), and that varity 2 context matching, where variables occur at most twice, is NP-complete. || Nous présentons des algorithmes pour le <b>filtrage</b> de contextes linéaires et généraux et nous montrons comment les performances dans le cas général peuvent être améliorées en utilisant l'information dérivée d'approximations calculables en temps polynomial...|$|E
40|$|This paper {{deals with}} the {{presentation}} of geometric methods used to manipulate discrete binary images, in the aim to perform image understanding. This discussion {{is based on the}} use of two methods for representation of an image : by contour and by region. We shah discuss about filtering and methods related to the analysis of the global shape : decomposition and partitionning. We shah notice that shape analysis currently used decomposition processes into simpler entities, completed by a relational graph between the resulting primitives. We will present such a method based on the medial axis transform. This study is justified by a problem of treatment of submarine maps. To solve it, several of the presented methods have been used. On traite les 2 modes de représentation utilisés pour une image, à savoir la représentation par contour et la représentation par région. Procédé de <b>filtrage,</b> de description ou de décomposition de formes en vue d'une interprétation locale ou globale des objets représenté...|$|E
40|$|We are {{interested}} {{here in the}} long time behaviour of the conditional law for a special case of filtering problem: there is no noise on the state equation and the prior law of the state process concentrate fast in some neighborhood of a limit cycle with strictly negative characteristic exponents. Then assuming a deterministic observability property on the cycle we show the concentration of the conditional law on an arbitrary neighborhood of the current (unknown) state as the time goes to infinity. This work {{can be considered as}} illustrating how the tools of dynamical systems theory can be use to study the long time behavior of the filtering process. Key-words: Non-linear filtering, long time asymptotics, measure concentration (R'esum'e : tsvp) Unite de recherche INRIA Sophia-Antipolis 2004 route des Lucioles, BP 93, 06902 SOPHIA-ANTIPOLIS Cedex (France) Telephone : (33) 93 65 77 77 [...] Telecopie : (33) 93 65 77 Asymptotiques en temps long pour des probl`emes de <b>filtrage</b> non-lin'eair [...] ...|$|E
40|$|This article {{deals with}} finite domains binary {{constraint}} satisfaction problems and presents a practical {{use of the}} notion of path consistency. Starting from a weak form, called restricted path consistency, we develop two filtering algorithms that offer a good tradeoff between the traditional arc and path consistency filtering: they improve the power of the former while avoiding the drawbacks that caused the loss of fondness for the latter, that is, a high computational cost and an action that induces some changes {{in the structure of the}} filtered problems. MOTS-CLE S : consistance partielle, consistance de chemin, <b>filtrage,</b> CSP. KEY WORDS: partial consistency, path consistency, problem filtering, CSP. 1 Introduction La notion de consistance partielle occupe une place centrale dans la plupart des traitements appliques aux problemes de satisfaction de contraintes en domaines finis (CSP). Parmi les diverses proprietes de consistance qui ont pu e tre definies, les plus re fe rencees son [...] ...|$|E
40|$|The {{asymptotic}} behaviour of a nonlinear continuous time filtering problem is studied when {{the variance of}} the observation noise tends to 0. We suppose that the signal is a two-dimensional process from which {{only one of the}} components is noisy, and that a onedimensional function of this signal, depending only of the unnoisy component, is observed in a low noise channel. An approximate filter is considered in order to solve this problem. Under some detectability assumptions we prove that the filtering error converges to 0 and an upper bound for the convergence rate is given. The efficiency of the approximate filter is compared with the efficiency of the optimal filter, and the order of magnitude of the error between the two filters, as the observation noise vanishes, is obtained. R'esum'e Nous 'etudions un probl`eme de <b>filtrage</b> non lin'eaire avec petit bruit d'observation o`u le signal est un processus de dimension 2 dont seule une composante est bruit'ee, et o`u la fonction d'observat [...] ...|$|E
40|$|Multifilament yarns made of {{alkali-resistant}} glass {{are used}} as reinforcement element in textile reinforced concrete. To analyse the bond between concrete-matrix and yarn, cross-sections of a specimen are successively taken by a scanning electron microscope (SEM). Positions and diameter of the filaments are detected by image analysing methods in each cross-section. For {{the reconstruction of the}} yarn structure (third dimension) the filament-positions are matched between the different cross-sections by using measured distances of the cross-sections. Transforming methods like an iterative closest point set (icp) algorithm and a kalman-filter are used to find the right matches. The bond area, which is represented by the contact area of concrete and filament, is detected after an idealisation of the image in three types: filament, concrete and pores. The section around each filament is tested of bond areas. The locations of the bond areas are interpolated between the cross-sections, so that a kind of canals of bond are generated, which visualizes the spatial bond characteristics. RÉSUMÉ: L’armature de fibres de verre à multi-filament est étudiée comme alternative pour le ferraillage du béton traditionnel. Pour étudier l’adhérence entre la matrice de béton et les fibres, des coupes transversales successives d’une épreuve sont examinées avec un microscope électronique a balayage. Les positions et les diamètres des filaments sont détectés à l’aide de traitement d’image dans chaque coupe. Pour la modélisation tridimensionnelle à base de l’analyse de l’image bidimensionnelle, les filaments détectés entre les coupes se sont attribués à l’aide de méthodes de tracking, comme la méthode ICP en combinaison avec un <b>filtrage</b> Kalman. Un...|$|E
40|$|Abstract. Collaborative {{filtering}} and, more generally, recommender systems {{represent an}} increasingly popular and important set of personalization technologies that help people navigate through the {{vast amounts of}} information. The performance of recommender systems can be evaluated along several dimensions, such as the accuracy of recommendations for each user and the diversity of recommendations across different users. Intuitively, there is a tradeoff between accuracy and diversity, because high accuracy may often be obtained by safely recommending to users the most popular (“bestselling”) items, {{which can lead to}} the reduction in recommendation diversity, i. e., less personalized recommendations. And conversely, higher diversity can be achieved by trying to uncover and recommend highly idiosyncratic/personalized items for each user, which are inherently more difficult to predict and, thus, may lead to a decrease in recommendation accuracy. In our research we explore different ways to overcome this accuracy-diversity tradeoff, and in this paper we discuss a variance-based approach that can improve both the accuracy and diversity of recommendations obtained from a traditional collaborative filtering technique. We provide empirical results based on several real-world movie rating datasets. Résumé. L'exécution des systèmes de recommandation peut être évaluée selon plusieurs dimensions, telles que l'exactitude prédictive des recommandations pour chaque utilisateur aussi bien que la diversité des recommandations parmi des utilisateurs différents. Dans notre recherche nous présentons une approche dispersion-basée qui peut améliorer l'exactitude et la diversité des recommandations obtenues à partir d'une technique traditionnelle de <b>filtrage</b> collaboratif. Nous fournissons des résultats empiriques basés sur plusieurs ensembles de données réels de cotes de films. 1...|$|E
40|$|Interstitial {{water samples}} have been {{obtained}} from the thawed layer beneath the sea bed at Prud-hoe Bay, Alaska, using a probe method. The electrical conductivities of the water samples, and there-fore the salinities, are about 25 per cent {{higher than those of}} normal sea water. This difference may be due to a density-filtering process caused by convection of the interstitial water. The high salinity causes the phase boundary temperature {{at the bottom of the}} thawed layer, where ice-bearing perma-frost exists, to be lower than the freezing point of normal sea water. A rather uniform value of- 2. 4 OC, corresponding to a salinity of about 43 per mille, is found out to 3. 5 km from shore. A downward salt flux exists at the bottom of the thawed layer, and the electrical conductivity of the interstitial water at one site shows evidence for a thin boundary layer there, in which the salt transport regime seems to change from convective to diffusive. Above this layer, the salinity gradients are low, as would be expected in a well-developed convective regime. A characteristic interstitial water speed at a site 700 m from shore appears to be of the order of a few tenths of a metre per year. On a prelevk des echantillons d'eau interstitielle dans la couche degelee du fond sous-marin de la baie de Prudhoe en Alaska, a I'aide d'une sonde. La conductivite, et par suite la salinitk des echantil-Ions d'eau, etaient de 25 pour cent superieures aux valeurs caracterisant I'eau de mer normale; il s'est peut-hre produit un <b>filtrage</b> suivant les densites, cause par la convection de I'eau interstitielle. La sali...|$|E
40|$|We {{consider}} particle filters {{in a model}} {{where the}} hidden states and the observations form jointly a Markov chain, {{which means that the}} hidden states alone do not necessarily form a Markov chain. This model includes as a special case non [...] linear state [...] space models with correlated Gaussian noise. Our contribution is to study propagation of errors, stability properties of the filter, and uniform error estimates, using the framework of LeGland and Oudjane [5]. 1. EXTENSIONS OF HIDDEN MARKOV MODELS In the classical HMM situation, the hidden state sequence 999 is a Markov chain taking values in the space. It is not observed, but instead an observation sequence 	 taking values in the space is available, with the property that given the hidden states - 29, the observations are mutually independent, and the conditional probability distribution of depends only on the hidden state at the same time instant. In addition, varies, all the conditional probability distributions ! " $# are assumed absolutely continuous w. r. t. a nonnegative measure ') (+*, - which does not depend on. The situation is completely described by the initial distribution and local characteristics $. /!% # 10. *,- 2 ! 34 " 576 # % # 98 *,) 3 :-;! 2 " # &% #= 1. 1. Conditionally Markovian observations Alternatively, the following more general assumption could be made : given the hidden states @	, the observations A 2 	B form a Markov chain, and the conditional probability distribution of given A 576 depends This work was partially supported by CNRS, under the MathSTIC project Chanes de Markov Cachees et <b>Filtrage</b> Particu [...] ...|$|E
40|$|International audienceThe images {{acquired}} by a Polarimetric Synthetic Aperture Radar (POLSAR) {{are characterized by}} an inherentmultiplicative noise called speckle. It corrupts both the amplitude and the phase, which complicates data interpretation,degrades segmentation performance and reduces target detectability. Hence, we need to preprocess the images byadaptive filtering methods before their analysis. In this paper, we present the Lee Sigma filter and its two enhanced versions {{that are based on}} knowledge of the radarprobability density. The Sigma filter uses the calculation of the local mean from the pixels included in a two-Sigmainterval. The improved Sigma filter is based on the estimation of the Minimum Mean Square Error MMSE. Theimproved Sigma filter with strong scatterer detection is based on the point targets preservation and application of theMMSE on the pixels of image. The methods are applied on three polarimetric images (HH, HV, VV) {{acquired by}} the ESAR sensor corresponding tothe area located in Oberpfaffenhofen, in Munich, Germany, and on the bi - polarization images acquired by theENVISAT satellite corresponding to the region of Oran in Algeria, and on the fully polarimetric images acquired by theRADARSAT- 2 satellite on the area of Algiers, Algeria. Les images acquises par les systèmes radar SAR polarimétrique (POLSAR) sont caractérisées par la présence d'un bruit nommé bruit de chatoiement ou speckle. Ce dernier, de nature multiplicative, corrompt à la fois l'amplitude des images et la phase, ce qui complique l'interprétation des données, dégrade la performance de segmentation et réduit la détectabilité des cibles. D'où la nécessité de prétraiter les images par des méthodes de <b>filtrage</b> adaptées, avant de procéder à leur analyse. Dans cet article, nous présentons le filtre Sigma de Lee et ses deux versions améliorées qui se basent sur la connaissance de la densité de probabilité du signal radar. Le filtre Sigma utilise le calcul de la moyenne locale à partir des pixels compris dans un intervalle de valeur égale à deux-Sigma. Le filtre Sigma amélioré repose sur l'estimation de l'erreur quadratique moyenne minimale EQMM (MMSE : Minimum Mean Square Error). Le filtre Sigma amélioré avec détection de forts diffuseurs se base sur la préservation des points fortement réfléchis et l'application de l'EQMM sur les pixels de l'image. Les méthodes sont appliquées sur trois images polarimétriques (HH, HV, VV) acquises par le capteur ESAR correspondant à la zone d'Oberpfaffenhofen, de Munich en Allemagne, sur des images d'Oran en Algérie en bi-polarisations acquises par le satellite ENVISAT, ainsi que sur des images totalement polarimétriques d'Alger, Algérie, acquises par le satellite RADARSAT- 2. Mots clés : Images POLSAR, <b>filtrage</b> de chatoiement, polarimétrie, filtre de Lee adaptatif, filtre Sigma de Lee, filtre Sigma amélioré. Abstract The images acquired by a Polarimetric Synthetic Aperture Radar (POLSAR) are characterized by an inherent multiplicative noise called speckle. It corrupts both the amplitude and the phase, which complicates data interpretation, degrades segmentation performance and reduces target detectability. Hence, we need to preprocess the images by adaptive filtering methods before their analysis. In this paper, we present the Lee Sigma filter and its two enhanced versions that are based on knowledge of the radar probability density. The Sigma filter uses the calculation of the local mean from the pixels included in a two-Sigma interval. The improved Sigma filter is based on the estimation of the Minimum Mean Square Error MMSE. The improved Sigma filter with strong scatterer detection is based on the point targets preservation and application of the MMSE on the pixels of image. The methods are applied on three polarimetric images (HH, HV, VV) acquired by the ESAR sensor corresponding to the area located in Oberpfaffenhofen, in Munich, Germany, and on the bi-polarization images acquired by the ENVISAT satellite corresponding to the region of Oran in Algeria, and on the fully polarimetric images acquired by the RADARSAT- 2 satellite on the area of Algiers, Algeria...|$|E
40|$|In a {{large number}} of applications, the {{processing}} relies on objects or area of interests, and the pixel-based image representation is notwell adapted. These applications would benefit from a region-based processing. Early examples of region-based processing {{can be found in the}} area of image segmentation, such as the quad tree. Recently, in mathematical morphology, the connected operators have received much attention. They are region-based filtering tools that act by merging flat zones. They have good contour preservation properties in the sense that they do not create any new boundaries, neither do they shift the existing ones. One popular implementation for connected operators relies on tree-based image representations, notably threshold decomposition representations and hierarchical representations. Those tree-based image representations are widely used in many image processing and computer vision applications. Tree-based connected operators consist in constructing a set of nested or disjoint connected components, followed by a filtering of these connected components based on an attribute function characterizing each connected component. Finally, the filtered image is reconstructed from the simplified tree composed of the remaining connected components. In the work presented in this thesis, we propose to expand the ideas of tree-based connected operators. We introduce the notion of tree-based shape spaces, built from tree-based image representations. Many state-of-the-art methods relying on tree-based image representations consist of analyzing this shape space. A first consequence of this change of point of view is our proposition of a local feature detector, called the tree-based Morse regions (TBMR). It can be seen as a variant of the MSER method. The selection of TBMRs is based on topological information, and hence it extracts the regions independently of the contrast, which makes it truly contrast invariant and quasi parameters free. The accuracy and robustness of the TBMR approach are demonstrated by the repeatability test and by applications to image registration and 3 D reconstruction, as compared to some state-of-the-art methods. The basic idea of the main proposition in this thesis is to apply connected filters on the shape space. Such a processing is called the framework of shape-based morphology. It is a versatile framework that deals with region-based image representations. It has three main consequences. 1) For filtering purpose, it is a generalization of the existing tree-based connected operators. Indeed, the framework encompasses classical existing connected operators by attributes. Besides, It also allows us to propose two classes of novel connected operators: shape-based lower/upper levelings and shapings. 2) This framework can be used to object detection/segmentation by selecting relevant points in the shape space. 3) We can also use this framework to transform the hierarchies using the extinction values, so that a hierarchical simplification or segmentation is obtained. Some applications are developed using the framework of shape-based morphology to demonstrate its usefulness. The applications of the shape-based filtering to retinal image analysis show that a mere filtering step that we compare to more evolved processings, achieves state-of-the-art results. An efficient shaping used for image simplification is proposed by minimizing Mumford-Shah functional subordinated to the topographic map. For object detection/segmentation, we proposed a context-based energy estimator that is suitable to characterize object meaningfulness. Last, we extend the hierarchy of constrained connectivity using the aspect of hierarchy transformation of constrained connectivity using the aspect ofhierarchy transformation. Dans le travail présenté dans cette thèse, nous proposons d'élargir les idées des opérateurs connexes à base d'arbres. Nous introduisons la notion d'espaces de formes à base d'arbres, construit à partir des représentations d'image à base d'arbres. De nombreuses méthodes de l'état de l'art, s'appuyant sur ces représentations d'images à base d'arbres, consistent à analyser cet espace de forme. Une première conséquence de ce changement de point de vue est notre proposition d'un détecteur de caractéristiques locales, appelé les « tree-based Morse regions » (TBMR). Cette approche peut être considérée comme une variante de la méthode des MSER. La sélection des TBMRs est basée sur des informations topologiques, et donc extrait les régions indépendamment du contraste, ce qui la rend vraiment invariante aux changements de contraste; de plus, la méthode peut être considérée sans paramètres. La précision et la robustesse de l'approche TBMR sont démontrées par le test de reproductibilité et par des applications au recalage d'image et à la reconstruction 3 D, en comparaison des méthodes de l'état de l'art. L'idée de base de la proposition principale dans cette thèse est d'appliquer les opérateurs connexes à l'espace des formes. Un tel traitement est appelé la morphologie basée sur la forme. Ce cadre polyvalent traite des représentations d'images à base de région. Il a trois conséquences principales. 1) Dans un but de <b>filtrage,</b> il s'agit d'une généralisation des opérateurs connexes à base d'arbres. En effet, le cadre englobe les opérateurs connexes classiques par attributs. En outre, il permet également de proposer deux nouvelles classes d'opérateurs connexes: nivellements inférieurs/supérieurs à base de forme et shapings. 2) Ce cadre peut être utilisé pour la détection/segmentation d'objets en sélectionnant les points pertinents dans l'espace des formes. 3) Nous pouvons également utiliser ce cadre pour transformer les hiérarchies en utilisant les valeurs d'extinction, obtenant ainsi une simplification/segmentation hiérarchique. Afin de montrer l'utilité de l'approche proposée, plusieurs applications sont développées. Les applications à l'analyse d'images rétinenne de <b>filtrage</b> basé sur la forme montrent qu'une simple étape de <b>filtrage,</b> comparée à des traitements plus évolués, réalise des résultats au niveau de l'état de l'art. Une application de shaping pour la simplification d'image est proposée, fondée sur une minimisation de la fonctionnelle de Mumford-Shah subordonnée à l'arbre de formes. Pour la détection/segmentation d'objets, nous proposons un estimateur de l'énergie basée sur le contexte. Cet estimateur est approprié pour caractériser la signification d'objet. Enfin, nous étendons le cadre de la connectivité contrainte en utilisant l'aspect de transformation de hiérarchi...|$|E
40|$|The {{purpose of}} this work was to develop RF {{bandpass}} filters in PCB technology,with three main objectives. The first objective was to develop synthesis formulas tosimplify the design procedure of the filter. The second was to achieve wideout-of-band rejection without modifying the in-band filtering characteristics. Thethird objective was to control the center frequency of the filter by using varactordiode. The bandpass filter topology treated in this thesis is based on Stub-LoadedResonators (SLR). The main features of this filter topology were treated. Equivalentcircuits based on J-inverters and susceptance parameters were derived. Based onthese equivalent circuits, synthesis formulas were developed. Simulations werepresented to validate the synthesis theory. For a proof-of-concept, third orderstripline bandpass filters were designed and fabricated based on this synthesis. Analysis technique using odd- and even- mode was achieved on the SLR. Thusresonance odd- and even-mode conditions were derived. These conditions aim toeasily control the first spurious frequency. Moreover, to go further in improving theout-of-band rejection a new technique, called “U corner structure”, was developedand design rules were derived. Based on these design rules an extended out-of-bandrejection was achieved without any modification in the passband and by maintainingthe compactness of the filter. A first spurious frequency was localized at up to ninetimes the working frequency {{in the case of}} the Parallel-coupled Stub-Loadedresonator (PC-SLR) filter. Also, by applying this technique into the classicalparallel-coupled filter the first and second spurious frequencies were rejected. Toaddress the issue of tunable filters, the SLRs were correctly loaded by variablecapacitors (varactor diode). The center frequency of the PC-SLR filter was easilycontrolled by maintaining a large out-of-band rejection. Le but de ce travail était de développer des filtres passe-bande RF dans la technologie de PCB, avec trois objectifs principaux. Le premier objectif était de développer des formules de synthèse tosimplify la procédure de conception du filtre. Le deuxième était de parvenir à un rejet wideout bande sans modifier les caractéristiques de la bande de <b>filtrage.</b> Le troisième objectif est de contrôler la fréquence centrale du filtre en utilisant diode varicap...|$|E
40|$|It {{is widely}} {{accepted}} today that an assumption of a constant standard-deviation for the stock-return is not realistic. Indeed the traditional Samuelson-Black-Scholes {{framework of a}} lognormal distribution fails to explain the existence of leptokurticity (fat tails) {{as well as the}} asymmetry (negative skew) observed in the stock-return distribution. Many different theories have been recently suggested to deal with this phenomenon, but they could all be classified under the title of Stochastic Volatility (SV). Popular SV models include GARCH, Jump-Diffusion, Heston and the Variance-Gamma models. Most of them use either Gaussian innovations with Poisson jumps or other Levy distributions such as Gamma or Ornstein-Uhlenbeck. One of the main difficulties while working with an SV model is that the actual instantaneous volatility is not observable in the market and therefore needs to be modeled as a hidden state. This means that in order to calibrate a model to the stock market, one needs to use a usually nonlinear and/ or non-Gaussian Filter. An alternative would be to use a Bayesian Markov-Chain Monte-Carlo approach. This calibration will then provide us with an estimation of the statistical (or real-world) distribution of the stock-return. This thesis focuses on Nonlinear and Non-Gaussian Filtering as well as the comparison between the Statistical and Risk-Neutral distributions. Il est bien connu que l'hypothèse d'une volatilité constante pour le rendement des prix d'actions est insuffisante. En effet le cadre traditionnel de Samuelson-Black-Scholes ne pourrait pas expliquer l'asymétrie de la distribution ou sa leptokurticité. Plusieurs théories ont été proposées pour expliquer ces phénomènes, mais elles pourraient toutes être considérées comme faisant partie de la théorie de la volatilité stochastique. Ces modèles incluent Heston, GARCH, Variance-Gamma et utilisent des mouvements Browniens ainsi que des sauts de Poisson ou Lévy. Une des difficultés principales de la volatilité est qu'elle n'est pas directement observable. Par conséquent, pour estimer les paramètres du modèle, on a besoin du <b>filtrage</b> non-linéaire. On pourrait également utiliser des méthodes Bayesiennes comme les Chaînes de Markov Monte-Carlo. Cette thèse est centrée sur les filtrages non-Gaussiens de même que sur la comparaison des distributions obtenues dans le monde réel avec celles obtenues dans le cadre risque-neutre...|$|E
40|$|In this work, {{we present}} our {{approach}} for automatic checking of rule based systems. Atfer {{an overview of}} the related works, this report consists of two distincts parts. The first one deals with the consistency of rules in Knowledge Bases (KB) using the attribute-value formalism with forward chaining. We present the MELOMIDIA system, a consistency checker looking for the specifications of Initial Fact Bases (IFB) such that the deductive closure of IFB and the rule base contains contradictory facts. In the same process, MELOMIDIA also detects the redundant or unfireable rules of the KB. The second one considers the problem of verification of first-order Rule Bases (RB). Our approach consists of matching two kinds of knowledge: on one hand, the KB to be studied, on the other hand, specifications of properties of some predicates used in the KB. This latter core of knowledge is some kind of implicit knowledge, which must be acquired from the KB designer. This is made possible through a descriptive model of predicate we designed for this task. Going further, this model allows us to tackle some methodological aspects of the development of KB Systems. Nous présentons dans cette thèse les travaux réalisés concernant la vérification automatique de Bases de Connaissances de Systèmes Experts. Cette étude comporte un panorama des systèmes existants, puis deux parties indépendantes. La première traite de la Cohérence de bases de règles d'ordre Zéro Plus. Nous présentons le système MELOMIDIA, qui fournit, quand elles existent, les bases de faits initiales conduisant à des déductions contradictoires. Le système permet aussi d'améliorer l'exploitation de la base de règles analysée (élimination des règles inutiles, <b>filtrage</b> de la base de faits initiale), et de visualiser les contradictions qu'elle contient (traceur d'incohérences). La seconde concerne la vérification de bases de règles d'ordre Un. Elle consiste à s'assurer que la base respecte des spécifications exprimées sous la forme de propriétés attachées aux prédicats présents dans la base de règles. Ces propriétés sont fournies suivant un modèle qui distingue quatre niveaux de description des prédicats : les niveaux syntaxique, physique, mathématique et fonctionnel...|$|E
40|$|One {{should not}} {{underestimate}} {{the import of}} being able to verify the conformance between a program and its specification. Furthermore, being able to verify it using a dedicated program allows one to check the correctness of this verification. For this purpose, one uses proof assistants, which are programs that allows to describe the problem, build proofs, then check them. There are {{more than one way to}} achieve this goal : one could either generate the program from its specification; one could use an annotated program which carries the specification and hints on how to prove the conformance, then check it afterwards; or one could start from the specification and the program, then prove the conformance of the latter with respect to the first. For our system, we chose the last approach. We implemented a system in which the user describes the speci- fication of a program in a dedicated logical framework, then writes the program in the ML programming language, restricted to the functional subset (with pattern matching, inductive definitions and partial functions), then, after all, interactively builds the proof of conformance of the program with respect to its specification. There are three different aspects in our work : – formalization of a logical framework dedicated to the verification of programs written in the functional part of ML; – precize specification of the proof assistant, its user interface, and the protocol used by both in order to communicate; – the implementation of the proof assistant in Objective Caml, using an original architecture which mixes object oriented programming and functional programming. All these elements may be found in this document, including a precize description of the implementation, the choices we made and the reasons of these choices. The reader will also find here a description on how to use our system, and some examples of problems handled with it. Pouvoir vérifier la conformité d'un programme avec sa spécification représente un enjeu important. On peut utiliser un assistant de preuve : un logiciel permettant la description du problème, la construction des preuves et leur vérification. Nous avons implémenté un système où l'utilisateur décrit la spécification du programme dans un formalisme logique ad hoc, donne le programme dans le sous-ensemble fonctionnel de ML (comprenant <b>filtrage,</b> définitions récursives et fonctions partielles), puis construit interactivement les preuves de correction nécessaires pour prouver la validité du programme...|$|E
40|$|Most current {{wireless}} LANs {{and future}} Beyond 3 G and 4 G mobile networks involve the multicarrier OFDM transmission, based {{itself on the}} digital processing of the fast Fourier transform. These systems should cover bandwidths {{in the order of}} several tens or even hundreds of MHz. The aim of this thesis was to study the architectures of integrated circuit for a high speed and multi-standard OFDM digital processing. These architectures require both higher speed processing to meet the required throughput, and reconfiguration for multi-standard applications. Moreover, these architectures should meet the requirement of reduced power consumption due to the embedded environment of mobile terminals. In terms of advanced solutions, one considers two different OFDM modulation patterns, the OFDM/ QAM and OFDM/OQAM. This latter requires a pulse shaping polyphase filter implemented in our study on the IOTA prototype function. One considers also SISO/MIMO functionalities. A comparative analysis of various FFT algorithms and architectures has led to identify the best approach which gives a good algorithm architecture adequation. This solution also incorporates the pulse shaping filter, more precisely implementing the IOTA function. One has therefore proposed a memory-based architecture using a time multiplexed operations on a coarse grained matrix optimized for the treatment of the FFT and of the pulse shaping filtering. This time approach allows a realization of advanced OFDM modulation for values of the parameter N, the number of subcarrier, from 64 to 8192 and the parameter L, the truncation length for pulse shaping filter, equal to 2, 4 and 8. The architecture of the matrix applies the same treatment on two or four streams of different samples, for modes MIMO 2 x 2 and 4 x 4 respectively. A strategy to manage memories has also been proposed. It is based on a memory banks approach to obtain various memory sizes and to enable the turn of the unnecessary memories. A first FPGA prototyping and an ASIC layout design have validated the functioning and the feasibility of the architecture. The FPGA prototyping platform used was the ML 402 from Xilinx incorporating the FPGA XC 4 VSX 35 from the Virtex- 4 family. The ASIC layout design has been done using the submicronic 65 nm CMOS technology from STMicroelectronics. The performances obtained out of this architecture makes it a good candidate to cover the different standards based on OFDM modulation. Cette thèse a pour but d'étudier les architectures de circuits intégrés pour le traitement numérique de l'OFDM avancé, très haut débit et multi-standard. Ces architectures visent à développer à la fois des puissances de calculs plus élevées pour répondre aux exigences de débit, ainsi que des capacités de reconfiguration pour des applications multi-standard. Elles doivent aussi respecter une contrainte de consommation réduite du fait de l'environnement embarqué des terminaux mobiles. En termes de solutions avancées, nous considérons deux schémas différents de la modulation OFDM, l'OFDM/QAM et l'OFDM/OQAM. Nous proposons une architecture à base de mémoires utilisant un multiplexage temporel des opérations sur une matrice de calcul à gros grain optimisée pour le traitement de la transformée de Fourier rapide et le <b>filtrage</b> polyphase. Nous proposons aussi une stratégie pour la gestion des mémoires...|$|E
40|$|Cette thèse a pour but d'étudier les {{architectures}} de circuits intégrés pour le traitement numérique de l'OFDM avancé, très haut débit et multi-standard. Ces architectures visent à développer à la fois des puissances de calculs plus élevées pour répondre aux exigences de débit, ainsi que des capacités de reconfiguration pour des applications multi-standard. Elles doivent aussi respecter une contrainte de consommation réduite du fait de l'environnement embarqué des terminaux mobiles. En termes de solutions avancées, nous considérons deux schémas différents de la modulation OFDM, l'OFDM/QAM et l'OFDM/OQAM. Nous proposons une architecture à base de mémoires utilisant un multiplexage temporel des opérations sur une matrice de calcul à gros grain optimisée pour le traitement de la transformée de Fourier rapide et le <b>filtrage</b> polyphase. Nous proposons aussi une stratégie pour la gestion des mémoires. Most current wireless LANs {{and future}} Beyond 3 G and 4 G mobile networks involve the multicarrier OFDM transmission, based {{itself on the}} digital processing of the fast Fourier transform. These systems should cover bandwidths {{in the order of}} several tens or even hundreds of MHz. The aim of this thesis was to study the architectures of integrated circuit for a high speed and multi-standard OFDM digital processing. These architectures require both higher speed processing to meet the required throughput, and reconfiguration for multi-standard applications. Moreover, these architectures should meet the requirement of reduced power consumption due to the embedded environment of mobile terminals. In terms of advanced solutions, one considers two different OFDM modulation patterns, the OFDM/ QAM and OFDM/OQAM. This latter requires a pulse shaping polyphase filter implemented in our study on the IOTA prototype function. One considers also SISO/MIMO functionalities. A comparative analysis of various FFT algorithms and architectures has led to identify the best approach which gives a good algorithm architecture adequation. This solution also incorporates the pulse shaping filter, more precisely implementing the IOTA function. One has therefore proposed a memory-based architecture using a time multiplexed operations on a coarse grained matrix optimized for the treatment of the FFT and of the pulse shaping filtering. This time approach allows a realization of advanced OFDM modulation for values of the parameter N, the number of subcarrier, from 64 to 8192 and the parameter L, the truncation length for pulse shaping filter, equal to 2, 4 and 8. The architecture of the matrix applies the same treatment on two or four streams of different samples, for modes MIMO 2 x 2 and 4 x 4 respectively. A strategy to manage memories has also been proposed. It is based on a memory banks approach to obtain various memory sizes and to enable the turn of the unnecessary memories. A first FPGA prototyping and an ASIC layout design have validated the functioning and the feasibility of the architecture. The FPGA prototyping platform used was the ML 402 from Xilinx incorporating the FPGA XC 4 VSX 35 from the Virtex- 4 family. The ASIC layout design has been done using the submicronic 65 nm CMOS technology from STMicroelectronics. The performances obtained out of this architecture makes it a good candidate to cover the different standards based on OFDM modulation. GRENOBLE 1 -BU Sciences (384212103) / SudocSudocFranceF...|$|E
40|$|A Wireless Sensor Network (WSN) {{consists}} {{of a set of}} wireless nodes where their number can be very large. The WSN is used to perform a specific task for an application. The WSN has become increasingly popular due to their wide range of civil and military applications: smart planet, smart home, smart care, M 2 M (Machine to Machine) [...] . The ultimate goal of a WSN is to monitor an area of interest, but to achieve that, several objectives (services) must be implemented. These services include: synchronization between nodes, localization, topology management, data aggregation, data storage, etc. The node's localization {{is one of the most}} important services. It associates to each node the coordinates of its position. Several methods are used to achieve this function. A set of systems (technologies) have been developed to provide this service. The most commonly used outdoors, worldwide, by far is the GPS (Global Positioning System). It has the advantage of providing the position (and time) continuously and in all weather conditions. However, the accuracy of GPS does not meet the requirements of all applications, thus it is necessary to improve GPS accuracy. For this, the differential mode (DGPS, Differential GPS) is introduced. It uses a base station that monitors the GPS errors, calculates corrections and transmits them through a wireless access medium; but the installation of a differential station (RTK, Real-Time Kinematic) is difficult and very expensive. The differential mode inspires this thesis, but in contrast to the RTK solution that uses a large base station, the original solution proposed in this thesis (LCD-GPS, Low Cost Differential GPS, Local Cooperative DGPS) is based on using a WSN equipped with low cost standard GPS receivers. Works conducted to improve the GPS accuracy include the use of a digital map (Map Matching), the simple difference, the intelligent difference, the filtering and a global correction. To evaluate this solution, a hardware and software platform has been developed; it {{consists of}} a network of wireless nodes called LiveNodes (LIMOS Versatile Embedded Node). The software part is composed particularly of an embedded operating system called LIMOS (Lightweight Multi-Threaded Operating System) and a wireless communication protocol called CIVIC (Inter Vehicle Communication and Intelligent Cooperative) and finally the specific processing of LCD-GPS solution. The results show that the LCD-GPS can be used for various applications but it is very sensitive to the environment (multipath). However, it is noted that the WSN can generally achieve a robust and reliable location sensor. Les travaux menés dans cette thèse visent à améliorer la précision du GPS en s'inspirant du mode différentiel (DGPS, Differential GPS). Mais à l'inverse de la solution RTK (RTK, Real-Time Kinematic) qui utilise une grande station de base et qui est difficile à installer et surtout très onéreuse, la solution originale proposée dans cette thèse (LCD-GPS, Low Cost Differential GPS, Local Cooperative DGPS) est basée sur l'utilisation d'un Réseau de Capteurs Sans Fil (RCSF) équipés de récepteurs standarts à faible coût. Ces travaux incluent l'utilisation d'une carte numérique (Map matching), la différence simple, la différence intelligente le <b>filtrage</b> la correction globale [...] . Afin d'évaluer cette solution, une plateforme matérielle et logiciel a été développée, elle consiste en un réseau de capteurs appelés LiveNodes (LIMOS Versatile Embedded Node). La partie logicielle est composée notamment d'un système d'exploitation embarqué appelé LIMOS (Lighweight Multi-thtreading Operating System) et d'un protocole de communication sans fil appelé CIVIC (Communication Inter Vehicule Intelligente et Coopérative) et enfin les traitements propres à la solution LCD-GP...|$|E
40|$|Optical fiber, Raman fiber laser, fiber bragg mirror, optical power spectrum, waveturbulence theory, Non-Linear Schrödinger equation, {{non-linear}} optics. The work {{presented in}} this thesis deals {{with the formation of}} the optical power spectrum in Raman fiber lasers. We carried out an experimental study on a Raman fiber laser oscillating in a Pérot-Fabry cavity closed by fiber Bragg grating mirrors. We report that the shape of the optical spectrum is power dependent. Developing several models, we show that fiber Bragg gratings are responsible for this property of the optical power spectrum. In particular, at low power, the asymetric shape of the spectrum is due to dispersive effects occurring in fiber Bragg grating mirrors. At high laser power, those dispersive effects are dominated by the filtering action of fiber Bragg grating mirrors, which results in a symetrisation of the optical power spectrum observed in our experiment. In addition, we have also studied numerically the statistics of the intracavity Stokes field. We show that the field statistics strongly depends on whether Stokes light is incident or reflected by cavity mirrors. This result allows us to question the validity of a model recently developped in order to describe the formation of the optical power spectrum in Raman fiber lasers. This model relies on the recent tools of wave kinetic theory which is valid exclusively in the case of nearly gaussian fields statistics. However, our numerical study seems to indicate this condition is not fulfilled in Raman fiber lasers, and the shape of the optical spectrum observed in our experiment contrasts with the one predicted by this statistical approach of the formation of the optical power spectrum in Raman fiber lasers. Le travail présenté dans ce mémoire s'inscrit dans la problématique générale de la formation du spectre optique dans les lasers Raman à fibre. Nous avons mené une étude expérimentale sur un laser Raman à fibre oscillant dans une cavité Pérot-Fabry fermée par des miroirs de Bragg. Cette étude montre que la forme du spectre optique diffère selon la puissance du laser. En développant différents modèles, nous avons montré que les miroirs de Bragg sont à l'origine de ce changement de forme du spectre optique. En particulier, à faible puissance, la forme asymétrique du spectre provient d'effets dispersifs lors de la réflexion sur les miroirs de Bragg. A forte puissance, ces effets dispersifs sont dominés par les effets de <b>filtrage</b> des miroirs, ce qui conduit à la symétrisation du spectre du laser observée dans notre expérience. Par ailleurs, nous avons également étudié numériquement la statistique du champ Stokes intracavité. Nous avons montré que celle-ci change fortement selon que l'onde Stokes est incidente ou réfléchie par les miroirs de Bragg. Ce résultat nous a permis de questionner la validité d'un modèle récemment publié sur la formation du spectre optique du laser Raman à fibre. Ce modèle s'appuie sur les outils de la théorie cinétique des ondes, valable uniquement dans le cas de champs possédant une statistique gaussienne. Toutefois, notre étude numérique indique que cette condition n'est pas respectée dans le laser Raman à fibre, et la forme du spectre optique observé dans notre étude expérimentale s'oppose fortement à celle prédite par cette approche statistique de la formation du spectre optique du laser Raman à fibre...|$|E
40|$|La {{commande}} et l'estimation des systèmes bilinéaires restent un problème ouvert en automatique du fait de {{la nature}} non linéaire des systèmes. Même si ces systèmes semblent proches des systèmes linéaires, leur étude nécessite une approche différente. En effet, les entrées peuvent générer des singularités qui doivent être explicitement prises en compte dans la synthèse de lois de commande et des observateurs. Cette spécifité du rôle des entrées dans les propriétés des systèmes bilinéaires nous a amenés à traiter le problème de l'observation à partir de deux approches : - une approche de type LPV (Linear Parameter Varying, Linéaire à Paramètres Variants); une approche basée sur l'analyse structurelle des systèmes. L'une des contributions présentées dans ce mémoire réside dans l'utilisation de l'approche LPV pour la prise en compte des entrées de commande afin de concevoir un observateur pour les systèmes bilinéaires. L'approche H 8 pour le <b>filtrage</b> des perturbations a été étudiée pour deux types d'observateurs, l'observateur à grand gain et l'observateur fonctionnel. Des approches LMI permettent la synthèse et l'optimisation de ces filtres. Ainsi différentes commandes saturées basées sur ces observateurs ont été proposées : la commande bang bang, la commande quadratique, la commande linéaire, l'utilisation couplée de di_érentes commandes. Une autre partie de mon travail a consisté à exploiter la structure des systèmes bilinéaires afin de relaxer le conservatisme dans le traitement des entrées lors de la synthèse d'un observateur. Nous avons ainsi conçu des observateurs à deux étages afin de séparer les dynamiques uniformément observables et celles qui ne le sont pas afin d'y appliquer différents types d'observateurs LPV. Cette approche montre en outre les possibilités d'association de divers types d'observateurs pour les systèmes de grandes dimensions. The {{control and the}} estimation of the bilinear systems remain an unsolved problem in control theory because of the nonlinear nature of the systems. Even if these systems seem close to the linear systems, their study requires a different approach. Indeed, control input can generate singularities which must be explicitly {{taken into account in}} the synthesis of the control laws and the observers. Therefore the problem of the observation was treated following two main approaches : LPV approach; approach based on the structural analysis of the bilinear systems. One of the contributions presented in this thesis is the use of the LPV approach taking into account the inputs of command in order to design an observer for the bilinear systems. The H 8 approach for filtering of the disturbances was studied for two kinds of observers, the observer with high gain, especially adapted to the uniformly observable systems, and the functional observer. LMI approaches allow the synthesis and the optimization of these filters. So thanks to this approach, various saturated control laws based on these observers were proposed : bang bang control, quadratic control, linear control, coupled use of controls. Another part of this thesis focuses on exploiting the structure of the bilinear systems in order to reduce conservatism in the processing of the inputs in observer synthesis. A two-stage observer is designed in order to separate dynamics of the uniformly observable part and those of the non uniformly observable part, in order to apply various types of LPV observers to each part. Moreover this approach shows the possibilities of association of various types of observers for the large-scale systems...|$|E
40|$|The 60 -GHz {{unlicensed}} band is a promising alternative {{to perform the}} high data rate required {{in the next generation}} of wireless communication systems. Complex modulations such as OFDM or 64 -QAM allow reaching multi-gigabits per second throughput over up to several tens of meters in standard CMOS technologies. This performance rely on the use of high performance millimeter-wave frequency synthesizer in the RF front-end. In this work, an original architecture is proposed to generate this high performance millimeter-wave frequency synthesizer. It is based on a high order (several tens) multiplication of a low frequency reference (few GHz), that is capable of copying the low frequency reference spectral properties. This high order frequency multiplication is performed in two steps. Firstly, a multi-harmonic signal which power is located around the harmonic of interest is generated from the low frequency reference signal. Secondly, the harmonic of interest is filtered out from this multi-harmonic signal. Both steps rely on the specific use of oscillators. This work deals with the circuit design on advanced CMOS technologies (40 nm CMOS, 55 nm BiCMOS) for the proof of concept and on the theoretical study of this system. This novel technique is experimentally validated by measurements on the fabricated circuits and exhibit state-of-the-art performance. The analytical study of this high order frequency multiplication led to the discovery of a particular kind of synchronization in oscillators and to approximated solutions of the Van der Pol equation in two different practical cases. The perspectives of this work include the design of the low frequency reference and the integration of this frequency synthesizer in a complete RF front-end architecture. La bande de fréquence non-licensée autour de 60 GHz est une alternative prometteuse pour couvrir les besoins en bande passante des futurs systèmes de communication. L'utilisation de modulations complexes (comme OFDM ou 64 -QAM) à ces fréquences permet d'atteindre, en utilisant une technologie CMOS standard, des débits de plusieurs gigabits par seconde sur quelques mètres voire quelques dizaines de mètres. Pour atteindre ces performances, la tête d'émission-réception RF (front-end RF) doit être dotée d'une référence de fréquence haute performance. Dans ce travail, une architecture originale est proposée pour générer cette référence de fréquence haute performance. Elle repose sur la multiplication de fréquence d'ordre élevé (plusieurs dizaines) d'un signal de référence basse fréquence (moins de quelques GHz), tout en recopiant les propriétés spectrales du signal basse fréquence. Cette multiplication est réalisée en combinant la production d'un signal multi-harmonique dont la puissance est concentrée autour de la fréquence à synthétiser. L'harmonique d'intérêt est ensuite extraite au moyen d'un <b>filtrage.</b> Ces deux étapes reposent sur l'utilisation d'oscillateurs dans des configurations spécifiques. Ce travail porte à la fois sur la mise en équation et l'étude du fonctionnement de ce système, et sur la conception de circuits dans des technologies CMOS avancées (CMOS 40 nm, BiCMOS 55 nm). Les mesures sur les circuits fabriqués permettent de valider la preuve de concept ainsi que de montrer des performances à l'état de l'art. L'étude du fonctionnement de ce système a conduit à la découverte d'une forme particulière de synchronisation des oscillateurs ainsi qu'à l'expression de solutions approchées de l'équation de Van der Pol dans deux cas pratiques particuliers. Les perspectives de ce travail sont notamment l'intégration de cette synthèse innovante dans un émetteur-récepteur complet...|$|E
40|$|This thesis {{takes place}} in the {{framework}} of the calibration of low order models from experimental sequences acquired by time resolved PIV around at profil NACA 0012 with various angles of attack and numbers of Reynolds. A reduced-order modelling approach issued from the Galerkin projection of the incompressible flow Navier-Stokes equations onto a low-dimensional basis extracted by Proper Orthogonal Decomposition (POD) is used. A state space model governing the evolution of the state variables of the reduced-order model POD-Galerkin and mapping directly or indirectly a part or the whole of these state variables is then used {{to solve the problem of}} the estimation of the state of the reduced-order model POD-Galerkin during time. The Bayesian inference on the reduced-order model POD-Galerkin depending on different sets of observations is proposed. The first part is devoted to the application of Bayesian estimators from the assimilation of sequential data on the linear and quadratic reduced-order models POD-Galerkin in the case where time resolved observations are available. The Bayesian estimators used are the linear Kalman filters and the ensemble Kalman filter (EnKF). These Kalman filters are experimentally validated on the flow fields. They allow the reduced-order model to describe the dynamics of the considered flow in time and reproduce a significant percentage of the flow. The second part deals with the reconstruction of missing velocity fields after under-sampling the experimental data. The missing coefficients are reconstructed using the EM algorithm which proceeds by maximization of a likelihood calculated with a Kalman filter and smoother. Different types of under-sampling of the snapshots were then tested. A last part is devoted to the stochastic filtering of the reduced-order model POD-Galerkin with the EnKF filter using observations of different physical nature. The signal used for the observations is a voltage signal obtained by hot film anemometry downstream of the NACA 0012 profile. Due to the very high collinearity of the signals obtained by hot film, the PLSR has been used to define a linear operator of observations in the Kalman filter EnKF. Results concerning the use and application of the PLSR with the EnKF filter are presented. The application of these methods for the reconstruction of velocity fields is then validated on experimental data. Cette thèse se place dans le cadre de la calibration de modèles réduits d'écoulement à partir de séquences expérimentales acquises par PIV résolue en temps autour d'un profil NACA 0012 à différents angles d'incidence et nombres de Reynolds. Un modéle à espace d'état régissant l'évolution des variables d'état du modèle réduit POD-Galerkin et mesurant de manière directe ou indirecte une partie ou l'ensemble de ces variables d'état est alors utilisé. Une première partie est consacrée à l'application d'estimateurs bayésiens issus de l'assimilation séquentielle de données sur le modèle réduit POD-Galerkin linéaire et quadratique dans le cas où l'ensemble des observations est pris en compte. Les estimateurs bayésiens utilisés sont les filtres de Kalman linéaire et d'ensemble EnKF. Ils permettent au modèle réduit de restituer la dynamique de l'écoulement considéré au cours du temps et de reconstruire un pourcentage significatif de l'écoulement. La seconde partie traite de la reconstruction de champs de vitesse manquants après un sous-échantillonnage des données. Les coefficients manquants sont ensuite reconstruits à l'aide de l'algorithme EM. Une dernière partie est consacrée au <b>filtrage</b> stochastique du modèle réduit POD-Galerkin à l'aide du filtre EnKF en fonction d'un signal de tension obtenu par anémométrie à film chaud en aval du profil NACA 0012. La PLSR a été mise en place pour définir un opérateur linéaire des observations dans le filtre de Kalman EnKF. Ces méthodes sont ensuite validées expérimentalement pour la reconstruction de champs de vitesse d'écoulements d'une des congurations étudiées...|$|E
40|$|L'utilisation d'outils acoustiques à émetteurs-récepteurs {{multiples}} et enregistrement numérique {{permet de}} faire une microsismique de puits en utilisant des techniques de traitement dérivées du traitement sismique. Comme les enregistrements acoustiques sont composés de différents types d'ondes (ondes de volume réfractées ou réfléchies et ondes d'interface), une étape importante du traitement acoustique est la séparation des ondes. Cet article montre que la séparation des ondes peut être optimisée en fonction du choix du type de collection des enregistrements acoustiques et de la performance des algorithmes utilisés, dépendante du nombre de traces par collection. Les différents types de collection sont la collection émetteur ou récepteur commun et la collection à déport constant. Trois exemples de traitement de diagraphie acoustique sont présentés. Le premier exemple montre que les interférences des ondes qui conduisent à des anomalies sur l'estimation des logs acoustiques tels que le log de lenteur (Delta t) sont réduites après une bonne séparation des ondes. Le deuxième exemple est un exemple d'imagerie en puits vertical. Le traitement par <b>filtrage</b> de Wiener sur une section à déport constant permet de différencier les modes réfractés et d'interface des diffractions profondes (environ 4 m) créées par la présence d'intercalations dolomitiques en milieu argileux. Le troisième exemple est un exemple d'imagerie en puits horizontal. Le traitement est réalisé sur une collection point de tir commun. La combinaison de <b>filtrage</b> en vitesse apparente pour extraire les différents types d'ondes et de <b>filtrage</b> matriciel pour améliorer le rapport signal sur bruit a permis d'extraire un jeu de réflexions. La connaissance a priori de la zone réservoir a permis d'identifier les événements réfléchis en-dessous et en-dessus du drain. Cet exemple montre la nécessité d'utiliser des techniques spécifiques pour lever l'ambiguïté sur l'origine des réflexions. The fule waveforms recorded by {{an array of}} receivers in a borehole sonic tool contain a set of waves that can be fruitfully used to obtain detailed information about the nearborehole lithology and structure. Each wave contains {{information that can be}} disturbed by the presence of the other waves. The great amount of full-waveform sonic data leads geophysicists and log analysts to implement algorithms for separating waves. The log analyst must pay attention to the choice of parameters for acquisition. The acoustic data must be recorded with well suited spatial and temporal sampling-rates to avoid spatial and temporal aliasings. The efficiency of the wave-separation algorithms depends on the choice of acoustic data gathers. Acoustic data are sorted either in a common-shot gather or in a constant-offset gather. The wave separation filters commonly used are apparent velocity filters. Less usual filters are a Wiener filter and a spectral matrix filter. The efficiency of an apparent velocity filter is enhanced by application of spectrum equalization. The signal to noise ratio is enhanced by spectral matrix filtering. We describe different processing sequences applied to a set of field examples. First field example: full-waveform sonic data in a vertical wellFull-waveform sonic data were acquired in a vertical well drilled in an anticline structure used for underground gas storage by Gaz de France. The sonic tool used was a Schlumberger Dipole Imaging tool. This tool is an eight-receiver/three-transmitter device. The receiver section contains eight dipole-monopole stations spanning 3. 5 ft, each station spaced 6 inches (15 cm) from its neighbor. The distance between the monopole transmitter and the first receiver station is 9 ft. The source was fired at equal spacings of 6 inches throughout the open hole section. For this experiment, the monopole transmitter was activated with a low frequency pulse for the purpose of generating low frequency Stoneley waves. The full waveforms taken at each receiver were recorded on magnetic tape. The data sampling rate was 40 µs, and 20 ms of the data were acquired for each trace. In the part of the well studied, the reservoir layers are clean sandstones or shaly sandstones overlain by impervious shale cap rock. The well is completed by a casing down {{to the top of the}} reservoir layers. The sonic data were recorded to study the reflected Stoneley waves. The borehole Stoneley waves are trapped modes that can be reflected when the direct borehole Stoneley wave encounters permeable fractures (Hornby, Johnson, Winkler and Plumb, 1989) or a major change in lithology (Hornby, 1989). Figure 6 shows the common shot gather processing applied to the waveforms recorded when the source was at depth 913. 3 m. The figure shows, from right to left, the raw data (A), the flattened raw data (B), the first eigensection obtained by matrix spectral filtering (C) and its associated residual eigensection (D). These sonic sections were time shifted to be directly comparable with the raw data, using the picked time of the direct wave arrival. Sonic section E shows the direct Stoneley wave and its downgoing reflected waves (mainly between 13 and 15 ms). Sonic section F shows the upgoing reflected Stoneley waves. Figure 7 shows a constant offset gather. On this section we can observe good correlation between the frequency log and lithology derived from an independent analysis. In clean sandstone, the Stoneley wave have an apparent frequency of 1150 Hz; in shaly sandstone the frequency ranges from 800 Hz to 1100 Hz. The value of the Stoneley frequency is directly related to shaliness. Maximum shalk ness is observed in the 898 - 903 depth interval. Figure 8 shows from top to bottom the slowness and the standard deviation of the slowness computed from raw sonic data and filtered sonic data (first eigensection) as well as the frequency log and its standard deviation. We can note that the standard deviation is very low (average value = 20 µs/ft) by comparison with the time sampling rate (40 µs) and the depth sampling rate (0. 5 ft). After filtering, the standard deviation is reduced whatever the depth, but mainly in the 897 - 900 depth interval. This depth interval is associated with maximum shaliness. The maximum standard deviation observed at 903 m is due to the interference of downgoing reflected waves and a direct Stoneley wave. This reflected Stoneley wave is not filtered in the commonshot gather space. Figure 9 shows the energy log derived from the set of first eigenvalues associated with the first eigensections. This log correlates in depth with the frequency log and exhibits the interval depth where the reflected downgoing Stoneley waves are very strong. Figs 10 to 13 show the sonic fullwaveform sections after constant offset gather processing. Fig. 10 shows a constant offset gather obtained from the set of the first eigensections (Fig. 6 (E)). Fig. 11 shows a constant offset gather obtained from the set of the residual eigensections (Fig. 6 (F)). This sonic section mainly contains upgoing reflected Stoneley waves. The sonic section shown in Figure 10 has been filtered in order to separate the direct Stoneley wave and downgoing reflected Stoneley waves. The results are shown in Figs. 12 and 13. Second field example: full-waveform sonic data and VSP (vertical seismic profiling) The sonic tool used was a SEMM monopole sonic imaging tool. This tool is a four receiver/one transmitter flexible device. In the configuration used, the distance between two receivers was 50 cm. The distance between the monopole transmitter and the first receiver station was 3. 25 m. The source was fired at equal spacings of 5 cm throughout the open hole section of the well. The data sampling rate was 5 µs and 5 ms of the data were acquired for each trace. The sonic data were used to compute an accurate compressional-wave velocity-log obtained after picking of the refracted arrivals (Fig. 14) and then processed to obtain a near-borehole image. For that purpose, the sonic data were collected in constant offset gathers. For each gather (Fig. 15) versus depth, the wavefield separation was performed using a set of trace-pair Wiener filters. Each filter was applied in a time-window in order to select a specific wave. After filtering, the residual sonic section contains the noise and the other waves. In this field case, wave separation was performed in two steps. In the first step, a Wiener filter was applied in the 2 -to- 5 ms time-interval. It was used to enhance the interface modes (Fig. 16). The interface modes are very different in, sandstones and in shales. The transition between these two geological formations are clearly marked at 1 018 m. In shales (1 002 - 1 018 m depth interval), the interface modes have a high frequency content and propagate with a constant velocity, which is the mud velocity. They are mud waves. In sandstones, the interface modes have strong amplitudes and a low frequency content. They are Stoneley waves. After filtering, the residual sonic section shows the refracted modes and the leaky modes (Fig. 17). In sandstones, we can notice a residue of Stoneley modes at about 2. 5 ms. In shales, dipping sonic events appear and the residual waves and noise have much stronger amplitudes than they can have in sandstones (1 018 - 1 060 m depth interval). In the second step, a Wiener filter was applied to the constant-offset sonic-data gather, after elimination of interface modes. The filter was applied in the 0 to 2. 5 ms time interval to enhance the refracted and leaky modes (Fig. 18). The residual constant-offset section was corrected for normal moveout using the velocity function derived from the acoustic log. At about 1 010 m, diffractions can be observed on the filtered sonic section (Fig. 19). The interpretation of these diffracted patterns shows that they were created by heterogeneities in the shale medium due to dolomitic intercalations situated about 4 m from the well. Note that the diffractions, at the acoustic log scale, appear as variations in the shape of the reflected event associated with the dolomitic zone, at the VSP-CDP stack scale (Fig. 19). Third exemple: full waveform sonic data in horizontal well on test siteA highly deviated well and a vertical well were drilled in a limestone quarry situated in Burgundy, France. The vertical well was drilled to establish a vertical geologic cross-section of the quarry and to identify the acoustic impedance variation boundaries. For this purpose a VSP and a set of logs were recorded in the vertical well using a SEMM well seismic-logging unit. The set of logs includes : caliper, density, resistivity, neutron and full waveform acoustic logs. Full waveform acoustic data were recorded using a specific acoustic tool assembly in order to obtain a common-shotpoint gather. The source-receiver distance varied from 3. 60 to 7. 88 m. The sampling rate was 10 µs. The recording length was 10 ms. The receiver spacing was 1 cm. In the common-shot gather, the refracted arrivals and borehole modes have linear moveout across the offset range, while the reflected events have hyperbolic moveout. The acoustic data must be recorded with well suited spatial and temporal sampling-rates to avoid spatial and temporal aliasings. Under these conditions, all the refracted arrivals and borehole modes can easily be filtered by a set of apparent velocity filters. Each filter is designed to cancel a selected wave, characterized by its velocity. The processing included spectrum equalization in the 15 - 35 kHz frequency bandwidth (Fig. 22) and filtering of the linear moveout arrivals. After processing, the acoustic data (Fig. 26) show reflected events up to 6 ms. These reflections come from reflectors situated in the oolithe blanche reservoir layer up to a distance of approximately 10 m. The use of information coming from vertical well (VSP and log) and synthetic modeling leads us to differentiate the reflections coming from discontinuities situated above (A) and below (B) the horizontal well. Without a priori information, the problem of above/belowambiguity must be solved before stacking. For this purpose, the above/below reflection-separation can be performed by comparison of a set of acoustic runs obtained with an off centered logging tool (Mari, 1991). Another way is to use an acoustic logging tool with a modified radiation-pattern in order to illuminate either aboveor belowdiscontinuities...|$|E
40|$|Classiquement, la {{technique}} de <b>filtrage</b> utilisant la matrice spectrale proposée par Mermoz ne permet une séparation automatique des ondes au sens des indicatrices sismiques que dans certains cas particuliers, à savoir lorsque les ondes à séparer sont naturellement alignées sur les vecteurs propres de la matrice spectrale. Dans les autres cas, nous montrons que l'introduction d'information {{a priori}} sur la vitesse apparente de quelques ondes et une limitation de la durée temporelle de ces dernières permettent d'estimer leurs vecteurs d'ondes. L'utilisation de ces vecteurs et une technique de projection au sens des moindres carrés conduit à une extraction optimale de ces ondes, sans dégrader les autres ondes. La technique de <b>filtrage</b> proposée a été appliquée sur des données sismiques de type PSV (profil sismique vertical) déporté. Le PSV a été enregistré dans un puits entre les cotes 1050 m et 1755 m; la source est déportée de 654 m par rapport à la tête de puits. L'outil utilisé est un géophone de puits à trois composantes. Le puits traverse une structure géologique complexe. Le traitement réalisé a mis en évidence des réflexions sismiques d'ondes de compression et de cisaillement, associées à des marqueurs fortement pentés (10 à 25 °). Après estimation des champs de vitesse et des pendages à l'aide d'abaques, la migration en profondeur des horizons temps pointés a permis d'obtenir un modèle structural faillé. Detailed structural analysis {{can be achieved}} by using 3 -component vertical seismic profiling method which gives structural information at several hundred meters from the wellhead. The use of an offset VSP on the Auzance structure has led to obtain a structural model composed by faulted dipping reflectors. This is due to the robust nature of the wave separation method which is based on the spectral matrix and uses an a priori information. This method preserves the true amplitude and the local apparent velocity of the reflected waves. The proposed filtering technique was applied to seismic waves of the offset VSP type. The VSP was recorded in a well between the depths of 1050 and 1755 m. The tool used was a well geophone with three components. The well crossed a complex geological structure. Processing revealed seismic reflections of compressional and shear waves, associated with steeply sloping markers (10 to 25 degrees). Once we have estimated the velocity fields and the dips by means of charts, the depth migration of the picked time horizons provided a faulted structural model. Wave Separation Method - Generally speaking, the filtering technique using the spectral matrix proposed by Mermoz only allows an automatic separation of the waves in the sense of seismic time/distance curves in a number of specific cases, namely when the waves to be separated are naturally aligned with the eigenvectors of the spectral matrix. In the other situations, the introduction of an a priori information on the apparent velocity of some waves and a limitation of their time duration enable to estimate their associated wave vectors. The use of these vectors and a least square projection method lead to an optimal extraction of these waves, without degrading the other waves. In the Fourier domain, for seismic data D (f) composed of N recordings, the spectral matrix M consists in N x N components Mk, 1 (f). This term represents the cross-spectrum averaged between the recordings dk(t) and d 1 (t). The average is introduced to decorrelate the waves. Two types of averaging are routinely used, frequency averaging and distance averaging. Frequency averaging implies a local stationarity of the signals and favors the waves with very high apparent velocities. Distance averaging favors the signals that are coherent on several recordings, regardless of their apparent velocity. We propose a matrix calculated from the recordings modified to optimize the extraction of the desired wave Wi(f). Optimization is carried out in two steps: (a) First step: Realignment of the data, according to an apparent velocity model given a priori or introduced by picking, to give to the wave Wi(f) an infinite apparent velocity. (b) Second step: Limitation of the recording time in a time window centered on the wave to be extracted so as to minimize the interference of the other waves. The average applied to calculate the matrix is a high frequency average and a low distance average to preserve the variation in the character of the wave (phase and amplitude). The first eigenvector of the spectral matrix thus estimated represents the wave vector Si(f) of the desired wave Wi (f). To extract p waves, it is necessary to calculate p matrices. This gives a set of normalized vectors Si(f) for i = 1 to p. We shall apply this type of treatment to seismic data of the VSP type, recorded in a well drilled on a complex geological structure. Presentation of the data - The VSP consists in 48 measurement points located between the depths of 1050 and 1755 m, at intervals of 15 m. The source is a vertical vibrator, transmitting a vibroseismic signal in the frequency band 14 to 125 Hz, over a duration of 8 s. The source is offset 654 m from the wellhead. The average number of vibrations per measurement point is 3. The well geophone is the Schlumberger SAT. C probe, equipped with a system of three gimbal-mounted geophones, the time sampling interval is 2 ms, for a recording time of 2 s after correlation. Data preprocessing includes correlation, printing, stacking of the unit recordings at each level, and the re-orientation of the horizontal components, designed to compensate for the rotation of the tool and to obtain a seismic recording located in the plane passing through the well and the source. Figures 1 and 2 show the vertical component Z of the VSP and the horizontal component X after re-orientation. Wave separation. Separation is aimed to extract the downgoing and upgoing P compressional waves and the SV shear waves, using the multi [...] component data (X and Z). Separation is carried out in three steps :(a) estimation of the wave vectors Sp and Ssv associated with the downgoing P and SV waves. (b) Extraction on each component (Z and X) of the downgoing P (Figs. 3 and 6) and SV waves (Figs. 4 and 5), by least square projection on the vectors Sp and Ssv. (c) Analysis of the residues Rz and Rx (Figs. 7 and 8). On the residue Rz, the poorly estimated part of the downgoing P field is observed in a time window centered on the direct P arrival, as well as two P type reflections clearly individualized in depth. The poorly estimated part of the downgoing SV field interferes very little with the reflected upgoing P waves in the depth interval analyzed. In order {{to improve the quality of}} these reflections, the residual downgoing P field was subsequently filtered. The enhanced upgoing P waves are shown on Fig. 13, for the Z components. On the residue Rx, the poorly estimated part of the downgoing SV field is significant and requires supplementary filtering (Fig. 14). The VSP section (Fig. 13) shows three markers denoted A, B and C. Interpretation. Geophysical interpretation by means of charts (Fig. 18) conducted to an estimation of the structural dips, and led to the proposal of a faulted geological model (Fig. 20). Conclusion. It has been demonstrated that the acquisition, processing and interpretation of 3 -component offset VSP data on the Auzance structure have led to such an improved structural interpretation with an accurate fault location. This is due to the recording of the complete wavefield and the efficiency of the wave separation method which preserves the true amplitude and the local apparent velocity of the reflected waves. The filtering technique uses a spectral matrix proposed by Mermoz and consists in estimating separately the wave vector associated with each wave in a limited time window, by introducing its apparent velocity. The separation is then achieved by least square projection of the initial data on the different vectors...|$|E
40|$|Video {{signals are}} {{sequences}} of natural images, where images are often modeled as piecewise-smooth signals. Hence, video {{can be seen}} as a 3 D piecewise-smooth signal made of piecewise-smooth regions that move through time. Based on the piecewise-smooth model and on related theoretical work on rate-distortion performance of wavelet and oracle based coding schemes, one can better analyze the appropriate coding strategies that adaptive video codecs need to implement in order to be efficient. Efficient video representations for coding purposes require the use of adaptive signal decompositions able to capture appropriately the structure and redundancy appearing in video signals. Adaptivity needs to be such that it allows for proper modeling of signals in order to represent these with the lowest possible coding cost. Video is a very structured signal with high geometric content. This includes temporal geometry (normally represented by motion information) as well as spatial geometry. Clearly, most of past and present strategies used to represent video signals do not exploit properly its spatial geometry. Similarly to the case of images, a very interesting approach seems to be the decomposition of video using large over-complete libraries of basis functions able to represent salient geometric features of the signal. In the framework of video, these features should model 2 D geometric video components as well as their temporal evolution, forming spatio-temporal 3 D geometric primitives. Through this PhD dissertation, different aspects on the use of adaptivity in video representation are studied looking toward exploiting both aspects of video: its piecewise nature and the geometry. The first part of this work studies the use of localized temporal adaptivity in subband video coding. This is done considering two transformation schemes used for video coding: 3 D wavelet representations and motion compensated temporal filtering. A theoretical R-D analysis as well as empirical results demonstrate how temporal adaptivity improves coding performance of moving edges in 3 D transform (without motion compensation) based video coding. Adaptivity allows, at the same time, to equally exploit redundancy in non-moving video areas. The analogy between motion compensated video and 1 D piecewise-smooth signals is studied as well. This motivates the introduction of local length adaptivity within frame-adaptive motion compensated lifted wavelet decompositions. This allows an optimal rate-distortion performance when video motion trajectories are shorter than the transformation "Group Of Pictures", or when efficient motion compensation can not be ensured. After studying temporal adaptivity, the second part of this thesis is dedicated to understand the fundamentals of how can temporal and spatial geometry be jointly exploited. This work builds on some previous results that considered the representation of spatial geometry in video (but not temporal, i. e, without motion). In order to obtain flexible and efficient (sparse) signal representations, using redundant dictionaries, the use of highly non-linear decomposition algorithms, like Matching Pursuit, is required. General signal representation using these techniques is still quite unexplored. For this reason, previous to the study of video representation, some aspects of non-linear decomposition algorithms and the efficient decomposition of images using Matching Pursuits and a geometric dictionary are investigated. A part of this investigation concerns the study on the influence of using a priori models within approximation non-linear algorithms. Dictionaries with a high internal coherence have some problems to obtain optimally sparse signal representations when used with Matching Pursuits. It is proved, theoretically and empirically, that inserting in this algorithm a priori models allows to improve the capacity to obtain sparse signal approximations, mainly when coherent dictionaries are used. Another point discussed in this preliminary study, on the use of Matching Pursuits, concerns the approach used in this work for the decompositions of video frames and images. The technique proposed in this thesis improves a previous work, where authors had to recur to sub-optimal Matching Pursuit strategies (using Genetic Algorithms), given the size of the functions library. In this work the use of full search strategies is made possible, at the same time that approximation efficiency is significantly improved and computational complexity is reduced. Finally, a priori based Matching Pursuit geometric decompositions are investigated for geometric video representations. Regularity constraints are taken into account to recover the temporal evolution of spatial geometric signal components. The results obtained for coding and multi-modal (audio-visual) signal analysis, clarify many unknowns and show to be promising, encouraging to prosecute research on the subject. Video signals are sequences of natural images, where images are often modeled as piecewise-smooth signals. Hence, video {{can be seen as}} a 3 D piecewise-smooth signal made of piecewise-smooth regions that move through time. Based on the piecewise-smooth model and on related theoretical work on rate-distortion performance of wavelet and oracle based coding schemes, one can better analyze the appropriate coding strategies that adaptive video codecs need to implement in order to be efficient. Efficient video representations for coding purposes require the use of adaptive signal decompositions able to capture appropriately the structure and redundancy appearing in video signals. Adaptivity needs to be such that it allows for proper modeling of signals in order to represent these with the lowest possible coding cost. Video is a very structured signal with high geometric content. This includes temporal geometry (normally represented by motion information) as well as spatial geometry. Clearly, most of past and present strategies used to represent video signals do not exploit properly its spatial geometry. Similarly to the case of images, a very interesting approach seems to be the decomposition of video using large over-complete libraries of basis functions able to represent salient geometric features of the signal. In the framework of video, these features should model 2 D geometric video components as well as their temporal evolution, forming spatio-temporal 3 D geometric primitives. Through this PhD dissertation, different aspects on the use of adaptivity in video representation are studied looking toward exploiting both aspects of video: its piecewise nature and the geometry. The first part of this work studies the use of localized temporal adaptivity in subband video coding. This is done considering two transformation schemes used for video coding: 3 D wavelet representations and motion compensated temporal filtering. A theoretical R-D analysis as well as empirical results demonstrate how temporal adaptivity improves coding performance of moving edges in 3 D transform (without motion compensation) based video coding. Adaptivity allows, at the same time, to equally exploit redundancy in non-moving video areas. The analogy between motion compensated video and 1 D piecewise-smooth signals is studied as well. This motivates the introduction of local length adaptivity within frame-adaptive motion compensated lifted wavelet decompositions. This allows an optimal rate-distortion performance when video motion trajectories are shorter than the transformation "Group Of Pictures", or when efficient motion compensation can not be ensured. After studying temporal adaptivity, the second part of this thesis is dedicated to understand the fundamentals of how can temporal and spatial geometry be jointly exploited. This work builds on some previous results that considered the representation of spatial geometry in video (but not temporal, i. e, without motion). In order to obtain flexible and efficient (sparse) signal representations, using redundant dictionaries, the use of highly non-linear decomposition algorithms, like Matching Pursuit, is required. General signal representation using these techniques is still quite unexplored. For this reason, previous to the study of video representation, some aspects of non-linear decomposition algorithms and the efficient decomposition of images using Matching Pursuits and a geometric dictionary are investigated. A part of this investigation concerns the study on the influence of using a priori models within approximation non-linear algorithms. Dictionaries with a high internal coherence have some problems to obtain optimally sparse signal representations when used with Matching Pursuits. It is proved, theoretically and empirically, that inserting in this algorithm a priori models allows to improve the capacity to obtain sparse signal approximations, mainly when coherent dictionaries are used. Another point discussed in this preliminary study, on the use of Matching Pursuits, concerns the approach used in this work for the decompositions of video frames and images. The technique proposed in this thesis improves a previous work, where authors had to recur to sub-optimal Matching Pursuit strategies (using Genetic Algorithms), given the size of the functions library. In this work the use of full search strategies is made possible, at the same time that approximation efficiency is significantly improved and computational complexity is reduced. Finally, a priori based Matching Pursuit geometric decompositions are investigated for geometric video representations. Regularity constraints are taken into account to recover the temporal evolution of spatial geometric signal components. The results obtained for coding and multi-modal (audio-visual) signal analysis, clarify many unknowns and show to be promising, encouraging to prosecute research on the subject. Le signal vidéo est une séquence d'images en mouvement, dont les images sont souvent modelées comme des signaux réguliers par morceaux. Ainsi, le signal vidéo peut être considéré comme un signal 3 D régulier par morceaux, et composé de régions qui suivent un certain mouvement a travers le temps. La modélisation du signal vidéo par morceaux permet d'analyser en détail le comportement de différentes stratégies de codage, et ainsi de déterminer quelles sont les approches les plus appropriées pour maximiser le taux de compression. Afin de permettre un codage efficace de la vidéo, il est nécessaire d'utiliser des méthodes adaptatives de décomposition du signal. Cette adaptabilité doit être optimisée pour garantir une modélisation du signal avec un coût de codage minimum. La nature du signal vidéo est fortement liée à sa structure, avec une forte composante géométrique. Celle-ci inclut tant la géométrie temporelle (normalement représentée par l'information du mouvement) que la géométrie spatiale. La plupart des méthodes utilisées pour la représentation du signal vidéo ne tient pas compte de sa géométrie spatiale. De même que dans le cas des images, une stratégie prometteuse pour exploiter conjointement la structure géométrique spatio-temporelle est celle qui utilise des dictionnaires redondants avec une forte composante géométrique. Dans le contexte de la vidéo, les primitives géométriques 2 D doivent suivre une évolution temporelle en formant des complexes primitives 3 D qui ont la fonction de représenter, en même temps, les composantes géométriques spatiales et temporelles du signal. Dans cette thèse de doctorat, plusieurs aspects concernant l'utilisation de méthodes adaptatives pour la modélisation du signal vidéo sont traités. Cette thèse traite particulièrement des aspects structurels de la vidéo ainsi que de sa nature géométrique. La première partie du présent travail porte sur l'étude de l'utilisation de décompositions temporelles adaptatives dans des approches basées sur la décomposition de la vidéo en sous-bandes. L'influence de l'adaptabilité est notamment discutée pour deux stratégies de codage: les transformées en ondelettes 3 D et le <b>filtrage</b> temporel avec compensation de mouvement. Les avantages de l'utilisation d'adaptabilité dans des représentations basées sur la transformée en ondelettes 3 D sont démontrés à l'aide d'une étude théorique R-D ainsi que par des résultats expérimentaux. L'utilisation de l'adaptabilité dans le cadre du <b>filtrage</b> temporel avec compensation du mouvement est aussi étudiée en faisant une analogie entre le signal vidéo, avec le mouvement compensé, et les signaux réguliers par morceaux 1 D. Cette analogie suggère l'introduction des transformées de longitude localement variable dans des schémas de décomposition par ondelettes bases sur des lifting steps. Cette modification permet une plus forte compression du signal grâce à une meilleure adaptation de la représentation des trajectoires de mouvement avec une longueur inférieure à celle du Groupe d'Images (GOP - en anglais -) ou quand l'erreur due à la compensation de mouvement est trop élevé. Après l'étude d'adaptabilité temporelle, une deuxième partie de cette thèse se concentre également sur l'étude et la compréhension des concepts de base pour exploiter, conjointement, la structure géométrique spatio-temporelle du signal. Cette recherche se base sur des études précédentes qui tenaient compte de la géométrie spatiale de la vidéo, sans considérer son évolution temporelle (mouvement). Afin d'obtenir des représentations flexibles et efficaces (parcimonieuses), avec des dictionnaires redondants, il faut utiliser des algorithmes de décomposition hautement non linéaires, tels que les algorithmes gloutons (Greedy algorithms et Matching Pursuits en anglais). L'utilisation de ces techniques est encore peu explorée. Pour cette raison, avant d'étudier de telles représentations, certains aspects liés à l'utilisation de ces algorithmes conjointement avec des dictionnaires cohérents pour l'approximation des images et de la vidéo sont étudiés. Une partie de cette étude présente l'utilisation des modèles à priori dans des algorithmes non-linéaires comme les Matching Pursuits. En fonction du dictionnaire utilisé, et du signal, les Matching Pursuits peuvent avoir des grandes difficultés pour arriver à obtenir des expansions parcimonieuses optimales. Basé sur ce résultat, il peut être démontré, de manière théorique et expérimentale, que l'utilisation des modèles à priori (comme par exemple des modèles probabilistes) peut contribuer très significativement à l'amélioration des performances de ces algorithmes. Une autre partie de l'étude préliminaire, pour l'utilisation des Matching Pursuits, concerne l'approche utilisée dans cette thèse pour la décomposition du signal vidéo et des images. L'approche proposé dans cette thèse améliore une méthode existante de Matching Pursuits utilisée dans le passé dans un but similaire. Cette méthode était basée, pour des raisons de complexité calculatoire, sur l'utilisation d'algorithmes génétiques. La méthode proposée dans cette thèse rend possible, d'une manière plus rapide et efficace, la substitution de l'algorithme génétique par une recherche exhaustive. Finalement, l'utilisation des Matching Pursuits avec des modèles à priori pour la décomposition du signal vidéo est étudiée. Des critères de régularité ont étés imposés afin de capturer l'évolution temporelle des composantes géométriques 2 D. Les résultats obtenus pour le codage de ces représentations, ainsi que les résultats issus des analyses multimodales (audio/vidéo) d'une séquence, permettent d'éclaircir une grand partie des points incompris jusqu'alors sur l'utilisation des dictionnaires redondants avec des Matching Pursuits pour les représentations géométriques adaptatives en espace et en temps du signal vidéo...|$|E
40|$|The {{past few}} years has {{witnessed}} tremendous upsurge in information availability in the electronic form, attributed to the ever mounting use of the World Wide Web (WWW). For many people, the World Wide Web has become an essential means of providing and searching {{for information leading to}} large amount of data accumulation. Searching web in its present form is however an infuriating experience {{for the fact that the}} data available is both superfluous and diverse in form. Web users end up finding huge number of answers to their simple queries, consequentially investing more time in analyzing the output results due to its immenseness. Yet many results here turn out to be irrelevant and one can find some of the more interesting links left out from the result set. Chapter 1 Introduces our motivation behind the research: One of the principal explanations for the unsatisfactory condition in information retrieval is the reason that majority of the existing data resources in its present form are designed for human comprehension. When using these data with machines, it becomes highly infeasible to obtain good results without human interventions at regular levels. So, one of the major challenges faced by the users as providers and consumers of web era is to imagine intelligent tools and theories in knowledge representation and processing for making the present data, machine understandable. Chapter 2 evaluates and studies the existing methods and their short falls: Several researches has been carried out in enable machines to understand data and some of the most interesting solutions proposed are the semantic web based ontology to incorporate data understanding by machines. The objective here is to intelligently represent data, enabling machines to better understand and enhance capture of existing information. Here the main emphasis is given to the thought for constructing meaning related concept networks for knowledge representation. Eventually the idea is to direct machines in providing output results of high quality with minimum or no human intervention. In recent years the development of ontology is fast gaining attention from various research groups across the globe. There are several definitions of ontology purely contingent on the application or task it is intended for. Chapter 3 presents the platform ToxNuc-E and positioning of our research around this platform: Given the practical and theoretical importance of ontology development, it is not surprising to find a large number of enthusiastic and committed research groups in this field. Extended Semantic Network is one such innovative approach proposed by us for knowledge representation and ontology like network construction, which looks for sets of associations between nodes semantically and proximally. Our objective here is to achieve semi-supervised knowledge representation technique with good accuracy and minimum human intervention, using the heuristically developed information processing and integration methods. The main goal of our research is to find an approach for automatic knowledge representation that can eventually be used in classification and search algorithms in the platform ToxNuc-E. Chapter 4 elaborates on the concept of Proximal Network modeling, generated by mathematical models: As stated earlier the basic idea of Extended Semantic Network is to identify an efficient knowledge representation and ontology construction method to overcome the existing constraints in information retrieval and classification problems. To realize this we put our ideas into practice via a two phase approach. The first phase consists in processing large amount of textual information using mathematical models to make our proposal of automatic ontology construction scalable. This phase of our proposal is carried out by realising a network of words mathematically computed using different statistical and clustering algorithms. Thus creating a proximal network computationally developed, depending essentially on word proximity in documents. The proximal network is basically representing the recall part of our approach. Chapter 5 investigates the semantic network modelling and introduces a design model proposed by us to enable efficient cost effective design: Semantic Network is basically a labelled, directed graph permitting the use of generic rules, inheritance, and object-oriented programming. It is often used as a form of knowledge representation where concepts represented by nodes are connected to one another using the relational links represented by arcs. Semantic network is constructed with the help of expert knowledge and understanding of a domain. Hence it is mainly a human constructed network with very good precision. Chapter 6 in effect details the extended semantic network: The second phase of our research mainly consists in examining carefully and efficiently the various possibilities of integrating information obtained from our mathematical model with that of the manually developed mind model. This phase is ensured by a heuristically developed method of network extension using the outputs from the mathematical approach. This is achieved by considering the manually developed semantic mind model as the entry point of our concept network. Here, the primary idea is to develop a innovative approach obtained by combining the features of man and machine theory of concepts, whose results can be of enormous use in the latest knowledge representation, classification, retrieval, pattern matching and ontology development research fields. In this research work we illustrate the methods used by us for information processing and integration aimed at visualising a novel method for knowledge representation and ontology construction. Chapter 7 illustrates some of the experiments carried out using our extended semantic network and opens directions for future perspectives: The question on knowledge representation, management, sharing and retrieval are both fascinating and complex, essentially with the co-emergence between man and machine. This research presents a novel collaborative working method, specifically in the context of knowledge representation and retrieval. The proposal is to attempt at making ontology construction faster and easier. The advantages of our methodology with respect to the previous work, is our innovative approach of integrating machine calculations with human reasoning abilities. The resulting network so obtained is later used in several tools ex: document classifier to illustrate our research approach. We use the precise, non estimated results provided by human expertise in case of semantic network and then merge it with the machine calculated knowledge from proximal results. The fact that we try to combine results from two different aspects forms one of the most interesting features of our current research. We view our result as structured by mind and calculated by machines. One of the main future perspectives of this research is finding the right balance for combining the concept networks of semantic network with the word network obtained from the proximal network. Our future work would be to identify this accurate combination between the two vast methods and setting up a benchmark to measure our prototype efficiency. Ces dernières années ont vu le déferlement d'une vague d'information sous forme électronique liée à l'usage croissant du World Wide Web (WWW). Pour beaucoup, le World Wide Web est devenu un moyen essentiel pour mettre à disposition ou rechercher de l'information, conduisant à une forte accumulation de données. La recherche sur Internet dans sa forme présente devient vite exaspérante car les données disponibles peuvent être superficielles et de formes très diverses. Les utilisateurs du Web en ont assez d'obtenir des ensembles gigantesques de réponses à leurs requêtes simples, ce qui les oblige à investir de plus en plus de temps pour analyser les résultats. De nombreux résultats s'avèrent non pertinents et les liens les plus intéressants restent souvent en dehors de l'ensemble des résultats. Le chapitre 1 introduit la motivation de notre travail de recherche. L'une des principales explications concernant la difficulté à effectuer une recherche d'information efficace est que les ressources existantes sur le web sont exprimées sous une forme destinée à la compréhension humaine. En d'autres termes, ces données sont difficilement utilisables par la machine et l'intervention humaine s'avère indispensable. Ainsi, l'un des principaux challenges est d'imaginer des outils intelligents fondés sur les concepts et méthodes autour de la représentation et du traitement des connaissances pour créer des données exploitables par la machine et obtenir de meilleurs résultats. Le chapitre 2 évalue et étudie les méthodes existantes et leurs limitations. De nombreux chercheurs ont travaillé sur la problématique de la compréhension des données par la machine et certaines des solutions les plus intéressantes sont les ontologies basées sur le « web sémantique ». Les ontologies permettent une meilleure « compréhension » des documents et facilitent à l'aide d'outils appropriés la qualité des recherches dans l'information existante. L'accent est mis sur la réflexion nécessaire à la construction de la signification du concept relié aux réseaux pour la représentation des connaissances. L'idée est de tendre vers la production semi-automatique voire complètement automatique de résultats de grande qualité. Autrement dit, l'objectif est de minimiser l'intervention humaine est de maximiser la qualité des résultats obtenus. Le chapitre 3 présente la plate-forme ToxNuc-E et le positionnement de notre recherche autour de cette plate-forme. Etant donné l'importance pratique et théorique du développement d'ontologies, il n'est pas surprenant de retrouver un grand nombre de chercheurs, fervents et engagés dans ce domaine de recherche. Dans le cadre de notre travail de recherche nous proposons une approche nouvelle, dite ESN (« Extended Semantic Network »), qui contrairement aux approches classiques, basées sur les mots clés, fonde la construction d'ontologie sur la convergence d'associations entre concepts ou nœuds sémantiques sur un ensemble de thèmes et la proximité des termes dans un ensemble de documents. Notre terrain d'application est le programme de toxicologie nucléaire environnementale français : ToxNuc-E. Le chapitre 4 précise le concept de « réseau de proximité », généré par des modèles mathématiques. L'idée de base de notre approche ESN est de construire efficacement une ontologie adaptée à la recherche d'information dans de larges corpus. La première phase consiste à traiter une grande quantité d'information textuelle en utilisant des modèles mathématiques pour automatiser la construction d'un embryon d'ontologie. L'objectif est d'obtenir un réseau de mots qui peut être assez volumineux. Celui-ci est calculé en utilisant des outils mathématiques venant de l'analyse de données et la classification automatique. Ainsi, la création d'un réseau de proximité repose alors sur la proximité des mots dans un document. Le chapitre 5 présente la construction des « réseaux sémantiques » et introduit notre modèle de conception pour gagner en efficacité. Le réseau sémantique est essentiellement un graphe orienté étiqueté permettant l'utilisation de règles génériques, de l'héritage, et de la représentation orientée objet. Il est souvent utilisé comme une forme de représentation des connaissances, où les concepts représentés par les nœuds sont connectés l'un à l'autre en utilisant les liens relationnels représentés par des arcs. Le réseau sémantique est construit de façon manuelle avec l'aide d'experts de la connaissance possédants la compréhension d'un domaine. Il est donc principalement construit par les hommes, dans notre approche de taille assez réduite, et d'une très bonne précision. Le chapitre 6 détaille le « réseau sémantique étendu ». La deuxième phase de traitement consiste à examiner attentivement et de manière efficace les différentes possibilités d'intégrer les informations issues du modèle mathématique (réseau de proximité) et du modèle cognitif développé manuellement (réseau sémantique). Cette phase se base sur une méthode heuristique développée dans l'extension des réseaux et utilisant les résultats de la méthode mathématique. Cette phase se termine en considérant le modèle humain (développé manuellement) comme le point d'entrée de notre réseau de concepts. L'idée principale est de développer une approche novatrice combinant les caractéristiques humaines et la théorie des concepts utilisée par la machine. Les résultats peuvent présenter un grand intérêt dans différents champs de recherche tels que la représentation des connaissances, la classification, l'extraction, ainsi que le <b>filtrage</b> des données. Le chapitre 7 illustre quelques expérimentations réalisées à l'aide de notre réseau sémantique étendu et ouvre des orientations pour les perspectives d'avenir. Les questions concernant la représentation des connaissances, la gestion, le partage et l'extraction d'information sont passionnantes et complexes. Cet attrait est en toute évidence essentiellement du aux rapports entre l'homme et la machine. Le fait que nous essayons de combiner les résultats de deux aspects différents constitue l'une des caractéristiques les plus intéressantes de notre recherche actuelle. Notre proposition peut faciliter la construction d'ontologies de manière plus rapide et plus simple. Le réseau sémantique étendu peut être utilisé, à la place d'une ontologie plus classique, par des outils comme par exemple : un classificateur de documents. Nous considérons notre résultat comme étant structuré par l'esprit et calculé par la machine. L'une des principales perspectives pour le travail à suivre est de trouver un bon compromis entre concepts du réseau sémantique et graphes de mot issus du réseau de proximité. D'autres perspectives à ce travail consistent à mettre en place des benchmarks dans différents contextes pour mesurer l'efficacité de notre prototype...|$|E
40|$|The European Nitrates Directive 91 / 6 / 76 /EEC aims {{to ensure}} water quality by {{preventing}} pollution of surface and groundwater induced by nitrates originating from agricultural sources and by promoting agronomical good practices. While {{the implementation of}} this Directive seems effective, it appears however {{that the use of}} nitrogen has still increased by 6 % {{over the last four years}} in 27 European countries. Furthermore, agricultural sources would be still at the origin of 50 % of the total amount of nitrogen discharged into surface waters ([URL] In Wallonia (Belgium), the Nitrates Directive has been transposed under the Sustainable Nitrogen Management in Agriculture Program (PGDA). Launched in 2002, it involves different sets of actions, like rules definitions concerning fertilizers application, specific and appropriate crop management in vulnerable areas, the control of potentially leachable nitrogen (APL) levels in soils, etc. This is the global context in which lies the present thesis. The main aim is to optimise the nitrogen fertiliser practices to ensure that the needs of a winter wheat culture (Triticum aestivum L.) could be met while reducing the environmental pressure. It relies on the use of crop models, which describe the growth and the development of a culture interacting with its environment, namely the soil and the atmosphere. The major difficulty while working with crop models and model-based decision support tools lies in the fact that different sources of uncertainties have an impact on the modelled phenomena. Indeed, crop models are constituted by a consequent number of differential non-linear equations, involving a lot of parameters which need to be determined as accurately as possible in order to match as close as possible observed sequences of measurements. The first source of uncertainty is thus constituted by the parameters definition. Once the model has been correctly and robustly calibrated it can be used to perform predictions. However, in an agronomical context, the time-delay between sowing and harvest is consequent. As the end-season yield is often the expected output, the uncertainty linked to the non-knowledge of the future implies for the modeller to refer to different hypothesis concerning upcoming climatic scenarios. Finally, moving from models to decision systems dealing with N management involves a last source of uncertainty. Indeed the main problem is that the impact of a given practice is delayed in time from its realisation. In addition to the uncertainty linked to climatic projections themselves, it is highly important to consider the interactions between the practices and the climate. Furthermore, in a decision-making process, it could be highly relevant to know the uncertainty's estimation that could be tolerated on the decision [...] Therefore, the present thesis aims to study these different sources of uncertainty in order to design an efficient decision support system. It is divided into five parts. In the first part, a Bayesian sampling algorithm, known as DREAM (DiffeRential Evolution Adaptative Metropolis) will be presented. It was successfully coupled with the STICS soil-crop model used in this study. The a posteriori probability density function of many parameters was sampled in order to improve the simulations of the growth of a winter wheat culture (Triticum aestivum L.). The DREAM algorithm offers different advantages in comparison to usual methods. Among these, it is possible to study i) the most probable a posteriori parameters distributions, ii) the parameters correlations, and iii) the uncertainties impacted on model outputs. Furthermore, a new version of the likelihood function was proposed, making an explicit use of the coefficient of variation. Results showed that it allowed the noise existing on measurements to be considered, but also the heteroscedasticity phenomenon usually encountered in biological growth processes. In parallel, assimilation data is another way to improve models simulations. These techniques allow considering measurements performed in real-time (e. g. remote measures of LAI or soil water content) in order to correct and adjust the possible drift of model simulations. In particular, a recently developed algorithm, known as variational filter, was evaluated. Its superiority, both in term of state variables simulations improvement and parameter resampling, was demonstrated. The third part of the research focuses on the real-time end season yield prediction. It involves building climate matrix ensembles, combining different time ranges of projected mean data and real measured weather originating from the historical records. As the crop growing season progresses, the effects of real monitored data plays a greater role and the prediction reliability increases. Our results demonstrated that a reliable predictive delay of 3 - 4 weeks before harvest could be obtained. Finally, using real-time data acquired with a micrometeorological station enabled to (i) predict, daily, potential yield at the local level, (ii) detect stress occurrence, and (iii) quantify yield losses (or gains). Being based on projected seasonal norms, this methodology is in opposition to another technique that consists to offer a panel of solution for what concerns the future. Such probabilistic technique relies on the use of stochastic weather generator (LARS-WG in this case). However, in the fourth part of this thesis, on the basis of the convergence in law theorem, it was demonstrated that in 90 % of the climatic situations, both approaches were equivalent, exhibiting RRMSE and normalised deviation criteria inferior to 10 %. Furthermore the two approaches offered similar predictive delay-time. The main difference between techniques lies in the finality. The first allows to quickly simulate the remaining yield potential, while the second aims to quantify the uncertainty level associated to the predictions. In the fifth and last part of this thesis, in order to quantify the uncertainty level associated to different modalities of N applications, the STICS model answers were studied under stochastic climatic realisations. It was demonstrated that, if no N was applied, under our temperate climatic conditions, the yield distribution could be considered as normal. However, with increasing N practices, the asymmetry level was found itself increasing. As soon as N was applied, not only were the yields higher, but also was the probability to achieve yields that were at least superior to the mean of the distribution. This undoubtedly reduced the risk for the farmer to achieve low yields levels. To summary all the researches conducted in this thesis, a N strategic decision support system was developed. In a general way, for what concerns the Hesbaye Region, the superiority of three fractions N protocols was demonstrated. In addition, the three rates fertilisation management based on the systematic applications of 60 kgN. ha- 1 at tillering and stem extension stages and offering the possibility to adapt the flag-leaf fraction in real-time appeared as an optimal strategy. Within this tool, the uncertainty associated to climatic variability could be finely characterised, and the risk encountered by the farmer was quantified for different investigated practices. But far more important, it was demonstrated that N management could be optimised in real-time. In a general way, the research should be pursued by studying more fundamentally and systematically a wide range of different agro-environmental situations. In particular, it would be interesting to study of the Genotype × Environment × Cultural practices interactions to ensure food security in a climatic changing world. La directive Européenne 91 / 6 / 76 /EEC vise à protéger la qualité de l’eau en prévenant la pollution des eaux souterraines et superficielles par les nitrates provenant de sources agricoles et en promouvant l’usage de bonnes pratiques. Si la mise en œuvre de cette directive s’avère efficace, il apparaît cependant que l’utilisation d’azote a augmenté de 6 % au cours des quatre dernières années dans les 27 Etats membres et que l’agriculture est toujours à l’origine de plus de 50 % de la quantité totale d’azote déversée dans les eaux superficielles ([URL] En Wallonie, la directive «Nitrates» est transposée sous la forme du Programme de Gestion Durable de l'Azote (PGDA). Entré en vigueur en 2002, celui-ci comporte un ensemble de mesures comme, par exemple, la fixation de règles au niveau de l’épandage des fertilisants, la mise en place de modes de gestion spécifiques des cultures en zones vulnérables, la mesure des teneurs en azote potentiellement lessivable (APL) dans les sols, etc. La thèse de doctorat s'inscrit dans ce contexte général qui vise à optimiser la quantité de fertilisants azotés à apporter à une culture de blé d’hiver (Triticum aestivum L.). Elle s’appuie sur un modèle de culture ou modèle écophysiologique, qui décrit la croissance et le développement d’une culture en interaction avec le sol et l’atmosphère. La difficulté majeure dans l’utilisation d’un tel modèle en tant qu'outil d’aide à la décision est liée aux nombreuses incertitudes qui interviennent à différents niveaux. En effet, les modèles de culture dynamiques sont constitués d’un grand nombre d’équations différentielles non-linéaires, comportant de nombreux paramètres à estimer pour que les sorties se rapprochent le plus possible de séquences observées. Une première source d’incertitude existe donc au niveau des paramètres du modèle. Une fois le modèle robustement calibré, il peut être utilisé à des fins prédictives. Toutefois, dans un contexte agronomique, le délai entre la date de semis et celle de récolte est important. Comme il est souhaitable de prédire le rendement final d’un point de vue quantitatif et qualitatif, différentes hypothèses sur les scénarios climatiques qui interviennent pendant la saison culturale doivent être posées. Une deuxième source d’incertitude est donc liée au climat. Enfin, le passage d'un modèle à un outil fonctionnel de gestion des fertilisants azotés nécessite de franchir une étape supplémentaire car l’impact réel des fertilisants n’est connu qu'en fin de saison. Une troisième source d’incertitude est donc liée à la quantification du niveau d'erreur toléré sur la gestion des fertilisants, compte tenu des interactions existant entre la pratique envisagée et la réalisation climatique. Cette thèse de doctorat a pour objectif d'étudier les différentes sources d'incertitudes afin d’aboutir à un outil d'aide à la décision efficace. Elle comporte cinq parties. Dans la première partie, une approche Bayésienne d'identification des paramètres du modèle de culture STICS est présentée. Elle repose sur l'algorithme DREAM (DiffeRential Evolution Adaptative Metropolis). Les distributions a posteriori de plusieurs paramètres du modèle STICS sont échantillonnées en vue d'améliorer les simulations de la croissance et du développement du blé d'hiver. Par rapport aux algorithmes de type Simplex habituellement utilisés dans les modèles de culture, le couplage de STICS avec DREAM fournit une approximation de la distribution a posteriori des paramètres, une évaluation des corrélations qui existent entre eux et surtout quantifie l'incertitude dans l'estimation des rendements. Par ailleurs, une fonction de vraisemblance faisant explicitement mention du coefficient de variation est proposée. Les résultats montrent que cette dernière permet de prendre en compte le bruit sur les mesures et l'hétéroscédasticité régulièrement rencontré dans les modèles de culture. En parallèle ou en complément aux approches d’identification paramétrique, des techniques de <b>filtrage</b> permettent d’améliorer les simulations des modèles de culture. Au moyen d'algorithmes couplés au modèle de culture, ces techniques visent à intégrer dans le modèle des mesures réalisées en cours de saison, comme par exemple l'indice de développement foliaire LAI ou la teneur en eau du sol, de manière à corriger la trajectoire modélisée de l'évolution des variables d'état. La capacité d'un algorithme récemment développé et connu sous le nom de filtre variationnel est évaluée et sa supériorité en termes d’estimation paramétrique et de réduction des incertitudes sur les variables de sortie est démontrée par rapport à celle de méthodes plus conventionnelles, basées sur le filtre de Kalman. La troisième partie de cette étude se focalise sur la prédiction du rendement à l'aide du modèle STICS en s’attachant aux incertitudes liées aux entrées climatiques. La méthodologie utilisée consiste à construire des 'ensembles de matrices' climatiques évoluant au cours de la saison, constitués d'une part de données climatiques réelles mesurées au fur et à mesure de la croissance de la culture, et d'autre part de données climatiques moyennes calculées au départ d'une base de données locale historique. Au fur et à mesure de l'avancement de la saison, la part des premières données devient prépondérante par rapport à celle des secondes. Il est montré que cette méthodologie, que nous appellerons ‘climat moyen’ permet de prédire les rendements environ un mois avant la date de récolte avec un intervalle de confiance de 10 %. En outre, cette approche permet de quantifier les potentialités restantes de rendement d'une culture donnée dans des conditions agro-environnementales données et de détecter l'occurrence de stress lors de la croissance des cultures. Dans la quatrième partie, une méthode reposant sur la génération d'un grand nombre de réalisations climatiques stochastiques (LARS-Weather Generator) est mise en œuvre. En se basant sur le théorème de la convergence en loi, il est démontré que les approches stochastiques et ‘climat moyen’ présentent des critères statistiques comme la RRMSE et la déviation normalisée inférieurs à 10 % et sont dans 90 % des situations climatiques équivalentes en termes de simulation du rendement. Elles offrent par ailleurs le même délai prédictif. La distinction entre les méthodes se situe surtout au niveau de la finalité visée, l'une permettant de simuler rapidement le potentiel restant à la culture, l'autre assurant la quantification du niveau d'incertitude sur la prédiction. Dans la cinquième et dernière partie, la réponse du modèle STICS est étudiée sous des réalisations climatiques stochastiques en vue de quantifier le niveau d'incertitude associé à différentes modalités d'apports de fertilisants azotés. Il est montré que, sous nos latitudes, en l’absence de fertilisant azoté, les distributions de rendements sont normales. Avec l'accroissement des niveaux de fertilisation, un degré d'asymétrie croissant est observé. Dès l'instant où un fertilisant azoté est appliqué, non seulement les rendements augmentent, mais la fréquence des rendements au moins supérieurs à la moyenne croit également. Ceci réduit inéluctablement le risque pour l'agriculteur d'obtenir des rendements bas. Pour synthétiser les études menées au long de cette thèse, un outil de gestion stratégique de l'azote, c'est-à-dire sans connaissance a priori des conditions climatiques à venir, est mis au point. D'une manière générale, pour ce qui concerne nos régions limoneuses, la supériorité des fertilisations en trois fractions est mise en évidence. Un système de gestion de l'azote, basé sur un apport de deux modalités de 60 kilos d'azote par hectare appliqués aux stades tallage et redressement et offrant la possibilité de moduler la fraction de dernière feuille, semble optimal. L'incertitude associée aux prédictions et issue de la variabilité climatique peut être caractérisée finement et il est possible de quantifier le risque encouru par l'agriculteur qui envisage différentes pratiques afin de procéder à une optimisation de celles-ci. D’une manière générale, les recherches pourraient être poursuivies en étudiant de façon plus fondamentale et plus systématique différentes situations agro-environnementales. En particulier, il serait intéressant de développer l’étude de l'interaction Genotype × Environnement × Pratiques culturales pour garantir la sécurité alimentaire dans un contexte de changement climatique...|$|E
