6911|2855|Public
5|$|If {{the trial}} judge erroneously permits the {{striking}} of a juror under Batson, and the error is preserved, the only remedy is automatic reversal. If {{the trial judge}} erroneously prevents the striking of a juror under Batson, and the juror is seated, the Constitution permits a jurisdiction to utilize harmless <b>error</b> <b>analysis.</b> The race of the defendant is irrelevant to a Batson claim. Batson also permits the prosecutor to challenge defense peremptory strikes ("reverse Batson"). And, Batson applies equally to race and gender.|$|E
5|$|If a {{defendant}} is convicted, the usual remedy for {{a violation of}} one of these provisions is reversal of the conviction or modification of the defendant's sentence. With the exception of structural errors (such as the total denial of counsel), constitutional errors are subject to harmless <b>error</b> <b>analysis,</b> although they must be harmless beyond a reasonable doubt. With the exception of a Double Jeopardy or Speedy Trial violation, the government will usually be permitted to retry the defendant. Pursuant to the Antiterrorism and Effective Death Penalty Act of 1996 (AEDPA), these provisions are the source of nearly all reviewable errors in federal habeas review of state convictions.|$|E
25|$|Interval {{arithmetic}} is {{used with}} <b>error</b> <b>analysis,</b> to control rounding errors arising from each calculation.|$|E
3000|$|... [...].The aim of {{the present}} work is to compute the {{eigenvalues}} of (1.1)-(1.5) numerically by the sinc-Gaussian technique with <b>errors</b> <b>analysis,</b> truncation <b>error</b> and amplitude error.|$|R
40|$|Abbreviations: (CG-EGA) {{continuous}} glucose <b>error</b> grid <b>analysis,</b> (CGM) continuous glucose monitoring, (CRU) {{clinical research}} unit, (FDA) Food and Drug Administration, (HbA 1 c) hemoglobin A 1 c, (ISF) interstitial fluid, (MARD) mean absolute relative difference, (MAD) mean absolute difference, (P-EGA) point <b>error</b> grid <b>analysis,</b> (R-EGA) rate-of-change <b>error</b> grid <b>analysis,</b> (SMBG) self-monitoring of blood glucose, (YSI) Yellow Springs Instrument...|$|R
40|$|Abstract. The error {{compensation}} {{is an essential}} issue for improving {{the accuracy of the}} machining process. To solve the {{error compensation}} for parallel grinding of the noncoaxial aspheric lens, a kinematics model of six-axis ultra-precision machining system has been developed in the present study. Based on the theory of multi-body system, a kinematics <b>errors</b> <b>analysis</b> is presented. The interpolation errors of the parallel grinding method are discussed according to the kinematics <b>errors</b> <b>analysis.</b> Simulation results show that the rotation errors of the grinding system are crucial factors affecting the accuracy of the machining process...|$|R
25|$|Other tools {{which may}} be used to detect {{fraudulent}} data include <b>error</b> <b>analysis.</b> Measurements generally have a small amount of error, and repeated measurements of the same item will generally result in slight differences in readings. These differences can be analyzed, and follow certain known mathematical and statistical properties. Should a set of data appear to be too faithful to the hypothesis, i.e., the amount of error that would normally be in such measurements does not appear, a conclusion can be drawn that the data may have been forged. <b>Error</b> <b>analysis</b> alone is typically not sufficient to prove that data have been falsified or fabricated, but it may provide the supporting evidence necessary to confirm suspicions of misconduct.|$|E
25|$|The {{birth of}} modern {{interval}} arithmetic {{was marked by}} the appearance of the book Interval Analysis by Ramon E. Moore in 1966. He had the idea in Spring 1958, and a year later he published an article about computer interval arithmetic. Its merit was that starting with a simple principle, it provided a general method for automated <b>error</b> <b>analysis,</b> not just errors resulting from rounding.|$|E
25|$|The Euler–Maclaurin {{formula is}} also used for {{detailed}} <b>error</b> <b>analysis</b> in numerical quadrature. It explains the superior performance of the trapezoidal rule on smooth periodic functions and is used in certain extrapolation methods. Clenshaw–Curtis quadrature is essentially a change of variables to cast an arbitrary integral in terms of integrals of periodic functions where the Euler–Maclaurin approach is very accurate (in that particular case the Euler–Maclaurin formula {{takes the form of}} a discrete cosine transform). This technique is known as a periodizing transformation.|$|E
40|$|In case of {{coordinate}} {{machines that}} use CAA correction matrix, {{the issue of}} kinematic <b>errors</b> <b>analysis</b> may {{be based on the}} determination of residual error distribution. Temperature changes have an impact on CMM kinematic structure, which may cause the differences in the map of residual errors. As for today, the residual errors were analysed only for the reference temperature. No research was undertaken on the residual errors changes depending on the temperature variations. This paper presents the experiment aimed at residual <b>errors</b> <b>analysis</b> and resulting <b>errors</b> distributions for different temperatures...|$|R
40|$|Uncertainties in {{the initial}} {{conditions}} are {{a major source of}} errors in numerical weather forecasts. An iterative procedure that uses the adjoint version of the forecast model to minimize the short-term forecast error yields so-called key <b>analysis</b> <b>errors.</b> Under the assumption that the growth of forecast errors is mainly due to errors in the forecast initial state rather than model <b>errors,</b> the key <b>analysis</b> <b>errors</b> are expected to represent substantial parts of the <b>analysis</b> <b>error</b> that is responsible for a poor forecast. Furthermore, the identifcation of the <b>analysis</b> <b>error</b> can be achieved by monitoring the differences of observations and analysis fields (analysis departures). Airborne Doppler wind lidar observations over the Northern Atlantic THORPEX Regional Campaign (A-TReC) are analysed to test these considerations. The wind observations, taken with the DLR (Deutsches Zentrum für Luft- und Raumfahrt, Oberpfaffenhofen) 2 -µm Doppler wind lidar, were passively monitored and actively assimilated in experiments using the ECMWF global model to form the basis for the computation of analysis departures and analysis differences. The aim of this thesis is to gain a further understanding of the structure of key <b>analysis</b> <b>errors</b> and to investigate the question how well key <b>analysis</b> <b>errors</b> are related to <b>analysis</b> <b>errors.</b> The results confrm that analysis departures and key <b>analysis</b> <b>errors</b> optimized for both the Northern hemisphere and a targeted area are not correlated. Quantitative and qualitative comparisons show substantial differences of key <b>analysis</b> <b>errors</b> and <b>analysis</b> departures. It is also shown that similar discrepancies are found between key <b>analysis</b> <b>errors</b> and <b>analysis</b> differences...|$|R
40|$|Inaccurate initial {{conditions}} can produce significant failures in forecasts made with {{numerical weather prediction}} models. An iterative algorithm that uses the adjoint forecast model and is aimed at minimizing the forecast error leads to the so–called key <b>analysis</b> <b>errors.</b> Under the assumption that forecast error growth {{is dominated by the}} <b>analysis</b> <b>error,</b> key <b>analysis</b> <b>errors</b> should represent that part of the <b>analysis</b> <b>errors</b> mainly responsible for a poor forecast. Thus, the key <b>analysis</b> <b>errors</b> indicate how to improve analyses. In addition, analysis departures should also identify this direction. The {{purpose of this study is}} to gain a further understanding of the structure of key <b>analysis</b> <b>errors</b> and to investigate the question of how well key <b>analysis</b> <b>errors</b> are related to <b>analysis</b> <b>errors.</b> Airborne Doppler wind LIDAR measurements over the northern Atlantic collected during the Atlantic THORPEX Regional Campaign (A-Trec) are analysed to test these considerations. These wind observations were taken with the DLR (Deutsches Zentrum für Luft- und Raumfahrt, Oberpfaffenhofen) 2 μm Doppler wind LIDAR. They were passively and actively assimilated in ECMWF T 511 L 60 experiments to form the basis for the computation of analysis departures. Results indicate that analysis departures and key <b>analysis</b> <b>errors</b> optimized for both the northern hemisphere and a predefined forecast domain represent different parts of the <b>analysis</b> <b>error...</b>|$|R
25|$|However, the JRASC {{article has}} been criticized as lacking any <b>error</b> <b>analysis.</b> Since the {{triangulation}} base used by the astronomers in their calculations was very narrow, even very small errors in determination of directions {{could result in a}} very different triangulated trajectory. Measurement errors of slightly more than one-half degree would make possible a straight-line trajectory towards the Kecksburg area and a much shallower angle of descent than reported in the JRASC article. It was also pointed out that the photos used actually show the fireball trail becoming progressively thinner, suggesting motion away from the cameras, or in the direction of Pennsylvania. Had the trajectory been sideways to the cameras, as contended in the JRASC article, the trail would likely have remained roughly constant in thickness.|$|E
2500|$|... can {{be found}} by {{interval}} methods. This provides an alternative to traditional propagation of <b>error</b> <b>analysis.</b>|$|E
2500|$|... for {{the error}} term of that {{particular}} approximation. (Note that {{this is precisely the}} error we calculated for the example [...]) Using more derivatives, and by tweaking the quadrature, we can do a similar <b>error</b> <b>analysis</b> using a Taylor series (using a partial sum with remainder term) for f. This <b>error</b> <b>analysis</b> gives a strict upper bound on the error, if the derivatives of f are available.|$|E
50|$|The textual {{criticism}} of the New Testament is {{the analysis of the}} manuscripts of the New Testament, whose goals include identification of transcription <b>errors,</b> <b>analysis</b> of versions, and attempts to reconstruct the original.|$|R
30|$|To {{confirm the}} best {{model for the}} {{adsorption}} system, {{it is necessary to}} analyze the data using <b>errors</b> <b>analysis</b> like sum of the absolute errors (SAE) combined with the values of determined correlation coefficient r 2.|$|R
5000|$|In May 2015, {{the citizen}} journalism team Bellingcat wrote that <b>error</b> level <b>analysis</b> {{revealed}} that the Russian Ministry of Defense had edited satellite images related to the Malaysia Airlines Flight 17 disaster. In a reaction to this, image forensics expert J. Kriese said about error level analysis: [...] "The method is subjective and not based entirely on science", {{and that it is}} [...] "a method used by hobbyists". On his Hacker Factor Blog the inventor of <b>error</b> level <b>analysis</b> N. Krawetz criticized both Bellingcat's use of <b>error</b> level <b>analysis</b> as [...] "misinterpreting the results" [...] but also on several points J. Kriese's [...] "ignorance" [...] regarding <b>error</b> level <b>analysis.</b>|$|R
2500|$|We {{can convert}} this into an <b>error</b> <b>analysis</b> for the Riemann sum (*), giving an upper bound of ...|$|E
2500|$|Machine {{precision}} is {{a quantity}} {{that characterizes the}} accuracy of a floating-point system, and is used in backward <b>error</b> <b>analysis</b> of floating-point algorithms. [...] It {{is also known as}} unit roundoff or machine epsilon. [...] Usually denoted Εmach, its value depends on the particular rounding being used.|$|E
2500|$|Backward <b>error</b> <b>analysis,</b> {{the theory}} of which was {{developed}} and popularized by James H. Wilkinson, {{can be used to}} establish that an algorithm implementing a numerical function is numerically stable. The basic approach is to show that although the calculated result, due to roundoff errors, will not be exactly correct, it is the exact solution to a nearby problem with slightly perturbed input data. If the perturbation required is small, on the order of the uncertainty in the input data, then the results are in some sense as accurate as the data [...] "deserves". The algorithm is then defined as backward stable. Stability {{is a measure of the}} sensitivity to rounding errors of a given numerical procedure; [...] by contrast, the condition number of a function for a given problem indicates the inherent sensitivity of the function to small perturbations in its input and is independent of the implementation used to solve the problem.|$|E
40|$|Abbreviations: (ARD) {{absolute}} relative deviation, (BG) blood glucose, (CG-EGA) continuous glucose <b>error</b> grid <b>analysis,</b> (CGM) continuous glucose monitoring, (CLSI) Clinical and Laboratory Standards Institute, (EGA) <b>error</b> grid <b>analysis,</b> (MARD) mean absolute relative deviation, (PARD) precision absolute relative deviation, (SD) standard deviatio...|$|R
40|$|Work {{done on the}} NASA-Ames {{data base}} of digital flight records from airliners {{involved}} in severe turbulence incidences is summarized. The summary includes descriptions of the archived cases, data processing procedures, estimated <b>errors,</b> <b>analysis</b> procedures, and significant results to date. Thirteen severe turbulence cases are listed...|$|R
5000|$|Establishing Confidence in Digital Forensic Results by <b>Error</b> Mitigation <b>Analysis</b> ...|$|R
2500|$|Some {{details of}} the {{algorithm}} require careful thought. For many cases, estimating the error from quadrature over an interval for a function f(x) isn't obvious. One popular solution is to use two different rules of quadrature, and use their difference as {{an estimate of the}} error from quadrature. The other problem is deciding what [...] "too large" [...] or [...] "very small" [...] signify. A local criterion for [...] "too large" [...] is that the quadrature error should not be larger than tnbsp&middot&nbsp&h where t, a real number, is the tolerance we wish to set for global error. Then again, if h is already tiny, it may not be worthwhile to make it even smaller even if the quadrature error is apparently large. A global criterion is that the sum of errors on all the intervals should be less thannbsp&t. [...] This type of <b>error</b> <b>analysis</b> is usually called [...] "a posteriori" [...] since we compute the error after having computed the approximation.|$|E
2500|$|Miller {{worked on}} {{increasingly}} larger interferometers, culminating in {{one with a}} [...] (effective) arm length that he tried at various sites, including {{on top of a}} mountain at the Mount Wilson Observatory. To avoid the possibility of the aether wind being blocked by solid walls, his mountaintop observations used a special shed with thin walls, mainly of canvas. From noisy, irregular data, he consistently extracted a small positive signal that varied with each rotation of the device, with the sidereal day, and on a yearly basis. His measurements in the 1920s amounted to approximately [...] instead of the nearly [...] expected from the Earth's orbital motion alone. He remained convinced this was due to partial entrainment or aether dragging, though he did not attempt a detailed explanation. He ignored critiques demonstrating the inconsistency of his results and the refutation by the Hammar experiment. Miller's findings were considered important at the time, and were discussed by Michelson, Lorentz and others at a meeting reported in 1928. There was general agreement that more experimentation was needed to check Miller's results. Miller later built a non-magnetic device to eliminate magnetostriction, while Michelson built one of non-expanding Invar to eliminate any remaining thermal effects. Other experimenters from around the world increased accuracy, eliminated possible side effects, or both. So far, no one has been able to replicate Miller's results, and modern experimental accuracies have ruled them out. Roberts (2006) has pointed out that the primitive data reduction techniques used by Miller and other early experimenters, including Michelson and Morley, were capable of creating apparent periodic signals even when none existed in the actual data. After reanalyzing Miller's original data using modern techniques of quantitative <b>error</b> <b>analysis,</b> Roberts found Miller's apparent signals to be statistically insignificant.|$|E
2500|$|The Principal Triangulation of Great Britain was {{initiated}} by the Board of Ordnance in 1791 and carried out {{under the direction of}} William Mudge and Thomas Frederick Colby. The field work was completed in 1853, just as Clark joined the Board. The methods of analysis had been planned in outline by William Yolland, his predecessor {{at the head of the}} Trigonometric Section, but it fell to Clarke to finalize the methods and carry them through to completion. This he achieved in the four years from 1854 to 1858: the report was published as [...] but it is entirely Clarke's work. The basic data was the collection of angle bearings taken from each of the 289 stations towards a number of other stations, typically from three to ten in number. The multiple observations were first subjected to a least squares <b>error</b> <b>analysis</b> to extract the most likely angles and then the triangles formed by the corrected bearings were adjusted simultaneously, again by least squares methods, to find the most likely geometry for the whole mesh. This was an immense undertaking which involved the solution of 920 equations without the aid of matrix methods or digital computers. The only available computers were the living personnel of the Trigonometric Section, twenty one of them. Once the triangles had been fixed it was then possible to calculate all the sides of the mesh in terms of the length of either of the bases, one by Lough Foyle in Ireland and the other on Salisbury plain. The accuracy of the survey was such that when the length of the Lough Foyle base was calculated through the triangulation mesh from the Salisbury base the error was only 5 inches when compared with its measured length (of 41,640.887 feet or about 8 miles). The final step was to use the distances and angles to work out the latitude and longitude of each triangulation point on the Airy ellipsoid.|$|E
40|$|<b>Error</b> rate <b>analysis</b> {{applied to}} {{experimental}} data testing choice theories proves useful because {{it uses the}} entire distribution of observations - over predicted and unpredicted outcomes - to test theories. In this paper we apply <b>error</b> rate <b>analysis</b> to experimental data testing ash refinements. Evaluating theories using the proportion of consistent responses ignores systematic variation over inconsistent outcomes; a scale of inconsistency is necessary to evaluate refinements as some inconsistent observations are more supportive of a refinement than others. Previous conclusions about the accuracy of refinements reached using the proportion of consistent responses are reversed by the <b>error</b> rate <b>analysis...</b>|$|R
40|$|This paper {{presents}} {{a comparison of}} measurement results of polytetrafluoroethylene (PTFE) is loaded waveguide between Nicolson-Ross-Weir (NRW) method and Finite Element Method (FEM). The permittivity of PTFE values obtain from optimization technique and <b>errors</b> <b>analysis</b> have been conducted for scattering parameters in calculation and simulation results, variation mean relative errors with thickness of sample have been found and plotted...|$|R
40|$|This paper {{presents}} an online algorithm for dependency parsing problems. We propose {{an adaptation of}} the passive and aggressive online learning algorithm to the dependency parsing domain. We evaluate the proposed algorithms on the 2007 CONLL Shared Task, and report <b>errors</b> <b>analysis.</b> Experimental {{results show that the}} system score is better than the average score among the participating systems. ...|$|R
50|$|In {{numerical}} analysis, <b>error</b> <b>analysis</b> comprises both forward <b>error</b> <b>analysis</b> {{and backward}} <b>error</b> <b>analysis.</b>|$|E
5000|$|<b>Error</b> <b>analysis</b> in SLA was {{established}} in the 1960s by Corder and colleagues. [...] <b>Error</b> <b>analysis</b> (EA) was an alternative to contrastive analysis, an approach influenced by behaviorism through which applied linguists sought to use the formal distinctions between the learners' first and second languages to predict errors. <b>Error</b> <b>analysis</b> showed that contrastive analysis was unable to predict a great majority of errors, although its more valuable aspects have been incorporated into the study of language transfer. A key finding of <b>error</b> <b>analysis</b> has been that many learner errors are produced by learners making faulty inferences about the rules of the new language.|$|E
5000|$|... #Subtitle level 3: Machine {{precision}} and backward <b>error</b> <b>analysis</b> ...|$|E
40|$|We {{present a}} {{quantitative}} approach for identifying software modules and signals {{which will not}} be able to contain data errors that may be present in a software system, thus rendering the system non-dependable. Based on <b>error</b> propagation <b>analysis</b> in combination with <b>error</b> effect <b>analysis</b> we discuss how the results can be used to identify a) modules/signals which have a high “ablity ” to let propagating errors pass through them on their way through the system, and b) modules/signals which, when be subjected to errors, have a severe negative effect on the results produced by the system. This knowledge is very useful for directing and allocating resources for increased software reliability. Both the <b>error</b> propagation <b>analysis</b> and the <b>error</b> effect <b>analysis</b> are based on the Error Permeability measure. Using this measure we define a range of subsequent measures which allow us to quantify error propagation as well as error effect. 1...|$|R
25|$|Inadequate {{systems to}} share {{information}} about <b>errors</b> hamper <b>analysis</b> of contributory causes and improvement strategies.|$|R
40|$|This paper {{describes}} {{the importance of}} the incorporation of ergonomics, human factor, <b>errors</b> <b>analysis</b> and cognitive engi-neering approaches in the design of human–robot systems, how consideration of these subjects help designers and workers to avoid hazardous situations and make human–robot interaction in vicinity more effective, reliable and safe. Basing on acquired knowledge and guiding by acquired knowledg e we propose our trial application for disassembly cell...|$|R
