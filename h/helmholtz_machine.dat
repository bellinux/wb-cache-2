32|11|Public
5000|$|<b>Helmholtz</b> <b>machine,</b> a {{neural network}} model trained by the wake-sleep algorithm.|$|E
5000|$|The <b>Helmholtz</b> <b>machine</b> {{is a type}} of {{artificial}} neural network that can account for the hidden structure of a set of data by being trained to create a generative model of the original set of data. The hope is that by learning economical representations of the data, the underlying structure of the generative model should reasonably approximate the hidden structure of the data set. A <b>Helmholtz</b> <b>machine</b> contains two networks, a bottom-up recognition network that takes the data as input and produces a distribution over hidden variables, and a top-down [...] "generative" [...] network that generates values of the hidden variables and the data itself.|$|E
5000|$|The wake-sleep {{algorithm}} is an unsupervised learning algorithm for a stochastic multilayer neural network. The algorithm adjusts the parameters {{so as to}} produce a good density estimator. There are two learning phases, the “wake” phase and the “sleep” phase, which are performed alternately. It was first designed {{as a model for}} brain functioning using variational Bayesian learning. After that, the algorithm was adapted to machine learning. It {{can be viewed as a}} way to train a <b>Helmholtz</b> <b>Machine</b> ...|$|E
50|$|<b>Helmholtz</b> <b>machines</b> {{are usually}} trained using an {{unsupervised}} learning algorithm, {{such as the}} wake-sleep algorithm.|$|R
50|$|<b>Helmholtz</b> <b>machines</b> {{may also}} be used in {{applications}} requiring a supervised learning algorithm (e.g. character recognition, or position-invariant recognition of an object within a field).|$|R
40|$|Abstract. This paper {{presents}} {{an approach to}} the topic extraction from text documents using probabilistic graphical models. Multiple-cause networks with latent variables are used and the <b>Helmholtz</b> <b>machines</b> are utilized to ease the learning and inference. The learning in this model is conducted in a purely data-driven way and does not require prespecified categories of the given documents. Topic words extraction experiments on the TDT- 2 collection are presented. Especially, document clustering results on a subset of TREC- 8 ad-hoc task data show the substantial reduction of the inference time without significant deterioration of performance. ...|$|R
50|$|This {{idea was}} taken up in {{research}} on unsupervised learning, in particular the Analysis by Synthesis approach, branches of machine learning. In 1983 Geoffrey Hinton and colleagues proposed the brain {{could be seen as}} a machine making decisions based on the uncertainties of the outside world. During the 1990s researchers including Peter Dayan, Geoffrey Hinton and Richard Zemel proposed that the brain represents knowledge of {{the world in terms of}} probabilities and made specific proposals for tractable neural processes that could manifest such a <b>Helmholtz</b> <b>Machine.</b>|$|E
40|$|In {{this letter}} {{we present a}} coupled <b>Helmholtz</b> <b>machine</b> for {{principal}} component analysis (PCA), where sub-machines are related through sharing some latent variables and associated weights. Then, we present a wake-sleep PCA algorithm for training the coupled <b>Helmholtz</b> <b>machine,</b> showing that the algorithm iteratively determines principal eigenvectors of a data covariance matrix without any rotational ambiguity, in contrast to some existing methods that performs factor analysis or principal subspace analysis. The coupled <b>Helmholtz</b> <b>machine</b> provides a unified view of principal component analysis, including various existing algorithms as its special cases. The validity of the wake-sleep PCA algorithm is confirmed by numerical experiments...|$|E
40|$|We {{present an}} {{approach}} for text analysis, especially for topic words extraction and document classification, {{based on a}} probabilistic generative model. Generative models are useful since they can extract the underlying causal structure of data objects. For this model, a stochastic <b>Helmholtz</b> <b>machine</b> is used and it is fitted using the wake-sleep algorithm, a simple stochastic leaming algorithm. Given a document set, the <b>Helmholtz</b> <b>machine</b> tries to capture the correlation of the words used in the set, thus can extract various semantic features {{for a set of}} documents. We present some experimental results on topic words extraction for TDT- 2 and TREC- 8 ad-hoc data sets. And for another real-world document set, 20 Newsgroup collection, a categorization is performed and the performance is compared with that of naive Bayes classifier, another simple generative model. Additionally, we present a preliminary work to make Helmholtz machines more appropriate for processing text documents...|$|E
40|$|Many recent analysis-by-synthesis density {{estimation}} {{models of}} cortical learning and processing {{have made the}} crucial simplifying assumption that units within a single layer are mutually independent given the states of units in the layer below or the layer above. In this article, we suggest using either a Markov random field or an alternative stochastic sampling architecture to capture explicitly particular forms of dependence within each layer. We develop the architectures {{in the context of}} real and binary <b>Helmholtz</b> <b>machines.</b> Recurrent sampling can be used to capture correlations within layers in the generative or the recognition models, and w...|$|R
40|$|The {{nature of}} Bayesian Ying [...] Yang harmony {{learning}} is reexamined from an information theoretic perspective. Not only its ability for model selection and regularization is explained with new insights, but also discussions {{are made on}} its relations and differences from the studies of minimum description length (MDL), Bayesian approach, the bit-back based MDL, Akaike information criterion (AIC), maximum likelihood, information geometry, <b>Helmholtz</b> <b>machines,</b> and variational approximation. Moreover, a generalized projection geometry is introduced for further understanding such a new mechanism. Furthermore, new algorithms are also developed for implementing Gaussian factor analysis (FA) and non-Gaussian factor analysis (NFA) such that selecting appropriate factors is automatically made during parameter learning...|$|R
40|$|Efforts {{toward a}} key {{challenge}} of statistical learning, namely making learning on a finite size of samples with model selection ability, {{have been discussed}} in two typical streams. Bayesian Ying Yang (BYY) harmony learning provides a promising tool for solving this key challenge, with new mechanisms for model selection and regularization. Moreover, not only the BYY harmony learning is further justified from both an information theoretic perspective and a generalized projection geometry, but also comparative discussions are made on its relations and differences from the studies of minimum description length (MDL), the bit-back based MDL, Bayesian approach, maximum likelihood, information geometry, <b>Helmholtz</b> <b>machines,</b> and variational approximation. In addition, bibliographic remarks are made on the advances of BYY harmony learning studies...|$|R
40|$|An {{increasing}} {{amount of}} behavioral and neurophysiological data {{suggests that the}} brain performs optimal (or near-optimal) probabilistic inference and learning during perception and other tasks. Although many machine learning algorithms exist that perform inference and learning in an optimal way, the complete description of how one of those algorithms (or a novel algorithm) can be implemented in the brain is currently incomplete. There have been many proposed solutions that address how neurons can perform optimal inference {{but the question of}} how synaptic plasticity can implement optimal learning is rarely addressed. This paper aims to unify the two fields of probabilistic inference and synaptic plasticity by using a neuronal network of realistic model spiking neurons to implement a well studied computational model called the <b>Helmholtz</b> <b>Machine.</b> The <b>Helmholtz</b> <b>Machine</b> is amenable to neural implementation as the algorithm it uses to learn its parameters, called the wake-sleep algorithm, uses a local delta learning rule. Our spiking-neuron network implements both the delta rule and a small example of a <b>Helmholtz</b> <b>machine.</b> This neuronal network can learn an internal model of continuous-valued training data sets without supervision. The network can also perform inference on the learned internal models. We show how various biophysical features of the neural implementation constrain the parameters of the wake-sleep algorithm, such as the duration of the wake and sleep phases of learning and the minimal sample duration. We examine the deviations from optimal performance and tie them to the properties of the synaptic plasticity rule...|$|E
40|$|Probabilistic {{computing}} forms {{a relatively}} new computational style, of significant practical interest because stochastic behaviour is common and {{must be taken into}} accountin in biological and other real-world processes. We examine a particular stochastic ANN architecture, the <b>Helmholtz</b> <b>Machine,</b> investigating its characteristics, with particular respect to its wake-sleep training algorithm, and showing how its representational power might be increased. We also explain how we are implementing a simple version of the machine in analogue VLSI hardware...|$|E
40|$|The <b>Helmholtz</b> <b>machine</b> {{is a new}} {{unsupervised}} learning architecture that uses topdown and bottom up connections to build probability density models of input and inverses to those models. The wake-sleep learning algorithm for Helmholtz machines involves just the purely local delta rule. This paper suggests {{a number of different}} varieties of Helmholtz machines, each with its own strengths and weaknesses, and draws conclusions from the hypothesis that cortical information processing can be viewed in their terms. ...|$|E
5000|$|Hinton's {{research}} investigates ways {{of using}} neural networks for machine learning, memory, perception and symbol processing. He has authored or co-authored over 200 {{peer reviewed publications}} in these areas. He {{was one of the}} first researchers who demonstrated the use of generalized back-propagation algorithm for training multi-layer neural networks that has been widely used for practical applications. He co-invented Boltzmann machines with David Ackley and Terry Sejnowski His other contributions to neural network research include distributed representations, time delay neural network, mixtures of experts, <b>Helmholtz</b> <b>machines</b> and Product of Experts. In 2007 Hinton coauthored an unsupervised learning paper titled [...] "Unsupervised learning of image transformations". An accessible introduction to Geoffrey Hinton's research can be found in his articles in Scientific American in September 1992 and October 1993.|$|R
50|$|George Frederick Chapline, Jr. (born May 6, 1942) is an American {{theoretical}} physicist, {{based at}} the Lawrence Livermore National Laboratory. His most recent interests have mainly been in quantum information theory, condensed matter, and quantum gravity. In 2003 he received the Computing Anticipatory Systems award for a new interpretation of quantum mechanics based on the similarity of quantum mechanics and <b>Helmholtz</b> <b>machines.</b> He was awarded the E. O. Lawrence Award in 1982 by the United States Department of Energy for leading the team that first demonstrated a working X-ray laser (see photo). In the field of condensed matter physics Chapline {{is best known as}} the originator of the concept of a gossamer metal; i.e. a metal where the density of states at the Fermi surface is depressed because of pairing correlations. Both the actinides and high Tc superconductors are examples of gossamer metals.|$|R
40|$|Real-valued random hidden {{variables}} can {{be useful}} for modelling latent structure that explains correlations among observed variables. I propose a simple unit that adds zero-mean Gaussian noise to its input before passing it through a sigmoidal squashing function. Such units can produce a variety of useful behaviors, ranging from deterministic to binary stochastic to continuous stochastic. I show how "slice sampling" {{can be used for}} inference and learning in top-down networks of these units and demonstrate learning on two simple problems. 1 Introduction A variety of unsupervised connectionist models containing discrete-valued hidden units have been developed. These include Boltzmann machines (Hinton and Sejnowski 1986), binary sigmoidal belief networks (Neal 1992) and <b>Helmholtz</b> <b>machines</b> (Hinton et al. 1995; Dayan et al. 1995). However, some hidden variables, such as translation or scaling in images of shapes, are best represented using continuous values. Continuous-valued Bolt [...] ...|$|R
40|$|Abstract. In {{this paper}} we {{describe}} two models for neural grounding of robotic language processing in actions. These models {{are inspired by}} concepts of the mirror neuron system {{in order to produce}} learning by imitation by combining high-level vision, language and motor command inputs. The models learn to perform and recognise three behaviours, ‘go’, ‘pick ’ and ‘lift’. The first single-layer model uses an adapted <b>Helmholtz</b> <b>machine</b> wake-sleep algorithm to act like a Kohonen self-organising network that receives all inputs into a single layer. In contrast, the second, hierarchical model has two layers. In the lower level hidden layer the <b>Helmholtz</b> <b>machine</b> wake-sleep algorithm is used to learn the relationship between action and vision, while the upper layer uses the Kohonen self-organising approach to combine the output of the lower hidden layer and the language input. On the hidden layer of the single-layer model, the action words are represented on non-overlapping regions and any neuron in each region accounts for a corresponding sensory-motor binding. In the hierarchical model rather separate sensory- and motor representations on the lower level are bound to corresponding sensory-motor pairings via the top level that organises according to the language input. ...|$|E
40|$|We {{present a}} {{formulation}} of mean-field approximation for layered feed-forward stochastic networks. In this formulation, one can obtain not only estimates of averages for state variables {{of the networks}} but also those of intra-layer correlations, the latter of which cannot be obtained by the conventional mean-field approximation. Moreover, this formulation provides a framework to treat "conditional" expectations, expectations under the constraint that external information about statistics are fed to some layers of the network, which {{plays an important role}} in several applications such as the <b>Helmholtz</b> <b>machine...</b>|$|E
40|$|This paper {{addresses}} an algorithm {{for independent}} component {{analysis on the}} basis of <b>Helmholtz</b> <b>machine,</b> which is an unsupervised learning machine. The algorithm is constructed by two terms. One is a term for obtaining a desired demixing process, which is the conventional rule on the base of the information theory. The other is a term for evaluating whether the inverse system and the set of obtained components are desirable or not. Due to this term, the algorithm effectively works in blind separation of overdetermined mixture with additive noise. We demonstrate the effectiveness by computer simulation...|$|E
40|$|Training deep {{directed}} graphical {{models with}} many hidden variables and perform-ing inference remains a major challenge. <b>Helmholtz</b> <b>machines</b> and deep belief networks are such models, and the wake-sleep algorithm {{has been proposed}} to train them. The wake-sleep algorithm relies on training not just the directed gen-erative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of la-tent given visible. We propose a novel interpretation of the wake-sleep algorithm which suggests that better estimators of the gradient {{can be obtained by}} sampling latent variables multiple times from the inference network. This view is based on importance sampling as an estimator of the likelihood, with the approximate inference network as a proposal distribution. This interpretation is confirmed ex-perimentally, showing that better likelihood can be achieved with this reweighted wake-sleep procedure, which also provides a natural way to estimate the likeli-hood itself. Based on this interpretation, we propose that a sigmoid belief network is not sufficiently powerful for the layers of the inference network, in order to recover a good estimator of the posterior distribution of latent variables. Our ex-periments show that using a more powerful layer model, such as NADE, yields substantially better generative models. ...|$|R
40|$|Inferences from {{time-series data}} can be greatly {{enhanced}} by taking into account multiple modalities. In some cases, such as audio of speech and the corresponding video of lip gestures, the different time-series are tightly coupled. We are interested in loosely-coupled time series where only the onset of events are coupled in time. We present {{an extension of the}} forward-backward algorithm {{that can be used for}} inference and learning in event-coupled hidden Markov models and give results on a simplified multi-media indexing task where the objective is to detect an event whose onset is loosely coupled in audio and video. 1. FOREGROUND The combination of multiple modalities for inference has proven to be a very powerful way to increase detection and recognition performance (Yuhas et al. 1988; Becker and Hinton 1992; Bregler et al. 1994; de Sa and Ballard 1998). By combining the soft information provided by models of the different modalities, weakly incorrect evidence in one modality often can be corrected by another modality. This “diversity effect ” lies at the heart of many powerful algorithms in a variety of areas, including the best error-correcting decoding algorithm (see Frey 1998 for a review). In general, the state spaces representing the different modalities may be richly interdependent, making exact inference intractable. Methods for approximate inference include Markov chain Monte Carlo (Neal 1992), recognition models in <b>Helmholtz</b> <b>machines</b> (Dayan et al. 1995; Hinton et al. 1995), variational techniques (Jordan et al. 1998) and iterated inferences based on local independence assumptions (Frey and MacKay 1998). In time-series data, each modality is often modeled with a hidden Markov model (HMM) and the states of the different HMMs are coupled in time. If the coupling is highly synchronized in time, then the modalities can be modeled using a single HMM with a number of states equal to the product of the numbers of states in the separate HMMs. I...|$|R
40|$|The W-S (Wake-Sleep) {{algorithm}} {{is a simple}} learning rule for the models with hidden variables. It is shown that this algorithm {{can be applied to}} a factor analysis model which is a linear version of the <b>Helmholtz</b> <b>machine.</b> But even for a factor analysis model, the general convergence is not proved theoretically. In this article, we describe the geometrical understanding of the W-S algorithm in contrast with the EM (ExpectationMaximization) algorithm and the em algorithm. As the result, we prove the convergence of the W-S algorithm for the factor analysis model. We also show the condition for the convergence in general models. ...|$|E
40|$|This paper {{suggests}} that probabilistic VLSI architectures may provide insights into naturally-stochastic processes, and describes a particular application to perform robust data fusion, using unsupervised feature extraction and compensation of sensor drift. Two very interesting, stochastic networks are (1) the <b>Helmholtz</b> <b>machine,</b> {{which uses a}} local learning rule and whose hidden units choose states according to a probability distribution, rather than deterministically; and (2) the Product-of-Experts (PoE) machine, which uses the same computational elements, but has a training algorithm that is both more reliable and more amenable to on-chip implementation. Stochasticity can be provided in hardware by sampling an oscillator's binary output that varies its mark-to-period ratio, provided that the outputs {{of a set of}} oscillators remains uncorrelated...|$|E
40|$|Recently, several {{evolutionary}} algorithms {{have been}} proposed that build and use an explicit distribution model of the population to perform optimization. One of the main issues in this class of algorithms is how to estimate the distribution of selected samples. In this paper, we present a Bayesian evolutionary algorithm (BEA) that learns the sample distribution by a probabilistic graphical model known as Helmholtz machines. Due to the generative nature and availability of the wake-sleep learning algorithm, the Helmholtz machines provide an e ective tool for modeling and sampling from the distribution of selected individuals. The proposed method {{has been applied to}} a suite of GA-deceptive functions. Experimental results show that the BEA with the <b>Helmholtz</b> <b>machine</b> outperforms the simple genetic algorithm...|$|E
40|$|A b s t r a c t. Recently, several {{evolutionary}} algorithms {{have been}} proposed that build and use an explicit distribution model of the population to perform optimization. One of the main issues in this class of algorithms is how to estimate the distribution of selected samples. In this paper, we present a Bayesian evolutionary algorithm (BEA) that learns the sample distribution by a probabilistic graphical model known as Helmholtz ma-chines. Due to the generative nature and availability of the wake-sleep learning algorithm, the Helmholtz machines provide an effective tool for modeling and sampling from the distribution of selected individuals. The proposed method {{has been applied to}} a suite of GA-deceptive functions. Experimental results show that the BEA with the <b>Helmholtz</b> <b>machine</b> outperforms the simple genetic algorithm. ...|$|E
40|$|In {{this paper}} we propose an {{approach}} to robot learning by imitation that uses the multimodal inputs of language, vision and motor. In our approach a student robot learns from a teacher robot how to perform three separate behaviours based on these inputs. We considered two neural architectures for performing this robot learning. First, a one-step hierarchial architecture trained with two different learning approaches either based on Kohonen's self-organising map or based on the <b>Helmholtz</b> <b>machine</b> {{turns out to be}} inefficient or not capable of performing differentiated behavior. In response we produced a hierarchial architecture that combines both learning approaches to overcome these problems. In doing so the proposed robot system models specific aspects of learning using concepts of the mirror neuron system (Rizzolatti and Arbib, 1998) with regards to demonstration learning...|$|E
40|$|A unified {{statistical}} learning approach called Bayesian YingYang (BYY) {{system and}} {{theory has been}} developed by the present author in recent years. This paper is {{the first part of}} a recent effort on systematically summarizing this theory. In this paper, we show how the theory functions as a general theory for unsupervised learning and its semi-unsupervised extension on parameter learning, regularization, structural scale or complexity selection, architecture design and data sampling. Specifically, it is shown how the general theory provides new theories for unsupervised pattern recognition and clustering analysis, factorial encoding, data dimension reduction, and independent component analysis, such that not only several existing popular unsupervised learning approaches, (e. g., finite mixture with the EM algorithm, K-means clustering algorithm, <b>Helmholtz</b> <b>machine,</b> principal component analysis plus various extensions, Informax and minimum mutual information approaches for independent co [...] ...|$|E
40|$|Most of {{estimation}} of distribution algorithms (EDAs) try to represent explicitly {{the relationship between}} variables with factorization techniques or with graphical models such as Bayesian networks. In this paper, we propose to use latent variable models such as <b>Helmholtz</b> <b>machine</b> and probabilistic principal component analysis for capturing the probabilistic distribution of given data. The latent variable models are statistical models that specify the relationships between a set of random variables {{and a set of}} latent variables; Latent variables are not directly observable and there are a smaller number of variables in latent variables than in input variables. In statistics latent variable models are used for density estimation. Since latent variable models are generative models, it is easy to sample a new data. Our experimental results support that the proposed latent variable models can find good solutions more efficiently than other EDAs for continuous functions. ...|$|E
40|$|This thesis {{presents}} a novel hardware {{implementation of a}} binary-state, probabilistic artificial neuron using the pulse-stream analogue integrated circuit design methodology. The artificial neural network architecture targeted for implementation is the <b>Helmholtz</b> <b>Machine,</b> an auto-encoder trained by the unsupervised Wake-Sleep algorithm. A dual-layer network was implemented on one of two integrated circuit prototypes, intended for hardware-software comparative experiments in unsupervised probabilistic neural computation. Circuit modules were designed to perform the synaptic multiplication and integration functions, the sigmoid activation function, and to generate probabilistic output. All circuit design was modular and scaleable, with particular attention given to silicon area utilization and power consumption. The neuron outputs the calculated probability as a mark-to-period modulated stream of pulses, which is then randomly sampled to determine the next state for the neuron. Implementation issues are discussed, such as a tendency for the probabilistic oscillators inside each neuron to phase-lock or become unstable at highe...|$|E
40|$|This work {{argues that}} it should be {{possible}} to combine pulse-based VLSI techniques with the relatively simple training rules of the <b>Helmholtz</b> <b>Machine</b> stochastic neural architecture, in order to build an analogue probabilistic hardware model of the latter. An overview of the necessary components is presented, as well as a design for a pulsewidth modulation oscillator, capable of transforming a current input (which represents the squashed, post-synaptic signal processed by a particular neuron) into the probability associated with the binary state of that neuron. A CMOS hardware prototype has been designed and fabricated, and precautions were taken during the design and simulation stages in order to prevent the oscillators on the same chip from locking together. Apart from testing the hardware prototype, future plans involve the hardware implementation of other modules, such as the synapse, the squashing function and weight changing circuitry. 1. Stochastic Computing The issue of computa [...] ...|$|E
40|$|The "Neural Heat Exchanger" is an alternative, {{supervised}} learning method for multi-layer neural nets. It {{is inspired by}} the physical heat exchanger. Unlike backprop, it is entirely local. This makes its parallel implementation trivial. It was first presented during occasional talks since 1990, and {{is closely related to}} Hinton et. al. 's recent <b>Helmholtz</b> <b>Machine</b> (1995). For the first time, this paper presents the basic ideas in written form. To fully understand the Neural Heat Exchanger's advantages and limitations, however, much theoretical and empirical work remains to be done. 1 Introduction Most conventional supervised algorithms for multi-layer neural nets are not local in space and time. Backprop, for instance, requires a global control mechanism that first propagates activation signals through all successive layers, then waits until the error signals come back, then changes the weights. Many suspect, however, that the brain does use an entirely local algorithm. One advantage of [...] ...|$|E
40|$|Efficient {{unsupervised}} {{training and}} inference in deep generative models remains a challenging problem. One basic approach, called <b>Helmholtz</b> <b>machine,</b> involves training a top-down directed generative model {{together with a}} bottom-up auxiliary model used for approximate inference. Recent results indicate that better generative models can be obtained with better approximate inference procedures. Instead of improving the inference procedure, we here propose a new model which guarantees that the top-down and bottom-up distributions can efficiently invert each other. We achieve this by interpreting both the top-down and the bottom-up directed models as approximate inference distributions and by defining the model distribution to be the geometric mean of these two. We present a lower-bound for the likelihood of this model and we show that optimizing this bound regularizes the model so that the Bhattacharyya distance between the bottom-up and top-down approximate distributions is minimized. This approach results in {{state of the art}} generative models which prefer significantly deeper architectures while it allows for orders of magnitude more efficient approximate inference...|$|E
40|$|This article {{explores the}} notion that Freudian {{constructs}} may have neurobiological substrates. Specifically, we propose that Freud’s descriptions of the primary and secondary processes are consistent with self-organized activity in hierarchical cortical systems and that his descriptions of the ego {{are consistent with the}} functions of the default-mode and its reciprocal exchanges with subordinate brain systems. This neurobiological account rests on a view of the brain as a hierarchical inference or <b>Helmholtz</b> <b>machine.</b> In this view, large-scale intrinsic networks occupy supraordinate levels of hierarchical brain systems that try to optimize their representation of the sensorium. This optimization has been formulated as minimizing a free-energy; a process that is formally similar to the treatment of energy in Freudian formulations. We substantiate this synthesis by showing that Freud’s descriptions of the primary process are consistent with the phenomenology and neurophysiology of rapid eye movement sleep, the early and acute psychotic state, the aura of temporal lobe epilepsy and hallucinogenic drug states...|$|E
40|$|The <b>Helmholtz</b> <b>machine</b> {{is a new}} {{unsupervised}} learning architecture that uses topdown connections to build probability density models of input and and bottom up connections to build inverses to those models. The wake-sleep learning algorithm for the machine involves just the purely local delta rule. This paper suggests {{a number of different}} varieties of Helmholtz machines, each with its own strengths and weaknesses, and relates them to cortical information processing. 1 Introduction This special issue focuses on four important questions about neural computation: 1) How is the world represented in the firing of neurons? 2) What are the functional roles of bottom-up and top-down connections between cortical areas? 3) Does the brain use internal models of the external world? 4) What are the basic synaptic plasticity rules? We believe that answers to these questions are indicated by the new theory of {{unsupervised learning}} that we present here. Supervised learning algorithms for neural netwo [...] ...|$|E
