41|18|Public
50|$|An <b>{{incremental}}</b> <b>compiler</b> {{is a kind}} of incremental computation {{applied to}} the field of compilation. Quite naturally, whereas ordinary compilers make so called clean build, that is, (re)build all program modules, <b>incremental</b> <b>compiler</b> recompiles only those portions of a program that have been modified.|$|E
5000|$|The IBM VisualAge C++ {{compiler}} 4.0 is an <b>incremental</b> <b>compiler</b> for C++ ...|$|E
5000|$|Eclipse Compiler for Java (ECJ), an {{open source}} <b>incremental</b> <b>compiler</b> {{used by the}} Eclipse project.|$|E
5000|$|<b>Incremental</b> <b>compilers</b> in {{interactive}} programming {{environments and}} runtime systems: ...|$|R
40|$|Poplog is a {{portable}} interactive AI development environment {{available on a}} of op""ating systems and machines. It includes <b>incremental</b> <b>compilers</b> "ur}g" for bommon Lisp, Pop-ll, Prolog and Standard ML, along with tools for adding new <b>incremental</b> <b>compilers.</b> All the languages share a common development environment and data structures can be shared between programs written in the different languages. The power and portability of iroptog depend on its two virtual machines, a high level virtual machine tpWf-tne Poplog Virtual Machine) serving as a target for compilers for interactive languages and a low level virtual machine (PIM-the Poplog Implementation Machine) {{as a base for}} translation to machine code. A machine-independent and language-independent code generator translates from the PVM to thePIM, enormously simplifyingboththe taskofproducing a new compiler and porting to new machines. 1...|$|R
40|$|Chez Scheme is {{now over}} 20 years old, {{the first version}} having been {{released}} in 1985. This paper takes a brief {{look back on the}} history of Chez Scheme’s development to explore how and why it became the system it is today. Categories and Subject Descriptors D. 3. 4 [Programming languages]: Processors—compilers, <b>incremental</b> <b>compilers,</b> optimization, interpreters, memory management (garbage collection), runtime environments; D. 3. 2 [Programming languages]: Language classifications—applicative (functional) languages, Schem...|$|R
50|$|One {{downside}} {{to this type}} of <b>incremental</b> <b>compiler</b> is that it cannot easily optimize the code that it compiles, due to locality and the limited scope of what is changed. This is usually not a problem, because for optimization is usually only carried out on release, an <b>incremental</b> <b>compiler</b> would be used throughout development, and a standard batch compiler would be used on release.|$|E
50|$|In {{imperative}} {{programming and}} software development, an <b>incremental</b> <b>compiler</b> {{is one that}} when invoked, takes only the changes of a known set of source files and updates any corresponding output files (in the compiler's target language, often bytecode) that may already exist from previous compilations.By effectively building upon previously compiled output files, the <b>incremental</b> <b>compiler</b> avoids the wasteful recompiling of entire source files, {{where most of the}} code remains unchanged. For most incremental compilers, compiling a program with small changes to its source code is usually near instantaneous.It can be said that an <b>incremental</b> <b>compiler</b> reduces the granularity of a language's traditional compiling units while maintaining the language's semantics, such that the compiler can append and replace smaller parts.|$|E
50|$|The PECAN Programming Environment Generator was an <b>incremental</b> <b>compiler,</b> {{developed}} by Steven P. Reiss {{in the early}} 1980s.|$|E
40|$|Dynamically typed {{language}} implementations often use more memory and execute slower than their statically typed cousins, {{in part because}} operations on collections of elements are unoptimised. This paper describes storage strategies, which dynamically optimise collections whose elements are instances of the same primitive type. We implement storage strategies in the PyPy virtual machine, giving a performance increase of 18 % on wide-ranging benchmarks of real Python programs. We show that storage strategies are simple to imple-ment, needing only 1500 LoC in PyPy, and have applicability {{to a wide range}} of virtual machines. Categories and Subject Descriptors D. 3. 4 [Programming Languages]: Processors—run-time environments, code gen-eration, <b>incremental</b> <b>compilers,</b> interpreter...|$|R
50|$|Common Lisp {{implementations}} may use any mix {{of native}} code compilation, byte code compilation or interpretation. Common Lisp {{has been designed}} to support <b>incremental</b> <b>compilers,</b> file compilers and block compilers. Standard declarations to optimize compilation (such as function inlining or type specialization) are proposed in the language specification. Most Common Lisp implementations compile source code to native machine code. Some implementations can create (optimized) stand-alone applications. Others compile to interpreted bytecode, which is less efficient than native code, but eases binary-code portability. There are also compilers that compile Common Lisp code to C code. The misconception that Lisp is a purely interpreted language is most likely because Lisp environments provide an interactive prompt and that code is compiled one-by-one, in an incremental way. With Common Lisp incremental compilation is widely used.|$|R
40|$|The work {{reported}} here {{is part of}} the IPSEN 3 project whose very goal is the development of an Integrated Project Support ENvironment. Within this project directed, attributed, node- and edge- labeled graphs (diane graphs) are used to model the internal structure of software documents and PROgrammed Graph REwriting SyStems are used to specify the operational behavior of document processing tools like syntax-directed editors, static analyzers, or <b>incremental</b> <b>compilers</b> and interpreters. Recently a very high-level language, named PROGRESS, has been developed to support these activities. This language offers its users a convenient, partly textual, partly graphical concrete syntax and a rich system of consistency checking rules (mainly type compatibility rules) for the underlying calculus of programmed diane-graph rewriting systems. This paper presents a partly imperative, partly rule-oriented sublanguage of PROGRESS for composing complex graph queries and graph transformations (transa [...] ...|$|R
5000|$|The Eclipse {{platform}} has a Java <b>incremental</b> <b>compiler</b> {{included as}} a part of the Java Development Tools project ...|$|E
5000|$|GNU Compiler Collection has {{branched}} off {{its development}} with the IncrementalCompiler project, concentrating in providing C/C++ with a fast <b>incremental</b> <b>compiler</b> ...|$|E
5000|$|IF (Interactive FORTRAN, an <b>incremental</b> <b>compiler</b> and {{environment}} for executing and debugging FORTRAN programs, {{developed at the}} University of British Columbia) ...|$|E
40|$|One of {{the nice}} {{properties}} of a tracing just-in-time compiler (JIT) {{is that many of}} its optimizations are simple, requiring one forward pass only. This is not true for loop-invariant code motion which is a very important optimization for code with tight kernels. Especially for dynamic languages that typically perform quite a lot of loop invariant type checking, boxed value unwrapping and virtual method lookups. In this paper we explain a scheme pioneered {{within the context of the}} LuaJIT project for making basic optimizations loop-aware by using a simple pre-processing step on the trace without changing the optimizations themselves. We have implemented the scheme in RPython’s tracing JIT compiler. PyPy’s Python JIT executing simple numerical kernels can become up to two times faster, bringing the performance into the ballpark of static language compilers. Categories and Subject Descriptors D. 3. 4 [Programming Languages]: Processors—code generation, <b>incremental</b> <b>compilers,</b> interpreters, run-time environment...|$|R
40|$|Abstract. We use the {{fundamental}} object-oriented notion of record and enhance it with meta-information {{at various levels}} (the record level and the attribute/method level) {{in order to build}} behavioural models as extensions or generalizations of object-oriented programming. With these ingredients as building blocks, we are able to produce notions such as inheritance of any kind by handling meta-information with the use of algorithms. We are particularly interested in the dynamic aspects of the framework which are defined at two distinct levels: the dynamic definition of behavioural models through meta-information and their dynamic execution by using suitable pluggable algorithms. Our primary goal is to arrive at a unified framework for the definition and implementation of object-oriented behavioural models. Our on-going research should lead to — among other things — the study of models of dynamic behaviour, the design of <b>incremental</b> <b>compilers</b> and the exploitation of the “growing ” language concept. ...|$|R
40|$|The {{emerging}} {{category of}} self-service enterprise applications motivates support for “live programming” {{in the database}}, where the user’s iterative data exploration triggers changes to installed application code and its output in real time. This paper discusses the technical challenges in supporting live programming in the database and presents the solution implemented in the LogicBlox commercial system. The workhorse architectural component is a “meta-engine” that incrementally maintains metadata representing application code, guides its compilation into an internal representation in the database kernel, and orchestrates maintenance of materialized views based on those changes. Our approach mirrors LogicBlox’s declarative programming model and describes the maintenance of application code using declarative meta-rules; the meta-engine is essentially a “bootstrap” version of the database engine proper. Beyond live programming, the meta-engine turns out effective {{for a range of}} static analysis and optimization tasks. Outside of the database context, we speculate that our design may even provide a novel means of building <b>incremental</b> <b>compilers</b> for general-purpose programming language...|$|R
5000|$|The [...]NET Compiler Platform (codename Roslyn) is an {{open source}} <b>incremental</b> <b>{{compiler}}</b> for C# and Visual Basic [...]NET, and is the default compiler from Visual Studio 2015 onwards ...|$|E
5000|$|... 5.3, March 2003: <b>Incremental</b> <b>compiler</b> {{technology}} {{available for}} Eiffel [...]NET. Eiffel2Java Java interface, EiffelStore (relational database interface) now available for [...]NET, first Mac OS version, performance enhancements (press release).|$|E
50|$|The system {{includes}} an <b>incremental</b> <b>compiler</b> which translates source code into virtual machine code.The compiler optimizes index selection, unification order, inlining of control constructs and cantake mode information into account.|$|E
40|$|Dynamic binary {{translation}} (DBT) {{can provide}} security, virtualization, resource management and other desirable services to embedded systems. Although DBT has many benefits, its run-time performance overhead can be relatively high. The run-time overhead {{is important in}} embedded systems due to their slow processor clock speeds, simple microarchitectures, and small caches. This paper addresses how to implement efficient DBT for ARM-based embedded systems, taking into account instruction set and cache/TLB nuances. We develop several techniques that reduce DBT overhead for the ARM. Our techniques focus on cache and TLB behavior. We tested the techniques on an ARM-based embedded device and found that DBT overhead was reduced by 54 % in comparison to a general-purpose DBT configuration that is known to perform well, thus further enabling DBT {{for a wide range}} of purposes. Categories and Subject Descriptors C. 3 [Computer Systems Organization]: Special-purpose and application- based systems–Realtime and embedded systems; D. 3. 4 [Programming Languages]: Processors–Code generation, <b>Compilers,</b> <b>Incremental</b> <b>compilers...</b>|$|R
50|$|Pop-11 is {{the core}} {{language}} of the Poplog system. The fact that the compiler and compiler subroutines are available at run-time (a requirement for incremental compilation) gives it the ability to support a far wider range of extensions than would be possible using only a macro facility. This {{made it possible for}} <b>incremental</b> <b>compilers</b> to be added for Prolog, Common Lisp and Standard ML, which could be added as required to support either mixed language development or development in the second language without using any Pop-11 constructs. This made it possible for Poplog to be used by teachers, researchers, or developers who were interested in only one of the languages. The most successful product developed in Pop-11 was the Clementine data-mining system, developed by ISL, as described in the entry on Poplog. After SPSS bought ISL they decided to port Clementine to C++ and Java, and eventually succeeded with great effort (and perhaps some loss of the flexibility provided by the use of an AI language!).|$|R
50|$|Petite Chez Scheme is its sibling {{implementation}} {{which uses}} a threaded interpreter design in place of Chez Scheme's <b>incremental</b> native-code <b>compiler.</b> Programs written for Chez Scheme run unchanged in Petite Chez Scheme, {{as long as they}} do not depend specifically on the compiler (for example foreign function interface is only available in the compiler). Petite Chez Scheme is freely distributable and may be used without royalty fees, subject to the license agreement.|$|R
50|$|Ikarus Scheme {{is a free}} {{software}} optimizing <b>incremental</b> <b>compiler</b> for R6RS Scheme that compiles directly to the x86 architecture. Ikarus is the first public implementation of {{a large part of}} the R6RS Scheme standard.|$|E
50|$|In {{languages}} {{that support}} reflection, such as interactive evaluation of source code (using an interpreter or an <b>incremental</b> <b>compiler),</b> identifiers are also runtime entities, sometimes even as first-class objects {{that can be}} freely manipulated and evaluated. In Lisp, these are called symbols.|$|E
50|$|After an <b>incremental</b> <b>compiler</b> for Prolog {{had been}} added to an {{implementation}} of POP-11, the name Poplog was adopted, to {{reflect the fact that}} it supported programming in both languages. The name was retained, as a trade mark of the University of Sussex, when the system was later extended as incremental compilers were added for Lisp and Standard ML.|$|E
40|$|At {{the heart}} {{of much of the}} {{controversy}} over Microsoft's impact on the software industry are conflicting views about the company's record as an innovator. Here, we begin by distinguishing between invention, which is the creation of something new, and innovation, which is the commercialization of products or services not previously commercialized. We then examine several objective measures of Microsoft's contributions to the innovative process. Finally, we offer highlights from a case study on the Developer Tools and Platforms group at Microsoft, which puts the evolution of Microsoft innovation in a more tangible context. We find that Microsoft has excelled as an innovator in the last decade, as measured by a variety of factors: R&D spending, patent activity, academic publication and product quality, with the latter measured by tallies of independent reviews. The case study strengthens this finding, suggesting that Microsoft's Tools and Platforms group has been a key innovator in areas ranging from <b>incremental</b> <b>compilers</b> to web services, and that its development of better platforms has enabled thousands of other software companies to be better innovators in their own right. Technology and Industry...|$|R
40|$|There is {{a growing}} {{utilization}} gap between modern hardware and modern programming languages for data analysis. Due to power and other constraints, recent processor design has sought improved performance through increased SIMD and multi-core parallelism. At the same time, high-level, dynamically typed languages for data analysis have become popular. These languages emphasize ease of use and high productivity, but have, in general, low performance and limited support for exploiting hardware parallelism. In this paper, we describe Riposte, a new runtime for the R lan-guage, which bridges this gap. Riposte uses tracing, a technique commonly used to accelerate scalar code, to dynamically discover and extract sequences of vector operations from arbitrary R code. Once extracted, we can fuse traces to eliminate unnecessary mem-ory traffic, compile them to use hardware SIMD units, and schedule them to run across multiple cores, allowing us to fully utilize the available parallelism on modern shared-memory machines. Our evaluation shows that Riposte can run vector R code near the speed of hand-optimized C, 5 – 50 x faster than the open source implemen-tation of R, and can also linearly scale to 32 cores for some tasks. Across 12 different workloads we achieve an overall average speed-up of over 150 x without explicit programmer parallelization. Categories and Subject Descriptors D. 3. 4 [Processors]: <b>incremental</b> <b>compilers,</b> code generation, inter-preter...|$|R
40|$|The {{contribution}} {{of this work}} is the design, implementation and evaluation of a new aspect-oriented intermediate language model that we call Nu. The primary motivation behind {{the design of the}} Nu model is to maintain the aspect-oriented design modularity in the intermediate code for the responsiveness of <b>incremental</b> <b>compilers</b> and source-level debuggers. Nu extends the object-oriented intermediate language model with two primitives: bind and remove. We demonstrate that these primitives are capable of expressing statically deployed constructs such as AspectJ 2 ̆ 7 s aspect, dynamic deployment construct such as CaeserJ 2 ̆ 7 s deploy as well as dynamic control flow constructs such as AspectJ 2 ̆ 7 s cflow by presenting compilation techniques from high-level languages to Nu for these constructs. Moreover, these compilation techniques also serve to show that aspect-oriented design modularity is indeed preserved in the Nu intermediate code. We also present the design and implementation of a prototype extension of the Sun Hotspot virtual machine that supports the Nu model, which serves to show that it is feasible to implement Nu in a production level virtual machine. A key concern for dynamic language models is the performance overhead of their implementation. Our performance analysis results show that method dispatch time is not degraded in our prototype implementation. Also, advice dispatch time remains fairly close to the manually inlined version...|$|R
50|$|MetaOCaml is a {{multi-stage}} programming {{extension of}} OCaml enabling incremental compiling of new machine code during runtime. Under some circumstances, significant speedups are possible using multistage programming, because more {{detailed information about}} the data to process is available at runtime than at the regular compile time, so the <b>incremental</b> <b>compiler</b> can optimize away many cases of condition checking, etc.|$|E
50|$|The Eclipse SDK {{includes}} the Eclipse Java development tools (JDT), offering an IDE {{with a built-in}} Java <b>incremental</b> <b>compiler</b> and a full model of the Java source files. This allows for advanced refactoring techniques and code analysis. The IDE also makes use of a workspace, {{in this case a}} set of metadata over a flat filespace allowing external file modifications as long as the corresponding workspace resource is refreshed afterward.|$|E
50|$|The {{computer}} had an <b>incremental</b> <b>compiler</b> for a language, PAF (Programmation Automatique des Formules) {{similar to}} Fortran, designed by Dimitri Starynkevitch in 1957-1959. CAB 500's first model was delivered in February, 1961, {{and more than}} a hundred exemplars were built. It had a magnetic drum memory of 16 K words (of 32 bits) rotating at 3000 rpm (50 rotations/s) and could invert a square matrix of order 25 in half an hour.|$|E
40|$|Recently, the {{pressure}} for fast processing and efficient storage of large data with complex?relations increased beyond the capability of traditional databases. Typical examples include iPhone applications, computer aided design - both electrical and mechanical, biochemistry applications, and <b>incremental</b> <b>compilers.</b> Serialization, which is sometimes used in such situations is notoriously tedious and error prone. In this book, Jiri Soukup and Petr Macha?ek show in detail how to write programs which store their internal data automatically and transparently to disk. Together with special data structure libraries which treat relations among objects as first-class entities, and with a UML class-diagram generator, the core application code is much simplified. The benchmark chapter shows a typical example where persistent data is faster by the order of magnitude than with a traditional database, in both traversing and accessing the data. The authors explore and exploit advanced features of object-oriented languages in a depth hardly seen in print before. Yet, you as a reader need only a basic knowledge of C++, Java, C#, or Objective C. These languages are quite similar with respect to persistency, and the authors explain their differences where necessary. The book targets professional programmers working on any industry applications, it teaches you how to design your own persistent data or {{how to use the}} existing packages efficiently. Researchers in areas like language design, compiler construction, performance evaluation, and no-SQL applications will find a wealth of novel ideas and valuable implementation tips. Under [URL] you will find a blog and other information, including a downloadable zip file with the sources of all the listings that are longer than just a few lines - ready to compile and run...|$|R
5000|$|Some of {{the ideas}} behind the design of Ikarus Scheme are {{detailed}} in [...] "An <b>Incremental</b> Approach to <b>Compiler</b> Construction" [...] by the developer. [...] In particular, Ikarus is self-hosting {{with the majority of}} the compiler and primitives written in Scheme and only a few parts of the runtime written in C. Also, rather than using an external intermediate language like C, LLVM or C--, it compiles directly to machine code in order to better take advantage of the underlying machine architecture.|$|R
40|$|So far C++ {{has made}} few inroads {{into the realm}} of {{scientific}} computing, which is still largely dominated by Fortran. Of the few attempts that have been made to apply C++ to numerically intensive codes, the results have often suffered from severe performance problems. A careful examination of these problems indicates that they are unlikely to be solved by <b>incremental</b> improvements in <b>compiler</b> optimization technology. The flow of this article will: motivate the discussion by describing a common efficiency problem that arises when numerical codes are programmed in C++; discuss some potential solution strategies that we believe are viable in the near term, but not over the long term; introduce a mechanism by which a compiler can load domain-specific and class-specific optimizations on an as needed basis. A simple interface that will enable this feature will be presented. Althoug our immediate motivation is that of numerically intensive codes, our approach is applicable to all application domains...|$|R
