0|76|Public
5000|$|Richard Paul [...] "Rick" [...] Bayan (born January 27, 1950) is a U.S. author, {{webmaster}} {{and advertising}} copywriter, {{best known for}} his advertising <b>thesaurus</b> <b>Words</b> That Sell; his trenchant satirical lexicon, The Cynic's Dictionary; his political blog, The New Moderate; and his darkly humorous online essays.|$|R
50|$|WordNet {{properties}} {{have been}} studied from a network theory perspective and compared to other semantic networks created from Roget's <b>Thesaurus</b> and <b>word</b> association tasks. From this perspective {{the three of them}} are a small world structure.|$|R
5000|$|Peter Mark Roget (1779-1869) {{creator of}} the <b>Thesaurus</b> of English <b>Words</b> and Phrases (Roget's Thesaurus) ...|$|R
5000|$|Hunt, Jennifer Margaret. [...] "The Major Text of Konráðs saga keisarasonar with a <b>Thesaurus</b> of <b>Word</b> Forms." [...] M. Phil. Thesis, Univ. of London, 1972. (based on Stockholm Perg. 4:o nr 7; Perg. fol. nr 7; Perg. 4:o nr 6.) ...|$|R
40|$|A {{new type}} of <b>thesaurus</b> for <b>word</b> {{processing}} is proposed. It comprises 7 semantic and 8 syntagmatic types of links between Russian words and collocations. The original version now includes ca. 76, 000 basic dictionary entries, 660, 000 semantic and 292, 000 syntagmatic links, English interface, and communication with any text editor...|$|R
5000|$|On {{his writing}} process: “I don’t do drafts. I edit as I go along. So I’m always {{throwing}} stuff out. And then when I finish, I read and read and re-read. I {{do so at}} the computer about 10 or 15 times, all the way through, hammering things out here and there. Then when I have it pretty close, I print it out, and I read and read and read some more, while I pace. Because walking helps, for some reason. We live in our own heads too much. It’s good to make writing as physical as possible. Sometimes I read out loud, not because I need to sound out big <b>thesaurus</b> <b>words,</b> but because it’s easier to tell if you’re missing a beat or have an extra beat too many. Writing and music - same difference. It’s all about rhythm. And I look like an idiot doing this, quite frankly.” ...|$|R
25|$|In December 2002, {{the first}} sister project, Wiktionary, was created; aiming {{to produce a}} {{dictionary}} and <b>thesaurus</b> of the <b>words</b> in all languages. It uses the same software as Wikipedia.|$|R
5000|$|Model Futures OWL Editor (Free) Able to {{work with}} very large OWL files (e.g. Cyc) and has {{extensive}} import and export capabilities (inc. UML, <b>Thesaurus</b> Descriptor, MS <b>Word,</b> CA ERwin Data Modeler, CSV, etc.) ...|$|R
40|$|This paper {{proposes a}} method for {{extending}} an existing thesaurus through classitlcatlon of new words {{in terms of that}} <b>thesaurus.</b> New <b>words</b> are classified on the basis of relative probabilities of. a word belonging to a given word class, with the probabilities calculated using nounverb co-occurrence pairs. Experiments using the Japanese Bunruigoihy thesaurus on about 420, 000 co-occurrences showed that new words can be classified correctly with a maximum ac- curacy of more than 80 %...|$|R
40|$|This paper {{describes}} our work, which {{participated in}} the Cross-Language Information Retrieval (CLIR) at the Cross-Language Evaluation Forum. Our objectives for this experiment have three folds. Firstly, {{the coverage of the}} Thai-bilingual dictionary was evaluated when translating queries. Secondly, whether the segmentation process has effected the CLIR. Lastly, this research investigates the query formations techniques. Since this is the first international experimental in CLIR, our approach used dictionary-based technique to translate Thai queries into English queries. Four runs are submitted to the CLEF: (a) single mapping translation with manual segmentation, (b) multiple mapping translation with manual segmentation, (c) single mapping translation with automatic segmentation and (d) Single mapping with query enhancing with the Thai <b>thesaurus</b> <b>words.</b> The retrieval effectiveness is worse than our expected. The simple dictionary mapping technique is unable to achieve the retrieval effectiveness, although the dictionary lookup gave very good high percentage of mapping word. The words from the dictionary lookup are not specific terms but each is mapped to a definition or meaning of that term. Furthermore, Thai stopword, stemmed word and word separation have effected in Thai CLIR. 1...|$|R
5000|$|Peter Mark Roget FRS ( [...] or 18 January 1779 - 12 September 1869) was a British physician, natural theologian and lexicographer. He is {{best known}} for publishing, in 1852, the <b>Thesaurus</b> of English <b>Words</b> and Phrases (Roget's Thesaurus), a {{classified}} collection of related words.|$|R
40|$|We are {{attempting}} to model travel route choice behaviour with language to describe the thinking process of travelers because words can directly and clearly reflect their psychological states from a bottom-up viewpoint. This paper shows a method that extracts impressions and feelings, i. e., cognition results of travel routes, out of open-ended questionnaire texts with a <b>thesaurus.</b> Complex <b>words</b> are also allowed as cognition results. Additional considerations and training contents are also reported. Finally, an experiment on the extraction of cognition results from unseen texts is reported...|$|R
40|$|In this study, the <b>Thesaurus</b> and the <b>Word</b> Form Dictionaries are merged, and the {{performance}} of this new dictionary is compared to that of its individual elements. The new dictionary yields better normalized precisions and recalls, but the improvement is only slight and the results are sometimes inconsistent. 1...|$|R
50|$|In his research, Yuen {{pioneered the}} {{utilization}} of dictionary databases for Thai word splitting and machine translation, created the first Thai language <b>thesaurus</b> and developed <b>word</b> and sentence reconstruction methods for use in spell checking applications, among other things. Much of his work was presented at various national and international conferences.|$|R
40|$|The Internet is an ever growing {{source of}} {{information}} stored in documents of different languages. Hence, cross-lingual resources are needed {{for more and more}} NLP applications. This paper presents (i) a graph-based method for creating one such resource and (ii) a resource created using the method, a cross-lingual relatedness <b>thesaurus.</b> Given a <b>word</b> in one language, the <b>thesaurus</b> suggests <b>words</b> in a second language that are semantically related. The method requires two monolingual corpora and a basic dictionary. Our general approach is to build two monolingual word graphs, with nodes representing words and edges representing linguistic relations between words. A bilingual dictionary containing basic vocabulary provides seed translations relating nodes from both graphs. We then use an inter-graph node-similarity algorithm to discover related words. Evaluation with three human judges revealed that 49 % of the English and 57 % of the German words discovered by our method are semantically related to the target words. We publish two resources in conjunction with this paper. First, noun coordinations extracted from the German and English Wikipedias. Second, the cross-lingual relatedness thesaurus which can be used in experiments involving interactive cross-lingual query expansion. 1...|$|R
40|$|A {{self-organizing}} map (SOM) {{is used to}} classify software documents and the associated software components with the aim to facilitate software reuse. SOM learns from input stimuli rather than training data, therefore the quality of input data representation {{is crucial to the}} success of SOM. In this paper, we use automatic indexing method to represent a document collection as the input data to train a SOM. The automatic indexing uses a phrase formation method to promote precision and a domain <b>dependent</b> relational <b>thesaurus</b> to enhance recall. A retrieval experiment based on a document collection containing 97 Unix manual pages was conducted {{to evaluate the effectiveness of}} this input data representation scheme. Promising retrieval results were observed...|$|R
40|$|We {{introduce}} {{an interactive}} visualization component for the JoBimText project. JoBim-Text {{is an open}} source platform for large-scale distributional semantics based on graph representations. First we describe the underlying technology for computing a distributional <b>thesaurus</b> on <b>words</b> using bipartite graphs of words and context features, and contextualizing the list of semantically similar words towards a given sentential context using graphbased ranking. Then we demonstrate the capabilities of this contextualized text expansion technology in an interactive visualization. The visualization {{can be used as}} a semantic parser providing contextualized expansions of words in text as well as disambiguation to word senses induced by graph clustering, and is provided as an open source tool. ...|$|R
40|$|This paper {{presents}} an unsupervised system for all-word domain specific {{word sense disambiguation}} task. This system tags target word with the most frequent sense which is estimated using a <b>thesaurus</b> and the <b>word</b> distribution information in the domain. The thesaurus is automatically constructed from bilingual parallel corpus using paraphrase technique. The recall of this system is 43. 5 % on SemEval- 2 task 17 English data set. ...|$|R
40|$|This paper {{describes}} {{a course of}} instruction in the compilation of a personal documentation system, at Twente University of Technology, the Netherlands. Students are introduced to indexing with key <b>words,</b> <b>thesaurus</b> construction, bibliographical description, {{and the use of}} inverted files. They are taught the use of the 2 ̆ 7 uniterm card system 2 ̆ 7. The advantages of automation of a personal documentaion system are discussed...|$|R
5000|$|Bizarre Creations {{started as}} Raising Hell Software, founded by Martyn Chudley. Sega scorned [...] "Hell", {{and the company}} went nameless for a short time. In 1994, a pending {{submission}} to Psygnosis/Sony forced the decision of a new name. The founder tentatively left [...] "Weird Concepts" [...] on the submission documentation. Then a staff member used Microsoft <b>Word's</b> <b>thesaurus,</b> and [...] "Bizarre Creations" [...] stuck.|$|R
40|$|Photograph of Mrs. Dolly Reed & flowers, Southern California, 1930. "[ilg]akes Good in Big Job; Big Buisness Men [ilt] Mexico for Hunting" [...] on {{newspaper}} clippings. "Oil! by Upton Sinclair; Roget's <b>Thesaurus</b> of English <b>Words</b> and Phrases, Nawson, International {{large type}} edition, Crowell; 1929 The World Almanac; Webster's New Illustr[ilg] Dictionary; Doings in [ilg], Vol 1, 1925 " [...] on books. "The cat's [ilg] will you [ilg]" [...] on card...|$|R
5000|$|Roget Rocks (...) is a {{small group}} of rocks 4 {{nautical}} miles (7 km) southwest of Spring Point in Hughes Bay, Graham Land. Surveyed by K.V. Blaiklock of Falkland Islands Dependencies Survey (FIDS).It is named after Peter Mark Roget (1779-1869), British physician, natural theologian and lexicographer, best known as author of <b>Thesaurus</b> of English <b>words</b> and phrases (London, 1852), a work frequently consulted in connection with Antarctic place-name proposals.|$|R
40|$|If {{one hears}} the term thesaurus, one {{may think of}} two things: one is thesauri like Roget’s {{well-known}} <b>thesaurus</b> with <b>words</b> grouped and classified to help writers select the best word to convey a specific nuance of meaning; the other is thesauri as tools for indexers and searchers in information science with a listing of words and phrases authorized for use in an indexing system, together with relationships, variants and synonyms, and aids to navigation through the thesaurus (Craven, sec. 1). From this aspect a thesaurus {{is much more than}} just a dictionary with the definitions of terms, because it reveals different semantic relations as well. My work belongs to the second type; it is a so-called information retrieval thesaurus [...] . (Introduction) egyetemiangol nyelv és irodalo...|$|R
40|$|Automatic {{thesaurus}} extraction {{techniques are}} applied to computer-generated related word vocabulary questions. These questions assess and provide practice for an aspect of word knowledge found to be important for language learning. Automatic generation of such questions reduces the need for human authoring of practice materials. In evaluations with real teachers, most of the generated questions {{were considered to be}} usable in real classrooms. Also, performance of native and nonnative speakers on these automatically generated questions was similar to their performance on manually generated questions for the same words. This application of natural language processing techniques to English as a Second language education is a promising step toward automatically producing vocabulary practice and assessment materials for language learners. Index Terms: automatic <b>thesaurus</b> extraction, <b>Word</b> Associates Test, computer assisted language learnin...|$|R
40|$|Abstract:- Natural {{language}} has attracted importance to some researchers working on Emotion recognizing. Our study {{focuses on the}} semi-automatic acquisition technique to obtain the emotion information with the constructed emotion thesaurus we have made. In this paper, we made two models, one is the emotion thesaurus, {{the other is the}} emotion analyzer. Firstly we construct Chinese language emotion <b>thesaurus,</b> the <b>word</b> in which <b>thesaurus</b> has its own classifying of the corresponding emotion information. Secondly, using the thesaurus we set up a model to recognize the object sentence including functions of lexical analysis syntax analysis, emotion sensing and emotion computing. The following investigations are presented to show that the concept of “image value ” can be used by the constructed system to obtain the emotion information from the sentence based on the constructed emotion thesaurus...|$|R
40|$|Abstract. Homographs {{are words}} with {{identical}} spellings but different origins and meanings. Natural language processing {{must deal with}} the disambiguation of homographs and the attribution of senses to them. Advances have been made using context to discriminate homographs, {{but the problem is}} still open. Disambiguating homographs is possible using formal concept analysis. This paper discusses the issues, illustrated by examples, using data from Roget’s Thesaurus. Keywords: Type- 10 chains, partitions, components, Roget’s <b>Thesaurus,</b> plus operator, <b>word</b> fields, neighbourhood lattices...|$|R
40|$|In {{addressing}} {{the question of}} how the temporal Arts are ‘preserved’, this paper aims to throw light on the mind of the listener as interpreter-conspirator, by proposing that Peter Mark Roget (1779 - 1869) ’s ‘Classification of Ideas’ from the introduction to his <b>Thesaurus</b> of English <b>Words</b> and Phrases (1852), can be viewed as the subjective presentation of the structure of a shared system of signification by which artists, spectators and auditors alike, make sense of their art...|$|R
30|$|Finally, {{the work}} in [22] is the closest to the present discussion, {{introducing}} a preliminary—but still purely statistical—version of our system and the training and test data sets that we have presently reused. However, the system presented in [22] was still lacking many of the current functionalities, including the rule-based methods to limit over-generation {{and the use of}} a <b>thesaurus</b> to exploit <b>word</b> synonymy. The current system, by contrast, outperforms its early version in [22], but it is of course more language-dependent.|$|R
40|$|This paper {{presents}} {{a method to}} resolve word sense ambiguity in a Korean-to-Japanese machine translation system using neural networks. The execution of our neural network model {{is based on the}} concept codes of a <b>thesaurus.</b> Most previous <b>word</b> sense disambiguation approaches based on neural networks have limitations due to their huge feature set size. By contrast, we reduce the number of features of the network to a practical size by using concept codes as features rather than the lexical words themselves...|$|R
40|$|<b>Thesaurus</b> of key <b>words</b> used in {{the annual}} subject indexes (valid from January 1997) The list is common to Astronomy and Astrophysics, The Astrophysical Journal and Monthly Notices of the Royal Astronomical Society. In order to ease the search, the key words are {{subdivided}} into broad categories. No more than six codes all together should be listed for a paper as this is the limit fixed by the computer program. The key words in boldface listed under the code number...|$|R
40|$|Abstract. Diana McCarthy et al. (ACL- 2004) {{obtain the}} {{predominant}} sense for an ambiguous word {{based on a}} weighted <b>thesaurus</b> of <b>words</b> related to the ambiguous <b>word.</b> This <b>thesaurus</b> is obtained using Dekang Lin’s (COLING-ACL- 1998) distributional similarity method. Lin averages the distributional similarity by the whole training corpus; thus the list of words related to a given <b>word</b> in his <b>thesaurus</b> is given for a word as type and not as token, i. e., {{does not depend on}} a context in which the word occurred. We observed that constructing a list similar to Lin’s thesaurus but for a specific context converts the method by McCarthy et al. into a word sense disambiguation method. With this new method, we obtained a precision of 69. 86 %, which is even 7 % higher than the supervised baseline. ...|$|R
40|$|The Thesaurus of Nuovo Soggettario {{contains}} 13000 terms {{linked together}} by standard semantic relationships. They {{are divided into}} two groups of 7660 preferred terms and 5340 non-preferred. The morphologic rules (singular/plural, decomposition) are in accordance with ISO standard 2788 / 1986. About 2190 terms have been changed since the 1956 edition of the Soggettario producing new connections and give the derivations called "historical variations". The <b>Thesaurus</b> includes current <b>words,</b> technical and specialised terms, names of plants and animals, neologisms and foreign words used in the italian language...|$|R
50|$|The {{original}} 1987 edition contained The Original Roget's <b>Thesaurus</b> of English <b>Words</b> and Phrases, The American Heritage Dictionary of the English Language, World Almanac and Book of Facts, Bartlett's Familiar Quotations, The Chicago Manual of Style (13th Edition), the U.S. ZIP Code Directory, Houghton Mifflin Usage Alert, Houghton Mifflin Spelling Verifier and Corrector, Business Information Sources, and Forms and Letters. Titles in non-US {{versions of}} Bookshelf were different. For example, the 1997 UK edition included the Chambers Dictionary, Bloomsbury Treasury of Quotations, and Hutchinson Concise Encyclopedia.|$|R
40|$|This paper {{proposes a}} method for {{extending}} an existing thesaurus through classification of new words {{in terms of that}} <b>thesaurus.</b> New <b>words</b> are classified on the basis of relative probabilities of a word belonging to a given word class, with the probabilities calculated using nounverb co-occurrence pairs. Experiments using the Japanese Bunruigoihyo thesaurus on about 420, 000 co-occurrences showed that new words can be classified correctly with a maximum accuracy of more than 80 %. 1 Introduction For most natural language processing (NLP) systems, thesauri comprise indispensable linguistic knowledge. Roget's International Thesaurus [Chapman, 1984] and WordNet [Miller et al., 1993] are typical English thesauri which have been widely used in past NLP research [Resnik, 1992; Yarowsky, 1992]. They are handcrafted, machine-readable and have fairly broad coverage. However, since these thesauri were originally compiled for human use, they are not always suitable for computer-based na [...] ...|$|R
40|$|Abstract. In {{this paper}} {{we present a}} new {{approach}} for measuring the relatedness between text segments, based on implicit semantic links between their words, as offered by a <b>word</b> <b>thesaurus,</b> namely WordNet. The approach does not require any type of training, since it exploits only WordNet to devise the implicit semantic links between text words. The paper presents a prototype on-line demo of the measure, that can provide word-to-word relatedness values, even for words of different part of speech. In addition the demo allows for the computation of relatedness between text segments. ...|$|R
40|$|Thesaurus is an {{important}} tool, which is well suited to find more and/or better terms during writing and reading the documents. Such monolingual thesaurus for Hindi language has been presented in this paper. The use of thesaurus in documents {{is considered to be}} the most important resource to writers. A <b>thesaurus</b> contains synonyms (<b>words</b> which have basically the same meaning) and antonyms, which is important for many other applications in NLP too. This paper describes the script of Hindi language and importance of Hindi thesaurus written in Hindi documents...|$|R
