56|2274|Public
25|$|Bennet, P. 1995. A {{course in}} <b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammar.</b> London: UCL Press Limited.|$|E
5000|$|<b>Generalized</b> <b>phrase</b> <b>structure</b> <b>grammar</b> (GPSG; now largely out of date) ...|$|E
50|$|Bennet, P. 1995. A {{course in}} <b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammar.</b> London: UCL Press Limited.|$|E
50|$|Gazdar defined Linear Indexed Grammars and pioneered, {{along with}} his colleagues Ewan Klein, Geoffrey Pullum and Ivan Sag, the {{framework}} of <b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammars.</b>|$|R
40|$|Within {{the project}} KIT-FAST an {{experimental}} machine translation (MT) {{system has been}} developed and implemented, which translates written German texts into English. For that reason a syntactic, semantic and aspects of a conceptual level of representation have been realized. In general each level has three dimensions, which are a sentence and a text representation, which are constructed during translation, as well as predefined background knowledge (domain and world knowledge, linguistic knowledge). Our first step towards the translation of German texts instead of single sentences was to interpret anaphoric relations in the source language. For that reason a knowledege representation system has been integrated into the MT system in order to represent text and background knowledge in a terminological logic. The syntactic and semantic sentence representations are structures, which are generated by <b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammars</b> (GPSG), and Functor-Argument Structures (FAS), respec [...] ...|$|R
40|$|A {{number of}} {{researchers}} have proposed applications of Girard 's Linear Logic [7] to computational linguistics. Most have focused primarily on the connection between linear logic and categorial grammars. In this work we show how linear logic {{can be used to}} provide an attractive encoding of <b>phrase</b> <b>structure</b> <b>grammars</b> for parsing <b>structures</b> involving unbounded dependencies. The resulting grammars are closely related to <b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammars</b> [4, 5]. As part of the presentation we show how a variety of issues, such as island and coordination constraints can be dealt with in this model. 1 Introduction Over the last several years a {{number of researchers}} have proposed applications of Girard's Linear Logic [7] to computational linguistics. On the semantics side, Dalrymple, Lamping, Pereira, and Saraswat have shown how deduction in linear logic can be used to enforce various constraints during the construction of semantic terms [1, 2, 3]. On the syntax side, Hepple, Jo [...] ...|$|R
50|$|Gazdar, G, E. Klein, G. Pullum, and I. Sag. 1985. <b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammar.</b> Oxford: Blackwell.|$|E
5000|$|Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey K.; and Sag, Ivan A. (1985). <b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammar.</b> Basil Blackwell, Oxford.|$|E
50|$|<b>Generalized</b> <b>phrase</b> <b>structure</b> <b>grammar</b> (GPSG) is a {{framework}} for describing the syntax and semantics of natural languages. It {{is a type of}} constraint-based phrase structure grammar, as opposed to a dependency grammar. GPSG was initially developed in the late 1970s by Gerald Gazdar. Other contributors include Ewan Klein, Ivan Sag, and Geoffrey Pullum. Their book <b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammar,</b> published in 1985, is the main monograph on GPSG, especially as it applies to English syntax.|$|E
40|$|This {{paper is}} a formal {{analysis}} of whether <b>generalized</b> <b>phrase</b> <b>structure</b> <b>grammar's</b> (GPSG) weak context-free generative power will {{allow it to}} achieve three of its central goals: (1) to characterize all and only the natural language grammars, (2) to algorithmically determine membership and generative power consequences of GPSG's and (3) to embody the universalism of natural language entirely in the formal system. I prove that "=E*?" is undecidable for GPSGs and, {{on the basis of}} this result and the unnaturalness of E*, I argue that GPSG's three goals and its weak context-free generative power conflict with each other: there is no algorithmic way of knowing whether any given GPSG generates a natural language or an unnatural one. The paper concludes with a diagnosis of the result and suggests that the problem might be met by abandoning the weak context-free framework and assuming substantive constraints...|$|R
50|$|The idea {{first came}} to {{prominence}} as part of <b>generalized</b> <b>phrase</b> <b>structure</b> grammar; the ID/LP approach is also used in head-driven <b>phrase</b> <b>structure</b> <b>grammar,</b> lexical functional grammar, and other unification grammars.|$|R
50|$|Subcategorization {{frames are}} {{essential}} {{parts of a}} number of <b>phrase</b> <b>structure</b> <b>grammars,</b> e.g. Head-Driven <b>Phrase</b> <b>Structure</b> <b>Grammar,</b> Lexical Functional Grammar, and Minimalism.|$|R
50|$|Gazdar, Gerald, Ewan Klein, Geoffrey K. Pullum, and Ivan A. Sag. 1985. <b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammar.</b> Cambridge, MA: Harvard University Press and Oxford: Basil Blackwellâ€™s.|$|E
5000|$|Valence {{plays an}} {{important}} role in a number of the syntactic frameworks that have been developed in the last few decades. In <b>generalized</b> <b>phrase</b> <b>structure</b> <b>grammar</b> (GPSG), many of the phrase structure rules generate the class of verbs with a particular valence. For example, the following rule generates the class of transitive verbs: ...|$|E
50|$|His {{early work}} {{was as a}} member of the {{research}} teams that invented and developed HPSG as well as <b>generalized</b> <b>phrase</b> <b>structure</b> <b>grammar,</b> HPSG's immediate intellectual predecessor. More recently, he worked on Sign-Based Construction Grammar, which blended HPSG with ideas from Berkeley Construction Grammar. In general, his research late in life primarily concerned constraint-based, lexicalist models of grammar, and their relation to theories of language processing.|$|E
50|$|In <b>phrase</b> <b>structure</b> <b>grammars,</b> such as {{generalised}} <b>phrase</b> <b>structure</b> <b>grammar,</b> head-driven <b>phrase</b> <b>structure</b> <b>grammar</b> and lexical functional <b>grammar,</b> {{a feature}} <b>structure</b> {{is essentially a}} set of attribute-value pairs. For example, the attribute named number might have the value singular. The value of an attribute may be either atomic, e.g. the symbol singular, or complex (most commonly a feature structure, but also a list or a set).|$|R
50|$|The term <b>phrase</b> <b>structure</b> <b>grammar</b> was {{originally}} introduced by Noam Chomsky as {{the term for}} grammars as defined by <b>phrase</b> <b>structure</b> rules, i.e. rewrite rules of the type studied previously by Emil Post and Axel Thue (Post canonical systems). Some authors, however, reserve the term for more restricted grammars in the Chomsky hierarchy: context-sensitive grammars or context-free grammars. In a broader sense, <b>phrase</b> <b>structure</b> <b>grammars</b> are also known as constituency grammars. The defining trait of <b>phrase</b> <b>structure</b> <b>grammars</b> is thus their adherence to the constituency relation, {{as opposed to the}} dependency relation of dependency grammars.|$|R
5000|$|... #Subtitle level 2: Controversy amongst <b>phrase</b> <b>structure</b> <b>grammars</b> ...|$|R
50|$|Merge is {{commonly}} seen as merging smaller constituents to greater constituents until the greatest constituent, the sentence, is reached. This bottom-up view of structure generation is rejected by representational (non-derivational) theories (e.g. <b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammar,</b> Head-Driven Phrase Structure Grammar, Lexical Functional Grammar, most dependency grammars, etc.), {{and it is}} contrary to early work in Transformational Grammar. The phrase structure rules of context free grammar, for instance, were generating sentence structure top down.|$|E
50|$|Nerbonne's {{research}} has ranged broadly within Linguistics and Computational Linguistics. His Ph.D. thesis was completed under thesupervision of David Dowty and concerns the syntax and semantics of temporal expressions in German. The grammar fragment there served asthe {{basis for a}} number of early computational implementations of <b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammar</b> (GPSG), e.g., in Berlin and Stuttgart (LILOG) and led to several journal publications. After his Ph.D. he pursued computational linguistics in an applied setting, at Hewlett-Packard Labs, where he supervised linguistic work.|$|E
50|$|Head-driven phrase {{structure}} grammar (HPSG) is {{a highly}} lexicalized, constraint-based grammar developed by Carl Pollard and Ivan Sag. It {{is a type of}} phrase structure grammar, as opposed to a dependency grammar, and it is the immediate successor to <b>generalized</b> <b>phrase</b> <b>structure</b> <b>grammar.</b> HPSG draws from other fields such as computer science (data type theory and knowledge representation) and uses Ferdinand de Saussure's notion of the sign. It uses a uniform formalism and is organized in a modular way which makes it attractive for natural language processing.|$|E
5000|$|... #Subtitle level 3: Head-driven <b>phrase</b> <b>structure</b> <b>grammar</b> (HPSG) ...|$|R
5000|$|<b>Phrase</b> <b>structure</b> <b>grammars</b> almost {{unanimously}} {{assume that}} the finite VP in bold in the first sentence is a constituent. DGs, in contrast, do not see finite VPs as constituents. Both <b>phrase</b> <b>structure</b> <b>grammars</b> and DGs do, however, see non-finite VPs as constituents. The dependency structure of the example sentence is as follows: ...|$|R
50|$|Borsley, R. 1996. Modern <b>phrase</b> <b>structure</b> <b>grammar.</b> Cambridge, MA: Blackwell Publishers.|$|R
50|$|The phrase {{structure}} {{trees are}} {{again on the}} left, and the dependency trees on the right. To mark the small clause in the phrase structure trees, the node label SC is used. The main difference between these layered trees and the flat trees further above resides with {{the status of the}} small clause as a constituent or not. The layered analysis is preferred by those working in the Government and Binding framework and its tradition,whereas the flat analysis is preferred by those working in dependency grammars and representational phrase structure grammars (e.g. <b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammar</b> and Head-Driven Phrase structure Grammar).|$|E
50|$|The {{phenomenon}} of parasitic gaps {{appears to have}} been discovered by John Robert Ross in the 1960s, but remained undiscussed until papers by Knut Tarald Taraldsen and Elisabet Engdahl explored the properties of the phenomenon in detail. The knowledge of parasitic gaps was central {{to the development of the}} GPSG framework (<b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammar)</b> in the mid 1980s, this knowledge then being refined later in the HPSG framework (Head-driven Phrase Structure Grammar) of Carl Pollard and Ivan Sag. In the 90s, a debate centered around the best theoretical analysis of parasitic gaps (extraction vs. percolation), this debate culminating in a collection of essays edited by Peter Culicover and Paul Postal in 2001.|$|E
40|$|Includes bibliographical {{references}} (pages 41 - 43) This thesis {{represents an}} investigation of <b>generalized</b> <b>phrase</b> <b>structure</b> <b>grammar</b> (GPSG). The main goal {{of this investigation was}} to empirically evaluate the potential of the GPSG formalism in the creation of an efficient, wide coverage parser. To this end, I have created an experimental parser based on the concepts of <b>generalized</b> <b>phrase</b> <b>structure</b> <b>grammar.</b> The target language was English. <b>Generalized</b> <b>phrase</b> <b>structure</b> <b>grammar</b> is a description formalism for the syntax of natural languages that is still under active development, and rooted in theoretical linguistics. A major attraction of this formalism for the computer scientist is the promise of computational tractability. The weak generative capacity of generalized phrase structure grammars is the set of context-free languages, for which efficient parsing algorithms are well known. The approach I took was to construct a pre-processor to convert <b>generalized</b> <b>phrase</b> <b>structure</b> <b>grammar</b> into context-free phrase structure grammar (CF-PSG), and use this as input to a context-free parser. This approach produces a large number of context-free phrase structure grammar rules. This means that the context-free parser, while still of cubic time complexity, is in practice slow and requires considerable memory space on the host system. In order to make a faster and smaller parsing system the conversion to CF-PSG must be avoided. The parser I developed did allow considerable experimentation with <b>generalized</b> <b>phrase</b> <b>structure</b> <b>grammar.</b> The formalism does seem capable of handling a wide variety of constructs. However, problems with the formalism still exist. "Exhaustive constant partial ordering", is shown to be a weak claim. The handling of conjunctions seems problematic. Solutions to these problems still await developmen...|$|E
50|$|Pollard, C. and I. Sag. 1994. Head-Driven <b>Phrase</b> <b>Structure</b> <b>Grammar.</b> University of Chicago Press.|$|R
5000|$|The {{distinction}} between configurational and non-configurational languages can exist for <b>phrase</b> <b>structure</b> <b>grammars</b> only. Dependency grammars (DGs), since they lack a finite VP constituent altogether, do not acknowledge the distinction. In other words, all languages are non-configurational for DGs, even English, which all <b>phrase</b> <b>structure</b> <b>grammars</b> {{take for granted}} as having a finite VP constituent. The point is illustrated with the following examples: ...|$|R
5000|$|In linguistics, <b>phrase</b> <b>structure</b> <b>grammars</b> are {{all those}} grammars {{that are based}} on the {{constituency}} relation, as opposed to the dependency relation associated with dependency grammars; hence, <b>phrase</b> <b>structure</b> <b>grammars</b> are also known as constituency grammars. [...] Any of several related theories for the parsing of natural language qualify as constituency grammars, and most of them have been developed from Chomsky's work, including ...|$|R
40|$|There {{are many}} motivations for proposing and {{developing}} a theoretical framework in linguistics. For example, a leading idea {{in the development of}} <b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammar</b> (Gazdar, et al. 1985) was to determine whether natural language syntax could be adequately described in terms of a context-free phrase structure grammar. Lexical Functiona...|$|E
40|$|The {{theory of}} <b>Generalized</b> <b>Phrase</b> <b>Structure</b> <b>Grammar,</b> as given in Gazdar et al. (1985) - {{henceforth}} GKPS, {{is defined in}} Immediate Dominance/Linear Precedence format, which states immediate dominance and linear precedence relations separately. Immediate Dominance (ID) rules specify the constituent relation between the mother and daughter categories but do not imply an...|$|E
40|$|In this paper, I revise <b>generalized</b> <b>phrase</b> <b>structure</b> <b>grammar</b> (GPSG) {{linguistic}} theory {{so that it}} is more tractable and linguis- tically constrained. Revised GPSG is also {{easier to}} understand, use, and implement. I provide an account of topicalization, explicative pronouns, and parasitic gaps in the revised system and conclude with suggestions for efficient parser design...|$|E
40|$|In this paper, {{we shall}} present a {{generalization}} of <b>phrase</b> <b>structure</b> <b>grammar,</b> {{in which all}} functional categories (such as verbs and adjectives) have type restrictions, that is, their argument types are specific domains. In ordinary <b>phrase</b> <b>structure</b> <b>grammar,</b> there is just one universal domain of individuals. The grammar {{does not make a}} distinction between verbs and adjectives in terms of domains of applicability. Consequently, it fails to distinguish between sentences like every line intersects every line, which is well typed, and every line intersects every point, which is ill typed. Our generalization relates to ordinary <b>phrase</b> <b>structure</b> <b>grammar</b> {{in the same way as}} the higherlevel constructive type theory of Martin-Lof (see Nordstrom et al. 1990, part III, or Ranta 1994, chapter 8) relates to the simple type theory of Church (Church 1940). Simple type theory has been used in linguistics and related with <b>phrase</b> <b>structure</b> <b>grammar,</b> especially in the tradition based on the work of Montagu [...] ...|$|R
2500|$|Pollard, C. and I. Sag. 1994. Head-driven <b>phrase</b> <b>structure</b> <b>grammar.</b> Chicago: University of Chicago Press.|$|R
50|$|Situation {{semantics}} is {{the first}} semantic theory {{that was used in}} Head-driven <b>phrase</b> <b>structure</b> <b>grammar</b> (HPSG).|$|R
