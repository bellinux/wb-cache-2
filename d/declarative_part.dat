22|15|Public
50|$|The XML Process Definition Language (XPDL) is {{a format}} {{standardized}} by the Workflow Management Coalition (WfMC) to interchange business process definitions between different workflow products, i.e. between different modeling tools and management suites.XPDL defines an XML schema for specifying the <b>declarative</b> <b>part</b> of workflow / business process.|$|E
5000|$|CREATE PROCEDURE create_email_address ( [...] -- Procedure heading part begins name1 VARCHAR2, name2 VARCHAR2, company VARCHAR2, email OUT VARCHAR2) -- Procedure heading part endsAS-- <b>Declarative</b> <b>part</b> begins (optional)error_message VARCHAR2(30) := 'Email {{address is}} too long.';BEGIN -- Executable part begins (mandatory) email := name1 || '.' || name2 || '@' || company;EXCEPTION -- Exception-handling part begins (optional)WHEN VALUE_ERROR THEN DBMS_OUTPUT.PUT_LINE(error_message);END create_email_address; ...|$|E
5000|$|DASL {{combines}} a declarative syntax with a Java-like procedural syntax. The <b>declarative</b> <b>part</b> {{of the language}} enables defining applications {{at a higher level}} of abstraction than 3rd generation languages such as Java. In DASL, the programmer does not describe inter-process communication between client processes, web servers, application servers, databases, or details of the user interface of the desired application. Rather, the programmer describes the application as a set of related domain objects (including their behavior), and as a set of forms and actions annotated with basic layout properties.|$|E
40|$|Types and Subprograms 3. 10 Access Types 3. 10. 1 Incomplete Type Declarations 3. 10. 2 Operations of Access Types 3. 11 <b>Declarative</b> <b>Parts</b> 3. 11. 1 Completions of Declarations 4. Names and Expressions 4. 1 Names 4. 1. 1 Indexed Components 4. 1. 2 Slices 4. 1. 3 Selected Components 4. 1. 4 Attributes 4. 2 Literals 4. 3 Aggregates 4. 3. 1 Record Aggregates 4. 3. 2 Extension Aggregates 4. 3. 3 Array Aggregates 4. 4 Expressions 4. 5 Operators and Expression Evaluation 4. 5. 1 Logical Operators and Short-circuit Control Forms 4. 5. 2 Relational Operators and Membership Tests 4. 5. 3 Binary Adding Operators 4. 5. 4 Unary Adding Operators 4. 5. 5 Multiplying Operators 4. 5. 6 Highest Precedence Operators 4. 6 Type Conversions 4. 7 Qualified Expressions 4. 8 Allocators 4. 9 Static Expressions and Static Subtypes ISO/IEC 8652 : 1995 (E) [...] - RM 95; 5. 95 25 November 1994 4. 9. 1 Statically Matching Constraints and Subtypes 5. Statements 5. 1 Simple and Compound Statements - Sequences of Statements 5. 2 Assignment Statements 5. 3 If Statement [...] ...|$|R
40|$|Workflows are {{composite}} {{activities that}} achieve interoperation {{of a variety}} ofsystem and human tasks. Workflows must satisfy subtle domain-specific integrity and organizational requirements. Consequently, exibility in execution is crucial. A promising means to achieve flexibility is through <b>declarative</b> specifications (<b>Part</b> 1) with automatic distributed scheduling techniques (Part 2). Intertask dependencies are constraints among the tasks that constitute a workflow. We propose a rigorous formal semantics for workflow computations and dependencies. Importantly, our approach uses symbolic reasoning to capture scheduler transitions. It includes an equational system that is guaranteed to yield the most general answers for scheduling, yet is sound and complete. workflows, temporal logic, formal semantics...|$|R
40|$|Workflows are {{composite}} {{activities that}} achieve interoperation {{of a variety}} of system and human tasks. Workflows must satisfy subtle domain-specific integrity and organizational requirements. Consequently, exibility in execution is crucial. A promising means to achieve flexibility is through <b>declarative</b> specifications (<b>Part</b> 1) with automatic distributed scheduling techniques (Part 2). We address the problem of scheduling workflows from declarative specifications given in terms of intertask dependencies and event attributes. Our approach involves distributed events, which are automatically set up to exchange the necessary messages. Our approach uses symbolic reasoning to (a) determine the initial constraints on events or guards, (b) preprocess the guards, and (c) execute the events. It has been implemented. workflows, temporal logic, scheduling...|$|R
50|$|The {{basic unit}} of a PL/SQL source {{program is the}} block, which groups {{together}} related declarations and statements. A PL/SQL block {{is defined by the}} keywords DECLARE, BEGIN, EXCEPTION, and END. These keywords divide the block into a <b>declarative</b> <b>part,</b> an executable part, and an exception-handling part. The declaration section is optional and may be used to define and initialize constants and variables. If a variable is not initialized then it defaults to NULL value. The optional exception-handling part is used to handle run time errors. Only the executable part is required. A block can have a label.|$|E
40|$|Abstract. This paper {{presents}} {{a proposal to}} use continuations as an implementation mechanism for ATL. We introduce the notion of continuation, showing its applicability to model-to-model transformations, and develop a simple mechanism to enable continuations in model transformations. Then, the <b>declarative</b> <b>part</b> of ATL is mapped to this mechanism. ...|$|E
40|$|The article {{discusses}} {{the questions of}} software architecture design for distributed systems that consist of multiple independent software entities (such as CORBA components or software agents). The main motivation of our approach {{is to provide a}} developer with ability to reduce difficulties in integration and adaptation of software entities into a heterogeneous distributed system. According to our approach a software entity consists of procedural and declarative parts. Procedural part can be concerned as a number of abstract software patterns and <b>declarative</b> <b>part</b> is coarse-grained knowledge of entity preferences with relevant external information. ...|$|E
40|$|This paper investigates intonational {{variation}} in Liverpool English, a dialect of British English that is recognised {{as having a}} number of distinctive phonetic characteristics [15, 16]. One previously reported aspect of Liverpool intonation is the pres- ence of rising contours in <b>declaratives</b> as <b>part</b> of the traditional dialect [7]. Here we present a phonological Autosegmental Metrical analysis [9] and a phonetic analysis of intonation in different sentence types from 9 speakers. Results suggest that traditional Liverpool rising nuclear contours are common among 20 – 22 year olds from Liverpool. Through analysis of these data, we aim to contribute to descriptions of intonational {{variation in}} the UK, and wider studies of intonational variation and typology...|$|R
40|$|Workflows are {{composite}} {{activities that}} achieve interoperation {{of a variety}} of system and human tasks. Workflows must satisfy subtle domain-specific integrity and organizational requirements. Consequently, flexibility in execution is crucial. A promising means to achieve flexibility is through <b>declarative</b> specifications (<b>Part</b> 1) with automatic distributed scheduling techniques (Part 2). Intertask dependencies are constraints among the tasks that constitute a workflow. We propose a rigorous formal semantics for workflow computations and dependencies. Importantly, our approach uses symbolic reasoning to capture scheduler transitions. It includes an equational system that is guaranteed to yield the most general answers for scheduling, yet is sound and complete. workflows, temporal logic, formal semantics. 1 Introduction Workflows are composite activities that typically involve a variety of computational and human tasks, and span multiple systems. Workflows arise naturally in heterogene [...] ...|$|R
50|$|The {{atoms of}} plans and {{behavior}} sequences are triplets of a (partial, hierarchical) situation description, forming a condition, an operator (a hierarchical action description) and an expected {{outcome of the}} operation as another (partial, hierarchical) situation description. Object descriptions (mainly <b>declarative)</b> are also <b>part</b> of long-term memory and the product of perceptual processes and affordances. Situations and operators in long-term memory {{may be associated with}} motivational relevance, which is instrumental in retrieval and reinforcement. Operations on memory content are subject to emotional modulation.|$|R
40|$|In {{this paper}} we generalize {{the notion of}} method for proof planning. While we adopt the general {{structure}} of methods introduced by Alan Bundy, we make an essential advancement in that we strictly separate the declarative knowledge from the procedural knowledge. This change of paradigm not only leads to representations easier to understand, it also enables modeling the important activity of formulating meta-methods, that is, operators that adapt the <b>declarative</b> <b>part</b> of existing methods to suit novel situations. Thus this change of representation leads to a considerably strengthened planning mechanism. After presenting our declarative approach towards methods we describe the basic proof planning process with these. Then we define the notion of meta-method, {{provide an overview of}} practical examples and illustrate how meta-methods can be integrated into the planning process...|$|E
40|$|The {{management}} of object dynamics, commonly called role {{management in the}} database field, refers {{to the evolution of}} object structure and behaviour. However, this task is complicated by many inherent constraints such as uniqueness of object identifier, strong typing, persistence, encapsulation, late binding, inheritance, etc. These constraints form the basis, together with data de nition and manipulation languages, of the ODMG standard. However, none of the approaches proposed so far take into account all these constraints in a satisfactory way. This paper introduces a model and its associated languages, that extends ODMG specifications and includes an integrated role mechanism based on declarative specifications. An object can therefore display several types while it conserves its unique identity. The <b>declarative</b> <b>part</b> provides straightforward modelling capabilities such as, for example, the control of access authorizations, multiple representation in CAO and Geographical Info [...] ...|$|E
40|$|Abstract This paper {{presents}} the concurrent object oriented model embedded in SYNERGY and an application that illustrates {{this use of}} the language. Basic features of the SYNERGY ’ concurrent object oriented model are: 1) an active object, with its message queue, its <b>declarative</b> <b>part</b> and its procedural part (script + body) {{is described as a}} labeled context, 2) both “coupled ” communication mode (an object communicates directly with another object) and “non-coupled ” communication mode (objects share and communicate via a common space) are provided, 3) objects types form a hierarchy used by inheritance mechanism and, 4) both inter- and intra-concurrency are provided; several objects can be active in parallel and several methods of an object can be executed in parallel. This paper shows also that the integration of the concurrent object oriented model to SYNERGY is based on the use of key elements as CG structure, contexts and coreferences. 1...|$|E
40|$|Workflows are {{composite}} {{activities that}} achieve interoperation {{of a variety}} of system and human tasks. Workflows must satisfy subtle domain-specific integrity and organizational requirements. Consequently, flexibility in execution is crucial. A promising means to achieve flexibility is through <b>declarative</b> specifications (<b>Part</b> 1) with automatic distributed scheduling techniques (Part 2). We address the problem of scheduling workflows from declarative specifications given in terms of intertask dependencies and event attributes. Our approach involves distributed events, which are automatically set up to exchange the necessary messages. Our approach uses symbolic reasoning to (a) determine the initial constraints on events or guards, (b) preprocess the guards, and (c) execute the events. It has been implemented. workflows, temporal logic, scheduling. This is a greatly extended and revised version of a paper that appears in the Proceedings of the International Conference on Data [...] ...|$|R
40|$|We {{introduce}} a declarative approach to kernel design based on background knowledge {{expressed in the}} form of logic programs. The theoretical foundation of declarative kernels is mereotopology, a general theory for studying parts and wholes and for defining topological relations among <b>parts.</b> <b>Declarative</b> kernels can be used to specify a broad class of kernels over relational data and represent a step towards bridging statistical learning and inductive logic programming. The flexibility and the effectiveness of declarative kernels is demonstrated in a number of real world problems. ...|$|R
40|$|Despite its rising {{popularity}} as {{data format}} especially for web services, the software ecosystem around the JavaScript Object Notation (JSON) {{is not as}} widely distributed as that of XML. For both data formats there exist schema languages to specify the structure of instance documents, but there is currently no opportunity to translate already existing XML Schema documents into equivalent JSON Schemas. In this paper we introduce an implementation of a language translator. It takes an XML Schema and creates its equivalent JSON Schema document. Our approach is based on Prolog and CHR. By unfolding the XML Schema document into CHR constraints, {{it is possible to}} specify the concrete translation rules in a <b>declarative</b> way. Comment: <b>Part</b> of CHR 2014 proceedings (arXiv: 1406. 1510...|$|R
40|$|The {{purpose of}} the present paper is to propose a new {{approach}} to representing complex objects which are used as object identity in the context of object-orientation and to give a <b>declarative</b> <b>part</b> of its semantics on the domain of labeled graphs. QUIXOT E[8] is a knowledge representation language based on the complex objects proposed here, and provides knowledge representation and inference services, including representation of partial information, merging (unification) of partial information, inheritance of properties, and so on. Our metatheory is ZFC/AFA, a set theory proposed by P. Aczel[1] which brings to bear all of the familiar set-theoretic techniques to deal with circular phenomena. Barwise[2] and Mukai[5],[4] shows important results on constraints over hypersets. Actually, the part on constraints over labeled graphs in the present paper much owes to Mukai's work. 1 Introduction Complex data structure is used for various data and knowledge representation in many application: c [...] ...|$|E
40|$|Formal {{semantics}} is {{a convenient}} tool to equip a model transformation language with precise meaning for its model transformations. Hence, clarifying their usage in complex scenarios and helping {{in the development}} of robust model transformation engines. In this paper, we focus on the formal specification of a model transformation engine for the <b>declarative</b> <b>part</b> of ATL. We present an implementation-agnostic, big-step, structural operational semantics for ATL transformation rules and a rule scheduler, which form the specification of an interpreter for ATL. Hence, avoiding a complex compilation phase. The resulting semantics for rules enjoys a compositional nature and we illustrate its advantages by reusing an interpreter for OCL. The semantics discussed has been validated with the implementation of an interpreter in Maude, enabling the execution of model transformations and their formal analysis using Maude’s toolkit. We also present an evaluation of the interpreter’s performance and scalability. Post-printICMT ’ 17 : 10 th International Conference on Model Transformation, Marbur...|$|E
40|$|Existing {{systems to}} acquire {{knowledge}} from expository texts do not perform any learning beyond interpreting {{the contents of}} the text. The opportunity to learn from examples included in texts is not exploited. This is a needless limitation because examples in texts usually show the reader how to integrate the <b>declarative</b> <b>part</b> of the text into an operational concept or procedure. Explanation-based Learning (EBL) seems to fill this gap as it explains the example within the domain theory, generalizes the explanation and operationalizes the concept definition by compiling necessary knowledge from the domain theory into the definition. In this paper, we study the synergistic combination of automatic text analysis and EBL. EBL is used realistically, where the domain theory and the training examples are obtained from a specification or a regulation by a text analysis program, rather than being given a priori. We present a prototype system which demonstrates the potential of this approach. The [...] ...|$|E
40|$|Database {{applications}} are typically written using {{a mixture of}} imperative languages and declarative frameworks for data processing. Application logic gets distributed across the <b>declarative</b> and imperative <b>parts</b> of a program. Often, {{there is more than}} one way to implement the same program, whose efficiency may depend on a number of parameters. In this paper, we propose a framework that automatically generates all equivalent alternatives of a given program using a given set of program transformations, and chooses the least cost alternative. We use the concept of program regions as an algebraic abstraction of a program and extend the Volcano/Cascades framework for optimization of algebraic expressions, to optimize programs. We illustrate the use of our framework for optimizing database applications. We show through experimental results, that our framework has wide applicability in real world applications and provides significant performance benefits...|$|R
40|$|International audienceCapture devices rise {{large scale}} {{trajectory}} data from moving objects. These devices use different technologies like global navigation satellite system (GNSS), wireless communication, radio-frequency identification (RFID), and other sensors. Huge trajectory {{data are available}} today. In this paper, we use an ontological data modeling approach to build a trajectory ontology from such large data. This ontology contains temporal concepts, so we map it to a temporal ontology. We present an implementation framework for <b>declarative</b> and imperative <b>parts</b> of ontology rules in a semantic data store. An inference mechanism is computed over these semantic data. The computational time and memory of the inference increases very rapidly {{as a function of}} the data size. For this reason, we propose a two-tier inference filters on data. The primary filter analyzes the trajectory data considering all the possible domain constraints. The analyzed data are considered as the first knowledge base. The secondary filter then computes the inference over the filtered trajectory data and yields to the final knowledge base, that the user can query...|$|R
40|$|This {{chapter is}} an esemplastic systematization of {{declarative}} computational cognitive modeling, {{a field that}} cuts across cognitive modeling based on cognitive architectures (such as ACT-R, Soar, and Clarion), human-level artificial intelligence (AI), logic itself, and psychology of reasoning (especially of the computational kind). The hallmarks of declarative computational cognitive modeling are the following two intertwined constraints: (1) The central units of information used in the approach are (at least in significant <b>part)</b> <b>declarative</b> in nature, and the central process carried out over these units is inference. (2) The approach to modeling the mind is top-down, rather than bottom-up. (These two points are interconnected because once one commits to (1), (2) becomes quite unavoidable, since bottom-up processing in the brain, as reflected in relevant formalisms (e. g., artificial neural networks), is based on units of information that are numerical, not declarative.) The systematization of declarative computational cognitive modeling is achieved by using formal logic, and hence declarative computational cognitive modeling, from the formal perspective, becomes logic-based computational cognitive modeling (LCCM). The chapter covers some prior research that falls under LCCM; this research {{has been carried out}} b...|$|R
40|$|This paper {{presents}} work on programming methodologies for {{the programming}} tool GCLA. Three methods are discussed which show how {{to construct the}} control part of a GCLA program, where {{the definition of a}} specific problem and the set of intended queries are given beforehand. The methods are described by a series of examples, but we also try to give a more explicit description of each method. We also discuss some important characteristics of the methods. 1 Introduction This paper contributes to the as yet poorly known domain of programming methodology for the programming tool GCLA. A GCLA program consists of two separate parts; a <b>declarative</b> <b>part</b> and a control part. When writing GCLA programs we therefore have to answer the question: "Given a definition of a specific problem and a set of queries, how can we construct the control knowledge that is required for the resulting program to have the intended behavior?" Of course there is no definite answer to this question, new problems [...] ...|$|E
40|$|This paper {{describes}} the logic programming language GCLA II, its operational semantics {{and parts of}} its theoretical foundations. GCLA II is a generalization of the language GCLA (Generalized Horn Clause Language) augmented by a method to guide and constrain proof search. The method is based on specification of strategies in a meta language that is a sub language of GCLA itself. A GCLA II program is partitioned into two distinct parts. One is used to express the declarative content of the program, while the other is used to define the possible inferences made from this declarative knowledge. Although the intended use of the <b>declarative</b> <b>part</b> and the procedural parts are quite different, they can both be described in the formalism of partial inductive definitions. Thus we preserve a declarative reading of the program as a whole. In particular, given certain syntactical constraints on the program, the heuristics associated with proof search {{does not affect the}} declarative reading of th [...] ...|$|E
40|$|This work {{proposes a}} {{language}} for describing reactive synthesis problems that integrates imperative and declarative elements. The semantics {{is defined in}} terms of two-player turn-based infinite games with full information. Currently, synthesis tools accept linear temporal logic (LTL) as input, but this description is less structured and does not facilitate the expression of sequential constraints. This motivates the use of a structured programming language to specify synthesis problems. Transition systems and guarded commands serve as imperative constructs, expressed in a syntax based on that of the modeling language Promela. The syntax allows defining which player controls data and control flow, and separating a program into assumptions and guarantees. These notions are necessary for input to game solvers. The integration of imperative and declarative paradigms allows using the paradigm that is most appropriate for expressing each requirement. The <b>declarative</b> <b>part</b> is expressed in the LTL fragment of generalized reactivity(1), which admits efficient synthesis algorithms. The implementation translates Promela to input for the Slugs synthesizer and is written in Python...|$|E
40|$|Abstract. While {{the idea}} of {{declarative}} debugging {{has been around for}} a quarter of a century, the technology still hasn’t been adopted by working programmers, even by those working in declarative languages. The reason is that making declarative debuggers practical requires solutions to a whole host of problems. In this paper we address one of these problems, which is that retaining a complete record of every step of the execution of a program is infeasible unless the program’s runtime is very short, yet this record forms the space searched by the <b>declarative</b> debugger. Most <b>parts</b> of this search space therefore have to be stored in an implicit form. Each time the search algorithm visits a previously unexplored region of the search space, it must decide how big a part of the search space to rematerialize (which it does by reexecuting a call in the program). If it materializes too much, the machine may start to thrash or even run out of memory and swap space. If it materializes too little, then materializing all the parts of the search space required by a debugging session will require too many reexecutions of (parts of) the program, which will take too long. We present a simple algorithm, the ideal depth strategy, for steering the ideal middle course: minimizing reexecutions while limiting memory consumption to what is feasible. We show that this algorithm performs well even when used on quite long running programs. ...|$|R
40|$|Abstract] One of {{the most}} {{important}} conditions for the adequacy of communication process is, as it’s well known, the intentionality, by which the information between transmitter and receiver takes place. This is the case for verbal component of communication. What about the nonverbal one, more exactly, do we consciously use, or do us not this language? And because the intentionality is based on conscious character of our inter-relations, {{it can be said that}} the communication occurs only when the transmitter send intentionally a message to the receiver. This would include speech and some nonverbal behaviors, such as different aspects pointing or attention maintaining. From the producing and receiving signs perspective, the intentional and conscious behavior might be differently approached. The tacit dimensions of communication may have an enormous impact for interlocutor further behavior and reactions. As Paul Watzlawick stated, the behavior might be “non-communicative” only if there is no other person around (in one way or another). Once the other person is present, the whole behavior becomes communicative. The paper aims to identify an answer to the following question: to what extent can we talk about intentionality, by referring to posture, gestures, and facial expressions, shortly, to nonverbal communication component? This is justified by different researches’ points of view: those who support this possibility and those ones who amend it. Another issue the present paper aims to approach is that that people send messages about them through body language, but not in a <b>declarative</b> manner, as <b>part</b> of a discourse; this raises some questions about nonverbal valence...|$|R
40|$|First {{works on}} {{constraint}} satisfaction problems (CSP) {{can be traced}} in the 70 ’th but their widespread study and application started {{since the end of}} 80 ’th when the extension of unification algorithm of logic programming led to the proposal of constraint logic programming (CLP) scheme. Introductory part of the thesis introduces both proposals as approaches allowing to express properties of the problem declaratively by means of constraints and to search for its solution via specialized algorithms. Presented thesis is focused on CSPs on finite domains because of their practical applicability. Generally 95 % of all industrial constraint-based applications are formulated as CSPs on finite domains. Constraint programming is applied to solve wide range of problems, among them scheduling, configuration, hardware verification, graphical interfaces, or molecular biology. However, in most real-life situations we need to express fuzziness, possibilities, probabilities, costs, [...] . While problems may become over-constrained it does not make sense to say there is no solution. Many problems require finding of best or optimal solution wrt. one or even more optimization criteria. Others may be ill-defined and we need to include uncertainty of the problem definition into final solution. All these problems need to apply some type of preferences which have to be included into both <b>declarative</b> and control <b>part</b> of the solution. That led us to the study of CSPs with preferences from both theoretical (proposal of new methodologies and uniform view to the existing ones) and practical (their application in timetabling area) points of view within this thesis. ...|$|R
40|$|This paper {{investigates the}} {{phenomenon}} of stand-up comedians performing {{in more than one}} language, which poses the question of whether and how they translate their material. Past research on stand-up comedy underlines its conversational nature, sometimes at the expense of recognizing its content. Empirical evidence collected from interviews with bilingual stand-up comedians, on the other hand, suggests that they perform a form of oral self-translation, which implies a tertium comparationis, the transfer of content. The notion of mental text, borrowed from ethnography, is then productively used to define this content. As is then suggested, two types of memory, namely declarative and procedural, are involved in the memorization of this mental text. The <b>declarative</b> <b>part</b> accounts for what is repeatable across performances and is the part involved in conscious translation; its minimal content is identified in the punch lines. The procedural part accounts for variation, improvisation and interaction. A model of the oral-self translation process of stand-up is then proposed. I conclude that refocusing on the (mental) text of stand-up comedy can offer a better understanding of its translation, which in turn can contribute {{to a better understanding of}} humor in a multilingual and multicultural context in future research...|$|E
40|$|The {{meanings}} {{and functions of}} the Chinese modal particle a and ba in yes-no questions have long been issues debated by scholars of Chinese linguistics and grammar. This data-oriented empirical study {{has led to the}} following conclusions: 1) Particle a in yes-no questions has “Surprisive” meaning as its core meaning and “Non-affirmative” meaning as derived from the core; 2) Particle ba in yes-no questions conveys the meaning of speaker’s “Non-assertiveness”, i. e. the speaker’s lack of total confidence in the truth of what is presented in the <b>declarative</b> <b>part</b> of the sentence before the particle. Both a-ending and ba-ending questions are confirmation-seeking questions in use, but the former is triggered by things in the speech context that are surprising or arousing a feeling of uncertainty to the speaker; the latter is triggered by the speaker’s lack of total confidence in his/her own judgment or inference about a situation, the truth of which he/she does not know. The interrogative function or question force that a or ba can add to a sentence is attributive to the identified inherent modal meanings of the particles and the contexts in which the speaker uses them...|$|E
40|$|This paper {{proposes a}} {{language}} for describing reactive synthesis problems that integrates imperative and declarative elements. The semantics {{is defined in}} terms of two-player turn-based infinite games with full information. Currently, synthesis tools accept linear temporal logic (LTL) as input, but this description is less structured and does not facilitate the expression of sequential constraints. This motivates the use of a structured programming language to specify synthesis problems. Transition systems and guarded commands serve as imperative constructs, expressed in a syntax based on that of the modeling language PROMELA. The syntax allows defining which player controls data and control flow, and separating a program into assumptions and guarantees. These notions are necessary for input to game solvers. The integration of imperative and declarative paradigms allows using the paradigm that is most appropriate for expressing each requirement. The <b>declarative</b> <b>part</b> is expressed in the LTL fragment of generalized reactivity(1), which admits efficient synthesis algorithms, ex-tended with past LTL. The implementation translates PROMELA to input for the SLUGS synthesizer and is written in PYTHON. The AMBA AHB bus case study is revisited and synthesized efficiently, identifying the need to reorder binary decision diagrams during strategy construction, in order to prevent the exponential blowup observed in previous work. ...|$|E
40|$|Automating {{business}} processes {{has been a}} popular business strategy {{in the past few}} years. These processes were modeled using an imperative modeling language. A known issue of this way of modeling is that it is dificult to model exible parts of processes, since every sequence needs to be modeled explicitly. Flexible parts or parts that need user input of a process need to be made structural {{in order to be able}} to automate them. On the other hand, with a declarative modeling language it is not convenient to model structured parts of a process. A modeling approach that is able to model both the structured and exible parts does not exist. This thesis presents a hybrid process modeling approach that addresses this issue. The hybrid modeling language consists of a combination of the imperative modeling language BPMN and declarative modeling language Declare. It allows the user to jointly model exible and structured parts of a process. A method is developed to create a hybrid process from a process description and guidelines are created to help the user decide between modeling a group of activities using the imperative or <b>declarative</b> <b>part</b> of the hybrid modeling language. An empirical experiment is conducted to evaluate the set of guidelines. The analysis of the results indicates that these guidelines are found useful, but the `ease-of-use 2 ̆ 6 apos; aspect needs improvement. Automating {{business processes}} has been a popular business strategy in the past few years. These processes were modeled using an imperative modeling language. A known issue of this way of modeling is that it is dificult to model exible parts of processes, since every sequence needs to be modeled explicitly. Flexible parts or parts that need user input of a process need to be made structural in order to be able to automate them. On the other hand, with a declarative modeling language it is not convenient to model structured parts of a process. A modeling approach that is able to model both the structured and exible parts does not exist. This thesis presents a hybrid process modeling approach that addresses this issue. The hybrid modeling language consists of a combination of the imperative modeling language BPMN and declarative modeling language Declare. It allows the user to jointly model exible and structured parts of a process. A method is developed to create a hybrid process from a process description and guidelines are created to help the user decide between modeling a group of activities using the imperative or <b>declarative</b> <b>part</b> of the hybrid modeling language. An empirical experiment is conducted to evaluate the set of guidelines. The analysis of the results indicates that these guidelines are found useful, but the `ease-of-use 2 ̆ 6 apos; aspect needs improvement...|$|E
40|$|We {{present in}} this paper a {{methodology}} for the industrial Automation Engineer to help him design, build and run computer-based systems containing, physical and software parts. This methodology enables him to define a system through its requirements and its specifications, and it allows him to validate and then to run it (for instance on a PLC, a Programmable Logical Controller). This methodology {{is based on the}} description of the computer based systems in terms of hierarchical temporal-components, which are described as synchronous subsystems validated through the use of temporal logic. The concept of temporal-component is an extension of the concept of objects, it is made of four parts: (i) a <b>declarative</b> <b>part</b> giving the links with other components, the signals and the variables, (ii) a set of local operations (hard/soft), (iii) a requirements part (named ‘Goals') and (iv) a working specification part (named 'Behavior ’ or ‘Controller’), these two last parts are written using an "engineering dialect " of the Temporal logic. The validation process consists in proving that the Behavior specification of the whole embedded-system satisfies its main goal using all the subcomponents ' Goals (for this, one can use an automatic prover such as the Stanford's STEP prover). These Temporal logic requirements and specifications can easily be translated in Ladder Diagrams and run on a PLC (Programmable Logical Controller) using for instance the international standard IEC 1131 - 3. Our "engineering dialect " of the Temporal logic is built so that the translation rules from the TCOM notation to provable Temporal Logic and also to executable Ladder Diagrams (PLC) are simple. A bottle filler system example is presented...|$|E
40|$|The {{subject of}} studies is Poland’s policy towards Russia, Ukraine and Belarus {{in the years}} 1989 – 2010. It {{encompasses}} analysis of the bilateral diplomatic relations {{from the point of}} view of political science. It is not an attempt at a comprehensive presentation of the problem but only an interpretation of the aspect connected with strictly political relations, which are the most dynamic element of the eastern policy of Poland. The other components, phenomena and events connected with Polish eastern policy were presented as a background or to supplement the cause-and-effect process. The formation of the first conceptions of Polish eastern policy and the place in it of Russia, Ukraine and Belarus was burdened with the necessity to take into consideration the still existing USSR. It was only after its fall that making foreign policy without this “extra” burden was possible. Against all appearances starting diplomatic relations with eastern neighbours was not easy and, at the beginning, hopes were accompanied by fears connected with the persistent domination of the historical issues. Their exposing in the first half of the 1990 s in case of Poland’s political relations with Russia as well as with Ukraine and Belarus, became a source of the first conflicts between Poland and these three countries. Hope and endeavours to make friendly political relations strongly corresponded with the fears and threats which were perceived in the bilateral relations. Polish authorities faced a difficult task of building foreign policy and defining goals and methods of their realisation in relations with eastern neighbours. The purpose of this book is to present as objectively and credibly as possible the long process of formation and evolution of Polish-Russian, Polish-Ukrainian and Polish-Belarusian relations. Political facts and events, which have significantly influenced Poland’s policy towards eastern neighbours, were taken into account. Poland’s political situation, the advancing evolution of goals of Polish foreign policy, coalition governments and their programmes of co-operation with eastern partners, international and many other factors which have an important impact on Poland’s eastern policy were submitted to a thorough analysis {{from the point of view}} of their positive and negative impact on bilateral political relations with Russia, Ukraine and Belarus. Another goal of this publication is to show to its readers the current state of bilaterial relations of Poland with Russia, Ukraine and Belarus and to answer the question whether during the last two decades Polish authorities have done everything that was possible to exist today in a friendly international environment to the East of its borders. The aim of this work is also to take into consideration the question of how Russia, Ukraine and Belarus perceive Polish eastern policy and its influence on the state of Poland’s bilateral relations with these countries. This book consists of five chapters. Chapter One deals with the question of defining foreign policy and the process of making foundations of Poland's eastern policy in the new geopolitical situation. In his analysis, the author aims to explain the concept and essence of foreign policy and, its varying conditions, the process of setting goals in foreign policy and the question of methods and means applied in foreign policy. An important task of the book was to try to define the concept of raison d’état, which is too often overused in foreign policies. The subject of analysis were the evolutional process of formulating goals of eastern policy and the influence of the question of security of the state and priorities of foreign policy. The “two-way policy” towards Russia, Ukraine and Belarus, the crisis of eastern policy in the years 1993 – 1995 and the question of integration of Poland with the European Union and its impact on the goals set for foreign eastern policy were also presented. The subject of Chapter Two is the analysis of Poland’s policy towards the Russian Federation. An important issue was to answer the question: Why in Poland’s Russian policy the factor of threat dominated excessively and why the Russian partner was treated as an opponent in the region? The process of evolutional formation of geopolitical divergencies, lack of opportunities for a compromise as far as the issue of European security is concerned and the policy of crises in the Polish-Russian relations. An importatnt question was to show the role and significance of the Kaliningrad Region as a factor which has a negative influence on the Polish-Russian relations. In order to verify the initial hypotheses it was important to give an answer as to the causes of the defensive character of Poland’s policy towards Russia. In Chapter Three the author analysed Poland’s policy towards Ukraine, trying to answer the question of what was the Polish-Ukrainian “strategic partnership”? Whether it was really the result of special ties and interests uniting the two countries, or only a media and purely <b>declarative</b> <b>part</b> of Poland's policy towards the Ukrainian partner? Why did Ukraine so strongly strive to form a political and military alliance with Poland in the first half of the 90 s of the 20 th century and why it met refusal on the part of Poland? An important factor which helps to better understand the state and character of Polish-Ukrainian political relations was an analysis of the historical policy in mutual relations and its influence on the perception of bilateral relations. Poland’s policy towards Belarus is disccused in Chapter Four. An element which should be analysed was underestimation by the Polish party in its policy towards the Belorusian party of their fears connected with Polonisation and activities of the Catholic Church. Initially, issues which were of slight importance to Poland, later became the main reason of serious divergencies in bilateral relations. Also of importance is the question of the awareness of the Polish party of the change in the geopolitical future of Belarus because of Poland’s accession to the NATO. Political changes in Belarus in the years 1994 – 1996 were analysed in this work as well as their influence on the deterioration of mutual relations. It was also important to answer the question about the reasons of ineffectiveness of the policy of critical dialogue towards Belarus and causes of the conflict concerning Polish minority in 2005. Chapter Five contains an analysis of the current development of Poland’s policy towards Russia, Ukraine and Belarus and presents short- and long-term perspectives of further development of Polish eastern policy. Attention was paid to the perspectives of Polish policy towards each of the eastern neighbours in case of realisation of different variants of opportunities and threats. It was important how external and internal factors of Russia, Ukraine and Belarus generate challenges, threats and opportunities for Poland's eastern policy. The author took up this subject, encouraged by his wish to draw attention to the important role of Polish eastern policy during the past two decades in the realisation of the main goals of foreign policy and the security of the state. Initially, left out of consideration and underestimated, eastern policy began to be treated as an important and indispensable element of the foreign policy of the state only after the events of “the Orange Revolution” in Ukraine in 2004. Another factor, which motivated the author to take up this subject, is a still relatively small number of studies on the subject. Attention should be drawn to the fact that the output of works which directly deal with Polish eastern policy and bilateral relations of Poland with Russia, Ukraine and particularly with Belarus is still rather small. In the available literature, works devoted to historical aspects dominate, while decisively there is a shortage of monographs written from the point of view of political science. Although there are many scientific articles in collections of articles and studies, scientific journals and conference proceedings, they still do not offer a full diagnosis of the state of mutual relations of Poland with Russia, Ukraine and Belarus...|$|E
