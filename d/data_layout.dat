678|377|Public
25|$|Note {{that the}} <b>data</b> <b>layout</b> in dual link is {{designed}} so that the primary link can be fed into a single-link interface, and still produce usable (though somewhat degraded) video. The secondary link generally contains things like additional LSBs (in 12-bit formats), non-cosited samples in 4:4:4 sampled video (so that the primary link is still valid 4:2:2), and alpha or data channels. If the second link of a 1080P dual link configuration is absent, the first link still contains a valid 1080i signal.|$|E
5000|$|<b>Data</b> <b>layout</b> and {{decomposition}} {{is handled}} automatically by directives.|$|E
5000|$|... {{ensure that}} {{everyone}} {{uses the same}} version of a <b>data</b> <b>layout</b> definition or procedural code throughout a program.|$|E
30|$|In this work, {{the major}} {{research}} {{objective is to}} evaluate the impact of <b>data</b> <b>layouts</b> on the efficiency of GPU-accelerated IDW interpolation. Before describing our GPU-accelerated implementations of the IDW algorithm, in this section we first briefly introduce {{the principal of the}} underlying algorithm, IDW interpolation, and commonly used <b>data</b> <b>layouts.</b>|$|R
40|$|Parity-declustered <b>data</b> <b>layouts</b> were {{developed}} to reduce the time for on-line failure recovery in disk arrays. They generally require perfect balancing of reconstruction workload among the disks; this restrictive balance condition makes such <b>data</b> <b>layouts</b> difficult to construct. In this paper, we consider approximately balanced <b>data</b> <b>layouts,</b> where some variation in the reconstruction workload over the disks is permitted. Such layouts are considerably easier to construct than perfectly balanced layouts. We consider three methods for constructing approximately balanced <b>data</b> <b>layouts,</b> and analyze their performance both theoretically and experimentally. We conclude that on uniform workloads, approximately balanced layouts have performance nearly identical to that of perfectly balanced layouts. A preliminary {{version of this paper}} appears in the Proceedings of the Fourth Annual Workshop on I/O in Parallel and Distributed Systems, pp. 41 [...] 54, May 1996. y schwabe@eecs. nwu. edu. Supported by N [...] ...|$|R
5000|$|... {{save time}} by not needing to code {{extensive}} <b>data</b> <b>layouts</b> (minor, but useful).|$|R
50|$|Data {{oriented}} {{design is}} an approach intended to maximise the locality of reference, by organising data {{according to how}} it is traversed {{in various stages of}} a program, contrasting with the more common object oriented approach (i.e. organising such that <b>data</b> <b>layout</b> explicitly mirrors the access pattern).|$|E
5000|$|Contours and isosurfaces can be {{extracted}} from all data types using scalars or vector components. The {{results can be}} colored by any other variable or processed further. When possible, structured data contours/isosurfaces are extracted with fast and efficient algorithms which {{make use of the}} efficient <b>data</b> <b>layout.</b>|$|E
50|$|NimbleOS is Nimble's {{operating}} system {{heavily dependent on}} a patented file-system architecture, cache accelerated sequential layout (CASL). NimbleOS includes: flexible flash scaling, adaptive flash service levels, dynamic flash-based read caching, write-optimized <b>data</b> <b>layout,</b> inline compression, scale-to-fit flexibility, scale out, snapshots and integrated data protection, efficient replication, deduplication, and zero-copy clones.|$|E
30|$|This {{paper is}} {{organized}} as follows. “Background” section gives a brief {{introduction to the}} IDW interpolation and two basic <b>data</b> <b>layouts,</b> the SoA and AoS. “GPU implementations” section concentrates on the GPU implementations that are performed by using five different <b>data</b> <b>layouts.</b> “Results and discussion” section presents some experimental tests that are performed on single and/or double precision, and discusses the experimental results. Finally, “Conclusion” section draws some conclusions.|$|R
3000|$|Implementing five {{groups of}} those three GPU {{versions}} based upon five different <b>data</b> <b>layouts</b> on both single and/or double precision; [...]...|$|R
40|$|AbstractRecently, parity-declustered layouts {{have been}} studied {{as a tool for}} {{reducing}} the time needed to reconstruct a failed disk in a disk array. Construction of such layouts for large disk arrays generally involves the use of a balanced incomplete block design (BIBD), a type of subset system over the set of disks. This research has been somewhat hampered by the dearth of effective, easily implemented constructions of BIBDs on large sets and by inefficiencies in some parity-distribution methods that create layouts that are larger than necessary. We make progress on these problems in several ways. In particular, we•demonstrate new BIBD constructions that generalize some previous constructions and yield simpler BIBDs that are optimally small in some cases,•show how relaxing some of the balance constraints on <b>data</b> <b>layouts</b> leads to constructions of approximately-balanced layouts that greatly increase the number of feasible layouts for large arrays, and•give a new method for distributing parity that produces smaller <b>data</b> <b>layouts,</b> resulting in tight bounds on the size of <b>data</b> <b>layouts</b> derived from BIBDs. Our results use a variety of algebraic, combinatorial, and graph-theoretic techniques, and together greatly increase the number of parity-declustered <b>data</b> <b>layouts</b> that are appropriate for use in large disk arrays...|$|R
5000|$|There {{are some}} key words {{related to this}} norm: Securities, Messages, Dictionaries, Databases, <b>Data</b> <b>layout,</b> Data organization, Data processing, banking documents, finance, {{electronic}} data interchange, Electronic messaging, Syntax, Character sets, Control characters, Information separators, Sets of data, Data blocks, Codes, Numerical designations, Tags (data processing), Data, Data elements, Coding (data conversion), Access, EDIFACT, Data transmission ...|$|E
50|$|SDXF {{data can}} express {{arbitrary}} levels of structural depth. Data elements are self-documenting, {{meaning that the}} metadata (numeric, character string or structure) are encoded into the data elements. The design of this format is simple and transparent: computer programs access SDXF data {{with the help of}} well-defined functions, exempting programmers from learning the precise <b>data</b> <b>layout.</b>|$|E
50|$|In computing, data-oriented design (not to be {{confused}} with data-driven design) is a program optimization approach motivated by cache coherency, used in video game development (usually in the programming languages C or C++). The approach is to focus on the <b>data</b> <b>layout,</b> separating and sorting fields according to when they are needed, and to think about transformations of data. Proponents include Mike Acton.|$|E
40|$|Driven by {{the goals}} of {{efficient}} and generic communication of noncontiguous <b>data</b> <b>layouts</b> in GPU memory, for which solutions do not currently exist, we present a parallel, noncontiguous data-processing methodology through the MPI datatypes specification. Our processing algorithm utilizes a kernel on the GPU to pack arbitrary noncontiguous GPU data by enriching the datatypes encoding to expose a fine-grained, data-point level of parallelism. Additionally, the typically tree-based datatype encoding is preprocessed to enable efficient, cached access across GPU threads. Using CUDA, we show that the computational method outperforms DMA-based alternatives for several common <b>data</b> <b>layouts</b> {{as well as more}} complex <b>data</b> <b>layouts</b> for which reasonable DMA-based processing does not exist. Our method incurs low overhead for <b>data</b> <b>layouts</b> that closely match best-case DMA usage or that can be processed by layout-specific implementations. We additionally investigate usage scenarios for data packing that incur resource contention, identifying potential pitfalls for various packing strategies. We also demonstrate the efficacy of kernel-based packing in various communication scenarios, showing multifold improvement in point-to-point communication and evaluating packing {{within the context of the}} SHOC stencil benchmark and HACC mesh analysis...|$|R
30|$|Evaluating {{the impact}} of five <b>data</b> <b>layouts</b> on the {{computational}} efficiency of three versions of GPU-accelerated IDW interpolation on single and/or double precision.|$|R
40|$|This paper {{presents}} a concurrent object model based on distributed recursive sets for data intensive applications that use complex, recursive <b>data</b> <b>layouts.</b> The set abstraction {{is used to}} represent irregular (recursive) <b>data</b> <b>layouts.</b> The distributed set abstraction is used to transparently distribute large data across multiple address spaces. We effectively map data to processors by using using information about data accesses to preserve locality. This results in a consistency scheme with small communication requirements. The programming model is easy to use since the system automatically does the data distribution, mapping and consistency. 1...|$|R
50|$|The {{experimental}} {{programming language}} JAI {{being developed by}} Jonathan Blow has explicit support for data-oriented design, while eschewing the traditional OOP paradigm. This is facilitated {{by being able to}} transparently move fields between records without extensive source code changes to functions using them (or without extensive boilerplate code to enable this), and by adding direct support for structure of arrays (SoA) <b>data</b> <b>layout.</b>|$|E
50|$|Vector {{processors}} {{use this}} technique with one additional trick. Because the <b>data</b> <b>layout</b> is in a known format — {{a set of}} numbers arranged sequentially in memory — the pipelines can be tuned to improve the performance of fetches. On the receipt of a vector instruction, special hardware sets up the memory access for the arrays and stuffs the data into the processor as fast as possible.|$|E
5000|$|... eXtremeDB Financial Edition {{provides}} {{features for}} managing market data (tick data) in {{applications such as}} algorithmic trading and order matching. [...] A “sequences” data type supports columnar <b>data</b> <b>layout</b> and enables eXtremeDB to offer {{the benefits of a}} column-oriented database in handling time series data. The Financial Edition also provides a library of vector-based statistical functions to analyze data in sequences, and a performance monitor.|$|E
30|$|This {{result is}} due to the fact that the accesses to global memory have been {{optimized}} using the strategy “tiling” and the impact of different <b>data</b> <b>layouts</b> on accessing global memory is not significant.|$|R
30|$|In this paper, {{the naive}} version {{presented}} in Huraj et al. (2010), the tiled version developed in Mei (2014), and the improved CDP version described above are accepted {{to be implemented}} according to five <b>data</b> <b>layouts</b> for benchmark tests on single precision and/or double precision.|$|R
30|$|On single precision, we {{implement}} {{three groups}} of the GPU implementations using three types of <b>data</b> <b>layouts,</b> including the SoA, the AoS, and the AoaS. We find that: for the AoS and AoaS layouts, the second one always obtains better performance than the first.|$|R
50|$|The UPC {{language}} {{evolved from}} experiences {{with three other}} earlier languages that proposed parallel extensions to ISO C 99: AC, Split-C, and Parallel C Preprocessor (PCP). UPC is not a superset of these three languages, but rather an attempt to distill the best characteristics of each. UPC combines the programmability advantages of the shared memory programming paradigm and the control over <b>data</b> <b>layout</b> and performance of the message passing programming paradigm.|$|E
50|$|CSO {{provides}} nine {{levels of}} compression. While {{the highest levels}} of compression can introduce slowdown and lengthy load-times in software which relies heavily on disc streaming, even the lower levels are capable of substantial compression. This is partially due to the <b>data</b> <b>layout</b> of a UMD, though more frequently due to the use of Dummy Files as both an anti-piracy tool and a means to more optimally lay the data out physically on the disc.|$|E
50|$|<b>Data</b> <b>layout</b> is {{critical}} for correctly passing arrays between programs written in different programming languages. It is also important for performance when traversing an array because modern CPUs process sequential data more efficiently than nonsequential data. This is primarily due to CPU caching. Contiguous access makes it also possible to use SIMD instructions that operate on vectors of data. In some media such as tape or NAND flash memory, accessing sequentially is orders of magnitude faster than nonsequential access.|$|E
30|$|This paper {{focuses on}} {{evaluating}} {{the impact of}} different <b>data</b> <b>layouts</b> on the computational efficiency of GPU-accelerated Inverse Distance Weighting (IDW) interpolation algorithm. First we redesign and improve our previous GPU implementation that was performed by exploiting the feature of CUDA dynamic parallelism (CDP). Then we implement three versions of GPU implementations, i.e., the naive version, the tiled version, and the improved CDP version, based upon five <b>data</b> <b>layouts,</b> including the Structure of Arrays (SoA), the Array of Structures (AoS), the Array of aligned Structures (AoaS), the Structure of Arrays of aligned Structures (SoAoS), and the Hybrid layout. We also carry out several groups of experimental tests to evaluate the impact. Experimental results show that: the layouts AoS and AoaS achieve better performance than the layout SoA for both the naive version and tiled version, while the layout SoA is {{the best choice for}} the improved CDP version. We also observe that: for the two combined <b>data</b> <b>layouts</b> (the SoAoS and the Hybrid), there are no notable performance gains when compared to other three basic layouts. We recommend that: in practical applications, the layout AoaS is the best choice since the tiled version is the fastest one among three versions. The source code of all implementations are publicly available.|$|R
30|$|For {{the naive}} version, all the <b>data</b> <b>layouts</b> except the SoA achieve {{nearly the same}} speedups, i.e., about 11.88. Noticeably, among these four layouts, i.e., the AoS, the AoaS, the SoAoS, and the Hybrid, the best one is the AoS, in which the {{alignment}} is not used.|$|R
30|$|On double precision, we {{implement}} {{two groups}} of those three GPU implementations using additional two types of combined <b>data</b> <b>layouts,</b> the SoAoS and the Hybrid; we also implement the GPU implementations using the layouts SoA, AoS, and AoaS. The experimental results in this case are presented in Fig.  8.|$|R
50|$|Note {{that the}} <b>data</b> <b>layout</b> in dual link is {{designed}} so that the primary link can be fed into a single-link interface, and still produce usable (though somewhat degraded) video. The secondary link generally contains things like additional LSBs (in 12-bit formats), non-cosited samples in 4:4:4 sampled video (so that the primary link is still valid 4:2:2), and alpha or data channels. If the second link of a 1080P dual link configuration is absent, the first link still contains a valid 1080i signal.|$|E
50|$|ELKI {{is modeled}} around a {{database}} core, {{which uses a}} vertical <b>data</b> <b>layout</b> that stores data in column groups (similar to column families in NoSQL databases). This database core provides nearest neighbor search, range/radius search, and distance query functionality with index acceleration {{for a wide range}} of dissimilarity measures. Algorithms based on such queries (e.g. k-nearest-neighbor algorithm, local outlier factor and DBSCAN) can be implemented easily and benefit from the index acceleration.The database core also provides fast and memory efficient collections for object collections and associative structures such as nearest neighbor lists.|$|E
50|$|Releases of Data ONTAP since 7.3.1 have {{included}} {{a number of}} techniques to optimize spatial <b>data</b> <b>layout</b> such as the reallocate command to perform scheduled and manual defragmentation, and the Write after Read volume option which detects and automatically corrects suboptimal data access patterns caused by spatial fragmentation. Releases of Data ONTAP 8.1.1 include other techniques to automatically optimize contiguous free-space within the filesystem which also helps to maintain optimal data layouts for most data access patterns. Prior to 7G, the wafl scan reallocate command {{would need to be}} invoked from an advanced privilege level and could not be scheduled.|$|E
40|$|The paper {{introduces}} some sorting {{networks and}} their simulation with P systems, {{in which each}} processor/membrane can hold more than one piece of data, and perform operations on them internally. Several <b>data</b> <b>layouts</b> are discussed in this context, and an optimal one is proposed, together with its implementation as a P system with dynamic communication graphs...|$|R
40|$|This {{research}} aims {{at creating}} {{and providing a}} framework to describe algorithmic redistribution methods for various block cyclic decompositions. To do so properties of this data distribution scheme are formally exhibited. The examination {{of a number of}} basic dense linear algebra operations illustrates the application of those properties. This study analyzes {{the extent to which the}} general two-dimensional block cyclic data distribution allows for the expression of efficient as well as flexible matrix operations. This study also quantifies theoretically and practically how much of the efficiency of optimal block cyclic <b>data</b> <b>layouts</b> can be maintained. The general block cyclic decomposition scheme is shown to allow for the expression of flexible basic matrix operations with little impact on the performance and efficiency delivered by optimal and restricted kernels available today. Second, block cyclic <b>data</b> <b>layouts,</b> such as the purely scattered distribution, which seem less promising as far [...] ...|$|R
40|$|Distributed memory {{architectures}} such as Linux clusters {{have become}} increasingly common butremain difficult to program. We target this problem and present a noveltechnique to automatically generate data distribution plans, and subsequently MPI implementations in C++,from programs written in a functional core language. This framework encodes distributed <b>data</b> <b>layouts</b> as types, which are then used both to search (via type inference) for optimal data distribution plans and to generate the MPI implementations. The main novelty of our approach is that it supports multiple collections, distributed arrays, maps, and lists, rather than just arrays. We introduce the core language and explain our formalization of distributed <b>data</b> <b>layouts.</b> We describe how to search for data distribution plans using a type inference algorithm, and how we generate MPI implementations in C++ from such plans. We then show how our types can be extended to support local <b>data</b> <b>layouts</b> and improved array distributions. We also show how a theorem prover and suitable equational theories {{can be used to}} yield a better (i. e., more complete) type inference algorithm. We then describe the design of our implementation, and explain how we use a runtime performance-feedback directed search algorithm to find the best data distribution plans for different input programs. Finally, we present some conceptual and experimental evaluation which analyses the capabilities of our approach, and shows that our implementation can find distributed memory implementations of several example programs, and that the performance of generated programs {{is similar to that of}} hand-coded versions. <br/...|$|R
