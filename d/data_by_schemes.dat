0|10000|Public
50|$|The <b>data</b> {{produced}} <b>by</b> the <b>scheme</b> {{provides a}} continuous and consistent record {{of quality of}} rivers across the UK for more than 30 years {{and can be used}} to demonstrate changes in water quality over that period.|$|R
30|$|As {{a future}} work, {{we would like}} to improve our {{proposed}} <b>data</b> aggregation <b>scheme</b> <b>by</b> utilizing spatial and temporal data correlation characteristics together. Furthermore, {{we would like to}} implement automatic ARIMA modeling-based data aggregation scheme into a WSN testbed and evaluate its performance too.|$|R
40|$|Experimental {{rates of}} W and dijets diffractively {{produced}} at the Tevatron Collider have recently become available. We use parametrizations of the pomeron structure function obtained from HERA <b>data</b> <b>by</b> two different <b>schemes</b> to compare theoretical expectations with the measured rates. Comment: 10 pages, 4 figures, {{to appear in}} the Proceedings of the Workshop on Diffractive Physics LISHEP 98, held in Rio de Janeiro on February 199...|$|R
40|$|We {{show that}} the method of maximum {{likelihood}} (MML) provides us with an efficient scheme for reconstruction of quantum channels from incomplete measurement <b>data.</b> <b>By</b> construction this <b>scheme</b> always results in estimations of channels that are completely positive. Using this property we use the MML for a derivation of physical approximations of un-physical operations. In particular, we analyze the optimal approximation of the universal NOT gate {{as well as a}} physical approximation of a quantum nonlinear polarization rotation. Comment: 4 pages, 3 figure...|$|R
40|$|We {{show that}} modifying a Bayesian <b>data</b> {{assimilation}} <b>scheme</b> <b>by</b> incorporating kinematically-consistent displacement corrections produces a scheme that is demonstrably better at estimating partially observed state vectors {{in a setting}} where feature information important. While the displacement transformation is not tied to any particular assimilation scheme, here we implement it within an ensemble Kalman Filter and demonstrate its effectiveness in tracking stochastically perturbed vortices. Comment: 26 Pages, 9 figures, 5 table...|$|R
40|$|The {{significant}} database performance gains {{can be had}} {{by implementing}} light-weight compression schemes and operators that work directly on compressed <b>data.</b> <b>By</b> classifying compression <b>schemes</b> according {{to a set of}} basic properties, operating directly on compressed data in column-oriented DBMS systems (c-store). This paper consist a new technique using cheating text (meaningful) with the FCE. It helps, for fast search of data, to improve the efficiency, for providing more security to the data base systems and to overcome the existing systems limitations. This approach provides a combination of rchitectural simplicity and the ability to configure data...|$|R
50|$|Throughout {{this time}} Eurostat and the OECD has {{collected}} {{data in this}} field in a joint <b>data</b> collection <b>scheme,</b> <b>by</b> gentlemen's agreement. This means that the countries receiving the questionnaire are not obliged to answer under any legal obligation but do so out of voluntary agreement. A simplified EPEA has been the requirement and investments, current expenditure, revenues and subsidies/transfers for environmental protection has been the targeted variables of statistics. These variables are often also considered official statistics.|$|R
40|$|This paper {{describes}} {{a set of}} changes and additions that were carefully chosen so as to retain the flavor of Scheme. I hope this paper {{marks the beginning of}} a community effort to design a next generation Scheme dialect. Imitation is the most sincere form of flattery, and I believe the Scheme community should flatter the ML community. One practice we should copy from ML [5] is the pervasive use of immutable data structures. Most <b>data</b> structures created <b>by</b> <b>Scheme</b> programs are not modified. Programmers should be allowed to write code which creates and shares data with other modules, while being assured that no other module modifies that data. Implementations should be abl...|$|R
40|$|A {{new class}} of schemes of the DSMC type for {{computing}} near-continuum flows has been recently suggested: the time-relaxed Monte Carlo (TRMC) methods. An important step preceding the wide use of these schemes is their validation by classical homogeneous and one-dimensional problems of gas dynamics. For this purpose, a plane Couette flow is considered in the present paper. A comparison of TRMC results with the <b>data</b> obtained <b>by</b> time-proved <b>schemes</b> of the DSMC method (here we used the Majorant Frequency Scheme) {{in a wide range}} of Knudsen numbers and for different values of wall velocity is presente...|$|R
40|$|In this paper, a <b>data</b> hiding <b>scheme</b> <b>by</b> simple LSB {{substitution}} is proposed. By applying {{an optimal}} pixel adjustment process to the stego-image {{obtained by the}} simple LSB substitution method, the image quality of the stego-image can be greatly improved with low extra computational complexity. The worst case mean-square-error between the stego-image and the cover-image is derived. Experimental {{results show that the}} stego-image is visually indistinguishable from the original cover-image. The obtained results also show a signi cant improvement with respect to a previous work...|$|R
40|$|Peer-to-peer {{file sharing}} {{applications}} {{have become increasingly}} popular. Measurements of P 2 P systems indicate large heterogeneity in the availability of individual nodes. Many have cyclic behavior, whereas others are always available. This paper proposes a cooperative storage technique which employs erasure coding schemes on a collection of data objects and provides various levels of data redundancy. Based on this technique, we study a historybased hill climbing scheme that takes advantage of varied time zones in a global p 2 p system. Our simulation results show the improved <b>data</b> availability <b>by</b> this <b>scheme.</b> We also investigate several climbing strategies including choice of coding schemes and laziness of data movement. 1...|$|R
40|$|A {{pressure}} based implicit {{procedure to}} solve the Euler equations on a nonorthogonal mesh with collocated finite volume formulations is described which uses pressure as a working variable. The boundedness criteria for this procedure are determined from Uniformly high order accurate Non Oscillatory (UNO) schemes, {{which are based on}} characteristic variables and are applied to the fluxes of the convected quantities directly, including mass flow rate. The developed scheme is applied to the computation of steady transonic and supersonic flow over a bump in channel geometry for various Mach number {{as well as to the}} transient shock tube problem. Also the results of steady supersonic flow over a ramp are presented. Then the results are compared with <b>data</b> predicted <b>by</b> TVD <b>schemes</b> based on characteristic variables...|$|R
40|$|A {{transportable}} scanning multiwavelength lidar {{has been}} installed {{for the independent}} and simultaneous determination of the particle backscatter coefficient at 6 wavelengths between 355 and 1064 nm and of the particle extinction coefficient at 355 and 532 nm. The physical particle parameters including the complex refractive index are retrieved from the optical <b>data</b> <b>by</b> an inversion <b>scheme</b> based on the Tikhonov's regularization technique. The optical and physical parameter sets serve as input in radiative transfer calculations to estimate the radiative forcing of the particles {{at the top of}} the atmosphere and at the surface. Quite different particle properties could be observed during the Aerosol Characterization Experiment, (Portugal, 1997), the Lindenberger Aerosol Experiment (Germany, 1998) and the Indian Ocean Experiment (Maldives, 1999 - 2000). We present measurement examples which demonstrates this approach of comprehensive aerosol characterization...|$|R
40|$|This paper {{presents}} {{a new image}} <b>data</b> fusion <b>scheme</b> <b>by</b> combining median filtering with self-organizing feature map (SOFM) neural networks. The scheme consists of three steps: (1) pre-processing of the images, where weighted median filtering removes part of the noise components corrupting the image, (2) pixel clustering for each image using self-organizing feature map neural networks, and (3) fusion of the images obtained in Step (2), which suppresses the residual noise components and thus further improves the image quality. It proves that such a three-step combination offers an impressive effectiveness and performance improvement, which is confirmed by simulations involving three image sensors (each of which has a different noise structure) ...|$|R
40|$|The {{preference}} utilization ratio, i. e., {{the share}} of imports under preferential tariff schemes out of total imports, has been a popular indicator for measuring the usage of preferential tariffs vis-à-vis tariffs on a most-favored-nation basis. A crucial shortcoming of this measure is the data requirements, particularly for import value <b>data</b> classified <b>by</b> tariff <b>schemes,</b> which are not available in most countries. This study proposes an alternative measure for preferential tariff utilization, termed the "tariff exemption ratio. " This measure offers the unique advantage of needing only publicly available data, such as those provided by the World Development Indicators, for its computations. We can thus calculate this measure for most countries for an international comparison. Our finding is that tariff exemption ratios differ widely across countries, with a global average of approximately 50 %...|$|R
40|$|Recently {{there has}} been a surge in {{interest}} in coupling ensemble-based data assimilation methods with variational methods (commonly referred to as 4 DVar). Here we discuss a number of important differences between ensemble-based and variational methods that ought to be considered when attempting to fuse these methods. We note that the Best Linear Unbiased Estimate (BLUE) of the posterior mean over a data assimilation window can only be delivered <b>by</b> <b>data</b> assimilation <b>schemes</b> that utilise the 4 -dimensional (4 D) forecast covariance of a prior distribution of non-linear forecasts across the data assimilation window. An ensemble Kalman smoother (EnKS) may be viewed as a BLUE approximating data assimilation scheme. In contrast, we use the dual form of 4 DVar to show that the most likely non-linear trajectory corresponding to the posterior mode across a data assimilation window can only be delivered <b>by</b> <b>data</b> assimilation <b>schemes</b> that create counterparts of the 4 D prior forecast covariance using a tangent linear model. Since 4 DVar schemes have the required structural framework to identify posterior modes, in contrast to the EnKS, they may be viewed as mode approximating data assimilation schemes. Hence, when aspects of the EnKS and 4 DVar data assimilation schemes are blended together in a hybrid, one {{would like to be able}} to understand how such changes would affect the mode- or mean-finding abilities of the data assimilation schemes. This article helps build such understanding using a series of simple examples. We argue that this understanding has important implications to both the interpretation of the hybrid state estimates and to their design...|$|R
40|$|The recent {{popularity}} of peer-to-peer (P 2 P) systems for file sharing {{has led to}} increased demand for higher data availability from such systems. Measurements of popular P 2 P systems indicate {{that a large number}} of hosts appear to be available on a cyclic basis, and their availability shows great heterogeneity. This paper proposes a cooperative storage technique which employs erasure coding schemes on a collection of data objects and provides various levels of data redundancy. Based on this technique, we propose a history-based hill climbing scheme that takes advantage of varied time zones in a global system, and compare it to more conventional approaches to providing data availability. Our simulation results show the improved <b>data</b> availability <b>by</b> this <b>scheme.</b> We also investigate several climbing strategies including choice of coding schemes and laziness of data movement...|$|R
40|$|The optimistic {{consistency}} scheme {{has been}} established with respect to data consistency and availability in distributed systems. Wireless networks are becoming popular in data communication and suitable for data sharing among users using multicast capability, but have hidden node problems. This paper proposes a simple data coherency protocol for mobile devices that is less data traffic in data collaboration using multicast communication feature and capable of automatic recovery from unpredictable disconnection, such as hidden node problem, <b>by</b> <b>data</b> versioning <b>scheme</b> using version vector and update log management. We evaluated efficiency of data sharing over multicast communication and the recovery cost from failure caused by hidden nodes and confirmed {{the efficiency of the}} protocol. I...|$|R
40|$|OBJECTIVE: To perform {{molecular}} typing of vancomycin resistant Enterococcus spp. (VRE) strains endemic in various hospitals of Karachi, {{to characterize the}} mechanism of glycopeptide resistance and assess the genetic relatedness, for understanding its transmission locally. METHOD: This was a cross sectional study conducted in the clinical and research laboratory of Aga Khan University Hospital (AKUH), Karachi, Pakistan from October 2007 to September 2008. Non-duplicate 86 (65 AKUH and 21 non-AKUH) VRE strains were included. Molecular typing of nosocomial isolates of VRE {{was carried out by}} using Pulsed field gel electrophoresis (PFGE) and identification of vanA and vanB genes were performed by conventional Polymerase Chain Reaction (PCR). RESULTS: Analysis of PFGE <b>data</b> <b>by</b> Tenover <b>scheme</b> showed single major pulsotype A with its subtypes A 1, A 2 and A 3 present among different tertiary hospitals in Karachi. The dice coefficient of similarity among AKUH, non-AKUH and total 86 (AKUH and non-AKUH) had a value of 90 %, 88 % and 89 % reflecting their clonal relatedness. In all 60 / 65 (92 %) and 19 / 21 (90 %) AKUH and non-AKUH isolates had vanA gene respectively. None had vanB gene. CONCLUSION: Molecular typing suggested that VRE isolates had same clonal origin indicating nosocomial transmission. Institution of strict infection control measures with active surveillance should be taken to avoid its further spread...|$|R
40|$|Abstract. A {{potentially}} {{serious problem}} with current digital signature schemes {{is that their}} underlying hard problems from number theory may be solved by an innovative technique or {{a new generation of}} computing devices such as quantum computers. Therefore while these signature schemes represent an efficient solution to the short term integrity (unforgeability and non-repudiation) of digital data, they provide no confidence on the long term (say of 20 years) integrity of <b>data</b> signed <b>by</b> these <b>schemes.</b> In this work, we focus on signature schemes whose security does not rely on any unproven assumption. More specifically, we establish a model for unconditionally secure digital signatures in a group, and demonstrate practical schemes in that model. An added advantage of the schemes is that they allow unlimited transfer of signatures without compromising the security of the schemes. Our scheme represents the first unconditionally secure signature that admits provably secure transfer of signatures. ...|$|R
40|$|In this study, {{we focus}} on the {{development}} of energy efficient and achievable load balancing mechanisms for wireless sensor networks. Due to resource constraint and tremendous amount of sensors, one possible way of achieving maximum lifetime of the network is applying data aggregation on sensor data. However, existing approaches introduce significant computation and control overheads that often not suitable for sensor networks applications. In view of this, we propose an in-network <b>data</b> aggregation <b>scheme</b> <b>by</b> exploiting the inherent spatial correlation of the sensed data. Each sensor multiplies its reading with a random coefficient and sends the product to the next hop to calculate a weighted sum of all the massage. Instead of receiving individual sensor readings, the sink will receive all weighted sum and restore the original <b>data.</b> <b>By</b> doing so, each node only performs one addition and one multiplication in order to compute one weighted sum and consume the same amount of energy. Simulation results demonstrate that the proposed scheme is efficient and outperforms existing schemes in terms of energy gain and network lifetime...|$|R
40|$|In this study, two {{different}} initial conditions for November 2004 {{are used in}} experiments with the Modular Ocean Model. The first initial condition is the steady state reached after a 33 year-simulation with reanalysis wind stresses. The second one is based on observed ocean <b>data,</b> obtained <b>by</b> <b>data</b> assimilation <b>schemes</b> and interpolated to the model grid. The objective is to point out {{which one is better}} to simulate the sea surface temperature (SST) in selected areas of the global-tropical ocean (180 W 180 E, 40 S 40 N). The analysis are based on differences between simulated and observed SST values. Apparently, the skill of the model on reproducing the SST increases with the second initial condition, based on observed ocean data, in some coastal areas and in the Central Pacific. On the other hand, the skill in the Tropical Atlantic, a region known by low SST predictability, are almost the same whatever initial condition is used. Pages: 563 - 56...|$|R
40|$|A {{major issue}} in the {{application}} of multistage stochastic programming to model the cost-optimal generation and trading of electric power is the approximation of the underlying stochastic <b>data</b> processes <b>by</b> tree-structured <b>schemes.</b> We present a methodology for the generation of scenario trees for the stochastic load process from historical load pro les. The statistical modeling of the load process exploits the decomposition of the load process into a daily mean load process and a mean-corrected load series. The probability distribution of the load process over the optimization horizon is derived by using a time series model for the daily mean load process and regression models for the mean-corrected load series. We utilize the explicit representation of the distribution to compute approximate load scenarios and their probabilities. In a nal step we reduce the number of load scenarios by a scenario deletion procedure. We report on the application of our approach to the cost-optimal generation of electric power in the hydro-thermal generation system of a German power utility...|$|R
40|$|Abstract—Various novel {{applications}} of underwater acoustic sensor networks have emerged or been proposed in recent years. In this paper, {{we present a}} complete system implementation of a robust multihop underwater network for sensing applications. At {{the core of the}} system is a robust data delivery scheme which uses opportunistic automatic repeat request (ARQ) with bidirectional overhearing. We demonstrate the modular and hardware-independent nature of our implementation by porting and deploying the software architecture and underwater network stack into hydroacoustic modems from different vendors. We integrate off-the-shelf GPS receivers and temperature sensors and evaluate the performance of the <b>data</b> delivery <b>scheme</b> <b>by</b> transmitting actual sensor data over two hops in shallow underwater environments in Singapore. Received sensor data are then visualized using Google Earth as they arrive at the sink node. I...|$|R
40|$|Compressive sensing (CS) {{provides}} an energy-efficient paradigm for data gathering in {{wireless sensor networks}} (WSNs). However, the existing work on spatial-temporal data gathering using compressive sensing only considers either multi-hop relaying based or multiple random walks based approaches. In this paper, we exploit the mobility pattern for spatial-temporal data collection and propose a novel mobile <b>data</b> gathering <b>scheme</b> <b>by</b> employing the Metropolis-Hastings algorithm with delayed acceptance, an improved random walk algorithm for a mobile collector to collect data from a sensing field. The proposed scheme exploits Kronecker compressive sensing (KCS) for spatial-temporal correlation of sensory <b>data</b> <b>by</b> allowing the mobile collector to gather temporal compressive measurements from a small subset of randomly selected nodes along a random routing path. More importantly, from the theoretical perspective we prove that the equivalent sensing matrix constructed from the proposed scheme for spatial-temporal compressible signal can satisfy the property of KCS models. The simulation results demonstrate that the proposed scheme can not only significantly reduce communication cost but also improve recovery accuracy for mobile data gathering {{compared to the other}} existing schemes. In particular, we also show that the proposed scheme is robust in unreliable wireless environment under various packet losses. All this indicates that the proposed scheme can be an efficient alternative for data gathering application in WSNs...|$|R
40|$|Abstract—Compressive sensing (CS) {{has been}} viewed as a {{promising}} technology to greatly improve the communication efficiency of data gathering in wireless sensor networks. However, this new data collection paradigm may bring in new threats but few study has paid attention to prevent information leakage during compressive data gathering. In this paper, we identify two statistical inference attacks and demonstrate that traditional compressive data gathering may suffer from serious information leakage under these attacks. In our theoretical analysis, we quantitatively analyze the estimation error of compressive data gathering through extensive statistical analysis, based on which we propose a new secure compressive <b>data</b> aggregation <b>scheme</b> <b>by</b> adaptively changing the measurement coefficients at each sensor and correspondingly at the sink without the need of time synchronization. In our analysis, we show that the proposed scheme could significantly improve data confidentiality at light computational and communication overhead. I...|$|R
40|$|In this paper, {{we present}} a {{technique}} for fingerprinting relational <b>data</b> <b>by</b> extending Agrawal et al. 2 ̆ 7 s watermarking scheme. The primary new capability provided <b>by</b> our <b>scheme</b> is that, under reasonable assumptions, it can embed and detect arbitrary bit-string marks in relations. This capability, which is not provided by prior techniques, permits our scheme {{to be used as}} a fingerprinting scheme. We then present quantitative models of the robustness properties of our scheme. These models demonstrate that fingerprints embedded <b>by</b> our <b>scheme</b> are detectable and robust against a wide variety of attacks including collusion attacks...|$|R
40|$|Cloud storage {{has been}} {{recognized}} as the popular solution {{to solve the problems}} of the rising storage costs of IT enterprises for users. However, outsourcing data to the cloud service providers (CSPs) may leak some sensitive privacy information, as the data is out of user’s control. So how to ensure the integrity and privacy of outsourced data has become a big challenge. Encryption and data auditing provide a solution toward the challenge. In this paper, we propose a privacy-preserving and auditing-supporting outsourcing <b>data</b> storage <b>scheme</b> <b>by</b> using encryption and digital watermarking. Logistic map-based chaotic cryptography algorithm is used to preserve the privacy of outsourcing data, which has a fast operation speed and a good effect of encryption. Local histogram shifting digital watermark algorithm is used to protect the data integrity which has high payload and makes the original image restored losslessly if the data is verified to be integrated. Experiments show that our scheme is secure and feasible...|$|R
40|$|International audienceMobile phones {{equipped}} with a rich set of embedded sensors enhance participatory sensing to collect data for different applications. However, many challenges arise when selecting participants to perform sensing tasks. Among these challenges, we can cite energy consumption, users' mobility impact {{and the quality of}} retrieved data, recently defined as Quality of Information (QoI). In this work, we study the QoI and Energy-aware Mobile Sensing (QEMSS) problem. Hence, for a given set of users, a sensing area and data quality requirements, the objective of QEMSS is to find the subset of users that maximizes QoI in terms of spatial and temporal metrics while minimizing the overall energy consumption and reducing the redundancy during the sensing process. We propose a meta-heuristic algorithm based on Tabu-Search to provide a sub-optimal solution. Simulation results, for both deterministic and unknown participants' trajectories, are compared to other state-of-the-art methods. This allows showing that our approach outperforms both the greedy- based and the random selection strategies. Particularly, the achieved <b>data</b> quality <b>by</b> our <b>scheme</b> is significantly higher in challenging scenarios such as low dense areas or scarce users' energy resources...|$|R
40|$|We {{introduce}} and analyse univariate, linear, and stationary subdivision {{schemes for}} refining noisy <b>data,</b> <b>by</b> fitting local least squares polynomials. We first present primal schemes, based on fitting linear polynomials to the data, and study their convergence, smoothness, and basic limit functions. We provide several numerical experiments that illustrate the limit functions generated <b>by</b> these <b>schemes</b> from initial noisy data, {{and compare the}} results with approximations obtained from noisy <b>data</b> <b>by</b> an advanced local linear regression method. We conclude by discussing several extension and variants. Comment: 20 pages, 11 figures, 3 table...|$|R
5000|$|Dangerfield and Bellamira (miles gloriosus is fooled <b>by</b> <b>scheming</b> heroine) ...|$|R
5000|$|Qualification for HOV status varies <b>by</b> <b>scheme,</b> but the {{following}} vehicles may be included: ...|$|R
40|$|Wireless ad hoc {{networks}} {{have seen a}} great deal of attention in the past years, especially in cases where no infrastructure is available. The main goal in these networks is to provide good data accessibility for participants. Because of the wireless nodes 2 ̆ 7 continuous movement, network partitioning occurs very often. In order to subside the negative effects of this partitioning and improve data accessibility and reliability, data is replicated in nodes other than the original owner of data. This duplication costs in terms of nodes 2 ̆ 7 storage space and energy. Hence, autonomous nodes may behave selfishly in this cooperative process and do not replicate data. This kind of phenomenon is referred to as a strategic situation and is best modeled and analyzed using the game theory concept. In order to address this problem we propose a game theory <b>data</b> replication <b>scheme</b> <b>by</b> using the repeated game concept and prove that it is in the nodes 2 ̆ 7 best interest to cooperate fully in the replication process if our mechanism is used...|$|R
40|$|Location-based {{services}} (LBSs) {{have emerged}} as one of the killer applications for mobile and pervasive computing. Due to limited wireless channel bandwidth and scarce client resources, client-side data caching is essential to enhance the data availability and to improve the data access time. In this demonstration, we present a CS Cache Engine for LBS. The engine adopts our Complementary Space Caching (CS caching) scheme [3] that differs from conventional <b>data</b> caching <b>schemes</b> <b>by</b> preserving a global view of the database in the cache. The global view consists of cached objects and Complementary Regions (CRs) representing those objects in the server but not in the cache. The CS Cache Engine supports various location-based queries. In addition, it allows the clients to determine whether queries are completely answerable by the cache, thereby effectively avoiding unnecessary traffic over the wireless channel. In this paper, the architecture and the functionality of the CS Caching Engine are discussed. Furthermore, a tourist information application called TravelGuide powered by this cache engine is demonstrated. 1...|$|R
40|$|International audienceThe aim of {{this paper}} is to {{investigate}} and quantify the effect of vibration on experimental tomographic particle image velocimetry (TPIV) measurements. The experiment consisted of turbulence measurements in an open channel flow. Specifically, five trash rack assemblies, composed of regular grids, divided a 5  m long flume into four sequential, identical pools. This set-up established a globally stationary flow, with each pool generating a controlled amount of turbulence that is reset at every trash rack. TPIV measurements were taken in the central pool. To eliminate the vibration from the measurements, three vibration correction regimes are proposed and compared to a global volume self-calibration (Wieneke 2008 Exp. Fluids 45 549 – 56), a now standard calibration procedure in TPIV. As the amplitude of the vibrations was small, it was possible to extract acceptable reconstruction re-projection qualities (Q I >  75 %) and velocity fields from the standard treatment. This paper investigates the effect of vibration on the cross-correlation signal and turbulence statistics, and shows the improvement to velocity field <b>data</b> <b>by</b> several correction <b>schemes.</b> A synthetic model was tested that simulated camera vibration to demonstrate its effects on key velocity parameters and to observe the effects on reconstruction and cross-correlation metrics. This work has implications for experimental measurements where vibrations are unavoidable and seemingly undetectable such as those in large open channel flows...|$|R
50|$|The Harmonised {{monitoring}} {{scheme is}} a long term river water quality monitoring scheme in the United Kingdom. The term {{is also used to}} refer to the long term <b>data</b> sets produced <b>by</b> the <b>scheme.</b>|$|R
