7|602|Public
40|$|In {{order to}} improve {{efficiency}} of video coding, temporal redundancy between neighboring frames can be reduced. In MPEG- 2, some frames, named <b>interframes,</b> are predicted using a motion estimation based on the conservation of the intensity over time. In the new standard MPEG- 4, frames are separated into several objects that are transmitted separately. Therefore, the prediction has to be performed on objects instead of frames. So, interframe-objects have to be predicted in {{order to improve}} video coding efficiency...|$|E
30|$|In video {{compression}} technologies, such as MEPG and H. 264 [4], encoded pictures (or frames) {{are arranged in}} groups of pictures (GOPs). An encoded video stream consists of successive GOPs. A GOP can contain the following frame types: I-frame, P-frame, and B-frame. The order of intraframes and <b>interframes</b> is specified in a GOP. An I-frame is a reference picture which is intracoded corresponding to a fixed image and it is independent of other pictures. A P-frame is predictive-coded frame which contains motion-compensated difference information from the preceding I- or P-frame. A B-frame is bidirectionally predictive-coded frame which contains different information from the preceding and following I- or P-frame within a GOP. I-frame and P-frame are {{often referred to as}} anchor frames. A GOP always begins with an I-frame. Afterwards, several P-frames follow. The B-frames are inserted between two consecutive anchor frames.|$|E
30|$|At the Wyner-Ziv encoder, the <b>interframes</b> are {{compressed}} {{using an}} integer 4 × 4 block-based discrete cosine transform (DCT). The DCT coefficients are then fed to a uniform quantizer, and the bitplanes are extracted. The bitplanes are in turn fed to a turbo encoder with two rate 1 / 2 recursive systematic convolutional (RSC) encoders. Each RSC associates the parity bits to the bitplanes. To achieve compression, the systematic bits are discarded since the decoder has already an interpolated {{version of the}} WZ frames. The parity bits are stored in a buffer and sent gradually, packet by packet, upon decoder feedback requests according to a periodic puncturing pattern. The feedback channel helps in adapting the forward transmission rate to the time-varying virtual channel statistics. The WZ decoding process implies several turbo decoding iterations. To alleviate the decoder computational hurdle, an initial number of parity bits packets is estimated {{by way of a}} hybrid encoder/decoder rate control mechanism [13]. These parity bits packets are sent once to the decoder, and subsequent packets will eventually be sent afterwards.|$|E
30|$|The DCF {{access method}} imposes an idle {{interval}} between consecutive frames, {{which is called}} the <b>Interframe</b> Space (IFS). Different IFSs are defined in order to impose different priorities to multiple frame types as following: SIFS (Short <b>Interframe</b> Space), PIFS (PCF <b>Interframe</b> Space), DIFS (Distributed <b>Interframe</b> Space), and EIFS (Extended <b>Interframe</b> Space). SIFS is the shortest of the <b>interframe</b> spaces and it is used for ACK frames. Only stations operating under the Point Coordination Function (PCF) will use PIFS. DIFS is used by stations operating under the DCF mechanism to transmit data frames and management frames. EIFS is used in communication-error conditions.|$|R
5000|$|Data {{frames and}} remote frames are {{separated}} from preceding frames by a bit field called <b>interframe</b> space. <b>Interframe</b> space consists {{of at least three}} consecutive recessive (1) bits. Following that, if a dominant bit is detected, it will be regarded as the [...] "Start of frame" [...] bit of the next frame. Overload frames and error frames are not preceded by an <b>interframe</b> space and multiple overload frames are not separated by an <b>interframe</b> space. <b>Interframe</b> space contains the bit fields intermission and bus idle, and suspend transmission for error passive stations, which have been transmitter of the previous message.|$|R
5000|$|The {{destination}} XGXS adds to or deletes {{from the}} <b>interframe</b> as needed for clock rate disparity compensation prior to converting the <b>interframe</b> code sequence back into XGMII Idle control characters.|$|R
40|$|In H. 264 /AVC, the {{encoding}} {{process can}} occur {{according to one}} of the 13 intraframe coding modes or {{according to one of}} the 8 available <b>interframes</b> block sizes, besides the SKIP mode. In the Joint Model reference software, the choice of the best mode is performed through exhaustive executions of the entire encoding process, which significantly increases the encoder's computational complexity and sometimes even forbids its use in real-time applications. Considering this context, this work proposes a set of heuristic algorithms targeting hardware architectures that lead to earlier selection of one encoding mode. The amount of repetitions of the encoding process is reduced by 47 times, at the cost of a relatively small cost in compression performance. When compared to other works, the fast hierarchical mode decision results are expressively more satisfactory in terms of computational complexity reduction, quality, and bit rate. The low-complexity mode decision architecture proposed is thus a very good option for real-time coding of high-resolution videos. The solution is especially interesting for embedded and mobile applications with support to multimedia systems, since it yields good compression rates and image quality with a very high reduction in the encoder complexity...|$|E
40|$|The {{presented}} work {{addresses the}} reduction of computational complexity for transcoding of <b>interframes</b> from H. 264 to H. 263 baseline profiles maintaining {{the quality of a}} full search approach. This scenario aims to achieve fast backward compatible interoperability inbetween new and existing video coding platforms, e. g. between DVB-H and UMTS. By exploiting side information of the H. 264 input bitstream the encoding complexity of the motion estimation is strongly reduced. Due to the possibility to divide a macroblock (MB) into partitions with different motion vectors (MV), one single MV has to be selected for H. 263. It will be shown, that this vector is suboptimal for all sequences, even if all existing MVs of a MB of H. 264 are compared as candidate. Also motion vector refinement with a fixed-pel refinement window as used by transcoders throughout the literature is not sufficient for scenes with fast movement. We propose an algorithm for selecting a suitable vector candidate from the input bitstream and this MV is then refined using an adaptive window. Using this technique, the complexity is still low at nearly optimum rate-distortion results compared to an exhaustive full-search approach. Index Terms — Video Transcoding, H. 264 /AVC, H. 263 1...|$|E
40|$|Abstract This paper {{presents}} and analyzes {{a new approach}} to data hiding that embeds in both the intra- and <b>interframes</b> from the H. 264 /AVC video codec. Most of the current video data hiding algorithms take into account only the intraframes for message embedding. This may be attributed to the perception that inter-frames are highly compressed due to the motion compensation, and any embedding message inside these may adversely affect the compression efficiency significantly. Payload of the inter-frames is also thought to be less, compared with the intra-frames, because of the lesser residual data. We analyze data hiding in both intra- and inter-frames over a wide range of QP values and observe that the payload of the inter is comparable with that of the intra-frames. Message embedding, in only those non-zero quantized transform coefficients (QTCs) which are above a specific threshold, enables us to detect and extract the message on the decoding side. There is no significant effect on the overall bitrate and PSNR of the video bitstream because instead of embedding message in the compressed bitstream, we have embedded it during the encoding process by taking into account the reconstruction loop. For the non-zero QTCs, in the case of intra-frames, we benefit from the spatial masking, while in the case of inter-frames, we exploit the motion and texture masking. We can notice that the data hiding is done during the compression process and the proposed scheme takes into account the reconstruction loop. The proposed scheme does not target robustness and the obtaine...|$|E
30|$|The {{measurements}} of throughput {{for a given}} frame length under different <b>interframe</b> gap are shown in Figure  10. It can be found that for a given frame length under a specified <b>interframe</b> gap, the throughput decreases clearly {{with the growth of}} <b>interframe</b> gap. It is obvious that the downlink throughput is never less than the upstream throughput. From the intersection between the downlink throughput and upstream throughput, the maximum throughput of the communication path can be determined. So, the accurate bottleneck bandwidth can be gained. The bottleneck bandwidth is 871, 910, and 931 Mbps when the data frame length is 256, 512 and 1, 024 bytes, respectively. Besides, the frame length under the specific throughput is approximately proportional to the <b>interframe</b> gap, which is drawn by the scope of the <b>interframe</b> gap under different frame length.|$|R
50|$|M-JPEG is an intraframe-only {{compression}} scheme (compared {{with the}} more computationally intensive technique of <b>interframe</b> prediction). Whereas modern <b>interframe</b> video formats, such as MPEG1, MPEG2 and H.264/MPEG-4 AVC, achieve real-world compression ratios of 1:50 or better, M-JPEG's lack of <b>interframe</b> prediction limits its efficiency to 1:20 or lower, depending on the tolerance to spatial artifacting in the compressed output. Because frames are compressed independently of one another, M-JPEG imposes lower processing and memory requirements on hardware devices.|$|R
40|$|Multi-microphone noise {{reduction}} methods often {{operate in the}} time-frequency domain in which a complex gain is applied to each time-frame and subband. These methods can achieve good {{noise reduction}} with little speech distortion by exploiting {{the fact that the}} desired signal is correlated across the channels. In the context of single-microphone noise reduction, it has been shown recently that the performance in terms of noise reduction and speech distortion can be improved by exploiting the correlation between subsequent time-frames, i. e., by exploiting the <b>interframe</b> correlation. In this paper, we exploit both interchannel and <b>interframe</b> correlations in the context of multi-microphone noise reduction. Now the <b>interframe</b> correlation is taken into account, i. e., a filter is applied in each subband and channel instead of just a gain. The results of our experimental study show that we can improve the fullband signal-to-noise ratios (SNRs) by using interchannel and <b>interframe</b> correlations when dealing with signals, such as speech, that exhibit a sufficiently large <b>interframe</b> correlation...|$|R
40|$|In a prescient paper Karl Lashley (1951) {{rejected}} reflex chaining {{accounts of}} the sequencing of behavior and argued instead for a more cognitive account in which behavioral sequences are typically controlled with central plans. An important feature of such plans, according to Lashley, {{is that they are}} hierarchical. Lashley offered several sources of evidence for the hierarchical organization for behavioral plans, and others afterward provided more evidence for this hypothesis. We briefly review that evidence here and then shift from a focus on the structure of plans (Lashley's point of concentration) to the processes by which plans are formed in real time. Two principles emerge from the studies we review. One is that plans are not formed from scratch for each successive movement sequence but instead are formed by making whatever changes are needed to distinguish the movement sequence to be performed next from the movement sequence that has just been performed. This plan-modification view is supported by two phenomena discovered in our laboratory: the parameter remapping effect, and the handpath priming effect. The other principle we review is that even single movements appear to be controlled with hierarchically organized plans. At the top level are the starting and goal postures. At the lower level are the intermediate states comprising the transition from the starting posture to the goal posture. The latter principle is supported by another phenomenon discovered in our lab, the end-state comfort effect, and by a computational model of motor planning which accounts for a large number of motor phenomena. Interestingly, the computational model hearkens back to a classical method of generating cartoon animations that relies on the production of keyframes first and the production of <b>interframes</b> (intermediate frames) second...|$|E
50|$|One of {{the most}} {{powerful}} techniques for compressing video is <b>interframe</b> compression. <b>Interframe</b> compression uses one or more earlier or later frames in a sequence to compress the current frame, while intraframe compression uses only the current frame, effectively being image compression.|$|R
3000|$|... where PS is {{the packet}} size, ACK is {{the length of}} an ACK packet, SIFS is {{the length of a}} short <b>interframe</b> space, DIFS is the length of a {{distributed}} <b>interframe</b> space, PD is the propagation delay where PD is usually very small compared to the slot size v.|$|R
40|$|Abstract. In {{this paper}} we {{introduce}} a simple method {{for the detection}} of hard cuts using only <b>interframe</b> differences. The method is inspired in the computational gestalt theory. The key idea in this theory is to define a meaningful event as large deviation from the expected background process. That is, an event that has little probability to occur given a probabilistic background model. In our case we will define a hard cut when the <b>interframe</b> differences have little probability to be produced by a given model of <b>interframe</b> differences of non-cut frames. Since we only use <b>interframe</b> differences, {{there is no need to}} perform motion estimation, or other type of processing, and the method turns to be very simple with low computational cost. The proposed method outperforms similar methods proposed in the literature. ...|$|R
5000|$|... <b>interframe</b> {{motion is}} limited, {{so as to}} reduce {{temporal}} or spatial judder ...|$|R
5000|$|PCF <b>Interframe</b> Space (PIFS) {{is one of}} the <b>interframe</b> space used in IEEE 802.11 based [...] Wireless LANs. [...] PCF enabled {{access point}} wait for PIFS {{duration}} rather than DIFS to occupy the wireless medium. PIFS duration is less than DIFS and greater than SIFS (DIFS > PIFS > SIFS). Hence AP always has more priority to access the medium.|$|R
40|$|Abstract—Elastography is a noninvasive {{method of}} imaging tissue {{elasticity}} using standard ultrasound equipment. In conventional elastography, axial strain elastograms are generated by cross-correlating pre- and postcompression digitized radio frequency (RF) echo frames acquired from the tissue {{before and after}} a small uniaxial compression, respectively. The time elapsed between the pre- and the postcompression frames {{is referred to as}} the <b>interframe</b> interval. For in vivo elastography, the <b>interframe</b> interval is critical because uncontrolled physiologic motion such as heartbeat, muscle motion, respiration and blood flow introduce <b>interframe</b> decorrelation that reduces the quality of elastograms. To obtain a measure of this decorrelation, in vivo experimental data (from human livers and thyroids) at various <b>interframe</b> intervals were obtained from 20 healthy subjects. To further examine the effect of the different <b>interframe</b> intervals on the elastographic image quality, the experimental data were also used in combination with elastographic simulation data. The deterioration of elastographic image quality was objectively evaluated by computing the area under the strain filter (SF) at a given resolution. The experimental results of this study demonstrate a statistical exponential behavior of the temporal decay of the echo signal cross-correlation amplitudes from the in vivo tissues due to uncontrollable motion. The results also indicate that the dynamic range and height of the SF are reduced at increased interfram...|$|R
30|$|The {{overload}} frame {{serves the}} purpose of extending the <b>interframe</b> space to handle overload conditions.|$|R
40|$|The <b>interframe</b> {{transform}} coding has been seldom used in practice {{because of the}} considerable computational complexity. To reduce the computational complexity, a fast algorithm is proposed in this paper. The algorithm reduces the number of operations by limiting the calculation of transform coefficients without significant quality degradation based {{on the distribution of}} transform coefficients. Then, different modes of transformation are performed according to frame difference. This proposed fast <b>interframe</b> coding, like the MPEG, has an asymmetric property with decoding being much faster than encoding. Computer simulations show that this fast algorithm can significantly reduce the computational burden in <b>interframe</b> {{transform coding}} and is even faster than MPEG-like coding. Department of Electrical Engineerin...|$|R
50|$|Between {{successive}} frames {{a sequence}} of (at least) six primitives must be transmitted, sometimes called <b>interframe</b> gap.|$|R
5000|$|Because <b>interframe</b> {{compression}} copies {{data from}} one frame to another, if the original frame is simply cut out (or lost in transmission), the following frames cannot be reconstructed properly. Making 'cuts' in intraframe-compressed video while video editing {{is almost as}} easy as editing uncompressed video: one finds the beginning and ending of each frame, and simply copies bit-for-bit each frame that one wants to keep, and discards the frames one doesn't want. Another difference between intraframe and <b>interframe</b> compression is that, with intraframe systems, each frame uses a similar amount of data. In most <b>interframe</b> systems, certain frames (such as [...] "I frames" [...] in MPEG-2) aren't allowed to copy data from other frames, so they require much more data than other frames nearby.|$|R
30|$|In the {{proposed}} algorithm, we determine the subsample ratio {{at the beginning}} of each GOP because the ZMVC of the first <b>interframe</b> prediction is the most accurate. The reference frame in the first <b>interframe</b> prediction is a reconstructed I-frame but others are not for each GOP. Only the reconstructed I-frame does not incur the influence resulted from the quality degradation of the inaccurate <b>interframe</b> prediction. That is, we only calculate the ZMVC of the first P-frame for the subsample ratio selection to efficiently save the computational load of ZMVC. Note that the ZMVC of the first P-frame is calculated by using 16 [*]:[*] 16 subsample ratio. Given the ZMVC of the first P-frame, the motion-level is determined by comparing the ZMVC with preestimated threshold values. The threshold values is decided statistically using popular video clips.|$|R
40|$|In this paper, we {{are proposing}} a simple {{lossless}} prediction based coding method for videos. Our algorithm works on 3 modes of operations and {{selection of the}} mode is done on a pixel-by-pixel basis. Selection of the mode is based on intensity variation of pixels in temporal direction. If {{there is a large}} intensity value variation, intraframe prediction mode is chosen. Otherwise <b>interframe</b> prediction mode is activated. Intraframe prediction uses Gradient Adaptive Predictor [5] whereas <b>Interframe</b> prediction switches between two algorithms. In <b>interframe</b> case, one prediction mode takes care of local characteristics of pixels of current and motion compensated frames while the other incorporates global characteristics. The proposed method is computationally very simple and results into better performance as compared to competitive but complex methods reported in literature. © 2012 IEEE...|$|R
5000|$|Because <b>interframe</b> {{compression}} copies {{data from}} one frame to another, if the original frame is simply cut out (or lost in transmission), the following frames cannot be reconstructed properly. Some video formats, such as DV, compress each frame independently using intraframe compression. Making 'cuts' in intraframe-compressed video {{is almost as}} easy as editing uncompressed video: one finds the beginning and ending of each frame, and simply copies bit-for-bit each frame that one wants to keep, and discards the frames one doesn't want. Another difference between intraframe and <b>interframe</b> compression is that, with intraframe systems, each frame uses a similar amount of data. In most <b>interframe</b> systems, certain frames (such as [...] "I frames" [...] in MPEG-2) aren't allowed to copy data from other frames, so they require much more data than other frames nearby.|$|R
40|$|In current <b>interframe</b> video {{compression}} systems, the encoder performs predictive coding {{to exploit the}} similarity of successive frames. The Wyner-Ziv Theorem on source coding with side information available only at the decoder suggests that an asymmetric video codec, where individual frames are encoded separately, but decoded conditionally (given the previous decoded frame) could achieve a similar efficiency. We report first results on a Wyner-Ziv coding scheme for motion video that uses intraframe encoding, but <b>interframe</b> decoding...|$|R
40|$|Decomposition {{representation}} can {{be generalized}} {{a number of}} times, to extend the scope of its algebraic characteristics as much as possible. With these generalizations, the Morphological Shape Decomposition 's role {{to serve as an}} efficient image decomposition tool is extended to grayscale images. This work follows the above line, and further develops it. A new evolutionary branch is added to the 3 D Morphological Shape Decomposition's development, by the introduction of a 3 D Multi Structuring Element Morphological Shape Decomposition, which permits 3 D Morphological Shape Decomposition of 3 D binary images (grayscale images) into &quot;multiparameter &quot; families of elements. At the beginning, 3 D Morphological Shape Decomposition representations are based only on &quot; 1 parameter &quot; families of elements for image decomposition. This paper addresses the gray scale <b>interframe</b> interpolation by means of mathematical morphology. The new <b>interframe</b> interpolation method is based on generalized morphological 3 D Shape Decomposition. This article will present the theoretical background of the morphological <b>interframe</b> interpolation, deduce the new representation and show some application examples. Computer simulations could illustrate results. Keywords — 3 D shape decomposition representation, mathematical morphology, gray scale <b>interframe</b> interpolatio...|$|R
30|$|In <b>interframe</b> coding, motion {{estimation}} and compensation have become powerful techniques {{to eliminate the}} temporal redundancy due to high correlation between consecutive frames [1].|$|R
40|$|Morphological <b>interframe</b> {{interpolation}} transforms one binary object {{into another}} thorough their metamorphosis producing {{a series of}} consecutive objects converting the initial object into the final one. Several mathematical morphology approaches have already been proposed for <b>interframe</b> image interpolation, however their definitions were different. In this paper a unified formula for the interpolation is given- an universal interpolator, which covers the existing morphological interpolation schemes. Due to its flexibility, it also allows constructing new interpolators according to one’s particular needs. 1...|$|R
40|$|Image data {{compression}} {{is an active}} topic of research in image processing. Traditionally, most image {{data compression}} schemes have been dominated by digital processing {{due to the fact}} that digital systems are inherently flexible and reliable. However, it has been demonstrated that optical processing can be used for spatial image data compression, using a method called interpolated differential pulse code modulation (IDPCM). This is a compression scheme which functions analogously with conventional digital DPCH compression, except that the specific compression steps are implemented by incoherent optical processing. The main objective of this research is to extend IDPCM to <b>interframe</b> compression, design such systems, and evaluate the compression performance limitation under no channel errors, given the subjectively acceptable image quality by means of digital simulation. We start with a review of digital spatial and <b>interframe</b> compression techniques and their implications for optical implementation. Then, the technological background of electro-optical devices which has made possible hybrid optical/digital processing for image data compression will be briefly discussed. Also, a detailed description of IDPCM coding is given, along with the ways that IDPCM can be extended to <b>interframe</b> compression. Finally, two architectures of hybrid and optical/digital <b>interframe</b> compression are proposed, simulated, and evaluated in order to discover potential performances of optically implemented <b>interframe</b> compression systems. Excellent reconstructed image quality is obtained by the proposed adaptive hybrid (O/D) IDPCM/frame replenishment technique at an overall transmission rate of 3 Mbits/sec, average bit rate of 1. 5 bits/pixel, and the average compression ratio of 5. 2 : 1...|$|R
3000|$|AIFSN {{specifies}} {{the number}} of [...] "slot" [...] periods within the AIFS (Arbitration <b>Interframe</b> Space) value used by an access category during contention (Table 1).|$|R
3000|$|... where T{{overhead}} = 5 TSIFS +TDIFS +TRTS +THTS +TCTS is {{the overhead}} cost, {{which is the}} summation of <b>interframe</b> space, control frame and random backo time.|$|R
40|$|A new {{technique}} for estimating <b>interframe</b> displacement of small blocks with minimum {{mean square error}} is presented. An efficient algorithm for searching the direction of displacement has been described. The results of applying the technique to two sets of images are presented which show 8 - 10 dB improvement in <b>interframe</b> variance reduction due to motion compensation. The motion com-pensation is applied for analysis and design of a hybrid coding scheme and the results show a factor of two gain at low bit rates...|$|R
40|$|Twodifficult {{issues in}} optical flow aremotion {{discontinuities}} and large <b>interframe</b> motion. Wepresent an algorithm that addresses both issues by first performing feature tracking and motion segmentation and then warping {{one of the}} imagestoreduce the <b>interframe</b> motion and avoid the motion discontinuities. The algorithm consists of three major phases: 1) featureselection, 2) featuretracking and segmentation 3) optical flow. Weused the Lucas and Kanade algorithm to compute the flow. The experiments on real imagesaswell as synthetic imageswith ground truth showed that this method is very accurate...|$|R
30|$|According to {{sampling}} theory [41], {{the decrease}} of sampling frequency {{will result in}} aliasing problem for high-frequency band. On the other hand, when the bandwidth of signal is narrow, higher downsample ratio or lower sampling frequency is allowed without aliasing problem. When applying the generic subsample algorithm for video compression, for high-variation sequences, the aliasing problem occurs and leads to considerable quality degradation because the high-frequency band is messed up. Papers [42, 43] hence propose adaptive subsample algorithms to solve the problem. They employed the variable subsample pattern for spatial high-frequency band, that is, edge pixels. However, the motion estimation is used for <b>interframe</b> prediction and temporal high-frequency band should be mainly treated carefully. Therefore, we determine the subsample ratio by the <b>interframe</b> variation. The <b>interframe</b> variation can be characterized by the motion-level of content. The ZMVC {{is a good sign}} for the motion-level detection because it is feasible for measurement and requires low computation load. The high ZMVC means that the <b>interframe</b> variation is low and vice versa. Hence, we can set high subsample ratio for high ZMVCs and low subsample ratio for low ZMVCs. Doing so, the aliasing problem can be alleviated and the quality can be frozen within an acceptable range.|$|R
