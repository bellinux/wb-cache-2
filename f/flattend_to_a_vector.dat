0|10000|Public
2500|$|... curl takes <b>a</b> <b>vector</b> field (1-form) <b>to</b> <b>a</b> <b>vector</b> field (2-form); ...|$|R
5000|$|Thus, {{the complex}} {{conjugate}} <b>to</b> <b>a</b> <b>vector</b> , particularly in finite dimension case, may be denoted as [...] (v-star, <b>a</b> row <b>vector</b> {{which is the}} conjugate transpose <b>to</b> <b>a</b> column <b>vector</b> [...] ).In quantum mechanics, the conjugate <b>to</b> <b>a</b> ket <b>vector</b> [...] is denoted as [...] - <b>a</b> bra <b>vector</b> (see bra-ket notation).|$|R
5000|$|... {{and will}} {{transform}} <b>a</b> <b>vector,</b> U, in E system <b>to</b> <b>a</b> <b>vector,</b> v, in the e system as ...|$|R
5000|$|But <b>to</b> <b>a</b> <b>vector</b> processor, {{this task}} looks {{considerably}} different: ...|$|R
2500|$|Similarly the even-graded element [...] {{corresponds}} <b>to</b> <b>a</b> <b>vector</b> {{rotation of}} 180°: ...|$|R
5000|$|... #Caption: The fast Walsh-Hadamard {{transform}} applied <b>to</b> <b>a</b> <b>vector</b> {{of length}} 8 ...|$|R
2500|$|... {{corresponds}} <b>to</b> <b>a</b> <b>vector</b> rotation through <b>an</b> angle θ {{about an}} axis defined by <b>a</b> unit <b>vector</b> [...]|$|R
50|$|More generally, the {{definition}} applies <b>to</b> <b>a</b> <b>vector</b> space over <b>an</b> ordered field.|$|R
5000|$|... {{is a kind}} of rubber-index <b>to</b> reshape <b>a</b> slice(sub-array) of array <b>to</b> <b>a</b> <b>vector.</b>|$|R
2500|$|And so {{the first}} {{prolongation}} of V <b>to</b> <b>a</b> <b>vector</b> field on J1(π) is ...|$|R
2500|$|Continuing on further, the even-graded element [...] {{corresponds}} <b>to</b> <b>a</b> <b>vector</b> {{rotation of}} 360°: ...|$|R
5000|$|And so {{the first}} {{prolongation}} of V <b>to</b> <b>a</b> <b>vector</b> field on J1(π) is ...|$|R
5000|$|Similarly the even-graded element γ = −σ1σ2 {{corresponds}} <b>to</b> <b>a</b> <b>vector</b> {{rotation of}} 180°: ...|$|R
500|$|In a Hilbert space , a {{sequence}} [...] is weakly convergent <b>to</b> <b>a</b> <b>vector</b> [...] when ...|$|R
5000|$|Continuing on further, the even-graded element γ = −1 {{corresponds}} <b>to</b> <b>a</b> <b>vector</b> {{rotation of}} 360°: ...|$|R
50|$|<b>To</b> conduct <b>a</b> Bayes linear {{analysis}} it {{is necessary}} to identify some values that you expect to know shortly by making measurements D and some future value which you would like to know B. Here D refers <b>to</b> <b>a</b> <b>vector</b> containing data and B <b>to</b> <b>a</b> <b>vector</b> containing quantities you would like to predict. For the following example B and D are taken to be two-dimensional vectors i.e.|$|R
5000|$|... {{corresponds}} <b>to</b> <b>a</b> <b>vector</b> rotation through <b>an</b> angle θ {{about an}} axis defined by <b>a</b> unit <b>vector</b> v = a1σ1 + a2σ2 + a3σ3.|$|R
3000|$|... {{operator}} applied <b>to</b> <b>a</b> <b>vector</b> builds <b>a</b> {{diagonal matrix}} with the vector entries {{placed on the}} main diagonal.|$|R
5000|$|In a Hilbert space H, a {{sequence}} {xn} is weakly convergent <b>to</b> <b>a</b> <b>vector</b> x ∈ H when ...|$|R
2500|$|... {{corresponds}} <b>to</b> <b>a</b> <b>vector</b> {{rotation of}} 90° from σ1 around towards σ2, {{which can be}} checked by confirming that ...|$|R
3000|$|... {{capture the}} {{discontinuities}} and abrupt changes in coefficient and base <b>vectors,</b> respectively. <b>A</b> <b>vector</b> with <b>a</b> smaller value of discontinuity feature is smoother compared <b>to</b> <b>a</b> <b>vector</b> with <b>a</b> larger discontinuity feature.|$|R
50|$|In vector calculus, the {{derivative}} of <b>a</b> <b>vector</b> function y with respect <b>to</b> <b>a</b> <b>vector</b> x whose components represent a space {{is known as}} the pushforward (or differential), or the Jacobian matrix.|$|R
50|$|Such a metric {{is called}} a pseudo-Riemannian metric. Applied <b>to</b> <b>a</b> <b>vector</b> field, its value can be positive, {{negative}} or zero.|$|R
5000|$|And, in {{the same}} manner, a dot product, which evaluates <b>to</b> <b>a</b> <b>vector,</b> of <b>a</b> <b>vector</b> by the {{gradient}} of another <b>vector</b> (<b>a</b> tensor of 2nd degree) {{can be seen as}} a product of matrices: ...|$|R
50|$|As {{well as the}} {{addition}} and multiplication by scalar operations which arise from the vector space structure, {{there are several other}} standard operations defined on differential forms. The most important operations are the exterior product of two differential forms, the exterior derivative of a single differential form, the interior product of a differential form and <b>a</b> <b>vector</b> field, the Lie derivative of a differential form with respect <b>to</b> <b>a</b> <b>vector</b> field and the covariant derivative of a differential form with respect <b>to</b> <b>a</b> <b>vector</b> field on <b>a</b> manifold with a defined connection.|$|R
2500|$|The {{algebraic}} (non-differential) {{operations in}} vector calculus {{are referred to}} as vector algebra, being defined for <b>a</b> <b>vector</b> space and then globally applied <b>to</b> <b>a</b> <b>vector</b> field. The basic algebraic operations consist of: ...|$|R
3000|$|... of p-integrable {{functions}} {{with respect}} <b>to</b> <b>a</b> <b>vector</b> measure on <b>a</b> δ-ring. This allows, for instance, to prove factorization theorems through [...]...|$|R
5000|$|Patterson {{algorithm}} converts <b>a</b> syndrome <b>to</b> <b>a</b> <b>vector</b> of errors. The syndrome {{of a word}} [...] {{is expected}} <b>to</b> take <b>a</b> form of ...|$|R
50|$|A TVP is {{a direct}} {{projection}} of <b>a</b> high-dimensional tensor <b>to</b> <b>a</b> low-dimensional <b>vector,</b> which is {{also referred to as}} the rank-one projections. As TVP projects <b>a</b> tensor <b>to</b> <b>a</b> <b>vector,</b> it can be viewed as multiple projections from <b>a</b> tensor <b>to</b> <b>a</b> scalar. Thus, the TVP of <b>a</b> tensor <b>to</b> <b>a</b> P-dimensional <b>vector</b> consists of P projections from the tensor <b>to</b> <b>a</b> scalar. The projection from <b>a</b> tensor <b>to</b> <b>a</b> scalar is an elementary multilinear projection (EMP). In EMP, a tensor is projected <b>to</b> <b>a</b> point through N unit projection vectors. It is the projection of a tensor on a single line (resulting a scalar), with one projection vector in each mode. Thus, the TVP of <b>a</b> tensor object <b>to</b> <b>a</b> <b>vector</b> in <b>a</b> P-dimensional <b>vector</b> space consists of P EMPs. This projection is an extension of the canonical decomposition, also known as the parallel factors (PARAFAC) decomposition.|$|R
3000|$|... {{is based}} on the same basic idea as linear regression, namely <b>to</b> relate <b>a</b> {{response}} Y <b>to</b> <b>a</b> <b>vector</b> of predictor variables X[*]=[*](X [...]...|$|R
3000|$|In (17.2), A(t)x {{is either}} {{interpreted}} as applying A(t) <b>to</b> <b>a</b> <b>vector</b> x ∈ H or as left multiplication if x ∈ L([...] H).|$|R
5000|$|... {{defining}} a general class of potential estimators as quadratic {{functions of the}} observed data, where the estimators relate <b>to</b> <b>a</b> <b>vector</b> of model parameters; ...|$|R
5000|$|... {{actively}} transforms <b>a</b> <b>vector</b> u <b>to</b> <b>a</b> <b>vector</b> v {{such that}} where v and u are {{measured in the}} same space and their coordinates representation is {{with respect to the}} same basis (denoted by the [...] "e").|$|R
50|$|This is {{a direct}} result of the {{linearity}} of expectation and is usefulwhen applying a linear transformation, such as <b>a</b> whitening transformation, <b>to</b> <b>a</b> <b>vector.</b>|$|R
5000|$|Vector bundles over [...] {{correspond}} to projective finitely generated modules over , via the functor [...] which associates <b>to</b> <b>a</b> <b>vector</b> bundle its module of sections.|$|R
3000|$|... where A(x)=ΦT(x) given T(·) as the {{transformation}} of <b>a</b> matrix <b>to</b> <b>a</b> <b>vector</b> by overlaying one column of x on another. Φ is a p×MN random matrix.|$|R
