22|0|Public
25|$|First, a ray {{is created}} at an <b>eyepoint</b> and traced through a pixel {{and into the}} scene, where it hits a diffuse surface. From that surface the {{algorithm}} recursively generates a reflection ray, which is traced through the scene, where it hits another diffuse surface. Finally, another reflection ray is generated and traced through the scene, where it hits the light source and is absorbed. The color of the pixel now depends on {{the colors of the}} first and second diffuse surface and the color of the light emitted from the light source. For example, if the light source emitted white light and the two diffuse surfaces were blue, then the resulting color of the pixel is blue.|$|E
50|$|Note {{that the}} image of the pole's top will overlay that of some terrain point which is on the same slant range arc but at a shorter {{horizontal}} range ("ground-range"). Images of scene surfaces which faced both the illumination and the apparent <b>eyepoint</b> will have geometries that resemble those of an optical scene viewed from that <b>eyepoint.</b> However, slopes facing the radar will be foreshortened and ones facing away from it will be lengthened from their horizontal (map) dimensions. The former will therefore be brightened and the latter dimmed.|$|E
50|$|First, a ray {{is created}} at an <b>eyepoint</b> and traced through a pixel {{and into the}} scene, where it hits a diffuse surface. From that surface the {{algorithm}} recursively generates a reflection ray, which is traced through the scene, where it hits another diffuse surface. Finally, another reflection ray is generated and traced through the scene, where it hits the light source and is absorbed. The color of the pixel now depends on {{the colors of the}} first and second diffuse surface and the color of the light emitted from the light source. For example, if the light source emitted white light and the two diffuse surfaces were blue, then the resulting color of the pixel is blue.|$|E
5000|$|Initially, the F3 {{model with}} the DE-2 {{eye-level}} finder was introduced, soon {{followed by the}} popular F3HP, or High Point camera, with the DE-3 High <b>Eyepoint</b> prism/finder. The major advantage of this finder was that the entire viewfinder image could be seen {{from a distance of}} 2.5 cm from the viewfinder. This made the F3 more usable by those who wear glasses when shooting, or were forced to shoot in high-glare situations while wearing sunglasses. The only down-side to this was a smaller image through the viewfinder compared to the standard prism. With the exception of the [...] "P" [...] spec camera, all viewfinders are completely interchangeable. The F3 and F3HP unfortunately retained the somewhat awkward flash mount on the rewind dial, which (with flash mounted) obstructed that area of the camera.|$|E
40|$|We {{present a}} {{practical}} technique for pointing and selection {{using a combination}} of eye gaze and keyboard triggers. <b>EyePoint</b> uses a two-step progressive refinement process fluidly stitched together in a look-press-look-release action, which makes it possible to compensate for the accuracy limitations of the current state-of-the-art eye gaze trackers. While research in gaze-based pointing has traditionally focused on disabled users, <b>EyePoint</b> makes gaze-based pointing effective and simple enough for even able-bodied users to use for their everyday computing tasks. As the cost of eye gaze tracking devices decreases, it will become possible for such gaze-based techniques {{to be used as a}} viable alternative for users who choose not to use a mouse depending on their abilities, tasks and preferences...|$|E
40|$|Eye gaze {{interaction}} enables {{users to}} interact with computers using their eyes. A wide variety of eye gaze interaction techniques {{have been developed to}} support this type of interaction. Gaze selection techniques, a class of eye gaze interaction techniques which support target selection, are the subject of this research. Researchers developing these techniques face a number of challenges. The most significant challenge is the limited accuracy of eye tracking equipment (due to the properties of the human eye). The design of gaze selection techniques is dominated by this constraint. Despite decades of research, existing techniques are still significantly less accurate than the mouse. A recently developed technique, <b>EyePoint,</b> represents {{the state of the art}} in gaze selection techniques. <b>EyePoint</b> combines gaze input with keyboard input. Evaluation results for this technique are encouraging, but accuracy is still a concern. Early trigger errors, resulting from users triggering a selection before looking at the intended target, were found to be the most commonly occurring errors for this technique. The primary goal of this research was to improve the usability of gaze selection techniques. In order to achieve this goal, novel gaze selection techniques were developed. New techniques were developed by combining elements of existing techniques in novel ways. Seven novel gaze selection techniques were developed. Three of these techniques were selected for evaluation. A software framework was developed for implementing and evaluating gaze selection techniques. This framework was used to implement the gaze selection techniques developed during this research. Implementing and evaluating all of the techniques using a common framework ensured consistency when comparing the techniques. The novel techniques which were developed were evaluated against <b>EyePoint</b> and the mouse using the framework. The three novel techniques evaluated were named TargetPoint, StaggerPoint and ScanPoint. TargetPoint combines motor space expansion with a visual feedback highlight whereas the StaggerPoint and TargetPoint designs explore novel approaches to target selection disambiguation. A usability evaluation of the three novel techniques alongside <b>EyePoint</b> and the mouse revealed some interesting trends. TargetPoint was found to be more usable and accurate than <b>EyePoint.</b> This novel technique also proved more popular with test participants. One aspect of TargetPoint which proved particularly popular was the visual feedback highlight, a feature which was found to be a more effective method of combating early trigger errors than existing approaches. StaggerPoint was more efficient than <b>EyePoint,</b> but was less effective and satisfying. ScanPoint was the least popular technique. The benefits of providing a visual feedback highlight and test participants' positive views thereof contradict views expressed in existing research regarding the usability of visual feedback. These results have implications for the design of future gaze selection techniques. A set of design principles was developed for designing new gaze selection techniques. The designers of gaze selection techniques can benefit from these design principles by applying them to their technique...|$|E
30|$|The {{fluocinolone acetonide}} intravitreal implant is commercially {{available}} in Europe, the Middle-East and the USA (ILUVIEN, marketed by Alimera Sciences) for treating DME, and recently received FDA approval in the USA (YUTIQ, marketed by <b>EyePoint</b> Pharmaceuticals) {{as a treatment}} for non-infectious uveitis affecting the posterior segment of the eye (NIU-PS).|$|E
40|$|The correct spatial {{registration}} between virtual {{and real}} objects in optical see-through augmented reality implies accurate {{estimates of the}} user’s <b>eyepoint</b> relative to the location and orientation of the display surface. A common approach is to estimate the display parameters through a calibration procedure involving a subjective alignment exercise. Human postural sway and targeting precision contribute to imprecise alignments, which in turn adversely affect the display parameter estimation resulting in registration errors between virtual and real objects. The technique commonly used has its origin incomputer vision, and calibrates stationary cameras using hundreds of correspondence points collected instantaneously in one video frame where precision is limited only by pixel quantization and image blur. Subsequently the input noise level is several order of magnitudes greater when a human operator manually collects correspondence points one by one. This paper investigates the effect of human alignment noise on view parameter estimation in an optical see-through head mounted display to determine how well astandard camera calibration method performs at greater noise levels than documented in computer vision literature. Through Monte-Carlo simulations we show that it is particularly difficult to estimate the user’s <b>eyepoint</b> in depth, but that a greater distribution of correspondence points in depth help mitigate the effects of human alignment noise...|$|E
40|$|This paper {{presents}} a new, simple and practical algorithm to avoid artifacts when switching between discrete levels of detail (LOD) by smoothly blending LOD representations in image space. We analyze the alternatives of conventional alpha-blending and so-called late-switching (the switching of LODs “far enough ” from the <b>eyepoint),</b> widely thought {{to solve the}} LOD switching discontinuity problem, and conclude that they either do not work in practice, or defeat the concept of LODs. In contrast we show that our algorithm produces visually pleasing blends for static and animated discrete LODs, for discrete LODs with different types of LOD representations (e. g. billboards and meshes) and even to some extent totally different objects with similar spatial extent, with a very small runtime overhead...|$|E
40|$|Many {{applications}} now demand {{interaction with}} visualizations of 3 D scenes and data sets. Current flat 2 D displays {{are limited in}} their capacity to provide this {{not only by the}} display technology but the interaction metaphors and devices used. The Desktop Bat is a device that has 5 degrees of freedom whilst retaining the simplicity of use of a mouse. To use it for general 3 D interaction several metaphors were created for the tasks of navigation and cursor manipulation and a set of experiments were conducted to determine which metaphors were the most efficient in use. Of these metaphors, a velocity control metaphor was the best for navigation and a metaphor that applied rotations and translations relative to the <b>eyepoint</b> coordinate system was best for object control. Keywords: 3 D Interaction, Interaction Devices, Virtual Environments, Bat. 1...|$|E
40|$|This is {{a conference}} paper. A current Brite-Euram project is {{concerned}} with life-cycle aspects of car seating with Loughborough being responsible for driver comfort assessment. This is being carried out through road and laboratory trials, with the results to be incorporated within the SAMMIE design system. Driver comfort is in part determined by seat pressure distributions which lead to deformation of the human flesh and the seat and movement of important design locations such as the driver's <b>eyepoint.</b> Accommodation of these effects requires a more realistic representation {{of the human body}} using surface rather than solid representations. Hence a shadow scanning technique is used to capture human body shape which is processed into the DUCT surface modelling system and via IGES files into SAMMIE. Finite element techniques are then used to predict deformations at the seat/driver interface...|$|E
40|$|For a {{systematic}} {{investigation of the}} perception of real spaces, photographs oer a chance to combine pictorial realism with laboratory experimental conditions. Psychophysical methods, however, often need a large variety of fully controlled stimuli, which is dicult to achieve with photographs of real scenes. Virtual scenes, on the other hand, provide the necessary exibility, but their generation by hand is usually too labor-intensive for larger quantities. Our SceneGen toolbox is capable to integrate the advantages of both in a fully automated process. SceneGen combines the good pictorial quality of photo textures, a physics-based radiosity lighting simulation (POVRay renderer), and the complete and convenient control of a high level feature-oriented XML-based description language. Thus, all scene features and rendering parameters are independently adjustable. External objects or scene parts can be integrated via a VRML interface. All this allows for an automated generation of an unlimited number of 3 D multi-textured realtime-capable OpenGL models or panoramic images with exactly dened dierences. The applicability of the scenes as psychophysical stimuli is demonstrated by our current work on the in uence of view parameters on distance estimates and semantic dierential ratings in virtual reality. Nine subjects in two groups rated two sets of 20 precomputed rectangular interiors. The rooms diered in dimensions, proportions and the number and form of openings in similar ranges like real rooms, but had identical surface properties and illumination. The results show a signicant eect of the main experimental parameter <b>eyepoint</b> height on perceived egocentric distances {{as well as on}} allocentric distances perpendicular to gaze direction. Surprisingly, allocentric distance estimates parallel to gaze direction are not signicantly in uenced. This suggests that the participants' horizontal self-location is aected by the simulated <b>eyepoint</b> height. Our experimental paradigm allowed us to investigate spatial perception solely depending on pictorial cues under fully controlled but diverse and comparatively natural conditions. SceneGen is expected to be especially useful for the eld of empirical research touching the disciplines of architecture, virtual reality and perceptual psychophysics...|$|E
40|$|Presented {{is a new}} {{algorithm}} {{to generate}} soft shadows. It employs graphics hardware, including texture mapping and accumulation buffering, to produce shadows resulting from area light sources quickly. Many shadow-generation algorithms have addressed the problem of quickly rendering the shadows which result from point light sources, {{as evidenced by the}} sharp, hard-edged outlines that are common in computer graphics. However, most shadows seen in real life have penumbras, the soft edges produced by extended light sources. We are introducing an algorithm which uses graphics workstation hardware to produce soft-edged shadows quickly. For each receiver, or object that has shadows cast onto it, we create a texture map to represent the radiance at each point on the receiver. First, a set of light samples (spread across the lights in the scene) is chosen to illuminate each receiver. Each sample is then used as an <b>eyepoint</b> from which to view the receiver polygon. We shade the polygon by illumi [...] ...|$|E
40|$|This paper {{describes}} prototype {{software for}} three-dimensional display of aircraft movement based on realtime radar and other Air Traffic Control (ATC) information. This prototype {{can be used}} to develop operational tools for controllers in ATC Towers who cannot view aircraft in low or zero visibility (LZV) weather conditions. The controller could also use the software to arbitrarily reposition his virtual <b>eyepoint</b> to overcome physical obstructions or increase situation awareness. The LZV Tower tool prototype consists of server and client components. The server interfaces to operational ATC radar and communications systems, sending processed data to a client process written in java. This client process runs under Netscape Communicator to provide an interactive perspective display of aircraft in the airport environment. Prototype VRML airport models were derived from 3 -D databases used in FAA-certified high fidelity flight-simulators. The web-based design offers potential efficiency increases and decreased costs in the development and deployment of operational LZV Tower tools...|$|E
40|$|A light {{field is}} a 4 D {{function}} describing the radiance across a {{boundary between the}} volume containing a scene, and the disjoint volume in which the <b>eyepoint</b> may be placed. Light field rendering {{is the process of}} rendering novel views of a scene captured by the light field function. It is a purely image-based rendering technique which uses no geometric knowledge of the scene. Although the lack of needed geometric information make light fields an attractive way of capturing real-world scenes, it has made it difficult to effectively use light fields in combination with other rendering techniques, or even rendering multiple light fields in the same scene. We present a rendering method for light fields which allows multiple instances of a light field to be rendered in combination with a polygon rendered scene. Of particular note is our ability to render multiple light fields which appear to intersect in space consistently when viewed from any direction. The method is demonstrated by creating a forest consisting of thousands of instances of a tree light field, rendered on a polygon terrain...|$|E
40|$|Previewing human {{capabilities}} in a computer-aided engineering mode has assisted greatly in planning well-designed systems without {{the cost and}} time involved in mockups and engineering models. To date, the computer models have focused on such variables as field of view, accessibility and fit, and reach envelopes. Program outputs have matured from simple static pictures to animations viewable from any <b>eyepoint.</b> However, while kinematics models are available, there are few biomechanical models available for estimating strength and motion patterns. Those, such as Crew Chief, that are available are based on strength measurements taken in specific positions. Johnson Space Center is pursuing a biomechanical model which will use strength data collected on single joints at two or three velocities to attempt to predict compound motions of several joint simultaneously and the resulting force at the end effector. Two lines of research are coming together to produce this result. One {{is an attempt to}} use optimal control theory to predict joint motion in complex motions, and another is the development of graphical representation of human capabilities. The progress to date in this research is described...|$|E
40|$|Perspective {{synthetic}} displays that supplement, or supplant, {{the optical}} windows traditionally used for guidance {{and control of}} aircraft are accompanied by potentially significant human factors problems related to the optical geometric conformality of the display. Such geometric conformality is broken when optical features {{are not in the}} location they would be if directly viewed through a window. This often occurs when the scene is relayed or generated from a location different from the pilot s <b>eyepoint.</b> However, assuming no large visual/vestibular effects, a pilot cad often learn to use such a display very effectively. Important problems may arise, however, when display accuracy or consistency is compromised, and this can usually be related to geometrical discrepancies between how the synthetic visual scene behaves and how the visual scene through a window behaves. In addition to these issues, this paper examines the potentially critical problem of the disorientation that can arise when both a synthetic display and a real window are present in a flight deck, and no consistent visual interpretation is available...|$|E
40|$|The {{research}} work {{presented in this}} thesis {{is concerned with the}} analysis of the human body as a calibration platform for estimation of a pinhole camera model used in Augmented Reality environments mediated through Optical See-Through Head-Mounted Display. Since the quality of the calibration ultimately depends on a subject’s ability to construct visual alignments, the research effort is initially centered around user studies investigating human-induced noise, such as postural sway and head aiming precision. Knowledge about subject behavior is then applied to a sensitivity analysis in which simulations are used to determine the impact of user noise on camera parameter estimation. Quantitative evaluation of the calibration procedure is challenging since {{the current state of the}} technology does not permit access to the user’s view and measurements in the image plane as seen by the user. In an attempt to circumvent this problem, researchers have previously placed a camera in the eye socket of a mannequin, and performed both calibration and evaluation using the auxiliary signal from the camera. However, such a method does not reflect the impact of human noise during the calibration stage, and the calibration is not transferable to a human as the <b>eyepoint</b> of the mannequin and the intended user may not coincide. The experiments performed in this thesis use human subjects for all stages of calibration and evaluation. Moreover, some of the measurable camera parameters are verified with an external reference, addressing not only calibration precision, but also accuracy...|$|E
40|$|This is {{a conference}} paper. A {{recently}} completed Brite-Euram (European Community) research project {{was concerned with}} life-cycle aspects of car seating with Loughborough University being responsible for driver comfort assessment. This was achieved by road and laboratory trials, with the results to be incorporated within the SAMMIE computer-aided ergonomic design system. Driver comfort is in part determined by seat pressure distributions which lead to deformation of the human flesh and the seat and result in uncertainty {{in the position of}} important design locations such as the driver's <b>eyepoint.</b> Accommodation of these effects requires a realistic representation of the human body using surface rather than solid representations. Hence a shadow scanning technique was used to capture human body shape which was processed into the DUCT surface modelling system and via IGES files into SAMMIE. Finite element techniques were then used to predict deformations at the seat/driver interface. Having established an anthropometrically correct representation of body shape, current research is aimed at improving the kinematic and analytic capabilities of the human model by introducing a multi-segment spine that can respond to external and internal loadings. This spine model is intended for use in the evaluation of human working postures (such as car driving) where, although the loadings might be viewed as well within human capabilities, previous studies have· shown that back pain or damage might result. The model described is based on an arch representation rather than the pin-jointed rigid link systems which are perhaps more usual, but which {{have been shown to be}} deficient in several respects...|$|E
40|$|Augmented reality (AR) systems combine {{three-dimensional}} computer-generated imagery {{with the}} view of the real environment in order to make unseen objects visible or to present additional information. A critical problem is that the computer-generated objects do not currently remain correctly registered with the real environment [...] -objects aligned from one viewpoint appear misaligned from another and appear to swim about as the viewer moves. This registration error is caused by a number of factors, such as system delay, optical distortion, and tracker measurement error, and is difficult to correct with existing technology. This dissertation presents a registration error model for AR systems and uses it to gain insight into the nature and severity of the registration error caused by the various error sources. My thesis is that a mathematical error model enables the system architect to determine. which error sources are the most significant,. the sensitivity of the net registration error to each error,. the nature of the distortions caused by each type of error,. the level of registration accuracy one can expect, and also provides insights on how best to calibrate the system. Analysis of a surgery planning application yielded the following main results:. Even for moderate head velocities, system delay causes more registration error than all other sources combined;. Using the eye's center of rotation as the <b>eyepoint</b> in the computer graphics model reduces the error due to eye rotation to zero for points along the line of gaze. This should obviate the need for eye tracking;. Tracker error is a significant problem both in head tracking and in system calibration;. The World coordinate system should be omitted when possible;. Optical distortion is a significant err [...] ...|$|E
40|$|The goal of {{this study}} is to extend the desktop {{panoramic}} static image viewer concept (e. g., Apple QuickTime VR; IPIX) to support immersive real time viewing, so that an observer wearing a head-mounted display can make free head movements while viewing dynamic scenes rendered in real time stereo using video data obtained from a set of fixed cameras. Computational experiments by Seitz and others have demonstrated the feasibility of morphing image pairs to render stereo scenes from novel, virtual viewpoints. The user can interact both with morphed real world video images, and supplementary artificial virtual objects (“Augmented Reality”). The inherent congruence of the real and artificial coordinate frames of this system reduces registration errors commonly found in Augmented Reality applications. In addition, the user’s <b>eyepoint</b> is computed locally so that any scene lag resulting from head movement will be less than those from alternative technologies using remotely controlled ground cameras. For space applications, this can significantly reduce the apparent lag due to satellite communication delay. This hybrid VR/view-morphing display (“Virtual Video”) has many important NASA applications including remote teleoperation, crew onboard training, private family and medical teleconferencing, and telemedicine. The technical objective of this study developed a proof-of-concept system using a 3 D graphics PC workstation of one of the component technologies, Immersive Omnidirectional Video, of Virtual Video. The management goal identified a system process for planning, managing, and tracking the integration, test and validation of this phased, 3 -year multi-university research and development program. by William E. Hutchison. Thesis (S. M.) [...] Massachusetts Institute of Technology, System Design & Management Program, 2000. This electronic version was submitted by the student author. The certified thesis is available in the Institute Archives and Special Collections. Includes bibliographical references (p. 84 - 89) ...|$|E
40|$|This {{thesis is}} {{concerned}} with improvements to algorithms for volume rendering; a technique that provides scientists with the means for visual exploration of three-dimensional data. Despite its numerous successes, and its increasing use within the scientific community, state-of-the-art volume rendering algorithms have many shortcomings. Difficulties include: ensuring {{the accuracy of the}} rendered images, producing images with modest computational resources, and rendering the diverse types of data that are currently being produced. The work in this thesis was motivated by the demands of an ongoing visualization project in four-dimensional cardiac visualization. We present solutions to some key problems in ensuring accuracy and in producing algorithms that can scale to handle large datasets. Although the theoretical work in this thesis applies to arbitrary data topologies, our implementations have assumed that the data is defined by sample points on a regular rectilinear grid. In the area of accuracy, we focus on the error that is introduced during volume projection. This phase of the volume rendering process involves the evaluation of the emission-absorption volume rendering line integral. This thesis presents four techniques for controlled precision volume integration. These schema depart from existing approaches in that they provide error bounds along with the solutions they generate. In each case, the error analysis leads to an algorithm for evaluating the integral to any specified tolerance. Our investigations into efficiency issues have resulted in two advances. First, an adaptive error bracketing scheme is presented that builds on the controlled precision volume integration methods. Using adaptive error bracketing, the solution for a viewing ray is continually refined until a user-specified error tolerance is met. The algorithm allows processing of the data without imposing a strict front-to-back or back-to-front evaluation order. Second, a suite of tools are presented {{that can be used to}} efficiently compute perspective projections of volume data. These include a paging strategy that is useful when a dataset is too large to fit into RAM memory and a ray splitting technique for adaptive supersampling. The latter technique ensures that all data features contribute to the final image while avoiding overcomputation in regions close to the <b>eyepoint...</b>|$|E

