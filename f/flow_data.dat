3145|10000|Public
25|$|Specialist {{products}} would include:- Small population groups; microdata; <b>flow</b> <b>data</b> (also {{known as}} origin-destination statistics). It is {{also suggested that}} data organised around alternative population bases may be produced.|$|E
25|$|A flow bench {{is capable}} of giving <b>flow</b> <b>data</b> which is closely but not {{perfectly}} related to actual engine performance. There {{are a number of}} limiting factors which contribute to the discrepancy.|$|E
25|$|Efficiency improvements: The manufacturer’s {{calculation}} of efficiency {{improvements to the}} upgraded generating equipment and the anticipated annual generation for the facility {{based on the same}} water <b>flow</b> <b>data</b> used to support the {{calculation of}} Historic Average Annual Hydropower Production baseline.|$|E
5000|$|<b>Data</b> <b>flow</b> and <b>data</b> analysis: makes a {{comparison}} possible between the business area models and the systems currently supporting this area, these current systems are analyzed using <b>data</b> <b>flow</b> and <b>data</b> analysis techniques.|$|R
40|$|Abstract—This paper {{investigates the}} limits of current <b>data</b> <b>flow</b> testing {{approaches}} from a radically novel viewpoint, and shows that the static <b>data</b> <b>flow</b> techniques used so far in <b>data</b> <b>flow</b> testing to identify the test objectives fail to represent the universe of <b>data</b> <b>flow</b> relations entailed by a program. This paper compares the <b>data</b> <b>flow</b> relations computed with static <b>data</b> <b>flow</b> approaches with the ones observed while executing the program. To this end, the paper introduces a dynamic <b>data</b> <b>flow</b> technique that collects the <b>data</b> <b>flow</b> relations observed during testing. The experimental data discussed in the paper suggest that <b>data</b> <b>flow</b> testing based on static techniques misses many <b>data</b> <b>flow</b> test objectives, and indicate {{that the amount of}} missing objectives (false negatives) can be more limiting than the amount of infeasible <b>data</b> <b>flow</b> relations identified statically (false positives). This opens a new area of research of (dynamic) <b>data</b> <b>flow</b> testing techniques that can better encompass the test objectives of <b>data</b> <b>flow</b> testing. I...|$|R
40|$|<b>Data</b> <b>flow</b> {{testing is}} a {{particular}} form of testing that identifies <b>data</b> <b>flow</b> relations as test objectives. <b>Data</b> <b>flow</b> testing has recently attracted new interest {{in the context of}} testing object oriented systems, since <b>data</b> <b>flow</b> information is well suited to capture relations among the object states, and can thus provide useful information for testing method interactions. Unfortunately, classic <b>data</b> <b>flow</b> testing, which is based on static analysis of the source code, fails to identify many important <b>data</b> <b>flow</b> relations due to the dynamic nature of object oriented systems. This thesis presents Dynamic <b>Data</b> <b>Flow</b> Testing, a technique which rethinks <b>data</b> <b>flow</b> testing to suit the testing of modern object oriented software. Dynamic <b>Data</b> <b>Flow</b> Testing stems from empirical evidence that we collect on the limits of classic <b>data</b> <b>flow</b> testing techniques. We investigate such limits by means of Dynamic <b>Data</b> <b>Flow</b> Analysis, a dynamic implementation of <b>data</b> <b>flow</b> analysis that computes sound <b>data</b> <b>flow</b> information on program traces. We compare <b>data</b> <b>flow</b> information collected with static analysis of the code with information observed dynamically on execution traces, and empirically observe that the <b>data</b> <b>flow</b> information computed with classic analysis of the source code misses a significant part of information that corresponds to relevant behaviors that shall be tested. In view of these results, we propose Dynamic <b>Data</b> <b>Flow</b> Testing. The technique promotes the synergies between dynamic analysis, static reasoning and test case generation for automatically extending a test suite with test cases that execute the complex state based interactions between objects. Dynamic <b>Data</b> <b>Flow</b> Testing computes precise <b>data</b> <b>flow</b> information of the program with Dynamic <b>Data</b> <b>Flow</b> Analysis, processes the dynamic information to infer new test objectives, which Dynamic <b>Data</b> <b>Flow</b> Testing uses to generate new test cases. The test cases generated by Dynamic <b>Data</b> <b>Flow</b> Testing exercise relevant behaviors that are otherwise missed by both the original test suite and test suites that satisfy classic <b>data</b> <b>flow</b> criteria...|$|R
25|$|With the {{information}} gathered on the flow bench, engine power curve and system dynamics can be roughly estimated by applying various formulae. With the advent of accurate engine simulation software, however, {{it is much more}} useful to use <b>flow</b> <b>data</b> to create an engine model for a simulator.|$|E
500|$|Trinity Dam, an earth {{embankment}} structure [...] high, was {{the tallest}} embankment dam {{in the world}} at its completion in 1962 (it was surpassed by Oroville Dam, also in California, in 1968). Trinity Lake can store a maximum of , or about twice the Trinity River's flow at this point. Below Trinity Dam is the much smaller Lewiston Dam, the actual point at which the water is diverted. The [...] Clear Creek Tunnel conveys water under the Trinity Mountains to the Whiskeytown Lake reservoir, and from there it flows [...] through the Spring Creek Tunnel to join the Sacramento River at Keswick Dam. Along the way the water drops some [...] through three hydroelectric plants, generating nearly one billion kilowatt hours each year. Water <b>flow</b> <b>data</b> for the Judge Francis Carr hydroelectric station located at the end of Trinity Tunnel indicates an annual average of , or , diverted from the river between 1963 and 2013.|$|E
500|$|New Melones Dam has a {{very large}} storage {{capacity}} relative to the watershed that feeds it, as compared to other major reservoirs in California. The potential water yield, and thus the supply contracts, of the Melones Project were calculated based on stream <b>flow</b> <b>data</b> between 1922 and 1978; however, this period may have been somewhat wetter than the long-term average. The Bureau of Reclamation estimated the additional water yield per year as [...] but the actual yield has been closer to [...] As a result, the reservoir is often at a low water level because in most years demand is greater than supply. In the 38 years between 1978 and 2016, New Melones has only reached capacity five times, and only once (in 1983) has it reached {{the level of the}} spillway. Between 1987 and 1992 Reclamation was forced to purchase water from other agencies to fulfill New Melones' water obligations. Even so, the reservoir fell to a record low level of , 3.5% of capacity, on October 1, 1992. Conversely, the great storage capacity also means that New Melones can capture more flood runoff while other major California reservoirs must release it to maintain flood control space.|$|E
5000|$|Track - coordinated {{tracking}} site <b>data</b> <b>flow</b> and <b>data</b> requests ...|$|R
50|$|Reactive {{programming}} {{could be}} said to be of higher order if it supports the idea that <b>data</b> <b>flows</b> could be used to construct other <b>data</b> <b>flows.</b> That is, the resulting value out of a <b>data</b> <b>flow</b> is another <b>data</b> <b>flow</b> graph that is executed using the same evaluation model as the first.|$|R
5000|$|Meta-Directory Server - {{a system}} that connects {{to a number of}} {{disparate}} <b>data</b> sources and <b>flows</b> <b>data</b> between them.|$|R
2500|$|If one Agency {{reported}} a cyber event, the 24/7 Watch at US-CERT {{could look at}} the incoming <b>flow</b> <b>data</b> and assist resolution.|$|E
2500|$|The Federal Computer Incident Response Capability (FedCIRC) {{was one of}} four watch {{centers that}} were {{protecting}} federal information technology when the E-Government Act of 2002 designated it the primary incident response center. With FedCIRC at its core, US-CERT was formed in 2003 as a partnership between the newly created DHS and the CERT Coordination Center which is at Carnegie Mellon University and funded by the U.S. Department of Defense. US-CERT delivered Einstein to meet statutory and administrative requirements that DHS help protect federal computer networks and the delivery of essential government services. Einstein was implemented to determine if the government was under cyber attack. Einstein did this by collecting <b>flow</b> <b>data</b> from all civilian agencies and compared that <b>flow</b> <b>data</b> to a baseline.|$|E
2500|$|Three {{constraints}} on Einstein that the DHS {{is trying to}} address are {{the large number of}} access points to U.S. agencies, the low number of agencies participating, and the program's [...] "backward-looking architecture". An OMB [...] "Trusted Internet Connections" [...] initiative was expected to reduce the government's 4,300 access points to 50 or fewer by June 2008. After agencies reduced access points by over 60% and requested more than their target, OMB reset their goal to the latter part of 2009 with the number to be determined. A new version of Einstein was planned to [...] "collect network traffic <b>flow</b> <b>data</b> in real time and also analyze the content of some communications, looking for malicious code, for example in e-mail attachments." [...] The expansion is known to be one of at least nine measures to protect federal networks.|$|E
50|$|Direct Integrations (DIs): Pre-built {{integrations}} that manage <b>data</b> <b>flows</b> and <b>data</b> synchronizations between Applications.|$|R
40|$|Classical <b>data</b> <b>flow</b> {{analysis}} {{determines whether}} a <b>data</b> <b>flow</b> fact may hold {{or does not}} hold at some program point. Probabilistic <b>data</b> <b>flow</b> systems compute a range, i. e. a probability, with which a <b>data</b> <b>flow</b> fact will hold at some program point. In this paper we develop a novel, practicable framework for probabilistic <b>data</b> <b>flow</b> problems. In contrast to other approaches, we utilize execution history for calculating the probabilities of <b>data</b> <b>flow</b> facts. In this way we achieve significantly better results. Effectiveness and efficiency of our approach are shown by compiling and running the SPECint 95 benchmark suite. 1 Introduction Classical <b>data</b> <b>flow</b> analysis determines whether a <b>data</b> <b>flow</b> fact may hold or does not hold at some program point. For generating highly optimized code, however, it is often necessary to know the probability with which a <b>data</b> <b>flow</b> fact will hold during program execution (cf. [10, 11]). In probabilistic <b>data</b> <b>flow</b> systems control flow graphs annotated with [...] ...|$|R
5000|$|<b>Data</b> <b>flow</b> diagram, a {{graphical}} {{representation of the}} <b>flow</b> of <b>data</b> through an information system ...|$|R
2500|$|Velocity-encoded MRI {{is based}} on the {{detection}} of changes in the phase of proton precession. These changes are proportional to the velocity of the protons' movement through a magnetic field with a known gradient. When using velocity-encoded MRI, the result is two sets of images, one for each time point in the cardiac cycle. One is an anatomical image and the other is an image in which the signal intensity in each pixel is directly proportional to the through-plane velocity. The average velocity in a vessel, i.e., the aorta or the pulmonary artery, is quantified by measuring the average signal intensity of the pixels in the cross-section of the vessel then multiplying by a known constant. The flow is calculated by multiplying the mean velocity by the cross-sectional area of the vessel. This <b>flow</b> <b>data</b> {{can be used in a}} flow-versus-time graph. The area under the flow-versus-time curve for one cardiac cycle is the stroke volume. The length of the cardiac cycle is known and determines heart rate; Q can be calculated using equation (...) [...] MRI is typically used to quantify the flow over one cardiac cycle as the average of several heart beats. It is also possible to quantify the stroke volume in real-time on a beat-for-beat basis.|$|E
50|$|TIC {{can be used}} {{to collect}} and create traffic and travel event data, as well as collect traffic <b>flow</b> <b>data</b> such as speed and volume, all {{referenced}} to a harmonized road network. Traffic and travel event data can be automatically created from traffic <b>flow</b> <b>data.</b>|$|E
5000|$|<b>Flow</b> (<b>data)</b> dependence: , [...] and [...] writes after {{something}} read by ...|$|E
40|$|Workflow {{technology}} {{has become a}} standard solution to managing increasingly complex business processes. Successful business process management depends on effective workflow modeling tools. Recently, researchers have developed a variety of workflow models, focusing mainly on the control and coordination of tasks, i. e. the control flow perspective. However, most of these workflow models {{found in the literature}} have paid little attention to the <b>data</b> <b>flow</b> perspective. In this paper, we investigate the <b>data</b> <b>flow</b> issues and propose a <b>data</b> <b>flow</b> modeling approach in the context of business process management. Our research objective is to develop a <b>data</b> <b>flow</b> modeling tool that supports <b>data</b> <b>flow</b> verification in workflow systems. Our contributions include also the classification of <b>data</b> <b>flow</b> operations and <b>data</b> <b>flow</b> anomalies...|$|R
40|$|Tennenbaum's <b>data</b> <b>flow</b> {{analysis}} based {{formulation of}} type inferencing is termed bidirectional in the “Dragon Book”; however, {{it fails to}} qualify as a formal <b>data</b> <b>flow</b> framework and is not amenable to complexity analysis. Further, the types discovered are imprecise. Here, we define a formal <b>data</b> <b>flow</b> framework (based on bidirectional <b>data</b> <b>flow</b> analysis) which discovers more precise type information and is amenable to complexity analysis. We compare <b>data</b> <b>flow</b> analyses with the more general constraint-based analyses and observe that <b>data</b> <b>flow</b> analyses represent program analyses without unbounded auxiliary store. We show that if unlimited auxiliary store is allowed then no <b>data</b> <b>flow</b> analysis would need more than two passes; if auxiliary store is disallowed then type inferencing requires bidirectional <b>data</b> <b>flow</b> analysis. © Elsevie...|$|R
40|$|We {{present a}} {{comprehensive}} approach to performing <b>data</b> <b>flow</b> analysis in parallel. We identify three types of parallelism inherent in the <b>data</b> <b>flow</b> solution process: independent-problem parallelism, separate-unit parallelism and algorithmic parallelism; and describe a unified framework to exploit them. Our investigations of typical Fortran programs reveal an abundance {{of the last two}} types of parallelism. In particular, we illustrate the exploitation of algorithmic parallelism in the design of our parallel hybrid <b>data</b> <b>flow</b> analysis algorithms. We report on the empirical performance of the parallel hybrid algorithm for the Reaching Definitions problem and the structural characteristics of the program flow graphs that affect algorithm performance. Keywords. <b>Data</b> <b>flow</b> analysis, parallel algorithms, parallel <b>data</b> <b>flow</b> analysis. 1 Introduction 1. 1 Motivation <b>Data</b> <b>flow</b> analysis is a compile-time analysis technique that gathers information about the <b>flow</b> of <b>data</b> in the program. <b>Data</b> <b>flow</b> i [...] ...|$|R
5000|$|Flow collector: {{responsible}} for reception, storage and pre-processing of <b>flow</b> <b>data</b> received from a flow exporter.|$|E
50|$|Traffic <b>flow</b> <b>data</b> {{collected}} from moving probes - GPS fitted taxis, public transport vehicles and others.|$|E
5000|$|Analysis application: {{analyzes}} received <b>flow</b> <b>data</b> in {{the context}} of intrusion detection or traffic profiling, for example.|$|E
25|$|Flow cytometry {{bioinformatics}} is {{the application}} of bioinformatics to <b>flow</b> cytometry <b>data,</b> which involves storing, retrieving, organizing and analyzing <b>flow</b> cytometry <b>data</b> using extensive computational resources and tools.|$|R
50|$|A multiplexer, switch, or router that {{provides}} round-robin scheduling has a separate queue for every <b>data</b> <b>flow,</b> where a <b>data</b> <b>flow</b> may {{be identified by}} its source and destination address. The algorithm lets every active <b>data</b> <b>flow</b> that has <b>data</b> packets in the queue to take turns in transferring packets on a shared channel in a periodically repeated order. The scheduling is work-conserving, meaning that if one flow is out of packets, the next <b>data</b> <b>flow</b> will take its place. Hence, the scheduling tries to prevent link resources from going unused.|$|R
40|$|This paper {{provides}} {{a detailed analysis}} of the gross worker <b>flows</b> <b>data</b> in the United Kingdom between 1997 and 2010, with particular emphasis on the 2008 - 2009 recession and its aftermath. Utilising <b>flows</b> <b>data</b> from the Labour Force Survey, the dominant macroeconomic factors driving unemployment in the United Kingdom before, during, and after the recession period are identified. Amongst the salient findings of this paper is a striking decline in job-to-job movements throughout and beyond the recent recession. Worker Gross Flows; Hazard Rates; Job-Finding Rate; Job-Separation Rate. ...|$|R
5000|$|... "A {{constitutive}} {{analysis of}} uniaxial elongational <b>flow</b> <b>data</b> of low-density polyethylene melt, Journal of Non-Newtonian Fluid Mechanics, ...|$|E
50|$|Dynamic mode {{decomposition}} {{was first}} introduced by Schmid as a numerical procedure for extracting dynamical features from <b>flow</b> <b>data.</b>|$|E
50|$|Unfortunately, the IMC {{literature}} {{contains some}} published papers {{in which the}} relation between heat <b>flow</b> <b>data</b> and microbial growth in closed ampoules has been misunderstood. However, in 2013 an extensive clarification was published, describing (a) details {{of the relation between}} IMC heat <b>flow</b> <b>data</b> and microbial growth, (b) selection of mathematical models which describe microbial growth and (c) determination of microbial growth parameters from IMC data using these models (Braissant et al. 2013).|$|E
5000|$|A manual {{approach}} {{involves the}} hand-writing of rules {{on the basis}} of subject matter expert interviews and the inspection of source code, job <b>flows,</b> <b>data</b> structures and observed behavior.|$|R
40|$|The {{classical}} {{theory of}} <b>data</b> <b>flow</b> analysis, {{which has its}} roots in unidirectional flows, is inadequate to characterize bidirectional <b>data</b> <b>flow</b> problems. We present a generalized theory of bit vector <b>data</b> <b>flow</b> analysis which explains the known results in unidirectional and bidirectional <b>data</b> <b>flows</b> and provides a deeper insight into the process of <b>data</b> <b>flow</b> analysis. Based on the theory, we develop a worklist-based generic algorithm which is uniformly applicable to unidirectional and bidirectional <b>data</b> <b>flow</b> problems. It is simple, versatile and easy to adapt for a specific problem. We show that the theory and the algorithm are applicable to all bounded monotone <b>data</b> <b>flow</b> problems which possess the property of the separability of solution...|$|R
40|$|The {{issue of}} using {{extended}} <b>data</b> <b>flow</b> notations to document object oriented designs and specifications is discussed. Extended <b>data</b> <b>flow</b> notations, {{for the purposes}} here, refer to notations {{that are based on}} the rules of Yourdon/DeMarco <b>data</b> <b>flow</b> analysis. The extensions include additional notation for representing real-time systems as well as some proposed extensions specific to object oriented development. Some advantages of <b>data</b> <b>flow</b> notations are stated. How <b>data</b> <b>flow</b> diagrams are used to represent software objects are investigated. Some problem areas with regard to using <b>data</b> <b>flow</b> notations for object oriented development are noted. Some initial solutions to these problems are proposed...|$|R
