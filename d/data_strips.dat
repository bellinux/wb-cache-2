8|168|Public
50|$|There are 36 cards, {{divided into}} two series: 18 for Series 1 and 18 for Series 2. In each package of 18 cards there are five demo cards, five level cards, eight {{power-up}} cards, and a promotional card without <b>data</b> <b>strips</b> which only contains an advertisement for the Pokémon Battle-e cards. More were released in Japan, however they never saw American release due to the discontinuation of the device.|$|E
40|$|To serve {{seamless}} mapping, airborne LiDAR {{data are}} usually collected with multiple parallel strips {{with one or}} two cross strip(s). Nevertheless, the overlapping regions of LiDAR <b>data</b> <b>strips</b> are usually found with unbalanced intensity values, resulting in the appearance of stripping noise. Despite that physical intensity correction methods are recently proposed, some of the system and environmental parameters are assumed as constant or not disclosed, leading to such an intensity discrepancy. This paper presents a new normalization technique to adjust the radiometric misalignment found in the overlapping LiDAR <b>data</b> <b>strips.</b> The normalization technique is built upon a second-order polynomial function fitted on the joint histogram plot, which is generated with a set of pairwise closest data points identified within the overlapping region. The method was tested on Teledyne Optech’s Gemini dataset (at 1064 nm wavelength), where the LiDAR intensity data were first radiometrically corrected based on the radar (range) equation. Five land cover features were selected to evaluate the coefficient of variation (cv) of the intensity values before and after implementing the proposed method. Reduction of cv was found by 19...|$|E
40|$|There {{has been}} much {{interest}} in analyzing the temporal organization of premature ventricular complexes. Direrent proposed mechanisms predict specific patterns. So far; these predictions have been mostly tested by inspecting short <b>data</b> <b>strips.</b> We present a novel method for representing cardiac time series as long as 24 hours using a temporal reorganization of the data. Our three dimensional rate-dependent histograms show the number of intervening beats or the interectopic time intervals for direrent sinus-RR intervals, an approach that {{may be useful to}} uncover hidden patterns of ectopy and to classify different dynamics of ectopy. 1...|$|E
40|$|We {{describe}} the base-line {{design and implementation}} of the Data Flow and High Level Trigger (HLT) part of the ATLAS Trigger and Data Acquisition (TDAQ) system. We then discuss improvements and generalization of the system design to allow the handling of events in parallel data streams and we present the possibility for event duplication, partial Event Building and <b>data</b> <b>stripping.</b> We then present tests on the deployment and integration of the TDAQ infrastructure and algorithms at the TDAQ â?pre-seriesâ cluster (~ 10 % of full ATLAS TDAQ). Finally, we tackle two HLT performance issues...|$|R
50|$|<b>Data</b> circuit-terminating {{equipment}} <b>strips</b> the layer-2 headers {{in order}} to encapsulate packets to the internal network protocol.|$|R
40|$|In this paper, we {{investigate}} how location access patterns influence the re-identification of seemingly anonymous data. In the real world, individuals visit different locations that gather similar information. For instance, multiple hospitals collect health {{information on the}} same patient. To protect anonymity for research purposes, hospitals share sensitive data, such as DNA sequences, stripped of explicit identifiers. Separately, for administrative functions, identified <b>data,</b> <b>stripped</b> of DNA, is made available. On a hospital by hospital basis, each pair of DNA and identified databases appears unlinkable, however, links can be established when multiple locations' database are studied. This problem, known as trail re-identification, is a generalized phenomenon and occurs because an individual 's location access pattern can be matched across the shared databases...|$|R
30|$|However, data {{velocity}} {{not only}} challenges communication networks but also processing capabilities of a constant inflow of data streams [59]. Big data technologies have to process information in real-time (streaming processing). According to Dumbill [51] {{some level of}} analysis is necessary during the data inbound {{in order to keep}} storages levels practical. This can include on-the-fly <b>data</b> <b>strips,</b> compression or heuristics. For example, the large hadrons collider (LHC) generates more raw data than the CERN computing grid can store; thus data has to be instantly analysed [60]. This problem seems to be straightforward, but real-time analytics challenges traditional parallel and distributed computation [61].|$|E
40|$|In this thesis, {{a number}} of {{effective}} algorithms and strategies were developed to improve the accuracy of terrestrial mobile LiDAR solutions {{in the field of}} engineering surveys. A detailed analysis for error budget of the terrestrial mobile LiDAR system has been presented in order to well interpret the effects of individual error sources. Firstly, the 3 D conformal coordinate transformation (3 DCCT) through Least Squares Method (LSM) was applied by employing the ground control points incorporating with feature constraints. Secondly, the multistrip adjustment (MA) algorithm was developed by taking advantage of the overlapped <b>data</b> <b>strips</b> and the repeated data acquisition over the same survey area using both of tie points and tie features. Lastly, the boresight angles of a terrestrial LiDAR system was preliminarily calibrated by using the planar and/or line features of two scans acquired during consecutive runs in opposite driving directions at the post-processing stage proposed by Keller et al. (2013) ...|$|E
40|$|The {{quality of}} laser {{scanning}} point clouds {{has become a}} topical research issue. The quality has been determined by the sum of several error sources caused by various factors affecting accuracy. In this paper, it is proposed that overlapping laser strips are favourable for inspecting {{the quality of the}} point clouds. The internal quality of five almost completely overlapping strips from TopoSys Falcon was investigated using the interactive orientation method. The orientation was solved in several small test sites located {{in different parts of the}} complete overlapping area. Each relative orientation between two laser point clouds revealed possible height or planimetric shifts at the examination area. When this procedure was repeated in various locations within laser scanning strips, internal deviations of laser <b>data</b> <b>strips</b> became visible. The comparison was done relatively. Therefore, no ground control points were used. As a result, the repeatability in heights was excellent, whereas the planimetric repeatability, however, included more systematic and non-systematic errors. Interestingly, the flight direction was the main error source, and visible in the observed bias and random errors. 1...|$|E
40|$|The {{author has}} {{identified}} the following significant results. Underflight photography {{has been used}} in the Baltimore County mined land inventory to determine areas of disturbed land where surface mining of sand and ground clay, or stone has taken place. Both active and abandoned pits and quarries were located. Aircraft data has been used to update cultural features of Calvert, Caroline, St. Mary's, Somerset, Talbot, and Wicomico Counties. Islands have been located and catalogued for comparison with older film and map data for erosion <b>data.</b> <b>Strip</b> mined areas are being mapped to obtain total area disturbed to aid in future mining and reclamation problems. Coastal estuarine and Atlantic Coast features are being studied to determine nearshore bedforms, sedimentary, and erosional patterns, and manmade influence on natural systems...|$|R
40|$|This thesis {{concerns}} {{the development of}} software techniques for polycrystalline fibre diffraction analysis {{and the application of}} the techniques to the determination of the structures of target molecules. Fibre diffraction analysis is one of a powerful method for determining the structure of polycrystalline fibres. However, one of the main shortcomings of this method is the paucity of intensity data; the observed intensities need to be modelled using suitable methods. This particular problem is further {{complicated by the fact that}} no direct Fourier synthesis methods can be used because of the cylindrical averaging inherent in fibre diffraction patterns. An attractive strategy is to use software programs for performing semi-automated data extraction from fibre diffraction patterns using refined method of <b>data</b> <b>stripping</b> and modelling. Even though there are some software packages available to do part of this, process the best one, the CCP 13 suite, required a great deal of improvement and development. In this thesis, the design, development and applicability of such a software package, FibreFix, are described. Using several practical examples, we show how this software package has aided in solving practical structure-determination problems. Covered in the thesis are: (i) Development,of several techniques as part of the CCP 13 software for performing <b>data</b> <b>stripping</b> and structure analysis; (ii) Design and development of a new integrated CCP 13 software package, FibreFix, incorporating these techniques; (iii) Evaluation of FibreFix to show that the analysis and modelling processes in data reduction for fibre diffraction can be simplified substantially using the FibreFix software. To show this two different, but related, case studies are described: crystalline natural rubber and E-DNA. Our results show that the structures modelled by the software package are well above our threshold for satisfactory modelling. This claim is supported by very satisfactory X-ray amplitude fitness and in reduction in the R-factor of the models. We have analysed the structure of oriented natural rubber, obtaining a very good fit with the observed X-ray amplitudes and a best R factor of 0. 18. We have also analysed different possible models for E-DNA. The R-factors indicate that 172 D-DNA-like models are better than 152 models. We have a found a satisfactory structure for E-DNA, which gives a good fit with the X-ray amplitudes and a best R-factor 0 f 28 %. This project has substantially, improved the CCP 13 fibre analysis software, now all incorporated within FibreFix, which can be used for <b>data</b> <b>stripping</b> of many forms of fibre diffraction pattern. The structures of rubber and E-DNA have also been used as targets to utilize and enhance the capability of the LALS (linked atom least-squares) modelling and refinement program. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
30|$|Depending on {{the text}} size, some common words may show up quite often. So {{concatenation}} of data bytes into bigger units may pay off in achieving better compression. For better performance, the principle of Rice coding is borrowed here. That is, the compression is done not on the data itself, but {{on the number of}} bits in each data unit. The (transformed) <b>data</b> <b>stripped</b> of the top bit (must be 1) is sent as it is. Decompression extracts the recorded number of bits back into each unit. Thus compression is primarily data mapping, with bit conversion playing a lesser role. This (recursive) concatenation scheme is somewhat like dictionary coding, such as LZW, in that it can also be applied to the likes of GIF, pNG and clip arts.|$|R
40|$|Recent {{development}} of radiometric calibration, correction and normalization approaches have facilitated {{the use of}} monochromatic LiDAR intensity and waveform data for land surface analysis and classification. Despite the recent successful attempts, the majority of existing approaches are mainly tailor made for monochromatic LiDAR toward specific land surface scenario. In view of the latest {{development of}} multispectral LiDAR sensor, such as the Optech Titan manufactured by Teledyne Optech, a more generic approach should be developed so that the radiometric correction model is able to handle and compensate the laser energy loss with respect to different wavelengths. In this study, we propose a semi-physical approach that aims to utilize high order polynomial functions to model the distortion effects due to the range and the angle. To estimate {{the parameters of the}} respect polynomial functions for the range and angle, our approach first locates a pair of closest points within the overlapping LiDAR <b>data</b> <b>strips</b> and subsequently uses a non-linear least squares adjustment to retrieve the polynomial parameters based on the Levenberg-Marquardt algorithm. The approach was tested on a multispectral airborne LiDAR dataset collected by the Optech Titan for the Petawawa Research Forest located in Ontario, Canada. The experimental results demonstrated that the coefficient of variation of the intensity of channel 1 (1550  nm), channel 2 (1064  nm) and channel 3 (532  nm) were reduced by 0. 1  ...|$|E
40|$|Literature {{across a}} range of social science {{disciplines}} highlights the existence of multiple masculinities, performed and negotiated through everyday practices. However, many studies of male consumers have not explicitly addressed how practices construct gender. In consumer research, themes of masculinity have mainly emerged in studies of advertising images, subcultural consumption, brands, events and consumer tribes. Few studies have explored men’s consumption and the construction of masculinity through and across practices. Previous studies also appear to have examined gender, practices and identities at either individual or group levels. This study therefore sought to address the role of consumption in young men's construction of masculine identities, across {{a range of}} contexts, and at individual and group levels. Working within the Consumer Culture Theory tradition, these issues were explored through ethnographic research with young Scottish men aged 18 - 22, developed from contact with members of a football-themed University society. Data on collective practices were generated through non-participant observation followed by participant observation over a 13 -month period. Practices included playing, watching and supporting football, visiting pubs and nightclubs, and playing poker. Accompanied shopping trips also formed part of the study. To gain further insights into individual identities long interviews with nine key informants were conducted. The analysis involved the iterative cycle of de-contextualising and re-contextualsing of <b>data</b> <b>strips</b> in the form of detailed reflexive fieldnotes, interview transcripts, photographs and film material. Masculinities emerged as contextualised, shifting and deeply rooted within practices of these young men. Their consumption produced normative ideals within groups. It also played a role in practices during which ‘masculine capital’ was sought. This capital was expressed through knowledge and experience in practices rather than objects and brands. Practices came to resemble games in which this capital was constantly contested. Through these games, groups also negotiated their place within the cultural context of gender relations. Consumption within practices constructed 'invisible’ gender identities through collectively shared meanings of masculinity. However, seemingly normal meanings of masculinity and consumption emerged as highly complex and layered as individuals constructed their multiple selves across practices. Rather than being fixed, consumption and masculinity was constantly (re) negotiated in changing contexts. This layered negotiation process of consumption meanings and masculinity was also reflected in informants’ discourse. This study suggests that various masculinities are 'played for’ through consumption across culturally situated practices. It shows how practices and consumption meanings shift during the negotiation of often contradictory and intertwined layers of gender identities. Methodologically, it offers insights into the challenges of gender differences between researcher and researched, and the role of new technologies such as mobile phones in ethnographic studies. Consumption and marketing messages may therefore allow young men to ‘do’, ‘talk’ and ‘be’ masculine across varying practices and contexts. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
5000|$|The {{most part}} of the tickets require to be {{validated}} just some minutes before to go aboard the train, doing this by electronic machine validation (blue or green colored, for small Magneto-Electronic SBME) or mechanics machine validation (yellow colored, for the 5 kind of normal paper tickets: ATB, Trenord, Lottomatica, Sisal and S.I.R.), both located in the station (2 blue and 2 yellow); this will start the countdown for expiration of the ticket, by stamping current date, time, location/validation-machine-id on it, and in case also reading and recording information on magnetic <b>data</b> <b>strip</b> of SBME [...] tickets. If all the machines validation you need are out of service, ask for validation to the ticket office; if the ticket office is close, ask soon for validation to [...] "chief train" [...] when aboard.|$|R
40|$|Strip logs for 491 wells were {{produced}} from a digital subsurface database of lithologic {{descriptions of the}} Ferron Sandstone Member of the Mancos Shale. This subsurface database covers wells from the parts of Emery and Sevier Counties in central Utah that occur between Ferron Creek on the north and Last Chance Creek on the south. The lithologic descriptions were imported into a logging software application designed for the display of stratigraphic <b>data.</b> <b>Strip</b> logs {{were produced}} at {{a scale of one}} inch equals 20 feet. The strip logs were created as part of a study by the Utah Geological Survey to develop a comprehensive, interdisciplinary, and qualitative characterization of a fluvial-deltaic reservoir using the Ferron Sandstone as a surface analogue. The study was funded by the U. S. Department of Energy (DOE) under the Geoscience/Engineering Reservoir Characterization Program...|$|R
40|$|Preliminary {{analysis}} of test beam <b>data</b> from <b>strip</b> scintillator planes read-out with multi-anode PMTs (MAPMTs) is presented {{along with a}} description of the independent systematic measurements of relative response for all channels of several MAPMTs used in the tests. Test beam measurements for the response of a scintillator strip, read out with Si photo-sensors, is also described...|$|R
40|$|This paper {{presents}} a data-driven method for automatic building reconstruction from raw airborne laser scanner data. The method utilises a TIN-structure that is calculated into the point cloud. The parameters of every TIN-mesh are mapped into a 3 D trianglemesh parameter space, {{which is then}} analysed using a cluster analysis technique. By analysing the clusters, significant roof planes are derived from the parameter space while taking common knowledge of roofs into account. However, no prior knowledge of the roof as such {{as the number of}} roof faces is required. Analysing the intersected roof faces, the roof outlines are determined and the ground plan is derived. The derived building models were evaluated for their correctness and geometric accuracy. Well-defined building roof planes can be extracted and reconstructed successfully, while disturbances such as dorms on buildings or geometric discrepancies in laser scanner <b>data</b> <b>strip</b> overlaps may significantly reduce the applicability of the technique. ...|$|R
40|$|The project arose out o f a need {{to update}} and improve the overall {{efficiency}} o f a small/medium sized manufacturing and distribution company. The re-engineered system streamlines the process o f ordering, production delivery and accounting. In addition, it increases end-user involvement in the networked processing system. The project was separated into two phases. • Phase 1 : Re-engineer the business processes as part o f a cost benefit analysis. • Phase 2 : The development o f a document imaging system, which could {{be integrated into the}} networked transaction processing and accounting system. This involved the utilization o f bar coding as a unique identifier for each order/delivery docket. Documents are scanned and the <b>data</b> <b>stripped</b> out and stored as simple data files in a database. This speeds up the process o f data storage and retrieval and facilitates access to the data by other functions within the organization...|$|R
40|$|The paper {{describes}} {{a method that}} extracts planar roof faces of building objects from point clouds obtained by a pre-segmentation of airborne laserscanner data. The method utilises a TIN-structure that is calculated into the point cloud. The parameters of every TINmesh are mapped into a triangle-mesh parameter space, which is then analysed by cluster analysis techniques. Both the utilisation of a 2 -D parameter space (containing only triangle-mesh slope and orientation) and a 3 -D parameter space (containing all three plane parameters) are described and tested. By cluster analysis and available knowledge on possible roof shapes, significant planes are derived from triangle parameter space. Results show {{that the use of}} a full 3 -D plane parameter space is superior to the analysis of triangle-mesh slope and orientation only. Well-defined building roof planes can be extracted successfully, while disturbances such as dorms on buildings or geometric discrepancies in laserscanner <b>data</b> <b>strip</b> overlaps may significantly reduce the applicability of the technique. ...|$|R
25|$|After {{rebuilding}} the locomotives did not retain their numbers, {{for example the}} first locomotive converted was CL2 which emerged as CLF1. All were back in service {{by the end of}} 1993. The CLFs appeared in the standard Australian National green with yellow <b>data</b> panel, <b>strip</b> and B-end. The CLPs received a unique livery, with a lighter green nose, silver carbody, and a yellow stripe running from the nose to the rear.|$|R
40|$|Many {{emerging}} real-time applications generate layered data streams, {{which can}} be used effectively to adapt multicast real-time transmissions to heterogeneous bandwidth environments. However, these applications have not changed with regard to being sensitive to transient congestion and cannot wait a full round-trip time for sender-initiated adaptation. In this paper we propose the selective truncating internetwork protocol (<b>STRIP)</b> supporting layered <b>data</b> transfer, and capable of handling congestion at a finer level of granularity by truncating packets, i. e., stripping off less important <b>data.</b> <b>STRIP</b> inter-operates with the traditional IP infrastructure and can be introduced in a step-by-step fashion, starting where the benefits are obvious, for example with routers which are connected to bandwidth constrained links and which have a surplus of processing capacity. We describe the design and architecture of STRIP and compare it with solutions for differentiated forwarding on a per-packet basis. STRIP provides a simple mechanism that can meet the demands for real-time flows effectively by supporting low delay forwarding, avoiding data-unit reordering, and supporting various drop priorities at the same timeGodkänd; 2000; 20080327 (ysko...|$|R
40|$|This paper {{deals with}} the {{characterization}} and modeling of the deformation behavior of titanium alloy Ti-X© under quasi-static conditions at room temperature. The material investigated is a 1. 2 mm thick rolled sheet with a strong basal texture. Monotonic tensile, compressive and cyclic tests in various directions relative to the rolling direction were conducted in order to characterize the material behavior. Textural information was obtained from electron backscatter diffraction (EBSD) <b>data.</b> <b>Strip</b> drawing tests were also carried out. The experimental data has been extended to biaxial stress states using the visco-plastic self-consistent texture model (VPSC). The CPB 06 ex 2 model Plunkett et al. (Int J Plast 24 : 847 - 866, 2008) was {{used to describe the}} yield surface. This model can describe the anisotropy and the tension-compression asymmetry. Distortional hardening was described taking account of the evolution of the model parameters. An extension of the model using the Arm strong and Frederick approach makes it possible to account for the Bauschinger effect during cyclic loading. There was a high level of correlation between the FE simulation and experimental data...|$|R
40|$|We explore {{relationships}} between health information seeking activities and engagement with healthcare professionals via a privacy-preserving analysis of geo-tagged data from mobile devices. Materials and Methods We analyze logs of mobile interaction <b>data</b> <b>stripped</b> of individually identifiable information and location data. The data analyzed consists of time-stamped search queries and distances {{to medical care}} centers. We examine search activity that precedes the observation of salient evidence of healthcare utilization (EHU) (i. e., data suggesting that the searcher is using healthcare resources), in our case taken as queries occurring at or near medical facilities. Results We show that the time between symptom searches and observation of salient evidence of seeking healthcare utilization depends on the acuity of symptoms. We construct statistical models that make predictions of forthcoming EHU based on observations about the current search session, prior medical search activities, and prior EHU. The predictive accuracy of the models varies (65 - 90 %) depending on the features used and the timeframe of the analysis, which we explore via a sensitivity analysis. Discussio...|$|R
40|$|In {{the spring}} of 2016, the Boston University {{libraries}} surveyed BU faculty, graduate students, and undergraduate students to determine their use of and satisfaction with library services and resources. This dataset contains {{the results of the}} faculty responses, questions used in the survey, and normalization tables for analysis. The <b>data</b> was <b>stripped</b> of qualitative <b>data</b> to ensure anonymity. A report describing the protocol and analyzing the data gathered in this survey may be found at this location, [URL]...|$|R
40|$|Progress is {{reported}} in the following areas: remote sensing applications to land use planning Lowndes County, applications of LANDSAT <b>data</b> to <b>strip</b> mine inventory and reclamation, white tailed deer habitat evaluation using LANDSAT data, remote sensing data analysis support system, and discrimination of unique forest habitats in potential lignite areas of Mississippi. Other projects discussed include LANDSAT change discrimination in gravel operations, environmental impact modeling for highway corridors, and discrimination of fresh water wetlands for inventory and monitoring...|$|R
40|$|We {{would like}} to thank, first and foremost, the 594 {{individuals}} who responded to this survey. The survey was initiated and developed in collaboration with 15 organizations based in California, who share {{an interest in the}} sustainable management and stewardship of the state’s coastal and marine resources. We thank them for their participation, collaborative spirit, and for useful feedback on earlier drafts of the survey instrument and this analysis. We thank the six individuals who tested the 2011 survey instrument and provided critical feedback, and USC Sea Grant’s intern, Marika Schulhof, who spent many hours organizing the <b>data</b> (<b>stripped</b> of personal information) in preparation for the analysis presented in this report. REPORT IMAGES- CALIFORNIA KING TIDE INITIATIVE Most of the photographs utilized on the cover and throughout this report are from the California King Tides Initiative. This initiative encourages members of the public to document the highest seasonal king tides that occur along the state’s coast. These photos not only help identify places that are vulnerable to sea level rise, they also can be used to build public awareness and develop initiatives to help ou...|$|R
40|$|International Telemetering Conference Proceedings / October 24 - 27, 1983 / Sheraton-Harbor Island Hotel and Convention Center, San Diego, CaliforniaFlight test {{activities}} can be completed quicker if the test engineers can evaluate test data in realtime on-board the aircraft or at the PCM ground station. Teledyne Controls is developing a software package for several customers that provides realtime EU data in several formats. Data presented to the test engineer includes: stacked or overlayed scrolling EU curves, limit exceedance tests and alarm generation, tabular EU data presentations and operator requested hardcopy of CRT presentations supplemented by the classical raw <b>data</b> <b>strip</b> chart recordings. Incorporated into this software are facilities permitting semi-automatic calibration of sensors installed in the aircraft during preflight operations as well as generation of tape headers for automatic PCM tape reading by the ground station. Other features include semi-automatic processing of sensor calibration data gathered in the calibration laboratory for entry into the ground station’s data base, and a software/hardware link coupling the data reduction software in the PCM ground station to the generation, loading and test of the data cycle map in the airborne PCM system...|$|R
40|$|FORTRAN program {{controls}} {{stepping motors}} and signal-processing electronics via S- 100 computer bus. Group of spectroreflectometers modified to provide computer-controlled operation and data collection. Analog and digital signal-processing circuits enable use of computerized automatic {{data collection and}} analysis. Previously, spectroreflectometer <b>data</b> recorded on <b>strip</b> charts...|$|R
40|$|This project {{explored}} {{the relationship between}} the global far-infrared and neutral hydrogen (H I) emission from galaxies, based on data from the Infrared Astronomy Satellite (IRAS) and published radio data. 100 and 60 micron IRAS fluxes were used to establish a temperature corrected measure of the cold dust emission, and H I fluxes were drawn from the literature with the greatest possible consistency. The degree of correlation between the FIR and H I fluxes was found to be better than in previous studies, comparable to the correlation previously found between FIR and CO fluxes. The improvement was obtained largely by (1) separating 'stripped' from 'unstripped' galaxies, and (2) using compatible sources of H I <b>data.</b> <b>Stripping</b> occurs in clusters of galaxies and is probably caused by ram-pressure effects as a galaxy travels through the intergalactic medium. Our results suggest that stripped galaxies have had their outer-disk gas removed (approximately 80 % of their total H I) while retaining most of their 100 -micron-emitting dust. This strongly shifts the ratio of their 100 -micron-to-H I fluxes. The second problem, arising from diverse sources of data, arises because differing telescopes and observational techniques give rise to substantial disagreement in the measured H I flux, and this degrades the correlation of the FIR and H I fluxes...|$|R
40|$|<b>Data</b> <b>stripping</b> is a {{technique}} for increasing the throughput and reducing the response time of large access to a storage system. In striped magnetic or optical disk arrays, a single file is striped or interleaved across several disks; in a striped tape system, files are interleaved across tape cartridges. Because a striped file can be accessed by several disk drives or tape recorders in parallel, the sustained bandwidth to the file is greater than in non-striped systems, where access to the file are restricted to a single device. It is argued that applying striping to tertiary storage systems will provide needed performance and reliability benefits. The performance benefits of striping for applications using large tertiary storage systems is discussed. It will introduce commonly available tape drives and libraries, and discuss their performance limitations, especially focusing on the long latency of tape accesses. This section will also describe an event-driven tertiary storage array simulator {{that is being used}} to understand the best ways of configuring these storage arrays. The reliability problems of magnetic tape devices are discussed, and plans for modeling the overall reliability of striped tertiary storage arrays to identify the amount of error correction required are described. Finally, work being done by other members of the Sequoia group to address latency of accesses, optimizing tertiary storage arrays that perform mostly writes, and compression is discussed...|$|R
40|$|Release eEcology {{annotation}} {{tool for}} annotation GPS trackers. Features: 	View track data over time with syncronized charts {{of data and}} derived data 	Location of GPS trackers plotted in 3 D and 2 D 	Plot of acceleration <b>data</b> as movie <b>strip</b> 	Syncronized video 	Annotation time ranges with labels 	Integrate custom data Fixed 	Correct trackers and track urls in app. j...|$|R
5000|$|Anonymization is {{a process}} in which PHI {{elements}} are eliminated or manipulated with the purpose of hindering the possibility of going back to the original data set. This involves removing all identifying data to create unlinkable data.De-identification under the Health Insurance Portability and Accountability Act Privacy rule occurs when <b>data</b> has been <b>stripped</b> of common identifiers by two methods: ...|$|R
30|$|Saarela et al. (2015 a) {{proposed}} to use probability-proportional-to-size sampling of laser scanning strips in a two-phase model-assisted sampling study where the total growing stock volume was estimated in a boreal forest area in Kuortane, Finland. It {{was also found}} that full cover of Landsat auxiliary information improved the precision of estimators compared to using only sampled LiDAR <b>strip</b> <b>data.</b>|$|R
40|$|Many {{educational}} institutions have repositories for research outputs. The {{number of items}} available through institutional repositories is growing, {{and is expected to}} continue to do so due to requirements for outputs from public-funded research to be open access. But how much usage are institutional repositories and their individual items getting? The Jisc-funded service IRUS-UK is designed to help institutions understand more about the usage of their institutional repositories. IRUS-UK collects raw usage data from participating repositories and processes these into COUNTER-compliant statistics. This provides repositories with comparable, authoritative, standards-based data and opportunities for profiling and benchmarking. It enables institutions to run reports at both repository level (e. g. total download figures) and at item level. IRUS-UK utilises a robust, multistage ingest process, validating <b>data,</b> <b>stripping</b> out robot and unusual accesses, and filtering out double clicks, to transform raw usage data into COUNTER-compliant statistics. IRUS-UK currently has data from 83 UK institutional repositories (using Eprints, DSpace and Fedora software) and has recorded over 35 million downloads since July 2012. The data from IRUS-UK can be used to provide information for management reporting, for usage monitoring, and for external reporting. Data can be viewed within the online portal, downloaded for further analysis, or harvested using the SUSHI service (NISO Z 39. 93). IRUS-UK is also working with and contributing to other groups and initiatives involved in a range of activities relating to usage statistics. These include: the Distributed Usage Logging/CrossRef DOI Event Tracker Working Group, OpenAIRE 2020 and COAR Working Group...|$|R
40|$|Over {{the last}} 20 years in Canada and elsewhere, the climatefor {{research}} examining disease etiology has changed. Administrative databases {{have made it}} easier to identify individuals with diseases beyond those with research registries (typ-ically cancer), opening the possibility of population-based study designs. However, this opportunity has raised concerns about releasing records to researchers for whom the data were not origi-nally collected. In the late 1990 s, some jurisdictions enacted legis-lation and ethics review boards implemented policies that restricted release of personal identifying information to enable subject con-tact. However, much etiological research requires subject contact to elicit details about lifestyle, occupations, residences, and other information not routinely recorded in administrative databases. Academic and legal interest in the impact of research on subject privacy and, more recently, of privacy legislation on research, has resulted in ethics policies and policy commentaries by researchers, lawyers, and government agents. 1 - 11 A surprising aspect of the work to date is that little attention has been given to the opinions of those whose privacy is being protected, i. e., what do members of the public think? Some opinion research has examined the issue of consent prior to analyses of administrative <b>data</b> <b>stripped</b> of iden-tifiers. 12 - 18 Surveys examining the public’s desire to actively partic-ipate in research and the method of selection and contact are rare. 19 - 21 To better understand public opinions, we conducted a survey of the willingness of British Columbia adults to participate in health research and how willingness was affected by the methods of selec-tion, contact, and other factors...|$|R
