216|265|Public
25|$|While not {{instantaneous}} {{like its}} machine counterparts such as Google Translate and Yahoo! Babel Fish, web-based <b>human</b> <b>translation</b> {{has been gaining}} popularity by providing relatively fast, accurate translation for business communications, legal documents, medical records, and software localization. Web-based <b>human</b> <b>translation</b> also appeals to private website users and bloggers.|$|E
25|$|Web-based <b>human</b> <b>translation</b> is {{generally}} favored by companies and individuals that wish to secure more accurate translations. In {{view of the}} frequent inaccuracy of machine translations, <b>human</b> <b>translation</b> remains the most reliable, most accurate form of translation available. With the recent emergence of translation crowdsourcing, translation-memory techniques, and internet applications, translation agencies {{have been able to}} provide on-demand human-translation services to businesses, individuals, and enterprises.|$|E
25|$|Podec, a trojan {{discovered}} by the security company Kaspersky, forwards CAPTCHA requests to an online <b>human</b> <b>translation</b> service that converts the image to text, fooling the system. Podec targets Android mobile devices.|$|E
40|$|N-gram {{measures}} of translation quality, such as BLEU {{and the related}} NIST metric, are becoming increasingly important in machine translation, yet their behaviors are not fully understood. In this paper we examine the performance of these metrics on profes-sional <b>human</b> <b>translations</b> into German of two literary genres, the Bible and Tom Sawyer. The most surprising result is that some machine translations outscore some professional <b>human</b> <b>translations.</b> In addition, {{it can be difficult}} to distinguish some other <b>human</b> <b>translations</b> from machine translations with only two reference translations; with four reference translations it is much easier. Our results lead us to conclude that much care must be taken in using n-gram measures in formal evaluations of machine translation quality, though they are still valuable as part of the iterative development cycle...|$|R
40|$|This paper reports {{results from}} an {{experiment}} that {{was aimed at}} comparing evaluation metrics for machine translation. Implemented as a workshop at a major conference in 2002, the experiment defined an evaluation task, description of the metrics, as well as test data consisting of <b>human</b> and machine <b>translations</b> of two texts. Several metrics, either applicable by human judges or automated, were used, and the overall results were analyzed. It appeared that most human metrics and automated metrics provided in general consistent rankings of the various candidate translations; the ranking of the <b>human</b> <b>translations</b> matched the one provided by <b>translation</b> professionals; and <b>human</b> <b>translations</b> were distinguished from machine translations...|$|R
50|$|In <b>human</b> <b>translations,</b> in {{particular}} {{on the part}} of interpreters, selectivity {{on the part of}} the translator in performing a translation is often commented on when one of the two parties being served by the interpreter knows both languages.|$|R
25|$|Meedan’s {{core product}} is {{a forum for}} cross-language {{conversation}} and media sharing in Arabic and English. Users can browse aggregated sources around world events – blogs and mainstream sources; opinion and reporting; Arabic and English writing – and help expand the news narrative by posting articles and comments themselves. All sources and comments are mirrored across Arabic and English {{using a combination of}} machine and <b>human</b> <b>translation.</b> Users can also make friends with other users and upload their profile to their My Meedan page.|$|E
500|$|BLEU (bilingual {{evaluation}} understudy) is an algorithm {{for evaluating}} the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine's output and that of a human: [...] "the closer a machine translation is to a professional <b>human</b> <b>translation,</b> the better it is" [...] this is the central idea behind BLEU. BLEU {{was one of the first}} metrics to claim a high correlation with human judgements of quality, and remains one of the most popular automated and inexpensive metrics.|$|E
2500|$|These tools {{speed up}} and {{facilitate}} <b>human</b> <b>translation,</b> {{but they do}} not provide translation. [...] That is a function of tools known broadly as machine translation.|$|E
50|$|Jollo was {{an online}} {{translation}} website where users can instantly translate texts into 23 languages, request <b>human</b> <b>translations</b> from {{a community of}} volunteers {{around the world and}} compare the correctness of several leading machine translation websites. It is currently dead.|$|R
5000|$|Initially, UTX {{was created}} to absorb the {{differences}} between various user dictionary formats for machine translation. The scope of the format was later expanded to include other purposes, such as glossaries for <b>human</b> <b>translations,</b> natural language processing, thesaurus, text-to-speech, input method, etc.|$|R
50|$|Stepes Translate {{expands the}} {{capabilities}} of on-demand machine translation systems such as Google Translate and Bing Translator. With Stepes Translate allows businesses and individuals to obtain instant quality <b>human</b> <b>translations</b> of text directly on their smartphone or desktop PC. The platform is available 24/7.|$|R
2500|$|Meedan is a {{non-profit}} social technology company {{which aims to}} increase cross-language interaction on the web, with particular emphasis on translation and aggregation services in Arabic and English. Through its use of Machine Translation (MT), Machine Augmented Translation (MAT), and distributed <b>human</b> <b>translation,</b> Meedan’s goal is to increase dialogue and exchange between Arabic and English speakers primarily by launching a cross-language forum for conversation and media sharing. [...] This service will be designed to stand as “a digital gathering place for a linguistically, culturally, and geographically diverse community of Arabic and English speaking Internet users”.|$|E
50|$|While not {{instantaneous}} {{like its}} machine counterparts such as Google Translate and Yahoo! Babel Fish, web-based <b>human</b> <b>translation</b> {{has been gaining}} popularity by providing relatively fast, accurate translation for business communications, legal documents, medical records, and software localization. Web-based <b>human</b> <b>translation</b> also appeals to private website users and bloggers.|$|E
5000|$|Make <b>human</b> <b>translation</b> easier, {{faster and}} more cost {{effective}} ...|$|E
40|$|Many {{automatic}} evaluation metrics for machine translation (MT) rely on making comparisons to <b>human</b> <b>translations,</b> a resource {{that may not}} always be available. In this work, we present a method for developing sentence-level MT evaluation metrics that do not directly rely on <b>human</b> reference <b>translations.</b> Our metrics are developed using regression learning and are based on a set of weaker indicators of fluency and adequacy (pseudo references). Experimental results suggest that they rival standard reference-based metrics in terms of correlations with human judgments on new test instances. ...|$|R
5000|$|... #Caption: Comparison of GENCODE <b>Human</b> {{versions}} (<b>Translations)</b> ...|$|R
40|$|N-gram {{measures}} of translation quality, such as BLEU {{and the related}} NIST metric, are becoming increasingly important in machine translation, yet their behaviors are not fully understood. In this paper we examine the performance of these metrics on professional <b>human</b> <b>translations</b> into German of two literary genres, the Bible and Tom Sawyer...|$|R
5000|$|Support {{for both}} <b>human</b> <b>translation</b> and fully {{integrated}} machine translation (MT) ...|$|E
50|$|Web-based <b>human</b> <b>translation</b> is {{generally}} favored by companies and individuals that wish to secure more accurate translations. In {{view of the}} frequent inaccuracy of machine translations, <b>human</b> <b>translation</b> remains the most reliable, most accurate form of translation available. With the recent emergence of translation crowdsourcing, translation-memory techniques, and internet applications, translation agencies {{have been able to}} provide on-demand human-translation services to businesses, individuals, and enterprises.|$|E
50|$|One {{field of}} {{research}} in the industry includes the possibility of machine translation fully replacing <b>human</b> <b>translation.</b>|$|E
5000|$|Charanusi mrov ... The {{destiny of}} a <b>Human</b> - <b>Translation</b> - stageplay ...|$|R
40|$|This paper {{describes}} the machine translation technology of Kielikone Ltd. and gives {{an outline of}} TranSmart, a Finnish-English MT system which is a commercial application of that base technology. We argue that MT is fundamentally empirical research. Product development is a slow and strenuous process and MT systems will remain incomplete not only vis-a-vis <b>human</b> <b>translations</b> but also {{with respect to the}} system's own potential translation quality. An evaluation method is described which measures the progress in MT development. This method can also be used for system and technology evaluation. This paper ends with the claim that the real contribution of MT will be seen in a longer run in applications that do not compete with <b>human</b> <b>translations</b> but in which MT is the only choice. 1. KIELIKONE MT This section {{describes the}} base MT technology developed by Kielikone. The technology is language independent and can be used for building MT systems for various language pairs. Kielikone has built [...] ...|$|R
40|$|The {{dramatic}} improvements {{shown by}} {{statistical machine translation}} systems in recent years clearly demonstrate the benefits of having large quantities of manually translated parallel text for system training and development. And while many competing evaluation metrics exist to evaluate MT technology, most of those methods also crucially rely {{on the existence of}} one or more high quality <b>human</b> <b>translations</b> to benchmark system performance. Given the importance of <b>human</b> <b>translations</b> in this framework, understanding the particular challenges of human translation-for-MT is key, as is comprehending the relative strengths and weaknesses of human versus machine translators {{in the context of an}} MT evaluation. Vanni (2000) argued that the metric used for evaluation of competence in human language learners may be applicable to MT evaluation; we apply similar thinking to improve the prediction of MT performance, which is currently unreliable. In the current paper we explore an alternate model based upon a set of genre-defining features that prove to be consistently challenging for both humans and MT systems. ...|$|R
50|$|These tools {{speed up}} and {{facilitate}} <b>human</b> <b>translation,</b> {{but they do}} not provide translation. That is a function of tools known broadly as machine translation.|$|E
50|$|Translate.com is a human-powered {{translation}} service based in Chicago, Illinois. The company offers a web-based <b>human</b> <b>translation</b> subscription platform {{in combination with}} artificial intelligence technologies.|$|E
5000|$|Score: To {{automatically}} score Moses translations {{against a}} <b>human</b> <b>translation</b> {{taken as a}} gold standard (BLEU and NIST metrics) {{in order to have}} an idea of the level of performance ...|$|E
5|$|BLEU’s {{output is}} always a number between 0 and 1. This value {{indicates}} how similar the candidate text is to the reference texts, with values closer to 1 representing more similar texts. Few <b>human</b> <b>translations</b> will attain a score of 1, since this would indicate that the candidate is identical {{to one of the}} reference translations. For this reason, {{it is not necessary to}} attain a score of 1. Because there are more opportunities to match, adding additional reference translations will increase the BLEU score.|$|R
40|$|Named entity phrases {{are some}} of the most {{difficult}} phrases to translate because new phrases can appear from nowhere, and because many are domain specific, not to be found in bilingual dictionaries. We present a novel algorithm for translating named entity phrases using easily obtain-able monolingual and bilingual resources. We report on the application and evalua-tion of this algorithm in translating Arabic named entities to English. We also com-pare our results with the results obtained from <b>human</b> <b>translations</b> and a commer-cial system for the same task. ...|$|R
40|$|International audienceIn {{this paper}} we analyze {{the problem of}} {{recognition}} of temporal expressions and their translations. Starting from named entities describing time (e. g. of the TIMEX type) and using FST cascades, temporal expressions were extracted from a text in French, followed by an analysis of (<b>human)</b> <b>translations</b> of these expressions in several languages, {{by means of an}} aligned corpus. The goal {{of this paper is to}} develop a formal description of such expressions, which enables the extraction of equivalent temporal expressions from aligned text...|$|R
5000|$|Podec, a trojan {{discovered}} by the security company Kaspersky, forwards CAPTCHA requests to an online <b>human</b> <b>translation</b> service that converts the image to text, fooling the system. Podec targets Android mobile devices.|$|E
50|$|Unbabel is an {{artificial}} intelligence powered <b>human</b> <b>translation</b> platform. Headquartered in San Francisco, California, {{with offices in}} Lisbon, Portugal, the company currently works with over 50,000 crowdsourced translators on desktop and mobile across 45 language pairs.|$|E
50|$|Translation Services USA is a {{translation}} technology company headquartered in New York City. The company {{has developed a}} number of popular online translation platforms such as Ackuna and TranslationCloud.net, {{as well as providing}} traditional <b>human</b> <b>translation</b> services.|$|E
40|$|To lay the {{foundations}} for a systematic procedure that {{could be applied to}} any scientific translation, this experiment evaluates the error variances attributable to various sources inherent in a design in which discrete, randomly ordered sentences from translations are rated for intelligibility and for fidelity to the original. The procedure is applied to three human and three mechanical translations into English of four passages from a Russian work on cybernetics, yielding mean scores for the <b>translations.</b> <b>Human</b> and mechanical <b>translations</b> are clearly different in over-all quality, although substantial overlap is noted when individual sentences are considered. The procedure also clearly differentiates within sets of <b>human</b> <b>translations</b> and within sets of mechanical translations. Results from the two scales are highly correlated, and these in turn are highly correlated with reading times. A procedure in which highly intelligent "monolingual" raters (i. e., without knowledge of the foreign language) compare a test translation with a carefully prepared translation is found to be more reliable than one in which "bilingual " raters compare the English translation with the Russian original...|$|R
40|$|Most {{state-of-the-art}} statistical {{machine translation}} systems use log-linear models, which are {{defined in terms of}} hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using heldout test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases can be used to cut the number of <b>human</b> reference <b>translations</b> needed in half. In this paper, we take the idea a step further, asking how far it is possible to get with just a single good reference translation for each item in the development set. Our analysis suggests {{that it is necessary to}} invest in four or more <b>human</b> <b>translations</b> in order to significantly improve on a single translation augmented by monolingual paraphrases. ...|$|R
5000|$|... (with others) The {{fundamentals}} of <b>human</b> and machine <b>translations,</b> 1966 ...|$|R
