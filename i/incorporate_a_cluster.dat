3|10000|Public
50|$|Work on the A10 was resumed in late 1944 {{under the}} Projekt Amerika codename, and the A10's design was amended to <b>incorporate</b> <b>a</b> <b>cluster</b> of 6 A4 {{combustion}} chambers feeding {{into a single}} expansion nozzle. This was later altered to a massive single chamber and single nozzle. Test stands were constructed at Peenemunde for firings of the 200 tonne thrust motor.|$|E
40|$|Firm {{formation}} {{has been}} advocated by policy-makers and examined by researchers {{as a vehicle}} for job creation and economic development. Both industrial structures and the firm formation rates of individual industries vary regionally. For instance, Ashcroft et al (1991) showed that firm formation rates vary significantly between U. K. counties. While shift-share analysis {{has been used as a}} decomposition technique (Dunn, 1960) to account for these differences, a shortcoming is that the regional shift is affected by the level of regional employment in a given industry. Also, firm formation rates in each industry are likely to be partly determined by the industrial structure of a region. This paper extends the shift-share methodology developed by Johnson (1983) to <b>incorporate</b> <b>a</b> <b>cluster</b> analysis of the industrial structure of regional employment in order to further separate regional and sectoral components of firm formation in British counties in the 1980 s and 1990 s. Firm formation is measured using VAT registration rates. ...|$|E
40|$|An {{ever-increasing}} {{number of}} functional {{magnetic resonance imaging}} (fMRI) studies are now using information-based multi-voxel pattern analysis (MVPA) techniques to decode mental states. In doing so, they achieve a significantly greater sensitivity compared to when they use univariate analysis frameworks. Two most prominent MVPA methods for information mapping are searchlight decoding and classifier weight mapping. The new MVPA brain mapping methods, however, have also posed new challenges for analysis and statistical inference on the group level. In this thesis, I discuss why the usual procedure of performing t-tests on MVPA derived information maps across subjects in order to produce a group statistic is inappropriate. I propose a fully nonparametric solution to this problem, which achieves higher sensitivity than the most commonly used t-based procedure. The proposed method is based on resampling methods and preserves the spatial dependencies in the MVPA-derived information maps. This enables to <b>incorporate</b> <b>a</b> <b>cluster</b> size control for the multiple testing problem. Using a volumetric searchlight decoding procedure and classifier weight maps, I demonstrate the validity and sensitivity of the new approach using both simulated and real fMRI data sets. In comparison to the standard t-test procedure implemented in SPM 8, the new results showed a higher sensitivity and spatial specificity. The second goal of this thesis is the comparison of the two widely used information mapping approaches [...] the searchlight technique and classifier weight mapping. Both methods take into account the spatially distributed patterns of activation in order to predict stimulus conditions, however the searchlight method solely operates on the local scale. The searchlight decoding technique has furthermore been found to be prone to spatial inaccuracies. For instance, the spatial extent of informative areas is generally exaggerated, and their spatial configuration is distorted. In this thesis, I compare searchlight decoding with linear classifier weight mapping, both using the formerly proposed non-parametric statistical framework using a simulation and ultra-high-field 7 T experimental data. It was found that the searchlight method led to spatial inaccuracies that are especially noticeable in high-resolution fMRI data. In contrast, the weight mapping method was more spatially precise, revealing both informative anatomical structures as well as the direction by which voxels contribute to the classification. By maximizing the spatial accuracy of ultra-high-field fMRI results, such global multivariate methods provide a substantial improvement for characterizing structure-function relationships...|$|E
40|$|Summary: SARGE is a {{tool for}} creating, {{visualizing}} and manipulating a putative genetic network from time series microarray data. The tool assigns potential edges through time-lagged correlation, <b>incorporates</b> <b>a</b> <b>clustering</b> mechanism, <b>an</b> interactive visual graph representation and employs simulated annealing for network optimization. Availability: The application is available as a. jar file fro...|$|R
40|$|The {{concept of}} a “mutualistic teacher” is {{introduced}} for unsupervised learning of the mean vectors {{of the components of}} a mixture of multivariate normal densities, when the number of classes is also unknown. The unsupervised learning problem is formulated here as a multi-stage quasi-supervised problem <b>incorporating</b> <b>a</b> <b>cluster</b> approach. The mutualistic teacher creates a quasi-supervised environment at each stage by picking out “mutual pairs” of samples and assigning identical (but unknown) labels to the individuals of each mutual pair. The number of classes, if not specified, can be determined at an intermediate stage. The risk in assigning identical labels to the individuals of mutual pairs is estimated. Results of some simulation studies are presented...|$|R
40|$|International audienceThis {{research}} aims {{to develop}} a new team building method on competency modelling in the field of project management. This method is divided into three main stages. First, a competency matrix based on a task-actor compatibility indicator helps to characterise the competency levels. Second, we apply <b>a</b> <b>clustering</b> algorithm {{in order to reduce the}} problem complexity and favour the employees expertise. The clustering algorithm will decompose large sets of tasks and actors into smaller task groups related to different actor groups. It facilitates the project leaders to organise the actors into teams. Finally, the proposed task assignment model <b>incorporates</b> <b>a</b> learning curve in order to take the competency dynamics into account. Our computational experiments suggest that <b>incorporating</b> <b>a</b> <b>clustering</b> algorithm as <b>a</b> step of the method results in preserving expertise and thus helps project managers to find better tradeoffs between project cost (short term goal) and competency dynamics (long term goal) ...|$|R
40|$|Food {{product safety}} {{is one of}} the most {{promising}} areas for the application of electronic noses. The performance of a portable electronic nose has been evaluated in monitoring the spoilage of beef fillet stored aerobically at different storage temperatures (0, 4, 8, 12, 16 and 20 °C). This paper proposes a fuzzy-wavelet neural network model which <b>incorporates</b> <b>a</b> <b>clustering</b> pre-processing stage for the definition of fuzzy rules. The dual purpose of the proposed modeling approach is not only to classify beef samples in the respective quality class (i. e. fresh, semi-fresh and spoiled), but also to predict their associated microbiological population directly from volatile compounds fingerprints. Comparison results indicated that the proposed modeling scheme could be considered as a valuable detection methodology in food microbiolog...|$|R
40|$|Freshness {{and safety}} of muscle foods are {{generally}} considered {{as the most important}} parameters for the food industry. To address the rapid detection of meat spoilage microorganisms during aerobic or modified atmosphere storage, an electronic nose with the aid of fuzzy wavelet network has been considered in this research. The proposed model <b>incorporates</b> <b>a</b> <b>clustering</b> pre-processing stage for the definition of fuzzy rules. The dual purpose of the proposed modelling approach is not only to classify beef samples in the respective quality class (i. e. fresh, semi-fresh and spoiled), but also to predict their associated microbiological population directly from volatile compounds fingerprints. Comparison results against neural networks and neurofuzzy systems indicated that the proposed modelling scheme could be considered as a valuable detection methodology in food microbiolog...|$|R
40|$|This report {{constructs}} {{a profile}} of the 225, 000 eighth graders attending Catholic schools in the United States in 1988 and compares them to eighth graders attending public schools. The analysis focused on themes of excellence and equity. Study data were taken from the National Education Longitudinal Study of 1988. The study design <b>incorporated</b> <b>a</b> <b>clustered,</b> stratified national probability sample of approximately 1, 000 schools (approximately 800 public schools and. 200 private schools), with an average of 25 students in each school participating. Following an introduction, chapter 1, "Catholic Schools and Their Eighth Grade Students," provides an overview of Catholic schools with eighth grades; compares them to public schools, with particular attention to urban areas; and notes that with respect to achievement, urban Catholic students clearly outperform their public school counterparts. Chapter 2...|$|R
40|$|Semiconductor {{fabrication}} facilities {{require an}} increasingly expensive and inte-grated set of processes. The bounds on efficiency and repeatability for each process step continue to tighten {{under the pressure}} of economic forces and product performance requirements. This thesis addresses these issues and describes the concept of an “Equip-ment Cell, ” which integrates sensors and data processing software around an individual piece of semiconductor equipment. Distributed object technology based on open standards is specified and utilized for software modules that analyze and improve semiconductor equipment processing capabilities. A testbed system for integrated, model-based, run-to-run control of epitaxial sili-con (epi) film deposition is developed, <b>incorporating</b> <b>a</b> <b>cluster</b> tool with <b>a</b> single-wafer epi deposition chamber, an in-line epi film thickness measurement tool, and off-line thickness and resistivity measurement systems. Automated single-input-single-output, run-to-ru...|$|R
40|$|Markov {{models have}} been widely {{utilized}} for modelling user web navigation behaviour. In this work we propose a dynamic clustering-based method to increase a Markov model’s accuracy in representing a collection of user web navigation sessions. The method makes use of the state cloning concept to duplicate states {{in a way that}} separates in-links whose corresponding second-order probabilities diverge. In addition, the new method <b>incorporates</b> <b>a</b> <b>clustering</b> technique which determines an efficient way to assign in-links with similar second-order probabilities to the same clone. We report on experiments conducted with both real and random data and we provide a comparison with the N-gram Markov concept. The results show that the number of additional states induced by the dynamic clustering method can be controlled through a threshold parameter, and suggest that the method’s performance is linear time {{in the size of the}} model...|$|R
40|$|Software {{testing is}} the most {{important}} part of software development life cycle. There are various types in software testing which have their own different functionalities. Among them regression testing is most useful functional type of testing which is done in the software maintence phase. This testing is used to check the errors when any change is made in the existing system. To make system efficient and effective, techniques of test case prioritization are use. The reduction in the cost of testing and fault detection capabilities of testing should be done by test case prioritization. This technique is also applied on different algorithms to improve their efficiency. Many clustering algorithms may also use test case prioritization method to increase the efficiency in code coverage. Prioritization techniques that <b>incorporate</b> <b>a</b> <b>clustering</b> approach and utilize code coverage, code complexity to increase the effectiveness of the prioritization...|$|R
40|$|Target {{tracking}} {{is a well}} studied {{topic in}} wireless sensor networks. It is a procedure that nodes in the network collaborate in detecting targets and transmitting their information to the base-station continuously, which leads to data implosion and redundancy. To reduce traffic load of the network, a data compressing based target tracking protocol is proposed in this work. It first <b>incorporates</b> <b>a</b> <b>clustering</b> based data gather method to group sensor nodes into <b>clusters.</b> Then <b>a</b> novel threshold technique with bounded error is proposed to exploit the spatial correlation of sensed data and compress the data in the same cluster. Finally, the compact data presentations are transmitted to the base-station for targets localization. We evaluate our approach with a comprehensive set of simulations. It can be concluded that the proposed method yields excellent performance in energy savings and tracking quality...|$|R
30|$|Data are {{expressed}} as mean[*]±[*]SD or median with interquartile range (IQR) {{according to the}} distribution of the data. One-way analysis of variance or Wilcoxon signed-rank test (according to {{the distribution of the}} data) was performed to compare continuous variables. To identify variables associated with depression, logistic regressions (forward-stepwise selection) were performed. All variables with a p value[*]<[*] 0.2 in the univariate analysis were entered in the model. The final models expressed the odds ratios (OR) and 95 % confidence intervals (CI). Because some of the predictor variables used in the analyses was collected {{at the level of the}} ICU rather than the level of the physician, an analytic approach that <b>incorporates</b> <b>a</b> <b>clustered</b> design [generalized estimating equations (GEE) methodology)] was also used. A p value[*]<[*] 0.05 indicated significance. The statistical analyses were performed by using the SPSS software package version 15.0 (SPSS Inc., Chicago, IL).|$|R
40|$|Overexpression {{of certain}} tumor-associated {{carbohydrate}} antigens (TACA) caused by malignant transformation offers promising targets to develop novel antitumor vaccines, provided {{the ability to}} break their inherent low immunogenicity and overcome the tolerance of the immune system. We designed, synthesized, and immunologically evaluated a number of fully synthetic new chimeric constructs <b>incorporating</b> <b>a</b> <b>cluster</b> {{of the most common}} TACA (known as Tn antigen) covalently attached to T-cell peptide epitopes derived from polio virus and ovalbumin and included a synthetic built-in adjuvant consisting of two 16 -carbon lipoamino acids. Vaccine candidates were able to induce significantly strong antibody responses in mice without the need for any additional adjuvant, carrier protein, or special pharmaceutical preparation (e. g., liposomes). Vaccine constructs were assembled either in a linear or in a branched architecture, which demonstrated the intervening effects of the incorporation and arrangement of T-cell epitopes on antibody recognition...|$|R
40|$|Food {{product safety}} {{is one of}} the most {{promising}} areas for the application of electronic noses. During the last twenty years, these sensor-based systems have made odour analyses possible. Their application into the area of food is mainly focused on quality control, freshness evaluation, shelf-life analysis and authenticity assessment. In this paper, the performance of a portable electronic nose has been evaluated in monitoring the spoilage of beef fillets stored either aerobically or under modified atmosphere packaging, at different storage temperatures. A novel multi-output fuzzy wavelet neural network model has been developed, which <b>incorporates</b> <b>a</b> <b>clustering</b> pre-processing stage for the definition of fuzzy rules. The dual purpose of the proposed modelling approach is not only to classify beef samples in the relevant quality class (i. e. fresh, semi-fresh and spoiled), but also to predict their associated microbiological population. Comparison results against advanced machine learning schemes indicated that the proposed modelling scheme could be considered as a valuable detection methodology in food microbiology...|$|R
40|$|The {{experimental}} {{evaluation of}} many graph algorithms for practical use involves both tests on real-world data and on artificially generated data sets. In particular {{the latter are}} useful for systematic and very specific evaluations. Roughly speaking, {{we are interested in}} the generation of dynamic random graphs that feature a community structure of scalable clarity such that (i) the graph changes dynamically by node/edge insertions/deletions and (ii) the graph <b>incorporates</b> <b>a</b> <b>clustering</b> structure (communities), which also changes dynamically. The wide variety of generators for random graphs has not yet tackled such dynamically changing preclustered graphs. In this work we describe a random graph generator which is based on the Erdős-Rényi model but adds to it tunable dynamics and a tunable and evolving clustering structure. More precisely, <b>an</b> evolving ground-truth <b>clustering</b> known by the our generator motivates the changes to the graph by sound probabilities, such that the visible <b>clustering</b> changes accordingly. <b>A</b> <b>clustering</b> in this context is a partition of the node set into clusters, which internally are rather densely interconnected, but do not share many edges in between one another. We detail our implementation as a module of the software tool visone and as a standalone tool, alongside the data structures we use. 1 Ou...|$|R
40|$|Gold nano-clusters {{were grown}} on {{chemically}} modified graphene by direct sputter deposition. Transmission electron microscopy of the nano-clusters on these electron-transparent substrates reveals an unusual bimodal island size distribution (ISD). A kinetic Monte Carlo model of growth <b>incorporating</b> <b>a</b> size-dependent <b>cluster</b> mobility rule uniquely reproduces the bimodal ISD, providing strong {{evidence for the}} mobility of large clusters during surface growth. The cluster mobility exponent of − 5 / 3 is consistent with cluster motion via one-dimensional diffusion of gold atoms {{around the edges of}} the nano-clusters...|$|R
40|$|International audienceWe {{address the}} problem of {{detecting}} multiple audiovisual events related to the edit structure of <b>a</b> video by <b>incorporating</b> <b>an</b> unsupervised <b>cluster</b> analysis technique into <b>a</b> <b>cluster</b> selection method designed to measure coherence between audio and visual segments. First, mutual information measure is used to select audio-visually consistent clusters from two dendrograms representing hierarchical clustering results respectively for the audio and visual modalities. <b>A</b> <b>cluster</b> analysis technique is then applied to define events from the audio-visual (AV) clusters with segments co-occurring frequently. Candidate events are then characterized by groups of AV clusters from which models are built by automatically selecting positive and negative examples. Experiments on the standard Canal 9 data set demonstrates that our method is capable of discovering multiple audiovisual events in a totally unsupervised manner...|$|R
40|$|Abstract. One of {{the main}} {{challenges}} {{in the design of}} modern clustering algorithms is that, in many applications, new data sets are continuously added into an already huge database. As a result, it is impractical to carry out data clustering from scratch whenever there are new data instances added into the database. One way to tackle this challenge is to <b>incorporate</b> <b>a</b> <b>clustering</b> algorithm that operates incrementally. Another desirable feature of clustering algorithms is that <b>a</b> <b>clustering</b> dendrogram is generated. This feature is crucial for many applications in biological, social, and behavior studies, due to the need to construct taxonomies. This paper presents the GRIN algorithm, <b>an</b> incremental hierarchical <b>clustering</b> algorithm for numerical data sets based on gravity theory in physics. The GRIN algorithm delivers favorite clustering quality and generally features O(n) time complexity. One main factor that makes the GRIN algorithm be able to deliver favorite clustering quality is that the optimal parameters settings in the GRIN algorithm are not sensitive to the distribution of the data set. On the other hand, many modern clustering algorithms suffer unreliable or poor clustering quality when the data set contains highly skewed local distributions so that no optimal values can be found for some global parameters. This paper also reports the experiments conducted to study the characteristics of the GRIN algorithm...|$|R
40|$|An {{innovative}} profile {{monitoring methodology}} is introduced for Phase I analysis. The proposed technique, which {{is referred to}} as the cluster-based profile monitoring method, <b>incorporates</b> <b>a</b> <b>cluster</b> analysis phase to aid in determining if non conforming profiles are present in the historical data set (HDS). To cluster the profiles, the proposed method first replaces the data for each profile with an estimated profile curve, using some appropriate regression method, and clusters the profiles based on their estimated parameter vectors. This cluster phase then yields <b>a</b> main <b>cluster</b> which contains more than half of the profiles. The initial estimated population average (PA) parameters are obtained by fitting a linear mixed model to those profiles in the main cluster. In-control profiles, determined using the Hotelling’s 2 T statistic, that are not contained in the initial main cluster are iteratively added to the main cluster and the mixed model is used to update the estimated PA parameters. A simulated example and Monte Carlo results demonstrate the performance advantage of this proposed method over a current non-cluster based method with respect to more accurate estimates of the PA parameters and better classification performance in determining those profiles from an in-control process from those from an out-of-control process in Phase I. ...|$|R
40|$|In food industry, {{quality and}} safety are {{considered}} important issues worldwide that {{are directly related}} to health and social progress. The use of vision technology for quality testing of food production has the obvious advantage of being able to continuously monitor a production using non-destructive methods, thus increasing the quality and minimizing cost. The performance of an intelligent decision support system has been evaluated in monitoring the spoilage of minced beef stored either aerobically or under modified atmosphere packaging, at different storage temperatures (0, 5, 10, and 15 °C) utilising multispectral imaging information. This paper utilises a neuro-fuzzy model which <b>incorporates</b> <b>a</b> <b>clustering</b> pre-processing stage for the definition of fuzzy rules, while its final fuzzy rule base is determined by competitive learning. Initially, meat samples are classified according to their storage conditions, while identification models are then utilised for the prediction of the Total Viable Counts of bacteria. The innovation of the proposed approach is further extended to the identification of the temperature used for storage, utilizing only imaging spectral information. Results indicated that spectral information in combination with the proposed modelling scheme could be considered as an alternative methodology for the accurate evaluation of meat spoilage...|$|R
40|$|AbstractIn recent years, {{there have}} been many time series methods {{proposed}} for forecasting enrollments, weather, the economy, population growth, and stock price, etc. However, traditional time series, such as ARIMA, expressed by mathematic equations are unable to be easily understood for stock investors. Besides, fuzzy time series can produce fuzzy rules based on linguistic value, which is more reasonable than mathematic equations for investors. Furthermore, from the literature reviews, two shortcomings are found in fuzzy time series methods: (1) they lack persuasiveness in determining the universe of discourse and the linguistic length of intervals, and (2) only one attribute (closing price) is usually considered in forecasting, not multiple attributes (such as closing price, open price, high price, and low price). Therefore, this paper proposes a multiple attribute fuzzy time series (FTS) method, which <b>incorporates</b> <b>a</b> <b>clustering</b> method and adaptive expectation model, to overcome the shortcomings above. In verification, using actual trading data of the Taiwan Stock Index (TAIEX) as experimental datasets, we evaluate the accuracy of the proposed method and compare the performance with the (Chen, 1996 [7], Yu, 2005 [6], and Cheng, Cheng, & Wang, 2008 [20]) methods. The proposed method is superior to the listing methods based on average error percentage (MAER) ...|$|R
40|$|We {{consider}} {{the problem of}} producing item recommenda-tions that are personalized based on a user’s social network, while simultaneously preventing the disclosure of sensitive user-item preferences (e. g., product purchases, ad clicks, web browsing history, etc.). Our main contribution is a privacy-preserving framework for a class of social recommendation algorithms that provides strong, formal privacy guarantees under the model of differential privacy. Existing mechanisms for achieving differential privacy lead to an unacceptable loss of utility when applied to the social recommendation prob-lem. To address this, the proposed framework <b>incorporates</b> <b>a</b> <b>clustering</b> procedure that groups users according to the natural community structure of the social network and sig-nificantly reduces the amount of noise required to satisfy differential privacy. Although this reduction in noise comes {{at the cost of}} some approximation error, we show that the benefits of the former significantly outweigh the latter. We explore the privacy-utility trade-off for several different in-stantiations of the proposed framework on two real-world data sets and show that useful social recommendations can be produced without sacrificing privacy. We also experimen-tally compare the proposed framework with several existing differential privacy mechanisms and show that the proposed framework significantly outperforms all of them in this set-ting. 1...|$|R
40|$|ABSTRACT: The {{prototype}} ferredoxin maquette, FdM, is a 16 -amino acid peptide which efficiently <b>incorporates</b> <b>a</b> single [4 Fe- 4 S] 2 +/+ cluster with spectroscopic and electrochemical {{properties that}} {{are typical of}} natural bacterial ferredoxins. Using this synthetic protein scaffold, we have investigated {{the role of the}} nonliganding amino acids in the assembly of the iron-sulfur <b>cluster.</b> In <b>a</b> stepwise fashion, we truncated FdM to a seven-amino acid peptide, FdM- 7, which <b>incorporates</b> <b>a</b> <b>cluster</b> spectroscopically identical to FdM but in lower yield, 29 % relative to FdM. FdM- 7 consists solely of the âCIACGACâ consensus ferredoxin core motif observed in natural protein sequences. Initially, all of the nonliganding amino acids were substituted for either glycine, FdM- 7 -PolyGly (âCGGCGGCâ), or alanine, FdM- 7 -PolyAla (âCAACAACâ), on the basis of analysis of natural ferredoxin sequences. Both FdM- 7 -PolyGly and FdM- 7 -PolyAla incorporated little [4 Fe- 4 S] 2 +/+ cluster, 6 and 7 %, respectively. A systematic study of the incorporation of a single isoleucine into each of the four nonliganding positions indicated that placement either in the second or in the sixth core motif positions, âCIGCGGCâ or âCGGCGICâ, restored the iron-sulfur cluster binding capacity of the peptides to the level of FdM- 7. Incorporation of an isoleucine into the fifth position, âCGGCIGCâ, which in natural ferredoxins is predominately occupied by a glycine, resulted in a loss of [4 Fe- 4 S] affinity. The substitution of leucine, tryptophan, and arginine into the secon...|$|R
40|$|<b>Clustering</b> {{algorithms}} conduct <b>a</b> {{search through}} {{the space of}} possible organizations of a data set. In this paper, we propose two types of instance-level clustering constraints – must-link and cannot-link constraints – and show {{how they can be}} <b>incorporated</b> into <b>a</b> <b>clustering</b> algorithm to aid that search. For three of the four data sets tested, our results indicate that the incorporation of surprisingly few such constraints can increase clustering accuracy while decreasing runtime. We also investigate the relative effects of each type of constraint and find that the type that contributes most to accuracy improvements depends on the behavior of the clustering algorithm without constraints. 1...|$|R
40|$|This paper {{presents}} a scheme for segmenting {{images on the}} basis of di#erences in localised measures of spatial texture. The scheme used was originally proposed by Wilson and Spann [1] but <b>incorporates</b> <b>a</b> new <b>clustering</b> algorithm which gives improved overall segmentation performance. The Wilson and Spann [1] algorithm uses <b>a</b> <b>clustering</b> algorithm which proved susceptible to initial input parameters and gave poor segmentation on our images. Our algorithm uses a modification of the Koontz, Narendra and Fukunaga [2] clustering algorithm. By linking the clustering to the resolution of the image, significant clusters were able to be realised, yielding a more robust segmentation scheme. The adaptation also resulted in a significant reduction in run-time. The paper is directed towards the problem of segmenting satellite synthetic aperture radar (SAR) images and we give comparisons of the techniques on SAR and other images...|$|R
40|$|Evolutionary {{algorithms}} (EA’s) {{are often}} well-suited for optimization problems involving several, often conflicting objectives. Since 1985, various evolutionary approaches to multiobjective optimization {{have been developed}} {{that are capable of}} searching for multiple solutions concurrently in a single run. However, the few comparative studies of different methods presented up to now remain mostly qualitative and are often restricted to a few approaches. In this paper, four multiobjective EA’s are compared quantitatively where an extended 0 / 1 knapsack problem is taken as a basis. Furthermore, we introduce a new evolutionary approach to multicriteria optimization, the Strength Pareto EA (SPEA), that combines several features of previous multiobjective EA’s in a unique manner. It is characterized by a) storing nondominated solutions externally in a second, continuously updated population, b) evaluating an individual’s fitness dependent on the number of external nondominated points that dominate it, c) preserving population diversity using the Pareto dominance relationship, and d) <b>incorporating</b> <b>a</b> <b>clustering</b> procedure {{in order to reduce the}} nondominated set without destroying its characteristics. The proof-of-principle results obtained on two artificial problems as well as a larger problem, the synthesis of a digital hardware–software multiprocessor system, suggest that SPEA can be very effective in sampling from along the entire Pareto-optimal front and distributing the generated solutions over the tradeoff surface. Moreover, SPEA clearly outperforms the other four multiobjective EA’s on the 0 / 1 knapsack problem...|$|R
40|$|One of {{the current}} trends in {{molecular}} material science concerns the preparation {{and the study of}} materials combining several physical properties in a synergistic way. 1 Organic/inorganic hybrids, in which organic- or organo-metallic- moieties having a special physicochemical prop-erty are <b>incorporated</b> into <b>a</b> transition-metal <b>cluster,</b> may exhibit certain coupling phenomena between the d-electrons of inorganic transition-metal networks and the mobile π-electrons of the organic conjugated networks...|$|R
40|$|In this paper, {{we present}} FastPlace 3. 0 – an {{efficient}} and scalable multilevel quadratic placement algorithm for large-scale mixed-size designs. The main contributions {{of our work}} are: (1) A multilevel global placement framework, by <b>incorporating</b> <b>a</b> two-level <b>clustering</b> scheme within the flat analytical placer FastPlace [27, 28]. (2) An efficient and improved Iterative Local Refinement technique that can handle placement blockages and placement congestion constraints. (3) A congestion aware standard-cell legalization technique {{in the presence of}} blockages. On the ISPD- 2005 placement benchmarks [19], our algorithm is 5. 12 ×, 11. 52 × and 16. 92 × faster than mPL 6, Capo 10. 2 and APlace 2. 0 respectively. In terms of wirelength, we are on average...|$|R
40|$|The main {{objective}} of this work {{is to improve the}} quality of the results provided by search engines in the Internet. In order to get this aim, we have developed the metasearcher FISS (Fuzzy Interrelations and Synonymy based Searcher) that <b>incorporates</b> <b>a</b> soft <b>clustering</b> component whose similarity function considers the cooccurrence of concepts of the retrieved documents. The metasearcher uses an extension of the vector space model, FIS-CRM, which provides a mechanism to represent the concepts contained in a document. This model is based on the study of two types of fuzzy interrelations (synonymy and generality). The final result of the search process is a set of groups of conceptually related web pages...|$|R
40|$|This work is {{supported}} by the Co-Operative Research Centre for Sensor Signal and Information Processing. This paper presents a scheme for segmenting images on the basis of differences in localised measures of spatial texture. The scheme used was originally proposed by Wilson and Spann [1] but <b>incorporates</b> <b>a</b> new <b>clustering</b> algorithm which gives improved overall segmentation performance. The Wilson and Spann [1] algorithm uses <b>a</b> <b>clustering</b> algorithm which proved susceptible to initial input parameters and gave poor segmentation on our images. Our algorithm uses a modification of the Koontz, Narendra and Fukunaga [2] clustering algorithm. By linking the clustering to the resolution of the image, significant clusters were able to be realised, yielding a more robust segmentation scheme. The adaptation also resulted in a significant reduction in run-time. The paper is directed towards the problem of segmenting satellite synthetic aperture radar (SAR) images and we give comparisons of the techniques on SAR and other images. ...|$|R
40|$|BACKGROUND: Road traffic-related {{injury is}} a major global public health problem. In most countries, {{pedestrian}} injuries occur predominantly to the poorest in society. A number of evaluated interventions are effective in reducing these injuries. Very {{little research has been}} carried out into the distribution and determinants of the uptake of these interventions. Previous research has shown an association between local political influence and the distribution of traffic calming after adjustment for historical crash patterns. This led to the hypothesis that advocacy could be used to increase local politicians knowledge of pedestrian injury risk and effective interventions, ultimately resulting in improved pedestrian safety. OBJECTIVE: To design an intervention to improve the uptake of pedestrian safety measures in deprived communities. SETTING: Electoral wards in deprived areas of England and Wales with a poor record of pedestrian safety for children and older adults. METHODS: Design mixedmethods study, <b>incorporating</b> <b>a</b> <b>cluster</b> randomized controlled trial. Data mixture of Geographical Information Systems data collision locations, road safety interventions, telephone interviews, and questionnaires. Randomization 239 electoral wards clustered within 57 local authorities. Participants 615 politicians representing intervention and control wards. Intervention a package of tailored information including maps of pedestrian injuries was designed for intervention politicians, and a general information pack for controls. OUTCOME MEASURES: Primary outcome number of road safety interventions 25 months after randomization. Secondary outcomes politicians interest and involvement in injury prevention cost of interventions. Process evaluation use of advocacy pack, facilitators and barriers to involvement, and success...|$|R
40|$|Many cluster {{management}} systems (CMSs) {{have been proposed}} to share <b>a</b> single <b>cluster</b> with multiple distributed computing systems. However, none of the existing approaches can handle distributed machine learning (ML) workloads given the following criteria: high resource utilization, fair resource allocation and low sharing overhead. To solve this problem, we propose a new CMS named Dorm, <b>incorporating</b> <b>a</b> dynamically-partitioned <b>cluster</b> management mechanism and an utilization-fairness optimizer. Specifically, Dorm uses the container-based virtualization technique to partition <b>a</b> <b>cluster,</b> runs one application per partition, and can dynamically resize each partition at application runtime for resource efficiency and fairness. Each application directly launches its tasks on the assigned partition without petitioning for resources frequently, so Dorm imposes flat sharing overhead. Extensive performance evaluations showed that Dorm could simultaneously increase the resource utilization {{by a factor of}} up to 2. 32, reduce the fairness loss by a factor of up to 1. 52, and speed up popular distributed ML applications by a factor of up to 2. 72, compared to existing approaches. Dorm's sharing overhead is less than 5 % in most cases...|$|R
40|$|Abstract — In this paper, {{we present}} FastPlace 3. 0 – an {{efficient}} and scalable multilevel quadratic placement algorithm for large-scale mixed-size designs. The main contributions {{of our work}} are: (1) A multilevel global placement framework, by <b>incorporating</b> <b>a</b> two-level <b>clustering</b> scheme within the flat analytical placer FastPlace [27, 28]. (2) An efficient and improved Iterative Local Refinement technique that can handle placement blockages and placement congestion constraints. (3) A congestion aware standard-cell legalization technique {{in the presence of}} blockages. On the ISPD- 2005 placement benchmarks [19], our algorithm is 5. 12 ×, 11. 52 × and 16. 92 × faster than mPL 6, Capo 10. 2 and APlace 2. 0 respectively. In terms of wirelength, we are on average...|$|R
40|$|Spectral {{reflectance}} {{estimation of}} an object via low-dimensional snapshot requires both image acquisition and a post numerical estimation analysis. In this study, we set up <b>a</b> system <b>incorporating</b> <b>a</b> homemade <b>cluster</b> of LEDs with spectral modulation for scene illumination, and a multi-channel CCD to acquire multichannel images by means of fully digital process. Principal component analysis (PCA) and pseudo inverse transformation were used to reconstruct the spectral reflectance in a constrained training set, such as Munsell and Macbeth Color Checker. The average reflectance spectral RMS error from 34 patches of a standard color checker were 0. 234. The purpose is to investigate the use of system {{in conjunction with the}} imaging analysis for industry or medical inspection in a fast and acceptable accuracy, where the approach was preliminary validated...|$|R
