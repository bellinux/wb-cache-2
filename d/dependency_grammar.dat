383|209|Public
25|$|Owens, J. 1984. On {{getting a}} head: A problem in <b>dependency</b> <b>grammar.</b> Lingua 66, 25–42.|$|E
25|$|Osborne, T. 2008. Major constituents: And two <b>dependency</b> <b>grammar</b> {{constraints}} on sharing in coordination. Linguistics 46, 6, 1109-1165.|$|E
25|$|Groß, T. and T. Osborne 2009. Toward a {{practical}} <b>dependency</b> <b>grammar</b> theory of discontinuities. SKY Journal of Linguistics 22, 43-90.|$|E
5000|$|While phrase {{structure}} grammars (constituency grammars) acknowledge both finite and non-finite VPs as constituents (complete subtrees), <b>dependency</b> <b>grammars</b> {{reject the}} former. That is, <b>dependency</b> <b>grammars</b> acknowledge only non-finite VPs as constituents; finite VPs do not qualify as constituents in <b>dependency</b> <b>grammars.</b> For example: ...|$|R
40|$|Abstract. We link {{generative}} <b>dependency</b> <b>grammars</b> meeting natural modularity requirements with underspecified semantics of Discourse Plans {{intended to}} account for exactly those meaning components that grammars of languages mark for. We complete this link with a natural compilation of the modular <b>dependency</b> <b>grammars</b> into strongly equivalent efficiently analysed categorial <b>dependency</b> <b>grammars.</b> ...|$|R
40|$|In this paper, {{we define}} <b>Dependency</b> Structure <b>Grammars</b> (DSG), which are {{rewriting}} rule grammars generating sentences {{together with their}} dependency structures, are more expressive than CF-grammars and non-equivalent to mildly context-sensitive grammars. We show that DSG are weakly equivalent to Categorial <b>Dependency</b> <b>Grammars</b> (CDG) recently introduced. In particular, these <b>dependency</b> <b>grammars</b> naturally express long distance dependencies and enjoy good mathematical properties...|$|R
25|$|Osborne, T., M. Putnam, and T. Groß 2011. Bare phrase structure, label-less trees, and specifier-less syntax: Is Minimalism {{becoming}} a <b>dependency</b> <b>grammar?</b> The Linguistic Review 28, 315–364.|$|E
25|$|Alternate {{theoretical}} {{approaches to}} syntax make different assumptions regarding {{what is considered}} a constituent. In mainstream phrase structure grammar (and its derivatives), individual words are constituents {{in and of themselves}} as well as being parts of other constituents, whereas in <b>dependency</b> <b>grammar,</b> certain core words in each phrase are not a constituent by themselves, but only members of a phrasal constituent. The following trees show the same sentence in two different theoretical representations, with a phrase structure representation on the left and a <b>dependency</b> <b>grammar</b> representation on the right. In both trees, a constituent is understood to be the entire tree or any labelled subtree (a node plus all the nodes dominated by that node); note that words like killed and with, for instance, form subtrees (and are considered constituents) in the phrase structure representation but not in the dependency structure representation.|$|E
25|$|The {{traditional}} {{focus on}} hierarchical order generated {{the impression that}} DGs have little to say about linear order, and it {{has contributed to the}} view that DGs are particularly well-suited to examine languages with free word order. A negative result of this focus on hierarchical order, however, {{is that there is a}} dearth of dependency-based explorations of particular word order phenomena, such as of standard discontinuities. Comprehensive <b>dependency</b> <b>grammar</b> accounts of topicalization, wh-fronting, scrambling, and extraposition are mostly absent from many established dependency-based frameworks. This situation can be contrasted with constituency grammars, which have devoted tremendous effort to exploring these phenomena.|$|E
40|$|We present “CDG LAB”, a toolkit for {{development}} of <b>dependency</b> <b>grammars</b> and treebanks. It uses the Categorial <b>Dependency</b> <b>Grammars</b> (CDG) as a formal model of <b>dependency</b> <b>grammars.</b> CDG are very expressive. They generate unlimited dependency structures, are analyzed in polynomial time and are conservatively extendable by regular type expressions without loss of parsing efficiency. Due to these features, they are well adapted to definition of large scale grammars. CDG LAB supports the analysis of correctness of treebanks developed in parallel with evolving grammars. ...|$|R
40|$|Results of {{computational}} complexity {{exist for}} a wide range of phrase structure-based grammar formalisms, while there is an apparent lack of such results for dependencybased formalisms. We here adapt a result on the complexity of ID/LP-grammars to the dependency framework. Contrary to previous studies on heavily restricted <b>dependency</b> <b>grammars,</b> we prove that recognition (and thus, parsing) of linguistically adequate <b>dependency</b> <b>grammars</b> is N P-complete. ...|$|R
40|$|The {{question}} arises {{from time to}} time what the relation is between <b>dependency</b> <b>grammars</b> (DG’s) and phrase-structure grammars. A classic paper by Gaifman [1] would appear to have laid the issue to rest, by proving that <b>dependency</b> <b>grammars</b> are a special case of context-free grammars (CFG’s). Gaifman proves that <b>dependency</b> <b>grammars</b> are equivalent to a proper subset of phrasestructure grammars, those of degree ≤ 1, which I will dub d 1 -CFG’s. (As degree cannot be explained in a few words, I will leave it undefined for the moment.) Under a weaker notion of correspondence, which Gaifman attributes to Hays [2], <b>dependency</b> <b>grammars</b> correspond to finite-degree CFG’s, which represent a larger subset of CFG’s, but a proper subset nonetheless. I submit, however, that Gaifman correlates DG’s with a proper subset of CFG’s only by suppressing the essential property of DG’s, namely, their headedness. I would like to show that, if we take headedness seriously, we are led to the conclusion that DG’s and CFG’s both represent equivalence classes of what I will call headed context-free grammars (HCFG’s), but that neither DG’...|$|R
25|$|The {{dependency}} representations above (and further below) show syntactic dependencies. Indeed, most work in <b>dependency</b> <b>grammar</b> {{focuses on}} syntactic dependencies. Syntactic dependencies are, however, {{just one of}} three or four types of dependencies. Meaning–text theory, for instance, emphasizes the role of semantic and morphological dependencies in addition to syntactic dependencies. A fourth type, prosodic dependencies, can also be acknowledged. Distinguishing between these types of dependencies can be important, in part because if one fails to do so, the likelihood that semantic, morphological, and/or prosodic dependencies will be mistaken for syntactic dependencies is great. The following four subsections briefly sketch each of these dependency types. During the discussion, the existence of syntactic dependencies is taken for granted and used as an orientation point for establishing the nature of the other three dependency types.|$|E
25|$|<b>Dependency</b> <b>grammar</b> (DG) is a {{class of}} modern {{syntactic}} theories that are all based on the dependency relation (as opposed to the constituency relation) {{and that can be}} traced back primarily to the work of Lucien Tesnière. Dependency is the notion that linguistic units, e.g. words, are connected to each other by directed links. The (finite) verb is taken to be the structural center of clause structure. All other syntactic units (words) are either directly or indirectly connected to the verb in terms of the directed links, which are called dependencies. DGs are distinct from phrase structure grammars (constituency grammars), since DGs lack phrasal nodes, although they acknowledge phrases. Structure is determined by the relation between a word (a head) and its dependents. Dependency structures are flatter than constituency structures in part because they lack a finite verb phrase constituent, and they are thus well suited for the analysis of languages with free word order, such as Czech, Slovak, Turkish, and Warlpiri.|$|E
25|$|The {{distinction}} between dependency- and constituency-based grammars derives {{in large part}} from the initial division of the clause. The constituency relation derives from an initial binary division, whereby the clause is split into a subject noun phrase (NP) and a predicate verb phrase (VP). This division is certainly present in the basic analysis of the clause that we find in the works of, for instance, Leonard Bloomfield and Noam Chomsky. Tesnière, however, argued vehemently against this binary division, preferring instead to position the verb as {{the root of all}} clause structure. Tesnière's stance was that the subject-predicate division stems from term logic and has no place in linguistics. The importance of this distinction is that if one acknowledges the initial subject-predicate division in syntax as something real, then one is likely to go down the path of constituency grammar, whereas if one rejects this division, then the only alternative is to position the verb as the root of all structure, which means one has chosen the path of <b>dependency</b> <b>grammar.</b>|$|E
40|$|Results of {{computational}} complexity {{exist for}} a wide range of phrase structure-based grammar formalisms, while there is an apparent lack of such results for dependency-based formalisms. We here adapt a result on the complexity of ID/LP-grammars to the dependency framework. Contrary to previous studies on heavily restricted <b>dependency</b> <b>grammars,</b> we prove that recognition (and thus, parsing) of linguistically adequate <b>dependency</b> <b>grammars</b> is NP-complete. Comment: 8 pages, requires LaTeX 2 e, epsfig, latexsym, amsmat...|$|R
50|$|An {{important}} aspect of IC-analysis in phrase structure grammars is that each individual word is a constituent by definition. The process of IC-analysis always ends when the smallest constituents are reached, which are often words (although the analysis can also be extended into the words to acknowledge {{the manner in which}} words are structured). The process is, however, much different in <b>dependency</b> <b>grammars,</b> since many individual words do not end up as constituents in <b>dependency</b> <b>grammars.</b>|$|R
40|$|Grammar formalisms {{built on}} the notion of word-to-word {{dependencies}} make attractive alternatives to formalisms built on phrase structure representations. However, {{little is known about the}} formal properties of <b>dependency</b> <b>grammars,</b> and few such grammars have been implemented. We present results from two strands of research that address these issues. The aims of this research were to classify <b>dependency</b> <b>grammars</b> in terms of their generative capacity and parsing complexity, and to systematically explore their expressive power in the context of a practical system for grammar development and parsing...|$|R
25|$|Syntactic {{dependencies}} are {{the focus}} of most work in <b>dependency</b> <b>grammar,</b> as stated above. How the presence and the direction of syntactic dependencies are determined is of course often open to debate. In this regard, it must be acknowledged that the validity of syntactic dependencies in the trees throughout this article is being taken for granted. However, these hierarchies are such that many dependency grammars can largely support them, although there will certainly be points of disagreement. The basic question about how syntactic dependencies are discerned has proven difficult to answer definitively. One should acknowledge in this area, however, that the basic task of identifying and discerning the presence and direction of the syntactic dependencies of dependency grammars is no easier or harder than determining the constituent groupings of constituency grammars. A variety of heuristics are employed to this end, basic constituency tests being useful tools; the syntactic dependencies assumed in the trees in this article are grouping words together in a manner that most closely matches the results of standard permutation, substitution, and ellipsis constituency tests. Etymological considerations also provide helpful clues about the direction of dependencies. A promising principle upon which to base the existence of syntactic dependencies is distribution. When one is striving to identify the root of a given phrase, the word that is most responsible for determining the distribution of that phrase as a whole is its root.|$|E
2500|$|This modern {{understanding}} of predicates {{is compatible with}} the <b>dependency</b> <b>grammar</b> approach to sentence structure, which places the finite verb as the root of all structure, e.g.|$|E
2500|$|Modern {{dependency}} grammars, however, begin {{primarily with}} the work of Lucien Tesnière. Tesnière was a Frenchman, a polyglot, and a professor of linguistics at the universities in Strasbourg and Montpellier. His major work Éléments de syntaxe structurale was published posthumously in 1959 – he died in 1954. The basic approach to syntax he developed seems to have been seized upon independently by others in the 1960s {{and a number of other}} dependency-based grammars have gained prominence since those early works. DG has generated a lot of interest in Germany in both theoretical syntax and language pedagogy. In recent years, the great development surrounding dependency-based theories has come from computational linguistics and is due, in part, to the influential work that David Hays did in machine translation at the RAND Corporation in the 1950s and 1960s. Dependency-based systems are increasingly being used to parse natural language and generate tree banks. Interest in <b>dependency</b> <b>grammar</b> is growing at present, international conferences on dependency linguistics being a relatively recent development ( [...] , , [...] ).|$|E
40|$|The paper {{presents}} Colored Multiplanar Link Grammars (CMLG). These grammars are reducible to extended right-linear S-grammars (Wartena 2001) {{where the}} storage type S is a concatenation of c pushdowns. The number of colors available in these grammars induces {{a hierarchy of}} Classes of CMLGs. By fixing also another parameter in CMLGs, namely the bound t for non-projectivity depth, we get c-Colored t-Non-projective <b>Dependency</b> <b>Grammars</b> (CNDG) that generate acyclic dependency graphs. Thus, CNDGs form a two-dimensional hier- archy of <b>dependency</b> <b>grammars.</b> A part of this hierarchy is mildly context-sensitive and non-projective...|$|R
50|$|Phrase {{structure}} grammars view both finite and nonfinite verb phrases as constituent phrases and, consequently, do {{not draw}} any key distinction between them. <b>Dependency</b> <b>grammars</b> (described below) are much different in this regard.|$|R
50|$|The more layered {{analysis}} is favored in the GB framework and {{a variation of}} it certainly obtains in current Minimalism as well. The flat {{analysis is}} certainly the one preferred by <b>dependency</b> <b>grammars.</b>|$|R
5000|$|Ibn Maḍāʾ, {{the first}} {{linguist}} {{to write about}} <b>dependency</b> <b>grammar,</b> ...|$|E
5000|$|... #Subtitle level 2: Structural {{analysis}} of V2 in <b>Dependency</b> <b>Grammar</b> ...|$|E
50|$|Osborne T. 2005. Coherence: A <b>dependency</b> <b>grammar</b> analysis. SKY Journal of Linguistics 18, 223-286.|$|E
25|$|Worth noting is {{that many}} {{theories}} of syntax do not acknowledge Logical Form (e.g. Lexical Functional Grammar, Head-Driven Phrase Structure <b>Grammar,</b> <b>Dependency</b> <b>Grammars,</b> Tree-Adjoining Grammar, etc.), {{at least not in}} the way it is understood in Government and Binding Theory and the Minimalist Program. The postulation of such a level of representation remains a subject of debate.|$|R
50|$|Traditional <b>dependency</b> <b>grammars</b> (e.g. Lucien Tesnière's Structural Syntax and Igor Mel'čuk's Meaning-Text Theory) {{approach}} discontinuities much differently. They tend to (posit {{one or more}} {{levels of}} syntactic structure that) abstract away from linear order and acknowledge hierarchical order alone. If linear order is taken to be (in a sense) secondary in this manner, discontinuities present less of a challenge and are therefore of secondary importance to the theory. Other <b>dependency</b> <b>grammars,</b> in contrast, take linear and hierarchical order to be of equal importance. These theories are likely to (also) pursue some sort of feature passing mechanism that passes information about the displaced unit {{up and down the}} tree.|$|R
25|$|In linguistics, {{some authors}} {{use the term}} phrase {{structure}} grammar to refer to context-free grammars, whereby phrase-structure grammars are distinct from <b>dependency</b> <b>grammars.</b> In computer science, a popular notation for context-free grammars is Backus–Naur form, or BNF.|$|R
50|$|A {{variety of}} {{theories}} exist regarding {{the structure of}} syntax, including generative grammar, categorial grammar, and <b>dependency</b> <b>grammar.</b>|$|E
50|$|Osborne, T. 2006. Shared {{material}} and grammar: A <b>dependency</b> <b>grammar</b> theory of non-gapping coordination. Zeitschrift für Sprachwissenschaft 25, 39-93.|$|E
50|$|Groß, T. and T. Osborne 2009. Toward a {{practical}} <b>dependency</b> <b>grammar</b> theory of discontinuities. SKY Journal of Linguistics 22, 43-90.|$|E
5000|$|<b>Dependency</b> <b>grammars</b> {{point to}} the results of many {{standard}} constituency tests to back up their stance. For instance, topicalization, pseudoclefting, and answer ellipsis suggest that non-finite VP does, but finite VP does not, exist as a constituent: ...|$|R
50|$|<b>Dependency</b> <b>grammars</b> have {{explored}} the projectivity principle {{in great detail}} and have formalized it rigorously. The concept is, however, a simple one. If crossing lines obtain in the tree, projectivity has been violated, meaning a discontinuity is present.|$|R
50|$|Theories that assume {{sentence}} {{structure to}} be less layered than the analyses just given sometimes employ a special convention to distinguish adjuncts from arguments. Some <b>dependency</b> <b>grammars,</b> for instance, use an arrow dependency edge to mark adjuncts, e.g.|$|R
