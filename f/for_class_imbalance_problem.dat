4|10000|Public
40|$|Abstract- Class {{imbalance}} {{is one of}} {{the challenges}} of machine learning and data mining fields. Imbalance data sets degrades the performance of data mining and machine learning techniques as the overall accuracy and decision making be biased to the majority class, which lead to misclassifying the minority class samples or furthermore treated them as noise. This paper proposes a general survey <b>for</b> <b>class</b> <b>imbalance</b> <b>problem</b> solutions and the most significant investigations recently introduced by researchers. I...|$|E
40|$|Abstract. In this paper, a novel inverse random under {{sampling}} (IRUS) {{method is}} proposed <b>for</b> <b>class</b> <b>imbalance</b> <b>problem.</b> The main {{idea is to}} severely under sample the negative class (majority class), thus creating {{a large number of}} distinct negative training sets. For each training set we then find a linear discriminant which separates the positive class from the negative class. By combining the multiple designs through voting, we construct a composite between the positive class and the negative class. The proposed methodology is applied on 11 UCI data sets and experimental results indicate a significant increase in Area Under Curve (AUC) when compared with many existing class-imbalance learning methods. ...|$|E
40|$|Aim {{of study}} {{of the study is}} to {{effectively}} solve the imbalanced data sets classification problem, the field of machine learning proposed many effective algorithms. On Seabrook’s view, now the classification methods <b>for</b> <b>class</b> <b>imbalance</b> <b>problem</b> can be broadly divided into two categories, one class is to create new methods or improve the existing methods based on the characteristics of class imbalance. Another is to reduce class imbalance effect re-use of existing methods by resembling technique. But there are also some drawbacks in resembling method. Sample set sampling may lead to excessive learning; the next sample may result in the training set. Therefore, by dividing the training set, it does not increase the number of training samples; also it is without loss of useful information in the sample to obtain a certain degree of balance in a subset of the problem...|$|E
40|$|This thesis {{studies the}} {{diversity}} issue of classification ensembles <b>for</b> <b>class</b> <b>imbalance</b> learning <b>problems.</b> <b>Class</b> <b>imbalance</b> learning refers to learning from imbalanced data sets, {{in which some}} classes of examples (minority) are highly under-represented comparing to other classes (majority). The very skewed class distribution degrades the learning ability of many traditional machine learning methods, especially in the recognition of examples from the minority classes, which are often deemed {{to be more important}} and interesting. Although quite a few ensemble learning approaches have been proposed to handle the problem, no in-depth research exists to explain why and when they can be helpful. Our objectives are to understand how ensemble diversity affects the classification performance <b>for</b> a <b>class</b> <b>imbalance</b> <b>problem</b> according to single-class and overall performance measures, and to make best use of diversity to improve the performance. As the first stage, we study the relationship between ensemble diversity and generalization performance <b>for</b> <b>class</b> <b>imbalance</b> <b>problems.</b> We investigate mathematical links between single-class performance and ensemble diversity. It is found that how the single-class measures change along with diversity falls into six different situations. These findings are then verified in <b>class</b> <b>imbalance</b> scenarios through empirical studies. The impact of diversity on overall performance is also investigated empirically. Strong correlations between diversity and the performance measures are found. Diversity shows a positive impact on the recognition of the minority class and benefits the overall performance of ensembles in <b>class</b> <b>imbalance</b> learning. Our results help to understand if and why ensemble diversity can help to deal with <b>class</b> <b>imbalance</b> <b>problems.</b> Encouraged by the positive role of diversity in <b>class</b> <b>imbalance</b> learning, we then focus on a specific ensemble learning technique, the negative correlation learning (NCL) algorithm, which considers diversity explicitly when creating ensembles and has achieved great empirical success. We propose a new learning algorithm based on the idea of NCL, named AdaBoost. NC, for classification problems. An ``ambiguity" term decomposed from the 0 - 1 error function is introduced into the training framework of AdaBoost. It demonstrates superiority in both effectiveness and efficiency. Its good generalization performance is explained by theoretical and empirical evidences. It can be viewed as the first NCL algorithm specializing in classification problems. Most existing ensemble methods <b>for</b> <b>class</b> <b>imbalance</b> <b>problems</b> suffer from the problems of overfitting and over-generalization. To improve this situation, we address the <b>class</b> <b>imbalance</b> issue by making use of ensemble diversity. We investigate the generalization ability of NCL algorithms, including AdaBoost. NC, to tackle two-class <b>imbalance</b> <b>problems.</b> We find that NCL methods integrated with random oversampling are effective in recognizing minority class examples without losing the overall performance, especially the AdaBoost. NC tree ensemble. This is achieved by providing smoother and less overfitting classification boundaries <b>for</b> the minority <b>class.</b> The results here show the usefulness of diversity and open up a novel way to deal with <b>class</b> <b>imbalance</b> <b>problems.</b> Since the two-class imbalance is not the only scenario in real-world applications, multi-class <b>imbalance</b> <b>problems</b> deserve equal attention. To understand what problems multi-class can cause and how it affects the classification performance, we study the multi-class difficulty by analyzing the multi-minority and multi-majority cases respectively. Both lead to a significant performance reduction. The multi-majority case appears to be more harmful. The results reveal possible issues that a <b>class</b> <b>imbalance</b> learning technique could have when dealing with multi-class tasks. Following this part of analysis and the promising results of AdaBoost. NC on two-class <b>imbalance</b> <b>problems,</b> we apply AdaBoost. NC to a set of multi-class imbalance domains with the aim of solving them effectively and directly. Our method shows good generalization in minority classes and balances the performance across different classes well without using any class decomposition schemes. Finally, we conclude this thesis with how the study has contributed to <b>class</b> <b>imbalance</b> learning and ensemble learning, and propose several possible directions for future research that may improve and extend this work. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Protein-nucleotide {{interactions}} are ubiquitous {{in a wide}} variety of biological processes. Accurately identifying interaction residues solely from protein sequences is useful for both protein function annotation and drug design, especially in the post-genomic era, as large volumes of protein data have not been functionally annotated. Protein-nucleotide binding residue prediction is a typical <b>imbalanced</b> learning <b>problem,</b> where binding residues are extremely fewer in number than non-binding residues. Alleviating the severity of <b>class</b> <b>imbalance</b> has been demonstrated to be a promising means of improving the prediction performance of a machine-learning-based predictor <b>for</b> <b>class</b> <b>imbalance</b> <b>problems.</b> However, little attention has been paid to the negative impact of <b>class</b> <b>imbalance</b> on protein-nucleotide binding residue prediction. In this study, we propose a new supervised over-sampling algorithm that synthesizes additional minority class samples to address <b>class</b> <b>imbalance.</b> The experimental results from protein-nucleotide interaction datasets demonstrate that the proposed supervised over-sampling algorithm can relieve the severity of <b>class</b> <b>imbalance</b> and help to improve prediction performance. Based on the proposed over-sampling algorithm, a predictor, called TargetSOS, is implemented for protein-nucleotide binding residue prediction. Cross-validation tests and independent validation tests demonstrate the effectiveness of TargetSOS. The web-server and datasets used in this study are freely available a...|$|R
30|$|The {{emergence}} of Big Data brings new problems and challenges <b>for</b> the <b>class</b> <b>imbalance</b> <b>problem.</b> Big Data posits {{the challenges of}} Volume, Velocity, and Variety [14, 15]. In addition, other attributes {{have been linked to}} Big Data such as Veracity or Value, among others [16]. The scalability issue must be properly addressed to develop new solutions or adapt existing ones for Big Data case studies [17, 18]. Spark [19] has emerged as a popular choice to implement large-scale Machine Learning applications on Big Data.|$|R
40|$|This paper {{proposes a}} novel {{algorithm}} Virtual Instances Resampling Technique Using Active Learning (VIRTUAL) <b>for</b> <b>class</b> <b>imbalance</b> <b>problem</b> in Support Vector Machine (SVM) learning. In supervised learning, prediction {{performance of the}} classification algorithms deteriorate when the training set is imbalanced. Class imbalance problem occurs when {{at least one of}} the classes are represented by substantially less number of instances than the others in the training set. Various real-world classification tasks, such as medical diagnosis and text categorization suffer from this phenomenon. VIRTUAL is a hybrid method of oversampling and active learning to form an adaptive technique for resampling of the minority class instances. Unlike traditional resampling methods which require preprocessing of the data, VIRTUAL generates virtual instances for the minority class support vectors during the training process, therefore it removes the need for an extra preprocessing stage. Our empirical results show that VIRTUAL outperforms other competitive oversampling techniques and active learning strategy in terms of prediction capability. In addition, VIRTUAL is more efficient in generating new instances and has a shorter training time than the other oversampling techniques due to its adaptive nature and its decision capability in creating virtual instances. ...|$|E
40|$|In this paper, the {{detection}} of mines or other objects on the seabed frommultiple side-scan sonar views is considered. Two frameworks are provided {{for this kind of}} classification. The first framework is based upon the Dempster–Shafer (DS) concept of fusion from a single-view kernel-based classifier and the second framework is based upon the concepts of multi-instance classifiers. Moreover, we consider the <b>class</b> <b>imbalance</b> <b>problem</b> which is always presents in sonar image recognition. Our experimental results show that both of the presented frameworks can be used in mine-like object classification and the pre-sented methods <b>for</b> multi-instance <b>class</b> <b>imbalanced</b> <b>problem</b> are also effective in such classification. ...|$|R
40|$|In medical {{datasets}} classification, {{support vector machine}} (SVM) {{is considered}} {{to be one of the}} most successful methods. However, most of the real-world medical datasets usually contain some outliers/noise and data often have <b>class</b> <b>imbalance</b> <b>problems.</b> In this paper, a fuzzy support machine (FSVM) <b>for</b> the <b>class</b> <b>imbalance</b> <b>problem</b> (called FSVM-CIP) is presented, which can be seen as a modified class of FSVM by extending manifold regularization and assigning two misclassification costs <b>for</b> two <b>classes.</b> The proposed FSVM-CIP can be used to handle the <b>class</b> <b>imbalance</b> <b>problem</b> in the presence of outliers/noise, and enhance the locality maximum margin. Five real-world medical datasets, breast, heart, hepatitis, BUPA liver, and pima diabetes, from the UCI medical database are employed to illustrate the method presented in this paper. Experimental results on these datasets show the outperformed or comparable effectiveness of FSVM-CIP...|$|R
40|$|In this paper, a novel inverse random under {{sampling}} (IRUS) {{method is}} proposed <b>for</b> the <b>class</b> <b>imbalance</b> <b>problem.</b> The main {{idea is to}} severely under sample the majority class thus creating {{a large number of}} distinct training sets. For each training set we then find a decision boundary which separates the minority class from the majority class. By combining the multiple designs through fusion, we construct a composite boundary between the majority class and the minority class. The proposed methodology is applied on 22 UCI data sets and experimental results indicate a significant increase in performance when compared with many existing class-imbalance learning methods. We also present promising results for multi-label classification, a challenging research problem in many modern applications such as music, text and image categorization...|$|R
40|$|Learning in non-stationary environments is an {{increasingly}} important problem {{in a wide variety}} of real-world applications. In non-stationary environments data arrives incrementally, however the underlying generating function may change over time. In addition to the environments being non-stationary, they also often exhibit <b>class</b> <b>imbalance.</b> That is one class (the majority class) vastly outnumbers the other class (the minority class). This combination of <b>class</b> <b>imbalance</b> with non-stationary environments poses significant and interesting practical problems for classification. To overcome these issues, we introduce a novel instance selection mechanism, as well as provide a modification to the Heuristic Updatable Weighted Random Subspaces (HUWRS) method <b>for</b> the <b>class</b> <b>imbalance</b> <b>problem.</b> We then compare our modifications of HUWRS (called HUWRS. IP) to other state of the art algorithms, concluding that HUWRS. IP often achieves vastly superior performance...|$|R
30|$|A hybrid method <b>for</b> {{addressing}} <b>class</b> <b>imbalance</b> {{may include}} {{two or more}} individual methods used <b>for</b> addressing the <b>class</b> <b>imbalance</b> <b>problem,</b> or may use multiple algorithms for a specific {{part of the overall}} solution. Among the hybrid methods in published works, many are centered around SVM, Artificial Neural Networks, and Decision Trees [8]. A decision tree is a classifier that is modeled on a tree-like structure of internal nodes, branches, and terminal nodes (class labels) [38]. Hybrid approaches have the burden to ensure that the differences in the individual approaches properly complement each other as a whole, and together yield better performance compared to the individual methods alone.|$|R
40|$|SMOTE is an {{effective}} oversampling technique <b>for</b> a <b>class</b> <b>imbalance</b> <b>problem</b> due to its simplicity and relatively high recall value. One drawback of SMOTE is a requirement {{of the number of}} nearest neighbors as a key parameter to synthesize instances. This paper introduces a new adaptive algorithm called Adaptive neighbor Synthetic Minority Oversampling Technique (ANS) to dynamically adapt the number of neighbors needed for oversampling around different minority regions. This technique also defines a minority outcast as a minority instance having no minority class neighbors. Minority outcasts are neglected by most oversampling techniques but instead, an additional outcast handling method is proposed for the performance improvement via a 1 -nearest neighbor model. Based on our experiments in UCI and PROMISE datasets, generated datasets from this technique have improved the accuracy performance of a classification, and the improvement can be verified statistically by the Wilcoxon signed-rank test...|$|R
40|$|In classification, {{when the}} {{distribution}} of the training data among classes is uneven, the learning algorithm is generally dominated by the feature of the majority classes. The features in the minority classes are normally difficult to be fully recognized. In this paper, a method is proposed to enhance the classification accuracy <b>for</b> the minority <b>classes.</b> The proposed method combines Synthetic Minority Over-sampling Technique (SMOTE) and Complementary Neural Network (CMTNN) to handle the <b>problem</b> of classifying <b>imbalanced</b> data. In order to demonstrate that the proposed technique can assist classification of imbalanced data, several classification algorithms have been used. They are Artificial Neural Network (ANN), k-Nearest Neighbor (k-NN) and Support Vector Machine (SVM). The benchmark data sets with various ratios between the minority class and the majority class are obtained from the University of California Irvine (UCI) machine learning repository. The results show that the proposed combination techniques can improve the performance <b>for</b> the <b>class</b> <b>imbalance</b> <b>problem...</b>|$|R
40|$|This paper mainly {{deals with}} how kernel method {{can be used}} for {{software}} defect prediction, since the <b>class</b> <b>imbalance</b> can greatly reduce the performance of defect prediction. In this paper, two classifiers, namely, the asymmetric kernel partial least squares classifier (AKPLSC) and asymmetric kernel principal component analysis classifier (AKPCAC), are proposed <b>for</b> solving the <b>class</b> <b>imbalance</b> <b>problem.</b> This is achieved by applying kernel function to the asymmetric partial least squares classifier and asymmetric principal component analysis classifier, respectively. The kernel function used for the two classifiers is Gaussian function. Experiments conducted on NASA and SOFTLAB data sets using F-measure, Friedman’s test, and Tukey’s test confirm the validity of our methods...|$|R
40|$|In Data {{mining and}} Knowledge Discovery hidden and {{valuable}} knowledge {{from the data}} sources is discovered. The traditional algorithms used for knowledge discovery are bottle necked due to wide range of data sources availability. <b>Class</b> <b>imbalance</b> is a one of the problem arises due to data source which provide unequal class i. e. examples of one class in a training data set vastly outnumber examples of the other class(es). This paper proposes a method belonging to under sampling approach which uses OPTICS {{one of the best}} visualization clustering technique <b>for</b> handling <b>class</b> <b>imbalance</b> <b>problem.</b> In the proposed approach, further Classification of new data is performed by applying C 4. 5 algorithm as the base algorithm. The method is optimized by the selection of the most suitable clusters for deletion of the majority dataset based on visualization algorithms. An experimental analysis is carried out over a wide range of highly imbalanced data sets and uses the statistical tests suggested in the specialized literature. The results obtained show that our novel proposal outperforms other classic and recent models in terms of Area under the ROC Curve, F-measure, precision, TP rate and TN rate. Index Terms — Classification, <b>class</b> <b>imbalance,</b> CIL-OP. 1...|$|R
40|$|In {{last few}} years there are major changes and {{evolution}} {{has been done on}} classification of data. As the application area of technology is increases the size of data also increases. Classification of data becomes difficult because of unbounded size and imbalance nature of data. <b>Class</b> <b>imbalance</b> <b>problem</b> become greatest issue in data mining. <b>Imbalance</b> <b>problem</b> occur where one of the two classes having more sample than other classes. The most of algorithm are more focusing on classification of major sample while ignoring or misclassifying minority sample. The minority samples are those that rarely occur but very important. There are different methods available for classification of imbalance data set which is divided into three main categories, the algorithmic approach, datapreprocessing approach and feature selection approach. Each of this technique has their own advantages and disadvantages. In this paper systematic study of each approach is define which gives the right direction <b>for</b> research in <b>class</b> <b>imbalance</b> <b>problem...</b>|$|R
40|$|Abstract — <b>Class</b> <b>imbalance</b> {{learning}} {{is an important}} research area in machine learning, where instances in some classes heavily outnumber the instances in other classes. This unbal-anced class distribution causes performance degradation. Some ensemble solutions have been proposed <b>for</b> the <b>class</b> <b>imbalance</b> <b>problem.</b> Diversity has been {{proved to be an}} influential aspect in ensemble learning, which describes the degree of different decisions made by classifiers. However, none of those proposed solutions explore the impact of diversity on imbalanced data sets. In addition, most of them are based on re-sampling techniques to rebalance class distribution, and over-sampling usually causes overfitting (high generalisation error). This paper investigates if diversity can relieve this problem by using negative correlation learning (NCL) model, which encourages diversity explicitly by adding a penalty term in the error function of neural networks. A variation model of NCL is also proposed – NCLCost. Our study shows that diversity has a direct impact on the measure of recall. It is also a factor that causes the reduction of F-measure. In addition, although NCL-based models with extreme settings do not produce better recall values of minority class than SMOTEBoost [1], they have slightly better performance of F-measure and G-mean than both independent ANNs and SMOTEBoost and better recall than independent ANNs. I...|$|R
40|$|Identifying {{customers}} {{who are more}} likely to respond to a product offering is an important issue in direct marketing. In direct marketing, data mining has been used extensively to identify potential customers for a new product (target selection). Using historical purchase data, a predictive response model with data mining techniques is developed to predict a probability that a customer is going to respond to a promotion or an offer. The purpose of this thesis is to identify the Parsian bank {{customers who}} {{are more likely to}} respond positively to a new product offering. To reach this purpose a predictive response model using customer historical purchase data is build with data mining techniques. Response modeling procedure consists of several steps. In building a response model one has to deal with some issues, such as: constructing all purchase behavior variables (RFM variables), determining the inputs to the model (feature selection) and <b>class</b> <b>imbalance</b> <b>problem.</b> The {{purpose of this study is}} to deal with all these issues and steps of modeling. Thus various data mining techniques and algorithms are used to implement each step of modeling and alleviate related difficulties. For modeling purpose customers' data (30, 000 customers) were gathered from Parsian bank. Based on literature and domain knowledge 85 RFM features and their two-way interactions were constructed from collected data. Since irrelevant or redundant features result in bad model performance thus feature selection was performed in order to determine the inputs to the model. Feature selection was done in three steps using F-score and backward elimination on Random Forest. The data was highly unbalanced. We used under- sampling <b>for</b> solving <b>class</b> <b>imbalance</b> <b>problem.</b> Finally SVM was used as a classifier for classification purpose. The result indicates that Parsian bank can reach three times as many respondents as if they use no model (random sampling) for target selection. By using this model Parsian bank not only can significantly reduce the overall marketing cost but also can maximize customers' response to a product offering, prevent customer annoyance and improve customer relationship management. Validerat; 20101217 (root...|$|R
40|$|Abstract Background Medical and {{biological}} data are commonly with small sample size, missing values, and most importantly, <b>imbalanced</b> <b>class</b> distribution. In {{this study we}} propose a particle swarm based hybrid system <b>for</b> remedying the <b>class</b> <b>imbalance</b> <b>problem</b> in medical {{and biological}} data mining. This hybrid system combines the particle swarm optimization (PSO) algorithm with multiple classifiers and evaluation metrics for evaluation fusion. Samples from the majority class are ranked using multiple objectives according to their merit in compensating the <b>class</b> <b>imbalance,</b> and then combined with the minority class to form a balanced dataset. Results One important {{finding of this study}} is that different classifiers and metrics often provide different evaluation results. Nevertheless, the proposed hybrid system demonstrates consistent improvements over several alternative methods with three different metrics. The sampling results also demonstrate good generalization on different types of classification algorithms, indicating the advantage of information fusion applied in the hybrid system. Conclusion The experimental results demonstrate that unlike many currently available methods which often perform unevenly with different datasets the proposed hybrid system has a better generalization property which alleviates the method-data dependency problem. From the biological perspective, the system provides indication for further investigation of the highly ranked samples, which may result in the discovery of new conditions or disease subtypes. </p...|$|R
40|$|This {{chapter is}} {{concerned}} with the <b>class</b> <b>imbalance</b> <b>problem,</b> which has been recognized as a crucial problem in machine learning and data mining. The problem occurs when there are significantly fewer training instances of one class compared to another class. Most machine learning algorithms work well with balanced data sets since they aim to optimize the overall classification accuracy or a related measure. For imbalanced data sets, the decision boundary established by standard machine learning algorithms tends to be biased towards the majority class; therefore, the minority class instances {{are more likely to be}} misclassified. There are many problems that arise from learning with imbalanced data sets. The first problem concerns measures of performance. Evaluation metrics are known to playa vital role in machine learning. They are used to guide the learning algorithm towards the desired solution. Therefore, if the evaluation metric does not take the minority class into consideration, the learning algorithm will not be able to cope with <b>class</b> <b>imbalance</b> very well. With standard evaluation metrics, such as the overall classification accuracy, the minority class has less impact compared to the majority class. The second problem is related to lack of data. In an imbalanced h-aining set, a class may have very few samples. As a result, it is difficult to construct accurate decision boundaries between <b>classes.</b> <b>For</b> a <b>class</b> consisting of multiple clusters, some clusters may contain a small number of samples compared to other clusters; therefore, the lack of data can occur within the class itself. The third problem in learning from imbalanced data is noise. Noisy data have a serious impact on minority classes than on majority classes. Furthermore, standard machine learning algorithms tend to h-eat samples from a minority class as noise_ In this chapter, we review the existing approaches <b>for</b> solving the <b>class</b> <b>imbalance</b> <b>problem,</b> and discuss the various metrics used to evaluate the performance of classifiers. Furthermore, we introduce a new approach to dealing with the <b>class</b> <b>imbalance</b> <b>problem</b> by combining both unsupervised and supervised learning. The rest of the chapter is organized as follows. Section 2 describes the <b>problems</b> caused by <b>class</b> <b>imbalance.</b> Section 3 reviews current stateof- the-art techniques for tackling these problems. Section 4 describes existing classification performance measures for imbalanced data. Section 5 describes our proposed learning approach to handle the <b>class</b> <b>imbalance</b> <b>problem.</b> Section 6 presents experimental results, and Section 7 gives concluding remarks...|$|R
40|$|Thesis (Ph. D.), School of Electrical Engineering and Computer Science, Washington State UniversityEpigenetics {{refers to}} the changes in gene {{expression}} which are caused by other mechanisms other than the DNA sequence. Environmental influences can alter epigenetic states in the germ line that can be further transmitted to future generations. Therefore, epigenetic markers can be correlated to exposures and health risk for diseases. DNA methylation based biomarkers are very promising {{and a large number}} of potential biomarkers have been identified for diseases such as cancer. Our goal is to identify regions with susceptibility to be differentially methylated in the genome. Biological datasets come with inherent challenges such as having low volume and high dimensionality. Most data we are interested in (e. g., positive cases of disease state) are rare and come with many characteristics or features. Such interesting computational problems can be approached using Machine Learning techniques. Our goal is to answer two fundamental challenges. They are to find most relevant genomic features for learning and to perform efficient learning when the <b>classes</b> are <b>imbalanced.</b> Efficient learning can be performed only when targetconcepts from both the classes (DMR and non-DMR) are learned well to distinguish them separately while learning from only the relevant features. We propose Generalized Query Based Active Learning (GQAL) which constructs intelligent queries by removing irrelevant features from the query which an Oracle (e. g. a human expert) can answer easily. This approach allows the learner to label multiple instances at thesame time, makes use of the most relevant features per query and performs our first challenge. <b>For</b> the <b>class</b> <b>imbalance</b> <b>problem</b> we use a boosting technique called AdaBoost or "Adaptive Boosting" in our study. This approach allows creating a learner which will learn target conceptswell from both the classes which addresses our second challenge. Currently there are no machine learning approaches applied to epigenetic datasets addressing these problems. We apply GQAL and TAN+AdaBoost on several datasets and show that our method is better for prediction for epigenetic datasets than other popular learners. This proposed two-step DMR identification framework will allow to predict novel epigenetic biomarkers which will assist in diagnosis of disease susceptibility and provide new therapeutic targets. School of Electrical Engineering and Computer Science, Washington State Universit...|$|R
40|$|Lung nodule {{classification}} is a <b>class</b> <b>imbalanced</b> <b>problem</b> because nodules {{are found}} with much lower frequency than non-nodules. In the <b>class</b> <b>imbalanced</b> <b>problem,</b> conventional classifiers {{tend to be}} overwhelmed by the majority class and ignore the minority class. We therefore propose cascaded convolutional neural networks {{to cope with the}} <b>class</b> <b>imbalanced</b> <b>problem.</b> In the proposed approach, multi-stage convolutional neural networks that perform as single-sided classifiers filter out obvious non-nodules. Successively, a convolutional neural network trained with a balanced data set calculates nodule probabilities. The proposed method achieved the sensitivity of 92. 4 % and 94. 5...|$|R
40|$|Lung nodule {{detection}} is a <b>class</b> <b>imbalanced</b> <b>problem</b> because nodules {{are found}} with much lower frequency than non-nodules. In the <b>class</b> <b>imbalanced</b> <b>problem,</b> conventional classifiers {{tend to be}} overwhelmed by the majority class and ignore the minority class. We therefore propose cascaded convolutional neural networks {{to cope with the}} <b>class</b> <b>imbalanced</b> <b>problem.</b> In the proposed approach, cascaded convolutional neural networks that perform as selective classifiers filter out obvious non-nodules. Successively, a convolutional neural network trained with a balanced data set calculates nodule probabilities. The proposed method achieved the detection sensitivity of 85. 3 % and 90. 7 % at 1 and 4 false positives per scan in FROC curve, respectively...|$|R
40|$|Lung nodule {{classification}} is a <b>class</b> <b>imbalanced</b> <b>problem,</b> as nodules {{are found}} with much lower frequency than non-nodules. In the <b>class</b> <b>imbalanced</b> <b>problem,</b> conventional classifiers {{tend to be}} overwhelmed by the majority class and ignore the minority class. We showed that cascaded convolutional neural networks can classify the nodule candidates precisely <b>for</b> a <b>class</b> <b>imbalanced</b> nodule candidate data set in our previous study. In this paper, we propose Fusion classifier {{in conjunction with the}} cascaded convolutional neural network models. To fuse the models, nodule probabilities are calculated by using the convolutional neural network models at first. Then, Fusion classifier is trained and tested by the nodule probabilities. The proposed method achieved the sensitivity of 94. 4 % and 95. 9 % at 4 and 8 false positives per scan in Free Receiver Operating Characteristics (FROC) curve analysis, respectively. Comment: Draft of ISBI 2018. arXiv admin note: text overlap with arXiv: 1703. 0031...|$|R
40|$|This {{research}} {{presents the}} development of techniques to handle two issues in data classification: noise and <b>imbalanced</b> data <b>problems.</b> Noise is a significant problem that can degrade the quality of training data in any learning algorithm. Learning algorithms trained by noisy instances generally increase misclassification when they perform classification. As a result, the classification performance tends to decrease. Meanwhile, the <b>imbalanced</b> data <b>problem</b> is another problem affecting the performance of learning algorithms. If some classes have a much larger number of instances than the others, the learning algorithms tend {{to be dominated by}} the features of the majority classes, and the features of the minority classes are difficult to recognise. As a result, the classification performance of the minority classes could be significantly lower than that of the majority classes. It is therefore important to implement techniques to better handle the negative effects of noise and <b>imbalanced</b> data <b>problems.</b> Although there are several approaches attempting to handle noise and <b>imbalanced</b> data <b>problems,</b> shortcomings of the available approaches still exist. For the noise handling techniques, even though the noise tolerant approach does not require any data preprocessing, it can tolerate only a certain amount of noise. The classifier developed from noisy data tends to be less predictive if the training data contains a great number of noise instances. Furthermore, for the noise elimination approach, although it can be easily applied to various problem domains, it could degrade the quality of training data if it cannot distinguish between noise and rare cases (exceptions). Besides, for the <b>imbalanced</b> data <b>problem,</b> the available techniques used still present some limitations. For example, the algorithm-level approach can perform effectively only on specific problem domains or specific learning algorithms. The data-level approach can either eliminate necessary information from the training set or produce the over-fitting problem over the minority class. Moreover, when the <b>imbalanced</b> data <b>problem</b> becomes more complex, such as for the case of multi-class classification, it is difficult to apply the re-sampling techniques (the data-level approach), which perform effectively for <b>imbalanced</b> data <b>problems</b> in binary classification, to the multi-class classification. Due to the limitations above, these lead to the motivation of this research to propose and investigate techniques to handle noise and <b>imbalanced</b> data <b>problems</b> more effectively. This thesis has developed three new techniques to overcome the identified problems. Firstly, a cleaning technique called the Complementary Neural Network (CMTNN) data cleaning technique has been developed in order to remove noise (misclassification data) from the training set. The results show that the new noise detection and removal technique can eliminate noise with confidence. Furthermore, the CMTNN cleaning technique can increase the classification accuracy across different learning algorithms, which are Artificial Neural Network (ANN), Support Vector Machine (SVM), k- Nearest Neighbor (k-NN), and Decision Tree (DT). It can provide higher classification performance than other cleaning methods such as Tomek links, the majority voting filtering, and the consensus voting filtering. Secondly, the CMTNN re-sampling technique, which is a new under-sampling technique, has been developed to handle the <b>imbalanced</b> data <b>problem</b> in binary classification. The results show that the combined techniques of the CMTNN resampling technique and Synthetic Minority Over-sampling Technique (SMOTE) can perform effectively by improving the classification performance of the minority class instances in terms of Geometric Mean (G-Mean) and the area under the Receiver Operating Characteristic (ROC) curve. It generally provides higher performance than other re-sampling techniques such as Tomek links, Wilson’s Edited Nearest Neighbor Rule (ENN), SMOTE, the combined technique of SMOTE and ENN, and the combined technique of SMOTE and Tomek links. For the third proposed technique, an algorithm named One-Against-All with Data Balancing (OAA-DB) has been developed {{in order to deal with}} the <b>imbalanced</b> data <b>problem</b> in multi-class classification. It can be asserted that this algorithm not only improves the performance <b>for</b> the minority <b>class</b> but it also maintains the overall accuracy, which is normally reduced by other techniques. The OAA-DB algorithm can increase the performance in terms of the classification accuracy and F-measure when compared to other multi-class classification approaches including One-Against-All (OAA), One-Against-One (OAO), All and One (A&O), and One Against Higher Order (OAHO) approaches. Furthermore, this algorithm has shown that the re-sampling technique is not only used effectively <b>for</b> the <b>class</b> <b>imbalance</b> <b>problem</b> in binary classification but it has been also applied successfully to the <b>imbalanced</b> data <b>problem</b> in multi-class classification...|$|R
30|$|The {{remainder}} {{of this article is}} organized as follows. In “Methods addressing <b>class</b> <b>imbalance</b> in traditional data” section, we provide an overview of strategies and methods for handling traditional data with the <b>class</b> <b>imbalance</b> <b>problem.</b> While the primary focus of this paper is on high-class imbalance in big data, we present “Methods addressing <b>class</b> <b>imbalance</b> in traditional data” section to provide the reader with a more complete picture of existing approaches <b>for</b> <b>class</b> <b>imbalance,</b> since similar methods are generally used for both traditional data and big data. In “Methods addressing <b>class</b> <b>imbalance</b> in big data” section, we discuss the Data-Level methods and Algorithm-Level techniques for handling big data defined by high degrees of <b>class</b> <b>imbalance.</b> In “Discussion summary of surveyed works” section, we provide our insights into existing problems that still need focus (or more focus) in the context of effective solutions for big data with high-class imbalance. In “Conclusion” section, we conclude with the main points of our paper and suggest some directions for future work.|$|R
40|$|Abstract In machine {{learning}} problems, dierences in prior class probabilities|or class imbalances|have {{been reported to}} hinder the performance of some standard classi ers, such as decision trees. This paper presents a systematic study aimed at answering three dierent questions. First, we attempt {{to understand what the}} <b>class</b> <b>imbalance</b> <b>problem</b> is by establishing a relationship between concept complexity, size of the training set and <b>class</b> <b>imbalance</b> level. Second, we discuss several basic re-sampling or cost-modifying methods previously proposed to deal with <b>class</b> <b>imbalances</b> and compare their eectiveness. Finally, we investigate the assumption that the <b>class</b> <b>imbalance</b> <b>problem</b> does not only aect decision tree systems but also aects other classi cation systems such as Neural Networks and Support Vector Machines...|$|R
30|$|The <b>class</b> <b>imbalance</b> <b>problem</b> {{was first}} {{introduced}} to the ML research community {{a little over a}} decade ago [24]. Typically, the <b>class</b> <b>imbalance</b> <b>problem</b> occurs when there are significantly more instances from one class relative to other classes. In such cases the classifier tends to misclassify the instances of the less represented classes. More and more researchers realized that the performance of their classifiers may be sub-optimal {{due to the fact that}} the datasets are not balanced. This problem is even more relevant in fields where the natural datasets are highly imbalanced in the first place [25], as in the problem we describe.|$|R
40|$|Abstract. Recently, the <b>class</b> <b>imbalance</b> <b>problem</b> in neural networks, is {{receiving}} growing attention in works of machine learning and data mining. This problem appears when the samples of some classes are {{much smaller than}} those in the other classes. The classes with small size can be ignored in the learning process and the convergence of these classes is very slow. This paper studies empirically the <b>class</b> <b>imbalance</b> <b>problem</b> {{in the context of the}} RBF neural network trained with backpropagation algorithm. We propose to introduce a cost function in the training process to compensate <b>imbalance</b> <b>class</b> and one strategy to reduce the impact of the cost function in the data probability distribution. ...|$|R
30|$|The {{objective}} of the trial work is to validate the efficiency of planned techniques {{for dealing with the}} <b>class</b> <b>imbalance</b> <b>problem</b> in Big Data sets. The projected techniques are investigated and evaluated against benchmarking techniques.|$|R
40|$|Classification of {{data with}} <b>imbalanced</b> <b>class</b> {{distribution}} has posed a significant drawback {{of the performance}} attainable by most standard classifier learning algorithms, which assume a relatively balanced class distribution and equal misclassification costs. The significant difficulty and frequent occurrence of the <b>class</b> <b>imbalance</b> <b>problem</b> indicate the need for extra research efforts. The objective {{of this paper is}} to investigate meta-techniques applicable to most classifier learning algorithms, with the aim to advance the classification of imbalanced data. The AdaBoost algorithm is reported as a successful meta-technique for improving classification accuracy. The insight gained from a comprehensive analysis of the AdaBoost algorithm in terms of its advantages and shortcomings in tacking the <b>class</b> <b>imbalance</b> <b>problem</b> leads to the exploration of three cost-sensitive boosting algorithms, which are developed by introducing cost items into the learning framework of AdaBoost. Further analysis shows that one of the proposed algorithms tallies with the stagewise additive modelling in statistics to minimize the cost exponential loss. These boosting algorithms are also studied with respect to their weighting strategies towards different types of samples, and their effectiveness in identifying rare cases through experiments on several real world medical data sets, where the <b>class</b> <b>imbalance</b> <b>problem</b> prevails...|$|R
30|$|The {{two general}} {{categories}} of methods {{that address the}} <b>class</b> <b>imbalance</b> <b>problem</b> are Data-Level methods and Algorithm-Level methods, as noted by Ali et al. [8]. Each of the two categories can be further sub-divided into groups, as shown in Table  1.|$|R
40|$|Abstract—Class {{imbalance}} is {{a common}} problem in real world applications and it affects significantly the prediction accuracy. In this study, investigation on better handling <b>class</b> <b>imbalance</b> <b>problem</b> in customer behavior prediction is performed. Using a more appropriate evaluation metric (AUC), we investigated the increase of performance for under-sampling and two machine learning algorithms (weight Random Forests and RUSBoost) against a benchmark case of just using Random Forests. Results show that under-sampling {{is the most effective}} way to deal with <b>class</b> <b>imbalance.</b> RUSBoost, as a specific algorithm designed to deal with <b>class</b> <b>imbalance</b> <b>problem,</b> is also effective but not as good as under-sampling. Weighted Random Forests, as a cost-sensitive learner, only improves the performance of appetency classification problem out of three classification <b>problems.</b> Keywords—Class <b>imbalance,</b> Random forests, RUSBoost, Under-sampling, Customer behavior, Predictio...|$|R
40|$|The <b>class</b> <b>imbalance</b> <b>problem</b> {{has been}} known to hinder the {{learning}} performance of classification algorithms. Various real-world classification tasks such as text categorization suffer from this phenomenon. We demonstrate that active learning is capable of solving the problem. Categories and Subject Descriptor...|$|R
40|$|Abstract—In {{classification}} <b>problems,</b> the <b>class</b> <b>imbalance</b> <b>problem</b> {{will cause}} a bias on the training of classifiers and {{will result in the}} lower sensitivity of detecting the minority class examples. The Mahalanobis-Taguchi System (MTS) is a diagnostic and forecasting technique for multivariate data. MTS establishes a classifier by constructing a continuous measurement scale rather than directly learning from the training set. Therefore, it is expected that the construction of an MTS model will not be influenced by data distribution, and this property is helpful to overcome the <b>class</b> <b>imbalance</b> <b>problem.</b> To verify the robustness of MTS for imbalanced data, this study compares MTS with several popular classification techniques. The results indicate that MTS is the most robust technique to deal with the classification <b>problem</b> on <b>imbalanced</b> data. In addition, this study develops a “probabilistic thresholding method ” to determine the classification threshold for MTS, and it obtains a good performance. Finally, MTS is employed to analyze the radio frequency (RF) inspection process of mobile phone manufacturing. The data collected from the RF inspection process is typically an imbalanced type. Implementation results show that the inspection attributes are significantly reduced and that the RF inspection process can also maintain high inspection accuracy. Index Terms—Data mining, classification, <b>class</b> <b>imbalance</b> <b>problem,</b> <b>imbalanced</b> data, Mahalanobis-Taguchi System (MTS), threshold, mobile phone inspection. Ç...|$|R
