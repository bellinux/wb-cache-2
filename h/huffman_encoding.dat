128|35|Public
25|$|The {{resulting}} {{data for}} all 8×8 blocks is further compressed with a lossless algorithm, {{a variant of}} <b>Huffman</b> <b>encoding.</b>|$|E
2500|$|In {{order to}} encode the above {{generated}} coefficient pattern, JPEG uses <b>Huffman</b> <b>encoding.</b> [...] The JPEG standard provides general-purpose Huffman tables; encoders may also choose to generate Huffman tables optimized {{for the actual}} frequency distributions in images being encoded.|$|E
5000|$|The {{resulting}} {{data for}} all 8×8 blocks is further compressed with a lossless algorithm, {{a variant of}} <b>Huffman</b> <b>encoding.</b>|$|E
40|$|This paper {{presents}} an algorithm in spatial domain which gives less distortion to the cover image during embedding process. Minimizing embedding impact and maximizing embedding capacity {{are the key}} factors of any steganography algorithm. Peak Signal to Noise Ratio (PSNR) is the familiar metric used in discriminating the distorted image (stego image) and cover image. Here matrix embedding technique is chosen to embed the secret image which is initially <b>Huffman</b> <b>encoded.</b> The <b>Huffman</b> <b>encoded</b> image is overlaid on the selected bits of all the channels of pixels of cover image through matrix embedding. As a result, the stego image is constructed with very less distortion {{when compared to the}} cover image ends up with higher PSNR value. A secret image which cannot be embedded in a normal LSB embedding technique can be overlaid in this proposed technique since the secret image is <b>Huffman</b> <b>encoded.</b> Experimental results for standard cover images, which obtained higher PSNR value during the operation is shown in this paper...|$|R
30|$|The {{noise data}} {{consists}} of three parts: a gain, a spectral, and a temporal envelope. The gain is quantized uniformly on a dB scale and <b>Huffman</b> <b>encoded.</b> The prediction coefficients describing the spectral envelope are mapped onto Log Area Ratios (LARs) and quantized with an accuracy according to index number. The prediction coefficients describing the temporal envelope are mapped to Line Spectral Frequencies (LSFs) and quantized.|$|R
40|$|SPIHT is {{computationally}} {{very fast}} {{and among the}} best image compression algorithms known today. According to statistic analysis of the output binary stream of SPIHT encoding, propose a simple and effective method combined with <b>Huffman</b> <b>encode</b> for further compression. In this paper {{the results from the}} SPHIT algorithm are compared with the existing methods for compression like discrete cosine transform (DCT) and discrete wavelet transform (DWT) ...|$|R
50|$|It {{supports}} 4 KiB sliding window, {{with support}} of maximum 60 bytes of matching length. Dynamic <b>Huffman</b> <b>encoding</b> is used.|$|E
50|$|This method {{supports}} 8 KiB sliding window, {{with support}} of maximum 256 bytes of matching length. Dynamic <b>Huffman</b> <b>encoding</b> is used.|$|E
5000|$|At {{least one}} {{significant}} compression software program, bzip2, deliberately discontinued {{the use of}} arithmetic coding in favor of Huffman coding due to the perceived patent situation at the time. Also, encoders and decoders of the JPEG file format, which has options for both <b>Huffman</b> <b>encoding</b> and arithmetic coding, typically only support the <b>Huffman</b> <b>encoding</b> option, which was originally because of patent concerns; {{the result is that}} nearly all JPEG images in use today use <b>Huffman</b> <b>encoding</b> although JPEG's arithmetic coding patents have expired due to the age of the JPEG standard (the design of which was approximately completed by 1990). There are some archivers like PackJPG, that can losslessly convert Huffman encoded files to ones with arithmetic coding (with custom file name [...]pjg), showing up to 25% size saving.|$|E
40|$|Abstract. In {{this paper}} we propose an {{efficient}} approach to the compressed string matching problem on <b>Huffman</b> <b>encoded</b> texts, based on the Boyer-Moore strategy. Once a candidate valid shift has been located, a subsequent verification phase checks whether the shift is codeword aligned {{by taking advantage of}} the skeleton tree data structure. Our approach leads to algorithms with a sublinear behavior on the average, as shown by extensive experimentation...|$|R
50|$|Huffman {{encodings}} {{are fairly}} CPU intensive, {{in comparison to}} other board representations which seek to minimize required processor and memory cycles. However, {{the small size of}} the final representation makes this approach well suited to storage of long-term knowledge, for instance in storing positions in an opening book, where minimizing the board representation's size is more important than minimizing CPU cycles. <b>Huffman</b> <b>encoded</b> boards are also sometimes used in transposition tables for shallow entries.|$|R
40|$|In {{this paper}} we propose a novel {{approach}} to image compression based on three-dimensional Discrete Cosine Transformation (DCT). The basic idea is to de-correlate similar pixel blocks through three-dimensional DCT transformation. A number of adjacent pixel blocks are grouped together to form a three-dimensional data cube. Each data cube is 3 D DCT transformed, quantized, and <b>Huffman</b> <b>encoded.</b> Experimental results demonstrate the effectiveness of the new approach, specifically for medical and space exploration images. 1...|$|R
5000|$|... xpress - Microsoft {{compression}} protocol used by Windows 8 {{and later}} for Windows Store application updates. LZ77-based compression optionally using a <b>Huffman</b> <b>encoding.</b>|$|E
5000|$|Both {{converted}} (8-bit) and unconverted (12-bit) {{data can}} be losslessly compressed. The Cassini hardware data compressor uses a modified <b>Huffman</b> <b>encoding</b> scheme {{as part of its}} adaptive compressor.|$|E
50|$|In {{order to}} encode the above {{generated}} coefficient pattern, JPEG uses <b>Huffman</b> <b>encoding.</b> The JPEG standard provides general-purpose Huffman tables; encoders may also choose to generate Huffman tables optimized {{for the actual}} frequency distributions in images being encoded.|$|E
40|$|A filter bank–based {{algorithm}} for ECG compression {{is developed}} in this paper. The proposed method utilises a nearly-perfect reconstruction cosine modulated filter bank {{to split the}} incoming signals into several subband signals that are then quantised through thresholding and <b>Huffman</b> <b>encoded.</b> The advantage of the proposed method is that the threshold is chosen so {{that the quality of}} the retrieved signal is guaranteed. In this paper it is shown that the compression ratio achieved is an improvement over those obtained by previously reported thresholding-based algorithms...|$|R
30|$|The {{transient}} data comprises the transient position, transient type, envelope data, and sinusoids. The transient position and type are directly encoded. The envelopes {{are restricted to}} a small dictionary. The sinusoids underneath the envelope are characterized by their amplitude, frequency, and phase. Amplitude and frequency quantization {{can be done with}} different levels of accuracy. The amplitudes are uniformly quantized on a dB scale with at least 1.5 [*]dB accuracy. The frequencies are uniformly quantized on an ERB scale [50]. For a 1 [*]kHz frequency the accuracy is at least 0.75 %. Both amplitude and frequency are <b>Huffman</b> <b>encoded.</b> The phases are encoded using 5 [*]bit uniform quantization.|$|R
5000|$|The Lempel-Ziv (LZ) {{compression}} {{methods are}} among the most popular algorithms for lossless storage. [...] DEFLATE is a variation on LZ optimized for decompression speed and compression ratio, but compression can be slow. DEFLATE is used in PKZIP, Gzip, and PNG. LZW (Lempel-Ziv-Welch) is used in GIF images. LZ methods use a table-based compression model where table entries are substituted for repeated strings of data. For most LZ methods, this table is generated dynamically from earlier data in the input. The table itself is often <b>Huffman</b> <b>encoded</b> (e.g. SHRI, LZX).Current LZ-based coding schemes that perform well are Brotli and LZX. LZX is used in Microsoft's CAB format.|$|R
50|$|Methods 4, 5, 6, 7 support 4, 8, 32, 64 KiB {{sliding window}} respectively, with support of maximum 256 bytes of {{matching}} length. Static <b>Huffman</b> <b>encoding</b> is used. lh5 is first introduced in LHarc 2, followed by lh6 in LHA 2.66 (MSDOS), lh7 in LHA 2.67 beta (MSDOS). LHA itself never compresses into lh4.|$|E
5000|$|Huffman coding a two-symbol {{alphabet}} (such as {{just the}} bits 0 and 1) cannot compress at all, whereas arithmetic coding compresses bits well. If such an alphabet ({0, 1}) has probabilities {0.95, 0.05}, <b>Huffman</b> <b>encoding</b> assigns 1 bit to each value, leaving input (and thus its length) unchanged. Arithmetic coding, by contrast, approaches the optimal compression ratio of ...|$|E
50|$|Range {{encoding}} conceptually encodes all {{the symbols}} of the message into one number, unlike Huffman coding which assigns each symbol a bit-pattern and concatenates all the bit-patterns together. Thus range encoding can achieve greater compression ratios than the one-bit-per-symbol lower bound on <b>Huffman</b> <b>encoding</b> {{and it does not}} suffer the inefficiencies that Huffman does when dealing with probabilities that are not exact powers of two.|$|E
40|$|Multimedia {{transmission}} over time-varying channels such as wireless channels {{has recently}} motivated {{the research on}} the joint source-channel technique. In this paper, we present a method for joint source-channel soft decision decoding of <b>Huffman</b> <b>encoded</b> multiple sources. By exploiting the a priori bit probabilities in multiple sources, the decoding performance is greatly improved. Compared with the single source decoding scheme addressed by Marion Jeanne, the proposed technique is more practical in wideband wireless communications. Simulation results show our new method obtains substantial improvements with a minor increasing of complexity. For two sources, the gain in SNR is around 1. 5 dB by using convolutional codes when symbol-error rate (SER) reaches 10 - 2 and around 2 dB by using Turbo codes. </span...|$|R
40|$|A {{filter bank}}-based {{algorithm}} for ECG compression is developed. The proposed method utilises a nearly-perfect reconstruction cosine modu-lated filter bank {{to split the}} incoming signals into several subband signals that are then quantised through thresholding and <b>Huffman</b> <b>encoded.</b> The advantage of the proposed method is that the threshold is chosen so {{that the quality of}} the retrieved signal is guaranteed. It is shown that the compression ratio achieved is an improvement over those obtained by previously reported thresholding-based algorithms. Introduction: Several wavelet-based ECG compression methods have recently been developed that report good performance [1 – 3]. Among the reported algorithms, those based on wavelet coefficient thres-holding {{have been shown to be}} efficient and yield high compression ratios (CRs) [3]. In this Letter, we present an easy to use and efficien...|$|R
40|$|Abstract—Multimedia {{transmission}} over time-varying channels such as wireless channels {{has recently}} motivated {{the research on}} the joint source-channel technique. In this paper, we present a method for joint source-channel soft decision decoding of <b>Huffman</b> <b>encoded</b> multiple sources. By exploiting the a priori bit probabilities in multiple sources, the decoding performance is greatly improved. Compared with the single source decoding scheme addressed by Marion Jeanne, the proposed technique is more practical in wideband wireless communications. Simulation results show our new method obtains substantial improvements with a minor increasing of complexity. For two sources, the gain in SNR is around 1. 5 dB by using convolutional codes when symbol-error rate (SER) reaches 10 - 2 and around 2 dB by using Turbo codes. Index Terms—Joint source-channel decoding (JSCD), variable-length codes (VLCs), Convolutional codes (CCs), Turbo codes (TCs) I...|$|R
50|$|The BWT {{in itself}} allows for some {{compression}} with, for instance, move to front and <b>Huffman</b> <b>encoding,</b> but the transform has even more uses. The rows in the matrix are essentially the sorted suffixes {{of the text}} and the first column F of the matrix shares similarities with suffix arrays. How the suffix array relates to the BWT {{lies at the heart of}} the FM-index.|$|E
50|$|Most {{compressible}} {{data will}} end up being encoded using method , the dynamic <b>Huffman</b> <b>encoding,</b> which produces an optimised Huffman tree customised for each block of data individually. Instructions to generate the necessary Huffman tree immediately follow the block header. The static Huffman option is used for short messages, where the fixed saving gained by omitting the tree outweighs the percentage compression loss due to using a non-optimal (thus, not technically Huffman) code.|$|E
50|$|PPM {{compression}} implementations {{vary greatly}} in other details. The actual symbol selection is usually recorded using arithmetic coding, {{though it is}} also possible to use <b>Huffman</b> <b>encoding</b> or even some type of dictionary coding technique. The underlying model used in most PPM algorithms can also be extended to predict multiple symbols. It is also possible to use non-Markov modeling to either replace or supplement Markov modeling. The symbol size is usually static, typically a single byte, which makes generic handling of any file format easy.|$|E
40|$|A {{scheme is}} {{examined}} for using two alternating <b>Huffman</b> codes to <b>encode</b> a discrete independent and identically distributed source with a dominant symbol. This combined strategy, or alternating runlength Huffman (ARH) coding, {{was found to}} be more efficient than ordinary coding in certain circumstances...|$|R
5000|$|The {{boundary}} encoding leverages {{the fact}} that regions in natural images {{tend to have a}} smooth contour. This prior is used by <b>Huffman</b> coding to <b>encode</b> the difference chain code of the contours in an image. Thus, the smoother a boundary is, the shorter coding length it attains.|$|R
40|$|The {{control system}} of NASA's Orbital Maneuvering Vehicle (OMV) will employ range/range-rate radar, a forward command link, and a {{compressed}} video return link. The video data is compressed by sampling every sixth frame of data; {{a rate of}} 5 frames/sec is adequate for the OMV docking speeds. Further axial compression is obtained, albeit {{at the expense of}} spatial resolution, by averaging adjacent pixels. The remaining compression is achieved on the basis of differential pulse-code modulation and <b>Huffman</b> run-length <b>encoding.</b> A concatenated error-correction coding system is used to protect the compressed video data stream from channel errors...|$|R
5000|$|PackIt II was {{released}} in early 1986 and added <b>Huffman</b> <b>encoding.</b> However the encoding was applied after the file had already been [...] "grouped" [...] together, meaning that the compressor had {{to work on the}} file as a whole, or not at all. Since Mac files often consisted of text in the data fork and binary data in the resource fork, compressing these separately and then joining them together would likely offer better compression overall. PackIt III, released in mid-1986, added DES encryption. According to Chesley this option was not widely used, but nevertheless PackIt III became the de facto standard compression/archiving system on the Mac through this period. In December 1986 he joined Apple Computer, and work on PackIt ended.|$|E
5000|$|Photo CD images use three {{forms of}} {{compression}} {{in order to}} reduce image storage requirements. Firstly, chroma subsampling reduces the size of the images by approximately 50%. This subsampling is by a factor of 4 for 4Base images, and a factor of 2 for all other resolutions. Secondly an additional reduction in size is achieved by decomposing the highest-resolution image data, and storing the 4Base, 16Base and 64Base components as residuals (differences from pixels at the previous level of resolution). Thirdly and finally, the Photo CD system employs a form of quantization and Huffman coding to further compress this residual data. This <b>Huffman</b> <b>encoding</b> is performed on an image-row-by-image-row basis. The Huffman tables are encoded into the Photo CD image itself, and have different lengths depending on the compression class. These classes are: ...|$|E
50|$|IBM BLU Acceleration is a {{collection}} of technologies from the IBM Research and Development Labs for analytical database workloads. BLU Acceleration integrates a number of different technologies including in-memory processing of columnar data, Actionable Compression (which uses approximate <b>Huffman</b> <b>encoding</b> to compress and pack data tightly), CPU Acceleration (which exploits SIMD technology and provides parallel vector processing), and Data Skipping (which allows data that's of no use to the current active workload to be ignored). The term ‘BLU’ does not stand for anything in particular; however it has an indirect play on IBM's traditional corporate nickname Big Blue. (Ten IBM Research and Development facilities around the world filed more than 25 patents while working on the Blink Ultra project, which has resulted in BLU Acceleration.)BLU Acceleration does not require indexes, aggregates or tuning. BLU Acceleration is integrated in Version 10.5 of IBM DB2 for Linux, Unix and Windows,(DB2 for LUW) and uses the same storage and memory constructs (i.e., storage groups, table spaces, and buffer pools), SQL language interfaces, and administration tools as traditional DB2 for LUW databases. BLU Acceleration is available on both IBM POWER and x86 processor architectures.|$|E
40|$|Block-Sorting Compression, or Burrows-Wheeler Transform, is {{a process}} that modify the input string to improve the final result for a {{statistical}} data compression algorithm, like <b>Huffman</b> or Run-Length <b>Encoding.</b> In this paper, we propose some modifications on the second step of the BWTCA, the Move-To-Front heuristic, to provide encryption without losses its characteristics of the whole system. I...|$|R
40|$|It {{is shown}} that the maximum code length and {{the sum of all}} code lengths is {{dependent}} upon the method of merging combined frequencies in a <b>Huffman</b> minimum redundancy <b>encoding.</b> By bottom merging a combined frequency below a set of frequencies with equal weight an optimum encoding is obtained that has minimum ∑ piLi, minimum Lmax, and minimum ∑ Li...|$|R
5000|$|... #Caption: Visualisation {{of the use}} of <b>Huffman</b> coding to <b>encode</b> {{the message}} [...] "A_DEAD_DAD_CEDED_A_BAD_BABE_A_BEADED_ABACA_BED". In steps 2 to 6, the letters are sorted by {{increasing}} frequency, and the least frequent two at each step are combined and reinserted into the list, and a partial tree is constructed. The final tree in step 6 is traversed to generate the dictionary in step 7. Step 8 uses it to encode the message.|$|R
