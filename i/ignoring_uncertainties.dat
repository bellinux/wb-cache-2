8|431|Public
40|$|Abstract. Decisions {{are often}} {{based on the results}} of {{quantitative}} analysis. To gain confidence in these results, some indication of their quality is needed. Measurement uncertainty, as proposed by the International Organization for Standardization (ISO), is a way to express this quality. Most measurement techniques compare references and therefore need to be calibrated. Usually least squares methods <b>ignoring</b> <b>uncertainties</b> associated with the calibration references are applied. In this article we show how the measurement uncertainty can be computed in compliance with the ISO proposal, taking into account the covariances of the calibration references. Two new fitting methods, XiP–fit and P–fit, are devised and compared to other fitting methods. For our examples the P–fit method outperforms the other methods in terms of parameter recovery. Additionally, we introduce a solver which is well suited for the class of problems arising from calibration and measurement uncertainty estimation...|$|E
40|$|A {{guide to}} facts and fictions {{about climate change}} It has become fashionable {{in some parts of}} the UK media to portray the {{scientific}} evidence that has been collected about climate change and the impact of greenhouse gas emissions from human activities as an exaggeration. Some articles have claimed that scientists are <b>ignoring</b> <b>uncertainties</b> in our understanding of the climate and the factors that affect it. Some have questioned the motives of the scientists who have presented the most authoritative assessments of the science of climate change, claiming that they have a vested interest in ‘playing up ’ the potential effects that climate change is likely to have. This document examines twelve misleading arguments (presented in bold typeface) put forward by the opponents of urgent action on climate change and highlights the scientific evidence that exposes their flaws. It has been prepared by a group led by Sir David Wallace FRS, Treasurer of the Royal Society, and Sir John Houghton FRS, former chair of Working Group I of the Intergovernmental Panel on Climate Change (IPCC). This document has been endorsed by the Council of the Royal Society, and draws primarily on scientific papers published in leading peer-reviewed journals and the work of authoritative scientific organisations, such as the IPCC and the United States National Academy o...|$|E
40|$|Measuring {{transportation}} network reliability in destabilizing {{events is}} a complex task because an accurate modeling requires the inclusion of uncertainty in both the infrastructure and the users' behavior. This paper presents an approach for evaluating the performance reliability, considering the uncertainty in both demand and supply {{sides of the road}} network due to an unexpected event. These uncertainties are likely due to the effect of natural disasters on road networks. On the supply side, in addition to link capacity, environment parameters of roads, which indirectly influence parameters of link travel time, are degraded after disasters. Road environment parameters, such as visibility, geometric, pavement condition, and safety elements, impact road capacity by a perceived increased cost or inability to travel. A generalized link travel cost is suggested to capture these effects. On the demand side, elastic demand is modeled with lognormal distribution and a logit-based stochastic user equilibrium is formulated to presents the traveler's uncertain behavior in route choice. In this study, the first order second moment reliability method is used to evaluate network reliability. This paper presents a numerical example that shows the result of <b>ignoring</b> <b>uncertainties</b> after a disaster is overestimated. Also, it was observed that increasing variation of demand and supply decreases the network performance and network reliability, and the increasing knowledge of the user in route choice behavior increases the network efficiency. PublishedN/...|$|E
5000|$|To reach a decision, a {{very common}} {{practice}} is to <b>ignore</b> <b>uncertainty.</b> Decisions are reached through quantitative analysis and model building by simply using a best guess (single value) for each input variable. Decisions are then made on computed point estimates. In many cases, however, <b>ignoring</b> <b>uncertainty</b> can lead to very poor decisions, with estimations for result variables often misleading the decision maker ...|$|R
5000|$|When not {{including}} uncertainty, the optimal decision is found using only , the expected {{value of the}} uncertain quantity. Hence, the decision <b>ignoring</b> <b>uncertainty</b> is given by: ...|$|R
50|$|In {{decision}} theory and quantitative policy analysis, the expected value of including uncertainty (EVIU) is the expected {{difference in the}} value of a decision based on a probabilistic analysis versus a decision based on an analysis that <b>ignores</b> <b>uncertainty.</b>|$|R
40|$|Resource {{allocation}} promises significant {{benefits in}} wireless networks. In order to fully reap these benefits, {{it is important}} to design efficient resource allocation algorithms. Here, we develop relay power allocation (RPA) algorithms for coherent and noncoherent amplify-and-forward (AF) relay networks. The goal is to maximize the output signal-to-noise ratio under individual as well as aggregate relay power constraints. We show that these RPA problems, in the presence of perfect global channel state information (CSI), can be formulated as quasiconvex optimization problems. In such settings, the optimal solutions can be efficiently obtained via a sequence of convex feasibility problems, in the form of second-order cone programs. The benefits of our RPA algorithms, however, depend {{on the quality of the}} global CSI, which is rarely perfect in practice. To address this issue, we introduce the robust optimization methodology that accounts for uncertainties in the global CSI. We show that the robust counterparts of our convex feasibility problems with ellipsoidal uncertainty sets are semi-definite programs. Our results reveal that <b>ignoring</b> <b>uncertainties</b> associated with global CSI often leads to poor performance, highlighting the importance of robust algorithm designs in practical wireless networks. United States. Office of Naval Research (Young Investigator Award N 000140610064) National Science Foundation (U. S.) (Grant no. ANI- 0335256) National Science Foundation (U. S.) (Grant no. ECS- 0636519) DoCoMoCharles Stark Draper LaboratoryInstitute of Advanced Study. Natural Science and Technology FellowshipFP 7 European Project EUW...|$|E
40|$|We {{estimated}} and modelled how uncertainties in stochastic {{population dynamics}} and biases in parameter estimates affect {{the accuracy of}} the projections of a small island population of song sparrows which was enumerated every spring for 24 years. The estimate of the density regulation in a theta-logistic model (theta = 1. 09 suggests that the dynamics are nearly logistic, with specific growth rate r 1 = 0. 99 and carrying capacity K = 41. 54. The song sparrow population was strongly influenced by demographic (ŝigma 2 (d) = 0. 66) and environmental (ŝigma 2 (d) = 0. 41) stochasticity. Bootstrap replicates of the different parameters revealed that the uncertainties in the estimates of the specific growth rate r 1 and the density regulation theta were larger than the uncertainties in the environmental variance sigma 2 (e) and the carrying capacity K. We introduce the concept of the population prediction interval (PPI), which is a stochastic interval which includes the unknown population size with probability (1 - alpha). The width of the PPI increased rapidly with time because of uncertainties in the estimates of density regulation as well as demographic and environmental variance in the stochastic population dynamics. Accepting a 10 % probability of extinction within 100 years, neglecting uncertainties in the parameters will lead to a 33 % overestimation of the time it takes for the extinction barrier (population size X = 1) to be included into the PPI. This study shows that <b>ignoring</b> <b>uncertainties</b> in population dynamics produces a substantial underestimation of the extinction risk...|$|E
40|$|Abstract—In {{wireless}} networks, power allocation is {{an effective}} technique for prolonging network lifetime, achieving better quality-of-service (QoS), and reducing network interference. However, these benefits depend on knowledge of the channel state information (CSI), which is hardly perfect. Therefore, robust algorithms that take into account such CSI uncertainties {{play an important role}} in the design of practical systems. In this paper, we develop relay power allocation algorithms for noncoherent and coherent amplify-and-forward (AF) relay networks. The goal is to minimize the total relay transmission power under individual relay power constraints, while satisfying a QoS requirement. To make our algorithms practical and attractive, our power update rate is designed to follow large-scale fading, i. e., in the order of seconds. We show that, in the presence of perfect global CSI, our power optimization problems for noncoherent and coherent AF relay networks can be formulated as a linear program and a second-order cone program (SOCP), respectively. We then introduce robust optimization methodology that accounts for uncertainties in the global CSI. In the presence of ellipsoidal uncertainty sets, the robust counterparts of our optimization problems for noncoherent and coherent AF relay networks are shown to be an SOCP and a semi-definite program, respectively. Our results reveal that <b>ignoring</b> <b>uncertainties</b> associated with global CSI often leads to poor performance. We verify that our proposed algorithms can provide significant power savings over a naive scheme that employs maximum transmission power at each relay node. This work highlights the importance of robust algorithms with practical power update rates in realistic wireless networks. Index Terms—Amplify-and-forward, linear program, quality-of-service, relay networks, robust optimization, semi-definite program, slow power allocation. I...|$|E
40|$|A {{response}} to Reinhart & Rinella (2016) and Rinella & Reinhart (2017) ‘A common soil handling technique can generate incorrect estimates of soil biota effects on plants’ and ‘Mixing soil samples across experi- mental units <b>ignores</b> <b>uncertainty</b> and generates incorrect estimates of soil biota effects on plants...|$|R
5000|$|It {{may seem}} tempting {{to plug in}} a single best {{estimate}} [...] for , but this <b>ignores</b> <b>uncertainty</b> about , and because a source of <b>uncertainty</b> is <b>ignored,</b> the predicted distribution will be too narrow. Extreme values of [...] will occur more often than the posterior distribution suggests.|$|R
40|$|Abstract. Motion {{planning}} under {{uncertainty is}} an important problem in robotics. Although probabilistic sampling is highly successful for motion planning of robots with many degrees of freedom, sampling-based algorithms typically <b>ignore</b> <b>uncertainty</b> during planning. We introduce {{the notion of a}} bounded uncertainty roadmap (BURM) and use it to extend samplingbased algorithms for planning under uncertainty in environment maps. The key idea of our approach is to evaluate uncertainty, represented by collision probability bounds, at multiple resolutions in different regions of the configuration space, depending on their relevance for finding a best path. Preliminary experimental results show that our approach is highly effective: our BURM algorithm is at least 40 times faster than an algorithm that tries to evaluate collision probabilities exactly, and it is not much slower than classic probabilistic roadmap planning algorithms, which <b>ignore</b> <b>uncertainty</b> in environment maps. ...|$|R
40|$|Long-range {{transport}} of black carbon (BC) {{is a growing}} concern {{as a result of the}} efficiency of BC in warming the climate and its adverse impact on human health. We study transpacific {{transport of}} BC during HIPPO- 3 using a combination of inverse modeling and sensitivity analysis. We use the GEOS-Chem chemical transport model and its adjoint to constrain Asian BC emissions and estimate the source of BC over the North Pacific. We find that different sources of BC dominate the transport to the North Pacific during the southbound (29 March 2010) and northbound (13 April 2010) measurements in HIPPO- 3. While biomass burning in Southeast Asia (SE) contributes about 60 % of BC in March, more than 90 % of BC comes from fossil fuel and biofuel combustion in East Asia (EA) during the April mission. GEOS-Chem simulations generally resolve the spatial and temporal variation of BC concentrations over the North Pacific, but are unable to reproduce the low and high tails of the observed BC distribution. We find that the optimized BC emissions derived from inverse modeling fail to improve model simulations significantly. This failure indicates that uncertainties in BC removal as well as transport, rather than in emissions, account for the major biases in GEOS-Chem simulations of BC over the North Pacific. The aging process, transforming BC from hydrophobic into hydrophilic form, is one of the key factors controlling wet scavenging and remote concentrations of BC. Sensitivity tests on BC aging (<b>ignoring</b> <b>uncertainties</b> of other factors controlling BC long range transport) suggest that in order to fit HIPPO- 3 observations, the aging timescale of anthropogenic BC from EA may be several hours (faster than assumed in most global models), while the aging process of biomass burning BC from SE may occur much slower, with a timescale of a few days. To evaluate the effects of BC aging and wet deposition on transpacific transport of BC, we develop an idealized model of BC transport. We find that the mid-latitude air masses sampled during HIPPO- 3 may have experienced a series of precipitation events, particularly near the EA and SE source region. Transpacific transport of BC is sensitive to BC aging when the aging rate is fast; this sensitivity peaks when the aging timescale is in the range of 1 – 1. 5 d. Our findings indicate that BC aging close to the source must be simulated accurately at a process level in order to simulate better the global abundance and climate forcing of BC...|$|E
40|$|The initial {{masterplan}} of New Priok Development in Indonesia {{was developed}} in 2012 and is being updated to cater to new throughput demands. The masterplan of Phase I will be done as planned in which CT 1 was already operated since 2016 and CT 2 and CT 3 are in {{the final phase of}} land reclamation. In the new masterplan (2017), Phase II of the development will start in 2030 – onwards, depending on future conditions. However, whether the development of Phase II (continuation of Phase I) will be needed is still a question for the Port Authority. Hence, this study is about creating robust terminal masterplan of Phase II (2030 – onwards) which will remain functional under future uncertainties. The methodology/framework being discussed is Adaptive Port Planning (APP) which is considering flexibility design of port masterplan. Flexibility is very relevant for projects with high investment and surrounded by future uncertainties. In this study, considering the long-term project characteristics, the framework of Adaptive Port Planning (APP) Scenario-Based planning is developed. As a definition, the scenario-based approach helps to open up the perspective of the future condition of the port as a horizon of possibility and it gives chance to anticipate the vulnerabilities. Developing the scenarios are based on defined critical uncertainties using a 2 x 2 matrix approach. The economy (related to cargo performance) and environment are defined to be the axes of the matrix. The outcome is four plausible scenarios: Green Growth, Business As Usual, Moderate Expansion, and No Expansion. The next steps are examining these scenarios through some analysis to look at the characteristics and the promising industries in each scenario. In order to get the terminal needs in each scenario, traffic analysis in Port of Tanjung Priok is required; identifying the key influential cargos and commodities. There will also be a projection of containers based on defined scenarios. Producing the robust masterplan will lead us to a monitoring system and contingency planning for some highly relevant signposts. These signposts are the relevant future uncertainties which are not covered yet in the defined alternative layouts. Each of the signposts has to be analysed to prepare the contingency plans, mostly for IPC and some others for related stakeholders. In conclusion, this study proposes the adaptive framework of planning the terminal design for Phase II of New Priok, namely Adaptive Port Planning (APP) Scenario-Based Planning. It gives sequential work flow on designing future masterplan of the port, especially for the case of Phase II of New Priok. The alternative layouts and adaptive actions for IPC are some important outcomes of the framework. The port has to realize that uncertainties persevere and are very likely to influence the decision making for future layouts. Instead of <b>ignoring</b> <b>uncertainties,</b> the port needs to make contingency planning to deal with them. Hopefully, this study has benefits to make a robust terminal masterplan for Phase II of New Priok Development. New Priok Developmen...|$|E
3000|$|... “Standard {{statistical}} practice <b>ignores</b> model <b>uncertainty.</b> Data analysts typically {{select a}} model from some class of models and then proceed {{as if the}} selected model had generated the data. This approach <b>ignores</b> the <b>uncertainty</b> in model selection, leading to over-confident inferences and decisions that are more risky than one thinks they are.” (p. 382) [...]...|$|R
5000|$|In {{the context}} of {{centralized}} linear-quadratic control, with additive uncertainty in the equation of evolution but no uncertainty about coefficient values in that equation, the optimal solution for the control variables {{taking into account the}} uncertainty {{is the same as the}} solution <b>ignoring</b> <b>uncertainty.</b> This property, which gives a zero expected value of including uncertainty, is called certainty equivalence.|$|R
40|$|This paper {{explores the}} way in which society in general and economists in {{particular}} deal with fundamental uncertainty. It is argued that uncertainty is interdependent with the evolution of institutions and behavior, including that designed to help society cope with uncertainty. While some mainstream theory does address uncertainty, it employs a much narrower concept than fundamental uncertainty. But generally, in spite of the evident increase in fundamental uncertainty during the crisis, most mainstream theory ignores it. While <b>ignoring</b> <b>uncertainty</b> can at times be a successful coping mechanism, it is argued that, as a blanket coping mechanism, <b>ignoring</b> <b>uncertainty</b> seriously limits the realism of theory and therefore also practice and policy. It is concluded that economists should embrace uncertainty by tailoring methodologies and theories to address it. This would provide a more fruitful basis for policy aimed at reducing uncertainty in the economy and also reducing our own uncertainty...|$|R
3000|$|... –[*]The {{publications}} mentioned <b>ignore</b> the <b>uncertainties</b> {{associated with}} the climate sensitivity of GHGs.|$|R
40|$|Empirical Bayes {{regression}} {{procedures are}} often used in educational and psychological testing as extensions to latent variables models. The National Assessment of Educational Progress (NAEP) is an important national survey using such procedures. The NAEP applies empirical Bayes methods to models from item response theory to calibrate student responses to questions of varying difficulty. Due partially to the limited computing technology that existed when the NAEP was first conceived, NAEP analyses are carried out using a two-stage estimation procedure that <b>ignores</b> <b>uncertainty</b> about some model parameters. Furthermore, the item response theory model that the NAEP uses ignores the effect of item clustering created by {{the design of a}} test form. Using Markov chain Monte Carlo, we simultaneously estimate all parameters of an expanded model that considers item clustering to investigate the impact of item clustering and <b>ignoring</b> <b>uncertainty</b> about model parameters on an important outcome measure that the NAEP reports. Ignoring these two effects causes substantial underestimation of standard errors and induces a modest bias in location estimates...|$|R
40|$|The {{standard}} practice of selecting a single model from some class of models and then making inferences {{based on this}} model <b>ignores</b> model <b>uncertainty.</b> <b>Ignoring</b> model <b>uncertainty</b> can impair predictive performance and lead to overstatement {{of the strength of}} evidence via p-values that are too small. Bayesian model averaging provides a coherent approach for accounting for model uncertainty. A variety of methods for implementing Bayesian model averaging have been developed. A brief overview of Bayesian model averaging is provided and recently developed methodology to perform Bayesian model averaging in specific model classes is described. Literature references as well as software descriptions and relevant webpage addresses are provided. ...|$|R
50|$|There {{are several}} policy {{implications}} of multiplier uncertainty: (1) If the multiplier uncertainty is uncorrelated with additive uncertainty, its presence causes greater cautiousness to be optimal (the policy tools {{should be used}} to a lesser extent). (2) In the presence of multiplier uncertainty, it is no longer redundant to have more policy tools than there are targeted economic variables. (3) Certainty equivalence no longer applies under quadratic loss: optimal policy is not equivalent to a policy of <b>ignoring</b> <b>uncertainty.</b>|$|R
40|$|Azar (2007) {{argues that}} an {{appropriate}} market-based {{estimate of the}} US real social discount rate is 5. 66 %, with a 95 % confidence interval ranging from 5. 62 to 5. 71 %. However, this line of argument implicitly and wrongly equates the risk on public sector projects with that for the optimal portfolio of risky and risk free assets. It also vastly underestimates the confidence interval on the discount rate primarily through <b>ignoring</b> <b>uncertainty</b> surrounding the expected return on risky assets. ...|$|R
40|$|A {{computer}} model {{was developed to}} find optimal long-term investment strategies for the electric power sector under uncertainty with respect to future regulatory regimes and market conditions. The model {{is based on a}} multi-stage problem formulation and uses approxi-mate dynamic programming techniques to find an optimal solution. The model was tested under various scenarios. The model results were an-alyzed with regards to the optimal first-stage investment decision, the final technology mix, total costs, the cost of <b>ignoring</b> <b>uncertainty</b> and the cost of regulatory uncertainty...|$|R
40|$|Based on {{the recent}} {{approach}} of Bertsimas and Sim (2004, 2003) to robust optimization {{in the presence of}} data uncertainty, we prove an easily computable and simple bound on the probability that the robust solution gives an objective function value worse than the robust objective function value, under the assumption that only cost coefficients are subject to uncertainty. We exploit the binary nature of the optimization problem in proving our results. A discussion on the cost of <b>ignoring</b> <b>uncertainty</b> is also included. © 2004 Springer-Verlag Berlin/Heidelberg...|$|R
40|$|A {{simulation}} {{was carried}} out to investigate the methods of analyzing uncertain binary responses for success or failure at first insemination. A linear mixed model that included, herd, year, and month of mating as fixed effects; and unrelated service sire, sire and residual as random effects was used to generate binary data. Binary responses were assigned using the difference between days to calving and average gestation length. Females deviating from average gestation length lead to uncertain binary responses. Thus, the methods investigated were the following: (1) a threshold model fitted to certain (no uncertainty) binary data (M 1); (2) a threshold model fitted to uncertain binary data <b>ignoring</b> <b>uncertainty</b> (M 2); and (3) analysis of uncertain binary data, accounting for uncertainty from day 16 to 26 (M 3) or from day 14 to 28 (M 4) after introduction of the bull, using a threshold model with fuzzy logic classification. There was virtually no difference between point estimates obtained from M 1, M 3, and M 4 with true values. When uncertain binary data were analyzed <b>ignoring</b> <b>uncertainty</b> (M 2), sire variance and heritability were underestimated by 22 and 24 %, respectively. Thus, for noisy binary data, a threshold model contemplating uncertainty is needed to avoid bias when estimating genetic parameters...|$|R
40|$|When {{using the}} K-nearest neighbors method, one often <b>ignores</b> <b>uncertainty</b> {{in the choice}} of K. To account for such uncertainty, Holmes and Adams (2002) {{proposed}} a Bayesian framework for K-nearest neighbors (KNN). Their Bayesian KNN (BKNN) approach uses a pseudo-likelihood function, and standard Markov chain Monte Carlo (MCMC) techniques to draw posterior samples. Holmes and Adams (2002) focused on the performance of BKNN in terms of misclassification error but did not assess its ability to quantify uncertainty. We present some evidence to show that BKNN still significantly underestimates model uncertainty...|$|R
40|$|It {{has been}} widely {{recognized}} that uncertainty is an inevitable aspect of {{diagnosis and treatment of}} medical disorders. Such uncertainties hence, need to be considered in computerized medical models. The existing medical modeling techniques however, have mainly focused on capturing uncertainty associated with diagnosis of medical disorders while <b>ignoring</b> <b>uncertainty</b> of treatments. To tackle this issue, we have proposed using a fuzzy-based modeling and description technique for capturing uncertainties in treatment plans. We have further contributed a formal framework which allows for goal-oriented modeling and analysis of medical treatments. Comment: Idea Pape...|$|R
40|$|We {{study the}} impact of {{parameter}} uncertainty in the expected utility of a multiperiod investor subject to quadratic transaction costs. We characterize the utility loss associated with <b>ignoring</b> parameter <b>uncertainty,</b> and show that it {{is equal to the}} product between the single-period utility loss and another term that captures the effects of the multiperiod mean-variance utility and transaction cost losses. To mitigate {{the impact of}} parameter uncertainty, we propose two multiperiod shrinkage portfolios and demonstrate with simulated and empirical datasets that they substantially outperform portfolios that <b>ignore</b> parameter <b>uncertainty,</b> transaction costs, or both...|$|R
40|$|The {{hierarchical}} normal-normal model considered. Standard Empirical Bayes methods underestimate variability {{because they}} <b>ignore</b> <b>uncertainty</b> about the hyperparameters. Bayes' theorem solves this problem. We provide fast, exact inference that requires only a simple, univariate numerical integration {{to obtain the}} posterior distribution of the means. However, when standard, scale-invariant, vague priors are used for the variance parameters, the posterior is improper. This is solved by an intuitive reparameterization of the problem. Then standard, scale-invariant priors yield proper posterior distributions. KEY WORDS: Empirical Bayes procedure, scale-invariant prior distribution, distribution, numerical integration. posterio...|$|R
40|$|Field {{data from}} Australian Angus herds {{were used to}} {{investigate}} 2 methods of analyzing uncertain binary responses for success or failure at first insemination. A linear mixed model that included herd, year, and month of mating as fixed effects; unrelated service sire, additive animal, and residual as random effects; and linear and quadratic effects of age at mating as covariates was used to analyze binary data. An average gestation length (GL) derived from artificial insemination data was used to assign an insemination date to females mated to natural service sires. Females that deviated from this average GL led to uncertain binary responses. Two analyses were carried out: 1) a threshold model fitted to uncertain binary data, <b>ignoring</b> <b>uncertainty</b> (M 1); and 2) a threshold model fitted to uncertain binary data, accounting for uncertainty via fuzzy logic classification (M 2). There was practically no difference between point estimates obtained from M 1 and M 2 for service sire and herd variance; however, when uncertain binary data were analyzed <b>ignoring</b> <b>uncertainty</b> (M 1), additive variance and heritability estimates were greater than with M 2. Pearson correlations indicated that no major re-ranking would be expected for service sire effects and animal breeding values using M 1 and M 2. Given {{the results of the}} current study, a threshold model contemplating uncertainty is suggested for noisy binary data to avoid bias when estimating genetic parameters...|$|R
40|$|In many {{macroeconomic}} applications, impulse {{responses and}} their (bootstrap) confidence intervals are constructed by estimating a VAR model in levels - thus <b>ignoring</b> <b>uncertainty</b> regarding the true (unknown) cointegration rank. While {{it is well}} known that using a wrong cointegration rank leads to invalid (bootstrap) inference, we demonstrate that even if the rank is consistently estimated, <b>ignoring</b> <b>uncertainty</b> regarding the true rank can make inference highly unreliable for sample sizes encountered in macroeconomic applications. We investigate the effects of rank uncertainty in a simulation study, comparing several methods designed for handling model uncertainty. We propose a new method - Weighted Inference by Model Plausibility (WIMP) - that takes rank uncertainty into account in a fully data-driven way and outperforms all other methods considered in the simulation study. The WIMP method is shown to deliver intervals that are robust to rank uncertainty, yet allow for meaningful inference, approaching fixed rank intervals when evidence for a particular rank is strong. We study the potential ramifications of rank uncertainty on applied macroeconomic analysis by re-assessing the effects of fiscal policy shocks based on a variety of identification schemes that have been considered in the literature. We demonstrate how sensitive the results are to the treatment of the cointegration rank, and show how formally accounting for rank uncertainty can affect the conclusions...|$|R
40|$|We {{propose a}} Bayesian factor {{analysis}} model to rank {{the health of}} localities. Mor-tality and morbidity variables empirically contribute to the resulting rank, and popu-lation and spatial correlation are incorporated into a measure of uncertainty. We use county-level data from Texas and Wisconsin to compare our approach to conventional rankings that assign deterministic factor weights and <b>ignore</b> <b>uncertainty.</b> Greater dis-crepancies in rankings emerge for Texas than Wisconsin since the di¤erences between the empirically-derived and deterministic weights are more substantial. Uncertainty is evident in both states but becomes especially large in Texas after incorporating noise from imputing its considerable missing data...|$|R
40|$|Standard {{statistical}} practice <b>ignores</b> model <b>uncertainty.</b> Data analysts typically {{select a}} model from some class of models and then proceed {{as if the}} selected model had generated the data. This approach <b>ignores</b> the <b>uncertainty</b> in model selection, leading to over-con dent inferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA haverecently emerged. We discuss these methods and present anumber of examples. In these examples, BMA provides improved out-of-sample predictive performance. We also provide a catalogue o...|$|R
40|$|It is {{a commonplace}} in {{statistics}} that uncertainty about parameters drives learning. Indeed {{one of the}} most influential models of behavioural learning has uncertainty at its heart. However, many popular theoretical models of learning focus exclusively on error, and <b>ignore</b> <b>uncertainty.</b> Here we review the links between learning and uncertainty from three perspectives: statistical theories such as the Kalman filter, psychological models in which differential attention is paid to stimuli with an effect on the speed of learning associated with those stimuli, and neurobiological data on the influence of the neuromodulators acetylcholine and norepinephrine on learning and inference. ...|$|R
40|$|This paper {{argues that}} {{the problem with the}} targets and {{timetable}} approach to climate policy is that it <b>ignores</b> <b>uncertainty</b> about costs. An alternative approach on coordinating short run prices within a long run emissions targeting framework is outlined. A global economic model is used to demonstrate the nature of the economic uncertainty about climate policy and the gains to be achieved by equalizing carbon prices across countries. The paper also shows that although price and quantity-based systems appear to be similar they can fundamentally change the international transmission of economic shocks. Climate change Hybrid Intertemporal modeling Kyoto Protocol...|$|R
40|$|Sustainable, sustained, or survivable {{development}} are defined here as distinct "sustainability" constraints on intertemporal distribution, {{in a context}} which <b>ignores</b> <b>uncertainty,</b> environmental, and intratemporal concerns. Such constraints are neither self-contradictory; nor redundant, for they can conflict with maximizing {{the present value of}} utility ("optimality"); nor inferior, since the axioms underlying "optimality" are ethically arbitrary and refuted by empirical psychological data. Axiomatic arguments may clarify but cannot resolve debates about intertemporal concerns. Data suggest that sustainability constraints are not respected, and that intertemporal welfare functions containing a preference for "sustained improvement" may better reflect what bodies politic mean by sustainability. ...|$|R
