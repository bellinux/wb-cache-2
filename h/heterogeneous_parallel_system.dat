1|9523|Public
40|$|Abstract — <b>Heterogeneous</b> <b>parallel</b> <b>system</b> with multi {{processors}} and accelerators are becoming ubiquitous due to better cost-performance and energy-efficiency. These heterogeneous processor architectures have different instruction sets and are optimized for either task-latency or throughput purposes. Challenges occur {{in regard to}} programmability and performance when executing SPMD computations on heterogeneous architectures simultaneously. In order to meet these challenges, we implemented a MapReduce runtime system to co-process SPMD job on GPUs and CPUs on shared memory system. We are proposing a heterogeneous MapReduce programming interface for the developer and leverage the two-level scheduling approach in order to efficiently schedule tasks with heterogeneous granularities on the GPUs and CPUs. Experimental results of C-means clustering, matrix multiplication and word count indicate that using all CPU cores increase the GPU performance by 11. 5 %, 5. 1 %, and 41. 9 % respectively...|$|E
40|$|Abstract:- This article {{describes}} a parallel OpenMP and/or MPI implementation of an algorithm for solving the Recursive Least Squares (RLS) problem, suitable for <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> A model for automatic/adaptive load balancing is proposed for either homogeneous or <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> The algorithm {{is based on}} a variant of the Kalman Filter named the square root and extended version Information Filter and is oriented to be used in real time applications...|$|R
40|$|Abstract — In a <b>heterogeneous</b> <b>parallel</b> {{computer}} <b>system,</b> the computational {{power of}} each of the processors differs from one another. Furthermore, with distributed memory, the capacity of the memory, which is distributed to each of the processors, differs from one another. Using queuing system to describe a distributed memory <b>heterogeneous</b> <b>parallel</b> computer <b>system,</b> each of the heterogeneous processors will have its own heterogeneous queue. The variation of waiting time of <b>heterogeneous</b> <b>parallel</b> computer <b>system</b> with distributed memory needs to be modeled because it will help designers of <b>parallel</b> computer <b>system</b> {{to determine the extent of}} variation of the waiting time. It will also help users to know when to realize minimum variation of the waiting time. This paper models the variation of the waiting time of distributed memory <b>heterogeneous</b> <b>parallel</b> computer <b>system</b> using recursive models. It also uses the statistical method of Z-Transform to verify and validate the recursive model...|$|R
40|$|The {{emergence}} of <b>heterogeneous</b> <b>parallel</b> <b>systems</b> opens {{the possibility of}} higher performance for complex, heterogeneous applications. Unfortunately, <b>heterogeneous</b> <b>parallel</b> <b>systems</b> are even more complex to program than homogeneous <b>parallel</b> <b>systems.</b> Programmers {{should not have to}} handle all the added complexity of these systems. Instead, compilers should be extended to automatically handle as much of this complexity as possible. In previous work [50, 48], we argue that a compiler for heterogeneous systems requires a substantially more flexible software architecture than found in current compilers. Achieving this flexibility requires changes in the compiler's intermediate representation (IR). This proposal presents an intermediate representation, Score, designed to support heterogeneity. We identify four capabilities than an IR for heterogeneous systems should support (i. e., reordering of transformations, representing of multiple models of parallelism, extending to new models of parallelis [...] ...|$|R
40|$|In a <b>heterogeneous</b> <b>parallel</b> {{computer}} <b>system,</b> thecomputational {{power of}} each of the processors differs from oneanother. Furthermore, with distributed memory, the capacity ofthe memory, which is distributed to each of the processors, differsfrom one another. Using queuing system to describe a distributedmemory <b>heterogeneous</b> <b>parallel</b> computer <b>system,</b> each of theheterogeneous processors will have its own heterogeneous queue. The variation of waiting time of <b>heterogeneous</b> <b>parallel</b> computersystem with distributed memory needs to be modeled because itwill help designers of <b>parallel</b> computer <b>system</b> to determine theextent of variation of the waiting time. It will also help users toknow when to realize minimum variation of the waiting time. Thispaper models the variation of the waiting time of distributedmemory <b>heterogeneous</b> <b>parallel</b> computer <b>system</b> using recursivemodels. It also uses the statistical method of Z-Transform to verifyand validate the recursive model...|$|R
40|$|This paper {{presents}} {{the main features}} of the TURNUS co-exploration environment, an unified design space exploration framework suitable for <b>heterogeneous</b> <b>parallel</b> <b>systems</b> designed using an high level dataflow representation. The main functions of this tool are illustrated through the analysis of a video decoder implemented in the RVC-CAL dataflow language...|$|R
40|$|Abstract — Recently we {{proposed}} algorithms for concurrent execution {{on multiple}} clusters [9]. In this case, data partitioning is done at two levels; first, {{the data is}} distributed to a collection of <b>heterogeneous</b> <b>parallel</b> <b>systems</b> with different resources and startup time, then, on each system the data is evenly partitioned to the available nodes. In this paper, we report on a simulation study of the algorithms. I...|$|R
40|$|International audienceIn {{continuation}} of a successful series of events, the 6 th symposium of the Many-core Applications Research Community (MARC) took place at ONERA research center in Toulouse. On July 19 th and 20 th 2012, researchers from different fields presented their current and future work on many-core hardware architectures, their programming models, and the resulting research questions for the upcoming generation of <b>heterogeneous</b> <b>parallel</b> <b>systems...</b>|$|R
40|$|Abstract — In {{this paper}} we present our joint efforts towards the {{development}} of a parallel version of the GNU Scientific Library for heterogeneous systems. Two well-known operations arising in discrete mathematics and sparse linear algebra allow us to describe the design and the implementation of the library, and to report experimental results on heterogeneous clusters of personal computers. Index Terms — GNU Scientific Library, scientific computing, parallel algorithms and architectures, <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> I...|$|R
40|$|This paper investigates {{architectural}} {{and communication}} issues in designing <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> As {{the speed of}} processors and <b>parallel</b> <b>systems</b> keep on increasing over years, electronic interconnections like HIPPI and FDDI are reaching their limit to interconnect several <b>parallel</b> <b>systems</b> to provide <b>heterogeneous</b> <b>parallel</b> computing environment. This paper explores the suitability of passive star-coupled optical interconnection using wavelength division multiplexing as the system interconnect to provide high bandwidth (Gbits/sec) communication demanded by heterogeneous systems. Several different communication strategies (combinations of communication topologies and protocols) over Wavelength Division Multiplexed (WDM) communication media like optic fiber are investigated. A representative master-slave computational model is used to evaluate and determine suitable communication architecture for such systems. The interplay between system speed, network speed, task gr [...] ...|$|R
40|$|Part 6 : Poster SessionsInternational audienceParray (or Parallelizing ARRAYs) is an {{extension}} of C language that supports system-level succinct programming for <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> Parray extends mainstream C programming with novel array types. This leads to shorter, more portable and maintainable parallel codes, while the programmer still has control over performance-related features necessary for deep manual optimization. This paper uses the case study on stepwise program refinement of matrix transposition to illustrate the basic techniques of Parray programming...|$|R
40|$|<b>Heterogeneous</b> <b>parallel</b> <b>systems</b> are {{becoming}} increasingly more common, especially with {{the increasing use of}} cluster computers, such as Beowulf systems, and networks of workstations for parallel computing. Measuring and evaluating the performance of such <b>parallel</b> <b>systems</b> is not straightforward. In particular, conventional techniques such as computing speedup and efficiency are not appropriate for evaluating the performance of a heterogeneous system, and even have their limitations for homogeneous systems. This paper looks at alternative ways of measuring and evaluating the parallel performance of a heterogeneous system, such as linear speed, and extends this with the concept of linear efficiency...|$|R
40|$|Abstract. With {{the advent}} of <b>heterogeneous</b> <b>parallel</b> <b>systems,</b> a method is {{required}} for scheduling and mapping applications on the available hardware. We propose the concept of attributes {{in order to allow}} a runtime system to decide which implementation is usable given certain constraints such as available hardware resources or power usage. The proposed approach does not break compatibility with existing source code and binary files and can be tightly integrated into a runtime system that handles these additional specifications and allows for reconfigurable hardware to be used according to application requirements and system configuration...|$|R
40|$|In this study, at first, Parallel Virtual Machine is reviewed. Since It {{is based}} upon {{parallel}} processing, it is similar to <b>parallel</b> <b>systems</b> in principle in terms of architecture. Parallel Virtual Machine is neither an operating system nor a programming language. It is a specific software tool that supports <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> However, it {{takes advantage of the}} features of both to make users close to <b>parallel</b> <b>systems.</b> Since tasks can be executed in <b>parallel</b> on <b>parallel</b> <b>systems</b> by <b>Parallel</b> Virtual Machine, there is an important similarity between PVM and distributed systems and multiple processors. In this study, the relations in question are examined by making use of Master-Slave programming technique. In conclusion, the PVM is tested with a simple factorial computation on a distributed system to observe its adaptation to parallel architects...|$|R
40|$|In {{continuation}} of a successful series of events, the Many-core Applications Research Community (MARC) {{took place at the}} RWTH Aachen University. On November 29 th and 30 th, researchers from different fields presented their current and future work on many-core hardware architectures, their programming models, and the resulting research questions for the upcoming generation of <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> While the Intel Single-chip Cloud Computer (SCC) serves as common research platform for most MARC members, other interesting research on next generation many-core platforms (in particular the forthcoming Many Integrated Core (MIC) platform) were also presented at this event...|$|R
40|$|As <b>heterogeneous</b> <b>parallel</b> <b>systems</b> become dominant, {{application}} {{developers are}} being forced to turn to an incompatible mix of low level programming models (e. g. OpenMP, MPI, CUDA, OpenCL). However, these models do little to shield developers from the difficult problems of parallelization, data decomposition and machine-specific details. Most programmers are having a difficult time using these programming models effectively. To provide a programming model that addresses the productivity and performance requirements for the average programmer, we explore a domainspecific approach to <b>heterogeneous</b> <b>parallel</b> programming. We propose language virtualization as a new principle that enables the construction of highly efficient parallel domain specific languages that are embedded in a common host language. We define criteria for language virtualization and present techniques to achieve them. We present two concrete case studies of domain-specific languages that are implemented using our virtualization approach...|$|R
40|$|Abstra t: This paper {{reports a}} study {{held on the}} inter-pro essor ommuni ation performan e of {{homogeneous}} and <b>heterogeneous</b> <b>parallel</b> <b>systems</b> when dierent data stru ture allo ation s hemes are implemented, regarding internal and/or external pro essor memory. To evaluate the performan e parameters some ase-study algo-rithms were implemented on homogeneous and heterogeneous ar hite tures. Due to algorithm-ma hine dependen y, hardware and software features have to be onsidered. Where the a ess to external memory is not eÆ ient, some internal memory buering methods are analysed. A omparison of the results obtained is presented, enabling establishment of memory management referen es, regarding the parallel ar hite tures employed...|$|R
40|$|This paper {{reports a}} study {{held on the}} inter-processor {{communication}} performance of homogeneous and <b>heterogeneous</b> <b>parallel</b> <b>systems</b> when different data structure allocation schemes are implemented, regarding internal and/or external processor memory. To evaluate the performance parameters some case-study algorithms were implemented on homogeneous and heterogeneous architectures. Due to algorithm-machine dependency, hardware and software features have to be considered. Where the access to external memory is not efficient, some internal memory buffering methods are analysed. A comparison of the results obtained is presented, enabling establishment of memory management references, regarding the parallel architectures employed. Copyright (C) 2000 IFAC. Spanish Soc Automat Control, Int Federat Automat Control, Tech Comm Algorithms & Architectures Real Time Contro...|$|R
40|$|Large {{applications}} tend {{to contain}} several models of parallelism, {{but only a}} few of these map efficiently to the single model of parallelism embodied in a homogeneous <b>parallel</b> <b>system.</b> <b>Heterogeneous</b> <b>parallel</b> <b>systems</b> incorporate diverse models of parallelism within a single machine or across machines. These systems are already pervasive in industrial and academic settings and offer a wealth of underutilized resources for achieving high performance. Unfortunately, heterogeneity complicates software development. We believe that compilers can and should assist in managing this complexity. We identify four goals for extending compilers to assist with managing heterogeneity: exploiting available resources, targeting changing resources, adjusting optimization to suit a target, and allowing programming models and languages to evolve. These goals do not require changes to the individual pieces of a compiler so much as a restructuring of a compiler's software architecture to increase i [...] ...|$|R
40|$|Programming <b>parallel</b> <b>systems</b> is {{difficult}} especially when such systems incorporate heterogeneous components. This paper describes some approaches we are developing to cope effectively with {{various forms of}} <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> The {{first part of the}} paper describes Fortran 90 V, an extension of Fortran 90, which can support nested parallelism for expressing irregular problems more efficiently and naturally. This is especially suited to high-performance vector architectures, such as the Fujitsu VPP 300. By compiling Fortran- 90 V to Fortran 90, the language is available for all parallel machines, provided the machines have Fortran 90 compilers. The second part of the paper describes the latest development in SPP(X) which was introduced in PCW' 95. SPP(X) is a language which can co-ordinating the activities of a heterogeneous system. Further by associating performance models with the operators of the language, it is possible to aid the resource allocation in a program writt [...] ...|$|R
40|$|In {{this article}} we present SkePU 2, the next {{generation}} of the SkePU C++ skeleton programming framework for <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> We critically examine the design and limitations of the SkePU 1 programming interface. We present a new, flexible and type-safe, interface for skeleton programming in SkePU 2, and a source-to-source transformation tool which knows about SkePU 2 constructs such as skeletons and user functions. We demonstrate how the source-to-source compiler transforms programs to enable efficient execution on <b>parallel</b> <b>heterogeneous</b> <b>systems.</b> We show how SkePU 2 enables new use-cases and applications by increasing the flexibility from SkePU 1, and how programming errors can be caught earlier and easier thanks to improved type safety. We propose a new skeleton, Call, unique {{in the sense that it}} does not impose any predefined skeleton structure and can encapsulate arbitrary user-defined multi-backend computations. We also discuss how the source-to-source compiler can enable a new optimization opportunity by selecting among multiple user function specializations when building a parallel program. Finally, we show that the performance of our prototype SkePU 2 implementation closely matches that of SkePU 1. Funding Agencies|EU; SeRC; Swedish National Graduate School in Computer Science (CUGS); SNIC [2016 / 5 - 6]</p...|$|R
40|$|This paper investigates {{communication}} {{strategies for}} interconnecting <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> As {{the speed of}} processors and <b>parallel</b> <b>systems</b> keep on increasing over the years, electronic interconnections like HIPPI and FDDI are reaching their limit to provide <b>heterogeneous</b> <b>parallel</b> computing environment. This paper explores the suitability of the emerging passive star-coupled optical interconnection using wavelength division multiplexing as the system interconnect to provide high bandwidth (Gbits/sec) communication demanded by heterogeneous systems. Several di erent communication strategies (combinations of communication topologies and protocols) over Wavelength Division Multiplexed (WDM) communication media like optic ber are investigated under a representative master-slave computational model. The interplay between system speed, network speed, task granularity, and degree of parallelism is studied using both analytical modeling and simulations. It is shown that a hierarchical ALOHA-based communication strategy between the master and the slaves, implemented {{on top of the}} passive star-coupled network, leads to a considerable reduction in channel contention and provides 50 %; 80 % reduction in task completion time for applications with medium to high degrees of coarse grain parallelism. Comparable reduction in channel contention is also shown to be achieved by using tunable acoustooptic lters at master nodes...|$|R
40|$|<b>Heterogeneous</b> <b>parallel</b> <b>systems</b> {{incorporate}} {{different models}} of parallelism {{within a single}} machine or across machines and are well suited for many diverse applications [1, 2, 3]. These systems offer a wealth of underutilized resources for achieving high performance, but {{are more difficult to}} program than homogeneous systems. Compilers can and should assist in handling this complexity. Previous research has not considered the impact of architectural heterogeneity on the compiler structure [4]. A compiler for heterogeneous systems needs to order transformations based on component architectures, represent diverse architectural features, allow extension to cover new machines, and reuse its transformations and intermediate representation (IR) to target different architectures. A compiler's IR plays a central role in supporting these capabilities. This paper discusses each of these implications in detail and presents an intermediate representation, Score, that meets the needs of heterogeneit [...] ...|$|R
40|$|<b>Heterogeneous</b> <b>parallel</b> <b>systems</b> {{incorporate}} diverse {{models of}} parallelism {{within a single}} machine or across machines and are better suited for diverse applications [25, 43, 30]. These systems are already pervasive in industrial and academic settings and offer a wealth of underutilized resources for achieving high performance. Unfortunately, heterogeneity complicates software development. We believe that compilers can and should assist in handling this complexity. We identify four goals for extending compilers to manage heterogeneity: exploiting available resources, targeting changing resources, adjusting optimization to suit a target, and allowing programming models and languages to evolve. These goals do not require changes to the individual pieces of existing compilers {{so much as a}} restructuring of a compiler's software architecture to increase its flexibility. We examine six important parallelizing compilers to identify both existing solutions and where new technology is needed. 1 [...] ...|$|R
40|$|Abstract—This paper {{presents}} an {{implementation of the}} FDTD-compatible Green’s function on a <b>heterogeneous</b> <b>parallel</b> processing <b>system.</b> The developed implementation simultaneously utilizes computational power of the central processing unit (CPU) and the graphics processing unit (GPU) to the computational tasks best suited for each architecture. Recently, closed-form expression for this discrete Green’s function (DGF) was derived, which facilitates its applications in the FDTD simulations of radiation and scattering problems. Unfortunately, implementation of the new DGF formula in software requires a multiple precision arithmetic and may cause long runtimes. Therefore, an acceleration of the DGF computations on a CPU-GPU <b>heterogeneous</b> <b>parallel</b> processing <b>system</b> was developed using the multiple precision arithmetic and the OpenMP and CUDA parallel programming interfaces. The method avoids drawbacks of the CPU-and GPU-only accelerated implementations of the DGF, i. e., long runtime on the CPU and significant overhead of the GPU initialization respectively for long and short length of the DGF waveform. As a result, the sevenfold speedup was obtained relative to the reference DGF implementation on a multicore CPU thus applicability of the DGF in FDTD simulations was significantly improved. 1...|$|R
40|$|Abstract. Since {{the problem}} of {{scheduling}} independent jobs in heterogeneous computational resources is known as NP-complete [4], an approximation or heuristic algorithm is highly desirable. Grid {{is an example of}} the <b>heterogeneous</b> <b>parallel</b> computer <b>system.</b> Many researchers propose heuristic scheduling algo-rithm for Grid [1], [8], [9], [10]. In this paper, we propose a new on-line heuris-tic scheduling algorithm. We show that our scheduling algorithm has better per-formance than previous scheduling algorithms by extensive simulation. ...|$|R
40|$|High-performance {{computing}} {{is rapidly}} expanding to include distributed collections of <b>heterogeneous</b> sequential and <b>parallel</b> <b>systems</b> and irregular applications with complex, data dependent execution behavior and time varying resource demands. To provide adaptive resource management for dynamic applications, we are developing the Autopilot toolkit. Autopilot provides a flexible set of performance sensors, decision procedures, and policy actuators to realize adaptive control of applications and resource management policies on both parallel and wide area distributed systems. Keywords: Performance Analysis. Adaptive Control. Application Steering. 1 Introduction The scope of high-performance computing is rapidly expanding from single <b>parallel</b> <b>systems</b> to distributed collections of <b>heterogeneous</b> sequential and <b>parallel</b> <b>systems.</b> Moreover, emerging applications are irregular, with complex, data dependent execution behavior, and dynamic, with time varying resource demands. In consequence, applic [...] ...|$|R
40|$|<b>Heterogeneous</b> <b>parallel</b> <b>systems</b> are {{becoming}} mainstream computing platforms nowadays. One {{of the main}} challenges the development community is currently facing is how to fully exploit the available computational power when porting existing programs or developing new ones with available techniques. In this direction, several design space exploration methods have been presented and extensively adopted. However, defining the feasible design space of a dynamic dataflow program still remains an open issue. This paper proposes a novel methodology for defining such a space through a serial execution. Homotopy theoretic methods are used to demonstrate how the design space of a program can be reconstructed from its serial execution trajectory. Moreover, the concept of dependencies graph of a dataflow program defined in the literature is extended with the definition of two new kinds of dependencies - the Guard Enable and Disable - and the 3 -tuple notion needed to represent them...|$|R
40|$|In {{this paper}} we {{describe}} how performance models {{can be used}} as a more structured approach to the problem of resource allocation when programming <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> Functional skeletons are used in this paper to co-ordinate parallel computation in a heterogeneous system. An advantage of this approach is the ability to associate performance models with the implementations of a skeleton. We describe how these performance models can be used to predict the cost of of a particular resource allocation strategy over an entire program expressed using skeletons. Using a parallel conjugate gradient algorithm as a case study we investigate the approach on a mixed vector and scalar parallel machine when exploiting SPMD and simple MPMD parallelism in the algorithm. 1 Introduction Heterogeneous structures are becoming an important aspect of high performance computing. Examples of these structures arise from clustering together pools of non-uniform workstations and from using p [...] ...|$|R
40|$|This paper investigates {{architectural}} {{and communication}} issues in designing <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> The emerging high speed passive star-coupled optical interconnection using {{wavelength division multiplexing}} is considered as the system interconnect to provide high bandwidth communication demanded by heterogeneous systems. A representative master-slave computational model together with two different task scheduling strategies are used to evaluate and determine suitable communication strategy for such systems. It is shown that a hierarchical ALOHA-based communication strategy between the master and the slaves, implemented {{on top of the}} passive star-coupled network, leads' to a considerable reduction in contention and task completion time compared to the direct communication from the slaves to the master. The time division multiplexed access (TDMA) approach has performance comparable to the hierarchical strategy. Depending on the application characteristics', the number of levels' in the hierarchal scheme can be chosen to provide a balance between channel contention and overall execution time. These results' provide guidelines for designing scalable heterogeneous systems in near future...|$|R
40|$|This report concludes phase I of the project. Two {{significant}} accomplishments are reported, {{the first}} {{relating to the}} design of <b>heterogeneous</b> <b>parallel</b> <b>systems,</b> and the second related to the synthesis of parallel programs for the CM- 5 from Thinking Machines. A methodology for design and evaluation of <b>parallel</b> architectures with <b>heterogeneous</b> compo-nents has been developed. Specifically, the "communicating processes " (CP) domain in Ptolemy has been used to model heterogenous <b>parallel</b> hardware <b>systems.</b> Our first demonstra-tion design uses the "ALPS " concept (Alternative Low-level Primitive Structures). Use of the CP domain is a departure from our proposal, where we had proposed to use a discrete-event model. We have determined, however, that the CP model is considerably more convenient for high-level design of <b>heterogeneous</b> <b>parallel</b> hardware. The CP domain in Ptolemy has a multi-W tasking kernel managing concurrent processes. A process, or thread, is created for each com- 1 ý c putational and communications system resource in a particular design. The CP model of coin-L putation makes it easy to write and interconnect a variety of computational resources and-ýq simulate the execution of an application to evaluate the particular architectural configuration. We have developed a Target in Ptolemy for the CM- 5 from Thinking Machines Inc. The pro- 0 S] cessing nodes of the CM- 5 (Sparc processors with dedicated network interfaces) match wel...|$|R
40|$|This paper {{describes}} the architecture, implementation and performance, of a <b>heterogeneous</b> <b>parallel</b> deduction <b>system</b> (HPDS). The HPDS uses multiple deduction components, {{each of which}} attempts to find a refutation of the same input set, but using different deduction formats. The components cooperate by distributing clauses they generate to other components. The HPDS has been implemented in Prolog-D-Linda. Prolog-D-Linda provides appropriate data transfer and synchronisation facilities for implementing <b>parallel</b> deduction <b>systems.</b> The performance of the HPDS has been investigated. <b>Parallel</b> Deduction <b>Systems</b> A <b>parallel</b> deduction <b>system</b> {{is one in which}} multiple deduction components run in parallel on separate processors. This is distinct from those deduction systems which run multiple deduction components alternately, such as the unit preference system [Wos, Carlson & Robinson#G. A.,# 1964], and those which are only conceptually <b>parallel</b> <b>systems.</b> <b>Parallel</b> deduction <b>systems</b> can be categorised [...] ...|$|R
40|$|Although the {{research}} {{on the design of}} heterogeneous concurrent systems has a long and rich history, a unified design methodology and tool support have not emerged so far. Therefore, the creation of such systems remains a difficult, time-consuming and error-prone process. The absence of principled support for system evaluation and optimization at high level of abstraction makes the quality of the resulting implementation strongly dependent on the experience or individual preferences of the designer. In this work we are presenting TURNUS, a unified dataflow design space exploration framework for <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> This open source framework represents a decade of research on high-level modelling and simulation methods and tools for system level performance estimation and optimization. Last year we presented heuristic algorithms that were focused on the results of exploration in terms of algorithmic optimization, rapid performance estimation, application throughput, buffer size dimensioning and power optimization. This year we are presenting the novelties that have been introduced in TURNUS such as clock gating, pipelining optimization, kernel splitting algorithms, advanced partitioning algorithms and scheduling optimization based on model predictive control techniques...|$|R
40|$|<b>Heterogeneous</b> <b>parallel</b> <b>systems</b> {{are widely}} spread nowadays. Despite their availability, their usage and {{adoption}} are still limited, {{and even more}} rarely {{they are used to}} full power. Indeed, compelling new technologies are constantly developed and keep changing the technological landscape, but each of them targets a limited sub-set of supported devices, and nearly all of them require new programming paradigms and specific toolsets. Software, however, can hardly keep the pace with the growing number of computational capabilities, and developers are less and less motivated in learning skills that could quickly become obsolete. In this paper we present our effort {{in the direction of a}} transparent system optimization based on automatic code profiling and Just-In-Time compilation, that resulted in a fully-working embedded prototype capable of dynamically detect computing-intensive code blocks and automatically dispatch them to different computation units. Experimental results show that our system allows gains up to 32 x in performance [...] - after an initial warm-up phase [...] - without requiring any human intervention. Comment: 12 page...|$|R
40|$|<b>Heterogeneous</b> <b>parallel</b> {{computing}} combines {{general purpose}} processors with accelerators to efficiently execute both sequential control-intensive and data-parallel phases of applications. Existing programming models for <b>heterogeneous</b> <b>parallel</b> computing impose added coding complexity {{when compared to}} traditional sequential shared-memory programming models for homogeneous systems. This extra code complexity is assumable in supercomputing environments, where programmability is sacrificed in pursuit of high performance. However, <b>heterogeneous</b> <b>parallel</b> <b>systems</b> are massively reaching the desktop market (e. g., 425. 4 million of GPU cards were sold in 2009), where the trade-off between performance and programmability is the opposite. The code complexity required when using accelerators {{and the lack of}} compatibility prevents programmers from exploiting the full computing capabilities of <b>heterogeneous</b> <b>parallel</b> <b>systems</b> in general purpose applications. This dissertation aims to increase the programmability of CPU - accelerator systems, without introducing major performance penalties. The key insight is that general purpose application programmers tend to favor programmability at the cost of system performance. This fact is illustrated by the tendency to use high-level programming languages, such as C++, to ease the task of programming at the cost of minor performance penalties. Moreover, currently many general purpose applications are being developed using interpreted languages, such as Java, C# or python, which raise the abstraction level even further introducing relatively large performance overheads. This dissertation also takes the approach of raising the level of abstraction for accelerators to improve programmability and investigates hardware and software mechanisms to efficiently implement these high-level abstractions without introducing major performance overheads. <b>Heterogeneous</b> <b>parallel</b> <b>systems</b> typically implement separate memories for CPUs and accelerators, although commodity systems might use a shared memory at the cost of lower performance. However, in these commodity shared memory systems, coherence between accelerator and CPUs is not guaranteed. This system architecture implies that CPUs can only access system memory, and accelerators can only access their own local memory. This dissertation assumes separate system and accelerator memory and shows that low-level abstractions for these disjoint address spaces are the source of poor programmability of <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> A first consequence of having separate system and accelerator memories are the current data transfer models for <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> In this dissertation two data transfer paradigms are identified: per-call and double-buffered. In these two models, data structures used by accelerators are allocated in both, system and accelerator memories. These models differ on how data between accelerator and system memories is managed. The per-call model transfers the input data needed by accelerators before accelerator calls, and transfers back the output data produced by accelerators on accelerator call return. The per-call model is quite simple, but might impose unacceptable performance penalties due to data transfer overheads. The double-buffered model aims to overlap data communication and CPU and accelerator computation. This model requires a relative quite complex code due to parallel execution and the need of synchronization between data communication and processing tasks. The extra code required for data transfers in these two models is necessary {{due to the lack of}} by-reference parameter passing to accelerators. This dissertation presents a novel accelerator-hosted data transfer model. In this model, data used by accelerators is hosted in the accelerator memory, so when the CPU accesses this data, it is effectively accessing the accelerator memory. Such a model cleanly supports by-reference parameter passing to accelerator calls, removing the need to explicit data transfers. The second consequence of separate system and accelerator memories is that current programming models export separate virtual system and accelerator address spaces to application programmers. This dissertation identifies the double-pointer problem as a direct consequence of these separate virtual memory spaces. The double-pointer problem is that data structures used by both, accelerators and CPUs, are referenced by different virtual memory addresses (pointers) in the CPU and accelerator code. The double-pointer problem requires programmers to add extra code to ensure that both pointers contain consistent values (e. g., when reallocating a data structure). Keeping consistency between system and accelerator pointers might penalize accelerator performance and increase the accelerator memory requirements when pointers are embedded within data structures (e. g., a linked-list). For instance, the double-pointer problem requires increasing the numbers of global memory accesses by 2 X in a GPU code that reconstructs a linked-list. This dissertation argues that a unified virtual address space that includes both, system and accelerator memories is an efficient solution to the double-pointer problem. Moreover, such a unified virtual address space cleanly complements the accelerator-hosted data model previously discussed. This dissertation introduces the Non-Uniform Accelerator Memory Access (NUAMA) architecture, as a hardware implementation of the accelerator-hosted data transfer model and the unified virtual address space. In NUAMA an Accelerator Memory Collector (AMC) is included within the system memory controller to identify memory requests for accelerator-hosted data. The AMC buffers and coalesces such memory requests to efficiently transfer data from the CPU to the accelerator memory. NUAMA also implements a hybrid L 2 cache memory. The L 2 cache in NUAMA follows a write-throughwrite-non-allocate policy for accelerator hosted data. This policy ensures that the contents of the accelerator memory are updated eagerly and, therefore, when the accelerator is called, most of the data has been already transferred. The eager update of the accelerator memory contents effectively overlaps data communication and CPU computation. A write-backwrite-allocate policy is used for the data hosted by the system memory, so the performance of applications that does not use accelerators is not affected. In NUAMA, accelerator-hosted data is identified using a TLB-assisted mechanism. The page table entries are extended with a bit, which is set for those memory pages that are hosted by the accelerator memory. NUAMA increases the average bandwidth requirements for the L 2 cache memory and the interconnection network between the CPU and accelerators, but the instantaneous bandwidth, which is the limiting factor, requirements are lower than in traditional DMA-based architectures. The NUAMA architecture is compared to traditional DMA systems using cycle-accurate simulations. Experimental results show that NUAMA and traditional DMA-based architectures perform equally well. However, the application source code complexity of NUAMA is much lower than in DMA-based architectures. A software implementation of the accelerator-hosted model and the unified virtual address space is also explored. This dissertation presents the Asymmetric Distributed Shared Memory (ADSM) model. ADSM maintains a shared logical memory space for CPUs to access data in the accelerator physical memory but not vice versa. The asymmetry allows light-weight implementations that avoid common pitfalls of symmetrical distributed shared memory systems. ADSM allows programmers to assign data structures to performance critical methods. When a method is selected for accelerator execution, its associated data objects are allocated within the shared logical memory space, which is hosted in the accelerator physical memory and transparently accessible by the methods executed on CPUs. ADSM reduces programming efforts for <b>heterogeneous</b> <b>parallel</b> computing <b>systems</b> and enhances application portability. The design and implementation of an ADSM run-time, called GMAC, on top of CUDA in a GNU/Linux environment is presented. Experimental results show that applications written in ADSM and running on top of GMAC achieve performance comparable to their counterparts using programmer-managed data transfers. This dissertation presents the GMAC system, evaluates different design choices, and it further suggests additional architectural support that will likely allow GMAC to achieve higher application performance than the current CUDA model. Finally, the execution model of <b>heterogeneous</b> <b>parallel</b> <b>systems</b> is considered. Accelerator execution is abstracted in different ways in existent programming models. This dissertation explores three approaches implemented by existent programming models. OpenCL and the NVIDIA CUDA driver API use file descriptor semantics to abstract accelerators: user processes access accelerators through descriptors. This approach increases the complexity of using accelerators because accelerator descriptors are needed in any call involving the accelerator (e. g., memory allocations or passing a parameter to the accelerator). The IBM Cell SDK abstract accelerators as separate execution threads. This approach requires adding the necessary code to create new execution threads and synchronization primitives to use of accelerators. Finally, the NVIDIA CUDA run-time API abstract accelerators as Remote Procedure Calls (RPC). This approach is fundamentally incompatible with ADSM, because it assumes separate virtual address spaces for accelerator and CPU code. The <b>Heterogeneous</b> <b>Parallel</b> Execution (HPE) model is presented in this dissertation. This model extends the execution thread abstraction to incorporate different execution modes. Execution modes define the capabilities (e. g., accessible virtual address space, code ISA, etc) of the code being executed. In this execution model, accelerator calls are implemented as execution mode switches, analogously to system calls. Accelerator calls in HPE are synchronous, on the contrary of CUDA, OpenCL and the IBM Cell SDK. Synchronous accelerator calls provide full compatibility with the existent sequential execution model provided by most operating systems. Moreover, abstracting accelerator calls as execution mode switches allows application that use accelerator to run on system without accelerators. In these systems, the execution mode switch falls back to an emulation layer, which emulates the accelerator execution in the CPU. This dissertation further presents different design and implementation choices for the HPE model, in GMAC. The necessary hardware support for an efficient implementation of this model is also presented. Experimental results show that HPE introduces a low execution-time overhead while offering a clean and simple programming interface to applications. Postprint (published version...|$|R
40|$|In {{continuation}} of a successful series of events, the 4 th symposium of the Many-core Applications Research Community (MARC) {{took place at the}} Hasso Plattner Institute for Software Systems Engineering (HPI) in Potsdam. On December 8 th and 9 th 2011, researchers from different fields presented their current and future work on many-core hardware architectures, their programming models, and the resulting research questions for the upcoming generation of <b>heterogeneous</b> <b>parallel</b> <b>systems.</b> While the Intel Single Chip Cloud Computer (SCC) serves as common research platform for most MARC members, other interesting research on next generation many-core platforms was also discussed on this event. The symposium focused on topics such as • Operating system support for novel many-core architectures • Virtualization solutions to deal with hardware limitations • Dealing with legacy software on novel many-core architectures • New approaches for leveraging on-die messaging facilities • Traditional and new programming models for novel many-core hardware • Concepts for runtime systems on novel many-core hardware • Performance issues with modern on-die messaging facilities and caching infrastructures This proceedings include 14 papers from 5 symposium sessions. Every paper was reviewed by at least three reviewers from the program committee, consisting of...|$|R
