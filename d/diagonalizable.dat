945|1|Public
25|$|The {{family of}} Hermitian {{matrices}} is a proper subset of matrices that are unitarily <b>diagonalizable.</b> A matrix M is unitarily <b>diagonalizable</b> if {{and only if}} it is normal, i.e. M*M = MM*. Similar statements hold for compact normal operators.|$|E
25|$|Such a {{transformation}} {{is called a}} <b>diagonalizable</b> matrix since in the eigenbasis, the transformation is represented by a diagonal matrix. Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form. Not all matrices are <b>diagonalizable</b> (even over an algebraically closed field).|$|E
25|$|A is <b>diagonalizable</b> if {{and only}} if, for every {{eigenvalue}} λ of A, its geometric and algebraic multiplicities coincide.|$|E
25|$|Any {{subspace}} {{spanned by}} eigenvectors of T is an invariant subspace of T, and the restriction of T {{to such a}} subspace is <b>diagonalizable.</b> Moreover, if the entire vector space V can be spanned by the eigenvectors of T, or equivalently if the direct sum of the eigenspaces associated with all the eigenvalues of T is the entire vector space V, then a basis of V called an eigenbasis can be formed from linearly independent eigenvectors of T. When T admits an eigenbasis, T is <b>diagonalizable.</b>|$|E
25|$|Conversely, {{suppose a}} matrix A is <b>diagonalizable.</b> Let P be a {{non-singular}} square matrix such that P−1AP is some diagonal matrix D. Left multiplying both by P, AP = PD. Each column of P {{must therefore be}} an eigenvector of A whose eigenvalue is the corresponding diagonal element of D. Since the columns of P must be linearly independent for P to be invertible, there exist n linearly independent eigenvectors of A. It then follows that the eigenvectors of A form a basis {{if and only if}} A is <b>diagonalizable.</b>|$|E
25|$|The {{degree of}} an {{elementary}} divisor {{is the size}} of the corresponding Jordan block, therefore the dimension of the corresponding invariant subspace. If all elementary divisors are linear, A is <b>diagonalizable.</b>|$|E
25|$|But {{this simple}} {{procedure}} also works for defective matrices, in a generalization due to Buchheim. This is illustrated {{here for a}} 4×4 example of a matrix which is not <b>diagonalizable,</b> and the s are not projection matrices.|$|E
25|$|The Jordan–Chevalley {{decomposition}} {{is particularly}} simple {{with respect to}} a basis for which the operator takes its Jordan normal form. The diagonal form for <b>diagonalizable</b> matrices, for instance normal matrices, is a special case of the Jordan normal form.|$|E
25|$|While this {{provides}} a valid proof (for matrices over the complex numbers), {{the argument is}} not very satisfactory, since the identities represented by the theorem do {{not in any way}} depend {{on the nature of the}} matrix (<b>diagonalizable</b> or not), nor on the kind of entries allowed (for matrices with real entries the <b>diagonalizable</b> ones do not form a dense set, and it seems strange one would have to consider complex matrices to see that the Cayley–Hamilton theorem holds for them). We shall therefore now consider only arguments that prove the theorem directly for any matrix using algebraic manipulations only; these also have the benefit of working for matrices with entries in any commutative ring.|$|E
25|$|Augustin-Louis Cauchy {{proved the}} {{spectral}} theorem for self-adjoint matrices, i.e., that every real, symmetric matrix is <b>diagonalizable.</b> In addition, Cauchy {{was the first}} to be systematic about determinants. The spectral theorem as generalized by John von Neumann is today perhaps the most important result of operator theory.|$|E
25|$|Since X is a Hermitian matrix, {{it should}} be <b>diagonalizable,</b> {{and it will be}} clear from the {{eventual}} form of P that every real number can be an eigenvalue. This makes some of the mathematics subtle, since there is a separate eigenvector for every point in space.|$|E
25|$|In {{the last}} 20 years, {{there have been}} many works trying to find a measure-classification theorem similar to Ratner's theorems but for <b>diagonalizable</b> actions, {{motivated}} by conjectures of Furstenberg and Margulis. An important partial result (solving those conjectures with an extra assumption of positive entropy) was proved by Elon Lindenstrauss, and he was awarded the Fields medal in 2010 for this result.|$|E
25|$|A matrix {{that is not}} <b>diagonalizable</b> {{is said to be}} {{defective}}. For defective matrices, {{the notion}} of eigenvectors generalizes to generalized eigenvectors and the diagonal matrix of eigenvalues generalizes to the Jordan normal form. Over an algebraically closed field, any matrix A has a Jordan normal form and therefore admits a basis of generalized eigenvectors and a decomposition into generalized eigenspaces.|$|E
25|$|For example, the {{spectral}} theory of compact operators on Banach spaces takes {{a form that}} {{is very similar to}} the Jordan canonical form of matrices. In the context of Hilbert spaces, a square matrix is unitarily <b>diagonalizable</b> if and only if it is normal. A corresponding result holds for normal compact operators on Hilbert spaces. More generally, the compactness assumption can be dropped. But, as stated above, the techniques used to prove e.g. {{the spectral}} theorem are different, involving operator valued measures on the spectrum.|$|E
25|$|A can {{therefore}} be decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the inverse of the matrix of eigenvectors. This is called the eigendecomposition {{and it is a}} similarity transformation. Such a matrix A is said to be similar to the diagonal matrix Λ or <b>diagonalizable.</b> The matrix Q is the change of basis matrix of the similarity transformation. Essentially, the matrices A and Λ represent the same linear transformation expressed in two different bases. The eigenvectors are used as the basis when representing the linear transformation asΛ.|$|E
500|$|... is in [...] {{but there}} is no [...] such that [...] are traceless. But then [...] is <b>diagonalizable,</b> hence [...] is <b>diagonalizable,</b> which is a contradiction.|$|E
500|$|The PBH {{test was}} {{originally}} discovered by Elmer G. Gilbert in 1963, but Gilbert's version only applied to systems {{that could be}} represented by a <b>diagonalizable</b> matrix. [...] The test was subsequently generalised by Vasile M. Popov (in 1966), Belevitch (in Classical Network Theory, 1968) and Malo Hautus in 1969.|$|E
500|$|The eigendecomposition or {{diagonal}}ization expresses A as {{a product}} VDV−1, where D is a diagonal matrix and V is a suitable invertible matrix. [...] If A can be written in this form, it is called <b>diagonalizable.</b> More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, {{that is to say}} matrices whose only nonzero entries are the eigenvalues λ1 to λn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right. Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated via ...|$|E
2500|$|An n [...] n matrix A is <b>diagonalizable</b> if {{and only}} if the sum of the {{dimensions}} of the eigenspaces is n. Or, equivalently, if {{and only if}} A has n linearly independent eigenvectors. Not all matrices are <b>diagonalizable.</b> Consider the following matrix: ...|$|E
2500|$|This result {{also allows}} one to exponentiate <b>diagonalizable</b> matrices. If ...|$|E
2500|$|Not all {{matrices}} are <b>diagonalizable,</b> but for matrices {{with complex}} coefficients {{many of them}} are: the set of <b>diagonalizable</b> complex square matrices of a given size is dense in the set of all such square matrices (for a matrix to be <b>diagonalizable</b> it suffices for instance that its characteristic polynomial not have any multiple roots). Now {{if any of the}} [...] expressions that the theorem equates to [...] would not reduce to a null expression, in other words if it would be a nonzero polynomial in the coefficients of the matrix, then the set of complex matrices for which this expression happens to give [...] would not be dense in the set of all matrices, which would contradict the fact that the theorem holds for all <b>diagonalizable</b> matrices. Thus one can see that the Cayley–Hamilton theorem must be true.|$|E
2500|$|If Q is <b>diagonalizable,</b> {{the matrix}} {{exponential}} can be computed directly: let Q=U−1ΛU be a diagonalization of Q, with ...|$|E
2500|$|On {{the other}} hand, if [...] {{does not have}} [...] linearly {{independent}} eigenvectors associated with it, then [...] is not <b>diagonalizable.</b>|$|E
2500|$|If the {{intersection}} form is definite Donaldson's theorem [...] gives a complete answer: {{there is a}} smooth structure {{if and only if}} the form is <b>diagonalizable.</b>|$|E
2500|$|On {{the other}} hand, if [...] is not <b>diagonalizable,</b> we choose [...] to be a {{generalized}} modal matrix for , such that [...] is the Jordan normal form of [...] [...] The system [...] has the form ...|$|E
2500|$|In the {{terminology}} of physics A, A* give a single boson and L0 is the energy operator. It is <b>diagonalizable</b> with eigenvalues 0, 1/2, 1, 3/2, ...., each of multiplicity one. Such a representation {{is called a}} positive energy representation.|$|E
2500|$|In {{the general}} case, {{we try to}} diagonalize [...] and reduce the system (...) to a system like (...) as follows. [...] If [...] is <b>diagonalizable,</b> we have , where [...] is a modal matrix for [...] [...] Substituting , {{equation}} (...) takes the form , or ...|$|E
2500|$|Including multiplicity, the {{eigenvalues}} of A are λ = 1, 2, 4, 4. The {{dimension of}} the eigenspace corresponding to the eigenvalue 4 is 1 (and not 2), so A is not <b>diagonalizable.</b> However, there is an invertible matrix P such that A = PJP1, where ...|$|E
2500|$|Recall {{from above}} that an n×n matrix [...] {{amounts to a}} linear {{combination}} of the first −1 powers of [...] by the Cayley-Hamilton theorem. [...] For <b>diagonalizable</b> matrices, as illustrated above, e.g. in the 2×2 case, [...] Sylvester's formula yields [...] , where the s are [...] the Frobenius covariants of [...]|$|E
2500|$|There {{are several}} {{equivalent}} ways to define an ordinary eigenvector. [...] For our purposes, an eigenvector [...] {{associated with an}} eigenvalue [...] of an [...] × [...] matrix [...] is a nonzero vector for which , where [...] is the [...] × [...] identity matrix and [...] is the zero vector of length [...] [...] That is, [...] is in the kernel of the transformation [...] [...] If [...] has [...] linearly independent eigenvectors, then [...] {{is similar to a}} diagonal matrix [...] [...] That is, there exists an invertible matrix [...] such that [...] is <b>diagonalizable</b> through the similarity transformation [...] [...] The matrix [...] is called a spectral matrix for [...] [...] The matrix [...] is called a modal matrix for [...] [...] <b>Diagonalizable</b> matrices are of particular interest since matrix functions of them can be computed easily.|$|E
2500|$|Every {{vector space}} over [...] can be {{provided}} with an inner product. A representation [...] of a group [...] in a vector space endowed with an inner product is called unitary, if [...] is unitary for every [...] This means that in particular every [...] is <b>diagonalizable.</b> For more details see the article on unitary representations.|$|E
2500|$|The {{spectral}} theorem {{extends to}} a more general class of matrices. Let [...] be an operator on a finite-dimensional inner product space. [...] {{is said to be}} normal [...] if [...] One can show that [...] is normal if and only if it is unitarily <b>diagonalizable.</b> Proof: By the Schur decomposition, we can write any matrix as , where [...] is unitary and [...] is upper-triangular.|$|E
2500|$|The {{classification}} {{result for}} Hermitian [...] matrices is the spectral theorem: If M = M*, then M is unitarily <b>diagonalizable</b> and the diagonalization of M has real entries. Let T be a compact self adjoint operator on a Hilbert space H. We will prove the same statement for T: the operator T can be diagonalized by an orthonormal set of eigenvectors, {{each of which}} corresponds to a real eigenvalue.|$|E
2500|$|Together {{with the}} matrix {{multiplication}} [...] is an infinite group. [...] acts on [...] by matrix-vector multiplication. We consider the representation [...] for all [...] The subspace [...] is a -invariant subspace. However, there exists no -invariant complement to this subspace. The assumption, {{that such a}} complement exists, results in the statement, that every matrix is <b>diagonalizable</b> over [...] This {{is known to be}} wrong and thus presents the contradiction.|$|E
2500|$|Using {{generalized}} eigenvectors, {{a set of}} linearly independent eigenvectors of [...] can be extended, if necessary, to {{a complete}} basis for [...] [...] This basis {{can be used to}} determine an [...] "almost diagonal matrix" [...] in Jordan normal form, similar to , which is useful in computing certain matrix functions of [...] [...] The matrix [...] is also useful in solving the system of linear differential equations [...] where [...] need not be <b>diagonalizable.</b>|$|E
2500|$|As stated earlier, {{from the}} {{equation}} [...] (if exists) the stationary (or steady state) distribution π is a left eigenvector of row stochastic matrix P. Then assuming that P is <b>diagonalizable</b> or equivalently that P has n linearly independent eigenvectors, speed of convergence is elaborated as follows. (For non-diagonalizable, i.e. defective matrices, one may start with the Jordan normal form of P and proceed with a bit more involved set of arguments in a similar way.) ...|$|E
