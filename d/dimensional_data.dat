2600|1022|Public
25|$|Star and {{snowflake}} schemas {{are most}} {{commonly found in}} <b>dimensional</b> <b>data</b> warehouses and data marts where speed of data retrieval {{is more important than}} the efficiency of data manipulations. As such, the tables in these schemas are not normalized much, and are frequently designed at a level of normalization short of third normal form.|$|E
25|$|Using the {{equations}} 5 to 13 and the <b>dimensional</b> <b>data</b> in, the thermal resistance for the fins {{was calculated for}} various air flow rates. The data for the thermal resistance and heat transfer coefficient are shown in the diagram, which shows that for an increasing air flow rate, the thermal resistance of the heat sink decreases.|$|E
25|$|In 2007 model {{manufacturer}} Bachmann Branchline and the National Railway Museum {{announced the}} release of an OO scale model of the DP1 for sale at the NRM Shop. Bachmann used laser-scanning (see 3D scanner) of the machine at the National Railway Museum Shildon to collect <b>dimensional</b> <b>data</b> of the locomotive. The model was praised for its attention to fine detail and smooth and powerful running. Bachmann released a N-scale model under the Graham Farish brand in 2010.|$|E
40|$|Abstract. The K-Nearest Neighbor (KNN) join is an {{expensive}} but important operation in many data mining algorithms. Several recent ap-plications need to perform KNN join for high <b>dimensional</b> sparse <b>data.</b> Unfortunately, all existing KNN join algorithms are designed for low di-mensional data. To fulfill this void, we investigate the KNN join problem for high <b>dimensional</b> sparse <b>data.</b> In this paper, we propose three KNN join algorithms: a brute force (BF) algorithm, an inverted index-based(IIB) algorithm and an improved inverted index-based(IIIB) algorithm. Extensive experiments on both synthetic and real-world datasets were conducted to demonstrate the effectiveness of our algorithms for high <b>dimensional</b> sparse <b>data.</b> ...|$|R
40|$|The method {{involves}} {{creating a}} preoperative three <b>dimensional</b> image <b>data</b> {{set of the}} navigation pane. The two dimensional fluoroscopy image of the navigation pane is generated. The image position at {{a portion of the}} medical instrument in the two dimensional fluoroscopy image is determined. The two dimensional fluoroscopy image is registered with respect to the three <b>dimensional</b> image <b>data</b> set. The portion of the medical instrument in a signal generated from the three <b>dimensional</b> image <b>data</b> is inserted based on the image position and properties of instrument. An independent claim is included for apparatus for supporting image in navigation area of patient medical instrument...|$|R
40|$|High <b>dimensional</b> image <b>data</b> {{that are}} now {{becoming}} available, offer new possibilities in image classification, specially when dealing with classes that present very similar spectral response. High <b>dimensional</b> image <b>data</b> poses, however, the problem of obtaining accurate estimates of the parameters required by statistical classifiers. This problem {{is caused by the}} small number of training samples usually available in real world conditions. Different approaches have been proposed in the literature aiming to mitigate this problem. One approach involves the techniques of regularization of the covariance matrix. This study investigates the applications of one regularization technique to high <b>dimensional</b> image <b>data.</b> Tests are performed using AVIRIS data, covering agricultural fields, and the results are presented and discussed. Pages: 1061 - 106...|$|R
5000|$|Since {{almost all}} high <b>dimensional</b> <b>data</b> {{in the real}} world will just by chance exhibit some fair degree of {{collinearity}} across at least some variables, the problem that LARS has with correlated variables may limit its application to high <b>dimensional</b> <b>data.</b>|$|E
5000|$|Cluster Analysis of High <b>Dimensional</b> <b>Data</b> using Particle Trajectory Sonification http://icad.org/icad2017/icad2017_paper_22.pdf ...|$|E
50|$|A <b>dimensional</b> <b>data</b> {{element is}} similar to a {{categorical}} variable in statistics.|$|E
40|$|We {{present a}} {{technique}} for clustering categorical data by generating many dissimilarity matrices and averaging over them. We begin by demonstrating our technique on low <b>dimensional</b> categorical <b>data</b> and {{comparing it to}} several other techniques that have been proposed. Then we give conditions under which our method should yield good results in general. Our method extends to high <b>dimensional</b> categorical <b>data</b> of equal lengths by ensembling over many choices of explanatory variables. In this context we compare our method with two other methods. Finally, we extend our method to high <b>dimensional</b> categorical <b>data</b> vectors of unequal length by using alignment techniques to equalize the lengths. We give examples to show that our method continues to provide good results, in particular, better {{in the context of}} genome sequences than clusterings suggested by phylogenetic trees...|$|R
40|$|Partial {{derivative}} {{values are}} required to construct a smooth interpolated surface which passes through given three <b>dimensional</b> scattered <b>data</b> points. These partial derivative values are not avaliable in practice for raw three <b>dimensional</b> scattered <b>data</b> and the estimation of these values is thus desirable. This research concentrates on estimating the partial derivative values {{up to the second}} order. Many current methods concentrate on first order partial derivatives estimation as most applications require only up to the first order derivatives...|$|R
5000|$|Sometimes {{processing}} {{must take}} place sequentially. For example, <b>dimensional</b> (reference) <b>data</b> are needed before {{one can get}} and validate the rows for main [...] "fact" [...] tables.|$|R
5000|$|... 1970s - ACNielsen and IRI provide <b>dimensional</b> <b>data</b> marts for retail sales.|$|E
5000|$|Irrelevant attributes: in high <b>dimensional</b> <b>data,</b> a {{significant}} number of attributes may be irrelevant ...|$|E
5000|$|... where [...] is the [...] <b>dimensional</b> <b>data</b> vectorSimilarly we express [...] {{in terms}} of [...] by ...|$|E
40|$|International audienceIn this paper, an {{adaptation}} of the eikonal equation is proposed by considering the latter on weighted graphs of arbitrary structure. This novel approach {{is based on a}} family of discrete morphological local and nonlocal gradients expressed by partial difference equations (PdEs). Our formulation of the eikonal equation on weighted graphs generalizes local and nonlocal configurations in the context of image processing and extends this equation for the processing of any unorganized high <b>dimensional</b> discrete <b>data</b> that can be represented by a graph. Our approach leads to a unified formulation for image segmentation and high <b>dimensional</b> irregular <b>data</b> processing...|$|R
30|$|Feature {{selection}} techniques use a search-criteria driven {{approach for}} ranked feature subset selection. Often, selecting an optimal subset of ranked features using the existing methods is intractable for high <b>dimensional</b> gene <b>data</b> classification problems.|$|R
40|$|Uncovering driver genes {{is crucial}} for {{understanding}} heterogeneity in cancer. L 1 -type regularization approaches {{have been widely used}} for uncovering cancer driver genes based on genome-scale data. Although the existing methods have been widely applied in the field of bioinformatics, they possess several drawbacks: subset size limitations, erroneous estimation results, multicollinearity, and heavy time consumption. We introduce a novel statistical strategy, called a Recursive Random Lasso (RRLasso), for high <b>dimensional</b> genomic <b>data</b> analysis and investigation of driver genes. For time-effective analysis, we consider a recursive bootstrap procedure in line with the random lasso. Furthermore, we introduce a parametric statistical test for driver gene selection based on bootstrap regression modeling results. The proposed RRLasso is not only rapid but performs well for high <b>dimensional</b> genomic <b>data</b> analysis. Monte Carlo simulations and analysis of the "Sanger Genomics of Drug Sensitivity in Cancer dataset from the Cancer Genome Project" show that the proposed RRLasso is an effective tool for high <b>dimensional</b> genomic <b>data</b> analysis. The proposed methods provide reliable and biologically relevant results for cancer driver gene selection...|$|R
50|$|The major {{disadvantage}} {{of this method}} is that although we could extend this algorithm to any <b>dimensional</b> <b>data</b> we only use it for Two dimension applications. Because the computation time of higher <b>dimensional</b> <b>data</b> would be proportional {{to the number of}} IMF's of the succeeding dimensions. Hence it could exceed the computation capacity for a Geo-Physical data processing system when the number of EMD in the algorithm is large. Hence we have mentioned below faster and better techniques to tackle this disadvantage.|$|E
5000|$|Stride {{memory accesses}} of high-dimensional data: High <b>dimensional</b> <b>data</b> {{are stored in}} non-continuous memory locations. Accesses along high {{dimensions}} are thus strided and uncoalesced, wasting available memory bandwidth.|$|E
50|$|A typical {{scenario}} is where an inspection device collects <b>dimensional</b> <b>data</b> and sends {{the information to}} an SPC package for process analysis or a database for long term storage.|$|E
5000|$|<b>Dimensional</b> {{exploration}} of <b>data,</b> for example analyzing sales by product line, by region, by time period ...|$|R
40|$|Nowadays, reverse {{engineering}} {{is very important}} for mechanical manufacturing. As one of the key processes of {{reverse engineering}}, three <b>dimensional</b> surface <b>data</b> measurement is the basis of three dimensional model reconstruction. And non-contact laser probe measurement technology leads the development of three <b>dimensional</b> surface <b>data</b> acquirement. In this paper, based on optical trigonometry theory, a non-contact laser probe measurement system was designed and implemented including the selection of the main hardware, the construction of the hardware system, and the design of the VC++ software system. The feasibility of the measurement system was testified through measuring a groove model and acquiring the surface data. the Natural Science Foundation of LiaoningProvince of China under Grant Nos. L 05021...|$|R
40|$|Abstract. The Generative T opographicMapping (GTM) was devel-oped and {{introduced}} as a principle dalternativ eto the Self-Organising Map for, principally, visualising high <b>dimensional</b> continuous <b>data.</b> There are many {{cases where the}} observation data is ordinal and discrete {{and the application of}} methods developed specically for continuous data is inappropriate. Based on the continuous GTM data model a non-linear latent variable model for modeling sparse high <b>dimensional</b> binary <b>data</b> is presen ted. The primary motivation forthis w ork is the requirement for a dense and low dimensional representation of sparse binary vector space models of text documents based on the multiv ariate Bernoulli event model. The method is however applicable to binary data in general. 1...|$|R
50|$|In a {{traditional}} single <b>dimensional</b> <b>data</b> structure (e.g. hash), a search {{on a single}} criterion is usually very simple but searching for a second criterion can be much more complex.|$|E
5000|$|B. L. Milenova and M. M. Campos. O-Cluster: {{scalable}} clustering {{of large}} high <b>dimensional</b> <b>data</b> sets. In {{proceedings of the}} 2002 IEEE International Conference on Data Mining: ICDM 2002. pp290-297, [...]|$|E
5000|$|Demonstrating {{the ability}} to infer causal {{relationships}} among features in high <b>dimensional</b> <b>data</b> using DNA variation information, Schadt {{and his colleagues at}} Merck began reconstructing predictive networks that were shown to be causally associated with disease, ...|$|E
40|$|Abstract. High <b>dimensional</b> {{structured}} <b>data</b> such as {{text and}} images is often poorly understood and misrepresented in statistical modeling. Typical approaches to modeling such data involve, either explicitly or implicitly, arbitrary geometric assumptions. In this paper, we review a framework introduced by Lebanon and Lafferty {{that is based}} on Čencov’s theorem for obtaining a coherent geometry for data. The framework enables adaptation of popular models to the new geometry and in the context of text classification yields superior performance with respect to classification error rate on held out data. The framework demonstrates how information geometry may be applied to modeling high <b>dimensional</b> structured <b>data</b> and points at new directions for future research. 1...|$|R
40|$|International audienceCo-{{clustering}} is {{more useful}} than one-sided clustering {{when dealing with}} high <b>dimensional</b> sparse <b>data.</b> We propose to address the aim of document clustering with a generative model-based co-clustering approach. To this end, we rely on a particular mixture of von Mises-Fisher distributions and propose a new parsimonious model allowing to reveal a block diagonal structure {{as well as a}} good partitioning of documents and terms. Then, by setting the estimate of the model parameters under the maximum likelihood (ML) approach, we derive three novel co-clustering algorithms: a soft one and two stochastic variants. Empirical results on numerous simulated and real-world datasets, demonstrate the advantages of our approach to model and co-cluster high <b>dimensional</b> sparse <b>data...</b>|$|R
40|$|The {{invention}} is {{a system}} and method of compressing a DTM {{to be used in}} an Auto-GCAS system using a semi-regular geometric compression algorithm. In general, the invention operates by first selecting the boundaries of the three dimensional map to be compressed and dividing the three <b>dimensional</b> map <b>data</b> into regular areas. Next, a type of free-edged, flat geometric surface is selected which will be used to approximate terrain data of the three <b>dimensional</b> map <b>data.</b> The flat geometric surface is used to approximate terrain data for each regular area. The approximations are checked to determine if they fall within selected tolerances. If the approximation for a specific regular area is within specified tolerance, the data is saved for that specific regular area. If the approximation for a specific area falls outside the specified tolerances, the regular area is divided and a flat geometric surface approximation is made for each of the divided areas. This process is recursively repeated until all of the regular areas are approximated by flat geometric surfaces. Finally, the compressed three <b>dimensional</b> map <b>data</b> is provided to the automatic ground collision system for an aircraft...|$|R
5000|$|Friedman has {{authored}} and co-authored many {{publications in}} the field of data-mining including [...] "nearest neighbor classification, logistical regressions, and high <b>dimensional</b> <b>data</b> analysis. His primary research interest is in the area of machine learning." [...] A selection: ...|$|E
50|$|A TDSC {{detector}} digitises {{the original}} calls and derives a two <b>dimensional</b> <b>data</b> string by analysing {{the parameters of}} each call with respect to time. This is analysed by a neural network to provide pattern recognition for each species.|$|E
5000|$|Hydrocarbon Processing: Laser Scanning Technology Improves Plant Quality, Safety and Training, Digitize Your Plant Using Laser Scanning for Easier Access to Information, Comparing Historical and Laser Scan Surveys to Identify Discrepancies and Laser Scanning Solutions Secure Accurate <b>Dimensional</b> <b>Data.</b>|$|E
40|$|Direct volume {{rendering}} is {{an efficient}} method for plotting three <b>dimensional</b> scientific <b>data.</b> However, {{the technique is}} not as frequently used as it could be. In this article, we summarize direct volume rendering and discuss the barriers {{that need to be}} overcome {{to take advantage of this}} powerful technique. ...|$|R
5000|$|Network-based {{approaches}} for analyzing high <b>dimensional</b> genomic <b>data</b> sets. For example, weighted correlation network analysis {{is often used}} for identifying clusters (referred to as modules), modeling the relationship between clusters, calculating fuzzy measures of cluster (module) membership, identifying intramodular hubs, and for studying cluster preservation in other data sets.|$|R
30|$|The axial {{resolution}} of most SS-OCT systems {{is on the}} order of 10  μm in tissue and doesn’t match high resolution SOCT systems. Due to the high imaging speed, FDOCT systems enable the acquisition of three <b>dimensional</b> image <b>data</b> in-vivo which is especially beneficial for numerous ophthalmic imaging applications [12].|$|R
