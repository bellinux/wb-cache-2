10000|10000|Public
5|$|Simonsohn {{also raised}} the {{possibility}} of reverse causality in the case of Anseel and Duyck's analysis of a large <b>data</b> <b>set</b> consisting of Belgians' last names and the companies they work for. Anseel and Duyck concluded that people tend to choose to work for companies that match their initial.But Simonsohn suspected that, likeWalt Disney working for Disney, many people work for companies named after themselves or a family member. When he controlled for reverse causality in a large US <b>data</b> <b>set,</b> he could not see any evidence for people choosing to work for companies matching their initial.|$|E
5|$|Multiple {{data sets}} may be {{necessary}} for certain phasing methods. For example, MAD phasing requires that the scattering be recorded at least three (and usually four, for redundancy) wavelengths of the incoming X-ray radiation. A single crystal may degrade too much during the collection of one <b>data</b> <b>set,</b> owing to radiation damage; in such cases, data sets on multiple crystals must be taken.|$|E
5|$|The {{modified}} Atkins diet reduces seizure frequency by {{more than}} 50% in 43% of patients who try it and {{by more than}} 90% in 27% of patients. Few adverse effects have been reported, though cholesterol is increased and the diet has not been studied long term. Although based on a smaller <b>data</b> <b>set</b> (126 adults and children from 11 studies over five centres), these results from 2009 compare favourably with the traditional ketogenic diet.|$|E
40|$|A {{large number}} of <b>Data</b> <b>Sets</b> are available, but they are {{incomplete}} in nature so that they cannot be used for real applications. These incomplete <b>data</b> <b>sets</b> are produced due to various reasons like system failure, privacy of data, incomplete input, time delay in system, lack of available resources. As common examples Weather <b>data</b> <b>sets,</b> sensor image <b>data</b> <b>sets,</b> Measurement Reflection in Agriculture <b>Data</b> <b>Sets.</b> For these cases we use conceptual data reconstruction by using statistical models of multiple linear Regressions to Predict Missing values in <b>Data</b> <b>Sets.</b> </p...|$|R
40|$|Most {{previous}} work on multiple models {{has been done}} on a few domains. We present a comparsion of three ways of learning multiple models on 29 <b>data</b> <b>sets</b> from the UCI repository. The methods are bagging, k-fold partition learning and stochastic search. By using 29 <b>data</b> <b>sets</b> of various kinds [...] artificial <b>data</b> <b>sets,</b> artificial <b>data</b> <b>sets</b> with noise, molecular-biology and real-world noisy <b>data</b> <b>sets</b> [...] we are able to draw robust experimental conclusions about the kinds of <b>data</b> <b>sets</b> for which each learning method works best. We also compare four evidence combination methods (Uniform Voting, Bayesian Combination, Distribution Summation and Likelihood Combination) and characterize the kinds of <b>data</b> <b>sets</b> for which each method works best...|$|R
30|$|In this subsection, we {{describe}} the <b>data</b> <b>sets</b> that we use for our benchmarking. We use <b>data</b> <b>sets</b> {{from a variety of}} different mathematical and scientific areas and applications. In each case, when possible, we use <b>data</b> <b>sets</b> that have already been studied using PH. Our list of <b>data</b> <b>sets</b> is far from complete; we view this list as an initial step towards building a comprehensive collection of benchmarking <b>data</b> <b>sets</b> for PH.|$|R
25|$|Volume {{rendering}} is {{a technique}} used to display a 2D projection of a 3D discretely sampled <b>data</b> <b>set.</b> A typical 3D <b>data</b> <b>set</b> {{is a group of}} 2D slice images acquired by a CT or MRI scanner.|$|E
25|$|A {{regular user}} deletes a <b>data</b> <b>set</b> from the {{database}} using the API.|$|E
25|$|An {{administrator}} user edits {{an existing}} <b>data</b> <b>set</b> using the Firefox browser.|$|E
5000|$|In many disciplines, {{two-dimensional}} <b>data</b> <b>sets</b> {{are also}} called panel data. While, strictly speaking, two- and higher- dimensional <b>data</b> <b>sets</b> are [...] "multi-dimensional," [...] the term [...] "multidimensional" [...] {{tends to be}} applied only to <b>data</b> <b>sets</b> with three or more dimensions. For example, some forecast <b>data</b> <b>sets</b> provide forecasts for multiple target periods, conducted by multiple forecasters, and made at multiple horizons. The three dimensions provide more information than can be gleaned from two dimensional panel <b>data</b> <b>sets.</b>|$|R
40|$|Land {{evapotranspiration}} (ET) {{estimates are}} available from several global <b>data</b> <b>sets.</b> Here, Monthly Global Land et Synthesis Products, Merged from These Individual <b>Data</b> <b>Sets</b> over the Time Periods 1989 - 1995 (7 Yr) and 1989 - 2005 (17 Yr), Are Presented. the Merged Synthesis Products over the Shorter Period Are Based on A Total of 40 Distinct <b>Data</b> <b>Sets</b> while Those over the Longer Period Are Based on A Total of 14 <b>Data</b> <b>Sets.</b> in the Individual <b>Data</b> <b>Sets,</b> et Is Derived from Satellite And/or in Situ Observations (Diagnostic <b>Data</b> <b>Sets)</b> or Calculated Via Land-surface Models (LSMs) Driven with Observations-based Forcing or Output from Atmospheric Reanalyses. Statistics for Four Merged Synthesis Products Are Provided, One Including All <b>Data</b> <b>Sets</b> and Three Including only <b>Data</b> <b>Sets</b> from One Category Each (Diagnostic, LSMs, and Reanalyses). the Multi-annual Variations of et in the Merged Synthesis Products Display Realistic Responses. They Are Also Consistent with Previous Findings of A Global Increase in et between 1989 and 1997 (0. 13 Mm y...|$|R
40|$|Results {{using this}} {{approach}} {{will be presented}} on simulated <b>data</b> <b>sets</b> and on public <b>data</b> <b>sets.</b> |$|R
25|$|A {{regular user}} adds a new <b>data</b> <b>set</b> to the {{database}} using the native tool.|$|E
25|$|Summarization – {{providing}} a more compact {{representation of the}} <b>data</b> <b>set,</b> including visualization and report generation.|$|E
25|$|Further {{examining}} the <b>data</b> <b>set</b> in secondary analyses, to suggest new hypotheses for future study.|$|E
5000|$|For {{partitioned}} <b>data</b> <b>sets,</b> IEBCOMPR {{considers the}} <b>data</b> <b>sets</b> equal if {{the following conditions}} are met: ...|$|R
40|$|The {{missing values}} are not {{uncommon}} in real <b>data</b> <b>sets.</b> The algorithms and methods used for the data analysis of complete <b>data</b> <b>sets</b> cannot always be applied to missing value data. In order to use the existing methods for complete data, the missing value <b>data</b> <b>sets</b> are preprocessed. The other {{solution to this problem}} is creation of new algorithms dedicated to missing value <b>data</b> <b>sets...</b>|$|R
40|$|The {{increasing}} size of <b>data</b> <b>sets</b> has necessitated {{advancement in}} exploratory techniques. Methods that are practical for moderate to small <b>data</b> <b>sets</b> become infeasible {{when applied to}} massive <b>data</b> <b>sets.</b> Advanced techniques such as binned kernel density estimation, tours, and mode-based projection pursuit will be explored. Mean-centered binning will be introduced as an improved method for binned density estimation. The density grand tour will be demonstrated {{as a means of}} exploring massive high-dimensional <b>data</b> <b>sets.</b> Projection pursuit by clustering components will be described as a means to find interesting lower-dimensional subspaces of <b>data</b> <b>sets...</b>|$|R
25|$|The Rand {{index is}} used to test whether two or more {{classification}} systems agree on a <b>data</b> <b>set.</b>|$|E
25|$|A bucket sort {{works best}} when the {{elements}} of the <b>data</b> <b>set</b> are evenly distributed across all buckets.|$|E
25|$|The {{modified}} Thompson Tau test {{is used to}} {{find one}} outlier at a time (largest value of δ is removed if it is an outlier). Meaning, if a data point {{is found to be}} an outlier, it is removed from the <b>data</b> <b>set</b> and the test is applied again with a new average and rejection region. This process is continued until no outliers remain in a <b>data</b> <b>set.</b>|$|E
5000|$|When {{comparing}} sequential <b>data</b> <b>sets,</b> IEBCOMPR {{considers the}} <b>data</b> <b>sets</b> equal if {{the following conditions}} are met: ...|$|R
30|$|In this section, {{we try to}} {{investigate}} the effectiveness of different <b>data</b> <b>sets</b> in various domains {{on the performance of}} discussing similarity measurement. We consider six different <b>data</b> <b>sets</b> from different application domains including Webkb, Reuters 8, Reuters 52, News, dblp and cstr <b>data</b> <b>sets.</b> Table 11 shows the results of evaluating all classification methods and Table 12 presents the results of clustering methods. We use various performance evaluations for both classification and clustering. For each performance metric, we specify a row which represents the number of <b>data</b> <b>sets</b> where the given technique is in group A and also the average performance across all six <b>data</b> <b>sets.</b> Based on Tables 11 and 12 regardless of learners and <b>data</b> <b>sets,</b> ISC similarity and cosine similarity measures are always in group A. On the other hand, the Gaussian-based similarity measurement is in group B for some <b>data</b> <b>sets</b> while we use classification learners.|$|R
40|$|<b>Data</b> <b>sets</b> {{involving}} linear ordered sequences are {{a recurring}} theme in bioinformatics. Dynamic query tools that support exploration of these <b>data</b> <b>sets</b> can be useful for identifying patterns of interest. This paper describes the use of one such tool – TimeSearcher- to interactively explore linear sequence <b>data</b> <b>sets</b> taken from two bioinformatics problems. Microarray time course <b>data</b> <b>sets</b> involve expression levels for large numbers of genes over multiple time points. TimeSearcher {{can be used to}} interactively search these <b>data</b> <b>sets</b> for genes with expression profiles of interest. The occurrence frequencies of short sequences of DNA in aligned exons can be used to identify sequences that {{play a role in the}} pre-mRNA splicing. TimeSearcher can be used to search these <b>data</b> <b>sets</b> for candidate splicing signals. 1...|$|R
25|$|The {{following}} <b>data</b> <b>set</b> gives average {{heights and}} weights for American women aged 30–39 (source: The World Almanac and Book of Facts, 1975).|$|E
25|$|A {{method for}} quality control in higher-dimensional space {{is to use}} {{probability}} binning with bins fit to the whole <b>data</b> <b>set</b> pooled together.|$|E
25|$|Encode the {{persistent}} homology of a <b>data</b> <b>set</b> {{in the form}} of a parameterized version of a Betti number, which is called a barcode.|$|E
40|$|In the {{developed}} countries, especially {{over the last}} decade, {{there has been an}} explosive growth in the capability to generate, collect and use very large <b>data</b> <b>sets.</b> The objects of these <b>data</b> <b>sets</b> could be simultaneously described by quantitative and qualitative attributes. At present, algorithms able to process either very large <b>data</b> <b>sets</b> (in metric spaces) or mixed (qualitative and quantitative) incomplete <b>data</b> (missing value) <b>sets</b> have been developed, but not for very large mixed incomplete <b>data</b> <b>sets.</b> In this paper we introduce a new clustering method named GLC+ to process very large mixed incomplete <b>data</b> <b>sets</b> in order to obtain a partition in connected sets...|$|R
40|$|Abstract- Cloud {{computing}} provides massive computation {{power and}} storage capacity which enable users to deploy computation and data-intensive applications without infrastructure investment. Along {{the processing of}} such applications, a large volume of intermediate <b>data</b> <b>sets</b> will be generated, and often stored to save the cost of re computing them. However, preserving the privacy of intermediate <b>data</b> <b>sets</b> becomes a challenging problem because adversaries may recover privacy-sensitive information by analyzing multiple intermediate <b>data</b> <b>sets.</b> Encrypting ALL <b>data</b> <b>sets</b> in cloud is widely adopted in existing approaches to address this challenge. But we argue that encrypting all intermediate <b>data</b> <b>sets</b> are neither efficient nor cost-effective because it is very time consuming and costly for data-intensive applications to en/decrypt <b>data</b> <b>sets</b> frequently while performing any operation on them. In this paper, we propose a novel upper bound privacy leakage constraint-based approach to identify which intermediate <b>data</b> <b>sets</b> need to be encrypted and which do not, so that privacy-preserving cost can be saved while the privacy requirements of data holders can still be satisfied. Evaluation results demonstrate that the privacy-preserving cost of intermediate <b>data</b> <b>sets</b> can be significantly reduced with our approach over existing ones where all <b>data</b> <b>sets</b> are encrypted. Index Terms- Cloud computing, data storage privacy, privac...|$|R
30|$|Our {{proposed}} We/Sp/Co-MIML {{solutions are}} categorized as multi-instance multi-label learning methods, thus {{the effectiveness of}} these solutions should be first tested on MIML benchmark <b>data</b> <b>sets.</b> The detailed characteristics of the six benchmark <b>data</b> <b>sets</b> are summarized in Table  2, all <b>data</b> <b>sets</b> are as same as [11].|$|R
25|$|Volume {{rendering}} is {{a technique}} used to display a 2D projection of a 3D discretely sampled <b>data</b> <b>set.</b> A typical 3D <b>data</b> <b>set</b> {{is a group of}} 2D slice images acquired by a CT or MRI scanner. Usually these are acquired in a regular pattern (e.g., one slice every millimeter) and usually have a regular number of image pixels in a regular pattern. This {{is an example of a}} regular volumetric grid, with each volume element, or voxel represented by a single value that is obtained by sampling the immediate area surrounding the voxel.|$|E
25|$|Before {{data mining}} {{algorithms}} can be used, a target <b>data</b> <b>set</b> must be assembled. As data mining can only uncover patterns actually {{present in the}} data, the target <b>data</b> <b>set</b> must {{be large enough to}} contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.|$|E
25|$|Focusing {{the light}} beam {{to a point}} {{on the surface of}} the sample under test, and {{recombining}} the reflected light with the reference will yield an interferogram with sample information corresponding to a single A-scan (Z axis only). Scanning of the sample can be accomplished by either scanning the light on the sample, or by moving the sample under test. A linear scan will yield a two-dimensional <b>data</b> <b>set</b> corresponding to a cross-sectional image (X-Z axes scan), whereas an area scan achieves a three-dimensional <b>data</b> <b>set</b> corresponding to a volumetric image (X-Y-Z axes scan), also called full-field OCT.|$|E
40|$|<b>Data</b> <b>sets</b> {{that are}} used for {{answering}} a single query only once (or just a few times) before they are replaced by new <b>data</b> <b>sets</b> appear frequently in practical applications. The cost of buiding indexes to accelerate query processing would not be repaid for such <b>data</b> <b>sets.</b> We consider {{an extension of the}} popular (K) Nearest-Neighbor Query, called the (K) Group Nearest Neighbor Query (GNNQ). This query discovers the (K) nearest neighbor(s) to a group of query points (considering the sum of distances to {{all the members of the}} query group) and has been studied during recent years, considering <b>data</b> <b>sets</b> indexed by efficient spatial data structures. We study (K) GNNQs, considering non-indexed RAM-resident <b>data</b> <b>sets</b> and present an existing algorithm adapted to such <b>data</b> <b>sets</b> and two Plane-Sweep algorithms, that apply optimizations emerging from the geometric properties of the problem. By extensive experimentation, using real and synthetic <b>data</b> <b>sets,</b> we highlight the most efficient algorithm...|$|R
40|$|Statistics tend {{to become}} {{interesting}} to non-methodologists when taught in a research context that is relevant to them. Real <b>data</b> <b>sets</b> supplemented by sufficient background information can provide just such a context. Despite this, many textbook authors and instructors of applied statistics rely on artificial <b>data</b> <b>sets</b> to illustrate statistical techniques. In this paper, {{it is argued that}} artificial <b>data</b> <b>sets</b> should be eliminated from the curriculum and that they should be replaced with real <b>data</b> <b>sets.</b> Towards this end, a rationale for using real <b>data</b> <b>sets</b> and the characteristics that make <b>data</b> <b>sets</b> particularly good for instructional use are described. The difficulties encountered when using real data and strategies for compensating for these drawbacks are also discussed. Two authentic <b>data</b> <b>sets</b> and an annotated bibliography of dozens of primary and secondary data sources are included. (Author/PK) Reproductions supplied by EDRS are the best that can be made from the original document...|$|R
40|$|It {{has been}} claimed that {{blending}} {{processes such as}} trade and exchange have always been {{more important in the}} evolution of cultural similarities and differences among human populations than the branching process of population fissioning. In this paper, we report the results of a novel comparative study designed to shed light on this claim. We fitted the bifurcating tree model that biologists use to represent the relationships of species to 21 biological <b>data</b> <b>sets</b> that have been used to reconstruct the relationships of species and/or higher level taxa and to 21 cultural <b>data</b> <b>sets.</b> We then compared the average fit between the biological <b>data</b> <b>sets</b> and the model with the average fit between the cultural <b>data</b> <b>sets</b> and the model. Given that the biological <b>data</b> <b>sets</b> can be confidently assumed to have been structured by speciation, which is a branching process, our assumption was that, if cultural evolution is dominated by blending processes, the fit between the bifurcating tree model and the cultural <b>data</b> <b>sets</b> should be significantly worse than the fit between the bifurcating tree model and the biological <b>data</b> <b>sets.</b> Conversely, if cultural evolution is dominated by branching processes, the fit between the bifurcating tree model and the cultural <b>data</b> <b>sets</b> should be no worse than the fit between the bifurcating tree model and the biological <b>data</b> <b>sets.</b> We found that the average fit between the cultural <b>data</b> <b>sets</b> and the bifurcating tree model was not significantly different from the fit between the biological <b>data</b> <b>sets</b> and the bifurcating tree model. This indicates that the cultural <b>data</b> <b>sets</b> are not less tree-like than are the biological <b>data</b> <b>sets.</b> As such, our analysis does not support the suggestion that blending processes have always been more important than branching processes in cultural evolution. We conclude from this that, rather than deciding how cultural evolution has proceeded a priori, researchers need to ascertain which model or combination of models is relevant in a particular case and why...|$|R
