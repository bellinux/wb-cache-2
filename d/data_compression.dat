6050|900|Public
5|$|The {{preferred}} {{data format}} for files {{submitted to the}} SRA is the BAM format, which is capable of storing both aligned and unaligned reads. Internally the SRA relies on the NCBI SRA Toolkit, used at all three INSDC member databases, to provide flexible <b>data</b> <b>compression,</b> API access and conversion to other formats such as FASTQ.|$|E
5|$|Combinatorial {{game theory}} {{provides}} alternative reals as well, with infinite Blue-Red Hackenbush as one particularly relevant example. In 1974, Elwyn Berlekamp described a correspondence between Hackenbush strings and binary expansions of real numbers, {{motivated by the}} idea of <b>data</b> <b>compression.</b> For example, the value of the Hackenbush string LRRLRLRL... is 0.0101012...=. However, the value of LRLLL... (corresponding to 0.111...2) is infinitesimally less than 1. The difference between the two is the surreal number , where ω is the first infinite ordinal; the relevant game is LRRRR... or 0.000...2. follows directly from Berlekamp's Rule.|$|E
5|$|In September 2008, Intel {{announced}} the X25-M SATA SSD with a reported WA {{as low as}} 1.1. In April 2009, SandForce {{announced the}} SF-1000 SSD Processor family with a reported WA of 0.5 which appears to come from some form of <b>data</b> <b>compression.</b> Before this announcement, a write amplification of 1.0 was considered the lowest that could be attained with an SSD. Currently, only SandForce employs compression in its SSD controller.|$|E
40|$|This paper proposes {{an image}} hiding method which first {{compresses}} a secret image by base switching transformation (BST) and then ranks the <b>compression</b> <b>data</b> {{according to their}} significance. Meanwhile, the cover image {{is transformed into a}} frequency domain image by discrete wavelet transformation. After that, the dominant <b>compression</b> <b>data</b> are hidden in the lower frequency sub-bands and the trivial <b>compression</b> <b>data</b> in the higher frequency sub-bands of the frequency domain image. When the dominant <b>compression</b> <b>data</b> are damaged, the hamming code method is used to correct the damaged data. If the trivial <b>compression</b> <b>data</b> are changed, only the secret image data relating to the damaged data are affected. This proposed method also can yield a high hiding capacity of 2. 25 bits/pixels...|$|R
50|$|You {{can check}} a server {{to see if}} it is sending out {{compressed}} <b>data,</b> and <b>compression</b> compatibility of your browser for example here.|$|R
30|$|PDCP [36]: The PDCP layer {{provides}} several {{services such}} as transfer of user and control plane <b>data,</b> header <b>compression,</b> re-transmissions of SDUs lost during handover, ciphering, detection of duplicate data, and in-sequence delivery.|$|R
5|$|Once Chrono Trigger was completed, {{the team}} resumed discussions for Final Fantasy VII in 1995. The team {{reviewed}} {{the option of}} continuing the 2D strategy, {{which would have been}} the safe and immediate path just prior to the imminent industry shift toward the future of 3D gaming, and the radical new development models that these 3D consoles would require. The team decided to take the riskier option and make a 3D game on new generation hardware, but had yet to choose between the cartridge-based Nintendo 64 or the CD-ROM based PlayStation from Sony Computer Entertainment. The team also considered Sega's new Saturn console and the Microsoft Windows personal computer (PC). Their decision was influenced by two things: a tech demo based on Final Fantasy VI created using the new Softimage 3D software that impressed both staff and external developers, and the escalating price of cartridge-based games, which was limiting Square's audience. Tests were made for a Nintendo 64 version of the game, which would use the planned 64DD peripheral, {{even in the face of}} a lack of 64DD development kits and the prototype device's changing hardware specifications. This version was discarded during early testing, as the 2000 polygons needed to render the Behemoth monster put too much of a strain on the Nintendo 64 hardware, causing a low frame rate. It was later estimated that it would have required thirty 64DD discs to run Final Fantasy VII properly with the <b>data</b> <b>compression</b> methods of the day. Faced with both technical and economic issues on Nintendo's current hardware, and favorably impressed by the increased storage capacity of CD-ROM when compared to the Nintendo 64 cartridge, Square shifted development of Final Fantasy VII in addition to all other planned game projects, onto Sony's PlayStation.|$|E
25|$|For example, {{physicists}} {{are motivated}} to create experiments leading to observations obeying previously unpublished physical laws permitting better <b>data</b> <b>compression.</b> Likewise, composers receive intrinsic reward for creating non-arbitrary melodies with unexpected but regular harmonies that permit wow-effects through <b>data</b> <b>compression</b> improvements.|$|E
25|$|Some {{implementations}} do {{support such}} <b>data</b> <b>compression</b> within dynamic sparse tries and allow insertions and deletions in compressed tries. However, this usually {{has a significant}} cost when compressed segments need to be split or merged. Some tradeoff {{has to be made}} between <b>data</b> <b>compression</b> and update speed. A typical strategy is to limit the range of global lookups for comparing the common branches in the sparse trie.|$|E
50|$|By its nature, <b>data</b> without lossy <b>compression,</b> {{such as a}} PNG image, {{cannot be}} {{subjected}} to error level analysis. Consequently, since editing could have been performed on <b>data</b> without lossy <b>compression</b> with lossy compression applied uniformly to the edited, composite data, {{the presence of a}} uniform level of compression artifacts does not rule out editing of the data.|$|R
5000|$|U3D - an ECMA {{standard}} open {{format for}} 3D <b>data</b> that supports <b>compression.</b>|$|R
40|$|Increase {{in design}} {{complexity}} and fabrication technology results in high test data volume. As test size increases memory capacity also increases, which becomes the major difficulty in testing System-on-Chip (SoC). To reduce the test <b>data</b> volume, several <b>compression</b> techniques have been proposed. Code based schemes is one among those compression techniques. Run length coding {{is one of}} the most popular coding methodology in code based compression. Run length codes like Golomb code, Frequency directed run Length Code (FDR code), Extended FDR, Modified FDR, Shifted Alternate FDR and OLEL coding compress the test <b>data</b> and the <b>compression</b> ratio increases drastically. For further reduction of test <b>data,</b> double <b>compression</b> technique is proposed using Huffman code. Compression ratio using Double compression technique is presented and compared with the compression ratio obtained by other Run length codes...|$|R
25|$|The Lempel-Ziv {{algorithm}} {{that has}} become an international standard for <b>data</b> <b>compression,</b> and an IEEE Milestone.|$|E
25|$|The LZMA {{lossless}} <b>data</b> <b>compression</b> algorithm combines Markov chains with Lempel-Ziv compression {{to achieve}} very high compression ratios.|$|E
25|$|Native <b>data</b> <b>compression</b> and deduplication, {{although}} {{the latter is}} largely handled in RAM and is memory hungry.|$|E
40|$|This paper {{proposes a}} <b>data</b> pruning-based <b>compression</b> scheme {{to improve the}} {{rate-distortion}} relation of compressed images and video sequences. The original frames are pruned to a smaller size before compression. After decoding, they are interpolated to their original size by an edge-directed interpolation. The data pruning is optimized to obtain the minimal distortion in the interpolation phase. Further-more, a novel high order interpolation is proposed to adapt the inter-polation to many edge directions. This high order ſltering uses extra surrounding pixels and achieves more robust edge-directed image interpolation. Simulation results are shown for both image interpo-lation and coding applications. Index Terms — interpolation, <b>data</b> pruning, <b>compression,</b> spa-tial ſltering, edge-directed interpolation, video coding...|$|R
50|$|Error Level Analysis is the {{analysis}} of compression artifacts in digital <b>data</b> with lossy <b>compression</b> such as JPEG.|$|R
40|$|<b>Compression</b> <b>data</b> to 45 kbar {{have been}} {{obtained}} for hexagonal selenium by static methods in a piston-cylinder apparatus. Cylindrical samples (1 cm dia. and 1 xB 7; 2 cm length) of polycrystalline hexagonal selenium, with a bulk density close to the X-ray density, were prepared by pressing the samples at 7 kbar and 170 xB 0;C. The <b>compression</b> <b>data</b> are described by a third degree polynomial 13; 13...|$|R
25|$|A {{structured}} {{storage system}} to bundle these elements and any associated content {{into a single}} file, with <b>data</b> <b>compression</b> where appropriate.|$|E
25|$|A {{theoretical}} {{lower bound}} for the encoding bit rate for lossless <b>data</b> <b>compression</b> is the source information rate, {{also known as the}} entropy rate.|$|E
25|$|PNG uses DEFLATE, a non-patented {{lossless}} <b>data</b> <b>compression</b> algorithm {{that uses}} a combination of LZ77 and Huffman coding. Permissively-licensed DEFLATE implementations, such as zlib, are widely available.|$|E
30|$|In {{addition}} to image <b>data,</b> low-memory <b>compression</b> {{methods have been}} applied to body signal monitoring IoT devices such as two-dimensional (2 D) electrocardiogram (ECG) recorders using high temporal correlations between ECG signals by converting 1 D ECG signals into 2 D ECG signals, thus reducing amounts of data transmitted [11].|$|R
5000|$|AIFF {{supports}} only uncompressed PCM data. AIFF-C {{also supports}} compression audio formats, {{that can be}} specified in the [...] "COMM" [...] chunk. The compression type is [...] "NONE" [...] for PCM audio <b>data.</b> The <b>compression</b> type {{is accompanied by a}} printable name. Common compression types and names include, but are not limited to: ...|$|R
40|$|Our {{ability to}} {{interpret}} seismic observations including the seismic discontinuities and the density and velocity profiles in the earth's interior is critically {{dependent on the}} accuracy of pressure measurements up to 364 GPa at high temperature. Pressure scales based on the reduced shock-wave equations of state alone may predict pressure variations up to 7 % in the megabar pressure range at room temperature and even higher percentage at high temperature, leading to large uncertainties in understanding {{the nature of the}} seismic discontinuities and chemical composition of the earth's interior. Here, we report <b>compression</b> <b>data</b> of gold (Au), platinum (Pt), the NaCl-B 2 phase, and solid neon (Ne) at 300 K and high temperatures up to megabar pressures. Combined with existing experimental <b>data,</b> the <b>compression</b> <b>data</b> were used to establish internally consistent thermal equations of state of Au, Pt, NaCl-B 2, and solid Ne. The internally consistent pressure scales provide a tractable, accurate baseline for comparing high pressure–temperature experimental data with theoretical calculations and the seismic observations, thereby advancing our understanding fundamental high-pressure phenomena and the chemistry and physics of the earth's interior...|$|R
25|$|Generally, an {{approximation}} to DWT is {{used for}} <b>data</b> <b>compression</b> if a signal is already sampled, and the CWT for signal analysis. Thus, DWT approximation is commonly used in engineering and computer science, and the CWT in scientific research.|$|E
25|$|Opera Mini uses cloud {{acceleration}} and <b>data</b> <b>compression</b> technology. Opera Mini servers {{act as a}} proxy which compresses and renders the data of web pages before sending it to users. This process helps to load web content faster.|$|E
25|$|Error-correcting codes (channel coding): While <b>data</b> <b>compression</b> removes as much {{redundancy}} as possible, {{an error}} correcting code adds just {{the right kind of}} redundancy (i.e., error correction) needed to transmit the data efficiently and faithfully across a noisy channel.|$|E
40|$|Active Grids are {{a form of}} grid {{infrastructure}} {{where the}} grid network is active and programmable. These grids directly support applications with value added services such as <b>data</b> migration, <b>compression,</b> adaptation and monitoring. Services such as these are particularly important for eResearch applications which {{by their very nature}} are performance critical and data intensive...|$|R
40|$|We {{develop an}} {{approximation}} theory in Hilbert spaces that generalizes the classical theory of approximation by entire functions of exponential type. The results advance harmonic analysis on manifolds and graphs, thus facilitating <b>data</b> representation, <b>compression,</b> denoising and visualization. These tasks are {{of great importance}} to machine learning, complex data analysis and computer vision. Comment: Submitte...|$|R
50|$|Some DVD players, line doublers, and {{personal}} video recorders {{are designed to}} detect and remove 2:3 pulldown from telecined video sources, thereby reconstructing the original 24 frame/s film frames. This technique is known as “reverse” or “inverse” telecine. Benefits of reverse telecine include high-quality non-interlaced display on compatible display devices {{and the elimination of}} redundant <b>data</b> for <b>compression</b> purposes.|$|R
25|$|Office Open XML {{is based}} on XML and uses the ZIP file container. According to Microsoft, {{documents}} created in this format are up to 75% smaller than the same documents saved with previous Microsoft Office file formats, owing to the ZIP <b>data</b> <b>compression.</b>|$|E
25|$|In June 1984, {{an article}} by Welch was {{published}} in the IEEE magazine which publicly described the LZW technique for the first time. LZW became a popular <b>data</b> <b>compression</b> technique and, when the patent was granted, Unisys entered into licensing agreements with over a hundred companies.|$|E
25|$|As an example, the goodput or data {{transfer}} rate of a V.92 voiceband modem {{is affected by the}} modem physical layer and data link layer protocols. It is sometimes higher than the physical layer data rate due to V.44 <b>data</b> <b>compression,</b> and sometimes lower due to bit-errors and automatic repeat request retransmissions.|$|E
50|$|Cloudian HyperStore Connect for Files (HCF) is {{a global}} file access product with native support for SMB, NFS, and FTP. HCF {{includes}} multi-site file locking, <b>data</b> deduplication, and <b>compression.</b>|$|R
40|$|This paper {{presents}} an eclectic approach for compressing weighted finite-state automata and transducers, with minimal impact on performance. The approach is eclectic {{in the sense}} that various complementary methods have been employed: row-indexed storage of sparse matrices, dictionary compression, bit manipulation, and lossless omission of <b>data.</b> The <b>compression</b> rate is over 83 % with respect to the current Bell Labs FSM library...|$|R
40|$|Abstract. Location data {{generated}} from GPS equipped moving objects are typically collected as streams of spatiotemporal 〈x, y, t 〉 points that when put together form corresponding trajectories. Most existing studies focus on building ad-hoc querying, analysis, {{as well as}} data mining techniques on formed trajectories. As a prior step, trajectory construction is evidently necessary for mobility data processing and understanding, including tasks like trajectory <b>data</b> cleaning, <b>compression,</b> and segmentation so as to identify semantic trajectory episodes like stops (e. g. while sitting and standing) and moves (while jogging, walking, driving etc). However, semantic trajectory construction methods in the current literature are typically based on offline procedures, which is not sufficient for real life trajectory applications that rely on timely delivery of computed trajectories to serve real-time query answers. Filling this gap, our paper proposes a platform, namely SeTraStream, for online semantic trajectory construction. Our framework is capable of providing real-time trajectory <b>data</b> cleaning, <b>compression,</b> segmentation over streaming movement data. ...|$|R
