86|77|Public
5000|$|Thus, quantum {{evolution}} of a Gaussian, which is the complex <b>diffusion</b> <b>kernel</b> K,amounts to the time-evolved state, ...|$|E
40|$|This paper {{presents}} a survey {{as well as}} a systematic empirical comparison of seven graph kernels and two related similarity matrices (simply referred to as graph kernels), namely the exponential <b>diffusion</b> <b>kernel,</b> the Laplacian exponential <b>diffusion</b> <b>kernel,</b> the von Neumann <b>diffusion</b> <b>kernel,</b> the regularized Laplacian kernel, the commute-time kernel, the random-walk-with-restart similarity matrix, and finally, three graph kernels introduced in this paper: the regularized commute-time kernel, the Markov <b>diffusion</b> <b>kernel,</b> and the cross-entropy diffusion matrix. The kernel-on-a-graph approach is simple and intuitive. It is illustrated by applying the nine graph kernels to a collaborative-recommendation task and to a semisupervised classification task, both on several databases. The graph methods compute proximity measures between nodes that help study the structure of the graph. Our comparisons suggest that the regularized commute-time and the Markov diffusion kernels perform best, closely followed by the regularized Laplacian kernel. ...|$|E
40|$|This paper {{presents}} a survey {{as well as}} an empirical comparison and evaluation of seven kernels on graphs and two related similarity matrices, that we globally refer to as "kernels on graphs" for simplicity. They are the exponential <b>diffusion</b> <b>kernel,</b> the Laplacian exponential <b>diffusion</b> <b>kernel,</b> the von Neumann <b>diffusion</b> <b>kernel,</b> the regularized Laplacian kernel, the commute-time (or resistance-distance) kernel, the random-walk-with-restart similarity matrix, and finally, a kernel first introduced in this paper (the regularized commute-time kernel) and two kernels defined in some of our previous work and further investigated in this paper (the Markov <b>diffusion</b> <b>kernel</b> and the relative-entropy diffusion matrix). The kernel-on-graphs approach is simple and intuitive. It is illustrated by applying the nine kernels to a collaborative-recommendation task, viewed as a link prediction problem, and to a semisupervised classification task, both on several databases. The methods compute proximity measures between nodes that help study the structure of the graph. Our comparisons suggest that the regularized commute-time and the Markov diffusion kernels perform best on the investigated tasks, closely followed by the regularized Laplacian kernel. © 2012 Elsevier Ltd...|$|E
5000|$|... 2002. <b>Diffusion</b> <b>Kernels</b> on Graphs and Other Discrete Input Spaces.|$|R
40|$|Information <b>diffusion</b> <b>kernels</b> - {{similarity}} metrics in non-Euclidean information spaces - {{have been}} found to produce state of the art results for document classification. In this paper, we present a novel approach to global sentiment classification using these kernels. We carry out a large array of experiments addressing the well-known movie review data set of Pang and Lee, a de facto benchmark, comparing information <b>diffusion</b> <b>kernels</b> with a standard RBF kernel machine. Our results show that interpolation of unigram and bigram information is beneficiary for sentiment classification. Copyright 2007 ACM...|$|R
40|$|Abstract: In {{this paper}} we study Hilbert space embeddings of {{dynamical}} systems and embeddings generated via dynamical systems. This {{is achieved by}} following the behavioural framework invented by Willems, namely by comparing trajectories of states. As important special cases we recover the <b>diffusion</b> <b>kernels</b> of Kondor and Lafferty, generalised versions of directed graph kernels of Gärtner, novel kernels on matrices and new similarity measures on Markov Models...|$|R
40|$|Motivation: The <b>diffusion</b> <b>kernel</b> is {{a general}} method for {{computing}} pairwise distances among all nodes in a graph, based on the sum of weighted paths between each pair of nodes. This technique has been used successfully, in conjunction with kernel-based learning methods, to draw inferences from several types of biological networks. Results: We show that computing the <b>diffusion</b> <b>kernel</b> is equivalent to maximizing the von Neumann entropy, subject to a global constraint on {{the sum of the}} Euclidean distances between nodes. This global constraint allows for high variance in the pairwise distances. Accordingly, we propose an alternative, locally constrained <b>diffusion</b> <b>kernel,</b> and we demonstrate that the resulting kernel allows for more accurate support vector machine prediction of protein functional classifications from metabolic and proteinprotein interaction networks...|$|E
40|$|We {{cast the}} problem of source {{localization}} on graphs as the simultaneous problem of sparse recovery and <b>diffusion</b> <b>kernel</b> learning. An l 1 regularization term enforces the sparsity constraint while we recover the sources of diffusion from a single snapshot of the diffusion process. The <b>diffusion</b> <b>kernel</b> is estimated by assuming the process to be as generic as the standard heat diffusion. We show with synthetic data that we can concomitantly learn the <b>diffusion</b> <b>kernel</b> and the sources, given an estimated initialization. We validate our model with cholera mortality and atmospheric tracer diffusion data, showing also that {{the accuracy of the}} solution depends on the construction of the graph from the data points. Comment: 5 pages, 5 figures, published in "Image, Video, and Multidimensional Signal Processing Workshop (IVMSP), 2016 IEEE 12 th...|$|E
3000|$|The {{diffusion}} maps scheme utilizes numerically induced random {{walks to}} analyze data sets, by forming the <b>diffusion</b> <b>kernel</b> {{and the corresponding}} discrete eigenfunction {ψ [...]...|$|E
40|$|A novel {{approach}} for noninvasively tracing brain white matter fiber tracts is presented using diffusion tensor {{magnetic resonance imaging}} (DT-MRI) data. This technique is based on performing anisotropic diffusion simulations over a series of overlapping three dimensional <b>diffusion</b> <b>kernels</b> that cover {{only a small portion}} of the human brain volume and are geometrically centered upon selected starting voxels where a seed is placed. The simulations conducted over <b>diffusion</b> <b>kernels</b> are initiated from those starting voxels and are utilized to construct diffusion fronts. The fiber pathways are determined by evaluating the distance and orientation from fronts to their corresponding diffusion seed voxels. Synthetic and real DT-MRI data are employed to demonstrate the tracking scheme. It is shown that the synthetic tracts can be accurately replicated, while several major white matter fiber pathways in the human brain can be reproduced noninvasively as well. Since the diffusion simulation makes use of the entire diffusion tensor data, including both the magnitude and orientation information, the proposed approach enhances robustness and reliability in DT-MRI based fiber reconstruction...|$|R
40|$|Establishing {{correspondence}} between shapes {{is a fundamental}} problem in geometry processing, arising {{in a wide variety}} of applications. The problem is especially difficult in the setting of non-isometric deformations, as well as in the presence of topological noise and missing parts, mainly due to the limited capability to model such deformations axiomatically. Several recent works showed that invariance to complex shape transformations can be learned from examples. In this paper, we introduce an intrinsic convolutional neural network architecture based on anisotropic <b>diffusion</b> <b>kernels,</b> which we term Anisotropic Convolutional Neural Network (ACNN). In our construction, we generalize convolutions to non-Euclidean domains by constructing a set of oriented anisotropic <b>diffusion</b> <b>kernels,</b> creating in this way a local intrinsic polar representation of the data (`patch'), which is then correlated with a filter. Several cascades of such filters, linear, and non-linear operators are stacked to form a deep neural network whose parameters are learned by minimizing a task-specific cost. We use ACNNs to effectively learn intrinsic dense correspondences between deformable shapes in very challenging settings, achieving state-of-the-art results on some of the most difficult recent correspondence benchmarks...|$|R
40|$|This paper investigates {{support vector machine}} (SVM) with a {{discrete}} kernel, named electric network kernel, defined on the vertex set of an undirected graph. Emphasis is laid on mathematical analysis of its theoretical properties {{with the aid of}} electric network theory. SVM with this kernel admits physical interpretations in terms of resistive electric networks; in particular, the SVM decision function corresponds to an electric potential. Preliminary computational results indicate reasonable promise of the proposed kernel in comparison with the Hamming and <b>diffusion</b> <b>kernels.</b> ...|$|R
40|$|Recently, we {{introduced}} {{a method to}} recover the controlling param-eters of linear systems using diffusion kernels. In this paper, we apply our approach {{to the problem of}} source localization in a rever-berant room using measurements from a single microphone. Prior recordings of signals from various known locations in the room are required for training and calibration. The proposed algorithm re-lies on a computation of a <b>diffusion</b> <b>kernel</b> with a specially-tailored distance measure. Experimental results in a real reverberant envi-ronment demonstrate accurate recovery of the source location. Index Terms — Source localization, acoustic localization, dif-fusion geometry, <b>diffusion</b> <b>kernel,</b> manifold learnin...|$|E
40|$|Abstract Background Machine-learning tools {{have gained}} {{considerable}} attention {{during the last}} few years for analyzing biological networks for protein function prediction. Kernel methods are suitable for learning from graph-based data such as biological networks, as they only require the abstraction of the similarities between objects into the kernel matrix. One key issue in kernel methods is the selection of a good kernel function. Diffusion kernels, the discretization of the familiar Gaussian kernel of Euclidean space, are commonly used for graph-based data. Results In this paper, we address the issue of learning an optimal <b>diffusion</b> <b>kernel,</b> {{in the form of a}} convex combination of a set of pre-specified kernels constructed from biological networks, for protein function prediction. Most prior work on this kernel learning task focus on variants of the loss function based on Support Vector Machines (SVM). Their extensions to other loss functions such as the one based on Kullback-Leibler (KL) divergence, which is more suitable for mining biological networks, lead to expensive optimization problems. By exploiting the special structure of the <b>diffusion</b> <b>kernel,</b> we show that this KL divergence based kernel learning problem can be formulated as a simple optimization problem, which can then be solved efficiently. It is further extended to the multi-task case where we predict multiple functions of a protein simultaneously. We evaluate the efficiency and effectiveness of the proposed algorithms using two benchmark data sets. Conclusion Results show that the performance of linearly combined <b>diffusion</b> <b>kernel</b> is better than every single candidate <b>diffusion</b> <b>kernel.</b> When the number of tasks is large, the algorithms based on multiple tasks are favored due to their competitive recognition performance and small computational costs. </p...|$|E
40|$|We {{study the}} {{diffusion}} equation in two-dimensional quantum gravity, {{and show that}} the spectral dimension is two {{despite the fact that}} the intrinsic Hausdorff dimension of the ensemble of two-dimensional geometries is very different from two. We determine the scaling properties of the quantum gravity averaged <b>diffusion</b> <b>kernel.</b> ...|$|E
40|$|The {{application}} of kernel-based learning algorithms has, so far, largely {{been confined to}} realvalued data and a few special data types, such as strings. In this {{paper we propose a}} general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential <b>kernels</b> called <b>diffusion</b> <b>kernels,</b> which are based on the heat equation and can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space...|$|R
40|$|A {{new family}} of kernels for {{statistical}} learning is introduced that exploits the geometric structure of statistical models. Based on the heat equation on the Riemannian manifold {{defined by the}} Fisher information metric, information <b>diffusion</b> <b>kernels</b> generalize the Gaussian kernel of Euclidean space, and provide a natural way of combining generative statistical modeling with non-parametric discriminative learning. As a special case, the kernels give {{a new approach to}} applying kernel-based learning algorithms to discrete data. Bounds on covering numbers for the new kernels are proved using spectral theory in differential geometry, and experimental results are presented for real data sets...|$|R
40|$|We {{introduce}} intrinsic, nonlinearly invariant, parameterizations {{of empirical}} data, {{generated by a}} nonlinear transformation of independent variables. This is achieved through anisotropic <b>diffusion</b> <b>kernels</b> on observable data manifolds that approximate a Laplacian on the inaccessible independent variable domain. The key idea is a symmetrized second order approximation of the unknown distances in the independent variable domain, using the metric distortion induced by the Jacobian of the unknown mapping from variables to data. This distortion is estimated using local principal component analysis. We obtain the non linear independent components of stochastic processes and indicate other possible applications. I...|$|R
40|$|Proteins do {{not carry}} out their {{functions}} alone. Instead, they often act by participating in macromolecular complexes and play different functional roles depending on {{the other members of}} the complex. It is therefore interesting to identify cocomplex relationships. Although protein complexes can be identified in a high-throughput manner by experimental technologies such as affinity purification coupled with mass spectrometry (APMS), these large-scale datasets often suffer from high false positive and false negative rates. Here, we present a computational method that predicts co-complexed protein pair (CCPP) relationships using kernel methods from heterogeneous data sources. We show that a <b>diffusion</b> <b>kernel</b> based on random walks on the full network topology yields good performance in predicting CCPPs from protein interaction networks. In the setting of direct ranking, a <b>diffusion</b> <b>kernel</b> performs much better than the mutual clustering coefficient. In the setting of SVM classifiers, a <b>diffusion</b> <b>kernel</b> performs much better than a linear kernel. We also show that combination of complementary information improves the performance of our CCPP recognizer. A summation of three diffusion kernels based on two-hybrid, APMS, and genetic interaction networks and three sequence kernels achieves better performance than the sequence kernels or diffusion kernels alone. Inclusion of additional features achieves a still better ROC 50 of 0. 937. Assuming a negative-to-positive ratio of 600 : 1, the final classifier achieves 89. 3 % coverage at an estimated false discovery rate of 10 %. Finally, we applied our prediction method to two recently described APMS datasets. We find that our predicte...|$|E
40|$|Description This package {{implements}} several functions {{useful for}} computing similarities between GO terms and gene products {{based on their}} GO annotation. Moreover it allows for computing a GO enrichment analysis License GPL (> = 2) biocViews GO, Clustering, Software, Network R topics documented: calc. <b>diffusion.</b> <b>kernel</b> [...] . 2 calcICs [...] . 3 clusterEvaluation [...] 4 filterGO [...] . 6 getAncestors [...] 7 getChildren [...] . 7 getDisjCommAnc [...] 8 getGeneFeatures [...] 9 getGeneFeaturesPrototypes [...] . 1...|$|E
40|$|In this paper, we {{consider}} learning problems defined on graph-structured data. We propose an incremental supervised learning algorithm for network-based estimators using diffusion kernels. <b>Diffusion</b> <b>kernel</b> nodes are iteratively {{added in the}} training process. For each new node added, the kernel function center and the output connection weight are decided according to an empirical risk driven rule based on an extended chained version of the Nadaraja–Watson estimator. Then the diffusion parameters are determined by a genetic-like optimization technique...|$|E
40|$|PosterInternational audienceComputer {{tomography}} (CT) has wide {{application in}} medical imaging and reverse engineering. Due {{to the limited}} number of projections used in reconstructing the volume, the resulting 3 D data is typically noisy. Contouring such data, for surface extraction, yields surfaces with localised artifacts of complex topology. To avoid such artifacts, we propose a method for feature-preserving smoothing of CT data. The smoothing is based on anisotropic diffusion, with a diffusion tensor designed to smooth noise up to a given scale, while preserving features. We compute these <b>diffusion</b> <b>kernels</b> from the directional histograms of gradients around each voxel, using a fast GPU implementation...|$|R
40|$|Let (Pt) t≥ 0 and (Pt) t≥ 0 be two {{diffusion}} semigroups on Rd(d≥ 2) {{associated with}} uniformly elliptic operatorsL=∇·(A∇) and L=∇·(A∇) with measurable coefficientsA=(aij) and A=(ãij), respectively. The corresponding <b>diffusion</b> <b>kernels</b> are denoted bypt(x,y) and pt(x,y). We derive a pointwise estimate on pt(x,y) - pt(x,y) {{as well as}} anLp-operator norm bound, wherep∈[1,∞], forPt-Ptin terms of the localL 2 -distance betweenaijandãij. This implies in particular that pt(x,y) - pt(x,y) converges to zero uniformly in (x,y) ∈Rd×Rdand that theLp-operator norm ofPt-Ptconverges to zero uniformly inp∈[1,∈] whenaij-ãijgoes to zero in the localL 2 -norm for each 1 ≤i,j≤n. © 1998 Academic Press...|$|R
40|$|We {{present an}} {{algorithm}} based on convex optimization for constructing kernels for semi-supervised learning. The kernel matrices {{are derived from}} the spectral decomposition of graph Laplacians, and combine labeled and unlabeled data in a systematic fashion. Unlike previous work using <b>diffusion</b> <b>kernels</b> and Gaussian random field kernels, a nonparametric kernel approach is presented that incorporates order constraints during optimization. This results in flexible kernels and avoids the need to choose among different parametric forms. Our approach relies on a quadratically constrained quadratic program (QCQP), and is computationally feasible for large datasets. We evaluate the kernels on real datasets using support vector machines, with encouraging results. ...|$|R
40|$|Several models {{exist for}} {{diffusion}} of signals across biological, social, or engineered networks. However, the inverse problem of identifying {{the source of}} such propagated information appears more difficult even {{in the presence of}} multiple network snapshots, and especially for the single-snapshot case, given the many alternative, often similar, progression of diffusion that may lead to the same observed snapshots. Mathematically, this problem can be undertaken using a dif-fusion kernel that represents diffusion processes in a given network, but computing this kernel is computationally challenging in general. Here, we propose a path-based network <b>diffusion</b> <b>kernel</b> which considers edge-disjoint shortest paths among pairs of nodes in the network and can be computed efficiently for both homogeneous and heterogeneous continuous-time diffusion models. We use this network <b>diffusion</b> <b>kernel</b> to solve the inverse diffusion problem, which we term Network Infusion (NI), using both likelihood maximization and error minimization. The minimum error NI algorithm is based on an asymmetric Hamming premetric function and can balance between false positive and false negative error types. We apply this framework for both single-source and multi-source diffusion, for both single-snapshot and multi-snapshot observa...|$|E
40|$|Aims. We {{determine}} the alpha effect and turbulent magnetic diffusivity for mean magnetic fields with profiles of different length scales from simulations of isotropic turbulence. We then relate these results to nonlocal formulations in which alpha and the turbulent magnetic diffusivity correspond to integral kernels. Methods. We solve evolution equations for magnetic fields {{that give the}} response to imposed test fields. These test fields correspond to mean fields with various wavenumbers. Both an imposed fully helical steady flow consisting of a pattern of screw-like motions (Roberts flow) and time-dependent, statistically steady isotropic turbulence are considered. In the latter case the evolution equations are solved simultaneously with the momentum and continuity equations. The corresponding results for the electromotive force are used to calculate alpha and magnetic diffusivity tensors. Results. For both, the Roberts flow under the second-order correlation approximation and the isotropic turbulence alpha and turbulent magnetic diffusivity are greatest on large scales and these values diminish toward smaller scales. In both cases, the alpha effect and turbulent diffusion kernels are approximated by exponentials, corresponding to Lorentzian profiles in Fourier space. For isotropic turbulence, the turbulent <b>diffusion</b> <b>kernel</b> is half {{as wide as the}} alpha effect kernel. For the Roberts flow beyond the second-order correlation approximation, the turbulent <b>diffusion</b> <b>kernel</b> becomes negative on large scales...|$|E
40|$|Abstract. We {{present a}} new unified kernel {{regression}} framework on manifolds. Starting with a symmetric positive definite kernel, we for-mulate a new bivariate kernel regression framework that {{is related to}} heat <b>diffusion,</b> <b>kernel</b> smoothing and recently popular diffusion wavelets. Various properties and performance of the proposed kernel regression framework are demonstrated. The method is subsequently applied in in-vestigating the influence of age and gender on the human amygdala and hippocampus shapes. We detected a significant age effect on the posterior regions of hippocampi {{while there is no}} gender effect present. ...|$|E
40|$|Abstract. In this paper, {{we propose}} a highly robust point-matching method (Graph Transformation Matching- GTM) relying on finding the {{consensus}} graph emerging from putative matches. Such method is a twophased {{one in the}} sense that after finding the consensus graph it tries to complete it as much as possible. We successfully apply GTM to image registration in the context of finding mosaics from retinal images. Feature points are obtained after properly segmenting such images. In addition, we also introduce a novel topological descriptor for quantifying disease by characterizing the arterial/venular trees. Such descriptor relies on <b>diffusion</b> <b>kernels</b> on graphs. Our experiments have showed only statistical significance for the case of arterial trees, which is consistent with previous findings. ...|$|R
40|$|AbstractWe {{introduce}} intrinsic, non-linearly invariant, parameterizations {{of empirical}} data, {{generated by a}} non-linear transformation of independent variables. This is achieved through anisotropic <b>diffusion</b> <b>kernels</b> on observable data manifolds that approximate a Laplacian on the inaccessible independent variable domain. The key idea is a symmetrized second-order approximation of the unknown distances in the independent variable domain, using the metric distortion induced by the Jacobian of the unknown mapping from variables to data. This distortion is estimated using local principal component analysis. Thus, the non-linear independent component analysis problem is solved whenever the generation of the data enables the estimation of the Jacobian. In particular, we obtain the non-linear independent components of stochastic Itô processes and indicate other possible applications...|$|R
40|$|In many fields {{including}} economics, {{collection of}} time series such as stocks or energy prices {{are governed by}} a similar non-linear dynamical process. These time series are often measured hourly, thus, each day {{can be viewed as}} a high-dimensional data point. In this paper, we apply a spectral method, which based on anisotropic <b>diffusion</b> <b>kernels</b> to the model high dimensional electricity price data. We demonstrate the proposed method on price data that was collected from several zones. We show that even though the observed output spaces differ by local spatial influences and noise, the common global parameters that drive the underlying process are extracted. Modeling zonal electricity prices by anisotropic diffusion embedding...|$|R
40|$|This {{paper is}} {{motivated}} by {{the behavior of the}} heat <b>diffusion</b> <b>kernel</b> p(t) (x) on a general unimodular Lie group. Indeed. contrary to what happens in R(n), the P-t(x) on a general Lie group is behaving like t(-delta(t) / 2) for two possibly distinct integers delta(t), one for t tending to 0 and another for t tending to infinity, namely d and D. This forces us to consider a natural generalization of Lorentz spaces with different indices at ''zero'' and at ''infinity. '' (C) 1996 Academic Press, Inc...|$|E
40|$|The {{spectral}} {{dynamics of}} individual bacterial light-harvesting- 2 pigment-protein complexes {{have been studied}} at 1. 4 K. The data provided the spectral <b>diffusion</b> <b>kernel</b> of the optical transitions of the embedded B 800 bacteriochlorophyll a pigments. This kernel can be described by either a single Gaussian function or a superposition of Gaussian functions. Moreover, {{we found that the}} chromophores interact with two classes of TLSs that can be distinguished by their distance from the chromophore and are most likely located outside (class 1) and inside (class 2) the protein matrix...|$|E
40|$|Aims. To {{determine}} alpha {{effect and}} turbulent magnetic diffusivity for mean magnetic fields with profiles of different length scale from simulations of isotropic turbulence, and to relate these results to nonlocal formulations in which alpha and the turbulent magnetic diffusivity correspond to integral kernels. Methods. A set of evolution equations for magnetic fields is solved {{which gives the}} response to imposed test fields, that is, mean magnetic fields with various wavenumbers. Both an imposed fully helical steady flow consisting of a pattern of screw-like motions (Roberts flow) and time-dependent statistically steady isotropic turbulence are considered. In the latter case the aforementioned evolution equations are solved simultaneously with the momentum and continuity equations. The corresponding results for the electromotive force are used to calculate alpha and magnetic diffusivity tensors. Results. For both the Roberts flow under the second–order correlation approximation and isotropic turbulence alpha and turbulent magnetic diffusivity are largest at large scales and these values diminish toward smaller scales. In both cases the alpha effect and turbulent diffusion kernels are well approximated by exponentials, corresponding to Lorentzian profiles in Fourier space. For isotropic turbulence the turbulent <b>diffusion</b> <b>kernel</b> is half {{as wide as the}} alpha effect kernel. For the Roberts flow beyond the second–order correlation approximation the turbulent <b>diffusion</b> <b>kernel</b> becomes negative at large scales. Key words. Magnetohydrodynamics (MHD) – Turbulence 1...|$|E
40|$|We {{developed}} a new self-adjoint, consistent, and stable coupling strategy for nonlocal diffusion models, inspired by the quasinonlocal atomistic-to-continuum method for crystalline solids. The proposed coupling model is coercive {{with respect to the}} energy norms induced by the nonlocal <b>diffusion</b> <b>kernels</b> as well as the $L^ 2 $ norm, and it satisfies the maximum principle. A finite difference approximation is used to discretize the coupled system, which inherits the property from the continuous formulation. Furthermore, we design a numerical example which shows the discrepancy between the fully nonlocal and fully local diffusions, whereas the result of the coupled diffusion agrees with that of the fully nonlocal diffusion. Comment: 28 pages, 3 figures, ams. or...|$|R
40|$|We {{present a}} new {{technique}} for noninvasively tracing brain white matter fiber tracts using diffusion tensor magnetic resonance imaging (DT-MRI). This technique is based on performing diffusion simulations over a series of overlapping three dimensional <b>diffusion</b> <b>kernels</b> that cover {{only a small portion}} of the human brain volume and are geometrically centered upon selected starting voxels where a seed is placed. Synthetic and real DT-MRI data are employed to demonstrate the tracking scheme. It is shown that the synthetic tracts can be accurately replicated, while several major white matter fiber pathways in the human brain can be reproduced noninvasively as well. The primary advantages of the algorithm lie in the handling of fiber branching and crossing and its seamless adaptation to the platform established by new imaging techniques, such as high angular, q-space, or generalized diffusion tensor imaging...|$|R
40|$|Application of Kernels to Link Analysis The {{application}} of kernel methods to link analysis is explored. In particular, Kandola et al. ’s Neumann kernels are shown to subsume {{not only the}} co-citation and bibliographic coupling relatedness but also Kleinberg’s HITS importance. These popular measures of relatedness and importance correspond to the Neumann kernels at the extremes of their parameter range, and hence these kernels {{can be interpreted as}} defining a spectrum of link analysis measures intermediate between co-citation/bibliographic coupling and HITS. We also show that the kernels based on the graph Laplacian, including the regularized Laplacian and <b>diffusion</b> <b>kernels,</b> provide relatedness measures that overcome some limitations of co-citation relatedness. The property of these kernel-based link analysis measures is examined with a network of bibliographic citations. Practical issues in applying these methods to real data are discussed, and possible solutions are proposed...|$|R
