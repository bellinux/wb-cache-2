0|3073|Public
40|$|International audienceThis paper {{deals with}} the <b>impact</b> of fault <b>prediction</b> <b>techniques</b> on {{checkpointing}} strategies. We extend the classical first-order analysis of Young and Daly {{in the presence of}} a fault prediction system, characterized by its recall and its precision. In this framework, we provide optimal algorithms to decide whether and when to take predictions into account, and we derive the optimal value of the checkpointing period. These results allow us to analytically assess the key parameters that impact the performance of fault predictors at very large scale...|$|R
40|$|This paper {{deals with}} the <b>impact</b> of fault <b>prediction</b> <b>techniques</b> on {{checkpointing}} strategies. We extend the classical first-order analysis of Young and Daly {{in the presence of}} a fault prediction system, characterized by its recall and its precision. In this framework, we provide an optimal algorithm to decide when to take predictions into account, and we derive the optimal value of the checkpointing period. These results allow to analytically assess the key parameters that impact the performance of fault predictors at very large scale. Comment: Supported in part by ANR Rescue. Published in Journal of Parallel and Distributed Computing. arXiv admin note: text overlap with arXiv: 1207. 693...|$|R
40|$|Abstract—Refactorings are behavior-preserving {{source code}} transformations. While tool support exists for (semi) {{automatically}} identifying refactoring solutions, applying {{or not a}} recommended refactoring is usually up to the software developers, who have {{to assess the impact}} that the transformation will have on their system. Evaluating the pros (e. g., the bad smell removal) and cons (e. g., side effects of the change) of a refactoring is far from trivial. We present RIPE (Refactoring <b>Impact</b> <b>PrEdiction),</b> a <b>technique</b> that estimates the impact of refactoring operations on source code quality metrics. RIPE supports 12 refactoring operations and 11 metrics and it can be used together with any refactoring recommendation tool. RIPE was used to estimate the impact on 8, 103 metric values, for 504 refactorings from 15 open source systems. 38 % of the estimates are correct, whereas the median deviation of the estimates from the actual values is 5 % (with a 31 % average). Index Terms—Refactoring Impact, Code Quality I...|$|R
40|$|This paper {{deals with}} the <b>impact</b> of fault <b>prediction</b> <b>techniques</b> on {{checkpointing}} strategies. We extend the classical analysis of Young and Daly {{in the presence of}} a fault prediction system, which is characterized by its recall and its precision, and which provides either exact or window-based time predictions. We succeed in deriving the optimal value of the checkpointing period (thereby minimizing the waste of resource usage due to checkpoint overhead) in all scenarios. These results allow to analytically assess the key parameters that impact the performance of fault predictors at very large scale. In addition, the results of this analytical evaluation are nicely corroborated by a comprehensive set of simulations, thereby demonstrating the validity of the model and the accuracy of the results. ...|$|R
40|$|This paper {{deals with}} the <b>impact</b> of fault <b>prediction</b> <b>techniques</b> on {{checkpointing}} strategies. We suppose that the fault-prediction system provides prediction windows instead of exact predictions, which dramatically complicates {{the analysis of the}} checkpointing strategies. We propose a new approach based upon two periodic modes, a regular mode outside prediction windows, and a proactive mode inside prediction windows, whenever the size of these windows is large enough. We are able to compute the best period for any size of the prediction windows, thereby deriving the scheduling strategy that minimizes platform waste. In addition, the results of this analytical evaluation are nicely corroborated by a comprehensive set of simulations, which demonstrate the validity of the model and the accuracy of the approach. ...|$|R
40|$|International audienceThis paper {{deals with}} the <b>impact</b> of fault <b>prediction</b> <b>techniques</b> on {{checkpointing}} strategies. We consider fault-prediction systems that do not provide exact prediction dates, but instead time intervals during which faults are predicted to strike. These intervals dramatically complicate {{the analysis of the}} checkpointing strategies. We propose a new approach based upon two periodic modes, a regular mode outside prediction windows, and a proactive mode inside prediction windows, whenever the size of these windows is large enough. We are able to compute the best period for any size of the prediction windows, thereby deriving the scheduling strategy that minimizes platform waste. In addition, the results of the analytical study are nicely corroborated by a comprehensive set of simulations, which demonstrate the validity of the model and the accuracy of the approach...|$|R
40|$|Abstract: This paper {{deals with}} the <b>impact</b> of fault <b>prediction</b> <b>techniques</b> on {{checkpointing}} strategies. We suppose that the fault-prediction system provides prediction windows instead of exact predictions, which dramatically complicates {{the analysis of the}} checkpointing strategies. We propose a new approach based upon two periodic modes, a regular mode outside prediction windows, and a proactive mode inside prediction windows, whenever the size of these windows is large enough. We are able to compute the best period for any size of the prediction windows, thereby deriving the scheduling strategy that minimizes platform waste. In addition, the results of this analytical evaluation are nicely corroborated by a comprehensive set of simulations, which demonstrate the validity of the model and the accuracy of the approach. Key-words: Fault-tolerance, checkpointing, prediction, algorithms, model, exascal...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThis paper investigates {{the feasibility of}} an air-to-surface missile delivery system employing <b>impact</b> <b>prediction</b> to assist a human operator in controlling the system. The <b>impact</b> <b>prediction</b> information was used to drive a display which the operator used in controlling the flight of the weapon. It was desired to find whether the <b>impact</b> <b>prediction</b> proved useful in weapons control. [URL] (junior grade), United States Nav...|$|R
40|$|AbstractRecently, a new {{numerical}} <b>prediction</b> <b>technique</b> {{has been}} developed {{for the analysis of}} vibro-acoustic problems in bounded domains. This wave-based <b>prediction</b> <b>technique</b> adopts an indirect Trefftz approach. Unlike the conventional element-based methods, the novel technique uses wave functions that exactly satisfy the governing dynamic equations to describe the dynamic variables. Fine domain discretizations are no longer required and the resulting model sizes are much smaller than with the element based methods. This results in a more time-efficient <b>prediction</b> <b>technique</b> which also allows handling of mid-frequency applications. This paper discusses how the wave-based <b>prediction</b> <b>technique</b> can be used for vibro-acoustic radiation problems in unbounded domains. Its beneficial characteristics compared to the element-based methods are demonstrated through a validation example...|$|R
3000|$|The {{predictive}} auto-scaling systems {{generate a}} scaling {{decision based on}} the future forecast of a performance indicator’s value. Therefore, to improve {{the accuracy of the}} predictive auto-scaling systems, researchers have strived to improve the accuracy of the <b>prediction</b> <b>techniques</b> that are being used in the auto-scaling systems (see [4] for a comprehensive overview of the auto-scaling <b>prediction</b> <b>techniques).</b> According to [4], the most dominant <b>prediction</b> <b>technique</b> in the IaaS layer of the cloud auto-scaling domain is time-series <b>prediction.</b> Time-series <b>prediction</b> <b>techniques</b> use the historical values of a performance indicator to forecast its future value. Although in recent years many innovative time-series <b>prediction</b> <b>techniques</b> have been proposed for the auto-scaling systems, the existing approaches neglect the influence of the performance indicator pattern (i.e., how the performance indicator values change over time) on the accuracy of the time-series <b>prediction</b> <b>techniques.</b> This paper proposes an autonomic prediction suite using the decision fusion technique for the resource provisioning of the IaaS layer of the cloud computing environment. The proposed suite identifies the pattern of the performance indicator and accordingly selects the most accurate technique to predict the near future value of the performance indicator for better resource management. The central hypothesis in this paper that serves as the fusion rule of the prediction suite is: [...]...|$|R
40|$|In location-based {{services}} (LBSs), {{the service}} is provided based on the users' locations through location determination and mobility realization. Most of the current location prediction research is focused on generalized location models, where the geographic extent is divided into regular-shaped cells. These models are not suitable for certain LBSs where the objectives are to compute and present on-road services. Such techniques are the new Markov-based mobility prediction (NMMP) and prediction location model (PLM) that deal with inner cell structure and different levels of prediction, respectively. The NMMP and PLM techniques suffer from complex computation, accuracy rate regression, and insufficient accuracy. In this paper, a novel cell splitting algorithm is proposed. Also, a new <b>prediction</b> <b>technique</b> is introduced. The cell splitting is universal {{so it can be}} applied to all types of cells. Meanwhile, this algorithm is implemented to the Micro cell in parallel with the new <b>prediction</b> <b>technique.</b> The <b>prediction</b> <b>technique,</b> compared with two classic <b>prediction</b> <b>techniques</b> and the experimental results, show the effectiveness and robustness of the new splitting algorithm and <b>prediction</b> <b>technique.</b> Peer reviewe...|$|R
30|$|In this section, the {{background}} concepts {{that are used}} in the paper and the related work are introduced. Sub-section Workload is an overview of the workload concept and its patterns. Sub-sections Decision making and <b>Prediction</b> <b>techniques</b> provide an overview of the most dominant auto-scaling approaches in two broad categories: decision making and <b>prediction</b> <b>techniques.</b>|$|R
40|$|International audienceIn {{software}} engineering, {{impact analysis}} consists {{in predicting the}} software elements (e. g. modules, classes, methods) potentially impacted by {{a change in the}} source code. Impact analysis is required to optimize the testing effort. In this paper, we propose a framework to predict error propagation. Based on 10 open-source Java projects and 5 classical mutation operators, we create 17000 mutants and study how the error they introduce propagates. This framework enables us to analyze <b>impact</b> <b>prediction</b> based on four types of call graph. Our results show that the sophistication indeed increases completeness of <b>impact</b> <b>prediction.</b> However, and surprisingly to us, the most basic call graph gives the highest trade-off between precision and recall for <b>impact</b> <b>prediction...</b>|$|R
5000|$|... #Caption: Support {{letter from}} Beijing FUDU Natural Disaster <b>Prediction</b> <b>Techniques</b> Application Co.,Ltd.|$|R
5000|$|ADEPT: Adaptive Dynamic Ensemble <b>Prediction</b> <b>Techniques,</b> University of Manchester (UK), University of Bristol (UK) ...|$|R
40|$|In this rejoinder, {{the authors}} {{respond to the}} {{insightful}} commentary of Strohmer and Arm, Chwalisz, and Hilton, Harris, and Rice about the meta-analysis on statistical versus clinical <b>prediction</b> <b>techniques</b> for mental health judgments. The authors address issues including the availability of statistical <b>prediction</b> <b>techniques</b> for real-life psychology applications, {{the development of these}} <b>prediction</b> <b>techniques</b> for future applications, and the training of counseling and other psychologists in using statistical <b>prediction</b> <b>techniques.</b> Many of these issues are couched in the historical debate about clinical versus statistical prediction. We were pleased to read the reactions to our article by such eminent scientist practitioners as Strohmer and Arm (2006 [this issue]), Chwalisz (2006 [this issue]), and Hilton, Harris, and Rice (2006 [this issue]). These scholars provide an important appraisal of our work, pose questions, sug-gest and extend implications for counseling psychology and mental health practice and training, and offer guidance for future research. The reactants provide similar yet slightly different vantage points on the utility (an...|$|R
40|$|Time series <b>prediction</b> <b>techniques</b> {{have been}} shown to {{significantly}} reduce the radio use and energy consumption of wireless sensor nodes performing periodic data collection tasks. In this paper, we propose an implementation of exponential smoothing, a standard time series <b>prediction</b> <b>technique,</b> for wireless sensors. We rely on a framework called Adaptive Model Selection (AMS), specifically designed for running time series <b>prediction</b> <b>techniques</b> on resource-constrained wireless sensors. We showcase our implementation with two demos, related to environmental monitoring and video games. The demos are implemented with TinyOS, a reference operating system for low-power embedded systems, and TMote Sky and TMote Invent wireless sensors. © 2012 ICST Institute for Computer Science, Social Informatics and Telecommunications Engineering. SCOPUS: cp. kinfo:eu-repo/semantics/publishe...|$|R
40|$|One of the trickiest {{challenges}} {{introduced by}} cellular communications networks is mobility prediction for Location Based-Services (LBSs). Hence, an accurate and efficient mobility <b>prediction</b> <b>technique</b> is particularly needed for these networks. The mobility <b>prediction</b> <b>technique</b> incurs overheads on the transmission process. These overheads affect {{properties of the}} cellular communications network such as delay, denial of services, manual filtering and bandwidth. The main goal {{of this research is}} to enhance a mobility prediction scheme in cellular communications networks through three phases. Firstly, current mobility <b>prediction</b> <b>techniques</b> will be investigated. Secondly, innovation and examination of new mobility <b>prediction</b> <b>techniques</b> will be based on three hypothesises that are suitable for cellular communications network and mobile user (MU) resources with low computation cost and high prediction success rate without using MU resources in the prediction process. Thirdly, a new mobility prediction scheme will be generated that is based on different levels of mobility prediction. In this thesis, a new mobility prediction scheme for LBSs is proposed. It could be considered as a combination of the cell and routing area (RA) prediction levels. For cell level prediction, most of the current location prediction research is focused on generalized location models, where the geographic extent is divided into regular-shape cells. These models are not suitable for certain LBSs where the objectives are to compute and present on-road services. Such techniques are the New Markov-Based Mobility Prediction (NMMP) and Prediction Location Model (PLM) that deal with inner cell structure and different levels of prediction, respectively. The NMMP and PLM techniques suffer from complex computation, accuracy rate regression and insufficient accuracy. In this thesis, Location Prediction based on a Sector Snapshot (LPSS) is introduced, which is based on a Novel Cell Splitting Algorithm (NCPA). This algorithm is implemented in a micro cell in parallel with the new <b>prediction</b> <b>technique.</b> The LPSS technique, compared with two classic <b>prediction</b> <b>techniques</b> and the experimental results, shows the effectiveness and robustness of the new splitting algorithm and <b>prediction</b> <b>technique.</b> In the cell side, the proposed approach reduces the complexity cost and prevents the cell level <b>prediction</b> <b>technique</b> from performing in time slots that are too close. For these reasons, the RA avoids cell-side problems. This research discusses a New Routing Area Displacement Prediction for Location-Based Services (NRADP) which is based on developed Ant Colony Optimization (ACO). The NRADP, compared with Mobility Prediction based on an Ant System (MPAS) and the experimental results, shows the effectiveness, higher prediction rate, reduced search stagnation ratio, and reduced computation cost of the new <b>prediction</b> <b>technique...</b>|$|R
40|$|There the {{physical}} simulators of microstructure of non-outburst-dangerous and outburst-dangerous anthracites, the criteria and the <b>prediction</b> <b>technique</b> of outburst danger of anthracite seams at the prospecting stage have been developed. The <b>prediction</b> <b>technique</b> {{has been used}} during the estimation of outburst danger of anthracite seams during the complementary prospecting of five mining areas in DonbassAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
30|$|Firstly, {{the error}} of control output {{produced}} by the <b>prediction</b> <b>technique</b> is less than 0.1 for the blurring levels.|$|R
40|$|Description of an {{in-flight}} visibility test {{conducted during}} the Apollo 14 mission {{for the purpose of}} validating and extending the mathematical visibility models used previously {{in the course of the}} Apollo program to examine the constraints on descent operations imposed by lunar visibility limitations. Following a background review of the effects on mission planning of the visibility limitations due to downsun lunar surface detail 'washout' and a discussion of the visibility <b>prediction</b> <b>techniques</b> previously used for studying lunar visibility problems, the visibility test rationale and procedures are defined and the test results presented. The results appear to confirm the validity of the visibility <b>prediction</b> <b>techniques</b> employed in lunar visibility problem studies. These results provide also a basis for improving the accuracy of the <b>prediction</b> <b>techniques</b> by appropriate modifications...|$|R
40|$|The {{prediction}} of the SRB and ET impact areas requires six separate processors. The SRB <b>impact</b> <b>prediction</b> processor computes the impact areas and related trajectory data for each SRB element. Output from this processor is stored on a secure file accessible by the SRB impact plot processor which generates the required plots. Similarly the ET RTLS <b>impact</b> <b>prediction</b> processor and the ET RTLS impact plot processor generates the ET impact footprints for return-to-launch-site (RTLS) profiles. The ET nominal/AOA/ATO <b>impact</b> <b>prediction</b> processor and the ET nominal/AOA/ATO impact plot processor generate the ET impact footprints for non-RTLS profiles. The SRB and ET impact processors compute {{the size and}} shape of the impact footprints by tabular lookup in a stored footprint dispersion data base. The location of each footprint is determined by simulating a reference trajectory and computing the reference impact point location. To insure consistency among all flight design system (FDS) users, much input required by these processors will be obtained from the FDS master data base...|$|R
40|$|Software is {{becoming}} an increasingly important part of automotive product development. While software in automotive domain enables important functionality and innovations, it also requires significant effort for its verification & validation {{to meet the demands}} of safety, high quality and reliability. To ensure that the safety and quality demands are meet within the available resource and time - requires efficient planning and control of test resources and continuous reliability assessment. By forecasting the expected number of defects and likely defect inflow profile over software life cycle, defect <b>prediction</b> <b>techniques</b> can be used for effective allocation of limited test resources. These techniques can also help with the assessment of maturity of software before release. This thesis presents research aimed at improving the use of software defect <b>prediction</b> <b>techniques</b> within the automotive domain. Through a series of empirical studies, different software defect <b>prediction</b> <b>techniques</b> are evaluated for their applicability in this context. The focus of the assessment have been on evaluation of these techniques, how to select the appropriate software reliability growth models and the factors that play important role in their adoption in industry. The results show that - defect <b>prediction</b> <b>techniques</b> (i) can be effectively used to forecast the expected defect inflow profile (shape and the asymptote); (ii) they are also useful for assessment of the maturity of software before release; (iii) executable models can be used for early reliability assessment by combining fault injection with mutation testing approach; and (iv) a number of factors beyond predictive accuracy such as setup, running, and maintenance costs are important for industrial adoption of machine learning based software defect <b>prediction</b> <b>techniques.</b> The effective use of software defect <b>prediction</b> <b>techniques</b> and doing early reliability assessment on executable models would allow (i) early planning and efficient use of limited test resources; (ii) reduced development time/ market lead time; and (iii) more robust software in automobiles which make them more intelligent, safe and also highly reliable...|$|R
40|$|Abstract. In {{this paper}} {{we present a}} new <b>prediction</b> <b>technique</b> to com-press a pair of {{satellite}} images that have significant overlap in the under-lying spatial areas. When this <b>prediction</b> <b>technique</b> is combined with an existing lossless image set compression algorithm, the results are signif-icantly better than those obtained by compressing each image individu-ally. Even when there are {{significant differences between the}} two images due to factors such as seasonal and atmospheric variations, the new pre-diction technique still performs very well to achieve significant reduction in storage requirements. ...|$|R
40|$|Aircraft noise {{measurements}} {{were made at}} Denver International Airport {{for a period of}} four weeks. Detailed operational information was provided by airline operators which enabled noise levels to be predicted using the FAA's Integrated Noise Model. Several thrust <b>prediction</b> <b>techniques</b> were evaluated. Measured sound exposure levels for departure operations were found to be 4 to 10 dB higher than predicted, depending on the thrust <b>prediction</b> <b>technique</b> employed. Differences between measured and predicted levels are shown to be related to atmospheric conditions present at the aircraft altitude...|$|R
40|$|The {{status of}} ejector {{development}} {{in terms of}} application to V/STOL aircraft is reported in three categories: aircraft systems and ejector concepts; ejector performance including <b>prediction</b> <b>techniques</b> and experimental data base available; and, integration of the ejector with complete aircraft configurations. Available <b>prediction</b> <b>techniques</b> are reviewed and performance of three ejector designs with vertical lift capability is summarized. Applications of the 'fuselage' and 'short diffuser' ejectors to fighter aircraft are related to current and planned research programs. Recommendations are listed for effort needed to evaluate installed performance...|$|R
40|$|Recently, we {{presented}} {{a study on}} residual <b>prediction</b> <b>techniques</b> {{that can be applied}} to voice conversion based on linear transformation or hidden Markov model-based speech synthesis. Our voice conversion experiments showed that none of the six compared techniques was capable of successfully converting the voice while achieving a fair speech quality. In this paper, we suggest a novel residual <b>prediction</b> <b>technique</b> based on unit selection that outperforms the others in terms of speech quality (mean opinion score = 3) while keeping the conversion performance. 1...|$|R
40|$|A {{spacecraft}} can {{be modeled}} {{as a collection}} of states, with certain actions (commands) leading the spacecraft from one state to another. Here, we present the outline of a technique to generate models from the existing sequence checking code in a semi-automated way. To further speedup the simulation on the hypercube, a technique for Parallel Discrete Event Simulation (PDES) based upon the <b>prediction</b> <b>techniques</b> is also presented. The <b>prediction</b> <b>technique</b> is introduced with simulation results obtained on the Ncube and the performance of the proposed technique is compared to the Time Warp algorithm. ...|$|R
30|$|Linear {{prediction}} (LP) is {{a technique}} widely used in the data processing field. It predicts the time series in the future or in the past according to obtained time series. In recent years linear <b>prediction</b> <b>technique</b> has found extensive applications {{in the fields of}} voice identification, image processing, and signal frequency estimation [10 – 12]. This paper incorporates spatial linear <b>prediction</b> <b>technique</b> into the ETAM method to propose a new hybrid technique called LP-ETAM method for the bearing estimation of multiple coherent underwater targets. The proposed method employs spatial linear <b>prediction</b> <b>technique</b> to do extrapolation on the synthetic aperture obtained by ETAM to further enlarge the aperture size, which is the main reason leading to superior bearing resolution to ETAM method. Results from simulations and applications of the ETAM and LP-ETAM method on real data with pure tone CW signals showed that the proposed LP-ETAM method achieved better angular resolution than ETAM method.|$|R
40|$|Abstract: The {{foremost}} {{objective of}} a wireless network is {{to facilitate the}} communication of mobile users regardless of their point of attachment to the network. The system must discern {{the location of the}} mobile terminal, to afford flawless service to the mobile terminal. Mobility prediction is widely used to assist handoff management, resource reservation and service preconfiguration. <b>Prediction</b> <b>techniques</b> that are currently used don’t consider the motivation behind the movement of mobile nodes and incur huge overheads to manage and manipulate the information required to make predictions. This paper proposes an activity based mobility <b>prediction</b> <b>technique</b> that uses activity prediction and Markov modeling techniques to devise a prediction methodology that could make accurate <b>predictions</b> than existing <b>techniques.</b> 1...|$|R
40|$|Previous {{studies have}} shown that human {{movement}} is predictable to a certain extent at different geographic scales. Existing <b>prediction</b> <b>techniques</b> exploit only the past history of the person taken into consideration as input of the predictors. In this paper, we show that by means of multivariate nonlinear time series <b>prediction</b> <b>techniques</b> it is possible to increase the forecasting accuracy by considering movements of friends, people, or more in general entities, with correlated mobility patterns (i. e., characterised by high mutual information) as inputs. Finally, we evaluate the proposed techniques on the Nokia Mobile Data Challenge and Cabspotting datasets. Comment: 21 pages, 9 figure...|$|R
40|$|Hybrid {{recommender}} {{systems are}} capable of providing better rec- ommendations than non-hybrid ones. Our approach to hybrid recommenders {{is the use of}} prediction strategies that determine which <b>prediction</b> <b>technique(s)</b> should be used at the moment an actual prediction is required. In this paper, we determine whether case-based reasoning can provide more accurate prediction strategies than rule-based predictions strategies created manually by experts. Experiments show that case-based reasoning can indeed be used to create prediction strategies; it can even increase the accuracy of the recommender in systems where the accuracy of the used <b>prediction</b> <b>techniques</b> is highly spread. ...|$|R
40|$|The {{prediction}} of probable words for more immediate selection has proven a valuable technique for augmenting the communication {{of persons with}} disabilities. Statistical <b>prediction</b> <b>techniques</b> have been limited to completion of the current word and {{prediction of}} the subsequent word. This study quantifies the impact of adopting higher-order <b>prediction</b> <b>techniques</b> that rely upon increased word context. Additionally, it establishes the dependence of performance upon {{the size of the}} text used to derive the statistical database. The results suggest that adoption of higher-order techniques and larger databases can increase keystroke savings by more than 7. 5 percentage points...|$|R
40|$|The failure {{prediction}} requirements and the pertinent accept/reject criteria for structural ceramics are derived, and the available failure <b>prediction</b> <b>techniques</b> are examined, vis-a-vis the {{failure prediction}} relations, {{in order to}} highlight the capabilities and limitations of each technique. The need for additional techniques is thereby demonstrated. The capabilities of the ultrasonic technique are extensively evaluated {{in order to determine}} its ability to satisfy the deficiencies in the existing failure prediction repertoire. The prospects are shown to be very encouraging, but the results of several key studies must be awaited before defining the ultimate role of ultrasonic failure <b>prediction</b> <b>techniques...</b>|$|R
30|$|Typically, VMs {{consists}} of different amounts {{and types of}} cloud resources (e.g., CPU, memory, bandwidth, etc.). The multi-resource nature of these VMs poses a unique challenge {{when it comes to}} developing <b>prediction</b> <b>techniques.</b>|$|R
40|$|This paper briefly reviews <b>prediction</b> <b>techniques</b> for {{determination}} of leakage and friction along auxiliary ventilation ducting systems. In order to compare various <b>prediction</b> <b>techniques</b> {{that have been}} developed over the past, a macroscopic investigation of air leakage and friction resistance of auxiliary ventilation ducting systems has been undertaken. Measurements were conducted on 450 and 915 mm diameter fabric ducting over 100 m duct length to determine frictional resistances and the extent of leakage. Due to the high degree of accuracy required and the large volume of data that needed to be collected, electronic pressure transducers were used with computer for data recording. Conceptual models that describe the leakage characteristics of auxiliary ventilation ducting systems were developed based on this information. It was found that these models provided good correlation with most of the existing <b>prediction</b> <b>techniques.</b> The experimental methodology relying on computer data acquisition has allowed the accuracy of measured values to be treated {{with a high degree of}} confidence. The reliability of the developed models allows prediction of leakage, frictional impedance and airflow with enhanced confidence...|$|R
