0|1218|Public
40|$|The {{effect of}} the {{ordering}} of questions in sample surveys has been well investigated. However, {{there has been no}} study so far that examined the {{effect of the}} ordering of <b>exam</b> <b>questions</b> on the <b>exam</b> behavior of students. In this paper, we will discuss the effect of the ordering of questions on the student behavior for an introductory long-distance statistics course. Because we were working with the electronic textbook CyberStats that records exact submission times of student answers to <b>exam</b> <b>questions,</b> the following research question could be answered: Do students usually answer <b>exam</b> <b>questions</b> in sequential order (independent from the <b>exam</b> <b>questions)</b> or do they tend to jump from one <b>exam</b> <b>question</b> to another while leaving some types of (apparentely more di cult) <b>exam</b> <b>questions</b> until the very end? As it turned out, many students answer <b>exam</b> <b>questions</b> in almost perfect sequential order. This may easily lead to the result that those students will miss simple <b>exam</b> <b>questions</b> towards the end of an exam when there is a tight time limit for the students...|$|R
50|$|All <b>exam</b> <b>questions</b> are multiple-choice.|$|R
40|$|I propose an ex-post {{modeling}} {{approach that}} allows educators to im- prove the weights {{that could be}} attributed to objective <b>exam</b> <b>questions.</b> This improvement is related to the ability to predict the overall exam score, based on reported learning experiences and objective measurements of learning activities. The point of this approach is that it allows educators (and scientists) to investigate the quality of <b>exam</b> <b>questions</b> and their rela- tionship with various cofactors, even if the student population is heteroge- neous. Based on a preliminary, empirical analysis of two large samples it is shown that: (1) <b>exam</b> <b>questions</b> may be relevant for one sub-population but inadequate for another - even if both populations completed the same course; (2) the relationship between <b>exam</b> <b>questions</b> and exogenous co- factors may be highly misleading because the weights of <b>exam</b> <b>questions</b> are poorly aligned; (3) an adequate evaluation of examinations strongly depends on the cofactors that are used; (4) the proposed approach has important consequences for educational research about computer assisted learning. status: publishe...|$|R
5000|$|... #Subtitle level 3: Scholarship <b>exam</b> <b>question</b> {{about killing}} protesters (2011) ...|$|R
5000|$|Students' Suggested Answers Booklets on past <b>exam.</b> <b>Questions</b> {{on various}} subject ...|$|R
5000|$|Practise {{with sample}} <b>exam</b> <b>questions.</b> You can find {{examination}} papers at: http://www.ufrgs.br/acervocelpebras/acervo ...|$|R
40|$|This {{research}} aims to: 1. Knowing {{the ability}} of DMR in analysis of difficulty level, discrimination power, and distractor functions of the National <b>Exam</b> <b>Questions,</b> 2. Knowing the assurance level of the difficulty analysis results of National <b>Exam</b> <b>Questions</b> by DMR compared to manual calculation, 3. Knowing analysis results of difficulty level, discrimination power, and distractor functions of the National <b>Exam</b> <b>Questions</b> and improvements to be made. Type {{of this research is}} qualitative research with the development of conceptual models. Analysis of the data used in this research is content analysis. Procedure development starting from product development, product testing and products assessment. Data was collected by questionnaire {{in the form of the}} National <b>Exam</b> <b>Questions.</b> The results of this research are: 1. Difficulty level analysis of Indonesian Questions in the enough or moderate category, English in the too difficult and enough or moderate category, Science in the too difficult and enough or moderate category, Mathematics in the category of too difficult and enough or moderate. 2) Difficulty level of National <b>Exam</b> <b>Questions</b> being analyzed with DMR or manually, showed no significant difference at all the National <b>Exam</b> <b>Questions</b> being tested. That is the difficulty analysis of questions, the teacher can perform difficulty analysis of questions using DMR. 3) Follow up difficulty level analysis result of of National <b>Exam</b> <b>Questions</b> that can be done for an easy category is by removing the item and are not issued anymore, while the difficult category, questions items subject to reexamination, tracked, and traced so it can be known a factor that causes the items in question difficult to answer by students, whether the questions sentence is less clear, whether instructions on how to do the questions is difficult to understand, or whether the question is there are terms that are not clear, and after repair, the items are removed again in the the next achievement test...|$|R
25|$|Following the allegations, the Schools {{minister}} {{asked for}} a review of rules for teachers who write <b>exam</b> <b>questions.</b>|$|R
40|$|Introduction: The {{purpose of}} this study is to {{describe}} an approach for evaluating assessments used in the first 2 years of medical school and report the results of applying this method to current first and second year medical student examinations. Methods: Three faculty members coded all <b>exam</b> <b>questions</b> administered during the first 2 years of medical school. The reviewers discussed and compared the coded <b>exam</b> <b>questions.</b> During the bi-monthly meetings, al...|$|R
50|$|Question Object File Format (QUOX) is a {{file format}} created by Quobject Software in 2009 for easy <b>exam</b> <b>question</b> exchange.QUOX {{is used for}} {{representing}} an <b>exam</b> <b>question</b> with text, pictures and MathML in any order together with the answer of the question in a single file. This allows for easy sharing of questions among teachers and quick combination of this files for rapid creation of worksheets, exams and online quizzes.|$|R
40|$|Document {{classification}} is {{a growing}} interest in the research of text mining. Classification can be done based on the topics, languages, and so on. This {{study was conducted to}} determine how Naive Bayes Updateable performs in classifying the SBMPTN <b>exam</b> <b>questions</b> based on its theme. Increment model of one classification algorithm often used in text classification Naive Bayes classifier has the ability to learn from new data introduces with the system even after the classifier has been produced with the existing data. Naive Bayes Classifier classifies the <b>exam</b> <b>questions</b> based on the theme of the field of study by analyzing keywords that appear on the <b>exam</b> <b>questions.</b> One of feature selection method DF-Thresholding is implemented for improving the classification performance. Evaluation of the classification with Naive Bayes classifier algorithm produces 84, 61 % accuracy...|$|R
50|$|Get your ACSP and ACMT {{questions}} and study guide, answer all question {{and be ready}} for your final <b>exam.</b> Real <b>Exam</b> <b>Questions.</b>|$|R
40|$|AbstractThe aim of {{this study}} is {{identification}} on <b>exam</b> <b>question</b> by educator. In Vocational and training high school, second year <b>exam</b> <b>question</b> are selected as a sample. In this study, the second-class machine program students were asked by teachers final question were examined in the last five years. The review, basic courses, vocational courses and mathematics courses for: openness, open-ended questions, pointless, irrelevant and error {{on the side of a}} commission in the direction of the questions were examined...|$|R
50|$|The table below {{lists the}} {{allocation}} of <b>exam</b> <b>questions</b> for each main job function of an investment company and variable contracts products representative.|$|R
50|$|Molly and Stink {{are accused}} of {{stealing}} <b>exam</b> <b>questions.</b> Stink gets {{kicked out of the}} Academy and sets out to become a stand-up comic.|$|R
40|$|The {{process of}} “writing to learn ” has been {{documented}} in many disciplines. In this study, {{a specific type of}} writing, microthemes, was implemented in a human anatomy and physiology course in order to determine whether this type of writing assignment enhances student exam performance. Student performance on <b>exam</b> <b>questions</b> dealing with topics covered in microtheme assignments was compared to performance on <b>exam</b> <b>questions</b> with no such related assignment. Statistically significant improvements were recorded on two of the four exams...|$|R
5000|$|In 2014, the Oxford, Cambridge and RSA (OCR) Exam board, having {{conducted}} {{an investigation into}} alleged exam malpractice, concluded that the school had redacted (deleted) questions involving the evolution of species on GCSE science <b>exam</b> <b>questions.</b> Ofqual subsequently ruled that blocking out <b>exam</b> <b>questions</b> is malpractice, and, accordingly, not permissible. However it was later revealed that OCR had privately agreed that the school could advise students not to answer particular questions if they [...] "need {{to do this in}} view of their religious beliefs." ...|$|R
40|$|This study {{evaluates the}} impact of an {{independent}} postmidterm question analysis exercise {{on the ability of}} students to answer subsequent <b>exam</b> <b>questions</b> on the same topics. It was conducted in three sections (∼ 400 students/section) of introductory biology. Graded midterms were returned electron-ically, and each student was assigned a subset of questions answered incorrectly by more than 40 % of the class to analyze as homework. The majority of questions were at Bloom’s application/analysis level; this exercise therefore emphasized learning at these higher levels of cognition. Students in each section answered final <b>exam</b> <b>questions</b> matched by topic to all homework questions, providing a within-class control group for each question. The percentage of students who correctly answered the matched final <b>exam</b> <b>question</b> was significantly higher (p < 0. 05) in the Topic Analysis versus Control Analysis group for seven of 19 questions. We identified two factors that influenced activity effectiveness: 1) similarity in topic emphasis of the midterm–final <b>exam</b> <b>question</b> pair and 2) quality of the completed analysis homework. Our data suggest that this easy-to-implement exercise will be useful in large-enrollment classes to help students develop self-regulated learning skills. Additional strategies to help introductory students gain a broader understanding of topic areas are discussed...|$|R
40|$|Student {{performance}} on examinations {{is influenced by}} the level of difficulty of the questions. It seems reasonable to propose therefore that assessment of the difficulty of <b>exam</b> <b>questions</b> could be used to gauge the level of skills and knowledge expected {{at the end of a}} course. This paper reports the results of a study investigating the difficulty of <b>exam</b> <b>questions</b> using a subjective assessment of difficulty and a purpose-built <b>exam</b> <b>question</b> complexity classification scheme. The scheme, devised for exams in introductory programming courses, assesses the complexity of each question using six measures: external domain references, explicitness, linguistic complexity, conceptual complexity, length of code involved in the question and/or answer, and intellectual complexity (Bloom level). We apply the scheme to 20 introductory programming exam papers from five countries, and find substantial variation across the exams for all measures. Most exams include a mix of questions of low, medium, and high difficulty, although seven of the 20 have no questions of high difficulty. All of the complexity measures correlate with assessment of difficulty, indicating that the difficulty of an <b>exam</b> <b>question</b> relates to each of these more specific measures. We discuss the implications of these findings for the development of measures to assess learning standards in programming courses...|$|R
40|$|We {{describe}} CMU’s UIMA-based modular automatic ques-tion answering (QA) system. This system answers multiple-choice <b>English</b> <b>questions</b> for {{the world}} history entrance <b>exam.</b> <b>Questions</b> are preceded by short descriptions providing a his-torical context. Given the context and question-specific in-structions, we generate verifiable assertions for each answer choice. These assertions are evaluated using several evidenc-ing modules, which assign a plausibility score to each asser-tion. These scores are then aggregated to produce the most plausible answer choice. In the NTCIR- 11 QALab evalua-tions, our system achieved 51. 6 % accuracy on the trainin...|$|R
40|$|For various Dutch {{students}} at secondary education the English national written exam {{is a difficult}} hurdle to take. Most students understand {{the contents of the}} texts they have to read, but make too many errors in answering the <b>exam</b> <b>questions.</b> <b>Exam</b> training, which consists of teaching tekst structures and reading strategies, does not always lead to better results. For students it can be of great importance to know where and why they go wrong in answering <b>exam</b> <b>questions.</b> In this study systematic error analysis (SEA) is applied in order to find out why students go wrong in answering <b>exam</b> <b>questions.</b> Students had to explain why they chose a particular answer and where they found the answer in the text. The outcome of the error analysis was used as a source for exam training. The outcome of this study showed that SEA as a source of exam training does not lead to immediate repair of students' errors. Despite the fact that students' explanations on why they gave a particular answer were correct, it did not lead to significantly more correct answers...|$|R
50|$|Each exam has {{a series}} of topics {{as part of the}} curriculum. Each topic has an {{associated}} weight that corresponds to the frequency of <b>exam</b> <b>questions</b> from that topic.|$|R
40|$|Evaluation {{of course}} outcome {{is one of}} the {{important}} elements in assessing students’ performance in higher institutions. Most of the courses depend on examination result as a medium to evaluate the level of students’ performances. In this study, the final <b>exam</b> <b>questions</b> of Vector Calculus course KKKQ 1123 is being used to assess the difficulty index and discrimination index. 80 students from Department of Mechanical and Materials (JKMB) were involved in this study. This study found that the range of difficulty index is between 0. 2 and 0. 8, where the question that was categorized as difficult is question number 6 whereas for easy questions is question number 3. Meanwhile the range obtained for discrimination index was in the acceptable range, that is, between 0. 2 and 0. 6 which indicates that the <b>exam</b> <b>questions</b> are good. With this study, it is hope that it will guide the lecturers in constructing and crafting a good and reliable <b>exam</b> <b>questions</b> consistent with the level of student's ability. ...|$|R
40|$|<b>Exam</b> <b>Question</b> Construction System (EQC-SYS) is {{a system}} that is able to create and manage {{examination}} questions that are stored in the database. The system is able to create <b>exam</b> <b>questions</b> automatically according to the topic and other options available. This sources of information required to create the <b>exam</b> <b>questions</b> are all obtained from the data sources such as slide notes that has been cleaned prior according to the template and format to assist the next process. This is done by applying Information Retrieval (IR) and Knowledge Extraction (KE). IR is the task of obtaining information resources such as documents that is related to the information needed from a collection of information resources using searching method that can be based on metadata or fulltext indexing. KE is the creation of knowledge either from structured or unstructured sources. An example of structured sources is relational databases while an example of unstructured sources is documents. The system is able to generate sets of exam paper together with its respective answers from the questions stored in the database. The system is developed to reduce the problem of using a lot of time just to create <b>exam</b> <b>questions</b> for an <b>exam,</b> the problem of maintaining the quality of questions created by referring to the cognitive level of Bloom’s Taxonomy and also the problem of balancing the questions according to the TST created by the resource person (RP). The system is developed according to the revised waterfall methodology and it will only cover one course only, which is Fundamentals of Information Systems (ITS 410). The type of questions that are being stored in the database is short answer questions. The developed system is able to minimize the problem, assist the managing and creation of <b>exam</b> <b>questions</b> process and also reduce the time for the previous tasks...|$|R
40|$|Abstract. Subjective {{question}} {{marking system}} at present {{is affected by}} the attention of people, the subjective topic grading principles are common contrast degree of <b>exam</b> <b>questions</b> {{similar to those of the}} reference answer, and based on the improved semantic similarity algorithm, calculation of sentence similarity, the similarity degree of <b>exam</b> <b>questions</b> and reference answer is obtained, thus give scores. And design based on semantic similarity experiment, the experiment results show that the proposed multi-level fusion similarity calculation method to improve the original method, on the basis of integration advantages of various methods, make the calculation results meet the requirements of the scoring system...|$|R
40|$|Perhaps {{the most}} {{important}} task confronting educators is the accurate assessment of student learning. All efforts at reform or innovation must ultimately address the question “Has student learning improved?”. In addition, the movement towards individualized learning {{must be able to}} answer the question “Has THIS student’s learning improved?” These questions will require that as educators, we develop the tools and the expertise to critically evaluate our assessment instruments. Large first-year science classes permit the application of learning measurement theories that are at the leading edge of cognitive and educational science. At the University of Guelph, first year Chemistry has approximately 2400 students who complete several multiple choice exams throughout the year in principally two courses. When a student’s grade is calculated {{at the end of the}} year, what should that grade mean? A careful study of <b>exam</b> <b>questions</b> using modern learning measurement theories begins to inform the process of exam creation and interpretation. Classical Test Theory (CTT) is commonly used to analyze the performance of both students and <b>exam</b> <b>questions,</b> but this older theory has several well-known weaknesses. Item Response Theory (IRT) is better suited to effectively characterizing the performance of both <b>exam</b> <b>questions</b> and students. I will present data on the analysis of <b>exam</b> <b>questions</b> used at Guelph over the past several years to understand the features that lead to better measures of student learning. Future pedagogical changes will be more readily accessed with these measurement tools...|$|R
40|$|Can {{archives}} of audiovisual TV interviews {{be used to}} make authors more visible to students, and thereby reduce the learning gap between native and non-native language speakers in college classes? We examined students in a college course who learned about one scholar's ideas through watching an audiovisual TV interview (i. e., visible author format) and about another scholar's ideas through reading a formal text description (i. e., invisible author format). For the invisible author, native language speakers scored significantly higher than the non-native language speakers on a corresponding <b>exam</b> <b>question</b> (i. e., a cognitive measure), generated more words on the <b>exam</b> <b>question</b> (i. e., a motivational measure), and mentioned the author's name more often in answering the <b>exam</b> <b>question</b> (i. e., an affective measure). For the visible author, the groups did not differ on any of these measures. These findings provide evidence for the idea that making the author visible through audiovisual TV interviews can eliminate the learning gap between native and non-native language speakers. 3 Universities around the world serve students who are non-native speakers of th...|$|R
40|$|Crafting <b>exam</b> <b>questions</b> {{is an art}} in itself. Even {{though there}} is no strict formula for {{producing}} perfect <b>exam</b> <b>questions,</b> the current line of thinking points in the direction of Outcome Based Education (OBE). In this study, we evaluate the final <b>exam</b> <b>questions</b> of the 2012 Linear Algebra course using OBE standards in two separate phases. Firstly, the questions are categorised according to the Course Learning Objectives (CLO) and evaluated in light of the six cognitive domains of Bloom’s taxonomy. This first phase aims to assess whether the examination paper has the right balance of questions in each domain. The second phase involves analysing the results {{of the students in the}} same examination using item analysis techniques. The analysis includes finding the discrimination index and difficulty index derived from the answers of the students. This second phase focuses on determining the effectiveness of the questions in discriminating students according to their grasp of the CLO. Based on the results, some improvements are suggested. We conclude that except for the analysis level, the cognitive levels of all the questions are generally acceptable...|$|R
40|$|The {{objectives}} {{of this study}} were to investigate the alignment of <b>exam</b> <b>questions</b> with course learning outcomes in a first year biology majors course, to examine gaps and overlaps in assessment of content amongst the sections of the course, and to use this information to provide feedback to the teaching team to further improve the course. Our ultimate goal was to provide students with learning outcomes that would clearly indicate the content and the level at which they would be expected to learn the content for this course, regardless of the section in which they were registered. We took an evidence-based approach to course evaluation and employed the Blooming Biology Tool to compare the learning outcomes and the <b>exam</b> <b>questions</b> of the course, investigating whether the cognitive skill level of each learning outcome as written matched the level at which it was assessed. We identified misalignments and recommended revising the learning outcomes to better reflect the intended level of learning for the course. We also investigated student performance on <b>exam</b> <b>questions</b> of different cognitive levels and found that students scored statisticall...|$|R
40|$|Sample <b>exam</b> <b>questions</b> These are sample <b>exam</b> <b>questions.</b> Their {{purpose is}} {{to give you the}} {{possibility}} to check your knowledge and understanding. This is not a comprehensive list. The problems on the exam will be similar but not exactly the same. You are strongly advised to concentrate on the problem solving approaches and methods rather than the specific solutions to these problems. Note: Tables with values for pdf-s and cdf-s will be available for some of the <b>exam</b> <b>questions.</b> It is strongly advised that you practice reading values from such tables and solving problems with them. Note: These problems are organized by lectures in order to help you study. The exam problems might combine several questions from different lectures. Note: Do bring a calculator if you think you will need it. Do bring the tables that you were given. New tables will not be provided. Note: More information about the statistics elements discussed in this course can be found in [1, 2]. Many papers about the microarray technology, data analysis and related statistical issues can be found at...|$|R
25|$|At the {{beginning}} of the novel, Conrad is reading Jude the Obscure in English class. Later, he reads an <b>exam</b> <b>question</b> that mentions Lord Jim and Of Human Bondage as other works he might have read.|$|R
40|$|Proceedings of: First International Conference, TECH-EDUCATION 2010, Athens, Greece, May 19 - 21, 2010 E-learning educative {{processes}} are {{a challenge for}} educative institutions and education professionals. In {{an environment in which}} learning resources are being produced, catalogued and stored using innovative ways, SOLE provides a platform in which <b>exam</b> <b>questions</b> can be produced supported by Web 2. 0 tools, catalogued and labeled via semantic web and stored and distributed using eLearning standards. This paper presents, SOLE, a social network of <b>exam</b> <b>questions</b> sharing particularized for Software Engineering domain, based on semantics and built using semantic web and eLearning standards, such as IMS Question and Test Interoperability specification 2. 1. Publicad...|$|R
50|$|The Supreme Court {{appoints}} memberships in the Committee of Bar Examiners, {{the official}} task force for formulating bar <b>exam</b> <b>questions,</b> instituting policy directives, executing procedures, grading bar examination papers, and releasing {{the results of}} the annual bar examination.|$|R
50|$|StuDocu is a crowdsourced online {{learning}} and sharing platform for students where students can share study material. StuDocu is mainly used for book summaries, lecture notes and <b>exam</b> <b>questions.</b> Their business model {{is based on}} subscription access for premium documents.|$|R
5000|$|She is fondly {{listed by}} a student, Margaret Lee née Ireland (1962 BSc Mathematics) {{as one of}} her {{favourite}} memories: [...] "Dr Nora Calderwood - what a woman who loved us so much, she could barely keep <b>exam</b> <b>questions</b> secret".|$|R
40|$|During his two hour take-home Newspaper Publishing 135 exam, Charles had {{difficulty}} getting {{the answer to}} one of the questions, so he copied the question into a document and brought it to the NQC to get help. However, the <b>exam</b> <b>question</b> didn’t copy and paste correctly into the document. Seeking clarification on the incorrectly pasted question, the student tutor working with Charles called over Professor Leland, who {{happened to be in the}} NQC. Professor Leland recognized it as an <b>exam</b> <b>question,</b> confronted Charles, and asked that he bring himself to Honor Council. In the subsequent trial, the jury concluded that Charles had violated the Honor Code by disregarding the exam instructions, since consultation of outside sources was explicitly forbidden...|$|R
