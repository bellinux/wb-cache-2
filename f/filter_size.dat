340|810|Public
2500|$|The {{discrete}} {{wavelet transform}} is less computationally complex, taking O(N) time {{as compared to}} O(Nnbsp&lognbsp&N) for the fast Fourier transform. This computational advantage is not inherent to the transform, but reflects {{the choice of a}} logarithmic division of frequency, in contrast to the equally spaced frequency divisions of the FFT (Fast Fourier Transform) which uses the same basis functions as DFT (Discrete Fourier Transform). It is also important to note that this complexity only applies when the <b>filter</b> <b>size</b> has no relation to the signal size. A wavelet without compact support such as the Shannon wavelet would require O(N2). (For instance, a logarithmic Fourier Transform also exists with O(N) complexity, but the original signal must be sampled logarithmically in time, which is only useful for certain types of signals.) ...|$|E
2500|$|Large eddy {{simulation}} {{involves the}} solution to the discrete filtered governing equations using computational fluid dynamics. [...] LES resolves scales from the domain size [...] down to the <b>filter</b> <b>size</b> , and as such a substantial portion of high wave number turbulent fluctuations must be resolved. [...] This requires either high-order numerical schemes, or fine grid resolution if low-order numerical schemes are used. [...] Chapter 13 of Pope addresses the question of how fine a grid resolution [...] is needed to resolve a filtered velocity field [...] [...] Ghosal found that for low-order discretization schemes, such as those used in finite volume methods, the truncation error can be the same order as the subfilter scale contributions, unless the filter width [...] is considerably larger than the grid spacing [...] [...] While even-order schemes have truncation error, they are non-dissipative, and because subfilter scale models are dissipative, even-order schemes will not affect the subfilter scale model contributions as strongly as dissipative schemes.|$|E
50|$|Doubling {{the number}} of {{receiver}} filters increases distance performance by about 20%. Maximum distance performance is achieved when receiver <b>filter</b> <b>size</b> {{is equal to the}} maximum FM noise riding on the transmit signal. Reducing receiver <b>filter</b> <b>size</b> below average amount of FM transmit noise will not improve range performance.|$|E
30|$|After that, we perform {{convolutions}} {{with different}} <b>filter</b> <b>sizes</b> over the embedded word vectors. In our experiment, we used <b>filter</b> <b>sizes</b> of 3, 4 and 5. Then we apply max pooling on {{the result of}} the convolution and add dropout regularization. The process concludes by using a softmax layer to classify our results.|$|R
5000|$|The first {{numbering}} {{was used}} in Daubechies' book Ten lectures on wavelets.Neither of this numbering is unique. The number of vanishing moments does not tell about the chosen factorization. A filter bank with <b>filter</b> <b>sizes</b> 7 and 9 can have 6 and 2 vanishing moments when using the trivial factorization, or 4 and 4 vanishing moments {{as it is the}} case for the JPEG 2000 wavelet. The same wavelet may therefore be referred to as [...] "CDF 9/7" [...] (based on the <b>filter</b> <b>sizes)</b> or [...] "biorthogonal 4, 4" [...] (based on the vanishing moments).|$|R
3000|$|... s are 4 and 2 for the Indian Pines and PaviaU images, respectively. The {{results support}} that these {{parameters}} cannot be too small or too large. If the <b>filtering</b> <b>size</b> and blur degree, i.e., σ [...]...|$|R
50|$|Regal is {{available}} in king size and regular <b>filter</b> <b>size.</b>|$|E
5000|$|Receiver <b>filter</b> <b>size</b> (bandwidth {{divided by}} the total number of filters) ...|$|E
5000|$|The {{two terms}} both appear due to inhomogeneities. The first {{is due to}} the spatial {{variation}} in the <b>filter</b> <b>size</b> [...] while the second {{is due to the}} domain boundary. Similarly, the commutation of the filter [...] with the temporal derivative leads to an error term resulting from temporal variation in the <b>filter</b> <b>size,</b> ...|$|E
50|$|Gemini 1:4.5 80-200mm Macro MC Zoom 55 (55mm <b>filter</b> thread <b>size).</b>|$|R
5000|$|... where [...] is {{the impulse}} {{response}} of the designed <b>filter</b> for <b>size</b> ...|$|R
40|$|OBJECTIVE: We {{investigated}} an algorithm {{to detect}} grey level transitions with multiple scales of resolution to improve edge detection and localisation in ultrasound {{images of the}} prostate. INTRODUCTION: We had developed a non-analytical operator for prostate contour determination implemented with minimum and maximum filters and locate edges. We implemented a technique for improved determination of boundary parts in prostatic ultrasound images by adjusting the edge detection parameter to signal information. METHODS: First the influence of prefilter settings and edge detection parameters is investigated in a test image and a real ultrasound image. Then, local standard deviation is used to identify or fewer homogeneous regions that are filtered with course resolution, while areas with larger deviation that grey level transitions occur, which should be preserved using smaller <b>filter</b> <b>sizes</b> to improve edge localisation. RESULTS: Analysis of images with different <b>filter</b> <b>sizes</b> indicated that areas are merged for increasing filter sizes: less pronounced edges disappear or displace for larger filters. Two scales of resolution lead to an improved localisation of edges when smaller <b>filter</b> <b>sizes</b> are used in areas with an increased local standard deviation. CONCLUSIONS: This paper illustrates an edge detection method suitable as pre-processing step in interpretation of medical images. By adapting input parameters to signal information, object recognition can be applied in images from different imaging modalities. Also, disadvantages are discussed, resulting in a new application combining a localisation algorithm to find the initial contour and a delineation algorithm to improve the outlining of the resulting contou...|$|R
5000|$|D = Doppler <b>filter</b> <b>size</b> (transmit pulses in each Fast Fourier transform) ...|$|E
5000|$|Vivitar RL edition 28-80mm 1:3.5-4.5 macro {{focusing}} zoom MC, 62mm <b>filter</b> <b>size</b> (made by Kobori), (also in RL Edition) ...|$|E
50|$|A CW radar {{is said to}} {{be matched}} when the {{receiver}} <b>filter</b> <b>size</b> matches the RMS bandwidth of the FM noise on the transmit signal.|$|E
2500|$|Smoothing the Wigner {{distribution}} {{through a}} <b>filter</b> of <b>size</b> larger than [...] (e.g., convolving with a ...|$|R
30|$|With image sized as M[*]×[*]N and <b>filter</b> <b>sized</b> as P[*]×[*]Q, MNPQ {{times of}} {{multiplying}} and adding operations {{will be made}} under no-separation condition; MNP times and MNQ times {{of operations for the}} first/second time respectively, namely MN(P[*]+[*]Q) times of operations, increasing operation speed by PQ/(P[*]+[*]Q) times. By adoption of 3 [*]×[*] 3 filter, the detection speed upon convolution separation will be increased by 1.5 times.|$|R
3000|$|... r, are too large, {{there may}} be a {{dramatic}} decrease in the average classification accuracy that may over-smooth the maps. So, those small-scale objects may be misclassified. Likewise, a very small <b>filtering</b> <b>size</b> or blur degree is also unsatisfactory for the proposed method since it connotes that only very limited local spatial information is used in the filtering process. Hereafter, the optimal values of σ [...]...|$|R
5000|$|... where [...] is the <b>filter</b> <b>size</b> {{parameter}} and [...] is {{the floor}} function. The bounds of this sum {{are such that}} the kernel is zero outside of them.|$|E
5000|$|... #Caption: The false {{positive}} probability [...] {{as a function}} of number of elements [...] in the filter and the <b>filter</b> <b>size</b> [...] An optimal number of hash functions [...] has been assumed.|$|E
50|$|The first {{zoom lens}} for still cameras was the Voigtländer-Zoomar 36-82mm f/2.8 (USA/West Germany) of 1959, for Voigtländer Bessamatic series (1959, West Germany) 35mm leaf shutter SLRs. It was {{designed}} by Zoomar in the United States and manufactured by Kilfitt in West Germany for Voigtländer. The Zoomar 36-82 was very large and heavy for the focal length - 95mm <b>filter</b> <b>size.</b>|$|E
40|$|Rectified linear units (ReLU) are {{well-known}} to {{be helpful}} in obtaining faster convergence and thus higher performance for many deep-learning-based applications. However, networks with ReLU tend to perform poorly {{when the number of}} filter parameters is constrained to a small number. To overcome it, in this paper, we propose a novel network utilizing maxout units (MU), and show its effectiveness on super-resolution (SR) applications. In general, the MU has been known to make the <b>filter</b> <b>sizes</b> doubled in generating the feature maps of the same sizes in classification problems. In this paper, we first reveal that the MU can even make the <b>filter</b> <b>sizes</b> halved in restoration problems thus leading to compaction of the network sizes. To show this, our SR network is designed without increasing the <b>filter</b> <b>sizes</b> with MU, which outperforms {{the state of the art}} SR methods with a smaller number of filter parameters. To the best of our knowledge, we are the first to incorporate MU into SR applications and show promising performance results. In MU, feature maps from a previous convolutional layer are divided into two parts along channels, which are then compared element-wise and only their max values are passed to a next layer. Along with some interesting properties of MU to be analyzed, we further investigate other variants of MU and their effects. In addition, while ReLU have a trouble for learning in networks with a very small number of convolutional filter parameters, MU do not. For SR applications, our MU-based network reconstructs high-resolution images with comparable quality compared to previous deep-learning-based SR methods, with lower filter parameters. Comment: 10 page...|$|R
40|$|This paper {{deals with}} the EMI (Electromagnetic Interference) issue related to the power {{electronic}} converters of a hybrid vehicle. The effects of different configurations of the power supply system that feeds all the power conversion units are investigated {{in order to minimize}} the overall conducted EMI. In particular, two different power supply distribution networks (centralized and distributed) and related EMI filters architectures have been analyzed and compared in terms of conducted emissions and EMI <b>filter</b> <b>sizes</b> and efficiency...|$|R
40|$|This paper {{presents}} an adaptive video denoising technique {{based on the}} intersection of confidence intervals (ICI) rule used for adaptive <b>filter</b> support <b>size</b> calculation. The method is applied to three real-life video signals and its denoising performance is compared to a fixed <b>size</b> <b>filter</b> support based method resulting in a significant estimation error reduction {{in terms of the}} average frame peak signal-to-noise ratio (PSNR) improvement. The average frame PSNR obtained by using the here presented ICI based video denoising method is increased by up to 14. 64 dB and by up to 23. 74 dB when compared to the fixed <b>size</b> <b>filter</b> support based method. Furthermore, unlike the fixed <b>size</b> <b>filter</b> support based method, the adaptive ICI based method is shown to be efficient in a moving object edge preserving, while avoiding its blurring. The method performs well for both video signals obtained by recording stationary scenes, and video signals of moving objects, which are far more often encountered in practical applications, whereas the fixed <b>size</b> <b>filter</b> support based method is limited only to video signals of stationary scenes...|$|R
50|$|The Nikon AF-S 17-55mm f/2.8 G IF-ED DX was {{announced}} in 2004. It is a F-mount professional zoom lens with a constant fast aperture of f/2.8 designed for Nikon DX digital SLR cameras. Nikon have incorporated their Silent Wave Motor for silent auto focusing. The lens is made mostly of metal and the rubber sealed rearmount makes the lens partially dust and water resistant. Dimensions of the lens are 85.5 x 110.5mm. The <b>filter</b> <b>size</b> is 77mm and the lens weighs 755g.|$|E
5000|$|Another {{simple way}} to prevent {{overfitting}} is {{to limit the number}} of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the <b>filter</b> <b>size</b> also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a [...] "zero norm".|$|E
50|$|The Nikon 24-70mm f/2.8G ED AF-S lens was {{announced}} in 2007 by Nikon, in Japan. It is a dust- and moisture-resistant professional wide-angle to short-telephoto high-performance zoom, featuring internal focusing with constant aperture of f/2.8 throughout the focal range, closest focusing distance of 0.38 m/0.9 ft with a <b>filter</b> <b>size</b> of 77 mm and a Silent Wave Motor for quiet auto-focusing. It is not a small or light lens - its dimensions are 83 x 133 mm and it weighs in at approximately 900 g.|$|E
30|$|Un{{filtered}} and filtered 3  ml syringes with 23 G needles were used. The <b>filter</b> pore <b>size</b> of the <b>filtered</b> needles was 5  μm.|$|R
30|$|In general, {{it is very}} {{difficult}} to find a single scale of smoothing which is optimal for all the edges in an image. One smoothing scale may keep good localization while giving detections sensitive to noise. Thus, multiscale edge detection is introduced as an alternative. In this approach, edge detectors with different <b>filter</b> <b>sizes</b> are applied to the image to extract edge maps at different smoothing scales. This information is then combined to result in a more complete final edge image.|$|R
50|$|The box <b>filter</b> of <b>size</b> 9×9 is an {{approximation}} of a Gaussian with σ=1.2 and represents {{the lowest level}} (highest spatial resolution) for blob-response maps.|$|R
5000|$|When {{compared}} to a full frame camera lens providing a similar angle of view, rather than weighing a few kilograms (several pounds) and generally having a length of over 60 cm (2 ft) end to end, the optically stabilized Panasonic Lumix G Vario 100-300 mm lens weighs just 520 grams (18.3 oz), is only 126 mm (5.0 in) long, and uses a relatively petite 67 mm <b>filter</b> <b>size.</b> [...] As a point of comparison, the Nikon 600 mm f5.6 telephoto weighs 3600 grams (7.9 lb), is 516.5 mm (20.3 in) in length and uses a custom 122 mm filter.|$|E
50|$|When sold with a Leitz Minolta CL, {{the lenses}} were called Minolta M-Rokkor 40mm f:2 (later just Minolta M-Rokkor 40mm f:2) and Minolta M-Rokkor 90mm f:4. It {{is said that}} the 40mm was made in Japan by Minolta while the 90mm was made by Leitz and is rare. With the later Minolta CLE, Minolta would produce lenses {{of the same name}} but with a {{different}} coupling system, the same as the Leica M lenses. A new Minolta M-Rokkor 28mm f:2.8 lens was introduced as well. All these lenses can be mounted on the CL too. Rokkor-branded lenses for the CL and CLE take the more easily found 40.5mm <b>filter</b> <b>size.</b>|$|E
50|$|Besides Depth Filtration, {{a number}} of {{membrane}} filtration methods are also used for different industrial applications such as Reverse Osmosis, nano-filtration and Microfiltration. The aforementioned processes operate under the same principle, by rejecting contaminants larger than the <b>filter</b> <b>size.</b> The main distinguishing feature amongst them is their effective pore size. For example, Microfiltration operates by allowing large particles {{to pass through the}} filter media, whilst Reverse Osmosis rejects all the particles except very small species.Most membrane filters can be utilized for final filtration whilst depth filters tend to be more effective when used in clarifying applications, hence {{a combination of the two}} processes can provide a suitable filtration system, which can be adapted to many applications.|$|E
30|$|We focused {{our study}} on two {{simulation}} regimes: large-scale distribution, simulated in boxes of size 500 Mpc, and small-scale distribution, with boxes of size 100 Mpc. For both configurations we ran 10 independent simulations. From these boxes, we {{cut out a}} total of 15, 000 thin, 2 D slices for each box size. We design a GAN model where both the discriminator and generator are deep convolutional neural networks. These networks consists of 5 layers, with 4 convolutional layers using <b>filter</b> <b>sizes</b> of 5 × 5 pixels.|$|R
40|$|In this paper, {{we propose}} a {{performance}} evaluation of orientation field smoothing {{according to the}} application of two different filters; average filter and Gaussian filters. Each is investigated at different sizes. The finding is that by applying the <b>filters</b> <b>size</b> of 2 - 4 times of ridges spacing, the orientation field can be smoothed with less distortion. Gaussian filter seems to offer slightly better performance compared to an average one. Success in core point detection using Poincare technique is {{used to measure the}} filtering performance...|$|R
40|$|Replacement of one module of {{the battery}} charge {{discharge}} unit (BCDU) of the International Space Station (ISS) by a {{flywheel energy storage}} unit (FESU) is under consideration. Integration of these two dissimilar systems is likely to surface difficulties in areas of system stability and fault protection. Other issues {{that need to be}} addressed include flywheel charge and discharge profiles and their effect on the ISS power system as well as <b>filter</b> <b>sizing</b> for power Ability purposes. This paper describes a SABER based simulation to study these issues...|$|R
