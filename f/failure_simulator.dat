5|17|Public
40|$|A {{real-time}} Sensor <b>Failure</b> <b>Simulator</b> (SFS) {{was designed}} and assembled for the Advanced Detection, Isolation, and Accommodation (ADIA) program. Various designs were considered. The design chosen features an IBM-PC/XT. The PC is used to drive analog circuitry for simulating sensor failures in real-time. A user defined scenario describes the failure simulation {{for each of the}} five incoming sensor signals. Capabilities exist for editing, saving, and retrieving the failure scenarios. The SFS has been tested closed-loop with the Controls Interface and Monitoring (CIM) unit, the ADIA control, and a real-time F 100 hybrid simulation. From a productivity viewpoint, the menu driven user interface has proven to be efficient and easy to use. From a real-time viewpoint, the software controlling the simulation loop executes at greater than 100 cycles/sec...|$|E
40|$|An {{effective}} way to suppress the cascading failure risk is the branch capacity upgrade, whose optimal decision making, however, may incur high computational burden. A practical way {{is to find out}} some critical branches as the candidates in advance. This paper proposes a simulation data oriented approach to identify the critical branches with higher importance in cascading failure propagation. First, a concept of cascading failure chain (CFC) is introduced and numerous samples of CFC are generated with an AC power flow based cascading <b>failure</b> <b>simulator.</b> Then, a directed weighted graph is constructed, whose edges denotes the severities of branch interactions. Third, the weighted hypertext-induced topic search (HITS) algorithm is used to rate and rank this graph's vertices,through which the critical branches can be identified accordingly. Validations on IEEE 118 bus and RTS 96 systems show that the proposed approach can identify critical branches whose capacity upgrades suppress cascading failure risk more greatly. Moreover, it is also shown that structural importance of a branch does not agree with its importance in cascading failure, which indicates the effectiveness of the proposed approach compared with structure vulnerabilities based identifying methods...|$|E
40|$|International audienceElectrical power {{transmission}} networks of {{many developed countries}} are undergoing deep transformations aimed at (i) facing the challenge offered by the stochastically fluctuating power contributions due to the continuously growing connections of renewable power generating units and (ii) decreasing their vulnerability to failures or malicious attacks and improving their resilience, {{in order to provide}} more reliable services, thus increasing both safety and profits. In this complex context, one of the major concerns is that related to the potentially catastrophic consequences of cascading failures triggered by rare and difficult to predict extreme weather events. In this work, we originally propose to combine an extreme weather stochastic model of literature to a realistic cascading <b>failure</b> <b>simulator</b> based on a direct current (DC) power flow approximation and a proportional re-dispatch strategy. The description of the dynamics of the network is completed by the introduction of a novel restoration model accounting for the operating conditions that a repair crew may encounter during an extreme weather event. The resulting model is solved by a customized sequential Monte Carlo scheme in order to quantify the impact of extreme weather events on the reliability/availability performances of the power grid. The approach is demonstrated with reference to the test case of the IEEE 14 {{power transmission}} networ...|$|E
5000|$|Different {{types of}} testing suggest {{different}} types of test environments, {{some or all of}} which may be virtualized to allow rapid, parallel testing to take place. For example, automated user interface tests may occur across several virtual operating systems and displays (real or virtual). Performance tests may require a normalized physical baseline hardware configuration, so that performance test results can be compared over time. Availability or durability testing may depend on <b>failure</b> <b>simulators</b> in virtual hardware and virtual networks.|$|R
40|$|A {{life cycle}} cost {{analysis}} of high efficiency cells was presented. Although high efficiency cells produce more power, they also cost more to make and {{are more susceptible to}} array hot-spot heating. Three different computer analysis programs were used: SAMICS (solar array manufacturing industry costing standards), PVARRAY (an array <b>failure</b> mode/degradation <b>simulator),</b> and LCP (lifetime cost and performance). The high efficiency cell modules were found to be more economical in this study, but parallel redundancy is recommended...|$|R
40|$|Abstract—When {{the modern}} {{electrical}} infrastructure is under-going a {{migration to the}} Smart Grid, vulnerability and security concerns have also been raised regarding the cascading failure threats in this interconnected transmission system with complex communication and control challenge. The DC power flow-based model has been a popular model to study the cascading failure problem due to its efficiency, simplicity and scalability in simula-tions of such failures. However, due to the complex nature of the power system and cascading failures, the underlying assumptions in DC power flow-based cascading <b>failure</b> <b>simulators</b> (CFS) may fail to hold during the development of cascading failures. This paper compares the validity of a typical DC power flow-based CFS in cascading failure analysis with a new numerical metric defined as the critical moment (CM). The adopted CFS is first implemented to simulate system behavior after initial contingen-cies and to evaluate the utility of DC-CFS in cascading failure analysis. Then the DC-CFS is compared against another classic, more precise power system stability methodology, i. e., the tran-sient stability analysis (TSA). The CM is introduced with a case study to assess the utilization of these two models for cascading failure analysis. Comparative simulations on the IEEE 39 -bus and 68 -bus benchmark reveal important consistency and discrepancy between these two approaches. Some suggestions are provided for using these two models in the power grid cascading failure analysis. Index Terms—Cascading failure, contingency analysis, DC power flow, transient stability, vulnerability assessment. I...|$|R
40|$|Abstract — The {{security}} issue of complex networks has drawn significant concerns recently. While pure topological analyzes from a network security perspective provide some effective techniques, {{their inability to}} characterize the physical principles requires a more comprehensive model to approximate failure behavior of a complex network in reality. In this paper, based on an extended topological metric, we proposed an approach to examine the vulnerability of {{a specific type of}} complex network, i. e., the power system, against cascading failure threats. The proposed approach adopts a model called extended betweenness that combines network structure with electrical characteristics to define the load of power grid components. By using this power transfer distribution factor-based model, we simulated attacks on different components (buses and branches) in the grid and evaluated the vulnerability of the system components with an extended topological cascading <b>failure</b> <b>simulator.</b> Influence of different loading and overloading situations on cascading failures was also evaluated by testing different tolerance factors. Simulation results from a standard IEEE 118 -bus test system revealed the vulnerability of network components, which was then validated on a dc power flow simulator with comparisons to other topological measurements. Finally, potential extensions of the approach were also discussed to exhibit both utility and challenge in more complex scenarios and applications. Index Terms — Complex network security, cascading failure, structural vulnerability, extended topological analysis. I...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedIt has become apparent {{that in order}} for an AUV to be a more reliable self-sufficient system, it must have on-line failure detection and resolution capability. In support of this the AUV must have reconfigurable systems so as {{to be able to take}} corrective action against resolvable <b>failures.</b> A <b>simulator</b> has been designed using SIMULINK in order to analyze failure modes associated with the NPS Phoenix AUV steering system. The analyses of these failure modes have been used to identify possible signals for steering system fault detection. Finally, a rule based algorithm was developed which can be converted into a format that ultimately could be implemented in a fuzzy logic set, for later insertion into the Phoenix tactical level software. This methodology will be applied to the Navy's UUV[URL] United States Nav...|$|R
40|$|This paper {{describes}} a fault-tolerant sliding-mode control allocation scheme capable {{of coping with}} the loss of all control surfaces resulting from a failure of the hydraulics system, during which time the scheme only uses the engines to control the aircraft. The paper presents tests of the scheme implemented on a six-degree-of-freedom motion research flight simulator at Delft University of Technology, using a realistic maneuver involving an emergency return to a near-landing condition on a runway in response to the <b>failure.</b> The <b>simulator</b> results show that not only does the controller provide high tracking performance during nominal fault-free conditions, this performance is also maintained after the total loss of all control surfaces. This shows the capability of the proposed sliding-mode control allocation scheme to achieve and maintain desired performance levels using only propulsion by redistributing the control signals to the engines when failures occur...|$|R
40|$|The design, implementation, and {{verification}} {{of the flight}} control software used in the F- 8 DFBW program are discussed. Since the DFBW utilizes an Apollo computer and hardware, the procedures, controls, and basic management techniques employed are based on those developed for the Apollo software system. Program assembly control, simulator configuration control, erasable-memory load generation, change procedures and anomaly reporting are discussed. The primary verification tools are described, {{as well as the}} program test plans and their implementation on the various <b>simulators.</b> <b>Failure</b> effects analysis and the creation of special failure generating software for testing purposes are described...|$|R
50|$|Nepalese {{authorities}} {{found that}} the probable causes of the accident were the captain's and controller's loss of situational awareness; language and technical problems caused the captain to experience frustration and a high workload; the first officer's lack of initiative and inconclusive answers to the captain's questions; the air traffic controller's inexperience, poor grasp of English, and reluctance to interfere with {{what he saw as}} piloting matters such as terrain separation; poor supervision of the inexperienced air traffic controller; Thai Airways International's <b>failure</b> to provide <b>simulator</b> training for the complex Kathmandu approach to its pilots; and improper use of the aircraft's flight management system.|$|R
40|$|There {{the study}} objects are the {{indicators}} {{and the methods}} to estimate the maintainability, the gear box of Kirovets tractors. The technique of high-rate estimation of maintainability indicators {{on the basis of}} analytical-tabular simulation of clearing process of <b>failures,</b> the analytical-tabular <b>simulator</b> of clearing process of failures, which allows to simulate the execution of disassembling-assembling works connecting with the access to the particular failed part, the algorithm of program to design by personal computer the change time of parts have been developed. The technique of high-rate estimation of maintainability indicators has been introduced. The introduction efficiency is 4 times decrease of estimation time of maintability indicatorsAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|This paper {{presents}} {{the importance of}} recruitment, properly functioning succession plans and individual development plans; how required training needs are diagnosed; and how a combined simulator and training program is being developed to improve Dynamic Positioning Operator (DPO) understanding of the operation of these complex integrated systems. All are critical to safety, efficiency, and therefore, productivity and profitability. An advanced simulator will {{have the ability to}} present the DPO {{with a wide range of}} failure modes that cannot easily be duplicated on an operating vessel, but which have caused loss of location numerous times. These failures will be different from the types now used, akin to a thruster or engine tripping off line. They will allow the operator to ascertain that a thruster or engine is about to trip off line by recognizing some precursor to the <b>failure.</b> This same <b>simulator</b> can be used to effectively measure the skill level of DPOs, regardless of experience level...|$|R
40|$|International audienceRecently, the {{benefits}} of co-scheduling several applications have been demonstrated in a fault-free context, {{both in terms of}} performance and energy savings. However, large-scale computer systems are confronted by frequent failures, and resilience techniques must be employed for large applications to execute efficiently. Indeed, failures may create severe imbalance between applications, and significantly degrade performance. In this paper, we aim at minimizing the expected completion time of a set of co-scheduled applications. We propose to redistribute the resources assigned to each application upon the occurrence of failures, and upon the completion of some applications, in order to achieve this goal. First, we introduce a formal model and establish complexity results. The problem is NP-complete for malleable applications, even in a fault-free context. Therefore, we design polynomial-time heuristics that perform redistributions and account for processor <b>failures.</b> A fault <b>simulator</b> is used to perform extensive simulations that demonstrate the usefulness of redistribution and the performance of the proposed heuristics...|$|R
40|$|Low-power {{wireless}} technology enables numerous applications in areas from environmental monitoring and smart cities, to healthcare and recycling. But resource-constraints and the distributed nature of applications make low-power wireless networks difficult {{to develop and}} understand, resulting in increased development time, poor performance, software bugs, or even network <b>failures.</b> Network <b>simulators</b> offer full non-intrusive visibility and control, and are indispensible tools during development. But simulators do not always adequately represent the real world, limiting their applicability. In this thesis I argue that high simulation timing accuracy is important when developing high-performance low-power wireless protocols. Unlike in generic wireless network simulation, timing becomes important since low-power wireless networks use extremely timing-sensitive software techniques such as radio duty-cycling. I develop the simulation environment Cooja that can simulate low-power wireless networks with high timing accuracy. Using timing-accurate simulation, I design and develop a set of new low-power wireless protocols that improve on throughput, latency, and energy-efficiency. The problems that motivate these protocols were revealed by timing-accurate simulation. Timing-accurate software execution exposed performance bottlenecks that I address with a new communication primitive called Conditional Immediate Transmission (CIT). I show that CIT can improve on throughput in bulk transfer scenarios, and lower latency in many-to-one convergecast networks. Timing-accurate communication exposed that the hidden terminal problem is aggravated in duty-cycled networks that experience traffic bursts. I propose the Strawman mechanism that makes a radio duty-cycled network robust against traffic bursts by efficiently coping with hidden terminals. The Cooja simulation environment is available for use by others and is the default simulator in the Contiki operating system since 2006. ContikiWISENET (NES) Promo...|$|R
40|$|Abstract-Privacy {{is needed}} in ad hoc networks. An ad hoc on-demand position-based private routing algorithm, called A 02 P, is {{proposed}} for communication anonymity. Only {{the position of the}} destination is exposed in the network for route discovery. To discover routes with the limited routing information, a receiver contention scheme is designed for determining the next hop. pseudo identifiers are used for data packet delivery after a route is established. Real identities (IDS) for the source nodes, the destination nodes, and the forwarding nodes in the end-to-end connections are kept private. Anonymity for a destination relies on the difficulty of matching a geographic position to a real node ID. This can be enforced by the use of secure position service systems. Node mobility enhances destination anonymity by making the match of a node ID with a position momentary. To further improve destination privacy, R-A 02 P is proposed. In this protocol, the position of a reference point, instead of the position of the destination, is used for route discovery. Analytical models are developed for evaluating the delay in route discovery and the probability of route discovery <b>failure.</b> A <b>simulator</b> based on 77 s- 2 is developed for evaluating network throughput. Analysis and simulation results show that, while A 02 P preserves communication privacy in ad hoc networks, its routing performance is comparable with other position-based routing algorithms. Index Terms-Ad hoc routing protocol, anonymity, communication privacy, channel access mechanism ROTECT~NG personal privacy is a prime concern for the P emerging pervasive systems. As an important part of privacy, the user anonymity can improve security b...|$|R
40|$|Objective: Teenage {{passengers}} affect teenage driving performance, {{possibly by}} social influence. To examine {{the effect of}} social norms on driving behavior, male teenagers were randomly assigned to drive in a simulator with a peer-aged confederate to whom participants were primed to attribute either risk-accepting or risk-averse social norms. It was hypothesized that teenage drivers would engage in more risky driving behavior in the presence of peer passengers than no passengers, and with a risk-accepting compared with a risk-averse passenger. Method: 66 male participants aged 16 to 18 years holding a provisional driver license were randomized to drive with a risk-accepting or risk-averse passenger in a <b>simulator.</b> <b>Failure</b> to Stop at a red light and percent Time in Red (light) were measured as primary risk-relevant outcomes of interest at 18 intersections, while driving once alone and once with their assigned passenger. Results: The effect of passenger presence on risky driving was moderated by passenger type for Failed to Stop in a generalized linear mixed model (OR = 1. 84, 95...|$|R
40|$|This paper firstly {{summarizes}} {{and defines}} data grid models {{and the process}} of job scheduling, simultaneously analyzes the time and cost of job execution in the data grid. The job scheduling strategy influences the QoS of the data grid immediately and then proposes a design proposal of the job scheduling simulator of data grid based on a grid simulator named GridSim and introduces the architecture, process and key technologies of the job scheduling simulator. By using this simulator we can determine the resource failure so that the performance of the data grid can be analysed. This involves implementation of job scheduling in a grid for handling resource <b>failure</b> using GridSim <b>simulator.</b> Moreover, the resources may suffer failures, and all of this obviously would affect the performance received by the users. In this paper we present an extension {{to one of the most}} popular grid simulators GridSim to support variable resource availability. Through which the overall performance can be determined. Finally, it proves that this scheduling simulator can satisfy the need of research on the data grid optimization in grid computing...|$|R
40|$|Early fault {{detection}} with data-analysis tools in {{nuclear power plants}} {{is one of the}} main goals in NoTeS-project (test case 4) in TEKES technology program MASI. The industrial partner in this project is Teollisuuden Voima Oy, Olkiluoto nuclear power plant. Data analysis is carried out with real <b>failure</b> data, training <b>simulator</b> data and design based data, such as data from isolation valve experiments. A control room tool, visualization tools and various visualizations are under development. A toolbox for data management using PCA (Principal Component Analysis) and WRLS (Weighted Recursive Least Squares) methods has been developed [1]. Visualizations for e. g. trends, transients, and variation index to detect leakages are used. Statistically significant variables of the system are detected and statistical properties and important visualizations are reported. Data mining methods and time series modelling are combined to detect abnormal events. X-detector tool based on feature subset selection has been developed. The idea is to do real-time monitoring and abnormality detection with efficient subsets. Measuring dependencies and cluster separation methods are used in variable selection in this visualization tool. U−matrix, t = 6 minutes Normal operation colorbar Steamline 1 steamflo...|$|R
40|$|We study fin {{failures}} on a torpedo-shaped unmanned {{underwater vehicle}} (UUV) and design control laws {{to compensate for}} a fin that gets stuck at an arbitrary deflection angle. Our control strategy combines sliding mode control with modified fin mixing and a method for handling fin deflection saturation. The saturationcompensating control design follows recent work of Teel and Kapoor and has the advantage {{that it is only}} active when there is saturation. This allows the nominal controller to be designed independently for desired performance under non-saturation conditions. Our complete controller has been tested on the simulator for the 21 UUV, a full-scale, 21 -inch diameter, torpedo-shaped UUV. We describe the results of these tests which demonstrate good tolerance to a fin failure. As our objective is the design of failure compensation, we assume in our work that we have available an identification package that provides fin <b>failure</b> information. Our <b>simulator</b> test results suggest, however, that only very little information about the failure is required for effective compensation. 1 Introduction The 21 UUV, built and operated by the Naval Undersea Warfare Center (NUWC) in Rhode Island, is an unmanned underwater vehicle (UUV) representative of a class of UUV designed to have the shape and size of a torpedo. This design allows for launching of the UUV from a submarine. The 21 UUV is controlled using four independent fins and a propeller at its tail. Looked at from the rear, the four fins form an &quot;X &quot; configuration...|$|R

