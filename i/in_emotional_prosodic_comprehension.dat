0|10000|Public
40|$|International audienceBACKGROUND The {{recognition}} of the emotion expressed during conversation relies on the integration of both semantic processing and decoding of emotional prosody. The integration of both types of elements is necessary for social interaction. No study has investigated how these processes are impaired in patients with schizophrenia during the comprehension of an emotional speech. Since patients with schizophrenia have difficulty in daily interactions, it would be {{of great interest to}} investigate how these processes are impaired. We tested the hypothesis that patients present lesser performances regarding both semantic and <b>emotional</b> <b>prosodic</b> processes during <b>emotional</b> speech comprehension compared with healthy participants. METHODS The paradigm is based on sentences built with emotional (anger, happiness, or sadness) semantic content uttered with or without congruent emotional prosody. The study participants had to decide with which of the emotional categories each sentence corresponded. RESULTS Patients performed significantly worse than their matched controls, even in the presence of emotional prosody, showing that their ability to understand emotional semantic content was impaired. Although prosody improved performances in both groups, it benefited the patients more than the controls. CONCLUSION Patients exhibited both impaired semantic and <b>emotional</b> <b>prosodic</b> <b>comprehensions.</b> However, they took greater advantage of emotional prosody adjunction than healthy participants. Consequently, focusing on emotional prosody during carrying may improve social communication...|$|R
40|$|Deficits <b>in</b> <b>emotional</b> <b>prosodic</b> processing, the {{expression}} of emotions in voice, have been widely reported in patients with schizophrenia, not only <b>in</b> comprehending <b>emotional</b> prosody but also expressing it. Given that prosodic cues are important in memory for voice and speaker identity, Cutting has proposed that prosodic deficits {{may contribute to the}} misattribution that appears to occur in auditory hallucinations in psychosis. The present study compared hallucinating patients with schizophrenia, non-hallucinating patients and normal controls on an <b>emotional</b> <b>prosodic</b> processing task. It was hypothesised that hallucinators would demonstrate greater deficits <b>in</b> <b>emotional</b> <b>prosodic</b> processing than non-hallucinators and normal controls. Participants were 67 patients with a diagnosis of schizophrenia or schizoaffective disorder (hallucinating = 38, non-hallucinating = 29) and 31 normal controls. The prosodic processing task used in this study comprised a series of semantically neutral sentences expressed in happy, sad and neutral voices which were rated on a 7 -point Likert scale from sad (- 3) through neutral (0) to happy (+ 3). Significant deficits in the prosodic processing tasks were found in hallucinating patients compared to non-hallucinating patients and normal controls. No significant differences were observed between non-hallucinating patients and normal controls. In the present study, patients experiencing auditory hallucinations were not as successful in recognising and using prosodic cues as the non-hallucinating patients. These results are consistent with Cutting's hypothesis, that prosodic dysfunction may mediate the misattribution of auditory hallucinations...|$|R
40|$|Previous {{evidence}} supports differential event-related brain potential (ERP) responses for <b>emotional</b> <b>prosodic</b> processing and integrative emotional prosodic/semantic processing. While latter process elicits a negativity {{similar to the}} well-known N 400 component, transitions <b>in</b> <b>emotional</b> <b>prosodic</b> processing elicit a positivity. To further substantiate this evidence, the current investigation utilized lexical-sentences and sentences without lexical content (pseudo-sentences) spoken in six basic emotions by a female and a male speaker. Results indicate that <b>emotional</b> <b>prosodic</b> expectancy violations elicit a right-lateralized positive-going ERP component independent of basic emotional prosodies and speaker voice. In addition, expectancy violations of integrative emotional prosody/semantics elicit a negativity with a whole-head distribution. The current results nicely complement previous evidence, and extend the results by showing the respective effects for {{a wider range of}} emotional prosodies independent of lexical content and speaker voice. © 2007 Elsevier Inc. All rights reserved...|$|R
40|$|To {{communicate}} emotionally entails that {{a listener}} understands a verbal message {{but also the}} emotional prosody going along with it. So far the time course and interaction of these emotional 'channels' is still poorly understood. The current set of event-related brain potential (ERP) experiments investigated both the interactive time course of emotional prosody with semantics and of emotional prosody independent of emotional semantics using a cross-splicing method. In a probe verification task (Experiment 1) prosodic expectancy violations elicited a positivity, while a combined prosodic-semantic expectancy violation elicited a negativity. Comparable ERP results were obtained <b>in</b> an <b>emotional</b> <b>prosodic</b> categorization task (Experiment 2). The present data support different ERP responses with distinct time courses and topographies elicited {{as a function of}} prosodic expectancy and combined prosodic-semantic expectancy during <b>emotional</b> <b>prosodic</b> processing and combined emotional prosody/emotional semantic processing. These differences suggest that the interaction of more than one emotional channel facilitates subtle transitions <b>in</b> an <b>emotional</b> sentence context. © 2007 Elsevier B. V. All rights reserved...|$|R
40|$|The {{basal ganglia}} (BG) have been {{functionally}} linked to emotional processing [Pell, M. D., Leonard, C. L., 2003. Processing emotional tone form speech in Parkinson's Disease: {{a role for}} the basal ganglia. Cogn. Affec. Behav. Neurosci. 3, 275 - 288; Pell, M. D., 2006. Cerebral mechanisms for understanding <b>emotional</b> prosody <b>in</b> speech. Brain Lang. 97 (2), 221 - 234]. However, few studies have tried to specify the precise role of the BG during <b>emotional</b> <b>prosodic</b> processing. Therefore, the current study examined deviance detection in healthy listeners and patients with left focal BG lesions during implicit <b>emotional</b> <b>prosodic</b> processing <b>in</b> an event-related brain potential (ERP) -experiment. In order to compare these ERP responses with explicit judgments of emotional prosody, the same participants were tested in a follow-up recognition task. As previously reported [Kotz, S. A., Paulmann, S., 2007. When emotional prosody and semantics dance cheek to cheek: ERP evidence. Brain Res. 1151, 107 - 118; Paulmann, S. & Kotz, S. A., 2008. An ERP investigation on the temporal dynamics of emotional prosody and <b>emotional</b> semantics <b>in</b> pseudo- and lexical sentence context. Brain Lang. 105, 59 - 69], deviance of prosodic expectancy elicits a right lateralized positive ERP component in healthy listeners. Here we report a similar positive ERP correlate in BG-patients and healthy controls. In contrast, BG-patients are significantly impaired in explicit recognition of emotional prosody when compared to healthy controls. The current data serve as first evidence that focal lesions in left BG do not necessarily affect implicit <b>emotional</b> <b>prosodic</b> processing but evaluative <b>emotional</b> <b>prosodic</b> processes as demonstrated in the recognition task. The {{results suggest that the}} BG may not play a mandatory role <b>in</b> implicit <b>emotional</b> <b>prosodic</b> processing. Rather, executive processes underlying the recognition task may be dysfunctional during <b>emotional</b> <b>prosodic</b> processing. © 2008 Elsevier B. V. All rights reserved...|$|R
40|$|Background. Female {{patients}} with schizophrenia are less impaired in social life than male patients. Because social impairment in schizophrenia {{has been found}} to be associated with deficits in emotion recognition, we examined whether the female advantage <b>in</b> processing <b>emotional</b> prosody and semantics is preserved in schizophrenia. Method. Forty-eight patients (25 males, 23 females) and 46 controls (23 males, 23 females) were assessed using an <b>emotional</b> language task (<b>in</b> which healthy women generally outperform healthy men), consisting of 96 sentences in four conditions: (1) neutral-content/emotional-tone (happy, sad, angry or anxious); (2) neutral-tone/emotional-content; (3) emotional-tone/incongruous emotional-content; and (4) emotional-content/incongruous emotional-tone. Participants had to ignore the emotional-content in the third condition and the emotional-tone in the fourth condition. In addition, participants were assessed with a visuospatial task (in which healthy men typically excel). Correlation coefficients were computed for associations between emotional language data, visuospatial data, IQ measures and patient variables. Results. Overall, on the emotional language task, patients made more errors than control subjects, and women outperformed men across diagnostic groups. Controlling for IQ revealed a significant effect on task performance in all groups, especially in the incongruent tasks. On the rotation task, healthy men outperformed healthy women, but male patients, female patients and female controls obtained similar scores. Conclusions. The advantage <b>in</b> <b>emotional</b> <b>prosodic</b> and semantic processing in healthy women is preserved in schizophrenia, whereas the male advantage in visuospatial processing is lost. These findings may explain, in part, why social functioning is less compromised in women with schizophrenia than in men...|$|R
40|$|Functional imaging {{studies have}} {{demonstrated}} involvement of the anterior temporal cortex in sentence comprehension. It is unclear, however, whether the anterior temporal cortex is essential for this function. We studied two aspects of sentence comprehension, namely syntactic and <b>prosodic</b> <b>comprehension</b> in temporal lobe epilepsy patients who were candidates for resection of the anterior temporal lobe. Methods: Temporal lobe epilepsy patients (n = 32) with normal (left) language dominance were tested on syntactic and <b>prosodic</b> <b>comprehension</b> before and after removal of the anterior temporal cortex. The <b>prosodic</b> <b>comprehension</b> test was also compared with performance of healthy control subjects (n = 47) before surgery. Results: Overall, temporal lobe epilepsy patients {{did not differ from}} healthy controls in syntactic and <b>prosodic</b> <b>comprehension</b> before surgery. They did perform less well on an affective prosody task. Post-operative testing revealed that syntactic and <b>prosodic</b> <b>comprehension</b> did not change after removal of the anterior temporal cortex. Discussion: The unchanged performance on syntactic and <b>prosodic</b> <b>comprehension</b> after removal of the anterior temporal cortex suggests that this area is not indispensable for sentence comprehension functions in temporal epilepsy patients. Potential implications for the postulated role of the anterior temporal lobe in the healthy brain are discussed...|$|R
40|$|Recently, {{research}} on the lateralization of linguistic and nonlinguistic (emotional) prosody has experienced a revival. However, both neuroimaging and patient evidence do not draw a coherent picture substantiating right-hemispheric lateralization of prosody and <b>emotional</b> prosody <b>in</b> particular. The current overview summarizes positions and data on the lateralization of emotion and <b>emotional</b> <b>prosodic</b> processing <b>in</b> the brain and proposes that: (1) the realization of <b>emotional</b> <b>prosodic</b> processing <b>in</b> the brain is based on differentially lateralized subprocesses and (2) methodological factors can influence the lateralization of <b>emotional</b> prosody <b>in</b> neuroimaging in vestigations. Latter evidence reveals that emotional valence effects are strongly right lateralized in studies using compact blocked presentation of <b>emotional</b> stimuli. <b>In</b> contrast, data obtained from event-related studies are indicative of bilateral or left-accented lateralization of <b>emotional</b> <b>prosodic</b> valence. These findings suggest a strong interaction between language and <b>emotional</b> <b>prosodic</b> processing. © 2006 Elsevier B. V. All rights reserved...|$|R
40|$|The {{topic of}} this work is an {{extension}} of our previous research on the development of a general data-driven procedure for creating a neutral "narrative-style" prosodic module for the Italian FESTIVAL Text-To-Speech (TTS) synthesizer, and it is focused on investigating and implementing new strategies for building a new emotional FESTIVAL TTS. The new <b>emotional</b> <b>prosodic</b> modules, similarly to the neutral case, are still based on the "Classification And Regression Tree" (CART) theory. The extension to the emotional speech synthesis is obtained using a differential approach: the <b>emotional</b> <b>prosodic</b> modules learn the differences between the neutral (without emotions) and the <b>emotional</b> <b>prosodic</b> data. Moreover, {{due to the fact that}} Voice Quality (VQ) is known to play an important role in emotive speech, a rule-based FESTIVAL-MBROLA VQ-modification module, for control of temporal and spectral characteristics of the synthesis, has also been implemented. Even if emotional synthesis still remains an attractive open issue, our preliminary evaluation results underline the effectiveness of the proposed solution...|$|R
40|$|Emotional {{encoding}} {{is central}} to human communication. To this end, {{it is not only}} important to understand the verbal content of the message (i. e. its semantic), but also to understand the emotional prosody that accompanies the message. So far it is yet not fully understood in which way and at which point <b>in</b> time <b>emotional</b> prosody and (emotional) semantics interact at the sentence level. However, previous evidence (Kotz et al., 2001) showed that the time course of emotional prosody and semantics differ. In order to investigate the interaction of the two channels we carried out two ERP experiments in which we tried to isolate the emotional prosody channel from the semantic content channel using a cross-splicing method. The ERP evidence shows that violations of the <b>emotional</b> <b>prosodic</b> contour (with neutral semantic content) elicit a positivity whereas combined <b>emotional</b> <b>prosodic</b> and semantic content violations elicit a negativity, comparable to the N 400 component. This pattern evolves irrespective of the task used, i. e. the same pattern emerges for an “implicit” (probe detection) task as well as for an “explicit” (emotional prosody categorization) task. We conclude that prosodic information can be processed separately from semantic information, and that each channel (prosody and semantic) contributes differently to the perception of emotional speech. Furthermore, it appears that semantic information overrides prosody when the two channels interact in time when the <b>emotional</b> <b>prosodic</b> contour agrees with the semantic content of a sentence...|$|R
40|$|The current efMRI {{experiment}} {{investigated the}} potential right hemisphere dominance of <b>emotional</b> <b>prosodic</b> processing under implicit task demands. Participants evaluated the relative tonal height (high, medium, low) of intelligible and unintelligible sentences spoken by a trained female speaker of German with three prosodic contours: happy, angry, and neutral. The results confirm the activation of a bilateral fronto-striato-temporal network {{with no clear}} right hemispheric preference for <b>emotional</b> <b>prosodic</b> processing. The data suggest that (1) task demands do not significantly alter lateralization of function in the current context, and (2) fronto-striatal brain areas engage during implicit processing of emotional prosody, thus do no seem to be task specific. 1...|$|R
40|$|Functional imaging {{studies have}} {{demonstrated}} involvement of the anterior temporal cortex in sentence comprehension. It is unclear, however, whether the anterior temporal cortex is essential for this function. We studied two aspects of sentence comprehension, namely syntactic and <b>prosodic</b> <b>comprehension</b> in temporal lobe epilepsy patients who were candidates for resection of the anterior temporal lobe. status: publishe...|$|R
40|$|Sentence prosody is {{long known}} to serve both {{linguistic}} functions (e. g. {{to differentiate between}} questions and statements) and emotional functions (e. g. to detect the emotional state of a speaker). These different functions of prosodic information need to be encoded rapidly during sentence comprehension to ensure successful speech communication. However, systematic investigations of the comparative nature of these two functions, i. e. are the two functions of prosody independent or interdependent, are sparse. The question at hand is whether the two prosodic functions engage a similar neural network and run a similar time-course or not. To this aim we investigated whether emotional and linguistic prosody are processed independently or dependently in an event-related brain potential (ERP) experiment. We merged a prosodically neutral head of a sentence to a second half of a sentence that differed <b>in</b> <b>emotional</b> and/or linguistic prosody. In a within-subjects design, two tasks were administered: in the " emotion task", participants judged whether the sentence that they had just heard was spoken in a neutral tone of voice or not (<b>emotional</b> task); <b>in</b> the " linguistic task", participants decided whether the sentence was a declarative sentence or not. As predicted, the previously reported prosodic expectancy positivity (PEP) was elicited by linguistic and <b>emotional</b> <b>prosodic</b> expectancy violations. However, the latency and distribution of the ERP component differed: whilst responses to <b>emotional</b> <b>prosodic</b> expectancy violations were elicited shortly after an expectancy violation (∼ 470. ms post splicing-point) and most prominently at posterior electrode-sites, the positivity in response to linguistic prosody had a later onset (∼ 620. ms post splicing-point) with a more frontal distribution. Interestingly, responses to combined (linguistic and emotional) expectancy violations resulted in a broadly distributed positivity with an onset of ∼ 170. ms post expectancy violation. These effects were found irrespective of the task setting. Given the differences in latency and distribution, we conclude that the processing of emotional and linguistic prosody relies at least partly on differing neural mechanisms and that <b>emotional</b> <b>prosodic</b> aspects of language are processed in a prioritized processing stream. © 2012 Elsevier Ltd...|$|R
40|$|Emotional {{encoding}} {{is central}} to human communication. Among others, emotional states are communicated by evaluating affective prosody and the accompanying semantics (i. e. the verbal content of the message). Obviously, the two channels do have to interact {{in order to be}} able to evaluate the emotional state of a message (or the messenger). However, so far it is yet not fully understood how and at which point <b>in</b> time <b>emotional</b> prosody and (emotional) semantics interact at the sentence level. Previous evidence (Kotz et. al, 2001 & Kotz & Paulmann, in prep.) has shown that the time course of the <b>emotional</b> <b>prosodic</b> processing and semantics differ. In order to further investigate the two channels we carried out two ERP experiments in which we tried to isolate the emotional prosody channel from the semantic content channel using a cross-splicing method. The ERP evidence shows that violations of the <b>emotional</b> <b>prosodic</b> contour (with neutral semantic content in Experiment 1 and pseudo-sentences used in Experiment 2) mainly elicit a positivity whereas violations of the semantic content mainly elicit an N 400 -like negativity. This pattern seems to evolve independet of speaker voice (male or female). Furthermore, it appears that this pattern accounts for all basic emotions (fear, sad, anger, disgust, happpiness, pleasant surprise and neutral serving as a baseline) investigated in these experiments. Only the results for happiness show deviant ERP results. We conclude that prosodic information is processed differently from semantic information, and that each channel (prosody and semantic) contributes individually to the perception of emotional speech. Furthermore, the data suggest that semantic information can override prosody when the two channels interact in time, that is when the <b>emotional</b> <b>prosodic</b> contour agrees with the semantic content of a sentence...|$|R
40|$|Behavioral {{studies have}} {{reported}} recognition deficits for <b>emotional</b> prosody <b>in</b> adults with Parkinson’s disease, implying a role for the basal ganglia (BG) in these functions (Breitenstein et al., 2001; Pell & Leonard, 2003). However, previous results do not allow underlying mechanisms substantiating emotional prosody {{to be separated from}} process-correlated task effects as reflected in behavioral responses. Therefore, the current study investigated <b>emotional</b> prosody processing <b>in</b> an on-line ERP-experiment using an implicit task. We tested <b>emotional</b> prosody perception <b>in</b> BG-lesion patients using vocal expressions (with and without lexical content) of anger, fear, disgust and happiness compared to a neutral baseline. In addition, the interaction between emotional prosody and emotional semantics was investigated. Using a cross-splicing method, violations of a combined <b>emotional</b> <b>prosodic</b> and <b>emotional</b> semantic contour were created. Results show that early <b>in</b> <b>emotional</b> prosody processing, the different emotions (except for fear) elicit similar differentiation in the ERP in both BG-patients and healthy controls reflected in P 200 amplitude differences. However, as demonstrated previously (Paulmann & Kotz, 2005), at later processing stages healthy listeners show a negativity in response to violations of combined <b>emotional</b> <b>prosodic</b> and semantic content irrespective of emotion, whereas BG-patients show this negativity only for the emotions “happiness” and “anger” (but not “fear” or “disgust”). The current data serve as renewed evidence that the emotional perception of disgust and fear is impaired in BG-patients. In particular, the integration between emotional prosody and semantics seems influenced by structural changes of the BG...|$|R
40|$|We studied 25 {{participants}} within 24 {{hours of}} acute right hemisphere ischemic stroke and 17 age and education matched hospitalized controls on tests of comprehension of affective prosody. Stroke patients {{were significantly more}} impaired than controls in identifying sarcasm versus sincerity in sentences and identifying affective prosody in sentences, monosyllables, and asyllabic utterances, and in discriminating differences in affective prosody in sentences. Impairments in <b>prosodic</b> <b>comprehension</b> were most associated with acute tissue dysfunction in right posterior frontal cortex, posterior inferior temporal cortex, and thalamus...|$|R
40|$|Speech prosody enables {{communication}} of emotional intentions via modulation of vocal intonations. Reciprocal interactions between superior temporal (STG) and inferior frontal gyri (IFG) {{have been shown}} to anchor a neural network for <b>prosodic</b> <b>comprehension,</b> which we refer to as the Prosody Neural Network (PNN). Although the amygdala is critical for socio-emotional processing, its integral functional and structural role in processing social information from speech prosody as well as its role in the PNN is largely unexplored including inconsistent recent empirical findings. Here, we used magnetoencephalography and diffusion magnetic resonance imaging of white-matter pathways to establish that the PNN is characterized by (1) a robust amygdala-cortical functional connectivity that dynamically evolves as prosodic interpretation progresses, (2) direct structural fiber connections between amygdala and STG/IFG traversing a ventral white-matter pathway, and (3) robust amygdala-insula functional connectivity and structural insula fiber projections to arcuate STG-IFG connections. These findings support a role for functional and structural amygdala-centric ventral pathways in combining speech features to form prosodic percepts. They also highlight insula contributions to <b>prosodic</b> <b>comprehension,</b> potentially via vertical integration of amygdala-centric ventral processing into dorsal pathways responsible for prosodic motor articulation and speech planning. Comment: Main document: 41 pages 6 figures 1 table supplemental material: 4 figure...|$|R
40|$|This study {{compared}} 27 depressed {{patients and}} 27 control participants on the lateralization of <b>prosodic</b> and linguistic <b>comprehension</b> using a dichotic listening task. The comprehension of linguistic material is a left hemisphere process, whereas the comprehension of emotional prosody {{is a right}} hemisphere process. Previous studies provide evidence for reduced left anterior (relative to right) and right posterior cortical activation in depression; it was therefore predicted that depressed patients would show different patterns of lateralization for these two processes. This study {{was the first to}} examine the lateralization of <b>prosodic</b> and linguistic <b>comprehension</b> using identical stimuli for both tasks in depressed patients. All subjects demonstrated a right ear advantage (left hemisphere) for linguistic processing and a left ear advantage (right hemisphere) for prosodic processing. Depression was not associated with significant differences in the lateralization of linguistic or <b>prosodic</b> <b>comprehension.</b> Some of the results were suggestive of reduced left anterior cortical activation but did not reach statistical significance. Results also suggest that depression and anxiety may have opposing effects on cerebral lateralization...|$|R
40|$|Emotional prosody carries {{information}} about the inner state of a speaker and therefore helps us to understand how other people feel. However, emotions are also transferred verbally. In or-der to further substantiate the underlying mechanisms of emo-tional prosodic processing we investigated the interaction of both emotional prosody and emotional semantics with event-related brain potentials (ERPs) utilizing a prosodic and inter-active (prosodic/semantic) violation paradigm. Results suggest that the time-course of <b>emotional</b> <b>prosodic</b> processing and emo-tional semantics differ. While a pure violation of a prosodic contour elicited a positivity between 450 ms and 600 ms, a vio-lation of both emotional prosody and semantics elicited a neg-ativity between 500 ms and 650 ms. These results suggest that emotional prosody and emotional semantics follow a different time-course. This holds true for all emotional prosodies (anger, disgust, fear, happy, pleasant surprise, sad) investigated. As the two conditions elicited two different electrophysiological com-ponents, the obtained results suggest that emotional prosody and semantics contribute differentially during the interaction of both information types. Furthermore, the data suggest that se-mantic information can override prosody when the two channels interact in time, that is, when the <b>emotional</b> <b>prosodic</b> contour agrees with the semantic content of a sentence. 1...|$|R
30|$|Support for {{the right}} {{hemisphere}} model has grown to include right hemisphere dominance during emotional provocation (Borod et al. 1988; Tucker et al. 1977) and in the comprehension and expression of <b>emotional</b> <b>prosodic</b> speech (Borod et al. 1992, 1998, 2000, 2002; Bowers et al. 1987; Emerson et al. 1999; Heilman et al. 1975; Schmitt et al. 1997). Additionally, there is evidence for right hemisphere specialization {{in the perception of}} negative emotional faces (Herridge et al. 2004; Mandel et al. 1991; Wittling and Roschmann 1993) and in the expression of emotional facial gestures (Borod et al. 1997; Herridge et al. 2004; Rhodes et al. 2000).|$|R
40|$|Background Identification of {{emotional}} facial expression and emotional prosody (i. e. speech melody) is often impaired in schizophrenia. For facial emotion identification, {{a recent study}} suggested that the relative deficit in schizophrenia is enhanced when the presented emotion is easier to recognize. It is unclear whether this effect is specific to face processing or {{part of a more}} general emotion recognition deficit. Method We used clarity-graded <b>emotional</b> <b>prosodic</b> stimuli without semantic content, and tested 25 in-patients with paranoid schizophrenia, 25 healthy control participants and 25 depressive in-patients on emotional prosody identification. Facial expression identification was used as a control task. Results Patients with paranoid schizophrenia performed worse than both control groups <b>in</b> identifying <b>emotional</b> prosody, with no specific deficit in any individual emotion category. This deficit was present in high-clarity but not in low-clarity stimuli. Performance in facial control tasks was also impaired, with identification {{of emotional}} facial expression being a better predictor of emotional prosody identification than illness-related factors. Of those, negative symptoms emerged as the best predictor for emotional prosody identification. Conclusions This study suggests a general deficit <b>in</b> identifying high-clarity <b>emotional</b> cues. This finding is in line with the hypothesis that schizophrenia is characterized by high noise in internal representations and by increased fluctuations in cerebral network...|$|R
40|$|In verbal communication, {{not only}} {{the meaning of the}} words convey {{information}}, but also the tone of voice (prosody) conveys crucial information about the emotional state and intentions of others. In various studies right frontal and right temporal regions have been found to play a role <b>in</b> <b>emotional</b> prosody perception. Here, we used triple-pulse repetitive transcranial magnetic stimulation (rTMS) to shed light on the precise time course of involvement of the right anterior superior temporal gyrus and the right fronto-parietal operculum. We hypothesized that information would be processed in the right anterior superior temporal gyrus before being processed in the right fronto-parietal operculum. Right-handed healthy subjects performed an emotional prosody task. During listening to each sentence a triplet of TMS pulses was applied to one of the regions at one of six time points (400 – 1900 ms). Results showed a significant main effect of Time for right anterior superior temporal gyrus and right fronto-parietal operculum. The largest interference was observed half-way through the sentence. This effect was stronger for withdrawal emotions than for the approach emotion. A further experiment with the inclusion of an active control condition, TMS over the EEG site POz (midline parietal-occipital junction), revealed stronger effects at the fronto-parietal operculum and anterior superior temporal gyrus relative to the active control condition. No evidence was found for sequential processing of <b>emotional</b> <b>prosodic</b> information from right anterior superior temporal gyrus to the right fronto-parietal operculum, but the results revealed more parallel processing. Our results sugges...|$|R
40|$|Although {{the left}} {{hemisphere}} {{is widely believed}} to be dominant in syntactic and phonological processes underlying language comprehension, the right-hemisphere may be essential for comprehension of affective prosody (Ross & Monnet, 2008; Heilman, Bowers, Speedie, & Coslett, 1984). In particular, the right posterior-superior temporal region has been implicated in the comprehension of affective prosody (Meyer, Alter, Friederici, Lohmann, & Cramon, 2002; Ross & Monnot, 2008). However, not all patients with damage to this area have impaired comprehension of prosody, and some patients with damage to other areas (e. g. thalamus, frontal cortex) have impaired <b>prosodic</b> <b>comprehension,</b> so {{it is unclear what}} areas are essential for comprehension of prosody. One limitation of most previous studies is that they have studied patients relatively late after stroke, after potential recovery and reorganization of structure-function relationships. Patients may have had impaired comprehension of prosody immediately after the lesion, but recovered. Therefore, we studied patients acutely, before the opportunity for recovery or reorganization. We also evaluated areas of hypoperfused (dysfunctional) brain tissue, as well as infarcted tissue that might account for the deficit...|$|R
40|$|Neurofeedback of {{functional}} {{magnetic resonance imaging}} (fMRI) can be used to acquire selective control over activation in circumscribed brain areas, potentially inducing behavioral changes, depending on the functional role of the targeted cortical sites. In the present study, we used fMRI-neurofeedback to train subjects to enhance regional activation in the right inferior frontal gyrus (IFG) to influence speech processing and to modulate language-related performance. Seven subjects underwent real-time fMRI-neurofeedback training and succeeded in achieving voluntary regulation of their right Brodmann's area (BA) 45. To examine short-term behavioral impact, two linguistic tasks were carried out immediately before and after the training. A significant improvement of accuracy was observed for the identification of <b>emotional</b> <b>prosodic</b> intonations but not for syntactic processing. This evidence supports a role for the right IFG in the processing of emotional information and evaluation of affective salience. The present study confirms the efficacy of fMRI-biofeedback for noninvasive self-regulation of circumscribed brain activity...|$|R
40|$|Autism {{spectrum}} disorders (ASD) {{are characterized}} by deficient social and communication skills, including difficulties in perceiving speech prosody. The present study addressed processing of <b>emotional</b> <b>prosodic</b> changes (sad, scornful and commanding) in natural word stimuli in typically developed school aged children and in children with ASD and language impairment. We found that the responses to a repetitive word were diminished in amplitude in the children with ASD, reflecting impaired speech encoding. Furthermore, the amplitude of the MMN/LDN component, reflecting cortical discrimination of sound changes, was diminished in the children with ASD for the scornful deviant. In addition, the amplitude of the P 3 a, reflecting involuntary orienting to attention-catching changes, was diminished in the children with ASD for the scornful deviant and tended to be smaller for the sad deviant. These results suggest that prosody processing in ASD is impaired at various levels of neural processing, including deficient pre-attentive discrimination and involuntary orientation to speech prosody. (C) 2016 Elsevier Ireland Ltd. All rights reserved...|$|R
40|$|Communication is an {{essential}} part of our life. Though, not only communication is the key – it is all about <b>emotional</b> (<b>prosodic)</b> communication. Due to empirical research, people, who are augmentative communicators and speak with a voice output communication aid, want to express their emotions in the same way as everybody else – it is one of their deepest interests (Portnuff, 2006; Hoffmann and Wülfing, 2010). So far, current devices lack the opportunity of emotional utterances. This circumstance leads not only to a huge usability deficit, but furthermore, it is an obstacle to develop emotional competence and to behave as well as regulate one´s emotion adequately (Blackstone and Wilkins, 2009). This article aims to increase the sensitivity for the importance of emotional communication. Furthermore, it tries to give first suggestions for implementing an usable device that supports users with a voice output communication aid to express emotional utterances. This could be done by using phrase-generation, as mentiond by Vanderheyden and Pennigton (1998) ...|$|R
40|$|Humans {{communicate}} emotion vocally by modulating acoustic cues such as pitch, {{intensity and}} voice quality. Research has documented how the relative {{presence or absence}} of such cues alters the likelihood of perceiving an emotion, but the neural underpinnings of acoustic cue-dependent emotion perception remain obscure. Using functional magnetic resonance imaging in 20 subjects we examined a reciprocal circuit consisting of superior temporal cortex, amygdala and inferior frontal gyrus that may underlie affective <b>prosodic</b> <b>comprehension.</b> Results showed that increased saliency of emotion-specific acoustic cues was associated with increased activation in superior temporal cortex (planum temporale (PT), posterior superior temporal gyrus (pSTG), and posterior superior middle gyrus (pMTG)) and amygdala, whereas decreased saliency of acoustic cues was associated with increased inferior frontal activity and temporo-frontal connectivity. These results suggest that sensory-integrative processing is facilitated when the acoustic signal is rich in affective information, yielding increased activation in temporal cortex and amygdala. Conversely, when the acoustic signal is ambiguous, greater evaluative processes are recruited, increasing activation in inferior frontal gyrus (IFG) and IFG STG connectivity. Auditory regions may thus integrate acoustic information with amygdala input to form emotion-specific representations, which are evaluated within inferior frontal regions...|$|R
40|$|Functional imaging {{studies of}} healthy {{participants}} and previous lesion studies have provided evidence that empathy involves dissociable cognitive functions {{that rely on}} at least partially distinct neural networks that can be individually impaired by brain damage. These studies converge {{in support of the}} proposal that affective empathy—making inferences about how another person feels—engages at least the following areas: prefrontal cortex, orbitofrontal gyrus, anterior insula, anterior cingulate cortex, temporal pole, amygdala and temporoparietal junction. We hypothesized that right-sided lesions to any one of these structures, except temporoparietal junction, would cause impaired affective empathy (whereas bilateral damage to temporopar-ietal junction would be required to disrupt empathy). We studied 27 patients with acute right hemisphere ischaemic stroke and 24 neurologically intact inpatients on a test of affective empathy. Acute impairment of affective empathy was associated with infarcts in the hypothesized network, particularly temporal pole and anterior insula. All patients with impaired affective empathy were also impaired in comprehension of affective prosody, but many patients with impairments in <b>prosodic</b> <b>comprehension</b> had spared affective empathy. Patients with impaired affective empathy were older, but showed no difference in performance o...|$|R
40|$|Abstract: Neurofeedback of {{functional}} {{magnetic resonance imaging}} (fMRI) can be used to acquire selective control over activation in circumscribed brain areas, potentially inducing behavioral changes, depending on the functional role of the targeted cortical sites. In the present study, we used fMRI-neu-rofeedback to train subjects to enhance regional activation in the right inferior frontal gyrus (IFG) to influence speech processing and to modulate language-related performance. Seven subjects underwent real-time fMRI-neurofeedback training and succeeded in achieving voluntary regulation of their right Brodmann’s area (BA) 45. To examine short-term behavioral impact, two linguistic tasks were carried out immediately before and after the training. A significant improvement of accuracy was observed for the identification of <b>emotional</b> <b>prosodic</b> intonations but not for syntactic processing. This evidence sup-ports a role for the right IFG in the processing of emotional information and evaluation of affective salience. The present study confirms the efficacy of fMRI-biofeedback for noninvasive self-regulation of circumscribed brain activity. Hum Brain Mapp 30 : 1605 – 1614, 2009. VC 2008 Wiley-Liss, Inc. Key words: operant conditioning; self-regulation; real-time {{functional magnetic resonance imaging}}; right inferior frontal gyrus; prosod...|$|R
40|$|The {{influence}} of emotional prosody on {{the evaluation of}} emotional facial expressions was investigated in an event-related brain potential (ERP) study using a priming paradigm, the facial affective decision task. <b>Emotional</b> <b>prosodic</b> fragments of short (200 -msec) and medium (400 -msec) duration were presented as primes, followed by an emotionally related or unrelated facial expression (or facial grimace, which does not resemble an emotion). Participants judged {{whether or not the}} facial expression represented an emotion. ERP results revealed an N 400 -like differentiation for emotionally related prime-target pairs when compared with unrelated prime-target pairs. Faces preceded by prosodic primes of medium length led to a normal priming effect (larger negativity for unrelated than for related prime-target pairs), but the reverse ERP pattern (larger negativity for related than for unrelated prime-target pairs) was observed for faces preceded by short prosodic primes. These results demonstrate that brief exposure to prosodic cues can establish a meaningful emotional context that influences related facial processing; however, this context does not always lead to a processing advantage when prosodic information is very short in duration. © 2010 The Psychonomic Society, Inc...|$|R
40|$|Deficits <b>in</b> <b>emotional</b> clarity, or {{difficulties}} identifying which emotions one feels, {{are increasingly}} associated with multiple forms of psychopathology. We addressed two fundamental, unresolved issues regarding the transdiagnostic {{nature of this}} dysfunction. First, we examined the relationship of deficits <b>in</b> <b>emotional</b> clarity to seven symptom types, accounting for possible confounding effects of overlapping symptoms. We found that deficits <b>in</b> <b>emotional</b> clarity were associated with symptoms of depression, social anxiety, borderline personality, binge eating, and alcohol use, but not anxious arousal or restrictive eating. Second, we tested whether deficits <b>in</b> <b>emotional</b> clarity would relate to psychopathology by way of impaired emotion regulation. Notably, the relationship between deficits <b>in</b> <b>emotional</b> clarity and each symptom type was mediated by a distinct, disorderspecific pattern of emotion regulation deficits. Findings suggest that deficits <b>in</b> <b>emotional</b> clarity can be conceptualized as a transdiagnostic process with diverging mechanisms involving emotion regulation difficulties that vary from disorder to disorder. We discuss these findings within a contextual approach to delineating transdiagnostic processes...|$|R
40|$|In verbal communication, {{prosodic}} codes may be phylogenetically {{older than}} lexical ones. Little is known, however, about early, automatic encoding of emotional prosody. This {{study investigated the}} neuromagnetic analogue of mismatch negativity (MMN) as an index of early stimulus processing of emotional prosody using whole-head magnetoencephalography (MEG). We applied two different paradigms to study MMN; {{in addition to the}} traditional oddball paradigm, the so-called optimum design was adapted to emotion detection. In a sequence of randomly changing disyllabic pseudo-words produced by one male speaker in neutral intonation, a traditional oddball design with emotional deviants (10 % happy and angry each) and an optimum design with emotional (17 % happy and sad each) and nonemotional gender deviants (17 % female) elicited the mismatch responses. The emotional category changes demonstrated early responses (< 200 ms) at both auditory cortices with larger amplitudes at the right hemisphere. Responses to the nonemotional change from male to female voices emerged later (approximately 300 ms). Source analysis pointed at bilateral auditory cortex sources without robust contribution from other such as frontal sources. Conceivably, both auditory cortices encode categorical representations of <b>emotional</b> <b>prosodic.</b> Processing of cognitive feature extraction and automatic emotion appraisal may overlap at this level enabling rapid attentional shifts to important social cues...|$|R
40|$|This study {{investigated}} cross-modal effects of emotional voice tone (prosody) on face processing during instructed visual search. Specifically, we evaluated whether <b>emotional</b> <b>prosodic</b> cues <b>in</b> speech have a rapid, mandatory influence on eye movements to an emotionally-related face, and whether these effects persist as semantic information unfolds. Participants viewed {{an array of}} six emotional faces while listening to instructions spoken in an emotionally congruent or incongruent prosody (e. g.; "Click on the happy face" spoken in a happy or angry voice). The duration and frequency of eye fixations were analyzed when only prosodic cues were emotionally meaningful (pre-emotional label window: "Click on the/ [...] . "), and after emotional semantic information was available (post-emotional label window: " [...] . /happy face"). In the pre-emotional label window, results showed that participants made immediate use of emotional prosody, as reflected in significantly longer frequent fixations to emotionally congruent versus incongruent faces. However, when explicit semantic information in the instructions became available (post-emotional label window), the influence of prosody on measures of eye gaze was relatively minimal. Our data show that emotional prosody has a rapid impact on gaze behavior during social information processing, but that prosodic meanings can be overridden by semantic cues when linguistic information is task relevant. © 2011 Elsevier B. V. All rights reserved...|$|R
40|$|Background: Comprehensive {{understanding}} of a spoken message relies on successful interpretation of both what is spoken, {{and the manner in}} which it is spoken. Deficits in the ability to interpret <b>emotional</b> tone <b>in</b> speech have been widely reported in schizophrenia, with some literature suggesting that such prosodic interpretation is especially impoverished in those who experience auditory hallucination. As a sound-based paralinguistic faculty, the interpretation of emotional prosody ultimately relies on the sensory processing of fundamental components of speech sound, namely pitch, loudness and tone duration. Similarly, many of the distinguishing phenomenological aspects of auditory hallucination in schizophrenia, such as anomalous auditory source localization and speaker identification, suggest the implication of problematic processing of these fundamental auditory cues, and especially that of pitch. The current study explored the possibility that deficits in fundamental auditory processing may be a sensory substrate of both auditory hallucination and deficits in the interpretation of emotional prosody. Methods: Three groups of participants were recruited for the current study: 25 schizophrenia patients experiencing auditory hallucination; 25 schizophrenia patients with no significant history of auditory hallucination; and 25 non-psychiatric controls. Following screening for hearing loss, participants performed a battery of auditory processing tasks, which assessed ability to discriminate between simple tones that varied in either pitch, loudness and tone duration. To assess prosodic processing, participants performed a task assessing ability to identify the emotion of sentences spoken in a range of emotions. In addition, participants performed a novel prosodic labeling task in which full speech waveforms were modulated by two of the three fundamental acoustic cues of interest at a time, such that only one intact cue was offered for prosodic interpretation. A cold cognitive, linguistic prosody-labeling task was included as a control measure to examine any independent deficits <b>in</b> <b>emotional</b> and non-emotional <b>prosodic</b> interpretation. Results: Preliminary analysis of the fundamental auditory processing data suggests that hallucinating schizophrenia patients displayed significant deficits in the discrimination of pitch and tone duration when compared with non-psychiatric controls. However, no deficit in the discrimination of loudness information was identified. This presentation will include reporting of the final results from all auditory and prosodic processing tasks. Discussion: The current study explored fundamental auditory processing deficits in schizophrenia and how these may relate to auditory hallucination and receptive prosody dysfunction. The initial findings suggesting that pitch and tone duration processing are impoverished in schizophrenia, and yet loudness processing may be intact, are congruent with previous findings. This presentation will draw upon the final results to establish a neurocognitive model of auditory processing deficit, prosodic dysfunction and auditory hallucination in schizophrenia, and discuss implications for treatment and future research...|$|R
40|$|Information that is {{broadcast}} live via radio involves complex human {{interactions between the}} sender and the receiver. The work done in radio programs benefits and builds upon numerous scientific studies conducted in fields like natural language processing, <b>emotional</b> and <b>prosodic</b> modelling, as well signal processing. With a view {{to all of the}} above, this article focuses on the means by which direct verbal communication characteristic of radio broadcasts can be made more efficient by optimizing the level of information conveyed by new messages, considering the level of information absorption on behalf of radio audience...|$|R
5000|$|Face Validity of Meta-Analyses <b>in</b> <b>Emotional</b> or Behavioral Disorders (2004) ...|$|R
