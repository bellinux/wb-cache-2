24|58|Public
50|$|In Fall of 2009, San Lorenzo High School transitioned off the 4x4 Block {{schedule}} {{in which students}} take 80 credits per year, to a 6-period day in which students take 60 credits per year. Therefore, the graduation requirements will be <b>incrementally</b> <b>adjusted</b> {{over the course of}} the next four years, 2009-2010 through 2012-2013.|$|E
40|$|Implementation of {{monitoring}} programmes in ecological impact assessment frequently requires rapid, cost-effective surveys of vegetation along a time scale that {{varies according to}} the nature of the studies being carried out. When time constraints are restrictive such surveys may be qualitative and therefore highly dependent on the experience and competence of the observer. Reduction of subjectivity may be achieved by quantifying observation, an approach that would however in crease time constraints and reduce cost-effectiveness. The sampling and assessment strategy being proposed here is a semi-quantitative approach to determination of relative abundance of species. It retains the low effort-demand of qualitative technique whilst introducing a flexible quantitative aspect that may be <b>incrementally</b> <b>adjusted</b> towards specific requirements. peer-reviewe...|$|E
40|$|This article {{presents}} a new sequence of steps, which was applied as a Strategic Marketing Planning and Control framework in three companies and <b>incrementally</b> <b>adjusted</b> {{over a period}} of three years based on the observed results in three companies. This approach differs from other planning proposals in that it views the firm as part of a production network and it makes active use of inter-firm relationships and collective action tools in designing marketing activities. Many collective actions can be undertaken by firms that operate in the same markets, as competitors or as suppliers of complementary products. This research revealed a considerable desire among the firms to act jointly in their marketing action...|$|E
3000|$|Step 3. Model selection: <b>incrementally</b> <b>adjusting</b> {{the model}} to fit the data, one {{fluorophore}} at a time. 3 B either adds a new fluorophore at a random position or selects a fluorophore in the model for removal.|$|R
30|$|In this article, {{the target}} model seta for SSS is {{obtained}} through interpolation. Model interpolation offers two distinctive advantages. First, {{the data collection}} cost is reduced compared to retraining or model adaptation. Second, {{the properties of the}} synthesized speech can be refined by <b>incrementally</b> <b>adjusting</b> the interpolation ratios. These features are analyzed in the following sections.|$|R
40|$|We {{consider}} {{a situation in}} which we see samples in R^d drawn i. i. d. from some distribution with mean zero and unknown covariance A. We wish to compute the top eigenvector of A in an incremental fashion - with an algorithm that maintains an estimate of the top eigenvector in O(d) space, and <b>incrementally</b> <b>adjusts</b> the estimate with each new data point that arrives. Two classical such schemes are due to Krasulina (1969) and Oja (1983). We give finite-sample convergence rates for both. Comment: NIPS 201...|$|R
40|$|International audienceWe propose {{abstract}} regular model checking {{as a new}} generic {{technique for}} verification of parametric and infinite-state systems. The technique combines the two approaches of regular model checking and verification by abstraction. We propose a general framework of the method {{as well as several}} concrete ways of abstracting automata or transducers, which we use for modelling systems and encoding sets of their states as usual in regular model checking. The abstraction is based on collapsing states of automata (or transducers) and its precision is being <b>incrementally</b> <b>adjusted</b> by analysing spurious counterexamples. We illustrate the technique on verification {{of a wide range of}} systems including a novel application of automata-based techniques to an example of systems with dynamic linked data structure...|$|E
40|$|Stream {{reasoning}} {{is the task}} of continuously de-riving conclusions on streaming data. To get re-sults instantly one evaluates a query repeatedly on recent data chunks selected by window operators. However, simply recomputing results from scratch is impractical for rule-based reasoning with seman-tics similar to Answer Set Programming, due to the trade-off between complexity and data throughput. To address this problem, we present a method to efficiently update models of a rule set. In particu-lar, we show how an answer stream (model) of a LARS program can be <b>incrementally</b> <b>adjusted</b> to new or outdated input by extending truth mainte-nance techniques. We obtain in this way a means towards practical rule-based stream reasoning with nonmonotonic negation, various window operators and different forms of temporal reference. ...|$|E
40|$|In this paper, {{we report}} our recent {{development}} of a novel discriminative learning technique which embeds the concept of discriminative margin into the well established minimum classification error (MCE) method. The idea is to impose an <b>incrementally</b> <b>adjusted</b> “margin ” in the loss function of MCE algorithm so that not only error rates are minimized but also discrimination “robustness ” between training and test sets is maintained. Experimental evaluation shows {{that the use of}} the margin improves a state-of-the-art MCE method by reducing 17 % digit errors and 19 % string errors in the TIDigits recognition task. The string error rate of 0. 55 % and digit error rate of 0. 19 % we have obtained are the best-ever results reported on this task in the literature. Index Terms: discriminative training, margin, minimum error 1...|$|E
5|$|A {{common theme}} {{throughout}} Zobel's {{work is the}} issue of impedance matching. The obvious approach to filter design is to design directly for the attenuation characteristics desired. With modern computing power, a brute force approach is possible and easy, simply <b>incrementally</b> <b>adjusting</b> each component while recalculating in an iterative process until the desired response is achieved. However, Zobel developed a more indirect line of attack. He realized very early on that mismatched impedances inevitably meant reflections, and reflections meant a loss of signal. Improving the impedance match, conversely, would automatically improve a filter's pass-band response.|$|R
40|$|Abstract — We propose novel {{reconfigurable}} rateless codes, {{that are}} capable of not only varying the block length but also adaptively modify their encoding strategy by <b>incrementally</b> <b>adjusting</b> their degree distribution according to the prevalent channel conditions without the availability of the channel state information at the transmitter. In particular, we characterize a reconfigurable rateless code designed for the transmission of 9, 500 information bits that achieves a performance, which is approximately 1 dB away from the discrete-input continuousoutput memoryless channel’s (DCMC) capacity over a diverse range of channel signal-to-noise (SNR) ratios. I...|$|R
50|$|A {{common theme}} {{throughout}} Zobel's {{work is the}} issue of impedance matching. The obvious approach to filter design is to design directly for the attenuation characteristics desired. With modern computing power, a brute force approach is possible and easy, simply <b>incrementally</b> <b>adjusting</b> each component while recalculating in an iterative process until the desired response is achieved. However, Zobel developed a more indirect line of attack. He realized very early on that mismatched impedances inevitably meant reflections, and reflections meant a loss of signal. Improving the impedance match, conversely, would automatically improve a filter's pass-band response.|$|R
40|$|Process {{discovery}} from {{model and}} text artefacts Modeling {{is an important}} and time consuming part of the Business Process Management life-cycle. An analyst reviews existing documentation and queries relevant domain experts to construct both mental and concrete models of the domain. To aid this exercise, we propose the Rapid Business Process Discovery (R-BPD) framework and prototype tool that can query heterogeneous information resources (e. g. corporate documentation, web-content, code e. t. c.) and rapidly construct proto-models to be <b>incrementally</b> <b>adjusted</b> to correctness by an analyst. This constitutes a departure from building and constructing models toward just editing them. We believe this rapid mixed-initiative modeling will increase analyst productivity by significant orders of magnitude over traditional approaches. Furthermore, {{the possibility of using}} the approach in distributed and real-time settings seems appealing and may help in significantly improving the quality of th...|$|E
40|$|Modeling is an {{important}} and time consuming part of the Business Process Management life-cycle. An analyst re-views existing documentation and queries relevant domain experts to construct both mental and concrete models of the domain. To aid this exercise, we propose the Rapid Business Process Discovery (R-BPD) framework and prototype tool that can query heterogeneous information resources (e. g. corporate documentation, web-content, code e. t. c.) and rapidly construct proto-models to be <b>incrementally</b> <b>adjusted</b> to correctness by an analyst. This constitutes a departure from building and constructing models toward just editing them. We believe this rapid mixed-initiative modeling will increase analyst productivity by significant orders of mag-nitude over traditional approaches. Furthermore, the pos-sibility of using the approach in distributed and real-time settings seems appealing and may help in significantly im-proving {{the quality of the}} models being developed w. r. t. be-ing consistent, complete, and concise. ...|$|E
40|$|Environmental sensing is {{becoming}} a significant way for understanding and transforming the environment, given recent technology advances in the Internet of Things (IoT). Current environmental sensing projects typically deploy commodity sensors, which {{are known to be}} unreliable and prone to produce noisy and erroneous data. Unfortunately, the accuracy of current cleaning techniques based on mean or median prediction is unsatisfactory. In this paper, we propose a cleaning method based on <b>incrementally</b> <b>adjusted</b> individual sensor reliabilities, called influence mean cleaning (IMC). By incrementally adjusting sensor reliabilities, our approach can properly discover latent sensor reliability values in a data stream, and improve reliability-weighted prediction even in a sensor network with changing conditions. The experimental results based on both synthetic and real datasets show that our approach achieves higher accuracy than the mean and median-based approaches after some initial adjustment iterations. Yihong Zhang, Claudia Szabo and Quan Shen...|$|E
40|$|We propose novel {{reconfigurable}} rateless codes, {{that are}} capable of not only varying the block length but also adaptively modify their encoding strategy by <b>incrementally</b> <b>adjusting</b> their degree distribution according to the prevalent channel conditions without the availability of the channel state information at the transmitter. In particular, we characterize a reconfigurable ratelesscode designed for the transmission of 9, 500 information bits that achieves a performance, which is approximately 1 dB away from the discrete-input continuous-output memoryless channel’s (DCMC) capacity over a diverse range of channel signal-to-noise (SNR) ratios...|$|R
40|$|Abstract—We propose novel {{reconfigurable}} rateless codes, {{that are}} capable of not only varying the block length but also adaptively modify their encoding strategy by <b>incrementally</b> <b>adjusting</b> their degree distribution according to the prevalent channel conditions without the availability of the channel state information at the transmitter. In particular, we characterize a reconfigurable rateless code designed for the transmission of 9, 500 information bits that achieves a performance, which is approximately 1 dB away from the discrete-input continuous-output memoryless channel’s (DCMC) capacity over a diverse range of channel signal-to-noise (SNR) ratios. Index Terms—Reconfigurable rateless codes, adaptive channel coding. I...|$|R
30|$|The basic {{operation}} of Step 3 is adjusting {{the model to}} fit the data. In this model selection step, 3 B makes many local decisions to <b>incrementally</b> <b>adjust</b> the model. It only allows one fluorophore to be either added or removed at a time: either a new fluorophore is added at a random position or a fluorophore in the model is selected for removal. 3 B optimizes this spot {{to search for a}} new position and then decides whether to keep it in the model. After a series of such decisions have been made, 3 B re-optimizes the entire model (Step 2) and then repeats the model selection step (Step 3).|$|R
40|$|Based on the {{observation}} that the unpredictable nature of conversational speech makes {{it almost impossible to}} reliably model sequential word constraints, the notion of word set error criteria is proposed for improved recognition of spontaneous dialogues. The single pass Adaptive Boosting (AB) algorithm enables the language model weights to be tuned using the word set error criteria. In the two pass version of the algorithm, the basic idea is to predict a set of words based on some a priori information, and perform a re-scoring pass wherein the probabilities of the words in the predicted word set are amplified or boosted in some manner. An adaptive gradient descent procedure for tuning the word boosting factor has been formulated which enables the boost factors to be <b>incrementally</b> <b>adjusted</b> to maximize accuracy of the speech recognition system outputs on held-out training data using the word set error criteria. Two novel models which predict the required word sets have been presented: u [...] ...|$|E
40|$|Simulations using reanalysis {{meteorological}} fields {{have long}} been used to understand the causes of atmospheric composition change in the recent past. Using the new MERRA- 2 reanalysis, we are conducting chemistry simulations to create products covering 1980 - 2016 for the atmospheric composition community. These simulations use the Global Modeling Initiative (GMI) chemical mechanism in two different models: the GMI Chemical Transport Model (CTM) and the GEOS- 5 model in Replay mode. Replay mode means an integration of the GEOS- 5 general circulation model that is <b>incrementally</b> <b>adjusted</b> each time step toward the MERRA- 2 reanalysis. The GMI CTM is a 1 deg x 1. 25 deg simulation and the MERRA- 2 GMI Replay simulation uses the native MERRA- 2 grid of approximately 1 / 2 deg horizontal resolution on the cubed sphere. A specialized set of transport diagnostics is included in both runs to better understand trace gas transport and its variability in the recent past...|$|E
40|$|In complex {{reasoning}} tasks, as expressible by Answer Set Programming (ASP), problems often {{permit for}} multiple solutions. In dynamic environments, where knowledge is continuously changing, {{the question arises}} how a given model can be <b>incrementally</b> <b>adjusted</b> relative to new and outdated information. This paper introduces Ticker, a prototypical engine for well-defined logical reasoning over streaming data. Ticker builds on a practical fragment of the recent rule-based language LARS which extends Answer Set Programming for streams by providing flexible expiration control and temporal modalities. We discuss Ticker's reasoning strategies: First, the repeated one-shot solving mode calls Clingo on an ASP encoding. We show how this translation can be incrementally updated when new data is streaming in or time passes by. Based on this, we build on Doyle's classic justification-based truth maintenance system (TMS) to update models of non-stratified programs. Finally, we empirically compare the obtained evaluation mechanisms. This paper is under consideration for acceptance in TPLP. Comment: Paper presented at the 33 nd International Conference on Logic Programming (ICLP 2017), Melbourne, Australia, August 28 to September 1, 2017, 24 pages, LaTeX, 4 PDF figures (arXiv: 1707. 05304...|$|E
40|$|Abstract — We {{present an}} {{equilibrium}} selection algorithm for reinforcement learning agents that <b>incrementally</b> <b>adjusts</b> {{the probability of}} executing each action based on the desirability of the outcome obtained in the last time step. The algorithm assumes {{that at least one}} coordination equilibrium exists and requires that the agents have a heuristic for determining whether or not the equilibrium was obtained. In deterministic environments with one or more strict coordination equilibria, the algorithm will learn to play an optimal equilibrium as long as the heuristic is accurate. Empirical data demonstrate that the algorithm is also effective in stochastic environments and is able to learn good joint policies when the heuristic’s parameters are estimated during learning, rather than known in advance. I...|$|R
40|$|In 21 st-Century VLSI design, {{clocking}} plays crucial {{roles for}} both performance and timing convergence. Minimumdelay/power zero-skew buffer-insertion/sizing and wire-sizing problems {{have long been}} considered intractable due to their non-convex nature. In this paper, we present ClockTune, a simultaneous buffer-insertion/sizing and wire-sizing algorithm which guarantees zero-skew and minimizes delay or power in pseudo-polynomial time. Extensive experimental results show that our algorithm executes very efficiently. For example, ClockTune achieves 40 X delay improvement for buffering and sizing an industrial clock-tree with 3101 sink nodes on a 1. 2 GHz Pentium IV PC in 12 minutes compared with the initinal routing. Our algorithm {{can also be used}} to achieve useful clock-skew to facilitate timing convergence and to <b>incrementally</b> <b>adjust</b> clock-tree for design convergence and explore delay-power tradeoffs during design cycles. 1...|$|R
40|$|Abstract — We {{present a}} new {{approach}} for generating virtual coordinates that produces usable coordinates quickly and improves the routing performance of existing geographic routing algorithms. Starting from a set of initial coordinates derived from a set of elected perimeter nodes, Greedy Embedding Spring Coordinates (GSpring) detects possible dead ends and uses a modified spring relaxation algorithm to <b>incrementally</b> <b>adjust</b> virtual coordinates to increase the convexity of voids in the virtual routing topology. This reduces the probability that packets {{will end up in}} dead ends during greedy forwarding. The coordinates derived by GSpring achieve routing stretch that is up to 50 % lower than that for NoGeo, the best existing algorithm for deriving virtual Euclidean coordinates for geographic routing. For realistic network topologies with obstacles, GSpring coordinates achieves from between 10 to 15 % better routing stretch than actual physical coordinates...|$|R
40|$|AbstractNon-Von Neumann {{computational}} architectures have lately aroused significant {{interest as}} substrates for complex computation. One recent development in this domain is the Polychronous Wavefront Computing (PWC) computational model based on multiple wavefront dynamics. This model is an abstraction and simplification of the {{artificial neural network}} paradigm based on temporal and spatial patterns of activity in a pulse propagating media and their interaction with transponders. While this framework is capable of computing basic logical functions and exhibiting interesting dynamic behaviors, methods for unsupervised training of the framework have not been identified. The lack of input weights and the spatio-temporal nature of the PWC framework make direct application of weight adjusting learning methods (e. g., backpropagation) impractical. The paper will describe research into unsupervised learning for PWCs inspired by Spike-Timing-Dependent Plasticity (STDP) methods used {{with other types of}} polychronous models. The method is based on adding Leaky Integrate-and-Fire semantics to the PWC framework allowing analysis of activating wavefronts and determination of the optimal location for future stimulation. The transponder's location is then <b>incrementally</b> <b>adjusted</b> to improve its future response. The paper will discuss the learning approach and examine the results of applying the method over a series of stimulations to sample configurations...|$|E
40|$|A 44 -year-old woman, who was {{suffering}} from widespread musculoskeletal pain, fatigue, and sleep disorder, was diagnosed as fibromyalgia. There was no apparent organic disease. Duloxetine therapy was introduced with a dose of 60 mg/day at bedtime. A few days later her husband noted severe teeth clenching and associated loud grinding noises during sleep. Then, duloxetine dosage was reduced to 30 mg/day. The bruxism continued with this dosage, thus the therapy was discontinued. The bruxism resolved after cessation. Three weeks later, duloxetine therapy was restarted at the dosage of 60 mg/day. On {{the third day of}} the therapy, bruxism started again and amitriptyline therapy at the dosage of 10 mg/day was added to duloxetine therapy. The dosage of amitriptyline was <b>incrementally</b> <b>adjusted</b> to 25 mg/ day. On the fourth day of the combined therapy, bruxism symptoms improved. Two months later, the bruxism symptoms were resolved and the complaints for fibromyalgia were under control. Although bruxism has been reported due to venlafaxine use, there is only one duloxetine-induced bruxism case in the literature which was treated with buspirone. However, we report duloxetine-induced bruxism treated successfully with amitriptyline in a patient with fibromyalgia. Tricyclic antidepressants have a suppression effect on the REM phase of the sleep cycle; this may help to cease the bruxism symptoms appearing in that phase of the sleep cycle. This is the first reported case of fibromyalgia with duloxetine-induced sleep bruxism successfully treated with amitriptyline...|$|E
40|$|Paper {{presented}} to the 5 th Annual Symposium on Graduate Research and Scholarly Projects (GRASP) held at the Hughes Metropolitan Complex, Wichita State University, May 1, 2009. Research completed at the School of Art & Design, College of Fine ArtsCeramic glazes are composed of three primary components: a flux, an alumina- bonding agent, and a glass former. These three materials can be adjusted for firing ranges as low as 1213 degrees Fahrenheit, up to 2419 degrees Fahrenheit. However, some of the oxides used are toxic, and can leach through the glaze matrix rendering them unsuitable for functional ware. One oxide is Copper Oxide, applicable in different forms: Carbonate-CuCo 3, Cupric Oxide- CuO, and Sulfate-CuSO 4 5 H 20. All three are toxic, and {{have been found in}} previous studies to leach to the surface of a glaze that contains more than 5 % Copper Oxide by weight. When acidic liquids such as citric juices, vinegar, or wine come into contact with these surfaces, mild poisons are created. While these materials can create a beautiful array of colors, food safe glaze surfaces are very important to a contemporary ceramicist working in the vein of functional dinner ware. Approaching the composition of the glazes in a line blend series-where each material is <b>incrementally</b> <b>adjusted</b> in percentages of 5 %- information has been obtained about the necessity of the proportions of each material present in the published glazes, arriving closer to an inert surface suitable for ceramic ware production...|$|E
40|$|Patient {{similarity}} assessment aims at {{providing a}} clinically meaningful distance measure for case retrieval {{in the context}} of clinical decision intelligence. Two of the key challenges are how to incorporate physician feedback with regard to the retrieval results and how to interactively update the underlying similarity measure based on the feedback. In this paper, we present the interactive Metric learning (iMet) method that can <b>incrementally</b> <b>adjust</b> theunderlyingdistance metric basedon latest supervision information. iMet is designed to scale linearly with the data set size based on matrix perturbation theory, which allows the derivation of sound theoretical guarantees. We show empirical results demonstrating that iMet outperforms the baseline by three orders of magnitude in speed while obtaining comparable accuracy on several benchmark datasets. We also describe the application of the algorithm in a real world physician decision support system. ...|$|R
40|$|Abstract- We {{present a}} new {{approach}} for generating virtual coordinates that produces usable coordinates quickly and improves the routing performance of existing geographic routing algorithms. Starting from a set of initial coordinates derived from a set of elected perimeter nodes, Greedy Embedding Spring Coordinates (GSpring) detects possible dead ends and uses a modified spring relaxation algorithm to <b>incrementally</b> <b>adjust</b> virtual coordinates to increase the convexity of voids in the virtual routing topology. This reduces the probability that packets {{will end up in}} dead ends during greedy forwarding. The coordinates derived by GSpring achieve routing stretch that is up to 50 % lower than that for NoGeo, the best existing algorithm for deriving virtual Euclidean coordinates for geographic routing. For realistic network topologies with obstacles, GSpring coordinates achieves from between 10 to 15 % better routing stretch than actual physical coordinates...|$|R
40|$|In quantum {{mechanics}} some properties are maximally incompatible, {{such as the}} position and momentum of a particle or the vertical and horizontal projections of a 2 -level spin. Given any definite state of one property the other property is completely random, or unbiased. For N-level systems, the 6 -level ones are the smallest for which a tomographically efficient set of N+ 1 mutually unbiased bases (MUBs) has not been found. To facilitate the search, we numerically extend the classification of unbiased bases, or Hadamards, by <b>incrementally</b> <b>adjusting</b> relative phases in a standard basis. We consider the non-unitarity caused by small adjustments with a second order Taylor expansion, and choose incremental steps within the 4 -dimensional nullspace of the curvature. In this way we prescribe a numerical integration of a 4 -parameter set of Hadamards of order 6. Comment: 5 pages, 2 figure...|$|R
40|$|Algal {{biodiesel}} {{has been}} a subject of growing importance {{in the realm of}} renewable energy due to carbon capture properties and its potential for photosynthetic efficiency with high lipid output. This study identified five isolates of freshwater green algae, belonging to the Chlorellaceae, and measured the lipid classes and fatty acid profiles of these species to determine suitability for biodiesel production. To induce the greater accumulation of lipids, especially in the form of triacylglycerols (TAGs) desired for biodiesel, we examined the lipid accumulation in cells stressed by nitrogen limitation, sulfur deficiency, or pH stress. Increases in biomass were monitored in order to determine if adjusting pH incrementally {{over the course of the}} experiment had any effect on growth and lipid accumulation of several isolates. TAG accumulation was visually screened by Nile Red fluorescence and further assessed by gas chromatography. Lipid amounts were comparably equal or better for pH stress treatments than for standard nutrient-deprivation treatments. <b>Incrementally</b> <b>adjusted</b> pH over the course of growth triggered lipid accumulation comparable to constant pH stress treatments, yet biomass accumulation was equivalent to unstressed growth. One isolate obtained from the Athabasca oil-sands region of Alberta, OS 4 - 2, is a good candidate for biodiesel production, having accumulated over 45 % of its dry weight as lipid, with over 80 % of the lipid as triacylglycerols, and contains an abundance of 18 : 1 fatty acids. This class of fatty acids improves the cold flow and oxidative stability of biodiesel and is ideal for biofuel used in a Canadian climate. Peer reviewed: YesNRC publication: Ye...|$|E
40|$|The {{cost and}} risk {{associated}} with mineral exploration in Australia increases significantly as companies move into deeper regolith-covered terrain. The ability to map the bedrock {{and the depth of}} weathering within an area has the potential to decrease this risk and increase the effectiveness of exploration programs. This paper is the second in a trilogy concerning the Grant 2 ̆ 7 s Patch area of the Eastern Goldfields. The recent development of the VPmg potential field inversion program in conjunction with the acquisition of high-resolution gravity data over an area with extensive drilling provided an opportunity to evaluate three-dimensional gravity inversion as a bedrock and regolith mapping tool. An apparent density model of the study area was constructed, with the ground represented as adjoining 200 m by 200 m vertical rectangular prisms. During inversion VPmg <b>incrementally</b> <b>adjusted</b> the density of each prism until the free-air gravity response of the model replicated the observed data. For the Grant 2 ̆ 7 s Patch study area, this image of the apparent density values proved easier to interpret than the Bouguer gravity image. A regolith layer was introduced into the model and realistic fresh-rock densities assigned to each basement prism according to its interpreted lithology. With the basement and regolith densities fixed, the VPmg inversion algorithm adjusted the depth to fresh basement until the misfit between the calculated and observed gravity response was minimised. The resulting geometry of the bedrock/regolith contact largely replicated the base of weathering indicated by drilling with predicted depth of weathering values from gravity inversion typically within 15...|$|E
40|$|AbstractCFTR mutation, {{which causes}} cystic {{fibrosis}} (CF), has also recently {{been identified as}} causing glutathione system dysfunction and systemic deficiency of reduced glutathione (GSH). Such dysfunction and deficiency regarding GSH {{may contribute to the}} pathophysiology of CF. We followed 13 patients (age range 1 – 27  years) with cystic fibrosis who were using a regimen of reduced glutathione (GSH), including oral glutathione and inhaled buffered glutathione in an uncontrolled, observational study. Dosage ranged from 66 – 148  mg/kg/day in divided doses, and the term examined was the initial 5. 5  months of GSH use (45  days of <b>incrementally</b> <b>adjusted</b> dose, plus 4  months of use at full dosage). Baseline and post-measurements of FEV 1 percent predicted, BMI percentile, and weight percentile were noted, in addition to bacterial status and pulmonary exacerbations. Significant improvement in the following clinical parameters was observed: average improvement in FEV 1 percent predicted (N= 10) was 5. 8 percentage points (p< 0. 0001), average weight percentile (N= 13) increased 8. 6 points (p< 0. 001), BMI percentile (N= 11) improved on average 1. 22 points (p< 0. 001). All patients improved in FEV 1 and BMI, if measured in their case; 12 of 13 patients improved in weight percentile. Positive sputum cultures of bacteria in 11 patients declined from 13 to 5 (p< 0. 03) with sputum cultures of Pseudomonas aeruginosa becoming negative in 4 of 5 patients previously culturing PA, including two of three patients chronically infected with PA as determined by antibody status. Use of a daily GSH regimen appears to be associated in CF patients with significant improvement in lung function and weight, and a significant decline in bacteria cultured in this uncontrolled study. These findings bear further clinical investigation in larger, randomized, controlled studies...|$|E
40|$|Dynamic {{queries are}} a novel {{approach}} to information seeking that may enable users to cope with information overload. They allow users to see {{an overview of the}} database, rapidly (100 msec updates) explore and conveniently filter out unwanted information. Users fly through information spaces by <b>incrementally</b> <b>adjusting</b> a query (with sliders, buttons, and other filters) while continuously viewing the changing results. Dynamic queries on the chemical table of elements, computer directories, and a real estate database were built and tested in three separate exploratory experiments. These results show statistically significant performance improvements and user enthusiasm more commonly seen with video games. Widespread application seems possible but research issues remain in database and display algorithms, and user interface design. Challenges include methods for rapidly displaying and changing many points, colors, and areas; multidimensional pointing; incorporation of sound and visual displ [...] ...|$|R
40|$|This paper {{addresses}} fault-tolerant topology {{control in}} a heterogeneous {{wireless sensor network}} consisting of several resource-rich supernodes used for data relaying {{and a large number}} of energy constrained wireless sensor nodes. We introduce the k-degree Anycast Topology Control (k-ATC) problem with the objective of selecting each sensor’s transmission range such that each sensor is k-vertex supernode connected and the maximum sensor transmission power is minimized. Such topologies are needed for applications that support sensor data reporting even in the event of failures of up to k − 1 sensor nodes. We propose two solutions for the k-ATC problem: a greedy centralized algorithm that produces the optimal solution and a distributed and localized algorithm that <b>incrementally</b> <b>adjusts</b> sensors’ transmission range such that the k-vertex supernode connectivity requirement is met. Simulation results are presented to verify our approaches...|$|R
40|$|The {{baseline}} stability margins for NASA's Space Launch System (SLS) {{launch vehicle}} were generated via the classical approach of linearizing the system {{equations of motion}} and determining the gain and phase margins from the resulting frequency domain model. To improve the fidelity of the classical methods, the linear frequency domain approach can be extended by replacing static, memoryless nonlinearities with describing functions. This technique, however, {{does not address the}} time varying nature of the dynamics of a launch vehicle in flight. An alternative technique for the evaluation of the stability of the nonlinear launch vehicle dynamics along its trajectory is to <b>incrementally</b> <b>adjust</b> the gain and/or time delay in the time domain simulation until the system exhibits unstable behavior. This technique has the added benefit of providing a direct comparison between the time domain and frequency domain tools in support of simulation validation...|$|R
