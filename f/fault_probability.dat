99|155|Public
40|$|Abstract: In {{the paper}} we propose a {{dependable}} and efficient data aggregation scheme based on fault map that is constructed by estimated <b>fault</b> <b>probability</b> using Bayesian Belief Network (BBN). Cluster head will forward aggregated event depend on the total dependability of collected event in which we assign each detecting node a dependence weight. The dependence weight of sensor node is mapped from its <b>fault</b> <b>probability</b> estimated by cluster head. We propose a mapping function to map node’s <b>fault</b> <b>probability</b> into a dependence weight. Cluster head accumulates the dependence weight instead of number of source node and transmits the aggregated event when the threshold has reached. The simulation result shows that the approach even though take some extra delay time, it will increase the credibility and dependability of the aggregated event in the naturally unreliable wireless sensor network...|$|E
40|$|We {{consider}} the least-recently-used (LRU) cache replacement rule with a Zipf-type page request distribution and investigate the asymptotic {{property of the}} <b>fault</b> <b>probability</b> according to the increase of cache size. We first derive the asymptotics of the <b>fault</b> <b>probability</b> for the i. i. d. request model and then extend it to a general dependent request model, where our result shows that, under some weak assumptions, the <b>fault</b> <b>probability</b> is asymptotically invariant from the dependence in the page request process. An existing result has derived a similar result by applying a Poisson embedding technique, that is, it has given a continuous-time proof with some assumptions based on a continuous-time modeling. However, the Poisson embedding is just a technique for the proof {{and the problem is}} essentially on discrete-time basis, and thus, it is preferable to make assumptions, if any, directly in the discrete-time setting. We here consider a general dependent request process model and give a direct discrete-time proof under di#erent assumptions. A key to the proof is that the numbers of requests for respective items construct conditionally negatively associated random variables...|$|E
40|$|Fault {{troubleshooting}} aims {{to diagnose}} and repair faults {{at the highest}} efficacy and a minimum cost. The efficacy depends on multiple criteria like <b>fault</b> <b>probability,</b> cost, time, and risk of a repair action. This paper proposes a novel fault troubleshooting approach by combining Bayesian network with multicriteria decision analysis (MCDA). Automobile engine start-up failure {{is used as a}} case study. Bayesian network is employed to establish fault diagnostic model for reasoning and calculating standard values of uncertain criteria like <b>fault</b> <b>probability.</b> MCDA is adopted to integrate the influence of the four criteria and calculate utility value of the actions in each troubleshooting step. The approach enables a cost-saving, high efficient, and low risky troubleshooting...|$|E
40|$|Graduation date: 2004 Various {{methods of}} {{estimating}} the <b>fault</b> <b>probabilities</b> based on defect data of random defects seen in integrated circuit manufacturing are examined. Estimates of <b>fault</b> <b>probabilities</b> based on defect data are less costly than those based on critical area analysis and are potentially more reliable {{because they are}} based on actual manufacturing data. Due to limited sample size, means of estimating the confidence interval associated with these estimates are also examined. Because the mathematical expressions associated with defect data-based estimates of the <b>fault</b> <b>probabilities</b> are not amenable to analytical means of obtaining confidence intervals, bootstrapping was employed. The results show that one method of estimating the <b>fault</b> <b>probabilities</b> based on defect data proposed previously is not applicable when using typical in-line data. Furthermore, the results indicate that under typical fab conditions, the assumption of a Poisson random defect distribution gives accurate <b>fault</b> <b>probabilities.</b> The yields as predicted by the <b>fault</b> <b>probabilities</b> estimated from the limited yield concept and kill ratio and those estimated from critical area simulation are shown to be comparable to actual yields observed in the fab. It is also shown that with in-line data, the FP estimated for a given inspection step is a weighted average of the <b>fault</b> <b>probabilities</b> of the defect mechanisms operating at that inspection step. Four bootstrapped based methods of confidence interval estimation for <b>fault</b> <b>probabilities</b> of random defects are examined. The study is based on computer simulation of randomly distributed defects with pre-assigned <b>fault</b> <b>probabilities</b> on dice and the resulting count of different categories of die. The results show that all four methods perform well when the number of fatal defects is reasonably high but deteriorate in performance as the number of fatal defects decrease. The results also show that the BCA (bias-corrected and accelerated) method is more likely to succeed with a smaller number of fatal defects. This success is attributed to its ability to account for change of the standard deviation of the sampling distribution of the FP estimates with the PP of the population, and to account for median bias in the sampling distribution...|$|R
40|$|A {{comparison}} of the X-ray powder diffraction (XRPD) patterns of face-centered cubic silver with twin faults simulated with the DIFFaX and MAUD computer programs is presented. MAUD was used to refine the DIFFaX-simulated patterns to determine systematic differences between these simulation programs. The results indicated the Warren model of twin faults is applicable for <b>fault</b> <b>probabilities</b> < 10 % and DIFFaX and MAUD <b>fault</b> <b>probabilities</b> are different by a constant factor of four. Experimental patterns of nanocrystalline silver with twin faults were assessed using a graphical method to determine the twin-fault probability and crystal size. The results of the graphical method agreed well with the probability and size determined using MAUD. The sizes determined by these methods were consistent with particle sizes measured by light scattering and electron microscopy...|$|R
40|$|Qualitative and infinitesimal {{probability}} schemes {{are consistent}} with the axioms of probability theory, but avoid the need for precise numerical probabilities. Using qualitative probabilities could substantially reduce the effort for knowledge engineering and improve the robustness of results. We examine experimentally how well infinitesimal probabilities (the kappa-calculus of Goldszmidt and Pearl) perform a diagnostic task - troubleshooting a car that will not start by comparison with a conventional numerical belief network. We found the infinitesimal scheme {{to be as good as}} the numerical scheme in identifying the true fault. The performance of the infinitesimal scheme worsens significantly for prior <b>fault</b> <b>probabilities</b> greater than 0. 03. These results suggest that infinitesimal probability methods may be of substantial practical value for machine diagnosis with small prior <b>fault</b> <b>probabilities.</b> Comment: Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI 1994...|$|R
40|$|International audienceWe {{report on}} a step-induced {{morphological}} modification in Cobalt thin films (from 5 to 90 monolayers (ML)) on Au(1 1 1) substrates studied by grazing incidence X-ray diffraction (GIXD). When the cobalt film is deposited on the highly stepped Au(2 3 3) substrate its structure {{is found to be}} mainly face centered cubic (FCC) whereas its natural bulk structure is hexagonal close-packed (HCP). Relaxation and stacking <b>fault</b> <b>probability</b> have been determined using a simple diffraction model which fits all the GIXD data. A vicinal Co interface including this stacking <b>fault</b> <b>probability</b> and a small rotation of the Co(1 1 1) planes with respect to the Au(1 1 1) planes is proposed which is in exact registry with the Au vicinal surface...|$|E
40|$|AbstractLet G be a given graph (modelling a {{communication}} network) which we assume suffers from static edge faults: That is we let each edge of G be present independently with probability p (or absent with <b>fault</b> <b>probability</b> f= 1 −p). In particular, {{we are interested}} in robustness results for the case that the graph G itself is a random member of the class of all regular graphs with given degree d. Here we deal with expansion properties of faulty random regular graphs and show: For fixed d⩾ 42 and p=κ/d,κ⩾ 20, a random regular graph with <b>fault</b> <b>probability</b> f= 1 −p contains a linear-size subgraph which is an expander almost surely. This subgraph can be found by a simple linear-time algorithm...|$|E
40|$|The {{transmission}} lines {{are exposed to}} the atmosphere nature and will be affected by adverse weather such as lightning storm, so that it will affect the reliability of transmission system. This paper studies the <b>fault</b> <b>probability</b> model of transmission line during the lightning storm, and evaluates the short-term reliability of transmission system in the forecasting weather condition. Firstly, build the lightning strike <b>fault</b> <b>probability</b> model of the transmission line based on histor-ical lightning record information, then calculate the lightning strike probability under the fore-casting weather conditions, furthermore evaluate the reliability index of transmission system. Uti-lizing IEEE RTS- 79 system to verify the validity of the proposed model and the results show that lightning has great negative influence on the {{transmission lines}} and the reliability of transmission system. The reliability evaluation model proposed in this paper can guide the short-term opera-tion and online scheduling for transmission system operators...|$|E
40|$|We {{investigate}} {{the estimation of}} <b>fault</b> <b>probabilities</b> and yield for analog VLSI implementations of neural computation. Our analysis is limited to structures that can be mapped directly onto silicon as truly distributed parallel processing systems. Our work improves on the framework suggested recently by Feltham and Maly [3] and is also applicable to analog or mixed analog/digital VLSI systems...|$|R
40|$|This paper {{presents}} a general diagnostic {{system that can}} be applied to semiconductor equipment to assist the operator in finding the causes of decreased machine performance. Based on conventional probability theory, the diagnostic system incorporates both shallow and deep level information. From the observed evidence, and from the conditional <b>probabilities</b> of <b>faults</b> initially supplied by machine experts (and subsequently updated by the system), the <b>fault</b> <b>probabilities</b> and their bounds are calculated, given a specified confidence level. The rate of convergence of the <b>fault</b> <b>probabilities</b> has been derived in detail in the paper, and the procedure for combining the estimates of conditional probabilities given by the machine experts has also been described in detail. We have implemented a software version of the diagnostic system, and tested it on real photolithography equipment malfunctions and performance drifts. Initial experimental results are encouraging. 1 Introduction Many controll [...] ...|$|R
40|$|The {{microstructural}} parameters {{like the}} average domain size, effective domain size {{at a particular}} crystallographic direction and microstrain within the domains of titanium and Ti- 5 %Ta- 2 %Nb, irradiated with 116 MeV O 5 + ion, have been characterized {{as a function of}} dose by X-Ray Diffraction Line Profile Analysis using different model based approaches. Dislocation Density and stacking <b>fault</b> <b>probabilities</b> have also been estimated from the analysis. The analysis revealed that there was a significant decrease of the average domain size with dose as compared to the unirradiated sample. The estimated values of dislocation density increased significantly for the irradiated samples and was found to be an order of magnitude more as compared to the unirradiated one. However, the dislocation density saturated with increase in dose. The deformation (stacking) <b>fault</b> <b>probabilities</b> were found to be negligible even with the increase in dose of irradiation...|$|R
40|$|AbstractAn {{algorithm}} is given {{for determining the}} stack distance density function for a system described by a first order Markov Chain. The {{algorithm is}} developed for and applied to a first order Markov model of program behavior. It is used to obtain an approximation to the page <b>fault</b> <b>probability</b> in a virtual memory system operating under demand paging and LRU replacement rule...|$|E
40|$|Majority of {{practical}} caching algorithms, in particular {{those used in}} the World Wide Web applications, {{are based on the}} so-called Least-Recently-Used (LRU) cache replacement heuristic whose desirable attributes include low complexity, quick adaptability and high cache hit (low <b>fault)</b> <b>probability.</b> Recent studies have developed asymptotic characterization of the LRU <b>fault</b> <b>probability</b> for the generalized Zipf’s (power) law request distributions. In this paper, we extend these results to include the distributions that decay faster than power laws but slower than exponential, hence named moderately heavy distributions. Informally, for these types of distributions and the independent reference model, the main result of this paper shows that the ratio between the cache fault probabilities of the LRU heuristic and the optimal static algorithm is, for large caches, equal to e γ ≈ 1. 78, where γ is Euler’s constant. Interestingly enough, this limiting ratio is constant, i. e., it is invariant to the underlying characteristics of the request distributions...|$|E
40|$|Let G be a given graph (modelling a {{communication}} network) which we assume suffers from static edge faults: That is we let each edge of G be present independently with probability p (or absent with <b>fault</b> <b>probability</b> f = 1 Γ p). In particular {{we are interested}} in robustness results for the case that the graph G itself is a random member of the class of all regular graphs with given degree d. Here we deal with expansion properties of faulty random regular graphs and show: For d 42, fixed and p = =d; 20, a random regular graph with <b>fault</b> <b>probability</b> f = 1 Γ p contains a linear-sized subgraph which is an expander almost surely. This subgraph can be found by a simple linear-time algorithm. Introduction Modern multiprocessor architectures and communication networks compute over structured interconnection graphs like meshes. Here several applications share the same network while executing concurrently. This may of course lead to unavailability of links and nodes in certain c [...] ...|$|E
30|$|We have {{presented}} two algorithms for fault estimation. Both {{of them are}} based on the proposed by the first two authors the Field Kalman Filter. Algorithm 1, i.e., direct application of FKF is attractive because it can be easily used for determination of <b>fault</b> <b>probabilities,</b> providing level of confidence in fault detection. There is however some numerical sensitivity connected to computations close to 0 / 0 operation.|$|R
40|$|This {{research}} {{focuses on}} developing a model to predict the yield of {{a printed circuit board}} manufactured on a given assembly line. Based on an extensive literature review as well as discussion with industrial partners, it was determined that there is no tool available for assisting engineers in determining reliable estimates of their production capabilities as they introduce new board designs onto their current production lines. Motivated by this need, a more in-depth study of manufacturing yield as well as the electronic assembly process was undertaken. The relevant literature research was divided into three main fields: process modeling, board design, and PCB testing. The model presented in this research combines elements from process modeling and board design into a single yield model. An optimization model was formulated to determine the <b>fault</b> <b>probabilities</b> that minimize the difference between actual yield values and predicted yield values. This model determines <b>fault</b> <b>probabilities</b> (per component type) based on past production yields for the different board designs assembled. These probabilities are then used t...|$|R
40|$|On-board vehicle {{diagnostic}} systems {{must have}} low development and hardware costs {{in order to}} be viable. Modelbased methods have shown promise since they use analytical redundancy to reduce costly physical redundancy. However, these methods must also be computationally efficient and function accurately even with simple, low-cost models. The approach presented in this paper uses multiple simple models to analyze dissimilar observable modes of a system. Residuals generated using the models are related and interpreted in a Bayesian network to determine <b>fault</b> <b>probabilities</b> and yield a diagnosis. The technique is demonstrated with a diagnostic system for automobile handling...|$|R
40|$|Abstract We {{study the}} problem of how {{resilient}} networks are to node faults. Specifically, we investigate the questionof how many faults a network can sustain and still contain a large (i. e., linear-sized) connected component with approximately the same expansion as the original fault-free network. We use a pruning technique that culls awaythose parts of the faulty network that have poor expansion. The faults may occur at random or be caused by an adversary. Our techniques apply in either case. In the adversarial setting, we prove that for every network withexpansion ff, a large connected component with basically the same expansion as the original network exists for upto a constant times ff * n faults. We show this result is tight in the sense that every graph G of size n and uniformexpansion ff(*) can be broken into components of size o(n) with!(ff(n) * n) faults. Unlike the adversarial case, the expansion of a graph gives a very weak bound on its resilience to random faults. While it is the case, as before, that there are networks of uniform expansion Ω (1 / log n) that are not resilient again-st a <b>fault</b> <b>probability</b> of a constant times 1 / log n, it is also observed that there are networks of uniform expansion O(1 /pn) that are resilient against a constant <b>fault</b> <b>probability.</b> Thus, we introduce a different parameter, called thespan of a graph, which gives us a more precise handle on the maximum <b>fault</b> <b>probability.</b> We use the span to sho...|$|E
40|$|We {{study the}} problem of how {{resilient}} networks are to node faults. Specifically, we investigate the question of how many faults a network can sustain and still contain a large (i. e., linear-sized) connected component with approximately the same expansion as the original fault-free network. We use a pruning technique that culls away those parts of the faulty network that have poor expansion. The faults may occur at random or be caused by an adversary. Our techniques apply in either case. In the adversarial setting, we prove that for every network with expansion α, a large connected component with basically the same expansion as the original network exists for up to a constant times α · n faults. We show this result is tight in the sense that every graph G of size n and uniform expansion α(·) can be broken into components of size o(n) with ω(α(n) · n) faults. Unlike the adversarial case, the expansion of a graph gives a very weak bound on its resilience to random faults. While it is the case, as before, that there are networks of uniform expansion Ω(1 / log n) that are not resilient against a <b>fault</b> <b>probability</b> of a constant times 1 / log n, it is also observed that there are networks of uniform expansion O(1 / √ n) that are resilient against a constant <b>fault</b> <b>probability.</b> Thus, we introduce a different parameter, called the span of a graph, which gives us a more precise handle on the maximum <b>fault</b> <b>probability.</b> We use the span to show the first known results for the effect of random faults on the expansion of d-dimensional meshes. ...|$|E
40|$|We analyze a {{class of}} {{randomized}} Least-Recently-Used (LRU) cache replacement algorithms under the independent reference model with generalized Zipf’s law request probabilities. The randomization was recently proposed for Web caching as a mechanism that discriminates between different document sizes. In particular, a requested document that is {{not found in the}} cache either replaces a necessary number of least recently used documents with probability depending on its size or the cache is left unchanged. In this framework, we provide explicit asymptotic characterization of the cache <b>fault</b> <b>probability.</b> Using the derived result we prove that the asymptotic performance of this class of algorithms is optimized when the randomization probabilities are chosen inversely proportional to document sizes. In addition, for this optimized and easy to implement policy, we show that its performance is within a constant factor from the optimal static algorithm. Keywords: randomized least-recently-used caching, Zipf’s law distribution, variable document sizes, Web caching, cache <b>fault</b> <b>probability,</b> averages-case analysis This work is supported by the NSF Grant No. 0092113. Submitted to Combinatorics, Probability & Computing. ...|$|E
40|$|This paper proposes {{an exact}} cell layout {{synthesis}} tech-nique {{to minimize the}} <b>probability</b> of wiring <b>faults</b> due to spot defects. We modeled the <b>probability</b> of <b>faults</b> on intra-cell routings with considering the spot defects size distribution and the end effect of critical areas. By using the model as a cost function, we comprehensively generate the minimum width layout of CMOS logic cells and select the optimum layouts. Experimental results show that our technique re-duces about 15 % of the <b>fault</b> <b>probabilities</b> compared with the wire-length-minimum layouts for CMOS logic circuits which have up to 14 transistors. 1...|$|R
40|$|The single {{crystals}} of InxMoSe₂ (0 ≤ x ≤ 1) and Re-doped MoSe₂ viz. MoRe₀. ₀₅Se₁. ₉₉₅, MoRe₀. ₀₀₁Se₁. ₉₉₉ and Mo₀. ₉₉₅Re₀. ₀₀₅Se₂ {{have been}} grown by a direct vapour transport technique (DVT) in the laboratory. Structural characterization of these crystals was made using the XRD method. The particle size {{for a number}} of reflections has been calculated using the Scherrer formula. There are considerable variations appearing in deformation (α) and growth (β) <b>fault</b> <b>probabilities</b> in InxMoSe₂ (0 ≤ x ≤ 1) and Re-doped MoSe₂ single crystals due to their off-stoichiometry, which possesses the stacking fault in the single crystal...|$|R
40|$|Abstract — As {{the feature}} size shrinks to the {{nanometer}} scale, SRAM-based FPGAs {{will become increasingly}} vulnerable to soft errors. Existing reliability-oriented placement and routing approaches primarily focus on reducing the <b>fault</b> occurrence <b>probability</b> (node error rate) of soft errors. However, our analysis shows that, besides the <b>fault</b> occurrence <b>probability,</b> the propagation probability (error propagation probability) {{plays an important role}} and should be taken into consideration. In this paper, we first propose a cube-based analysis algorithm to efficiently and accurately estimate the error propagation probability. Based on such a model, we propose a novel reliability-oriented placement and routing algorithm that combines both the <b>fault</b> occurrence <b>probability</b> and the error propagation probability together to enhance system-level robustness against soft errors. Experimental results show that, compared with the baseline versatile place and route technique, the proposed scheme can reduce the failure rate by 20. 73 %, and increase the mean time between failures by 39. 44 %. Index Terms — Cube-based analysis, failure rate, field-programmable gate arrays (FPGAs), mean time between failures (MTBFs), placement and routing, soft error mitigation. I...|$|R
40|$|Abstract In {{this paper}} we study {{the problem of}} how {{resilient}} networks are to node faults. Specifically, we investigatethe question of how many faults a network can sustain so that it still contains a large (i. e. linear-sized) connected component that still has approximately the same expansion as the original fault-free network. For this we apply apruning technique which culls away parts of the faulty network which have poor expansion. This technique can be applied to both adversarial faults and to random faults. For adversarial faults we prove that for every network withexpansion ff, a large connected component with basically the same expansion as the original network exists for up toa constant times ff * n faults. This result is tight in the sense that every graph G of size n and uniform expansion ff(*),i. e. G has an expansion of Θ (ff(n)) and every subgraph G 0 of size m of G has an expansion of O(ff(m)), can bebroken into sublinear components with!(ff(n) * n) faults. For random faults we observe that the situation is significantly different. In this case the expansion of a graph only gives a very weak bound on its resilience to random faults. Specifically, there are networks of uniform expansion O(1 /pn) that are resilient against a constant <b>fault</b> <b>probability</b> but there are also networks of uniform expansion Ω (1 / log n) that are not resilient against a O(1 / log n) <b>fault</b> <b>probability.</b> Thus, a different parameter is needed. Forthis we introduce the span of a graph which allows us to determine the maximum <b>fault</b> <b>probability</b> in a much better way than the expansion can. We use the span to show the first known results for the effect of random faults on theexpansion of d-dimensional meshes...|$|E
40|$|In {{this paper}} we study {{the problem of}} how {{resilient}} networks are to node faults. Specifically, we investigate the question of how many faults a network can sustain so that it still contains a large (i. e. linear-sized) connected component that still has approximately the same expansion as the original fault-free network. For this we apply a pruning technique which culls away parts of the faulty network which have poor expansion. This technique can be applied to both adversarial faults and to random faults. For adversarial faults we prove that for every network with expansion alpha, a large connected component with basically the same expansion as the original network exists for up to a constant times alpha n faults. This result is tight in the sense that every graph G of size n and uniform expansion alpha(.), i. e. G has an expansion of alpha(n) and every subgraph G' of size m of G has an expansion of O(alpha(m)), can be broken into sublinear components with omega(alpha(n) n) faults. For random faults we observe that the situation is significantly different, because in this case the expansion of a graph only gives a very weak bound on its resilience to random faults. More specifically, there are networks of uniform expansion O(sqrt{n}) that are resilient against a constant <b>fault</b> <b>probability</b> but there are also networks of uniform expansion Omega(1 /log n) that are not resilient against a O(1 /log n) <b>fault</b> <b>probability.</b> Thus, a different parameter is needed. For this we introduce the span of a graph which allows us to determine the maximum <b>fault</b> <b>probability</b> in a much better way than the expansion can. We use the span to show the first known results for the effect of random faults on the expansion of d-dimensional meshes. Comment: 8 pages; to appear at SPAA 200...|$|E
40|$|This paper {{presents}} an improved measure for the dynamic functionality of a logic circuit, called delay <b>fault</b> <b>probability</b> (DFP). The new measure reflects both the nominal delay of the paths {{and the fact}} that only few paths are critical for path delay fault testing. An efficient distributed algorithm for computing DFP is presented. The experimental results show that, in contrast to DFP, the conventional fault coverage is an inadequate criterion to assure the dynamic functionality of a circuit...|$|E
40|$|Advanced RAIM, or ARAIM, extends and {{improves}} upon the traditional RAIM algorithm {{to detect and}} mitigate independent, multiple, and correlated GPS signal faults. While ARAIM depends on multiple redundant satellites in view and will perform best for multi-constellation users, the modernization of the GPS satellite constellation and Operational Control Segment provides a basis for future improvements in ARAIM availability for GPS-only users due to improved ranging accuracy and lower satellite <b>fault</b> <b>probabilities.</b> This paper quantifies the effects of GPS modernization to evaluate the capability of ARAIM for military dual-frequency (L 1 -L 2) users. Different mixes of GPS satellite types (Blocks IIA, IIR, IIR-M, IIF, and III) are generate...|$|R
40|$|At room {{temperature}} ytterbium has a predominantly fcc structure. However, {{a small amount}} of the hcp structure corresponding to the low- as well as the high-temperature phases is also known to coexist with the room-temperature fcc structure. We have earlier reported that mechanical deformation of 99. 2 percent pure ytterbium specimens at {{room temperature}}, induces a partial structural transformation of the type fcc yields hcp. Based on the stacking <b>fault</b> <b>probabilities,</b> the matching of the unit cells of the fcc and the deformation induced hcp phases and the recrystallization, it is concluded that the deformation-induced transformation in ytterbium is essentially a stacking fault effect. 19 Refs...|$|R
40|$|In this paper, a {{new method}} for {{quantitative}} security risk assessment of complex systems is presented, combining Fault Tree Analysis, traditionally used in reliability analysis, with the recently introduced Attack Tree Analysis, {{which allows the}} study of malicious attack patterns. The combined use of fault trees and attack trees helps the analyst to effectively face the security challenges posed {{by the introduction of}} modern ICT technologies in the control systems of critical infrastructures. Formal definitions of Fault Tree and Attack Tree are provided and a mathematical model for the calculation of system <b>fault</b> <b>probabilities</b> is presented. JRC. G. 6 -Sensors, radar technologies and cybersecurit...|$|R
40|$|A {{range of}} Fe–Mn–Si-based shape memory alloys has been {{investigated}} to examine the interplay of composition, stacking <b>fault</b> <b>probability</b> (SFP) and Neél temperature on the shape memory effect (SME). It {{has been found that}} the SFP (inversely proportional to stacking fault energy) showed little correlation to the SME for the range of alloy compositions examined. Further, the Neél temperature was not found to exhibit a significant effect on the SME. The addition of interstitial elements, however, was found to markedly decrease the SME. <br /...|$|E
40|$|In {{countries}} {{with a high}} ambient temperature and strong solar irradiation, transformer winding hot-spot temperature may increase over its maximum permissible limit. This can considerably reduce the insulation life of the transformer by enhanced degradation of the paper insulation. According to current loading guides, for each 6 K increase in working temperature, the ageing rate increases with approximately a factor two. Therefore, {{it is important to}} take into consideration the impact of the sun on the power transformer thermal behavior. In this paper, a modified hot-spot temperature model is presented to account for the effect of transformer winding temperature rise by solar irradiation. The effects of solar irradiation on transformer winding paper insulation are shown by comparing the degree of polymerisation (DP), the <b>fault</b> <b>probability</b> and the remaining life. Here, the <b>fault</b> <b>probability</b> is defined as the probability that the estimated DP-value at a certain moment in time is below a certain end-of-life criterion (threshold value). An additional winding hot-spot temperature rise of 9 K during the summer and a temperature rise of 6 K during the winter may occur in {{countries with}} strong solar irradiation. This may result in a reduction of the remaining lifetime by up to 40 %...|$|E
30|$|Concerning the {{substations}} adopting EM type {{instrument transformers}} and secondary cable wiring, the transformer of current (TA) secondary cable is relatively {{long and the}} secondary load is heavy, which will easily induce saturation. Long secondary cable, large distribution area, complicated routes, large capacitance to ground, and serious interference and insulation problems bring about high <b>fault</b> <b>probability</b> in electrical secondary circuits, thus affecting the system reliability, in addition, the design, construction, debugging, maintenance, examination and repair, and reconstruction and expansion of the secondary circuit of traditional substations involve {{a large amount of}} work.|$|E
40|$|The single {{crystals}} of InxMoSe 2 10 x and Re-doped MoSe 2 viz. MoRe 0. 005 Se 1. 995, MoRe 0. 001 Se 1. 999 and Mo 0. 995 Re 0. 005 Se 2 {{have been}} grown by a direct vapour transport technique (DVT) in the laboratory. The structural characterizations of these crystals {{are made by}} XRD method. The particle size {{for a number of}} reflections has been calculated using the Scherrer’s formula. There are considerable variations are shown in deformation () and growth () <b>fault</b> <b>probabilities</b> in InxMoSe 2 10 x and Re-doped MoSe 2 single crystals due to off-stoichiometry, which possesses the stacking fault in the single crystal...|$|R
40|$|Abstract: This paper {{presents}} a distributed adaptive scheme for detecting faults in wireless sensor networks. Each sensor nodes makes a local {{decision based on}} the comparisons of its own readings with those of neighbors, along with the dissemination {{of the decision to}} them, if necessary. At the end of each fault detection cycle, each node dynamically adjusts critical parameters in the distributed fault detection algorithm, such as node degree and thresh-olds, resulting in high performance {{for a wide range of}} <b>fault</b> <b>probabilities.</b> By extensive computer simulation the scheme is shown to be scalable with the number of faulty sensor nodes except for sparse networks where the aver-age node degree is extremely low. Key–Words: sensor networks, distributed, adaptive, fault detection, fault diagnosis...|$|R
40|$|The {{architectural}} regularity of FPGAs {{provides an}} inherent redundancy {{which can be}} exploited for fault tolerance and yield enhancement. In this paper we examine the problem of reconfiguring the placement of a circuit on an FPGA to tolerate a given fault pattern in the array of CLBs. The primary objective of the placement reconfiguration is to minimize timing degradation. The concept of a slack neighborhood graph {{is used as a}} general tool for timing driven reconfiguration with a low increase in critical path delay. Our algorithm simultaneously achieves both provably low timing degradation and low re-programming cost. For a wide range of <b>fault</b> <b>probabilities</b> and circuits our algorithm successfully reconfigures the placement with less than 1 % degradation in the circuit delay...|$|R
