323|31|Public
5000|$|In <b>Instance-based</b> <b>learning,</b> regularization can be {{achieved}} varying the mixture of prototypes and exemplars.|$|E
50|$|In machine {{learning}}, <b>instance-based</b> <b>learning</b> (sometimes called memory-based learning) is {{a family}} of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory.|$|E
5000|$|... k-NN {{is a type}} of <b>instance-based</b> <b>learning,</b> or lazy learning, {{where the}} {{function}} is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.|$|E
40|$|This {{paper is}} a {{practical}} guide to the application of <b>instance-based</b> machine <b>learning</b> techniques to the solution of a financial problem. A broad class of instance-based families is considered for classification using the WEKA software package. The problem selected for analysis is a common one in financial and econometric work: the use of publicly available economic data to forecast future changes in a stock market index. This paper examines various stages {{in the analysis of}} this problem including: identification of the problem, considerations in obtaining and preprocessing data, model and parameter selection, and interpretation of results. Finally, the paper offers suggestions of areas of future study for applying <b>instance-based</b> machine <b>learning</b> in the setting of solving financial problems. 1...|$|R
40|$|<b>Instance-based</b> machine <b>learning</b> {{techniques}} {{have been applied}} to predict the next chord {{in a piece of}} jazz music. This study compares off-line (trained on a static data base of songs) and on-line (trained on-the-fly) prediction techniques. A hybrid model combines both on-line and off-line models. On average, chords in a jazz piece are predictable 53...|$|R
40|$|This paper {{concerns}} learning {{tasks that}} require the prediction of a continuous value rather than a discrete class. A general method is presented that allows predictions to use both <b>instance-based</b> and model-based <b>learning.</b> Results with three approaches to constructing models and with eight datasets demonstrate improvements due to the composite method...|$|R
50|$|Examples of <b>instance-based</b> <b>learning</b> {{algorithm}} are the k-nearest neighbor algorithm, kernel {{machines and}} RBF networks. These store (a subset of) their training set; when predicting a value/class {{for a new}} instance, they compute distances or similarities between this instance and the training instances to make a decision.|$|E
50|$|It {{is called}} {{instance-based}} because it constructs hypotheses {{directly from the}} training instances themselves.This means that the hypothesis complexity can grow with the data: in the worst case, a hypothesis {{is a list of}} n training items and the computational complexity of classifying a single new instance is O(n). One advantage that <b>instance-based</b> <b>learning</b> has over other methods of machine learning is its ability to adapt its model to previously unseen data. Instance-based learners may simply store a new instance or throw an old instance away.|$|E
50|$|Godfried T. Toussaint is a Professor of Computer Science and the Head of the Computer Science Program at New York University Abu Dhabi (NYUAD) in Abu Dhabi, United Arab Emirates. He does {{research}} on {{various aspects of}} computational geometry, discrete geometry, and their applications: pattern recognition (k-nearest neighbor algorithm, cluster analysis), motion planning, visualization (computer graphics), knot theory (stuck unknot problem), linkage (mechanical) reconfiguration, the art gallery problem, polygon triangulation, the largest empty circle problem, unimodality (unimodal function), and others. Other interests include meander (art), compass and straightedge constructions, <b>instance-based</b> <b>learning,</b> music information retrieval, and computational music theory.|$|E
40|$|Abstract. In {{this work}} {{we focus on}} the {{determination}} of the relative distributions of young, intermediate-age and old populations of stars in galaxies. Starting from a grid of theoretical population synthesis models we constructed a set of model galaxies with a distribution of ages, metallicities and intrinsic reddening. Using this set we have explored a new fitting method that presents several advantages over conventional methods. We propose an optimization technique that combines active <b>learning</b> with an <b>instance-based</b> machine <b>learning</b> algorithm. Experimental results show that this method can estimate with high speed and accuracy the physical parameters of the stellar populations. 1...|$|R
40|$|Abstract This paper {{presents}} work {{on using}} hierarchical {{long term memory}} to reduce the memory requirements of nearest sequence memory (NSM) learning, a previously published, <b>instance-based</b> reinforcement <b>learning</b> algorithm. A hierar-chical memory representation reduces the memory requirements by allowing traces to share common sub-sequences. We present moderated mechanisms for estimat-ing discounted future rewards and for dealing with hidden state using hierarchical memory. We also present an experimental analysis of how the sub-sequence length affects the memory compression achieved and show that the reduced memory re-quirements do not effect the speed of learning. Finally, we analyse and discuss {{the persistence of the}} sub-sequences independent of specific trace instances. ...|$|R
40|$|In {{this work}} {{we focus on}} the {{determination}} of the relative distributions of young, intermediate-age and old populations of stars in galaxies. Starting from a grid of theoretical population synthesis models we constructed a set of model galaxies with a distribution of ages, metallicities and intrinsic reddening. Using this set we have explored a new fitting method that presents several advantages over conventional methods. We propose an optimization technique that combines active <b>learning</b> with an <b>instance-based</b> machine <b>learning</b> algorithm. Experimental results show that this method can estimate with high speed and accuracy the physical parameters of the stellar populations. Comment: 4 pages, 1 figure, To appear in Proceedings of ADASS XIII, Strasbourg, October 200...|$|R
5000|$|The <b>Instance-Based</b> <b>Learning</b> Theory (IBLT) is {{a theory}} of how humans make {{decisions}} in dynamic tasks developed by Cleotilde Gonzalez, Christian Lebiere, and Javier Lerch. The theory has been extended to two different paradigms of dynamic tasks, called sampling and repeated-choice, by Cleotilde Gonzalez and Varun Dutt. Gonzalez and Dutt [...] have shown that in these dynamic tasks, IBLT provides the best explanation of human behavior and performs better than many other competing models and approaches. According to IBLT, individuals rely on their accumulated experience to make decisions by retrieving past solutions to similar situations stored in memory. Thus, decision accuracy can only improve gradually and through interaction with similar situations.|$|E
5000|$|The {{ultimate}} goal of the PRAC system is to make knowledge about everyday activities from websites like wikihow.com available for service robots, such that they can autonomously acquire new high-level skills by browsing the Web. PRAC addresses the problem that natural language is inherently vague and unspecific. To this end, PRAC maintains probabilistic first-order knowledge bases over semantic networks represented in Markov logic networks. As opposed to other semantic learning initiatives like NELL or IBM's Watson, PRAC does not aim at answering questions in natural language, but to disambiguate and infer information pieces that are missing in natural-language instructions, such {{that they can be}} executed by a robot. [...] "This problem formulation is substantially different to the problem of text understanding for question answering or machine translation. In those reasoning tasks, the vagueness and ambiguity of natural-language expressions can often be kept and translated into other languages. In contrast, robotic agents have to infer missing information pieces and disambiguate the meaning of the instruction in order to perform the instruction successfully." [...] In addition to probabilistic relational models, PRAC uses the prinicples of analogical reasoning and <b>instance-based</b> <b>learning</b> to infer completions of roles in semantic networks.|$|E
40|$|AbstractA {{method of}} <b>instance-based</b> <b>learning</b> is {{introduced}} which {{makes use of}} possibility theory and fuzzy sets. Particularly, a possibilistic version of the similarity-guided extrapolation principle underlying the <b>instance-based</b> <b>learning</b> paradigm is proposed. This version is compared to the commonly used probabilistic approach from a methodological point of view. Moreover, aspects of knowledge representation such as the modeling of uncertainty are discussed. Taking the possibilistic extrapolation principle {{as a point of}} departure, an <b>instance-based</b> <b>learning</b> procedure is outlined which includes the handling of incomplete information, methods for reducing storage requirements and the adaptation of the influence of stored cases according to their typicality. First theoretical and experimental results showing the efficiency of possibilistic <b>instance-based</b> <b>learning</b> are presented as well...|$|E
30|$|Both {{simulated}} and naturalistic driving patterns {{have been}} studied in the literature using different features extracted mainly from the in-vehicle’s CAN Bus (the steering wheel, the vehicle speed, and the engine speed, etc.). The number of these features may range from one to twelve. Using these features, different machine learning methods (e.g. Bayesian algorithms, Decision Tree algorithms, <b>instance-based</b> algorithms, deep <b>learning</b> algorithms) have been proposed to learn driving styles.|$|R
40|$|Abstract. <b>Instance-based</b> {{transfer}} <b>learning</b> methods utilize labeled ex-amples {{from one}} domain to improve learning performance in another domain via knowledge transfer. Boosting-based transfer learning algo-rithms {{are a subset}} of such methods and have been applied successfully within the transfer learning community. In this paper, we {{address some of the}} weaknesses of such algorithms and extend the most popular transfer boosting algorithm, TrAdaBoost. We incorporate a dynamic factor into TrAdaBoost to make it meet its intended design of incorporating the ad-vantages of both AdaBoost and the “Weighted Majority Algorithm”. We theoretically and empirically analyze the effect of this important factor on the boosting performance of TrAdaBoost and we apply it as a “cor-rection factor ” that significantly improves the classification performance. Our experimental results on several real-world datasets demonstrate the effectiveness of our framework in obtaining better classification results...|$|R
30|$|The K-nearest {{neighbor}} (KNN) classifier {{is one of}} {{the simplest}} machine learning algorithms. It classifies foregrounds based on the closest training examples in the feature space and is a type of <b>instance-based,</b> or lazy, <b>learning</b> in which the function is only approximated locally, and all computation is deferred until classification [56]. The neighbors are selected from a set of foregrounds for which the correct classification is known, a set that can be considered the training set for the algorithm, though no explicit training step is required.|$|R
40|$|Abstract. This paper {{presents}} an inductive learning system called the Genetic <b>Instance-Based</b> <b>Learning</b> (GIBL) system. This system combines <b>instance-based</b> <b>learning</b> approaches with evolutionary computation {{in order to}} achieve high accuracy in the presence of irrelevant or redundant attributes. Evolutionary computation is used to find a set of attribute weights that yields a high estimate of classification accuracy. Results of experiments on 16 data sets are shown, and are compared with a non-weighted version of the <b>instance-based</b> <b>learning</b> system. The results indicate that the generalization accuracy of GIBL is somewhat higher than that of the non-weighted system on regular data, and is significantly higher on data with irrelevant or redundant attributes. 1...|$|E
40|$|A {{method of}} <b>instance-based</b> <b>learning</b> is {{introduced}} which {{makes use of}} possibility theory and fuzzy sets. Particularly, a possibilistic version of the similarity-guided extrapolation principle underlying the instancebased learning paradigm is proposed. This version is compared to the commonly used probabilistic approach from a methodological point of view. Moreover, aspects of knowledge representation such as the modeling of uncertainty are discussed. Taking the possibilistic extrapolation principle {{as a point of}} departure, an <b>instance-based</b> <b>learning</b> procedure is outlined which includes the handling of incomplete information, methods for reducing storage requirements and the adaptation of the influence of stored cases according to their typicality. First theoretical and experimental results showing the e#ciency of possibilistic <b>instance-based</b> <b>learning</b> are presented as well...|$|E
40|$|Adaptive {{clustering}} uses {{reinforcement learning}} to learn the reward values of successive data clusterings. Adaptive clustering applies when external feedback exists for a clustering task. It supports the reuse of clusterings by memorizing what worked well in a previous context. It explores multiple paths in a reinforcement learning environment when {{the goal is to}} find better cluster representatives based on arbitrary environmental feedback. Our experiments apply adaptive clustering to <b>instance-based</b> <b>learning</b> relying on a distance function modification approach. The results show that adaptive clustering can find better representatives, if compared with traditional <b>instance-based</b> <b>learning,</b> such as k-nearest neighbor classifiers. Moreover, we introduce as a by-product a new <b>instance-based</b> <b>learning</b> technique that classifies examples by solely using cluster representatives; the technique shows high promise in our experimental evaluation...|$|E
40|$|Different {{successful}} heuristic {{approaches have}} been proposed for solving combinatorial optimization problems. Commonly, each of them is specialized to serve a different purpose or address specific difficulties. However, most combinatorial problems that model real world applications have a priori well known measurable properties. Embedded machine learning methods may aid towards the recognition and utilization of these properties for the achievement of satisfactory solutions. In this paper, we present a heuristic methodology which employs the <b>instance-based</b> machine <b>learning</b> paradigm. This methodology can be adequately configured for several types of optimization problems which {{are known to have}} certain properties. Experimental results are discussed concerning two well known problems, namely the knapsack problem and the set partitioning problem. These results show that the proposed approach is able to find significantly better solutions compared to intuitive search methods based on heuristics which are usually applied to the specific problems. 1...|$|R
40|$|We have {{developed}} a method for fast and accurate stellar population parameters determination in order to apply it to high resolution galaxy spectra. The method {{is based on an}} optimization technique that combines active <b>learning</b> with an <b>instance-based</b> machine <b>learning</b> algorithm. We tested the method with the retrieval of the star-formation history and dust content in "synthetic" galaxies {{with a wide range of}} S/N ratios. The "synthetic" galaxies where constructed using two different grids of high resolution theoretical population synthesis models. The results of our controlled experiment shows that our method can estimate with good speed and accuracy the parameters of the stellar populations that make up the galaxy even for very low S/N input. For a spectrum with S/N= 5 the typical average deviation between the input and fitted spectrum is less than 10 **{- 5 }. Additional improvements are achieved using prior knowledge. Comment: 14 pages, 25 figures, accepted by Monthly Notice...|$|R
40|$|Reinforcement {{learning}} {{agents are}} faced with the problem of perceptual aliasing when two or more states are perceptually identical but require different actions. To address this problem, several researchers have incorporated memory of preceding events into the definition of states to distinguish perceptually-aliased states. Recently, McCallum (1995 b) has offered Utile Suffix Memory (USM), an instance-based algorithm using a tree to store instances and to represent states for reinforcement learning. USM's use of online <b>instance-based</b> state <b>learning</b> permits state definitions to be updated quickly based on the latest results of reinforcement learning. The use of a fringe (an extension of the tree to a prespecified depth below the `real' tree) provides the algorithm a limited degree of lookahead capability. However, the use of a fringe incurs a large cost in terms of tree size and provides only limited lookahead capability. We introduce a modification of USM, Greedy Utile Suffix Memory (GU [...] ...|$|R
40|$|Abstract. Even though <b>instance-based</b> <b>learning</b> {{performs}} well in practice, {{it might}} be criticized for its neglect of uncertainty: An estimation is usually given {{in the form of}} a predicted label, but without characterizing the confidence of this prediction. In this paper, we propose an instancebased learning method that allows for deriving “credible ” estimations, namely set-valued predictions that cover the true label of a query object with high probability. Our method is built upon a formal model of the heuristic inference principle underlying <b>instance-based</b> <b>learning.</b> ...|$|E
40|$|This paper {{presents}} PAC-learning {{analyses for}} <b>instance-based</b> <b>learning</b> algorithms for both symbolic and numeric-prediction tasks. The algorithms analyzed employ {{a variant of}} the k-nearest neighbor pattern classifier. The main results of these analyses are that the IB 1 <b>instance-based</b> <b>learning</b> algorithm can learn, using a polynomial number of instances, a wide range of symbolic concepts and numeric functions. In addition, we show that a bound on the degree of difficulty of predicting symbolic values may be obtained by considering the size of the boundary of the target concept, and a bound on the degree of difficulty in predicting numeric values may be obtained by considering the maximum absolute value of the slope between instances in the instance space. Moreover, the number of training instances required by IBl is polynomial in these parameters. The implications of these results for the practical application of <b>instance-based</b> <b>learning</b> algorithms are discussed...|$|E
40|$|This paper {{proposes a}} new {{algorithm}} for acquisition of preference predicates by a learning apprentice, termed Compositional <b>Instance-Based</b> <b>Learning</b> (CIBL), that permits multiple instances of a preference predicate to be composed, directly exploiting the transitivity of preference predicates. In an empirical evaluation, CIBL was consistently {{more accurate than}} a 1 -NN <b>instance-based</b> <b>learning</b> strategy unable to compose instances. The relative performance of CIBL and decision tree induction was found to depend upon (1) {{the complexity of the}} preference predicate being acquired and (2) the dimensionality of the feature space...|$|E
40|$|An {{important}} {{area of application}} for machine learning is in automating the acquisition of knowledge bases required for expert systems. In this paper, we review the major paradigms for machine learning, including neural networks, <b>instance-based</b> methods, genetic <b>learning,</b> rule induction, and analytic approaches. We consider rule induction in greater detail and review some of its recent applications, in each case stating the problem, how rule induction was used, {{and the status of}} the resulting expert system. In closing, we identify the main stages in elding an applied learning system and draw some lessons from successful applications...|$|R
40|$|Maintaining {{contact between}} the robot and plume is {{significant}} in chemical plume tracing (CPT). In the time immediately following the loss of chemical detection {{during the process of}} CPT, Track-Out activities bias the robot heading relative to the upwind direction, expecting to rapidly re-contact the plume. To determine the bias angle used in the Track-Out activity, we propose an online <b>instance-based</b> reinforcement <b>learning</b> method, namely virtual trail following (VTF). In VTF, action-value is generalized from recently stored instances of successful Track-Out activities. We also propose a collaborative VTF (cVTF) method, in which multiple robots store their own instances, and learn from the stored instances, in the same database. The proposed VTF and cVTF methods are compared with biased upwind surge (BUS) method, in which all Track-Out activities utilize an offline optimized universal bias angle, in an indoor environment with three different airflow fields. With respect to our experimental conditions, VTF and cVTF show stronger adaptability to different airflow environments than BUS, and furthermore, cVTF yields higher success rates and time-efficiencies than VTF...|$|R
40|$|Multi-label {{classification}} {{in which}} each instance may belong {{to more than one}} class is a challenging research problem. Recently, a considerable amount of research has been concerned with the development of "good" multi-label learning methods. Despite the extensive research effort, many scientific challenges posed by e. g. curse-of-dimensionality and correlation among labels remain to be addressed. In this paper, we propose a new approach to multi-label classification which combines stacked Kernel Discriminant Analysis using Spectral Regression (SR-KDA) with state-of-the-art <b>instance-based</b> multi-label (ML) <b>learning</b> method. The proposed system is validated on two multi-label databases. The results indicate significant performance gains when compared with the state-of-the art multi-label methods for multi-label classification...|$|R
40|$|A common {{practice}} in cognitive modeling is to develop new models specific to each particular task. We question this approach and draw on an existing theory, <b>instance-based</b> <b>learning</b> theory (IBLT), to explain learning behavior in three different choice tasks. The same <b>instance-based</b> <b>learning</b> model generalizes accurately to choices in a repeated binary choice task, in a probability learning task, and in a repeated binary choice task within a changing environment. We assert that, although the three tasks are different, the source of learning is equivalent and therefore, the cognitive process elicited should be captured by one single model. This evidence supports previous findings that <b>instance-based</b> <b>learning</b> is a robust learning process that is triggered {{in a wide range}} of tasks from the simple repeated choice tasks to the most dynamic decision making tasks. Copyright # 2010 John Wiley & Sons, Ltd. key words instance-based learning; repeated choice; decisions from experience; model comparison; probability learning; model generalizatio...|$|E
40|$|We propose an <b>instance-based</b> <b>learning</b> {{algorithm}} named IBRL 3 which acquires {{some kind}} of control rules not to fail in a dynamic environment, and we examine its performance via application to the cart-pole balancing problem. In this algorithm, a tuple of input, output and preference value of each execution cycle are stored in memory verbatim, and the action of each cycle is decided by retrieving the nearest neighbor of the current input data. The number of stored instances is reduced by replacing the nearest but less reliable instance by new one. Experimental results of computer simulation show that IBRL 3 is robust for distinct settings of parameters and for noisy environments, and is efficient enough to apply to some kinds of real-time control problems. Keywords: Applied adaptive behavior, Learning control, Cart-pole balancing problem, <b>Instance-based</b> <b>learning,</b> Reinforcement learning method. 1 Introduction We propose an <b>instance-based</b> <b>learning</b> algorithm named IBRL 3 which can be applied [...] ...|$|E
40|$|Adaptive {{clustering}} uses external {{feedback to}} improve cluster quality; past experience serves {{to speed up}} execution time. An adaptive clustering environment is proposed that uses Q-learning to learn the reward values of successive data clusterings. Adaptive clustering supports the reuse of clusterings by memorizing what worked well in the past. It has the capability of exploring multiple paths in parallel when searching for good clusters. In a case study, we apply adaptive clustering to <b>instance-based</b> <b>learning</b> relying on a distance function modification approach. A distance function adaptation scheme that uses external feedback is proposed and compared with other distance function learning approaches. Experimental {{results indicate that the}} use of adaptive clustering leads to significant improvements of <b>instance-based</b> <b>learning</b> techniques, such as k-nearest neighbor classifiers. Moreover, as a by-product a new <b>instance-based</b> <b>learning</b> technique is introduced that classifies examples by solely using cluster representatives; this technique shows high promise in our experimental evaluation. 1...|$|E
30|$|Even though {{transfer}} {{learning has}} many great applications in {{natural language processing}} and visual recognition [25, 28], not many studies have applied it to the network attack detection problem. Bekerman et al. [4] mentioned that transfer learning can improve robustness in detecting unknown malware between non-similar environments. However, they did not present much detailed and formal work on this idea. The study in [29] applied an <b>instance-based</b> transfer <b>learning</b> approach in network intrusion detection. However, they require plenty of labeled data from target domain. Gao et al. [30] proposed a model-based transfer learning approach {{and apply it to}} the KDD 99 cup network dataset. Both of these instance and model-based transfer learning approaches depend heavily on the assumption of homogeneous features. This is often not the case for network attack detection, which typically exhibits heterogeneous features. Another advantage of feature-based approaches is its flexibility to adopt different base classifiers according to different cases, which motivated us to derive a feature-based transfer learning approach for our network attack detection study. To our best knowledge, this paper is the first effort in applying a feature-based transfer learning approach for improving the robustness of network attack detection.|$|R
40|$|We {{present and}} test an {{instance}} model of associative learning. The model, Minerva-AL, treats associative learning as cued recall. Memory preserves {{the events of}} individual trials in separate traces. A probe presented to memory contacts all traces in parallel and retrieves a weighted sum of the traces, a structure called the echo. Learning of a cue–outcome relationship {{is measured by the}} cue’s ability to retrieve a target outcome. The theory predicts a number of associative learning phenomena, including acquisition, extinction, reacquisition, conditioned inhibition, external inhibition, latent inhibition, discrimination, generalization, blocking, overshadowing, overexpectation, superconditioning, recovery from blocking, recovery from overshadowing, recovery from overexpectation, backward blocking, backward conditioned inhibition, and second-order retrospective revaluation. We argue that associative learning is consistent with an <b>instance-based</b> approach to <b>learning</b> and memory...|$|R
40|$|International audiencePattern drift is {{a common}} issue for machine {{learning}} in real applications, as the distribution generating the data may change under nonstationary environmental/operational conditions. In our previous work, a strategy based on Feature Vector Selection (FVS) has been proposed for enabling a Support Vector Regression (SVR) model to adaptively update with streaming data, but the proposed strategy suffers from the incapability of treating recurring patterns. An <b>instance-based</b> online <b>learning</b> approach is proposed in this paper, which can adaptively update an SVR-based ensemble model with steaming data points. The proposed approach reduces the computational complexity of the updating process by selecting {{only part of the}} newly available data and allows following timely the ongoing patterns by resorting to FVS. The proposed approach creates new sub-models directly from a basic model and the sub-models represent separately the data stream at different periods. A dynamic ensemble selection strategy is integrated in the approach to select the sub-models most relevant to the new data point for deriving the prediction, while reducing the influence of the irrelevant ones. The weights of the different models in the ensemble are updated, based on their prediction errors. Comparison results with several benchmark approaches on several synthetic datasets and on the dataset concerning the leakage from the first seal in a Reactor Coolant Pump, prove the efficiency and accuracy of the proposed online learning ensemble approach...|$|R
