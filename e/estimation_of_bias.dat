19|10000|Public
40|$|In this paper, an {{improved}} method of model complexity selection for nonnative speech recognition is proposed by using maximum a posteriori <b>estimation</b> <b>of</b> <b>bias</b> distributions. An algorithm is described for estimating the hyper-parameters of the prior distributions, and an automatic accent detection algorithm is also proposed for integration with dynamic model selection and adaptation. Experiments {{were performed on}} the WSJ 1 task with American English speech, British accent speech, and mandarin Chinese accent speech. Results show {{that the use of}} prior knowledge of accents enabled reliable <b>estimation</b> <b>of</b> <b>bias</b> distributions in the case of very small amount of adaptation speech, or without adaptation speech. Recognition results show that the new approach is superior to the previous MEL method, especially when the adaptation data are extremely limited. 1...|$|E
40|$|The {{bootstrap}} (Efron, 1979, 1982) is a {{very simple}} resampling plan and has shown {{to be successful in}} estimating the bias and other measures of statistical error of a number of estimators. It gives freedom from the constraints of traditional parametric theory at the cost of performing the usual statistical calculations a hundred or a thousand times over. In this letter another example of bootstrap <b>estimation</b> <b>of</b> <b>bias</b> is given. It is interesting that Quenouille's (1949) jackknife (see Miller, 1964, for a review) fails completely in this case. Bootstrap bias...|$|E
40|$|ABSTRACT — It {{is shown}} a simple {{extension}} of the conventional behavioral characterization of amplifier nonlinearity {{can be used to}} quantify power amplifier performance including many memory effects. External variables that influence the amplifier behavior (such as power supply voltage, input bias or temperature) are identified. Measurements of gain and phase (AM-AM and AM-PM conversion) are subsequently made over a range of these external variables. The variation of the external variables is explicitly taken into account with linear equivalent circuits at baseband. The method is shown to be useful for the <b>estimation</b> <b>of</b> <b>bias</b> circuit effects and self-heating effects. I...|$|E
40|$|This paper {{considers}} the <b>estimation</b> <b>of</b> population mean under a super population model and presents {{a class of}} improved estimators. Dominance of this class over the conventional unbiased estimator with respect to predictive mean squared error is studied and a simple condition is deduced. Unbiased <b>estimation</b> <b>of</b> the <b>bias</b> and efficiency gain is also discussed. Survey sampling Prediction Linear regression Super population Stein-rule estimation...|$|R
40|$|Definitions of the {{concepts}} <b>of</b> <b>bias</b> and recovery are discussed and approaches to dealing with them described. The Guide To Uncertainty in Measurement (GUM) recommends correction for all significant systematic effects, {{but it is also}} possible to expand measurement uncertainty to take account <b>of</b> uncorrected <b>bias.</b> Run, laboratory and method bias can be defined as components <b>of</b> the <b>bias</b> <b>of</b> a particular measurement result, and can be useful as concepts used in method validation. <b>Estimation</b> <b>of</b> run <b>bias</b> allows a simplification <b>of</b> the <b>estimation</b> <b>of</b> measurement uncertainty. Multivariate calibration brings its own biases that must be quantified and minimised. © 2007 Elsevier B. V. All rights reserved...|$|R
40|$|Magnetometers {{are widely}} used for LEO small {{satellites}} attitude determination and control system. In order to estimate satellite dynamics and control attitude accurately, scale factor and <b>bias</b> <b>of</b> magnetometer must be estimated. In this study a linear Kalman filter (LKF) based algorithm for the <b>estimation</b> <b>of</b> magnetometer <b>biases</b> and scale factors is proposed. Proposed algorithms are simulated through attitude dynamics of a small satellite...|$|R
40|$|Abstract—We present two {{results on}} {{attitude}} estimation using vector and rate gyro measurements. The first result concerns an observer previously presented by Hamel, Mahony, and Pflimlin, with proven stability results when (i) the reference vectors are stationary; or (ii) the gyro measurements are unbiased. We prove semiglobal stability without {{either of these}} assumptions when a parameter projection is added, and convergence from all initial attitudes when using a resetting strategy. The second result is an algorithm for <b>estimation</b> <b>of</b> <b>bias</b> in the body-fixed vector measurements, which is analyzed {{in combination with the}} attitude and gyro bias observer. Index Terms—Navigation, estimation I...|$|E
40|$|The paper {{deals with}} the {{uncertainty}} in measurement based on digital signal processing algorithms, like those achievable with the virtual instruments. The correct <b>estimation</b> <b>of</b> <b>bias</b> and uncertainty is discussed with reference to a simple case study. Three possible approaches to this question are examined and compared. It is shown how a Monte Carlo method, based on numerical simulations and implemented with commercial software packages, can allow virtual instruments to perform an auto-evaluation of both bias and uncertainty affecting their results. Some theoretical considerations, computer simulations and experimental tests are shown to support the proposed technique...|$|E
40|$|This paper {{presents}} the calibration results and uncertainty {{analysis of a}} high-precision reference pressure measurement system currently used in wind tunnels at the NASA Langley Research Center (LaRC). Sensors, calibration standards, and measurement instruments are subject to errors due to aging, drift with time, environment effects, transportation, the mathematical model, the calibration experimental design, and other factors. Errors occur at every link {{in the chain of}} measurements and data reduction from the sensor to the final computed results. At each link of the chain, bias and precision uncertainties must be separately estimated for facility use, and are combined to produce overall calibration and prediction confidence intervals for the instrument, typically at a 95 % confidence level. The uncertainty analysis and calibration experimental designs used herein, based on techniques developed at LaRC, employ replicated experimental designs for efficiency, separate <b>estimation</b> <b>of</b> <b>bias</b> a [...] ...|$|E
30|$|A common {{characteristic}} of trade data is {{the existence of}} missing and zero trade values. As the natural logarithm of zero does not exist, the estimated regressions do not consider the zero trade values that are important information about the trade pattern. The dropped data are information not used in the estimation, possibly leading to a bias in the regression results. 14 To avoid this problem, we estimate the regressions using the pseudo-Poisson maximum likelihood (PPML) method (Santos Silva and Tenreyro 2006). The PPML is the most accepted technique in the gravity model literature, allowing us {{to account for the}} observations with zero trade values. 15 In addition, trade data are plagued by heteroscedasticity, consequently the use of OLS can lead to the <b>estimation</b> <b>of</b> <b>biased</b> elasticities (Santos Silva and Tenreyro 2006).|$|R
40|$|The {{focus of}} this article is on the {{accelerometer}} on board the two GRACE (Gravity Recovery and Climate Experiment) satellites. In a first analysis the accelerometer system is studied. The behavior of the test mass and its capacitive feedback system is simulated in the time and the frequency domain for one degree of freedom. Only linear accelerations are considered so far. The second part of the analysis is about the practical implementation of the simulation model: Non-gravitational forces were derived from the GFZ (GeoForschungsZentrum) EPOS (Earth Parameter and Orbit System) software. In a closed loop the accelerometer measurements derived from these data in connection with simulated tracking data have been used for orbit recovery including the <b>estimation</b> <b>of</b> <b>biases</b> and scale factors for the accelerometer data. Details and results of this procedure are presented. Key words. GRACE – sensor analysis – acceleromete...|$|R
40|$|Abstract- This paper {{examines}} {{issues in}} characterizing {{the performance of}} information sources as necessary for data fusion and coordination in a net-centric environment. In many practical applications, interacting agents have various degrees – and possibly time-varying degrees – of allegiance, common purpose, cooperativeness, information fidelity, controllability, etc. Agents share information with friends, foes and innocent bystanders alike, {{with varying degrees of}} cooperativeness and openness. In such cases, each network node needs to explicitly estimate the performance, trustworthiness and allegiance of all other contributing nodes {{as a part of the}} general multi-sensor/multi-target state estimation process. A sensor’s or information system’s reporting bias – which may include intentional or unintentional human biases – is distinguished from its measurement bias. The problem is compared with that <b>of</b> measurement <b>bias</b> <b>estimation,</b> e. g. in target tracking. Formulations for <b>estimation</b> <b>of</b> <b>biases</b> in discrete variable reporting – e. g. in target classification or activity state reporting – are explored...|$|R
40|$|Abstract: We present two {{results on}} {{attitude}} estimation using vector and rate gyro measurements, when {{both sets of}} measurements are biased. The first result concerns an observer for attitude and gyro bias that has previously been presented by Hamel and Mahony, and by Mahony, Hamel, and Pflimlin, with proven almost-global stability results when either (i) the reference vectors in the inertial frame are stationary; or (ii) the reference vectors are time-varying but the gyro measurements are unbiased. We prove that the same observer with an added parameter projection is semiglobally exponentially stable when bias estimation is included and the reference vectors are time-varying. The second result concerns <b>estimation</b> <b>of</b> <b>bias</b> in the body-fixed vector measurements. We show how the bias can be estimated {{in a manner that}} is decoupled from the attitude and gyro bias estimation, provided the measurements are sufficiently excited relative to the level of measurement noise...|$|E
40|$|In {{the past}} twenty years, the {{sampling}} Standards documents {{of a number of}} countries have been revised more than once. Each of the Standards, particularly those concerned with coal and coke sampling, have made minor alterations to the recommended procedures for <b>estimation</b> <b>of</b> <b>bias</b> in a sampling system or procedure. None of the procedures correctly take into account the fact that assay results from both the reference method and the procedure to be tested must be considered as random variables if the statistical analysis is to be correct. Most Standards rely upon a statistical analysis of paired differences to detect bias. This paper presents a procedure for bias test result analysis that accounts for random variation in reference method and test method assays and demonstrates, using some coal bias test data, that the method of analysis of paried differences can fail to detect serious bias of scale...|$|E
40|$|Abstract. Modern {{statistical}} inference techniques {{may be able}} to improve the sensitivity and specificity of resting state functional MRI (rs-fMRI) connectivi-ty analysis through more realistic characterization of distributional assumptions. In simulation, the advantages of such modern methods are readily demonstra-ble. However quantitative empirical validation remains elusive in vivo as the true connectivity patterns are unknown and noise/artifact distributions are chal-lenging to characterize with high fidelity. Recent innovations in capturing finite sample behavior of asymptotically consistent estimators (i. e., SIMulation and EXtrapolation- SIMEX) have enabled direct <b>estimation</b> <b>of</b> <b>bias</b> given single da-tasets. Herein, we leverage the theoretical core of SIMEX to study the proper-ties of inference methods in the face of diminishing data (in contrast to increas-ing noise). The stability of inference methods with respect to synthetic loss of empirical data (defined as resilience) is used to quantify the empirical perfor-mance of one inference method relative to another. We illustrate this new ap-proach in a comparison of ordinary and robust inference methods with rs-fMRI...|$|E
40|$|Filters {{developed}} {{in order to}} detect short bursts of gravitational waves in interferometric detector outputs are compared according to three main points. Conventional Receiver Operating Characteristics (ROC) are first built for all considered filters and for three typical burst signals. Optimized ROC are shown for a simple pulse signal in order to estimate the best detection efficiency of the filters in the ideal case, while realistic ones obtained with filters working with several ``templates'', show how detection efficiencies can be degraded in a practical implementation. <b>Estimations</b> <b>of</b> <b>biases</b> and statistical errors on th resonctruction {{of the time of}} arrival of pulse-like signals are then given for each filter. As most of the filters require a prewhitening of the detector noise, the sensitivity to a non perfect noise whitening procedure is finally analysed. The comparison of the different filters finally show that they are rather complementary than actually concurrent...|$|R
40|$|The scalar {{method of}} fault {{diagnosis}} {{systems of the}} inertial measurement unit (IMU) is described. All inertial navigation systems consist of such IMU. The scalar calibration method is a base of the scalar method for quality monitoring and diagnostics. In accordance with scalar calibration method algorithms of fault diagnosis systems are developed. As a result of quality monitoring algorithm verification is implemented in the working capacity monitoring of IMU. A failure element determination is based on diagnostics algorithm verification and after {{that the reason for}} such failure is cleared. The process of verifications consists of comparison <b>of</b> the calculated <b>estimations</b> <b>of</b> <b>biases,</b> scale factor errors, and misalignments angles of sensors to their data sheet certificate, kept in internal memory of computer. As a result of such comparison the conclusion for working capacity of each IMU sensor can be made and also the failure sensor can be determined...|$|R
40|$|A {{method to}} {{estimate}} the time-dependent correlation via an empirical <b>bias</b> estimate <b>of</b> the time-delayed mutual information for a time-series is proposed. In particular, the <b>bias</b> <b>of</b> the time-delayed mutual information is shown to often be equivalent to the mutual information between two distributions of points from the same system separated by infinite time. Thus intuitively, <b>estimation</b> <b>of</b> the <b>bias</b> is reduced to <b>estimation</b> <b>of</b> the mutual information between distributions of data points separated by large time intervals. The proposed bias estimation techniques are shown to work for Lorenz equations data and glucose time series data of three patients from the Columbia University Medical Center database...|$|R
40|$|Humans {{are able}} to pump gas into a car {{with little or no}} difficulty. This task is {{characterized}} by two sources of force: that from the nozzle contacting the car and that from the hose attached to the pump. The task succeeds due to the appreciable skill of a human and a forgiveness in the connection. The robotic mating of connectors burdened by forces from sources like the gas hose is beyond the current state of art. The research presented in this dissertation develops technology for robots to mate connectors that concurrently experience appreciable forces from encumbrances, like those from hoses, cables and oscillating masses, in addition to forces from contact. Effective force guided assembly under the influence of these bias forces, requires the differentiation of contact forces from bias forces, a task that is impossible using traditional sensing configurations. Emulating the contribution of bias during contact allows the <b>estimation</b> <b>of</b> <b>bias</b> forces and, subsequently, contact forces. By measuring and modeling bias prior to contact, when the only forces on the connector are from bias, a model of th...|$|E
40|$|<b>Estimation</b> <b>of</b> <b>bias</b> {{with the}} single-zone {{assumption}} in measurement of residential air exchange using the perfluorocarbon tracer gas method Abstract Residential air exchange rates (AERs) are vital {{in understanding the}} temporal and spatial drivers of indoor air quality (IAQ). Several methods to quantify AERs {{have been used in}} IAQ research, often with the assumption that the home is a single, well-mixed air zone. Since 2005, Health Canada has conducted IAQ studies across Canada in which AERs were measured using the perfluorocarbon tracer (PFT) gas method. Emitters and detectors of a single PFT gas were placed on the main floor to estimate a single-zone AER (AER 1 z). In three of these studies, a second set of emitters and detectors were deployed in the basement or second floor in approximately 10 % of homes for a two-zone AER estimate (AER 2 z). In total, 287 daily pairs of AER 2 z and AER 1 z estimates were made from 35 homes across three cities. In 87 % of the cases, AER 2 z was higher than AER 1 z. Overall, the AER 1 z estimates underestimated AER 2 z b...|$|E
40|$|This paper {{presents}} a novel variational approach for simultaneous <b>estimation</b> <b>of</b> <b>bias</b> field and segmentation of images with intensity inhomogeneity. We model intensity of inhomogeneous objects to be Gaussian distributed with different means and variances, and then introduce a sliding window {{to map the}} original image intensity onto another domain, where the intensity distribution of each object is still Gaussian but can be better separated. The means of the Gaussian distributions in the transformed domain can be adaptively estimated by multiplying the bias field with a piecewise constant signal within the sliding window. A maximum likelihood energy functional is then defined on each local region, which combines the bias field, the membership function of the object region, and the constant approximating the true signal from its corresponding object. The energy functional is then extended to the whole image domain by the Bayesian learning approach. An efficient iterative algorithm is proposed for energy minimization, via which the image segmentation and bias field correction are simultaneously achieved. Furthermore, the smoothness of the obtained optimal bias field is ensured by the normalized convolutions without extra cost. Experiments on real images demonstrated {{the superiority of the}} proposed algorithm to other state-of-the-art representative methods...|$|E
40|$|A maximum {{likelihood}} method {{is used for}} <b>estimation</b> <b>of</b> unknown <b>bias</b> errors in measured airplane responses. The mathematical model of an airplane is represented by six-degrees-of-freedom kinematic equations. In these equations the input variables are replaced by their measured values which {{are assumed to be}} without random errors. The resulting algorithm is verified with a simulation and flight test data. The {{maximum likelihood}} estimates from in-flight measured data are compared with those obtained by using a nonlinear-fixed-interval-smoother and an extended Kalmar filter...|$|R
40|$|In the {{measured}} decay {{properties of the}} tau there is a discrepancy between the total branching fraction for the one charged particle decay modes and {{the sum of the}} branching fractions for the known individual modes. This discrepancy _ ‘is derived from about 60 different measurements of branching fractions and some use of weak interaction theory. Our statistical study of these 60 measurements shows there are problems in some of the measurements in the <b>estimation</b> <b>of</b> exper-imental <b>bias</b> or systematic error. But {{there is no evidence that}} the discrepancy derives from experimental bias or from incorrect <b>estimation</b> <b>of</b> systematic error...|$|R
40|$|Determining {{the extent}} to which {{citation}} flows, and hence bibliometric indicators based on them, reflect some intrinsic value of scientific works is an important task made very difficult by endogeneity issues. This paper presents an approach which allows to go beyond the abundant anecdotal evidence by testing whether the citation behavior is free from environmental factors. The hypothesis of independence is strongly rejected, providing causal evidence of a Matthew effect at work: namely, the publication of a new work on behalf of an author increases the flow of citations to previous works. Such result is a step towards the <b>estimation</b> <b>of</b> <b>biases</b> affecting bibliometric indicators, at least when interpreted as measures of scientific productivity. The study is based on a novel framework for the study of endogenous network growth subject to constraints. Constraints can be both positive and negative, and change in time depending on the actions of the agents. The framework is not limited to citation networks, and can be applied to any context in which the formation of a link inhibits or implies the formation of another one...|$|R
40|$|A {{final sample}} of 30 {{characters}} (15 white, 15 black) was selected from 11 television shows and 9 short, silent clips {{were selected for}} each character. Please see Table S 1 {{for a list of}} these characters and shows. The selection process began with television programs. We aimed to sample a variety of popular television programs in order to broadly estimate exposure to nonverbal race bias. To permit the <b>estimation</b> <b>of</b> <b>bias</b> in a fashion that could be generalized beyond a single day, week, or season, the sample was further restricted to shows with recurring themes and characters (television content otherwise varies widely on a daily basis). Scripted television shows and longrunning reality shows fit criteria of popularity and recurrent themes/characters and these shows were the focus of Study 1. To control for any potential confound at the level of television show (e. g., white characters coming from more light-hearted shows), white and black characters were status-matched within, rather than between shows. Of the television shows that included both white and black recurring characters, many included a built-in confound in which black characters were much less central to the show’s theme or had much lower job status. These programs were excluded. The remaining 11 programs wer...|$|E
40|$|Abstract—This paper {{presents}} a novel variational approach for simultaneous <b>estimation</b> <b>of</b> <b>bias</b> field and segmentation of images with intensity inhomogeneity. We model intensity of inhomoge-neous objects to be Gaussian distributed with different means and variances, and then introduce a sliding window {{to map the}} orig-inal image intensity onto another domain, where the intensity distribution of each object is still Gaussian but can be bet-ter separated. The means of the Gaussian distributions in the transformed domain can be adaptively estimated by multiplying the bias field with a piecewise constant signal within the slid-ing window. A maximum likelihood energy functional is then defined on each local region, which combines the bias field, the membership function of the object region, and the constant approximating the true signal from its corresponding object. The energy functional is then extended to the whole image domain by the Bayesian learning approach. An efficient iterative algorithm is proposed for energy minimization, via which the image seg-mentation and bias field correction are simultaneously achieved. Furthermore, the smoothness of the obtained optimal bias field is ensured by the normalized convolutions without extra cost. Experiments on real images demonstrated {{the superiority of the}} proposed algorithm to other state-of-the-art representative methods. Index Terms—Bias field, computer vision, energy minimization, image segmentation, variational approach. I...|$|E
40|$|Retrievability is an {{independent}} evaluation measure that offers insights to an aspect of retrieval systems that performance and efficiency measures do not. Retrievability {{is often used to}} calculate the retrievability bias, an indication of how accessible a system makes all the documents in a collection. Generally, computing the retrievability bias of a system requires a colossal number of queries to be issued for the system to gain an accurate estimate of the bias. However, it is often the case that the accuracy of the estimate is not of importance, but the relationship between the estimate of bias and performance when tuning a systems parameters. As such, reaching a stable <b>estimation</b> <b>of</b> <b>bias</b> for the system is more important than getting very accurate retrievability scores for individual documents. This work explores the idea of using topical subsets of the collection for query generation and bias estimation to form a local estimate of bias which correlates with the global estimate of retrievability bias. By using topical subsets, {{it would be possible to}} reduce the volume of queries required to reach an accurate estimate of retrievability bias, reducing the time and resources required to perform a retrievability analysis. Findings suggest that this is a viable approach to estimating retrievability bias and that the number of queries required can be reduced to less than a quarter of what was previously thought necessary...|$|E
3000|$|... {{with its}} lagged values {{so that the}} {{possibility}} of feedback effects from migration responses to labor market changes as source <b>of</b> <b>estimation</b> <b>bias</b> is limited. This should lead to consistent estimates of the coefficients for the explanatory variables. 3 [...]...|$|R
40|$|We study halo {{clustering}} bias with second- and third-order {{statistics of}} halo and matter density {{fields in the}} MICE Grand Challenge simulation. We verify that two-point correlations deliver reliable estimates <b>of</b> the linear <b>bias</b> parameters at large scales, while estimations from the variance can be significantly affected by non-linear and possibly non-local contributions to the bias function. Combining three-point auto- and cross-correlations we find, {{for the first time}} in configuration space, evidence for the presence of such non-local contributions. These contributions are consistent with predicted second-order non-local effects on the bias functions originating from the dark matter tidal field. Samples of massive haloes show indications <b>of</b> <b>bias</b> (local or non-local) beyond second order. Ignoring non-local bias causes 20 - 30 % and 5 - 10 % overestimation <b>of</b> the linear <b>bias</b> from three-point auto- and cross-correlations respectively. We study two third-order bias estimators which are not affected by second-order non-local contributions. One is a combination of three-point auto- and cross- correlation. The other is a combination of third-order one- and two-point cumulants. Both methods deliver accurate <b>bias</b> <b>estimations</b> <b>of</b> the linear <b>bias.</b> Furthermore their <b>estimations</b> <b>of</b> second-order <b>bias</b> agree mutually. Ignoring non-local bias causes higher values <b>of</b> the second-order <b>bias</b> from three-point correlations. Our results demonstrate that third-order statistics can be employed for breaking the growth-bias degeneracy. Comment: 19 pages, 11 figure...|$|R
40|$|This {{research}} addresses {{some fundamental}} problems in GNSS data processing of triple frequency signals, and proposes geometry-free approaches to <b>estimation</b> <b>of</b> hardware <b>biases</b> <b>estimation</b> and measurement noise analysis. As a result, the hardware delay time series for each code and phase signal {{can also be}} determined to the precision of centimetres and millimetres respectively. The covariance matrices of the code and phase measurements in each frequency and each line of sight can be computed epoch-by-epoch, leading to overall 10 % improvement of GNSS positioning accuracy...|$|R
40|$|Published {{estimates}} of the sensitivity and specificity of PCR and ligase chain reaction (LCR) for detecting Chlamydia trachomatis are potentially biased because of study design limitations (confirmation of test results was limited to subjects who were PCR or LCR positive but culture negative). Relative measures of test accuracy are less prone to bias in incomplete study designs. We estimated the relative sensitivity (RSN) and relative false-positive rate (RFP) for PCR and LCR versus cell culture among 1, 138 asymptomatic men and evaluated the potential bias of RSN and RFP estimates. PCR and LCR testing in urine were compared to culture of urethral specimens. Discordant results (PCR or LCR positive, but culture negative) were confirmed by using a sequence including the other DNA amplification test, direct fluorescent antibody testing, and a DNA amplification test to detect chlamydial major outer membrane protein. The RSN estimates for PCR and LCR were 1. 45 (95 % confidence interval [CI] = 1. 3 to 1. 7) and 1. 49 (95 % CI = 1. 3 to 1. 7), respectively, indicating that both methods are more sensitive than culture. Very few false-positive results were found, indicating that the specificity levels of PCR, LCR, and culture are high. The potential bias in RSN and RFP estimates were < 5 and < 20 %, respectively. The <b>estimation</b> <b>of</b> <b>bias</b> {{is based on the}} most likely and probably conservative parameter settings. If the sensitivity of culture is between 60 and 65 %, then the true sensitivity of PCR and LCR is between 90 and 97 %. Our findings indicate that PCR and LCR are significantly more sensitive than culture, while the three tests have similar specificities...|$|E
40|$|The {{simulation}} of radiation transfer (RT) {{is used in}} many FSPM models and applications. This preeminence is explained by {{the central role of}} light in plant growth and development, light being both the energetic source of photosynthesis and an important mediator for the adaptation of plant development to their environment (photomorphogenesis). Radiation is also a key component of the energy budget of plant organs and a factor driving stomata. Thus RT models are required to simulate organ temperature, transpiration and the {{simulation of}} water fluxes within plants. At a larger scale, radiation transfer models allow to quantify the light interception efficiency of complex tree crowns or of a canopy, which are important traits for breeding or crop modelling. They also makes it possible to determine the sharing of light between different individuals and species within natural and artificial plant communities; both in field or controlled conditions. Parallel to this variety of applications, different RT models were developed or adapted for use in the FSPM community. They differ both on the way they apprehend plant geometry (volumic vs surfacic objects) and on the way they approximate the physics of radiation transfers and light-plant interactions. The aim {{of this paper is to}} provide a practical help to modelers for choosing, correctly use and compare RT models for a given application. Based on a synthetic view of the rationale of the principal approaches used in RT models, we identified and standardised the different types of inputs of RT models, and proposed a unified interface for running them. Second, we defined several simulation scenarios that cover the main applications cited above, both for crop and tree plants (wheat, maize, apple tree, communities of grassland plants). For each scenario, four different models (Caribu, RATP, Muslim, Fractalysis), that covers a large range of approaches are run and compared on pre-defined target variables. All models and scenarios are available on the OpenAlea platform, and can be connected as components with others models. Models are provided as plugins of a common service that expose radiation models with a uniform interface. New RT models can be added dynamically and compared with others. The comparison includes an <b>estimation</b> <b>of</b> <b>bias</b> and errors, as well as its efficiency in terms of computational time. This work is a first initiative towards a benchmark proposal, open to the whole FSPM community, similar to the RAMI initiative for the inter-comparison of radiation transfer model for remote sensing applicatio...|$|E
40|$|Nonparametric {{regression}} is a {{very popular}} tool for data analysis because thesetechniques impose few assumptions about {{the shape of the}} mean function. Hence,they are extremely flexible tools for uncovering nonlinear relationships betweenvariables. A disadvantage of these methods is their computational complexitywhen considering large data sets. In order to reduce the complexity for leastsquares support vector machines (LS-SVM), we propose a method called Fixed-Size LS-SVM which is capable of handling large data set on standard personalcomputers. We study the properties of the LS-SVM regression when relaxing the Gauss-Markov conditions. We propose a robust version of LS-SVM based on iterativereweighting with weights based on the distribution of the error variables. We showthat the empirical maxbias of the proposed robust estimator increases slightly withthe number of outliers in region and stays bounded right up to the breakdownpoint. We also establish three conditions to obtain a fully robust nonparametricestimator. We investigate the consequences when the i. i. d. assumptions is violated. Weshow that, for nonparametric kernel based regression, classical model selectionprocedures such as cross-validation, generalized cross-validation and v-fold crossvalidationbreak down in the presence of correlated data and not the chosensmoothing method. Therefore, we develop a model selection procedure for LSSVMin order to effectively handle correlation in the data without requiring anyprior knowledge about the correlation structure. Next, we propose bias-corrected 100 (1 &# 8722; &# 945;) % approximate confidence andprediction intervals (pointwise and uniform) for linear smoothers, in particularlyfor LS-SVM. We prove, under certain conditions, the asymptotic normality of LSSVM. Further, we show the practical use of these interval estimates by means oftoy examples for regression and classification. Finally, we illustrate the capabilities of the proposed methods on a number ofapplications i. e. system identification, hypothesis testing and density estimation. 1 Introduction 1 1. 1 Historical Evolution and General Background 1. 2 Practical Applications 1. 2. 1 Biomedical Data 1. 2. 2 Financial Data 1. 2. 3 System Identification 1. 2. 4 Time Series Analysis 1. 3 Organization and Contributions of the Thesis 2 Model Building 2. 1 Regression Analysis and Loss Functions 2. 2 Assumptions, Restrictions and Slow Rate 2. 3 Curse of Dimensionality 2. 4 Parametric and Nonparametric Regression Estimators: An Overview 2. 4. 1 Parametric Modeling 2. 4. 2 Local Averaging 2. 4. 3 Local Modeling 2. 4. 4 Global Modeling 2. 4. 5 Penalized Modeling 2. 5 Support Vector Machines 2. 5. 1 Basic Idea of Support Vector Machines 2. 5. 2 Primal-Dual Formulation of Support Vector Machines 2. 5. 3 Least Squares Support Vector Machines 2. 6 Conclusions 3 Model Selection Methods 3. 1 Introduction 3. 2 Cross-Validation Procedures 3. 2. 1 Cross-Validation Philosophy 3. 2. 2 Leave-One-Out Cross-Validation 3. 2. 3 v-fold Cross-Validation 3. 2. 4 Generalized Cross-Validation 3. 3 Complexity Criteria: Final Prediction Error, AIC, Mallows’ Cp and BIC 3. 4 Choosing the Learning Parameters 3. 4. 1 General Remarks 3. 4. 2 Optimization Strategy 3. 5 Conclusions 4 Fixed-Size LS-SVM 4. 1 Introduction 4. 2 Estimation in the Primal Space 4. 2. 1 Finite Approximation to the Feature Map 4. 2. 2 Solving the Problem in Primal Space 4. 3 Active Selection of a Subsample 4. 3. 1 Subsample Based on Entropy Criteria 4. 3. 2 Bandwidth Selection for Density Estimation 4. 3. 3 Solve-the-Equation Plug-In Method 4. 3. 4 Maximizing R´enyi Entropy vs. Random Sampling 4. 4 Selecting the number of prototype vectors 4. 5 Fast v-fold Cross-Validation for FS-LSSVM 4. 5. 1 Extended Feature Matrix Can Fit Into Memory 4. 5. 2 Extended Feature Matrix Cannot Fit Into Memory 4. 6 Computational Complexity and Numerical Experiments on v-fold CV 4. 6. 1 Computational Complexity Analysis 4. 6. 2 Numerical Experiments 4. 7 Classification and Regression Results 4. 7. 1 Description of the Data Sets 4. 7. 2 Description of the Reference Algorithms 4. 7. 3 Performance of binary FS-LSSVM classifiers 4. 7. 4 Performance of multi-class FS-LSSVM classifiers 4. 7. 5 Performance of FS-LSSVM for Regression 4. 8 Conclusions 5 Robustness in Kernel Based Regression 5. 1 Introduction 5. 2 Measures of Robustness 5. 2. 1 Influence Functions and Breakdown Points 5. 2. 2 Empirical Influence Functions 5. 3 Residuals and Outliers in Regression 5. 3. 1 Linear Regression 5. 3. 2 Kernel Based Regression 5. 4 Robustifying LS Kernel Based Regression 5. 4. 1 Problems with Outliers in Nonparametric Regression 5. 4. 2 Theoretical Background 5. 4. 3 Application to Least Squares Support Vector Machines 5. 4. 4 Weight Functions 5. 4. 5 Speed of Convergence-Robustness Tradeoff 5. 4. 6 Robust Selection of Tuning Parameters 5. 5 Simulations 5. 5. 1 Empirical Maxbias Curve 5. 5. 2 Toy example 5. 5. 3 Real Life Data Sets 5. 6 Conclusions 6 Kernel Regression with Correlated Errors 6. 1 Introduction 6. 2 Problems with Correlation 6. 3 New Developments in Kernel Regression with Correlated Errors 6. 3. 1 No Positive Definite Kernel Constraint 6. 3. 2 Positive Definite Kernel Constraint 6. 3. 3 Drawback of Using Bimodal Kernels 6. 4 Simulations 6. 4. 1 CC-CV vs. LOO-CV with Different Noise Models 6. 4. 2 Evolution of the Bandwidth Under Correlation 6. 4. 3 Comparison of Different Bimodal Kernels 6. 4. 4 Real life data sets 6. 5 Conclusions 7 Confidence and Prediction Intervals 7. 1 Introduction 7. 2 <b>Estimation</b> <b>of</b> <b>Bias</b> and Variance 7. 2. 1 LS-SVM Regression and Smoother Matrix 7. 2. 2 Bias Estimation 7. 2. 3 Variance Estimation 7. 3 Confidence and Prediction Intervals: Regression 7. 3. 1 Pointwise Confidence Intervals 7. 3. 2 Simultaneous Confidence Intervals 7. 3. 3 Pointwise and Simultaneous Prediction Intervals 7. 4 Bootstrap Based Confidence and Prediction Intervals 7. 4. 1 Bootstrap Based on Residuals 7. 4. 2 Construction of Bootstrap Confidence and Prediction Intervals 7. 5 Simulations: The Regression Case 7. 5. 1 Empirical Coverage Probability 7. 5. 2 Homoscedastic Examples 7. 5. 3 Heteroscedastic Examples and Error Variance Estimation 7. 6 Confidence Intervals: Classification 7. 6. 1 Classification vs. Regression 7. 6. 2 Illustration and Interpretation of the Method 7. 6. 3 Simulations: The Classification Case 7. 7 Conclusions 8 Applications and Case Studies 8. 1 System Identification with LS-SVMLab 8. 1. 1 General Information 8. 1. 2 Model Identification 8. 2 SYSID 2009 : Wiener-Hammerstein Benchmark 8. 2. 1 Model Structure 8. 2. 2 Data Description and Training Procedure 8. 2. 3 Estimation and Model Selection 8. 2. 4 Results on Test Data 8. 3 Nonparametric Comparison of Densities Based on Statistical Bootstrap 8. 3. 1 Introduction to the Problem 8. 3. 2 Kernel Density Estimation 8. 3. 3 Formulation and Construction of the Hypothesis Test 8. 3. 4 Illustrative Examples 8. 4 Finding the Maximum in Hysteresis Curves 8. 5 Conclusions 9 Summary, Conclusions and Future Research 9. 1 Summary and Main Conclusions 9. 2 Future Research A Coupled Simulated Annealing References Curriculum vitaenrpages: 246 status: publishe...|$|E
40|$|Tuning {{parameters}} in {{supervised learning}} problems are often estimated by cross-validation. The minimum {{value of the}} cross-validation error can be biased downward as {{an estimate of the}} test error at that same value of the tuning parameter. We propose a simple method for the <b>estimation</b> <b>of</b> this <b>bias</b> that uses information from the cross-validation process. As a result, it requires essentially no additional computation. We apply our bias estimate to a number of popular classifiers in various settings, and examine its performance. 1. Introduction. Cross-validatio...|$|R
40|$|As a {{consequence}} of the study of the strategy for TanDEM-X baseline determination, based on the experience of the GRACE mission, the potential existence <b>of</b> a <b>bias</b> in the order of several mm in the baseline product has been realized. This means that the fulfilment of the absolute accuracy requirement of 1 mm in the baseline determination is not anymore guaranteed. In order to overcome this problem, methods for the <b>estimation</b> <b>of</b> this <b>bias</b> with the help interferometry are proposed. This technical note sets the basis for the characterisation and correction <b>of</b> the baseline <b>bias.</b> It includes some simulations of the effect of this error and the performance and accuracy of the characterisation and correction methods. ...|$|R
30|$|Through QIBA, scientists, clinicians and mathematicians hope to {{validate}} quantitative imaging biomarkers, based on metrological {{practices such as}} identification and characterisation {{of the sources of}} error. In addition, a detailed analysis of the entire imaging chain will need to be undertaken, from acquisition to processing, to be able to establish the presence or not <b>of</b> a <b>bias</b> along the entire measurement procedure. Here again, <b>estimation</b> <b>of</b> a <b>bias</b> size is generally made through the use of objects serving as ‘gold standards’ or benchmarks for the measurements done. These objects are generally called phantoms and their role will, therefore, be more and more important within the growing field of quantitative radiology.|$|R
