0|9517|Public
40|$|Abstract. Iterated <b>local</b> <b>search</b> is a {{stochastic}} <b>local</b> <b>search</b> (SLS) {{method that}} combines a perturbation step with an embedded <b>local</b> <b>search</b> algorithm. In this article, we propose {{a new way}} of hybridizing iterated <b>local</b> <b>search.</b> It consists in using an iterated <b>local</b> <b>search</b> as the embedded <b>local</b> <b>search</b> algorithm inside another iterated <b>local</b> <b>search.</b> This nesting of <b>local</b> <b>searches</b> and iterated <b>local</b> <b>searches</b> can be further iterated, leading to a hierarchy of iterated <b>local</b> <b>searches.</b> In this paper, we experimentally examine this idea applying it to the quadratic assignment problem. Experimental results on large, structured instances show that the hierarchical iterated <b>local</b> <b>search</b> can offer advantages over using a “flat ” iterated <b>local</b> <b>search</b> and make it a promising technique to be further considered for other applications. ...|$|R
40|$|This paper {{presents}} a real-coded memetic algorithm that combines a high diversity global exploration with an adaptive <b>local</b> <b>search</b> method {{to the most}} promising individuals that adjusts the <b>local</b> <b>search</b> probability and the <b>local</b> <b>search</b> depth. In our proposal we use the individual fitness to decide when <b>local</b> <b>search</b> will be applied (<b>local</b> <b>search</b> probability) and how many effort should be applied (the <b>local</b> <b>search</b> depth), focusing the <b>local</b> <b>search</b> effort on the most promising regions. We divide the individuals of the population into three different categories and we assign different values of the above <b>local</b> <b>search</b> parameters to the individual in function of the category to which that individual belongs. In this study, we analyze the performance of our proposal when tackling the test problems proposed for th...|$|R
40|$|This paper {{presents}} a <b>local</b> <b>search</b> algorithm based on variable depth search, called the k-opt <b>local</b> <b>search,</b> for the maximum clique problem. The k-opt <b>local</b> <b>search</b> performs add and drop moves, {{each of which}} can be interpreted as 1 -opt move, to search a k-opt neighborhood solution at each iteration until no better k-opt neighborhood solution can be found. To evaluate our k-opt <b>local</b> <b>search</b> algorithm, we repeatedly apply the <b>local</b> <b>search</b> for each of DIMACS benchmark graphs and compare with the state-of-the-art metaheuristics such as the genetic <b>local</b> <b>search</b> and the iterated <b>local</b> <b>search</b> reported previously. The computational results show that in spite of the absence of major metaheuristic components, the k-opt <b>local</b> <b>search</b> is capable of finding better (at least the same) solutions on average than those obtained by these metaheuristics for all the graphs...|$|R
40|$|We {{present a}} survey of {{parallel}} <b>local</b> <b>search</b> algorithms in which we review the concepts {{that can be used}} to incorporate parallelism into <b>local</b> <b>search.</b> For this purpose we distinguish between single-walk and multiple-walk parallel <b>local</b> <b>search</b> and between asynchronous and synchronous parallelism. Within the class of single-walk algorithms we differentiate between multiple-step and single-step parallelism. To describe parallel <b>local</b> <b>search</b> we introduce the concepts of hyper neighborhood structures and distributed neighborhood structures. Furthermore, we present templates that capture most of the parallel <b>local</b> <b>search</b> algorithms proposed in the literature. Finally, we discuss some complexity issues related to parallel <b>local</b> <b>search...</b>|$|R
40|$|The aim of {{this paper}} is to clearly {{demonstrate}} the importance of finding a good balance between genetic <b>search</b> and <b>local</b> <b>search</b> in the implementation of hybrid evolutionary multicriterion optimization (EMO) algorithms. We first modify the <b>local</b> <b>search</b> part of an existing multi-objective genetic <b>local</b> <b>search</b> (MOGLS) algorithm. In the modified MOGLS algorithm, the computation time spent by <b>local</b> <b>search</b> can be decreased by two tricks: to apply <b>local</b> <b>search</b> to only selected solutions (not all solutions) and to terminate <b>local</b> <b>search</b> before all neighbors of the current solution are examined. Next we show that the <b>local</b> <b>search</b> part of the modified MOGLS algorithm can be combined with other EMO algorithms. We implement a hybrid version of a strength Pareto evolutionary algorithm (SPEA) ...|$|R
30|$|Second, in a {{parallel}} environment, various <b>local</b> <b>searches</b> can simultaneously operate to cooperatively locally optimize {{various parts of the}} same encoding. In this parallel environment, the <b>local</b> <b>searches</b> can communicate with one another so that each <b>local</b> <b>search</b> can ignore those parts of the encoding fixed by other <b>local</b> <b>searches</b> and concentrate only on manipulating its own part.|$|R
40|$|Guided <b>Local</b> <b>Search</b> is {{a general}} penalty-based {{optimisation}} method that sits on top of <b>local</b> <b>search</b> methods to help them escape local optimum. It {{has been applied to}} a variety of problems and demonstrated effective. The aim of this paper is not to produce further evidence that Guided <b>Local</b> <b>Search</b> is an effective algorithm, but to present an extension of Guided <b>Local</b> <b>Search</b> that potentially has no parameter to tune. Compared to other algorithms, Guided <b>Local</b> <b>Search</b> is relatively easy to apply, as there is only one major parameter (#) to set. In some applications, performance of Guided <b>Local</b> <b>Search</b> is insensitive to the value of this parameter. Nevertheless, the value of this parameter can affect the performance of Guided <b>Local</b> <b>Search</b> in some problems. In this paper, we show how (a) an aspiration criterion and (b) random moves may be added to Guided <b>Local</b> <b>Search</b> to reduce the sensitivity of its performance to the parameter value. The extended Guided <b>Local</b> <b>Search</b> is tested on the SAT, weighted MAX-SAT and Quadratic Assignment Problems with positive results...|$|R
30|$|It is {{important}} to note that there are different methods to apply a <b>local</b> <b>search</b> in a GA. Therefore, <b>local</b> <b>search</b> can be used with a probabilistic factor just like a mutation, or when the algorithm reaches a given number of generations. Likewise, <b>local</b> <b>search</b> can be applied on every created individual. The choice of how to use the <b>local</b> <b>search</b> and how to maintain an equilibrium between the genetic operator and the <b>local</b> <b>search</b> is crucial to obtain good results.|$|R
40|$|This paper {{examines}} the following {{issues related to}} the implementation of <b>local</b> <b>search</b> in hybrid multi-objective genetic algorithms: specification of an objective function to be optimized by <b>local</b> <b>search,</b> early termination of <b>local</b> <b>search</b> before finding a locally optimum solution, choice of individuals to which <b>local</b> <b>search</b> is applied, and timing of the application of <b>local</b> <b>search.</b> These issues are examined through computer simulations on a flowshop scheduling problem using a hybrid version of a wellknown multi-objective genetic algorithm: the strength Pareto evolutionary algorithm (SPEA). Simulation results show that the hybridization with <b>local</b> <b>search</b> degrades the search ability of the SPEA when the implementation of <b>local</b> <b>search</b> is not appropriate. It is also shown that the hybridization has the possibility to improve the convergence speed of the SPEA to the Pareto front. 1...|$|R
40|$|This paper {{examines}} how the search ability of evolutionary multi-objective optimization (EMO) algorithms {{can be improved}} by the hybridization with <b>local</b> <b>search</b> through computational experiments on multi-objective permutation flowshop scheduling problems. The task of EMO algorithms {{is to find a}} variety of nondominated solutions of multi-objective optimization problems. First we describe our multi-objective genetic <b>local</b> <b>search</b> (MOGLS) algorithm, which is the hybridization of a simple EMO algorithm with <b>local</b> <b>search.</b> Next we discuss some implementation issues of <b>local</b> <b>search</b> in our MOGLS algorithm such as the choice of initial (i. e., starting) solutions for <b>local</b> <b>search</b> and a termination condition of <b>local</b> <b>search.</b> Then we implement hybrid EMO algorithms using well-known EMO algorithms: SPEA and NSGA-II. Finally we compare those EMO algorithms with their hybrid versions through computational experiments. Experimental results show that the hybridization with <b>local</b> <b>search</b> can improve the search ability of the EMO algorithms when <b>local</b> <b>search</b> is appropriately implemented in their hybrid versions...|$|R
40|$|We {{have already}} {{proposed}} a multi-objective genetic <b>local</b> <b>search</b> algorithm for finding nondominated solutions of multi-objective optimization problems (Ishibuchi & Murata 1998). In our hybrid algorithm, a <b>local</b> <b>search</b> procedure {{is applied to}} each solution generated by genetic operations (i. e., selection, crossover, and mutation). Since our optimization problem involves multiple objectives, {{the application of the}} <b>local</b> <b>search</b> is not straightforward. In this paper, we examine various methods for implementing <b>local</b> <b>search</b> procedures in our multi-objective genetic <b>local</b> <b>search</b> algorithm. One method uses a weighted sum of multiple objectives as a scalar fitness function where weight values are randomly updated whenever a pair of parent solutions is selected. Such a fitness function is used in the <b>local</b> <b>search</b> as well as the selection of parent solutions. In a variant of this method, weight values for a solution in the <b>local</b> <b>search</b> are specified according to its location in the objective space. Another method uses an inequality relation between solutions based on multiple objectives when a <b>local</b> <b>search</b> procedure determines whether the current solution is to be replaced with a new solution. The performance of multi-objective genetic <b>local</b> <b>search</b> algorithms with various <b>local</b> <b>search</b> procedures is examined by computer simulations on two-objective flowshop scheduling problems. 1...|$|R
30|$|In this subsection, we {{improve the}} {{capability}} of our algorithm to escape from local optima by using a metaheuristic technique. In multi-start <b>local</b> <b>search,</b> a number of initial solutions are generated, and <b>local</b> <b>search</b> procedure is applied to each of them. Finally, the best solution obtained in the entire search is output. In iterated <b>local</b> <b>search,</b> which is an effective variant of multi-start <b>local</b> <b>search,</b> the initial solutions for <b>local</b> <b>search</b> are generated by perturbating good solutions found in the previous search. On this point, we have the big advantage of non-deteriorating perturbations being available.|$|R
40|$|<b>Local</b> <b>search</b> {{techniques}} have widespread use for solving propositional satisfiability problems. We investigate {{the use of}} adaptive <b>local</b> <b>search</b> techniques for model generation problems for modal logics; {{we focus on the}} modal logic S 5. A <b>local</b> <b>search</b> algorithm extended with an adaptive heuristic is presented and tested on an ensemble of randomly generated problem instances. We briefly discuss the limitations of using <b>local</b> <b>search</b> for other NP-complete modal logics...|$|R
40|$|The Graph Colouring Problem (GCP) is a {{well known}} -hard problem with many {{theoretical}} and practical applications. In this paper we introduce a new <b>local</b> <b>search</b> algorithm based on a very large scale neighbourhood. We provide an extensive numerical comparison between this method and several other <b>local</b> <b>search</b> techniques considering also the embedding of the <b>local</b> <b>search</b> into more complex schemes like Iterated <b>Local</b> <b>Search</b> or Tabu Search...|$|R
40|$|Memetic algorithmswith {{continuous}} local searchmethods {{have arisen}} as effective tools {{to address the}} difficulty of obtaining reliable solutions of high precision for complex continuous optimisation problems. There exists a group of continuous <b>local</b> <b>search</b> algorithms that stand out as exceptional <b>local</b> <b>search</b> optimisers. However, on some occasions, they may become very expensive, {{because of the way}} they exploit local information to guide the search process. In this paper, they are called intensive con-tinuous <b>local</b> <b>search</b> methods. Given the potential of this type of local optimisation methods, it is interesting to build prospective memetic algorithm models with them. This paper presents the concept of <b>local</b> <b>search</b> chain as a springboard to design memetic algorithm approaches that can effectively use intense continuous <b>local</b> <b>search</b> methods as <b>local</b> <b>search</b> operators. <b>Local</b> <b>search</b> chain concerns the idea that, at one stage, the <b>local</b> <b>search</b> operator may continue the operation of a previous invocation, starting from the final configuration (initial solution, strategy parameter values, inter-nal variables, etc.) reached by this one. The proposed memetic algorithm favours th...|$|R
40|$|Abstract. This paper {{demonstrates}} that the performance of multiobjective memetic algorithms (MOMAs) for combinatorial optimization strongly depends on the choice of solutions to which <b>local</b> <b>search</b> is applied. We first {{examine the effect of}} the tournament size to choose good solutions for <b>local</b> <b>search</b> on the performance of MOMAs. Next we examine the effectiveness of an idea of applying <b>local</b> <b>search</b> only to non-dominated solutions in the offspring population. We show that this idea has almost the same effect as the use of a large tournament size because both of them lead to high selection pressures. Then we examine different configurations of genetic operators and <b>local</b> <b>search</b> in MOMAs. For example, we examine the use of genetic operators after <b>local</b> <b>search.</b> In this case, improved solutions by <b>local</b> <b>search</b> are used as parents for recombination while <b>local</b> <b>search</b> is applied to the current population after generation update...|$|R
40|$|This book covers <b>local</b> <b>search</b> for {{combinatorial}} optimization and its extension to mixed-variable optimization. Although not yet understood from the theoretical point of view, <b>local</b> <b>search</b> is {{the paradigm of}} choice for tackling large-scale real-life optimization problems. Today's end-users demand interactivity with decision support systems. For optimization software, this means obtaining good-quality solutions quickly. Fast iterative improvement methods, like <b>local</b> <b>search,</b> are suited to satisfying such needs. Here the authors show <b>local</b> <b>search</b> in a new light, in particular presenting {{a new kind of}} mathematical programming solver, namely LocalSolver, based on neighborhood search. First, an iconoclast methodology is presented to design and engineer <b>local</b> <b>search</b> algorithms. The authors' concern about industrializing <b>local</b> <b>search</b> approaches is of particular interest for practitioners. This methodology is applied to solve two industrial problems with high economic stakes. Software based on <b>local</b> <b>search</b> induces extra costs in development and maintenance in comparison with the direct use of mixed-integer linear programming solvers. The authors then move on to present the LocalSolver project whose goal is to offer the power of <b>local</b> <b>search</b> through a model-and-run solver for large-scale 0 - 1 nonlinear programming. They conclude by presenting their ongoing and future work on LocalSolver toward a full mathematical programming solver based on <b>local</b> <b>search...</b>|$|R
40|$|Abstract Constraint-Based <b>Local</b> <b>Search</b> (CBLS) {{consist in}} using <b>Local</b> <b>Search</b> methods [4] for solving Constraint Satisfaction Problems (CSP). In order to further improve the {{performance}} of <b>Local</b> <b>Search,</b> one possible option is {{to take advantage of}} the increasing availability of parallel computational resources. Parallel implementation of <b>local</b> <b>search</b> meta- heuristics has been studied since the early 90 's, when multiprocessor machines started to become widely available, see [6]. One usually distinguishes between single-walk and [...] ...|$|R
40|$|Abstract- We {{show how}} <b>local</b> <b>search</b> can be {{combined}} with cellular multi-objective genetic algorithms for designing fuzzy rule-based classification systems. For achieving a good balance between genetic <b>search</b> and <b>local</b> <b>search,</b> <b>local</b> <b>search</b> is applied to only non-dominated solutions in each generation. Simulation results show the effectiveness of our approach. I...|$|R
5000|$|<b>Local</b> <b>search</b> {{is the use}} of {{specialized}} Internet search engines that allow users to submit geographically constrained searches against a structured database of local business listings. Typical <b>local</b> <b>search</b> queries include not only information about [...] "what" [...] the site visitor is searching for (such as keywords, a business category, or the name of a consumer product) but also [...] "where" [...] information, such as a street address, city name, postal code, or geographic coordinates like latitude and longitude. Examples of <b>local</b> <b>searches</b> include [...] "Hong Kong hotels", [...] "Manhattan restaurants", and [...] "Dublin car rental". <b>Local</b> <b>searches</b> exhibit explicit or implicit <b>local</b> intent. A <b>search</b> that includes a location modifier, such as [...] "Bellevue, WA" [...] or [...] "14th arrondissement", is an explicit <b>local</b> <b>search.</b> A search that references a product or service that is typically consumed locally, such as [...] "restaurant" [...] or [...] "nail salon", is an implicit <b>local</b> <b>search.</b>|$|R
50|$|Guided <b>Local</b> <b>Search</b> {{builds up}} {{penalties}} during a search. It uses penalties to help <b>local</b> <b>search</b> algorithms escape from local minimal and plateaus. When the given <b>local</b> <b>search</b> algorithm settles {{in a local}} optimum, GLS modifies the objective function using a specific scheme (explained below). Then the <b>local</b> <b>search</b> will operate using an augmented objective function, {{which is designed to}} bring the search out of the local optimum. The key is {{in the way that the}} objective function is modified.|$|R
40|$|<b>Local</b> <b>search</b> {{is known}} as an {{effective}} technique for solving combinatorial optimization problems. However, there are few tools that provide high level facilities for users to implement their own <b>local</b> <b>search</b> algorithms. In this paper, we introduce ZLoc, a new C++ library for <b>local</b> <b>search.</b> ZLoc supports many high-level features usually found in modeling languages, such as Zinc. It allows users to define their models in terms of variables and constraints, then it specifies their favorite <b>local</b> <b>search</b> method for solving the mode...|$|R
40|$|The <b>local</b> <b>search</b> with {{orthogonal}} {{design of experiment}} in its neighborhood determination (ODLS) outperforms the <b>local</b> <b>search</b> with the conventional neighborhood when the objective function includes noise. This models practical optimization problems that contains uncontrolledorunobserved variables. ODLS is robust and efficient since it shares all evaluations for direction determinationofeach variable. We illustrate the characteristics and demonstrate its performanceinsimple quadratic function plus random noise, and discuss their improvedparallel processing capability from the conventional <b>local</b> <b>search.</b> Keywords: Orthogonal Experimental Design, <b>Local</b> <b>Search...</b>|$|R
40|$|<b>Local</b> <b>search</b> is an {{integral}} part of many meta-heuristic strategies that solve single objective optimisation problems. Essentially, the meta-heuristic is responsible for generating a good starting point from which a greedy <b>local</b> <b>search</b> will find the local optimum. Indeed, the best known solutions to many hard problems (such as the travelling salesman problem) have been generated in this hybrid way. However, for multiple objective problems, explicit <b>local</b> <b>search</b> strategies are relatively under studied, compared to other aspects of the search process. In this paper, a generic <b>local</b> <b>search</b> strategy is developed, particularly for problems where it is difficult or impossible to determine the contribution of individual solution components (often referred to as inseparable problems). The meta-heuristic adopted to test this is extremal optimisation, though the <b>local</b> <b>search</b> technique may be used by any meta-heuristic. To supplement the <b>local</b> <b>search</b> strategy a diversification strategy that draws from the external archive is incorporated into the <b>local</b> <b>search</b> strategy. Using benchmark problems, and a real-world airfoil design problem, it is shown that this combination leads to improved solutions. Full Tex...|$|R
40|$|Recent Advances in Parallel Virtual Machine and Message Passing InterfaceTo {{practically}} solve NP-hard combinatorial optimizationproblems, <b>local</b> <b>search</b> algorithms {{and their}} parallel implementations on PVM or MPI have been frequently discussed. Since {{a huge number}} of neighbors may be examined to discover a locally optimal neighbor in each of <b>local</b> <b>search</b> calls, many of parallelization schemes, excluding so-called the multi-start parallel scheme, try to extract parallelism from a <b>local</b> <b>search</b> by distributing the examinations of neighbors to processors. However, in straightforward implementations, when the next <b>local</b> <b>search</b> starts, all the processors will be assigned to the neighbors of the latest solution, and the results of all (but one) examinations in the previous <b>local</b> <b>search</b> are thus discarded in vain, despite that they would contain useful information on further search. This paper explores the possibility of extracting information even from unsuccessful neighbor examinations in a systematic way to boost parallel <b>local</b> <b>search</b> algorithms. Our key concept is neighborhood composition. We demonstrate how this idea improves parallel implementations on PVM, by taking as examples well-known <b>local</b> <b>search</b> algorithms for the Traveling Salesman Problem...|$|R
40|$|Abstract. To {{practically}} solve NP-hard {{combinatorial optimization}} problems, <b>local</b> <b>search</b> algorithms and their parallel implementations on PVM or MPI have been frequently discussed. Since {{a huge number}} of neighbors may be examined to discover a locally optimal neighbor in each of <b>local</b> <b>search</b> calls, many of parallelization schemes, excluding so-called the multi-start parallel scheme, try to extract parallelism from a <b>local</b> <b>search</b> by distributing the examinations of neighbors to processors. However, in straightforward implementations, when the next <b>local</b> <b>search</b> starts, all the processors will be assigned to the neighbors of the latest solution, and the results of all (but one) examinations in the previous <b>local</b> <b>search</b> are thus discarded in vain, despite that they would contain useful information on further search. This paper explores the possibility of extracting information even from unsuccessful neighbor examinations in a systematic way to boost parallel <b>local</b> <b>search</b> algorithms. Our key concept is neighborhood composition. We demonstrate how this idea improves parallel implementations on PVM, by taking as examples well-known <b>local</b> <b>search</b> algorithms for the Traveling Salesman Problem. ...|$|R
40|$|A {{template}} {{is presented}} that captures {{a vast majority}} of the <b>local</b> <b>search</b> algorithms proposed in the literature, including iterative improvement, simulated annealing, threshold accepting, tabu search and genetic algorithms. The template leads to a classification of existing <b>local</b> <b>search</b> algorithms and offers the possibility to fit in new types of <b>local</b> <b>search</b> approaches...|$|R
30|$|Among many {{different}} procedures developed {{to cope with}} the JSP, those which employ <b>local</b> <b>searches</b> have the most robust and effective aspects. A <b>local</b> <b>search</b> differs from a systematic tree search in that systematic tree search expands a graph of partial solutions, whereas a <b>local</b> <b>search</b> explores a virtual graph connecting each complete solution to its neighboring complete solutions. The number of arcs in this virtual graph is affected by the neighborhood scheme employed by the <b>local</b> <b>search,</b> with the larger size of neighborhood leading to higher number of arcs and consequently larger or even impractical required computational times. That is why the endeavor of defining a proper neighborhood scheme highly determines the success of any <b>local</b> <b>search</b> algorithm.|$|R
40|$|<b>Local</b> <b>search</b> is a {{traditional}} technique to solve combinatorial search problems and has raised much interest in recent years. The design and implementation of <b>local</b> <b>search</b> algorithms {{is not an easy}} task in general and may require considerable experimentation and programming effort. However, contrary to global search, little support is available to assist the design and implementation of <b>local</b> <b>search</b> algorithms. This paper is an attempt to support the implementation of <b>local</b> <b>search.</b> It presents the preliminary design of LOCALIZER, a modeling language which makes it possible to express <b>local</b> <b>search</b> algorithms in a notation close to their informal descriptions in scientific papers. Experimental results on our first implementation show the feasibility of the approach...|$|R
40|$|Abstract:- A <b>local</b> <b>search</b> {{technique}} {{usually is}} locally convergent and, as result, outputs a local optimum. For many optimization problems, {{there is an}} attractor in the search space that drives the <b>local</b> <b>search</b> trajectories to converge into a small region. To gain insight into the basic properties of the attractor, this paper presents a framework for studying the attractor of <b>local</b> <b>search</b> space for the traveling salesman problem (TSP). The experimental results {{show that there is}} an empirical evidence for the existence of the attractor in the <b>local</b> <b>search</b> space for the TSP. The attractor is taken shape from the local optima that are located around the globally optimal solution. Key-Words:- solution attractor, <b>local</b> <b>search</b> heuristics, traveling salesman problem...|$|R
40|$|In {{this paper}} we present and {{investigate}} partial neighborhood <b>local</b> <b>searches,</b> which only explore {{a sample of}} the neighborhood at each step of the search. We particularly focus on establishing link between the structure of optimization problems and the efficiency of such <b>local</b> <b>search</b> algorithms. In our experiments we compare partial neighborhood <b>local</b> <b>searches</b> to state-of-the-art tabu <b>search</b> and iterated <b>local</b> <b>search</b> and perform a parameter sensitivity analysis by observing the efficiency of partial neighborhood <b>local</b> <b>searches</b> with different size of neighborhood sample. In order to facilitate the extraction of links between instances structure and search algorithm behavior we restrain the scope to binary fitness landscapes, such as NK landscapes and landscapes derived from UBQP...|$|R
30|$|As {{mentioned}} in the previous sections, MAs are different from GAs because of their applied <b>local</b> <b>search.</b> The <b>local</b> <b>search</b> is employed to refine the solutions and enhance their fitness. <b>Local</b> <b>search</b> moves from one solution to another in a neighborhood of solutions to find the local optimal. The process of search continues until a solution considered optimal is acquired, or the time limit for search is passed. The proposed MA in this study uses the TS defined by Zanjirani Farahani et al. (2007) as the <b>local</b> <b>search.</b>|$|R
40|$|International audienceCommon {{evolutionary}} {{approaches to}} protein-ligand docking optimization use mutation operators based on Gaussian and Cauchy distributions, with <b>local</b> <b>search</b> hybrids. The {{choice of a}} <b>local</b> <b>search</b> method is important for an efficient algorithm. We investigate the impact of <b>local</b> <b>search</b> with mutation operators by performing a locality analysis. High locality means that small variations in the genotype imply small variations in the phenotype. Results show that <b>local</b> <b>search</b> hybrids reduce locality and act as local optimizers with the solution as a starting point...|$|R
40|$|This paper {{studies the}} utility of using substructural neighborhoods for <b>local</b> <b>search</b> in the Bayesian {{optimization}} algorithm (BOA). The probabilistic model of BOA, which automatically identifies important problem substructures, is used to define {{the structure of the}} neighborhoods used in <b>local</b> <b>search.</b> Additionally, a surrogate fitness model is considered to evaluate the improvement of the <b>local</b> <b>search</b> steps. The results show that performing substructural <b>local</b> <b>search</b> in BOA significatively reduces the number of generations necessary to converge to optimal solutions and thus provides substantial speedup...|$|R
40|$|<b>Local</b> <b>search,</b> {{in either}} best or first {{admissible}} form, generally suffers from poor solution qualities as search cannot be continued beyond locally optimal points. Even multiple start <b>local</b> <b>search</b> strategies can suffer this problem. Meta-heuristic search algorithms, such as simulated annealing and tabu search, implement often computationally expensive optimisation strategies in which <b>local</b> <b>search</b> becomes a subordinate heuristic. To overcome this, {{a new form}} of <b>local</b> <b>search</b> is proposed. The Probabilistic Heuristic In <b>Local</b> (PHIL) <b>search</b> meta- strategy uses a recursive branching mechanism in order to overcome local optima. This strategy imposes only a small computational load over and above classical <b>local</b> <b>search.</b> A comparison between PHIL search and ant colony system on benchmark travelling salesman problem instances suggests that the new meta-strategy provides competitive performance. Extensions and improvements to the paradigm are also given...|$|R
30|$|Each non-dominated {{solution}} {{found during}} the <b>local</b> <b>search</b> {{is added to}} the archive. In that way, the archive is built directly using the <b>local</b> <b>search</b> procedure. The last solution found in the <b>local</b> <b>search</b> procedure (<b>local</b> optimum) {{is added to the}} population used in genetic algorithm in order to be used to build the next generation.|$|R
