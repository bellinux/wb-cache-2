48|10000|Public
25|$|Artificial {{intelligence}} - Several of {{the first}} AI software programs were created at Carnegie Mellon. These include the Logic Theorist, <b>General</b> <b>Problem</b> <b>Solver,</b> and Soar.|$|E
25|$|The {{earliest}} work in computerized {{knowledge representation}} {{was focused on}} general problem solvers such as the <b>General</b> <b>Problem</b> <b>Solver</b> (GPS) system developed by Allen Newell and Herbert A. Simon in 1959. These systems featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal.|$|E
2500|$|In Alchemy and AI (1965) and What Computers Can't Do (1972), Dreyfus {{summarized}} {{the history of}} artificial intelligence and ridiculed the unbridled optimism that permeated the field. For example, Herbert A. Simon, following {{the success of his}} program <b>General</b> <b>Problem</b> <b>Solver</b> (1957), predicted that by 1967: ...|$|E
25|$|It was {{the failure}} of these efforts {{that led to the}} {{cognitive}} revolution in psychology and to the phase of AI focused on knowledge representation that resulted in expert systems in the 1970s and 80s, production systems, frame languages, etc. Rather than <b>general</b> <b>problem</b> <b>solvers,</b> AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.|$|R
40|$|Next {{generation}} reasoning {{systems will}} consist of several domain specific <b>problem</b> <b>solvers</b> that cooperate to solve a <b>general</b> <b>problem.</b> These <b>problem</b> <b>solvers</b> will need to use database systems to retrieve domain specific data and perhaps {{to communicate with each}} other. Given the dynamic nature of database systems {{we need to make sure}} that the data retrieved by a <b>problem</b> <b>solver</b> remains both consistent with the data in the database and the data retrieved by other <b>problem</b> <b>solvers</b> throughout the <b>problem</b> solving process. The solution we discuss here involves use of Truth-Maintenancebased <b>problem</b> <b>solvers</b> and active database system(s). 1 Introduction Next generation reasoning systems will be based on distributed domain specific <b>problem</b> <b>solvers</b> that cooperate to solve a more <b>general</b> <b>problem.</b> These <b>problem</b> <b>solvers</b> could use one or more database systems to both retrieve somedomain specific data and to communicate with each other. The main problem addressed in this paper is the data consistency prob [...] ...|$|R
40|$|In {{this paper}} {{we present a}} {{mechanism}} to improve the solution quality of an existing heuristic based <b>general</b> assignment <b>problem</b> <b>solver</b> by adjusting the heuristic. The heuristic is tweaked using a set of parameters suggested by a genetic algorithm. The genetic algorithm is applied {{in a way that}} reduces the amount of involvement required to understand the existing solution. To illustrate the proposed approach we present the design implementation and simulation results for an application. This application addresses the problem of target allocation for a team of unmanned air vehicles that results in an efficient use of available resources. This approach could be easily applied to an existing heuristic based <b>general</b> assignment <b>problem</b> <b>solver</b> to improve the solution quality...|$|R
2500|$|Simon was {{a pioneer}} in the field of {{artificial}} intelligence, creating with Allen Newell the Logic Theory Machine (1956) and the <b>General</b> <b>Problem</b> <b>Solver</b> (GPS) (1957) programs. GPS may possibly be the first method developed for separating problem solving strategy from information about particular problems. Both programs were developed using the Information Processing Language (IPL) (1956) developed by Newell, Cliff Shaw, and Simon. Donald Knuth mentions the development of list processing in IPL, with the linked list originally called [...] "NSS memory" [...] for its inventors. In 1957, Simon predicted that computer chess would surpass human chess abilities within [...] "ten years" [...] when, in reality, that transition took about forty years.|$|E
2500|$|Linked lists were {{developed}} in 1955–1956 by Allen Newell, Cliff Shaw and Herbert A. Simon at RAND Corporation as the primary data structure for their Information Processing Language. IPL {{was used by the}} authors to develop several early artificial intelligence programs, including the Logic Theory Machine, the <b>General</b> <b>Problem</b> <b>Solver,</b> and a computer chess program. Reports on their work appeared in IRE Transactions on Information Theory in 1956, and several conference proceedings from 1957 to 1959, including Proceedings of the Western Joint Computer Conference in 1957 and 1958, and Information Processing (Proceedings of the first UNESCO International Conference on Information Processing) in 1959. The now-classic diagram consisting of blocks representing list nodes with arrows pointing to successive list nodes appears in [...] "Programming the Logic Theory Machine" [...] by Newell and Shaw in Proc. WJCC, February 1957. Newell and Simon were recognized with the ACM Turing Award in 1975 for having [...] "made basic contributions to artificial intelligence, the psychology of human cognition, and list processing".|$|E
50|$|Artificial {{intelligence}} - Several of {{the first}} AI software programs were created at Carnegie Mellon. These include the Logic Theorist, <b>General</b> <b>Problem</b> <b>Solver,</b> and Soar.|$|E
40|$|Argumentative formalisms {{have been}} widely {{recognized}} as knowledge representation and reasoning tools {{able to deal with}} incomplete and potentially contradictory informa- tion. All such formalisms are computationally demanding. Hence, optimizing argumen- tative systems has been approached from di erent views. We have developed a new proposal to solve the aforementioned problem. The key to our approach consists in maintaining a module associated with the knowledge base of the system, with additional information that may help to speed up the inference process. As truth maintenance systems optimize <b>general</b> <b>problem</b> <b>solvers,</b> this module could play a similar role in argumentative systems. Eje: Lógica e Inteligencia artificia...|$|R
50|$|In {{practice}} these theorem provers and <b>general</b> <b>problem</b> <b>solvers</b> were seldom {{useful for}} practical applications and required specialized users {{with knowledge of}} logic to utilize. The first practical application of automated reasoning were expert systems. Expert systems focused on much more well defined domains than <b>general</b> <b>problem</b> solving such as medical diagnosis or analyzing faults in an aircraft. Expert systems also focused on more limited implementations of logic. Rather than attempting to implement {{the full range of}} logical expressions they typically focused on modus-ponens implemented via IF-THEN rules. Focusing on a specific domain and allowing only a restricted subset of logic improved the performance of such systems so that they were practical for use in the real world and not merely as research demonstrations as most previous automated reasoning systems had been. The engine used for automated reasoning in expert systems were typically called inference engines. Those used for more general logical inferencing are typically called theorem provers.|$|R
40|$|At the School of Chemical and Bioengineering the Master {{programmes}} are drastically changed through engineering capabilities driven approach. <b>General</b> <b>problems</b> <b>solvers</b> and {{the ability}} to work on open ended problems are important features of these new engineers. One of the leading visions in this major change is integration on many levels. Integration between different chemistry subjects is not new but the parallel integration between numerical analysis, mathematics and engineering subjects is much more developed than in many other similar projects. Integration exists in terms of subjects but also in terms of different learning environmental (computer studios), teacher/student/student interactions and a more continuous assessment procedure in many subjects. This reformation program started in 2002 with about 180 new students and some results from the preliminarily evaluation will be presented together with the main ideas behind this work...|$|R
50|$|IPL {{was used}} to {{implement}} several early artificial intelligence programs, also by the same authors: the Logic Theorist (1956), the <b>General</b> <b>Problem</b> <b>Solver</b> (1957), and their computer chess program NSS (1958).|$|E
50|$|The first {{reasoning}} {{systems were}} theorem provers, systems that represent axioms and statements in First Order Logic {{and then use}} rules of logic such as modus ponens to infer new statements. Another early type of reasoning system were general problem solvers. These were systems such as the <b>General</b> <b>Problem</b> <b>Solver</b> designed by Newell and Simon. General problem solvers attempted to provide a generic planning engine that could represent and solve structured problems. They worked by decomposing problems into smaller more manageable sub-problems, solving each sub-problem and assembling the partial answers into one final answer. Another example <b>general</b> <b>problem</b> <b>solver</b> was the SOAR family of systems.|$|E
5000|$|Merlin Donald {{argues that}} over {{evolutionary}} time the mind has gained adaptive advantage {{from being a}} <b>general</b> <b>problem</b> <b>solver.</b> The mind, as described by Donald, includes module-like [...] "central" [...] mechanisms, in addition to more recently evolved [...] "domain-general" [...] mechanisms.|$|E
40|$|Fuzzy {{inductive}} reasoning (FIR) is a modelling and simulation methodology {{derived from}} the <b>General</b> Systems <b>Problem</b> <b>Solver.</b> It compares favourably with other soft computing methodologies, such as neural networks, genetic or neuro-fuzzy systems, and with hard computing methodologies, such as AR, ARIMA, or NARMAX, when {{it is used to}} predict future behaviour of different kinds of systems. This paper contains an overview of the FIR methodology, its historical background, and its evolution. Postprint (published version...|$|R
40|$|Argumentation {{systems have}} {{substantially}} {{evolved in the}} past few years, resulting in adequate tools to model some forms of common sense reasoning. This has sprung a new set of argument-based applications in diverse areas. In previous work, we defined how to use precompiled knowledge to obtain significant speed-ups in the inference process of an argument-based system. This development is based on a logic programming system with an argumentationdriven inference engine, called Observation Based Defeasible Logic Programming (ODeLP). In this setting was first presented the concept of dialectical databases, that is, data structures for storing precompiled knowledge. These structures provide precompiled information about inferences and can be used to speed up the inference process, as TMS do in <b>general</b> <b>problem</b> <b>solvers.</b> In this work, we present detailed algorithms for the creation of dialectical databases in ODeLP and analyze these algorithms in terms of their computational complexity...|$|R
40|$|Abstract—Pure {{scientists}} do not only invent {{new methods to}} solve given problems. They also invent new problems. The recent POWERPLAY framework formalizes this type of curiosity and creativity in a new, general, yet practical way. To acquire problem solving prowess through playing, POWERPLAY-based artificial explorers by design continually {{come up with the}} fastest to find, initially novel, but eventually solvable problems. They also continually simplify or speed up solutions to previous problems. We report on results of first experiments with POWERPLAY. A self-delimiting recurrent neural network (SLIM RNN) is used as a general computational architecture to implement the system’s solver. Its weights can encode arbitrary, self-delimiting, halting or non-halting programs affecting both environment (through effectors) and internal states encoding abstractions of event sequences. In open-ended fashion, our POWERPLAY-driven RNNs learn to become increasingly <b>general</b> <b>problem</b> <b>solvers,</b> continually adding new problem solving procedures to the growing repertoire, exhibiting interesting developmental stages. I...|$|R
5000|$|In Alchemy and AI (1965) and What Computers Can't Do (1972), Dreyfus {{summarized}} {{the history of}} artificial intelligence and ridiculed the unbridled optimism that permeated the field. For example, Herbert A. Simon, following {{the success of his}} program <b>General</b> <b>Problem</b> <b>Solver</b> (1957), predicted that by 1967: ...|$|E
5000|$|The MEA {{technique}} as a problem-solving {{strategy was}} first introduced in 1961 by Allen Newell and Herbert A. Simon in their computer problem-solving program <b>General</b> <b>Problem</b> <b>Solver</b> (GPS). [...] In that implementation, the correspondence between differences and actions, also called operators, is provided a priori as knowledge in the system. (In GPS this knowledge was {{in the form of}} table of connections.) ...|$|E
50|$|The {{earliest}} work in computerized {{knowledge representation}} {{was focused on}} general problem solvers such as the <b>General</b> <b>Problem</b> <b>Solver</b> (GPS) system developed by Allen Newell and Herbert A. Simon in 1959. These systems featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal.|$|E
40|$|Truth Maintenance Systems (TMSs) are {{important}} tools for caching beliefs and inferences that support search and explanation tasks of <b>general</b> <b>problem</b> <b>solvers.</b> For embedded real-time systems, fast {{response from the}} TMSs is especially required. To achieve this, Nayak and Williams [9] presented an Incremental Truth Maintenance System (ITMS) based on depth-first numbering to avoid spending {{a significant percentage of}} time on calculating labels that remain constant between contexts. However, the goal of incremental context switching is downgraded by the approximate method they chose, because it may miss qualified propositions or even choose the unqualified for resupport; it may also bring the propagation to stop unintentionally. The algorithm based on root antecedents shown in this paper serves exactly the same purpose, while it avoids the problems encountered in Nayak and Williams' ITMS by adopting a mechanism based on propositional root antecedents that allows constant conclusions to be identified precisely...|$|R
40|$|Post-evaluation {{analysis}} of the model of a constrained optimization problem is conducted after obtaining preliminary optimal or heuristically good solutions. The primary goal of post-evaluation analysis is to reconsider assumptions made in the model {{in the light of}} information generated while finding the good solutions as well as information not previously detailed in the model. We seek extensions of the techniques presently available for the special case of linear programming problems because these special problems allow excellent post-evaluation analysis as a side-effect of seeking solutions. Unfortunately, more <b>general</b> <b>problem</b> <b>solvers</b> presently provide little if any information for post-evaluation analysis. We consider general metaheuristic methods that evolve populations of settings of the decision variables. These methods can contribute greatly to reconsideration of modeling assumptions. This is because the evolving populations taken in total provide a great number of samples for conducting post-evaluation analysis in a data-driven fashion. This is a very general claim. It is illustrated in this paper by a single, rather simple, constrained optimization problem...|$|R
40|$|A {{learning}} <b>problem</b> <b>solver</b> {{consists of}} three components: (1) a <b>problem</b> <b>solver,</b> (2) a memory of problem-solving knowledge, and (3) a learning component for deriving new problemsolving knowledge from experience. Previous learning <b>problem</b> <b>solvers</b> have acquired knowledge as macros, control knowledge, or cases. SteppingStone is a <b>general</b> learning <b>problem</b> <b>solver</b> that improves its performance by learning subgoal sequences. The underlying <b>problem</b> <b>solver</b> for SteppingStone {{is a combination of}} means-ends analysis and brute-force search. It depends primarily upon means-ends analysis and its problem-solving knowledge to solve the subgoals of the problem. Learning occurs only when this approach fails. Upon failure, search is applied and a new subgoal sequence is derived and added to its problem-solving knowledge. Before SteppingStone attempts to solve any of the problem subgoals, it first orders them with a domain independent heuristic which we call openness. Openness is used to order the subgoals to minimize interactions. Stepping-Stone's ability to improve its performance and scale to difficult problems is demonstrated with an implemented system. We show that a small memory of appropriate subgoals yields multiple orders of magnitude savings in problem-solving cost. ...|$|R
50|$|Newell and Simon {{formed a}} lasting partnership. They founded an {{artificial}} intelligence laboratory at Carnegie Mellon University {{and produced a}} series of important programs and theoretical insights throughout the late fifties and sixties. This work included the <b>General</b> <b>Problem</b> <b>Solver,</b> a highly influential implementation of means-ends analysis, and the physical symbol systems hypothesis, the controversial philosophical assertion that all intelligent behavior could be reduced {{to the kind of}} symbol manipulation that Newell's programs demonstrated.|$|E
5000|$|First, in {{the early}} decades of AI {{research}} there were a number of very successful programs that used high level symbol processing, such as Newell and Herbert A. Simon's <b>General</b> <b>Problem</b> <b>Solver</b> or Terry Winograd's SHRDLU. John Haugeland named this kind of AI research [...] "Good Old Fashioned AI" [...] or GOFAI. Expert systems and logic programming are descendants of this tradition. The success of these programs suggested that symbol processing systems could simulate any intelligent action.|$|E
50|$|The <b>General</b> <b>Problem</b> <b>Solver</b> (GPS) is a {{particular}} computer program created in 1957 by Herbert Simon, J.C. Shaw, and Allen Newell intended {{to work as a}} universal problem solver, that theoretically can be used to solve every possible problem that can be formalized in a symbolic system, given the right input configuration. It was the first computer program which separated its knowledge of problems (in the form of domain rules) from its strategy of how to solve problems (as a general search engine).|$|E
40|$|Reasoning {{systems have}} reached {{a high degree of}} {{maturity}} in the last decade. However, even the most successful systems are usually not <b>general</b> purpose <b>problem</b> <b>solvers</b> but are typically specialised on problems in a certain domain. The MathWeb SOftware Bus (Mathweb-SB) is a system for combining reasoning specialists via a common osftware bus. We described the integration of the lambda-clam systems, a reasoning specialist for proofs by induction, into the MathWeb-SB. Due to this integration, lambda-clam now offers its theorem proving expertise to other systems in the MathWeb-SB. On the other hand, lambda-clam can use the services of any reasoning specialist already integrated. We focus on the latter and describe first experimnents on proving theorems by induction using the computational power of the MAPLE system within lambda-clam...|$|R
40|$|Fuzzy Inductive Reasoning (FIR), a {{methodology}} {{based on the}} <b>General</b> System <b>Problem</b> <b>Solver</b> (GSPS), is a tool for identification of qualitative models of dynamical systems {{as well as for}} prediction of their future behavior from past behavior. The identification of qualitative models in FIR is a complex search problem of behavior analysis. The search space grows exponentially with the number of variables (m-inputs). In this paper, different classical techniques (Hill climbing, Branch and Bound and Beam searches with dynamic programming) have been implemented in order to assess their performance in the search problem at hand. A comparative study of these techniques when applied to qualitative model identification of real systems in Biomedical and Biology fields is also presented in the paper...|$|R
40|$|Abstract. Evolutionary {{algorithms}} are randomized search heuristics {{that are}} often described as robust <b>general</b> purpose <b>problem</b> <b>solvers.</b> It is known, however, that the performance of an evolutionary algorithm may be very sensitive to the setting {{of some of its}} parameters. A different perspective is to investigate changes in the expected optimization time due to small changes in the fitness landscape. A class of fitness functions where the expected optimization time of the (1 + 1) evolutionary algorithm is of the same magnitude for almost all of its members is the set of linear fitness functions. Using linear functions as a starting point, a model of a fitness landscape is devised that incorporates important properties of linear functions. Unexpectedly, the expected optimization time of the (1 + 1) evolutionary algorithm is clearly larger for this fitness model than on linear functions. 1...|$|R
50|$|The LISP {{programming}} language has survived since 1958 {{as a primary}} language for Artificial Intelligence research. This text was published in 1992 as the Common LISP standard was becoming widely adopted. Norvig introduces LISP programming {{in the context of}} classic AI programs, including <b>General</b> <b>Problem</b> <b>Solver</b> (GPS) from 1959, ELIZA: Dialog with a Machine, from 1966, and STUDENT: Solving Algebra Word Problems, from 1964. The book covers more recent AI programming techniques, including Logic Programming, Object-Oriented Programming, Knowledge Representation, Symbolic Mathematics and Expert Systems.|$|E
5000|$|Some of the {{earliest}} work in AI used networks or circuits of connected units to simulate intelligent behavior. Examples {{of this kind of}} work, called [...] "connectionism", include Walter Pitts and Warren McCullough's first description of a neural network for logic and Marvin Minsky's work on the SNARC system. In the late '50s, most of these approaches were abandoned when researchers began to explore symbolic reasoning as the essence of intelligence, following the success of programs like the Logic Theorist and the <b>General</b> <b>Problem</b> <b>Solver.</b>|$|E
5000|$|Allen Newell ( [...] ; March 19, 1927 [...] - [...] July 19, 1992) was a {{researcher}} {{in computer science}} and cognitive psychology at the RAND Corporation and at Carnegie Mellon University’s School of Computer Science, Tepper School of Business, and Department of Psychology. He contributed to the Information Processing Language (1956) {{and two of the}} earliest AI programs, the Logic Theory Machine (1956) and the <b>General</b> <b>Problem</b> <b>Solver</b> (1957) (with Herbert A. Simon). He was awarded the ACM's A.M. Turing Award along with Herbert A. Simon in 1975 for their basic contributions to artificial intelligence and the psychology of human cognition.|$|E
40|$|Genetic {{algorithms}} (GAs) {{have been}} used to solve difficult optimization problems in a number of fields. One of the advantages of these algorithms is that they operate well even in domains where little is known, thus giving the GA the flavor of a <b>general</b> purpose <b>problem</b> <b>solver.</b> However, in order to solve a problem with the GA, the user usually has to specify a number of parameters that {{have little to do with}} the user's problem, and have more to do with the way the GA operates. This dissertation presents a technique that greatly simplifies the GA operation by relieving the user from having to set these parameters. Instead, the parameters are set automatically by the algorithm itself. The validity of the approach is illustrated with artificial problems often used to test GA techniques, and also with a simplified version of a network expansion problem...|$|R
40|$|Evolutionary {{algorithms}} are <b>general</b> <b>problem</b> <b>solvers</b> {{that have}} been successfully used in solving combinatorial optimization problems. However, due to the great amount of randomness in these algorithms, theoretical understanding of them is quite challenging. In this thesis we analyse the parameterized complexity of evolutionary algorithms on combinatorial optimization problems. Studying the parameterized complexity of these algorithms can help us understand how different parameters of problems influence the runtime behaviour of the algorithm and consequently lead us in finding better performing algorithms. We focus on two NP-hard combinatorial optimization problems; the generalized travelling salesman problem (GTSP) and the vertex cover problem (VCP). For solving the GTSP, two hierarchical approaches with different neighbourhood structures have been proposed in the literature. In this thesis, local search algorithms and simple evolutionary algorithms based on these approaches are investigated from a theoretical perspective and complementary abilities of the two approaches are pointed out by presenting instances where they mutually outperform each other. After investigating the runtime behaviour of the mentioned randomised algorithms on GTSP, we turn {{our attention to the}} VCP. Evolutionary multi-objective optimization for the classical vertex cover problem has been previously analysed in the context of parameterized complexity analysis. We extend the analysis to the weighted version of the problem. We also examine a dynamic version of the classical problem and analyse evolutionary algorithms with respect to their ability to maintain a 2 -approximation. Inspired by the concept of duality, an edge-based evolutionary algorithm for solving the VCP has been introduced in the literature. Here we show that this edge-based EA is able to maintain a 2 -approximation solution in the dynamic setting. Moreover, using the dual form of the problem, we extend the edge-based approach to the weighted vertex cover problem. Thesis (Ph. D.) [...] University of Adelaide, School of Computer Science, 2017...|$|R
5000|$|A 2010 {{review by}} {{evolutionary}} psychologists Confer et al. suggested that domain general theories, such as for [...] "rationality," [...] has several problems: 1. Evolutionary theories using {{the idea of}} numerous domain-specific adaptions have produced testable predictions that have been empirically confirmed; the theory of domain-general rational thought has produced no such predictions or confirmations. 2. The rapidity of responses such as jealousy due to infidelity indicates a domain-specific dedicated module rather than a general, deliberate, rational calculation of consequences. 3. Reactions may occur instinctively (consistent with innate knowledge) even if a person have not learned such knowledge. One example being that in the ancestral environment {{it is unlikely that}} males during development learn that infidelity (usually secret) may cause paternal uncertainty (from observing the phenotypes of children born many months later and making a statistical conclusion from the phenotype dissimilarity to the cuckolded fathers). With respect to <b>general</b> purpose <b>problem</b> <b>solvers,</b> Barkow, Cosmides, and Tooby (1992) have suggested in The Adapted Mind: Evolutionary Psychology and The Generation of Culture that a purely <b>general</b> <b>problem</b> solving mechanism is impossible to build due to the frame problem. Clune et al. (2013) have argued that computer simulations of the evolution of neural nets suggest that modularity evolves because, compared to non-modular networks, connection costs are lower.|$|R
