168|10000|Public
50|$|Dr. Rabiner has pioneered a {{range of}} novel {{algorithms}} for digital filtering and digital spectrum analysis. The most well known of these algorithms are the Chirp z-Transform method (CZT) of spectral analysis, {{a range of}} optimal FIR (finite impulse response) digital filter design methods based on linear programming and Chebyshev approximation methods, and a class of decimation/interpolation methods for digital sampling rate conversion. In the area of speech processing, Dr. Rabiner has made contributions to the fields of pitch detection, speech synthesis and speech recognition. Dr. Rabiner built {{one of the first}} digital speech synthesizers that was able to convert arbitrary text to intelligible speech. In the area of speech recognition, Dr. Rabiner was a major contributor {{to the creation of the}} statistical method of representing speech that isknown as <b>hidden</b> <b>Markov</b> <b>modeling</b> (HMM). Dr. Rabiner was the first to publish the scaling algorithm for the Forward-Backward method of training of HMM recognizers. His research showed how to successfully implement an HMM system based on either discrete or continuous density parameter distributions. His tutorial paper on HMM is highly cited. Dr. Rabiner’s research resulted in a series of speech recognition systems that went into deployment by AT&T to enable automation of a range of ‘operator services’ that previously had been carried out using live operators. One such system, called the Voice Recognition Call Processing (VRCP) system, automated a small vocabulary recognition system (5 active words) with word spotting and barge-in capability. It resulted in savings of several hundred millions of dollars annually for AT&T.|$|E
40|$|Application {{specific}} voice interfaces {{in local}} languages {{will go a}} long way in reaching the benefits of technology to rural India. A continuous speech recognition system in Hindi tailored to aid teaching Geometry in Primary schools is the goal of the work. This paper presents the preliminary work done towards that end. We have used the Mel Frequency Cepstral Coefficients as speech feature parameters and <b>Hidden</b> <b>Markov</b> <b>Modeling</b> to model the acoustic features. <b>Hidden</b> <b>Markov</b> <b>Modeling</b> Tool Kit — 3. 4 was used both for feature extraction and model generation. The Julius recognizer which is language independent was used for decoding. A speaker independent system is implemented and results are presented...|$|E
40|$|A new {{iterative}} {{approach for}} <b>hidden</b> <b>Markov</b> <b>modeling</b> of information sources which aims at minimizing the discrimination information (or the cross-entropy) between {{the source and}} the model is proposed. This approach {{does not require the}} commonly used assumption that the source to be modeled is a hidden Markov process. The algorithm is started from the model estimated by the traditional maximum likelihood (ML) approach and alternatively decreases the discrimination information over all probability distributions of the source which agree with the given measurements and all hidden Markov models. The proposed procedure generalizes the Baum algorithm for ML <b>hidden</b> <b>Markov</b> <b>modeling.</b> The procedure is shown to be a descent algorithm for the discrimination information measure and its local convergence is proved. 1...|$|E
40|$|This paper {{proposes a}} novel method for {{decoding}} any high-order <b>hidden</b> <b>Markov</b> <b>model.</b> First, the high-order <b>hidden</b> <b>Markov</b> <b>model</b> {{is transformed into}} an equivalent first-order <b>hidden</b> <b>Markov</b> <b>model</b> by Hadar’s transformation. Next, the optimal state sequence of the equivalent first-order <b>hidden</b> <b>Markov</b> <b>model</b> is recognized by the existing Viterbi algorithm of the first-order <b>hidden</b> <b>Markov</b> <b>model.</b> Finally, the optimal state sequence of the high-order <b>hidden</b> <b>Markov</b> <b>model</b> is inferred from the optimal state sequence of the equivalent first-order <b>hidden</b> <b>Markov</b> <b>model.</b> This method provides a unified algorithm framework for decoding <b>hidden</b> <b>Markov</b> <b>models</b> including the first-order <b>hidden</b> <b>Markov</b> <b>model</b> and any high-order <b>hidden</b> <b>Markov</b> <b>model...</b>|$|R
40|$|Abstract. The <b>hidden</b> <b>markov</b> <b>model</b> {{is a kind}} of {{important}} probability model of series data processing and statistical learning and it has been successfully applied in many engineering tasks. This paper introduces the basic principle of <b>hidden</b> <b>markov</b> <b>model</b> firstly, and then discusses the limitations of <b>hidden</b> <b>markov</b> <b>model,</b> as well as the improved <b>hidden</b> <b>markov</b> <b>model</b> which is put forward to solve these problems...|$|R
40|$|Abstract-The <b>hidden</b> <b>markov</b> <b>model</b> {{is a kind}} of {{important}} probability model of series data processing and statistical learning and it has been successfully applied in many engineering tasks. This paper introduces the basic principle of <b>hidden</b> <b>markov</b> <b>model</b> firstly, and then discusses the limitations of <b>hidden</b> <b>markov</b> <b>model,</b> as well as the improved <b>hidden</b> <b>markov</b> <b>model</b> which is put forward to solve these problems. Keywords- HMM, <b>Markov</b> family <b>model,</b> DDBHMM...|$|R
40|$|We {{present and}} {{evaluate}} {{a novel approach}} towards automatically detecting a speaker’s level of dominance in a meeting scenario. Since previous studies reveal that audio {{appears to be the}} most important modality for dominance recognition, we focus on the analysis of the speech signals recorded in multiparty meetings. Unlike recently published techniques which concentrate on frame-level <b>hidden</b> <b>Markov</b> <b>modeling,</b> we propose a recognition framework operating on segmental data and investigate context modeling on three different levels to explore possible performance gains. First, we apply a set of statistical functionals to capture large-scale feature-level context within a speech segment. Second, we consider bidirectional Long Short-Term Memory recurrent neural networks for long-range temporal context modeling between segments. Finally, we evaluate the benefit of situational context incorporation by simultaneously modeling speech of all meeting participants. Overall, our approach leads to a remarkable increase of recognition accuracy when compared to <b>hidden</b> <b>Markov</b> <b>modeling.</b> Index Terms: dominance recognition, meeting analysis, Long Short-Term Memory, audio feature extractio...|$|E
40|$|Date stream mining {{techniques}} {{can be used}} in tracking user behaviors as they attempt to achieve their goals. Quality metrics over stream-mined models identify potential changes in user goal attainment. When the quality of some data mined models varies significantly from nearby models—as defined by quality metrics—then the user’s behavior is automatically flagged as a potentially significant behavioral change. Decision tree, sequence pattern and <b>Hidden</b> <b>Markov</b> <b>modeling</b> being used in this study. These three types of modeling can expose different aspect of user’s behavior. In case of decision tree modeling, the specific changes in user behavior can automatically characterized by differencing the data-mined decision-tree models. The sequence pattern modeling can shed light on how the user changes his sequence of actions and <b>Hidden</b> <b>Markov</b> <b>modeling</b> can identifies the learning transition points. This research describes how model-quality monitoring and these three types of modeling as a generic framework can aid recognition and diagnoses of behavioral changes in a case study of cognitive rehabilitation via emailing. The date stream mining techniques mentioned are used to monitor patient goals as part of a clinical plan to aid cognitive rehabilitation. In this context, real time data mining aids clinicians in tracking user behaviors as they attempt to achieve their goals. This generic framework can be widely applicable to other real-time data-intensive analysis problems. In order to illustrate this fact, the similar <b>Hidden</b> <b>Markov</b> <b>modeling</b> is being used for analyzing the transactional behavior of a telecommunication company for fraud detection. Fraud similarly can be considered as a potentially significant transaction behavioral change...|$|E
40|$|Abstract: A {{methodology}} using <b>Hidden</b> <b>Markov</b> <b>Modeling</b> (HMM) is used {{to analyze}} medical procedural data from the E-Pelvis database. The {{focus is on the}} method of selection of HMM parameters. K-Means {{is used to}} choose the alphabet size. Successful classification rates of near 60 % are observed. This result is a less accurate than seen when using Markov Modeling in a previous study. 1...|$|E
40|$|We {{describe}} pair <b>hidden</b> <b>Markov</b> <b>models,</b> with {{an emphasis}} on their relationship to evolutionary <b>models</b> and <b>hidden</b> <b>Markov</b> <b>models.</b> We then explain the statistical interpretation of alignment with pair <b>hidden</b> <b>Markov</b> <b>models,</b> and highlight connections to the Needleman–Wunsch algorithm and other dynamic programming–based alignment algorithms...|$|R
40|$|Abstract: In this paper, the second-order <b>hidden</b> <b>Markov</b> <b>models</b> (HMM 2 s) {{have been}} used to enhance the {{recognition}} performance of isolated-word text-dependent speaker authentication systems under the neutral talking condition. Our results show that HMM 2 s enhance the speaker authentication performance under such a condition compared to the firstorder <b>hidden</b> <b>Markov</b> <b>models</b> (HMM 1 s). The speaker authentication performance has been improved by 6 %. Key words: First-order <b>hidden</b> <b>Markov</b> <b>models,</b> neutral talking condition, second-order <b>hidden</b> <b>Markov</b> <b>models,</b> speaker authentication performance...|$|R
50|$|The layered <b>hidden</b> <b>Markov</b> <b>model</b> (LHMM) is a {{statistical}} model {{derived from the}} <b>hidden</b> <b>Markov</b> <b>model</b> (HMM). A layered <b>hidden</b> <b>Markov</b> <b>model</b> (LHMM) consists of N levels of HMMs, where the HMMs on level i + 1 correspond to observation symbols or probability generators at level i.Every level i of the LHMM consists of Ki HMMs running in parallel.|$|R
40|$|In this paper, we {{describe}} a typical approach for implementing a voice based control solution. Iso-lated word speech recognition {{is performed using}} cep-stral feature extraction and <b>hidden</b> <b>Markov</b> <b>modeling</b> of speech. The merit of this document lies in the amalga-mation of the simplest yet most successful relevant methods into a coherent design guideline, aiming to trivialize the integration of speech technology into daily applications...|$|E
40|$|Abstract − In this paper, {{we present}} the results of {{evaluating}} {{the performance of the}} forward link of a CDMA cellular radio system. We use a semianalytical approach consisting of the waveform level simulation of the analog channel followed by the <b>hidden</b> <b>Markov</b> <b>modeling</b> of the discrete channel. The performance parameters of the discrete channel were evaluated analytically using the hidden Markov model and compared with the results of simulation. I...|$|E
40|$|Abstract. In {{this paper}} we {{consider}} the problem of joint segmentation of hyperspectral images in the Bayesian framework. The proposed approach {{is based on a}} <b>Hidden</b> <b>Markov</b> <b>Modeling</b> (HMM) of the images with common segmentation, or equivalently with common hidden classification label variables which is modeled by a Potts Markov Random Field. We introduce an appropriate Markov Chain Monte Carlo (MCMC) algorithm to implement the method and show some simulation results...|$|E
40|$|Hidden <b>Markov</b> <b>models</b> are {{extensions}} of <b>Markov</b> <b>models</b> where each observation {{is the result}} of a stochastic process in one of several unobserved states. Though favored by many scientists because of its unique and applicable mathematical structure, its independence assumption between the consecutive observations hampered further application. Autoregressive <b>hidden</b> <b>Markov</b> <b>model</b> is a combination of autoregressive time series and <b>hidden</b> <b>Markov</b> chains. Observations are generated by a few autoregressive time series while the switches between each autoregressive time series are controlled by a <b>hidden</b> <b>Markov</b> chain. In this thesis, we present the basic concepts, theory and associated approaches and algorithms for <b>hidden</b> <b>Markov</b> <b>models,</b> time series and autoregressive <b>hidden</b> <b>Markov</b> <b>models.</b> We have also built a bivariate autoregressive <b>hidden</b> <b>Markov</b> <b>model</b> on the temperature data from the Pacific Ocean to understand the mechanism of El Nino. The parameters and the state path of the model are estimated through the Segmental K-mean algorithm and the state estimations of the autoregressive <b>hidden</b> <b>Markov</b> <b>model</b> have been compared with the estimations from a conventional <b>hidden</b> <b>Markov</b> <b>model.</b> Overall, the results confirm the strength of the autoregressive <b>hidden</b> <b>Markov</b> <b>models</b> in the El Nino study and the research sets an example of ARHMM's application in the meteorology...|$|R
40|$|An on-line {{unsupervised}} algorithm for {{estimating the}} <b>hidden</b> <b>Markov</b> <b>models</b> (HMM) parame-ters is presented. The problem of <b>hidden</b> <b>Markov</b> <b>models</b> adaptation to emotional speech is solved. To increase {{the reliability of}} estimated HMM parameters, a mechanism of forgetting and updating is proposed. A functional block diagram of the <b>hidden</b> <b>Markov</b> <b>models</b> adaptation algorithm is also provided with obtained results, which improve the efficiency of emotional speech recognition...|$|R
40|$|The biggest {{difficulty}} of <b>hidden</b> <b>Markov</b> <b>model</b> applied to multistep attack is {{the determination of}} observations. Now the research of the determination of observations is still lacking, and it shows {{a certain degree of}} subjectivity. In this regard, we integrate the attack intentions and <b>hidden</b> <b>Markov</b> <b>model</b> (HMM) and support a method to forecasting multistep attack based on <b>hidden</b> <b>Markov</b> <b>model.</b> Firstly, we train the existing <b>hidden</b> <b>Markov</b> <b>model(s)</b> by the Baum-Welch algorithm of HMM. Then we recognize the alert belonging to attack scenarios with the Forward algorithm of HMM. Finally, we forecast the next possible attack sequence with the Viterbi algorithm of HMM. The results of simulation experiments show that the <b>hidden</b> <b>Markov</b> <b>models</b> which have been trained are better than the untrained in recognition and prediction...|$|R
40|$|Amharic is the Semitic {{language}} {{that has the}} second large number of speakers after Arabic (Hayward and Richard 1999). Its writing system is syllabic with Consonant-Vowel (CV) syllable structure. Amharic orthography has {{more or less a}} one to one correspondence with syllabic sounds. We have used this feature of Amharic to develop a CV syllable-based speech recognizer, using <b>Hidden</b> <b>Markov</b> <b>Modeling</b> (HMM), and achieved 90. 43 % word recognition accuracy. ...|$|E
40|$|This paper {{provides}} a tutorial on key issues in <b>hidden</b> <b>Markov</b> <b>modeling.</b> Hidden Markov models {{have become very}} popular models for time series and longitudinal data in recent years due {{to a combination of}} (relative) simplicity and flexibility in adapting the model to novel situations. The tutorial covers the conceptual description of the model, estimation of parameters through maximum likelihood, and ends with an application to real data illustrating the possibilities...|$|E
40|$|Current {{approaches}} to model responses and response times to psychometric tests solely focus on between-subject differences in speed and ability. Within subjects, speed and ability {{are assumed to}} be constants. Violations of this assumption are generally absorbed in the residual of the model. As a result, within-subject departures from the between-subject speed and ability level remain undetected. These departures may be of interest to the researcher as they reflect differences in the response processes adopted on the items of a test. In this article, we propose a dynamic approach for responses and response times based on <b>hidden</b> <b>Markov</b> <b>modeling</b> to account for within-subject differences in responses and response times. A simulation study is conducted to demonstrate acceptable parameter recovery and acceptable performance of various fit indices in distinguishing between different models. In addition, both a confirmatory and an exploratory application are presented to demonstrate the practical value of the modeling approach. KEYWORDS: Conditional independence, dynamic modeling, <b>hidden</b> <b>Markov</b> <b>modeling,</b> item response theory, latent class models, response time modelin...|$|E
40|$|Master of ScienceDepartment of Computing and Information SciencesDavid A. GustafsonThe use of <b>hidden</b> <b>Markov</b> <b>models</b> in autism {{recognition}} and analysis is investigated. More speciﬁcally, {{we would like}} to be able to determine a person's level of autism (AS, HFA, MFA, LFA) using <b>hidden</b> <b>Markov</b> <b>models</b> trained on observations taken from a subject's behavior in an experiment. A preliminary model is described that includes the three mental states self-absorbed, attentive, and join-attentive. Futhermore, observations are included that are more or less indicative of each of these states. Two experiments are described, the ﬁrst on a single subject and the second on two subjects. Data was collected from one individual in the second experiment and observations were prepared for input to <b>hidden</b> <b>Markov</b> <b>models</b> and the resulting <b>hidden</b> <b>Markov</b> <b>models</b> were studied. Several questions subsequently arose and tests, written in Java using the JaHMM <b>hidden</b> <b>Markov</b> <b>model</b> tool- kit, were conducted to learn more about the <b>hidden</b> <b>Markov</b> <b>models</b> being used as autism recognizers and the training algorithms being used to train them. The tests are described along with the corresponding results and implications. Finally, suggestions are made for future work. It turns out that we aren't yet able to produce <b>hidden</b> <b>Markov</b> <b>models</b> that are indicative of a persons level of autism and the problems encountered are discussed and the suggested future work is intended to further investigate the use of <b>hidden</b> <b>Markov</b> <b>models</b> in autism recognition...|$|R
40|$|AbstractThe basic {{theory of}} <b>hidden</b> <b>Markov</b> <b>models</b> was {{developed}} and applied to problems in speech recognition in the late 1960 s, and has since then been applied to numerous problems, e. g. biological sequence analysis. Most applications of <b>hidden</b> <b>Markov</b> <b>models</b> are based on efficient algorithms for computing the probability of generating a given string, or computing the most likely path generating a given string. In this paper we consider the problem of computing the most likely string, or consensus string, generated by a given model, and its implications on the complexity of comparing <b>hidden</b> <b>Markov</b> <b>models.</b> We show that computing the consensus string, and approximating its probability within any constant factor, is NP-hard, and that the same holds for the closely related labeling problem for class <b>hidden</b> <b>Markov</b> <b>models.</b> Furthermore, we establish the NP-hardness of comparing two <b>hidden</b> <b>Markov</b> <b>models</b> under the L∞- and L 1 -norms. We discuss {{the applicability of the}} technique used for proving the hardness of comparing two <b>hidden</b> <b>Markov</b> <b>models</b> under the L 1 -norm to other measures of distance between probability distributions. In particular, we show that it cannot be used for proving NP-hardness of determining the Kullback–Leibler distance between two <b>hidden</b> <b>Markov</b> <b>models,</b> or of comparing them under the Lk-norm for any fixed even integer k...|$|R
40|$|This paper reviews recent {{advances}} in Bayesian nonparametric techniques for constructing and performing inference in infinite <b>hidden</b> <b>Markov</b> <b>models.</b> We focus on variants of Bayesian nonparametric <b>hidden</b> <b>Markov</b> <b>models</b> that enhance a posteriori state-persistence in particular. This paper also introduces a new Bayesian nonparametric framework for generating left-to-right and other structured, explicit-duration infinite <b>hidden</b> <b>Markov</b> <b>models</b> that we call the infinite structured hidden semi-Markov model. Comment: 23 pages, 10 figure...|$|R
40|$|This thesis investigates {{a method}} for using contextual {{information}} in text recognition. This {{is based on the}} premise that, while reading, humans recognize words with missing or garbled characters by examining the surrounding characters and then selecting the appropriate character. The correct character is chosen based on an inherent knowledge of the language and spelling techniques. We can then model this statistically. The approach taken by this Thesis is to combine feature extraction techniques, Neural Networks and <b>Hidden</b> <b>Markov</b> <b>Modeling.</b> This method of character recognition involves a three step process: pixel image preprocessing, neural network classification and context interpretation. Pixel image preprocessing applies a feature extraction algorithm to original bit mapped images, which produces a feature vector for the original images which are input into a neural network. The neural network performs the initial classification of the characters by producing ten weights, one for each character. The magnitude of the weight is translated into the confidence the network has in each of the choices. The greater the magnitude and separation, the more confident the neural network is of a given choice. The output of the neural network is the input for a context interpreter. The context interpreter uses <b>Hidden</b> <b>Markov</b> <b>Modeling</b> (HMM) techniques to determine the most probable classification for all characters based on the characters that precede that character and character pair statistics. The HMMs are built using an a priori knowledge of the language: a statistical description of the probabilities of digrams. Experimentation and verification of this method combines the development and use of a preprocessor program, a Cascade Correlation Neural Network and a HMM context interpreter program. Results from these experiments show the neural network successfully classified 88. 2 percent of the characters. Expanding this to the word level, 63 percent of the words were correctly identified. Adding the <b>Hidden</b> <b>Markov</b> <b>Modeling</b> improved the word recognition to 82. 9 percent...|$|E
40|$|Abstract — This {{correspondence}} {{presents the}} first known results of complete recognition of continuous Mandarin speech for the Chinese language with very large vocabulary but very limited training data. Various acoustic and linguistic processing techniques were developed, and a prototype system of a continuous speech Mandarin dictation machine has been successfully implemented. The best recognition accuracy achieved is 92. 2 % for finally decoded Chinese characters. Index Terms — Mandarin speech, recognition, very large vocabulary, continuous speech, tone recognition, <b>hidden</b> <b>Markov</b> <b>modeling,</b> Chinese language model I...|$|E
40|$|In {{this paper}} we {{consider}} the problem of joint segmentation of hyperspectral images in the Bayesian framework. The proposed approach {{is based on a}} <b>Hidden</b> <b>Markov</b> <b>Modeling</b> (HMM) of the images with common segmentation, or equivalently with common hidden classification label variables which is modeled by a Potts Markov Random Field. We introduce an appropriate Markov Chain Monte Carlo (MCMC) algorithm to implement the method and show some simulation results. Comment: 8 pages, 2 figures, presented at MaxEnt 2004, Inst. Max Planck, Garching, German...|$|E
40|$|Automatic {{detection}} of semantic events in sport videos is a challenging task. In this paper, we propose a multimodal multilayer statistical inference framework for semantic sports video analysis using Dynamic Bayesian Networks (DBNs). Based on this framework, three instances including factorial hierarchical <b>hidden</b> <b>Markov</b> <b>model</b> (FHHMM), coupled hierarchical <b>hidden</b> <b>Markov</b> <b>model</b> (CHHMM), and product hierarchical <b>hidden</b> <b>Markov</b> <b>model</b> (PHHMM), are constructed and compared. Play-break detection in soccer videos {{is used as}} a testbed with hierarchical <b>hidden</b> <b>Markov</b> <b>model</b> (HHMM) as a baseline. Experimental results indicate the superior capability of the PHHMM, because it not only effectively models dynamic interactions between different modalities, but also sufficiently utilizes context constraints in multilayer structures...|$|R
40|$|In this paper, {{we study}} {{the problem of}} {{learning}} phylogenies and <b>hidden</b> <b>Markov</b> <b>models.</b> We call a <b>Markov</b> <b>model</b> nonsingular if all transition matrices have determinants bounded away from 0 (and 1). We highlight {{the role of the}} nonsingularity condition for the learning problem. Learning <b>hidden</b> <b>Markov</b> <b>models</b> without the nonsingularity condition is at least as hard as learning parity with noise. On the other hand, we give a polynomial-time algorithm for learning nonsingular phylogenies and <b>hidden</b> <b>Markov</b> <b>models...</b>|$|R
40|$|In {{the current}} thesis several {{selected}} {{aspects of the}} two related latent class models; finite mixtures and <b>hidden</b> <b>Markov</b> <b>models,</b> were considered. The problem of calculating the MLE of a Gaussian mixture with Newton's method and the consistency of penalized MLE were investigated. A penalized MLE procedure for univariate Gaussian <b>hidden</b> <b>Markov</b> <b>models</b> was introduced and shown to be consistent. Furthermore, a result on identifiability of nonparametric <b>hidden</b> <b>Markov</b> <b>models</b> is derived...|$|R
40|$|Abstract. Education {{research}} has identified strategic flexibility {{as an important}} aspect of math proficiency and learning. This aspect of student learning has been largely ignored by Intelligent Tutoring Systems (ITSs). In the current study, we demonstrate how <b>Hidden</b> <b>Markov</b> <b>Modeling</b> can be used to identify groups of students who use similar strategies during tutoring and relate these findings to a measure of strategic flexibility. We use these results to explore how strategy use is expressed in an ITS and consider how tutoring systems could integrate a measure of strategy use to improve learning. ...|$|E
40|$|Abstract—We propose an {{algorithm}} for {{the classification}} of structural damage based {{on the use of}} the continuous <b>hidden</b> <b>Markov</b> <b>modeling</b> (HMM) technique. Our approach employs HMMs to model time-frequency damage features extracted from structural data using the matching pursuit decomposition algorithm. We investigate modeling with continuous observation-density HMMs and discuss the trade-offs involved as compared to the discrete HMM case. A variational Bayesian method is employed to automatically estimate the HMM state number and adapt the classifier for real-time use. We present results that classify structural and material (fatigue) damage in a bolted-joint structure. I...|$|E
40|$|This paper {{describes}} a novel technique for producing smooth speech parametric representation evolution {{by means of}} an application of traditional <b>hidden</b> <b>Markov</b> <b>modeling</b> techniques. It is based on the consideration that HMM training is able to locate the main acoustic events which occur during the speech process; thus the obtained decomposition can be used to reproduce the linguistic units utilized for training. This is accomplished by interpolating the state-related features values with some weighting functions, as it is done for temporal decomposition technique [1] [15]. Results will be given for isolated word synthesis...|$|E
40|$|Background: Baum-Welch {{training}} is an expectation-maximisation algorithm for training the emission and transition probabilities of <b>hidden</b> <b>Markov</b> <b>models.</b> Until know, no memory-efficient algorithm was available. Methods and results: We introduce a linear space algorithm for Baum-Welch training. For a <b>hidden</b> <b>Markov</b> <b>model</b> with M states, an input sequence of length L and |T | transition and |E | emission parameters, it requires O(M) memory and O(L(|T | + |E|) |T |) time rather than O(LM) memory and O(L|T | + L(|T | + |E|)) time. For a pair <b>hidden</b> <b>Markov</b> <b>model</b> with input sequences Lx and Ly, {{the requirement of}} O(LxLyM) memory and O(LxLy|T | + LxLy(|T | + |E|)) time is reduced to O(min{Lx, Ly}M) memory and O(LxLy(|T | + |E|) |T |) time. For <b>hidden</b> <b>Markov</b> <b>models</b> used on long sequences, for example in gene finding, this means a huge memory saving compared to the best available current algorithms which need O (√ LM) (<b>hidden</b> <b>Markov</b> <b>models)</b> or O (√ LxLyM) (pair <b>hidden</b> <b>Markov</b> <b>models)</b> memory. Additionally, our new algorithm is very simple {{in the sense that}} its implementation does not require sophisticated programming techniques like checkpoints or recursive functions. Conclusions: Using our algorithm, Baum-Welch training can now be performed on standard personal computers, even for sophisticated pair <b>hidden</b> <b>Markov</b> <b>models</b> and very long biological sequences. ...|$|R
40|$|This {{paper is}} an {{exploration}} of <b>hidden</b> <b>Markov</b> <b>models</b> and their applications. After a brief discussion of ordinary <b>Markov</b> <b>models</b> (<b>Markov</b> chains) {{and some of their}} applications in mathematics, I will focus on <b>hidden</b> <b>Markov</b> <b>models.</b> I will explain several algorithms which are used in conjunction with <b>hidden</b> <b>Markov</b> models: the Viterbi algorithm, the forward algorithm, and the backward algorithm. I will also discuss the estimation of parameters of a <b>hidden</b> <b>Markov</b> <b>model.</b> The main applications this paper examines are DNA sequence analysis, speech recognition, and protein family alignment and sequencing...|$|R
40|$|We {{propose a}} dynamic {{graphical}} model which generalizes nonhomogeneous <b>hidden</b> <b>Markov</b> <b>models.</b> Inference and forecast procedures are developed. A comparison with an exact propagation algorithm is established and equivalence is stated. Graphical <b>models</b> Nonhomogeneous <b>hidden</b> <b>Markov</b> <b>models</b> Inference and forecast procedures...|$|R
