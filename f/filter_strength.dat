29|57|Public
50|$|In {{contrast}} with older MPEG-1/2/4 standards, the H.264 deblocking filter {{is not an}} optional additional feature in the decoder. It is a feature on both the decoding path and on the encoding path, so that the in-loop effects of the filter {{are taken into account}} in reference macroblocks used for prediction. When a stream is encoded, the <b>filter</b> <b>strength</b> can be selected, or the filter can be switched off entirely. Otherwise, the <b>filter</b> <b>strength</b> is determined by coding modes of adjacent blocks, quantization step size, and the steepness of the luminance gradient between blocks.|$|E
50|$|The filter {{operates}} {{on the edges}} of each 4×4 or 8×8 transform block in the luma and chroma planes of each picture. Each small block's edge is assigned a boundary strength based on whether it is also a macroblock boundary, the coding (intra/inter) of the blocks, whether references (in motion prediction and reference frame choice) differ, and whether it is a luma or chroma edge. Stronger levels of filtering are assigned by this scheme where there is likely to be more distortion. The filter can modify as many as three samples on either side of a given block edge (in the case where an edge is a luma edge that lies between different macroblocks and {{at least one of them}} is intra coded). In most cases it can modify one or two samples on either side of the edge (depending on the quantization step size, the tuning of the <b>filter</b> <b>strength</b> by the encoder, the result of an edge detection test, and other factors).|$|E
40|$|Product SpecificationLogiCORE IP Image Noise Reduction v 1. 0 The noise_reg 02 _filt_strength {{register}} is double buffered in {{hardware to}} ensure no image tearing {{happens if the}} <b>filter</b> <b>strength</b> value is modified in the active area of a frame. This double buffering provides system control that is more flexible {{and easier to use}} by decoupling the register updates from the blanking period, allowing software a much larger window with which to update the parameter values. The updated value for the <b>filter</b> <b>strength</b> register is latched into the shadow register immediately after writing, while the actual <b>filter</b> <b>strength</b> used is stored in the working register. Any reads of registers during operation return the values stored in the shadow registers. The rising edge of vblank_in triggers the values from the shadow registers to be copied to the working registers when bit 1 of noise_reg 00 _control is set to 1. This semaphore bit helps to prevent changing the <b>filter</b> <b>strength</b> mid-frame. Figure 6 shows a software flow diagram for updating registers during the operation of the core...|$|E
3000|$|Noise {{is one of}} {{the most}} {{critical}} problems in digital images, especially in low-light conditions. The relative amount of [...] "chroma" [...] and [...] "luminance" [...] noise varies depending on the exposure settings and on the camera model. In particular, low-light no-flash photography suffers from severe noise problems. A complete elimination of luminance noise can be unnatural and the full chroma noise removal can introduce false colors; so the denoising algorithms should vary properly the <b>filtering</b> <b>strength,</b> depending on the input local characteristics.|$|R
40|$|International audienceWe show {{theoretically}} and validate experimentally {{the effect}} of filtering on the nonlinear behavior of slow and fast light links based on coherent population oscillations in semiconductor optical amplifiers. The existence of {{a dip in the}} power-versus-current characteristics for the fundamental frequency, {{as well as for the}} third-order intermodulation product, is clearly evidenced. These two dips occur at different bias currents. Their depths increase as the <b>filtering</b> <b>strength</b> of the red sideband is increased, and they completely vanish in the unfiltered case. Influence on the microwave photonics link is discussed...|$|R
40|$|Abstract- This paper {{describes}} our proposed {{method of}} prioritising contents {{in the video}} by applying the variable bilateral filter. The aim of this investigation is {{to determine whether the}} variable bilateral filtering technique can improve subjective video quality at low bit rates. The proposed system has been designed to prioritise human faces in the video. It uses a weighting function to produce a spatial resolution fall-off map that matches the resolution variance of the human visual system. The variable bilateral filter parameters are calculated based on this resolution fall-off map. Brief descriptions of the algorithm and the performance of the system are illustrated for a range of bilateral <b>filter</b> <b>strengths.</b> The results demonstrate that this approach can significantly improve perceptual quality of coded video at low bit-rates...|$|R
40|$|Low-field {{magnetic}} resonance imaging (MRI) has recently been integrated with radiation therapy systems to provide image guidance for daily cancer radiation treatments. The main benefit of the low-field strength is minimal electron return effects. The main disadvantage of low-field strength is increased image noise compared to diagnostic MRIs conducted at 1. 5 T or higher. The increased image noise affects both the discernibility of soft tissues and the accuracy of further image processing tasks for both clinical and research applications, such as tumor tracking, feature analysis, image segmentation, and image registration. An innovative method, adaptive anatomical preservation optimal denoising (AAPOD), was developed for optimal image denoising, i. e., to maximally reduce noise while preserving the tissue boundaries. AAPOD employs a series of adaptive nonlocal mean (ANLM) denoising trials with increasing denoising <b>filter</b> <b>strength</b> (i. e., the block similarity filtering parameter in the ANLM algorithm), and then detects the tissue boundary losses on the differences of sequentially denoised images using a zero-crossing edge detection method. The optimal denoising <b>filter</b> <b>strength</b> per voxel is determined by identifying the denoising <b>filter</b> <b>strength</b> value at which boundary losses start to appear around the voxel. The final denoising result is generated by applying the ANLM denoising method with the optimal per-voxel denoising filter strengths. The experimental results demonstrated that AAPOD was capable of reducing noise adaptively and optimally while avoiding tissue boundary losses. AAPOD is useful for {{improving the quality of}} MRIs with low-contrast-to-noise ratios and could be applied to other medical imaging modalities, e. g., computed tomography. (C) 2017 Society of Photo-Optical Instrumentation Engineers (SPIE...|$|E
40|$|An intense {{negative}} {{hydrogen ion}} source has been developed, {{which has a}} strong external magnetic filter field in the wide area of 35 cm x 62 cm produced {{by a pair of}} permanent magnet rows located with 35. 4 cm separation. The <b>filter</b> <b>strength</b> is 70 G in the center and the line-integrated <b>filter</b> <b>strength</b> is 850 G cm, which keeps the low electron temperature in the extraction region. Strong cusp magnetic field, 1. 8 kG on the chamber surface, is generated for improvement of the plasma confinement. These resulted in the high arc efficiency at the low operational gas pressure. A 16. 2 A of the H ^- ion current with the energy of 47 keV was obtained at the arc efficiency of 0. 1 A/kW at the gas pressure of 3. 8 mTorr in the cesium-mode operation. The magnetic field in the extraction gap is also strong, 450 G, for the electron suppression. The ratio of the extraction to the negative ion currents was less than 2. 2 at the gas pressure of 3 mTorr. The two-stage acceleration was tried, and a 13. 6 A of the H^- ion beam was accelerated to 125 keV. ...|$|E
40|$|This paper {{describes}} {{a new approach}} for video sequences noise reduction that directly processes raw data frames acquired by an image sensor. The proposed noise reduction filter works on CFA- (Color Filter Array) Bayer matrixed video data instead of the canonical YUV format. Working on CFA raw frames allows saving resources {{in terms of time}} and space; this is particularly relevant for real time processing. Noise level is continuously monitored in order to modify the <b>filter</b> <b>strength</b> adaptively. Experiments demonstrate the effectiveness of the proposed method...|$|E
40|$|We have {{demonstrated}} a reproducible and accurate way of tuning the coupling wavelength of a mismatched twin-core filter. This method allows high quality all-fiber filters {{to be implemented}} with low loss at any desired wavelength over a range of few hundred nanometers from the same fiber. An ~ 500 -nm tuning range is demonstrated, limited only by the measurement setup and not by the technique itself. Highly accurate control of the filtering wavelength in combination with control of <b>filtering</b> <b>strength</b> by adjustment {{of the length of}} the twin-core fibers allows complex filter profiles to be implemented by cascading of several different filters. These filters are also very stable to a change in temperature (~ 0. 26 nm/ 100 °C at 1. 55 µm) and strain (less than 0. 3 nm/mstrain at 1. 55 µm), permitting easy packaging...|$|R
40|$|The {{stability}} and resolution of iterative PIV image analysis methods is investigated. The study {{focuses on the}} effects of stabilization by means of spatial filtering when implemented into the iterative process. Two filtering approaches are studied: predictor and corrector filtering respectively. A family of convolution filters is proposed, which allows to vary the <b>filtering</b> <b>strength</b> in a systematic way and primarily affects the system {{stability and}} to a smaller extent its spatial response. A critical value for the filter parameter is identified which guarantees the stability of the iterative process. A theoretical analysis is provided that determines the asymptotic properties of the iterative method with varying filter parameters. The study is completed with a numerical assessment and concludes with an application to real experiments, showing the consequence of an incorrect implementation of the iterative scheme under experimental conditions. Aerospace Engineerin...|$|R
30|$|When the Bubble Index is 1, the L 2 -IBI {{processor}} calculates {{the correlation}} coefficient between <b>filtered</b> residual <b>strength</b> and <b>filtered</b> plasma density. The {{square of the}} correlation coefficient is used as the Bubble Probability. With higher Bubble Probability it is more probable that the related magnetic field data point is affected by EPBs. As stated above, the Bubble Flag is 1 only if the Bubble Probability exceeds a certain threshold (e.g., 0.5). If the Bubble Index is not 1, or there is no plasma density data around the magnetically detected events (‘Unconfirmed Bubble’), the Bubble Probability is set to 0 automatically. Combining the Bubble Probability and the Bubble Index end users may impose stricter criterion on the correlation between <b>filtered</b> residual <b>strength</b> and <b>filtered</b> plasma density. For example, end users may take only those data points with Bubble Index 1 and Bubble Probability ≥ 0.8 as ‘Confirmed Bubble’.|$|R
3000|$|The SGBNR is {{designed}} to smooth image areas where there are few or no details, but preserving small structures and contrast. In order {{to achieve this goal}} it uses a lowpass filter with a strength that depends on edge features of the image. The filtering intensity is driven by the filter size, the [...] "Amount" [...] parameter, which fixes the percentage of the original pixel value to be preserved, and the [...] "Edges Protection Threshold", which evaluates the [...] "edgeness" [...] degree of each pixel (also depending on the luminance level) and modulates the <b>filter</b> <b>strength</b> consequently.|$|E
40|$|The {{possibility}} of immunized {{and improving the}} entanglement of accelerated systems via local filtering is discussed. The maximum bounds of entanglement depend on {{the dimensions of the}} accelerated and the filtered subsystems. If the small dimensional subsystem is accelerated and the large dimensional subsystem is filtered, one can get a long-lived entanglement. Moreover, if the larger subsystem is accelerated, then by filtering any subsystem, the upper bounds of entanglement of the filtered state are larger then that for the initial states. For any accelerated subsystem, the entanglement always increases as the <b>filter</b> <b>strength</b> of the large dimensional subsystem increases...|$|E
40|$|This paper {{presents}} the constrained directional enhancement filter {{designed for the}} AV 1 royalty-free video codec. The in-loop filter {{is based on a}} non-linear low-pass filter and is designed for vectorization efficiency. It takes into account the direction of edges and patterns being filtered. The filter works by identifying the direction of each block and then adaptively filtering {{with a high degree of}} control over the <b>filter</b> <b>strength</b> along the direction and across it. The proposed enhancement filter is shown to improve the quality of the Alliance for Open Media (AOM) AV 1 and Thor video codecs in particular in low complexity configurations. Comment: 5 page...|$|E
40|$|International audienceA noncentered {{interpolation}} {{technique has}} been constructed to perform simulations using overlapping grids for complex geometries. High-order centered Lagrange polynomial interpolations and interpolations optimized in the Fourier space are first generalized to the noncentered case. These noncentered interpolations either generate significant dispersion errors or strongly amplify high-wavenumber components. Accordingly, a noncentered high-order wavenumber-based optimized interpolation method is developed {{with the addition}} of a nonlinear constraint for the control of the amplitude amplification induced by decentering. High-order piecewise polynomial regressions of the obtained interpolation coefficients are performed. The time stability of the method is investigated in the 1 -D case when the interpolation method is used in conjunction with explicit high-order differencing, filtering schemes, as well as a 6 -step Runge-Kutta time integration algorithm. A criterion is formulated to predict its stability as a function of the <b>filtering</b> <b>strength</b> and the Courant-Friedrichs-Lewy constant. Finally, 1 -D convection simulations are presented to illustrate the stability and the accuracy of the developed noncentered interpolations...|$|R
40|$|Abstract. A noncentered {{interpolation}} {{technique has}} been constructed to perform simulations using overlapping grids for complex geometries. High-order centered Lagrange polynomial interpolations and interpolations optimized in the Fourier space are first generalized to the noncentered case. These noncentered interpolations either generate significant dispersion errors or strongly amplify high-wavenumber components. Accordingly, a noncentered high-order wavenumber-based optimized interpolation method is developed {{with the addition}} of a nonlinear constraint for the control of the amplitude amplification induced by decentering. High-order piecewise polynomial regressions of the obtained interpolation coefficients are performed. The time stability of the method is investigated in the 1 -D case when the interpolation method is used in conjunction with explicit high-order differencing, filtering schemes, as well as a 6 -step Runge–Kutta time integration algorithm. A criterion is formulated to predict its stability as a function of the <b>filtering</b> <b>strength</b> and the Courant–Friedrichs– Lewy constant. Finally, 1 -D convection simulations are presented to illustrate the stability and the accuracy of the developed noncentered interpolations. Key words. space noncentered interpolations, decentering, high order, overlapping grids, Fourie...|$|R
40|$|Results {{obtained}} for a Taylor-Green vortex at a Reynolds number of 3000, using Large-Eddy Simulation (LES) based on Relaxation Filtering (RF), {{are presented in}} order to assess the quality of the RF-LES methodology. The RF is applied every time step to the velocity components, using a standard filters of orders k ≥ 4 at a fixed strength σ, to relax subgrid energy from scales at wave numbers close to the grid cut-off wave number. Various combinations of k and σ are consid-ered, for k ranging from 4 to 14 and σ from 0. 15 to 1. Error landscapes are obtained by comparing the 643 LES results, filtered in post-processing to an effective resolution of four points per wavelength, to 3843 Direct Numerical Simulation data, filtered at identical resolution. For filters of order k ≤ 6, the LES accuracy is found to be rather poor and varies sig-nificantly with the <b>filtering</b> <b>strength</b> σ. However, for higher order filters, i. e. for k> 6, the accuracy is good and nearly independent of the strength σ...|$|R
40|$|Abstract:- This paper {{presents}} a Gaussian noise filter {{that uses a}} sigmoid shaped membership function to model image information in the spatial domain. This function acts as a tunable smoothing intensification operator. With a proper choice of two sigmoid parameters ‘t ’ and ‘a’, the <b>filter</b> <b>strength</b> can be tuned for removal of Gaussian noise in intensity images. An image information measure, Total Compatibility is used to adaptively select these sigmoid parameters. A visible improvement in the smoothness of images is observed, and the output of filter is also {{compared with those of}} other standard smoothing methods. Key-words:- image, noise, parameter, fuzzy logic, Gaussian, filter...|$|E
40|$|This paper {{concerns}} a Wavelength-Swept Fiber Laser (WSFL) incorporating frequency shifted feedback and an intra-cavity passband filter, {{in which the}} wavelength of the modeless output is linearly, continuously and repeatedly tuned (in time) over a given range by modulation of the filter peak wavelength and <b>filter</b> <b>strength.</b> We show both numerically and experimentally that amplifier noise {{plays a key role}} in determining the operating modes of frequency-shifted fiber laser systems and that a noisy amplifier can be used to suppress the natural tendency of such lasers to pulse allowing for continuous wave, modeless operation. Furthermore, we show that significant narrowing of a WSFL instantaneous swept linewidth can be obtained if the filter peak transmission wavelength is resonantly swept so as to follow the wavelength shift per pass due to the acousto-optic frequency shift. Using these ideas we go on to demonstrate and characterize a high power, diode-driven, Er 3 +/Yb 3 + WSFL incorporating a bulk-optic Acousto-Optic Tunable Filter (AOTF). Linewidths as narrow as 9 GHz, sweep ranges up to 38 nm and output powers as high as 100 mW are obtained. Furthermore, we demonstrate the generation of user definable average spectral output by synchronous modulation of the <b>filter</b> <b>strength</b> and multi-wavelength pulsed output at higher sweep rates. Excellent agreement between the experimental results and those of the numerical modelling is obtained. Our simulations show that reduced linewidth (< 0. 02 nm) and improved scan linearity should be readily achievable with realistic system improvements. We believe such sources to be of considerable physical and practical interest with applications ranging from sensor array monitoring, device characterization through to low coherence measurement technology...|$|E
40|$|Abstract. In the {{considering}} problem usual {{tasks of}} designing ground dams (evaluation of <b>filter</b> <b>strength,</b> compressibility and ground permeability) are overlaid by new tasks, {{that have never}} touched upon earlier, that provide reliability of earth dams under extreme conditions and that are exposed by new additional forces such as intensive deformation and curvature of base. Here are dependences allowing determination of accepted values of earth surface deformation of ground dams on undermined territories. Using this methodology, you can compute required constructively technologic protection measures. Solution of such problems as predicting behavior and degree of reliability from external influence of a dam at given expected deformations,- can be brought into action. Prediction of conventional dams reliability may been carried out without underworking...|$|E
40|$|Fingerprint {{verification}} is {{a socially}} accepted biometric method for {{identification of a}} person. In this paper we propose Hybrid Fingerprint Matching using Block <b>Filter</b> and <b>Strength</b> Factor (HFMBFS). The minutiae and ridge based methods are combined to verify the fingerprint matching using strength factors Alpha (Î±) and Beta (Î²). For minutiae and ridge extraction Block Filter and Hough Transform are used respectively. It is observed that the matching percentage of two different fingerprints is improved compared to the existing algorithms. Â© 2010 IEEE...|$|R
50|$|The AeroPress is {{a device}} for brewing coffee. It {{was invented in}} 2005 by Aerobie {{president}} Alan Adler. Coffee is steeped for 10-50 seconds (depending on grind and preferred strength) and then forced through a filter by pressing the plunger through the tube. The filters used are either the AeroPress paper filters or disc shaped thin metal filters. The maker describes the result as an espresso strength concentration of coffee, but its most frequent use is more in the <b>filter</b> brew <b>strength.</b>|$|R
30|$|For further {{processing}} the <b>filtered</b> residual <b>strength</b> is rectified. The filtered/rectified residual strength (Fig. 1 (c)) is then {{compared with an}} event detection threshold (e.g., 0.2 nT): see the upper dashed line in Fig. 1 (c). Fluctuations exceeding the threshold are considered as an event. If two events are separated by less than a certain time interval (e.g., 60 seconds), they are merged to one event including the interval in-between. An event {{can be considered as}} an ‘EPB’ if it first satisfies the following morphological criteria (Stolle et al., 2006): fluctuations should not stand alone, but be accompanied by similar fluctuations in the surrounding (i.e. data points above the lower dashed line in Fig. 1 (c)) as well as by calm background (i.e. data points below the lower dashed line in Fig. 1 (c)). Our EPB detection approach goes one step beyond that of Stolle et al. (2006) by considering also the concurrent change in plasma density. In this final step <b>filtered</b> residual <b>strength</b> (Fig. 1 (b)) and plasma density filtered in the same way (Fig. 1 (e)) are correlated around the detected events (Fig. 1 (g)). If the square of the correlation coefficient (the ‘Bubble Probability’: see Subsection 3.3 for details) is higher than a certain threshold (e.g., 0.5), which confirms the diamagnetic effect, the event is deemed a ‘Confirmed Bubble’. In Fig. 1 (f) a ‘Confirmed Bubble’ appears with Bubble Index 1 with nonzero Bubble Probability: detailed descriptions of the Bubble Index and the Bubble Probability will be given in Section 3. If (1) the square of the correlation coefficient is lower than the threshold, (2) there is no plasma density data, or (3) the morphological criteria are not satisfied, the event remains an ‘Unconfirmed Bubble’. In Fig. 1 (f) an ‘Unconfirmed Bubble’ appears with Bubble Index 1 and Bubble Probability 0. If data points are obtained outside the night-side low-latitude region, or if the data quality of the <b>filtered</b> residual <b>strength</b> is poor (see Subsection 3.2 for details), the corresponding data are considered as ‘Unanalyzable’: Bubble Index − 1.|$|R
40|$|This {{assessment}} of High Efficiency Particulate Air (HEPA) filter vulnerability was {{requested by the}} USDOE Office of River Protection (ORP) to satisfy a DOE-HQ directive to evaluate the effect of filter degradation on the facility authorization basis assumptions. Within {{the scope of this}} assessment are ventilation system HEPA filters that are classified as Safety-Class (SC) or Safety-Significant (SS) components that perform an accident mitigation function. The objective of the assessment is to verify whether HEPA filters that perform a safety function during an accident are likely to perform as intended to limit release of hazardous or radioactive materials, considering factors that could degrade the filters. Filter degradation factors considered include aging, wetting of filters, exposure to high temperature, exposure to corrosive or reactive chemicals, and exposure to radiation. Screening and evaluation criteria were developed by a site-wide group of HVAC engineers and HEPA filter experts from published empirical data. For River Protection Project (RPP) filters, the only degradation factor that exceeded the screening threshold was for filter aging. Subsequent evaluation of the effect of filter aging on the <b>filter</b> <b>strength</b> was conducted, and the results were compared with required performance to meet the conditions assumed in the RPP Authorization Basis (AB). It was found that the reduction in <b>filter</b> <b>strength</b> due to aging does not affect the filter performance requirements as specified in the AB. A portion of the HEPA filter vulnerability assessment is being conducted by the ORP and {{is not part of the}} scope of this study. The ORP is conducting an {{assessment of}} the existing policies and programs relating to maintenance, testing, and change-out of HEPA filters used for SC/SS service. This document presents the results of a HEPA filter vulnerability assessment conducted for the River protection project as requested by the DOE Office of River Protection...|$|E
30|$|H. 264 /AVC in-loop filter {{performs}} simple {{operations to}} detect and analyze artifacts on coded blocking boundaries and attenuates those by applying a selected filter [23]. The H. 264 /AVC loop filter is an adaptive filter. The amount of filtering performed each position of the block edge depends on some factors. The filter {{can be divided into}} three levels: slice level, block level, and pixel level. The <b>filter</b> <b>strength,</b> i.e., the amount of filtering is computed based on the help of parameter boundary strength (BS), depending on the current quantizer, the macroblock type, the motion vector, gradient of the image samples across the blocking boundary, and other parameters. However, the filtering algorithm is highly complex and is account for 33 % of the total decode time {{in the study of the}} baseline profile decoder [24].|$|E
40|$|A {{recently}} introduced framework of semidiscretisations for hyperbolic conservation laws known as correction procedure via reconstruction (CPR, {{also known as}} flux reconstruction) is considered in the extended setting of summation-by-parts (SBP) operators using simultaneous approximation terms (SATs). This reformulation can yield stable semidiscretisations for linear advection and Burgers' equation as model problems. In order to enhance these properties, modal filters are introduced to this framework. As a second part of a series, the results of Ranocha, Glaubitz, Öffner, and Sonar ("Enhancing stability of correction procedure via reconstruction using summation-by-parts operators I: Artificial dissipation", 2016) concerning artificial dissipation / spectral viscosity are extended, yielding fully discrete stable schemes. Additionally, a new adaptive strategy to compute the <b>filter</b> <b>strength</b> is introduced and different possible applications of modal filters are compared both theoretically and numerically. Comment: 16 pages, 6 figures, submitte...|$|E
40|$|Abstract—Edge {{is a type}} of {{valuable}} clues for scene character detection task. Generally, the existing edge-based methods rely on the assumption of straight text line to prune away the non-character candidates. This paper proposes a new edge-based method, called edge-ray filter, to detect the scene character. The main contribution of the proposed method lies in filtering out complex backgrounds by fully utilizing the essential spatial layout of edges instead of the assumption of straight text line. Edges are extracted by a combination of Canny and Edge Preserv-ing Smoothing Filter (EPSF). To effectively boost the <b>filtering</b> <b>strength</b> of the designed edge-ray filter, we employ a new Edge Quasi-Connectivity Analysis (EQCA) to unify complex edges as well as contour of broken character. Label Histogram Analy-sis (LHA) then filters out non-character edges and redundant rays through setting proper thresholds. Finally, two frequently-used heuristic rules, namely aspect ratio and occupation, are exploited to wipe off distinct false alarms. In addition {{to have the ability to}} handle special scenarios, the proposed method can accommodate dark-on-bright and bright-on-dark characters simultaneously, and provides accurate character segmentation masks. We perform experiments on the benchmark ICDAR 2011 Robust Reading Competition dataset as well as scene images with special scenarios. The experimental results demonstrate the validity of our proposal. I...|$|R
40|$|Overlap fermions have {{an exact}} chiral {{symmetry}} on the lattice {{and are thus}} an appropriate tool for investigating the chiral and topological structure of the QCD vacuum. We study various chiral and topological aspects of quenched gauge field configurations. This includes the localization and chiral properties of the eigenmodes, the local structure of the ultraviolet <b>filtered</b> field <b>strength</b> tensor, {{as well as the}} structure of topological charge fluctuations. We conclude that the vacuum has a multifractal structure. Comment: 68 pages, 31 figures, file size: 1. 7 MB (PDF...|$|R
40|$|Abstract. String {{matching}} is a {{very important}} problem in computer science. The problem consists in finding all the occurrences of a pattern P of length m in a text T of length n. We describe a special device which can do string matching by performing n − m + 1 text-to-pattern comparisons. The proposed device uses light and optical filters for performing computations. Two physical implementations are proposed. One of them uses colored glass and the other one uses polarizing <b>filters.</b> The <b>strengths</b> and the weaknesses of each method are deeply discussed. ...|$|R
40|$|This article {{deals with}} the design of earth dams (estimating <b>filter</b> <b>strength,</b> {{compressibility}} and permeability of soils, studing of the stress-strain state, etc.) The authors look through a new, not previously discussed task of reliability of groundwater dams in extreme conditions like an influence of new and additional loads {{in the form of}} intense deformation of elongation and the base curvature. The dependences that allow determining the permissible values of deformations of ground dams earth surface on undermined territories are presented. The proposed method allows calculating the required parameters of structural and technological protection activities. In addition, the solution of such problems as predicting the behavior of the dam and its degree of reliability for a given expected deformation from external influence is found. The reliability forecasting of conventional dams, without undermining, can also be made...|$|E
40|$|Abstract — A new {{perceptually}} adaptive joint deringing-deblocking filtering {{method for}} wavelet based {{scalable video coding}} is proposed. A structure based on in-loop filtering is designed for the decoding of scalable coded video considering the update step in motion compensated temporal filtering. Since ringing and blocking artifacts are visually disturbing, {{the characteristics of the}} human visual system (HVS) are considered in the underlying Bilateral filtering. In the proposed approach, the <b>filter</b> <b>strength</b> is adjusted according to the perceptual distortion by integrating the HVS models of luminance masking, activity masking and temporal masking. Consequently, different filtering strength is applied to areas with different perceptual sensitivity in terms of the HVS. A comparative evaluation using conventional loop filtering structure and Bilateral filter was conducted. The experimental assessment demonstrates a superior performance of the proposed adaptive filtering, providing substantial and consistent better objective and subjective quality. Keywords- scalable video coding; deringing; deblocking; bilateral filter; human visual system I...|$|E
40|$|We {{adapt the}} concept of {{spectral}} vanishing viscosity to a discontinuous Galerkin method solving hyperbolic conservation laws on triangular grids. In this context, modal filtering applied to the elementwise Dubiner expansion of the numerical solution is newly related to a super viscosity formulation based on the associated Sturm-Liouville operator. This connection allows to specify conditions on the <b>filter</b> <b>strength</b> with respect to time step choice and mesh refinement. The small amount of artificial dissipation introduced to the scheme stabilizes the approximation, but weaker oscillations are still present. The digital total variation filter, {{which has not been}} applied to DG methods before, reveals to be capable of removing the remaining oscillations in a postprocessing step. Numerical experiments carried out for a two-dimensional linear advection equation confirm the designed order of accuracy for polynomial degrees up to 8. The influence of global and adaptive spectral filtering is investigated as well. In addition, the spectral and DTV filtering techniques are tested for a nonlinear conservation law with discontinuous solution. ...|$|E
40|$|Most of {{the digital}} cameras use color filter arrays instead of beam {{splitters}} to capture image data so as {{to reduce the cost}} and to gain more efficiency. As a result of this, only one of the required three color samples becomes available at each pixel location and the other two needs to be interpolated. This procedure is called Color Filter Array (CFA) interpolation or demosaicing. So as to improve subjective and objective interpolation quality many demosaicing algorithms have been introduced. We propose an orientation-free edge <b>strength</b> <b>filter</b> and apply it to the demosaicing problem. Output of edge <b>strength</b> <b>filter</b> is utilized both to improve the initial green channel interpolation and to apply the constant color difference rule adaptively. This simple edge method yields visually pleasing results with high CPSNR...|$|R
40|$|Basically we use beam {{splitters}} {{to capture}} data or image. Using beam splitters is very expensive, so inorder to overcome this we go for an alternative technique called color filters arrays. As a result of this, {{only one of the}} required three color samples becomes available at each pixel location and the other two need to be interpolated. This process is called Color Filter Array (CFA) interpolation or demosaicing. Many demosaicing algorithms have been introduced over the years to improve subjective and objective interpolation quality. Wepropose an orientation-free edge <b>strength</b> <b>filter</b> and apply it to the demosaicing problem. Edge <b>strength</b> <b>filter</b> output is utilized both to improve the initial green channel interpolation and to apply the constant color difference rule adaptively. This simple edge directed method yields visually pleasing results with high CPSNR...|$|R
40|$|Introduction: Filtration media {{selection}} {{is dependent upon}} filtration level required and operational pressures. Traditionally, the filter portion is formed from woven wire mesh manufactured from drawn wire, arranged in a grid pattern although different weaves are available, allowing different levels of filtration using different wire diameters. The open area of the filter {{is limited by the}} wire diameters, affecting the overall strength of the woven mesh. The design freedoms of additive manufacturing (AM) allows filter media design with known aperture sizes whilst reducing equivalent wire diameter to produce filter media with a greater open area compared to the equivalent woven wire mesh. Method: AM filter media were designed to have aperture sizes of 500 µm and 1000 µm as found in # 32 and # 18 mesh respectively, but with a decrease in ‘wire’ diameter to increase the ratio of open area. Strength was increased through addition of further filtration layers to form depth filters. The filter media were then tested for aperture size, collapse pressure and pressure drop. Results: AM filter media designs delivered an increase in open area compared to comparable woven wire mesh media. Addition of further layers increased the <b>filters</b> <b>strength</b> and the complexity of the particulate path through the filter and so decreased the path size. Conclusions: Filter media designs produced using AM can be manipulated to produce an increase in open area, retaining strength compared to conventional woven wire mesh. The increased open area decreases the pressure drop across the filter and so less pumping energy is required...|$|R
