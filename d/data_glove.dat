210|137|Public
50|$|In 1982 Thomas G. Zimmerman filed {{a patent}} (US Patent 4542291) on an optical flex sensor mounted in a glove to measure finger bending. Zimmerman worked with Jaron Lanier to {{incorporate}} ultrasonic and magnetic hand position tracking technology {{to create the}} Power Glove and <b>Data</b> <b>Glove,</b> respectively (US Patent 4988981, filed 1989). The optical flex sensor used in the <b>Data</b> <b>Glove</b> was invented by Young L. Harvill who scratched the fiber near the finger joint to make it locally sensitive to bending.|$|E
50|$|Another {{method of}} adding hand and finger motion {{is to have}} the {{animator}} or actor wear a <b>data</b> <b>glove</b> or wired glove. These devices very accurately measure and record the position and bend of each finger, thumb, and even the hand. Measurand's ShapeHand is made of flexible ribbons and includes 40 Fiber Optic bend/twist sensors.|$|E
50|$|This device {{originally}} {{started as}} an input system for computers. It was later used for virtual reality systems. Thomas Zimmerman invented {{the prototype of}} the <b>Data</b> <b>Glove</b> and began looking for other people to help work on it. The device was using 6502 microcontrollers. Zimmerman met Mitch Altman {{and asked him to}} join VPL part-time because Altman knew how to program the microcontrollers.|$|E
30|$|Moreover, the learners’ {{engagement}} can {{be strengthened}} through low-cost peripheral {{devices such as}} headphones, smart glasses and <b>data</b> <b>gloves.</b>|$|R
3000|$|... more image {{processing}} on the tracker camera is useful, for example, to segment the user's hand and fingers making unhandy <b>data</b> <b>gloves</b> superfluous.|$|R
30|$|More image {{processing}} on the tracker camera is useful, for example, to segment the user's hand and fingers making unhandy <b>data</b> <b>gloves</b> superfluous. Segmenting walking people enables virtual objects to encircle them.|$|R
50|$|Following the P5 Glove is 5th Glove. A <b>data</b> <b>glove</b> and flexor strip kit (5th Glove DFK) sold by Fifth Dimension Technologies. The package uses {{flexible}} optical-bending sensing {{to track}} hand and arm movement. The glove {{can be used}} with 5DT's ultrasonic tracking system, the 5DT Head and 5DT Hand tracker, which can track movement from up to two metres away from the unit's transmitter.|$|E
50|$|Introduced in {{the late}} 1980s, the Data Suit by VPL Research {{was one of the}} {{earliest}} mo-cap suits in the market. Sensors stitched in the Data Suit were connected by fiber-optic cables to computers that updated the visuals 15 to 30 times a second. The Data Suit was ahead of its time, selling for up to $500,000 for a complete system (along with the EyePhone and the <b>Data</b> <b>Glove).</b>|$|E
50|$|In {{addition}} to the CyberGlove, Immersion Corp also developed three other <b>data</b> <b>glove</b> products: the CyberTouch, which vibrates each individual finger of the glove when a finger touches an object in virtual reality; the CyberGrasp which actually simulates squeezing and touching of solid as well as spongy objects; and the CyberForce device which does {{all of the above}} and also measures the precise motion of the user's entire arm.|$|E
40|$|The {{thesis is}} focused on the {{description}} of the basic principles of the virtual reality, on utilization of <b>data</b> <b>gloves</b> in the virtual reality and it implements the application with usage of the input device P 5 glove. The implementation contains among the others various visual techniques of OpenGL and also shows up-to-date work with the popular physics engine Bullet Physics. The theoretical bases of these techniques are analysed in the thesis. So the output of the thesis will serve as a source of information to people interested in the knowledge of the <b>data</b> <b>gloves</b> problems and other similar peripherals, but also to interested people who {{want to learn more about}} the realization of the computer graphics and creation of physicist models. ...|$|R
40|$|We {{present a}} {{simulated}} underwater {{world to be}} experienced in a cave-like virtual environment [Cruz-Neira 1992]. The user, wearing stereo glasses and <b>data</b> <b>gloves,</b> moves among three-dimensionally modeled plants and schools of fish in the virtual water with hand gestures that simulate swimming. In addition to this physically base...|$|R
40|$|Virtual Reality (VR) {{is usually}} {{described}} by the media as a particular collection of technological hardware: a computer capable of 3 D real-time animation, a head-mounted display, <b>data</b> <b>gloves</b> equipped {{with one or more}} position trackers. However, this focus on technology is disappointing for communication researchers and VR designers...|$|R
5000|$|... "Videoplace" [...] {{has been}} {{exhibited}} widely in both {{art and science}} contexts in the United States and Canada, {{and it was also}} shown in Japan. It was included in the SIGGRAPH Art Show in 1985 and 1990. [...] "Videoplace" [...] was also the featured exhibit at SIGCHI (Computer-Human Interaction Conference) in 1985 and 1989, and at the 1990 Ars Electronica Festival. Instead of taking the virtual reality track of head-mounted display and <b>data</b> <b>glove</b> (which would come later in the 1980s), he investigated projections onto walls.|$|E
50|$|In 1977, with Tom DeFanti and Rich Sayre, he {{designed}} the Sayre Glove, the first <b>data</b> <b>glove,</b> {{as part of}} a grant from the National Endowment for the Arts. This device used light based sensors with flexible tubes with a light source at one end and a photocell at the other. As the fingers were bent, the amount of light that hit the photocells varied, thus providing a measure of finger flexion. It was mainly used to manipulate sliders, but was lightweight and inexpensive.|$|E
5000|$|In California, Lanier {{worked for}} Atari, {{where he met}} Thomas Zimmerman, {{inventor}} of the <b>data</b> <b>glove.</b> After Atari Inc. was split into two companies in 1984, Lanier became unemployed. The free time enabled him to concentrate on his own projects, including VPL, a [...] "post-symbolic" [...] visual programming language. Along with Zimmerman, Lanier founded VPL Research, focusing on commercializing virtual reality technologies; the company prospered for a while, but filed for bankruptcy in 1990. In 1999, Sun Microsystems bought VPL's virtual reality and graphics-related patents.|$|E
40|$|Abstract. Context {{awareness}} {{plays an}} essential role in implementing the Ambient Intelligence vision (AmI). Instrumentalized <b>data</b> <b>gloves</b> are used for task tracking based on the information provided by sensors embedded into them. This article presents some preliminary results of the use of Genetic Algorthms for sensor selection in person independent task recognition system...|$|R
40|$|For {{computer}} games, communications using avatars, and real-time animation systems, users {{want to move}} {{a character}} freely in a virtual world. However, {{the flexibility of the}} current motion control interface is very limited because currently character motion is simply controlled with pre-defined motion data. In this paper, we present a motion control method that uses two <b>data</b> <b>gloves</b> as an input device, making a virtual character perform various motions. Each part of the character’s body is controlled using input from <b>data</b> <b>gloves.</b> For example, a user can control the character’s left arm and left leg using their left hand. However, there are limited degrees of freedom using <b>data</b> <b>gloves</b> to control all the character’s body parts directly. Moreover, it is difficult for users to perform complex motions such as stepping or jumping because multiple body parts have to be controlled at the same time. In order to solve these problems, we introduce three novel ideas. First, we change the mapping between the user’s hand and the character’s body parts dynamically. For example, when the both hands are moving in the same direction, they are used to control the pelvis instead of arms or legs. Then, we introduce a manual switch between arms and legs. A hand is used for controlling the arms or the legs by switching the mode manually with a finger. Second, we use mechanisms of real puppets to control multiple bod...|$|R
50|$|Virtual {{art is a}} {{term for}} the {{virtualization}} of art, made with the technical media developed {{at the end of}} the 1980s (or a bit before, in some cases). These include human-machine interfaces such as visualization casks, stereoscopic spectacles and screens, digital painting and sculpture, generators of three-dimensional sound, <b>data</b> <b>gloves,</b> <b>data</b> clothes, position sensors, tactile and power feed-back systems, etc. As virtual art covers such a wide array of mediums it is a catch-all term for specific focuses within it. Much contemporary art has become, in Frank Popper's terms, virtualized.|$|R
5000|$|Young Harvill - In {{his spare}} time, Harvill created {{a program called}} Swivel 3D which was used for {{creating}} computer art. It gave the users the ability to generate virtual worlds on a Macintosh computer. He licensed that software to VPL. Shortly after, Harvill joined VPL as their fourth employee in 1985. During his time there, he worked on a project called [...] "Reality Built for Two (RB2)" [...] {{which was the first}} VR system at that time. He also helped with the <b>Data</b> <b>Glove</b> as well.|$|E
5000|$|Also notable {{among the}} earlier {{hypermedia}} and virtual reality systems was the Aspen Movie Map, {{which was created}} at MIT in 1978. The program was a crude virtual simulation of Aspen, Colorado in which users could wander the streets {{in one of the}} three modes: summer, winter, and polygons. The first two were based on photographs—the researchers actually photographed every possible movement through the city's street grid in both seasons—and the third was a basic 3-D model of the city. Atari founded a research lab for virtual reality in 1982, but the lab was closed after two years due to Atari Shock (North American video game crash of 1983). However, its hired employees, such as Tom Zimmerman, Scott Fisher, Jaron Lanier and Brenda Laurel, kept their research and development on VR-related technologies. By the 1980s the term [...] "virtual reality" [...] was popularized by Jaron Lanier, one of the modern pioneers of the field. Lanier had founded the company VPL Research in 1985. VPL Research has developed several VR devices like the <b>Data</b> <b>Glove,</b> the Eye Phone, and the Audio Sphere. VPL licensed the <b>Data</b> <b>Glove</b> technology to Mattel, which used it to make an accessory known as the Power Glove. While the Power Glove was hard to use and not popular, at US$75, it was an early affordable VR device.|$|E
50|$|Concerned {{about the}} high cost of the most {{complete}} commercial solutions, Pamplona et al. propose a new input device: an image-based <b>data</b> <b>glove</b> (IBDG). By attaching a camera to the hand of the user and a visual marker to each finger tip, they use computer vision techniques to estimate the relative position of the finger tips. Once they have information about the tips, they apply inverse kinematics techniques in order to estimate the position of each finger joint and recreate the movements of the fingers of the user in a virtual world. Adding a motion tracker device, one can also map pitch, yaw, roll and XYZ-translations of the hand of the user, (almost) recreating all the gesture and posture performed by the hand of the user in a low cost device.|$|E
40|$|Today’s Virtual Reality {{applications}} {{require a}} high degree of immersion. This may be achieved by using a CAVE incorporating powerful 3 D computer graphics workstations, panoramic multiple screen displays and projection walls, stereovision and interaction devices such as trackers and <b>data</b> <b>gloves.</b> This article describes Virtual Reality applications from the fields of scientific visualization in medicine and education embedded within the Blue-C environment, a state-of-the-art CAVE infrastructure. 1...|$|R
30|$|Augmented reality technology, which {{typically}} layers virtual {{information on a}} real scene, utilizes different hardware (personal computers (PC), laptops, head mounted displays (HMD), GPS, <b>data</b> <b>gloves,</b> smart boards, etc.) and software (AutoCAD, Photoshop, AC 3 D, 3 D Studio, building information model (BIM), etc.). From a technology perspective articles are classified into three categories: (1) user experience: (a) immersive or (b) non-immersive, i.e., desktop-based; (2) device: (a) mobile, (b) stationary or non-mobile; (3) delivery: (a) web-based, (b) standalone.|$|R
50|$|Popper {{uses the}} term, virtual art, in {{reference}} to all the art made with the technical media developed {{at the end of}} the 1980s (or a bit before, in some cases). These include human-machine interfaces such as visualization casks, stereoscopic spectacles and screens, generators of three-dimensional sound, <b>data</b> <b>gloves,</b> <b>data</b> clothes, position sensors, tactile and power feed-back systems, etc. All these technologies allowed immersion into the image and interaction with it. The impression of reality felt under these conditions was not only provided by vision and hearing, but also by the other bodily senses. This multiple sensing was so intensely experienced at times, that Popper could speak of it as an immersive virtual reality (VR).|$|R
5000|$|In {{the summer}} of 1993, Richard was asked to perform in The [...] "Tomorrows Realities Gallery" [...] at the annual SIGGRAPH {{convention}} held in Anaheim, CA. as “Dynamation Man” giving voice and movement to a live, animated character. This event {{turned out to be}} the first live, operatic performance in digital real-time computer generated animation using a full upper-body Waldo and <b>data</b> <b>glove</b> with a single performing vocal artist interfaced with a character in total control. This project led Richard to perform and direct [...] "A Musical Performance Animation" [...] with composer Steven Bowen, an accomplished musician and arranger for the creation of a seven-minute singing musical story giving life to three computer generated characters; Pirate King, Reggae Man and the Sunny Boy, featured on CNN and in Computer Graphics World Magazine.|$|E
5000|$|In 1973, {{he joined}} {{the faculty of the}} University of Illinois at Chicago. With Dan Sandin, he founded the Circle Graphics Habitat, now known as the Electronic Visualization Laboratory (EVL). At UIC, DeFanti further {{developed}} the GRASS language, and later created an improved version, ZGRASS, implemented on the low-cost Datamax UV-1. [...] The GRASS and ZGRASS languages have been used by a number of computer artists, including Larry Cuba, in his film 3/78 and the animated Death Star sequence for Star Wars. [...] Later significant work done at EVL includes development of the graphics system for the Bally home computer, invention of the first <b>data</b> <b>glove,</b> co-editing the 1987 NSF-sponsored report Visualization in Scientific Computing that outlined the emerging discipline of scientific visualization, invention of PHSColograms, and invention of the CAVE Automatic Virtual Environment. [...] DeFanti's current work includes heading the TransLight/StarLight international multi-gigabit networking project and co-directing the OptIPuter optical networking and visualization project.|$|E
5000|$|The {{history of}} {{automatic}} sign language translation {{started with the}} development of hardware such as finger-spelling robotic hands. In 1977, a finger-spelling hand project, Ralph created a robotic hand that can translate alphabets into finger-spellings. Later, the use of gloves with motion sensors became the mainstream, and some projects such as the CyberGlove and VPL <b>Data</b> <b>Glove</b> were born. The wearable hardware made it possible to capture the signers’ hand shapes and movements {{with the help of the}} computer software. However, the cameras {{with the development of}} computer vision replaced those wearable devices due to the efficiency and less physical restrictions on signers. To process the data collected through the devices, researchers implemented neural networks such as the Stuttgart Neural Network Simulator for the pattern recognition in their projects such as the CyberGlove. Researchers also use many other approaches for sign recognition. For example, Hidden Markov Models is used to analyze the data statistically,and the GRASP and other machine learning programs use the training sets to improve the accuracy of sign recognition.|$|E
40|$|This paper {{presents}} a novel interaction paradigm to support musical performance using spatial audio. This method reduces the interface bottleneck between artistic intent and spatial sound rendering and allows dynamic positioning of sounds in space. The system supports collaborative performance, allowing multiple artists to simultaneously control the audio spatialization. The interface prototype is built upon standard virtual reality software and user interface technology. Tracked <b>data</b> <b>gloves</b> {{are used to}} manipulate audio objects and stereoscopic projection to display the virtual 3 D sound stage...|$|R
40|$|VizClass is a {{university}} classroom environment designed to offer students in computer graphics and engineering courses up-to-date visualization technologies. Three digital whiteboards and a three-dimensional stereoscopic display provide complementary display surfaces. Input devices include touchscreens on the digital whiteboards, remote keyboards, <b>data</b> <b>gloves,</b> and hand-position sensors. We use observations, interviews, and surveys {{to examine the}} pedagogical impacts of VizClass for teaching and learning computer graphics and virtual reality. Preliminary findings include positive student and teacher attitudes and greater learner engagement in after-class collaborations...|$|R
50|$|Trackers detect or monitor head, hand or body {{movements}} and send {{that information to}} the computer. The computer then translates it and ensures that position and orientation are reflected accurately in the virtual world. Tracking is important in presenting the correct viewpoint, coordinating the spatial and sound information presented to users as well the tasks or functions that they could perform. 3D trackers {{have been identified as}} mechanical, magnetic, ultrasonic, optical, and hybrid inertial. Examples of trackers include motion trackers, eye trackers, and <b>data</b> <b>gloves.</b>|$|R
5000|$|Franziska Baumann (born April 9, 1965) is a Swiss {{musician}} and composer, specializing in improvisation and composed music (vocals, flute, live electronics). Baumann {{studied at the}} Winterthur Conservatory, majoring in flute with a minor in singing. Following her conservatory studies, she completed improvisation classes with Fred Frith, Barre Phillips, and George Lewis. She also studied with vocal artists such as Phil Campanella, Lauren Newton, and Joan La Barbara. As a vocalist she makes use of extended and microtonal, with clicking and percussive sounds, tone changes and language-related techniques. Baumann performs solo and with musicians including Pierre Favre, Joëlle Léandre, Lê Quan Ninh, Jacques Demierre, Peter Schaerli, and Matthias Ziegler. She {{is also part of}} the improvisation trio, Potage du Jour, alongside Jürg Solothurnmann and Christoph Baumann. Her repertoire as a composer ranges from improvised works and electro-acoustic compositions to sound installations and large scale surround sound projects. [...] As Artist in Residence at the Amsterdam Centre For Electro Instrumental Music (STEIM) Baumann programmed a <b>data</b> <b>glove</b> so that she could trigger voice and sound articulations in real time via gesture. Since 2006 Baumann has served as a lecturer in vocal performance and improvisation at the Hochschule der Künste in Bern, where she is also involved in research projects on topics such as sound without body and Gesture performance. Franziska Baumann lives and works in Bern, Switzerland.|$|E
5000|$|Between 1989-1992, {{she created}} Angels, the first {{immersive}} movie. The project foundation {{was laid out}} at the Visual Arts Program at MIT, employing Wavefront's Advanced Visualizer on a Silicon Graphics personal IRIS. The VR work was done at the Hitlab (University of Washington) using VPL's Virtualization interface and its Body Electric software running on the IRIS. Angels is a real-time interactive immersive movie, a kind of travel in a virtual paradise. The participant uses a VPL Dataglove and high-resolution HRX goggles developed by Jaron Lanier. Following Tom Furness' theory, the artwork was developed for the three senses: vision, audio and touch, though the technological restraints at the time could only implement vision, audio, and a non tactile <b>data</b> <b>glove.</b> Each user starts his/her experience {{in front of an}} odd carousel that is a passage to more VR worlds. Touching one of the three angels’ hearts in the carousel, defines the range in which the following three segments will appear. The duration of the sections varies from just about 30" [...] to 2'30". The brilliantly colourful environments are a gateway to more scenes. The angels' voices ask the users to interact with them, causing a story to open. The music was composed by Diane Thome.While at MIT, Stenger also contributed to the seminal Cyberspace First Steps edited by Michael L. Benedikt with the now famous essay [...] "Mind is a Leaking Rainbow".|$|E
50|$|Gabriel explores human reality {{while her}} work {{includes}} robotics, some VR work, installations, performative formats and painting. There {{is something called}} Perceptual Arena, {{and it is an}} interactive virtual reality installation designed by Ulrike Gabriel and otherspace. This work has been shown at V2 Exhibition and also the Netherlands and Serious Chiller Lounge in Munich. Perceptual Arena is a realtime virtual environment that explores what the mind can do. This creates an audio-visual space texture that has the interaction of the user as an important attribute. The interaction is simply to be in the space, to distinguish it and to move around and to grab on to the resulting virtual clay. This defines the space and is what’s used for the further interaction. The space matches the individual perception. The perceivable evolves through perception of it. To be involved and possibly mess up in this process can transform the world, but can also push it out of balance and therefore destroy. The use of polygons and sounds is created and always changing by the personal views. The viewer {{is the only thing that}} can manipulate what is happening. The complexity of the arena world results out of the total interrelation of all its factors which in the end all depend on the users input data. These are some things the viewers might encounter. The view onto the space, their movement in the space, a history of movement and view, a virtual sensor in the field of view which applies history onto the space, the total access with a <b>data</b> <b>glove</b> onto the space through the field of view, and the virtual clay which is object and result of all this factors.|$|E
30|$|In the paper, we have {{presented}} two different <b>data</b> <b>gloves</b> {{based on a}} same soft bending sensor. The sensor {{is a combination of}} electrical components and mechanical design. The sensor has excellent sensitivity as well as repeatability. Compared with existing bending sensors, the soft bending sensor is also flexible and can be stretched. The unrecoverable elongation caused by stretch is avoided by a novel structure. The size of the sensor can also be customized. The fabric data-collecting glove solves the problem of sensor slipping, {{and the size of the}} glove can be fully customized. The soft rubber data-collecting glove, in which have totally get rid of traditional design methods, provides a new way for making <b>data</b> <b>gloves.</b> The soft glove acts like a layer of personal customized skin on the back of the hand, and bending motion can be measured directly. The modular design of the glove also simplifies the design and manufacturing procedures. Besides, both the sensor and the two kinds of glove have low cost. The production cost of one such bending sensor is less than 2 dollars, and the costs of two kinds of gloves are less than 30 dollars. Such sensing technology plays a huge role in promoting data-collecting devices from science laboratory into clinical use.|$|R
40|$|Bending sensors enable compact, {{wearable}} designs {{when used}} for measuring hand configurations in <b>data</b> <b>gloves.</b> While existing <b>data</b> <b>gloves</b> can accurately measure angular {{displacement of the}} finger and distal thumb joints, accurate measurement of thumb carpometacarpal (CMC) joint movements remains challenging due to crosstalk between the multi-sensor outputs required to measure the degrees of freedom (DOF). To properly measure CMC-joint configurations, sensor locations that minimize sensor crosstalk must be identified. This paper presents a novel approach to identifying optimal sensor locations. Three-dimensional hand surface data from ten subjects was collected in multiple thumb postures with varied CMC-joint flexion and abduction angles. For each posture, scanned CMC-joint contours were used to estimate CMC-joint flexion and abduction angles by varying the positions and orientations of two bending sensors. Optimal sensor locations were estimated by the least squares method, which minimized {{the difference between the}} true CMC-joint angles and the joint angle estimates. Finally, the resultant optimal sensor locations were experimentally validated. Placing sensors at the optimal locations, CMC-joint angle measurement accuracies improved (flexion, 2. 8 ° ± 1. 9 °; abduction, 1. 9 ° ± 1. 2 °). The proposed method for improving the accuracy of the sensing system can be extended to other types of soft wearable measurement devices...|$|R
30|$|Most of the {{existing}} bending sensors are non-stretchable, and they will slip when fingers bend. So traditional <b>data</b> <b>gloves</b> {{have to be made}} larger than the hand size to reserve some space for the sensors to slip. The gap between the glove and hand may cause some significant error. For the fabric data-collecting glove, the glove size can be fully customized due to the stretch ability of the EPR-based bending sensor. Each sensor can be fixed on the fabric glove without slipping so the error caused by sensor slipping can be minimized.|$|R
