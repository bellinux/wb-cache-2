10|10000|Public
5000|$|<b>Error</b> <b>of</b> <b>a</b> <b>measurement</b> is {{an inverse}} measure of {{accurate}} measurement i.e. smaller the error greater {{the accuracy of}} the measurement. Errors are expressed relatively as: ...|$|E
5000|$|In metrology, physics, and engineering, the {{uncertainty}} or margin of <b>error</b> <b>of</b> <b>a</b> <b>measurement,</b> when explicitly stated, {{is given by}} a range of values likely to enclose the true value. This may be denoted by error bars on a graph, or by the following notations: ...|$|E
5000|$|... if {{at least}} one of the values does not equal zero. This {{approach}} is especially useful when comparing floating point values in programming languages for equality with a certain tolerance. Another application is in the computation of approximation errors when the relative <b>error</b> <b>of</b> <b>a</b> <b>measurement</b> is required.|$|E
40|$|This paper {{describes}} a setup {{with a low}} sensitivity to temperature variations for determining the random <b>measurement</b> <b>errors</b> <b>of</b> <b>a</b> <b>measurement</b> system applying <b>a</b> moving scale. This moving-scale system is developed for advanced equipment such as ultra-precision machine tools and should operate with <b>a</b> <b>measurement</b> uncertainty <b>of</b> 15 nm for <b>a</b> <b>measurement</b> length <b>of</b> 109 mm and temperature variations of 1 °C. Temperature drift is identified as the most contributing source <b>of</b> <b>errors</b> and therefore should be accurately determined. A dedicated setup has been designed for this task. status: publishe...|$|R
5000|$|When {{the results}} <b>of</b> <b>a</b> series <b>of</b> <b>measurements</b> are {{described}} by a normal distribution with standard deviation [...] and expected value 0, then [...] is {{the probability that the}} <b>error</b> <b>of</b> <b>a</b> single <b>measurement</b> lies between &minus;a and +a, for positive a. This is useful, for example, in determining the bit <b>error</b> rate <b>of</b> <b>a</b> digital communication system.|$|R
40|$|The Monte Carlo models ARIADNE, HERWIG and LEPTO are {{compared}} to deep-inelastic scattering data {{measured at the}} ep-collider HERA. 1 Introduction Monte Carlo generators are an essential tool in modern day experimental High Energy Physics. They play a crucial r^ole {{in the analysis of}} the data, often in assessing the systematic <b>errors</b> <b>of</b> <b>a</b> <b>measurement.</b> For that reason it is of great importance that the Monte Carlo programs give results that agree closely with the experimental data. This paper aims to describe the agreement, deciencies and tuning of the Monte Carlo models with the neutral current deep inelastic scattering (DIS) data at HERA. Extensive use is made of the utility package HzTool [1], which is a FORTRAN library containing <b>a</b> collection <b>of</b> experimental results from the H 1 and ZEUS collaborations. The work described here is part <b>of</b> <b>an</b> ongoing program. During the workshop a forum was established between the H 1 and ZEUS collaborations for a joint coordinated investigation of th [...] ...|$|R
50|$|A {{number of}} other studies have {{demonstrated}} {{the reliability and validity}} of STAR Reading, STAR Math, and STAR Early Literacy. Additionally, a number of studies have demonstrated the correlation between STAR assessments and other tests of similar skills. As with any assessment or test, however, student scores will vary each time the assessment or test is administered. This concept is called the statistical <b>error</b> <b>of</b> <b>a</b> <b>measurement.</b>|$|E
40|$|The {{uncertainty}} principle is often {{interpreted by the}} tradeoff between the <b>error</b> <b>of</b> <b>a</b> <b>measurement</b> and the consequential disturbance to the followed ones, which originated long ago from Heisenberg himself but now falls into reexamination and even heated debate. Here we show that the tradeoff is switched on or off by the quantum uncertainties of two involved non-commuting observables: if one is more certain than the other, there is no tradeoff; otherwise, they do have tradeoff and the Jensen-Shannon divergence gives it a good characterization. Comment: 8 pages, 2 figure...|$|E
40|$|The {{uncertainty}} relation {{formulated by}} Heisenberg in 1927 describes a trade-off between the <b>error</b> <b>of</b> <b>a</b> <b>measurement</b> of one observable and the disturbance caused on another complementary observable {{so that their}} product should be no less than a limit set by Planck's constant. In 1980, Braginsky, Vorontsov, and Thorne claimed that this relation leads to a sensitivity limit for gravitational wave detectors. However, in 1988 a model of position measurement was constructed that breaks both this limit and Heisenberg's relation. Here, we discuss the problems as to how we reformulate Heisenberg's relation to be universally valid and how we experimentally quantify the error and the disturbance to refute the old relation and to confirm the new relation. Comment: 12 pages, submitted to EmQM 13 proceeding...|$|E
40|$|In his seminal paper, {{which was}} {{published}} in 1927, Heisenberg originally introduced a relation between the precision <b>of</b> <b>a</b> <b>measurement</b> and the disturbance it induces onto another measurement. Here, we report a neutron-optical experiment that records the <b>error</b> <b>of</b> <b>a</b> spin-component <b>measurement</b> as well as the disturbance caused on <b>a</b> <b>measurement</b> <b>of</b> another spin-component to test error-disturbance uncertainty relations (EDRs). We demonstrate that Heisenberg’s original EDR is violated and the Ozawa and Branciard EDRs are valid in <b>a</b> wide range <b>of</b> experimental parameters...|$|R
2500|$|If we let [...] {{represent}} the <b>error</b> (i.e., inaccuracy) <b>of</b> <b>a</b> <b>measurement</b> <b>of</b> <b>an</b> observable A and [...] the disturbance produced on <b>a</b> subsequent <b>measurement</b> <b>of</b> the conjugate variable B {{by the former}} <b>measurement</b> <b>of</b> <b>A,</b> then the inequality proposed by Ozawa — encompassing both systematic and statistical errors — holds: ...|$|R
40|$|Measurements of {{reference}} spheres {{are widely used}} in verification and calibration of coordinate measuring systems. In this paper, {{a model for the}} propagation <b>of</b> random <b>errors</b> in the estimation of sphere parameters is outlined. The dependence <b>of</b> the <b>error</b> propagation on sampling area and sample density is demonstrated using both simulated and measured data. The model can be used to characterise the random <b>error</b> <b>of</b> <b>a</b> coordinate <b>measurement</b> system. It {{can also be used to}} determine the error limits on a single sphere parameter estimate or to determine the required sampling strategy to ensure that the confidence limits on an estimate remain within predetermined bounds. (C) 2005 Elsevier Ltd. All rights reserved...|$|R
40|$|If rounded {{data are}} used in {{estimating}} moments and regression coefficients, the estimates are typically more or less biased. The purpose of the paper is to study the bias inducing effect of rounding, which is also seen when population moments instead of their estimates are considered. Under appropriate conditions this effect can be approximately specified by versions of Sheppard's correction formula. We discuss {{the conditions under which}} these approximations are valid. We also investigate the efficiency loss that comes along with rounding. The rounding error, which corresponds to the measurement <b>error</b> <b>of</b> <b>a</b> <b>measurement</b> error model, has a marginal distribution which can be approximated by the uniform distribution. We generalize the concept of simple rounding to that of asymmetric rounding and study its effect on the mean and variance of a distribution under similar circumstances as with simple rounding. ...|$|E
40|$|The direct count {{method for}} {{enumerating}} bacteria in natural environments is widely used. This paper analyzes {{the sources of}} variation contributed by the various levels of the method: subsamples, filters, and microscope fields. Based on a nested analysis of variance, we show {{that most of the}} variance (less than 80 %) is caused by the fields and that the filters contributed nearly all of the remaining variance. The replication at each of the levels determines the total cost and <b>error</b> <b>of</b> <b>a</b> <b>measurement.</b> We compared several sampling schemes, including an optimal strategy which gives the lowest possible variance for a given cost. We recommend that preparing one filter from one subsample is adequate only if the samples are closely spaced in time or distance; otherwise, one filter should be prepared from two or preferably three subsamples. This sampling scheme emphasizes the importance of the highest level of replication. Our analysis shows that the accuracy of the direct count method can be substantially improved (by 20 to 50 %) without a large increase in cost when the proper degree of replication at each level is performed...|$|E
40|$|There is {{method of}} control of {{temperature}} field of thermocouple based sensor with controlled profile of temperature field (TBS with CPTF) along electrodes of main thermocouple (MTC) considered in this paper. This mentioned above method is based on neural networks. MTC measure temperature of an object directly. Stable profile of the temperature field along electrodes of MTC doesn?t allow to the heterogeneity error of thermoelectrodes of MTC appear itself. Such stability of the temperature field is achieved due to control subsystems that create and support their own temperature field for MTC. Thus, change of temperature field of an object of measurement doesn?t lead to change of the temperature field along electrodes of MTC. And, as it was mentioned above, heterogeneity error of thermoelectrodes of MTC don?t affect on total <b>error</b> <b>of</b> <b>a</b> <b>measurement.</b> There are 2 main properties of the method. The first property is to measure temperature deviation from given temperature for each zone. After that, create ordered sets of those deviations and corresponding temperatures of zones. Then it?s necessary to input those sets to neural network. Second property is to create training vectors for neural network. Those vectors are created by measurement of temperature deviations after random change of control action. Training of a neural network without precise mathematical model of an object is possible by this way. ? ?????? ????????? ???????????? ????? ????????? ????????? ??????? ?????????????? ???? ????? ???????? ????????? (???) ??????????????????? ??????????????? ? ??????????? ???????? ?????????????? ???? (??? ? ????). ??? ??????????????? ???????? ??????????? ??????? ?????????. ????????? ????????? ??????? ?????????????? ???? ?? ????????? ???????? ???? ??????????? ??-?? ????????????? ?????????????????? ?????????????? ?????????? ???. ???????? ??????????? ??????: (?) ?????? ?? ??????????????? ????? ????????? ???? ?? ?????? ???????? ?????????? ?????????? ??? ?? ????????, ? ? ????? ???????? ??????????? (??? ????????? ?????? ?????????? ???????? ??? ? ???? ??? ??????? ??????????); (??) ???????????? ??????? ???????? ??? ????????? ???? ????? ???????????? ??? ?????????????? ????????? ????????? ????????? ???????????? ??????????? ? ???????????? ????????? ?????????? ????????? ??????????? ????????? ??? (??? ????????? ??????? ????????? ???? ??? ?????????? ?????????? ?????? ?????????????? ?????? ???????? ????????? ? ??? ? ????) ...|$|E
30|$|Figure  2 d, e {{exhibit a}} similar trend and {{uniformity}} {{level for the}} linewidth and the normalized integral intensity, respectively. The former one presents the linewidth distribution over the entire structure with the minimum values concentrated in the wafer’s centre. The narrowest linewidth indicated by the shades of blue in Fig.  2 d also fills a similar area as the one indicated by the dashed circle in Fig.  2 a, spreading a bit {{to the top of}} the sample. Such high uniformity of the linewidth over the entire wafer is actually of the order <b>of</b> <b>a</b> standard <b>error</b> <b>of</b> <b>an</b> individual <b>measurement,</b> which will be discussed below. The linewidth is also distorted by the high-energy tail, indicating that the spectral broadening results mostly from the thermal broadening.|$|R
50|$|In psychometrics and psychophysics, {{the term}} {{accuracy}} is interchangeably used with validity and constant error. Precision is {{a synonym for}} reliability and variable <b>error.</b> The validity <b>of</b> <b>a</b> <b>measurement</b> instrument or psychological test is established through experiment or correlation with behavior. Reliability is established with <b>a</b> variety <b>of</b> statistical techniques, classically through an internal consistency test like Cronbach's alpha to ensure sets of related questions have related responses, and then comparison of those related question between reference and target population.|$|R
40|$|The {{frequency}} response function (FRF) <b>of</b> <b>a</b> system is often measured by taking {{the ratio of the}} output to the input Fourier coefficients of the steady-state response of the system to a periodic excitation. Generator noise, common noise disturbances picked up by the acquisition channels of the <b>measurement</b> device, and <b>an</b> external feedback loop, cause a correlation between the input/output <b>errors</b> <b>of</b> such <b>an</b> FRF <b>measurement.</b> This paper quantifies the bias error and the variance of the FRF measurement due to correlated input/output errors. status: publishe...|$|R
40|$|Pearce et al. [2004] {{compared}} tephra {{that was}} retrieved from Greenlandic ice (by Hammer et al. [2003]) with tephra from Aniakchak, Alaska. Largely {{on the basis}} of statistical analyses of the geochemical data, Pearce et al. claimed that the Greenlandic tephra matches with Aniakchak. Herein, those statistical analyses are examined. Pearce et al. [2004] devote a section of their paper to discussing the “statistical distance ” between two sets of tephrochronological data. The formula for statistical distance involves standard deviations. The term “standard deviation”, though, can mean different things in different contexts: either the potential <b>error</b> <b>of</b> <b>a</b> <b>measurement</b> (i. e. precision) or the inter-particle dispersion (i. e. indicating variation among different particles). Pearce et al. [2004, table 2, caption] note this distinction when presenting their data. In applying the formula for statistical distance, however, they confuse the two meanings: the formula requires measurement precision, but they use inter-particle dispersion instead. This confusion makes the statistical-distance calculations of Pearce et al. [2004, sect. 6] erroneous. (There is no immediate way of fixing the calculations, because when the correct meaning of “standard deviation ” is used, the D 2 statistic of Pearce et al. no longer has a chi-squared distribution.) A second statistical problem with the work of Pearce et al. [2004] is in the use of “relative differences”. The problem is easily seen via a hypothetical example: consider two tephras, both measured by two laboratories, with the following results for SiO 2 (on bulk samples, with measurement standard deviations indicated) ...|$|E
40|$|In its {{original}} formulation, Heisenberg's uncertainty principle {{dealt with the}} relationship between the <b>error</b> <b>of</b> <b>a</b> quantum <b>measurement</b> and the thereby induced disturbance on the measured object. Meanwhile, Heisenberg's heuristic arguments {{have turned out to be}} correct only for special cases. A new universally valid relation was derived by Ozawa in 2003. Here, we demonstrate that Ozawa's predictions hold for projective neutron-spin measurements. The experimental inaccessibility <b>of</b> <b>error</b> and disturbance claimed elsewhere has been overcome using a tomographic method. By <b>a</b> systematic variation <b>of</b> experimental parameters in the entire configuration space, the physical behavior <b>of</b> <b>error</b> and disturbance for projective spin- 1 / 2 measurements is illustrated comprehensively. The violation of Heisenberg's original relation, as well as, the validity of Ozawa's relation become manifest. In addition, our results conclude that the widespread assumption <b>of</b> <b>a</b> reciprocal relation between error and disturbance is not valid in general. Comment: 17 pages, 13 figure...|$|R
40|$|A {{method to}} measure the top quark mass in the {{inclusive}} lepton plus jets channel at LHC is presented. This method is specially designed {{in order to reduce}} systematic <b>errors</b> <b>of</b> the <b>measurement.</b> <b>A</b> simulation <b>of</b> <b>a</b> tt production and an ATLAS detector response is performed. It is shown that for 10 fb^{- 1 } integrated luminosity the proposed method will give <b>a</b> statistical accuracy <b>of</b> the order of 50 MeV and <b>a</b> systematic <b>error</b> <b>of</b> the order of 360 MeV for the top quark mass estimate...|$|R
3000|$|... e 1 hh 1 map. It {{reveals a}} normal {{distribution}} with <b>an</b> FWHM <b>of</b> 2  meV and {{is consistent with}} the previously discussed histogram for the GaSb-based sample. In order to estimate the standard <b>error</b> <b>of</b> <b>an</b> individual <b>measurement</b> <b>of</b> Ee 1 hh 1, the linewidths and integral intensities, the PL signal has been measured ten times for a selected grid point, marked with a tiny red circle in Figs.  2 a and 3 a, for the GaSb- and InAs-based samples, respectively. It is worth mentioning that the estimated overall standard error resulting from the spectrometer inaccuracy, the measurement conditions and the fitting procedure is of the order of 1  meV. This indicates that the previously mentioned energy and linewidth fluctuations of 1  meV in the region of ultra-high uniformity marked with the dashed circle in Fig.  2 a result rather from the experimental spectrometer inaccuracies than from the interface roughness, layer fluctuations or composition fluctuations {{in that part of the}} samples.|$|R
40|$|This study {{documents}} intra-session and inter-day reproducibility (coefficient {{of variation}} [V%]) and single measurement reliability (intra-class correlations [RI]; standard <b>error</b> <b>of</b> <b>a</b> single <b>measurement</b> [SEM%] [95 % confidence limits]) of indices of neuromuscular performance elicited during peripheral nerve magnetic stimulation. Twelve adults (five men and seven women) completed 3 assessment sessions on 3 days, during which multiple assessments of knee flexor volitional and magnetically-evoked indices of electromechanical delay (EMDV; EMDE), rate of force development (RFDV; RFDE), peak force (PFV; PTFE), and compound muscle action potential latency (LATE) and amplitude (AMPE) were obtained. Results showed that magnetically-evoked indices of neuromuscular performance offered statistically equivalent {{levels of measurement}} reproducibility (V%: 4. 3 – 31. 2 %) and reliability (RI: 0. 98 – 0. 51) compared to volitional indices (V%: 3. 7 – 25. 2 %; RI: 0. 98 – 0. 64), which support the efficacy of both approaches to assessment and the indices PFV, EMDV, EMDE and LATE offer the greatest practical utility for assessing neuromuscular performance...|$|R
40|$|Unpolarized inelastic neutron {{scattering}} {{is used to}} study the temperature and wave vector dependence of the dynamical magnetic susceptibility, ~"(q, ~), <b>of</b> <b>a</b> well-characterized single-crystal YBa 2 Cu 30 ~. 6 (T c = 62. 7 K). We find that a pseudogap opens in the spin-fluctuation spectrum at temperatures well above T c. We speculate that {{the appearance of the}} low-frequency incommensurate fluctuations i associated {{with the opening of the}} pseudogap. To within the <b>error</b> <b>of</b> the <b>measurements,</b> <b>a</b> gap in the spin-fluctuation spectrum is found in the superconductin...|$|R
40|$|Unpolarized inelastic neutron {{scattering}} {{is used to}} study the temperature and wave vector dependence of the dynamical magnetic susceptibility, {xi}`` (q,{omega}), <b>of</b> <b>a</b> well characterized single crystal YBa{sub 2 }Cu{sub 3 }O{sub 6. 6 } (T{sub c} = 62. 7 K). We find that a pseudogap opens in the spin fluctuation spectrum at temperatures well above T{sub c}. We speculate that {{the appearance of the}} low frequency incommensurate fluctuations is associated {{with the opening of the}} pseudogap. To within the <b>error</b> <b>of</b> the <b>measurements,</b> <b>a</b> gap in the spin fluctuation spectrum is found in the superconducting state...|$|R
40|$|We {{demonstrate}} that secure communication using coherent states is possible. The optimal eavesdropping strategy for an M-ry ciphering scheme {{shows that the}} minimum probability <b>of</b> <b>error</b> in <b>a</b> <b>measurement</b> for bit determination can be made arbitrarily close to the pure guessing value P_e= 1 / 2. This ciphering scheme can be optically amplified without degrading the security level. New avenues are open to secure communications at high speeds in fiber-optic or free-space channels. Comment: 2 1 / 2 pages, 1 figure, Proceedings of the Sixth International Conference on Quantum Communication, Measurement and Computing Proceedings (QCMC' 02), July 200...|$|R
40|$|We discuss nuclear {{effects in}} the Paschos-Wolfenstein {{relationship}} {{in the context of}} extraction of the weak mixing angle. We point out that the neutron excess correction to the Paschos-Wolfenstein relationship for a neutron-rich target is negative and large on the scale <b>of</b> experimental <b>errors</b> <b>of</b> <b>a</b> recent NuTeV <b>measurement.</b> We found <b>a</b> larger neutron excess correction to the Paschos-Wolfenstein relationship for total cross sections than that discussed by the NuTeV collaboration. Phenomenological applications of this observation are discussed {{in the context of the}} NuTeV deviation. Uncertainties in the neutron excess correction are estimated. Effects due to Fermi motion, nuclear binding, and nuclear shadowing are also discussed in the context of total cross sections. Comment: 7 pages, uses REVTeX...|$|R
40|$|Abstract: Dierent {{measurement}} schemes in multistation machining systems carry dierent {{amounts of}} information about the root causes <b>of</b> dimensional machining <b>errors.</b> The choice <b>of</b> <b>a</b> <b>measurement</b> strategy in <b>a</b> multistation machining system is therefore crucial for subsequent successful identi®cation <b>of</b> the machining <b>error</b> root causes. Recent advances in the linear state-space modelling <b>of</b> dimensional <b>errors</b> in multistation machining processes facilitate a formal and systematic characterization of measurement schemes. In this paper, the stream-of-variation methodology is employed to characterize various measurement schemes quantitatively in multistation machining systems using the Bayesian approach in statistics. Application of these methods is demonstrated in the characterization of measurement schemes in the machining process used for machining <b>of</b> <b>an</b> automotive cylinder head...|$|R
40|$|Unpolarized inelastic neutron {{scattering}} {{is used to}} study the temperature and wave vector dependence of the dynamical magnetic susceptibility, χ"(q,ω), <b>of</b> <b>a</b> well characterized single crystal YBa_ 2 Cu_ 3 O_ 6. 6 (T_c= 62. 7 K). We find that a pseudogap opens in the spin fluctuation spectrum at temperatures well above T_c. We speculate that {{the appearance of the}} low frequency incommensurate fluctuations is associated {{with the opening of the}} pseudogap. To within the <b>error</b> <b>of</b> the <b>measurements,</b> <b>a</b> gap in the spin fluctuation spectrum is found in the superconducting state. Comment: 6 pages, 3 ps figs, Proceedings of ICNS, Physica B, to be publishe...|$|R
40|$|The {{uncertainty}} principle generally prohibits determination of certain pairs of quantum mechanical observables with arbitrary precision and {{forms the basis}} of indeterminacy in quantum mechanics. It was Heisenberg who used the famous gamma-ray microscope thought experiment to illustrate this indeterminacy. A lower bound was set for the product <b>of</b> the <b>measurement</b> <b>error</b> <b>of</b> <b>an</b> observable and the disturbance caused by the measurement. Later on, the uncertainty relation was reformulated in terms of standard deviations, which focuses solely on indeterminacy of predictions and neglects unavoidable recoil in measuring devices. <b>A</b> correct formulation <b>of</b> the error-disturbance relation, taking recoil into account, is essential for <b>a</b> deeper understanding <b>of</b> the {{uncertainty principle}}. However, the validity of Heisenberg's original error-disturbance uncertainty relation is justifed only under limited circumstances. Another error-disturbance relation, derived by rigorous and general theoretical treatments of quantum measurements, {{is supposed to be}} universally valid. Here, we report a neutron optical experiment that records the <b>error</b> <b>of</b> <b>a</b> spin-component <b>measurement</b> as well as the disturbance caused on another spin-component measurement. The results confirm that both error and disturbance completely obey the new, more general relation but violate the old one in <b>a</b> wide range <b>of</b> <b>an</b> experimental parameter. Comment: 11 pages, 5 figures, Nature Physics (in press...|$|R
40|$|One of {{the most}} {{important}} parameters <b>of</b> <b>a</b> Differential Absorption Lidar (DIAL) system is the delay time between the on and off resonant pulses. It is important that this delay time is sufficiently small to ensure that the atmosphere is effectively frozen between the pulses. Therefore, most Dial systems were designed with two lasers firing alternately less than 1 msec apart. Despite the importance of this parameter in the design of DIAL systems and its contribution to the overall <b>error</b> <b>of</b> <b>a</b> column <b>measurement,</b> very {{little is known about the}} size <b>of</b> the <b>error</b> for the case <b>of</b> <b>a</b> direct-detection system using atmospheric backscatter. The ultraviolet DIAL system uses two independent YAG/dye lasers and is therefore suitable for measuring the effects of different pulse delays on the variance <b>of</b> column <b>measurements</b> for <b>a</b> variety <b>of</b> atmospheric conditions. <b>A</b> set <b>of</b> DIAL returns were acquired with the two lasers tuned to the same wavelength and with <b>a</b> range <b>of</b> pulse delay times between 250 microseconds and several minutes. This data set was recorded in full on a computer and was used both to test different averaging techniques and also to evaluate atmospheric contributions to DIAL columns...|$|R
40|$|Abstract. We {{consider}} two variants <b>of</b> <b>a</b> quantum-statistical generalization <b>of</b> the Cramér-Rao inequality that establishes an invariant {{lower bound}} on the mean square <b>error</b> <b>of</b> <b>a</b> generalized quantum <b>measurement.</b> The proposed complex variant of this inequality leads to <b>a</b> precise formulation <b>of</b> <b>a</b> generalized uncertainty principle for arbitrary states, {{in contrast to}} Helstrom’s variant [1] in which these relations are obtained only for pure states. <b>A</b> notion <b>of</b> canonical states is introduced and the lower mean square error bound is found for estimating of the parameters of canonical states, in particular, the canonical parameters <b>of</b> <b>a</b> Lie group. It is shown that these bounds are globally attainable only for canonical states for which there exist efficient measurements or quasimeasurements. 1...|$|R
40|$|Determination of {{transformation}} coefficient of primary measuring transducers of ionizing radiation {{is one of}} complicated problems in metrology as well as estimation <b>of</b> systematic <b>error</b> <b>of</b> <b>measurement.</b> <b>As</b> a possible approach for its solution authors suggested <b>a</b> method <b>of</b> computer modelling of radiation-transducer interaction processes [1]. Electron radiation is described through its spacial and energetic characteristics, transducer is set by means of its real geometrical parameters and elemental content. Elaborated in CERN code GEANT for modelling high-energy radiation-detector interaction {{seems to be very}} promising within such approach. Report contents the results of GEANT based analysis of two Faraday cup type transducers for electron energy range 1 [...] . 50, MeV. The transducers belong to State Measurement Standard o...|$|R
40|$|<b>A</b> theory <b>of</b> local {{temperature}} <b>measurement</b> <b>of</b> <b>an</b> interacting quantum electron system {{far from}} equilibrium via a floating thermoelectric probe is developed. It is {{shown that the}} local temperature so defined {{is consistent with the}} zeroth, first, second, and third laws of thermodynamics, provided the probe-system coupling is weak and broad band. For non-broad-band probes, the local temperature obeys the Clausius form of the second law and the third law exactly, but there are corrections to the zeroth and first laws that are higher-order in the Sommerfeld expansion. The corrections to the zeroth and first laws are related, and can be interpreted in terms <b>of</b> the <b>error</b> <b>of</b> <b>a</b> nonideal temperature <b>measurement.</b> These results also hold for systems at negative absolute temperature. Comment: 19 pages, 1 figur...|$|R
40|$|In this work, we {{consider}} the systematic <b>error</b> <b>of</b> quantum metrology by weak measurements under decoherence. We derive the systematic <b>error</b> <b>of</b> maximum likelihood estimation in general to the first-order approximation <b>of</b> <b>a</b> small deviation in the probability distribution, and study the robustness of standard weak measurement and postselected weak measurements against systematic errors. We show that, with a large weak value, the systematic <b>error</b> <b>of</b> <b>a</b> postselected weak <b>measurement</b> when the probe undergoes decoherence can be significantly lower than that <b>of</b> <b>a</b> standard weak <b>measurement.</b> This indicates the advantage of weak value amplification in improving the performance of parameter estimation. We illustrate the results by an exact numerical simulation of decoherence arising from a bosonic mode and {{compare it to the}} first-order analytical result we obtain. Comment: 9 pages, 3 figures. V 2 : close to the published versio...|$|R
40|$|This paper {{presents}} an algorithm to select <b>a</b> good set <b>of</b> gate sizes for the primitive gates <b>of</b> <b>a</b> standard cell library. <b>A</b> <b>measurement</b> error on <b>a</b> gate is dened {{to quantify the}} discrepancy resulting from replacing the size required by a synthesis sizing algorithm with a size available in a discrete cell library. The criterion for gate size selection is <b>a</b> set <b>of</b> gate sizes that minimizes the cumulative <b>error</b> <b>of</b> <b>a</b> prescribed <b>measurement.</b> Optimal solutions to the gate size selection problem targetting size and delay measurements are presented for cases when the probability distribution and the delay equations are simple. A realistic probability distribution is obtained using <b>a</b> sample space <b>of</b> gates derived fromagroup of designs that is synthesized under the semi-custom synthesis methodology [1]. A -match &quot; (minimizing delay error) and a &quot; (minimizing size <b>error)</b> set <b>of</b> gate sizes are obtained numerically, and are subsequently realized as discrete cell libraries. The previous group of designs are synthesized using the two selected cell libraries and two other cell libraries, one with -spacing &quot; of cell sizes {{and the other with}} -spacing &quot; of cell sizes. The -match &quot; library gives the best overall slack and area results. ...|$|R
