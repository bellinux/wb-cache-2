9|10000|Public
40|$|The {{last decade}} {{has seen a}} {{proliferation}} of linguistically annotated corpora coding many phenomena in support of <b>empirical</b> <b>natural</b> <b>language</b> research – both computational and theoretical. Because the annotated phenomena and annotation representations vary widely across different schemes, {{there is a need}} for making them compatible with each other, to ensure effective merging, comparison and manipulation with common software. Ensuring compatibility is even more necessary when different types of annotations are done on the same source text, for example the Wall Street Journal (WSJ) corpus. Multi-level annotations on the WSJ include part of speech tagging, syntactic constituency, coreference, semantic role labeling, events, and discourse relations. In many cases, <b>empirical</b> <b>natural</b> <b>language</b> research using the WSJ would benefit immensely from using information from multiple layers of annotation, but in order to allow for this, it is imperative to ensure the efficient interoperability of the annotations. In Bunt (2010), an annotation scheme design methodology is proposed, providing a for...|$|E
40|$|In {{the field}} of <b>empirical</b> <b>natural</b> <b>language</b> processing, {{researchers}} constantly deal with large amounts of marked-up data, whether the markup {{is done by the}} researcher or someone else, human nature dictates that it will have errors in it. This paper will more fully characterise the problem and discuss whether and when (and how) to correct the errors. The discussion is illustrated with specific examples involving function tagging in the Penn treebank...|$|E
40|$|Statistical {{significance}} {{testing of}} differences in values of metrics like recall, precision and balanced F-score is {{a necessary part of}} <b>empirical</b> <b>natural</b> <b>language</b> processing. Unfortunately, we find in a set of experiments that many commonly used tests often underestimate the significance and so are less likely to detect differences that exist between different techniques. This underestimation comes from an independence assumption that is often violated. We point out some useful tests that do not make this assumption, including computationally-intensive randomization tests. ...|$|E
50|$|<b>Empirical</b> Methods in <b>Natural</b> <b>Language</b> Processing or EMNLP is {{a leading}} conference {{in the area of}} <b>Natural</b> <b>Language</b> Processing. EMNLP is {{organized}} by the ACL special interest group on linguistic data (SIGDAT).|$|R
25|$|Tu, K. and Honavar, V. (2012). Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars. In: Proceedings of EMNLP-CoNLL 2012 : Conference on <b>Empirical</b> Methods in <b>Natural</b> <b>Language</b> Processing and Computational <b>Natural</b> <b>Language</b> Learning. pp.1324–1334.|$|R
5000|$|Snow, R.; S. Prakash, D. Jurafsky, A. Y. Ng. 2007. Learning to Merge Word Senses, Proceedings of the 2007 Joint Conference on <b>Empirical</b> Methods in <b>Natural</b> <b>Language</b> Processing and Computational <b>Natural</b> <b>Language</b> Learning (EMNLP-CoNLL).|$|R
40|$|In this paper, {{we discuss}} {{experiments}} applying machine learning techniques {{to the task}} of confusion set disambiguation, using three orders of magnitude more training data than has previously been used for any disambiguation-in-string-context problem. In an attempt to determine when current learning methods will cease to benefit from additional training data, we analyze residual errors made by learners when issues of sparse data have been significantly mitigated. Finally, in the context of our results, we discuss possible directions for the <b>empirical</b> <b>natural</b> <b>language</b> research community...|$|E
40|$|This paper {{discusses}} a novel probabilistic synchronous TAG formalism, synchronous Tree Substitution Grammar with sister adjunction (TSG+SA). We use it {{to parse}} a language {{for which there is}} no training data, by leveraging off a second, related language for which there is abundant training data. The grammar for the resource-rich side is automatically extracted from a treebank; the grammar on the resource-poor side and the synchronization are created by handwritten rules. Our approach thus represents a combination of grammar-based and <b>empirical</b> <b>natural</b> <b>language</b> processing. We discuss the approach using the example of Levantin...|$|E
40|$|Banko and Brill (2001) {{suggested}} {{that the development of}} very large training corpora may be more effective for progress in <b>empirical</b> <b>Natural</b> <b>Language</b> Processing than improving methods that use existing smaller training corpora. This work tests their claim by exploring whether a very large corpus can eliminate the sparseness problems associated with estimating unigram probabilities. We do this by empirically investigating the convergence behaviour of unigram probability estimates on a one billion word corpus. When using one billion words, as expected, we do find that many of our estimates do converge to their eventual value. However, we also find that for some words, no such convergence occurs. This leads us to conclude that simply relying upon large corpora is not in itself sufficient: we must pay attention to the statistical modelling as well. ...|$|E
5000|$|Tu, K. and Honavar, V. (2012). Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars. In: Proceedings of EMNLP-CoNLL 2012 : Conference on <b>Empirical</b> Methods in <b>Natural</b> <b>Language</b> Processing and Computational <b>Natural</b> <b>Language</b> Learning. pp. 1324-1334.|$|R
5000|$|Gliozzo, A.; B. Magnini and C. Strapparava. 2004. Unsupervised domain {{relevance}} estimation {{for word}} sense disambiguation. In Proceedings of the 2004 Conference on <b>Empirical</b> Methods in <b>Natural</b> <b>Language</b> Processing (EMNLP, Barcelona, Spain).|$|R
5000|$|Navigli, R.; G. Crisafulli. Inducing Word Senses to Improve Web Search Result Clustering. Proc. of the 2010 Conference on <b>Empirical</b> Methods in <b>Natural</b> <b>Language</b> Processing (EMNLP 2010), MIT Stata Center, Massachusetts, USA.|$|R
40|$|Memory-Based Language Processing (MBLP) views {{language}} processing as being {{based on the}} direct reuse of previous experience {{rather than on the}} use of rules or other structures extracted from that experience. In such a framework, language acquisition is modeled as the storage of examples in memory, and {{language processing}} as analogical or similarity-based reasoning. We briefly discuss the properties and origins of this family of techniques, and provide an overview of current approaches and issues. 1 <b>Empirical</b> <b>Natural</b> <b>Language</b> Processing Natural Language Processing (NLP) studies the knowledge representation and problem solving issues involved in learning, producing, and understanding language. Language Technology, or Language Engineering, uses the formalisms and theories developed within NLP in applications ranging from spelling error correction to machine translation and automatic extraction of knowledge from text. Although the origins of NLP are both logical and statistical, as in o [...] ...|$|E
40|$|Transformation-based {{learning}} (TBL) {{and frequent}} pattern discovery (FPD) are two popular research paradigms, {{one from the}} domain of <b>empirical</b> <b>natural</b> <b>language</b> processing, the second {{from the field of}} data mining. This paper describes how Eric Brill's original TBL algorithm can be improved via incorporation of FPD techniques. The algorithm B-Warmr is presented that upgrades TBL to first-order logic and speeds up the search for transformations, also in the original propositional case. We demonstrate some scaling properties of B-Warmr and discuss how the algorithm can be tuned to generate (first-order) decision lists. We also propose a new method, Disjunctive TransformationBased Learning (DTBL) that combines the advantages of TBL and decision lists. 1 Introduction Basic idea. Empirical methods for natural language processing (Brill and Mooney, 1997) are commonly situated in one of two main families according to whether they operate on the sub-symbolic level (the statistical family), or [...] ...|$|E
40|$|Opinions are everywhere. The op/ed {{pages of}} newspapers, {{political}} blogs, and consumer websites like epinions. com {{are just some}} examples of the textual opinions available to readers. And there are many consumers {{who are interested in}} following these opinions - intelligence analysts who track the opinions of foreign countries, public relation firms who want to ensure positive opinions for their clients, pollsters who want to know the public's opinions about politicians, and companies who want to know customers' opinions about their products. The problem faced by all of these consumers of opinion is {{that there is such a}} wealth of text to process that it is hard to read it all. Central to processing the opinions in these text will be solving two specific problems - identifying expressions of opinion, and identifying their hierarchical structure. We demonstrate solutions involving <b>empirical</b> <b>natural</b> <b>language</b> processing techniques. Although empirical, data-driven methods such as these have become the norm in natural language processing, little work has been done in analyzing their impact on the reproducibility, efficiency, and effectiveness of research. We address two specific problems in this area. We introduce a lightweight computational workflow system to improve the reproducibility and efficiency of machine learning and natural language processing experiments. And we investigate the process of feature generation, setting out desiderata for an ideal process and exploring the effectiveness of several alternatives. Both are investigated in the context of the natural language learning tasks set out earlier...|$|E
40|$|Comparative machine {{learning}} experiments {{have become an}} important methodology in <b>empirical</b> approaches to <b>natural</b> <b>language</b> processing (i) to investigate which {{machine learning}} algorithms have the `right bias' to solve specific <b>natural</b> <b>language</b> processing tasks, and (ii) to investigate which sources of information add to accuracy in a learning approach...|$|R
40|$|Treebanks, or {{syntactically}} annotated corpora, are {{an invaluable}} resource {{for the development}} and evaluation of syntactic parsers, {{as well as for}} <b>empirical</b> research on <b>natural</b> <b>language</b> syntax. Treebanks for Swedish have a long and venerable history, represented by the pioneering work on Talbanken (Einarsson...|$|R
2500|$|Xu, Peng; Ahmad Emami and Frederick Jelinek (2003). [...] "". In Michael Collins and Mark Steedman, eds. EMNLP '03 Proceedings of the 2003 {{conference on}} <b>Empirical</b> methods in <b>natural</b> <b>language</b> processing. East Stroudsburg, Penn.: Association for Computational Linguistics. pp.160–167[...] [...] (won [...] "best paper" [...] award) ...|$|R
5000|$|Xu, Peng; Ahmad Emami and Frederick Jelinek (2003). [...] "Training Connectionist Models for the Structured Language Model". In Michael Collins and Mark Steedman, eds. EMNLP '03 Proceedings of the 2003 {{conference on}} <b>Empirical</b> methods in <b>natural</b> <b>language</b> processing. East Stroudsburg, Penn.: Association for Computational Linguistics. pp. 160-167[...] [...] (won [...] "best paper" [...] award) ...|$|R
40|$|This paper {{presents}} a corpus-based method to assign grammatical subject/object relations to ambiguous German constructs. It {{makes use of}} an unsupervised learning procedure to collect training and test data, and the back-off model to make assignment decisions. Comment: To appear in Proceedings of the Second Conference on <b>Empirical</b> Methods in <b>Natural</b> <b>Language</b> Processing, 7 pages, LaTe...|$|R
40|$|Abstract meaning {{representation}} for sembanking. In Proceedings of the 7 th linguistic annotation {{workshop and}} interoperability with discourse (pp. 178 – 186). Sofia, Bulgaria. Baroni, M., & Zamparelli, R. (2010). Nouns are vectors, adjectives are matrices: Representing adjective-noun con-structions in semantic space. In Proceedings of the 2010 conference on <b>empirical</b> methods in <b>natural</b> <b>language</b> processing (pp. 1183 – 1193). Stroudsburg, PA, USA. Carpuat, M., & Wu, D. (2005, June). Word sense disambiguation vs. statistical machine translation. In Proceedings of the 43 rd {{annual meeting of}} the association for computational linguistics (acl’ 05) (pp. 387 – 394). Ann Arbor, Michigan: Association for Computational Linguistics. Carpuat, M., & Wu, D. (2007). Improving statistical machine translation using word sense disambiguation. In Pro-ceedings of the 2007 joint conference on <b>empirical</b> methods in <b>natural</b> <b>language</b> processing and computational <b>natural</b> <b>language</b> learning (emnlp-conll) (pp. 61 – 72). Chan, Y. S., Ng, H. T., & Chiang, D. (2007, June). Word sense disambiguation improves statistical machine trans-lation. In Proceedings of the 45 th {{annual meeting of the}} association of computational linguistics (pp. 33 – 40) ...|$|R
40|$|Language {{learning}} {{has thus far}} not been a hot application for machine-learning (ML) research. This limited attention for work on empirical learning of language knowledge and behaviour from text and speech data seems unjusti ed. After all, it is becoming apparent that <b>empirical</b> learning of <b>Natural</b> <b>Language</b> Processing (NLP) can alleviate NLP's all-time main problem, viz. the knowledge acquisition bottleneck: empirical ML methods such as rule induction, top down induction of decision trees, lazy learning, inductive logic programming, and some types of neural network learning, seem to be excellently suited to automatically induce exactly that knowledge {{that is hard to}} gather by hand. In this paper we address the question why NLP is an interesting application for empirical ML, and provide a brief overview of current work in this area. 1 <b>Empirical</b> Learning of <b>Natural</b> <b>Language</b> Looking at the ML literature of the last decade, it is clear that language {{learning has}} not been an important application area of ML techniques. Especially the absence o...|$|R
40|$|We {{explore the}} use of the orthographic syllable, a variable-length consonant-vowel sequence, as a basic unit of {{translation}} between related languages which use abugida or alphabetic scripts. We show that orthographic syllable level translation significantly outperforms models trained over other basic units (word, morpheme and character) when training over small parallel corpora. Comment: 7 pages, 1 figure, compiled with XeTex, to be published at the Conference on <b>Empirical</b> Methods in <b>Natural</b> <b>Language</b> Processing (EMNLP), 201...|$|R
40|$|<b>Empirical</b> {{methods in}} <b>Natural</b> <b>Language</b> Processing (NLP) and Machine Translation (MT) have become {{mainstream}} {{in the research}} field. Accordingly, {{it is important that}} the tools and techniques in these paradigms be taught to potential future researchers and developers in University courses. While many dedicated courses on Statistical NLP can be found, there are few, if any courses on Empirical Approaches to MT. This paper presents the development and assessment of one such course as taught to final year undergraduates taking a degree in NLP. ...|$|R
40|$|Workshop at 2016 Conference on <b>Empirical</b> Methods in <b>Natural</b> <b>Language</b> Processing (5 November 2016) We {{present a}} corpus for protest event mining that {{combines}} token-level annotation with the event schema and ontology of entities and events from protest {{research in the}} social sciences. The dataset uses newswire reports from the English Gigaword corpus. The token-level annotation is inspired by annotation standards for event extraction, in particular that of the Automated Content Extraction 2005 corpus (Walker et al., 2006). Domain experts perform the entire annotation task. We report competitive intercoder agreement results. ERC POLCON project funded...|$|R
40|$|We {{propose a}} method for {{embedding}} two-dimensional locations in a continuous vector space using a neural network-based model incorporating mixtures of Gaussian distributions, presenting two model variants for text-based geolocation and lexical dialectology. Evaluated over Twitter data, the proposed model outperforms conventional regression-based geolocation and provides a better estimate of uncertainty. We also show {{the effectiveness of the}} representation for predicting words from location in lexical dialectology, and evaluate it using the DARE dataset. Comment: Conference on <b>Empirical</b> Methods in <b>Natural</b> <b>Language</b> Processing (EMNLP 2017) September 2017, Copenhagen, Denmar...|$|R
40|$|We {{propose a}} novel deep {{learning}} model for joint document-level entity disambiguation, which leverages learned neural representations. Key components are entity embeddings, a neural attention mechanism over local context windows, and a differentiable joint inference stage for disambiguation. Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention-entity maps. Extensive experiments show {{that we are able}} to obtain competitive or state-of-the-art accuracy at moderate computational costs. Comment: Conference on <b>Empirical</b> Methods in <b>Natural</b> <b>Language</b> Processing (EMNLP) 2017 long pape...|$|R
40|$|Comparative machine {{learning}} experiments {{have become an}} important methodology in <b>empirical</b> approaches to <b>natural</b> <b>language</b> processing (i) to investigate which {{machine learning}} algorithms have the 'right bias' to solve specific <b>natural</b> <b>language</b> processing tasks, and (ii) to investigate which sources of information add to accuracy in a learning approach. Using automatic word sense disambiguation as an example task, we show that with the methodology currently used in comparative machine learning experiments, the results may often not be reliable because {{of the role of}} and interaction between feature selection and algorithm parameter optimization. We propose genetic algorithms as a practical approach to achieve both higher accuracy within a single approach, and more reliable comparisons...|$|R
40|$|LangPro is an {{automated}} theorem prover for <b>natural</b> <b>language</b> ([URL] Given {{a set of}} premises and a hypothesis, {{it is able to}} prove semantic relations between them. The prover is based on a version of analytic tableau method specially designed for natural logic. The proof procedure operates on logical forms that preserve linguistic expressions to a large extent. %This property makes the logical forms easily obtainable from syntactic trees. %, in particular, Combinatory Categorial Grammar derivation trees. The nature of proofs is deductive and transparent. On the FraCaS and SICK textual entailment datasets, the prover achieves high results comparable to state-of-the-art. Comment: 6 pages, 8 figures, Conference on <b>Empirical</b> Methods in <b>Natural</b> <b>Language</b> Processing (EMNLP) 201...|$|R
40|$|Word embeddings allow <b>natural</b> <b>language</b> {{processing}} systems to share statistical information across related words. These embeddings are typically based on distributional statistics, {{making it difficult}} for them to generalize to rare or unseen words. We propose to improve word embeddings by incorporating morphological information, capturing shared sub-word features. Unlike previous work that constructs word embeddings directly from morphemes, we combine morphological and distributional information in a unified probabilistic framework, in which the word embedding is a latent variable. The morphological information provides a prior distribution on the latent word embeddings, which in turn condition a likelihood function over an observed corpus. This approach yields improvements on intrinsic word similarity evaluations, and also in the downstream task of part-of-speech tagging. Comment: Appeared at the Conference on <b>Empirical</b> Methods in <b>Natural</b> <b>Language</b> Processing (EMNLP 2016, Austin...|$|R
40|$|Despite <b>empirical</b> {{evidence}} that <b>natural</b> <b>language</b> dialog capabilities {{are necessary for}} the success of tutorial sessions, only few state-of-the-art tutoring systems use natural-language style interaction. Since domain knowledge, tutoring and pedagogical knowledge, and dialog management are tightly intertwined, the modeling and integration of proper <b>natural</b> <b>language</b> dialog capabilities in a tutoring system turns out to be barely manageable. In the DIALOG project, we aim at a mathematical tutoring dialog system that employs an elaborate <b>natural</b> <b>language</b> dialog component. To tutor naive set theory, we use a formally encoded mathematical theory including definitions and theorems along with their proofs. In this paper we present how we enhance this ontology by making relations explicit and we show how these relations can be used by the socratic tutoring strategy, which we employ, in planning the next system utterance. The decisive characteristic of the socratic strategy is the use of hints in order to achieve self-explanation. ...|$|R
40|$|We {{present a}} coding scheme, {{based on a}} Bayesian Network (BN) formalism, for {{describing}} probabilistic and causal information in arguments in medical genetics. The scheme was applied to a corpus of genetic counseling letters and evaluated for intercoder reliability. Results show that the model is highly relevant to the corpus while intercoder reliability of the coding scheme is good. We plan to use the coding scheme in an empirical study of argument strategies. Since the coding scheme refers only to BN concepts and general concepts in medical diagnosis, {{it may be useful}} to other researchers for <b>empirical</b> studies of <b>natural</b> <b>language</b> corpora in medicine. ...|$|R
40|$|The {{recent past}} has {{witnessed}} a predominance of robust <b>empirical</b> methods in <b>natural</b> <b>language</b> structure prediction, but mostly through {{the analysis of}} syntax. Wide-coverage analysis of the underlying meaning or semantics of <b>natural</b> <b>language</b> utterances still remains a major obstacle for language understanding. A primary bottleneck lies in the scarcity of high-quality and large amounts of annotated data that provide complete information about the semantic structure of <b>natural</b> <b>language</b> expressions. In this thesis proposal, we study structured probabilistic models tailored to solve problems in computational semantics, {{with a focus on}} modeling structure that is not visible in annotated text data. First, we investigate the problem of paraphrase identification, which attempts to recognize whether two sentences convey the same meaning. Our approach towards solving this problem systematically blends <b>natural</b> <b>language</b> syntax and lexical semantics within a probabilistic framework, and achieves state-of-the-art accuracy on a standard corpus, when trained on a set of true and false sentential paraphrases (Das and Smith, 2009). Given a pair of sentences, the presented model recognizes the paraphrase relationship by predicting the structure of one sentence given the other, allowing loose syntactic transformation and lexical semantic alteration at the level of aligned words...|$|R
40|$|ION CONSIDERED HARMFUL: LAZY LEARNING OF LANGUAGE PROCESSING Walter Daelemans Computational Linguistics Tilburg University, The Netherlands, and Center for Dutch Language and Speech, University of Antwerp, Belgium walter. daelemans@kub. nl 1 <b>Empirical</b> Learning of <b>Natural</b> <b>Language</b> Browsing {{through the}} Machine Learning literature, {{we find that}} {{language}} learning is not a hot topic in machine learning, and that most work in the area addresses either the computational modeling of child language acquisition or the extraction of domain knowledge from text. The algorithms used are also predominantly analytic (symbol-level learning) rather than empirical (knowledge-level learning). While these are obviously interesting research areas and approaches, the absence of more research on the empirical learning of language knowledge and behaviour from text and speech data strikes me as strange. After all, the main problem of the AI discipline of <b>Natural</b> <b>Language</b> Processing (NLP) is a knowledge acquisi [...] ...|$|R
40|$|Hauser (et al.) [1] {{claim that}} a core {{property}} of the human language faculty is recursion and that this property “yields discrete infinity ” of <b>natural</b> <b>languages.</b> On the other hand, recursion is often motivated by the observation that there are infinitely many sentences that should be generated by {{a finite number of}} rules. It should be obvious that one cannot pursue both arguments simultaneously, on pain of circularity. We want to argue that discrete infinity is not derived, but a modeling choice. Thus, discrete infinity is not an <b>empirical</b> fact about <b>natural</b> <b>languages</b> (see [2, 3] for arguments related to that point). Attributing discrete infinity to <b>natural</b> <b>languages</b> confuses a model with empirical reality. A common motivation for assuming an infinite model is that assuming a finite model implies that there is a least upper bound on sentence length. Although there may be performance considerations to believe that there is an upper bound, there seem to be no grammatical reasons for assuming a least upper bound. It is certainly true that recursive rules yield an infinite model (infinitel...|$|R
40|$|We {{introduce}} gap inheritance, a new {{structural property}} on trees, {{which provides a}} way to quantify {{the degree to which}} intervals of descendants can be nested. Based on this property, two new classes of trees are derived that provide a closer approximation to the set of plausible <b>natural</b> <b>language</b> dependency trees than some alternative classes of trees: unlike projective trees, a word can have descendants in more than one interval; unlike spanning trees, these intervals cannot be nested in arbitrary ways. The 1 -Inherit class of trees has exactly the same <b>empirical</b> coverage of <b>natural</b> <b>language</b> sentences as the class of mildly nonprojective trees, yet the optimal scoring tree can be found in an order of magnitude less time. Gap-minding trees (the second class) have the property that all edges into an interval of descendants come from the same node, and thus an algorithm which uses only single intervals can produce trees in which a node has descendants in multiple intervals. ...|$|R
