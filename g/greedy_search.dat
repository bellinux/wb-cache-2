538|650|Public
5000|$|... {{forward-backward}} <b>greedy</b> <b>search</b> and exact methods using branch-and-bound techniques, ...|$|E
5000|$|The {{most common}} {{realisation}} of -medoid clustering is the Partitioning Around Medoids (PAM) algorithm. PAM uses a <b>greedy</b> <b>search</b> {{which may not}} find the optimum solution, but it is faster than exhaustive search [...] It works as follows: ...|$|E
50|$|The {{second step}} {{performs}} a <b>greedy</b> <b>search</b> for possible power edges between candidate power nodes.Power edges abstracting the most edges {{in the original}} network are added first to the power graph. Thus bicliques, cliques and stars are incrementally replaced with power edges, until all remaining single edges are also added.Candidate power nodes that are not the end point of any power edge are ignored.|$|E
25|$|Stochastic {{gradient}} descent runs many <b>greedy</b> <b>searches</b> from random initial locations.|$|R
40|$|Abstract This paper {{presents}} a novel real-time multiple object tracking algorithm, which contains three parts: region correlation based foreground segmentation, merging-splitting based data association and <b>greedy</b> <b>searching</b> based occluded object localization. The main {{characteristics of the}} proposed algorithm are summarized as follows: 1) the multiple object tracking and occlusion handling problem is successfully changed into an image classification problem with prior knowledge of object number and feature; 2) a highly efficient <b>greedy</b> <b>searching</b> method is presented to meet real-time capability; 3) it has good performance in expansibility, and it has no constraints {{about the number of}} occluded objects, the occlusion ratio and the object′s motion model. Experiment results with hand labeled IBM database demonstrate that the method is effective and efficient...|$|R
25|$|What sets A* {{apart from}} a <b>greedy</b> best-first <b>search</b> {{algorithm}} is that it takes the cost/distance already traveled, , into account.|$|R
50|$|After the {{preliminary}} step of interest point detection, feature generation and clustering, {{we have a}} large set of candidate parts over the training images. To learn the model, Weber & Welling first perform a <b>greedy</b> <b>search</b> over possible model configurations, or equivalently, over potential subsets of the candidate parts. This is done in an iterative fashion, starting with random selection. At subsequent iterations, parts in the model are randomly substituted, the model parameters are estimated, and the performance is assessed. The process is complete when further model performance improvements are no longer possible.|$|E
5000|$|Recent action {{learning}} methods take various approaches and employ {{a wide variety}} of tools from different areas of artificial intelligence and computational logic. As an example of a method based on propositional logic, we can mention SLAF (Simultaneous Learning and Filtering) algorithm, which uses agent's observations to construct a long propositional formula over time and subsequently interprets it using a satisfiability (SAT) solver. Another technique, in which learning is converted into a satisfiability problem (weighted MAX-SAT in this case) and SAT solvers are used, is implemented in ARMS (Action-Relation Modeling System).Two mutually similar, fully declarative approaches to {{action learning}} were based on logic programming paradigm Answer Set Programming (ASP) and its extension, Reactive ASP. In another example, bottom-up inductive logic programming approach was employed. Several different solutions are not directly logic-based. For example, the action model learning using a perceptron algorithm [...] or the multi level <b>greedy</b> <b>search</b> over the space ofpossible action models. In the older paper from 1992, the action model learning was studied as an extension of reinforcement learning.|$|E
5000|$|One {{possible}} {{way to solve}} NNS is to construct a graph , where every point [...] is uniquely associated with vertex [...] The search of {{the point in the}} set S closest to the query q takes the form of the search of vertex in the graph [...]One of the basic vertex search algorithms in graphs with metric objects is the <b>greedy</b> <b>search</b> algorithm. It starts from the random vertex [...] The algorithm computes a distance value from the query q to each vertex from the neighborhood [...] of the current vertex , and then selects a vertex with the minimal distance value. If the distance value between the query and the selected vertex is smaller than the one between the query and the current element, then the algorithm moves to the selected vertex, and it becomes new current vertex. The algorithm stops when it reaches a local minimum: a vertex whose neighborhood does not contain a vertex that is closer to the query than the vertex itself.This idea was exploited in the VoroNet system for the plane, in the RayNet system for the , and for the general metric space in the Metrized Small World algorithm.|$|E
40|$|ParaEval is an {{automated}} evaluation method for comparing reference and peer summaries. It facilitates a tieredcomparison strategy where recall-oriented global optimal and local <b>greedy</b> <b>searches</b> for paraphrase matching are enabled {{in the top}} tiers. We utilize a domainindependent paraphrase table extracted from a large bilingual parallel corpus using methods from Machine Translation (MT). We show {{that the quality of}} ParaEval’s evaluations, measured by correlating with human judgments, closely resembles that of ROUGE’s. ...|$|R
3000|$|The <b>greedy</b> pruning <b>search</b> {{strategy}} {{is shown in}} Figure 1, which with appropriate modifications can also represent the sorted pruning variation.Let [...]...|$|R
40|$|<b>Greedy</b> Best-First <b>Search</b> (GBFS) is a {{powerful}} algorithm {{at the heart of}} many state of the art satisficing planners. One major weakness of GBFS is its behavior in so-called uninfor-mative heuristic regions (UHRs) - parts of the search space in which no heuristic provides guidance towards states with improved heuristic values. This work analyzes the problem of UHRs in planning in de-tail, and proposes a two level search framework as a solution. In <b>Greedy</b> Best-First <b>Search</b> with Local Exploration (GBFS-LE), a local exploration is started from within a global GBFS whenever the search seems stuck in UHRs. Two different local exploration strategies are developed an...|$|R
40|$|Abstract—For {{compressed}} sensing, in {{the framework}} of <b>greedy</b> <b>search</b> reconstruction algorithms, we introduce the notion of initial support-set. The initial support-set is an estimate given to a reconstruction algorithm to improve the performance of the reconstruction. Furthermore, we classify existing <b>greedy</b> <b>search</b> algorithms as being serial or parallel. Based on this classification and the goal of robustness to errors in the initial support-sets we develop a new <b>greedy</b> <b>search</b> algorithm called FROGS. We end the paper with careful numerical experiments concluding that FROGS perform well compared to existing algorithms (both in terms of performance and execution time) and that it is robust against errors in the initial support-set. Index Terms—Compressed sensing, <b>greedy</b> <b>search,</b> greedy pur-suit, initial support. I...|$|E
40|$|Machine {{learning}} is the inference of general patterns from data. Machine-learning algorithms search large spaces of potential hypotheses for {{the hypothesis that}} best fits the data. Since the search space for most induction problems grows exponentially {{in the number of}} features used to describe the data, most induction algorithms use <b>greedy</b> <b>search</b> to minimize search cost. <b>Greedy</b> <b>search</b> is a polynomial-time algorithm that achieves its efficiency by exploring {{only a tiny fraction of}} all hypotheses. While <b>greedy</b> <b>search</b> has good performance, it often misses the best hypotheses. This thesis proposes massive search as an alternative to <b>greedy</b> <b>search.</b> Massive search aggressively searches as many hypotheses as possible in the time available. Since massive search explores a larger portion of the hypothesis space, it is less [...] ...|$|E
40|$|<b>Greedy</b> <b>search</b> is {{commonly}} used {{in an attempt to}} generate solutions quickly at the expense of completeness and optimality. In this work, we consider learning sets of weighted action-selection rules for guiding <b>greedy</b> <b>search</b> with application to automated planning. We make two primary contributions over prior work on learning for <b>greedy</b> <b>search.</b> First, we introduce weighted sets of action-selection rules as a new form of control knowledge for <b>greedy</b> <b>search.</b> Prior work has shown the utility of action-selection rules for <b>greedy</b> <b>search,</b> but has treated the rules as hard constraints, resulting in brittleness. Our weighted rule sets allow multiple rules to vote, helping to improve robustness to noisy rules. Second, we give a new iterative learning algorithm for learning weighted rule sets based on RankBoost, an efficient boosting algorithm for ranking. Each iteration considers the actual performance of the current rule set and directs learning based on the observed search errors. This is in contrast to most prior approaches, which learn control knowledge independently of the search process. Our empirical results have shown significant promise for this approach in a number of domains...|$|E
3000|$|... [43] {{along with}} <b>greedy</b> {{stepwise}} <b>search</b> {{which will be}} used to create their EI. The CFS method finds variables that are both strongly correlated to the final prediction and that are weakly correlated between them. The CFS method with <b>greedy</b> stepwise <b>search</b> found a subset of 52 features which was broken down further by manual reduction followed by another round of CFS bringing the subset down to 23. This subset was then populated by one variable (gender) giving the final EI subset of 24 variables. The top 6 attributes in the EI (ranked by information gain) are age, comorbidity count, amount of hospitalization a year prior to admission, high blood urea nitrogen levels, low calcium, and mean albumin.|$|R
40|$|Estimation of Bayesian network algorithms, which adopt Bayesian {{networks}} as the {{probabilistic model}} {{were one of}} the most sophisticated algorithms in the estimation of distribution algorithms. However the estimation of Bayesian network is key topic of this algorithm, conventional EBNAs adopt <b>greedy</b> <b>searches</b> to search for better network structures. In this paper, we propose a new EBNA, which adopts genetic algorithm to search the structure of Bayesian network. In order to reduce the computational complexity of estimating better network structures, we elaborates the fitness function of the GA module, based upon the synchronicity of specific pattern in the selected individuals. Several computational simulations on multidimensional knapsack problems show us the effectiveness of the proposed method. </p...|$|R
40|$|Learning Bayesian network (BN) {{structure}} {{from data}} {{is a typical}} NP-hard problem. But almost existing algorithms have the very high complexity {{when the number of}} variables is large. In order to solve this problem(s), we present an algorithm that integrates with a decomposition-based approach and a scoring-function-based approach for learning BN structures. Firstly, the proposed algorithm decomposes the moral graph of BN into its maximal prime subgraphs. Then it orientates the local edges in each subgraph by the K 2 -scoring <b>greedy</b> <b>searching.</b> The last step is combining directed subgraphs to obtain final BN structure. The theoretical and experimental results show that our algorithm can efficiently and accurately identify complex network structures from small data set...|$|R
40|$|Machine Learning as Massive Search by Richard B. Segal Chairperson of Supervisory Committee: Associate Professor Oren Etzioni Department of Computer Science and Engineering Machine {{learning}} is the inference of general patterns from data. Machine-learning algorithms search large spaces of potential hypotheses for {{the hypothesis that}} best fits the data. Since the search space for most induction problems grows exponentially {{in the number of}} features used to describe the data, most induction algorithms use <b>greedy</b> <b>search</b> to minimize search cost. <b>Greedy</b> <b>search</b> is a polynomial-time algorithm that achieves its efficiency by exploring {{only a tiny fraction of}} all hypotheses. While <b>greedy</b> <b>search</b> has good performance, it often misses the best hypotheses. This thesis proposes massive search as an alternative to <b>greedy</b> <b>search.</b> Massive search aggressively searches as many hypotheses as possible in the time available. Since massive search explores a larger portion of the hypothesis space, it is less [...] ...|$|E
40|$|For {{compressed}} sensing, in {{the framework}} of <b>greedy</b> <b>search</b> reconstruction algorithms, we introduce the notion of initial support-set. The initial support-set is an estimate given to a reconstruction algorithm to improve the performance of the reconstruction. Furthermore, we classify existing <b>greedy</b> <b>search</b> algorithms as being serial or parallel. Based on this classification and the goal of robustness to errors in the initial support-sets we develop a new <b>greedy</b> <b>search</b> algorithm called FROGS. We end the paper with careful numerical experiments concluding that FROGS perform well compared to existing algorithms (both in terms of performance and execution time) and that it is robust against errors in the initial support-set. QC 20121108 </p...|$|E
3000|$|... [...]. The most {{fundamental}} approach for solving {{this problem is}} a maximum likelihood-based <b>greedy</b> <b>search</b> approach, in which estimates are made by scanning the frequency and the transition time simultaneously, and finding {{the values of the}} frequency and time that maximize (8). However, the <b>greedy</b> <b>search</b> approach has an extremely large computational complexity although it may yield the optimal solution in some sense.|$|E
40|$|We {{present a}} {{data-driven}} approach that synthesizes tree animations from {{a set of}} pre-computed motion data. Our approach improves previous motion synthesis algorithms for character animation in several aspects. We first introduce a simple yet effective sampling scheme to generate a rich and reusable motion database for each tree model. We also propose a novel technique to generate a fine set of transitions that are uniformly distributed in the motion database. The transition lengths are adaptively determined according to the similarity of the transiting frame pairs. In the runtime, we employ a <b>greedy</b> <b>searching</b> algorithm to synthesize smooth tree animations under an adjustable wind condition. Experimental results show that our approach achieves comparable quality to physically based methods, while in orders of magnitude faster performance. Copyright...|$|R
40|$|C 4. 5 {{algorithm}} {{is the most}} widely used algorithm in the decision trees so far and obviously the most popular heuristic function is gain ratio. This heuristic function has a serious disadvantage – towards dealing with irrelevant featured data sources. The hill climbing is a machine learning technique used in searching. It has good searching mechanism. Considering the relationship between hill climbing and <b>greedy</b> <b>searching,</b> it can be used as the heuristic function of decision tree, in order to overcome the disadvantage of gain ratio. This paper proposes a composite splitting criterion equal to a greedy hill climbing approach and gain ratio. The experimental results shown that the proposed new heuristic function can Scale up accuracy, especially when processing high dimension datasets...|$|R
40|$|In this paper, a new mixed integer {{mathematical}} programme {{is proposed}} {{for the application of}} Hub Location Problems (HLP) in public transport planning. This model is among the few existing ones for this application. Some classes of valid inequalities are proposed yielding a very tight model. To solve instances of this problem where existing standard solvers fail, two approaches are proposed. The first one is an exact accelerated Benders decomposition algorithm and the latter a <b>greedy</b> neighborhood <b>search.</b> The computational results substantiate the superiority of our solution approaches to existing standard MIP solvers like CPLEX, both in terms of computational time and problem instance size that can be solved. The <b>greedy</b> neighborhood <b>search</b> heuristic is shown to be extremely efficient...|$|R
40|$|Weighted A * is {{the most}} popular {{satisficing}} algorithm for heuristic search. Although there is no formal guarantee that increasing the weight on the heuristic cost-to-go estimate will decrease search time, it is commonly assumed that increasing the weight leads to faster searches, and that <b>greedy</b> <b>search</b> will provide the fastest search of all. As we show, however, in some domains, increasing the weight slows down the search. This has an important consequence on the scaling behavior of Weighted A*: increasing the weight ad infinitum will only speed up the search if <b>greedy</b> <b>search</b> is effective. We examine several plausible hypotheses as to why <b>greedy</b> <b>search</b> would sometimes expand more nodes than A * and show that each of the simple explanations has flaws. Our contribution is to show that <b>greedy</b> <b>search</b> is fast if and only if there is a strong correlation between h(n) and d ∗ (n), the true distance-to-go, or if the heuristic is extremely accurate...|$|E
40|$|Thesis (Ph. D.) [...] University of Washington, 1997 Machine {{learning}} is the inference of general patterns from data. Machine-learning algorithms search large spaces of potential hypotheses for {{the hypothesis that}} best fits the data. Since the search space for most induction problems grows exponentially {{in the number of}} features used to describe the data, most induction algorithms use <b>greedy</b> <b>search</b> to minimize search cost. <b>Greedy</b> <b>search</b> is a polynomial-time algorithm that achieves its efficiency by exploring {{only a tiny fraction of}} all hypotheses. While <b>greedy</b> <b>search</b> has good performance, it often misses the best hypotheses. This thesis proposes massive search as an alternative to <b>greedy</b> <b>search.</b> Massive search aggressively searches as many hypotheses as possible in the time available. Since massive search explores a larger portion of the hypothesis space, it is less likely to miss good hypotheses. This thesis develops a massive-search algorithm for rule learning called Brute. Experiments with Brute show that massive search is both practical and effective. Brute can completely search the hypothesis spaces of most benchmark problems in only a few minutes. Brute learns better rules than <b>greedy</b> <b>search</b> on 13 of 18 databases, while performing equally well on the remaining five. We demonstrate massive search's wide applicability by extending Brute to handle data-mining and classification problems with comparable results...|$|E
40|$|The Weapon-Target Assignment (WTA) {{problem is}} one of the most {{important}} problems of military applications of operations research. The objective of the WTA problem is to find proper assignments of weapons to targets which minimize the expected damage of defensive side. The WTA problem is known to be NP-complete. In this paper, hybrid Nested Partitions (NP) method is proposed to solve WTA problems. The proposed algorithm is named as 2 ̆ 2 Hybrid NP method with intelligent <b>greedy</b> <b>search</b> 2 ̆ 2. The NP method has been found to be very effective for solving complex large-scale discrete optimization problems. In addition to that, due to the inherent flexibility of the NP method, any other heuristic for generating good feasible solutions can be incorporated and improve the performance of the NP method. The intelligent <b>greedy</b> <b>search</b> is an improved version of <b>greedy</b> <b>search</b> which finds good solutions very quickly. The proposed algorithm combines the advantages of the NP method and intelligent <b>greedy</b> <b>search.</b> The simulation results show that the proposed algorithm is very efficient for solving the WTA problem...|$|E
30|$|GRASP-ELS: {{the hybrid}} {{algorithm}} of Labadie et al. (2011) that combines <b>greedy</b> randomized adaptive <b>search</b> procedure with evolutionary local search.|$|R
3000|$|Therefore, {{some kind}} of {{post-processing}} is required that checks for possible improvements of sum rate R. After the initial (greedy) assignment, the improved <b>Greedy</b> CA <b>searches</b> for subcarriers that should be reassigned to other source nodes. A subcarrier will be moved to a different allocation set [...]...|$|R
5000|$|The A* search {{algorithm}} {{is an example}} of best-first search, as is B*. Best-first algorithms are often used for path finding in combinatorial search. (Neither A* nor B* is a <b>greedy</b> best-first <b>search</b> as they incorporate the distance from start in addition to estimated distances to the goal.) ...|$|R
3000|$|A robust blind {{frequency}} and timing estimation algorithm is developed for frequency hopping systems. The proposed scheme {{has a lower}} computational load than the ML-based <b>greedy</b> <b>search</b> algorithm. The multivariable search problem is reduced to a single variable search problem. The algorithm {{does not require the}} simultaneous search of all times and frequencies, and its performance is comparable with that of the ML-based <b>greedy</b> <b>search</b> algorithm. The problem of unbalanced situations (where [...]...|$|E
30|$|<b>Greedy</b> <b>search</b> is deterministic, as {{it always}} greedily selects the best next node, but there exists a variety of {{stochastic}} variations [39]. In addition to the deterministic approach, we also evaluated all simulations with an ϵ-greedy approach, in which the next node is selected uniformly at random with a random chance of 5 %, thus modeling a degree of uncertainty. However, this only led to minor changes in the results, and for sake of brevity, we only report the results for deterministic <b>greedy</b> <b>search.</b>|$|E
40|$|Techniques for robot {{monitoring}} and diagnosis {{have been developed}} that perform state estimation using probabilistic hybrid discrete/continuous models. Exact inference in hybrid dynamic systems is, in general, intractable. Approximate algorithms are based on either 1) <b>greedy</b> <b>search,</b> {{as in the case}} of k-best enumeration or 2) stochastic search, {{as in the case of}} Rao-Blackwellised Particle Filtering (RBPF). In this paper we propose a new method for hybrid state estimation. The key insight is that stochastic and <b>greedy</b> <b>search</b> methods, taken together, are often particularly effective in practice. The new method combines the stochastic methods of RBPF with the <b>greedy</b> <b>search</b> of k-best in order to create a method that is effective for a wider range of estimation problems than the individual methods alone. We demonstrate this robustness on a simulated acrobatic robot, and show that this benefit comes at only a small performance penalty...|$|E
40|$|Abstract – In {{the last}} years {{quite a lot of}} {{algorithms}} concerning frequent graph pattern mining have been published. In this paper an overview on the different methods for graph data mining is given, starting with the <b>greedy</b> <b>searches</b> proposed {{in the middle of the}} ninties. The ILP-based approaches are taken into account as well as ideas influenced by basket analyses proposed lately. A remaining question is how the different approaches can be tailored to meet the needs for mining molecules. In this area special problems occur as molecules are not just “normal arbitrary graphs”. There are structures that are typical and frequent as rings and chains, some node types resp. atoms occur more often than others. It is an unsolved question how chemically isomorphic mining can be handled...|$|R
40|$|We {{consider}} structure {{learning of}} linear Gaussian structural equation models with weak edges. Since {{the presence of}} weak edges {{can lead to a}} loss of edge orientations in the true underlying CPDAG, we define a new graphical object that can contain more edge orientations. We show that this object can be recovered from observational data under a type of strong faithfulness assumption. We present a new algorithm for this purpose, called aggregated <b>greedy</b> equivalence <b>search</b> (AGES), that aggregates the solution path of the <b>greedy</b> equivalence <b>search</b> (GES) algorithm for varying values of the penalty parameter. We prove consistency of AGES and demonstrate its performance in a simulation study and on single cell data from Sachs et al. (2005). The algorithm will be made available in the R-package pcalg. Comment: 18 pages, 17 figures, UAI 201...|$|R
40|$|We {{consider}} {{the problem of}} combining a <b>greedy</b> motif <b>search</b> algorithm [16] with a self-adapting genetic algorithm employing multiple genomic representations {{in order to find}} high scoring substring patterns of size k in a set of t DNA sequences of size n thus improving the results of a stand-alone <b>greedy</b> motif <b>search.</b> We propose using multiple genomic (redundant) representations in a self-adapting genetic algorithm (GA) employing various codes with different locality properties. These encoding schemes insure feasibility after performing the operations of crossover and mutation and also ensure the feasibility of the initial randomly generated population (i. e., generation 0). The GAs applied in solving this problem employ non-locality or locality representations when appropriate (i. e., the GA adapts to its current search needs) which makes the GAs more efficient [15]...|$|R
