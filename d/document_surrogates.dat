40|18|Public
25|$|An {{object is}} an entity that is {{represented}} by information in a database. User queries are matched against the database information. Depending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by <b>document</b> <b>surrogates</b> or metadata.|$|E
50|$|Depending on the {{application}} the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by <b>document</b> <b>surrogates</b> or metadata.|$|E
5000|$|There {{are several}} {{scientific}} papers {{that use the}} term to describe the browsing of [...] "graphical representations" [...] of documents. In this context [...] "metabrowsing" [...] refers to a high-level way of browsing through information: instead of browsing through document contents or <b>document</b> <b>surrogates,</b> the user browses through a graphical representation of the documents and their relations to the domain.|$|E
50|$|Little Man (stylized {{as little}} man) is a 2005 American {{documentary}} film by Nicole Conn. The film {{was intended to}} <b>document</b> <b>surrogate</b> pregnancy but the baby was delivered 100 days early so the film documents experiences of a family dealing with an extremely premature birth involving 158 days in a NICU (neonatal intensive care unit).|$|R
40|$|Abstract. Documents are {{unstructured}} data consisting of natural language. <b>Document</b> <b>surrogate</b> means the structured data converted from original documents to process them in computer systems. <b>Document</b> <b>surrogate</b> is usually represented into {{a list of}} words. Because not all words in a document reflect its content, {{it is necessary to}} select imp ortant words related with its content among them. Such important words are called keywords and they are selected with a particular equation based on TF (Term Frequency) and IDF (inverted Document Frequency). Actually, not only TF and IDF but also the position of each word in the document and the inclusion of the word in the title should be considered to select keywords among words contained in the text. The equation based on these factors gets too complicate to be applied to the selection of keywords. This paper proposes the neural network model, back propagation, in which these factors are used as the features and feature vectors are generated, and with which keywords are selected. This paper will show that backpropagation outperforms the equation in distinguishing keywords. ...|$|R
40|$|We {{present the}} design of a {{visualization}} tool that graphically displays the strength of query concepts in the retrieved documents. Graphically displaying <b>document</b> <b>surrogate</b> information enables set-at-a-time perusal of documents, rather than document-at-a-time perusal of textual displays. By prvialing additional relevance information about the retrieved documents, the tool aids the user in accurately identifying relevant documents. Results of an experiment evaluating the tool shows that when users have the tool they are able to identify relevant documents in a shorter period of time than without the tool, and with increased accuracy. We have evidence to believe that appropriately designed graphical displays can enable users to better interact with the system. ...|$|R
50|$|An {{object is}} an entity that is {{represented}} by information in a database. User queries are matched against the database information. Depending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by <b>document</b> <b>surrogates</b> or metadata.|$|E
40|$|We {{introduce}} {{the notion of}} time-centered snippets, called TSnippet, as <b>document</b> <b>surrogates</b> for document retrieval and exploration. We propose an alternative document snippet based on temporal {{information that can be}} useful for supporting exploratory search. The idea of using sentences that contain the most frequent chronons (units of time) can be used for constructing <b>document</b> <b>surrogates.</b> We conducted a series of experiments to evaluate this new approach using a crowdsourcing approach. The evaluation against two Web search engines shows that our technique produces good snippets and users like to see time-sensitive information in search results...|$|E
40|$|Most web {{search engines}} use a list-based {{representation}} of web search results, promoting a sequential {{evaluation of the}} <b>document</b> <b>surrogates.</b> Commonly, these search engines only display ten <b>document</b> <b>surrogates</b> per page, limiting the users ’ ability to explore the search results. We have devel-oped two systems to support the visual exploration of web search results: HotMap and Concept Highlighter. In both of these systems, the search results are provided at two lev-els of detail: an overview map that provides a compact and abstract representation of the top 100 documents returned by the underlying search engine; and a detail window that shows 20 to 25 documents at a time. In this paper, we will discuss how these coordinated views support the visual ex-ploration of web search results. ...|$|E
40|$|Suggestions {{and ideas}} for {{acquiring}} <b>documents</b> or their <b>surrogates</b> for a planned or fledgling information system are offered. The problems of selectivity of <b>documents</b> or their <b>surrogates,</b> both in superabundant quantities, and duplicate checking are highlighted. Acquisitioning flow, a semiautomated duplicate search technique, and alerting methods for prospective documentation are described. Appendices include two category systems, selected definitions and acronyms, and a selected address list for document procurement...|$|R
40|$|As {{outlined}} in the Association of College and Research Libraries (ACRL) report Value of Academic Libraries: A Comprehensive Research Review and Report([URL] Helmke Library is <b>documenting</b> <b>surrogate</b> measures to demonstrate its value to students, faculty and staff on an academic campus and to the community in general. We are documenting and leveraging opportunities and partnerships to enhance awareness and use of library resources and services and most importantly, enhance and publicize the varied skills and contributions of the library’s staff and librarians. Using a simple spreadsheet, we are documenting these outreach and collaborative activities as well as librarians’ involvement in campus committees and projects in a Library Collaboration Index. This document, which lists the Activity, Audience, Contact Hours and other information, describes and quantifies the library’s impact, value and integration with campus and community activities...|$|R
40|$|Automatic {{document}} summarization is {{a problem}} of creating a <b>document</b> <b>surrogate</b> that adequately represents the full document content. We aim at a summarization system that can replicate the quality of summaries created by humans. In this paper we investigate the machine learning method for extracting full sentences from documents based on the document semantic graph structure. In particular, we explore how the Support Vector Machines (SVM) learning method is affected by the quality of linguistic analyses and the corresponding semantic graph representations. We apply two types of linguistic analysis: (1) a simple part-of-speech tagging of noun phrases and verbs and (2) full logical form analysis which identifies Subject-Predicate-Object triples, and then build the semantic graphs. We train the SVM classifier to identify summary nodes and use these nodes to extract sentences. Experiments with the DUC 2002 and CAST datasets show that the SVM based extraction of sentences does not differ significantly for the simple and the sophisticated syntactic analysis. In both cases the graph attributes used in learning are essential for the classifier performance and the quality of extracted summaries...|$|R
40|$|It {{is common}} for web searchers to have {{difficulties}} crafting queries to fulfill their information needs. Even when they provide a good query, users often find it challenging to evaluate {{the results of their}} web searches. Sources of these problems include the lack of support for query refinement, and the static nature of the list-based representations of web search results. To address these issues, we have developed WordBars, an interactive tool for web information retrieval. WordBars visually represents the frequencies of the terms found in the first 100 <b>document</b> <b>surrogates</b> returned from the initial query. This system allows the users to interactively re-sort the search results based on the frequencies of the selected terms within the <b>document</b> <b>surrogates,</b> as well as to add and remove terms from the query, generating a new set of search results. Examples illustrate how WordBars can provide valuable support for query refinement and search results exploration, both when specific and vague initial queries are provided...|$|E
40|$|This is {{the report}} of a project {{supported}} by the Council on Library Resources. The four project objectives are: 1. To identify and retrieve published literature on the library of the future. 2. To formulate <b>document</b> <b>surrogates</b> for this literature and add them to a computerized database. 3. To generate an analytical bibliography of published library of the future literature. And 4. To synthesize literature in the bibliography with a thinkpiece on the library of the future...|$|E
40|$|Perusal {{of textual}} {{displays}} of <b>document</b> <b>surrogates</b> produced by Web-based ranked-output retrieval services may require much user time, effort, and money. In this paper we present VIEWER, a graphical interface that allows visualization and manipulation of views of retrieval results, where a view is the subset of retrieved surrogates that contain a specified subset of query terms. We argue that VIEWER helps the user focus on relevant {{parts of the}} results and, in addition, it may facilitate query reformulation. We present an experimental evaluation in which VIEWER, used as an interactive ranking systems, outperforms both best match ranking and coordination level-based ranking...|$|E
40|$|Presentation {{of search}} results in Web-based {{information}} retrieval (IR) systems {{has been dominated}} by a textual form of information such as the title, snippet, URL, and/or file type of retrieved documents. On the other hand, document’s visual aspects such as the layout, colour scheme, or presence of images have been studied in a limited context with regard to their effectiveness of search result presentation. This paper presents a comparative evaluation of textual and visual forms of document summaries as the additional <b>document</b> <b>surrogate</b> in the search result presentation. In our study, a sentence-based summarisation technique was used to create a textual document summary, and the thumbnail image of web pages was used to represent a visual summary. The experimental results suggest that both have the cases where the additional elements contributed to a positive effect not only in users’ relevance assessment but also in query re/formulation. The results also suggest that the two forms of document summary are likely to have different contexts to facilitate user’s search experience. Therefore, our study calls for further research on adaptive models of IR systems to make use of their advantages in appropriate contexts...|$|R
40|$|A {{surrogate}} is {{an object}} that stands for a document and enables navigation to that document. Hypermedia is often represented with textual surrogates, even though {{studies have shown that}} image and text surrogates facilitate the formation of mental models and overall understanding. Surrogates may be formed by breaking a document down into a set of smaller elements, each of which is a surrogate candidate. While processing these surrogate candidates from an HTML document, relevant information may appear together with less useful junk material, such as navigation bars and advertisements. This paper develops a pattern recognition based approach for eliminating junk while building the set of surrogate candidates. The approach defines features on candidate elements, and uses classification algorithms to make selection decisions based on these features. For the purpose of defining features in surrogate candidates, we introduce the <b>Document</b> <b>Surrogate</b> Model (DSM), a streamlined Document Object Model (DOM) -like representation of semantic structure. Using a quadratic classifier, we were able to eliminate junk surrogate candidates with an average classification rate of 80 %. By using this technique, semiautonomous agents can be developed to more effectively generate surrogate collections for users. We end by describing a new approach for hypermedia and the semantic web, which uses the DSM to define value-added <b>surrogates</b> for a <b>document...</b>|$|R
50|$|Assessment include psychiatric, {{psychological}} and social functioning, risks posed to the individual and others, problems required to address from any co-morbidity, personal circumstances including family or other carers. Other factors are the person's housing, financial and occupational status and physical needs. Assessments when categorized, it particularly includes Life history of the client that include data collection of living situation and finances, social history and supports, family history, coping skills, religious/cultural factors, trauma from systemic issues or abuse and medico-legal factors (Assess' client’s awareness of legal <b>documents,</b> <b>surrogate</b> decision-making, power of attorney and consent). Components that include in resource assessment of the client include psycho-spiritual strengths; substance abuse; coping mechanisms, styles and patterns (Individual, family level, workplace, and use of social support systems); sleeping pattern; needs and impacts of the problem etc. Advanced clinician's incorporate individual scales, batteries and testing instruments in their assessments. In the late 1980s by Hans Eysenck through an issue of Psychological Inquiry raised controversies on then assessment methods and it gave way to comprehensive Bio-Psycho-Social assessment, this theoretical model states behavior {{as a function of}} biological factors, psychological issues and the social context. Qualified healthcare professionals conduct physiological part of these assessments. This thrust on biology expands the field of approach for the client with the client through the interaction of these disciplines and in a domain that mental illnesses are also physical, just as physical conditions have mental components. Likewise, the emotional is both {{psychological and}} physical.|$|R
40|$|Abstract. The {{interaction}} model supported by web search engines has changed very little {{since the early}} days of web search. Users are required to formulate their queries with very little support from the system, and are provided with a list-based representation of the web search results that promotes a sequential evaluation of the <b>document</b> <b>surrogates.</b> The short queries used by web searchers, and the few pages viewed as a re-sult of a web search are indications of the inadequate support provided for the users ’ information retrieval tasks. We propose a model for web information retrieval that uses visualization and interactive visual ma-nipulation to support the users as they take an active role in satisfying their information needs. ...|$|E
40|$|Abstract: Perusal {{of textual}} {{displays}} of <b>document</b> <b>surrogates</b> produced by Web-based rankedoutput retrieval services may require much user time, effort, and money. In this paper we present VIEWER, a graphical interface that allows visualization and manipulation of views of retrieval results, where a view is the subset of retrieved surrogates that contain a specified subset of query terms. We argue that VIEWER helps the user focus on relevant {{parts of the}} results and, in addition, it may facilitate query reformulation. We present {{the results of an}} experiment performed by six subjects on two medium size bibliographical test collections in which VIEWER, used as an interactive ranking systems, outperformed both best match ranking and coordination level-based ranking...|$|E
40|$|This paper {{examines}} the problems involved in subject retrieval from full-text databases of secondary {{materials in the}} humanities. Ten such databases were studied and their search functionality evaluated, focusing on factors such as Boolean operators, <b>document</b> <b>surrogates,</b> limiting by subject area, proximity operators, phrase searching, wildcards, weighting of search terms, limiting by type of document, controlled vocabulary indexing and ranking, and display of search results. The author suggests ways in which full-text searching might be improved, whether by enhancement of database records, by introduction of enhanced search functionality, or by the education of searchers in more effective search techniques. The conclusion is that current digitisation projects are not producing databases that {{meet the needs of}} scholars...|$|E
40|$|Okapi BM 25 scoring of anchor text <b>surrogate</b> <b>documents</b> {{has been}} shown to {{facilitate}} e#ective ranking in navigational search tasks over web data. We hypothesize that even better ranking can be achieved in certain important cases, particularly when anchor scores must be fused with content scores, by avoiding length normalisation and by reducing the attentuation of scores associated with high tf. Preliminary results are presented...|$|R
40|$|Improving rankings in {{small-scale}} web search using click-implied descriptions Abstract When a searcher submits a query Q and clicks on document R in {{the corresponding}} result set, we may plausibly interpret the click as a vote that Q is {{a description of}} R. We call the Q and R pairing a ‘click description’. Click descriptions thus derived from search engine logs can be accumulated into <b>surrogate</b> <b>documents</b> and used to boost retrieval effectiveness {{in a similar fashion}} to anchor text. We investigate the usefulness of click description <b>surrogate</b> <b>documents</b> in processing queries for an external web site search service for four organisations. Using the mean reciprocal rank of best answers as the measure of performance, we show that, for popular queries, click description surrogates significantly outperform both anchor text surrogates and the original proprietary rankings. The amount of click data needed to achieve a high level of retrieval performance is surprisingly small for popular queries. Thanks to terms shared between queries, click description surrogates can answer queries for which no specific click data is available. We show a 92 % improvement due to this effect for a set of lengthy, less popular queries. We also discuss issues such as spam rejection, unpopular queries, and how to combine click description scores with other evidence. We argue the potential of click descriptions in non-web applications where link and anchor text evidence is unavailable...|$|R
40|$|Social robots need to {{understand}} the affective state of the humans with whom they interact. Successful interactions require understanding mood and emotion in the short term, and personality and attitudes over longer periods. Social robots should also be able to infer the desires, wishes, and preferences of humans without being explicitly told. We investigate how effectively affective state can be inferred from corpora in which <b>documents</b> are plausible <b>surrogates</b> for what a robot might hear. For mood, emotions, wishes, desires, and attitudes we show highly ranked documents; for personality dimensions, estimates of ground truth are available and we report performance accuracy. The results are surprisingly strong given the limited information in short documents...|$|R
40|$|The organisation, {{content and}} {{presentation}} of <b>document</b> <b>surrogates</b> has a substantial {{impact on the}} effectiveness of web search result interfaces. Most interfaces include textual information, including for example the document title, URL, and a short query-biased summary of the content. Other interfaces include additional browsing features, such as topic clustering, or thumbnails of the web pages. In this study we analyse three search interfaces, and compare the effectiveness of textual information and additional browsing features. Our analysis indicates that most users spend a substantially larger proportion of time looking at text information, and that those interfaces that focus on text-based representations of document content tend to lead to quicker task completion times for named-page finding search tasks...|$|E
40|$|Collection sizes, query rates, and {{the number}} of users of Web search engines are increasing. Therefore, there is {{continued}} demand for innovation in providing search services that meet user information needs. In this article, we propose new techniques to add additional terms to documents with the goal of providing more accurate searches. Our techniques are based on query association, where queries are stored with documents that are highly similar statistically. We show that adding query associations to documents improves the accuracy of Web topic finding searches by up to 7 %, and provides an excellent complement to existing supplement techniques for site finding. We conclude that using <b>document</b> <b>surrogates</b> derived from query association is a valuable new technique for accurate Web searching...|$|E
40|$|The {{presentation}} of search {{results on the}} web {{has been dominated by}} the textual form of document representation. On the other hand, the document’s visual aspects such as the layout, colour scheme, or presence of images have been studied in a limited context with regard to their effectiveness of search result presentation. This article presents a comparative evaluation of textual and visual forms of document representation as additional components of <b>document</b> <b>surrogates.</b> A total of 24 people were recruited for our task-based user study. The experimental results suggest that an increased level of document representation available in the search results can facilitate users’ interaction with a search interface. The results also suggest that the two forms of additional representations are likely beneficial to users’ information searching process in different contexts...|$|E
40|$|Along {{with the}} {{explosive}} growth of the Web has come a great increase in on-line scholarly literature. More and moNot only is Web retrieval usually faster than a walk to the library, but the information obtained from the Web is generally more current than what appears in printed publications. The increasing proportion of on-line scholarly literature {{makes it possible to}} implement functionality desirable to all researchers [...] the ability to access cited documents immediately from the citing paper. Implementing this direct access is called "reference linking". While many authors insert explicit hyperlinks into their papers to support reference linking, {{it is by no means}} a universal practice. The approach taken by the Digital Library Research Group at Cornell employs "value-added surrogates" as a generalizable mechanism for providing reference-linking behavior in Web documents. Given the URL of an on-line <b>document,</b> a <b>surrogate</b> object is constructed for that paper, which then processes document's content to extract reference linking data. The surrogate then exposes the reference linking data through a well-defined API, permitting the construction of reference linking services by external clients. We present two examples of the many possible reference linking applications buildable on this API. We also describe a metric that measures the API's performance; currently we are (automatically) extracting reference linking information from HTML papers with more than 8...|$|R
40|$|We {{describe}} {{a method for}} predicting query difficulty in a precision-oriented web search task. Our approach uses visual features from retrieved <b>surrogate</b> <b>document</b> representations (titles, snippets, etc.) to predict retrieval effectiveness for a query. By training a supervised machine learning algorithm with manually evaluated queries, visual clues indicative of relevance are discovered. We show that this approach has a moderate correlation of 0. 57 with precision at 10 scores from manual relevance judgments of the top ten documents retrieved by ten web search engines over 896 queries. Our findings indicate that difficulty predictors which {{have been successful in}} recall-oriented ad-hoc search, such as clarity metrics, are not nearly as correlated with engine performance in precision-oriented tasks such as this, yielding a maximum correlation of 0. 3. Additionally, relying only on visual clues avoids the need for collection statistics that are required by these prior approaches. This enables our approach to be employed in environments where these statistics are unavailable or costly to retrieve, such as metasearch...|$|R
40|$|This {{working paper}} about {{preservation}} and conservation of bibliographic materials, emphasizes all the special considerations {{that are important}} in planning to prevent disasters in collection of academic libraries. When materials aren't available due to deterioration, missing pages, disconnected covers, or other problems, it can be frustrating for users and librarians alike. We must know the answer that can care for the collection from the outset, while also guiding staff on making needed repairs. In Preservation and Conservation, the experts show library administrators and decision makers the optimal collection preservation techniques, what it takes to set up a conservation work area, and safe ways to mount a small exhibit at the library. We analyse the biological attack in books and numbers of journals which date of publication is XVIIIth century. We have also considered the effects of the variation of temperature and humidity that could produce the deterioration of the paper and the covers. We must analyse the benefits and problems of digitising different types of materials; the long-term viability of digital media; issues of access to digital <b>surrogate</b> <b>documents</b> as opposed to the original medium; and the challenges in the digital context of bibliographical control, cataloguing, metadata, distribution and copyright protection...|$|R
40|$|A {{large amount}} of {{computing}} literature has become available over the Internet, as university departments and research institutions have made their technical reports, preprints, and theses available electronically. Access to these items has been limited, however, by the difficulties involved in locating documents of interest. We describe {{a proposal for a}} New Zealand-based index of computer science technical reports, where the reports themselves are located in repositories that are distributed world-wide. Our scheme is unique in that it is based on indexing the full text of the technical reports, rather than on <b>document</b> <b>surrogates.</b> The index is constructed so as to minimize network traffic and local storage costs (of particular importance for geographically isolated countries like New Zealand, which incur high Internet costs). We also will provide support for bibliometric/scientometric studies of the computing literature and our users. 1. Introduction The migration of information fro [...] ...|$|E
40|$|In most {{information}} retrieval systems, {{especially those in}} the Web, users suffer from too many search results force them to go through a long list of <b>document</b> <b>surrogates</b> that may not reveal their contents sufficiently. This paper describes our attempt to provide a solution to this problem: a new visual interface for search results and a system architecture that supports the visualization technique. The visual interface is evaluated for its efficacy with human subjects. Our interface, DART, displays document clustering information and the similarity values between individual documents and different sub-queries. With the distinguishing colors and brightness of the icons representing documents scattered on a series of concentric circles resembling a dart target, users can make intuitive judgment as to which one they want to read next. Based on our experiments, users acquire relevant documents more quickly with the DART display than with the conventional document list. 1. Introduction I [...] ...|$|E
40|$|With {{the rapid}} {{expansion}} of scientific research, the ability to effectively find or integrate new domain knowledge in the sciences is proving increasingly difficult. The development of methods and tools for assisting researchers to effectively extract problem-oriented knowledge from heterogeneous and massive information sources, and for using this knowledge in problem-solving {{is one of the most}} fundamental research directions for the information and computer sciences today. There is a need for new tools to support more precise identification of relevant research articles and provide visual clues regarding relationships among the document sets. We present the Telemakus system in which aggregated citation information and extracted research findings are displayed in a schema-based document surrogate and an interactive mapping tool provides graphical displays of research interrelationships from documents across a domain. This system is an innovative approach to creating useful and precise <b>document</b> <b>surrogates</b> and may re-conceptualize the way we currently represent, retrieve, and assimilate research findings from the published literature. Keywords...|$|E
40|$|This paper {{describes}} and evaluates a retrieval scheme, or {{more precisely}} an additional retrieval mechanism based on interdocument relationships, that can be integrated in almost all existing retrieval schemes (e. g., Boolean, hybrid Boolean, vector-processing or probabilistic models). The intent of our approach consists of inferring knowledge about document contents based on the relevance assessments of past queries. Through a learning process, our scheme establishes relevance links between documents found relevant for the same request. Based on this information {{and a list of}} retrieved records for the current request, the proposed mechanism tries to improve the ranking of the retrieved items in a sequence most likely to satisfy user intent. The underlying hypothesis of this mechanism states that future requests addressed to the system should have some degree of similarity with previous queries, or that the retrieval apparatus will process requests for which it has already found a partial, appropriate answer in the past. Participation: Category: B Query: ad-hoc, fully automatic Introduction To find pertinent information from a large text collection, most retrieval models represent both documents and requests by a set of weighted keywords. To extract relevant records from this collection, the retrieval function computes a similarity value or estimates a probability of relevance based on both <b>document</b> and query <b>surrogates.</b> When applying such a scheme, the system considers documents as separate entities. To relax this assumption, some studies have proposed various techniques and have reported evaluations describing the importance of interdocument relationships (e. g., [Kwok 88], [Turtle 91]). Our main research objective is also to analyze and assess interdocument relationshi [...] ...|$|R
40|$|Recent {{advances}} in technology have provided alternative solutions and approaches to everyday tasks. One of which is in education, where the learning process is no longer confined to conventional classrooms. E-learning or distance learning are now a popular alternative which {{involves the use of}} learning objects for teachers to convey instructional content. These learning objects are accessible to students on a digital repository and can come in many different forms. Recently multimedia files such as videos are also used. In order to enable efficient indexing and retrieval of these videos, they must be encapsulated into effective media learning objects. Although many querying methods exist, users are most accustomed to query by keyword methods, giving text based queries to retrieve corresponding videos. Generally these keywords are selected manually, but this method is not favorable because manual annotation is restrictive to a set of words, subjective to the annotator, and overall a labor intensive process. This research explores semantic keyword selection methods for automatic video annotation. Cross document annotation is used to extract potential keywords by taking into consideration <b>surrogate</b> <b>documents,</b> e. g. transcript, slides, lecture notes, etc. These potential keywords are then refined based on a set of preselected seed words in order to obtain highly related keywords, based on WordNet and visualness similarity scores. Three novel objective scoring methods are proposed to select top-ranking keywords based on visualness similarity and word sense disambiguation. These developed methods are then evaluated based on questionnaire responses of selected keywords for a set of videos. The three developed objective scoring methods correlate well with the scores of the subjectives responses and generally outperform the traditional term frequency inverse document frequency (TF-IDF) method. The proposed LVD-F method obtains the highest precision and recall of all...|$|R
40|$|The Emissions Database for Global Atmospheric Research EDGAR, version 4 {{is managed}} at a JRC-server and {{documented}} with an emipedia website. The final resulting datasets of sector-specific emission inventories for all world {{countries and the}} resulting gridmaps have been published for all greenhouse gases and air pollutants and PM 10 particulate matter on edgar. jrc. ec. europa. eu. These are accompanied so far with a description online, which is then worked out in more detail in the EDGAR Manual series. A {{first part of the}} EDGAR Manual series is this report "Gridding: EDGAR emissions distribution on global gridmaps". The report <b>documents</b> all geospatial <b>surrogates</b> that were collected, mainly from publicly available information sources, which were manipulated to arrive to a full set of consistent proxy data with spatial resolution of 0. 1 degx 0. 1 deg. The set of a hundred proxy datasets are applied to all EDGARv 4 emissions yielding the sector-specific global emission gridmaps for all substances. Except for population and the derived urban and rural population proxy datasets, the proxy do not change over time. The population proxy, taken from CIESIN is an important default for the gridding and changes over time, reflecting the migration of people. More in particular the urban population proxy, derived in-house based on a combination of population and urban settlements, the urbanisation process {{over the past two decades}} is taken up. All spatial datasets are two-dimensional, covering the globe, except for aviation. The airline distribution occurs with three distinct layers at different heights, in order to cover a minimum third dimension. The other emissions data are upon request split into two height layers, one at stack height, one at ground level, based on the type of activity. The global coverage and geospatial consistency allow to provide a consistent input to the atmospheric chemistry. In particular multipollutant sources are represented as one single point source. The usefulness of these proxy datasets is reflected by the requests for applying it on other emission inventories, such as the UNEP emissions from IIASA or the EMEP emissions at CEIP. JRC. H. 2 -Air and Climat...|$|R
