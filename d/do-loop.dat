21|20|Public
25|$|In FORTRAN and PL/I though, the keyword DO is {{used and}} it is called a <b>do-loop,</b> but it is {{otherwise}} identical to the for-loop described here and {{is not to be}} confused with the do-while loop.|$|E
5000|$|... schedule(type, chunk): This {{is useful}} if the work sharing {{construct}} is a <b>do-loop</b> or for-loop. The iteration(s) {{in the work}} sharing construct are assigned to threads according to the scheduling method defined by this clause. The three types of scheduling are: ...|$|E
40|$|OpenMP {{has been}} marketed as THE {{emerging}} standard for shared memory parallelism (SMP). The first compiler for OpenMP {{is now available}} on the Cray Origin 2000. In this paper we report on some early experiences with this compiler on a (quasi-) application code, an implementation of the NAS, BT benchmark. OpenMP includes, of course, the traditional <b>do-loop</b> parallelization. For programmers familiar with Power Fortran this is simply a question of adapting to a slight modification of syntax. Not surprisingly the performance equals that of Power Fortran. This part is easy and straightforward. The interesting thing about OpenMP is that it goes beyond the simple <b>do-loop</b> parallelization and opens possibilities for a more course grain parallelism by splitting the programming in different independent tasks (Parallel sections) and/or creating parallel regions. Combining this with the more fine grained <b>do-loop</b> parallelism keeps the promise of higher scalability than one usually find in SMPprogramming [...] ...|$|E
40|$|Data {{distribution}} {{has been}} one of the most important research topics in parallelizing compilers for distributed memory parallel computers. Good data distribution schema should consider both the computation load balance and the communication overhead. In this paper, we show that data re-distribution is necessary for executing a sequence of <b>Do-loops</b> if the communication cost due to performing this sequence of <b>Do-loops</b> is larger than a threshold value. Based on this observation, we can prune the searching space and derive efficient dynamic programming algorithms for determining effective data distribution schema to execute a sequence of <b>Do-loops</b> with a general structure. Experimental studies on a 32 -node nCUBE- 2 computer are also presented. Keywords: component alignment, data distribution, distributed memory computer, <b>Do-loops,</b> dynamic programming algorithm for data distribution, parallelizing compiler. 1 Introduction This paper is concerned with designing efficient algorithms for data [...] ...|$|R
40|$|A set of macro {{libraries}} {{has been}} developed that allows programmers to write portable FORTRAN code for multiprocessors. This document presents, in tutorial form, the macros used to implement three common synchronization patterns: self-scheduling <b>DO-loops,</b> barrier synchronization, and the askfor monitor...|$|R
40|$|The Kendall Square Research KSR 1 MPP {{system has}} a shared address space, which spreads over {{physically}} distributed memory modules. Thus, memory access time can vary {{over a wide}} range even when accessing the same variable, depending on how this variable is being referenced and updated by the various processors. Since the processor stalls during this access time, the KSR 1 performance depends considerably on the program's locality of reference. The KSR 1 provides two novel features to reduce such long memory latencies: prefetch and post-store instructions. This paper analyzes the various memory latency factors which stalls the processor during program execution. A suitable model for evaluating these factors is developed for the execution of FORTRAN <b>DO-loops</b> parallelized with the Tile construct using the Slice strategy. The <b>DO-loops</b> used in the benchmark program perform sparse matrix-vector multiply, vector-vector dot product, and vector-vector addition, which are typically executed in an it [...] ...|$|R
40|$|Any {{high-level}} {{computer program}} can be written using just three fundamental constructs: Sequence, Selection, and Repetition. The latter forms the foundation of program automation, {{making it possible to}} execute a group of instructions repeatedly, modifying them from iteration to iteration. In SAS ® language, explicit repetition is implemented as a stand-alone structural unit via the DO loop- a powerful construct laden with a myriad of features. Many of them still remain overshadowed by the tendency to structure code around the implied loop- even when it makes the program more complex or error-prone. We will endeavor to both straighten out some such incongruities and give the sense of depth and breadth of the magnificent SAS construct known as the DO loop. In the SAS ® Language, explicit repetition is implemented via different forms of the construct known as the <b>DO-loop.</b> I. PROPAEDEUTICS 1. The Anatomy We need this brief section just to establish some anatomical <b>DO-loop</b> terminology that we will use liberally thereafter throughout the paper. Let us look at the <b>DO-loop</b> at large...|$|E
40|$|This {{paper is}} {{concerned}} with designing efficient algorithms for determining data distribution and generating communication sets on distributed memory multicomputers. First, we propose a dynamic programming algorithm to automatically determine data distribution at compiling time. The proposed algorithm also can determine whether data redistribution is necessary between two consecutive <b>DO-loop</b> program fragments. Second, we propose closed forms to represent communication sets among processing elements for executing doall statements, when data arrays are distributed in a restricted block-cyclic fashion. Our methods can be included in current compilers and used when programmers fail to provide any data distribution directives. Experimental studies on a nCUBE- 2 multicomputer are also presented. 1 Introduction Arrays distribution and communication sets generation are two problems we must solve when dealing with the compilation of <b>DO-loop</b> program fragments for distributed memory multicomputer [...] ...|$|E
40|$|To {{parallelize}} a <b>DO-loop</b> in a Fortran program, {{the independence}} of various iterations of the <b>DO-loop</b> has to be detected. Two commonly used tests to check for iteration independence are the GCD test and the Banerjee test. In this paper we introduce a new data dependence test called the Diagonal test {{that is based on}} some very simple geometric properties of straight lines and hyper-planes. We show that the Diagonal test is equivalent to Banerjee test. However, the Diagonal test is more intuitive, especially for singly-nested loops. The Diagonal test also inspires a new approach to parallelizing DO-loops by repeatedly partitioning a loop into smaller loops, and scheduling these smaller loops for parallel execution based on the data-dependence analysis. Keywords: Parallelizing Compilers, Data-dependence Analysis, Diophantine Equation, Euclidean Geometry, Scientific Computing. 1. Introduction Automatic parallelization of Fortran programs for parallel execution on multiprocessors [...] ...|$|E
40|$|Exact array {{dataflow}} {{analysis can}} be achieved in the general case if the only control structures are <b>do-loops</b> and structural ifs, and if loop counter bounds and array subscripts are affine expressions of englobing loop counters and possibly some integer constants. In this paper, we begin the study of dataflow analysis of dynamic control programs, where arbitrary ifs and whiles are allowed. In the general case, this dataflow analysis can only be fuzzy...|$|R
50|$|Aspects of the {{language}} were still being designed as PL/I F was implemented, so some were omitted until later releases. PL/I RECORD I/O was shipped with PL/I F Release 2. The list processing functions Based Variables, Pointers, Areas and Offsets and LOCATE-mode I/O were first shipped in Release 4. In a major attempt to speed up PL/I code to compete with Fortran object code, PL/I F Release 5 does substantial program optimization of <b>DO-loops</b> facilitated by the REORDER option on procedures.|$|R
40|$|STYLE is a two-pass {{special purpose}} text editor, {{specifically}} designed to reformat FORTRAN source code. STYLE accepts FORTRAN programs set up in logical segments as its data input, and produces as its output the equivalent program edited into a specific stylized subprogram system following all ANSI stnadards and coventions. Some provisions of STYLE are sequencing, editing of statement labels, indentation of <b>DO-loops,</b> spacing of text, restriction of text to column boundary, and editing of comment statements. Usage of the STYLE systen us discussed in this paper...|$|R
40|$|A fast, {{accurate}} Choleski {{method for}} {{the solution of}} symmetric systems of linear equations is presented. This direct method {{is based on a}} variable-band storage scheme and takes advantage of column heights {{to reduce the number of}} operations in the Choleski factorization. The method employs parallel computation in the outermost <b>DO-loop</b> and vector computation via the "loop unrolling" technique in the innermost <b>DO-loop.</b> The method avoids computations with zeros outside the column heights, and as an option, zeros inside the band. The close relationship between Choleski and Gauss elimination methods is examined. The minor changes required to convert the Choleski code to a Gauss code to solve non-positive-definite symmetric systems of equations are identified. The results for two large-scale structural analyses performed on supercomputers, demonstrate the accuracy and speed of the method. Nomenclature e a error norm for solution residuals e s strain energy error norm {f} load vector hpm har [...] ...|$|E
40|$|A {{simulator}} for a {{multiprocessor system}} consisting of p identical processors and m shared memories {{connected by a}} crossbar switch is presented. Its main features are a <b>DO-loop</b> unwrapper, the scheduler and the emulator. Special attention {{is given to the}} automatic partitioning of a high level program into parallel tasks. This allows the user to concentrate on different processor configurations and experience with many algorithms. The simulator takes into account communication delays and variable execution times for different operations...|$|E
40|$|A fast, {{accurate}} Choleski {{method for}} {{the solution of}} symmetric systems of linear equations is presented. This direct method {{is based on a}} variable-band storage scheme and takes advantage of column heights {{to reduce the number of}} operations in the Choleski factorization. The method employs parallel computation in the outermost <b>DO-loop</b> and vector computation via the &quot;loop unrolling &quot; technique in the innermost <b>DO-loop.</b> The method avoids computations with zeros outside the column heights, and as an option, zeros inside the band. The close relationship between Choleski and Gauss elimination methods is examined. The minor changes required to convert the Choleski code to a Gauss code to solve non-positive-definite symmetric systems of equations are identified. The results for two large-scale structural analyses performed on supercomputers, demonstrate the accuracy and speed of the method. Nomenclature ea error norm for solution residuals es strain energy error norm {f} load vector hpm hardware performance monitor (Cray) i,j,k DO loop indices ja job accounting utility (Cray) [K] stiffness matrix MFLOPS Million FLoating point OPerations/Second mij multipliers for forward substitution n number of equations NP number of processors {R} error residual for solution: [K] {x}- {f} RAM Random Access Memory SAXPY ∑ ax + y, or scalar * vector + vector second CPU time function (Cray) SRB space shuttle Solid Rocket Booster timef elapsed time function (Cray) [U] upper triangular, factored stiffness matrix uij terms of upper-triangular matrix {x} static structural displacement...|$|E
40|$|In {{this paper}} we present an {{approach}} to the static evaluation of the parallelism of <b>DO-loops</b> in an automatic parallelizing environment. The evaluation is performed from the dependence graph {{with a set of}} dependence relations between statements of the loop. The presence of control dependences due to conditional statements is also considered, evaluating in this case bounds of the loop parallelism obtained for particular branch sequences predicted at compile-time. A parallelizing compiler could use the measure of parallelism to take a priory decisions when parallelizing its associated loop. It can also be used as an absolute reference to compare different parallelization techniques...|$|R
40|$|AbstractVarious {{principles}} of proof {{have been proposed}} to reason about fairness. This paper addresses—for the first time—the question in what formalism such fairness arguments can be couched. To wit: we prove that Park's monotone first-order μ-calculus, augmented with constants for all recursive ordinals can serve as an assertion-language for proving fair termination of <b>do-loops.</b> In particular, the weakest precondition for fair termination of a loop w. r. t. some postcondition is definable in it. The relevance of this result to proving eventualities in the temporal logic formalism of Manna and Pnuelis (in “Foundations of Computer Science IV, Part 2,” Math. Centre Tracts, Vol. 159, Math. Centrum, Amsterdam, 1983) is discussed...|$|R
40|$|Various {{principles}} of proof {{have been proposed}} to reason about fairness. This paper addresses—for the first time—the question in what formalism such fairness arguments can be couched. To wit: we prove that Park's monotone first-order µ-calculus, augmented with constants for all recursive ordinals can serve as an assertion-language for proving fair termination of <b>do-loops.</b> In particular, the weakest precondition for fair termination of a loop w. r. t. some postcondition is definable in it. The relevance of this result to proving eventualities in the temporal logic formalism of Manna and Pnuelis (in 2 ̆ 2 Foundations of Computer Science IV, Part 2, 2 ̆ 2 Math. Centre Tracts, Vol. 159, Math. Centrum, Amsterdam, 1983) is discussed...|$|R
40|$|The {{results of}} {{multitasking}} {{implementation of a}} domain decomposition fast Poisson solver on eight processors of the Cray Y-MP are presented. The object {{of this research is}} to study the performance of domain decomposition methods on a Cray supercomputer and to analyze the performance of different multitasking techniques using highly parallel algorithms. Two implementations of multitasking are considered: macrotasking (parallelism at the subroutine level) and microtasking (parallelism at the <b>do-loop</b> level). A conventional FFT-based fast Poisson solver is also multitasked. The results of different implementations are compared and analyzed. A speedup of over 7. 4 on the Cray Y-MP running in a dedicated environment is achieved for all cases...|$|E
40|$|Article dans revue scientifique avec comité de lecture. The parallelization of a sequentiel {{molecular}} dynamics (MD) program using OpenMP directives for a Fortran compiler is described. The {{strength of this}} approach lies in the possibility to proceed incrementally, with no to only few alterations of the original, scalar code. If <b>do-loop</b> splitting with OpenMP is certainly less effecient than domain decomposition (DD) approaches, its fast implementation, requiring little knowledge of the source code, also involves significantly less programming effort. Efficiency of the parallel executable is probed on a distributed-shared memory Silicon Graphics Origin 2000 for MD applications of various complexities. Linear wallclock speed-up ratios using four processors, and ranging between 6. 0 and 7. 0 with eight, undeline the cost-effectiveness of OpenMP directives for fine-grained MD simulations on limited computational resources...|$|E
40|$|It {{is widely}} {{accepted}} that distributed memory parallel computers will {{play an important}} role in solving computation-intensive problems. However, the design of an algorithm in a distributed memory system is time-consuming and error-prone, because a programmer is forced to manage both parallelism and communication. In this paper, we present techniques for compiling programs on distributed memory parallel computers. We will study the storage management of data arrays and the execution schedule arrangement of <b>Do-loop</b> programs on distributed memory parallel computers. First, we introduce formulas for representing data distribution of specific data arrays across processors. Then, we define communication cost for some message-passing communication operations. Next, we derive a dynamic programming algorithm for data distribution. After that, we show how to improve the communication time by pipelining data, and illustrate how to use data-dependence information for pipelining data. Jacobi's iter [...] ...|$|E
40|$|This paper {{presents}} the mathematical notions for the parallelization of <b>DO-Loops</b> {{used in the}} tool OPERA currently under development in our team. It aims at giving the user an environment to parallelize problems described by systems of parameterized affine recurrence equations which formalize single-assignment loop nests. The parallelization technique used in OPERA {{is based on a}} classical linear space transformation. Its objectives are to visualize the affine dependences of a problem and to propose a set of different parallel solutions depending on various architectural constraints. Keywords loop parallelization, parameterized affine recurrence equations, parameterized domains. 1 INTRODUCTION Scientific computing as well as many other application domains, has always required large amounts of memory and hours of CPU time. An answer to achieve such highperformance computing {{can be found in the}} use of parallelism. Over the past decades many improvements have been made in the e [...] ...|$|R
40|$|Lockup-free caches in {{conjunction}} to non-blocking processor loads {{have been proposed}} to hide miss latencies in high performance processors. One problem with current approaches is the increased complexity of the processor and of the cache controller due to non-blocking. In this paper, we introduce a simple mechanism to support non-blocking loads and a lockup-free cache. A modified SPARC architecture with non-blocking loads {{and support for the}} new mechanism has been simulated. To be effective the architecture also needs compiler support. Five fortran <b>do-loops</b> are selected and transformed with simple software pipelining techniques to drive the simulator. The simulator is used to investigate the effectiveness of the architecture and the compiler transformations at hiding miss latencies up to 200 processor cycles. For a given program after transformation we identify a critical latency. We show that for lower latencies the cache is effective by simply overlapping misses with processor execut [...] ...|$|R
40|$|This paper {{describes}} {{an approach to}} the evaluation of bounds of the execution time and number of processors needed to execute DO-like loops on MIMD systems. In {{the scope of this}} paper, we only consider single-nested loops. The evaluation of these bounds is done from information about dependences known at compile time. Due to the lack of information at compile time about the dynamic execution of the loop when conditional statements appear, an upper bound of the number of processors is obtained. In addition, a measure of the worst efficiency in terms of processor utilization is obtained by predicting the sequence of execution branches with the lowest parallelism. We also present some simulation results that show how the efficiency varies with the probability of the execution branches. Parallel code generation for shared-memory multiprocessors is outlined. Keywords: Conditional statements, critical path, dependence graph, <b>DO-loops,</b> parallelizing compilers. 1...|$|R
40|$|In {{numerical}} codes, {{the regular}} interleaved accesses that occur within <b>do-loop</b> nests induce cache interference phenomena that can severely degrade program performance. Cache interferences can significantly increase {{the volume of}} memory traffic {{and the amount of}} communication in uniprocessors and multiprocessors. In this paper, we identify cache interference phenomena, determine their causes and the conditions under which they occur. Based on these results, we derive a methodology for computing an analytical expression of cache misses for most classic loop nests, which can be used for precise performance analysis and prediction. We show that cache performance is unstable, because some unexpected parameters such as arrays base address can {{play a significant role in}} interference phenomena. We also show that the impact of cache interferences can be so high, that the benefits of current data locality optimization techniques can be partially, if not totally, eradicated. Keywords: memory ref [...] ...|$|E
40|$|On {{shared memory}} {{parallel}} computers (SMPCs) {{it is natural}} to focus on decomposing the computation (mainly by distributing the iterations of the nested Do-Loops). In contrast, on distributed memory parallel computers (DMPCs) the decomposition of computation {{and the distribution of}} data must both be handled [...] -in order to balance the computation load and to minimize the migration of data. We propose and validate experimentally a method for handling computations and data synergistically to optimize the overall execution time. The method relies on a number of novel techniques, also presented in this paper. The core idea is to rank the "importance" of data arrays in a program and define some of the dominant. The intuition is that the dominant arrays are the ones whose migration would be the most expensive. Using the correspondence between iteration space mapping vectors and distributed dimensions of the dominant data array in each nested <b>Do-loop,</b> we are able to design algorithms fo [...] ...|$|E
40|$|Exact array {{dataflow}} {{analysis can}} be achieved in the general case if the only control structures are the sequence, the <b>do-loop</b> and restricted ifs, and if loop counter bounds and array subscripts are affine expressions of surrounding loop counters and possibly some integer constants. In this paper, we begin the study of dataflow analysis of dynamic control programs, where arbitrary ifs and whiles are allowed. In the general case, this dataflow analysis can only be approximate, hence the name Fuzzy Array Dataflow Analysis. Keywords: Array dataflow analysis, dynamic control programs, automatic parallelization Categories and Subject Descriptors: D. 3. 4 [Software]: Compilers/Optimization R'esum'e Une analyse pr'ecise du flot des donn'ees, en pr'esence de tableaux, ne peut etre effectu'ee dans le cas g'en'eral que si les seules structures de controle sont le s'equencement, la boucle do et une forme restreinte de tests, et si les bornes des compteurs des boucles et les indices des tableaux s [...] ...|$|E
40|$|The {{parallel}} {{execution of}} loop iterations often is inhibited by recurrence relations on scalar variables. Examples are {{the use of}} induction variables and recursive functions. Due to the cyclic dependence between the iterations, these loops have to be executed sequentially. A method is presented to convert a family of coupled linear recurrence relations into explicit functions of a loop index. When the cyclic dependency is the only factor preventing a parallel execution, the conversion effectively removes the dependency and allows the loop to be executed in parallel. The technique is based on constructing and solving a set of coupled linear difference equations at compile-time. The method is general for an arbitrary number of coupled scalar variables and can be implemented by a straight-forward algorithm. Results show that the parallelism of several sequential EISPACK <b>do-loops</b> is significantly enhanced by the converting them into do-all loops...|$|R
40|$|The parallelization of many {{algorithms}} can {{be obtained}} using space-time transformations which are applied on nested <b>do-loops</b> or on recurrence equations. In this paper, we analyze systems of linear recurrence equations, a generalization of uniform recurrence equations. The {{first part of the}} paper describes a method for finding automatically whether such a system can be scheduled by an affine timing function, independent of the size parameter of the algorithm. In the second part, we describe a powerful method that makes it possible to transform linear recurrences into uniform recurrence equations. Both parts rely on results on integral convex polyhedra. Our results are illustrated on the Gauss elimination algorithm and on the Gauss-Jordan diagonalization algorithm. 1 Introduction Designing efficient algorithms for parallel architectures {{is one of the main}} difficulties of the current research in computer science. As the architecture of super-computers evolves towards massive parallelism [...] ...|$|R
40|$|Exact array {{dataflow}} {{analysis can}} be achieved in the general case if the only control structures are <b>do-loops</b> and structural ifs, and if loop counter bounds and array subscripts are affine expressions of englobing loop counters and possibly some integer constants. In this paper, we begin the study of dataflow analysis of dynamic control programs, where arbitrary ifs and whiles are allowed. In the general case, this dataflow analysis can only be fuzzy. 1 Introduction Gathering information on data values is a classical task in advanced compilers, known as Dataflow Analysis [1]. However, this technique only deals with scalar data, and sees an array as an indivisible object. On the other hand, vectorization and parallelization methods are mainly based on the parallelism hidden by independent references to distinct parts of arrays. Various dependence tests have been proposed [2]. However, these tests are not exact, and, even when they are, cannot distinguish between true dependences, whic [...] ...|$|R
40|$|To date, data {{locality}} optimizing algorithms mostly aim at providing efficient {{strategies for}} blocking and reordering loops. But {{little research has}} been devoted to the final step, i. e., computing the optimal block size. Optimal block sizes are currently computed as if a cache behaves as a local memory, i. e., cache interference phenomena are ignored. Case-studies have already shown that cache interferences can greatly affect the optimal block size. The {{purpose of this paper is}} to propose a methodology for estimating interference misses in a regular <b>do-loop</b> nest, and use that knowledge to derive the optimal block size. First, the different types of interference phenomena are identified, and a method for predicting their occurrence and evaluating their impact is proposed. Second, current techniques for computing the optimal block size are analytically and experimentally shown to yield far below optimal performance. Third, cache interference phenomena and even TLB behavior are taken into [...] ...|$|E
40|$|The DOW-loop is a nested {{repetitive}} DATA step programming structure, intentionally {{organized in}} order to allow for programmatically and logically natural isolation of <b>DO-loop</b> instructions related to a certain break-event from actions performed before and after the loop, and without resorting to superfluous conditional statements. Readily recognizable in its basic and most well-known form by the DO UNTIL (LAST. ID) construct, which naturally lends itself to control-break BY-processing of grouped data, the DOW-loop, however, is much more morphologically diverse and general in nature. In this talk, we aim to examine the internal logic of the DOW-loop and use the power of example to reveal its aesthetic beauty and pragmatic utility. To some industries, for example, pharma, where “flagging” every observation in a group based on conditions within the group is ubiquitous, the DOW-loop lends itself as an ideal logical vehicle by greatly simplifying the alignment of stream-of-consciousness and SAS ® code. I. DOW-LOOP: THE CIRCUITRY 1. Intro What is the DOW-loop? It is not a "standard industry term". Rather, it is an abbreviatio...|$|E
40|$|Abstract We {{discuss how}} the action systems {{formalism}} {{can be used}} in constructing controllers for discrete event systems. Action systems are based on predicates and predicate transformers. Our approach is exemplified through applying action systems into a real-world control problem. We show, how the reachability of safe states in infinite unpredictable system, the safety conditions, can be stated as simple predicates. Verification is carried out in a standard manner with the weakest precondition calculus of Dijkstra. 1 Introduction Action systems, originally proposed by Back and Kurki-Suonio [2], are predicate transformer based systems that operate on some state base. Actions resemble events, but are more precise, as they express algorithmically, how the state base is changed. In action systems, actions are executed in a <b>do-loop</b> which iterates as long as any of the actions is enabled. In this loop, actions are connected with a choice operator which models parallel nondeterministic choice between enabled actions. This allows compact description of unpredictable behaviour. Therefore, action systems have been successfully applied in many non-trivial applications when modelling reactive and concurrent behaviour, see e. g. [3, 5]. The use of predicate transformers in discrete event systems is not new, see e. g. [9]. The novelty in our approach is that both the controller and its environment are modelled uniformly by action systems. This allows reasoning of combined behaviour with simple predicates...|$|E
40|$|AbstractThe {{complexity}} of the zero-inequivalence problem (deciding if a program outputs a nonzero value for some nonnegative integer input) for several simple classes of loop programs is studied. In particular, we show {{that the problem is}} NP-complete for L 1 -programs with only one input variable and two auxiliary variables. These are programs over the instruction set x ← 0, x ← x + 1, x ← y, do x … end, where <b>do-loops</b> cannot be nested. For K 1 -programs, where the instruction set is x ← x + 1, x ← x ∸ 1, do x … end, zero-inequivalence is NP-complete even for programs with only one input variable and one auxiliary variable. These results may be the best possible since there is a class of programs which properly contains two-variable L 1 -programs and one-variable K 1 -programs with a polynomial time decidable equivalence problem. Addition of other constructs, e. g., allowing K 1 -programs to use instruction x ← x + y, makes the zero-inequivalence problem undecidable...|$|R
40|$|Research in {{healthcare}} often involves identified data. These datasets contain protected health information (PHI) {{that has to}} be removed to meet use and disclosure requirements for research under HIPAA. There are varying databases in which patient information is stored. These can be multiple datasets on the same patients or different patients that need to be combined for analysis. When combining multiple datasets for further analysis, it becomes a challenge in aligning these datasets, and at the same time, protecting PHI. This paper highlights the importance of examining the format of each dataset, and discusses steps to prepare the datasets before removing the PHI. Subsequently, we introduce, in progressively complex methods, combinations of SAS ® functions, such as <b>DO-LOOPs,</b> SEED, RANUNI, NODUPKEY, EOF (end of file) functions, to replace the PPI with research appropriate identifiers, specifically variables embedded in medical records and administrative datasets. At the end of this paper, a MACRO LOOP routine with ARRAY, CALL VNAME, CALL SYMPUT, PROC APPEND that encompasses all these functions is presented to combine all six datasets for further analysis...|$|R
40|$|Exact {{polyhedral}} model (PM) can be {{built in}} the general case if the only control structures are <b>do-loops</b> and structured ifs, and if loop counter bounds, array subscripts and if-conditions are affine expressions of enclosing loop counters and possibly some integer constants. In more general dynamic control programs, where arbitrary ifs and whiles are allowed, in the general case the usual dataflow analysis can be only fuzzy. This {{is not a problem}} when PM is used just for guiding the parallelizing transformations, but is insufficient for transforming source programs to other computation models (CM) relying on the PM, such as our version of dataflow CM or the well-known KPN. The paper presents a novel way of building the exact polyhedral model and an extension of the concept of the exact PM, which allowed us to add in a natural way all the processing related to the data dependent conditions. Currently, in our system, only arbirary ifs (not whiles) are allowed in input programs. The resulting polyhedral model can be easily put out as an equivalent program with the dataflow computation semantics. Comment: 8 pages, 11 figures, was submitted to IMPACT- 2015 (however was not accepted...|$|R
