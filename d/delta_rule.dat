143|51|Public
5000|$|For a {{single-layer}} network, this expression {{becomes the}} <b>Delta</b> <b>Rule.</b>|$|E
5000|$|While the <b>delta</b> <b>rule</b> {{is similar}} to the perceptron's update rule, the {{derivation}} is different. The perceptron uses the Heaviside step function as the activation function , and that means that [...] does not exist at zero, and is equal to zero elsewhere, which makes the direct application of the <b>delta</b> <b>rule</b> impossible.|$|E
5000|$|In machine learning, the <b>delta</b> <b>rule</b> is a {{gradient}} descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. [...] It {{is a special}} case of the more general backpropagation algorithm. For a neuron [...] with activation function , the <b>delta</b> <b>rule</b> for 's th weight [...] is given by ...|$|E
50|$|When {{an atomic}} test is set off without his knowledge, Martin steals the data, then {{goes back to}} Soledad Flats and places the {{information}} under a stone. An FBI agent follows him, but Martin is able to elude him until he crashes his car. Now back at the hospital, he is given truth serum. Deep under the drug's influence, Martin tells a story about being held captive by space aliens, led by Denab, in their underground base., The aliens, with large, bulging eyes, are from the planet Astron <b>Delta,</b> <b>ruled</b> by a being called The Tala. They had revived his lifeless body as he had died in his aircraft.|$|R
5000|$|<b>Delta</b> Green: The <b>Rules</b> of Engagement (Tynes Cowan Corp, 2000), {{novel by}} John Tynes, [...]|$|R
40|$|Since it is unsound {{to reason}} about call-by-value {{languages}} using call-by name equational theories, we present two by-value combinatory logics and translations from the -value (v) calculus to the logics. The first by-value logic is constructed {{in a manner}} similar to the v -calculus: it is based on the byname combinatory logic, but the combinators are strict. The translation is non-standard to account for the strictness of the input program. The second by-value logic introduces laziness to K terms so that the translation can preserve the structure of functions that do not use their argument. Both logics include constants and <b>delta</b> <b>rules,</b> and we prove their equivalence with the v -calculus. 1 Introduction The translation of functional languages into combinatory logics captures the essence of compilation both from a theoretical as well as a practical perspective. Most of the research on this topic concerns the traditional call-by-name -calculus and combinatory logic. However, most r [...] ...|$|R
5000|$|The <b>delta</b> <b>rule</b> is {{commonly}} stated in simplified form for a neuron with a linear activation function as ...|$|E
5000|$|Artificial Neural {{networks}} library implements {{some common}} network architectures (multi-layer feed forward and distance networks) and learning algorithms (back propagation, <b>delta</b> <b>rule,</b> simple perceptron, evolutionary learning).|$|E
5000|$|The <b>delta</b> <b>rule</b> {{is derived}} by {{attempting}} to minimize the error in {{the output of the}} neural network through gradient descent. The error for a neural network with [...] outputs can be measured as ...|$|E
40|$|Connections between nodes {{of fully}} {{connected}} neural networks are usually represented by weight matrices. In this article, functional transfer matrices are introduced as {{alternatives to the}} weight matrices: Instead of using real weights, a functional transfer matrix uses real functions with trainable parameters to represent connections between nodes. Multiple functional transfer matrices are then stacked together with bias vectors and activations to form deep functional transfer neural networks. These neural networks can be trained {{within the framework of}} back-propagation, based on a revision of the <b>delta</b> <b>rules</b> and the error transmission rule for functional connections. In experiments, it is demonstrated that the revised rules can be used to train a range of functional connections: 20 different functions are applied to neural networks with up to 10 hidden layers, and most of them gain high test accuracies on the MNIST database. It is also demonstrated that a functional transfer matrix with a memory function can roughly memorise a non-cyclical sequence of 400 digits. Comment: 39 pages, 4 figures, submitted as a journal articl...|$|R
5|$|In July 2008, United States District Judge Karl Forester <b>ruled</b> <b>Delta</b> {{will not}} {{be held liable for}} the crash, because while Comair is a wholly owned {{subsidiary}} of the Atlanta-based airline, Comair maintains its own management and policies, and employs its own pilots.|$|R
40|$|The charge {{asymmetry}} in semi-leptonic kaon decays {{has been}} measured {{as a function}} of the kaon lifetime. High statistics data of K/sub e 3 //sup 0 / and K/sub mu 3 //sup 0 / decay modes agree with each other and with the general expectation of the CP violation phenomenology together with the Delta S- <b>Delta</b> Q <b>rule.</b> The K/sub L/-K/sub S/ mass difference obtained is Delta m=(0. 533 +or- 0. 004) * 10 /sup 10 / S/sup - 1 /. (5 refs) ...|$|R
50|$|Perceptrons can {{be trained}} by a simple {{learning}} algorithm that is usually called the <b>delta</b> <b>rule.</b> It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of gradient descent.|$|E
50|$|Below is {{an example}} of a {{learning}} algorithm for a (single-layer) perceptron. For multilayer perceptrons, where a hidden layer exists, more sophisticated algorithms such as backpropagation must be used. Alternatively, methods such as the <b>delta</b> <b>rule</b> can be used if the function is non-linear and differentiable, although the one below will work as well.|$|E
5000|$|Backpropagation is a {{generalization}} of the <b>delta</b> <b>rule</b> to multi-layered feedforward networks, {{made possible by}} using the chain rule to iteratively compute gradients for each layer. The backpropagation algorithm has been repeatedly rediscovered and is a special case of a more general technique called automatic differentiation in reverse accumulation mode. It {{is closely related to}} the Gauss-Newton algorithm, and is part of continuing research in neural backpropagation.|$|E
50|$|In 1998, Delta Green won the Origins Award for Best Roleplaying Supplement of 1997. The setting {{also won}} two awards in 2000: Best Game-Related Novel of 1999 for <b>Delta</b> Green: The <b>Rules</b> of Engagement and Best Roleplaying Supplement of 1999 for Delta Green: Countdown.|$|R
40|$|An {{artificial}} {{neural network}} is developed to recognize spatio-temporal bipolar patterns associatively. The function of a formal neuron is generalized by replacing multiplication with convolution, weights with transfer functions, and thresholding with nonlinear transform following adaptation. The Hebbian learn-ing <b>rule</b> and the <b>delta</b> learning <b>rule</b> are generalized accordingly, resulting in the learning of weights and delays. The neural network which was first developed for spatial patterns was thus generalized for spatio-temporal patterns. It was tested using a set of bipolar input patterns derived from speech signals, showing robust classification of 30 model phonemes...|$|R
40|$|The time-dependence of {{the decay}} rate of {{initially}} pure K/sup 0 / {{into the final}} states (pi /sup -/e/sup +/ nu) and (pi /sup +/e/sup -/ nu) has been studied to investigate {{the validity of the}} Delta S/ <b>Delta</b> Q <b>rule.</b> In a sample of 4724 events no evidence for weak hadronic current with Delta S=- Delta Q is found. The ratio of the Delta S= Delta Q amplitude and the Delta S= Delta Q amplitude is x=(0. 04 +or- 0. 03) -i (0. 06 +or- 0. 05). (11 refs) ...|$|R
5000|$|Kortge (1990) {{proposed}} a learning rule for training neural networks, called the 'novelty rule', to help alleviate catastrophic interference. As its name suggests, this rule helps the neural network to learn only {{the components of}} a new input that differ from an old input. Consequently, the novelty rule changes only the weights that were not previously dedicated to storing information, thereby reducing the overlap in representations at the hidden units. Thus, even when inputs are somewhat similar to another, dissimilar representations can be made at the hidden layer. In order to apply the novelty rule, during learning the input pattern {{is replaced by a}} novelty vector that represents the components that differ. The novelty vector for the first layer (input units to hidden units) is determined by taking the target pattern away from the current output of the network (the <b>delta</b> <b>rule).</b> For the second layer (hidden units to output units) the novelty vector is simply the activation of the hidden units that resulted from using the novelty vector as an input through the first layer. Weight changes in the network are computed by using a modified <b>delta</b> <b>rule</b> with the novelty vector replacing the activation value (sum of the inputs): ...|$|E
50|$|The output {{targets in}} the {{response}} units (i.e., the examinee attributes) are compared to the pattern associated with each stimulus input or exemplar (i.e., the expected response patterns). The solution produced initially with the stimulus and association connection weights {{is likely to be}} discrepant resulting in a relatively large error. However, this discrepant result can be used to modify the connection weights thereby leading to a more accurate solution and a smaller error term. One popular approach for approximating the weights so the error term is minimized is with a learning algorithm called the generalized <b>delta</b> <b>rule</b> that is incorporated in a training procedure called back propagation of error.|$|E
40|$|The <b>delta</b> <b>rule</b> of {{associative}} learning {{has recently been}} used in several models of human category learning, and applied to categories with different relative frequencies, or base rates. Previous research has emphasized predictions of the <b>delta</b> <b>rule</b> after extensive learning. Our first experiment measures the relative acquisition rates of categories with different base rates, and the <b>delta</b> <b>rule</b> significantly and systematically deviates from the human data. We suggest that two additional mechanisms are involved, namely, short-term memory and strategic guessing. Two additional experiments highlight {{the effects of these}} mechanisms. The mechanisms are formalized and combined with the <b>delta</b> <b>rule,</b> and provide good fits to the data from all three experiments. Several recent models of category learning in humans incorporated the <b>delta</b> <b>rule</b> of {{associative learning}} (Rumelhart, Hinton, & Williams, 1986). The <b>delta</b> <b>rule</b> posits that the growth in the strength of association between a cue and an outcome is error driven: The associative strength changes in magnitude proportionally to the discrepancy, or error, betwee...|$|E
30|$|The {{results showed}} that a four-layer {{perceptron}} network with training algorithm of back propagation, hyperbolic tangential activation function, and <b>Delta</b> training <b>rule</b> with ten neurons in the first hidden layer and four neurons in the second hidden layer had the best performance for the prediction of pellet density. The minimum {{root mean square error}} and coefficient of determination for the multilayer perceptron network were 0.01732 and 0.972, respectively. Also, the results of statistical analysis indicate that moisture content, speed of piston, and particle size significantly affected (P[*]<[*] 0.01) the density of pellets while the influence of die length was negligible (P[*]>[*] 0.05).|$|R
40|$|The leading order {{contribution}} to the direct CP asymmetry in tau^{+/-} -> K^{+/-} pi^ 0 nu_{tau} decay rates is evaluated within the Standard Model. The weak phase required for CP violation is introduced through an interesting mechanism involving second order weak interactions, which is also responsible for tiny violations of the Delta S= <b>Delta</b> Q <b>rule</b> in K_{l 3 } decays. The calculated CP asymmetry {{turns out to be}} of order 10 ^{- 12 }, leaving a large window for studying effects of non-standard sources of CP violation in this observable. Comment: 5 pages, 3 figures, version published in Phys. Rev. ...|$|R
40|$|It {{is shown}} that pure {{rotational}} transitions in doubly degenerate torsional states of C 2 H 6 (with selection <b>rules</b> <b>Delta</b> K = 0, {{plus or minus}} 1) are made allowed by Coriolis interaction between torsion and dipole-allowed vibrations. Expressions are presented for integrated intensities from which strengths of lines in the millimeter region can be calculated...|$|R
40|$|The {{generalized}} <b>delta</b> <b>rule</b> (which is {{also known}} as error backpropagation) is a significant advance over previous procedures for network learning. In this paper, we compare network learning using the generalized <b>delta</b> <b>rule</b> to human learning on two concept identification tasks: • Relative ease of concept identification • Generalizing from incomplete dat...|$|E
40|$|Abstract:- In {{this paper}} we propose a method to {{implement}} in FPGA circuits, a feedforward neural network with on-chip <b>delta</b> <b>rule</b> learning algorithm. The method implies the building of a neural network by generic blocks designed in Mathworks ’ Simulink environment. The main characteristics of this solution are on-chip learning algorithm implementation and high reconfiguration capability and operation under real time constraints. Key-Words:- MLP, learning on-chip, <b>Delta</b> <b>rule,</b> ANN, FPGA...|$|E
40|$|Current {{learning}} {{theories are}} {{based on the idea that}} learning is driven by the difference between expectations and experience (the <b>delta</b> <b>rule).</b> In extinction, one learns that certain expectations no longer apply. Here, we test the potential validity of the <b>delta</b> <b>rule</b> by manipulating memory retrieval (and thus expectations) during extinction learning. Adrenergic signaling is critical for the time-limited retrieval (but not acquisition or consolidation) of contextual fear. Using genetic and pharmacologic approaches to manipulate adrenergic signaling, we find that long-term extinction requires memory retrieval but not conditioned responding. Identical manipulations of the adrenergic system that do not affect memory retrieval do not alter extinction. The results provide substantial support for the <b>delta</b> <b>rule</b> of learning theory. In addition, the timing over which extinction is sensitive to adrenergic manipulation suggests a model whereby memory retrieval occurs during, and several hours after, extinction learning to consolidate long-term extinction memory...|$|E
40|$|We {{present the}} current status of the {{analysis}} of about 1. 7 billion K_S K_L pair events collected at DAΦNE with the KLOE detector to determine the branching ratio of K_S -> πeν decay and the lepton charge asymmetry. This sample is 4 times larger in statistics than the one used in the previous most precise result, from KLOE as well, allowing us to improve the accuracy on the measurement and related tests of CPT symmetry and Delta S = <b>Delta</b> Q <b>rule.</b> Comment: Talk given at 13 th International Workshop on Meson Production, Properties and Interaction 29 th May - 3 rd June 2014, (3 pages...|$|R
40|$|We {{present the}} current status of the {{analysis}} of about 1. 7 billion KSKL pair events collected at DAFNE with the KLOE detector to determine the branching ratio of KS -> pi e nu decay and the lepton charge asymmetry. This sample is ~ 4 times larger in statistics than the one used in a previous KLOE analysis, allowing us to improve the accuracy of the measurement and of the related tests of CPT symmetry and Delta S = <b>Delta</b> Q <b>rule.</b> Comment: Talk given at the 4 th Young Researchers Workshop "Physics Challenges in the LHC Era", Frascati, May 12 and 15, 2014 (6 pages...|$|R
40|$|We {{introduce}} the sequential equal surplus division for sharing the total welfare resulting form {{the cooperation of}} agents along a river with a <b>delta.</b> This allocation <b>rule</b> {{can be seen as}} a generalization of the contribution vectors introduced by Ju, Borm and Ruys (2007) in the context of TU-games. We provide two axiomatic characterizations of the sequential equal surplus division. ...|$|R
40|$|Abstract. In {{this paper}} a {{multilayer}} neural-net (NN) controller is applied for tracking control of robotic manipulator, {{which is a}} nonlinear object having unknown and changeable parameters. Dynamics equations of a rigid manipulator are presented. The NN controller is used for compensating manipulator nonlinearities. The controller is realized in a form of a multilayer NN, which is nonlinear in the weights. The standard <b>delta</b> <b>rule</b> using backpropagation tuning is inadequate, so a term correcting the <b>delta</b> <b>rule</b> {{as well as a}} robustifying term is added. The presented control law and tuning algorithm are derived from the Lyapunov’s direct method. Results of the experiment are presented in this paper...|$|E
40|$|In {{this paper}} the Sigma-if {{artificial}} neural network model is considered, which is a generalization of an MLP network with sigmoidal neurons. It {{was found to be}} a potentially universal tool for automatic creation of distributed classification and selective attention systems. To overcome the high nonlinearity of the aggregation function of Sigma-if neurons, the training process of the Sigma-if network combines an error backpropagation algorithm with the self-consistency paradigm widely used in physics. But for the same reason, the classical backpropagation <b>delta</b> <b>rule</b> for the MLP network cannot be used. The general equation for the backpropagation generalized <b>delta</b> <b>rule</b> for the Sigma-if neural network is derived and a selection of experimental results that confirm its usefulness are presented...|$|E
40|$|Active Noise Control (ANC) {{involves}} an electro acoustic or electromechanical system that cancels the primary (unwanted) noise {{based on the}} principle of superposition. An anti-noise signal of equal amplitude and opposite phase is generated and combined with the primary noise, resulting in the cancellation of the noise. A fundamental problem to be considered in ANC systems is the requirement of highly precise control, temporal stability and reliability. To produce high degree of attenuation, the amplitude and phase of both the primary and the secondary noise must match with the close precision. The adaptive filters are used to control the noise and it has a linear input and output characteristic. If a transfer path of the noise has nonlinear characteristics it will be difficult for the filter to generate an optimal anti-noise. In this study, we propose a algorithm, <b>delta</b> <b>rule</b> algorithm which uses non linear output function. <b>Delta</b> <b>rule</b> is used for learning complex patterns in Artificial Neural Networks. We have implemented the adaptive filters using Least Mean Square (LMS) algorithm, Recursive Least Square (RLS) algorithm and <b>Delta</b> <b>rule</b> algorithm and compared the results...|$|E
40|$|We {{present a}} fast method to price and hedge CMS spread {{options in the}} displaced-diffusion co-initial swap market model. Numerical tests {{demonstrate}} {{that we are able}} to obtain sufficiently accurate prices and Greeks with computational times measured in milliseconds. Further, we find that CMS spread options are weakly dependent on the at-the-money Black implied volatility skews. Spread option, Gaussian quadrature <b>rule,</b> <b>delta,</b> vega, market skew sensitivity...|$|R
40|$|In this paper, a {{geometrical}} {{scheme is}} presented to show how to overcome an encountered problem arising {{from the use of}} generalized <b>delta</b> learning <b>rule</b> within competitive learning model. It is introduced a theoretical methodology for describing the quantization of data via rotating prototype vectors on hyper-spheres. The proposed learning algorithm is tested and verified on different multidimensional datasets including a binary class dataset and two multiclass datasets from the UCI repository, and a multiclass dataset constructed by us. The proposed method is compared with some baseline learning vector quantization variants in literature for all domains. Large number of experiments verify the performance of our proposed algorithm with acceptable accuracy and macro f 1 scores...|$|R
40|$|In {{order to}} {{establish}} a regime of protection and conservation of the Danube Delta, but alsoto achieve international commitments of Romania, it was developed and adopted by the Parliament aspecial law, Law no. 82 / 1993, establishing the Biosphere Reservation of Danube <b>Delta.</b> Theestablished <b>rules</b> had in mind mainly the preservation and protection of the existing natural heritage,promoting the sustainable use of resources resulting from natural ecosystems of the reserve andreconstruction of areas damaged by the impact of human activities. Although repeatedly amended andsupplemented, this regulatory framework has always been overwhelmed by economic and socialdevelopment of the area, requiring practically a major reform that {{was carried out by}} Law no 136 ofJuly 5, 2011...|$|R
