5|103|Public
5000|$|To {{determine}} the lateral input direction (left, front, right), the auditory system analyzes the following <b>ear</b> <b>signal</b> information: ...|$|E
5000|$|... subsymbolic {{if it is}} made by {{constituent}} {{entities that}} are not representations in their turn, e.g., pixels, sound images as perceived by the <b>ear,</b> <b>signal</b> samples; subsymbolic units in neural networks can be considered particular cases of this category ...|$|E
40|$|We {{report on}} {{on-going}} work investigating {{the feasibility of}} using tissue conduction to evince auditory spatial perception. Early results indicate {{that it is possible}} to coherently control externalization, range, directionality (including elevation), movement and some sense of spaciousness without presenting acoustic signals to the outer <b>ear.</b> <b>Signal</b> control techniques so far have utilised discrete signal feeds, stereo and 1 st order ambisonic hierarchies. Some deficiencies in frontal externalization have been observed. We conclude that, whilst the putative components of the head related transfer function are absent, empirical tests indicate that coherent equivalents are perceptually utilisable. Some implications for perceptual theory and technological implementations are discussed along with potential practical applications and future lines of enquiry. 1...|$|E
50|$|While it {{has been}} {{suggested}} the cropping may interfere with a dog's ability to communicate using <b>ear</b> <b>signals,</b> some also argue that cropping increases a dog's ability to communicate with <b>ear</b> <b>signals.</b> There has been no scientific comparative study of ear communication in cropped and uncropped dogs.|$|R
30|$|The {{synthesis}} stage {{is based on}} a time-frequency selective HRTF filtering of one of the input microphone signals. This filtering is carried out selectively in the STFT domain according to the directions estimated in the analysis stage, resulting in the output signals for the left and right ears. Finally, the <b>ear</b> <b>signals</b> are transformed back to the time domain using the inverse STFT operator.|$|R
25|$|Somatic {{motor control}} – Some motor neurons send their axons to the reticular {{formation}} nuclei, {{giving rise to}} the reticulospinal tracts of the spinal cord. These tracts function in maintaining tone, balance, and posture—especially during body movements. The reticular formation also relays eye and <b>ear</b> <b>signals</b> to the cerebellum so that the cerebellum can integrate visual, auditory, and vestibular stimuli in motor coordination. Other motor nuclei include gaze centers, which enable the eyes to track and fixate objects, and central pattern generators, which produce rhythmic signals to the muscles of breathing and swallowing.|$|R
40|$|Presented at the 20 th International Conference on Auditory Display (ICAD 2014), June 22 - 25, 2014, New York, NY. We {{report on}} {{on-going}} work investigating {{the feasibility of}} using tissue conduction to evince auditory spatial perception. Early results indicate {{that it is possible}} to coherently control externalization, range, directionality (including elevation), movement and some sense of spaciousness without presenting acoustic signals to the outer <b>ear.</b> <b>Signal</b> control techniques so far have utilised discrete signal feeds, stereo and 1 st order ambisonic hierarchies. Some deficiencies in frontal externalization have been observed. We conclude that, whilst the putative components of the head related transfer function are absent, empirical tests indicate that coherent equivalents are perceptually utilisable. Some implications for perceptual theory and technological implementations are discussed along with potential practical applications and future lines of enquiry...|$|E
40|$|In recent years, {{the number}} of {{unilateral}} cochlear implant (CI) users with functional residual-hearing has increased and bimodal hearing has become more prevalent. According to the multi-source speech perception model, both bottom-up and top-down processes are important components of speech perception in bimodal hearing. Additionally, these two components are thought to {{interact with each other}} to different degrees depending {{on the nature of the}} speech materials and the quality of the bottom-up cues. Previous studies have documented the benefits of bimodal hearing as compared with a CI alone, but most of them have focused on the importance of bottom-up, low-frequency cues. Because only a few studies have investigated top-down processing in bimodal hearing, relatively little is known about the top-down mechanisms that contribute to bimodal benefit, or the interactions that may occur between bottom-up and top-down processes during bimodal speech perception. The research described in this dissertation investigated top-down processes of bimodal hearing, and potential interactions between top-down and bottom-up processes, in the perception of temporally interrupted speech. Temporally interrupted sentences were used to assess listeners 2 ̆ 7 ability to fill in missing segments of speech by using top-down processing. Young normal hearing listeners were tested in simulated bimodal listening conditions in which noise band vocoded sentences were presented to one ear with or without low-pass (LP) filtered speech or LP harmonic complexes (LPHCs) presented to the contralateral ear. Sentences were square-wave gated at a rate of 5 Hz with a 50 percent duty cycle. Two factors that were expected to influence bimodal benefit were examined: the amount of linguistic context available in the speech stimuli, and the continuity of low-frequency cues. Experiment 1 evaluated the effect of sentence context on bimodal benefit for temporally interrupted sentences from the City University of New York (CUNY) and Institute of Electrical and Electronics and Engineers (IEEE) sentence corpuses. It was hypothesized that acoustic low-frequency information would facilitate linguistic top-down processing such that the higher context CUNY sentences would produce more bimodal benefit than the lower context IEEE sentences. Three vocoder channel conditions were tested for each type of sentence (8 -, 12 -, and 16 -channels for CUNY; 12 -, 16 -, and 32 -channels for IEEE), in combination with either LP speech or LPHCs. Bimodal benefit was compared for similar amounts of spectral degradation (matched-channels) and similar ranges of baseline performance. Two gain measures, percentage point gain and normalized gain, were examined. Experiment 1 revealed clear effects of context on bimodal benefit for temporally interrupted speech, when LP speech was presented to the residual-hearing ear, thereby providing additional support for the notion that low-frequency cues can enhance listeners 2 ̆ 7 use of top-down processing. However, the bimodal benefits observed for temporally interrupted speech were considerably smaller than those observed in an earlier study that used continuous speech. In addition, unlike previous findings for continuous speech, no bimodal benefits were observed when LPHCs were presented to the LP ear. Experiments 2 and 3 further investigated the effects of low-frequency cues on bimodal benefit by systematically restoring continuity to temporally interrupted signals in the vocoder and/or LP ears. Stimuli were 12 -channel CUNY sentences presented to the vocoder ear, and LPHCs presented to the LP <b>ear.</b> <b>Signal</b> continuity was restored to the vocoder ear by filling silent gaps in sentences with envelope-modulated, speech-shaped noise. Continuity was restored to signals in the LP ear by filling gaps with envelope-modulated LP noise or by using continuous LPHCs. It was hypothesized that the restoration of continuity in one or both ears would improve bimodal benefit relative to the condition in which both ears received temporally interrupted stimuli. The results from Experiments 2 and 3 showed that restoring continuity to the simulated residual-hearing or CI ear improved bimodal benefits, but that the greatest improvement was observed when continuity was restored to both ears. These findings support the conclusion that temporal interruption disrupts top-down enhancement effects in bimodal hearing. Lexical segmentation and perceptual continuity were identified as factors that could potentially explain the increased bimodal benefit for continuous, as compared to temporally interrupted, speech. Taken together, the findings from Experiments 1 - 3 provide additional evidence that low-frequency sensory information can provide bimodal benefit for speech that is spectrally and/or temporally degraded by improving listeners 2 ̆ 7 ability to make use of top-down processing. Findings further suggest that temporal degradation reduces top-down enhancement effects in bimodal hearing, thereby reducing bimodal benefit for temporally interrupted speech as compared to continuous speech...|$|E
30|$|Our aim was {{to confirm}} the {{usefulness}} of the perilymphatic signal changes on T 2 -weighted (T 2 W) gradient-echo sequence to differentiate vestibular schwannomas from internal auditory canal (IAC) meningiomas, through a compartmental analysis of inner <b>ear</b> fluids <b>signal</b> intensity.|$|R
50|$|At low frequencies, {{where the}} {{wavelength}} is large {{compared to the}} human head, an incoming sound diffracts around it, {{so that there is}} virtually no acoustic shadow and hence no level difference between the ears. In this range, the only available information is the phase relationship between the two <b>ear</b> <b>signals,</b> called interaural time difference, or ITD.Evaluating this time difference allows for precise localisation within a cone of confusion: the angle of incidence is unambiguous, but the ITD is the same for sounds from the front or from the back. As long as the sound is not totally unknown to the subject, the confusion can usually be resolved by perceiving the timbral front-back variations caused by the ear flaps (or pinnae).|$|R
40|$|Abstract—The {{ability of}} human {{auditory}} systems {{to focus on}} one signal and ignore other signals in an auditory scene where several auditory events are taking place, {{often referred to as}} cocktail-party effect, is a key to localization of sound sources. This ability is partly made possible by interaural cues – Interaural Time Differences (ITDs) and Interaural Level Differences (ILDs) – between the input <b>ear</b> <b>signals</b> that assist the estimation of source azimuth angles, and separation of the signal of the desired direction from signals of non-desire directions. In this paper, we investigate simplified techniques to source separation of sound sources based on inter-channel cues. Particular emphasis is put on the selection of time-frequency masks and its effects on the quality of source separation. Index Terms—cocktail-party processing, source separation. I...|$|R
40|$|AbstractThe {{detection}} of interaural time differences (ITDs) for sound localization critically {{depends on the}} similarity between {{the left and right}} <b>ear</b> <b>signals</b> (interaural correlation). We show that, like humans, owls can localize phantom sound sources well until the correlation declines to a very low value, below which their performance rapidly deteriorates. Decreasing interaural correlation also causes the response of the owl’s tectal auditory neurons to decline nonlinearly, with a rapid drop followed by a more gradual reduction. A detection-theoretic analysis of the statistical properties of neuronal responses could account for the variance of behavioral responses as interaural correlation is decreased. Finally, cross-correlation analysis suggests that low interaural correlations cause misalignment of cross-correlation peaks across different frequencies, contributing heavily to the nonlinear decline in neural and ultimately behavioral performance...|$|R
40|$|Recent neurophysical {{studies suggest}} that {{binaural}} decoding is based on count comparison in cases of both ITD and ILD decoding. In such mechanisms, the neural signals are stronger in the auditory pathways leading to the ipsilateral hemisphere when a signal is presented earlier, or with higher level, to the contralateral ear. This paper describes a simple computational model implementing binaural cue decoding based on countcomparison principles. In the model, ITD and ILD are decoded using separate mechanisms, inspired by the functions of Medial Superior Olive and the Lateral Superior Olive found in the mammal brainstem. It is also assumed, based on psychoacoustic data, that ITD decoding is sluggish, and ILD decoding is faster. It is shown that the proposed mechanisms decode static ITD and ILD similarly as humans do. In addition to ITD and ILD, humans can also detect interaural coherence, or the similarity between <b>ear</b> <b>signals.</b> Interaural coherence is in the model decoded as the range of temporal variations of ILD...|$|R
40|$|In a {{stereoscopic}} system {{both eyes}} or cameras {{have a slightly}} different view. As a consequence small variations between the projected images exist ("disparities") which are spatially evaluated in order to retrieve depth information [Sanger, 1988, Fleet et al., 1991]. A strong similarity exists between the analysis of visual disparities and {{the determination of the}} azimuth of a sound source [Wagner and Frost, 1993]. The direction of the sound is thereby determined from the temporal delay between the left and right <b>ear</b> <b>signals</b> [Konishi and Sullivan, 1986]. Similarly, here we transpose the spatially defined problem of disparity analysis into the temporal domain and utilize two resonators implemented in the form of causal (electronic) filters to determine the disparity as local temporal phase differences between the left and right filter responses. This approach permits real-time analysis and can be solved analytically for step function contrast changes, which is an important case in all [...] ...|$|R
40|$|UNLABELLED: Abstract Conclusions: Human {{inner ear}} neurons have an innate {{regenerative}} capacity {{and can be}} cultured in vitro in a 3 -D gel. The culture technique is valuable for experimental investigations of human inner <b>ear</b> neuron <b>signaling</b> and regeneration. OBJECTIVES: To establish a new in vitro model to study human inner <b>ear</b> nerve <b>signaling</b> and regeneration. METHODS: Human superior vestibular ganglion (SVG) was harvested during translabyrinthine surgery for removal of vestibular schwannoma. After dissection tissue explants were embedded and cultured in a laminin-based 3 -D matrix (Matrigel™). 3 -D growth cone (GC) expansion was analyzed using time-lapse video microscopy (TLVM). Neural marker expression was appraised using immunocytochemistry with fluorescence and laser confocal microscopy. RESULTS: Tissue explants from adult human SVG could be cultured in 3 -D in a gel, indicating an innate potential for regeneration. Cultured GCs were found to expand dynamically in the gel. Growth cone expansion and axonal Schwann cell alignment were documented using TLVM. Neurons were identified morphologically and through immunohistochemical staining...|$|R
40|$|A new {{communication}} and control concept using tongue movements is introduced to generate, detect, and classify signals {{that can be}} used in novel hands-free human–machine interface applications such as communicating with a computer and controlling devices. The signals that are caused by tongue movements are the changes in the airflow pressure that occur in the ear canal. The goal is to demonstrate that the <b>ear</b> pressure <b>signals</b> that are acquired using a microphone that is inserted into the ear canal, due to specific tongue movements, are distinct and that the signals can be detected and classified very accurately. The strategy that is developed for demonstrating the concept includes energy-based signal detection and segmentation to extract <b>ear</b> pressure <b>signals</b> due to tongue movements, signal normalization to decrease the trial-to-trial variations in the signals, and pairwise cross-correlation signal averaging to obtain accurate estimates from ensembles of pressure signals. A new decision fusion classification algorithm is formulated to assign the pressure signals to their respective tongue-movement classes. The complete strategy of signal detection and segmentation, estimation, and classification is tested on four tongue movements of eight subjects. Through extensive experiments, it is demonstrated that the <b>ear</b> pressure <b>signals</b> due to the tongue movements are distinct and that the four pressure signals can be classified with an accuracy of more than 97 % averaged across the eight subjects using the decision fusion classification algorithm. Thus, it is concluded that, through the unique concept that is introduced in this paper, human–computer interfaces that use tongue movements can be designed for hands-free {{communication and}} control applications...|$|R
3000|$|... are the STFT of {{the output}} signals {{corresponding}} {{to the left and}} right <b>ears,</b> respectively. These <b>signals</b> are transformed back to the time domain using the inverse STFT operator following an overlap-add scheme.|$|R
40|$|A {{complete}} {{signal processing}} strategy {{is presented to}} detect and precisely recognize tongue movement by monitoring changes in airflow {{that occur in the}} ear canal. Tongue movements within the human oral cavity create unique, subtle pressure <b>signals</b> in the <b>ear</b> that can be processed to produce command signals in response to that movement. The strategy developed for the human machine interface architecture includes energy-based signal detection and segmentation to extract <b>ear</b> pressure <b>signals</b> due to tongue movements, signal normalization to decrease the trial-to-trial variations in the signals, and pairwise cross-correlation signal averaging to obtain accurate estimates from ensembles of pressure signals. A new decision fusion classification algorithm is formulated to assign the pressure signals to their respective tongue-movement classes. The complete strategy of signal detection and segmentation, estimation, and classification is tested on 4 tongue movements of 4 subjects. Through extensive experiments, it is demonstrated that the <b>ear</b> pressure <b>signals</b> due to the tongue movements are distinct and that the 4 pressure signals can be classified with over 96 % classification accuracies across the 4 subjects using the decision fusion classification algorithm...|$|R
5000|$|The medial {{superior}} olive {{is thought}} to help locate the azimuth of a sound, that is, the angle {{to the left or}} right where the sound source is located. Sound elevation cues are not processed in the olivary complex. The fusiform cells of the dorsal cochlear nucleus (DCN), which are thought to contribute to localization in elevation, bypass the SOC and project directly to the inferior colliculus. Only horizontal data is present, but it does come from two different ear sources, which aids in the localizing of sound on the azimuth axis. The way in which the superior olive does this is by measuring the differences in time between two <b>ear</b> <b>signals</b> recording the same stimulus. Traveling around the head takes about 700 μs, and the medial superior olive is able to distinguish time differences much smaller than this. In fact, it is observed that people can detect interaural differences down to 10 μs. The nucleus is tonotopically organized, but the azimuthal receptive field projection is [...] "most likely a complex, nonlinear map".|$|R
50|$|As {{the first}} stage of CASA processing, the cochleagram creates a time-frequency {{representation}} of the input signal. By mimicking the components of the outer and middle <b>ear,</b> the <b>signal</b> is broken up into different frequencies that are naturally selected by the cochlea and hair cells. Because of the frequency selectivity of the basilar membrane, a filter bank is used to model the membrane, with each filter associated with a specific point on the basilar membrane.|$|R
50|$|Proprioceptors are receptors {{located in}} your muscles, tendons, joints {{and the inner}} <b>ear,</b> which send <b>signals</b> to the brain {{regarding}} the body's position. Aircraft pilots sometimes refer {{to this type of}} sensory input as the “seat of your pants”.|$|R
30|$|Coronal and {{sagittal}} reconstructions {{were obtained}} from the original axial dataset of the 3 D T 2 W sequence. All measurements were carried out using the open-source OsiriX software (available at: [URL] A 5 th-year radiology resident and a senior radiologist with 35  years of experience in head and neck imaging read the high-resolution T 2 W gradient-echo sequence, blinded to the results of the T 1 W contrast-enhanced sequence. They visually analysed the signal intensity of the perilymph (vestibular cistern and cochlea) and endolymph (saccule and utricle) {{on the side of the}} tumour in comparison with those on the normal side, as well as CSF signal in the cerebello-pontine angle on the side of the tumour. The signal intensity was rated on a three-level scale from 0 to 2 (0 [*]=[*]if judged similar to the contralateral normal side and to the CSF of the adjacent cerebello-pontine angle; 1 [*]=[*]if intermediate, i.e. neither as bright as the contralateral <b>ear’s</b> <b>signal</b> and the surrounding CSF nor as hypointense as the tumour [all 203 tumors were hypointense in the study]; 2 [*]=[*]if judged clearly abnormal, i.e. as hypointense as the tumour signal).|$|R
40|$|Tongue {{movement}} <b>ear</b> pressure <b>signals</b> {{have been}} used to generate controlling commands in human-machine interfaces. The objective {{of this study is to}} classify the controlled movement relating to an intended action from interfering signals that can be experienced. These interfering signals include but are not limited to, speech, coughing and drinking. Thus data was collected for six types of controlled movement and the various interfering signals, when subjects spoke, coughed or drank. The signal processing involves detection, segmentation, feature extraction and selection, and classification of tongue motions. The segmented signals were initially transformed into the wavelet packet domain, allowing for various features to be extracted based on statistical properties of the wavelet coefficients. These are then used as input into a Bayesian classifier under multivariate Gaussian assumptions. The average classification performance for identifying controlled movements and interfering tongue signals achieved 98 % and 93. 5 % respectively. Thus the classification of tongue movement <b>ear</b> pressure <b>signals</b> based on the wavelet packet transform is robust. The application of this Bayesian classification strategy significantly reduces the interference of controlling commands when considered within a human-machine interface system operating in a challenging environment. <br/...|$|R
40|$|Recently, an {{increased}} interest {{has been demonstrated}} in evaluating hearing aids (HAs) inside controlled, {{but at the same}} time, realistic sound environments. A promising candidate that employs loudspeakers for realizing such sound environments is the listener-centered method of higher-order ambisonics (HOA). Although the accuracy of HOA has been widely studied, it remains unclear to what extent the results can be generalized when (1) a listener wearing HAs that may feature multi-microphone directional algorithms is considered inside the reconstructed sound field and (2) reverberant scenes are recorded and reconstructed. For the purpose of objectively validating HOA for listening tests involving HAs, a framework was developed to simulate the entire path of sounds presented in a modeled room, recorded by a HOA microphone array, decoded to a loudspeaker array, and finally received at the ears and HA microphones of a dummy listener fitted with HAs. Reproduction errors at the <b>ear</b> <b>signals</b> and at the output of a cardioid HA microphone were analyzed for different anechoic and reverberant scenes. It was found that the diffuse reverberation reduces the considered time-averaged HOA reconstruction errors which, depending on the considered application, suggests that reverberation can increase the usable frequency range of a HOA system. 19 page(s...|$|R
50|$|When {{continuous}} {{white noise}} (with a frequency content below about 2000 Hz) is presented by headphones {{to the left}} and right ear of a listener, and given a particular interaural phase relationship between the left and right <b>ear</b> <b>signals,</b> a sensation of pitch (psychophysics) may be observed. Thus, stimulation of either ear alone gives rise to the sensation of white noise only, but stimulation of both ears together produces pitch. Therefore, as a special case of dichotic listening, such a pitch is called dichotic pitch or binaural pitch. Generally, a dichotic pitch is perceived somewhere in the head amidst the noisy sound filling the binaural space. To be more specific, the dichotic pitch is characterized by three perceptual properties: pitch value, timbre, and in-head position (lateralization). Experiments on dichotic pitch were motivated {{in the context of the}} study of pitch in general, and of the binaural system in particular, relevant for sound localization and separation of competing sound sources (see cocktail party effect). In the past, various configurations of dichotic pitch were studied and several auditory models were developed. The great challenge for psychophysical and physiological acoustics is to predict both the pitch value and pitch-image position in one model. For more information, references, audio demos etc. see more.|$|R
40|$|Different {{spatial sound}} {{reproduction}} techniques are evaluated using a binaural auditory model. <b>Ear</b> canal <b>signals</b> for different microphone techniques and different loudspeaker reproduction are simulated. Directional auditory cues are calculated and directional quality is discussed. The results of recording techniques for stereophonic listening explain the subjective opinions presented in literature: With coincident microphone techniques directionally fairly stable and consistent virtual sources can be produced, and with spaced microphones more spread and ambiguous virtual sources are achieved. In multichannel reproduction, {{none of the}} existing microphone techniques are found to produce good directional quality. Both coincident and spaced microphone techniques produce spread virtual sources. ...|$|R
40|$|This paper {{proposes a}} method to model Head-Related Transfer Functions (HRTFs) based on the shape {{and size of the}} outer <b>ear.</b> Using <b>signal</b> {{processing}} tools, such as Prony’s signal modeling method, a dynamic model of the pinna has been obtained, that completes the structural model of HRTFs used for digital audio spatialization. Listening tests conducted on 10 subjects showed that HRTFs created using this pinna model were 5 % more effective than generic HRTFs in the frontal plane. This model has been able to reduce the computational and storage demands of audio spatialization, while preserving a sufficient number of perceptually relevant spectral cues...|$|R
40|$|The {{ability to}} detect a target signal masked by noise is {{improved}} in normal-hearing listeners when interaural phase differences (IPDs) between the <b>ear</b> <b>signals</b> exist either in the masker or in the signal. To improve binaural hearing in bilaterally implanted cochlear implant (BiCI) users, a coding strategy providing the best possible access to IPD is highly desirable. In this study, we compared two coding strategies in BiCI users provided with CI systems from MED-EL (Innsbruck, Austria). The CI systems were bilaterally programmed either with the fine structure processing strategy FS 4 or with the constant rate strategy high definition continuous interleaved sampling (HDCIS). Familiarization periods between 6 and 12 weeks were considered. The effect of IPD was measured in two types of experiments: (a) IPD detection thresholds with tonal signals addressing mainly one apical interaural electrode pair and (b) with speech in noise in terms of binaural speech intelligibility level differences (BILD) addressing multiple electrodes bilaterally. The results in (a) showed improved IPD detection thresholds with FS 4 compared with HDCIS in four out of the seven BiCI users. In contrast, 12 BiCI users in (b) showed similar BILD with FS 4 (0. 6 [*]±[*] 1. 9 [*]dB) and HDCIS (0. 5 [*]±[*] 2. 0 [*]dB). However, no correlation between results in (a) and (b) both obtained with FS 4 was found. In conclusion, the degree of IPD sensitivity determined on an apical interaural electrode pair was not an indicator for BILD based on bilateral multielectrode stimulation...|$|R
40|$|A novel {{analysis}} method for binaural room impulse responses (BRIRs) is presented. It {{is based on}} the analysis of <b>ear</b> canal <b>signals</b> with continuous wavelet transform (CWT). Then, the crosswavelet transform (XWT) is used for detection of the direct sound and individual reflections from a BRIR. The new method seems to time-localize the reflections quite accurately. In addition, the proposed {{analysis method}} enables detailed study of the frequency content of the early reflections. The algorithm is tested with both measured and modeled impulse responses. A comparison with an FFT-based cross-spectrogram is made. The results show that XWT has potential in audio signal analysis. 1...|$|R
3000|$|The paper {{entitled}} [...] "Combination {{of adaptive}} feedback cancellation and binaural adaptive filtering in hearing aids" [...] (A. Lombard et al.) studies a system combining adaptive feedback cancellation and adaptive filtering connecting inputs from both <b>ears</b> for <b>signal</b> enhancement in hearing aids. Such a binaural system is analysed {{in terms of}} system stability, convergence of the algorithms, and possible interaction effects. As major outcomes of this study, a new stability condition adapted to the considered binaural scenario is presented, some commonly used feedback cancellation performance measures for the unilateral case are adapted to the binaural case, and possible interaction effects between the algorithms are identified.|$|R
50|$|Historically, {{the concept}} of tuning was to {{maintain}} a specific musical scale. When choruses sing in tune, the music reinforces itself with higher harmonics; aesthetically, the notes are more pleasing to the <b>ear.</b> But when <b>signals</b> are out of tune, dissonance occurs; the effect is most noticeable for musical groups of small children, {{who have not been}} trained to sing in tune.|$|R
40|$|AbstractThis study {{aimed to}} improve the {{accuracy}} and robustness of a real-time assistive human machine interface system by classifying between the controlled movements related tongue-movement <b>ear</b> pressure (TMEP) <b>signals</b> and the interfering signals. The controlled movement TMEP signals were collected during left, right, up, down, flicking and pushing tongue motions. The TMEP signals were processed and classified using detection, segmentation, feature extraction and classification. The segmented signals were decomposed into the time-scale domain using a wavelet packet transform. The variance of the wavelet packet coefficients and its ratio between low-to-high scales were defined as features and the intended tongue movement commands and interfering signals were classified using both a Bayesian and support vector machine (SVM) classifiers for comparison. The average classification accuracy for discriminating between the controlled movements and the interfering signals achieved 97. 8 % (Bayesian) and 98. 5 % (SVM). The classifiers were robust remaining at a similar performance level when generalised interferences from all subjects were used. It was shown that the Bayesian classifier performed better than the SVM in a real-time environment. The approach of combining the Bayesian classifier and the wavelet packet transform provides a robust and efficient method for a real-time assistive human machine interface based on tongue-movement <b>ear</b> pressure <b>signals...</b>|$|R
40|$|The {{ability to}} detect a target signal masked by noise is {{improved}} in normal-hearing listeners when interaural phase differences (IPDs) between the <b>ear</b> <b>signals</b> exist either in the masker or in the signal. To improve binaural hearing in bilaterally implanted cochlear implant (BiCI) users, a coding strategy providing the best possible access to IPDs is highly desirable. Outcomes of a previous study (Zirn, Arndt et al. 2016) revealed that a subset of BiCI users showed improved IPD detection thresholds with the fine structure processing strategy FS 4 compared to the constant rate strategy HDCIS using narrowband stimuli. In contrast, little differences between the coding strategies were found for broadband stimuli with regard to binaural speech intelligibility level differences (BILD) as an estimate of binaural unmasking. Compared to normalhearing listeners (7. 5 ± 1. 2 dB) BILD were small in BiCI users (around 0. 5 dB with both coding strategies). In the present work, we investigated the influence of binaural fitting parameters on BILD. In our cohort of BiCI users many were implanted with electrode arrays differing in length left versus right. Because this length difference typically corresponded to the distance of two electrode contacts the first modification of bilateral fitting was a tonotopic adjustment by deactivation of the most apical electrode contact on the side with the deeper inserted array (tonotopic approach). The second modification was {{the isolation of the}} residual, most apical electrode contacts by deactivation of the basally adjacent electrode contact on each side (tonotopic sparse approach). Applying these modifications, BILD improved by up to 1. 5 dB...|$|R
40|$|The {{perception}} in a listener {{of the existence}} of a “virtual” source of sound at a prescribed spatial position can be produced by ensuring that the acoustic signals at the listener's ears faithfully replicate those that would be produced by a “real” source at the same position. When loudspeakers are used to transmit the signals, it is necessary to pass the signals intended for presentation at the listener's ears through a matrix of filters that provide the inverse of the matrix of transfer functions that relates the loudspeaker input signals to the listener's <b>ear</b> <b>signals.</b> The characteristics of such filter matrices are profoundly influenced by the conditioning of the matrix to be inverted. This filter design problem is reviewed here by representing the loudspeakers as simple point monopole sources the head of the listener as a rigid sphere. The case of a virtual acoustic imaging system that uses two loudspeakers in order to reproduce the signals at the two ears is first described in some detail and previous work is reviewed. It is confirmed that the time domain response of the reproduced field is of long duration at frequencies where the inversion problem is ill-conditioned. The influence of the presence of the listener's head on this time domain behaviour is also evaluated. The principle is then extended to four input–four output reproduction systems and the computational model is used to explain some previous experimental observations. Finally, the conditioning of five input–four output systems is also examined and shown to have some potentially desirable characteristics...|$|R
50|$|For {{channel-associated}} signaling (CAS) in Digital Signal 1 (T1) trunks {{that use}} E and M signaling (earth & magneto, or <b>ear</b> & mouth <b>signaling),</b> {{there are only}} two voice channel states. A channel is idle/on hook when there is no call on it or seized/off-hook/energized by an active call. There is no separate state for answered. It mimics an analog loopstart or groundstart line.|$|R
40|$|A {{recently}} proposed auditory {{model is}} examined using simulated binaural masking level difference (BMLD) and dichotic pitch (Huggins and binaural edge pitch) stimuli. The model {{is based on}} calculating the instantaneous interaural level difference, i. e. {{the difference between the}} left and right <b>ear</b> neural <b>signals.</b> The model output produces pronounced maximum at the signal frequency with BMLD stimulus. Both dichotic pitch stimuli produce a notable maxima at the pitch frequencies. Although the model can thus be interpreted to predict known psychoacoustical results, an exact quantitative comparison with the model responses and data from BMLD and binaural pitch experiments is not performed. Rather, this paper serves as a ”proof of concept”. 1...|$|R
