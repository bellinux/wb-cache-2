139|192|Public
25|$|In {{some cases}} local tone mapping is used {{even though the}} dynamic range of the source image could be {{captured}} on the target media, either to produce the distinctive appearance of a locally tone mapped image, or to produce an image closer to the photographer's artistic vision of the scene by removing sharp contrasts, which often look unattractive. In some cases, tone mapped images are produced from a single exposure which is then manipulated with conventional processing tools to produce the inputs to the HDR image generation process. This avoids the artifacts that can appear when different exposures are combined, due to moving objects in the scene or camera shake. However, when tone mapping is applied to a single exposure in this way, the <b>intermediate</b> <b>image</b> has only normal dynamic range, {{and the amount of}} shadow or highlight detail that can be rendered is only that which was captured in the original exposure.|$|E
60|$|In these images, the conferring, the abstracting, and the modifying {{powers of}} the Imagination, {{immediately}} and mediately acting, are all brought into conjunction. The stone is endowed with something {{of the power of}} life to approximate it to the sea-beast; and the sea-beast stripped of some of its vital qualities to assimilate it to the stone; which <b>intermediate</b> <b>image</b> is thus treated for the purpose of bringing the original image, that of the stone, to a nearer resemblance to the figure and condition of the aged Man; who is divested of so much of the indications of life and motion as to bring him {{to the point where the}} two objects unite and coalesce in just comparison. After what has been said, the image of the cloud need not be commented upon.|$|E
50|$|A phase {{telescope}} or Bertrand lens is {{an optical}} device used in aligning the various optical {{components of a}} light microscope. In particular it allows observation of the back focal plane of the objective lens and its conjugate focal planes. The phase telescope/Bertrand lens is inserted into the microscope in place of an eyepiece to move the <b>intermediate</b> <b>image</b> plane {{to a point where}} it can be observed.|$|E
50|$|Most wiggle images {{use only}} two images, {{yielding}} a jerky image. A smoother image can be composed by using several <b>intermediate</b> <b>images</b> {{and using the}} left and right images as end images of the <b>image</b> sequence. If <b>intermediate</b> <b>images</b> are not available, approximate images can be computed from the end images using techniques known as view interpolation. The two end images may be displayed for a longer time than the <b>intermediate</b> <b>images</b> to allow inspection of details in {{the left and right}} images.|$|R
40|$|Computed Tomography (CT) is an X-ray {{technique}} that produces 2 D biomedical slice image data. In such images, the slice spacing {{is much greater}} than the spacing of individual samples on a slice. Thus, a robust interpolation method to generate quality <b>intermediate</b> <b>images</b> for the reconstruction of 3 D biomedical model is necessary. The subject {{of this paper is to}} compare three different interpolation techniques: linear, cubic spline, and Fourier interpolation for generating intermediate slices. Linear interpolation is shown to be the best of the three interpolation techniques. This research also shows that without the image segmentation or the matching process poor <b>intermediate</b> <b>images</b> will be generated...|$|R
40|$|Image {{morphing}} {{techniques can}} create a smooth transition between two images. How-ever, {{one of the main}} weakness of the image morphing technique is that <b>intermediate</b> <b>images</b> in the transition often have physically incorrect shading such as highlights and shadows. Moreover, we cannot alter viewing and lighting conditions when creating the <b>intermediate</b> <b>images.</b> That is because those images are obtained by simply interpolat-ing pixel intensities of the two 2 D images without knowledge of 3 D object shape and re ectance properties. In this context, 3 D shape morphing techniques have a denite advantage in that arbitrary viewing and illumination conditions can be used for creating new images. Unfortunately, previous 3 D morphing techniques do not account for object surface re ectance properties or re ection models when generating <b>intermediate</b> <b>images.</b> This often results in undesired shading artifacts. In this paper, we consider a new ap-proach for 3 D shape and re ectance morphing of two real 3 D objects. Our morphing method consists of two components: shape and re ectance property measurement, and smooth interpolation of those measured properties. The measured shape and re ectance parameters are used to compute intermediate shape and re ectance parameters. Finally...|$|R
50|$|Wavefront Coding with cubic phase masks {{works to}} blur the image uniformly using a cubic shaped {{waveplate}} so that the <b>intermediate</b> <b>image,</b> the optical transfer function, is out of focus by a constant amount. Digital image processing then removes the blur and introduces noise depending upon the physical characteristics of the processor. Dynamic range is sacrificed to extend the depth of field depending upon the type of filter used. It can also correct optical aberration.|$|E
50|$|VidFIRE was {{developed}} by Peter Finklestone to address the motion differences caused by the telerecording process. It uses motion estimation software to create an <b>intermediate</b> <b>image,</b> which exists temporally between two film frames. For example, if all frames of a twenty-five-minute film recording were processed, {{the result would be}} double the amount of frames and a new running length of fifty minutes. Playback at this stage (at 25 frame/s) gives smooth movement at half speed, due to the presence of interpolated images.|$|E
50|$|In {{order to}} test the {{alignment}} of components on the light source image plane, the eyepiece must be removed to allow observation of the <b>intermediate</b> <b>image</b> plane (the position of the eyepiece diaphragm) either directly or by using a phase telescope/Bertrand lens. The light source (e.g. the bulb filament) and {{the edges of the}} condenser diaphragm should appear in focus. Any optical components at the back focal plane of the objective (e.g. the phase ring for phase contrast microscopy) and at the condenser diaphragm (e.g. the annulus for phase contrast microscopy) should also appear in focus.|$|E
40|$|When {{creating}} a cel animation, the animators often use 3 D character models {{to add some}} effects on the character or to generate <b>intermediate</b> <b>images</b> between the key frames. However, it is a troublesome and time-consuming task to create a 3 D model. In this paper, we present an easy-to-use approach for {{creating a}} set of consistent 3 D character models from the user-specified strokes on a 2 D image sequence. The created consistent 3 D models {{can be used in}} cel animation editing systems for adding shadowing effects, textures, etc. Moreover, since the vertices of the consistent 3 D models have one-to-one correspondence among the frames, by using 3 D morphing techniques, this approach {{can also be used to}} generate <b>intermediate</b> <b>images</b> between the key frames...|$|R
40|$|Time domain image interpolation, or image morphing, {{refers to}} a class of {{techniques}} for generating a set of smoothly changing <b>intermediate</b> <b>images</b> between two given images. Numerous methods have been proposed for this problem. In this note, we present a novel approach based on the theory of optimal mass transport...|$|R
50|$|Distributing objects among {{processing}} units {{is often}} referred to as sort last rendering. It provides good data scaling and can provide good performance scaling, but it requires the <b>intermediate</b> <b>images</b> from processing nodes to be alpha composited to create the final image. As the image resolution grows, the alpha compositing overhead also grows.|$|R
5000|$|It is {{difficult}} for a program to draw a display so that pixels do not change more than once. For instance, when updating a page of text, {{it is much easier}} to clear the entire page and then draw the letters than to somehow erase all the pixels that are not in both the old and new letters. However, this <b>intermediate</b> <b>image</b> is seen by the user as flickering. In addition, computer monitors constantly redraw the visible video page (at around 60 times a second), so even a perfect update may be visible momentarily as a horizontal divider between the [...] "new" [...] image and the un-redrawn [...] "old" [...] image, known as tearing.|$|E
50|$|In {{some cases}} local tone mapping is used {{even though the}} dynamic range of the source image could be {{captured}} on the target media, either to produce the distinctive appearance of a locally tone mapped image, or to produce an image closer to the photographer's artistic vision of the scene by removing sharp contrasts, which often look unattractive. In some cases, tone mapped images are produced from a single exposure which is then manipulated with conventional processing tools to produce the inputs to the HDR image generation process. This avoids the artifacts that can appear when different exposures are combined, due to moving objects in the scene or camera shake. However, when tone mapping is applied to a single exposure in this way, the <b>intermediate</b> <b>image</b> has only normal dynamic range, {{and the amount of}} shadow or highlight detail that can be rendered is only that which was captured in the original exposure.|$|E
5000|$|Dmitry Andreev {{devised a}} framing system {{that gave the}} {{illusion}} of operating at 60 frames per second (FPS), despite running at 30FPS. To familiarize himself with the process, Andreev observed various 120 Hz television sets that incorporated two frames in producing an <b>intermediate</b> <b>image,</b> resulting in a smoother and clearer picture. [...] The design team utilized a variety of interpolation techniques on multiple parts of an image, such as transparency and reflection. Andreev stated that [...] "as soon as I got back home, I started to play with it and soon after that realised {{that there are a}} lot of issues. Mostly the artifacts of a different kind, that appear in more or less complex scenes, as well as performance issues ...." [...] In response to the difficulties, he constructed a prototype that performed several enhancement techniques which examined images for vectors that demonstrated how [...] "elements of the image would move from one frame to the next". The LucasArts coder realized that such processes could be repackaged for a different use. [...] "We already know how things are moving as we have full control over them. This way we don't need to do any kind of estimation," [...] he professed.|$|E
5000|$|... #Caption: An <b>intermediate</b> power <b>image</b> of an endolymphatic sac tumor with bone (upper left).|$|R
3000|$|... b, c, e, edge {{directional}} interpolation {{was utilized}} {{to generate the}} <b>intermediate</b> <b>images</b> with quincuncial patterns [22], and then a full color image was obtained by applying the MSG based CI algorithm. Then, G was estimated by using a color correction matrix in case of the RWB CFA, and the HDR reconstruction algorithm was applied if the image contained W.|$|R
40|$|Time domain image interpolation, or image morphing, {{refers to}} a class of {{techniques}} for generating a series of smoothly changing <b>intermediate</b> <b>images</b> between two given related images. In this note, we present a novel approach based on the theory of optimal mass transport, using mutual information (MI) as the similarity measurement. The potential applications also include image registration, compression and coding...|$|R
40|$|This paper {{presents}} an efficient keyframeless image-based rendering technique. An <b>intermediate</b> <b>image</b> {{is used to}} exploit the coherences among neighboring frames. The pixels in the <b>intermediate</b> <b>image</b> are first rendered by a ray-casting method and then warped to the <b>intermediate</b> <b>image</b> at the current viewpoint and view direction. We use an offset buffer to record the precise positions of these pixels in the <b>intermediate</b> <b>image.</b> Every frame is generated in three steps: warping the <b>intermediate</b> <b>image</b> onto the frame, filling in holes, and selectively rendering a group of ”old ” pixels. By dynamically adjusting {{the number of those}} ”old ” pixels in the last step, the workload at every frame can be balanced. The pixels generated by the last two steps make contributions to the new <b>intermediate</b> <b>image.</b> Unlike occasional keyframes in conventional image-based rendering which need to be totally rerendered, intermediate images only need to be partially updated at every frame. In this way, we guarantee more stable frame rates and more uniform image qualities. The <b>intermediate</b> <b>image</b> can be warped efficiently by a modified incremental 3 D warp algorithm. As a specific application, we demonstrate our technique with a voxel-based terrain rendering system. Keywords: Image-based rendering, ray casting, voxel-based modeling, terrain rendering. ...|$|E
40|$|The device (100) has a micro-lens array (103) with {{multiple}} micro-lenses (107 a to 107 n) which are grouped in multiple micro-lens groups (105). Multiple pixel arrays (109) are arranged {{at a distance}} from each other. Each micro-lens is designed to form associated regions (111 a to 111 n) of an <b>intermediate</b> <b>image</b> (101) on a partial region of pixel arrays associated to micro-lens groups. The pixel arrays are laterally less extensive than the associated micro-lens groups. USE: Device for receiving an <b>intermediate</b> <b>image</b> generated by a main lens of a focused-type plenoptic camera (Claimed). ADVANTAGE: The device has a micro-lens array {{with multiple}} micro-lenses which are grouped in multiple micro-lens groups, where multiple pixel arrays are arranged {{at a distance from}} each other, and hence ensures an improved <b>intermediate</b> <b>image</b> receiving device with enhanced depth resolution and reduced minimum possible depth of focus, where manufacturing cost of the <b>intermediate</b> <b>image</b> receiving device is reduced. The drawing shows schematic sectional view of an <b>intermediate</b> <b>image</b> receiving device. 100 : <b>Intermediate</b> <b>image</b> receiving device 101 : <b>Intermediate</b> <b>image</b> 103 : Micro-lens array 105 : Micro-lens groups 107 a to 107 n : Micro-lenses 109 : Pixel arrays 111 a to 111 n : Associated regions...|$|E
40|$|An <b>intermediate</b> <b>image</b> beta-ray {{spectrometer}} {{has been}} constructed utilizing the focusing properties of a U-shaped magnetic field distribution. The construction {{and performance of}} the instrument are discussed. A transmission of ten percent has been attained with a resolution of about six percent. A scintillation spectrometer is {{used in conjunction with}} the <b>intermediate</b> <b>image</b> spectrometer for the purpose of making coincidence measurements...|$|E
40|$|A typical {{ray tracing}} {{algorithm}} traces a ray through each screen-pixel and spawns secondary rays at ray-object intersection points. Unlike traditional ray tracers which follow these rays recursively, we assign a priority value to each newly spawned ray and insert {{it into a}} priority queue. The priority assigned to each ray can {{be based on a}} variety of criteria, some of which we explore here. The next ray we trace is always the one with the highest priority in the queue. Occasionally, we trigger display updates when a checkpoint or predefined threshold is reached, providing <b>intermediate</b> <b>images</b> for review and evaluation. Classical ray tracers, once given the rendering specifications, are not controllable by the user. The priority-driven ray tracing, on the other hand, provides the user with a mechanism to steer rendering and deliver <b>intermediate</b> <b>images</b> amid processing. This paper describes the illumination model of the non-recursive priority-driven ray tracer and evaluates its memory and [...] ...|$|R
40|$|DE 19834718 A UPAB: 20000320 NOVELTY - The {{system is}} {{provided}} with units for carrying out the following work; Selection of the areas (11 - 14) of the textured surface (10) in a learning mode and assigned these areas as textural prototypes (11 a, 13 a). Extracting of <b>intermediate</b> <b>images</b> from further textured surfaces of a random test, which does not correspond to the learning random test of a continuous or discontinuous production process or medical receptions. The extracted <b>intermediate</b> <b>images</b> are compared with the textural prototypes, in order to automatically classify the textured surface of the further random test and to identify irregularities therein. USE - Industrial quality control. ADVANTAGE - Following determination of learning random test with manual classification the various classes are specified automatically, with data obtained from production. operation compared so that class assignment good or bad can be made. The new method is esp. good for the classification of colored textured surfaces with automatic evaluation...|$|R
50|$|The {{morphing}} function {{can transform}} photos {{from one person}} to another. To do so, the persons must be photographed in similar postures and their shapes need to be cut out {{with the help of the}} software. After doing so, points of support are set on noticeable spots (eyes, nose, mouth, etc.). Finally, the software calculates the <b>intermediate</b> <b>images</b> and creates a video of the transformation.|$|R
40|$|A {{framework}} of demosaicing and superresolution for color filter array (CFA) via residual image reconstruction and sparse representation is presented. Given the <b>intermediate</b> <b>image</b> produced by certain demosaicing and interpolation technique, a residual image between the final reconstruction {{image and the}} <b>intermediate</b> <b>image</b> is reconstructed using sparse representation. The final reconstruction image has richer edges and details {{than that of the}} <b>intermediate</b> <b>image.</b> Specifically, a generic dictionary is learned from a large set of composite training data composed of intermediate data and residual data. The learned dictionary implies a mapping between the two data. A specific dictionary adaptive to the input CFA is learned thereafter. Using the adaptive dictionary, the sparse coefficients of intermediate data are computed and transformed to predict residual image. The residual image is added back into the <b>intermediate</b> <b>image</b> to obtain the final reconstruction image. Experimental results demonstrate the state-of-the-art performance in terms of PSNR and subjective visual perception. Comment: the paper has been accepted by a journa...|$|E
3000|$|To convolve {{an image}} with a {{separable}} filter kernel, each {{row in the}} image is convolved with the horizontal projection to obtain an <b>intermediate</b> <b>image.</b> Next, we convolve each column of the <b>intermediate</b> <b>image</b> with the vertical projection of the filter. Hence the resulting image {{is identical to the}} direct convolution of the original image and the filter kernel. The convolution of an [...]...|$|E
3000|$|... 2 {{is used on}} {{this feature}} vector and second <b>intermediate</b> <b>image</b> hash, n[*]=[*][p t] (n of length 48 digits) is obtained.|$|E
40|$|A new {{algorithm}} for partition sequence interpolation is proposed. In a coding context, such a tool {{is necessary}} to reach high compression rates. Our scheme relies on a region-by-region approach. We propose a region ordering, based on an error criterion. Before ordering, some regions are merged according to a motion criterion. The shape of each region changes continuously, and the <b>intermediate</b> <b>images</b> are built from the new shaped regions with a dead leave model...|$|R
40|$|Photoacoustic {{tomography}} (PAT) is {{a relatively}} recent imaging modality that is promising for breast cancer detection and breast screening. It combines the high intrinsic contrast of optical radiation with acoustic imaging at submillimeter spatial resolution through the photoacoustic effect of absorption and thermal expansion. How-ever, image reconstruction from boundary measurements of the propagating wave field is still a challenging inverse problem. Here we propose a new theoretical framework, for which we coin the term eigensensing, to recover the heat absorption profile of the tissue. One of the main features of our method {{is that there is}} no explicit forward model that needs to be used within a (usually) slow iterative scheme. Instead, the eigensensing principle allow us to computationally obtain several <b>intermediate</b> <b>images</b> that are blurred by known convolution kernels which are chosen as the eigenfunc-tions of the spatial Laplace operator. The source image can then be reconstructed by a joint deconvolution algorithm that uses the <b>intermediate</b> <b>images</b> as input. Moreover, total variation regulariza-tion is added to make the inverse problem well-posed and to favor piecewise-smooth images. Index Terms — Photoacoustic Tomography, Wave equation, source imaging, joint sparsity, deconvolution, total variatio...|$|R
30|$|Next, {{we use the}} {{approach}} described in Section 3.3 for the morphing of two objects O 1 and O 2 into each other. As commented before, the morphing is automatic: we start from two images I 1 and I 2 to which the multiGaussian mixture energy function segmentation method is applied. For both images, an initial regular cage is used. Once segmented cages V 1 and V 2 are obtained, intermediate cages can be obtained, and corresponding <b>intermediate</b> <b>images</b> are computed using interpolation.|$|R
3000|$|Using the {{diagnosed}} transformation field, {{the experimental}} image is resampled over new grid points to affect this transformation and yield the <b>intermediate</b> <b>image,</b> I’ [...]...|$|E
40|$|We {{develop a}} {{framework}} for assessing the quality of stereoscopic images that have been afflicted by possibly asymmetric distortions. An <b>intermediate</b> <b>image</b> is generated which when viewed stereoscopically is designed to have a perceived quality close {{to that of the}} cyclopean image. We hypothesize that performing stereoscopic QA on the <b>intermediate</b> <b>image</b> yields higher correlations with human subjective judgments. The experimental results confirm the hypothesis and show that the proposed framework significantly outperforms conventional 2 D QA metrics when predicting the quality of stereoscopically viewed images that may have been asymmetrically distorted...|$|E
40|$|The {{invention}} {{relates to}} a device (1) for limiting a transmitted optical power, comprising {{at least one}} focusing optical unit (11) which images incident light onto an <b>intermediate</b> <b>image</b> plane (21), wherein at least one first protective element (10) is arranged in the <b>intermediate</b> <b>image</b> plane, characterized in that the device (1) furthermore contains a beam shaping unit (15) designed to generate a predefineable distribution of the intensity relative {{to the location of}} the incident light. Furthermore, the invention relates to a distance measuring apparatus comprising such a device, and to a method for limiting an optical power...|$|E
40|$|We {{studied the}} visual {{recognition}} of the facial emotions, on 20 chronic and institutionalized schizophrenics. They were paired ill age, sex, educational level with a reference group. We used the MARIE battery which uses a binary pairing in a continuum of <b>intermediate</b> <b>images.</b> The {{recognition of the}} disgust is decreased followed by the fear. The surprise is the least overdrawn. The recognition of anger and the bipolar series (emotion-emotion) is strictly identical {{to that of the}} controls. (C) 2009 Elsevier Masson SAS. All rights reserved...|$|R
40|$|A coder {{for light}} elds is presented. Due {{to the large}} amount of data needed to {{represent}} a complete light eld, a hierarchical decomposition is employed. The full light eld is built up by recursively predicting in-termediate images from a small subset of light-eld im-ages. <b>Intermediate</b> <b>images</b> are predicted by disparity-compensating multiple surrounding images. The pre-dicted images are rened using DCT coding. Rate-distortion measurements for two standard light elds verify the eÆciency of the proposed scheme. Compres-sion ratios of 1000 : 1 are achieved at acceptable quality of the rendered views. 1...|$|R
40|$|Abstract- Image scaling is {{the process}} of {{resizing}} a digital image, wherein an image is converted from one resolution/dimension to another resolution/dimension without losing the visual content. It has many terminologies in literature such as Image Interpolation, image re-sampling, digital zooming, image magnification or enhancement, etc [1] [2]. Image interpolation algorithms can be grouped in two categories, non-adaptive and adaptive [3] [4]. Interpolating from lower to higher resolution it is termed as up-scaling / up-sampling and from higher to lower resolution it is termed as down-scaling / down-sampling. In this article we propose a non-adaptive image interpolation algorithm to scale images for any given scaling ratio with an enhancement scheme to ensure a better image quality metric (PSNR in dB) of the scaled image. In the proposed algorithm, two reference images, one with higher resolution and another with lower resolutions are generated using the original input image and scaling factor. Later, two <b>Intermediate</b> <b>images</b> are generated from these reference images, using the filtering based re-sampling Lanczos 3 kernel. Finally, the desired scaled image is obtained by linearly interpolating the two <b>intermediate</b> <b>images.</b> The desired scaled image is then passed through an enhancement phase, where a High Boost Filter is employed, to obtain an enhanced scaled image. Finally the algorithm is compared with the other interpolating techniques such as Bilinear Interpolation, B-Spline interpolation, Lanczos interpolation etc. in terms of the Imag...|$|R
