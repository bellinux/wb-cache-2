8|241|Public
50|$|A more major update was the MK-747, which {{added a new}} antenna from Raytheon, the Diplex <b>Gating</b> <b>Unit</b> (DGU), a {{bandpass}} filter and other modifications, to produce the AN/FPS-91 and 91A. The similar MK-748 applied to the -60 series resulted in the AN/FPS-64A, -65A, -66A and -67A. Canadian AN/FPS-87s were also converted, becoming AN/FPS-93 and 93A. These units were used with the SAGE system.|$|E
40|$|ARGOS, {{the wide}} field Laser Guide Stars {{adaptive}} optics {{system for the}} Large Binocular Telescope, is now entering its installation phase. In the meanwhile, we have started a test bench in order to integrate various Argos sub-systems and demonstrate its wavefront measurements. To this objective, we rst validate three key components of the Argos wavefront sensor which we then integrate together. The test bench therefore comprises the Argos wavefront camera system - including a large frame, fast framerate, high quantum eciency and low readout noise pnCCD -, the slope computer, and a optical <b>gating</b> <b>unit.</b> While we present here the demonstration of those three key components, {{it is also a}} step to their system level integration that enables us to validate the wavefront measurements in term of noises, timing and computation. In the near future, those system will be integrated to the wavefront sensor system of ARGOS...|$|E
40|$|Abstract. Intravascular Ultrasound(IVUS) is an imaging {{technology}} which provides cross-sectional images of internal coronary vessel structures. The IVUS frames are acquired by pulling the catheter {{back with a}} motor running at a constant speed. However, during the pullback, some artifacts occur due to the beating heart. These artifacts cause inaccurate measurements for total vessel and lumen volume and limitation for further processing. Elimination of these artifacts are possible with an ECG (electrocardiogram) signal, which determines the time interval corresponding to a particular phase of the cardiac cycle. However, using ECG signal requires a special <b>gating</b> <b>unit,</b> which causes loss of important information about the vessel, and furthermore, ECG gating function may not be available in all clinical systems. To address this problem, we propose an image-based gating technique based on manifold learning. Quantitative tests are performed on 3 different patients, 6 different pullbacks and 24 different vessel cuts. In order to validate our method, {{the results of our}} method are compared to those of ECG-Gating method...|$|E
40|$|The {{subspace}} Restricted Boltzmann Machine (subspaceRBM) is a third-order Boltzmann machine where multiplicative {{interactions are}} between one visible and two hidden units. There {{are two kinds}} of hidden <b>units,</b> namely, <b>gate</b> <b>units</b> and subspace units. The subspace units reflect variations of a pattern in data and the <b>gate</b> <b>unit</b> is responsible for activating the subspace <b>units.</b> Additionally, the <b>gate</b> <b>unit</b> {{can be seen as a}} pooling feature. We evaluate the behavior of subspaceRBM through experiments with MNIST digit recognition task, measuring reconstruction error and classification error. Comment: 7 page...|$|R
50|$|The {{phase shift}} can be {{specified}} either in absolute terms (in delay chain <b>gate</b> <b>units),</b> or {{as a proportion of}} the clock period, or both.|$|R
40|$|The {{purpose of}} this master thesis {{was to create a}} model of an IGBT in a single pulse test circuit and connect this model to a model of a <b>Gate</b> <b>Unit.</b> The IGBT model and the single pulse test circuit were both {{implemented}} in MATLAB and the <b>Gate</b> <b>Unit</b> was implemented in Simulink. The {{purpose of this}} model was to test the actions of the <b>gate</b> <b>unit,</b> so that the initial tuning could be done before going to the lab. Since no tests were performed in the lab, {{it was not possible to}} see how much of the testing that could have been done by simulations. However, the actions of the IGBT model much resembled the actions of the real component, even though some drawbacks were clear, such as the lack of tail current and tail voltage. These comparisons could be made between simulated characteristics and recordings from a previous test with the same component...|$|R
40|$|The RCAF has {{programmed}} a new antenna {{for their}} dual-channel radar, the AN/FPS- 508. Fitted to this antenna is a Vari-Polarizer which permits simultaneous operation of both channels of the radar, i. e., operation in a diplex mode. At {{the request of}} the RCAF, the National Research Council developed the Diplex <b>Gating</b> <b>Unit</b> which automatically selects the outputs of the receivers in the channel affected least in a jammed environment. These outputs are then fed to the AN/FST- 2 Data Transmission Equipment and the AJ Console. The construction and testing of the experimental and service test models are described in this report. A solution is given to the crosstalk problem that is likely to be encountered {{with the use of the}} Wide-Band Dicke Fix receivers when operating in the diplex mode. Minor modifications are recommended for use in a production model in order to increase reliability and ease of maintenance. Peer reviewed: NoNRC publication: Ye...|$|E
40|$|Intravascular Ultrasound(IVUS) is an imaging {{technology}} which provides cross-sectional images of internal coronary vessel struc- tures. The IVUS frames are acquired by pulling the catheter {{back with a}} motor running at a constant speed. However, during the pullback, some artifacts occur due to the beating heart. These artifacts cause inaccu- rate measurements for total vessel and lumen volume and limitation for further processing. Elimination of these artifacts are possible with an ECG (electrocardiogram) signal, which determines the time interval cor- responding to a particular phase of the cardiac cycle. However, using ECG signal requires a special <b>gating</b> <b>unit,</b> which causes loss of impor- tant information about the vessel, and furthermore, ECG gating function may not be available in all clinical systems. To address this problem, we propose an image-based gating technique based on manifold learning. Quantitative tests are performed on 3 different patients, 6 different pull- backs and 24 different vessel cuts. In order to validate our method, {{the results of our}} method are compared to those of ECG-Gating method...|$|E
40|$|As {{the means}} for regularization, in {{providing}} the solution uniqueness, we propose spline networks which evaluate the derivative norm of the network output {{as well as the}} mean square error of the network. These networks incorporate the power units in the hidden layer and a <b>gating</b> <b>unit</b> to select the best power unit. We confirm the effectiveness of the proposed method by comparing the network output with those of the non-regularized network and the one with the Gaussian regularizer. We also consider the application of the network in medical field : In the neuro-surgical operations, the acquisition of accurate relationship between the patient and the tomogram, e. g. CT or MRI, is important to minimize the damages to the normal tissue and to shorten the surgical time. We propose a navigation system which calculates the 3 -D probe position from 2 -D values given by CCD images, with the consideration on the ill-posedness of the problem arising in the nonlinear transformation of the positional information with the neural network, using fewer tutorial data...|$|E
40|$|We {{introduce}} a neural network architecture and a learning algorithm to produce factorized symbolic representations. We propose to learn these concepts by observing consecutive frames, letting {{all the components}} of the hidden representation except a small discrete set (<b>gating</b> <b>units)</b> be predicted from the previous frame, and let the factors of variation in the next frame be represented entirely by these discrete <b>gated</b> <b>units</b> (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3 D transformations and Atari 2600 games. Comment: Under review as a workshop paper for ICLR 201...|$|R
50|$|It is {{a common}} {{tradition}} for any military unit visiting the installation to paint a design {{on one of the}} large rock formations near the main <b>gate.</b> <b>Units</b> of all types and locations are represented.|$|R
40|$|Recently {{recurrent}} {{neural networks}} (RNN) {{has been very}} successful in handling sequence data. However, understanding RNN and finding the best practices for RNN is a difficult task, partly {{because there are many}} competing and complex hidden units (such as LSTM and GRU). We propose a <b>gated</b> <b>unit</b> for RNN, named as Minimal <b>Gated</b> <b>Unit</b> (MGU), since it only contains one gate, which is a minimal design among all <b>gated</b> hidden <b>units.</b> The design of MGU benefits from evaluation results on LSTM and GRU in the literature. Experiments on various sequence data show that MGU has comparable accuracy with GRU, but has a simpler structure, fewer parameters, and faster training. Hence, MGU is suitable in RNN's applications. Its simple architecture also means that it is easier to evaluate and tune, and in principle it is easier to study MGU's properties theoretically and empirically...|$|R
40|$|In this work, {{we propose}} a novel {{recurrent}} neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global <b>gating</b> <b>unit</b> for {{each pair of}} layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions. Comment: 9 pages, removed appendi...|$|E
40|$|It is {{demonstrated}} that the monomeric and dimeric structures of the nicotinic acetylcholine receptor of Torpedo californica electric tissue, reconstituted in planar lipid bilayers, are functionally different. The native dimer D of Mr 500, 000 (heavy-form) exhibits a "single" channel conductance about {{twice as large as}} that of the monomer M of Mr 250, 000 (light form). Under conditions where monomers aggregate, the conductance changes from the level of the monomer M to that of dimers M 2. The dimer conductances (D and M 2) seem to result from synchronous opening and closing of the two channels in the dimer, giving the impression of "single channel" activity. This channel cooperativity is apparently mediated by noncovalent interactions between the two monomers, since it requires no disulfide linkage between monomers. Both the monomers M and the dimers D and M 2 show at least one substate of lower conductivity. The relative population of the two conductance levels depends on the ion type (Na+ and K+), indicating ion-specific channel states. Since the channel conductance of isolated dimers resembles those obtained from unextracted microsacs, the dimer with two synchronized channels appears to be the in vivo predominant <b>gating</b> <b>unit.</b> In the linear association of dimers, observed in the native membrane, channel synchronization may extend to more than two channels as suggested by oligomeric channel cooperativity in associations of monomers and dimers...|$|E
40|$|We {{present a}} {{complimentary}} objective for training recurrent neural networks (RNN) with <b>gating</b> <b>units</b> that helps with regularization and interpretability of the trained model. Attention-based RNN models have shown success in many difficult sequence to sequence classification problems with {{long and short}} term dependencies, however these models are prone to overfitting. In this paper, we describe how to regularize these models through an L 1 penalty on the activation of the <b>gating</b> <b>units,</b> and show that this technique reduces overfitting {{on a variety of}} tasks while also providing to us a human-interpretable visualization of the inputs used by the network. These tasks include sentiment analysis, paraphrase recognition, and question answering. Comment: In review at NIP...|$|R
40|$|The use of {{simulation}} tools is {{of great}} value {{in the process of}} developing new power electronics devices and new converter topologies. The development time of power electronics and converter topologies can be shortened by the use of simulation tools. Sentaurus Device combines a good numerical model of power electronics devices and complex external circuits. It’s capable to simulate the IGBT switching behaviours applying different <b>gate</b> <b>unit</b> settings in HVDC Light™. The numerical model of the IGBT is studied in the thesis. The results of the IGBT fine-tuning are carried out. Implementing the IGBT model with its <b>gate</b> <b>unit</b> circuit using Sentaurus Device, the impact of various gate drive settings on the IGBT switching losses is demonstrated...|$|R
40|$|This paper {{introduces}} two recurrent {{neural network}} structures called Simple <b>Gated</b> <b>Unit</b> (SGU) and Deep Simple <b>Gated</b> <b>Unit</b> (DSGU), which are general structures for learning long term dependencies. Compared to traditional Long Short-Term Memory (LSTM) and <b>Gated</b> Recurrent <b>Unit</b> (GRU), both structures require fewer parameters and less computation time in sequence classification tasks. Unlike GRU and LSTM, which {{require more than}} one gates to control information flow in the network, SGU and DSGU only use one multiplicative gate to control the flow of information. We show that this difference can accelerate the learning speed in tasks that require long dependency information. We also show that DSGU is more numerically stable than SGU. In addition, we also propose a standard way of representing inner structure of RNN called RNN Conventional Graph (RCG), which helps analyzing the relationship between input units and hidden units of RNN. Comment: This paper has been withdrawn by the author due to lacking of enough experiment...|$|R
40|$|Recurrent neural {{networks}} with {{various types of}} hidden units {{have been used to}} solve a diverse range of problems involving sequence data. Two of the most recent proposals, <b>gated</b> recurrent <b>units</b> (GRU) and minimal <b>gated</b> <b>units</b> (MGU), have shown comparable promising results on example public datasets. In this paper, we introduce three model variants of the minimal <b>gated</b> <b>unit</b> (MGU) which further simplify that design by reducing the number of parameters in the forget-gate dynamic equation. These three model variants, referred to simply as MGU 1, MGU 2, and MGU 3, were tested on sequences generated from the MNIST dataset and from the Reuters Newswire Topics (RNT) dataset. The new models have shown similar accuracy to the MGU model while using fewer parameters and thus lowering training expense. One model variant, namely MGU 2, performed better than MGU on the datasets considered, and thus may be used as an alternate to MGU or GRU in recurrent {{neural networks}}. Comment: 5 pages, 3 Figures, 5 Table...|$|R
5000|$|A {{good example}} of {{time-controlled}} noise gating is the well-known [...] "gated reverb" [...] effect heard on the drums on the Phil Collins hit single [...] "In the Air Tonight", created by engineer-producer Hugh Padgham, in which the powerful reverberation added to the drums is cut off by the noise gate after a few milliseconds, rather than being allowed to decay naturally. This can also be achieved by: sending the 'dry' snare signal to the reverb (or other process) unit, inserting a noise gate {{on the path of}} the reverb signal and connecting the snare sound to the side chain of the <b>gate</b> <b>unit.</b> With the <b>gate</b> <b>unit</b> set to 'external sidechain' (or 'external key'), the gate will respond to the snare signal level and 'cut off' when that has decayed below the threshold, not the reverberated sound.|$|R
5000|$|MAESTRO-II: a multi-chip module {{approximately}} {{the size of}} a dime that serves as the hardware core of several other products. The module contains a 66 MHz ARM7 processor, 4 MB of flash, 8 MB of RAM, and a FPGA with 500,000 <b>gates.</b> <b>Unit</b> cost: $3-4K (in 2008). It replaces the previous generation modules which were based on the HC12 microcontroller.|$|R
40|$|Recurrent neural {{networks}} (RNNs) have shown clear superiority in sequence modeling, particularly {{the ones with}} <b>gated</b> <b>units,</b> such as long short-term memory (LSTM) and <b>gated</b> recurrent <b>unit</b> (GRU). However, the dynamic properties behind the remarkable performance remain unclear in many applications, e. g., automatic speech recognition (ASR). This paper employs visualization techniques to study the behavior of LSTM and GRU when performing speech recognition tasks. Our experiments show some interesting patterns in the gated memory, {{and some of them}} have inspired simple yet effective modifications on the network structure. We report two of such modifications: (1) lazy cell update in LSTM, and (2) shortcut connections for residual learning. Both modifications lead to more comprehensible and powerful networks. Comment: ICASSP 201...|$|R
40|$|Recently, {{very deep}} {{convolutional}} neural networks (CNNs) have been attracting considerable attention in image restoration. However, as the depth grows, the long-term dependency problem is rarely realized for these very deep models, {{which results in}} the prior states/layers having little influence on the subsequent ones. Motivated {{by the fact that}} human thoughts have persistency, we propose a very deep persistent memory network (MemNet) that introduces a memory block, consisting of a recursive <b>unit</b> and a <b>gate</b> <b>unit,</b> to explicitly mine persistent memory through an adaptive learning process. The recursive unit learns multi-level representations of the current state under different receptive fields. The representations and the outputs from the previous memory blocks are concatenated and sent to the <b>gate</b> <b>unit,</b> which adaptively controls how much of the previous states should be reserved, and decides how much of the current state should be stored. We apply MemNet to three image restoration tasks, i. e., image denosing, super-resolution and JPEG deblocking. Comprehensive experiments demonstrate the necessity of the MemNet and its unanimous superiority on all three tasks over the state of the arts. Code is available at [URL] Accepted by ICCV 2017 (Spotlight presentation...|$|R
5000|$|The {{rectangular}} function (also {{known as}} the rectangle function, rect function, Pi function, <b>gate</b> function, <b>unit</b> pulse, or the normalized boxcar function) is defined as: ...|$|R
30|$|There {{are several}} {{improved}} RNN, such as simple RNN (SRNs), bidirectional RNN, deep (bidirectional) RNN, echo state networks, <b>Gated</b> Recurrent <b>Unit</b> RNNs, and clockwork RNN (CW-RNN.|$|R
50|$|Realizing that {{he needed}} more assets, the {{commander}} of 2nd Air Division, then Brig. Gen. Rollen Henry Anthis, asked for additional Air Force personnel and aircraft for Farm Gate use. Anthis wanted 10 more B-26s, five more T-28s, and two more SC-47s. McNamara reviewed the request, but he was cool {{to the idea of}} expanding Farm <b>Gate</b> <b>units</b> for combat use. His goal was to build up the VNAF so it could operate without American help. Still, McNamara approved the request for additional aircraft and also assigned two U-10s to Farm Gate.|$|R
40|$|The {{standard}} LSTM, {{although it}} succeeds in the modeling long-range dependences, {{suffers from a}} highly complex structure that can be simplified through modifications to its <b>gate</b> <b>units.</b> This paper was to perform an empirical comparison between the standard LSTM and three new simplified variants that were obtained by eliminating input signal, bias and hidden unit signal from individual gates, on the tasks of modeling two sequence datasets. The experiments show that the three variants, with reduced parameters, can achieve comparable performance with the standard LSTM. Due attention should be paid to turning the learning rate to achieve high accuraciesComment: 5 pages, 5 figure...|$|R
40|$|Abstract. We {{introduce}} low-overhead power optimization {{techniques to}} reduce leakage power in embedded processors. Our techniques improve previous work by a) {{taking into account}} idle time distribution for different execution units, and b) using instruction decode and control dependencies to wakeup the <b>gated</b> (but needed) <b>units</b> as soon as possible. We take into account idle time distribution per execution unit to detect an idle time period as soon as possible. This in turn results in increasing our leakage power savings. In addition, we use information already available in the processor to predict when a <b>gated</b> execution <b>unit</b> will be needed again. This results in early and less costly reactivation of <b>gated</b> execution <b>units.</b> We evaluate our techniques for a representative subset of MiBench benchmarks and for a processor using a configuration similar to Intels Xscale processor. We show that our techniques reduce leakage power considerably while maintaining performance. ...|$|R
40|$|A new approach, using {{electromagnetic}} analysis, {{is proposed}} for field-effect transistor model scaling and monolithic-microwave integrated-circuit (MMIC) design. It {{is based on}} an empirical distributed modeling technique where the active device is described in terms of an external passive structure connected to a suitable number of internal active sections. On this basis, an equivalent admittance matrix per <b>gate</b> <b>unit</b> width is obtained which, as confirmed by experimental results provided in this paper, is consistent with simple scaling rules. The same technique can also be adopted for a “global approach” to MMIC design where complex electromagnetic phenomena are also taken into account. An example of application concerning this aspect is presente...|$|R
40|$|We propose {{modeling}} {{time series}} by representing the transformations that take a frame {{at time t}} to a frame at time t+ 1. To this end we show how a bi-linear model of transformations, such as a gated autoencoder, {{can be turned into}} a recurrent net-work, by training it to predict future frames from the current one and the inferred transformation using backprop-through-time. We also show how stacking multi-ple layers of <b>gating</b> <b>units</b> in a recurrent pyramid makes it possible to represent the ”syntax ” of complicated time series, and that it can outperform standard recurrent neural networks in terms of prediction accuracy on a variety of tasks. ...|$|R
40|$|Recurrent Neural Networks (RNNs), {{which are}} a {{powerful}} scheme for modeling temporal and sequential data need to capture long-term dependencies on datasets and represent them in hidden layers with a powerful model to capture more information from inputs. For modeling long-term dependencies in a dataset, the gating mechanism concept can help RNNs remember and forget previous information. Representing the hidden layers of an RNN with more expressive operations (i. e., tensor products) helps it learn a more complex relationship between the current input and the previous hidden layer information. These ideas can generally improve RNN performances. In this paper, we proposed a novel RNN architecture that combine the concepts of gating mechanism and the tensor product into a single model. By combining these two concepts into a single RNN, our proposed models learn long-term dependencies by modeling with <b>gating</b> <b>units</b> and obtain more expressive and direct interaction between input and hidden layers using a tensor product on 3 -dimensional array (tensor) weight parameters. We use Long Short Term Memory (LSTM) RNN and <b>Gated</b> Recurrent <b>Unit</b> (GRU) RNN and combine them with a tensor product inside their formulations. Our proposed RNNs, which are called a Long-Short Term Memory Recurrent Neural Tensor Network (LSTMRNTN) and <b>Gated</b> Recurrent <b>Unit</b> Recurrent Neural Tensor Network (GRURNTN), are made by combining the LSTM and GRU RNN models with the tensor product. We conducted experiments with our proposed models on word-level and character-level language modeling tasks and revealed that our proposed models significantly improved their performance compared to our baseline models. Comment: Accepted at IJCNN 2016 URL : [URL]...|$|R
40|$|Many {{problems}} {{solved by}} multilayer neural networks (MLNNs) are reduced into pattern mapping. If the mapping includes several different rules, {{it is difficult}} to solve these problems by using a single MLNN with linear connection weights and continuous activation functions. In this paper, a structure trainable neural network has been proposed. The <b>gate</b> <b>units</b> are embedded, which can be trained together with the connection weights. Pattern mapping problems, which include several different mapping rules, can be realized using a single new network. Since, some parts of the network can be commonly used for different mapping rules, the network size can be reduced compared with the modular neural networks, which consists of several independent expert networks...|$|R
30|$|In this section, we extend {{conventional}} LSTM {{and investigate}} {{in more depth}} the <b>gated</b> recurrent <b>unit</b> (GRU) element introduced in our previous work [18]. We also propose a new element called a residual GRU (rGRU) to alleviate the vanishing gradient problem across multiple layers.|$|R
40|$|In this paper, a {{synthesis}} and learning method for the neural network with embedded <b>gate</b> <b>units</b> and a multi-dimensional input is proposed. When the input is multi-dimensional, gate functions are controlled in a multi-dimensional space. In this case, a hypersurface, {{on which the}} gate function is formed should be optimized. Furthermore, the switching points should be considered on the unit input. An initialization and a control methods for gate functions, which optimize the hypersurface, the switching point and the inclination, are proposed. The stabilization methods, already proposed, are further modified {{to be applied to}} the multi-dimensional environment. The gate functions can be trained together with the connection weights. Discontinuous function approximation is demonstrated to confirm usefulness of the proposed method...|$|R
40|$|Theoretical and {{empirical}} {{evidence indicates that}} the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive <b>gating</b> <b>units</b> to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures. Comment: 11 pages. Extends arXiv: 1505. 00387. Project webpage is at [URL] in Advances in Neural Information Processing Systems 201...|$|R
50|$|Also {{appearing}} in Europe, {{this one is}} commandeered by Kaiser Kikai himself and merged with a junked battleship on Dream Island. With the tank, Kaiser Kikai was able to defeat the combined (and overconfident) forces of <b>Gate</b> Robo <b>Units</b> 1-3, seriously injuring Jim Skylark in the process.|$|R
50|$|<b>Gated</b> {{recurrent}} <b>units</b> (GRUs) are a gating {{mechanism in}} recurrent neural networks, introduced in 2014. Their performance on polyphonic music modeling and speech signal modeling {{was found to}} be similar to that of long short-term memory. They have fewer parameters than LSTM, as they lack an output gate.|$|R
