58|367|Public
30|$|Step 2. Building distance-gray relationship. The gray {{center of}} each line within the edge area is fitted linearly to get the <b>edge</b> <b>contour</b> line. By calculating the {{vertical}} distance between each pixel and the <b>edge</b> <b>contour</b> line, the function between the distance and the pixel grey is obtained to depict the pixel distribution diagram. This step ensures that the algorithm can be applicable to the MTF calculation of the vertical edge and the inclined edge without image rotation.|$|E
3000|$|With {{the aid of}} the {{extracted}} stable <b>edge</b> <b>contour</b> {{in detail}} sub-image and the method described in Section  3.3, the circle center (C [...]...|$|E
40|$|One of {{the most}} {{significant}} research topics in computer vision is object detection. Most of the reported object detection results localise the detected object within a bounding box, but do not explicitly label the edge contours of the object. Since object contours provide a fundamental diagnostic of object shape, some researchers have initiated work on linear contour feature representations for object detection and localisation. However, linear contour feature-based localisation is highly dependent on the performance of linear contour detection within natural images, and this can be perturbed significantly by a cluttered background. In addition, the conventional approach to achieving rotation-invariant features is to rotate the feature receptive field to align with the local dominant orientation before computing the feature representation. Grid resampling after rotation adds extra computational cost and increases the total time consumption for computing the feature descriptor. Though it is not an expensive process if using current computers, it is appreciated that if each step of the implementation is faster to compute especially when the number of local features is increasing and the application is implemented on resource limited ”smart devices”, such as mobile phones, in real-time. Motivated by the above issues, a 2 D object localisation system is proposed in this thesis that matches features of <b>edge</b> <b>contour</b> points, which is an alternative method that takes advantage of the shape information for object localisation. This is inspired by <b>edge</b> <b>contour</b> points comprising the basic components of shape contours. In addition, edge point detection is usually simpler to achieve than linear <b>edge</b> <b>contour</b> detection. Therefore, the proposed localization system could avoid the need for linear contour detection and reduce the pathological disruption from the image background. Moreover, since natural images usually comprise many more <b>edge</b> <b>contour</b> points than interest points (i. e. corner points), we also propose new methods to generate rotation-invariant local feature descriptors without pre-rotating the feature receptive field to improve the computational efficiency of the whole system. In detail, the 2 D object localisation system is achieved by matching <b>edge</b> <b>contour</b> points features in a constrained search area based on the initial pose-estimate produced by a prior object detection process. The local feature descriptor obtains rotation invariance by making use of rotational symmetry of the hexagonal structure. Therefore, a set of local feature descriptors is proposed based on the hierarchically hexagonal grouping structure. Ultimately, the 2 D object localisation system achieves a very promising performance based on matching the proposed features of <b>edge</b> <b>contour</b> points with the mean correct labelling rate of the <b>edge</b> <b>contour</b> points 0. 8654 and the mean false labelling rate 0. 0314 applied on the data from Amsterdam Library of Object Images (ALOI). Furthermore, the proposed descriptors are evaluated by comparing to the state-of-the-art descriptors and achieve competitive performances in terms of pose estimate with around half-pixel pose error...|$|E
40|$|This study {{proposes a}} contour-based corner {{detector}} using the magnitude {{responses of the}} imaginary part of the Gabor filters on contours. Unlike the traditional contour-based methods that detect corners by analysing {{the shape of the}} <b>edge</b> <b>contours</b> and searching for local curvature maxima points on planar curves, the proposed corner detector combines the pixels of the <b>edge</b> <b>contours</b> and their corresponding grey-variation information. Firstly, <b>edge</b> <b>contours</b> are extracted from the original image using Canny edge detector. Secondly, the imaginary parts of the Gabor filters are used to smooth the pixels on the <b>edge</b> <b>contours.</b> At each <b>edge</b> pixel, the magnitude responses at each direction are normalised by their values and the sum of the normalised magnitude response at each direction is used to extract corners from <b>edge</b> <b>contours.</b> Thirdly, both the magnitude response threshold and the angle threshold are used to remove the weak or false corners. Finally, the proposed detector is compared with five state-of-the-art detectors on some grey-level images. The results from the experiment reveal that the proposed detector is more competitive with respect to detection accuracy, localisation accuracy, affine transforms and noise-robustness...|$|R
5000|$|Focus peaking is a {{focusing}} aid in live preview or electronic viewfinders on {{digital cameras}} that places a white or coloured highlight on in-focus <b>edges</b> (<b>contours)</b> within an image using an edge detect filter.|$|R
40|$|Hough {{transform}} for object detection Extract <b>edge</b> <b>contours</b> from images Sample interest points uniformly along edges Compute oriented bar and geometric blur at interest {{points and}} concatenate Match to nearest neighbor in training data to get object shift vector Cast votes for object position and scale Estimate discrete density and extract strong hypothese...|$|R
30|$|The image {{which has}} gone through the {{above-mentioned}} directionlets transform has a very sparse coefficient, and then can obtain more directional information, which can be better {{used to describe the}} <b>edge</b> <b>contour</b> of the infrared image.|$|E
40|$|<b>Edge</b> <b>contour</b> {{extraction}} {{plays an}} important role in computer vision because edge contours are relatively invariant to the changes of illumination conditions, sensor characteristics, etc. In particular, edge contours can be used as matching primitives for correspondence determination, an important step in video geo-registration. In this paper, we present a new approach for <b>edge</b> <b>contour</b> extraction based on a three-step procedure that using a RCBS-based scheme, inherently more accurate results can be produced, even though the edge model used for edges is relatively simple. We also present recursive filters that can efficiently smooth splines by approximating a signal with a complete set of coefficients subject to certain regularization constraints. We demonstrate our method on both synthetic and real images. Keywords: Edge contours, zero crossings, B-spline, LoG 1...|$|E
30|$|This paper takes Lena {{image as}} an example and executes cutting, filtering, and other signal {{processing}}. Because these processing attacks (except a larger degree of shear) have little influence on {{the shape of the}} <b>edge</b> <b>contour,</b> it does not affect the normal extraction of the watermark.|$|E
40|$|In {{this paper}} we present {{parallel}} solutions for performing image contour ranking on coarse-grained machines. In contour ranking, a linear {{representation of the}} <b>edge</b> <b>contours</b> is generated from the <b>edge</b> <b>contours</b> of a raw image. We describe solutions that employ different divide-and-conquer approaches and that use different communication patterns. The combining step of the divide-and-conquer solutions uses efficient sequential techniques for merging information about subimages. The proposed solutions are implemented on Intel Delta and Intel Paragon machines. We discuss performance results and present scalability analysis using different image and machine sizes. Keywords: Parallel processing, coarse-grained machines, contour ranking, list ranking, computer vision, scalability. Research {{supported in part by}} ARPA under contract DABT 63 - 92 -C- 0022 ONR. The views and conclusions contained in this paper {{are those of the authors}} and should not be interpreted as representing official policies, exp [...] ...|$|R
3000|$|... where σ 2 {{denotes the}} {{variance}} of the Gaussian filter and controls the degree of smoothing. After this process, candidate edge pixels are identified as the pixels that survive an additional thinning process known as nonmaximal suppression[15]. Then, the candidate edges are thresholded to keep only the significant ones. Moreover, Canny suggests hysteresis thresholding to eliminate streaking of <b>edge</b> <b>contours.</b>|$|R
40|$|In {{this paper}} we {{consider}} the problem of recognizing solid objects from a single two-dimensional image of a three-dimensional scene. We develop a new method for computing a transformation from a three-dimensional model coordinate frame to the two-dimensional image coordinate frame, using three pairs of model and image points. We show that this transformation always exists for three noncollinear points, and is unique up to a reflective ambigu-ity. The solution method is closed-form and only involves econd-order quations. We have implemented a recognition system that uses this transformation method to determine possible alignments of a model with an image. Each of these hypothesized matches is verified by comparing the entire <b>edge</b> <b>contours</b> of the aligned object with the image edges. Using the entire <b>edge</b> <b>contours</b> for verification, rather than a few local feature points, reduces the chance of finding false matches. The system has been tested on partly occluded objects in highly cluttered scenes. ...|$|R
30|$|In {{order to}} make the {{reversible}} image watermarking more robustness and imperceptibility, an adaptive reversible image watermarking scheme based on IWT and level set is proposed. The main contributions are illustrated as follows. A stable <b>edge</b> <b>contour</b> is obtained by using the geometric active contour model based on level set method, and the image stability <b>edge</b> <b>contour</b> extracted is determined by image content, so the contour shape has less impact with some attacks, and has strong robustness; since IWT has good time-frequency localization and multi-resolution analysis characteristic and SVD has the stability and anti-interference ability, the watermark embedding and extraction are implemented based on HVS feature of image by using IWT and SVD. This effectively improves the stability and robustness of the algorithm and guarantees better visual quality.|$|E
3000|$|... (ϕ) is {{the smooth}} Dirac function, {{which is the}} {{derivative}} of the smooth Heaviside function. A segmentation of the image is given by the two regions {x|ϕ(x,[*]t)[*]>[*] 0 } and {x|ϕ(x,[*]t)[*]<[*] 0 }. The steady state solution of the Equation[*] 3 hopefully gives a useful <b>edge</b> <b>contour</b> extraction or segmentation of the image.|$|E
40|$|This article {{examines}} how the human visual system represents {{the shapes of}} 3 -dimensional (3 D) objects. One long-standing hypothesis is that object shapes are represented in terms of volumetric component parts and their spatial configuration. This hypothesis is examined in 3 experiments using a whole–part matching paradigm in which participants match object parts to whole novel 3 D object shapes. Experi-ments 1 and 2, consistent with volumetric image segmentation, show that whole–part matching is faster for volumetric component parts than for either open or closed nonvolumetric regions of <b>edge</b> <b>contour.</b> However, the results of Experiment 3 show that an equivalent advantage is found for bounded regions of <b>edge</b> <b>contour</b> that correspond to object surfaces. The results are interpreted {{in terms of a}} surface-based model of 3 D shape representation, which proposes edge-bounded 2 -dimensional polygons as basic primitives of surface shape...|$|E
3000|$|... 1 {{is defined}} to {{estimate}} {{the difference between the}} ground truth and the contours extracted by our method. For one specific ground truth contour, we first select its nearest <b>contour</b> <b>edge</b> in <b>contours</b> from our proposed method, then we calculate the distance between the contour pair. The other metric m [...]...|$|R
40|$|Given a {{polyhedron}} (in 3 -space) and a view point, {{an edge of}} the polyhedron {{is called}} <b>contour</b> <b>edge,</b> {{if one of the}} two incident facets is directed towards the view point, and the other incident facet is directed away from the view point. Algorithms on polyhedra can exploit the fact that the number of <b>contour</b> <b>edges</b> is usually much smaller than the overall number of edges. The main goal {{of this paper is to}} provide evidence for (and quantify) the claim, that the number of <b>contour</b> <b>edges</b> is small in many situations. An asymptotic analysis of polyhedral approximations of a sphere with Hausdorff distance " shows that while the required number of edges for such an approximation grows like Θ(1 ="), the number of <b>contour</b> <b>edges</b> in a random orthogonal projection is Θ(1 = p "). In an experimental study we investigate a number of polyhedral objects from several application areas. We analyze the expected number of <b>contour</b> <b>edges</b> and the expected number of intersections of <b>contour</b> <b>edges</b> i [...] ...|$|R
30|$|We propose here a RR video {{quality metric}} well {{correlated}} with the perceived quality, based on the comparison of the edge information between the distorted image and the original one. The human eye is in fact very sensitive to the <b>edge</b> and <b>contour</b> information of an image, i.e., the <b>edge</b> and <b>contour</b> information gives a good indication {{of the structure of}} an image and it is critical for a human to capture the scene [6].|$|R
40|$|This paper {{introduces}} {{a method for}} detecting moving objects in a monocular image sequence that is obtained using a moving camera. The method first estimates {{the motion of the}} edge contours in a given image frame, by recovering a transformation that best matches each <b>edge</b> <b>contour</b> with the edges in the subsequent frame. Any contour that is not well accounted for by a single transformation is split into subparts. The transformation of each <b>edge</b> <b>contour</b> together with the relative spatial locations of the contours is used to partition the image into regions with similar motions. Hypotheses about the locations of possible moving objects are then made based on these motion regions. One of the key aspects of the approach is that it is based on estimating the motion of entire edge contours, as opposed to recovering a velocity field that measures the motion of individual points. We present some examples for image sequences taken of animate objects using a hand-held video camera. Keywords: Motion estimation, motion segmentation, edge matching...|$|E
40|$|The method {{involves}} {{carrying out}} the imprints of a form suitable for printing viscosity comprising electrically conductive polymer by masks. The openings are arranged corresponding to the geometric outer <b>edge</b> <b>contour</b> of electrodes. The electrically conductive polymer is printed in the openings. The electrically conductive polymer is carried out with pressure roller at the outer surface. A gravure groove is used for temporarily receiving the electrically conductive polymer, after printing a complete curing or polymerization is carried out...|$|E
30|$|For {{generating}} an attenuation correction map by the emission-based method, we applied an {{edge detection}} technique [8] which is {{implemented in the}} scanner software [9]. Briefly, an <b>edge</b> <b>contour</b> on a sinogram was detected by setting a threshold. The contour on the sinogram was smoothed by retaining lower order Fourier coefficients, and the smoothed contour was transformed to the <b>edge</b> <b>contour</b> on a reconstructed image. Then an attenuation map was generated using the algorithm implemented in the scanner software, assuming the tissue attenuation coefficient value to be uniform [9]. To apply the method, we first summed the measured sinogram in the following three phases: total scan duration phase, from 20 th (640  s) {{to the end of}} the scan phase (second tracer phase), and from 35 th (720  s) {{to the end of the}} scan phase (phase after second tracer inhalation). The edge of the brain tissue region on each summed sinogram was defined by setting a threshold value. The threshold value was set as 0.1 of the maximum value on the summed sinogram. The tissue attenuation coefficient value applied was 0.1  cm− 1, which was preliminarily obtained for ten subjects randomly chosen from the present data set.|$|E
40|$|This thesis {{focuses on}} the {{automatic}} recovery of three-dimensional hand motion from one or more views. A 3 D geometric hand model is constructed from truncated cones, cylinders and ellipsoids and is used to generate contours, which can be compared with <b>edge</b> <b>contours</b> and skin colour in images. The hand tracking problem is formulated as state estimation, where the model parameters define the internal state, {{which is to be}} estimated from image observations. In thew firs...|$|R
40|$|Abstract-A novel graph theoretic {{approach}} for data clustering is presented and {{its application to}} the image segmentation problem is demonstrated. The data to be clustered are represented by an undirected adjacency graph G with arc capacities assigned to reflect the similarity between the linked vertices. Clustering is achieved by removing arcs of G to form mutually exclusive subgraphs such that the largest inter-subgraph maximum flow is minimized. For graphs of moderate size (- 2000 vertices), the optimal solution is obtained through partitioning a flow and cut equivalent tree of 6, which can be efficiently constructed using the Gomory-Hu algorithm. However for larger graphs this approach is impractical. New theorems for subgraph condensation are derived and are then used to develop a fast algorithm which hierarchically constructs and partitions a partially equivalent tree of much reduced size. This algorithm results in an optimal solution equivalent to that obtained by partitioning the complete equivalent tree {{and is able to}} handle very large graphs with several hundred thousand vertices. The new clustering algorithm is applied to the image segmentation problem. The segmentation is achieved by effectively searching for closed <b>contours</b> of <b>edge</b> elements (equivalent to minimum cuts in G), which consist mostly of strong <b>edges,</b> while rejecting <b>contours</b> containing isolated strong edges. This method is able to accurately locate region boundaries {{and at the same time}} guarantees the formation of closed <b>edge</b> <b>contours.</b> Index Terms-Clustering, <b>edge</b> <b>contours,</b> flow and cut equivalent tree, graph theory, image segmentation, subgraph condensation. D I...|$|R
40|$|In {{this paper}} we present {{parallel}} solutions for performing image contour ranking on coarse-grained machines. In contour ranking, a linear {{representations of the}} <b>edge</b> <b>contours</b> is generated from the raw image. We describe solutions that employ different divide-and-conquer approaches and that use different communication patterns. The combining step of the divide-and-conquer solutions uses efficient sequential techniques for merging information about subimages. The proposed solutions are implemented on Intel Delta and Intel Paragon machines. We discuss performance results and present scalability analysis using different image and machine sizes...|$|R
40|$|Presbyopia is an age related, gradual loss of accommodation, mainly due {{to changes}} in the {{crystalline}} lens. As part of research efforts to understand and cure this condition, ex vivo, cross-sectional optical coherence tomography images of crystalline lenses were obtained by using the Ex-Vivo Accommodation Simulator (EVAS II) instrument and analyzed to extract their physical and optical properties. Various filters and edge detection methods were applied to isolate the <b>edge</b> <b>contour.</b> An ellipse is fitted to the lens outline to obtain central reference point for transforming the pixel data into the analysis coordinate system. This allows for the fitting of a high order equation to obtain a mathematical description of the <b>edge</b> <b>contour,</b> which obeys constraints of continuity as well as zero to infinite surface slopes from apex to equator. Geometrical parameters of the lens were determined for the lens images captured at different accommodative states. Various curve fitting functions were developed to mathematically describe the anterior and posterior surfaces of the lens. Their differences were evaluated and their suitability for extracting optical performance of the lens was assessed. The robustness of these algorithms was tested by analyzing the same images repeated times...|$|E
40|$|In this paper, a novel {{method is}} {{proposed}} to extract hand gesture features in real-time from RGB-D images {{captured by the}} Microsoft's Kinect. A contour length information based de-noise method is introduced for the hand gesture smooth segmentation and <b>edge</b> <b>contour</b> extraction. In addition, a finger earth mover's distance algorithm is applied with a novel approach to locate the palm image and extract fingertip features. Especially the proposed Lasso algorithm can effectively extract the fingertip feature from a hand contour curve correctly with excellent real-time performance. © 2014 IEEE...|$|E
40|$|This paper {{presents}} a micro optical probe, which is employed to evaluate edge contours of single point diamond tools {{with a size}} {{in a range of}} several millimetres. The micro optical probe consists of a laser source with a wavelength of 405 nm, an objective lens with a numerical aperture of 0. 25, a photodiode for measurement, and a compensating optical system including another photodiode for compensation of laser intensity. A collimated laser beam, which is divided by a beam splitter in the compensating optical system, is focused by the objective lens so that the focused spot can be used as the micro optical probe. The micro optical probe traces over an <b>edge</b> <b>contour</b> of an objective tool while the signals of both the two photodiodes are monitored. The output of the photodiode for measurement is compensated by using that of the photodiode for laser intensity compensation to eliminate the influence of the laser instability. The signal of the photodiode for measurement is used to define the deviation of <b>edge</b> <b>contour</b> within the diameter of the micro optical probe. To verify the feasibility of the developed optical probe, the optical system was mounted on a diamond turning machine, and some experiments were carried out. Two types of edge contours of the diamond tools having a straight cutting edge and a round cutting edge were measured on the machine...|$|E
40|$|DE 20317095 U UPAB: 20040514 NOVELTY - A {{cast metal}} part surface error {{inspection}} unit has a light source lighting the surface approximately perpendicular and light detector imaging (22) the reflected light with source detector beam paths coupled by {{a beam splitter}} with processor algorithm grouping pixels above a threshold for examination for errors and construction of object <b>edge</b> <b>contours.</b> USE - Surface inspection unit for cast metal machine, car and sanitary equipment parts. ADVANTAGE - The image processing algorithm is rapid enough for production inspection use and does not produce artifacts...|$|R
40|$|Images {{typically}} contain strong geometric features, such as edges, rhat {{impose a}} structure on pixel values and wavelet coefficients. Modeling the joint coherenr behavior of wavelet coeficients is difficult, and standard image coders fail to fully exploit this geometric regularity. We introduce wedgelets as a geometric tool for image compression. Wedgelets offer piecewise-linear approximations of <b>edge</b> <b>contours</b> {{and can be}} efficiently encoded. We describe the fundamental challenges that arise when applying such a tool to image compression. To meer these challenges, we also propose an efficient rate-distortion framework for natural image compression using wedgelets. 1...|$|R
40|$|Successful image {{interpolation}} requires proper {{enhancement of}} high frequency content of image pixels around edges. In this paper, we introduce a simple edge model to estimate high resolution edge profiles from lower resolution values. Pixels around edges {{are viewed as}} samples taken from one dimensional (1 -D) continuous edge profiles according to 1 -D smooth <b>edge</b> <b>contours</b> defining the sampling instants. The image is highpass filtered by wavelets and subpixel edge lo-cations are estimated by minimizing the modeling error in the wavelet domain. Interpolation is carried out by apply-ing the model, wherever applicable, together with a base-line interpolator (here, bilinear) {{in order to make}} edges look sharper without introducing artifacts. The results are com-pared to bilinear interpolation, and significant improvement in terms of SNR, <b>edge</b> sharpness and <b>contour</b> smoothness is observed. 1...|$|R
40|$|Compact ranges offer many {{advantages}} over {{other types of}} ranges, {{and as a result}} much effort is being directed toward the improvement of their performance. The use of concave edge contours and blended rolled edge terminations to reduce the unwanted energy diffracted into the target zone from the termination of the main reflector is discussed. The proposed shaping of the <b>edge</b> <b>contour</b> minimizes the diffracted fields by virtue of reducing the spread factor; whereas, the blended rolled edge terminations reduce the diffracted fields by creating a smooth transition in the reflected field. Two design examples are treated in order to illustrate these concepts...|$|E
40|$|TreeRipper is {{a command}} line c++ {{program for the}} fully-automated {{recognition}} of multifurcating phylogenetic trees. The program accepts a range of input image formats (PNG, JPG/JPEG, GIF, TIFF or PDF). Then follows a number of cleaning steps to detect lines, remove node labels, patch-up broken lines and corners and detect line edges. The <b>edge</b> <b>contour</b> is then determined to detect the branch length, tip label positions and the topology of the tree. Optical Character Recognition (OCR) is used to convert the tip labels into text with the freely available tesseract-ocr software. 32 % of images meeting the prerequisites for TreeRipper were successfully recognised, the largest tree had 115 leaves. |$|E
30|$|On {{the basis}} of other parts {{dimension}} measurement system based on machine vision technology, based on the geometric features of shaft parts, this paper presents a geometric dimension measurement system for shaft parts based on machine vision. It uses the CCD camera to get the image. First, it preprocesses the collected images. In view {{of the influence of}} the noise and other factors, the wavelet denoising is used to denoise the image. Then, an improved single pixel edge detection method is proposed based on the Canny detection operator to extract the <b>edge</b> <b>contour</b> of the part image. Finally, the geometrical quantity algorithm is applied to the measurement research, and the measured data are obtained and analyzed.|$|E
30|$|The {{coefficients}} in {{the high}} frequency subbands represent the detailed component of the source image. In traditional multiresolution fusion algorithms, such as [9, 31, 32], the multiresolution coefficients with larger absolute value are considered as sharp brightness changes or salient features in the corresponding source image, such as the <b>edges,</b> <b>contours,</b> and region boundaries, and so on. Thus, for the high frequency subbands coefficients, {{the most commonly used}} selection principle is the 'absolute-maximum-choosing' scheme (simplified and named 'Coef-abs-max') without taking any consideration of lowpass subband coefficients, that is, all the information in the lowpass subband is neglected.|$|R
3000|$|... which {{possesses}} the diffusion mechanisms in both tangential and normal {{directions to the}} isophote lines. Furthermore, Eq. 9 promotes varying degrees of diffusions depending upon the local image structures, particularly <b>edges</b> and <b>contours.</b>|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThe {{objective of this}} thesis is to evaluate the performance of an array antenna when it is installed on an complex structure, {{such as those that}} have unusual <b>edge</b> <b>contours,</b> curved surfaces, and mixed material composition. A dipole is used as the basic array element to study the effect of various changes in the array design parameters on the gain and sidelobe level. Data is generated using a computational electromagnetics code based on the method of moments. Among the issues addressed are the curvature of the array ground plane and shaping the ground plane edges to reduce wide angle sidelobes. [URL] Hellenic Nav...|$|R
