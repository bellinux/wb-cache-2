8|25|Public
5000|$|Step 1 of the {{instruction}} cycle is <b>fetch</b> <b>cycle,</b> {{which is the same}} for each instruction: ...|$|E
50|$|The {{discarded}} instructions at {{the branch}} and destination lines {{add up to}} nearly a complete <b>fetch</b> <b>cycle,</b> even for a single-cycle next-line predictor.|$|E
50|$|The Alpha 21264 and Alpha EV8 {{microprocessors}} used a fast single-cycle {{next line}} predictor {{to handle the}} branch target recurrence and provide a simple and fast branch prediction. Because the next line predictor is so inaccurate, and the branch resolution recurrence takes so long, both cores have two-cycle secondary branch predictors which can override the prediction of the next line predictor {{at the cost of}} a single lost <b>fetch</b> <b>cycle.</b>|$|E
40|$|This paper {{presents}} an extension to an existing instruction set architecture, which gains considerable reduction in power consumption. The reduction in power consumption is achieved through coding {{of the most}} commonly executed instructions in a short format done by the compiler based on a profile of previous executions. This leads to fewer accesses to the instruction cache and that more instructions can fit in the cache. As a secondary effect, {{this turned out to be}} very beneficial in terms of power. Another major advantage, which is the main concern of this paper is the reduction in the number of instruction <b>fetch</b> <b>cycles</b> which will also contribute significantly towards reduction in power consumption. The work involves implementing the new processor architecture in ASIC and estimation of power-consumption compared to the normal architecture Key words: low-power, computer architecture, instruction fetch, ASIC Implementation, RISC- processor. ...|$|R
40|$|This thesis {{presents}} {{implementation of}} an extension to an existing instruction set architecture, which gains considerable reduction in power consumption. The reduction in power consumption is achieved through coding {{of the most}} frequently executed instructions in a short format done by the compiler based on a profile of previous executions. This leads to fewer accesses to the instruction cache and that more instructions can fit in the cache. Another major advantage is the {{reduction in the number of}} instruction <b>fetch</b> <b>cycles</b> which will also contribute significantly towards reduction in power consumption. The work involves implementing the new processor front-end architecture in ASIC and estimation of power-consumption compared to the normal architecture. Power was estimated using both RTL and gate level estimation methods. As an extension to the thesis the possible architectures which could be implemented in the case of superscalar processors are also discussed. 3...|$|R
50|$|The hyperSPARC was {{a two-way}} superscalar microprocessor. It had four {{execution}} units: an integer unit, a floating-point unit, a load/store unit and a branch unit. The hyperSPARC has an on-die 8 KB instruction cache, from which two instructions were <b>fetched</b> per <b>cycle</b> and decoded. The decoder could not decode new instructions if the previously decoded instructions were not issued to the execution units.|$|R
50|$|The 620 {{was similar}} to the 604. It has a five-stage pipeline, same support for {{symmetric}} multiprocessing and the same number of execution units; a load/store unit, a branch unit, an FPU, and three integer units. With larger 32 KB instruction and data caches, support for a L2 cache that may have a capacity of 128 MB, and more powerful branch and load/store units that had more buffers, the 620 was very powerful. The branch history table was also larger and could dispatch more instructions so that the processor can handle out-of-order execution more efficiently than the 604. The floating point unit was also enhanced compared to the 604. With a faster <b>fetch</b> <b>cycle</b> and support for several key instruction in hardware (like sqrt) made it, combined with faster and wider data buses, more efficient than the FPU in the 604.|$|E
40|$|The {{purpose of}} this work is to improve {{performance}} of a 16 -bit stack processor. This processor is suitable for embedded applications. A stack processor has an advantage of low complexity but its performance can be improved. Observing the instruction fetch consumes 53 % of the execution cycle, focusing on improving instuction fetch is {{the primary goal of}} this work. The proposed scheme uses 16 -bit fetch with some additional path {{to reduce the number of}} control cycle. The work also suggests the use of instruction compression. The result shows the performance improvement of 32 % and 37 % respectively, achieving the reduction in instruction <b>fetch</b> <b>cycle</b> by 61 % and 70 %...|$|E
40|$|In {{this paper}} {{we present a}} {{straightforward}} technique for compressing the instruction stream for programs that overcomes some {{of the limitations of}} earlier proposals. After code generation, the instruction stream is analysed for frequently used sequences of instructions from within the program's basic blocks. These patterns of multiple instructions are then mapped into single byte opcodes. This constitutes a compression of multiple, multi-byte operations onto a single byte. When compressed opcodes are detected during the instruction <b>fetch</b> <b>cycle</b> of program execution, they are expanded within the CPU into the original (multi-cycle) sequence of instructions. We only examine patterns within a program's basic block, so branch instructions and their targets are unaffected by this technique allowing compression to be decoupled from compilation. 1. Introduction Compilers are universally used for program development, due to the complexity of managing the large applications developed today. How [...] ...|$|E
50|$|In the {{illustration}} at right, in cycle 3, the processor cannot decode the purple instruction, {{perhaps because the}} processor determines that decoding depends on results produced by {{the execution of the}} green instruction. The green instruction can proceed to the Execute stage and then to the Write-back stage as scheduled, but the purple instruction is stalled for one <b>cycle</b> at the <b>Fetch</b> stage. The blue instruction, which was due to be <b>fetched</b> during <b>cycle</b> 3, is stalled for one cycle, as is the red instruction after it.|$|R
50|$|As {{described}} above, the DMA {{graphics controller}} operated independently. One-bit pixels were fetched over a 16-bit data bus and output at 16 MHz, necessitating one million fetches per second. Each <b>fetch</b> took two <b>cycles</b> out of eight per microsecond, implying a memory bandwidth tax of 25%. This was mitigated to 17% by disabling DMA while the CRT scanned right to left.|$|R
40|$|SMT (Simultaneous Multithreading) {{processor}} has {{the ability}} to issue multiple instructions from multiple independent threads in one <b>cycle.</b> <b>Fetch</b> bandwidth has been demonstrated {{to be one of the}} main performance bottlenecks for SMT processor. Trace cache is an effective technique for fetching instructions. In this paper, we implement trace cache in SMT and find its effect for SMT performance. Also, we try to discuss about several different implementations of trace cache. 1...|$|R
40|$|The {{performance}} of instruction memory {{is a critical}} factor for both large, high performance applications and for embedded systems. With high performance systems, the bandwidth to the instruction cache can be the limiting factor for execution speed. Code density is often the critical factor for embedded systems. In this report we demonstrate a straightforward technique for compressing the instruction stream for programs. After code generation, the instruction stream is analysed for often reused sequences of instructions from within the program's basic blocks. These patterns of multiple instructions are then mapped into single byte opcodes. This constitutes a compression of multiple, multi-byte operations onto a single byte. When compressed opcodes are detected during the instruction <b>fetch</b> <b>cycle</b> of program execution, they are expanded within the CPU into the original (multi-cycle) set of instructions. Because we only operate within a program's basic block, branch instructions and their t [...] ...|$|E
40|$|This {{performance}} {{took place}} at the Centre Pompidou, Paris in September 2013. It was a public performance combining live voice, digital manipulation and projected text. The performance was programmed {{under the auspices of the}} Electronic Literature Organisation (ELO) Conference 'Chercher le texte', at which I also presented a peer-reviewed paper entitled 'Does electronic literature need a theory of language'. This paper is to be published by the ELO in its online journal. ELO is the most prestigious of the electronic literature umbrella organisations [URL] and its international bienniel conference is a showcase for cutting edge work in the field of experimental digital writing. Over a hundred and fifty creative works were submitted for consideration this year. The Fetch was one of 28 to be selected ([URL] The Centre Pompidou is a significant contemporary art venue. I also contributed to a public round table discussion on digital literature as a prelude to the performance at the Pompidou Centre., The performance itself comprised four original texts – a short story/pastiche on the subject of the doppelganger, a technical description of the <b>fetch</b> <b>cycle</b> in computing, a lyrical reflection on the notion of doubleness and walking, and an experimental text including found text from Gregory Bateson’s concept of 'double description' in Nature and Mind. The text was located within a piece of software developed by Caden Lovelace in 2009 which searches the Internet for 3 or 4 word phrases which mirror exactly the phrases used in the original texts. The outcomes of the searches were projected onto a screen during the course of the performance ([URL]...|$|E
5000|$|A super-scalar {{instruction}} pipeline {{pulls in}} multiple instructions in every clock cycle, {{as opposed to}} a simple scalar pipeline. This increases Instruction level parallelism (ILP) as many times as the number of instructions <b>fetched</b> in each <b>cycle,</b> except when the pipeline is stalled due to data or control flow dependencies. Even though the retire rate of superscalar pipelines is usually less than their fetch rate, the overall number of instructions executed per unit time (> 1) is generally greater than a scalar pipeline.|$|R
40|$|As superscalar {{processors}} {{become increasingly}} wide, {{it is inevitable}} that the large set of instructions to be <b>fetched</b> every <b>cycle</b> will span multiple noncontiguous basic blocks. The mechanism to fetch, align, and pass this set of instructions down the pipeline must do so as efficiently as possible. The concept of trace cache has emerged as the most promising technique to meet this high-bandwidth, low-latency fetch requirement. A new fill unit scheme, the Sliding Window Fill Mechanism, is proposed as a method to efficiently populate the trace cache. This method exploits trace continuity and identifies probable start regions to improve trace cache hit rate. Simulation yields a 7 % average hit rate increase over the Rotenberg fill mechanism. When combined with branch promotion, trace cache hit rates experienced a 19 % average increase along with a 17 % average improvement in fetch bandwidth...|$|R
40|$|This work {{proposes a}} new fetch unit model, {{inspired}} in the trace processor [8]. Instead of fetching instruction traces, our fetch unit will fetch instruction streams. An instruction stream is a sequential run of instructions, dened by the starting address and the stream length. All branches {{included in the}} stream {{are assumed to be}} not taken, except for the terminating one, which should be always taken (else, we are terminating the stream prematurely). We will show how stream fetching approaches the four factors determining instruction fetch performance: the width of instructions <b>fetched</b> per <b>cycle,</b> instruction cache misses, branch prediction throughput and branch prediction accuracy. 1 Fetch performance 1. 1 Width of instruction fetch All instructions in a stream are consecutive in memory, and a stream contains no taken branches. This makes it very simple to obtain several consecutive instruction cache lines from a multi-banked cache, and simply select the desired instruction [...] ...|$|R
40|$|In modern {{computer}} architectures, it is {{no longer}} the hardware that is solely responsible for achieving a high performance. Compiler technology is at least as important. Both have their own different techniques to increase the performance, but they have at least one technique in common: branch prediction. When branches in a program can be predicted correctly, dramatic increases in performance can be achieved. On the hardware level because the processor will not waste <b>cycles</b> <b>fetching</b> and executing instructions from paths of the program's control flow which will not be executed. On the compiler level because now the execution frequencies of the basic blocks in a program can be calculated, boosting the performance increase of global instruction scheduling. This report will focus on the latter aspect...|$|R
40|$|Techniques such as out-of-order {{issue and}} {{speculative}} execution aggressively exploit instruction level parallelism in modem superscalar processor architectures. The {{front end of}} such pipelined machines is concerned with providing a stream of schedulable instructions at a bandwidth that meets or exceeds the rate of instructions being issued and executed. As superscalar machines become increasingly wide, {{it is inevitable that}} the large set of instructions to be <b>fetched</b> every <b>cycle</b> will span multiple noncontiguous basic blocks. The mechanism to fetch, align, and pass this set of instructions down the pipeline must do so as efficiendy as possible, occupying a minimal number of pipeline cycles. The concept of trace cache has emerged as the most promising technique to meet this high-bandwidth, low-latency fetch requirement. This thesis presents the design, simulation and analysis of a microarchitecture simulator extension that incorporates trace cache. A new fill unit scheme, the Sliding Window Fill Mechanism is proposed. This method exploits trace continuity and identifies probable start regions to improve trace cache hit rate. A 7 % hit rate increase was observed over the Rotenberg fill mechanism. Combined with branch promotion, trace cache hit rates experienced a 19 % average increase along with a 17 % average rise in fetch bandwidth...|$|R
40|$|The {{implementation}} of modern {{high performance computer}} is increasingly directed toward parallelism in the hardware. However, most of the current fetch units are limited to one branch prediction per cycle and therefore, can fetch {{no more than one}} basic block per <b>cycle.</b> While <b>fetching</b> a single basic block each cycle is sufficient for implementations that issue small number of instructions per cycle, it is not for processors with higher peak issue rates. In this paper we proposed a new architecture, which combine the enhanced branch target buffer with the block cache to fetch multiple blocks. The enhanced branch target buffer enhanced the capability of traditional branch target buffer to store multiple branch targets. Block cache keeps the instruction streams that collected at commit stage as blocks. These blocks are renamed and the renamed index is stored in the enhanced branch target buffer as the basic fetching unit. The proposed architecture has been simulated using SimpleScalar 2. 0 simulator. The results show that the average instruction <b>fetch</b> pet <b>cycle</b> reaches 4. 84 instructions, the average instruction issued per cycle is 3. 44 instructions and the average IPC is 2. 07. As compare to the baseline machine, we got 87 %, 84 %, and 56 % increase respectively. ...|$|R
50|$|Each stage {{requires}} one {{clock cycle}} and an instruction {{passes through the}} stages sequentially. Without pipelining, a new instruction is fetched in stage 1 only after the previous instruction finishes at stage 5, therefore the number of clock cycles it takes to execute an instruction is 5 (CPI = 5 > 1). In this case, the processor {{is said to be}} subscalar. With pipelining, a new instruction is <b>fetched</b> every clock <b>cycle</b> by exploiting instruction-level parallelism, therefore, since one could theoretically have 5 instructions in the 5 pipeline stages at once (one instruction per stage), a different instruction would complete stage 5 in every clock cycle and on average the number of clock cycles it takes to execute an instruction is 1 (CPI = 1). In this case, the processor is said to be scalar.|$|R
40|$|We {{introduce}} the Micro-Operation Cache (Uop Cache – UC) {{designed to reduce}} processor’s frontend power and energy consumption without performance degradation. The UC caches basic blocks of instructions – pre-decoded into micro-operations (uops). The UC fetches a single basic-block worth of uops per <b>cycle.</b> <b>Fetching</b> complete pre-decoded basic-blocks eliminates the need to repeatedly decode variable length instructions and simplifies the process of predicting, fetching, rotating and aligning fetched instructions. The UC design enables even a small structure to be quite effective. Results: a moderate-sized UC eliminates about 75 % instruction decodes across {{a broad range of}} benchmarks and over 90 % in multimedia applications and high-power tests. For existing Intel P 6 family processors, the eliminated work may save about 10 % of the full-chip power consumption with no performance degradation...|$|R
40|$|The {{increasing}} widths of superscalar processors are placing greater demands {{upon the}} fetch mechanism. The trace cache meets these demands by placing logically contiguous instructions in physically contiguous storage. As a result, the trace cache delivers instructions {{at a high}} rate by supplying multiple <b>fetch</b> blocks each <b>cycle.</b> In this paper, we examine two techniques to improve the number of instructions delivered each cycle by the trace cache. The first technique, branch promotion, dynamically converts strongly biased branches into branches with static predictions. Because these promoted branches require no dynamic prediction, the branch predictor suffers less from {{the negative effects of}} interference. Branch promotion unlocks the potential of the second technique: trace packing. With trace packing, trace segments are packed with as many instructions as will fit, without regard to naturally occurring fetch block boundaries. With both techniques, the effective fetch rate of the trace [...] ...|$|R
40|$|The {{increasing}} widths of superscalar processors are placing greater demands {{upon the}} fetch mechanism. The trace cache meets these demands by placing logically contiguous instructions in physically contiguous storage. It {{is capable of}} supplying multiple <b>fetch</b> blocks each <b>cycle.</b> In this paper we examine two fetch and issue techniques, partial matching and inactive issue, that improve the overall performance of the trace cache by improving the effective fetch rate. We show that for the SPECint 95 benchmarks partial matching increases the overall performance by 12 % and adding inactive issue increases performance by 15 %. Furthermore we apply these two techniques to issue blocks from trace segments which contain multiple execution paths. We conclude with a performance comparison between a trace cache implementing partial matching and inactive issue and an aggressive single block fetch mechanism. The trace cache increases performance {{by an average of}} 25 % over the instruction cache...|$|R
40|$|We {{investigate}} the relative performance impact of non-blocking loads, stream buffers, and speculative execution both used individually and {{in conjunction with}} each other. We have simulated the SPEC 92 benchmarks on a statically scheduled quad-issue processor model, running code from the Multiflow compiler. Non-blocking loads and stream buffers both provide a significant performance advantage, and their combination performs significantly better than either alone. For example, with a 64 -byte, 2 -way set associative cache with 32 <b>cycle</b> <b>fetch</b> latency, non-blocking loads reduce the run-time by 21 % while stream-buffers reduce it by 26 %, and the combined use of the two yields a 47 % reduction. The addition of speculative execution further improves {{the performance of the}} systems that we have simulated, with or without non-blocking loads and stream buffers, by an additional 20 % to 40 %. We expect that the use of all three of these techniques will be important in future generations of microprocessor [...] ...|$|R
40|$|Abstract. Continuing {{advances}} in semiconductor technology {{and demand for}} higher performance {{will lead to more}} powerful, superpipelined and wider issue processors. Instruction caches in such processors will consume a significant fraction of the on-chip energy due to very wide <b>fetch</b> on each <b>cycle.</b> This paper proposes a new energy-effective design of the fetch unit that exploits the fact that not all instructions in a given I-cache fetch line are used due to taken branches. A Fetch Mask Determination unit is proposed to detect which instructions in an I-cache access will actually be used to avoid fetching any of the other instructions. The solution is evaluated for a 4 -, 8 - and 16 -wide issue processor in 100 nm technology. Results show an average improvement in the Icache Energy-Delay product of 20 % for the 8 -wide issue processor and 33 % for the 16 -wide issue processor for the SPEC 2000, with no negative impact on performance. ...|$|R
50|$|With a single-execution-unit processor, {{the best}} CPI {{attainable}} is 1. However, with a multiple-execution-unit processor, one may achieve even better CPI values (CPI < 1). In this case, the processor {{is said to}} be superscalar. To get better CPI values without pipelining, the number of execution units must be greater than the number of stages. For example, with 6 executions units, 6 new instructions are fetched in stage 1 only after the 6 previous instructions finish at stage 5, therefore on average the number of clock cycles it takes to execute an instruction is 5/6 (CPI = 5/6 < 1). To get better CPI values with pipelining, there must be at least 2 execution units. For example, with 2 executions units, 2 new instructions are <b>fetched</b> every clock <b>cycle</b> by exploiting instruction-level parallelism, therefore 2 different instructions would complete stage 5 in every clock cycle and on average the number of clock cycles it takes to execute an instruction is 1/2 (CPI = 1/2 < 1).|$|R
50|$|The Pentium Pro has an 8 KiB {{instruction}} cache, {{from which}} up to 16 bytes are <b>fetched</b> on each <b>cycle</b> {{and sent to}} the instruction decoders. There are three instruction decoders. The decoders are not equal in capability: only one can decode any x86 instruction, while the other two can only decode simple x86 instructions. This restricts the Pentium Pro's ability to decode multiple instructions simultaneously, limiting superscalar execution. x86 instructions are decoded into 118-bit micro-operations (micro-ops). The micro-ops are RISC-like; that is, they encode an operation, two sources, and a destination. The general decoder can generate up to four micro-ops per cycle, whereas the simple decoders can generate one micro-op each per cycle. Thus, x86 instructions that operate on the memory (e.g., add this register to this location in the memory) can only be processed by the general decoder, as this operation requires a minimum of three micro-ops. Likewise, the simple decoders are limited to instructions that can be translated into one micro-op. Instructions that require more micro-ops than four are translated with the assistance of a sequencer, which generates the required micro-ops over multiple clock cycles.|$|R
40|$|This {{thesis is}} {{concerned}} with hardware approaches for maximizing the number of independent instructions in the execution core and thereby maximizing the processing efficiency for a given amount of compute bandwidth. Compute bandwidth {{is the number of}} parallel execution units multiplied by the pipelining of those units in the processor. Keeping those computing elements busy is key to maximize processing efficiency and therefore power efficiency. While some applications have many independent instructions that can be issued in parallel without inefficiencies due to branch behavior, cache behavior, or instruction dependencies, most applications have limited parallelism and plenty of stalling conditions. This thesis presents two approaches to this problem, which in combination greatly increases the efficiency of the processor utilization of resources. The first approach addresses the problem of small basic blocks that arise when code has frequent branches. We introduce algorithms and mechanisms to predict multiple branches simultaneously and to fetch multiple non-continuous basic blocks every cycle along a predicted branch path. This makes what was previously an inherently serial process into a parallelized instruction fetch approach. For integer applications, the result is an increase in useful instruction fetch capacity of 40 % when two basic blocks are <b>fetched</b> per <b>cycle</b> and 63 % for three blocks per cycle. For floating point benchmarks, the associated improvement is 27 % and 59 %. The second approach addresses increasing the number of independent instructions to the execution core through simultaneous multi-threading (SMT). We compare to another multithreading approach, Switch-on-Event multithreading, and show that SMT is far superior. Intel Pentium 4 SMT microarchitecture algorithms are analyzed, and we look at the impact of SMT on power efficiency of the Pentium 4 Processor. A new metric, the SMT Energy Benefit is defined. Not only do we show that the SMT Energy Benefit for a given workload with SMT can be quite significant, we also generalize the results and build a model for what other future processors’ SMT Energy Benefit would be. We conclude that while SMT will continue to be an energy-efficient feature, as processors get more energy-efficient in general the relative SMT Energy Benefit may be reduced...|$|R
40|$|This paper {{presents}} a new design that implements the data-driven (i. e. dataflow) computation paradigm with intelligent memories. Also, a relevant prototype that employs FPGAs is presented {{for the support}} of intelligent memory structures. Instead of giving the CPU the privileged right to decide what instructions to <b>fetch</b> in each <b>cycle</b> (as is the case for control-flow CPUs), instructions in dataflow computers enter the execution unit on their own when they are ready to execute. This way, the application-knowledgeable algorithm, rather than the applicationignorant CPU, is in control. This approach could eventually result in outstanding performance and elimination of large numbers of redundant operations that plague current control-flow designs. Control-flow and dataflow machines are two extreme computation paradigms. In their pure form, the former machines follow an inherently sequential execution process while the latter are parallel in nature. The sequential nature of control-flow machines makes them relatively easy to implement compared to dataflow machines, which have to address a number of issues that are easily solved {{in the realm of the}} control-flow paradigm. Our dataflow design solves these issues at the intelligent memory level, separating the processor from dataflow maintenance tasks. It is shown that using intelligent memories with basic components similar to those of FPGAs produces a feasible approach. Expected improvements within the next few years in underlying intelligent memory and FPGA technologies will have the potential to make the effect of our approach even more dramatic...|$|R
40|$|This paper {{presents}} new architectural concepts for uniprocessor system designs. They {{result in}} a uniprocessor design that conforms to the data-driven (i. e., dataflow) computation paradigm. It is shown that usage of this, namely D -CPU (Data-Driven) processor, follows the natural flow of programs, minimizes redundant (micro) operations, lowers the hardware cost, and reduces the power consumption. We assume that programs are developed naturally using a graphical or equivalent language that can explicitly show all data dependencies. Instead of giving the CPU the privileged right of deciding what instructions to <b>fetch</b> in each <b>cycle</b> (as {{is the case for}} CPUs with a program counter), instructions are entering the CPU when they are ready to execute or when all their operand(s) are to be available within a few clock cycles. This way, the application-knowledgeable algorithm, rather than the application-ignorant CPU, is in control. The CPU is used just as a resource, the way it should normally be. This approach results in outstanding performance and elimination of large numbers of redundant operations that plague current processor designs. The latter, conventional CPUs are characterized by numerous redundant operations, such as the first memory <b>cycle</b> in instruction <b>fetching</b> that is part of any instruction cycle, and instruction and data prefetchings for instructions that are not always needed. A comparative analysis of our design with conventional designs proves that it is capable of better performance and simpler programming. Finally, VHDL implementation is used to prove the viability of this approach...|$|R
40|$|In superscalar processors, {{capable of}} issuing and {{executing}} multiple instructions per <b>cycle,</b> <b>fetch</b> performance represents an upper {{bound to the}} overall processor performance. Unless there is some form of instruction re-use mechanism, you cannot execute instructions {{faster than you can}} fetch them. Instruction Level Parallelism, represented by wide issue out oforder superscalar processors, was the trending topic during the end of the 90 's and early 2000 's. It is indeed the most promising way to continue improving processor performance {{in a way that does}} not impact application development, unlike current multicore architectures which require parallelizing the applications (a process that is still far from being automated in the general case). Widening superscalar processor issue was the promise of neverending improvements to single thread performance, as identified by Yale N. Patt et al. in the 1997 special issue of IEEE Computer about "Billion transistor processors" [1]. However, instruction fetch performance is limited by the control flow of the program. The basic fetch stage implementation can read instructions from a single cache line, starting from the current fetch address and up to the next control flow instruction. That is one basic block per cycle at most. Given that the typical basic block size in SPEC integer benchmarks is 4 - 6 instructions, fetch performance was limited to those same 4 - 6 instructions per cycle, making 8 -wide and 16 -wide superscalar processors impractical. It became imperative to find mechanisms to fetch more than 8 instructions per cycle, and that meant fetching more than one basic block per cycle. Postprint (published version...|$|R
40|$|Efficient {{programmable}} baseband processors {{are important}} to enable true multi-standard radio platforms as convergence of mobile communication devices and systems requires multi-standard processing devices. The processors do not only need the capability to handle differences in a single standard, often {{there is a great}} need to cover several completely different modulation methods such as OFDM and CDMA with the same processing device. Programmability {{can also be used to}} quickly adapt to new and updated standards within the ever changing wireless communication industry since a pure ASIC solution will not be flexible enough. ASIC solutions for multi-standard baseband processing are also less area efficient than their programmable counterparts since processing resources cannot be efficiently shared between different operations. However, as baseband processing is computationally demanding, traditional DSP architectures cannot be used due to their limited computing capacity. Instead VLIW- and SIMD-based processors are used to provide sufficient computing capacity for baseband applications. The drawback of VLIW-based DSPs is their low power efficiency due to the wide instructions that need to be <b>fetched</b> every clock <b>cycle</b> and their control-path overhead. On the other hand, pure SIMD-based DSPs lack the possibility to perform different concurrent operations. Since memory access power is the dominating part of the power consumption in a processor, other alternatives should be investigated. In this dissertation a new and unique type of processor architecture has been designed that instead of using the traditional architectures has started from the application requirements with efficiency in mind. The architecture is named ``Single Instruction stream Multiple Tasks'', SIMT in short. The SIMT architecture uses the vector nature of most baseband programs to provide a good trade-off between the flexibility of a VLIW processor and the processing efficiency of a SIMD processor. The contributions of this project are the design and research of key architectural components in the SIMT architecture as well as development of design methodologies. Methodologies for accelerator selection are also presented. Furthermore data dependency control and memory management are studied. Architecture and performance characteristics have also been compared between the SIMT and more traditional processor architectures. A complete system is demonstrated by the BBP 2 baseband processor that has been designed using SIMT technology. The SIMT principle has previously been proven in a small scale in silicon in the BBP 1 processor implementing a Wireless LAN transceiver. The second demonstrator chip (BBP 2) was manufactured early 2007 and implements a full scale system with multiple SIMD clusters and a controller core supporting multiple threads. It includes enough memory to run symbol processing of DVB-H/T, WiMAX, IEEE 802. 11 a/b/g and WCDMA, and the silicon area is 11 mm 2 in a 0. 12 um CMOS technology...|$|R

