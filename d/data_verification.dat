461|1253|Public
50|$|Create {{checksums}} for <b>data</b> <b>verification</b> of ISOs.|$|E
5000|$|<b>Data</b> <b>verification</b> (i.e., {{telephone}} numbers, zip codes, address scrubbing) ...|$|E
50|$|<b>Data</b> <b>Verification</b> is {{a process}} in which {{different}} types of data are checked for accuracy and inconsistencies after data migration is done.|$|E
40|$|<b>Data</b> {{integrity}} <b>verification</b> {{is becoming}} a major challenge in cloud storage which canâ€™t be ignored. This paper proposes an optimized variant of CRC (Checker Redundancy Cyclic) verification algorithm based on HDFS to improve the efficiency of <b>data</b> integrity <b>verification</b> in cloud storage through the research of CRC checksum algorithm and <b>data</b> integrity <b>verification</b> mechanism of HDFS. A new method is formulated to establish the deformational optimization and to accelerate the algorithm by researching characteristics of generating and checking the algorithm. Moreover, this method optimizes the code to improve the computational efficiency according to <b>data</b> integrity <b>verification</b> mechanism of HDFS. A <b>data</b> integrity <b>verification</b> system based on Hadoop is designed to verify proposed method. Experimental results demonstrate that proposed HDFS based CRC algorithm was able to improve the calculation efficiency and the utilization of system resource on the whole and outperformed well compared to existing models in terms of accuracy and time...|$|R
5000|$|Biometrics System (Sensors, Integration, <b>Data</b> Analysis, <b>Verification</b> Techniques) ...|$|R
5000|$|RFC 6668, SHA-2 <b>Data</b> Integrity <b>Verification</b> for the Secure Shell (SSH) Transport Layer Protocol (July 2012) ...|$|R
5000|$|... {{log message}} traffic during the {{simulation}} run, for post-processing analysis (including test <b>data</b> <b>verification,</b> response time calculation and screen image comparison across repeated simulations {{of the same}} scripts); ...|$|E
50|$|A type of <b>Data</b> <b>Verification</b> is double {{entry and}} {{proofreading}} data. Proofreading data involves someone checking the data entered against the original document. This is also time consuming and costly.|$|E
50|$|It is {{believed}} that site monitoring costs account for about 20% of the clinical trial budget. Site monitoring involves among other things, 100% source <b>data</b> <b>verification.</b> This is a very exhaustive and expensive process and often prone to errors resulting in FDA citations.|$|E
5000|$|There is no <b>data</b> {{standardization}} <b>verification</b> done at {{the central}} repository thus invalid data is possible.|$|R
50|$|Usually {{the easiest}} part of model {{evaluation}} is checking whether a model fits experimental measurements or other empirical data. In models with parameters, a common approach {{to test this}} fit is to split the data into two disjoint subsets: training <b>data</b> and <b>verification</b> <b>data.</b> The training data are used to estimate the model parameters. An accurate model will closely match the <b>verification</b> <b>data</b> even though these data {{were not used to}} set the model's parameters. This practice is referred to as cross-validation in statistics.|$|R
5000|$|Do not retain {{full track}} <b>data,</b> card <b>verification</b> code or value (CAV2, CID, CVC2, CVV2), or PIN block data.|$|R
50|$|After loading {{into the}} new system, results are {{subjected}} to <b>data</b> <b>verification</b> to determine whether data was accurately translated, is complete, and supports processes in the new system. During verification, {{there may be a}} need for a parallel run of both systems to identify areas of disparity and forestall erroneous data loss.|$|E
50|$|A DigSig {{stored in}} a barcode can be copied without {{influencing}} the <b>data</b> <b>verification.</b> For example; a birth or school certificate containing a DigSig barcode can be copied. The copied document can also be verified to contain the correct information and the issuer of the information. A DigSig barcode provides a method to detect tampering with the data.|$|E
50|$|Application Development System Online (ADSO) {{is a tool}} used to {{expedite}} the writing and testing of modular applications using IDMS databases. Activities such as flow-of-control processing, data storage definition, <b>data</b> <b>verification,</b> editing, error handling, terminal input and output, menu creation and menu display are specified by using a series of screens instead of conventional detailed code.|$|E
40|$|Data {{integrity}} {{is an important}} factor to ensure in almost any data and computation related context. It serves not only as one of the qualities of service, but also an important part of data security and privacy. With the proliferation of cloud computing and the increasing needs in big <b>data</b> analytics, <b>verification</b> of <b>data</b> integrity becomes increasingly important, especially on outsourced data. Therefore, research topics related to <b>data</b> integrity <b>verification</b> have attracted tremendous research interest. Among all the metrics, efficiency and security are two of the most concerned measurements. In this paper, we provide an analysis on authenticator-based efficient <b>data</b> integrity <b>verification.</b> we will analyze and provide a survey on the main aspects of this research problem, summarize the research motivations, methodologies as well as main achievements of several of the representative approaches, then try to bring forth a blueprint for possible future development...|$|R
40|$|Evidence of the {{challenge}} faced by the meteorological community to become skilled in applying risk management products from the financial markets is growing. This paper presents an approach to the pricing of weather derivatives that employs a combination of empirical <b>data</b> including forecast <b>verification</b> <b>data,</b> regional synoptic classification data, and data associated with climate indices on a global scale, such as the Southern Oscillation Index. The paper presents several illustrative examples that show how to price these options about the occurrence of an unusual weather event, using forecast <b>verification</b> <b>data</b> and synoptic classification data. 1...|$|R
40|$|SUPPLEMENTARNY NOTES iS. 0 KEy WOROS (Catcomm an #evr " eW if Meee dinf 411111 P Wee 114 ee 6 Ocean thermal {{structure}} Oceanic planetary {{boundary layer}} Mixed layer depth Mixed layer temperature I-D ocean model 41 A " RACT (9 [...] . 9. epfwee s to ee 00 " " am I U Weeko mbv) 7 ~ data assimilation technique for incorporating relatively sparse ocean thermal structure profiles into the Garwood (1977) Oceanic Plane-tary Boundary Layer (OPBL) model is proposed. A summnary of the data assimilation tests by Elsberry and Warrenfeltz (EW) is presented. The complete and perfect model generated <b>verification</b> <b>data</b> from the EW study were used to simulate incomplete and noisy data {{as might be expected}} in real <b>data</b> <b>verifications.</b> Random errors that are normally distributed-"...|$|R
50|$|DBC {{provides}} {{its data}} via DanBib, a joint bibliography {{for the entire}} Danish library system that includes a common reference database of national bibliography and holdings in all Danish public and research libraries. The system includes features to facilitate interlibrary loans, reuse of bibliographic <b>data</b> <b>verification</b> as well as links to relevant databases such as the Library of Congress and the ISSN system.|$|E
50|$|Allsec Technologies is {{a company}} with {{presence}} across India and enabled services in India, the United States, United Kingdom, and internationally. Allsec Technologies {{is a company}} providing web development, web design, search engine optimization, strategic teleservices, customer care, and quality management services. It also provides <b>data</b> <b>verification,</b> processing orders received through telephone calls, telemarketing, monitoring quality of calls, and customer services. Its clients include Genpact, Accenture, Wells Fargo etc.|$|E
50|$|Unlike some {{competing}} {{technologies such}} as Digital Linear Tape (DLT) and Linear Tape-Open (LTO), Travan technology does not automatically verify data after writing. <b>Data</b> <b>verification</b> must be done separately by the computer operator, to verify data was written successfully. If a separate verify operation is not performed after each backup, {{it is possible for}} backups to be found to be corrupt and unusable when the tapes need to be used.|$|E
50|$|Principle of unity: the {{databases}} {{belonging to}} the state information system must be compatible {{with each other and}} be capable of interoperability, exchange of <b>data</b> and <b>verification</b> of <b>data.</b>|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedA data assimilation technique for incorporating relatively sparse ocean thermal structure profiles into the Garwood (1977) Oceanic Planetary Boundary Layer (OPBL) model is proposed. A {{summary of the}} data assimilation tests by Elsberry and Warrenfeltz (EW) is presented. The complete and perfect model generated <b>verification</b> <b>data</b> from the EW study were used to simulate incomplete and noisy data {{as might be expected}} in real <b>data</b> <b>verifications.</b> Random errors that are normally distributed about the mean mixed layer depth (MLD) and temperature (MLT), are added to subsets of the EW <b>verification</b> <b>data</b> during the summer and winter regimes. From these simulated tests, it was concluded that a data assimilation technique with a 1 -D OPBL model can improve predictions of the ocean thermal structure even with incomplete and noisy <b>verification</b> <b>data.</b> Real bathythermographic temperature profiles from Ocean Weather Station PAPA are then inserted into the Garwood model to verify the EW data assimilation studies. The tests with real data demonstrate the necessity of defining the MLD in an observed profile that is consistent with the model output MLD. In addition, biases were observed that originated from the use of an imperfect model. After the elimination of the biases and the MLD descrepancies, it is suggested that a 1 -D model used for data assimilation can improve predictions of the ocean thermal structure. [URL] United States Nav...|$|R
40|$|Abstract. Aiming {{at current}} problem of high {{computing}} overhead of encryption and authentication, this paper proposes a confidentiality and integrity protection method {{based on one}} time encryption. The method packages <b>data</b> block and <b>verification</b> <b>data</b> together, and then encrypts it. The confidentiality is guaranteed by traditional encryption mode and the integrity is guaranteed by decrypted <b>verification</b> <b>data.</b> Analysis and simulation show that the method has less time and space overhead than standard encryption and authentication mode. The method {{can be applied to}} other integrity and confidentiality protection schemes, and can meet the performance requirements of the most applications...|$|R
50|$|To {{reduce the}} burden of site {{monitoring}} on clinical trials, the FDA now recommends a risk-based monitoring (RBM) approach. The essence of this recommendation is to gather meaningful metrics that ascertain the safety and welfare of the subjects and performance of the clinical trial, rather than doing 100% source <b>data</b> <b>verification.</b> Adopting RBM may reduce site monitoring costs by 25%. thus reducing overall cost and risk on clinical trials.|$|E
50|$|CRCs are {{so called}} because the check (<b>data</b> <b>verification)</b> value is a {{redundancy}} (it expands the message without adding information) and the algorithm {{is based on}} cyclic codes. CRCs are popular because they are simple to implement in binary hardware, easy to analyze mathematically, and particularly good at detecting common errors caused by noise in transmission channels. Because the check value has a fixed length, the function that generates it is occasionally used as a hash function.|$|E
50|$|In January 2002, Howard Fukada {{proposed}} {{that a new}} Par2 specification should be devised with the significant changes that <b>data</b> <b>verification</b> and repair should work on blocks of data rather than whole files, and that the algorithm should switch to using 16 bit numbers rather than the 8 bit numbers that PAR 1 used. Michael Nahas and Peter Clements took up these ideas in July 2002, with additional input from Paul Nettle and Ryan Gallagher (who both wrote Par1 clients). Version 2.0 of the Parchive specification was published by Michael Nahas in September 2002.|$|E
5000|$|<b>Data</b> {{validation}} and <b>verification</b> {{should be}} completed for each development iteration.|$|R
40|$|Abstractâ€”The {{establishment}} of trust relationships to a trusted platform {{relies on the}} process of validation. Validation allows an external entity to build trust in the expected behaviour of the platform based on provided evidence of the platformâ€™s configu-ration. In a validation mechanism such as remote attestation, the trusted platform exhibits <b>verification</b> <b>data</b> created during a start up process. These data consist in hardware-protected values of platform configuration registers, containing nested measurement values, i. e., hash values, of all loaded or started components. The values are created in linear order by the secured extend operation. Fine-grained diagnosis of components by the validator, based on the linear order of <b>verification</b> <b>data</b> and associated measurement logs, is inefficient. We propose a method to create a tree-formed <b>verification</b> <b>data,</b> in which component measurement values represent leaves and protected registers represent roots. It is shown how this is possible using a limited number of hardware-protected registers and the standard extend operation. In this way, the security of <b>verification</b> <b>data</b> is maintained, while the stored measurement log is consistently organised as a tree. We exhibit the basic mechanism of validating a platform using tree-formed measurement logs and <b>verification</b> <b>data.</b> I...|$|R
50|$|Historical <b>data</b> check <b>verification</b> {{services}} that use {{a national network}} with a negative check database can be difficult for consumers and businesses to remove themselves from once they get on, even {{in the case of}} errors.|$|R
50|$|One {{proposed}} {{smart meter}} <b>data</b> <b>verification</b> method involves analyzing the network traffic {{in real time}} to detect anomalies using an Intrusion Detection System (IDS) By identifying exploits as they are being leveraged by attackers, an intrusion detection system (IDS) will mitigate the suppliers' risks of energy theft by consumers and denial-of-service attacks by hackers. Energy utilities will {{have to choose between}} a centralized IDS, embedded IDS, or dedicated IDS depending on the individual needs of the utility. Researchers have found that for a typical advanced metering infrastructure, the centralized IDS architecture is superior in terms of cost efficiency and security gains.|$|E
5000|$|While a few {{universities}} and corporations {{are working on}} a method to defeat these fake reviews, none seems to have yet been reliably successful. Testimonial Shield, a private company that began third party testimonial and review verification in the mid-2000s was awarded an independent business award for [...] "the 2013 San Diego Award in the <b>Data</b> <b>Verification</b> Service category" [...] for their ability to automatically discern with a claimed 90+% accuracy the difference between a legitimate review and an fake review, which displays great progress towards the prevention of fake reviews and testimonials on the internet.|$|E
50|$|AS-Interface {{provides}} {{a basis for}} Functional Safety in machinery safety/emergency stop applications. Safety devices communicating over AS-Interface follow all the normal AS-Interface data rules. The required level of <b>data</b> <b>verification</b> is provided by dynamic changes in the data. This technology is called Safety at Work and allows safety devices and standard, non-safe devices {{to be connected to}} the same network. Using appropriate safe input hardware (e.g. light curtains, e-stop buttons, and door interlock switches), AS-Interface can provide safety support up to SIL (Safety Integrity Level) 3 according to EN 62061, CAT 4 according to EN954-1 as well as Performance Level e (PL e) according to EN ISO 13849-1.|$|E
40|$|We study <b>data</b> {{integrity}} <b>verification</b> in peer-to-peer media streaming {{for content}} distribution. Challenges include the timing constraint of streaming {{as well as}} the untrustworthiness of peers. We show the inadequacy of existing <b>data</b> integrity <b>verification</b> protocols, and propose Block-Oriented Probabilistic Verification (BOPV), an efficient protocol utilizing message digest and probabilistic verification. We then propose Treebased Forward Digest Protocol (TFDP) to further reduce the communication overhead. A comprehensive comparison is presented by comparing the performance of existing protocols and our protocols, with respect to overhead, security assurance level, and packet loss tolerance. Finally, experimental results are presented to evaluate the performance of our protocols. 1...|$|R
30|$|Existing <b>data</b> {{integrity}} <b>verification</b> algorithms mainly extract {{some data}} from the entire set of data randomly to execute the verification [23]. Thus, even though there exist different version files, each time the data extraction is independent and random.|$|R
50|$|Service Objects is {{a contact}} and data {{validation}} company specializing in <b>data</b> quality and <b>verification.</b>|$|R
