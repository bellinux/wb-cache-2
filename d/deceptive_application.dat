1|3|Public
40|$|The aim of {{this paper}} is to present and discuss the power of the exact {{likelihood}} ratio homogeneity testing procedure of the number of components k in the exponential mixture. First we present the likelihood ratio test for homogeneity (ELR), the likelihood ratio test for homogeneity against two-component exponential mixture (ELR 2), and finally the likelihood ratio test for homogeneity against three-component exponential mixture (ELR 3). Comparative power study of mentioned homogeneity tests against three-component subpopulation alternative is provided. Therein we concentrate on various setups of the scales and weights, which allow us to make conclusions for generic settings. The natural property is observed, namely increase of the power of exact likelihood ratio ELR, ELR 2 and ELR 3 tests with scale parameters considered in the alternative. We can state that the differences in power of ELR, ELR 2 and ELR 3 tests are small – therefore using of the computationally simpler ELR 2 test is recommended for broad usage rather than computationally more expensive ELR 3 test in the cases when unobserved heterogeneity is modelled. Anyhow caution should be taken before automatic usage of ELR 3 in more informative settings, since the application of automatic methods hoping that the data will enforce its true structure is <b>deceptive.</b> <b>Application</b> of obtained results in reliability, finance or social sciences is straightforward...|$|E
40|$|The Association for the Advancement of Artificial Intelligence {{presented}} the 2015 Fall Symposium Series, on Thursday through Saturday, November 12 - 14, at the Westin Arlington Gateway in Arlington, Virginia. The titles {{of the six}} symposia were as follows: AI for Human-Robot Interaction, Cognitive Assistance in Government and Public Sector <b>Applications,</b> <b>Deceptive</b> and Counter-Deceptive Machines, Embedded Machine Learning, Self-Confidence in Autonomous Systems, and Sequential Decision Making for Intelligent Agents. This article contains the reports from four of the symposia...|$|R
40|$|Electronic {{commerce}} {{has become}} more and more accepted from the time when the World Wide Web has emerged. Numerous websites permit Internet users to purchase and put on the market products and services online, which beneﬁts everybody in terms of expediency and proﬁtability.    The conventional online shopping business permit sellers to put up for sale a product or service at a predetermined price, where buyers can decide to obtain if they ﬁnd it to be a good agreement. Comparable to any platform supporting ﬁnancial connections, online auction be a focus for criminals to perform fraud. Due to the advantages from online trading, schemers are taking benefits to attain misleading activities against truthful parties to get hold of <b>deceptive</b> earnings. <b>Application</b> of Proactive fraud-detection moderation systems is usually functional to distinguish and avert such prohibited and deceptive activities in a most important Asian online auction site, where hundreds of thousands of novel auction cases are produced day by day. The moderation system by means of machine-learned models is proven to get better fraud detection importantly over the human-tuned weights. An online probit model framework is proposed in this paper that takes online feature range, coefficient bounds from human acquaintance and several instances learning into account concurrently and can possibly differentiate more frauds and comprehensively reduce customer objections which are based on a real-world online auction fraud detection data compared to several baseline models and the human-tuned rule-based system. </strong...|$|R
40|$|The {{popularity}} of the internet has made the use of web applications ubiquitous and essential to {{the daily lives of}} people, businesses and governments. Web servers and web applications are commonly used to handle tasks and data that can be critical and highly valuable, making them a very attractive target for attackers and a vector for successful attacks that are aimed at the application layer. Existing misuse and anomaly-based detection and prevention techniques fail to cope with the volume and sophistication of new attacks that are continuously appearing, which suggests {{that there is a need}} to provide new additional layers of protection. This work aims to design a new layer of defense based on deception that is employed in the context of web application-layer traffic with the purpose of detecting and preventing attacks. The proposed design is composed of five deception strategies: Deceptive Comments, Deceptive Request Parameters, Deceptive Session Cookies, Deceptive Status Codes and Deceptive JavaScript. The strategies were implemented as a software artifact and their performance evaluated in a testing environment using a custom test script, the OWASP ZAP penetration testing tool and two vulnerable web <b>applications.</b> <b>Deceptive</b> Parameter strategy obtained the best security performance results, followed by Deceptive Comments and Deceptive Status Codes. Deceptive Cookies and Deceptive JavaScript got the poorest security performance results since OWASP ZAP was unable to detect and use deceptive elements generated by these strategies. Operational performance results showed that the deception artifact could successfully be implemented and integrated with existing web applications without changing their source code and adding a low operational overhead...|$|R

