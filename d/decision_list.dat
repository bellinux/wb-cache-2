136|472|Public
25|$|Director of {{photography}} Rodrigo Prieto {{indicated that the}} film was shot in two distinct visual styles: scenes featuring the media were shot in the anamorphic format on 35mm film, while scenes focusing on the world of politics were shot in high-definition video with the Panavision Genesis digital video camera. Hand-held cameras have been used. For color management, Prieto employed Gamma & Density Company's 3cP color management and correction software, using the American Society of Cinematographers' Color <b>Decision</b> <b>List</b> to keep color consistent throughout shooting, dailies, post and digital intermediate finishing process. The digital effects were handled by Rhythm and Hues.|$|E
2500|$|In March 2005, he {{contributed to}} the Cedar Revolution demonstrations during which he gave the famous speech, [...] "In the name of God We, Muslims and Christians, Pledge that united we shall remain {{to the end of}} time to better defend our Lebanon". In May 2005, he was elected a member of Parliament of Lebanon for the Eastern Orthodox Christian seat in Beirut's first {{district}} on an anti-Syrian slate, namely Martyr Prime Minister Rafik Hariri's Beirut <b>Decision</b> <b>List,</b> led by Saad Hariri.|$|E
5000|$|A <b>decision</b> <b>list</b> {{algorithm}} is {{then used to}} identify other reliable collocations. This training algorithm calculates the probability Pr(Sense | Collocation), and the <b>decision</b> <b>list</b> is ranked by the log-likelihood ratio: ...|$|E
40|$|<b>Decision</b> <b>lists</b> and <b>decision</b> {{trees are}} two models of {{computation}} for boolean functions. Blum has shown in [Bl] (Information Processing Letters 42 (1992), 183 - 185) that rank-k decision trees are a subclass of <b>decision</b> <b>lists.</b> Here we identify precisely, by giving a syntactical characterization, the subclass of <b>decision</b> <b>lists</b> which correspond exactly {{to the class}} of bounded rank decision trees. Furthermore we give a more general algorithm to recover reduced decision trees from <b>decision</b> <b>lists.</b> 1 Introduction <b>Decision</b> <b>lists</b> have been introduced in [R] as a representation of Boolean functions. Here we define a subclass of decision lists: the class of tree-like <b>decision</b> <b>lists.</b> For the elements of this class we define a measure that, because its analogy with the rank measure for decision trees (see [EH]), we call, abusing the term, rank for tree-like <b>decision</b> <b>lists.</b> Blum in [Bl] showed that rank-k decision trees are a subclass of k-DL (<b>decision</b> <b>lists</b> in which any term has at most k variables) [...] . ...|$|R
5000|$|<b>Decision</b> <b>lists</b> are a {{representation}} for Boolean functions. [...] Single term <b>decision</b> <b>lists</b> are more expressive than disjunctions and conjunctions; however, 1-term <b>decision</b> <b>lists</b> are less expressive {{than the general}} disjunctive normal form and the conjunctive normal form.|$|R
40|$|This {{report is}} an {{exposition}} of <b>decision</b> <b>lists</b> and threshold <b>decision</b> <b>lists.</b> A {{version of this}} is to appear as a chapter in a book on Boolean functions, but the report itself is relatively self-contained. The key areas explored are the representation of Boolean functions by <b>decision</b> <b>lists</b> and threshold <b>decision</b> lists; properties of classes of decision list; and algorithmic questions associated with <b>decision</b> <b>lists.</b> ...|$|R
5000|$|Edit <b>Decision</b> <b>List</b> Processor (film/video production) (1989) Amiga ...|$|E
5000|$|The National <b>Decision</b> <b>List</b> - led by Sabah Abd al-Rasoul Abd al-Ridha Rashid al-Tammimi ...|$|E
5000|$|Video editing {{the picture}} of a {{television}} program using an edit <b>decision</b> <b>list</b> (EDL) ...|$|E
40|$|<b>Decision</b> <b>lists</b> and <b>decision</b> {{trees are}} two models of {{computation}} for boolean functions. Blum has shown (Information Processing Letters 42 (1992), 183 - 185) that rank-k decision trees are a subclass of <b>decision</b> <b>lists.</b> Here we identify precisely, by giving a syntactical characterization, the subclass of <b>decision</b> <b>lists</b> which correspond exactly {{to the class}} of bounded rank decision trees. Furthermore we give a more general algorithm to recover reduced decision trees from <b>decision</b> <b>lists.</b> Postprint (published version...|$|R
40|$|We {{study the}} learnability of {{monotone}} term <b>decision</b> <b>lists</b> {{in the exact}} model of equivalence and membership queries. We show that, for any constant k> 0, k-term monotone <b>decision</b> <b>lists</b> are exactly and properly learnable with n^O(k) membership queries in O(n^(k^ 3)) time. We also show n^Omega(k) membership queries are necessary for exact learning. In contrast, both k-term monotone <b>decision</b> <b>lists</b> (k> 1) and general monotone <b>decision</b> <b>lists</b> are not learnable with equivalence queries alone. Postprint (published version...|$|R
40|$|We {{introduce}} a new representation class of Boolean functions [...] -monotone term <b>decision</b> <b>lists</b> [...] -which combines compact representation size with tractability of essential operations. We present many properties of the class which make it an attractive alternative to traditional universal representation classes such as DNF formulas or decision trees. We study the learnability of monotone term <b>decision</b> <b>lists</b> in the exact model of equivalence and membership queries. We show that, for any constant k >= 0, k-term monotone <b>decision</b> <b>lists</b> are exactly and properly learnable with n^(O(k)) membership queries in n^(O(k^ 3)) time. We also show that n^(Omega (k)) membership queries are necessary for exact learning. In contrast, both k-term monotone <b>decision</b> <b>lists</b> (k >= 2) and general monotone term <b>decision</b> <b>lists</b> are not learnable with equivalence queries alone. We also show that a subclass of monotone term <b>decision</b> <b>lists</b> (disj-MDL) is learnable with equivalence and membership queries, while neither type of query alone suffices. Postprint (published version...|$|R
5000|$|Engineering Emmy Award: American Society of Cinematographers (ASC) Technology Committee for the ASC Color <b>Decision</b> <b>List</b> (ASC CDL) ...|$|E
50|$|The {{language}} {{specified by}} a k-length <b>decision</b> <b>list</b> includes as a subset the language specified by a k-depth decision tree.|$|E
50|$|Professional editing {{software}} records the editor's decisions in an edit <b>decision</b> <b>list</b> (EDL) that is exportable to other editing tools. Many generations and {{variations of the}} original source files can exist without storing many different copies, allowing for very flexible editing. It also {{makes it easy to}} change cuts and undo previous decisions simply by editing the edit <b>decision</b> <b>list</b> (without having to have the actual film data duplicated). Generation loss is also controlled, due to not having to repeatedly re-encode the data when different effects are applied.|$|E
40|$|AbstractWe {{introduce}} a new representation class of Boolean functions – monotone term <b>decision</b> <b>lists</b> – which combines compact representation size with tractability of essential operations. We present many properties of the class which make it an attractive alternative to traditional universal representation classes such as DNF formulas or decision trees. We study the learnability of monotone term <b>decision</b> <b>lists</b> in the exact model of equivalence and membership queries. We show that, for any constant k⩾ 0, k-term monotone <b>decision</b> <b>lists</b> are exactly and properly learnable with nO(k) membership queries in nO(k 3) time. We also show that nΩ(k) membership queries are necessary for exact learning. In contrast, both k-term monotone <b>decision</b> <b>lists</b> (k⩾ 2) and general monotone term <b>decision</b> <b>lists</b> are not learnable with equivalence queries alone. We also show that a subclass of monotone term <b>decision</b> <b>lists</b> (disj-MDL) is learnable with equivalence and membership queries, while neither type of query alone suffices...|$|R
40|$|We {{prove that}} log n-decision lists - {{the class of}} <b>decision</b> <b>lists</b> such that all their terms have low Kolmogorov {{complexity}} - are learnable in the simple PAC learning model. The proof {{is based on a}} transformation from an algorithm based on equivalence queries (found independently by Simon). Then we introduce the class of simple <b>decision</b> <b>lists,</b> and extend our algorithm to show that simple <b>decision</b> <b>lists</b> are simple-PAC learnable as well. This last result is relevant in that it is, to our knowledge, the first learning algorithm for <b>decision</b> <b>lists</b> in which an exponentially wide set of functions may be used for the terms. (This report supersedes LSI- 95 - 2 -R.) Postprint (published version...|$|R
40|$|We {{study the}} learnability of {{monotone}} term <b>decision</b> <b>lists</b> {{in the exact}} model of equivalence and membership queries. We show that, for any constant k 0, k-term monotone <b>decision</b> <b>lists</b> are exactly and properly learnable with n O(k) membership queries in O(n k 3) time. We also show n ΩΓ k) membership queries are necessary for exact learning. In contrast, both k-term monotone <b>decision</b> <b>lists</b> (k 2) and general monotone <b>decision</b> <b>lists</b> are not learnable with equivalence queries alone. 1 Supported by the Esprit EC program under project 7141 (ALCOM-II), the Working Group 8556 (NeuroColt), and the Spanish DGICYT (project PB 92 - 0709) 2 Supported by grant FP 93 13717942 from the Spanish Government 3 This work was supported by NSF grant CCR- 9510392. 1 Introduction <b>Decision</b> <b>lists</b> were introduced by Rivest [Riv 87], who showed that the class of k-decision lists is properly PAC-learnable in polynomial time, for constant k 0. Here the constant k refers to the maximum number [...] ...|$|R
50|$|Once {{the entire}} movie has been edited, an Edit <b>Decision</b> <b>List</b> of marked frames is {{turned over to}} a film {{laboratory}} where the actual pieces of film are spliced together in the correct order.|$|E
50|$|When the {{algorithm}} converges on a stable residual set, a final <b>decision</b> <b>list</b> {{of the target}} word is obtained. The most reliable collocations {{are at the top}} of the new list instead of the original seed words. The original untagged corpus is then tagged with sense labels and probabilities. The final <b>decision</b> <b>list</b> may now be applied to new data, the collocation with the highest rank in the list is used to classify the new data. For example, if the highest ranking collocation of the target word in the new data set is of sense A, then the target word is classified as sense A.|$|E
50|$|An edit <b>decision</b> <b>list</b> or EDL {{is used in}} the {{post-production}} {{process of}} film editing and video editing. The list contains an ordered list of reel and timecode data representing where each video clip can be obtained in order to conform the final cut.|$|E
40|$|In {{this paper}} we {{introduce}} a novel family of <b>decision</b> <b>lists</b> consisting of highly interpretable models {{which can be}} learned efficiently in a greedy manner. The defining property is that all rules are oriented in the same direction. Particular examples of this family are <b>decision</b> <b>lists</b> with monotonically decreasing (or increasing) probabilities. On simulated data we empirically confirm that the proposed model family is easier to train than general <b>decision</b> <b>lists.</b> We exemplify the practical usability of our approach by identifying problem symptoms in a manufacturing process. Comment: IEEE Big Data for Advanced Manufacturin...|$|R
40|$|AbstractThe paper {{introduces}} {{the notion of}} <b>decision</b> <b>lists</b> over regular patterns. This formalism provides a strict extension of regular erasing pattern languages and of containment <b>decision</b> <b>lists.</b> Formal properties of the resulting language class, a subclass of the regular languages, are investigated. In particular, we show that <b>decision</b> <b>lists</b> over regular patterns have exactly the same expressive power as decision trees over regular patterns. Moreover, we study the learnability of the resulting language class within different formal settings including Gold's model of learning in the limit as well as Valiant's model of approximately correct learning...|$|R
50|$|Learning <b>decision</b> <b>lists</b> can be {{used for}} {{attribute}} efficient learning.|$|R
50|$|Headquartered in Sunnyvale, California, {{the company}} pioneered in {{integrating}} computers with videotape editing, starting in 1971 with the CMX 600, the first {{non-linear video editing}} system. The 600 was designed primarily for off-line editing, by creating both a rough cut edit of a video program, along with an edit <b>decision</b> <b>list,</b> or EDL. It stored its video & audio content on disk pack drives supplied by Memorex for instant random access of the video content. The 600 was paired with the CMX-200, which took the edit <b>decision</b> <b>list</b> created by the 600, and automatically controlled several VTRs to auto-assemble the final program. The 600 was controlled using a Digital PDP-11 minicomputer, and the 200 used a Teletype Model 33 terminal to input EDLs from the 600.|$|E
50|$|Autoconform is {{the video}} editing post {{production}} process where an online editing system combines a timecode based edit <b>decision</b> <b>list</b> (EDL) created from an offline editing {{system with the}} original video and audio source material to produce {{a version of the}} edited video which is a high quality (usually broadcast quality) analogue of the programme produced in the offline editing system.|$|E
50|$|When editing {{was done}} using {{magnetic}} tapes that {{were subject to}} damage from excessive wear, it was common to use a window dub as a working copy {{for the majority of}} the editing process. Editing decisions would be made using a window dub, and no specialized equipment was needed to write down an edit <b>decision</b> <b>list</b> which would then be replicated from the high-quality masters.|$|E
40|$|Abstract. We present BUFOIDL, a new {{bottom-up}} algorithm {{for learning}} first order <b>decision</b> <b>lists.</b> Although first order <b>decision</b> <b>lists</b> have {{potential as a}} representation for learning concepts that include exceptions, such as language constructs, previous systems suffered from limitations that we seek to overcome in BUFOIDL. We present experiments comparing BUFOIDL to previous work in the area, demonstrating the system’s potential. ...|$|R
40|$|We {{consider}} two well-studied problems regarding attribute efficient learning: learning <b>decision</b> <b>lists</b> {{and learning}} parity functions. First, we give an algorithm for learning <b>decision</b> <b>lists</b> of length k over n variables using 2 Õ(k 1 / 3) log n examples and time n Õ(k 1 / 3). This {{is the first}} algorithm for learning <b>decision</b> <b>lists</b> that has both subexponential sample complexity and subexponential running time in the relevant parameters. Our approach {{is based on a}} new construction of low degree, low weight polynomial threshold functions for <b>decision</b> <b>lists.</b> For a wide range of parameters our construction matches a lower bound due to Beigel for <b>decision</b> <b>lists</b> and gives an essentially optimal tradeoff between polynomial threshold function degree and weight. Second, we give an algorithm for learning an unknown parity function on k out of n variables using O(n 1 − 1 /k) examples in poly(n) time. For k = o(log n) this yields the first polynomial time algorithm for learning parity on a superconstant number of variables with sublinear sample complexity. We also give a simple algorithm for learning an unknown size-k parity using O(k log n) examples in n k/ 2 time, which improves on the naive n k time bound of exhaustive search...|$|R
40|$|We show an {{algorithm}} that learns <b>decision</b> <b>lists</b> via equivalence queries, {{provided that}} a set G including all {{terms of the}} target list is given. The algorithm runs in time polynomial in the cardinality of G. From this last learning algorithm, we prove that log n-decision lists - the class of <b>decision</b> <b>lists</b> such that all their terms have low Kolmogorov complexity - are simple pac-learnable. Postprint (published version...|$|R
5000|$|Currency hedging {{can be done}} passively or actively. The {{stream of}} returns from passive {{currency}} overlay is negatively correlated with international equities, has an expected return of zero, and does not employ any capital. The overlay manager uses forward contracts to match the portfolio’s currency exposures {{in such a way}} as to insure against exchange rate fluctuations. A <b>decision</b> <b>list</b> for a passive overlay manager would include ...|$|E
5000|$|The DIT's role on-set {{has become}} {{especially}} prevalent through assisting cinematographers, normally accustomed to film stock, in achieving their desired look digitally. This {{is accomplished by}} the DIT through monitoring picture exposure, setting up Color <b>Decision</b> <b>List</b> (CDL) on daily basis and, if requested, [...] "look up tables" [...] (LUTs) for the post-production. Additionally, the DIT deals with settings in the digital camera's menu system, such as recording format and outputs.|$|E
5000|$|... where [...] is the th {{formula and}} [...] is the th boolean for [...] The last {{if-then-else}} is the default case, which means formula [...] is always equal to true. A -DL is a <b>decision</b> <b>list</b> {{where all of}} formulas have at most [...] terms. Sometimes [...] "decision list" [...] is {{used to refer to}} a 1-DL, where all of the formulas are either a variable or its negation.|$|E
40|$|This paper takes Yarowsky's {{work as a}} {{starting}} point, applying <b>decision</b> <b>lists</b> {{to the problem of}} context-sensitive spelling correction. <b>Decision</b> <b>lists</b> are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence. A new hybrid method, based on Bayesian classi#ers, is presented for doing this, and its performance improvements are demonstrate...|$|R
40|$|This paper {{describes}} a supervised algorithm for {{word sense disambiguation}} based on hierarchies of <b>decision</b> <b>lists.</b> This algorithm supports a useful degree of conditional branching while minimizing the training data fragmentation typical of decision trees. Classifications {{are based on a}} rich set of collocational, morphological and syntactic contextual features, extracted automatically from training data and weighted sensitive {{to the nature of the}} feature and feature class. The algorithm is evaluated comprehensively in the senseval framework, achieving the top performance of all participating supervised systems on the 36 test words where training data is available. Keywords: word sense disambiguation, <b>decision</b> <b>lists,</b> supervised machine learning, lexical ambiguity resolution, senseval 1. Introduction <b>Decision</b> <b>lists</b> have been shown to be effective at a wide variety of lexical ambiguity resolution tasks including word sense disambiguation (Yarowsky, 1994, 1995; Mooney, 1996; Wilks and S [...] ...|$|R
40|$|In Japanese {{constructions of}} the form [N 1 no Adj N 2], the {{adjective}} Adj modifies either N 1 or N 2. Determing the semantic dependencies of adjective in such phrase is an important task for machine translation. This paper describes a method for determining the adjective dependency in such constructions using <b>decision</b> <b>lists,</b> and inducing <b>decision</b> <b>lists</b> from training contexts with correct semantic dependencies and without. Based on evaluation, our method is able to determine adjective dependency with an precision of about 94 %. We further analyze rules in the induce...|$|R
