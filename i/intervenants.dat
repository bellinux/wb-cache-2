16|26|Public
5000|$|Oversee the {{protection}} of victims, juries, witnesses and all other <b>intervenants</b> in the criminal procedure.|$|E
5000|$|Following this colloquium, {{to which}} about 20 {{students}} participated (among them, Stefano Delle Chiaie and Mario Michele Merlino [...] ), Giannettini and other <b>intervenants</b> were {{hired by the}} Italian secret services. In April 1968, these students participated to a trip to Greece of 60 students from the [...] "League of Greek Fascist Students in Italy" [...] and of 51 Italian neo-fascist students, organized by the Greek junta. According to Frédéric Laurent, author of L'Orchestre noir (p. 75), [...] "more {{than half of the}} Italians (...) returned from Athens suddenly converted to Anarchism, Leftism, or to Communism, preferably Chinese".|$|E
40|$|Dans le domaine de la déficience intellectuelle, des {{relations}} de nature partenariale entre les parents et les <b>intervenants</b> sont maintenant souhaitées. D’ailleurs, les avantages d’établir un partenariat dans les relations entre les parents et les <b>intervenants</b> ne sont plus à démontrer. Pourtant, les écrits portant sur ce sujet dressent un portrait où le partenariat est plutôt absent des relations entre les <b>intervenants</b> et les parents. En situation d’hébergement, le partenariat entre les parents et les <b>intervenants</b> est encore plus pertinent puisqu’il constitue un facteur influençant positivement l’implication des parents auprès de leur enfant ayant une déficience intellectuelle. Par contre, le sujet spécifique des relations entre les <b>intervenants</b> et les parents de personnes ayant une déficience intellectuelle en contexte d’hébergement a été peu exploré dans les écrits. Dans le cadre du présent mémoire, une recherche qualitative a été menée afin de connaître la perception de parents d’adultes ayant une déficience intellectuelle de leurs relations avec les <b>intervenants</b> dans un contexte d’hébergement. Dix parents ont donc été rencontrés en entrevue. Les objectifs de cette recherche étaient de qualifier, à partir de la perception de parents, {{la nature}} des relations qu’ils entretiennent avec les <b>intervenants</b> et de cibler les facteurs qui influencent leurs relations. In the mental retardation field, partnership relationships {{between parents and}} caregivers are now a must. In fact, the advantages of a partnership in the relationships between parents and caregivers {{no longer need to}} be validated. Still, written documents on this subject usually depict a picture of a partnership that is somewhat lacking in the relationships between the caregivers and the parents. This partnership between caregivers and parents is even more important in foster homes settings as it strongly influences the parents’ involvement in the life of their child with learning disabilities. However, the actual subject of relationships between caregivers and parents of adult children with mental retardation in foster homes settings has been seldom documented. This thesis required a qualitative research be made to better understand the perception of parents of adult children with mental retardation with respect to their relationship with the caregivers in a foster home setting. Ten parents were interviewed. The purpose of this research was to better qualify, according to the parents’ perception, the type of relationships they have with the caregivers and to identify the factors that influence these relationships. The results showed that most parents are satisfied with their relationships with their adult child’s caregivers. The description they give of their relationships brings us to conclude that there is in fact a partnership. From an ecological analysis, we have grouped these factors according to different systems. The familial systemic model also enabled us to analyze the factors at a deeper level. Amongst the identified factors, some are more important than others. First, there is the factor that caregivers recognize the importance of the parents’ contribution. Then, there is the factor that the parents notice a sense of well-being in their child. Finally, there is the factor that the parents are able to voice their point of view and their expectations...|$|E
5000|$|<b>Intervenant</b> sur {{invitation}} à la “ Journée mondiale de la voix” à Toulouse (Colomiers) par Arpa et le Ministère de la Culture, service Inspection ...|$|R
5000|$|<b>Intervenant</b> [...] "La voix du Baroque au Pop". Rencontres autour de la voix / Journées préludes au congrès ICVT: [...] "Voix singulières, répertoires pluriels, {{technique}} commune ?" ...|$|R
40|$|International audienceWhat does a {{temporal}} perspective {{contribute to our}} understanding of the dynamics of the socially gendered division in our societies ? What are the implications of the changes taking place in the sphere of paid work for domestic and gender relations ? The links between the feminisation of the working population and the evolution of the gender division of labour show how important it is to view social relations between the genders against the background of the wider dynamics of social differentiation and polarisation. And the contemporary forms of temporal availability at work can be seen as obstacles on the way to an actual and progressive reduction in working time, as the French experience shows. Qu'apporte la perspective temporelle à la compréhension des dynamiques de la division socialement sexée dans nos sociétés ? On sait que ces dynamiques impliquent de manière solidaire et transversale tous les domaines de la vie sociale : travail et emploi, famille, loisirs, espace urbain, vie politique, mouvements sociaux, représentations culturelles. La réflexion est ici limitée aux implications des changements <b>intervenant</b> dans le domaine du travail professionnel sur les relations domestiques et de genre, domaines dans lesquels de nombreuses données d'ordre temporel sont disponibles...|$|R
40|$|The {{international}} conference "Mary of Burgundy. The Reign, the ‘Persona’ and the Legacy of an European princess" took place from 4 th to 7 th March 2015 in Brussels (University of Birmingham Brussels Office) and Bruges (Groeningemuseum). The conference was ended by a conclusive roundtable untitled “Mary 600 Years On: Where Next?”. The five <b>intervenants</b> were Dr Adrian Armstrong, Queen Mary, University of London, Dr Ann Roberts, Lake Forest College, Prof. Jelle Haemers, Katholieke Universiteit Leuven, Prof. W. Prevenier, Universiteit Gent, and myself. The present text {{is that of}} my intervention to this conclusive roundtable...|$|E
40|$|PROBLEM BEING ADDRESSED Communication between {{community-based}} providers {{is often}} sporadic and problem-focused. OBJECTIVE OF PROGRAM To implement collaborative community-based care among providers distant {{from one another}} and to improve or maintain the health of high-risk community-dwelling patients, with a focus on medication use. PROGRAM DESCRIPTION Six primary health care teams were formed of a family physician, a pharmacist, and a home care case manager (nurse). Three of these teams also had a family physician’s offi ce nurse. Teams received training and decided on processes of care that included a home visit, medication history, and weekly 1. 5 -hour face-to-face team meetings. In 151 team conferences, 705 medication or health issues were identifi ed for 182 patients over 6 months. Medication adherence was improved at 3 and 6 months. After 6 months, all providers had a greater understanding of the roles of the other providers. CONCLUSION Primary health care teams developed in this study require few structural changes to existing health care systems, but will require more reimbursement options. RÉSUMÉ PROBLÈME À L’ÉTUDE La communication entre les <b>intervenants</b> du milieu est souvent sporadique et axée sur des problèmes spécifi ques...|$|E
40|$|Cette thèse est une {{contribution}} à l'analyse de l'action médico-sociale comme forme particulière d'intervention. À partir du terrain des consultations gratuites de protection infantile, il s'agit d'interroger la prévention médico-sociale en acte et d'observer les décalages et tensions qui existent entre législation, organisation des consultations, positionnement professionnel des <b>intervenants,</b> et problématique spécifique du public auquel ils s'adressent en priorité. L'analyse des enjeux que recouvre l'articulation médical/ social et celle des éléments médicaux et sociaux effectivement mobilises en situation révèle que la prévention se réalise, s'invente et se redéfinit nécessairement au quotidien. This {{thesis is}} {{a contribution to}} the analysis of the medico-social action as a particular type of intervention. It is based on a field work in the free consultations of "child protection" (protection infantile in French) in France, and questions the medico-social prevention while being "performed". 1 bis research also observes the gaps and tensions which exist between legislation, organization of the consultations, professional positions taken by the protagonists, and the specific issue of the public addressed in priority by these consultations. There are numerous stakes within the articulation of medical and social, as well as various medical and social elements raised in the real-life situations. The analysis of these stakes and elements reveals that prevention is crafted, invented, and necessarily redefined in daily interactions. PARIS 3 -BU (751052102) / SudocSudocFranceF...|$|E
40|$|This is a {{qualitative}} study that aimed {{to examine the}} nonverbal communication mediated by music among musicians (Centre de Formation de Musicien <b>Intervenant)</b> and institutionalized aged, and produce a new education strategy for the course of musicians training developed at the University Marc Bloch, in France. After consent and approval by ethical review board {{data were collected from}} scenes filmed (without editing), for a previous documentary of Music in Hospitals Project. Two musical encounters were analyzed for decoding nonverbal communication and scenes of six similar musical encounters were included in the educational video to illustrate the theoretical content to be taught. The decoding of non-verbal communication was carried out by units of analysis (taken from the video camera). In the first musical encounter were assessed 68 units of analysis and in the second were assessed 50 units of analysis. The interpersonal relations were characterized by facial expressions, types of smile and ways of looking. The affective touch occurred several times between musicians and aged persons. The close distance between the participants was observed. The data analysis resulted in the production of an educational video with audio in Portuguese and in French...|$|R
40|$|International audienceThe {{existence}} and visibility of port {{facilities in the}} period of Antiquity are very closely linked to the evolutive history of the coastline. From the Neolithic period on, the slowing down of the sea level rise on the rocky coast of Provence allowed the filling of the rias which had been invaded by water following the postglacial warming. The opposing mechanisms of coastal sedimentation, {{on the one hand and}} erosion by the sea, on the other hand, greatly modified the conditions permitting the creation of port facilities as well as their approach by sailors. During the last thirty years, collaboration between geomorphologists and archaeologists in charge of emergency interventions in coastal areas has considerably renewed our knowledge in this field. This article sums up the knowledge acquired on a certain number of coastal sites, particularily Antibes, Olbia, Toulon and Marseilles. L’existence et la visibilité des implantations portuaires antiques sont étroitement dépendantes de l’histoire du trait de côte. Depuis la période néolithique, le ralentissement de la montée du niveau marin sur les côtes rocheuses de Provence a permis le comblement des rias envahies par les eaux à la suite du réchauffement postglaciaire. Les effets opposés de la sédimentation littorale et de l’érosion marine ont profondément modifié les conditions offertes à l’implantation des installations portuaires et à leur approche par les navigateurs. Depuis une trentaine d’années, les collaborations entre les géomorphologues et les archéologues <b>intervenant</b> dans des opérations d’archéologie préventives sur les littoraux ont profondément renouvelé les connaissances dans ce domaine. L’article présente des connaissances acquises sur un certain nombre de sites littoraux parmi lesquels se distinguent Antibes, Olbia, Toulon et Marseille...|$|R
40|$|MultilingualWeb Document (MWD) {{processing}} {{has become}} one of the major interests of research and development in the area of information retrieval. Therefore, we observed that the structure of the multilingual resources has not been enough explored in most of the research works in this area. We consider that links structure embed crucial information for both hyperdocument retrieving and mining process. In this context, we wonder to remind that each Web site is considered as a hyper-document that contains a set of Web documents (pages, screen, messages) which can be explored through the links paths. Therefore, detecting the dominant languages, in a Web Site, could be done in a different ways. The framework of this experimental research thesis is structures analysis for information extraction from a great number of heterogeneous structured or semi-structured electronic documents (essentially the Web document). It covers the following aspects : enumerating the dominants languages, setting-up (virtual) frontiers between those languages, enabling further processing, recognizing the dominants languages. Les ressources d'information multilingues sur le Web sont devenues de plus en plus des objets d'études importantes pour différents domaines <b>intervenant</b> au traitement de l'information. Néanmoins, nous constatons que la structure des ressources multilingues est très peu explorée par rapport à l'abondance des méthodes de traitement automatique des langues naturelles. Dans cette thèse, nous abordons l'aspect multilinguisme dans un contexte de catégorisation des sites Web multilingues. Nous apportons quelques connaissances expérimentales portant sur la représentation de documents multilingues, la modélisation des données en une structure homogène, la qualité de la recherche d'information dans un contexte multilingues et enfin sur les notions de frontière et de centre de gravité pour départager des langues dominantes sans puiser dans des connaissances linguistiques...|$|R
40|$|This {{thesis is}} a {{contribution}} to the analysis of the medico-social action as a particular type of intervention. It is based on a field work in the free consultations of “child protection” (protection infantile in French) in France, and questions the medico-social prevention while being "performed". This research also observes the gaps and tensions which exist between legislation, organization of the consultations, professional positions taken by the protagonists, and the specific issue of the public addressed in priority by these consultations. There are numerous stakes within the articulation of medical and social, as well as various medical and social elements raised in the real-life situations. The analysis of these stakes and elements reveals that prevention is crafted, invented, and necessarily redefined in daily interactions. Cette thèse est une contribution à l’analyse de l’action médico-sociale comme forme particulière d’intervention. À partir du terrain des consultations gratuites de protection infantile, il s’agit d’interroger la prévention médico-sociale « en acte » et d’observer les décalages et tensions qui existent entre législation, organisation des consultations, positionnement professionnel des <b>intervenants,</b> et problématique spécifique du public auquel ils s’adressent en priorité. L’analyse des enjeux que recouvre l’articulation médical/social et celle des éléments médicaux et sociaux effectivement mobilisés en situation révèle que la prévention se réalise, s’invente et se redéfinit nécessairement au quotidien...|$|E
40|$|L'ANALYSE D'ASSEMBLAGES TRIDIMENSIONNELS COMPLEXES SOUMIS A DES CHOCS PEUT CONDUIRE A DE SERIEUSES DIFFICULTES, NOTAMMENT LORSQU'UN COMPORTEMENT FORTEMENT NON-LINEAIRE DE LA STRUCTURE EST ENGENDRE PAR DE NOMBREUSES SURFACES (OU LE CONTACT PEUT FLUCTUER). DANS CE TYPE DE CONFIGURATION, MEME EN STATIQUE LES CODES INDUSTRIELS CONDUISENT A DES TEMPS DE CALCULS PROHIBITIFS. CE TRAVAIL PROPOSE UNE NOUVELLE APPROCHE CAPABLE DE FOURNIR LA REPONSE A UN CHOC D'UN ASSEMBLAGE OU D'UN ATTENUATEUR DE CHOC AVEC UNE BONNE QUALITE DE SOLUTION. UNE TELLE APPROCHE POURRAIT PAR EXEMPLE METTRE EN EXERGUE LES PHENOMENES MAJEURS <b>INTERVENANTS</b> LORS DE LA SEPARATION DES ETAGES D'UN LANCEUR, LORSQUE LE CHOC INITIE PAR L'EXPLOSION D'UN CORDEUR DECOUPEUR, IMPACTE LES ASSEMBLAGES UTILISES POUR FIXER LES APPAREILS ELECTRONIQUES EMBARQUES. AINSI, CETTE ETUDE FAIT PARTIE DU POLE CHOC PYROTECHNIQUE ANIME PAR LE CENTRE NATIONAL D'ETUDE SPATIALE (CNES), BASE A EVRY (FRANCE). L'APPROCHE CONCERNEE CONSTITUE LA CONTINUITE DE TRAVAUX ANTERIEURS EN STATIQUE POUR LESQUELS LES TEMPS DE CALCULS ONT PU ETRE DIVISES PAR 50 PAR RAPPORT AUX CODES INDUSTRIELS. CETTE TECHNIQUE EST BASEE A LA FOIS SUR UNE STRATEGIE ET UNE FORMULATION PARALLELE. ELLE EST L'ASSOCIATION D'UNE MESO-MODELISATION MODULAIRE DE LA STRUCTURE ET D'UN TECHNIQUE NON-INCREMENTALE DE RESOLUTION. LE CODE DE CALCUL DYGITA 3 D (DYNAMIQUE A GRAND INCREMENT DE TEMPS POUR LES ASSEMBLAGES 3 D) EST PRESENTE ET PERMET DE COMPARER ET DE VALIDER LES RESULTATS OBTENUS PAR RAPPORT AUX APPROCHES CLASSIQUES. PLUSIEURS EXEMPLES CONDUISENT AU TRAITEMENT FINAL D'UN CAS DE CALCULS CONTENANT TOUTES LES INGREDIENTS NECESSAIRES A LA REPRESENTATION DE LA TRAVERSEE D'UN ASSEMBLAGE PAR UN CHOC (CONTACT UNILATERAL, PRECHARGE DES BOULONS, PLASTICITE). SudocFranceF...|$|E
40|$|ENeRG, the European Network for Research in Geo-Energy, which groups various European {{research}} organizations {{involved in}} the development of energy resources contained in the earth's crust, has sponsored a study on The Outlookfor Oil and Gas Production in the North Sea, on behalf of the Commission of European Communities (DG XVII). The Institut français du pétrole (IFP) was the main participant in the group, which included the Centre for Marine and Petroleum Technology (CMPT, formerly PSTI, United Kingdom), Rogaland Research (RF, Norway), the Geological Survey of Denmark and Greenland (GEUS in association with the Danish Energy Agency, DEA, Denmark) and Eniricerche SpA (Italy). The main objectives of this analysis are as follows:- Identify the need for new technologies or technological improvements in order to increase the production of oil and gas in the North Sea. - Forecast recovery levels that result from the implementation of these technologies. - Estimate the impact of these forecasts on the volume of activity and employment in the European oil and gas industry. L'ENeRG (European Network for Research in Geo-Energy), qui regroupe de nombreux organismes de recherche européens impliqués dans la valorisation des ressources énergétiques contenues dans l'écorce terrestre, a suscité une étude sur les Perspectives de production de pétrole et de gaz en mer du Nord, pour le compte de la Commission des Communautés européennes (DG XVII). L'Institut français du pétrole (IFP) en a été le principal acteur; les autres <b>intervenants</b> comprenaient : le Centre for Marine and Petroleum Technology (CMPT, ex-PSTI, Royaume-Uni), Rogaland Research (RF, Norvège), Geological Survey of Denmark and Greenland (GEUS, en association avec la Danish Energy Agency, DEA, Danemark) et Eniricerche SpA (Italie). Les principaux objectifs de cette analyse* sont les suivants - identifier les besoins en technologies nouvelles ou en améliorations technologiques permettant d'accroître la production d'hydrocarbures en mer du Nord; - prévoir les niveaux de récupération résultant de la mise en Suvre de ces développements; - estimer l'impact de ces prévisions sur le volume d'activîté et l'emploi dans le secteur de l'industrie pétrolière et gazière...|$|E
40|$|LA TERMINAISON DE LA TRADUCTION INTERVIENT LORSQU'UN RIBOSOME RENCONTRE UN CODON STOP. CE DERNIER EST ALORS RECONNU PAR LE FACTEUR ERF 1 P QUI, EN INTERACTION AVEC ERF 3 P, PROVOQUE L'ARRET DE LA SYNTHESE PROTEIQUE. L'EFFICACITE DE TERMINAISON DE LA TRADUCTION EST DETERMINEE PAR LE CONTEXTE NUCLEOTIDIQUE DU CODON STOP. AINSI, LE CONTEXTE PEUT REPROGRAMMER LE CODON STOP EN PERMETTANT AUX RIBOSOMES D'INCORPORER UN ARNT AVEC UNE FORTE EFFICACITE, ON PARLE ALORS DE TRANSLECTURE. J'AI DEMONTRE QUE CHEZ S. CEREVISIAE LES SIX NUCLEOTIDES EN 3 DU CODON STOP SONT DETERMINANTS DANS L'EFFICACITE DE TRANSLECTURE, ET QUE LE MOTIF GENERAL -CA(A/G) N(T/C/G) A- PERMET D'OBTENIR LES PLUS FORTES EFFICACITES DE PASSAGE DES CODONS STOP. MON TRAVAIL MET EN EVIDENCE QUE LA TRANSLECTURE, IDENTIFIEE JUSQU'A PRESENT UNIQUEMENT DANS DES GENES VIRAUX, EST AUSSI RETROUVEE DANS DES GENES NUCLEAIRES. ELLE PERMET DE MODIFIER L'EXPRESSION DES PROTEINES CONCERNEES, EN FONCTION DES CONDITIONS ENVIRONNEMENTALES. C'EST NOTAMMENT LE CAS DU GENE PDE 2, POUR LEQUEL J'AI MONTRE QUE LA TRANSLECTURE PERMET DE CONTROLER LA STABILITE DE LA PHOSPHODIESTERASE DE L'AMPC, ET MODULE AINSI LE NIVEAU D'AMPC INTRACELLULAIRE. L'AUGMENTATION DU NIVEAU D'AMPC OBSERVEE DANS UNE SOUCHE PSI+ POURRAIT PERMETTRE D'EXPLIQUER LA GRANDE DIVERSITE DES PHENOTYPES ASSOCIES AU FACTEUR PSI+. DURANT CE TRAVAIL D'AUTRE GENES SOUMIS A UN MECANISME DE TRANSLECTURE ONT ETE IDENTIFIES : IL S'AGIT DES GENES IMP 3 ET RTG 8. FINALEMENT, LE CRIBLE D'UNE BANQUE MULTICOPIE D'ADNG M'A PERMIS D'IDENTIFIER DE NOUVEAUX CANDIDATS <b>INTERVENANT</b> DANS LA TERMINAISON DE LA TRADUCTION. CE TRAVAIL MET DONC EN EVIDENCE QUE LA TERMINAISON DE LA TRADUCTION EST UN POINT DE CONTROLE DE L'EXPRESSION DE CERTAINS GENES CHEZ S. CEREVISIAE. PARIS-BIUSJ-Thèses (751052125) / SudocCentre Technique Livre Ens. Sup. (774682301) / SudocPARIS-BIUSJ-Physique {{recherche}} (751052113) / SudocSudocFranceF...|$|R
40|$|LA CELLULE, POUR ASSURER SA SURVIE, A MIS EN PLACE DIVERS POINTS DE CONTROLE NECESSAIRES A LA BONNE ORGANISATION SPATIALE ET TEMPORELLE DES DIFFERENTES ETAPES DU CYCLE CELLULAIRE. CETTE REGULATION LUI PERMET D'ASSURER LA FIDELITE D'EVENEMENTS CRITIQUES TELS QUE LA REPLICATION DE L'ADN OU LA SEGREGATION DES CHROMOSOMES. LA MISE EN PLACE DE LA MITOSE SE CARACTERISE PAR LA CONDENSATION DES CHROMOSOMES, LA FORMATION DU KINETOCHORE ET LA SEPARATION DES CENTROSOMES. LA TOPOISOMERASE II EST IMPLIQUEE DANS LA CONDENSATION DES CHROMOSOMES ET LA FORMATION DU KINETOCHORE, CES EVENEMENTS ETANT TOUS DEUX REVERSIBLES. L'ENTAME DE LA MITOSE COINCIDE AVEC LA TRANSLOCATION NUCLEAIRE ET L'ACTIVATION DE KINASES MITOTIQUES MAJEURES, TELLES QUE LA KINASE CDC 2 ET LA FAMILLE DES KINASES MPM- 2. LA TOPOISOMERASE II EST PHOSPHORYLEE PAR LA KINASE CDC 2 ET CONSTITUE LA CIBLE MAJEURE ASSOCIEE AUX CHROMOSOMES DE L'ANTICORPS MPM- 2. AU COURS DE MON DOCTORAT, JE ME SUIS ATTACHE A MIEUX CARACTERISER LA REGULATION, AU COURS DES PHASES PRECOCES DE LA MITOSE, DE LA TOPOISOMERASE II. CE TRAVAIL AURA PERMIS DE MONTRER UNE INTERACTION DIRECTE ENTRE LA KINASE CDC 2 ET LA TOPOISOMERASE II, INTERACTION RESPONSABLE DU CIBLAGE DE L'ACTIVITE CDC 2 SUR LES CHROMOSOMES, DE LA STIMULATION DE L'ACTIVITE CATALYTIQUE DE LA TOPOISOMERASE II ET D'UN REMODELAGE SIGNIFICATIF DE LA CHROMATINE. CE TRAVAIL AURA PERMIS, AUSSI, DE MONTRER QUE LA PHOSPHORYLATION PAR LA PROTEINE KINASE CK 2 DE LA TOPOISOMERASE II HUMAINE SUR LE RESIDU 1469 INDUIT LA FORMATION DE L'EPITOPE MPM- 2. ENFIN, CE TRAVAIL AURA PERMIS DE MIEUX CARACTERISER LES PROPRIETES AUTO-ASSOCIATIVES DE LA TOPOISOMERASE II <b>INTERVENANT</b> LORS DE LA CONDENSATION DES CHROMOSOMES. L'ENSEMBLE DE CES DONNEES APPORTE UN ECLAIRAGE NOUVEAU SUR LES FONCTIONS ET REGULATIONS DE LA TOPOISOMERASE II AU COURS DES PHASES PRECOCES DE LA MITOSE. IL SEMBLE, EN EFFET, QUE SON IMPLICATION PASSE EN GRANDE PARTIE PAR DES ASSOCIATIONS PROTEINE-PROTEINE CAPABLES DE MODULER SES PROPRIETES BIOLOGIQUES ET D'ORIENTER DIVERSES ACTIVITES VERS DES COMPARTIMENTS NUCLEOPLASMIQUES BIEN DETERMINES. PARIS-BIUSJ-Thèses (751052125) / SudocCentre Technique Livre Ens. Sup. (774682301) / SudocPARIS-BIUSJ-Physique {{recherche}} (751052113) / SudocSudocFranceF...|$|R
40|$|THE DEVELOPPEMENT OF OPTICAL NETWORKS IN THE LAST DECADE HAS MADE POSSIBLE THE STRONG INCREASE IN WORLWIDE TELECOMUNICATIONS. PROCESSING OF HIGH-BITRATES SIGNALS (ABOVE 40 GB/S) AT FIBER INPUT AND OUTPUT REQUIRES NUMERICAL CIRCUITS WORKING VERY HIGH FREQUENCIES, AND THUS BASED ON VARY FAST ELECTRONICS DEVICES WITH CUTOFF FREQUENCIES OF 150 GHZ OR MORE. III-V SEMICONCTOR MATERIALS HAVE REMARKABLE PHYSICAL PROPERTIES, MAKING INP BASE HBT ONE OF THE FASTEST TRANSITOR AVAILABLE AT THIS TIME. THIS DEVICE ALLOWS DESIGN OF CIRCUITS WORKING AT THE VERY HIGH FREQUENCIES REQUIRED IN OPTICAL COMMUNICATIONS APPLICATIONS. FABRICATION OF GAINAS/INP HBT INVOLVES A LARGE NUMBER OF DESIGN AND PROCESSING STEPS (EPITAXIAL GROWTH, CLEANROOM PROCESSING, CARACTERISATION, [...] .), AND REQUIERS UNDERSTANDING OF VARIOUS PHYSICAL EFFECTS DETERMINING THE DEVICE BEHAVIOR. IN THIS WORK, WE STUDY THE MAIN PHYSICAL EFFECTS INVOLVED IN HBT BEHAVIOR, AND WE CARRY OUT OPTIMISATION OF THE DEVICE. L'ESSOR DES TÉLÉCOMMUNICATIONS À L'ÉCHELLE MONDIALE QU'A CONNU LA FIN DU XXÈME SIÈCLE A ÉTÉ RENDU POSSIBLE PAR L'EXISTENCE DE RÉSEAUX À BASE DE FIBRES OPTIQUES, CAPABLES DE TRANSMETTRE DES FLUX DE DONNÉES IMPORTANTS SUR DE LONGUES DISTANCES. LA GESTION DE CES IMPORTANTS FLUX D'INFORMATION EN AMONT ET EN AVAL DE LA FIBRE REQUIERT DES CIRCUITS ÉLECTRONIQUES NUMÉRIQUES ET ANALOGIQUES TRAITANT DES DÉBITS DE DONNÉES SUPÉRIEURS À 40 GB/S, CE QUI IMPLIQUE L'UTILISATION DE COMPOSANTS TRÈS RAPIDES, AVEC DES FRÉQUENCES DE TRANSITION AU-DELÀ DE 150 GHZ. GRÂCE AUX REMARQUABLES PROPRIÉTÉS DE LA FAMILLE DE MATÉRIAUX III-V EN TERMES DE TRANSPORT ÉLECTRIQUE, LE TRANSISTOR BIPOLAIRE À HÉTÉROJONCTION GAINAS/INP SE CLASSE COMME L'UN DES TRANSISTORS LES PLUS RAPIDES ACTUELLEMENT, ET PERMET LA RÉALISATION DE TELS CIRCUITS OPÉRATIONNELS À TRÈS HAUTE FRÉQUENCE. LA MISE EN PLACE D'UNE FILIÈRE COMPLÈTE DE FABRICATION DE CIRCUITS À BASE DE TBH NÉCESSITE QUE SOIT MAÎTRISÉ UN GRAND NOMBRE D'ÉTAPES. UN CERTAIN NOMBRE D'ENTRE ELLES TOUCHE DIRECTEMENT AU COMPOSANT: CONCEPTION, ÉPITAXIE, FABRICATION TECHNOLOGIQUE, CARACTÉRISATION [...] . A CES ÉTAPES DOIT ÊTRE AJOUTÉE UNE CERTAINE COMPRÉHENSION DES PHÉNOMÈNES PHYSIQUES <b>INTERVENANT</b> DANS LE DISPOSITIF. DANS CE TRAVAIL DE THÈSE, LES PROBLÉMATIQUES LIÉES À L'OPTIMISATION DU TBH GAINAS/INP, AINSI QUE CERTAINES QUESTIONS RELATIVES AUX MÉCANISMES PHYSIQUES MIS EN JEU SONT PRÉSENTÉES...|$|R
40|$|L'auteur cherche ici à établir un parallèle entre l'évolution du contexte affaires des organisations et le {{renouvellement}} de {{la fonction}} formation et développement de la main-d’œuvre. L'un des principaux défis des <b>intervenants</b> en ce domaine réside alors dans la capacité de ceux-ci à mobiliser des stratégies d'apprentissage qui favorisent la synergie entre les savoirs tacites et explicites facilitant ainsi la création de nouveaux savoirs collectifs qui sont à la base de l'innovation diffuse. Recent {{research on the}} new competitiveness of organizations indicates that the value added of the Human Resources function now depends on its capacity for strategie redeployment. This realignment also necessitates a restructuring of the roles played by HR professionals and {{a new set of}} skills that need to be acquired. These considerations also apply to the employee training and development function. The objective {{of this article is to}} establish a parallel between the changing business environment of organizations and the revamping of the employee training and development function. More particularly, we show that organizations are increasingly operating in the context of a knowledge-based economy in which their competitive advantage depends on their capacity to identify critical knowledge, to transform this knowledge and to integrate it into their business processes. This new competitiveness is also based on diffuse innovation, which can be equated to a process of creation of new organizational knowledge in context. From this perspective, one of the principal challenges facing those involved in employee training and development is their ability to develop learning strategies that will promote synergy between tacit and explicit knowledge, thereby facilitating the creation of the new collective knowledge upon which diffuse innovation is based. This process is based in particular on new information and communication technologies and on the work team as the site of learning. El autor busca en este documento establecer un paralelo entre la evoluciòn del contexto de los negocios de las organizaciones y la renovaciòn de la funciòn de la formaciòn y el desarrollo de la mano de obra. Uno de los principales desafìos de los intervenientes en este campo réside por lo tanto en la capacidad de estos de movilizar las estrategias de aprendizaje que favorisan la sinergia entre los conocimientos tàcitos y explicitas facilitando asì la creaciòn de nuevos conocimientos colectivos que son la base de la innovaciòn difusa...|$|E
40|$|Les systèmes sociosanitaire et de santé mentale au Québec ont été substantiellement transformés dans les dernières années. Au coeur des restructurations, les réformes ont visé la {{consolidation}} des soins primaires et une meilleure intégration du dispositif de soins, tendances centrales des réformes sur le plan international. Cet article résume les principaux axes de transformation des réformes du système sociosanitaire et de la santé mentale au Québec. Il présente aussi le rôle clé des omnipraticiens dans la prise {{en charge}} des troubles mentaux et les stratégies de coordination déployées. Les réformes visent principalement l’intensification du travail en réseau des omnipraticiens avec les <b>intervenants</b> psychosociaux et les psychiatres. L’article conclut sur l’importance d’optimiser le déploiement de réseaux intégrés de soins et de bonnes pratiques en santé mentale. Par ailleurs, les réformes devraient toujours être accompagnées de mesures et de stratégies d’implantation à juste titre aussi ambitieuses que les changements planifiés !Objectives: The {{health and mental}} health systems in Quebec have recently been substantially transformed. At the heart of this restructuring, reforms aimed to strengthen primary care and to better integrate services, which are central trends internationally. This article summarizes Quebec’s primary {{health and mental health}} reforms. It also presents the key role of general practitioners in the treatment of mental health disorders and their coordination strategies with the mental health care resources in the province. Methods: Numerous documents on the Quebec health and mental health reforms and the international literature on primary mental health care were consulted for this study. Information on general practitioner roles in mental health were based on administrative data from the Régie de l’assurance maladie du Québec (RAMQ) for all medical procedures performed in 2006. The data was compared with the results of a survey realized in the same year with 398 general practitioners in Quebec. Complementary qualitative data was collected through one hour interviews on a subsample of 60 of those general practitioners. Results: The central aim of the Quebec healthcare reform was to improve services integration by implementing local healthcare networks. A population health approach and a hierarchical service provision were promoted. For a better access and continuity of care, family medicine groups and network clinics were also developed. The mental health reform (Action Plan in Mental Health, 2005 - 2010) was launched in this general context. It prioritized the consolidation of primary care and shared-care (i. e. increased networking between general practitioners and psychosocial workers and psychiatrists) by reinforcing the role of general practitioners in mental health, developing mental health interdisciplinary teams in primary care and adding a psychiatrist-respondent function in each Quebec local healthcare network. In mental health, general practitioners played a central role as the primary source of care and networking to other resources either primary or specialized health care services. Between 20 - 25 % of visits to general practitioners are related to mental health problems. Nearly all general practitioners manage common mental disorders and believed themselves competent to do so; however, the reverse is true for the management of serious mental disorders. Mainly general practitioners practiced in silo without much relation with the mental health care resources. Numerous factors were found to influence the management of mental health problems: patients’ profiles (e. g. the complexity of mental health problems, concomitant disorders), individual characteristics of the general practitioners (e. g. informal network, training); professional culture (e. g. formal clinical mechanisms), the institutional setting (e. g. multidisciplinary or not) and organizations of services (e. g. policies). Conclusion: Unfortunately, the Quebec health and mental health care reforms have not been fully implemented yet. Family medicine groups and networks clinics, primary mental health teams and psychiatrists-respondent are not optimally operational and therefore, are not having a significant outcome. Support mechanisms to help implement the reforms were not prioritized. Hindering factors should be identified and minimized to increase positive changes in the health and mental health systems. This article concludes on the importance of implementing continuums of care, especially local healthcare networks and best practices in mental health. Furthermore, strong strategies to support the implementation of changes should always accompany sweeping reforms...|$|E
40|$|Le concept des coûts indirects des {{accidents}} du travail a de tout temps intéressé les <b>intervenants</b> en santé et sécurité au travail. Les auteurs tracent son évolution depuis la première étude d'Heinrich en 1931 jusqu'à ce jour. Ils constatent une très grande hétérogénéité dans les résultats obtenus par les études répertoriées. Ces écarts peuvent s'expliquer par des dissemblances {{au niveau}} des définitions, des méthodologies de recherche utilisées et des populations visées. Sixty years ago Heinrich discovered that employers' costs of work accidents far exceeded the amounts typically attributed to such events. It therefore became accepted practice for occupational {{health and safety}} (OHS) specialists to add to known or «direct» costs, a multiple, representing hidden, invisible or «indirect» costs which are entirely absorbed by the firm. This approach contains an implicit behavioral assumption that employer awareness of his total accident costs will increase the perceived returns on prevention activities and consequently lead to greater investment in accident reduction expenditures. The present description and analysis of the literature traces the concept of indirect costs from its origins up to the current period. The first section deals with the evolution of the notion of indirect costs. Then the economic aspects are surveyed. Finally, the definitions, methodologies and results of the empirical studies are analyzed. Work accident cost research may be divided into two types: «primary» and «secondary». In the first, original cost data on individual accidents are collected using a questionnaire distributed to employers in one or more industrial sectors. The second type measures total accident costs by applying the information (ratios) generated in primary studies to other, available data on direct or known costs. RESULTS Many primary studies generated ratios (indirect costs/direct costs) to express the magnitude of accident costs. The size of the ratios found vary from 1 : 1 to 10 : 1 and more; the most common, that of Heinrich (1931), being 4 : 1. Other authors, observing no linear relationship between indirect and direct costs, have preferred different methodologies to express the importance of indirect costs (e. g. Simonds and Grimaldi, 1956). In addition, the most recent ratio studies suggest much more moderate results than those found previously. OBSERVATIONS 1. Very few primary empirical studies have been carried out, the vast majority being of the secondary type. Of those primary surveys reported many are deficient since they reveal precious little on the population studied, methodologies, frequency tables and the actual statistical analysis of the data. These are often found as a chapter in a textbook or handbook on the management of OHS and therefore represent more of an exploratory treatment of the indirect cost concept than a rigorous, scientific procedure of systematic data-gathering, reporting and analysis. The application of their results for estimating real costs must thus be treated with a certain degree of caution. 2. The indirect cost coefficients generally in use today by OHS practitioners and by authors of secondary studies are based on research dating from as far as 1931 (Heinrich) and 1955 (Simonds). The ensuing technical and organisational changes in the workplace have rendered such results largely obsolete. 3. There are substantial variations in the definitions of direct and indirect costs as well as other aspects of research methodologies, making comparisons of the results difficult. Standardization of these aspects would permit more useful comparative studies. 4. Many studies are based on very small populations of accidents or on limited intra- or inter-sectorial representativeness. Sector-specific coefficients generated from multi-sector data would permit more accurate estimates of the extent of indirect costs since these can be expected to vary with firm size, technology, victim characteristics, production standards and organization, etc. 5. No broadly-based, scientific study of indirect work accident costs has ever been carried out in Canada. This is a serious weakness in our ability to estimate total accident costs particularly since relatively high Canadian incidence and severity rates warrant such an undertaking (see Brody, Rohan and Rompre, 1985). The present authors are currently carrying out a study of this kind on some 400 work accidents in Quebec using advanced statistical techniques such as multiple regression analysis...|$|E
40|$|International audienceBeatific {{vision and}} {{resurrection of the}} flesh: a few {{historical}} and doctrinal remarks. If eternal bliss consists in seeing God, the higher faculties of the soul should be sufficient to reach it. Under such conditions, what need is there for {{the resurrection of the}} flesh? Does not this doctrine however constitute an element of major importance in the difference between Christian hope and the philosophers'? What interpretation can then be given to Benedict XII's doctrinal statement in 1336 according to which the soul of the blessed ones which have been sufficiently purified see God directly, already now, without waiting for the resurrection and Judgement Day. We here propose elements of reflection setting beatific vision and resurrection of the flesh {{at the heart of the}} philosophical research carried out by the Christian authors of the late Antiquity and of the Middle Ages. We shall start with Augustine and his consciousness that the resurrection of the flesh distinguishes Christian hope from the philosophers'. In a second part we shall recall the stakes of the Avignon quarrel about beatific vision. Finally, from two examples appearing just after the doctrinal proclamation Benedictus Deus, we shall show the consequences it brings about concerning the anthropological reflection as regards the vision of God. Si la béatitude éternelle consiste à voir Dieu, les facultés supérieures de l'âme devraient y suffire. Quel besoin dans ces conditions de la résurrection de la chair ? Cette doctrine ne constitue-t-elle pas pourtant une composante de première importance de la différence entre l'espérance chrétienne et celle des philosophes ? Quel sens donner alors à la décision doctrinale de Benoît XII en 1336 selon laquelle les âmes des bienheureux suffisamment purifiées voient Dieu sans intermédiaire, et cela dès à présent, sans attendre la résurrection et le jugement final ? Nous proposons ici des éléments de réflexion situant vision béatifique et résurrection de la chair au cœur de la recherche philosophique menée par les auteurs chrétiens de l'Antiquité tardive et du Moyen Age. Nous partirons d'Augustin et de sa conscience que la résurrection de la chair distingue l'espérance des chrétiens de celle des philosophes. Dans un second temps nous rappellerons les enjeux de la querelle avignonnaise de la vision béatifique. Enfin, nous montrerons à partir de deux exemples <b>intervenant</b> au lendemain de la proclamation doctrinale Benedictus Deus, les conséquences qu'elle entraîne sur la réflexion anthropologique concernant la vision de Dieu...|$|R
40|$|International audienceWe {{propose to}} study speaker diarization from a {{collection}} of audio documents. The goal is to detect speakers appearing in several shows. In our approach, shows are processed independently of each other before being processed collectively, to group speakers involved in several shows. Two clustering methods are studied for the overall treatment of the collection: one uses the NCLR metric {{and the other is}} inspired by techniques based on i-vectors, used in the speaker verification field, and is expressed as an ILP problem. Both methods were evaluated on two sets of 15 shows from ESTER 2. The method based on i-vectors achieves performance slightly lower than those obtained by the NCLR method, however, the computation time is on average 17 times faster. Therefore, this method is suitable for processing large volumes of data. Nous proposons d'étudier la segmentation et le regroupement en locuteurs dans le cadre du traite-ment d'une collection de documents audio. L'objectif est de détecter les locuteurs qui apparaissent dans plusieurs émissions. Dans notre approche, les émissions sont traitées indépendamment les unes des autres avant d'être traitées globalement, afin de regrouper les locuteurs <b>intervenant</b> dans plusieurs émissions. Deux méthodes de regroupement sont étudiées pour le traitement global de la collection : l'une utilise la métrique NCLR et l'autre s'inspire des techniques à base de i-vecteurs, employées en vérification du locuteur, et est exprimé sous la forme d'un problème de PLNE. Ces deux méthodes ont été évaluées sur deux corpus de 15 émissions issues d'ESTER 2. La méthode basée sur l'utilisation des i-vecteurs réalise des performances légèrement inférieures à celles obtenues par la méthode NCLR, cependant le temps de calcul est en moyenne 17 fois plus rapide. Cette méthode est, par conséquent, adaptée au traitement de grandes quantités de données. ABSTRACT Cross-show speaker diarization We propose to study speaker diarization from {{a collection of}} audio documents. The goal is to detect speakers appearing in several shows. In our approach, shows are processed independently of each other before being processed collectively, to group speakers involved in several shows. Two clustering methods are studied for the overall treatment of the collection: one uses the NCLR metric and the other is inspired by techniques based on i-vectors, used in the speaker verification field, and is expressed as an ILP problem. Both methods were evaluated on two sets of 15 shows from ESTER 2. The method based on i-vectors achieves performance slightly lower than those obtained by the NCLR method, however, the computation time is on average 17 times faster. Therefore, this method is suitable for processing large volumes of data. MOTS-CLÉS : SRL, traitement de collection, i-vecteurs, regroupement PLNE...|$|R
40|$|Cet article est une {{contribution}} à l’étude des potentialités photosynthétiques hivernales des conifères en zone tempérée. Il met en évidence une action du régime thermique sur l’activité photosynthétique du douglas à deux niveaux d’échelle dans le temps. A l’échelle instantanée, la réponse photosynthétique à la température se caractérise par un large domaine optimal plutôt que par un optimum marqué. A l’échelle de la décade, la capacité photosynthétique oscille en permanence sous l’action d’un jeu d’endommagement - réparation de {{la fonction}} photosynthétique liée à l’alternance de périodes de gel et de dégel. A l’approche du débourrement, notre étude permet de relier les modifications <b>intervenant</b> dans les échanges gazeux de CO 2 au niveau du rameau au développement des bourgeons. In the eastern France, near Nancy, at different days from 02. 16 th. 1979 to 05. 16 th. 1979, shoots of a 19 year old Douglas-fir stand were cut and their net-photosynthesis response to temperature determined in laboratory under light-saturation conditions. 1. Until beginning of April : - The response of net-photosynthesis to temperature {{was characterized by}} a wide optimal zone centered on 16 - 18 °C rather than by a marked optimum. No significative transformation {{in the shape of}} that response curve was observed during the measurement period. - The photosynthetic capacity is submitted to important changes corresponding to a damage and reparation action on the photosynthetic function linked to the alternation of frost and thaw periods. The after-effects of the thermal regime act upon photosynthesis in a cumulative way and with a 10 day retentivity. 2. From beginning of April to bud-flushing (05. 16 th) - In a first time, a more and more important impairment of the CO 2 assimilation rate of the shoot was observed at the higher temperatures. Simultaneously the thermal optimum underwent a slight down to 8 - 10 °C, without the above mentionned climatic determinism of the photosynthetic capacity being significantly affected. That transformation appeared to be due to an increase of the respiratory activity of the developing buds. - In a second time, a few days before bud flushing, the photosynthetic capacity decreased in its turn under the influence of physiological changes which seemed to affect the photosynthetic apparatus itself. The Douglas-fir is characterized by a great capacity of reparation after a cold stress which allows it to turn to account mild temperature winter periods in a climate like that of Nancy. In the temperate zone such an evergreen conifer winter photosynthesis pattern is intermediate between the total winter photosynthetic rest which haracterizes the continental and mountain cold winter situations and a pattern without any winter depression as has been observed in mild winter oceanic conditions...|$|R
40|$|Pragmatic {{translation}} is thought {{as the area}} of specialized translators working for clients outside of the publishing industry, which is deemed to contract literary translators. This is true to some extend. However a large sector of the publishing industry is not concerned with literature but with pragmatic books dealing {{with all sorts of}} topics. Many pragmatic translators working for the publishing industry are in fact specialized in translating pragmatic books, not texts. They have to understand the way pragmatic books, made of visual and linguistic messages, convey information to their readers to translate them. This aspect of {{translation is}} little known and, outside the study of translation for advertising, research investigating the interaction of texts and iconography is scarce. There is even less on translator training. This thesis endeavours to contribute the observations of a professional translator turned translator trainer. It begins by describing the layout of pragmatic books and showing how the translator must take in the text and the iconography {{to make sense of the}} message. The double page is a visual unit splitting the information between text and images. As a result, a translation unit is a mix of texts and images. It follows that translators have to approach the translation of books as a multisemiotic activity. Therefore, when translating pragmatic books, translators have to consider them as signifying space in which textual units are no longer the only source of information. The core of the specialisation of pragmatic translators working in the publishing industry is a profound understanding of how books communicate meaning. Publishers expect both authors and translators to be able to write following style specifications for a given book series. Tapuscrits are proofread and sometimes partially rewritten to be put in agreement with the social communicative norms. At the end of the process leading to the publication of a book, the published text is a collective product and no longer the text originally provided by its “author”. To translate in this context, trainees must learn how to work in a team, albeit working from home. Translation training, at this point, aims at turning students into independent professional able to rewrite and adapt texts, taking into account the visual around, to accommodate readers’ expectations as defined by publishers. As they strive to improve their translation techniques, soon to be translators also need to learn to become cultural mediator and to criticize the books they are translating so as to improve them if need be. Invisible to readers, their contribution to the making of a book, appears in the writing of the translation and in comments on the book itself; and it is very visible to publishers they work for. La traduction pragmatique n’est pas le domaine réservé des traducteurs travaillant dans les secteurs économiques en dehors de l’édition. De nombreux traducteurs d’édition sont aussi des traducteurs pragmatiques. Dans ce domaine, leur spécialisation ne se confond pas avec le domaine dans lequel ils travaillent. Leur métier est méconnu et il n’existe aucune formation pour préparer les aspirants traducteurs à cette spécialisation. Cette recherche s’efforce de combler cette lacune. Elle commence par décrire les ouvrages pragmatiques afin de montrer que dans ce secteur, la réflexion traductive porte sur le texte dans sa mise en page. L’unité de traduction s’hybride, puisqu’elle est composée de rubriques textuelles aux fonctions communicatives précises et d’éléments visuels. Il s’ensuit que la réflexion traductive demande une approche multisémiotique qui s’appuie sur une connaissance approfondie du livre pris comme un espace signifiant dont le texte n’est qu’une composante parmi d’autres. La connaissance du livre est au cœur de la spécialisation des traducteurs pragmatiques actifs dans l’édition. Le milieu de l’édition attend des auteurs et traducteurs un travail de rédacteurs. Les tapuscrits fournis sont relus et corrigés, voire partiellement réécrits pour les améliorer, par les correcteurs. Il n’y a plus un auteur mais une fonction auteur qui réunit les différents <b>intervenants</b> de la chaîne du livre. L’apprentissage du métier doit donc comporter une part de socialisation seconde, intégrée aux exercices de traduction proposés, pour préparer les aspirants traducteurs à s’insérer dans cette équipe. Tout en se perfectionnant en traduction, les jeunes traducteurs doivent se muer en collaborateurs fiables capables de prendre part au processus de fabrication du livre en agissant en tant que médiateur culturel. Leur action s’exprime principalement par l‘écriture de la traduction mais aussi par leurs commentaires sur la fabrication du livre...|$|E
40|$|Après avoir décrit le contexte dans lequel évoluent les organisations contemporaines, les auteurs cherchent à en dégager l'impact sur le système canadien de {{relations}} industrielles, plus particulièrement la gestion des ressources humaines et les relations du travail. Since {{the beginning}} of the present decade, the management of modem organisations has gone through important changes in order to cope with a more and more turbulent environment. For some observers of the management scène, these changes are looked upon as "fads" and, for this reason, they are going to make room to new management Systems whose titles are eye-catching. For other observers, the new parameters are the product of ever-going adjustments to a threatening environment. Within this framework, we are going to analyse how these new parameters affect industrial relations Systems. The changing environmentEnvironmental changes which create pressures on organisations have already been dealt with by writers on organisations. Four dimensions, at the economy level, deserve to be singled out: industrial structures, conjunctures, market segmentation (product differentiation) and international competition. Technological changes and cultural values shared by people working in organisations are equally important as contextual dimensions. New organisational parametersFour parameters have been identified here as being types of adjustments to a turbulent environment. Toward a more strategic view of human resources managementHuman resource function within organisations is marked out for a strategic role, where as, in a recent past, it was more characterized by a reactive stand. This means a leading place for HR function in the corporate strategy formulation and strategic decisions bearing on the future of organisations are influenced a priori by HR considerations before actions are taken. HR departments and managers are then in a position to build a kind of organisational capability which is going to yield a competitive advantage, as technology, capital or marketing does. Recent literature points out that HRM must look at the link between HR strategy and its components in terms of manpower procurement, compensation, labor relations, etc. HR strategy must also match with corporate strategy, this being a requirement for HR optimal contribution to organisational efficiency. The search for total quality"Total quality" concept is not only a better substitute for "productivity", but it is taking a forefront place in management thinking and jargon. Productivity improvement, then, is achieved through total quality on the premise that non-quality is getting more expensive for organisations and society as a whole. In 1986, nonquality in Quebec amounts to 15 billions, that is to say, between three to five billions in the secondary sector and nine to ten billions in the commercial sector. It is said that, in America, one employee out of four does not produce anything, being busy to correct errors of others {Business Week, June 8, 1987). Strategies in the area of total quality can be classified in the following manner: - Actions bearing on customers (innovation and intrapreneurship). - Employee participation through parallel structures (quality circles, discussion groups, profit and stock-option sharing plans) or through structures integrated to the existing hierarchies taking the form of self-managed work groups, work units, and other applications of socio-tech concepts. This latter category of experimentations becomes more and more popular for the following reasons:The influence of technology being more felt at the level of work structuring or re-structuring, giving thus to the <b>intervenants</b> more room for socio-tech choices. The coming-up and so-called "ecologic plants" which tend to be the new way of building organisations of the future. - The new way of producing goods and services through concepts such as "just in time", Kanban, production "in pull" instead of "in push". Toward a more personnalized HR managementWe recognize that HR management must be more personnalized in order to match the growing importance of individualism in oui society. "People must be accountable for people" is the idea behind organisational climate survey, career planning and development, employee-assistance programmes. Toward more flexibilityThis concept, not yet well defined, is borrowed partly from European management literature and draws the attention of scholars as well as practitioners of HR management. Under this label, we identify at least four types of flexibility which pertain to the structuring of organisations, the structuring of work, compensation and employment flexibility. These cultural, economie and technological changes and the necessary adjustments made by business enterprises to cope with these changes and to remain competitive are an invitation to question the dominant model of industrial relations. By doing so, we join the cohort of scholars and practitioners searching for alternatives of new paradigms. As a matter of fact, the major impact of these changes can be translated in the emergence of an "alternative industrial relations System". The attached synopsis is a paired list of the charactenstics of both Systems: the existing and dominant one as compared to an emergent and alternative System. The comerstones of that alternative System are the "person at work" and the work group instead of being exclusively based on working conditions and work position. Then, {{it goes without saying that}} a multitude of questions can be raised as to the nature and the chances of survival in the future. As far as its premises are concerned, it seems obvious that the emergence of such System involves major changes at the level of corporate cultures and management philosophies: from a culture strongly oriented toward stability to a culture oriented toward flexibility. As to its nature, such alternative System is based on an idea of collaboration different from the one described in books such as In Search for Excellence, which exemplified behaviors creating an over emotional investment in the life of the organisation at the risk for the employees of loosing their personal identity. Instead, it would be more convenient to qualify the type of collaboration which is referred to in this alternative System as being a kind of "enlisted collaboration" instead of a "spontaneous one". The survival of an alternative System is far from being ascertained. Some contenders would hold that it is a magie way of getting out of the present economic crisis. Supporters would tend to say that the alternative System has reached a point of no return, on the basis that through the years participation is making its way. As a matter of fact, the various applications of socio-tech principles since {{the beginning of}} the sixties reveal that innovation in work organisation is possibly a deterrent to taylorism. However, we should take into account that the <b>intervenants</b> in this industrial relations System do not move at the same pace. For the time being, employers have taken the pole playing a leadership role in agreement with a few important unions within well-identified sectors of the economy. Top-level trade union leaders would hesitate to embark in the collaboration scheme, leaving to affiliates the decision of doing so, whereas State would hold an obvious laisser-faire position. Desde el principio de esta decada, la administracion de las organizaciones modernas ha pasado por cambios importantes para poder adaptarse a un ambiente cada vez mas turbulento. Para algunos observadores del escenario administrative estos cambios son observados como "tendencias de moda" y por esta razon, dan lugar a nuevos sistemas de administracion cuyos titulos son atractivos. Para otros observadores, los nuevos parametros son el producto de los ajustes continuos a un sistema siempre peligroso. Dentro de este marco, analizaremos como estos nuevos parametros afectan los sistemas de las relaciones industriales. El ambiente cambianteCambios ambientales que crean presiones en las organizaciones han sido ya tratados por escritores u organizaciones. En el nivel economico, cuatro dimensiones mecen ser mencionadas: estructuras industriales, coyunturas, segmentacion del mercado (diferenciacion del producto) y competencia internacional. Cambios tecnologicos y valores culturales compartidos por los trabajadores en las organizaciones, son igualmente importantes como dimensiones contextuales. Nuevos parametros organizativosCuatre paràmetros han sido identificados aqui como tipos de ajustes a un ambiente turbulento. Hacia una vision mis estrategica de la administraciàn de los recursos humanos La funcion recursos humanos dentro de las organizaciones a sido marcada para desempenar un papel estrategico, mientras que, en el pasado reciente, estaba mas caracterizada por una actitud reaccionaria. Esto significa que la funcion recursos humanos se convierte en lider en la formulation de estrategias corporativas, y decisiones estrategicas que afectan el futuro de las organizaciones, son influenciadas a priori por las consideraciones de los recursos humanos. Los departamentos de recursos humanos y administrativos, estan entonces en posicion de crear una capacidad organizativa, que dara una ventaja competitiva como la tecnologia, el capital o los mercados la dan. La literatura reciente, indica que la administracion de los recursos humanos debe ver la union entre la estrategia de recursos humanos y sus componentes, en terminos de la procuracion de la mano de obra, compensacion, relaciones laborales, etc. La estrategia de recursos humanos debe tambien armonizar con la estrategia corporativa, como un requerimiento para que los recursos humanos puedan ofrecer su optima contribucien a la eficacia organizativa. La busqueda de la calidad totalEl concepto "calidad total" no es solo un mejor substituto para "productividad", sino que ocupa un lugar privilegiado en el pensamiento y el lenguaje de la administracion. La lucha reciente por "el mejoramiento de la productividad por la calidad", esta sustentada en la premisa de que la no-calidad cuesta excesivamente caro a las organizaciones y la sociedad en gênerai. En 1986, la falta de calidad en Quebec costo 15 billones de dolares, es decir, entre très y cinco billones en el sector secundario y de nueve a diez billones en el sector comercial. Se ha dicho que, en America, un empleado de cuatro no produce nada, ocupandose de corregir los errores de otros (Business Week, 8 de junio, 1987). Las estrategias en el area de calidad total pueden ser clasificadas de la siguiente manera:- Acciones que afectan al consumidor (inovaciones e intrapreneurship) - La participacion del empleado a traves de estructuras paralelas (circulos de calidad, discusiones de grupo, planes de beneficio en las ganancias u opciones en las acciones de la empresa) o a traves de estructuras integradas a las jerarquias existentes, tomando la forma de grupos de trabajo controlados por los trabajadores, unidades de trabajo, y otras aplicaciones de los conceptos socio-tecnologicos. Esta ultima categoria es cada vez mas popular por las siguientes razones:. La influencia de la tecnologia al nivel de la estructuracion o re-estructuracion del trabajo, da a los participantes mas espacio para decidir entre las opciones socio-tecnologicas [...] El surgimiento de los llamados "planes ecologicos", que tienden a ser el nuevo estilo de creacion para las organizaciones en el futuro. - El nuevo estilo de producir productos y servicios a traves de conceptos como ("Just in time"), Kanban, produccion ("in pull") en lugar de ("in push"). Hacia una administracion mas personalizada de los recursos humanos Reconocemos que la administracion de los recursos humanos, debe ser personalizada para armonizar con la creciente importancia del individualismo en nuestra sociedad. "Gente debe de administrar gente" es la idea atras de los programas de: la encuesta del clima organizativo, planeacion de las carreras y su desarrollo, y asistencia laboral. Hacia mas flexibilidadEste concepto, aun no bien definido, nacio en parte de la literatura administrativa europea llamando la atencion de los estudiosos, asf como de los practicantes de la administracion de los recursos humanos. Bajo este concepto, identificamos al menos cuatro tipos de flexibilidad: la flexibilidad en la estructura de la empresa, en la organizacion, en la remuneracion y en el empleo. El Impacto en las relaciones industrialesEstos cambios culturales, economicos y tecnologicos y los ajustes necesarios hechos por las compariias para manejarlos y mantenerse competitivas son una invitacion para cuestionar el modelo dominante de las relaciones industriales. Con esto, nos unimos al grupo de estudiosos y practicantes buscando altemativas o nuevos paradigmas. Es un hecho que el mayor efecto de estos cambios puede ser traducido del surgimiento de un "sistema alternativo de relaciones industriales". La siguiente sinopsis es una lista de las caracteristicas de los dos sistemas: el existente y dominate, comparado con el emergente y alternativo. Las bases del sistema alternativo son la "persona en el trabajo" y el grupo de trabajo en lugar de estai basado solamente en las condiciones de trabajo y la posicion de trabajo. Por lo tanto, no es necesario mencionar que una multitud de preguntas pueden ser presentadas sobre su naturaleza y sus posibilidades de sobrevivencia en el futuro. En cuanto a las premisas, parace obvio que el surgimiento de tal sistema, comprende cambios mayores a nivel de culturas y filosofias corporativas: de una cultura fuertemente orientada hacia la estabilidad, a una cultura fuertemente orientada hacia la flexibilidad. En cuanto a la naturaleza de este sistema alternativo. podemos afirmar que debe basarse en la "colaboracion negociada", integrando los sindicatos al proceso en lugar de evitarlos o ignorarlos. Debemos omitir ciertos comportamientos de empresas "Premios de Excelencia", que originan en empleados muy identificados a la organizacion, la perdida de su identidad personal. La sobrevivencia de este sistema alternativo aun esta muy lejos de estar asegurada. Algunos de los contrincantes afirman que es una manera de escapar de la crisis economica presente. Quienes lo apoyan tenderan a decir que este sistema alternativo ha llegado a un punto del cual no hay retorno en base a que la participacion se ha ido incrementando a traves de los anos. Es mas, varias aplicaciones de los principios socio-technologicos desde el comienzo de los sesentas, nos dejà ver que la inovacion en las organizaciones de trabajo es posiblemente una proteccion contra el Taylorisme Sin embargo, debemos tomar en cuenta que los interventores en este sistema de relaciones industriales no se mueven a la misma velocidad. Por el momento, los empleadores han tomado el liderato con la ayuda de algunos sindicatos importantes dentro de sectores de la economia bien determinados. Las grandes centrales sindicales no dudan de tomar posiciones consistantes y el Estado, sea legislador o empleador,ha adoptado una posicion de laisser-faire...|$|E
40|$|This report {{analyzes}} {{and comments}} on the principal arguments put forward by the Crawford Panel to support {{the establishment of a}} single securities commission in Canada. One argument advanced is that the current rules-based regulatory structure should be replaced with a principles-based approach {{similar to that of the}} United Kingdom’s Alternative Investment Market (AIM). According to the Panel, this approach would allow for a relaxation of the conditions for corporate financing. We will point out the very distinctive characteristics of the Canadian market, which allows emerging companies—those without income and even without any revenue—to carry out initial and subsequent rounds of financing. Our estimates indicate that such financing is carried out at an advantageous cost, and the survival of new issuers seems more certain in Canada than in other countries where the rules for listing on a stock exchange are more restrictive. We believe it would be difficult to further relax the rules of a market in which 45 % of issuers are able to list their securities on a stock exchange without reporting any revenue and in which 71 % of new exchange registrants do not earn any income. This situation is unparalleled in the world. We will show that adopting a system similar to the AIM model would result in a significant percentage of existing issuers no longer being able to access the market. We therefore concur fully with the opinion expressed by one of the experts enlisted by the Panel, namely, that adopting a system similar to the AIM model in Canada is neither feasible nor desirable. The Panel expressed concern about the conditions for the financing of junior issuers. We will show that, in general, the direct costs of such financings are lower in Canada than in the United States. We will see that, in fact, there is a very high number of small offerings and issues in Canada. The Canadian markets seem to have developed strategies that are well suited to the characteristics of an economy heavily dependent on small-cap companies and on the resource sector. An analysis of all financings, including traditional and non-traditional stock exchange listings as well as subsequent financings (a total of more than 10, 000 transactions), clearly shows that financings are very small and are carried out locally and, in 77 % of cases, by issuers from outside Ontario. The Panel also expressed concern about the level of competitiveness of the Canadian market; this is a concern that we share. We will show that the principal challenge faced by the Canadian market is the gradual shift of transactions involving cross-listed securities to the U. S. market. In contrast to the Panel, we do not believe that AIM listings constitute a major problem. When Canadian companies cross-list their securities, they opt for the U. S. market at a ratio of eight to one. The migration of companies and transactions towards the U. S. market has many causes, but it would be very difficult to argue that the regulatory structure is a key factor. The argument whereby the costs of capital are lower in the U. S. does not stand up to analysis. Several recent and thorough studies indicate that the difference in these costs between the two countries is minimal, leaning in favour of one country or the other depending on the study. Our own findings show that Canadian companies that cross-list their securities do not benefit from any lower costs. The decision to list securities on a foreign market is driven primarily by strategic business factors and by the search for large pools of investors. In that regard, Canada has no advantage, and it seems unlikely that regulatory changes will convert Canada into a significant source of financing for foreign companies. In our opinion, efforts should be focused, above all, on improving and sustaining the financing options available to Canadian issuers. The Panel has argued that establishing a single commission is necessary for improving enforcement of securities laws in Canada. In this regard, Canada is often compared to the United States. An analysis of data on sanctions shows, firstly, that the SEC is far from being the source of the majority of sanctions imposed on financial market participants. It initiates less than 10 % of proceedings involving financial matters and imposes less than one quarter of all monetary sanctions. Secondly, there has been an increase in sanctions imposed in Canada in this area. Thirdly, there are major differences between Canada and other countries. This explains the differences observed and perceived as regards enforcement. The experts enlisted by the Panel have, in fact, recommended a series of eight actions and have suggested, in the eighth item, pan-Canadian enforcement of the law. Consequently, these experts have not concluded that centralization of the securities commissions is an indispensable condition for enhancing the enforcement of securities laws. The issue of costs arises very often in discussions regarding the Canadian regulatory system. Yet, there is little evidence showing that the current regulatory structure leads to significant costs for investors or issuers. The costs of the regulatory authorities represent a negligible percentage of the transaction costs borne by investors and of revenues from brokerage activities in Canada. The direct costs of regulatory authorities are lower than those incurred in other countries, when expressed on the basis of the number of reporting issuers. Finally, arguments to the effect that a single commission would generate substantial savings are less than convincing. Such savings would be possible only if the activities of securities commissions outside Ontario were virtually abolished. Three elements appear from our analysis. First and foremost, the principal arguments put forward by the Panel to justify the urgency of centralizing securities commissions in Canada do not stand up to analysis and are, at times, contradicted by the research and the experts mandated by the Panel itself. Secondly, the major challenge faced by the Canadian market—the shift of enterprises and transactions to the U. S. —does not seem to have been perceived as such or even discussed. Finally, we believe it is essential to recognize and preserve the distinctive characteristics of the existing market. It is a market that welcomes growth companies and small-cap companies, is highly decentralized and is apparently very favourable to issuers. Ce rapport analyse et commente les principaux arguments avancés par le Comité Crawford pour justifier l’instauration d’une commission des valeurs mobilières unique au Canada. Un premier argument défend que la structure actuelle de réglementation, basée sur des règles, devrait être remplacée par une approche basée sur des principes, inspirée de l’expérience de l’Alternative Investment Market (AIM) au Royaume-Uni. Selon le Comité, cette approche permettrait d’assouplir les conditions de financement des sociétés. Nous montrons les caractéristiques très particulières du marché canadien, qui permet les financements initiaux et subséquents de sociétés en développement, sans bénéfice et même sans revenu. Nos estimations indiquent que ce financement se fait à un coût avantageux, et la survie des nouveaux émetteurs semble plus assurée au Canada que dans d’autres pays où les règles d’admission en Bourse sont plus restrictives. Nous considérons qu’il semble difficile d’assouplir encore les règles d’un marché où 45 % des émetteurs s’inscrivent en Bourse sans rapporter de revenu et où 71 % des nouveaux inscrits en Bourse ne rapportent pas de bénéfice. Cette situation n’a pas d’équivalent au monde. Nous montrons que l’imposition d’un système semblable à celui de l’AIM ferait qu’une proportion importante des émetteurs actuels ne pourrait plus accéder au marché. Nous rejoignons donc totalement l’avis de l’un des experts engagés par le Comité, qui indique que l’imposition d’un système semblable à celui de l’AIM au Canada n’est ni possible, ni souhaitable. Le Comité est préoccupé par les conditions de financement des émetteurs de petite taille. Nous montrons que les coûts directs de ces financements sont, de façon générale, moins élevés au Canada qu’ils ne le sont aux États-Unis. Nous indiquons par ailleurs la fréquence très élevée des émissions et placements de petite taille au Canada. Les marchés canadiens semblent avoir développé des stratégies bien adaptées aux spécificités d’une économie qui repose fortement sur les entreprises faiblement capitalisées et sur le secteur des ressources. Lorsqu’on analyse l’ensemble des opérations de financement, incluant les entrées en Bourse conventionnelles ou non ainsi que les financements subséquents (soit plus de 10 000 opérations), il apparaît nettement que les financements sont de très petite taille, effectués localement et, à 77 %, levés par des émetteurs non ontariens. Le Comité est soucieux du niveau de compétitivité du marché canadien et nous partageons ses inquiétudes. Notamment, nous avons montré que le principal défi auquel fait face ce marché provient du glissement progressif des transactions portant sur les titres interlistés vers le marché américain. Contrairement au Comité, nous défendons que les inscriptions sur l’AIM ne constituent pas un problème majeur. Lorsqu’elles s’interlistent, les sociétés canadiennes optent pour le marché américain dans une proportion de huit à une. La migration des sociétés et des transactions vers le marché américain a des causes multiples, mais il semble très difficile de prétendre que la structure des organismes de réglementation y soit un facteur déterminant. L’argument voulant que le coût du capital soit inférieur aux États-Unis ne résiste pas à l’analyse. Plusieurs études récentes et rigoureuses indiquent que la différence, à ce niveau, est infime entre les deux pays, et le sens de la différence varie suivant les travaux. Nos propres résultats indiquent que les sociétés canadiennes qui s’interlistent ne bénéficient d’aucune diminution de ce coût. Les décisions de s’inscrire sur un marché étranger sont essentiellement liées aux considérations stratégiques des entreprises et à la recherche de bassins importants d’investisseurs. Sur ce plan, le Canada ne bénéficie d’aucun avantage et il semble peu vraisemblable que des changements réglementaires puissent faire du Canada une base importante de financement de sociétés non canadiennes. Il nous semble que les efforts devraient être, avant tout, consacrés à améliorer et pérenniser les possibilités de financement des émetteurs canadiens. Le Comité défend que l’instauration d’une commission unique est nécessaire pour améliorer l’application de la Loi au Canada. Sur ce plan le Canada est souvent comparé aux États-Unis. L’analyse des données sur les sanctions indique premièrement que la SEC n’est pas, et de loin, à l’origine de la majorité des sanctions obtenues contre des <b>intervenants</b> du marché financier. Elle est à l’origine de moins de 10 % des poursuites en matière financière et impose moins du quart du total des sanctions monétaires. Deuxièmement, nous observons une augmentation des sanctions imposées au Canada en ce domaine. Troisièmement, il existe d’importantes différences entre le Canada et les autres pays, qui expliquent les écarts observés et perçus dans la rigueur de l’application de la Loi. Les experts engagés par le Comité recommandent d’ailleurs un ensemble de huit actions différentes et suggèrent, au huitième point, une application pancanadienne de la Loi. Ces experts ne concluent donc pas que la centralisation des commissions des valeurs mobilières soit la condition indispensable à un renforcement de l’application de la Loi. La problématique des coûts est très souvent mentionnée dans le débat entourant la réglementation canadienne. Il existe pourtant peu d’évidence à l’effet que la structure de réglementation actuelle induit des coûts significatifs pour les investisseurs et même pour les émetteurs. Les coûts des organismes de réglementation représentent une proportion infime des coûts de transactions supportés par les investisseurs ou encore des revenus de l’activité du courtage au Canada. Les coûts directs provoqués par les organismes de réglementation sont inférieurs à ceux encourus dans les autres pays, dès qu’on les exprime sur la base du nombre d’émetteurs assujettis. Enfin, les évidences à l’effet qu’une commission unique permettrait des économies importantes semblent peu convaincantes. Ces économies ne sont possibles que si l’activité des commissions des valeurs mobilières en dehors de l’Ontario est pratiquement abolie. Trois éléments découlent de notre analyse. En premier lieu, les principaux éléments avancés par le Comité pour justifier l’urgence de centraliser les commissions des valeurs mobilières au Canada résistent mal à l’analyse et sont parfois contredits par la recherche ou par les experts mandatés par ce même Comité. En second lieu, le défi majeur auquel semble confronté le marché canadien ne semble ni être perçu comme tel, ni être traité : il s’agit du glissement des entreprises et des transactions vers les États-Unis. Enfin, il nous semble essentiel de reconnaître et préserver les caractéristiques particulières du marché actuel, largement ouvert aux sociétés en croissance et de petite taille, très décentralisé et apparemment très favorable aux émetteurs. ...|$|E
40|$|The aim of {{this study}} was to {{discover}} whether the working of a micro-organization such as a teaching team in physical education (P. E.) can explain teaching content in the work of each of its members. In order to make the observed contents comparable, we have chosen those whose objectives were to socialize or to train for citizenship in teams of P. E. teachers who had included these themes for P. E. pedagogical project. The theoretical model which we use combines the action logic model of Amblard et al. (1996) and that of Dubet (1994). The former makes it possible to pinpoint the various factors which intervene in the constitution of an action logic, and the latter enable us to distinguish between distinctive, integrating and strategic logics. Our hypothesis is that the dominant action logic in a P. E. team is an integrating one, and that this therefore produces what we have called "physical education team effect", that is an intentional similarity in the majority of the teaching contents chosen by the members of the team. The results show that, in the two junior secondary schools which were studied, there is a dominant logic of the integrating type and which effectively results in a "team effects" in terms of teaching content. However, this team effect is greater in the first school where the integrating orientation has been reinforced, whereas it il less marked in the other school where the teaching team has added a strong strategic orientation (taking into account personal interest) to the initial integrating logic. L'objectif de cette étude est de voir si le fonctionnement de la micro-organisation constituée par une équipe pédagogique d'EPS peut expliquer les contenus d'enseignement de chacun de ses membres. Pour rendre comparables les contenus observés, on étudie ceux qui ont pour objectifs de socialiser et former à la citoyenneté, dans les équipes pédagogiques qui ont inscrit ces thèmes dans leur projet d'EPS. Le modèle théorique retenu associe le modèle des logiques d'action d'Amblard et al. (1996) et celui de Dubet (1994). Le premier permet de cerner les différents facteurs <b>intervenant</b> dans la constitution des logiques d'action, le second permet leur caractérisation en logiques distinctive, intégratrice et stratégique. L'hypothèse est que la logique d'action dominante dans une équipe d'EPS est intégratrice et produit, de ce fait, ce qu'on a appelé un "effet équipe d'EPS", c'est-à-dire une parenté majoritaire et intentionnelle des contenus d'enseignement choisis par les membres de l'équipe. Les résultats font apparaître que, dans les deux collèges étudiés, il existe une logique dominante de nature intégratrice qui aboutit effectivement à un "effet équipe" au niveau des contenus. Mais cet effet équipe est maximum dans le premier collège où l'orientation intégratrice est renforcée, alors qu'il est plus restreint dans le deuxième collège où se superpose à la logique intégratrice primitive une orientation stratégique (c'est-à-dire d'intérêt personnel) importante de la part de l'équipe...|$|R
40|$|SINEs are mobile DNA {{elements}} {{found in}} almost all eukaryotes. Study of SINEs first focused on their retroposition mechanism, mutagenic effect and general impact on the structure and evolution of genomes. However SINE RNAs have recently been proposed to act as cellular riboregulators. Studying cis and trans elements {{taking part in the}} metabolism of SINE RNA in the model plant Arabidopsis thaliana, we aim to better understand SINE element biology. First, we experimentally defined the secondary structure of two tRNA-related SINE RNA: SB 1 from Brassica napus and SB 2 from Arabidopsis. Although unrelated at the primary sequence level, we found that these RNAs present similar secondary structures. Following this observation, an in silico analysis including tRNA-related SINE RNAs from various eukaryotes has been performed by FJ Sun, G Caetano-Anollés and JM Deragon. This study underlines the existence of common evolutionary trends for SINE RNA secondary structure that could be linked with the SINE RNA riboregulators function. Searching for trans factors involved in SINE RNA metabolism, we have chosen to characterise the La protein, an ubiquitous RNA-binding protein involved in the metabolism of various RNAs, from non-coding RNA to cellular or viral mRNA. Unlike other eukaryotes, which have only one La protein, we identified in Arabidopsis two proteins with the phylogenetic and structural characteristics of genuine La protein: At 32 and At 79. We showed that At 32 (renamed AtLa 1) is able to fulfil La nuclear functions in non-coding RNA maturation, including SINE RNA. We also demonstrate that loss of AtLa 1 function leads to embryonic lethality. Although AtLa 1 and At 79 have the same nuclear localisation, loss of At 79 function did not affect viability. AtLa 1 and At 79 have differing levels and profiles of expression. Furthermore, the AtLa 1 and At 79 proteins apparently bind distinct sets of RNA. We thus propose that Arabidopsis possess two functional homologues of the La protein, which have partially specialised to fulfil different aspects of the La function. L'étude des éléments cis et trans <b>intervenant</b> dans le métabolisme des ARN SINE a pour but de mieux appréhender la biologie des éléments mobiles SINE. Nous avons d'une part déterminé la structure secondaire des ARN des SINE de plante SB 1 et SB 2. Ces deux ARN qui n'ont pas d'homologie de séquence adoptent des structures secondaires similaires. Suite à cette observation, une étude menée par F. J. Sun et al. a mis en évidence un schéma évolutif commun des ARN SINE dérivés d'ARNt. Dans le cadre de la recherche de facteurs trans, nous avons entrepris la caractérisation de la protéine La, un facteur de liaison à l'ARN. Nous avons identifié chez Arabidopsis deux protéines présentant toutes les caractéristiques de la protéine La : At 32 et At 79. Nous avons montré que seule At 32 assure les fonctions nucléaires de la protéine La. At 32 et At 79 ont des profils d'expression différents et semblent lier des ARN distincts. Nous proposons donc qu'il existe deux homologues de la protéine La Chez Arabidopsi...|$|R
40|$|International audienceLife cycle {{assessment}} is actually used {{to quantify the}} environmental footprint of products. This approach {{can be applied to}} industrial processes or services, due to the advantage that it offers the possibility to integrate in the modeling all the parameters related to their entire lifecycle. The present study is dedicated to the presentation of a methodology allowing to realise a comparative life cycle assessment of several materials used to build exterior walls. The main objective is to compare several materials necessary to its construction, in order to use their environmental footprint as main choice criterion. The comparison is done by studying several constitutive elements of the wall, each component having a distinctive function (e. g. : thermal or acoustic insulation, fire resistance). This facilitates, beside the decisions taking, in optimizing the materials flows, their life duration or the durability of such a structure. Are considered in the study the components classically presented in an exterior wall : the external cladding, the water resistant membrane and sheating, the framing (main structure), the insulation, the vapor barrier and the interior finishing. The obtained results show on one part which are the more "impacting" materials {{and on the other side}} which is the lifecycle step having a major contribution on these impacts. It is then easy to observe that the impacts diminution in the end of life can increase them during the manufacturing step. Actuellement, l'analyse du cycle de vie (ACV) est un outil largement employé pour quantifier les conséquences environnementales des produits. Cette méthodologie s'applique aux procédés industriels et aux services, grâce à la possibilité d'intégrer la majorité des paramètres <b>intervenant</b> sur l'ensemble des étapes de leur cycle de vie. La présente étude est dédiée à la présentation d'une méthodologie permettant de mener à bien une analyse comparative des divers matériaux entrant dans la composition d'un élément de construction de type mur extérieur. Le but est de comparer différents matériaux nécessaires à la construction du mur pour utiliser comme critère de choix leur empreinte environnementale. Cette comparaison est faite sur les différents éléments constitutifs du mur, dont chacun a une fonction distincte (ex. : isolation thermique ou acoustique, résistance au feu, etc.). Ceci peut faciliter, outre la prise de décisions en termes de choix de matériaux de construction, l'optimisation des flux de matières, de leur durée de vie, ainsi que la durabilité d'une telle structure. On considère dans l'étude les éléments classiquement présents dans un mur extérieur : le bardage, le pare-pluie et la structure, l'ossature, l'isolation, le pare-vapeur et la finition intérieure. Les résultats obtenus montrent d'une part quels sont les matériaux les plus « impactants » et d'autre part quelle est l'étape de leur cycle de vie majoritairement contributrice sur ces impacts. On peut ainsi observer que la diminution des impacts en fin de vie peut engendrer leur augmentation au niveau de la fabrication. Mots clefs : analyse du cycle de vie (ACV), éco-conception, développement durable, matériaux de construction Abstract : Life cycle {{assessment is}} actually used to quantify the environmental footprint of products. This approach can be applied to industrial processes or services, due to the advantage that it offers the possibility to integrate in the modeling all the parameters related to their entire lifecycle. The present study is dedicated to the presentation of a methodology allowing to realise a comparative life cycle assessment of several materials used to build exterior walls. The main objective is to compare several materials necessary to its construction, in order to use their environmental footprint as main choice criterion. The comparison is done by studying several constitutive elements of the wall, each component having a distinctive function (e. g. : thermal or acoustic insulation, fire resistance). This facilitates, beside the decisions taking, in optimizing the materials flows, their life duration or the durability of such a structure. Are considered in the study the components classically presented in an exterior wall : the external cladding, the water resistant membrane and sheating, the framing (main structure), the insulation, the vapor barrier and the interior finishing. The obtained results show on one part which are the more "impacting" materials and on the other side which is the lifecycle step having a major contribution on these impacts. It is then easy to observe that the impacts diminution in the end of life can increase them during the manufacturing step...|$|R
40|$|In the {{framework}} of the conservation of early reinforced concrete structures from the last third of the 19 th century up to 1914, this research deals with superstructures (excluding foundations, roads, pipes, etc.) in reinforced concrete (in the modern sense of the term – i. e. concrete made with artificial cement and rebars supplying tensile strength; thus, the combination of a metal profile embedded in concrete is excluded). The development of reinforced concrete as a building material started around 1880 and became widespread {{around the time of the}} First World War. Some of the structures concerned are listed as heritage properties today. Therefore they deserve specific and careful study to ensure long-term preservation of their historic, architectural, technical and socio-economic value. They bear witness to a period in construction history when reinforced concrete was a new material. The outbreak of the First World War marked the end of the initial period of innovation, exploration and experimentation. By then, reinforced concrete had become widely accepted and adopted as a suitable and effective building material. However, present-day attempts at restoration often prove inadequate, due to incomplete understanding of this period of construction and the characteristics of the first generation of reinforced concrete. If the causes of degradation are incorrectly diagnosed, the repairs are likely to be inappropriate. Moreover, the number of reinforced concrete structures requiring repair work is currently increasing with the natural ageing of the material. This phenomenon will continue to grow in the coming years. With this in mind, the present research aims at identifying the specific structural characteristics of reinforced concrete structures erected before the First World War. Several axes of investigation were pursued in this PhD research and have resulted in the main observations detailed below. - Based on a case study of the region of Brussels (Belgium), a database of structures built in reinforced concrete prior to 1914 was drawn up in order to place the material in its historical and geographical context. The inventory currently contains 507 examples and provides a panorama of the uses of reinforced concrete, ranging from numerous foundations and slabs to a complete structure from the end of the 1890 s. This list is supplemented by a survey of a total of 605 patents filed for reinforced concrete in Belgium before the First World War. The early development of reinforced concrete was strongly related to national patenting, with a considerable number of systems being patented by private inventors for commercial purposes. Reinforced concrete profoundly transformed the building industry. All the professions working with the composite material had to change their approach, from the planning stage through to execution on the site. From the viewpoint of construction history, all these modifications make the time of the advent of reinforced concrete a particularly fruitful period to study. - From the survey of early reinforced concrete structures in Brussels and the database of Belgian patents, the supremacy of the Frenchman François Hennebique and his system on the Brussels market for reinforced concrete (and, by extension, on the Belgian market) before 1914 is incontestable. This commercial achievement resulted from a combination of factors: an efficient structural system, meticulous attention to the quality of on-site reinforced concrete execution, and the commercial acumen to develop the business through advertising and other media. The well-known Hennebique system represents a monolithic structure including slabs, beams and columns. In fact, this system changed over the decades of operation of Hennebique’s company, not so much in relation to the design methods (his original semi-empirical method continued to be used) but particularly in practical terms (the type and location of the rebars among others). The evolution of the system is analysed by means of technical drawings from about 30 Belgian projects designed by Hennebique between 1900 and 1930. - After the building contractors, who had been the first to believe in the structural and economic potential of reinforced concrete, engineers invented the calculation models and architects started developing new shapes. The Belgian engineer Paul Christophe was among the first theorists of reinforced concrete. The publication of his book Le béton armé et ses applications in 1899 is internationally recognised as a milestone in the rational modelling of structural reinforced concrete elements. Prior to the present study, details of his life and work remained largely uninvestigated, but the discovery of large parts of his personal archives has allowed clarification of his role in the popularisation of reinforced concrete, especially at the theoretical level. - Reinforced concrete structures around the beginning of the 20 th century were initially governed by empirical models of calculation (and execution) developed by the individual constructors. Gradually, reinforced concrete standards, published between 1904 and 1923 and based on working stress analysis and elastic modular ratio theory, replaced the utility of the patented systems. The different theoretical approaches are briefly described in this research. Mastering the theoretical assumptions and calculation methods used at the time represents the first step towards an appreciation of the structural behaviour and the possible weaknesses that can be expected. - A review, based on literature published at that time, of the properties of the components of reinforced concrete allows identification of the characteristic materials used in the concrete matrix and the metal reinforcements. The execution process and the available technological tools for erecting a reinforced concrete structure are also addressed, as these would have had a direct influence on the quality of construction. Non-destructive and destructive experimental laboratory tests were performed on original samples, mainly removed from the Colo-Hugues viaduct (1904, Braine-l’Alleud, Hennebique system) in order to assess the mechanical properties, chemical features and durability issues for concrete and ferrous reinforcements. Comparing the results obtained using different techniques also makes it possible to determine the extent to which these techniques are reliable for the appraisal of early reinforced concrete structures. - The structural efficiency of the Hennebique system is assessed based on an understanding of the principles of Hennebique’s semi-empirical method of calculation, but also – and primarily – by means of observations from experimental tests carried out on full-sized beams removed from the Colo-Hugues viaduct. Analysing and understanding the behaviour of the new composite material was a critical issue for promoting the use of reinforced concrete at the beginning of the 20 th century. Today, what is required is a re-assessment of its structural behaviour. Three bending tests up to failure in simply supported conditions were performed at the BATir Department of the Université libre de Bruxelles on T-beams from the Colo-Hugues viaduct. This case study is representative of the majority of Hennebique structures, because the typical continuous straight T-beam is the main structural element of any Hennebique structure (bridge, building, etc.). The first test is a four-point bending test on a complete span (6 m) of the viaduct to obtain the response of the central part under positive bending moment. The flexural failure was ductile and occurred through yielding of the reinforcements followed by crushing of the concrete at mid-span. The second and third tests are three-point bending tests on 4 m long specimens centred on the column, representing the behaviour of the beam around the supports. These showed a sudden slipping failure due to loss of the adhesive bond between rebars. The results of these three experiments combined reproduce the actual behaviour of the viaduct in service. The bearing capacity of the Hennebique system in service and at ultimate has been demonstrated, at least for one loading case. These experimental tests provide essential data for a better understanding of the mechanisms of failure and reveal the main weaknesses of the Hennebique T-beam. Two strengthening solutions are suggested as supplementary information. - The pathologies observed in early reinforced concrete structures (honeycombs, corrosion of the rebars, and so on) are mainly attributable to the tools and techniques that the builders had at their disposal (handmade compaction, high water-to-cement ratio, etc.) and by the limited contemporary knowledge of the physical and chemical phenomena, especially with regard to long-term effects. In fact, the concrete quality of the viaduct is surprisingly satisfactory despite its great age, due to the fact that the whole structure was covered with plaster, like the majority of reinforced concrete structures designed at that time. This research establishes that reinforced concrete structures from 1880 to 1914 differ from later reinforced concrete structures. Taking into consideration the features of early reinforced concrete structures will contribute to ensuring sustainable conservation with limited intervention, thus preserving as much as possible of the original structure when restoration work is undertaken. Working on existing buildings often requires a multidisciplinary and holistic approach. The present study could thus be extended in various areas. For example, other structural aspects could be studied more in depth, such as demonstration of the shear strength of the Hennebique system or detailed consideration of the reinforcements (low adherence, particular anchorage devices, etc.) / C'est dans le cadre de la conservation, au sens large du terme, que s'inscrit cette recherche sur les constructions en béton armé de première génération, c'est-à-dire de la fin du 19 ème siècle au début du 20 ème siècle. Cette recherche traite uniquement des superstructures, à l'exclusion des fondations, routes, tuyaux, etc. et en béton armé au sens moderne du terme, c'est-à-dire un béton réalisé à base de ciment artificiel et dont les armatures interviennent surtout pour reprendre les efforts de traction, ce qui exclut par exemple les utilisations de poutrelles métalliques enrobées de béton. Certains de ces ouvrages, réalisés entre 1880 et 1914, font aujourd'hui partie intégrante du patrimoine bâti, pour leurs valeurs architecturale, historique, technique ou aussi socio-économique. Ils jalonnent désormais l'histoire de la construction comme témoins d'une époque où le béton armé était un matériau nouveau. La Première Guerre mondiale marque la fin de cette période de premières innovations, d'explorations et d'expérimentations. Elle entérine l'acceptation et la diffusion du béton armé comme matériau de construction à part entière. Cependant, ainsi que le montrent certains projets de restauration actuels aux interventions inadéquates, il y a encore une méconnaissance des spécificités du béton armé de cette époque. Les causes de leurs dégradations mal diagnostiquées sont traitées de façon inappropriée. Or, dans les prochaines années, nombre de structures en béton armé construites dans la première moitié du 20 ème siècle seront amenées à subir une rénovation suite au vieillissement naturel du matériau. C'est pourquoi pour conserver au mieux ces structures, il est indispensable d'étudier en détails leurs caractéristiques techniques pour ensuite intervenir, si nécessaire, de façon précise et adaptée. Ce doctorat s'attèle donc à identifier les particularités des constructions en béton armé construites avant l'avènement de la Première Guerre mondiale, et plus spécifiquement à étudier leurs aspects structuraux. Plusieurs axes de recherche ont été développés et ont abouti aux principaux résultats suivants. - Basé sur le cas de la région de Bruxelles-Capitale (Belgique), un inventaire des interventions en béton armé, construites avant 1914, a été dressé pour replacer le matériau dans son contexte historique et géographique. Cette base de données, comprenant 507 biens jusqu'à présent, illustre les types d'utilisation du béton armé dans la construction au début du 20 ème siècle, d'abord des fondations ou simples planchers, jusqu'à une structure monolithique complète dès la fin des années 1890. Cet inventaire est complété par le relevé détaillé des brevets, au nombre de 605, déposés à ce sujet en Belgique avant la Première Guerre mondiale. Les brevets ont joué un rôle fondamental dans le développement du béton armé. Celui-ci était, en effet, régi par un foisonnement de systèmes commerciaux, majoritairement brevetés. L'introduction du béton armé a transformé en profondeur le secteur de la construction et notamment les professions liées tant à la phase de conception qu'au chantier lui-même. Du point de vue de l'histoire de la construction, toutes ces mutations font de l'avènement du béton armé une période historique riche. - A la lecture du panorama offert par les inventaires des constructions et des brevets, la prééminence de la compagnie du Français François Hennebique, et donc de son système, sur le marché bruxellois (et par extrapolation sur le marché belge) du béton armé avant 1914 est indéniable. La réussite commerciale de Hennebique résulte d'une combinaison de facteurs: un système efficace sur le plan structural, une qualité d'exécution de béton coulé en place fiable et méticuleuse ainsi qu'un sens développé des affaires, en maîtrisant l'art de la promotion et de la publicité notamment. Le système bien connu de Hennebique comprend un ensemble monolithique formé par des dalles (hourdis), poutres et colonnes. Ce système a, en réalité, évolué dans le temps, pas tant d'un point de vue théorique (les calculs de dimensionnement sont les mêmes) mais plutôt pratique (positionnement, type d'armatures, etc.). Cette évolution a été observée par l'étude d'une trentaine de cas pratiques exécutés par Hennebique entre 1900 et 1930 en Belgique. - Après les entrepreneurs, qui ont été les premiers à croire aux nouvelles possibilités constructives qu'offre le béton armé ainsi qu'à son succès commercial, les ingénieurs en inventent les principes de calcul et les architectes en révolutionnent les formes. L'ingénieur belge Paul Christophe fut parmi les premiers théoriciens du béton armé. La publication de son ouvrage Le béton armé et ses applications en 1899 constitue une étape importante, et internationalement reconnue, pour le dimensionnement rationnel d'éléments structuraux en béton armé. Jusqu'à la présente recherche, sa vie et son œuvre étaient restées assez confidentielles mais la découverte d'une partie de ses archives personnelles a permis de clarifier son rôle dans la diffusion, surtout théorique, du béton armé. - Les structures en béton armé d'avant la Première Guerre mondiale furent d'abord gouvernées par des méthodes empiriques de dimensionnement (et d'exécution) développées par chaque constructeur. L'apparition des premières règlementations entre 1904 et 1923, basées sur une analyse en contraintes admissibles et la théorie du coefficient d'équivalence, remplace ensuite peu-à-peu l'utilité des systèmes brevetés. Les différentes approches théoriques sont brièvement décrites dans cette recherche. Maitriser les hypothèses et les méthodes de calculs employées à l'époque est, en effet, une première étape pour comprendre le fonctionnement structural prévu et les potentielles défaillances de dimensionnement. - A travers une lecture attentive de la littérature publiée à cette période, les matériaux <b>intervenants</b> dans la fabrication du béton armé (c'est-à-dire le béton et les armatures) et utilisés couramment au début du 20 ème siècle ont été identifiés ainsi que les moyens disponibles à cette époque pour produire des structures en béton armé. Des méthodes d'essais non-destructives et destructives ont été appliquées principalement, sur le viaduc Colo-Hugues (1904, Braine-l'Alleud, système Hennebique) afin d'évaluer les caractéristiques mécaniques, les propriétés chimiques et la durabilité tant du béton que des renforcements métalliques. Comparer les résultats de ces différentes méthodes permet d'aborder les limites d'utilisation de ces techniques, lorsqu'il s'agit d'évaluer structuralement des bétons armés de première génération. - Grâce à la compréhension des principes, semi-empiriques, de dimensionnement appliqués par le bureau Hennebique en son temps mais surtout grâce aux observations déduites des essais expérimentaux réalisés sur des poutres de grandeur réelle, prélevées sur le viaduc Colo-Hugues, le fonctionnement structural réel du système Hennebique est évalué. Comprendre et modéliser le comportement du nouveau matériau composite fut une problématique fondamentale pour accroître l'usage du béton armé au début du 20 ème siècle. Actuellement, il s'agit de réévaluer le comportement de ces structures. Trois essais jusqu'à rupture ont été menés, au département BATir de l'Université libre de Bruxelles, sur des poutres à gousset en T provenant du viaduc Colo-Hugues en conditions isostatiques et soumises à flexion. Ce viaduc des chemins de fer vicinaux est un cas d'étude représentatif de la majorité des constructions Hennebique, car la poutre de section en T est la structure typique du système Hennebique, utilisée tant dans les ouvrages d'art que dans les bâtiments. Le premier essai est une flexion 4 points sur une travée complète du viaduc (6 m de portée) pour obtenir la réponse en zone de moment maximum positif. La rupture ductile a eu lieu par plastification des armatures suivie d'un écrasement du béton en zone centrale, c'est-à-dire dans la zone la plus sollicitée. Deux éléments identiques de longueur de 4 m ont été essayés en flexion 3 points pour représenter le comportement sur appuis. La rupture de ces deux dernières expériences s'est produite suite à un glissement des armatures sur appuis (goussets à côté de la colonne). Il s'agit donc d'une rupture à caractère fragile. Les trois essais combinés représentent correctement la structure hyperstatique du viaduc dans son fonctionnement en service. La capacité portante réelle du système Hennebique en service et à l'état limite ultime, du moins dans un cas de chargement, a pu être expliquée. Ces essais fournissent les données essentielles pour estimer l'efficacité structurale du système Hennebique et identifier ses faiblesses. Deux solutions de renforcement sont proposées en complément d'information. - Les pathologies observées dans les bétons armés datant du début du 20 ème siècle (nids de graviers, corrosion des armatures, etc.) sont, la plupart du temps, causées par les outils sommaires à la disposition des constructeurs (vibration à la main, rapport eau/ciment plus élevé qu'aujourd'hui, etc.) et par une connaissance limitée des phénomènes physiques et chimiques, surtout à long terme. En fait, la qualité du béton du viaduc Colo-Hugues est particulièrement satisfaisante malgré l'âge avancé du béton, grâce notamment à l'enduit recouvrant l'ensemble du viaduc, ce qui est le cas pour la majorité des structures de la période étudiée. Cette recherche démontre que les constructions en béton armé datant de 1880 à 1914 diffèrent des ouvrages postérieurs en béton armé et qu'il serait utile pour leur restauration de tenir compte de ces spécificités. La connaissance approfondie des particularités des constructions en béton armé de première génération permettra, espérons-le, de contribuer à leur longévité en intervenant le moins possible sur les structures d'origine. Etant donné que l'étude des structures existantes nécessite le plus souvent une approche pluridisciplinaire, ce travail pourrait être poursuivi dans plusieurs domaines variés. Il resterait notamment à approfondir d'autres aspects de stabilité, comme par exemple la démonstration de l'efficacité à l'effort tranchant du système Hennebique ou encore la prise en considération plus détaillée des armatures (adhérence limitée, forme d'ancrage particulier, etc.). Doctorat en Sciences de l'ingénieurinfo:eu-repo/semantics/nonPublishe...|$|E
40|$|Une approche ergonomique est ici présentée comme un moyen d'améliorer les {{conditions}} d'exécution du travail en <b>intervenant</b> lors de projets d'investissement. Cette approche s'appuie sur l'analyse de l'activité en situation réelle et s'articule aux structures mises en place pour la réalisation du projet en accordant une place importante aux travailleurs visés. Sur la base de trois interventions, l'article tente d'illustrer l'intérêt de recourir à cette approche pour optimiser les projets en intégrant une prise en compte de l'activité humaine. Investment projects, {{whether they}} involve new technologies or {{the design of}} new buildings, may provide ergonomists {{with the opportunity to}} participate in optimizing the design of new work situations. The future activity approachhas been developed to allow ergonomic involvement during the design process. This approach has two main aspects: it is based on "real work" as proposed by French ergonomics; and it considers the design process as a process of social interaction. The ergonomic intervention in this approach therefore requires observations in real work situations, carried out on reference sites. These sites can be situations to be changed or situations with characteristics similar to those that are to be changed. Using these analyses, the ergonomist participates in the design process. His role or her role is to retrace typical action situationson the reference sites so that the designers' proposais can be tested through human activity, or so that design criteriacan be developed. In a project, several areas of design can be the focus of ergonomic action: space, the technical device, work organization, and training. The future activity approach assumes that the ergonomic involvement is based on the company structures. By considering design as a process involving several actors (designers, managers, employees), the ergonomist makes use of the abilities of many different participants to carry out the reference site analysis as well as to anticipate the future activity and enhance the outcome of the project through design criteria. Participatory structures, in which the ergonomist plays an important role, are organized {{in such a way as}} to allow these abilities to be used. To illustrate how this approach is applied, three different interventions are presented. They are based on the same approach, but differ in certain respects due to the uniqueness of the projects and the contexts involved. First, in a project to reorganize production in a poultry slaughterhouse, the presence of working groups allowed variability (of people, raw materials, tools) to be better taken into account in the planned facilities. Second, in the design of a new residential and extended-care centre, the ergonomists enhanced the architectural program by becoming involved very early in the process. The working groups in fact revealed working methodsdeveloped by the employees, so that programming could focus on these aspects rather than on standards and regulations. Lastly, an ergonomic intervention during the modernization of a newspaper printing plant shows that reconstitution of the future activityallows not only modifications to improve a proposed project, but also an evolution in the representations that the different participants have of the work situations and, consequently of the relationships between them. These examples of involvement in various projects also point to the conditions that favour an ergonomic intervention which satisfies criteria to improve not only the health and safety of people, but also production efficiency. The care that is given to the social construction of the intervention has an impact on the ease of implementation of the structures and the rules of the game discussed and accepted by the parties involved. For example, a structure that manages the intervention (follow-up group) should probably be separated from another that participates more directly in the activity analyses and reconstitutions (working group). Early involvement in the projects allows more leeway in the changes to be made. Furthermore, by participating throughout the project in social interactions in a structure planned for this purpose, the ergonomist is better able to achieve something positive than if he or she adheres to recommendations produced at a given time in the process. We believe that a project to change work situations is an interesting opportunity to create synergy between the resources assigned to prevention and those responsible for improving production. The future activity approach seems to be a suitable process for this...|$|R
40|$|L'inhibiteur de la Voie du Facteur Tissulaire (TFPI) est une protéine régulatrice de la {{coagulation}} plasmatique <b>intervenant</b> à la phase initiale de la cascade. Il inhibe en présence de la protéine S (PS) le facteur Xa et ce complexe TFPI-Xa inactive ensuite le complexe FT-VIIa. Nous avons recherché une résistance à l'activité anticoagulante du TFPI. La sensibilité du plasma à une quantité fixe de TFPI a été évaluée sur la base d'un temps de thromboplastine diluée (TTD) réalisé avec et sans TFPI : - chez des patients ayant présenté une thrombose veineuse profonde inexpliquée; cette résistance suspectée sur une 1 ère étude n'a pas été confirmée sur la 2 ème. - chez des patientes enceintes; une résistance au TFPI acquise a été montrée et rapportée au déficit acquis en PS; cependant le degré de résistance au TFPI ne peut pas être utilisé comme marqueur de risque de pathologie vasculaire placentaire. Chez des patients obèses l'effet inhibiteur des taux élevés de Lp(a) sur l'activité TFPI décrit {{in vitro}} n'a pas été retrouvé in vivo pas plus que l'effet de l'aspirine sur la normalisation des taux de Lp(a). Le TFPI joue un rôle dans les manifestations hémorragiques des hémophiles. Nous avons montré que les hémophiles B ont comparativement aux A des taux moindres de TFPI ce qui pourrait expliquer leur différence en terme de manifestations hémorragiques. Les taux de TFPI libre sont bien corrélés aux paramètres de la génération de thrombine surtout au temps de latence. En présence d un anti TFPI humain la génération de thrombine est corrigée chez l'hémophile. Cette correction dépend de la concentration d'anti TFPI, est saturable et doit être étudiée sur du plasma riche en plaquettesTFPI is a multivalent Kunitz-type proteinase inhibitor that directly inhibits FXa and produces FXa-dependent feedback inhibition of the FVIIa TF complex. It was recently demonstrated that Protein S (PS) {{plays the role}} of TFPI cofactor by enhancing the TFPI inhibition of factor Xa in vivo. Approximately 80 % of plasma TFPI circulates as a complex with plasma lipoproteins, about 5 20 % circulating as free TFPI. Under quiescent conditions, approximately 50 80 % of intravascular TFPI is stored in association with the endothelium. Full-length TFPI a carried in platelets constitutes 8 - 10 % of the total amount of TFPI in the blood, corresponding to a quantity comparable to that of soluble full-length TFPI a in the plasma. We searched for resistance to TFPI activity in patients who presented idiopathic venous thrombosis at a young age. Plasma sensitivity to TFPI was evaluated on the basis of diluted prothrombin time (dPT) measured in patients and in control plasma in the presence (W) and absence (Wo) of exogenous TFPI. At the same time, dPT was measured on a reference plasma to establish a normalized ratio termed TFPI NR and defined as (dPT wTFPI/ dPT Wo TFPI) patient or control / (dPT wTFPI/ dPT Wo TFPI) reference plasma. In an initial study, we found that TFPI resistance could be considered as a new coagulation abnormality that could be related to unexplained thrombosis. In a second study, we failed to demonstrate a role of TFPI resistance in patients with venous thrombosis, abnormal TFPI NR being more likely related to the non-respect of preanalytical conditions rather than to an inherited trait. However, in another study, we showed that inherited or acquired PS deficiency was responsible for a TFPI resistance, providing an ex vivo demonstration that PS is the cofactor of TFPI activity. We showed that this TFPI resistance existed throughout pregnancy and that it disappeared when PS returned to normal values after delivery. We evaluated this TFPI resistance as a possible marker of the risk of a gestational vascular complication (GVC) in 72 patients at risk of developing a GVC. TFPI NR did not differ between GVC+ patients (n = 15) and GVC patients (n = 57). High levels of Lipoprotein(a) (Lp(a) {{have been shown to be}} an independent risk factor for cardiovascular disease, lowering of these levels not being achievable by any treatment except possibly aspirin. An in vitro study showed that TFPI activity could be inhibited by Lp(a). We did not confirm this TFPI inhibition in vivo in 20 obese patients with coronary insufficiency who had either normal Lp(a) levels (= 0. 3 g/L; n = 5). Moreover, we found no effect of aspirin treatment on Lp(a) whatever the initial level of Lp(a). Haemophilia B patients bleed less than haemophilia A patients. We showed that this difference in bleeding profile could be explained by lower free TFPI levels in haemophilia B patients compared to haemophilia A patients. In an ongoing study, we showed that in haemophilia A patients there was a strong correlation between the different parameters of thrombin generation (TG) and free TFPI. We also showed, in a TG assay performed in platelet-rich plasma (PRP) with a low TF concentration, that LT was sensitive to free TFPI levels whatever the type of haemophilia and whatever theseverity of the disease. We demonstrated that blocking TFPI by an anti-TFPI Antibody (Ab) allows complete correction of the TG profile in PRP. We showed that it is of major importance to perform a TG assay in PRP in order to evaluate the efficacy of anti-TFPI Ab in correcting TG parameters in haemophilia patientsST ETIENNE-Bib. électronique (422189901) / SudocSudocFranceF...|$|R
40|$|TFPI is a {{multivalent}} Kunitz-type {{proteinase inhibitor}} that directly inhibits FXa and produces FXa-dependent feedback inhibition of the FVIIa–TF complex. It was recently demonstrated that Protein S (PS) {{plays the role}} of TFPI cofactor by enhancing the TFPI inhibition of factor Xa in vivo. Approximately 80 % of plasma TFPI circulates as a complex with plasma lipoproteins, about 5 – 20 % circulating as free TFPI. Under quiescent conditions, approximately 50 – 80 % of intravascular TFPI is stored in association with the endothelium. Full-length TFPI α carried in platelets constitutes 8 - 10 % of the total amount of TFPI in the blood, corresponding to a quantity comparable to that of soluble full-length TFPI α in the plasma. We searched for resistance to TFPI activity in patients who presented idiopathic venous thrombosis at a young age. Plasma sensitivity to TFPI was evaluated on the basis of diluted prothrombin time (dPT) measured in patients and in control plasma in the presence (W) and absence (Wo) of exogenous TFPI. At the same time, dPT was measured on a reference plasma to establish a normalized ratio termed TFPI NR and defined as (dPT wTFPI/ dPT Wo TFPI) patient or control / (dPT wTFPI/ dPT Wo TFPI) reference plasma. In an initial study, we found that TFPI resistance could be considered as a new coagulation abnormality that could be related to unexplained thrombosis. In a second study, we failed to demonstrate a role of TFPI resistance in patients with venous thrombosis, abnormal TFPI NR being more likely related to the non-respect of preanalytical conditions rather than to an inherited trait. However, in another study, we showed that inherited or acquired PS deficiency was responsible for a TFPI resistance, providing an ex vivo demonstration that PS is the cofactor of TFPI activity. We showed that this TFPI resistance existed throughout pregnancy and that it disappeared when PS returned to normal values after delivery. We evaluated this TFPI resistance as a possible marker of the risk of a gestational vascular complication (GVC) in 72 patients at risk of developing a GVC. TFPI NR did not differ between GVC+ patients (n = 15) and GVC– patients (n = 57). High levels of Lipoprotein(a) (Lp(a) {{have been shown to be}} an independent risk factor for cardiovascular disease, lowering of these levels not being achievable by any treatment except possibly aspirin. An in vitro study showed that TFPI activity could be inhibited by Lp(a). We did not confirm this TFPI inhibition in vivo in 20 obese patients with coronary insufficiency who had either normal Lp(a) levels (≤ 0. 3 g/L; n = 15) or high Lp(a) levels (≥ 0. 3 g/L; n = 5). Moreover, we found no effect of aspirin treatment on Lp(a) whatever the initial level of Lp(a). Haemophilia B patients bleed less than haemophilia A patients. We showed that this difference in bleeding profile could be explained by lower free TFPI levels in haemophilia B patients compared to haemophilia A patients. In an ongoing study, we showed that in haemophilia A patients there was a strong correlation between the different parameters of thrombin generation (TG) and free TFPI. We also showed, in a TG assay performed in platelet-rich plasma (PRP) with a low TF concentration, that LT was sensitive to free TFPI levels whatever the type of haemophilia and whatever theseverity of the disease. We demonstrated that blocking TFPI by an anti-TFPI Antibody (Ab) allows complete correction of the TG profile in PRP. We showed that it is of major importance to perform a TG assay in PRP in order to evaluate the efficacy of anti-TFPI Ab in correcting TG parameters in haemophilia patientsL'inhibiteur de la Voie du Facteur Tissulaire (TFPI) est une protéine régulatrice de la coagulation plasmatique <b>intervenant</b> à la phase initiale de la cascade. Il inhibe en présence de la protéine S (PS) le facteur Xa et ce complexe TFPI-Xa inactive ensuite le complexe FT-VIIa. Nous avons recherché une résistance à l'activité anticoagulante du TFPI. La sensibilité du plasma à une quantité fixe de TFPI a été évaluée sur la base d'un temps de thromboplastine diluée (TTD) réalisé avec et sans TFPI : - chez des patients ayant présenté une thrombose veineuse profonde inexpliquée; cette résistance suspectée sur une 1 ère étude n'a pas été confirmée sur la 2 ème. - chez des patientes enceintes; une résistance au TFPI acquise a été montrée et rapportée au déficit acquis en PS; cependant le degré de résistance au TFPI ne peut pas être utilisé comme marqueur de risque de pathologie vasculaire placentaire. Chez des patients obèses l'effet inhibiteur des taux élevés de Lp(a) sur l'activité TFPI décrit in vitro n'a pas été retrouvé in vivo pas plus que l'effet de l'aspirine sur la normalisation des taux de Lp(a). Le TFPI joue un rôle dans les manifestations hémorragiques des hémophiles. Nous avons montré que les hémophiles B ont comparativement aux A des taux moindres de TFPI ce qui pourrait expliquer leur différence en terme de manifestations hémorragiques. Les taux de TFPI libre sont bien corrélés aux paramètres de la génération de thrombine surtout au temps de latence. En présence d’un anti TFPI humain la génération de thrombine est corrigée chez l'hémophile. Cette correction dépend de la concentration d'anti TFPI, est saturable et doit être étudiée sur du plasma riche en plaquette...|$|R
40|$|La connaissance, actuellement très limitée, du métabolisme des bactéries acétogènes <b>intervenant</b> dans la biodégradation anaérobie de l'acide butyrique et d'un de ses sous-produits, l'acide crotonique, est à l'origine de cette étude. Après avoir mis au point un réacteur anaérobie à biomasse fixée, cette dernière a, dans un premier temps, été adaptée à la biodégradation {{exclusive}} du butyrate. La dégradation du crotonate a ensuite été étudiée, selon différents protocoles expérimentaux (pulses de crotonate en alimentation continue avec du butyrate puis alimentation continue avec du crotonate). Des injections de crotonate ont également été effectuées en circuit fermé, avec une biomasse adaptée dans un premier temps à la dégradation d'un mélange d'AGV, le réacteur étant ensuite alimenté avec du propionate puis du butyrate seuls. Contrairement à ce que laissait penser la bibliographie, il a été constaté que les bactéries adaptées à la dégradation exclusive du butyrate sons très rapidement à même de dégrader le crotonate. Les résultats obtenus permettent d'approcher les spécificités bactériennes, la voie catabolique suivie par le crotonate, son mode de régulation enzymatique et les équilibres qui la gouvernent. C'est ainsi qu'il est possible de proposer un modèle explicatif relativement simple du mécanisme de biodégradation du crotonate. Volatile Fatty Acids (VFAs) are intermediate metabolites {{formed in}} the anaerobic biodegradation of organic matter. They are commonly found in sewage, municipal sanitary landfill leachate and effluents from agricultural and food-processing industries. A good knowledge of the microorganisms involved in VFA biodegradation is necessary to operate satisfactory biotreatment of those effluents. The objective {{of the present study}} is to better understand the metabolism of the anaerobic bacteria responsible for the degradation of butyric acid and one of its metabolites (crotonic acid), which is still poorly known. Syntrophomonaswolfei {{is one of the few}} butyrate-degrading acetogenic bacteria that bas been documented. First studios have shown that this microorganism is not capable of degrading crotonic acid (MCINERNEY et al., 1979, 1981). This is surprising since crotonyl-Coenzyme A, in its activated form, is an intermediate metabolite of n-butyrate ß-oxidation, which is the most common mechanism of butyrate biodegradation. In addition, ß-oxidatlon of crotonate is thermodynamically possible, even under standard conditions. These observations are al the origin of the present study, which investigates the anaerobic biodegradation of crotonate. Other Investigators have followed a similar approach and isolated S. wolfei in pure culture on crotonate. The degradation of crotonate was studied in a bench-scale up-flow anaerobic filter of twenty liters, operated in the dark, at 35 °C. A first set of experiments was carried out with a biomass exclusively adapted to the biodegradation of butyrate. Heat-expansed vermiculite was used as a packing medium. Various experimental protocols were successive followed. First, pulses of crotonate were injected into the reactor under conditions of continuous feeding with butyrate, and then, the reactor was continuously fed with crotonate. The objective was to determine whether a bacterial population exclusively adapted to butyrate biodegradation would be capable of degrading crotonate. It was found that crotonate was actually biodegraded in the reactor. Woth the first protocol, when pulses of crotonate were injected into the reactor, crotonate was totally removed in 55 hours (fig. 3). Butyrate and acetate concentrations increased as crotonate was degraded, but no significant increase in biogas production was observed. On the other hand, under the same conditions, it was found that iso-butyrate was not degraded, which is consistent with other published data (MCINERNEY et al., 1979, 1981; STIEB and SCHINK, 1985, 1989). With the second protocol (continuous feeding with crotonate at 5. 2 gg/l), crotonate was totally biodegraded in 48 hours after a 24 hours lag period. This biodegradation resulted in the accumulation of acetate and, in a lower extend, butyrate (fig. 4). Following this stage, the reactor was fed with a higher crotonate concentration (12 g/l), and it was observed that crotonate was totally degraded in 20 hours, without any lag period (fig. 5). These results showed that butyrate-degrading bacteria were capable of degrading crotonate effectively after a short period of adaptation. Further experiments were conducted with a biomass previously adapted to the degradation of a mixture of VFAs (acetate, propionate, iso-butyrate, butyrate and caproate). Berl saddles were used as a support for bacterial growth. The reactor was operated in a recirculated batch mode and spiked with crotonate. Finally, the reactor was successively fed for four weeks with propionate and for two weeks with butyrate, before being spiked with crotonate. In all these experiments, crotonate biodegradation was observed, but, in contrast to the results obtained with the “vermiculite reactor”, no butyrate accumulation occured (fig. 6). These results show that a bacterial population adapted to the degradation of a mixture of VFAs or to the degradation of individual VFAs such as propionate and n-butyrate, is capable of degrading crotonate. Based on the present study and on literature data, the following mechanism can be proposed for the biodegradation of crotonate (fig. 7). The first stage is the activation of crotonate into crotonyl-Coenzyme A by an acetyl-CoA/crotonyl-CoA transferase, as recently isolated from S. wolfei (BEATY and MCINERNEY, 1987). When present at low concentrations, crotonate is probably directly degraded into acetate, as shown by the results obtained with the “selles de Berl reactor”, in which no intermediate metabolite has been detected. At higher concentrations, enzymatic sites may be saturated and an equilibrium be established with butyrate, which is then released into the medium. This has been shown by the accumulation of butyrate under conditions of continuous feeding with crotonate. In addition, another intermediate metabolite has been formed, which has not been identified in the present study. This product is most probably poly-ß-hydroxy-butyrate, which has been found in S. wolfei (MCINERNEY et al, 1979) although if is not very common in chemiotrophic bacteria...|$|R
40|$|Un modèle décrivant la température d'équilibre des lagunes a été développé, tenant compte des différents flux de chaleur que celles-ci échangent avec l'air et le sol environnant. Six {{composantes}} différentes ont été inclues dans le calcul de ce bilan thermique: radiation solaire, évaporation, convection, rayonnement atmosphérique, rayonnement de la surface du plan d'eau, échange via les parois en contact avec le sol. Le modèle ainsi obtenu a été testé avec efficacité sur deux lagunes aérées et une lagune naturelle situées sous climat tempéré; sa précision sur l'estimation des températures d'équilibre étant de l'ordre de 0. 7 °C. Des simulations en continu ont également pu être effectuées au moyen d'une variante dynamique, tenant compte de l'inertie thermique qu'entraîne {{le volume}} des bassins. Quelle que soit la saison envisagée, la principale forme d'apport de chaleur est représentée par la radiation solaire tandis que la dissipation d'énergie se partage entre les flux d'évaporation et la balance des deux flux de rayonnement. Les bassins échangeraient en moyenne plus de 250 W/m 2; le maximum de transfert de chaleur correspondant au printemps et à la période estivale. Enfin, l'analyse de sensibilité du modèle nous a permis de mettre en évidence la contribution de chacun des termes <b>intervenant</b> dans le calcul de ce bilan thermique et de révéler sa dépendance vis-à-vis principalement de la température d'entrée, du rayonnement solaire et de la température de l'air. Very {{few studies have}} ever focused on the thermal balance of a wastewater treatment process, despite its major impact on various aspects of sanitary engineering, such as biological growth, oxygen transfer and, most importantly, purification kinetics. This lack of knowledge is particularly worrying {{for the design of}} aerated lagoons and waste stabilization ponds, since these two extensive treatment technologies are extremely dependent on climatic conditions and subject to high thermal variations. In temperate regions, a pond annual temperature range can even exceed 20 °C, while a 10 °C variation will induce a more than 60 % drop or increase in its removal yield. Our paper intends to present a comprehensive temperature prediction model which accounts for the main heat loss and gain terms exchanged through the pond surface and walls. Our approach includes six different energy inputs and outputs, namely: solar radiation, air-water surface convection, atmospheric radiation, back surface radiation, evaporation and ground-water-walls convection. Each of these components was described extensively by means of a literature review of all previous efforts made to predict equilibrium temperature in lakes, rivers, salt-gradient solar ponds, cooling tanks, even outdoor pools. The best aspects of each prediction model were then incorporated into a new computer model developed as two different but complementary variants: one for steady-state conditions and the other for continuous and therefore also transient simulations. The main difference between these two approaches is that the first one neglects enthalpy variation while the second one {{takes the form of a}} differential equation, with basin temperatures being estimated by an iterative calculation procedure and a numerical integration method, respectively. Two hypotheses were necessary to develop this model. The first one posits that pond hydrodynamics correspond to completely mixed conditions. Such hydraulic behavior is extremely frequent in aerated lagoons and waste stabilization ponds in temperate climates, but less so in tropical or Mediterranean regions, where thermal balances appear much more complex since stabilization ponds are often thermally stratified. The second hypothesis is that all radiation fluxes received by the ponds are completely absorbed by the pond's contents and are never reflected, even partially, by their bottoms or walls. This model, which is in fact the thermal balance of the basins, relies mainly on meteorological factors and pond characteristics. Only two out of the six estimated fluxes - evaporation rates and solar radiation - are measured directly in situ. It seemed too difficult to estimate them, since predictive equations found in literature constantly gave unsatisfactory results. To establish the validity of this model, experimental data were collected at a wastewater treatment plant located in the southern part of Belgium. This plant consists of a series of two aerated lagoons and four waste stabilization ponds, designed for a nominal capacity of 7, 500 inhabitant-equivalents. Five rounds of measurements, each lasting from five to twenty days, were conducted at different periods of the year. Meteorological factors were continuously monitored by a data acquisition unit while the pond water temperatures and hydraulic flows were measured hourly. Evaporation rates were determined daily with several floating evaporation pans set at the pond surfaces. Vertical temperature and illumination profiles were also measured in order to verify the strict applicability of the two previous hypotheses. Ninety-three experimental data sets were collected on this particular facility. Predicted temperatures were compared with measured temperatures as well as with the results of three other models previously developed for waste stabilization ponds. Our new model systematically proved more reliable and accurate than previous approaches, since equilibrium temperatures were predicted with a mean absolute error of only 0. 7 °C. More than 52 % of the deviations between calculated and observed temperatures were even below 0. 5 °C, which indicates their relatively low dispersion. Continuous simulations were also conducted during a one-day period to demonstrate the importance of the ponds' large thermal capacities. The steady-state approach, which does not account for this latter phenomenon, failed to give consistent results, unlike our dynamic heat balance approach, which yielded extremely good fits with experimental data. A sensitivity analysis allowed us to show the influence of the various meteorological factors on the basins' equilibrium temperatures. In decreasing order, the fits seemed particularly sensitive to inlet temperature, solar radiation, air temperature and evaporation. Surprisingly, wind speed made only a small contribution to the total heat balance. However, this must be seen as a direct consequence of the fact that in our model this latter parameter is no longer used to calculate the predominant evaporation rates but only to estimate the much smaller convection term. Whatever the season considered, more than 90 % of the ponds' energy inputs come from solar radiation while the dominating loss mechanisms are represented by the balance of the two infrared radiation fluxes (46 %) and evaporation (42 %). The sign of the air-water convection term varies according to the period of the year but never accounts for more than 10 % of the total heat balance. Heat losses or gains from basin walls always remain insignificant and could therefore easily be neglected in order to simplify our approach to basin equilibrium temperatures...|$|R
