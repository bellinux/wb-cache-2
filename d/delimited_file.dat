14|61|Public
5000|$|... eqboot is an {{open source}} syntax-based Java {{application}} for conducting IRT equating and computing the bootstrap standard error of equating developed by J. Patrick Meyer. The program runs on any 32- or 64-bit operating system that has the Java Runtime Environment (JRE) version 1.6 or higher installed. At the moment, the programs only support equating with binary items. EQBOOT will compute equating constants using the mean/mean, mean/sigma, Haebara, and Stocking-Lord procedures. It will also compute the standard error of equating if the user provides a comma <b>delimited</b> <b>file</b> of bootstrapped item parameter estimates from both forms, a comma <b>delimited</b> <b>file</b> of bootstrapped ability estimates for Form X examinees, and a comma <b>delimited</b> <b>file</b> of bootstrapped ability estimates for Form Y examinees. Options allow the user to specify the criterion function for the Haebara and Stocking-Lord methods. In addition, the examinee distribution over which the criterion function is minimized may be set to the observed theta estimates, a histogram of theta estimates, a kernel density estimate of theta estimates, or uniformly spaced values on the theta scale.|$|E
5000|$|TPL Tables {{can read}} files with data in fixed columns or <b>delimited</b> <b>file</b> types such as CSV Comma Separated Values [...] TPL-SQL, an {{optional}} add-on feature, provides direct access from TPL Tables to SQL databases produced by {{products such as}} Sybase and Oracle. In the Windows version, TPL-SQL can access databases for which there are ODBC drivers.|$|E
50|$|A delimited {{text file}} is a text file used to store data, {{in which each}} line {{represents}} a single book, company, or other thing, and each line has fields separated by the delimiter.Compared {{to the kind of}} flat file that uses spaces to force every field to the same width, a <b>delimited</b> <b>file</b> has the advantage of allowing field values of any length.|$|E
50|$|The above command {{uses the}} null {{character}} to <b>delimit</b> <b>file</b> names.|$|R
5000|$|Matrix IO {{classes that}} {{read and write}} {{matrices}} from/to Matlab and <b>delimited</b> <b>files.</b>|$|R
40|$|AminoTrack TM is a {{web based}} tool {{designed}} {{to increase the efficiency}} with which sequence data is recorded for further analysis. The main purpose of AminoTrack TM is to streamline the process and reduce human error in the identification of mutations present in multiple sequences compared to a reference sequence. Aligned protein sequences are entered in the web submission form and comma <b>delimited</b> <b>files</b> are generated in a zip file for loading into a spreadsheet. These files can be imported into any spreadsheet program that recognizes comma <b>delimited</b> <b>files</b> such as Microsoft Excel or SPSS. The sequences are analyzed for mutations in amino acids, charge changes, and potential N-linked glycosylation sites (PNG). The data may be viewed in a spreadsheet in a columnar binary format of “ 0 ” and “ 1 ” with one amino acid position per column. Currently this program is being used to identify mutations in viral proteins as these proteins evolve during infection...|$|R
50|$|Typically a <b>delimited</b> <b>file</b> {{format is}} {{indicated}} by a specification. Some specifications provide conventions for avoiding delimiter collision, others do not. Delimiter collision {{is a problem that}} occurs when a character that is intended as part of the data gets interpreted as a delimiter instead. Comma- and space-separated formats often suffer from this problem, since in many contexts those characters are legitimate parts of a data field.|$|E
50|$|More than 4000 {{bacterial}} genomes and 1000s of plasmid genomes {{have been}} sequenced {{thanks to the}} advance in DNA sequencing technology. CGView was developed to address the specialized needs for visualizing and annotating circular genomes, such as bacterial, plasmid, chloroplast, mitochondrial DNA sequences. Once installed, the CGView program accepts {{a number of different}} file formats where feature data and rendering information can be XML file, a tab <b>delimited</b> <b>file,</b> or an NCBI ptt file. CGView then converts the input into a graphical map in various (PNG, JPG, or SVG) image formats that can include labels, titles, legends and footnotes. The images can be static, interactive, or poster-sized images for printing or for embedding into web pages.|$|E
5000|$|In-memory data structures: One can use {{hash tables}} and {{two-dimensional}} arrays in memory {{in conjunction with}} attribute-grouping metadata to pivot data, one group at a time. This data is written to disk as a flat <b>delimited</b> <b>file,</b> with the internal names for each attribute in the first row: this format can be readily bulk-imported into a relational table. This [...] "in-memory" [...] technique significantly outperforms alternative approaches by keeping the queries on EAV tables {{as simple as possible}} and minimizing the number of I/O operations. Each statement retrieves a large amount of data, and the hash tables help carry out the pivoting operation, which involves placing a value for a given attribute instance into the appropriate row and column. Random Access Memory (RAM) is sufficiently abundant and affordable in modern hardware that the complete data set for a single attribute group in even large data sets will usually fit completely into memory, though the algorithm can be made smarter by working on slices of the data if this turns out not to be the case.|$|E
30|$|The data {{analysis}} {{was based on}} three program packages; Vbox Tools v 2.2. 2 b 042 of Racelogic ([URL] Excel 2007 of Microsoft ([URL] and Autocad 2009 of Autodesk ([URL] The original data files recorded at the SD card (extension *.vbo) opened using Vbox Tools and then were simply converted to comma <b>delimited</b> <b>files</b> (extension *.csv). Each *.csv file corresponded to a specific measurement and contained three columns; velocity, coordinate x and coordinate y. It is reminded that these values were recorded at a sampling rate of 10  Hz.|$|R
50|$|Create a <b>delimited</b> or fixed-width <b>file</b> for {{exchange}} with another party.|$|R
5000|$|TreeLine {{outlines}} can be exported as HTML, per-data type {{user defined}} formatting. In addition, it supports exporting outlines as an OpenDocument ODT <b>file,</b> various <b>delimited</b> text <b>file</b> formats, and as [...] "plain" [...] XML.|$|R
40|$|The data is a tab <b>delimited</b> <b>file</b> of user {{generated}} annotations. Field headers are: 	gsm_name. GEO sample GSM identifier 	gse_name: GEO Series GSE identifier 	gpl_name: GEO platform GSE identifier 	tag_name: User defined tag 	concept_full_id: User defined concept id. 	concept_name: Matching {{concept name}} 	ontology_id: Ontology 	annotation: Annotation 	annotations: Number of annotations 	authors: Number of authors 	fleiss_kappa: Fleiss Kappa statistic for all authors 	best_cohens_kappa. Best pairwise Cohens Kappa statisti...|$|E
30|$|The {{motivation}} for the tabular format was two-fold. First, {{we wanted to be}} able to load data into a variety of standard databases easily, and a tabular format made that relatively straightforward. Second, we wanted the state of agents to be editable offline. When saved to a <b>delimited</b> <b>file</b> format, the state of the simulation and its agents can be loaded into common tools, such as Microsoft Excel, and easily edited. Moreover, additional objects (e.g., agents) can be created offline by adding rows to the relevant table.|$|E
40|$|The University of California Santa Cruz (UCSC) Table Browser ([URL] {{provides}} text-based {{access to}} a large collection of genome assemblies and annotation data stored in the Genome Browser Database. A flexible alternative to the graphical-based Genome Browser, this tool offers an enhanced level of query support that includes restrictions based on field values, free-form SQL queries and combined queries on multiple tables. Output can be filtered to restrict the fields and lines returned, and may be organized into one of several formats, including a simple tab- <b>delimited</b> <b>file</b> that can be loaded into a spreadsheet or database as well as advanced formats that may be uploaded into the Genome Browser as custom annotation tracks. The Table Browser User’s Guide located on the UCSC website provides instructions and detailed examples for constructing queries and configuring output...|$|E
5000|$|HPRD data is {{available}} for download in tab <b>delimited</b> and XML <b>file</b> formats.|$|R
40|$|The aim of {{this project}} was to produce an {{electronic}} database of environmental archaeological (animal bone and plant remains) evidence for all archaeological sites in Iraq that have yielded such material. The data from phase I of the project is available to download {{as a series of}} comma <b>delimited</b> text <b>files...</b>|$|R
50|$|Data {{could be}} moved between these {{programs}} relatively easily by using comma <b>delimited</b> format <b>files</b> (now more {{commonly known as}} CSV files), which enhanced {{the utility of the}} package. The manuals assumed no computer background, the programs were straightforward to use, and thus it was possible to find the CEO of a small company developing the applications needed in-house.|$|R
40|$|This {{dissertation}} {{presents the}} development of two methodologies to migrate legacy data elements to an open environment. Changes {{in the global economy}} and the increasingly competitive business climate are driving companies to manage legacy data in new ways. Legacy data is used for strategic decisions as well as short-term decisions. Data migration involves replacing problematic hardware and software. The legacy data elements are being placed into different file formats then migrated to open system environments. The purpose of this study, was to develop migration methodologies to move legacy data to an XML format the techniques used for developing the intermediate <b>delimited</b> <b>file</b> and the XML schema involved the use of system development life cycles (SDLC) procedures. These procedures are part of the overall SDLC methodologies used to guide this project to a successful conclusion. SDLC procedures helped in planning, scheduling, and implementing of the project steps. This study presents development methodologies to create XML schemas which saves man-hours. XML technology is very flexible in that it can be published to many different platforms that are ODBC compliant and uses TCPIIP as its transport protocol. This study provides a methodology that steers the step-by-step migration of legacy information to an open environment. The incremental migration methodology was used to create and migrate the intermediate legacy data elements and the FAST methodology was used to develop the XML schema. As a result the legacy data can reside in a more efficient and useful data processing environment...|$|E
40|$|The One Semi-Automated Forces (OneSAF) Objective System (OOS) is a {{next-generation}} Computer Generated Forces (CGF) simulation that {{is intended}} to represent {{a full range of}} operations, systems, and control processes from individual combatant level and platform level to fully automated BLUFOR battalion level and fully automated OPFOR brigade level. Unlike past systems, OneSAF has had the benefit to be built from the ground-up utilizing eXtensible Markup Language (XML) as the means to define, store, share, and interchange data. Most OneSAF data is stored as XML, including parametric and initialization data. Given the magnitude of the data, and the simple fact that XML data is stored as text, some of the biggest challenges facing OneSAF have been the data normalization, verification, validation, and management of XML documents. This paper describes the methods of managing flat files during development {{in such a way as}} to maintain the flexibility of a truly data centric simulation without introducing fragility into the environment. For the purpose of this paper, the term flat files refers to files that contain data in a tablature or text based format like a text file, a spreadsheet or a comma <b>delimited</b> <b>file.</b> The process of taking data from this flat file format to XML, then into the final form as parametric data, is detailed including transformation, data normalization, and verification and validation. The use of eXtensible Style Sheets (XSL), XQuery, and XPath is discussed, as well as the incorporation of Perl scripting. I...|$|E
40|$|These are the VCF, pileup, {{and input}} files files {{produced}} by the Genome Analysis ToolKit, SAMtools, and our own scripts, respectively, used for comparing the genotypes estimated by the GATK UnifiedGenotyper and the new model for allopolyploids that we introduced in our paper. Additional processing of the files was conducting using Python and R for filtering and extracting error values and read counts (scripts available on GitHub: [URL] Main files: 	pendula-ug-filtered 30. vcf: VCF file from GATK with all called variants for Betula pendula (diploid). 	filtered 30 -pendula. pileup: SAMtools pileup file for variant sites identified by GATK in B. pendula. 	pubescens-ug-filtered 30. vcf: VCF file from GATK with all called variants for Betula pubescens (allotetraploid). 	filtered 30 -pubescens. pileup: SAMtools pileup file for variant sites identified by GATK in B. pubescens. Processed files: 	filtered 30 -variants. txt: tab <b>delimited</b> <b>file</b> of shared variants between B. pendula and B. pubescens. 	filtered 30 -vcf 1. vcf: VCF file for B. pendula with variants extracted from filtered 30 -variants. txt. 	filtered 30 -vcf 2. vcf: VCF file for B. pubescens with variants extracted from filtered 30 -variants. txt. Input files: 	filtered 30 -pubescens-tot. txt, filtered 30 -pubescents-alt. txt, filtered 30 -pubescens-err. txt: input files for running the alloSNP model in ebg. 	filtered 30 -pubescens-alloSNP-freqs 2. txt, filtered 30 -pubescens-alloSNP-g 1. txt, filtered 30 -pubescens-alloSNP-g 2. txt: output files from the alloSNP model. 	filtered 30 -pendula-tot. txt, filtered 30 -pendula-alt. txt, filtered 30 -pendula-err. txt: input files for running the hwe model in ebg. 	filtered 30 -pendula-hwe-freqs. txt, filtered 30 -pendula-hwe-genos. txt: output files from the hwe model. The allele frequency estimates here were also used as the reference panel for the alloSNP model...|$|E
5000|$|AQT Extended Edition {{includes}} a data loader tool {{that makes it}} easy to load data into tables from various sources. The sources can be other tables on the same database, a different database, a completely different kind of database (i.e. can load DB2 table from Oracle table), an Excel spreadsheet, a comma <b>delimited</b> (CSV) <b>file,</b> a flat file, or even a complicated ...|$|R
30|$|In {{the case}} of CNVs and SVs a 50 % {{reciprocal}} overlap rule [30 – 32] was applied to determine if two variants were the same or different between twins/parents. All CNVs/SVs had to meet the ≥  50 % rule {{when compared to the}} other CNV/SV, otherwise, the two were considered unique. Pairwise comparisons were performed for all CNVs and SVs. To increase the efficiency of this, HD-CNV was utilized [33]. Comma <b>delimited</b> <b>files</b> were prepared using a custom python script and the 50 % reciprocal overlap rule was applied to determine the CNVs and SVs that were unique to an individual. Interchromosomal and inversion comparisons were analyzed manually due to the limitations of HD-CNV. Inter-chromosomal events were considered the same if the origin and the destination chromosome numbers matched and the junction positions were less than 500  bp apart. Inversions had to share the same direction and 50 % or more identity to be classified as shared.|$|R
40|$|The dataset {{includes}} {{results of}} FLOOD change experiment performed in Virtual Water Science Lab. It {{consists of three}} parts: (1) Flood_change_results. txt presents a list of 629 analysed catchments, their characteristics and results of trend analysis. It is a tab <b>delimited</b> text <b>file</b> with 69 columns. (2) Catchments. zip contains a shape file (flood_changes_GRDC. shp) of catchment boundaries of 629 GRDC stations. (3) FloodCHange. R	is an R-script used to analyse flood peak changes...|$|R
40|$|M. S. University of Hawaii at Manoa 2015. Includes bibliographical references. Herbicide Ballistic Technology (HBT) is an {{electro-pneumatic}} {{delivery system}} designed for administering 17. 3 mm herbicide-filled projectiles (i. e., paintballs) to visually-acquired weed targets. Currently, HBT is being deployed from a Hughes 500 D helicopter platform in aerial surveillance operations to eliminate satellite populations of an invasive weed (Miconia calvescens) in remote watershed areas of East Maui (Hawaii, USA). Coordination {{and analysis of}} control operations require that the site of each HBT application be recorded with a handheld GPS data logger. This independent step, though critically important, adds significant time to the target acquisition process where flight time is extremely limited. Furthermore, herbicide use rates are calculated post-operation by rudimentary mean estimation of bulk projectile consumption relative {{to the number of}} recorded targets. In an effort to improve operations, GPS and other sensors were integrated directly into the electro-pneumatic device for instantaneously recording time, origin, and trajectory of each projectile discharged. These data are transmitted wirelessly to a custom android application that displays target information in real-time both textually and on a map. The application also records data into a comma <b>delimited</b> <b>file</b> so that it can easily be recalled for map display, or exported to other software such as for conducting additional GIS analysis. Static precision, reported as the Circular Map Accuracy Standard (CMAS) and Spherical Accuracy Standard (SAS), and accuracy, reported as root mean squared error (RMSE) in horizontal and three dimensional space, of the logger are 7. 33 m, 11. 41 m, 7. 27 m and 11. 47 m, respectively. Offset reproducibility was determined to be robust based on the convergence of CMAS, from 18. 18 m to 3. 28 m, and horizontal RMSE, from 12. 12 m to 2. 86 m, from four different marker positions. Operational data collected over the course of ten operations in 2014 and 2015 agreed with previous anecdotal observations and showed a high level of precision. The logger makes data collection a seamless part of the operation, facilitates logistics of applying airborne HBT, and improves our interpretations of operational HBT performance with more statistically robust measures of herbicide use rate and time-on-target. This provides enhanced capabilities for building operational intelligence relevant to landscape scale invasive species management...|$|E
40|$|This {{dissertation}} {{explores the}} development of a simulation model in ICAP to illustrate the effects of shading and mismatch of photovoltaic cells within a photovoltaic array. The intention of designing the simulation model was to build a simulation tool which could be utilised as a teaching resource for illustrating the effects of shading and mismatch of photovoltaic arrays for renewable energy engineering education. The process of building such a model initially involved building a single cell in ICAP which was based on a single diode model to illustrate the current and voltage output of a single cell. Once the output resembled the output for an ideal I-V curve, {{the next step was to}} build several cells in series with the inclusion of two bypass diodes in parallel with a series connection of 18 solar cells per bypass diode. To simulate unshaded conditions, the photo-generated current of each solar cell was the same and the output subsequently resembled the same output as that for an ideal I-V curve. Shaded conditions were simulated by reducing one of the solar cell photo-generated currents by half the original current value. The resulting output had an unchanged short circuit current and open circuit voltage. However, the overall I-V curve had a changed I-V curve characteristic, as well as a reduced power output. Once a module of 36 cells with two bypass diodes was constructed in ICAP, testing was conducted in the Spi-Sun Simulator 460 at the Remote Outdoor Testing Area (ROTA) compound to simulate the outputs for a mono-crystalline module with and without bypass diode connections as well as for unshaded and shaded conditions. The current and voltage results from the simulation were then converted to a Microsoft Excel file. Curve fitting was performed to obtain modified parameters to input back into the module model in ICAP. The modified parameters were then input back into the model in ICAP to illustrate the I-V curve outputs for a module with and without bypass diode connections as well as for unshaded and shaded conditions. The outputs from the ICAP simulation were then exported from the ICAP graph into a text <b>delimited</b> <b>file</b> which was then converted into a Microsoft Excel file. The ICAP results were compared with the ROTA results and whilst the short circuit current and open circuit voltage values were the same, there was some discrepancy with the curve fitting...|$|E
30|$|The time {{sequence}} of grayscale values must be read into LabVIEW software {{in order to}} be filtered. The file path to the saved text file is specified within a subroutine in LabVIEW called the “Read from Measurements File” Express VI. The subroutine imports the time {{sequence of}} grayscale values into LabVIEW from the tab <b>delimited</b> text <b>file.</b> Once in LabVIEW, the data is converted from the dynamic data datatype to an array of double precision floating point values.|$|R
40|$|Summary: Pedro is a Java ™ {{application}} that dynamically generates data entry forms for data models expressed in XML Schema, producing XML data files that validate against this schema. The software uses an intuitive tree-based navigation system, can supply context-sensitive help to users and features a sophisticated interface for populating data fields with terms from controlled vocabularies. The software {{also has the}} ability to import records from tab <b>delimited</b> text <b>files</b> and features various validation routines...|$|R
50|$|STDF is {{a binary}} format, {{but can be}} {{converted}} either to an ASCII format known as ATDF or to a tab <b>delimited</b> text <b>file.</b> Decoding the STDF variable length binary field data format to extract ASCII text is non-trivial as it involves a detailed comprehension of the STDF specification, the current (2007) version 4 specification being over 100 pages in length. Software tools exist for processing STDF generated files and performing statistical analysis on a population of tested devices.|$|R
50|$|This imposed {{structure}} is then exposed by <b>delimiting</b> the PostScript <b>file</b> with DSC comments, which normally begin with two percent signs {{followed by a}} keyword. Some keywords need {{to be followed by}} a colon, an optional space character, and then a series of arguments.|$|R
50|$|VIPP {{can be used}} in four {{different}} modes: Database mode, Line mode, XML mode and Native mode. In Database mode, the programmer can quickly implement a printing solution, for example a billing application, for a <b>delimited</b> database <b>file.</b> In Line mode, an existing print application can be enhanced with form overlays, font selection, color and other features offered by modern laser printers. In XML mode, an XML file can be turned into a readable document. In all modes, VIPP offers conditional logic manipulation of the data.|$|R
40|$|The Living {{with the}} Dead {{database}} documents the treatment, patterning and deposition of human skeletal material in Britain and Ireland between 10000 to 4000 BP. The database documents all contexts with radiocarbon dated human skeletal material. The database is archived with AHDS Archaeology {{and can be}} downloaded {{as a set of}} tables in comma <b>delimited</b> text <b>files.</b> Records are linked to a number of maps which are also available to download. There is a link to a live version of the database maintained by the author containing the most up-to-date additions and alterations...|$|R
40|$|The {{large-scale}} excavation at {{the site}} of the Royal Opera House in Covent Garden, Greater London, was undertaken by the Museum of London Archaeology Service in 1966. The site covered part of the Middle Saxon trading port of Lundenwic which was discovered in London's West End in the mid- 1980 s. The excavations added greatly to the understanding of this settlement. The archive currently contains texts, comma <b>delimited</b> text <b>files</b> suitable for importing into a database and GIS data. It provides artefact, ecofact and context information. Low-level interpretive groupings of contexts are described and illustrated as part of a GIS...|$|R
5000|$|CSV is a <b>delimited</b> text <b>file</b> {{that uses}} a comma to {{separate}} values (many implementations of CSV import/export tools allow other separators to be used). Simple CSV implementations may prohibit field values that contain a comma or other special characters such as newlines. More sophisticated CSV implementations permit them, often by requiring [...] " [...] (double quote) characters around values that contain reserved characters (such as commas, double quotes, or less commonly, newlines). Embedded double quote characters may then be represented {{by a pair of}} consecutive double quotes, or by prefixing an escape character such as a backslash (for example in Sybase Central).|$|R
40|$|To {{facilitate}} {{data output}} and data presentation from MODFLOW model runs, the post processor “MOD-PP” has been written. This program reads MODFLOW’s binary and ASCII output files and writes the results into comma <b>delimited</b> <b>files</b> {{to be viewed}} in Excel. The program is able to: write (i) head data for user specified observation boreholes for all stress periods, (ii) water balances for all stress periods for all boundary conditions used in a model, (iii) river flow {{to and from the}} aquifer for all river cells over time, as well as for up to five specified river sections and for the entire river course and iv) stream leakage to and from the aquifer for all stream cells and for the entire stream course over time, as well as flows within stream cells and within the entire stream over time. The program, which is written in Visual Basic 6. 0 can be applied to any steady-state or transient MODFLOW model and has no restrictions on layer, column or row numbers. This manual describing the post-processor is divided into 7 sections. After this introduction, section 2 gives an overview of how the program operates and which MODFLOW output files are required in the various modules of the program. Section 3 introduces the water balance module. The module for reading and writing groundwater head data for user-specified observation boreholes is described in section 4, while section 5 and 6 give information about running the river and stream modules...|$|R
40|$|Danebury is an Iron Age Hill Fort {{situated}} in the county of Hampshire in southern England. It {{has been the subject}} of a major programme of excavation between 1969 and 1988. The Danebury Excavations Digital Archive aims to provide a number of original datasets and images from these excavations in order to facilitate new research and interpretation. There are four datasets (pits, pottery, animal bones, daub) that are available for download as comma <b>delimited</b> text <b>files</b> and documentation is provided describing the relationships between these tables. There are 29 high resolution images available which illustrate the site, the excavations and a selection of the finds...|$|R
