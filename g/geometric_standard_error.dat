6|10000|Public
40|$|To {{understand}} better {{the mechanism of}} the increase in airway responsiveness associated with late asthmatic reactions, we determined the time course of toluene diisocyanate (TDI) effect on airway responsiveness in six sensitized subjects who exhibited a late asthmatic response after TDI exposure (0. 018 +/- 0. 005 ppm, 30 min) in the laboratory. Airway responsiveness was assessed before TDI exposure and then at 8 hr, 1 day, 1 wk, and 1 mo after TDI exposure. To assess responsiveness we determined the provocative dose of methacholine causing a decrease in FEV 1 of 20 % (PD 20 FEV 1). The methacholine PD 20 decreased from 0. 50 mg <b>geometric</b> <b>standard</b> <b>error</b> of the mean (GSEM = 1. 54) to 0. 06 mg (GSEM = 1. 55) (p less than 0. 001) at 8 hr after exposure to TDI, was still decreased to 0. 15 mg (GSEM = 1. 93) (p less than 0. 05) at 1 day, returned to 0. 26 mg (GSEM = 1. 91) (p greater than 0. 05) at 1 wk, and returned to 0. 43 mg (GSEM = 1. 71) at 1 mo, indicating that full recovery occurred within 1 to 4 wk. These results demonstrate that TDI-induced late asthmatic response is associated with a reversible increase in airway responsiveness to methacholine and suggest that the TDI effect is linked to an acute inflammatory response in the airways...|$|E
40|$|Cl 2, Br 2, and I 2 were {{measured}} in coastal Pacific air from 2 to 29 January 2006. Air was sampled at 10 m {{over the sea}} surface {{near the end of}} Scripps Pier (La Jolla, California). The measurements were made using atmospheric pressure chemical ionization with tandem mass spectrometry (APCI/MS/MS). Over the course of this study, Cl 2, Br 2, and I 2 levels ranged from below detection limits of 1. 0, 0. 5, and 0. 2 ppt, respectively, to maxima of 26, 19, and 8 ppt, respectively. Mean dihalogen levels for the study period were 2. 3 ± 1 ppt for Cl 2, 2. 3 ± 0. 4 ppt for Br 2, and 0. 7 ± 0. 1 ppt for I 2 (expressed as geometric mean ± 1 <b>geometric</b> <b>standard</b> <b>error).</b> The mixed dihalogens BrCl, ICl, and IBr had geometric mean levels below 0. 3 ± 1 ppt and never exceeded their detection limits of 0. 5 ppt. Consistent patterns of diurnal variability were observed for Cl 2 and I 2, with Cl 2 maxima during daytime and I 2 appearing almost exclusively at night. The detection of I 2 appeared {{to be related to the}} passage of air over nearby kelp beds. The observed dihalogen levels suggest that (1) chlorine atom oxidation of hydrocarbons makes a significant contribution to the formation of ozone and (2) halogen atom oxidation of airborne mercury contributes to mercury deposition in polluted coastal air...|$|E
40|$|To {{determine}} whether inhaled beclomethasone, both at low and at high doses, inhibits late asthmatic reactions {{and the associated}} increase in airway responsiveness induced by toluene diisocyanate (TDI), we studied 9 sensitised subjects. Low dose beclomethasone (200 micrograms bid), high dose beclomethasone aerosol (1000 micrograms bid), and placebo were administered for 7 days before TDI inhalation challenge to each subject, according to a double-blind, crossover study design. The washout period between the treatments was at least 1 week. When the subjects were treated with placebo, forced expiratory volume in 1 sec (FEV 1) markedly decreased after exposure to TDI. By contrast, high dose beclomethasone prevented the late asthmatic reaction and the low dose partially inhibited the reaction. With placebo the mean (+/- SE) value of FEV 1 4 h after exposure to TDI was 2. 6 +/- 0. 17 L, which went to 3. 3 +/- 0. 12 after low dose beclomethasone, and to 3. 5 +/- 0. 15 L after high dose of beclomethasone (significant difference in the decrease of FEV 1 in the 8 h after exposure to TDI, between treatments: F = 9. 87, (P less than 0. 001), After treatment with placebo or with low dose beclomethasone, airway responsiveness to methacholine increased 8 h after exposure to TDI. With placebo, the PD 20 decreased from 0. 66 mg (<b>Geometric</b> <b>Standard</b> <b>Error</b> of the Mean [GSEM], 1. 38) to 0. 18 mg (GSEM, 1. 46); with low dose inhaled beclomethasone, the PD 20 decreased from 0. 93 mg (GSEM, 1. 42) to 0. 36 mg (GSEM, 1. 63) ...|$|E
40|$|Imperfections in {{manufacturing}} processes (such as spindle errors of machine tools, thermal expansion of workpiece, and tool wear) cause machined workpieces {{to deviate from}} their ideal designed geometry. The manufactured geometric errors are usually measured with coordinate metrology devices in terms of <b>standard</b> <b>geometric</b> <b>errors</b> such as straightness errors, roundness errors, etc. Although these errors may have some links t...|$|R
40|$|Exposure to methyl mercury, a {{risk factor}} for neurodevelopmental toxicity, was {{assessed}} in U. S. children 1 - 5 years of age (n = 838) and women 16 - 49 years of age (n = 1, 726) using hair mercury analysis during the 1999 - 2000 National Health and Nutrition Examination Survey (NHANES). The data are nationally representative and are based on analysis of cross-sectional data for the noninstitutionalized, U. S. household population. The survey consisted of interviews conducted in participants' homes and standardized health examinations conducted in mobile examination centers. Distributions of total hair mercury levels expressed as micrograms per gram hair Hg and the association of hair Hg levels with sociodemographic characteristics and fish consumption are reported. <b>Geometric</b> mean (<b>standard</b> <b>error</b> of the <b>geometric</b> mean) hair mercury was 0. 12 microg/g (0. 01 microg/g) in children, and 0. 20 microg/g (0. 02 microg/g) in women. Among frequent fish consumers, geometric mean hair mercury levels were 3 -fold higher for women (0. 38 vs. 0. 11 micro g/g) and 2 -fold higher for children (0. 16 vs. 0. 08 microg/g) compared with nonconsumers. The NHANES 1999 - 2000 data provide population-based data on hair mercury concentrations for women and children in the United States. Hair mercury levels were associated with age and fish consumption frequency...|$|R
40|$|Pedotransfer {{functions}} (PTFs) are an {{easy way}} to predict saturated hydraulic conductivity (Ksat) without measurements. This study aims to auto calibrate 22 PTFs. The PTFs were divided into three groups according to its input requirements and the shuffled complex evolution algorithm was used in calibration. The results showed great modification in the performance of the functions compared to the original published functions. For group 1 PTFs, the geometric mean error ratio (GMER) and the <b>geometric</b> <b>standard</b> deviation of <b>error</b> ratio (GSDER) values were modified from range (1. 27 – 6. 09), (5. 2 – 7. 01) to (0. 91 – 1. 15), (4. 88 – 5. 85) respectively. For group 2 PTFs, the GMER and the GSDER values were modified from (0. 3 – 1. 55), (5. 9 – 12. 38) to (1. 00 – 1. 03), (5. 5 – 5. 9) respectively. For group 3 PTFs, the GMER and the GSDER values were modified from (0. 11 – 2. 06), (5. 55 – 16. 42) to (0. 82 – 1. 01), (5. 1 – 6. 17) respectively. The result showed that the automatic calibration is an efficient and accurate method to enhance the performance of the PTFs...|$|R
40|$|Marine N 2 fixing microorganisms, termed diazotrophs, {{are a key}} {{functional}} group in marine pelagic ecosystems. The biological fixation of dinitrogen (N 2 ) to bioavailable nitrogen provides an important new source of nitrogen for pelagic marine ecosystems and influences primary productivity and organic matter export to the deep ocean. As {{one of a series}} of efforts to collect biomass and rates specific to different phytoplankton {{functional group}}s, we have constructed a database on diazotrophic organisms in the global pelagic upper ocean by compiling about 12 000 direct field measurements of cyanobacterial diazotroph abundances (based on microscopic cell counts or qPCR assays targeting the nifH genes) and N 2 fixation rates. Biomass conversion factors are estimated based on cell sizes to convert abundance data to diazotrophic biomass. The database is limited spatially, lacking large regions of the ocean especially in the Indian Ocean. The data are approximately log-normal distributed, and large variances exist in most sub-databases with non-zero values differing 5 to 8 orders of magnitude. Lower mean N 2 fixation rate was found in the North Atlantic Ocean than the Pacific Ocean. Reporting the geometric mean and the range of one <b>geometric</b> <b>standard</b> <b>error</b> below and above the geometric mean, the pelagic N 2 fixation rate in the global ocean is estimated to be 62 (53 – 73) Tg N yr− 1 and the pelagic diazotrophic biomass in the global ocean is estimated to be 4. 7 (2. 3 – 9. 6) Tg C from cell counts and to 89 (40 – 200) Tg C from nifH-based abundances. Uncertainties related to biomass conversion factors can change the estimate of geometric mean pelagic diazotrophic biomass in the global ocean by about ± 70 %. This evolving database can be used to study spatial and temporal distributions and variations of marine N 2 fixation, to validate geochemical estimates and to parameterize and validate biogeochemical models. The database is stored in PANGAEA (<a href="[URL]...|$|E
40|$|Marine N 2 fixing microorganisms, termed diazotrophs, {{are a key}} {{functional}} group in marine pelagic ecosystems. The biological fixation of dinitrogen (N 2) to bioavailable nitrogen provides an important new source of nitrogen for pelagic marine ecosystems and influences primary productivity and organic matter export to the deep ocean. As {{one of a series}} of efforts to collect biomass and rates specific to different phytoplankton {{functional group}}s, we have constructed a database on diazotrophic organisms in the global pelagic upper ocean by compiling about 12 000 direct field measurements of cyanobacterial diazotroph abundances (based on microscopic cell counts or qPCR assays targeting the nifH genes) and N 2 fixation rates. Biomass conversion factors are estimated based on cell sizes to convert abundance data to diazotrophic biomass. The database is limited spatially, lacking large regions of the ocean especially in the Indian Ocean. The data are approximately log-normal distributed, and large variances exist in most sub-databases with non-zero values differing 5 to 8 orders of magnitude. Reporting the geometric mean and the range of one <b>geometric</b> <b>standard</b> <b>error</b> below and above the geometric mean, the pelagic N 2 fixation rate in the global ocean is estimated to be 62 (52 – 73) Tg N yr− 1 and the pelagic diazotrophic biomass in the global ocean is estimated to be 2. 1 (1. 4 – 3. 1) Tg C from cell counts and to 89 (43 – 150) Tg C from nifH-based abundances. Reporting the arithmetic mean and one standard error instead, these three global estimates are 140 ± 9. 2 Tg N yr− 1, 18 ± 1. 8 Tg C and 590 ± 70 Tg C, respectively. Uncertainties related to biomass conversion factors can change the estimate of geometric mean pelagic diazotrophic biomass in the global ocean by about ± 70 %. It was recently established that the most commonly applied method used to measure N 2 fixation has underestimated the true rates. As a result, one can expect that future rate measurements will shift the mean N 2 fixation rate upward and may result in significantly higher estimates for the global N 2 fixation. The evolving database can nevertheless be used to study spatial and temporal distributions and variations of marine N 2 fixation, to validate geochemical estimates and to parameterize and validate biogeochemical models, keeping in mind that future rate measurements may rise in the future. The database is stored in PANGAEA (doi: 10. 1594 /PANGAEA. 774851) ...|$|E
40|$|Large sample <b>standard</b> <b>errors</b> {{are derived}} for the Tucker linear test score equating method under the common item nonequivalent populations design. <b>Standard</b> <b>errors</b> are derived without the {{normality}} assumption that is commonly {{made in the}} derivation of <b>standard</b> <b>errors</b> of linear equating. The behavior of the <b>standard</b> <b>errors</b> is studied using a computer simulation and a real data example. In the simulation, the derived <b>standard</b> <b>errors</b> were reasonably accurate. In the real data example, the derived <b>standard</b> <b>errors</b> agreed closely with <b>standard</b> <b>errors</b> estimated using Efron’s (1982) bootstrap...|$|R
50|$|The {{topic of}} heteroscedasticity-consistent (HC) <b>standard</b> <b>errors</b> arises in {{statistics}} and econometrics {{in the context}} of linear regression as well as time series analysis. These are also known as Eicker-Huber-White <b>standard</b> <b>errors</b> (also Huber-White <b>standard</b> <b>errors</b> or White <b>standard</b> <b>errors),</b> to recognize the contributions of Friedhelm Eicker, Peter J. Huber, and Halbert White.|$|R
40|$|A {{regression}} estimator {{is said to}} be robust if it {{is still}} reliable in the presence of outliers. On the other hand, its <b>standard</b> <b>error</b> {{is said to be}} robust if it is still reliable when the regression errors are autocorrelated and/or heteroskedastic. This paper shows how robust <b>standard</b> <b>errors</b> can be computed for several robust estimators of regression, including MMestimators. The improvement relative to non-robust <b>standard</b> <b>errors</b> is illustrated by means of large-sample bias calculations, simulations, and a real data example. It turns out that non-robust <b>standard</b> <b>errors</b> of robust estimators may be severely biased. However, if autocorrelation and heteroscedasticity are absent, non-robust <b>standard</b> <b>errors</b> are more e. cient than the robust <b>standard</b> <b>errors</b> that we propose. We therefore also present a test of the hypothesis that the robust and non-robust <b>standard</b> <b>errors</b> have the same probability limit. robust regression, robust <b>standard</b> <b>errors,</b> autocorrelation, heteroskedasticity...|$|R
5000|$|... σg: <b>Geometric</b> <b>standard</b> deviation. This {{value is}} {{determined}} mathematically by the equation: ...|$|R
25|$|<b>Standard</b> <b>error</b> of {{regression}} is {{an estimate}} of σ, <b>standard</b> <b>error</b> of the error term.|$|R
3000|$|... <b>standard</b> <b>errors</b> {{for these}} models through the {{combination}} of separation regression <b>standard</b> <b>error</b> estimates: [...]...|$|R
5000|$|<b>Standard</b> <b>error</b> of {{regression}} is {{an estimate}} of σ, <b>standard</b> <b>error</b> of the error term.|$|R
5000|$|Heteroscedasticity-{{consistent}} <b>standard</b> <b>errors</b> (HCSE), {{while still}} biased, improve upon OLS estimates. [...] HCSE is a consistent estimator of <b>standard</b> <b>errors</b> in regression models with heteroscedasticity. This method corrects for heteroscedasticity without altering {{the values of}} the coefficients. This method may be superior to regular OLS because if heteroscedasticity is present it corrects for it, however, if the data is homoscedastic, the <b>standard</b> <b>errors</b> are equivalent to conventional <b>standard</b> <b>errors</b> estimated by OLS. Several modifications of the White method of computing heteroscedasticity-consistent <b>standard</b> <b>errors</b> have been proposed as corrections with superior finite sample properties.|$|R
50|$|The {{error of}} E(D) is the <b>standard</b> <b>error</b> of {{experimentally}} calculated E(D) {{values and the}} error of Var(D) is the <b>standard</b> <b>error</b> of experimentally calculated Var(D) values. These <b>standard</b> <b>error</b> values can be estimated using theoretical models or resampling methods (bootstrapping, jackknifing).|$|R
5000|$|LogNormal6(m,σg) with median, m, and <b>geometric</b> <b>standard</b> deviation, σg, both on {{the natural}} scale ...|$|R
40|$|Contains <b>standard</b> <b>errors</b> for {{estimates}} of vehicle trips, person trips, person miles, and household demographic characteristics; <b>standard</b> <b>errors</b> for percentages of vehicle trips and miles, person trips and miles; and <b>standard</b> <b>errors</b> of means for average annual vehicle trips, vehicle miles, person trips, trip duration and trip distance for the 1977 NPTS. "December 1982. ""Office of Highway Planning" [...] Cover. Prepared by Ruth H. Asin. Final report. Contains <b>standard</b> <b>errors</b> for {{estimates of}} vehicle trips, person trips, person miles, and household demographic characteristics; <b>standard</b> <b>errors</b> for percentages of vehicle trips and miles, person trips and miles; and <b>standard</b> <b>errors</b> of means for average annual vehicle trips, vehicle miles, person trips, trip duration and trip distance for the 1977 NPTS. Mode of access: Internet...|$|R
30|$|At-risk {{drinking}} and infection {{were not found}} to be independent predictors of circulating B lymphocytes, cytotoxic T lymphocytes, and CD 16 – monocytes after multiregression analysis. On the other hand, both at-risk drinking (β-coefficient[*]=[*]− 0.174, <b>standard</b> <b>error</b> of β-coefficient[*]=[*] 0.07, P[*]=[*] 0.01) and infection (β-coefficient[*]=[*]− 0.167, <b>standard</b> <b>error</b> of β-coefficient[*]=[*] 0.06, P[*]=[*] 0.01) were independently associated with noncytotoxic lymphocyte counts but not previous treatment with antibiotics (β-coefficient[*]=[*]− 0.161, <b>standard</b> <b>error</b> of β-coefficient[*]=[*] 0.11, P[*]=[*] 0.17), current smoking (β-coefficient[*]=[*]− 0.154, <b>standard</b> <b>error</b> of β-coefficient[*]=[*] 0.2, P[*]=[*] 0.24), and poor dental state (β-coefficient[*]=[*]− 0.174, <b>standard</b> <b>error</b> of β-coefficient[*]=[*] 0.12, P[*]=[*] 0.15).|$|R
30|$|We opt for {{a linear}} {{specification}} of the retirement equations and adjust the <b>standard</b> <b>error</b> by estimating robust <b>standard</b> <b>error.</b>|$|R
30|$|For annual {{international}} migration we cluster <b>standard</b> <b>errors</b> at the municipal level. We do not cluster <b>standard</b> <b>errors</b> for the state-level models {{as there are}} only 32 states, below the number needed for the clustered <b>standard</b> <b>errors</b> to be unbiased (Angrist and Pischke 2009, Kezdi 2004).|$|R
30|$|Conventionally, <b>standard</b> <b>errors</b> of {{propensity}} score matching estimates are obtained using bootstrap methods, but with large samples {{such as those}} available to this study, it is not feasible to calculate bootstrap <b>standard</b> <b>errors</b> for all estimates. We have chosen to report conditional <b>standard</b> <b>errors</b> using methods recommended by Imbens and Wooldridge (2008). We undertook limited comparisons with bootstrap <b>standard</b> <b>errors</b> and found similar results. Additional details {{are included in the}} Additional file 1.|$|R
40|$|Presents initial ONS {{estimates}} of <b>standard</b> <b>errors</b> for two growth rate {{measures of the}} gross sector output PPI. The calculation of <b>standard</b> <b>errors</b> for the output producer price index (PPI) has been investigated {{with the aim of}} measuring the quality of the growth rates of the published price indices. This article presents, for the first time, Office for National Statistics? (ONS) {{estimates of}} the <b>standard</b> <b>errors</b> for month-on-month and 12 -month growth rates of the gross sector output PPI. It provides an account of the initial investigation of <b>standard</b> <b>errors</b> within the PPI context and explains: the PPI structure and describes the index types for which <b>standard</b> <b>errors</b> have been calculated; what <b>standard</b> <b>errors</b> are and how they can be interpreted; the new method of calculation of <b>standard</b> <b>errors</b> for the PPI; the main findings from the analysis {{within the context of the}} PPI structure; and the publication policy. Economic & Labour Market Review (2007) 1, 31 – 35; doi: 10. 1057 /palgrave. elmr. 1410154...|$|R
40|$|The {{relationship}} between total <b>error,</b> bias, and <b>standard</b> <b>error</b> of vertically scaled tests {{were examined in}} two simulated conditions [...] an ideal Item Response Theory (IRT) fit condition and a condition of IRT model misfit which was intended to approximate the type of misfit observed in operational data. Analytical estimates of <b>standard</b> <b>error</b> using an IRT information function and empirical estimates of <b>standard</b> <b>error</b> from a bootstrap re-sampling method were compared. A sufficient number of bootstrap re-samples required to yield {{the same degree of}} accuracy as 2000 re-samples was explored. ^ Analytical estimates of <b>standard</b> <b>error</b> were found to over-estimate <b>standard</b> <b>error</b> of vertical scale scores. The bootstrap method yielded more accurate estimates of the <b>standard</b> <b>error</b> as evidenced by the width of the confidence intervals for the proficiency levels and the coverage probabilities. Finally, a bootstrap resampling level of around 1000 was found to estimate <b>standard</b> <b>errors</b> with similar precision as compared to using 2000 re-samples. ...|$|R
30|$|The <b>Standard</b> <b>error</b> values varied for {{different}} rheological models and {{were found to}} depend on several parameters like graphene/cement ratio, shear rate, resting time and test geometry. For all rheological models, generally, lower <b>standard</b> <b>error</b> values were found for concentric cylinders when compared to parallel plates. For both geometries (cylinders and parallel plates), HB model with lowest <b>standard</b> <b>error</b> value {{was found in the}} best-fitted model, while, Casson model estimated the most scattered and higher average <b>standard</b> <b>error</b> values.|$|R
40|$|A new Stata command, simsum, {{analyzes}} {{data from}} simulation studies. The data may comprise point estimates and <b>standard</b> <b>errors</b> from several analysis methods, possibly resulting from several different simulation settings. simsum can report bias, coverage, power, empirical <b>standard</b> <b>error,</b> relative precision, average model-based <b>standard</b> <b>error,</b> {{and the relative}} <b>error</b> of the <b>standard</b> <b>error.</b> Monte Carlo errors are available {{for all of these}} estimated quantities. Copyright 2010 by StataCorp LP. simsum, simulation, Monte Carlo error, normal approximation, sandwich variance...|$|R
40|$|A {{formula is}} derived for the {{asymptotic}} <b>standard</b> <b>error</b> of a true-score equating by item response theory. The equating method is applicable {{when the two}} tests to be equated are administered to different groups along with an anchor test. Numerical <b>standard</b> <b>errors</b> are shown for an actual equating (1) comparing the <b>standard</b> <b>errors</b> of IRT, linear, e and equipercentile methods and (2) illustrating {{the effect of the}} length of the anchor test on the <b>standard</b> <b>error</b> of the equating...|$|R
40|$|This paper {{shows how}} to compute {{asymptotic}} <b>standard</b> <b>errors</b> of the characteristic roots of a nonlinear econometric model. The system of simultaneous equations is linearized {{in the neighborhood}} of a given point, then characteristic roots and related <b>standard</b> <b>errors</b> are computed. Nonlinear econometric models; characteristic roots; eigenvalues; asymptotic <b>standard</b> <b>errors...</b>|$|R
40|$|Abstract. This paper proposes (apparently) novel <b>standard</b> <b>error</b> {{formulas}} for the density-weighted average derivative estimator of Powell, Stock, and Stoker (1989). Asymptotic {{validity of}} the <b>standard</b> <b>errors</b> developed in this paper {{does not require the}} use of higher-order kernels and the <b>standard</b> <b>errors</b> are “robust ” {{in the sense that they}} accommodate (but do not require) bandwidths that are smaller than those for which conventional <b>standard</b> <b>errors</b> are valid. Moreover, the results of a Monte Carlo experiment suggest that th...|$|R
5000|$|As {{an example}} of the use of the {{relative}} <b>standard</b> <b>error,</b> consider two surveys of household income that both result in a sample mean of $50,000. If one survey has a <b>standard</b> <b>error</b> of $10,000 and the other has a <b>standard</b> <b>error</b> of $5,000, then the relative <b>standard</b> <b>errors</b> are 20% and 10% respectively. The survey with the lower relative <b>standard</b> <b>error</b> can be said to have a more precise measurement, since it has proportionately less sampling variation around the mean. In fact, data organizations often set reliability standards that their data must reach before publication. For example, the U.S. National Center for Health Statistics typically does not report an estimated mean if its relative <b>standard</b> <b>error</b> exceeds 30%. (NCHS also typically requires at least 30 observations - if not more - for an estimate to be reported.) ...|$|R
30|$|We {{compared}} the <b>standard</b> <b>errors</b> {{of the cut}} scores without applying weights and with applying weights. The cut scores with smaller <b>standard</b> <b>errors</b> were chosen.|$|R
3000|$|... 10 The <b>standard</b> <b>errors</b> for the metro-level {{estimates}} are comparable to, {{and in many}} cases smaller than, the clustered <b>standard</b> <b>errors</b> for the microdata regressions.|$|R
3000|$|... 9 All {{presented}} models include robust <b>standard</b> <b>errors.</b> The reviewer {{raised the}} issue that due to the inclusion of national-level variables, clustered <b>standard</b> <b>errors</b> should be used. The author checked this point and also finds significant effects for the individual-level variables when clustered <b>standard</b> <b>errors</b> are used. The estimation results are available in Table 7.|$|R
40|$|This paper proposes (apparently) novel <b>standard</b> <b>error</b> {{formulas}} for the density-weighted average derivative estimator of Powell, Stock, and Stoker (1989). Asymptotic {{validity of}} the <b>standard</b> <b>errors</b> developed in this paper {{does not require the}} use of higher-order kernels and the <b>standard</b> <b>errors</b> are "robust" {{in the sense that they}} accommodate (but do not require) bandwidths that are smaller than those for which conventional <b>standard</b> <b>errors</b> are valid. Moreover, the results of a Monte Carlo experiment suggest that the finite sample coverage rates of confidence intervals constructed using the <b>standard</b> <b>errors</b> developed in this paper coincide (approximately) with the nominal coverage rates across a nontrivial range of bandwidths. Semiparametric estimation, density-weighted average derivatives...|$|R
3000|$|... 10 These <b>standard</b> <b>errors</b> are robust to {{clustering}} at {{the household}} level. Accounting for clustering has {{little effect on}} the <b>standard</b> <b>errors</b> in the linear model.|$|R
