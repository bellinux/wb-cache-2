128|72|Public
5000|$|ID5R (1989) output {{the same}} tree as ID3 for a dataset {{regardless}} of the <b>incremental</b> <b>training</b> order. This was accomplished by recursively updating the tree's subnodes. It did not handle numeric variables, multiclass classification tasks, or missing values.|$|E
50|$|On March 1, 2011, the {{battalion}} deployed to Okinawa {{and served as}} a quick reaction force in support of Operation Enduring Freedom. The unit was attached to the 4th Marine Regiment as Alpha Company. 2/23 deployed all over Southeast Asia and participated in the Korean <b>Incremental</b> <b>Training</b> Program. The unit returned home in September 2011.|$|E
40|$|Abstract — We {{propose a}} new {{algorithm}} for the <b>incremental</b> <b>training</b> of Support Vector Machines (SVMs) that {{is suitable for}} problems of sequentially arriving data and fast constraint parameter variation. Our method involves using a “warm-start” algorithm for the training of Support Vector Machines (SVMs), which allows us {{to take advantage of}} the natural incremental properties of the standard active set approach to linearly constrained optimisation problems. <b>Incremental</b> <b>training</b> involves quickly re-training a support vector machine after adding a small number of additional training vectors to the training set of an existing (trained) support vector machine. Similarly, the problem of fast constraint parameter variation involves quickly re-training an existing support vector machine using the same training set but different constraint parameters. In both cases we demonstrate the computational superiority of <b>incremental</b> <b>training</b> over the usual batch re-training method...|$|E
50|$|GE Transportation Systems' <b>Incremental</b> <b>Train</b> Control System (ITCS) is {{installed}} on Amtrak's Michigan line, allowing trains to travel at 110 mph.|$|R
50|$|In November 1995, all trains {{began using}} 4-5 Amtrak Superliners and Hi-Level cars, pulled by Via F40PH {{locomotives}}. The F40PH locomotives were not {{compatible with the}} newly installed <b>Incremental</b> <b>Train</b> Control System; they were replaced with Amtrak P32-8WH locomotives late in 1999. Horizon Fleet were used beginning in 2000 to allow the Superliners to add capacity to western trains, and Genesis locomotives {{were used in the}} final years.|$|R
50|$|In 2002, {{the section}} from Porter to Kalamazoo {{became the first}} {{passenger}} rail line in the United States to have positive train control (PTC) technology installed, specifically GE Transportation Systems' <b>Incremental</b> <b>Train</b> Control System (ITCS). In 2005, Amtrak received approval from the Federal Railroad Administration to run trains at up to 95 mph Most Amtrak trains outside of the Northeast are limited to 79 mph due to federal regulations. Regular service at 110 mph began from Porter to Kalamazoo on February 15, 2012.|$|R
40|$|Keywords:SVM, <b>incremental</b> <b>training,</b> support vector Abstract. In {{order to}} figure out the {{deficiency}} of the SVM on extensive sample, nature of SV is studied in this paper. An improved <b>incremental</b> <b>training</b> algorithm is put forward based on dimensional of samples. A chosen gene which got by density and distance criterion is used in this method. In this method the number of training samples is decreased and the space information is keeped. So, the training speed is improved while the precision is not reduced. And the simulation proved the efficiency of this method. ...|$|E
40|$|This paper {{investigates the}} <b>incremental</b> <b>training</b> of a Neural Network (NN) with the input {{attributes}} introduced in order. A specially designed NN {{is used to}} evaluate the individual discrimination ability of each input attribute. Attributes are then sorted in descending, ascending, and random orders of their individual discrimination abilities and introduced into another NN being trained with an <b>incremental</b> <b>training</b> algorithm, ITID. To reduce the inter-ference caused by irrelevant features and high-complexity tasks, only relevant features are involved and tasks are decomposed in the experiments. The experimental results of several benchmark problems show that descending order obtains the highest generalization accuracy among the three training orders for both classification and regression problems...|$|E
40|$|Abstract—In this paper, a {{supervised}} {{neural network}} training technique based on constrained optimization is developed for preserving {{prior knowledge of}} an input–output mapping during repeated <b>incremental</b> <b>training</b> sessions. The prior knowledge, referred to as long-term memory (LTM), is expressed {{in the form of}} equality constraints obtained by means of an algebraic training technique. <b>Incremental</b> <b>training,</b> which may be used to learn new short-term memories (STMs) online, is then formulated as an error minimization problem subject to equality constraints. The solution of this problem is simplified by implementing an adjoined error gradient that circumvents direct substitution and exploits classical backpropagation. A target application is neural network function approximation in adaptive critic designs. For illustrative purposes, constrained training is implemented to update an adaptive critic flight controller, while preserving prior knowledge of an established performance baseline that consists of classical gain-scheduled controllers. It is shown both analytically and numerically that the LTM is accurately preserved while the controller is repeatedly trained over time to assimilate new STMs. Index Terms—Adaptive critics, control, exploration, function approximation, <b>incremental</b> <b>training,</b> interference, knowledge acquisition and retention, memory, online learning, sigmoidal neural networks. I...|$|E
5000|$|Anthony R. Dickinson (born 1960) is a British academic, neuroscientist and a Scientific Advisor at the Beijing Genomics Institute. He specialises {{in brain}} {{development}} and <b>incremental</b> intelligence <b>training.</b> He invented and developed the first four-way implantable brain cannular implant device.|$|R
5000|$|... each Wolverine {{operates}} {{with two}} General Electric Genesis P42DC locomotives, 3-5 Horizon coaches, and an Amfleet cafe/business class car. In the winter, Superliners are sometimes used. The equipment pool for the Wolverines comprises 14 Horizon coaches and 3.5 Amfleet cafe/business class cars (one is {{shared with the}} Blue Water), split across three consists. The locomotives usually operate in a push-pull configuration, however sometimes both {{will be at the}} head end. Due to the FRA requirement of positive train control for operations above 79 mph, locomotives on the Wolverine are required to have Positive Train Control, supplied by Amtrak's <b>Incremental</b> <b>Train</b> Control System. Because of this modification the units are usually captive to the Michigan services.|$|R
40|$|A {{biological}} {{neural network}} is constituted by numerous subnetworks and modules with different functionalities. For an artificial neural network, {{the relationship between}} a network and its subnetworks is also important and useful for both theoretical and algorithmic research, i. e. it can be exploited to develop <b>incremental</b> network <b>training</b> algorithm or parallel network training algorithm. In this paper we explore {{the relationship between a}}n ELM neural network and its subnetworks. To the best of our knowledge, we are the first to prove a theorem that shows an ELM neural network can be scattered into subnetworks and its optimal solution can be constructed recursively by the optimal solutions of these subnetworks. Based on the theorem we also present two algorithms to train a large ELM neural network efficiently: one is a parallel network training algorithm and the other is an <b>incremental</b> network <b>training</b> algorithm. The experimental results demonstrate the usefulness of the theorem and the validity of the developed algorithms. Comment: 3 figure...|$|R
30|$|The online dataset is the {{complementary}} {{part of the}} training data set. After every optimization round, the collected dataset entry is appended to the latest training dataset. Accordingly, the ANN is trained with an <b>incremental</b> <b>training</b> dataset, increasing in size after each optimization round. This assures an adaptive behavior of the proposed solution.|$|E
40|$|How {{to use the}} <b>incremental</b> <b>training</b> corpus {{to improve}} the {{question}} classification accuracy rate {{in the process of}} question classification based on statistic learning. A question classification method based on the incremental modified Bayes was presented in this paper. The method used the modified Bayes and combined the incremental learning to correct the parameter by the <b>incremental</b> <b>training</b> set stage by stage, and established the question classification model based on the incremental modified Bayes. A question classification experiment was done in the domain of Yunnan tourism, the experimental results showed that the presented method evidently excelled than the modified Bayes method in the accuracy rate and the training time, the average accuracy rate was improved 3. 3 percentage points than the accuracy rate of the modified Bayes method; the average training time was improved 39. 1 percentage points than the training time efficiency of the modified Bayes method. 1...|$|E
40|$|<b>Incremental</b> <b>training</b> {{has been}} used for GA-based {{classifiers}} in a dynamic environment where training samples or new attributes/classes become available over time. In this paper, ordered incremental genetic algorithms (OIGAs) are proposed to address the <b>incremental</b> <b>training</b> of input attributes for classifiers. Rather than learning input attributes in batch as with normal GAs, OIGAs learn input attributes one after another. The resulting classification rule sets are also evolved incrementally to accommodate the new attributes. Furthermore, attributes are arranged in different orders by evaluating their individual discriminating ability. By experimenting with different attribute orders, different approaches of OIGAs are evaluated using four benchmark classification data sets. Their performance is also compared with normal GAs. The simulation results show that OIGAs can achieve generally better performance than normal GAs. The order of attributes does {{have an effect on the}} final classifier performance where OIGA training with a descending order of attributes performs the best...|$|E
40|$|Support vector {{machine is}} a popular method in machine learning. Incremental support vector machine {{algorithm}} is ideal selection {{in the face of}} large learning data set. In this paper a new incremental support vector machine learning algorithm is proposed to improve efficiency of large scale data processing. The model of this incremental learning algorithm is similar to the standard support vector machine. The goal concept is updated by <b>incremental</b> learning. Each <b>training</b> procedure only includes new training data. The time complexity is independent of whole training set. Compared with the other <b>incremental</b> version, the <b>training</b> speed of this approach is improved and the change of hyperplane is reduced. </p...|$|R
40|$|Neural {{substrates}} of visuomotor lear {{control and}} prediction orn and iversi r 200 cerebellum. The results demonstrate that <b>incremental,</b> learning-context of <b>training</b> under single and dual tasks (Grafton et al., 1995, 1998; Hazeltine et al., 1997). A second {{approach is to}} identify changes of network dynamics, rather than local BOLD magnitude...|$|R
40|$|This paper {{discusses}} several {{lessons learned}} from the SpamTREC 2006 challenge. We discuss issues related to decoding, preprocessing, and tokenization of email messages. Using the Winnow algorithm with orthogonal sparse bigram features, we construct an efficient, highly scalable <b>incremental</b> classifier, <b>trained</b> to maximize a discriminative optimization criterion. The algorithm easily scales to millions of training messages and millions of features. We address the composition of training corpora and discuss experiments that guide the construction of our SpamTREC entry. We describe our submission for the filtering tasks with periodical re-training and active learning strategies, and report on the evaluation on the publicly available corpora...|$|R
40|$|Feature {{selection}} {{plays an}} important role in classification systems. Using classifier error rate as the evaluation function, feature selection is integrated with <b>incremental</b> <b>training.</b> A neural network classifier is implemented with an <b>incremental</b> <b>training</b> approach to detect and discard irrelevant features. By learning attributes one after another, our classifier can find directly the attributes that make no contribution to classification. These attributes are marked and considered for removal. Incorporated with a Minimum Squared Error (MSE) based feature ranking scheme, four batch removal methods based on classifier error rate have been developed to discard irrelevant features. These feature selection methods reduce the computational complexity involved in searching among a large number of possible solutions significantly. Experimental results show that our feature selection methods work well on several benchmark problems compared with other feature selection methods. The selected subsets are further validated by a Constructive Backpropagation (CBP) classifier, which confirms increased classification accuracy and reduced training cost...|$|E
40|$|This project {{discovers the}} {{implementation}} of Artificial Neural Network (ANN) for forecasting weather based on past relevant data. Neural network is constructed using empirical network architecture and (17) training types. They are such as BFGS quasi-Newton backpropagation, Cyclical order <b>incremental</b> <b>training</b> w/learning functions, Levenberg-Marquardt backpropagation, Resilient backpropagation and others. The ANN has been trained using 2008 weather data and tested with data year 2009. As result, the system has successfully generating accuracy up to 78. 69 % for quantitative precipitation (QP) prediction. Analysis on time consumption of all those training types is made and shows that Resilient backpropagation with 1. 92 s training time consumption is the fastest and Cyclical order <b>incremental</b> <b>training</b> w/learning functions with 463. 215 s is the slowest. This project concluded that ANN is an alternative method in controlling and understanding the way of non-linear set of data and variables to become mutually correlated with each other. It is a powerful yet significant method in embedding intelligent system into application for meteorological tools...|$|E
40|$|Abstract — We prove tight bounds on {{the risk}} of models in the {{ensemble}} generated by <b>incremental</b> <b>training</b> of an arbitrary learning algorithm. Our result is based on proof techniques that are remarkably different from the standard risk analysis based on uniform convergence arguments, and improves on previous results published in [4]. Index Terms — Statistical learning theory, risk bounds, on-line learning, martingales. I...|$|E
40|$|This study {{investigated}} the effects of six weeks of normobaric hypoxic training on transcriptional expression of the genes associated with mitochondrial and glycolytic activities in Thoroughbred horses. Eight horses {{were divided into two}} groups of four. They completed an identical <b>incremental,</b> moderate intensity <b>training</b> program, except that one group trained in a hypoxic chamber with 15...|$|R
40|$|Considering {{the problem}} that {{stability}} of surface Electromyographic Signal (sEMG) based human-machine interface (HMI) gradually declines as fatigue takes place in muscles, we propose a novel method for updating samples to improve <b>incremental</b> online <b>training</b> algorithm for support vector machine (SVM). We study the changes of sEMG when muscle fatigue occurs using a method based on continuous wavelet transform, and then applies the improved incremental online SVM for sEMG classification. Experiment {{results show that the}} proposed algorithm can be used to improve the classification accuracy and training speed significantly. Furthermore, this method effectively diminish the influence of muscle fatigue during long-term operation of sEMG based HMI. © 2012 IEEE...|$|R
40|$|We {{present a}} fast {{iterative}} support vector training algorithm {{for a large}} variety of different formulations. It works by incrementally changing a candidate support vector set using a greedy approach, until the supporting hyperplane is found within {{a finite number of}} iterations. It is derived from a simple active set method which sweeps through the set of Lagrange multipliers and keeps optimality in the unconstrained variables, while discarding large amounts of bound-constrained variables. The hard-margin version {{can be viewed as a}} simple (yet computationally crucial) modification of the <b>incremental</b> SVM <b>training</b> algorithms of Cauwenberghs and Poggio. Experimental results for various settings are reported. In all cases our algorithm is considerably faster than competing methods such as Sequential Minimal Optimization or the Nearest Point Algorithm...|$|R
40|$|This paper {{presents}} expressions for {{the optimal}} step length {{to use when}} training a vector quantizer by stochastic approximation. By treating each update as an estimation problem it provides a unified framework covering both batch and <b>incremental</b> <b>training</b> which were previously treated separately and extends existing results to the semibatch case. In addition the new results presented here provide a measurable improvement over results which were previously thought to be optimal...|$|E
40|$|A {{key problem}} in spoken {{language}} identification (LID) {{is how to}} effectively model features from a given speech utterance. Recent techniques such as end-to-end schemes and deep neural networks (DNNs) utilising transfer learning such as bottleneck (BN) features, have demonstrated good overall performance, but have not addressed the extraction of LID-specific features. We thus propose a novel end-to-end neural network which aims to obtain effective LID-senone representations, which we define as being analogous to senones in speech recognition. We show that LID-senones combine a compact representation of the original acoustic feature space with a powerful descriptive and discriminative capability. Furthermore, a novel <b>incremental</b> <b>training</b> method is proposed to extract the weak language information buried in the acoustic features of insufficient language resources. Results on the six most confused languages in NIST LRE 2009 show good performance compared to state-of-the-art BN-GMM/i-vector and BN-DNN/i-vector systems. The proposed end-to-end network, coupled with an <b>incremental</b> <b>training</b> method which mitigates against over-fitting, has potential not just for LID, but also for other resource constrained tasks...|$|E
40|$|Abstract-This paper {{presents}} expressions for {{the optimal}} step length {{to use when}} training a vector quantizer by stochastic approximation. By treating each update as an estimation problem, it provides a unified framework covering both batch and <b>incremental</b> <b>training,</b> which were previously treated separately, and extends existing results to the semibatch case. In addition, the new results presented here provide a measurable improvement over results which were previously thought to be optimal. 1...|$|E
40|$|Online {{streaming}} {{companies such}} as Netflix have become dominant in the media distribution sector. However, such media delivery services often support very rudimentary search, especially for natural language queries. To provide a more natural search interface, we have developed a conversational movie search system, which parses the recognition hypothesis of a spoken query into semantic classes using conditional random fields (CRFs), and then searches an indexed database with the identified semantics. Topic modeling on user-generated content (e. g., movie reviews) is employed for query expansion. Thirteen searching schemas are supported (such as genre, plot, character and soundtrack search). A crowd-sourcing platform was utilized to automatically collect large-scale annotated data for <b>incremental</b> CRF <b>training.</b> Index Terms: conditional random fields, spoken dialogue system 1...|$|R
40|$|Adaptive {{information}} filtering is {{an emerging}} filtering technology that can learn the user interest/topic automatically during {{the filtering process}} and adjust its output accordingly. It provides a better performance and broader applicability than the traditional filtering technology, therefore is useful in Internet for managing sensitive information and presenting personalized content to Web user. In this {{paper we propose a}} new framework for online adaptive filtering, in which two different scoring/weighting and feedback mechanisms are implemented. Based on them, an <b>incremental</b> profile <b>training</b> method is introduced for locating user interest accurately, and a profile self-learning algorithm is also developed for adjusting user focus in test filtering. The experiments in the Reuters online news show our system performs better than the exist systems in the profile training and overall filtering results...|$|R
40|$|This paper {{presents}} an incremental learning procedure for improving generalization performance of multilayer feedforward networks. Whereas most existing algorithms. {{try to reduce}} the size of the network to improve the likelihood of finding solutions of good generalization, our method. constrains the search by <b>incremental</b> selection of <b>training</b> examples according to the estimated usefulness, called interestingness, of the example. It is shown in Boolean function tasks that the incremental algorithm achieves better generalization with smaller training sets than the nonincremental ones. ...|$|R
40|$|Approximation {{capabilities}} of single non-linear layer networks, that feature a single global minimum of the error function are addressed. Bases of different transfer functions are tested (Gaussian, sigmoidal, multiquadratics). These functions are orthogonalised in an incremental manner {{for training and}} restored {{back to the original}} basis for network deployment. Approximation results are given for a benchmark ECG signal. Results of <b>incremental</b> <b>training</b> with basis orthogonalisation are also shown for 2 D approximations. 1...|$|E
40|$|This paper explores an <b>incremental</b> <b>training</b> {{strategy}} for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives. Existing methods of neural word embeddings, including SGNS, are multi-pass algorithms and thus cannot perform incremental model update. To address this problem, we present a simple incremental extension of SGNS and provide a thorough theoretical analysis to demonstrate its validity. Empirical experiments demonstrated the correctness of the theoretical analysis {{as well as the}} practical usefulness of the incremental algorithm...|$|E
40|$|This unique text/reference {{describes}} {{in detail the}} latest advances in unsupervised process monitoring and fault diagnosis with machine learning methods. Abundant case studies throughout the text demonstrate the efficacy of each method in real-world settings. The broad coverage examines such cutting-edge topics {{as the use of}} information theory to enhance unsupervised learning in tree-based methods, the extension of kernel methods to multiple kernel learning for feature extraction from data, and the <b>incremental</b> <b>training</b> of multilayer perceptrons to construct deep architectures for enhanced dat...|$|E
50|$|Canadian Forces Training System {{provided}} {{training services}} to the operational commands. It operated 18 schools on five training bases and three schools on other commands' bases. Its strength was around 4.500 active troops, 2,400 of which were instructors. Another 500 military instructors from other commands served as <b>incremental</b> staff. The <b>training</b> system was underthe jurisdiction of the Assistant Deputy Minister (Personnel), whose mandate also included the National Defence College, the Military Colleges and the Staff Colleges. The Canadian forces also provided training facilities for allied nations.|$|R
40|$|EnMultivariate Additive PLS Splines, {{in short}} MAPLSS, are Partial Least-Squares models that study the {{dependence}} {{of a set of}} responses on spline transformations of the predictor variables which permit to capture additively non linear main effects and interactions. The aim {{of this paper is to}} present a way of selecting MAPLSS models through an adaptive <b>incremental</b> selection of <b>training</b> samples by a bootstrap procedure. This approach is attractive in the case of expensive data thus implying to construct efficient models based on small training data sets...|$|R
40|$|Extensive {{research}} activities {{have been observed}} on network-based intrusion detection systems (IDSs). However, there are always some attacks that penetrate trafficprofiling- based network IDSs. These attacks often cause very serious damages such as modifying host critical files. A host-based anomaly IDS is an effective complement to the network IDS in addressing this issue. This article proposes a simple data preprocessing approach to speed up a hidden Markov model (HMM) training for system-call-based anomaly intrusion detection. Experiments based on a public database demonstrate that this data preprocessing approach can reduce training time by up to 50 percent with unnoticeable intrusion detection performance degradation, compared to a conventional batch HMM training scheme. More than 58 percent data reduction has been observed compared to our prior <b>incremental</b> HMM <b>training</b> scheme. Although this maximum gain incurs more degradation of false alarm rate performance, the resulting performance is still reasonable...|$|R
