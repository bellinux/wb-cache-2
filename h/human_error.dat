3547|1828|Public
5|$|Car {{accidents}} are {{very common in}} the United States {{and the majority of}} these road crashes are caused by <b>human</b> <b>error.</b>|$|E
5|$|Human {{factors are}} the {{physical}} or cognitive properties of individuals, or social behaviour specific to humans, which influences functioning of technological systems {{as well as}} human-environment equilibrium. <b>Human</b> <b>error</b> is inevitable and everyone makes mistakes at some time, {{and the consequences of}} these errors are varied and depend on many factors. Most errors are minor and do not cause harm, but in a high risk environment, such as in diving, errors {{are more likely to have}} catastrophic consequences. Examples of <b>human</b> <b>error</b> leading to accidents are available in vast numbers, as it is the direct cause of 60% to 80% of all accidents. <b>Human</b> <b>error</b> and panic are considered to be the leading causes of diving accidents and fatalities. A study by William P. Morgan indicates that over half of all divers in the survey had experienced panic underwater at some time during their diving career, and these findings were independently corroborated by a survey that suggested 65% of recreational divers have panicked under water. Panic frequently leads to errors in a diver's judgement or performance, and may result in an accident. The safety of underwater diving operations can be improved by reducing the frequency of <b>human</b> <b>error</b> and the consequences when it does occur.|$|E
5|$|Much {{criticism}} {{focused on}} the role of <b>human</b> <b>error</b> in the collision; once the engineer had forgotten (or missed) the approach signal there was no automatic system to prevent the collision. The NTSB noted that had a positive train control system been in place on the Metropolitan Subdivision the collision would have been less likely: the system would have detected MARC #286's unauthorized speed and stopped the train. The NTSB also strongly criticized CSX and the Federal Transit Administration for the removal of a signal between Kensington and Georgetown Junction as part of capacity improvements on the Metropolitan Subdivision. The NTSB argued that having this signal after Kensington would have reduced the likelihood of the sort of <b>human</b> <b>error</b> which caused the crash, and in the NTSB's view CSX and the FTA did not properly assess the effects of removing the signal.|$|E
40|$|<b>Human</b> <b>errors</b> {{have been}} {{identified}} as the source of approximately 60 % of the incidents and accidents that occur in commercial aviation. It can be assumed that {{a very large number of}} <b>human</b> <b>errors</b> occur in aviation operations, even though in most cases the redundancies and diversities built into the design of aircraft systems prevent the errors from leading to serious consequences. In addition, when it is acknowledged that many system failures have their roots in <b>human</b> <b>errors</b> that occur in the design phase, it becomes apparent that the identification and elimination of potential <b>human</b> <b>errors</b> could significantly decrease the risks of aviation operations. This will become even more critical during the design of advanced automation-based aircraft systems as well as next-generation systems for air traffic management. Structured methods to identify and correct potential <b>human</b> <b>errors</b> in aviation operations have been developed and are currently undergoing testing at the Idaho National Engineering and Environmental Laboratory (INEEL) ...|$|R
40|$|We discuss factors or {{elements}} which play essential {{roles to}} cause <b>human</b> <b>errors</b> after {{the literature of}} "Shippai-gaku-no-susume". Although <b>human</b> <b>errors</b> can never be eliminated in principle, we may still avoid "fatal" errors by establishing some database through deep analysis of mistakes, errors, failures and accidents. ＜論説...|$|R
5000|$|... #Subtitle level 2: Reliability culture / <b>human</b> <b>errors</b> / <b>human</b> factors ...|$|R
5|$|Oil spills are {{recorded}} both {{in case of}} maritime routes and pipeline routes to the main refineries. Oil spills, amounting {{to as much as}} 7.56 billion liters of oil entering the oceans every year, occur due to damaged equipment or <b>human</b> <b>error.</b>|$|E
5|$|The Appalachian Trail is {{relatively}} safe. Most injuries or incidents {{are consistent with}} comparable outdoor activities. Most hazards are related to weather conditions, <b>human</b> <b>error,</b> plants, animals, diseases, and hostile humans encountered along the trail.|$|E
5|$|An enquiry {{into the}} crash was hastily set up. The commission's report (Aktenzeichen 52, Br.B.Nr. 270/42) {{concluded}} that the crash was caused by damage to the differential gear, which caused an oil leak. Then a number of teeth broke off the spur wheel and ignited the oil. Sabotage or <b>human</b> <b>error</b> was ruled out. The aircraft, W. Nr. 14256, was ferried to the unit via Bari, Italy. The mission that ended in its destruction was its first mission.|$|E
40|$|The role {{of human}} being {{as a part}} of a {{measuring}} system in a chemical analytical laboratory is discussed. It is argued that a measuring system in chemical analysis includes not only measuring instruments and other devices, reagents and supplies, but also a sampling inspector and/or analyst performing a number of important operations. Without this human contribution, a measurement cannot be carried out. <b>Human</b> <b>errors,</b> therefore, influence the measurement result, i. e., the measurand estimate and the associated uncertainty. Consequently, chemical analytical and metrological communities should devote more attention to the topic of <b>human</b> <b>errors,</b> in particular at the design and development of a chemical analytical/test method and measurement procedure. Also, mapping <b>human</b> <b>errors</b> ought to be included in the program of validation of the measurement procedure (method). Teaching specialists in analytical chemistry and students how to reduce <b>human</b> <b>errors</b> in a chemical analytical laboratory and how to take into account the error residual risk, is important. <b>Human</b> <b>errors</b> and their metrological implications are suggested for consideration in future editions of the relevant documents, such as the International Vocabulary of Metrology (VIM) and the Guide to the Expression of Uncertainty in Measurement (GUM) ...|$|R
5000|$|Maximize use of {{automation}} {{to enforce}} security controls, thereby negating <b>human</b> <b>errors,</b> and ...|$|R
30|$|We {{carried out}} manual {{analysis}} {{to verify the}} correctness of the clone detection using semi-automated tools/manual. The manual assessment can be subject to <b>human</b> <b>errors.</b> However, all the participants of this work are graduate students carrying out projects {{in the area of}} software clones. Thus we trust that each one has agent expertise to keep the plausible <b>human</b> <b>errors</b> to the minimum.|$|R
5|$|However, {{safety issues}} {{continue}} to be a persistent problem in Indonesian aviation. Several accidents have given Indonesia's air transport system the reputation of the least safe in the world. Indonesian aviation faces numerous challenges, including poorly maintained, outdated, and often overwhelmed infrastructure, the factor of <b>human</b> <b>error,</b> bad weather, haze problems caused by plantation fires, and volcanic ash spewed by numerous area volcanoes that disrupts air transportation.|$|E
5|$|The {{pipeline}} has {{at times}} been damaged due to sabotage, <b>human</b> <b>error,</b> maintenance failures, and natural disasters. By law, Alyeska {{is required to}} report significant oil spills to regulatory authorities. The Exxon Valdez oil spill is the best-known accident involving Alaska oil, {{but it did not}} involve the pipeline itself. Following the spill, Alyeska created a rapid response force that is paid for by the oil companies, including ExxonMobil, which was found liable for the spill.|$|E
5|$|Nucleosidic solid supports. In a {{historically}} first and still popular approach, the 3'-hydroxy {{group of the}} 3'-terminal nucleoside residue {{is attached to the}} solid support via, most often, 3’-O-succinyl arm as in compound 3. The oligonucleotide chain assembly starts with the coupling of a phosphoramidite building block respective to the nucleotide residue second from the 3’-terminus. The 3’-terminal hydroxy group in oligonucleotides synthesized on nucleosidic solid supports is deprotected under the conditions somewhat milder than those applicable for universal solid supports. However, the fact that a nucleosidic solid support has to be selected in a sequence-specific manner reduces the throughput of the entire synthetic process and increases the likelihood of <b>human</b> <b>error.</b>|$|E
40|$|INTRODUCTION: Studying <b>human</b> <b>errors</b> as a {{risk factor}} in the {{occurrence}} of accidents is necessary. Thus, {{the aim of this}} study was to identify, predict and control <b>human</b> <b>errors</b> in industrial control units. METHOD: This is a case study carried out using SHERPA in the first unit of Zagros Methanol of Asalooyeh, Iran, and its subunits. To collect the required data, various methods were used: observing, interviewing processing specialists and control unit operators, and studying technical documents and records. RESULTS: In total, 222 <b>human</b> <b>errors</b> were identified in various occupational tasks. This study showed that 48. 62 % of them were action errors, 31. 97 % were checking errors, 6. 75 % were retrieval errors, 11. 70 % were communication errors and 0. 90 % were selection errors. CONCLUSION: It can be inferred that this method is appropriate for different industries, and it is useful for identifying <b>human</b> <b>errors</b> leading to hazardous accidents...|$|R
40|$|The {{operation}} {{environment of}} main control rooms (MCRs) in modern {{nuclear power plants}} (NPPs) has considerably changed over the years. Advanced MCRs, which have been designed by adapting digital and computer technologies, have simpler interfaces using large display panels, computerized displays, soft controls, computerized procedure systems, and so on. The actions for the NPP operations are performed using soft controls in advanced MCRs. Soft controls have different features from conventional controls. Operators need to navigate the screens to find indicators and controls and manipulate controls using a mouse, touch screens, and so on. Due to these different interfaces, different <b>human</b> <b>errors</b> {{should be considered in}} the human reliability analysis (HRA) for advanced MCRs. In this work, <b>human</b> <b>errors</b> that could occur during operation executions using soft controls were analyzed. This work classified the <b>human</b> <b>errors</b> in soft controls into six types, and the reasons that affect the occurrence of the <b>human</b> <b>errors</b> were also analyzedclos...|$|R
30|$|Through {{a simple}} case, we will analyze {{the impact of}} <b>human</b> <b>errors</b> on {{maintenance}} availability.|$|R
5|$|Originally, the {{aircraft}} was to conduct flight tests at Gostomel {{for three months}} before transferring to the Gromov Flight Research Institute in Zhukovsky, near Moscow. However, the first prototype was lost when it was making its fourth flight on 10 February 1995. During the flight, {{the aircraft}} suffered a sudden deviation from its intended flightpath and collided with the An-72 chase plane before spiralling into the ground, erupting into flames; the test crew of seven were killed. Although there were initial allegations of technical issues with the aircraft, it was later determined that the crash had been caused by <b>human</b> <b>error.</b>|$|E
5|$|R2 {{was first}} {{announced}} in the April 2007 edition of Newtype. Early screening for the first episode was held in March 2008 in Tokyo Dome City and Osaka Mido Hall. The series premiered on April 6, 2008 on Mainichi Broadcasting System and Tokyo Broadcasting System; it was later broadcast on sixteen other stations. The third episode was partly leaked four days before its intended air date due to <b>human</b> <b>error.</b> The final episode aired on September 28, 2008. Bandai Visual encapsulated the series into nine volumes in DVD, Blu-ray, and Universal Media Disc formats; each volume contained a picture drama episode as a bonus. Bandai Visual later released a singular adaption of the series called Zero Requiem, and later released the series in a box collection.|$|E
5|$|Taiwan (Republic of China) {{reached an}} {{agreement}} with the U.S. to purchase 30 AH-64Es with weapons, and associated equipment in June 2011. On 5 November 2013, Taiwan received the first 6 AH-64s. On 25 April 2014, a Taiwanese AH-64E crashed into a three-story building during a training flight in bad weather conditions, the first airframe loss of an AH-64E. An investigation ruled out mechanical failure and concluded <b>human</b> <b>error</b> as responsible, that the pilots descended too fast through clouds at low altitude without checking flight panels to maintain adequate height; the Army responded by stepping up simulator training for pilots. In October 2014, the fifth and final batch of AH-64Es was delivered to Taiwan, completing the order.|$|E
5000|$|The {{outcomes}} of the <b>human</b> <b>errors</b> under consideration are constrained by previously defined sequences of PSA accidents ...|$|R
40|$|Abstract The main {{objective}} of this dissertation is to develop, validate and evaluate an error management taxonomy {{to be used in}} the analysis of error events in the area of Air Traffic Management (ATM). The goal of the taxonomy is to be able to analyse the mechanisms behind <b>human</b> <b>errors</b> and their recovery. Currently an abundance of taxonomies exist to describe the mechanisms behind <b>human</b> <b>errors</b> whereas very little is known about the mechanisms underlying error detection and recovery. This is unfortunate since timely and effective interventions can often prohibit errors from having serious consequences on system safety. The goal of the present Ph. D. project is therefore to gain more knowledge about how errors are captured. One of the desired outcomes of this project is to provide a basis for reinforcing incident prevention strategies. To do so, it is important to have a structured classification scheme (a taxonomy) in which operational data about the production, detection and recovery of <b>human</b> <b>errors</b> can be categorised including the underlying circumstances behind these <b>human</b> <b>errors</b> and their capture...|$|R
5000|$|... {{navigation}} <b>errors</b> {{and other}} <b>human</b> <b>errors,</b> leading to collisions (with another ship, rocks, an iceberg, etc.) or running aground ...|$|R
5|$|The report {{suggests}} two hypotheses for the accident. In the first, {{there was}} a flaw in the power control of the plane's engines, which would have kept one of the thrust levers into acceleration, regardless of their actual position. In such circumstances, there was mechanical failure of the aircraft. However, the occurrence of this failure is one in 400 billion hours of flight and therefore highly improbable. In the second hypothesis, the pilot has performed a procedure different from that provided in the manual, and put the thrust lever in an irregular position, a configuration of <b>human</b> <b>error</b> for the accident.|$|E
5|$|Statistics on {{injuries}} {{related to}} commercial diving are normally collected by national regulators. In the UK the Health and Safety Executive (HSE) {{is responsible for}} the overview of about 5,000 commercial divers; in Norway the corresponding authority is the Petroleum Safety Authority Norway (PSA), which has maintained the DSYS database since 1985, gathering statistics on over 50,000 diver-hours of commercial activity per year. The risks of dying during recreational, scientific or commercial diving are small, and for scuba diving, deaths are usually associated with poor gas management, poor buoyancy control, equipment misuse, entrapment, rough water conditions and pre-existing health problems. Some fatalities are inevitable and caused by unforeseeable situations escalating out of control, but the majority of diving fatalities can be attributed to <b>human</b> <b>error</b> {{on the part of the}} victim. According to a North American 1972 analysis of calendar year 1970 data, diving was, based on man hours, 96 times more dangerous than driving a car. According to a 2000 Japanese study, every hour of recreational diving is 36 to 62 times riskier than driving.|$|E
5|$|The first Ju 88s {{took off}} at 23:00 and began {{heading toward the}} Dutch coast where they dived to sea level and stayed at {{approximately}} 50 metres while they flew out to sea. Crews were forbidden to engage enemy aircraft over the North Sea {{in order to preserve}} surprise until the last possible moment. The rain and squall assisted crews in judging the location and distance of the water. However, with crews prone to <b>human</b> <b>error,</b> it was decided to use the very accurate FuG 101 radar altimeter and the Ju 88s blind-flying instruments. The strain on the crews was enormous, as a careful vigil was kept on them until the British coast was reached and the pilot began his ascent to the height of the returning bomber stream. As they did so, the Ju 88s released Düppel to obscure the radar of Mosquito fighters. The pilots were then free to begin their attacks.|$|E
30|$|The {{results of}} this {{questionnaire}} show more research is necessary in the <b>human</b> <b>errors</b> department as the public seems unclear as to what can go wrong and what will be done {{to overcome these issues}} when a driverless train system is in operation. With this information and more disaggregate data on general transport attitudes, operators can help the public by explaining how the system works and how <b>human</b> <b>errors</b> will be dealt with.|$|R
40|$|Industrial {{processes}} became safer with automation. However, {{when these}} systems fail, {{it can cause}} major accidents. Thus, it is fundamental for these systems to be installed and maintained to avoid failures that can be transformed into accidents. Random and systematic failures are failures of automation systems. The first are predictable, however the second are caused by human factors, hence, {{it is impossible to}} foresee them. Thus, it becomes necessary to have good human factor management to avoid these dangerous failures. This paper briefly discusses <b>human</b> <b>errors</b> and comes up with a management to avoid systematic failures, taking in consideration the types and behavior of the errors. Applying this management, it will reduce <b>human</b> <b>errors</b> and, therefore, systematic failures. These actions will enhance the safety of industrial processes. Keywords: <b>Human</b> <b>errors.</b> Automation. Safety. Management...|$|R
30|$|Classification {{based on}} PEFD. It {{consists}} of hidden failures caused by hardware faults (PEFD-A) and by <b>human</b> <b>errors</b> and protection settings (PEFD-B).|$|R
5|$|A third ship, the Manantico also {{crashed into}} shore {{due to a}} {{combination}} of the storm and <b>human</b> <b>error</b> in which the captain confused Cape Henry with Cape Charles after spotting another schooner. The Manantico was also where the two deaths associated with the storm occurred. The first was when a cook on the ship was crushed to death by the cargo of lumber being hauled by the ship. The ship was then pushed towards a sand bar. The captain, who had stayed high on the starboard side for safety, began climbing down to slip the ship cables, but the ship made a sudden stop. This flung the captain into the water, and he drowned. Both bodies were found after the storm and were very disfigured. The captain was sent to Middletown, Connecticut for interment while the body of the cook was buried on the beach. The final ship was the Harriet Thomas, which was the schooner spotted by the Manantico. After beaching, the crew managed to get a rope to shore where fishermen had tied the other end. The crew were able to climb ashore, although the captain had to be rescued via alternate means due to being too heavy for this method. The ship was written off as a $7000 (1887 USD) loss. Although all four ships were beached, due to the loss of communications, only one wreck – that of the Mary D. Cranmer – was reported in the Norfolk Virginian newspaper. As a result, news of the two deaths from the Manantico were initially unreported.|$|E
5|$|Accidents {{involving}} <b>human</b> <b>error</b> include American Airlines Flight 965, which {{crashed into}} a mountain in Buga, Colombia, on December 20, 1995, killing 151 passengers and eight crew members with four survivors, and the mid-air collision of DHL Flight 611 near Überlingen, Baden-Württemberg, Germany, on July 1, 2002, {{with the loss of}} two on board plus 69 on a Bashkirian Airlines Tupolev Tu-154. The American Airlines Flight 965 crash was blamed on navigational errors by the crew, while the collision of DHL Flight 611 involved air traffic control errors. Accidents attributed to pilot disorientation due to improperly maintained instruments include Birgenair Flight 301 on February 6, 1996, in Puerto Plata, Dominican Republic, with the loss of all 189 passengers and crew, and Aeroperú Flight 603 on October 2, 1996, off the coast of Pasamayo, Peru, with the loss of all 70 on board. In the Birgenair accident, investigators found that the aircraft had been stored without the necessary covers for its pitot tube sensors, thus allowing insects and debris to collect within, while in the Aeroperú accident, protective tape covering static vent sensors had not been removed.|$|E
5|$|Wrigley {{travelled}} to England in 1928 {{to attend}} RAF Staff College, Andover, {{becoming one of}} the first RAAF officers to complete the course. Remaining in England, he was appointed Australian Air Liaison Officer to the Air Ministry in 1929. That October, he initiated correspondence with the British Air Council to discuss a proposal for the RAAF to adopt as its own the Royal Air Force's motto Per Ardua Ad Astra; informal approval was granted by letter to Wrigley in March 1930. Returning to Australia, he became Director of Operations and Intelligence at RAAF Headquarters in October 1930, and Director of Organisation and Staff Duties in December 1931. He was promoted to wing commander in December 1932. In 1935 he published his history of No.3 Squadron, The Battle Below, which was considered an authoritative treatment on the subject of army co-operation. He was promoted group captain in July 1936, and that October took over as commanding officer of RAAF Station Laverton, Victoria, from Group Captain McNamara. Wrigley handed over the station's command to Group Captain Adrian Cole in February 1939. In May 1939, Wrigley served as the senior expert assessor on the panel of an inquiry into three recent accidents involving Avro Ansons; the full report handed down in October found that training on the type followed the syllabus, but that pilots needed more practical experience in dealing with in-flight incidents, as <b>human</b> <b>error</b> was the likely explanation for at least one crash.|$|E
40|$|Abstract: Decisions are {{the core}} {{transactions}} of organizations {{and have a}} profound influence on organizations achievement. Decisions {{can be described as}} a series of behavioural reactions in favour of something, a certain action where there is no doubt, or a judgment that is made among several alternatives. In most situations persons based on personal judgment make decisions. In such situations decision makers may not pay enough attention to some important factors. However, ignoring such factors during the process of decision-making, either they are obvious or hidden, might cause failure of the decision followed by unwanted consequences. Therefore, the aim {{of this study was to}} identify the most important <b>human</b> <b>errors</b> resulting failure of a decision in evaluation of the alternatives in the process of decision-making. Two case studies were selected from the literature and analyzed to find the <b>human</b> <b>errors</b> contribute to decision fail. Then the analysis of <b>human</b> <b>errors</b> was linked with mental models in evaluation of alternative step. The results of the study showed that five <b>human</b> <b>errors</b> occur in the evaluation of alternatives step; ignorance or neglect, overconfidence, underestimate, moral and fail to see, which led to un-achievement of objectives...|$|R
40|$|Abstract. Although the {{advancement}} of technology can provide a better and safer working environment to people, sometimes, overconfidence in technology can lead to unavoidable accidents and <b>human</b> <b>errors</b> in the workplace. This study aims {{to determine the effects}} of familiarity with office equipment on <b>human</b> <b>errors</b> and accidents in the workplace. A total of 150 surveys were collected and analysed with correlations analysis usingSPSS software version 19. 0. The results show that although familiarity can reduce the occurrence of <b>human</b> <b>errors,</b> it cannot reduce the occurrence of accidents in the workplace. While it is now understood that improving one's familiarity with office equipment can potentially reduce their mistakes at work, companies should still be observant of other factors that can potentially reduce accidents in the workplace. This study sheds light on potential possibilities to improve the occupational safety, health and environment of organizations...|$|R
50|$|Developers {{should avoid}} writing code {{by hand and}} instead write {{abstract}} high-level programs that generate code. This rule aims to reduce <b>human</b> <b>errors</b> and save time.|$|R
