28|99|Public
50|$|SME Server {{software}} supports both {{hardware and}} software <b>disk</b> <b>mirroring</b> (RAID 1), USB Disk backup & tape backup for additional data protection.|$|E
5000|$|<b>Disk</b> <b>mirroring</b> so {{that failure}} of {{internal}} disks does {{not result in}} system crashes. The Distributed Replicated Block Device is one example.|$|E
50|$|<b>Disk</b> <b>mirroring</b> {{differs from}} file {{shadowing}} that operates on the file level, and disk snapshots where data images are never re-synced with their origins.|$|E
5000|$|A clone BCV is a {{traditional}} method, and uses one-to-one separate physical storage (splitable <b>disk</b> <b>mirror)</b> ...|$|R
5000|$|Mirrored volumes: {{the volume}} {{contains}} several disks, and when data is written to one {{it is also}} written to the other disks. This means that if one disk fails, the data can be totally recovered from the other <b>disk.</b> <b>Mirrored</b> volumes are also known as RAID-1.|$|R
5000|$|Mirrored volumes, {{also known}} as RAID-1, store {{identical}} copies of their data on 2 or more identical <b>disks</b> (<b>mirrored).</b> This allows for fault tolerance; in the event one disk fails, the other disk(s) can keep the server operational until the server can be shut down for replacement of the failed disk.|$|R
50|$|Storage appliances: provide {{massive amounts}} of storage and {{additional}} higher level functionality (ex: <b>Disk</b> <b>mirroring</b> and Data striping) for multiple attached systems using the transparent local storage area networks computer paradigm.|$|E
50|$|Real-Time Compression can be {{combined}} with Easy Tiering, Thin Provisioning and Virtual <b>Disk</b> <b>Mirroring.</b> It was initially invented by the acquired startup Storwize Inc., which also served as new name for the SVC-derived IBM storage systems family.|$|E
5000|$|XRC is a z/Series {{asynchronous}} <b>disk</b> <b>mirroring</b> technique {{which is}} effective over any distance. It keeps the data time consistent across multiple ESS (Enterprise Storage Server) or HDS (Hitachi Data Systems) disk subsystems at the recovery site.|$|E
5000|$|Electromechanical {{television}} - Nipkow <b>disk</b> with <b>mirrors</b> {{instead of}} slots (ca. 1925) ...|$|R
40|$|GeckoFS is a file / {{storage system}} that saves power by {{spinning}} down hard disks. It overlays a log abstraction over a fault-tolerant mirrored multi-disk array. A log-structured storage system writes {{only to the}} log head, hence it continuously and sequentially writes to {{the same set of}} striped, <b>mirrored</b> <b>disks</b> for long periods of time. Read requests are served from the primary <b>disks,</b> while the <b>mirror</b> <b>disks</b> can be powered down to trade off read throughput for power savings...|$|R
50|$|In some implementations, the <b>mirrored</b> <b>disk</b> can {{be split}} off {{and used for}} data backup, {{allowing}} the first disk to remain active. However merging the two disks then may require a synchronization period if any write I/O activity has occurred to the <b>mirrored</b> <b>disk.</b>|$|R
50|$|In data storage, <b>disk</b> <b>mirroring</b> is the {{replication}} of logical disk volumes onto separate physical hard disks {{in real time}} to ensure continuous availability. It is most commonly used in RAID 1. A mirrored volume is a complete logical representation of separate volume copies.|$|E
5000|$|SFT II {{provides}} a <b>disk</b> <b>mirroring</b> or duplexing {{system based on}} RAID 1; mirroring refers to two disk drives holding the same data, duplexing uses two data channels/controllers to connect the disks (fault tolerance on the disk level and optionally on the data-channel level).|$|E
5000|$|A {{geographically}} distributed, highly available clustered storage setup leveraging {{the virtual}} <b>disk</b> <b>mirroring</b> feature across datacenters within 300 km distance. Stretched Clusters can span 2, 3 or 4 datacenters (chain or ring topology, a 4-site cluster requiring 8 cluster nodes). Cluster consistency is ensured {{by a majority}} voting set.|$|E
40|$|In this paper, {{we propose}} SiLo, a novel energy {{efficient}} shifted logging storage architecture, for write-oriented workloads. By organizing free storage space of redundant <b>mirrored</b> <b>disks</b> in a RAID 10 system into a logical logging space pool and shifting logger among different <b>mirrored</b> <b>disk</b> pairs, {{only one or}} few <b>mirrored</b> <b>disk</b> pairs are used as the on-duty logger {{at a time and}} all the other off-duty <b>mirrored</b> <b>disk</b> pairs are set to low-power state for power saving. SiLo is extremely effective for the write-oriented workloads. Extensive trace-driven evaluations demonstrate the power saving potentiality and performance scalability. Thorough analysis and comparison of reliability between SiLo and typical RAID 10 demonstrates that the MTTDL of SiLo is much larger than that of typical RAID 10. ...|$|R
40|$|Collaborating in the {{evaluation}} of GeckoFS, a file system based on KyotoFS, that saves power by spinning down hard disks. It overlays a log abstraction over a fault-tolerant mirrored multi-disk array. A log-structured storage system writes only to the log head, hence it continuously and sequentially writes to the same set of striped, <b>mirrored</b> <b>disks</b> for long periods of time. Read requests are served from the primary <b>disks,</b> while the <b>mirror</b> <b>disks</b> can be powered down to trade off read throughput for power savings. Power-Lean Cloud Storag...|$|R
5000|$|... #Caption: Fig.1. An optically-pumped <b>disk</b> laser (active <b>mirror).</b>|$|R
5000|$|<b>Disk</b> <b>mirroring.</b> This {{was done}} in the {{filesystem}} and not the device driver, so that slightly (or even completely) different devices could still be mirrored together. Mirroring a small hard disk to the floppy was a popular way to test mirroring as ejecting the floppy was an easy way to induce disk errors.|$|E
5000|$|Continuous data {{protection}} : Instead of scheduling periodic backups, the system immediately logs every {{change on the}} host system. This is generally done by saving byte or block-level differences rather than file-level differences. It differs from simple <b>disk</b> <b>mirroring</b> in that it enables a roll-back of the log and thus restoration of old images of data.|$|E
50|$|NetWare 2.x {{implemented}} {{a number of}} features inspired by mainframe and minicomputer systems that were not available in other operating systems of the day. The System Fault Tolerance (SFT) features included standard read-after-write verification (SFT-I) with on-the-fly bad block re-mapping (at the time, disks did not have that feature built in) and software RAID1 (<b>disk</b> <b>mirroring,</b> SFT-II). The Transaction Tracking System (TTS) optionally protected files against incomplete updates. For single files, this required only a file attribute to be set. Transactions over multiple files and controlled roll-backs were possible by programming to the TTS API.|$|E
2500|$|... fsck cannot always {{validate}} {{and repair}} data when checksums are stored with data (often {{the case in}} many file systems), because the checksums may also be corrupted or unreadable. ZFS always stores checksums separately from the data they verify, improving reliability {{and the ability of}} scrub to repair the volume. ZFS also stores multiple copies of data – metadata in particular may have upwards of 4 or 6 copies (multiple copies per disk and multiple <b>disk</b> <b>mirrors</b> per volume), greatly improving the ability of scrub to detect and repair extensive damage to the volume, compared to fsck.|$|R
50|$|The {{original}} processor {{that went}} into service at Baynard house {{was known as the}} MK2 BL processor. It was replaced (though not perhaps in Baynard House?) by the POPUS1. One of these was in Lancaster House in Liverpool. Later, these too were replaced with a much smaller system known as R2PU or release 2 processor utility. This was the 4 CPU per cluster and 8 cluster system described above. In more recent times, some of the clusters were replaced with more modern hardware while the original processing clusters 0 to 3 were upgraded in many ways. There were many very advanced features in the system that explain why these are still in use today like self fault detection and recovery, battery backed ram <b>disks,</b> <b>mirrored</b> <b>disk</b> storage, auto replacement of a failed memory unit, the ability to trial new software and roll back to the previous version and a bespoke instruction set.|$|R
5000|$|... #Caption: Fig.2. A <b>disk</b> laser (active <b>mirror)</b> {{configuration}} {{presented in}} 1992 at the SPIE conference.|$|R
5000|$|There is no native {{operating}} system {{support for the}} Cloud Files API {{so it is not}} yet possible to [...] "map" [...] or [...] "mount" [...] it as a virtual drive without third-party software like JungleDisk that translates to a supported standard such as WebDAV. There are no concepts of [...] "appending" [...] or [...] "locking" [...] data within Cloud Files (which may affect some <b>disk</b> <b>mirroring</b> or backup solutions), nor support for permissions or transcoding. Data is organised into [...] "containers" [...] but {{it is not possible to}} create nested folders without a translation layer.|$|E
50|$|In {{addition}} to providing an additional copy of {{the data for the}} purpose of redundancy in case of hardware failure, <b>disk</b> <b>mirroring</b> can allow each disk to be accessed separately for reading purposes. Under certain circumstances, this can significantly improve performance as the system can choose for each read which disk can seek most quickly to the required data. This is especially significant where there are several tasks competing for data on the same disk, and thrashing (where the switching between tasks takes up more time than the task itself) can be reduced. This is an important consideration in hardware configurations that frequently access the data on the disk.|$|E
50|$|The {{most basic}} method is <b>disk</b> <b>mirroring,</b> typical for locally {{connected}} disks. The storage industry narrows the definitions, so mirroring {{is a local}} (short-distance) operation. A replication is extendable across a computer network, so the disks can be located in physically distant locations, and the master-slave database replication model is usually applied. The purpose of replication is to prevent damage from failures or disasters that may occur in one location, or in case such events do occur, improve the ability to recover. For replication, latency is the key factor because it determines either how far apart the sites can be or the type of replication that can be employed.|$|E
25|$|In addition, pools {{can have}} hot spares to {{compensate}} for failing <b>disks.</b> When <b>mirroring,</b> block devices can be grouped according to physical chassis, so that the filesystem can continue {{in the case of}} the failure of an entire chassis.|$|R
40|$|Abstract. In {{this paper}} we study data {{replication}} in a <b>mirrored</b> <b>disk</b> system. Free disk space is exploited by keeping replicas of specific cylinders at appropriate disk locations. Assuming an organ-pipe arrangement we calculate the expected seek distance by varying the probability cylinder access under different distributions. Also, analytic formulae are derived for the expected seek distance under replication and {{comparison with the}} conventional (without replication) <b>mirrored</b> <b>disk</b> system is performed. ...|$|R
40|$|The {{dominant}} hurdle to {{the operation}} of optomechanical systems in the quantum regime is the coupling of the vibrating element to a thermal reservoir via mechanical supports. Here we propose a scheme that uses an optical spring to replace the mechanical support. We show that the resolved-sideband regime of cooling can be reached in a configuration using a high-reflectivity <b>disk</b> <b>mirror</b> held by an optical tweezer {{as one of the}} end-mirrors of a Fabry-Perot cavity. We find a final phonon occupation number of the trapped mirror n̅= 0. 14 for reasonable parameters, well within the quantum regime. This demonstrates the promise of dielectric disks attached to optical springs for the observation of quantum effects in macroscopic objects. Comment: 4 pages, 2 figure...|$|R
5000|$|Backup site or {{disaster}} recovery center (DR center): In {{the event of}} a disaster, the data on backup media will not be sufficient to recover. Computer systems onto which the data can be restored and properly configured networks are necessary too. Some organizations have their own data recovery centers that are equipped for this scenario. Other organizations contract this out to a third-party recovery center. Because a DR site is itself a huge investment, backing up is very rarely considered the preferred method of moving data to a DR site. A more typical way would be remote <b>disk</b> <b>mirroring,</b> which keeps the DR data as up to date as possible.|$|E
50|$|Tier 6 {{business}} continuity solutions maintain {{the highest levels}} of data currency. They are used by businesses with little or no tolerance for data loss and who need to restore data to applications rapidly. These solutions have no dependence on the applications or applications staffs to provide data consistency. Tier 6 solutions often require some form of <b>Disk</b> <b>mirroring.</b> There are various synchronous and asynchronous solutions available from the mainframe storage vendors. Each solution is somewhat different, offering different capabilities and providing different Recovery Point and Recovery Time objectives. Often some form of automated tape solution is also required. However, this can vary somewhat depending on the amount and type of data residing on tape.|$|E
50|$|A {{storage space}} behaves like a {{physical}} disk to the user, with thin provisioning of available disk space. The spaces are organized within a storage pool, i.e. {{a collection of}} physical disks, that can span multiple disks of different sizes, performance or technology (USB, SATA, SAS). The process of adding new disks or replacing failed or older disks is fully automatic, but can be controlled with PowerShell commands. The same storage pool can host multiple storage spaces. Storage Spaces have built-in resiliency from disk failures, {{which is achieved by}} either <b>disk</b> <b>mirroring</b> or striping with parity across the physical disks. Each storage pool on the ReFS filesystem is limited to 4 PB (4096 TB), but there are no limits on the total number of storage pools or the number of storage spaces within a pool.|$|E
5000|$|... #Caption: Close-up of an HDD head {{resting on}} a <b>disk</b> platter; its <b>mirror</b> {{reflection}} is visible on the platter surface.|$|R
40|$|The {{object of}} this study is the {{analysis}} of the <b>disk</b> <b>mirror</b> discovered in 2010 during the rescue archaeological excavations carried out in occasion of the construction of the Arad-Timişoara motorway, respectively the Arad-Seceani sector. The mirror was found at  ca. 1 m north grave 1 in site B 0 _ 6, where beside other two cremation graves, poorly preserved, other 129 archaeological features were also investigated. Though not exhaustively, we attempt herein to present the origin, distribution of this mirror type in the Sarmatian world and the chronological interval of their use within said environment. The author notes that these <b>disk</b> <b>mirror</b> types from the funerary Sarmatian features of the Great Hungarian Plain count amongst the most numerous, being found within funerary contexts on the entire duration of the Sarmatian inhabitancy of this geographical area. Further, the author notes that mirrors of the type are widely spread on broad geographical areas, hence the establishment of any production centres is highly difficult, but also that very likely, they were produced in various cultural environments over several centuries. Their high numbers in the Sarmatian world proves it is possible they made them, although there is no certain substantiating evidence. The author does not exclude either the possibility of the presence of travelling artisans in the Sarmatian environment making certain item categories upon order, mirrors of the type included. In terms of the dating of G 1 at Arad (site B_ 06), where the mirror most definitely originates, the author concludes that together with the other two graves (G 2 and G 3) are contemporary and date sometime {{to the end of the}} 2 nd century – early decades of the 3 rd century AD...|$|R
50|$|Mirroring, {{the other}} ZFS RAID option, is {{essentially}} the same as RAID 1, allowing any number of <b>disks</b> to be <b>mirrored.</b>|$|R
