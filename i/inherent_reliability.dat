80|48|Public
5000|$|... <b>inherent</b> <b>reliability</b> {{and very}} long life; tubes always degrade and fail over time. Some {{transistorized}} devices {{have been in}} service for more than 60 years ...|$|E
5000|$|Piezoelectric sensors are {{versatile}} {{tools for}} {{the measurement of}} various processes. They are used for quality assurance, process control, and {{for research and development}} in many industries. Pierre Curie discovered the piezoelectric effect in 1880, but only in the 1950s did manufacturers begin to use the piezoelectric effect in industrial sensing applications. Since then, this measuring principle has been increasingly used, and has become a mature technology with excellent <b>inherent</b> <b>reliability.</b>|$|E
50|$|Even today most {{spare parts}} are readily {{available}} as well as relatively inexpensive. Thanks to the design's <b>inherent</b> <b>reliability,</b> using a Nimbus {{on a daily basis}} is still considered easy and economical. Nevertheless (and with some notable exceptions), today most Nimbus owners rarely ride more than a few thousand kilometers a year. Also, as the Nimbus often came from the factory with a sidecar attached, many of the ones on the road have recently been fitted with such.|$|E
5000|$|<b>Inherent</b> (system) Design <b>Reliability</b> Analysis and derived {{requirements}} specification for both Hardware and Software design ...|$|R
50|$|Historically, a {{majority}} of outboards have been two-stroke powerheads fitted with a carburetor due to the design's <b>inherent</b> simplicity, <b>reliability,</b> low cost and light weight. Drawbacks include increased pollution, due to the high volume of unburned gasoline and oil in their exhaust, and louder noise.|$|R
5000|$|As {{interest}} in fault tolerance and system reliability {{increased in the}} 1960s and 1970s, dependability came to be a measure of x as measures of reliability came to encompass additional measures like safety and integrity. [...] In the early 1980s, Jean-Claude Laprie thus chose dependability as the term to encompass studies of fault tolerance and system reliability without the extension of meaning <b>inherent</b> in <b>reliability.</b>|$|R
50|$|As {{director}} of the computer science division at RAND, Uncapher pioneered work on the technology of packet switching, in which digital messages are broken into small packets, sent over a network and reassembled at their destination. Due to this methods <b>inherent</b> <b>reliability</b> and robustness it attracted {{a great deal of}} interest at the Department of Defense Advanced Projects Research Agency. He helped officials see the technology's vast potential for facilitating a revolution in computer communications. Uncapher also designed the first time-sharing computer system for mathematicians. He also led the RAND Tablet Project, a computer system for recognizing hand printed characters using a tablet and stylus. Packet switching research and development led first to the military's ARPANET, and then to the Internet itself.|$|E
5000|$|Obsolete chain designs {{previously}} used on bicycles {{included the}} block chain, the skip-link chain, and the Simpson lever chain. The first chains {{were of a}} simple, bushingless design. These had <b>inherent</b> <b>reliability</b> problems and a bit more friction (and mechanical efficiency losses) than modern chains. With these limitations in mind, the Nevoigt brothers, of the German Diamant Bicycle Company, designed the roller chain in 1898, which uses bushings, {{and it is the}} prevalent chain today. Whether it be singlespeed (for example with an internal-gears hub), fixed gear (such as track bikes and modern urban [...] "fixies") or multi-speeds with derailleurs, all modern chains in use today are of the [...] "roller chain" [...] design. Although it is still possible to order lower cost [...] "bushingless" [...] chains from China today, with generally lower manufacturing costs across the board, bushingless chains are generally considered undesirable and not prevalent.|$|E
50|$|Reliability {{needs to}} be {{evaluated}} and improved related to both availability {{and the cost of}} ownership (due to cost of spare parts, maintenance man-hours, transport costs, storage cost, part obsolete risks etc.). Often a trade-off is needed between the two. There might be a maximum ratio between availability and cost of ownership. Testability of a system should also be addressed in the availability plan as this is the link between reliability and maintainability. The maintenance strategy can influence the reliability of a system (e.g. by preventive and/or predictive maintenance), although it can never bring it above the <b>inherent</b> <b>reliability.</b> So, Maintainability and Maintenance strategies influences the availability of a system. In theory this can be almost unlimited if one would be able to always repair any fault in an infinitely short time. This is in practice impossible. Repair-ability is always limited due to testability, manpower and logistic considerations. Reliability is not limited (Reliable items can be made that outlast the life of a machine with almost 100% certainty). For high levels of system availability (e.g. the availability of engine trust in an aircraft), the use of redundancy may be the only option. Refer to reliability engineering.|$|E
40|$|Cooling by {{sound is}} a new {{environmentally}} friendly technology developed rapidly {{during the past three}} decades. The refrigeration system based on this technology is called thermoacoustic refrigeration. Due to <b>inherent</b> simplicity, <b>reliability</b> and no hazardous materials, thermoacoustic refrigeration systems have a strong potential to replace conventional refrigeration systems. However, at present, these devices have lower efficiency mainly attributed to the poor understanding of the fundamental processes and technical immaturity to design different components of these devices. Thus, significant efforts are needed to improve the fundamental understanding of the thermoacoustic process, and the role/impact of the main components of these devices on the overall performance. This will lead to the development of efficient devices tha...|$|R
40|$|Recently {{published}} {{test data}} for bolted and nailed connections was evaluated {{to assess the}} structural <b>reliability</b> <b>inherent</b> in current allowable stress design procedures for connections in wood structures. Reliability indices were determined for timber connections using standard firstorder, second moment (FOSM) procedures. For the connections considered in this study, reliability indices range from 2. 6 to 5. 1, generally providing higher levels of safety than the structural members in timber structures...|$|R
40|$|This paper {{examines}} {{the application of}} Neural Networks (NN) to the reliability analysis of complex structural systems in connection with Monte Carlo Simulation (MCS). The failure {{of the system is}} associated with the plastic collapse. The use of NN was motivated by the approximate concepts <b>inherent</b> in <b>reliability</b> analysis and the time consuming repeated analyses required for MCS. A Back Propagation algorithm is implemented for training the NN utilising available information generated from selected elasto-plastic analyses. The trained NN is then used to compute the critical load factor due to different sets of basic random variables leading to close prediction of the probability of failure. The use of MCS with Importance Sampling further improves the prediction of the probability of failure with Neural Networks...|$|R
5000|$|A {{reliability}} program plan {{may also}} be used to evaluate and improve availability of a system by the strategy of focusing on increasing testability & maintainability and not on reliability. Improving maintainability is generally easier than improving reliability. Maintainability estimates (repair rates) are also generally more accurate. However, because the uncertainties in the reliability estimates are in most cases very large, they are likely to dominate the availability calculation (prediction uncertainty problem), even when maintainability levels are very high. When reliability is not under control, more complicated issues may arise, like manpower (maintainers / customer service capability) shortages, spare part availability, logistic delays, lack of repair facilities, extensive retro-fit and complex configuration management costs, and others. The problem of unreliability may be increased also due to the [...] "domino effect" [...] of maintenance-induced failures after repairs. Focusing only on maintainability is therefore not enough. If failures are prevented, none of the other issues are of any importance, and therefore reliability is generally regarded as {{the most important part of}} availability. Reliability needs to be evaluated and improved related to both availability and the Total Cost of Ownership (TCO) due to cost of spare parts, maintenance man-hours, transport costs, storage cost, part obsolete risks, etc. But, as GM and Toyota have belatedly discovered, TCO also includes the downstream liability costs when reliability calculations have not sufficiently or accurately addressed customers' personal bodily risks. Often a trade-off is needed between the two. There might be a maximum ratio between availability and cost of ownership. Testability of a system should also be addressed in the plan, as this is the link between reliability and maintainability. The maintenance strategy can influence the reliability of a system (e.g., by preventive and/or predictive maintenance), although it can never bring it above the <b>inherent</b> <b>reliability.</b>|$|E
40|$|AbstractThe <b>inherent</b> <b>reliability</b> {{of product}} {{was formed in}} design and {{manufacturing}} process, and the <b>inherent</b> <b>reliability</b> prediction model of the product is foundation of the manufacturing process control and process improvement. The method of processing technology reliability evaluation based on Copula-SVM is presented. Firstly, {{the principle of the}} <b>inherent</b> <b>reliability</b> during manufacturing process was analyzed. Secondly, the dependency structure of the process characteristic is given by means of the copula function. Finally, the <b>inherent</b> <b>reliability</b> prediction method based on support vector machine is presented, and the feasibility and practicability of the method were indicated by an industry application...|$|E
40|$|The <b>inherent</b> <b>reliability</b> {{of a space}} launch system, or lack thereof, {{is pointed}} out to often be a more {{significant}} determinant of access to orbit than the nominal cost. System improvements that enhance <b>inherent</b> <b>reliability</b> may accordingly represent the most economical approach to follow. At some point, the limitations of launch technologies currently in use must be acknowledged and novel methods developed. The dearth of systematic research efforts toward superior technology in launcher propulsion over the last 20 years is identified as a major limitation {{in the search for}} near-term options...|$|E
50|$|While the GPS has a {{very high}} <b>inherent</b> precision, the <b>reliability</b> is not high enough for landing. GPS signals may be {{intentionally}} jammed, or lose integrity. In such cases, it may take the GPS receiver {{a few seconds to}} detect the malfunction, which is too long for critical flight stages. GPS can be used to lower the decision height below the unaided threshold, down to cat I decision height minima, but not lower.|$|R
40|$|Abstract- Data mining on high-dimensional {{heterogeneous}} data is {{a crucial}} component in information fusion application domains such as remote sensing, surveillance, and homeland security. The information processing requirements of these domains place a premium on security, robustness, performance, and sophisticated analytic methods. This paper introduces a database-centric approach that enables data mining and analysis of data that typically interest the information fusion community. The approach benefits from the <b>inherent</b> security, <b>reliability,</b> and scalability found in contemporary RDBMSs. The capabilities of this approach are demonstrated on satellite imagery. Hyperspectral data are mined using clustering (O-Cluster) and classification (Support Vector Machines) techniques. The data mining is performed inside the database, which ensures maintenance of data integrity and security throughout the analytic effort. Within the database, the clustering and classification results can be further combined with spatial processing components to enable additional analysis...|$|R
40|$|The {{accurate}} {{determination of}} the biological effects of low doses of pollutants is a major public health challenge. DNA microarrays are {{a powerful tool for}} investigating small intracellular changes. However, the <b>inherent</b> low <b>reliability</b> of this technique, the small number of replicates and the lack of suitable statistical methods for the analysis of such a large number of attributes (genes) impair accurate data interpretation. To overcome this problem, we combined results of two independent analysis methods (ANOVA and RELIEF). We applied this analysis protocol to compare gene expression patterns in Saccharomyces cerevisiae growing in the absence and continuous presence of varying low doses of radiation. Global distribution analysis highlights the importance of mitochondrial membrane functions in the response. We demonstrate that microarrays detect cellular changes induced by irradiation at doses that are 1000 -fold lower than the minimal dose associated with mutagenic effects...|$|R
40|$|A {{new class}} of {{electromechanical}} actuators is described. These dual drive actuators were developed for the NASA-JPL Galileo Spacecraft. The dual drive actuators are fully redundant and therefore have high <b>inherent</b> <b>reliability.</b> They {{can be used for}} a variety of tasks, and they can be fabricated quickly and economically...|$|E
40|$|Transmission line {{protective}} {{systems are}} sometimes very complex, incorporating many different equipment groups, often at widely separated places, and often requiring high-speed communications for proper operation. The <b>inherent</b> <b>reliability</b> of such complex systems {{is a concern}} of the protection engineer and presents a significant analytical problem. This paper describes the use of fault tree analysis as one method of analyzing the reliability of these complex systems...|$|E
40|$|Ring-based network {{overlays}} have attractive characteristics {{for group}} communications such as <b>inherent</b> <b>reliability</b> and single fault-tolerance. However, ring networks also generally have longer paths and thus higher delay and delay jitter. In {{order to provide}} scalability {{as the number of}} group members grows, large single rings may be broken into smaller multi-rings interconnected together at the same level or interconnected in a multi-level hierarchy of rings...|$|E
40|$|Currently, the {{requirements}} of Business sector promote more and more complex Information Systems. Reliability {{is one of the}} quality characteristics widely expected by users and developers. This characteristic is architectural by nature since it can be directly promoted by software architecture. This relation determines the importance of designing architectures that guarantee reliable systems. This article presents a research in progress whose objective is developing an architectural evaluation method based on Reliability. The first step considered for designing the method included: the construction of a Conceptual Model, a model to specify the architectural quality based on Reliability (Utility Tree), a set of scenarios associated to this characteristic. The first model allows identifying the concepts <b>inherent</b> to <b>Reliability</b> and their relationships; the second one covers all quality features related to Reliability in order to specify it; and the scenarios guide the software architect for anticipating context stimulus and evaluating the architectural responses...|$|R
40|$|This paper {{examines}} {{the application of}} neural networks (NN) to reliability-based structural optimization of large-scale structural systems. The failure of the structural system {{is associated with the}} plastic collapse, The optimization part is performed with evolution strategies, while the reliability analysis is carried out with the Monte Carlo simulation (MCS) method incorporating the importance sampling technique for the reduction of the sample size. In this study two methodologies are examined. In the first one an NN is trained to perform both the deterministic and probabilistic constraints check. In the second one only the clasto-plastic analysis phase, required by the MCS, is replaced by a neural network prediction of the structural behaviour up to collapse. The use of NN is motivated by the approximate concepts <b>inherent</b> in <b>reliability</b> analysis and the time consuming repeated analyses required by MCS. (C) 2002 Elsevier Science B. V. All rights reserved...|$|R
40|$|The overall {{reliability}} and maintenance costs (R&MC's) {{of past and}} current turboprop systems were examined. Maintenance cost drivers {{were found to be}} scheduled overhaul (40 %), lack of modularity particularly in the propeller and reduction gearbox, and lack of <b>inherent</b> durability (<b>reliability)</b> of some parts. Comparisons were made between the 501 -D 13 / 54 H 60 turboprop system and the widely used JT 8 D turbofan. It was found that the total maintenance cost per flight hour of the turboprop was 75 % higher than that of the JT 8 D turbofan. Part of this difference was due to propeller and gearbox costs being higher than those of the fan and reverser, but most of the difference was in the engine core where the older technology turboprop core maintenance costs were nearly 70 percent higher than for the turbofan. The estimated maintenance cost of both the advanced turboprop and advanced turbofan were less than the JT 8 D. The conclusion was that an advanced turboprop and an advanced turbofan, using similar cores, will have very competitive maintenance costs per flight hour...|$|R
40|$|Environmental hazards to {{industrial}} robots are summarized. The <b>inherent</b> <b>reliability</b> {{of the design}} of the Unimate robot is assessed and the data used in a management system to bring the reliability performance up to a level nearing what is theoretically available. The design is shown to be capable of a mean time between failure of 400 hours and an average up time of 98 %. Specific design decisions made in view of application requirements are explored...|$|E
40|$|A novel catastrophic {{breakdown}} mode in reversed biased Silicon carbide diodes {{has been}} seen for low LET particles. These particles are too low in LET to induce SEB, however SEB was seen from particles of higher LET. The low LET mechanism correlates with second breakdown in diodes due to increase leakage and assisted charge injection from incident particles. Percolation theory was used to predict some basic responses of the devices, but the <b>inherent</b> <b>reliability</b> issue with silicon carbide have proven challenging...|$|E
40|$|This paper {{describes}} {{an example of}} how modern engineering and safety techniques can be used to assure the reliable and safe operation of photovoltaic power systems. This particular application was for a solar cell power system demonstration project in Tangaye, Upper Volta, Africa. The techniques involve a definition of the power system natural and operating environment, use of design criteria and analysis techniques, an awareness of potential problems via the <b>inherent</b> <b>reliability</b> and FMEA methods, and use of a fail-safe and planned spare parts engineering philosophy...|$|E
40|$|Rigid flex {{circuits}} {{have historically}} been used in military or commercial aerospace applications where performance was primarily dictated by specification rather than function. Rigid flex {{has proved to be}} useful in other marketplaces, such as consumer electronic goods, in consideration of the <b>inherent</b> higher <b>reliability</b> and smaller available form factor. Unfortunately rigid flex manufacturers have had limited material choices. This has led to higher cost materials and processes being used for less stringent applications, such as personal computing devices or automotive products, where cost drivers are significantly more important. Additionally, there is a constant drive in these commercial markets to reduce overall system size while increasing, or maintaining, high reliability. This paper will focus on work done to define circuit board performance around application rather than specification controlled criteria. Several circuit board materials have been evaluated which will allow for lower cost rigid flex circuits to be produced and address specific functional performance requirements. Criteria such as flex bend life cycling and minimum bend radii will be presented for a number of material options...|$|R
40|$|As {{electro-optical}} {{semiconductor devices}} become smaller, the actual operating region within the device shrinks to the micron or submicron order. Since the electrical power {{consumed in the}} device is concentrated in this minute space, there is considerable heat generated in the operat-ing region. Heat causes degradation of the <b>inherent</b> properties and <b>reliability</b> of the device. Devices that handle high power are particularly subject to demands for high-power han-dling, and due to device miniaturization, confront the most acute problem. The direct means of clarifying the cause of heat gen-eration is to measure temperature, but in micron-order ar-eas, ordinary techniques of temperature measurement cannot be used...|$|R
40|$|In this study, {{we compare}} the {{performance}} of four different imputation strategies ranging from the commonly used Listwise Deletion to model based approaches such as the Maximum Likelihood on enhancing completeness in incomplete software project data sets. We evaluate the impact {{of each of these}} methods by implementing them on six different real-time software project data sets which are classified into different categories based on their <b>inherent</b> properties. The <b>reliability</b> of the constructed data sets using these techniques are further tested by building prediction models using stepwise regression. The experimental results are noted and the findings are finally discussed. Keywords: Hot-deck; maximum likelihood; imputation. 1...|$|R
40|$|Surface {{micromachining}} is {{a technique}} for building electromechanical systems in silicon. A number of electromechanical systems have been implemented in miniature using the fundamental structural building blocks of tensile and non-tensile springs, differential capacitance sensing cells, and electrostatic drive. The integration of complicated mechanical structures and electrical circuits onto a single chip is expected to improve reliability and testability of systems. Reduction in interconnect wiring, {{the increased use of}} automation, and the <b>inherent</b> <b>reliability</b> of integrated circuits will all contribute to increased reliability of systems...|$|E
40|$|Accurate {{and dynamic}} {{reliability}} modeling for the running manufacturing {{system is the}} prerequisite to implement preventive maintenance. However, existing studies could not output the reliability value in real time because their abandonment of the quality inspection data originated in the operation process of manufacturing system. Therefore, this paper presents an approach to model the manufacturing system reliability dynamically based on their operation data of process quality and output data of product reliability. Firstly, {{on the basis of}} importance explanation of the quality variations in manufacturing process as the linkage for the manufacturing system reliability and product <b>inherent</b> <b>reliability,</b> the RQR chain which could represent the relationships between them is put forward, and the product qualified probability is proposed to quantify the impacts of quality variation in manufacturing process on the reliability of manufacturing system further. Secondly, the impact of qualified probability on the product <b>inherent</b> <b>reliability</b> is expounded, and the modeling approach of manufacturing system reliability based on the qualified probability is presented. Thirdly, the preventive maintenance optimization strategy for manufacturing system driven by the loss of manufacturing quality variation is proposed. Finally, the validity of the proposed approach is verified by the reliability analysis and optimization example of engine cover manufacturing system...|$|E
40|$|Over {{the last}} decade TNO has {{developed}} a deformable mirror concept using electromagnetic actuators with the main advantages of having very low non-linearity and hysteresis, low power consumption, and high <b>inherent</b> <b>reliability</b> of the actuators. TNO recently started a program to redesign the electromagnetic actuator to improve the actuator efficiency, allowing higher actuator force per volume and per wattage. The increased actuator efficiency gives improvement of the DM performance in terms of dynamical performance, actuation range, and power dissipation. With this technology various applications {{in the fields of}} ground-based astronomy and space missions are targeted...|$|E
40|$|In {{real world}} {{engineering}} applications {{the uncertainties of}} the structural parameters are inherent and the scatter from their nominal ideal values is in most cases unavoidable. These uncertainties play a dominant role in structural performance {{and the only way}} to assess this influence is to perform Reliability-Based Design Optimization (RBDO) and Robust Design Optimization (RDO). Compared to the basic deterministic-based optimization problem, a RBDO problem considers additional non-deterministic constraint functions, while the RDO yields a design with a state of robustness, so that its performance is the least sensitive to the variability of the uncertain parameters. The first part of this study examines the application of Neural Networks (NN) to the RBDO of large-scale structural systems, while the second part investigates the structural RDO problem. The use of NN in the framework of the RBDO problem is motivated by the approximate concepts <b>inherent</b> in <b>reliability</b> analysis and the time-consuming repeated analyses required by Monte Carlo Simulation. On the other hand the RDO is a multi-criteria optimization problem where the aim is to minimize both the weight of the structure and the variance of the structural response. (c) 2005 Elsevier Ltd. All rights reserved...|$|R
40|$|Solar Electric Propulsion {{has evolved}} into a {{demonstrated}} operational capability performing station keeping for geosynchronous satellites, enabling challenging deep-space science missions, and assisting in the transfer of satellites from an elliptical orbit Geostationary Transfer Orbit (GTO) to a Geostationary Earth Orbit (GEO). Advancing higher power SEP systems will enable numerous future applications for human, robotic, and commercial missions. These missions are enabled by either the increased performance of the SEP system or by the cost reductions when compared to conventional chemical propulsion systems. Higher power SEP systems that provide very high payload for robotic missions also trade favorably for the advancement of human exploration beyond low Earth orbit. Demonstrated reliable systems are required for human space flight and due to their successful present day widespread use and <b>inherent</b> high <b>reliability,</b> SEP systems have progressively become a viable entrant into these future human exploration architectures. NASA studies have identified a 30 kW-class SEP capability as the next appropriate evolutionary step, applicable to wide range of both human and robotic missions. This paper describes the planning options, mission applications, and technology investments for representative 30 kW-class SEP mission concepts under consideration by NAS...|$|R
40|$|Many {{distributed}} applications depend on explicit ordering to affect their lifecycle operations (start up, shutdown, re-configuration, etc.). Normally {{this procedure is}} carried out in a serialized manner. Consequently for applications containing large numbers of components, state and configuration changes are something to be avoided. They are costly, error prone and time consuming. This paper details an approach to application configuration which is specifically designed to address the issues of scale and <b>reliability</b> <b>inherent</b> in large {{distributed applications}}. The proposed architecture leads to applications which are ‘self healing’ in nature, and whose configuration mechanism is only loosely coupled {{to the number of}} components in a deployed system. Additionally, in applications where configuration changes can be planned in advance, the proposed mechanism {{can be used to make}} effective use of network resources in bandwidth limited environments...|$|R
