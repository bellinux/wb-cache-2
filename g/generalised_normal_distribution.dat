0|10000|Public
30|$|The input {{parameters}} {{used to evaluate}} wellbore stability are derived from data that tend to be inconsistent. The uncertainty in data is thus manifested in the results of predicted safe mud pressures. The variability of each input parameter should be accounted for {{in a manner that}} properly represents their uncertainties. This can be accomplished by employing quantitative risk analysis (QRA), where, as applied by Moos et al. (2003) and McLellan and Hawkes (1998), cumulative distribution density (CDF) and probability density function (PDF) curves are used to measure uncertainties in {{input parameters}}. Where actual/real data are accessible, CDF and PDF curves that more accurately portray disparities in values of input parameters are used. In the absence of reliable data, the values of input parameters are varied between upper and lower bounds with the PDF defined by an appropriate distribution function. Examples of continuous distribution supported on bounded intervals are uniform <b>distributions,</b> truncated <b>normal</b> <b>distributions,</b> logit-normal distributions, logarithmic distributions and triangular distributions. An example of a continuous distribution supported on semi-infinite intervals is the log-normal distribution, and an example of a continuous distribution that takes values over the whole “real line” is the <b>generalised</b> <b>normal</b> (<b>generalised</b> Gaussian) <b>distribution.</b> At one end, if there is absolutely no indication of the likely values, the uniform distribution is adopted because of the high degree of caution required. At the other end, it is pertinent to use the triangular distribution where a specific value has been identified as most likely to occur.|$|R
5000|$|Multivariate <b>normal</b> <b>distribution</b> (a complex <b>normal</b> <b>distribution</b> is a bivariate <b>normal</b> <b>distribution)</b> ...|$|R
50|$|The <b>normal</b> <b>distribution</b> {{and multivariate}} <b>normal</b> <b>distributions.</b>|$|R
5000|$|Complex <b>normal</b> <b>distribution,</b> an {{application}} of bivariate <b>normal</b> <b>distribution</b> ...|$|R
5000|$|Combining {{the above}} for a <b>normal</b> <b>distribution</b> [...] with both μ and σ2 unknown yields the {{following}} ancillary statistic:This simple combination is possible because the sample mean and sample variance of the <b>normal</b> <b>distribution</b> are independent statistics; {{this is only}} true for the <b>normal</b> <b>distribution,</b> and in fact characterizes the <b>normal</b> <b>distribution.</b>|$|R
40|$|The {{mixture of}} <b>normal</b> <b>distributions</b> {{provides}} a useful extension of the <b>normal</b> <b>distribution</b> for modeling of daily changes in market variables with fatter-than-normal tails and skewness. An efficient analytical Monte Carlo method is proposed for generating daily changes using a multivariate mixture of <b>normal</b> <b>distributions</b> with arbitrary covariance matrix. The main purpose of this method is to transform (linearly) a multivariate normal with an input covariance matrix into the desired multivariate mixture of <b>normal</b> <b>distributions.</b> This input covariance matrix can be derived analytically. Any linear combination of mixtures of <b>normal</b> <b>distributions</b> can {{be shown to be}} a mixture of <b>normal</b> <b>distributions...</b>|$|R
5000|$|An {{alternative}} parametric {{approach is}} {{to assume that the}} residuals follow a mixture of normal distributions; in particular, a contaminated <b>normal</b> <b>distribution</b> in which the majority of observations are from a specified <b>normal</b> <b>distribution,</b> but a small proportion are from a <b>normal</b> <b>distribution</b> with much higher variance. That is, residuals have probability [...] of coming from a <b>normal</b> <b>distribution</b> with variance , where [...] is small, and probability [...] of coming from a <b>normal</b> <b>distribution</b> with variance [...] for some ...|$|R
5000|$|In general, noncentrality {{parameters}} {{occur in}} distributions that are transformations of a <b>normal</b> <b>distribution.</b> The [...] "central" [...] versions {{are derived from}} <b>normal</b> <b>distributions</b> that have a mean of zero; the noncentral versions generalize to arbitrary means. For example, the standard (central) chi-squared distribution is the distribution of a sum of squared independent standard <b>normal</b> <b>distributions,</b> i.e., <b>normal</b> <b>distributions</b> with mean 0, variance 1. The noncentral chi-squared distribution generalizes this to <b>normal</b> <b>distributions</b> with arbitrary mean and variance.|$|R
25|$|This simple {{combination}} is possible because the sample mean and sample variance of the <b>normal</b> <b>distribution</b> are independent statistics; {{this is only}} true for the <b>normal</b> <b>distribution,</b> and in fact characterizes the <b>normal</b> <b>distribution.</b>|$|R
30|$|The {{white noise}} with the <b>normal</b> <b>distribution</b> {{characteristics}} is {{the premise of}} the time series model, that is, the multi-element fitting residual errors δ (t) should obey the <b>normal</b> <b>distribution.</b> The existing function of the <b>normal</b> <b>distribution</b> test in MATLAB was used to get that the multi-element fitting residual errors δ (t) obeyed the <b>normal</b> <b>distribution.</b>|$|R
5000|$|The two {{generalized}} normal families described here, {{like the}} skew normal family, are parametric families that extends the <b>normal</b> <b>distribution</b> {{by adding a}} shape parameter. Due to {{the central role of}} the <b>normal</b> <b>distribution</b> in probability and statistics, many distributions can be characterized in terms of their relationship to the <b>normal</b> <b>distribution.</b> For example, the lognormal, folded normal, and inverse <b>normal</b> <b>distributions</b> are defined as transformations of a normally-distributed value, but unlike the generalized normal and skew-normal families, these do not include the <b>normal</b> <b>distributions</b> as special cases.Actually all distributions with finite variance are in the limit highly related to the <b>normal</b> <b>distribution.</b> The Student-t distribution, the Irwin-Hall distribution and the Bates distribution also extend the <b>normal</b> <b>distribution,</b> and include in the limit the <b>normal</b> <b>distribution.</b> So there is no strong reason to prefer the [...] "generalized" [...] <b>normal</b> <b>distribution</b> of type 1, e.g. over a combination of Student-t and a normalized extended Irwin-Hall - this would include e.g. the triangular distribution (which cannot be modeled by the generalized Gaussian type 1).|$|R
2500|$|Multivariate <b>normal</b> <b>distribution</b> — a {{generalization}} of the <b>normal</b> <b>distribution</b> in multiple dimensions ...|$|R
50|$|<b>Normal</b> <b>distributions</b> are symmetrical, {{bell-shaped}} distributions {{that are}} useful in describing real-world data. The standard <b>normal</b> <b>distribution,</b> represented by the letter Z, is the <b>normal</b> <b>distribution</b> having a mean of 0 and {{a standard deviation of}} 1.|$|R
5000|$|Each {{component}} of the velocity vector has a <b>normal</b> <b>distribution</b> with mean [...] and standard deviation , so the vector has a 3-dimensional <b>normal</b> <b>distribution,</b> {{a particular kind of}} multivariate <b>normal</b> <b>distribution,</b> with mean [...] and standard deviation [...]|$|R
5000|$|Sampling {{from the}} matrix <b>normal</b> <b>distribution</b> {{is a special}} case of the {{sampling}} procedure for the multivariate <b>normal</b> <b>distribution.</b> Let [...] be an n by p matrix of np independent samples from the standard <b>normal</b> <b>distribution,</b> so that ...|$|R
40|$|In searching a {{opportunity}} distribution {{a function}} {{from one or}} more random usually in finishing with selected method, to be obtained by data is correctness. method of Transformation variable {{is one of the}} technique all important in determining probability functions. Determination of function of densities the probability in pass Jacobian and process integrate from <b>Normal</b> <b>distribution</b> Lean. <b>Normal</b> <b>Distribution</b> of Standard is one of the distribution function which used many from at other distribution functions. At the writing of this thesis aim to show probability functions to be alighted from by <b>Normal</b> <b>distribution</b> of Standard by 2 random variable and 3 random variable in the form of variable transformation. The <b>Normal</b> <b>distribution</b> of Standard with function of transformation selected variable at 2 obtained by random variable of <b>normal</b> <b>distribution</b> form, distribution of exponential-(), distribution of Cauchy-(1, 0), distribution of beta 2 -, distribution of gamma-(1, 1), and distribution of beta 1 -. While from <b>Normal</b> <b>distribution</b> of Standard with function of transformation selected variable at 3 obtained by random variable of <b>normal</b> <b>distribution</b> for-, <b>normal</b> <b>distribution</b> -, and distribution of beta 2...|$|R
40|$|In [16] {{a theory}} of {{generalised}} strongly [...] . In this paper we initiate {{a study of the}} inverse problem for <b>generalised</b> strongly <b>normal</b> extensions. We will henceforth call <b>generalised</b> strongly <b>normal</b> extensions, differential Galois extensions. We may as well begin by stating a general conjecture, where notation will be explained subsequently...|$|R
5000|$|In {{a special}} case when [...] the split <b>normal</b> <b>distribution</b> reduces to <b>normal</b> <b>distribution</b> with {{variance}} [...]|$|R
5000|$|It also {{coincides with}} a zero-mean <b>normal</b> <b>distribution</b> {{truncated}} from below at zero (see truncated <b>normal</b> <b>distribution)</b> ...|$|R
30|$|<b>Normal</b> <b>distribution</b> of the {{variables}} was evaluated with Kolmogorov-Smirnov tests. The results showed <b>normal</b> <b>distribution</b> for all variables.|$|R
40|$|Abstract: The log <b>normal</b> <b>distribution</b> is {{as popular}} in {{engineering}} as the <b>normal</b> <b>distribution</b> is in statistics. However, {{there has been}} little work relating to order statistics from the log <b>normal</b> <b>distribution.</b> In this note, explicit expressions are derived for the moments of order statistics from the log <b>normal</b> <b>distribution</b> by using a formula due to Withers. The usefulness of the result is illustrated through two data sets...|$|R
2500|$|The split <b>normal</b> <b>distribution</b> is most {{directly}} {{defined in terms}} of joining scaled sections of the density functions of different <b>normal</b> <b>distributions</b> and rescaling the density to integrate to one. [...] The truncated <b>normal</b> <b>distribution</b> results from rescaling a section of a single density function.|$|R
5000|$|Cramér's theorem {{shows that}} while the <b>normal</b> <b>distribution</b> is {{infinitely}} divisible, {{it can only be}} decomposed into <b>normal</b> <b>distributions.</b>|$|R
30|$|The <b>normal</b> <b>distribution</b> {{is perhaps}} the most {{commonly}} used probability distribution in both statistical theory and applications. The <b>normal</b> <b>distribution</b> was first used by de Moivre (1733) in the literature as an approximation to the binomial distribution. However, the development of the <b>normal</b> <b>distribution</b> by Gauss (1809, 1816) became the standard used in the modern statistics. Hence, the <b>normal</b> <b>distribution</b> is also commonly known as the Gaussian distribution. Properties of the <b>normal</b> <b>distribution</b> have been well developed (e.g., see Johnson et al. 1994; Patel and Read 1996). The distribution also {{plays an important role in}} generating new distributions.|$|R
500|$|The {{simplest}} {{case of a}} <b>normal</b> <b>distribution</b> {{is known}} as the standard <b>normal</b> <b>distribution,</b> described by this probability density function: ...|$|R
50|$|Thus, {{while the}} <b>normal</b> <b>distribution</b> is {{infinitely}} divisible, {{it can only}} be decomposed into <b>normal</b> <b>distributions</b> (if the summands are independent).|$|R
5000|$|... (since X and &minus;X {{both have}} the same <b>normal</b> <b>distribution),</b> where [...] is the {{cumulative}} distribution function of the <b>normal</b> <b>distribution..</b>|$|R
40|$|We {{introduce}} the standard two-sided power <b>normal</b> <b>distribution</b> {{and consider the}} relation between the probability in standard two-sided power distribution and the probability in standard two-sided power <b>normal</b> <b>distribution</b> and obtain the even moment of the special two-sided power <b>normal</b> <b>distribution</b> including the cases considered by Gupta and Nadarajah(2004...|$|R
40|$|Hermite polynomials {{are used}} to derive {{expressions}} for the moments {{about the origin of}} univariate and multivariate <b>normal</b> <b>distributions.</b> A recurrence relation derived for multivariate Hermite polynomials leads to a recurrence relation for the multivariate normal moments. Bivariate <b>normal</b> <b>distribution</b> Chebyshev-Hermite polynomial Multivariate Hermite polynomial Multivariate <b>normal</b> <b>distribution...</b>|$|R
50|$|The split <b>normal</b> <b>distribution</b> {{results from}} merging {{two halves of}} <b>normal</b> <b>distributions.</b> In a general case the 'parent' <b>normal</b> <b>distributions</b> can have {{different}} variances which implies that the joined PDF would not be continuous. To ensure that the resulting PDF integrates to 1, the normalizing constant A is used.|$|R
50|$|The {{primary reason}} that the chi-squared {{distribution}} is used extensively in hypothesis testing is {{its relationship to the}} <b>normal</b> <b>distribution.</b> Many hypothesis tests use a test statistic, such as the t statistic in a t-test. For these hypothesis tests, as the sample size, n, increases, the sampling distribution of the test statistic approaches the <b>normal</b> <b>distribution</b> (Central Limit Theorem). Because the test statistic (such as t) is asymptotically normally distributed, provided the sample size is sufficiently large, the distribution used for hypothesis testing may be approximated by a <b>normal</b> <b>distribution.</b> Testing hypotheses using a <b>normal</b> <b>distribution</b> is well understood and relatively easy. The simplest chi-squared distribution is the square of a standard <b>normal</b> <b>distribution.</b> So wherever a <b>normal</b> <b>distribution</b> could be used for a hypothesis test, a chi-squared distribution could be used.|$|R
5000|$|If [...] {{follows a}} <b>normal</b> <b>distribution,</b> the rate {{function}} becomes a parabola with its apex at {{the mean of}} the <b>normal</b> <b>distribution.</b>|$|R
5000|$|Suppose [...] has a <b>normal</b> <b>distribution</b> {{and lies}} within the {{interval}} [...] Then [...] conditional on [...] has a truncated <b>normal</b> <b>distribution.</b>|$|R
5000|$|Simple {{examples}} can {{be given}} by a mixture of two <b>normal</b> <b>distributions.</b> (See Multimodal distribution#Mixture of two <b>normal</b> <b>distributions</b> for more details.) ...|$|R
50|$|A {{standard}} normal table, {{also called}} the unit normal table or Z table, is a mathematical table for the values of Φ, which are {{the values of the}} cumulative distribution function of the <b>normal</b> <b>distribution.</b> It is used to find the probability that a statistic is observed below, above, or between values on the standard <b>normal</b> <b>distribution,</b> and by extension, any <b>normal</b> <b>distribution.</b> Since probability tables cannot be printed for every <b>normal</b> <b>distribution,</b> as there are an infinite variety of <b>normal</b> <b>distributions,</b> it is common practice to convert a normal to a standard normal and then use the standard normal table to find probabilities.|$|R
40|$|It is {{well known}} that the hazard rate of a {{univariate}} <b>normal</b> <b>distribution</b> is increasing. In this paper, we prove that the hazard gradient, in the case of general multivariate <b>normal</b> <b>distribution,</b> is increasing in the sense of Johnson and Kotz. Multivariate <b>normal</b> <b>distribution</b> multivariate increasing hazard rate hazard gradient...|$|R
