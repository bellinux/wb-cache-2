2841|680|Public
5|$|The VA Office of Inspector General (OIG) {{reports in}} 2005, 2007, and 2008 found the {{reported}} outpatient waiting {{times to be}} unreliable because of <b>data</b> <b>integrity</b> concerns associated with VHA’s scheduling system., The discrepancies found by the OIG between requested appointment times documented in medical records and in the databases, and incomplete waiting lists are attributed to patient preference or the scheduler’s use of inappropriate scheduling procedures. Veterans Affairs officials warned the Obama-Biden transition team {{in the weeks after}} the 2008 presidential election that the department should not trust the wait times that its facilities were reporting.|$|E
5|$|The {{shift to}} {{outsourcing}} raises issues about <b>data</b> <b>integrity,</b> regulatory oversight, language difficulties, {{the meaning of}} informed consent among a much poorer population, the standards of clinical care, {{the extent to which}} corruption may be regarded as routine in certain countries, and the ethical problem of raising a population's expectations for drugs that most of that population cannot afford. It also {{raises the question of whether}} the results of clinical trials using one population can invariably be applied elsewhere. There are both social and physical differences: Goldacre asks whether patients diagnosed with depression in China are really the same as patients diagnosed with depression in California, and notes that people of Asian descent metabolize drugs differently from Westerners.|$|E
25|$|Enterprise HDDs {{can have}} sector sizes larger than 512 bytes (often 520, 524, 528 or 536 bytes). The {{additional}} per-sector space {{can be used}} by hardware RAID controllers or applications for storing <b>Data</b> <b>Integrity</b> Field (DIF) or <b>Data</b> <b>Integrity</b> Extensions (DIX) data, resulting in higher reliability and prevention of silent data corruption.|$|E
5000|$|... <b>data</b> whose <b>integrity</b> is of {{critical}} importance (for example, parameters or files).|$|R
30|$|In this section, {{we focus}} on two {{potential}} defenses against DSMAs: <b>Data</b> Flow <b>Integrity</b> and Data-Plane Randomization.|$|R
40|$|Although {{treatment}} fidelity {{strategies for}} enhancing {{the integrity of}} behavioral interventions have been well described, little {{has been written about}} monitoring <b>data</b> collection <b>integrity.</b> This article describes the principles and strategies developed to monitor <b>data</b> collection <b>integrity</b> of the "Stories and Music for Adolescent/Young Adult Resilience During Transplant" study (R 01 NR 008583; U 10 CA 098543; U 10 CA 095861) [...] a multi-site Children's Oncology Group randomized clinical trial of a music therapy intervention for adolescents and young adults undergoing stem cell transplant. The principles and strategies outlined in this article provide one model for development and evaluation of a <b>data</b> collection <b>integrity</b> monitoring plan for behavioral interventions that may be adapted by investigators and may be useful to funding agencies and grant application reviewers in evaluating proposals...|$|R
25|$|Data corroboration, {{including}} the use of check sum, double-keying, message authentication, and digital signature may be used to ensure <b>data</b> <b>integrity.</b>|$|E
25|$|This {{abstraction}} {{also allows}} upper layers {{to provide services}} that the lower layers do not provide. While the original OSI model was extended to include connectionless services (OSIRM CL), IP is not designed to be reliable and is a best effort delivery protocol. This means that all transport layer implementations must choose whether or how to provide reliability. UDP provides <b>data</b> <b>integrity</b> via a checksum but does not guarantee delivery; TCP provides both <b>data</b> <b>integrity</b> and delivery guarantee by retransmitting until the receiver acknowledges the reception of the packet.|$|E
25|$|GLBA {{compliance}} is mandatory; {{whether a}} financial institution discloses nonpublic information or not, {{there must be a}} policy in place to protect the information from foreseeable threats in security and <b>data</b> <b>integrity.</b>|$|E
5000|$|... <b>data</b> {{quality and}} <b>integrity</b> {{for use in}} resource/reserve estimation, and; ...|$|R
50|$|The <b>Data</b> Cap <b>Integrity</b> Act, {{also called}} the <b>Data</b> Measurement <b>Integrity</b> Act, is a bill {{introduced}} in the United States Senate by Senator Ron Wyden. The bill would require Internet service providers that have bandwidth caps to only apply caps on service to reduce network congestion rather than discourage Internet use, count all data usage equally toward caps, regardless of its source or content, and use a standard method of metering data use, {{which is to be}} defined by the Federal Communications Commission (FCC). The FCC would also be required to provide software to allow users to monitor their bandwidth usage.|$|R
30|$|Thus, in this paper, the IBS (Identity Based Signature) {{was applied}} for the safe {{transmission}} of weather <b>data,</b> where <b>integrity</b> and confidentiality were achieved as a discernment factor by generating a private key.|$|R
25|$|Message {{authentication}} code (MAC) is {{used for}} <b>data</b> <b>integrity.</b> HMAC {{is used for}} CBC mode of block ciphers and stream ciphers. AEAD is used for Authenticated encryption such as GCM mode and CCM mode.|$|E
25|$|Finally, DHTs {{must deal}} with more {{traditional}} distributed systems issues such as load balancing, <b>data</b> <b>integrity,</b> and performance (in particular, ensuring that operations such as routing and data storage or retrieval complete quickly).|$|E
25|$|As {{companies}} {{have become more}} integrated within their own departments and with other companies, such as suppliers and retailers, a desire for <b>data</b> <b>integrity</b> throughout the electronic data exchange process is also driving demand for continuous auditing.|$|E
40|$|Data bases {{are one of}} {{the most}} {{important}} components in every large informatics system which stores and processes data and information. Because data bases contain all of the valuable information about a company, its clients, its financial activity, they represent one of the key elements in the structure of an organization, which determines imperatives such as confidentiality, integrity and ease of data access. The current paper discuses the <b>integrity</b> of <b>data</b> bases and it refers to the validity and the coherence of stored <b>data.</b> Usually, <b>integrity</b> is defined in connection with terms of constraint, that are rules regarding coherence which the data base cannot infringe. <b>Data</b> base that <b>integrity</b> refers to information correctness and assumes to detect, correct and prevent errors that might have an effect on the data comprised by the data bases. data security; data base security; <b>data</b> protection; <b>integrity</b> restrictions; control...|$|R
5000|$|... 2006-2012 U.S. Department of Homeland Security <b>Data</b> Privacy and <b>Integrity</b> Advisory Committee (DPIAC) ...|$|R
5000|$|OptumInsight - Health <b>Data</b> Analytic, Payment <b>Integrity,</b> Life Science, Risk Quality & Network Solutions, Medical Billing ...|$|R
25|$|CDA and CCM are {{complementary}} processes. Neither {{process is}} self-sufficient or comprehensive. Even if no data faults are found it cannot {{be concluded that}} controls are fail-safe. Further, even if controls are being implemented, <b>data</b> <b>integrity</b> cannot be assumed. When combined, however, these monitoring approaches present a more complete reliance picture.|$|E
25|$|TLS {{supports}} {{many different}} methods for exchanging keys, encrypting data, and authenticating message integrity (see Algorithm below). As a result, secure configuration of TLS involves many configurable parameters, {{and not all}} choices provide all of the privacy-related properties described in the list above (see authentication and key exchange table, cipher security table, and <b>data</b> <b>integrity</b> table).|$|E
25|$|Originally, {{security}} concerns were not major design considerations for DNS software or any software for deployment {{on the early}} Internet, as the network was not open for participation by the general public. However, {{the expansion of the}} Internet into the commercial sector in the 1980s changed the requirements for security measures to protect <b>data</b> <b>integrity</b> and user authentication.|$|E
40|$|SSL {{splitting}} is a cryptographic {{technique to}} guarantee that public data served by caching Web proxies is endorsed by the originating server. When a client makes a request, the trusted server generates a stream of authentication records and sends them to the untrusted proxy, which combines them with a stream of data records retrieved from its local cache. The combined stream is relayed to the client, a standard Web browser, which verifies the <b>data’s</b> <b>integrity.</b> Since the combined stream simulates a normal Secure Sockets Layer (SSL) [7] connection, SSL splitting works with unmodified browsers; however, since it does not provide confidentiality, it is appropriate for applications that require only authentication. The serve...|$|R
5000|$|SHA-2 (Secure Hash Algorithm 2) {{is a set}} of {{cryptographic}} hash functions designed by the United States National Security Agency (NSA). Cryptographic hash functions are mathematical operations run on digital data; by comparing the computed [...] "hash" [...] (the output from execution of the algorithm) to a known and expected hash value, a person can determine the <b>data's</b> <b>integrity.</b> For example, computing the hash of a downloaded file and comparing the result to a previously published hash result can show whether the download has been modified or tampered with. A key aspect of {{cryptographic hash}} functions is their collision resistance: nobody should be able to find two different input values that result in the same hash output.|$|R
40|$|<b>Data</b> flow <b>integrity</b> {{enforcement}} overview <b>Data</b> flow <b>integrity</b> enforcement uses {{static analysis}} to computer a data flow graph. Program excute following such data – flow graph. <b>Data</b> flow <b>integrity</b> enforcement can be automatically applied to C & C++ without any mofification. It has no false positive and low overhead Compare other proposals to prevent attacks on software: • Its overhead is lower; • Not defend from attacks that overwrite specific targets or {{specific types of}} Vulnerabilities, but a broad class of attacks and prevent both control data attacks and non-control data attacks using data-flow integrity; • No false positives. TU Dresden, 04,Jun, 2007 Securing software by enforcing data-flow integrity Folie 3 von 23 Generel technology about data-flow integrity enforcement How to ensure runtime <b>data</b> flow <b>integrity?</b> The implement uses reaching definitions analysis to compute a static data-flow graph to enforce data-flow integrity at runtime. Maintains a table with the identifier of the last instruction to write to each memory positon. Each memory position Identifier of the last instruction to write • Only compute the set of instructions that may write the value to low overhead and increase performance • We need {{check to make sure}} whether the value read from the table is computed by the static analysis or not. If not, we raise an exception • Update the table before every write • Prevent the attacker from tampering with the table TU Dresden, 04,Jun, 2007 Securing software by enforcing data-flow integrity Folie 4 von 23 Reduce overhead Compute equivalence classes of instructions and assigns the same identifier to all the instructions in the same Class. Identifiers for different equivalence classes of instruction Different equivalence classes of instructio...|$|R
25|$|A 2012 {{research}} {{showed that}} neither {{any of the}} then-major and widespread filesystems (such as UFS, Ext, XFS, JFS, or NTFS) nor hardware RAID (which has some issues with <b>data</b> <b>integrity)</b> provided sufficient protection against data corruption problems. Initial research indicates that ZFS protects data better than earlier efforts. It is also faster than UFS and {{can be seen as}} its replacement.|$|E
25|$|As the {{standard}} definition interface carries no checksum, CRC, or other <b>data</b> <b>integrity</b> check, an EDH (Error Detection and Handling) packet may be optionally {{placed in the}} vertical interval of the video signal. This packet includes CRC values for both the active picture, and the entire field (excluding those lines at which switching may occur, and which should contain no useful data); equipment can compute their own CRC and compare it with the received CRC in order to detect errors.|$|E
25|$|Only a tiny {{fraction}} of the detected errors ends up as not correctable. For example, specification for an enterprise SAS disk (a model from 2013) estimates this fraction to be one uncorrected error in every 1016 bits, and another SAS enterprise disk from 2013 specifies similar error rates. Another modern (as of 2013) enterprise SATA disk specifies an error rate of less than 10 non-recoverable read errors in every 1016 bits. An enterprise disk with a Fibre Channel interface, which uses 520 byte sectors to support the <b>Data</b> <b>Integrity</b> Field standard to combat data corruption, specifies similar error rates in 2005.|$|E
5000|$|Integrity or {{validation}} rules, {{also known}} as constraints, restrict the set of facts and the transitions between the permitted sets of facts to those that are considered useful. In terms of <b>data</b> quality, <b>integrity</b> rules are used to guarantee {{the quality of the}} facts.|$|R
50|$|The GuideStar Exchange {{awards are}} given to a {{non-profit}} organization base on their commitment to <b>data</b> transparency and <b>integrity.</b>|$|R
40|$|Radio Frequency Identification (RFID) is an {{emerging}} wireless object identification technology with many potential {{applications such as}} supply chain management, personnel tracking and healthcare. However, security vulnerabilities of the RFID system have been a serious concern for its wide adoption in many applications. Although much {{work has been done}} to provide privacy and anonymity, little focus has been given to ensure RFID <b>data</b> confidentiality, <b>integrity</b> and to address the tampered data recovery problem. To this end, we propose a lightweight stenographic-based approach to ensure RFID <b>data</b> confidentiality and <b>integrity</b> as well as the recovery of tampered RFID data...|$|R
25|$|LMDB {{may also}} be used {{concurrently}} in a multi-threaded or multi-processing environment, with read performance scaling linearly by design. LMDB databases may have only one writer at a time, however unlike many similar key-value databases, write transactions do not block readers, nor do readers block writers. LMDB is also unusual in that multiple applications on the same system may simultaneously open and use the same LMDB store, {{as a means to}} scale up performance. Also, LMDB does not require a transaction log (thereby increasing write performance by not needing to write data twice) because it maintains <b>data</b> <b>integrity</b> inherently by design.|$|E
25|$|One {{major feature}} that distinguishes ZFS from other file systems {{is that it}} is {{designed}} with a focus on <b>data</b> <b>integrity</b> by protecting the user's data on disk against silent data corruption caused by data degradation, current spikes, bugs in disk firmware, phantom writes (the previous write did not make it to disk), misdirected reads/writes (the disk accesses the wrong block), DMA parity errors between the array and server memory or from the driver (since the checksum validates data inside the array), driver errors (data winds up in the wrong buffer inside the kernel), accidental overwrites (such as swapping to a live file system), etc.|$|E
25|$|The {{university}} {{agreed to}} lease 43 acres on the STAR campus to The Data Centers (TDC) {{for the construction}} of the data center. The data center plan included a combined heat cycle natural gas-fired power plant capable of generating 279 megawatts of energy. TDC claimed that the power plant was critical to ensuring an uninterrupted electrical power supply to the facility, which is critical for <b>data</b> <b>integrity.</b> The TDC business plan also called for sale of excess electricity. Portions of the Newark community questioned the business plan, claiming that the power plant is not an auxiliary part of the data center but a separate industrial use, which would violate the zoning of the STAR campus.|$|E
3000|$|... <b>integrity</b> <b>data</b> can be {{incorrect}} through {{independent and}} common cause failures {{stemming from the}} monitoring system at the ground segment.|$|R
50|$|The {{standard}} {{also provides}} two RSNA <b>data</b> confidentiality and <b>integrity</b> protocols, TKIP and CCMP, with implementation of CCMP being mandatory.|$|R
40|$|Major {{system design}} {{features}} of a distributed data management system for the NASA Deep Space Network (DSN) designed for continuous two-way deep space communications are described. The reasons for which the distributed data base utilizing third-generation minicomputers is selected as the optimum approach for the DSN are threefold: (1) with a distributed master data base, valid data is available in real-time to support DSN management activities at each location; (2) <b>data</b> base <b>integrity</b> {{is the responsibility of}} local management; and (3) the data acquisition/distribution and processing power of a third-generation computer enables the computer to function successfully as a data handler or as an on-line process controller. The concept of the distributed data base is discussed along with the software, <b>data</b> base <b>integrity,</b> and hardware used. The data analysis/update constraint is examined...|$|R
