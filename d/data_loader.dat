22|15|Public
50|$|ARINC 615 {{defines a}} {{high-speed}} <b>data</b> <b>loader</b> protocol layered upon the ARINC 429 physical layer.|$|E
5000|$|... 2006: Introduced Release 6, which added Flight Director, V-Speed & Heading on ADI, {{additional}} datalink weather {{products on}} the MFD, {{and support for the}} USB memory-stick <b>data</b> <b>loader.</b>|$|E
50|$|There are {{two levels}} of {{licensing}} of AQT: the Standard License, which allows most functionality, and the Extended License, which allows full use including the administration tools, data compare, and the <b>data</b> <b>loader.</b>|$|E
5000|$|Mid-1990s: Standard & Poor’s, Thomson Financial, DRI and FT Interactive <b>Data</b> product <b>loaders</b> created ...|$|R
40|$|Abstract: Several {{goals of}} Intelligent Transportation Systems are to improve productivity, ease traffic {{congestion}} and minimize road risks {{through the use}} of advanced communications technologies. The RITIS software system developed at University of Maryland inside the Center for Advanced Transportation Technology Laboratory aims to improve communication among transportation agencies through data integration and sharing. We present the RITIS Development Framework, a software framework that aims to improve the design and maintainability of agency-to-RITIS <b>data</b> <b>loaders</b> and translators, while also minimizing the training and learning required to develop <b>loaders</b> for future <b>data</b> sources...|$|R
40|$|Faster RCNN model Sequence to Sequence {{container}} and char_rae recurrent autoencoder model Reshape Layer that reshapes the input [# 221] Pip requirements in requirements. txt updated to latest versions [# 289] Remove deprecated <b>data</b> <b>loaders</b> and update docs Use NEON_DATA_CACHE_DIR envvar as archive dir to store DataLoader ingested data Eliminate type conversion for FP 16 for CUDA compute capability >= 5. 2 Use GEMV kernels for batch size 1 Alter delta buffers for nesting of merge-broadcast layers Support for ncloud real-time logging Add fast_style Makefile target Fix Python 3 builds on Ubuntu 16. 04 Run setup. py for sysinstall to generate version. py [# 282] Fix broken link in mnist docs Fix conv/deconv tests for CPU execution and fix i 32 data type Fix for average pooling with batch size 1 Change default scale_min to allow random cropping if omitted Fix yaml loading Fix bug with image resize during injest Update {{references to the}} ModelZoo and neon examples to their new location...|$|R
50|$|MonetDB has a SAM/BAM module for {{efficient}} {{processing of}} sequence alignment data. Aimed at the bioinformatics research, the module has a SAM/BAM <b>data</b> <b>loader</b> {{and a set}} of SQL UDFs for working with DNA data. The module uses the popular SAMtools library.|$|E
5000|$|AQT Extended Edition {{includes}} a <b>data</b> <b>loader</b> tool {{that makes it}} easy to load data into tables from various sources. The sources can be other tables on the same database, a different database, a completely different kind of database (i.e. can load DB2 table from Oracle table), an Excel spreadsheet, a comma delimited (CSV) file, a flat file, or even a complicated ...|$|E
50|$|Distributed R {{is an open}} source, {{high-performance}} {{platform for}} the R language. It splits tasks between multiple processing nodes to reduce execution time and analyze large data sets. Distributed R enhances R by adding distributed data structures, parallelism primitives to run functions on distributed data, a task scheduler, and multiple data loaders. It is mostly used to implement distributed versions of machine learning tasks. Distributed R is written in C++ and R, and retains the familiar {{look and feel of}} R. As of February 2015, Hewlett-Packard (HP) provides enterprise support for Distributed R with proprietary additions such as a fast <b>data</b> <b>loader</b> from the Vertica database.|$|E
40|$|As the availability, {{affordability}} {{and magnitude}} of genomics and genetics research increases so does {{the need to provide}} online access to resulting data and analyses. Availability of a tailored online database is the desire for many investigators or research communities; however, managing the Information Technology infrastructure needed to create such a database can be an undesired distraction from primary research or potentially cost prohibitive. Tripal provides simplified site devel-opment by merging the power of Drupal, a popular web Content Management System with that of Chado, a community-derived database schema for storage of genomic, genetic and other related biological data. Tripal provides an interface that extends the content management features of Drupal to the data housed in Chado. Furthermore, Tripal provides a web-based Chado installer, genomic <b>data</b> <b>loaders,</b> web-based editing of data for organisms, genomic features, biological libraries, controlled vocabularies and stock collections. Also available are Tripal extensions that support loading and visu-alizations of NCBI BLAST, InterPro, Kyoto Encyclopedia of Genes and Genomes and Gene Ontology analyses, as well as an extension that provides integration of Tripal with GBrowse, a popular GMOD tool. An Application Programming Interfac...|$|R
40|$|The {{design and}} {{characteristics}} of a multifunction display system are discussed. The system is a general purpose, time-shared, electro-optical display driven by onboard data management systems. It is capable of presenting information in forms such as alphanumerics, symbolics, and graphics, separately or in combination. The components of the system are: (1) <b>data</b> terminal simulator/input <b>loader,</b> (2) keyboard controller, (3) CRT display indicator, and (4) programmable symbol generator...|$|R
40|$|International audienceThe day-to-day {{instrument}} detection but automatic compensations were stable. Manual compensation settings were satisfactory using available facilities. Commercial and UK NEQAS {{quality control}} results were acceptable. The intra-experiment reproducibility was good (coefficient of variation (CV) 0. 87) and absolutes values {{were very close}} (slopes > 0. 89). The gating strategy, fluorochrome choice, and compensation setting are discussed. A few improvements are expected (sample <b>loader,</b> <b>data</b> management, auto-gating, acquisition parameters, sample mixing, absolute values calculation, etc). In conclusions, despite its complexity, 6 color staining is a reliable, stable, and highly informative technique for lymphocyte subset monitoring but remains to be optimized...|$|R
30|$|<b>Data</b> <b>loader</b> The <b>data</b> <b>loader</b> is {{responsible}} for transforming simulation data to the format required for storage in the database system. First, it can read and understand data generated by current simulation programs and stored in popular MS file formats (e.g., GROMACS, PDB). We developed, {{as a part of}} the <b>data</b> <b>loader,</b> a unified data transformation module to perform such translations. A user only needs to specify the file format his/her data follows and data loading will be performed automatically. Second, the raw data will also be sent to a Data Compression module to generate a volume-reduced copy to be stored as compressed files. The rationale of this design is to enable efficient transmission of MS data among different data centers where DCMS are deployed. We will address this issue in Section “Data compression”.|$|E
40|$|This report {{describes}} {{the development of}} an XML Schema <b>data</b> <b>loader</b> that loads XML Schema based data into an Amos II database whose schema has been generated from the data's XML Schema definition. The Waterloo Benchmark XBench is used as test case for the loader. The translator that maps XML Schem...|$|E
40|$|Update <b>Data</b> <b>Loader</b> to aeon [URL] for flexible, multi-threaded {{data loading}} and transformations Add Neural Machine Translation model Remove Fast RCNN model (use Faster RCNN model instead) Remove music_genres example Fix super {{blocking}} for small N with 1 D conv Fix update-direct conv kernel for small N Add gradient clipping to Adam optimizer Documentation updates and bug fixe...|$|E
40|$|The Ontology Lookup Service (OLS; [URL] {{has been}} {{providing}} several means to query, browse and navigate biomedical ontologies and controlled vocabularies since it first went into production 4 years ago, and usage statistics {{indicate that it}} has become a heavily accessed service with millions of hits monthly. The volume of data available for querying has increased 7 -fold since its inception. OLS functionality has been integrated into several high-usage databases and data entry tools. Improvements in the <b>data</b> model and <b>loaders,</b> as well as interface enhancements have made the OLS easier to use and capture more annotations from the source data. In addition, newly released software packages now provide easy means to fully integrate OLS functionality in external applications...|$|R
40|$|One {{fundamental}} property underlies most biological databanks: their availability in text format. We propose {{an approach}} to retrieve and convert biological data stored in textual flat files into information in formats more suitable for further analysis and use by client applications. Extracted data can be exported into different data structures (CORBA objects, DBMS relations or objects, C structures, HTML reports). These <b>data</b> structures are <b>loader</b> objects. They are generated based on loader specifications which specify how to get specific pieces of information and how to package them. This article focuses on CORBA loaders. There is a growing interest for CORBA in the Bioinformatics, and more generally the Life Science Research community, because CORBA {{is seen as a}} unifying framework that could help the integration of heterogeneous data sources and applications using these data. Keywords Semistructured Textual Data, Data Extraction, Data Conversion, CORBA Wrappers, Bioinformatics. The [...] ...|$|R
40|$|A recent upgrade of the TSRV {{research}} flight {{system at}} NASA Langley Research Center retained the original monochrome display system. However, the display memory loading equipment was replaced requiring design {{and development of}} new methods of performing this task. This paper describes the new techniques developed to load memory in the display system. An outdated paper tape method for loading the BOOTSTRAP control program was replaced by EPROM storage of the characters contained on the tape. Rather than move a tape past an optical reader, a counter was implemented which steps sequentially through EPROM addresses and presents the same <b>data</b> to the <b>loader</b> circuitry. A cumbersome cassette tape method for loading the applications software was replaced with a floppy disk method using a microprocessor terminal installed {{as part of the}} upgrade. The cassette memory image was transferred to disk and a specific software loader was written for the terminal which duplicates the function of the cassette loader...|$|R
40|$|Background: In the {{scientific}} biodiversity community, {{it is increasingly}} perceived the {{need to build a}} bridge between molecular and traditional biodiversity studies. We believe that the information technology could have a preeminent role in integrating the information generated by these studies with the large amount of molecular data we can find in bioinformatics public databases. This work is primarily aimed at building a bioinformatic infrastructure for the integration of public and private biodiversity data through the development of GIDL, an Intelligent <b>Data</b> <b>Loader</b> coupled with the Molecular Biodiversity Database. The system presented here organizes in an ontological way and locally stores the sequence and annotation data contained in the GenBank primary database. Methods: The GIDL architecture consists of a relational database and of an intelligent <b>data</b> <b>loader</b> software. The relational database schema is designed to manage biodiversity information (Molecular Biodiversity Database) and it is organized in four areas: MolecularData, Experiment, Collection and Taxonomy. The MolecularData area is inspired to an established standard in Generic Model Organism Databases, the Chado relational schema. The peculiarity of Chado, and also its strength, is the adoption of an ontological schema which makes use of the Sequence Ontology...|$|E
40|$|The thesis {{conducts a}} study in how {{datasets}} obtained from parallel simulation can represented and filtered in a visualization system. The primary result of this thesis is {{the implementation of a}} visualization system, named PVis, that is able to load and display parallel datasets. The visualization system is composed of a <b>data</b> <b>loader,</b> a user interface and filter modules. The main feature that separates PVis from other visualization tools is its ability to load a parallel dataset and treat the individual subdomain datasets collectively as a single dataset...|$|E
40|$|SDSFIE Implementation The Spatial Data Standards (SDSFIE) toolbox {{consists}} of tools {{to generate a}} SDSFIE geodatabase and migrate existing data into the geodatabase, either personal or multiuser. These tools are the <b>Data</b> <b>Loader,</b> Data Checker, and Data Analyzer. The object of migration is to not lose any data and {{to get as much}} across at one time as possible without losing any data. There are several constraints that have to be considered when migrating data. Feature level metadata is necessary and the tools will allow a user to generate this, too...|$|E
40|$|The Human Brain Project {{consortium}} {{continues to}} struggle with effective sharing of tools. To facilitate reuse of its tools, the Stanford Psychiatry Neuroimaging Laboratory (SPNL) has developed BrainImageJ, a new software framework in Java. The framework consists of two components—a set of four programming interfaces and an application front end. The four interfaces define extension pathways for new <b>data</b> models, file <b>loaders</b> and savers, algorithms, and visualization tools. Any Java class that implements one of these interfaces qualifies as a BrainImageJ plug-in—a self-contained tool. After automatically detecting and incorporating new plug-ins, the application front end transparently generates graphical user interfaces that provide access to plug-in functionality. New plug-ins interoperate with existing ones immediately through the front end. BrainImageJ is used at the Stanford Psychiatry Neuroimaging Laboratory to develop image-analysis algorithms and three-dimensional visualization tools. It {{is the goal of}} our development group that, once the framework is placed in the public domain, it will serve as an interlaboratory platform for designing, distributing, and using interoperable tools...|$|R
40|$|A single chip {{system for}} real [...] time mpeg [...] 2 {{decoding}} {{can be created}} by integrating a general purpose dual [...] issue risc processor, with a small dedicated hardware for the variable length decoding (vld) and block loading processes; a 32 kb instruction ram; and a 32 kb data ram. The vld hardware performs Huffman decoding on the input <b>data.</b> The block <b>loader</b> performs the half [...] sample prediction for motion compensation and acts as a direct memory access (dma) controller for the risc processor by transferring data between an external 2 mb dram and the internal 32 kb data ram. The dual [...] issue risc processor, running at 250 mhz, is enhanced {{with a set of}} key sub [...] word and multimedia instructions for a sustained peak performance of 1000 mops. With this setup for mpeg [...] 2 decoding applications, bi [...] directionally predicted non [...] intra video blocks are decoded in less than 800 cycles, leading to a single chip, real [...] time mpeg [...] 2 decoding system. Keywords: vliw processor, multimedia systems, mpeg [...] 2 dec [...] ...|$|R
40|$|Information about {{wheel loader}} usage {{can be used}} in several ways to {{optimize}} customer adaption. First, optimizing the configuration and component sizing of a wheel loader to customer needs can lead to a significant improvement in e. g. fuel efficiency and cost. Second, relevant driving cycles {{to be used in the}} development of wheel loaders can be extracted from usage data. Third, on-line usage identification opens up for the possibility of implementing advanced look-ahead control strategies for wheel loader operation. The main objective here is to develop an on-line algorithm that automatically, using production sensors only, can extract information about the usage of a machine. Two main challenges are that sensors are not located with respect to this task and that significant usage disturbances typically occur during operation. The proposed method is based on a combination of several individually simple techniques using signal processing, state automaton techniques, and parameter estimation algorithms. The approach is found to berobust when evaluated on measured <b>data</b> of wheel <b>loaders</b> loading gravel and shot rock...|$|R
40|$|Abstract Background In the {{scientific}} biodiversity community, {{it is increasingly}} perceived the {{need to build a}} bridge between molecular and traditional biodiversity studies. We believe that the information technology could have a preeminent role in integrating the information generated by these studies with the large amount of molecular data we can find in bioinformatics public databases. This work is primarily aimed at building a bioinformatic infrastructure for the integration of public and private biodiversity data through the development of GIDL, an Intelligent <b>Data</b> <b>Loader</b> coupled with the Molecular Biodiversity Database. The system presented here organizes in an ontological way and locally stores the sequence and annotation data contained in the GenBank primary database. Methods The GIDL architecture consists of a relational database and of an intelligent <b>data</b> <b>loader</b> software. The relational database schema is designed to manage biodiversity information (Molecular Biodiversity Database) and it is organized in four areas: MolecularData, Experiment, Collection and Taxonomy. The MolecularData area is inspired to an established standard in Generic Model Organism Databases, the Chado relational schema. The peculiarity of Chado, and also its strength, is the adoption of an ontological schema which makes use of the Sequence Ontology. The Intelligent <b>Data</b> <b>Loader</b> (IDL) component of GIDL is an Extract, Transform and Load software able to parse data, to discover hidden information in the GenBank entries and to populate the Molecular Biodiversity Database. The IDL is composed by three main modules: the Parser, able to parse GenBank flat files; the Reasoner, which automatically builds CLIPS facts mapping the biological knowledge expressed by the Sequence Ontology; the DBFiller, which translates the CLIPS facts into ordered SQL statements used to populate the database. In GIDL Semantic Web technologies have been adopted due to their advantages in data representation, integration and processing. Results and conclusions Entries coming from Virus (814, 122), Plant (1, 365, 360) and Invertebrate (959, 065) divisions of GenBank rel. 180 have been loaded in the Molecular Biodiversity Database by GIDL. Our system, combining the Sequence Ontology and the Chado schema, allows a more powerful query expressiveness compared with the most commonly used sequence retrieval systems like Entrez or SRS. </p...|$|E
40|$|The {{purpose of}} this {{document}} is to define the essential user requirements for the Requirements Management System Browser (RMSB) application. This includes specifications for the Graphical User Interface (GUI) and the supporting database structures. The RMSB application is needed to provide an easy to use PC-based interface to browse system engineering data stored and managed in a UNIX software application. The system engineering data include functions, requirements, and architectures {{that make up the}} Tank Waste Remediation System (TWRS) technical baseline. This document also covers the requirements for a software application titled ``RMSB <b>Data</b> <b>Loader</b> (RMSB- DL) ``, referred to as the ``Parser. `` The Parser is needed to read and parse a data file and load the data structure supporting the Browser...|$|E
40|$|Release 4. 1. 2 of SasView Bug Fixes 	Fixes # 984 : PDF Reports Generate Empty PDFs 	Fixes a path typo in the 32 bit build script 	 32 and 64 bit Windows executables are now {{available}} Github release page : [URL] Bug fixes included from 4. 1. 1 	Fixes # 948 : Mathjax CDN is going away 	Fixes # 938 : Cannot read canSAS 1 D file output by SasView 	Fixes # 960 : Save project throws error if empty fit page 	Fixes # 929 : Problem deleting data in first fit page 	Fixes # 918 : Test folders not bundled with release 	Fixes an issue with the live discovery of plugin models 	Fixes an issue with the NXcanSAS <b>data</b> <b>loader</b> 	Updates tutorials for SasView 4. x. ...|$|E
40|$|By {{combining}} both compliant {{foil bearing}} (CFB) and {{active magnetic bearing}} (AMB) advantages, the hybrid foil/magnetic bearing (HFMB) can obtain high load capacity at all speeds. A brief overview of a universal test rig with the largest 150 mm HFMB is presented. Experimental investigation was performed in different modes by means of switching AMB on or off: AMB mode at low speed, CFB mode at high speed, hybrid mode with AMB as a bearing and hybrid mode with AMB as a <b>loader.</b> <b>Data</b> {{from a series of}} rotor-bearing system tests are presented. Having recently made efforts to complete the HFMB test (side-by-side), the largest 150 mm CFB passed through its first (3, 820 rpm) and second (11, 770 rpm) critical speed and reached 26, 923 rpm (i. e. 4 MDN). On the other extreme side, the 150 mm CFB testing has shown that it can bear as low as 645 rpm without shaft/foil touch, high temperature rise and large motor torque increases. That means the lift off or touch down speed is very low with a surface velocity U of less than 5 m/s. The coating used on the foil was Korolon 800 and the rotor surface was electrolyzed. Transient tests, simulating magnetic bearing failures at speeds as low as 700 rpm were also completed. It is shown that CFB has the ability to be a reliable auxiliary/backup bearing for the magnetic bearing system. This I 150 mm foil bearing is sized to meet a wide range of potential applications such as gas turbine engines for high-performance commercial and general aviation aircraft systems as well as larger industrial compressors...|$|R
40|$|Seismic {{methods have}} proven to be {{effective}} for monitoring the movement and location of injected CO 2 within deep saline aquifers. However, a disadvantage of seismic monitoring is the high costs associated with many repeat seismic surveys as part of a long term monitoring strategy of a CO 2 storage site. As the cost {{for the use of the}} seismic source is often a significant part of the overall survey cost, affordable, smaller sources would increase the potential feasibility of a long term seismic monitoring strategy. A comparison of three land seismic sources is performed at the Ketzin CO 2 storage site, Germany. Two of these sources (Vibsist 500 and Bobcat drop hammer) can be considered to be smaller and more affordable sources than those conventionally used in the seismic monitor surveys at Ketzin. In this study these smaller sources are compared to a larger more conventional Vibsist 3000 source. The subsurface target for the three sources in this comparison is the CO 2 storage reservoir for the Ketzin site, located within the Triassic Stuttgart formation, which lies at a depth of approximately 600 m/ 500 ms. Two of the sources are Swept Impact (SIST) type courses (Vibsist 500 and 3000) which use hydraulic concrete breaking hammers. The third source uses a concrete breaking drop hammer tool mounted on a Bobcat <b>loader.</b> <b>Data</b> were collected along a 984 m long profile with 24 m receiver spacing and 12 m shot spacing in 2011, 2012 and 2013 using the three different sources. A quantitative and qualitative comparison of the raw data from the three sources was performed in order to assess their relative performance. Frequency content, signal to noise ratio and penetration depth curves were calculated for the raw data. Data from the three sources was also processed using a conventional workflow to produce stacked sections which were compared. Based on the results from this study the Bobcat drop hammer source appears to perform better than the Vibsist 500 source. However both of the smaller sources were capable of producing good images of the target CO 2 storage reservoir. Hence, both provide viable options as small affordable seismic sources for long term monitoring at the Ketzin site, or other shallow CO 2 storage sites...|$|R
40|$|A {{driving cycle}} is a {{representation}} of how vehicles are driven and  is usually represented {{by a set of}} data points of vehicle speed  versus time.   Driving cycles have been used to evaluate vehicles for  a long time. A traditional usage of driving cycles have been in  certification test procedures where the exhaust gas emissions from  the vehicles need to comply with legislation. Driving cycles are now  also used in product development for example to size components or  to evaluate different technologies.   Driving cycles can be just a  repetition of measured data, be synthetically designed from  engineering standpoints, be a statistically equivalent  transformation of either of the two previous, or be obtained as an  inverse problem e. g. obtaining driving/operation patterns.   New  methods that generate driving cycles and extract typical behavior  from large amounts of operational data have recently been proposed.   Other methods can be used for comparison of driving cycles, or to  get realistic operations from measured data.   This work addresses evaluation, transformation and extraction of  driving cycles and vehicle operations.   To be able to test a vehicle  in a controlled environment, a chassis dynamometer is an  option. When the vehicle is mounted, the chassis dynamometer  simulates the road forces that the vehicle would experience if it  would be driven on a real road. A moving base simulator is a  well-established technique to evaluate driver perception of e. g. the  powertrain in a vehicle, and by connecting these two simulators the  fidelity can be enhanced in the moving base simulator and at the  same time the mounted vehicle in the chassis dynamometer is  experiencing more realistic loads. This is due to the driver's  perception in the moving base simulator is close to reality.   If only a driving cycle is considered in the optimization of a  controller there is a risk that the controllers of vehicles are  tailored to perform well in that specific driving cycle and not  during real-world driving. To avoid the sub-optimization issues, the  operating regions of the engine need to be excited differently. This  can be attained by using a novel algorithm, which is proposed in  this thesis, that alters the driving cycle while maintaining that  the driving cycle tests vehicles in a similar way. This is achieved  by keeping the mean tractive force constant during the process.   From a manufacturers standpoint it is vital to understand how your  vehicles are being used by the customers. Knowledge about the usage  can be used for design of driving cycles, component sizing and  configuration, during the product development process, and in  control algorithms.   To get a clearer picture of the usage of wheel  loaders, a novel algorithm that automatically, using existing  sensors only, extracts information of the customers usage, is  suggested. The approach is found to be robust when evaluated on  measured <b>data</b> from wheel <b>loaders</b> loading gravel and shot rock...|$|R
40|$|AbstractThe Gene Expression Omnibus (GEO) is {{the largest}} {{resource}} of public gene expression data. While GEO enables data browsing, query and retrieval, additional tools can help realize its potential for aggregating and comparing data across multiple studies and platforms. This paper describes DSGeo—a collection of valuable tools that were developed for annotating, aggregating, integrating, and analyzing data deposited in GEO. The core set of tools include a Relational Database, a <b>Data</b> <b>Loader,</b> a Data Browser, and an Expression Combiner and Analyzer. The application enables querying for specific sample characteristics and identifying studies containing samples that match the query. The Expression Combiner application enables normalization and aggregation of data from these samples and returns these data to the user after filtering, according to the user’s preferences. The Expression Analyzer allows simple statistical comparisons between groups of data. This seamless integration makes annotated cross-platform data directly available for analysis...|$|E
40|$|RDF is a {{data model}} for {{representing}} labeled directed graphs, {{and it is}} used as an important building block of semantic web. Due to its flexibility and applicability, RDF {{has been used in}} applications, such as semantic web, bioinformat-ics, and social networks. In these applications, large-scale graph datasets are very common. However, existing tech-niques are not effectively managing them. In this paper, we present a scalable, efficient query processing system for RDF data, named SPIDER, based on the well-known par-allel/distributed computing framework, Hadoop. SPIDER consists of two major modules (1) the graph <b>data</b> <b>loader,</b> (2) the graph query processor. The loader analyzes and dis-sects the RDF data and places parts of data over multiple servers. The query processor parses the user query and dis-tributes sub queries to cluster nodes. Also, the results of sub queries from multiple servers are gathered (and refined if necessary) and delivered to the user. Both modules uti-lize the MapReduce framework of Hadoop. In addition, our system supports some features of SPARQL query language. This prototype will be foundation to develop real applica-tions with large-scale RDF graph data...|$|E
40|$|The Austrian Federal Ministry of Health aims {{to improve}} the health of all people living in Austria and to {{decrease}} health and social inequalities. This leads to a careful planning and distribution of the available health care resources to meet government aims. The research project SALUD, funded by the Federal Ministry for Transport, Innovation and Technology and the Austrian Science Fund, focuses on building a Spatial Microsimulation Model for Austria by combining survey and census data to model small area health issues based on individuals or households where no data exists, so called missing data. Within this project, a first prototype of simSALUD is developed, which is a Web-based spatial microsimulation application for health decision support as to date no flexible and free-available Web application {{in the area of}} spatial microsimulation exists. simSALUD targets two groups: experts in the area and health policy decision makers (HPDM). The present work focuses on the latter group and will highlight the benefits of simSALUD for HPDM, as they are mainly interested in the results and not primarily in the underlying modelling process. The first prototype of simSALUD is a Web-based application that can be used either offline or online and is therefore accessible anywhere and anytime. Functionally it consists of three modules called (i) <b>data</b> <b>loader,</b> (ii) visualisation area, and (iii) geoprocessing tools. - The <b>data</b> <b>loader</b> supports the management of different datasets (loading, overlaying, sorting, etc.). The main benefit here is that HPDM may access a shared pool of open simulated data that is otherwise hardly available. - The visualisation area presents an interactive map with different standard functionalities, including zoom and pan. This is especially valuable for HPDM as hotspots and problem areas on maps can often be identified more easily than lots of numbers in a list. - The geoprocessing tools, such as ?create buffer? or ?location optimization? allow doing spatial analysis on data to answer what-if scenarios. In general, the main advantage of simSALUD is that HPDM can use the application and perform various spatial analyses without needing specialist skills in spatial microsimulation modelling or geoinformation science. The first prototype of SimSALUD will be introduced to our project partners in their health departments in order to evaluate its practical usefulness and relevance in the area of regional health planning. This work will discuss the results of the evaluation, highlighting {{the pros and cons of}} the prototype as well as defining the road map for futures developments of simSALUD. Hence, with simSALUD we hope to make a positive contribution for the area of spatial microsimulation modelling, as it is the first open accessible Web application so far...|$|E
40|$|Abstract—NoSQL (Not only SQL) data stores {{become a}} vital {{component}} in many big data computing platforms {{due to its}} inherent horizontal scalability. HBase is an open-source distributed NoSQL store that is widely used by many Internet enterprises to handle their big data computing applications (e. g. Facebook handles millions of messages each day with HBase). Optimizations that can enhance the performance of HBase are of paramount interests for big data applications that use HBase or Big Table like key-value stores. In this paper we study the problems inherent in misconfiguration of HBase clusters, including scenarios where the HBase default configurations can lead to poor performance. We develop HConfig, a semi-automated configuration manager for optimizing HBase system performance from multiple dimensions. Due to the space constraint, this paper will focus {{on how to improve}} the performance of HBase <b>data</b> <b>loader</b> using HConfig. Through this case study we will highlight the importance of resource adaptive and workload aware auto-configuration management and the design principles of HConfig. Our experiments show that the HConfig enhanced bulk loading can significantly improve the performance of HBase bulk loading jobs compared to the HBase default configuration, and achieve 2 ~ 3. 7 x speedup in throughput under different client threads while maintaining linear horizontal scalability...|$|E
40|$|The CUAHSI Hydrologic Information System (HIS) Project is {{developing}} information technology infrastructure to support hydrologic science. One {{of the components}} of the HIS is a point Observations Data Model (ODM), which is a relational database schema that was designed for storing time series data. The purpose of ODM is to provide a framework for optimizing data storage and retrieval for integrated analysis of information collected by multiple investigators. The CUAHSI HIS ODM is being implemented by a number of local work groups throughout the country, and these work groups are using the ODM as a mechanism for publication of individual investigator data and for registering these data with the National HIS. At many of these sites, investigators are collecting continuous datasets in real time using sensor networks and telemetry systems. This document provides the design specifications for a set of software tools that will allow administrators of local instances of the ODM to automate the loading of these continuous data streams into the ODM. The main objective of the Streaming <b>Data</b> <b>Loader</b> is to provide administrators of work group instances of the ODM with tools {{that can be used to}} map their continuous data to the ODM schema and schedule the automated loading of the data into the ODM...|$|E
40|$|Currently large data sets, such as country wise LiDAR scans, {{are being}} {{collected}} and combined with large collections of semantically rich objects {{to form a}} new source of knowledge for urban planing and smart cities. Such data is often stored using domain specific file-based solutions. Although this allows efficient access to the data in its original format, data isolation, and application dependency on data formats are major drawbacks of this approach. Furthermore, complex ad-hoc queries are hard to express, particularly when faced with the challenge to combine numerous data sources. File-based solutions have also poor vertical and horizontal scalability. To integrate different data sets with spatial data, and to have efficient and flexible data exploration a Spatial Data Management System (SDBMS) is advised. However, current solutions are not capable of handling efficiently large LiDAR data sets due to the high cost of converting, loading, indexing and compressing Point Cloud data [4]. In this paper we present an efficient data management layer for geo-spatial data analysis with special emphasis on LiDAR data. It provides fast data ingestion through a specialized <b>data</b> <b>loader,</b> but also in-situ data access to LAS/LAZ data repositories containing billions of points. Through in-memory indexes, efficient data filtering and a refinement stage based on a divide and conquer approach our solution outperforms the well known PostgreSQL with Point Cloud extension. For complex queries, thanks to its efficient multi-core parallelism, our solution is an order magnitude faster than the file-based solution Rapidlasso LAStools...|$|E
