0|10000|Public
40|$|Abstract. Since {{training}} of a classifier takes time, usually some criterion {{other than the}} recognition rate is used for feature selection. This may, however, leads to deteriorating <b>the</b> <b>generalization</b> <b>ability</b> by feature selection. To overcome this problem, in this paper, we propose modified backward feature selection by cross validation. Initially, we determine the candidate set which consists of the features that do not deteriorate <b>the</b> <b>generalization</b> <b>ability,</b> if each is deleted from the initial set of features. If <b>the</b> <b>generalization</b> <b>ability</b> is not deteriorated even if all the candidate features are deleted, we terminate the algorithm. Otherwise, we delete by backward deletion the candidate feature that improves <b>the</b> <b>generalization</b> <b>ability</b> <b>the</b> most, and determine the candidate set that is {{a subset of the}} current candidate set. We iterate the above procedure until the candidate set is empty. We evaluate our method using support vector machines for some benchmark data sets and show that many features are deleted without deteriorating <b>the</b> <b>generalization</b> <b>ability.</b> ...|$|R
5000|$|Theory of {{controlling}} <b>the</b> <b>generalization</b> <b>ability</b> of learning processes ...|$|R
5000|$|How can one {{construct}} algorithms {{that can}} control <b>the</b> <b>generalization</b> <b>ability?</b> ...|$|R
40|$|To resolve unclassifiable regions for {{pairwise}} support vector machines, decision acyclic graph support vector machines have been proposed. But by this architecture, <b>the</b> <b>generalization</b> <b>ability</b> depends on <b>the</b> tree structure. In this paper, to improve <b>the</b> <b>generalization</b> <b>ability,</b> we propose {{to optimize the}} structure so that the class pairs with higher <b>generalization</b> <b>abilities</b> are put in the upper nodes of the tree. We show the effectiveness of our method for some benchmark data sets. 1...|$|R
5000|$|How can one {{control the}} rate of {{convergence}} (<b>the</b> <b>generalization</b> <b>ability)</b> of <b>the</b> learning process? ...|$|R
40|$|The noise {{injection}} {{into the}} training samples {{has been shown}} to lead to improvement of <b>the</b> <b>generalization</b> <b>ability</b> of artificial neural network(ANN) classifiers. In this paper, we investigate the positive effect of the noise injection on <b>the</b> <b>generalization</b> <b>ability</b> of ANN classifiers in high dimensions. We further show that the noise injection technique is very useful in situations where the true Bayes error is small. ...|$|R
40|$|An {{improved}} kernel parameter optimization method {{based on}} Structural Risk Minimization (SRM) principle is proposed to enhance <b>the</b> <b>generalization</b> <b>ability</b> of traditional Kriging surrogate model. This article first analyses {{the importance of}} <b>the</b> <b>generalization</b> <b>ability</b> as an assessment criteria of surrogate model {{from the perspective of}} statistics and proves the applicability to Kriging. Kernel parameter optimization method is used to improve the fitting precision of Kriging model. With the smoothness measure of <b>the</b> <b>generalization</b> <b>ability</b> and <b>the</b> anisotropy kernel function, the modified Kriging surrogate model and its analysis process are established. Several benchmarks are tested to verify the effectiveness of the modified method under two different sampling states: uniform distribution and nonuniform distribution. The results show that the proposed Kriging has better <b>generalization</b> <b>ability</b> and adaptability, especially for nonuniform distribution sampling...|$|R
40|$|Abstract. <b>The</b> <b>generalization</b> <b>ability</b> of {{different}} sizes architectures with one and two hidden layers trained with backpropagation combined with early stopping have been analyzed. The dependence of <b>the</b> <b>generalization</b> process on <b>the</b> complexity of the function being implemented is stud-ied using a recently introduced measure for the complexity of Boolean functions. For {{a whole set of}} Boolean symmetric functions it is found that large neural networks have a better <b>generalization</b> <b>ability</b> on a large complexity range of the functions in comparison to smaller ones and also that the introduction of a small second hidden layer of neurons further improves <b>the</b> <b>generalization</b> <b>ability</b> for very complex functions. Quasi-random generated Boolean functions were also analyzed and we found that in this case <b>the</b> <b>generalization</b> <b>ability</b> shows small variability across different network sizes both with one and two hidden layer network ar-chitectures. ...|$|R
40|$|We {{introduce}} a measure for {{the complexity of}} Boolean functions that is highly correlated with <b>the</b> <b>generalization</b> <b>ability</b> that could be obtained when the functions are implemented in feedforward neural networks. The measure, based on the calculation {{of the number of}} neighbour examples that differ in their output value, can be simply computed from the definition of the functions, independently of their implementation. Numerical simulations performed on different architectures show a good agreement between the estimated complexity and <b>the</b> <b>generalization</b> <b>ability</b> and training times obtained. The proposed measure could help as a useful tool for carrying a systematic study of the computational capabilities of network architectures by classifying in an easy and reliable way the Boolean functions. Also, {{based on the fact that}} <b>the</b> average <b>generalization</b> <b>ability</b> computed over <b>the</b> whole set of Boolean functions is 0. 5, a very complex set of functions was found for which <b>the</b> <b>generalization</b> <b>ability</b> is lower than for random functions...|$|R
40|$|This work explores diverse {{techniques}} for improving <b>the</b> <b>generalization</b> <b>ability</b> of supervised feed-forward neural networks via structural adaptation, and introduces a new network structure with sparse connectivity. Pruning methods which start {{from a large}} network and proceed in trimming it until a satisfactory solution is reached, are studied first. Then, construction methods, which build a network from a simple initial configuration, are presented. A survey of related results from the disciplines of function approximation theory, nonparametric statistical inference and estimation theory leads to methods for principled architecture selection and estimation of prediction error. A network based on sparse connectivity is proposed as an alternative approach to adaptive networks. <b>The</b> <b>generalization</b> <b>ability</b> of this network is improved by partly decoupling the outputs. We perform numerical simulations and provide comparative results for both classification and regression problems to show <b>the</b> <b>generalization</b> <b>abilities</b> of <b>the</b> sparse network. ...|$|R
40|$|Individual's {{behavior}} in dynamic {{systems such as}} social and economic systems {{can be thought of}} as response to stimulus in his circumstance. In this paper, we propose a method to obtain strategy coalitions, whose confidences are adjusted by genetic algorithm to improve <b>the</b> <b>generalization</b> <b>ability,</b> in <b>the</b> process of co-evolutionary learning with a social game called Iterated Prisoner's Dilemma (IPD) game. Experimental results show that several better strategies can be obtained through strategy coalition, and evolutionary optimization of the confidence for strategies within coalition improves <b>the</b> <b>generalization</b> <b>ability...</b>|$|R
3000|$|... aEarly {{stopping}} rule {{is a method}} to avoid over-fitting, but it improves <b>the</b> <b>generalization</b> <b>ability</b> of ANNs. This method halts the training process when the performance with validation data stops improving.|$|R
3000|$|Training set. The {{training}} patterns {{should give}} a sufficiently large and {{representative sample of}} the data population that one wants to generalize. In fact, <b>the</b> <b>generalization</b> <b>ability</b> of <b>the</b> eventual model [...]...|$|R
40|$|In {{this paper}} we analyse {{the effect of}} {{introducing}} a structure in the input distribution on <b>the</b> <b>generalization</b> <b>ability</b> of a simple perceptron. The simple case of two clusters of input data and a linearly separable rule is considered. We find that <b>the</b> <b>generalization</b> <b>ability</b> improves with <b>the</b> separation between the clusters, and is bounded from below by the result for the unstructured case, recovered as the separation between clusters vanishes. The asymptotic behaviour for large training sets, however, {{is the same for}} structured and unstructured input distributions. For small training sets, the dependence of <b>the</b> <b>generalization</b> error on <b>the</b> number of examples is observed to be non-monotonic for certain values of the model parameters. ...|$|R
40|$|We analyze <b>the</b> <b>generalization</b> <b>ability</b> of {{a simple}} {{perceptron}} acting on a structured input distribution for the simple case of two clusters of input data and a linearly separable rule. <b>The</b> <b>generalization</b> <b>ability</b> computed for three learning scenarios: maximal stability, Gibbs, and optimal learning, is found to improve with the separation between the clusters, and is bounded from below by the result for the unstructured case, recovered as the separation between clusters vanishes. The asymptotic behavior for large training sets {{is the same for}} structured and unstructured input distributions. For small training sets, <b>the</b> <b>generalization</b> error of <b>the</b> maximally stable perceptron exhibits a nonmonotonic dependence on the number of examples for certain values of the model parameters. ...|$|R
30|$|The INW–ESN is {{proposed}} for prognostic for hydraulic pump. The reservoir is modified and the disadvantages {{caused by the}} sparsely connection are solved. <b>The</b> <b>generalization</b> <b>ability</b> and predicting accuracy of the network have been effectively improved.|$|R
30|$|In addition, where σ is a {{constant}} parameter of the kernel and can either control the amplitude of the Gaussian function and <b>the</b> <b>generalization</b> <b>ability</b> of SVM. We have to optimize σ and find the optimal one.|$|R
40|$|Neural network {{ensemble}} {{can significantly}} improve <b>the</b> <b>generalization</b> <b>ability</b> of neural network based systems. In this paper, a novel rule learning algorithm is pro-posed, where neural network ensemble {{acts as a}} front-end process that generates data for the learning of rules. Experi-mental {{results show that the}} proposed algorithm can generate rules with strong <b>generalization</b> <b>ability...</b>|$|R
40|$|Abstract This work {{analyzes}} {{the problem of}} selecting an adequate neural network archi-tecture for a given function, comparing existing approaches and introducing a new one based {{on the use of}} the complexity of the function under analysis. Numerical simulations using a large set of Boolean functions are carried out and a comparative analysis of the results is done according to the architectures that the different techniques suggest and based on <b>the</b> <b>generalization</b> <b>ability</b> obtained in each case. The results show that a procedure that utilizes the complexity of the function can help to achieve almost optimal results despite the fact that some variability exists for <b>the</b> <b>generalization</b> <b>ability</b> of similar complexity classes of functions...|$|R
30|$|This paper {{demonstrates}} {{the ability of}} this class of genetic algorithms to produce the best combination of network parameters. Results obtained from test data adduce that <b>the</b> model <b>generalization</b> <b>ability</b> is satisfactory. For the case of 5 - and 10 -min prediction horizon, R 2 indices are at least 0.98, which evidently shows <b>the</b> model <b>generalization</b> <b>ability.</b>|$|R
40|$|This work characterizes <b>the</b> <b>generalization</b> <b>ability</b> of {{algorithms}} whose predictionsarelinearintheinputvector. Tothisend, weprovidesharpboundsfor RademacherandGaussiancomplexitiesof(constrained) linearclasses,whichdirectlyleadtoanumberofgeneralizationbounds. Thisderivationprovidessimplifiedproofsofanumberofcorollariesincluding:riskboundsforlinearprediction (includingsettingswheretheweightvectorsareconstrainedbyeither L 2 or L 1 constraints),marginbounds(includingboth L 2 and L 1 margins,alongwithmore generalnotionsbasedonrelativeentropy), aproofofthePAC-Bayestheorem, andupperboundson L 2 coveringnumbers(with Lpnormconstraintsandrelativeentropyconstraints). Inadditiontoprovidingaunifiedanalysis,theresults hereinprovidesomeofthesharpestriskandmarginbounds. Interestingly,our resultsshowthattheuniformconvergenceratesofempiricalriskminimization algorithmstightlymatchtheregretboundsofonlinelearningalgorithmsforlinear prediction,uptoaconstantfactorof 2...|$|R
40|$|The {{main goal}} of this course is to study <b>the</b> <b>generalization</b> <b>ability</b> {{of a number of}} popular machine {{learning}} algorithms such as boosting, support vector machines and neural networks. Topics include Vapnik-Chervonenkis theory, concentration inequalities in product spaces, and other elements of empirical process theory...|$|R
40|$|Abstract. Multi-label {{learning}} aims at predicting potentially multiple {{labels for}} a given instance. Conventional multi-label learning approaches focus on exploiting the label correlations to improve {{the accuracy of the}} learner by building an individual multi-label learner or a combined learner based upon a group of single-label learners. However, <b>the</b> <b>generalization</b> <b>ability</b> of such individual learner can be weak. It is well known that ensemble learning can effectively improve <b>the</b> <b>generalization</b> <b>ability</b> of learning systems by constructing multiple base learners and the performance of an ensemble is related to the both accuracy and diversity of base learners. In this paper, we study the problem of multi-label ensemble learning. Specifically, we aim at improving <b>the</b> <b>generalization</b> <b>ability</b> of multi-label learning systems by constructing a group of multilabel base learners which are both accurate and diverse. We propose a novel solution, called EnML, to effectively augment the accuracy as well as the diversity of multi-label base learners. In detail, we design two objective functions to evaluate the accuracy and diversity of multilabel base learners, respectively, and EnML simultaneously optimizes these two objectives with an evolutionary multi-objective optimization method. Experiments on real-world multi-label learning tasks validate the effectiveness of our approach against other well-established methods...|$|R
40|$|International audienceThis paper {{deals with}} <b>the</b> <b>generalization</b> <b>ability</b> of {{classifiers}} trained from non-iid evolutionary-related data {{in which all}} training and testing examples correspond to leaves of a phylogenetic tree. For the re-alizable case, we prove PAC-type {{upper and lower bounds}} based on symmetries and matchings in such trees...|$|R
40|$|Abstract—A new {{approach}} to promote <b>the</b> <b>generalization</b> <b>ability</b> of neural networks is presented. It {{is based on the}} point of view of fuzzy theory. This approach is implemented through shrinking or magnifying the input vector, thereby reducing the difference between training set and testing set. It is called “shrinking-magnifying approach ” (SMA). At the same time, a new algorithm; α-algorithm is presented to find out the appropriate shrinking-magnifying-factor (SMF) α and obtain better <b>generalization</b> <b>ability</b> of neural networks. Quite a few simulation experiments serve to study the effect of SMA and α-algorithm. The experiment results are discussed in detail, and the function principle of SMA is analyzed in theory. The results of experiments and analyses show that the {{new approach}} is not only simpler and easier, but also is very effective to many neural networks and many classification problems. In our experiments, the proportions promoting <b>the</b> <b>generalization</b> <b>ability</b> of neural networks have even reached 90 %. Keywords—Fuzzy theory, generalization, misclassification rate...|$|R
40|$|An {{analysis}} {{of the influence of}} weight and input perturbations in a multilayer perceptron (MLP) is made in this article. Quantitative measurements of fault tolerance, noise immunity, and <b>generalization</b> <b>ability</b> are provided. From the expressions obtained, it is possible to justify some previously reported conjectures and experimentally obtained results (e. g., the influence of weight magnitudes, the relation between training with noise and <b>the</b> <b>generalization</b> <b>ability,</b> <b>the</b> relation between fault tolerance and <b>the</b> <b>generalization</b> <b>ability).</b> <b>The</b> measurements introduced here are explicitly related to the mean squared error degradation in the presence of perturbations, thus constituting a selection criterion between different alternatives of weight configurations. Moreover, they allow us to predict the degradation of the learning performance of an MLP when its weights or inputs are deviated from their nominal values and thus, the behavior of a physical implementation can be evaluated before the weights are mapped on it according to its accuracy...|$|R
40|$|Ensembling neural {{classifiers}} {{can significantly}} improve <b>the</b> <b>generalization</b> <b>ability</b> of classification systems. In this paper, GASEN, a genetic algorithm based selective ensemble method {{that has been}} shown to be excellent in ensembling neural regressors, is applied to neural classifiers. Experiments on four large data sets show that this method can generate ensembles of neural classifiers with stronger <b>generalization</b> <b>ability</b> than those generated by Bagging, Adaboost, or Arc-x 4...|$|R
40|$|This {{paper is}} {{concerned}} with <b>the</b> <b>generalization</b> <b>ability</b> of learning to rank algorithms for information retrieval (IR). We {{point out that the}} key for addressing the learning problem is to look at it from the viewpoint of query, and we give a formulation of learning to rank for IR based on the consideration. We define a number of new concepts within the framework, including query-level loss, query-level risk, and query-level stability. We then analyze <b>the</b> <b>generalization</b> <b>ability</b> of learning to rank algorithms by giving query-level generalization bounds to them using query-level stability as a tool. Such an analysis is very helpful for us to derive more advanced algorithms for IR. We apply the proposed theory to the existing algorithms of Ranking SVM and IRSVM. Experimental results on the two algorithms verify the correctness of the theoretical analysis. 1...|$|R
40|$|This paper {{presents}} theoretical {{analysis on}} <b>the</b> <b>generalization</b> <b>ability</b> of listwise learning-to-rank algorithms using Rademacher Average. The paper first proposes {{a theoretical framework}} for ranking and then proves a theorem which gives a gen-eralization bound to a listwise ranking algorithm based on Rademacher Average {{of the class of}} compound functions operating on the corresponding listwise loss function and the ranking model. It then derives Rademecher Average of the com-pound function classes for the existing listwise ranking algorithms of ListMLE, ListNet and RankCosine. It also discusses <b>the</b> tightness of <b>generalization</b> bounds in different situations, such as the bounds w. r. t. different list lengths, different transformation functions, and so on. To the best of our knowledge, this is the first paper that formally addresses the theoretical framework and <b>the</b> <b>generalization</b> <b>ability</b> of listwise ranking algorithms. The theoretical findings are useful for the design and parameter tuning of listwise ranking algorithms. ...|$|R
3000|$|... in a {{feedforward}} {{neural network}} or the kernel function and its parameters in a SVM) determine <b>the</b> <b>generalization</b> <b>ability</b> of a ML model. Overly complex models exhibit higher risks of overfitting [41]. In the lack of established theoretical guidelines for model selection, one usually relies on empirical data-driven model selection criteria that proved effective [29].|$|R
30|$|In Subsection 3.1, {{we first}} derive a general {{framework}} {{for the inclusion of}} AAM fitting constraints. The remainder of this section gives {{a detailed description of the}} particular constraints used for the application on locomotion sequences. In Subsection 3.6, the necessary conditions of our approach and <b>the</b> <b>generalization</b> <b>ability</b> to other scenarios is discussed.|$|R
40|$|The parity {{function}} {{is one of}} the most used Boolean function for testing learning algorithms because both of its simple definition and its great complexity. Being one of the hardest problems, many different architectures have been constructed to compute parity, essentially by adding neurons in the hidden layer in order to reduce the number of local minima where gradient-descent learning algorithms could get stuck. We construct a family of modular architectures that implement the parity function in which, every member of the family can be characterized by the fan-in max of the network, i. e., the maximum number of connections that a neuron can receive. We analyze <b>the</b> <b>generalization</b> <b>ability</b> of <b>the</b> modular networks first by computing analytically the minimum number of examples needed for perfect generalization and second by numerical simulations. Both results show that <b>the</b> <b>generalization</b> <b>ability</b> of these networks is systematically improved by the degree of modularity of the network. We also analyze the influence of the selection of examples in <b>the</b> emergence of <b>generalization</b> <b>ability,</b> by comparing <b>the</b> learning curves obtained through a random selection of examples to those obtained through examples selected accordingly to a general algorithm we recently proposed...|$|R
40|$|We study <b>the</b> <b>generalization</b> <b>ability</b> of {{a simple}} {{perceptron}} which learns unlearnable rules. The rules are presented by a teacher perceptron with a non-monotonic transfer function. The student is trained in the on-line mode. The asymptotic behaviour of <b>the</b> <b>generalization</b> error is estimated under various conditions. Several learning strategies are proposed and improved to obtain the theoretical lower bound of <b>the</b> <b>generalization</b> error. Comment: LaTeX 20 pages using IOP LaTeX preprint style file, 14 figure...|$|R
30|$|One {{demonstration}} {{is sufficient}} to reproduce the task {{as far as the}} task condition is constant. However, our method has no advantage over conventional teaching/playback in such a constant condition. Thus we should perform two or more demonstrations in different conditions to adapt to changes of task conditions with the help of <b>the</b> <b>generalization</b> <b>ability</b> of neural networks.|$|R
30|$|Semi-supervised {{learning}} (SSL) {{methods are}} useful for learning tasks in which the quantity of labeled examples is so limited that compromises <b>the</b> <b>generalization</b> <b>ability</b> of <b>the</b> learning algorithm. Zhu and Goldberg [20] present a comprehensible survey of semi-supervised learning techniques. In our experimental section, we experiment with self-training, {{one of the most}} straightforward SSL methods available.|$|R
40|$|In this paper, genetic {{algorithm}} {{is used to}} help improve the tolerance of feedforward neural networks against open fault. The proposed method does not explicitly add any redundancy to the network, nor does it modify the training algorithm. Experiments show that it may profit the fault tolerance as well as <b>the</b> <b>generalization</b> <b>ability</b> of neural networks...|$|R
