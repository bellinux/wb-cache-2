1|24|Public
5000|$|The Data <b>Identifier</b> <b>word</b> (along {{with the}} SDID, if used), {{indicates}} {{the type of}} ancillary data that the packet corresponds to. Data identifiers range from 1 to 255 (FF hex), with 0 being reserved. As the serial digital interface is a 10-bit format, the DID word is encoded as follows: ...|$|E
5000|$|In a {{computer}} language, a reserved word (also {{known as a}} reserved <b>identifier)</b> is a <b>word</b> that cannot {{be used as an}} identifier, such as the name of a variable, function, or label - it is [...] "reserved from use". This is a syntactic definition, and a reserved word may have no meaning.|$|R
2500|$|A {{permalink}} {{is a type}} {{of persistent}} <b>identifier</b> and the <b>word</b> permalink is sometimes used as a synonym of persistent identifier. More often, though, permalink is applied to persistent identifiers which are generated by a content management system for pages served by that system. This usage is especially common in the blogosphere. Such links are not maintained by an outside authority, and their persistence is dependent on the durability of the content management system itself.|$|R
50|$|The {{distinct}} definitions {{are clear}} when a language is analyzed {{by a combination}} of a lexer and a parser, and the syntax of the language is generated by a lexical grammar for the words, and a context-free grammar of production rules for the phrases. This is common in analyzing modern languages, and in this case keywords are a subset of reserved words, as they must be distinguished from <b>identifiers</b> at the <b>word</b> level (hence reserved words) to be syntactically analyzed differently at the phrase level (as keywords).|$|R
50|$|There {{are various}} GIS tools {{available}} that can transform image data to some geographic control framework, like the commercial ArcMap, PCI Geomatica, TNTmips (MicroImages,Inc) or ERDAS Imagine. One can georeference {{a set of}} points, lines, polygons, images, or 3D structures. For instance, a GPS device will record latitude and longitude coordinates for a given point of interest, effectively georeferencing this point. A georeference must be a unique <b>identifier.</b> In other <b>words,</b> there must be only one location for which a georeference acts as the reference.|$|R
50|$|A {{broadcast}} address is an address that allows {{information to be}} sent to all interfaces in a given subnet, rather than a specific machine. Generally, the {{broadcast address}} is found by obtaining the bit complement of the subnet mask and performing a bitwise OR operation with the network <b>identifier.</b> In other <b>words,</b> the broadcast address is the last address in the address range of the subnet. For example, the broadcast address for the network 192.168.5.0 is 192.168.5.255. For networks of size /24 or larger, the broadcast address always ends in 255.|$|R
40|$|This paper {{describes}} a Chinese word segmentor (CWS) based on backward maximum matching (BMM) technique for the 2 nd Chinese Word Segmentation Bakeoff in the Microsoft Research (MSR) closed testing track. Our CWS comprises of a context-based Chinese unknown <b>word</b> <b>identifier</b> (UWI). All the context-based knowledge for the UWI is fully automatically {{generated by the}} MSR training corpus. According to the scored results of the MSR closed testing track and our analysis, it shows that our BMM-based CWS with the context-based UWI is a simple and effective system to achieve high Chinese word segmentation performance of more than 95. 5 % F-measure. ...|$|R
40|$|Syllable-to-word (STW) {{conversion}} {{is important}} in Chinese phonetic input methods and speech recognition. There are two major problems in the STW conversion: (1) resolving the ambiguity caused by homonyms; (2) determining the word segmentation. This paper describes a noun-verb event-frame (NVEF) <b>word</b> <b>identifier</b> {{that can be used}} to solve these problems effectively. Our approach includes (a) an NVEF word-pair identifier and (b) other <b>word</b> <b>identifiers</b> for the non-NVEF portion. Our experiment showed that the NVEF word-pair identifier is able to achieve a 99. 66 % STW accuracy for the NVEF related portion, and by combining with other identifiers for the non-NVEF portion, the overall STW accuracy is 96. 50 %. The result of this study indicates that the NVEF knowledge is very powerful for the STW conversion. In fact, numerous cases requiring disambiguation in natural language processing fall into such “chicken-and-egg ” situation. The NVEF knowledge can be employed as a general tool in such systems for disambiguating the NVEF related portion independently (thus breaking the chicken-and-egg situation) and using that as a good fundamental basis to treat the remaining portion. This shows that the NVEF knowledge is likely to be important for general NLP. To further expand its coverage, we shall extend the study of NVEF to that of other co-occurrence restrictions such as noun-noun pairs, noun-adjective pairs and verb-adverb pairs. We believe the STW accuracy can be further improved with the additional knowledge. 1...|$|R
40|$|Abstract—We {{propose a}} method for {{unsupervised}} many-to-many object matching from multiple networks, which is the task of finding correspondences between groups of nodes in different networks. For example, the proposed method can discover shared word groups from multi-lingual document-word networks without cross-language alignment information. We assume that multiple networks share groups, and each group has its own interaction pattern with other groups. Using infinite relational models with this assumption, objects in different networks are clustered into common groups depending on their interaction patterns, discovering a matching. The effectiveness of the proposed method is experimentally demonstrated by using synthetic and real relational data sets, which include applications to cross-domain recommendation without shared user/item <b>identifiers</b> and multi-lingual <b>word</b> clustering...|$|R
5000|$|For instance, the lexical grammar {{for many}} {{programming}} languages specifies that a string literal {{starts with a}} [...] " [...] character and continues until a matching [...] " [...] is found (escaping makes this more complicated), that an identifier is an alphanumeric sequence (letters and digits, usually also allowing underscores, and disallowing initial digits), and that an integer literal is a sequence of digits. So in the following character sequence [...] "abc" [...] xyz1 23 the tokens are string, identifier and number (plus whitespace tokens) because the space character terminates the sequence of characters forming the identifier. Further, certain sequences are categorized as keywords - these generally have the same form as <b>identifiers</b> (usually alphabetical <b>words),</b> but are categorized separately; formally {{they have a different}} token type.|$|R
40|$|The {{objective}} of this work 1 is to automatically generate {{a large number of}} images for a specified object class (for example, penguin). A multi-modal approach employing both text, meta data and visual features is used to gather many, high-quality images from the web. Candidate images are obtained by a text based web search querying on the object <b>identifier</b> (the <b>word</b> penguin). The web pages and the images they contain are downloaded. The task is then to remove irrelevant images and re-rank the remainder. First, the images are re-ranked using a Bayes posterior estimator trained on the text surrounding the image and meta data features (such as the image alternative tag, image title tag, and image filename). No visual information is used at this stage. Second, the top-ranked images are used as (noisy) training data and a SVM visual classifier is learnt to improve the ranking further. The principal novelty is in combining text/meta-data and visual features in order to achieve a completely automatic ranking of the images. Examples are given for a selection of animals (e. g. camels, sharks, penguins), vehicles (cars, airplanes, bikes) and other classes (guitar, wristwatch), totalling 18 classes. The results are assessed by precision/recall curves on ground truth annotated data and by comparison to previous approaches including those of Berg et al. [5] (on an additional six classes) and Fergus et al. [9]. 1...|$|R
40|$|Words are {{tools of}} life which is omnipresent in every {{language}}. All {{words in a}} language are unique havingtheir own function and meaning. The syntactic and semantic knowledge about individual words can beencapsulated in a highly structured repository known as computational lexicon which is very essential forMachine Translation. For designing a computational lexicon, the first and foremost task is to identify thehead words or root words in the language. The Root <b>Word</b> <b>Identifier</b> proposed in this work is a rule basedapproach which automatically removes the inflected part and derive the root words using morphophonemicrules. The system is tested with 2400 words from a Malayalam corpus to generate the linguistic informationsuch as the root form, their inflected forms and grammatical category. The performance is evaluated usingthe statistical measures like Precision, Recall and F-measure. The values obtained for these measures aremore than 90 %...|$|R
40|$|Abstract. Data {{words and}} data trees appear in {{verification}} and XML processing. The term “data ” means that {{positions of the}} word, or tree, are decorated with elements of an infinite set of data values, such as natural numbers or ASCII strings. This talk is {{a survey of the}} various automaton models that have been developed for data words and data trees. A data word is a word where every position carries two pieces of information: a label from a finite alphabet, and a data value from an infinite set. A data tree is defined likewise. As an example, suppose that the finite alphabet has two labels request and grant, and the data values are numbers (interpreted as process <b>identifiers).</b> A data <b>word,</b> such as the one below, can be seen as log of events that happened to the processes. request 1 request 2 reques...|$|R
5000|$|In {{computer}} programming, {{a declaration}} is a language construct that specifies properties of an identifier: it declares what a <b>word</b> (<b>identifier)</b> [...] "means. Declarations are {{most commonly used}} for functions, variables, constants, and classes, but {{can also be used}} for other entities such as enumerations and type definitions. Beyond the name (the identifier itself) and the kind of entity (function, variable, etc.), declarations typically specify the data type (for variables and constants), or the type signature (for functions); types may also include dimensions, such as for arrays. A declaration is used to announce the existence of the entity to the compiler; this is important in those strongly typed languages that require functions, variables, and constants, and their types to be specified with a declaration before use, and is used in forward declaration. The term [...] "declaration" [...] is frequently contrasted with the term [...] "definition", but meaning and usage varies significantly between languages; see below.|$|R
40|$|Automated {{software}} engineering tools (e. g., program search, concern location, code reuse, quality assessment, etc.) increasingly rely on natural language information from comments and identifiers in code. The {{first step in}} analyzing <b>words</b> from <b>identifiers</b> requires splitting identifiers into their constituent words. Unlike natural languages, where space and punctuation are used to delineate <b>words,</b> <b>identifiers</b> cannot contain spaces. One common way to split identifiers is to follow programming language naming conventions. For example, Java programmers often use camel case, where words are delineated by uppercase letters or non-alphabetic characters. However, programmers also create identifiers by concatenating sequences of words together with no discernible delineation, which poses challenges to automatic identifier splitting. In this paper, we present an algorithm to automatically split identifiers into sequences of words by mining word frequencies in source code. With these <b>word</b> frequencies, our <b>identifier</b> splitter uses a scoring technique to automatically select the most appropriate partitioning for an identifier. In an evaluation of over 8000 identifiers from open source Java programs, our Samurai approach outperforms the existing {{state of the art}} techniques. 1...|$|R
40|$|Abstract—The {{existing}} {{software engineering}} literature has empirically {{shown that a}} proper choice of identifiers influences software understandability and maintainability. Researchers have noticed that identifiers {{are one of the}} most important source of information about program entities and that the semantic of identifiers guide the cognitive process. Recognizing the <b>words</b> forming <b>identifiers</b> is not an easy task when naming conventions (e. g., Camel Case) are not used or strictly followed and–or when these words have been abbreviated or otherwise transformed. This paper proposes a technique inspired from speech recognition, i. e., dynamic time warping, to split <b>identifiers</b> into component <b>words.</b> The proposed technique has been applied to identifiers extracted from two different applications: JHotDraw and Lynx. Results compared to manually-built oracles and with Camel Case algorithm are encouraging. In fact, they show that the technique successfully recognize <b>words</b> composing <b>identifiers</b> (even when abbreviated) in about 90 % of cases and that it performs better than Camel Case. Furthermore, it was able to spot mistakes in the manually-built oracle. Keywords—Source code identifiers; program comprehension. I...|$|R
5000|$|The Locator/Identifier Separation Protocol (LISP or Loc/ID split) [...] {{has been}} {{proposed}} by IETF {{as a solution to}} issues as the scalability of the routing system. LISP main argument is that the semantics of the IP address are overloaded being to be both locator and identiﬁer of an endpoint. LISP proposes to address this issue by separating the IP address into a locator part, which is hierarchical, and an identifier, which is flat. However this is a false distinction: in Computer Science it is impossible to locate something without identifying it and to identify something without locating it, since all the names are using for locating an object within a context. Moreover, LISP continues to use the locator for routing, therefore routes are computed between locators (inter- faces). However, a path does not end on a locator but on an <b>identifier,</b> in other <b>words,</b> the locator is not the ultimate destination of the packet but a point on the path to the ultimate destination. This issue leads to path-discovery problems, as documented by [...] whose solution is known not to scale.|$|R
40|$|The IDENTIFY DEVICE command {{includes}} an optional World Wide Name field containing an IEEE NAA Registered <b>identifier</b> in <b>words</b> 108 - 111. Serial Attached SCSI HBAs may {{use this to}} identify that the drive is the same even if it changes locations in the SCSI domain (the SAS address given to a drive {{is based on the}} expander phy to which it is attached). e 05178 r 0 proposes the field be upgraded to mandatory. The IDENTIFY PACKET DEVICE command does not currently define that field. It would be helpful if it were optionally available, and if implemented by an ATAPI device supporting SCSI commands, matched the SCSI target device name returned in the SCSI INQUIRY command Device Identification VPD page (83 h). The other SCSI names/identifiers it could be each have problems: a) It cannot serve as the SCSI target port name, because if the SATA device is behind a SATA port selector, the same name appears in two different domains. b) It cannot serve as the SCSI target port identifier, because the value is not actually used for addressing within any domains. In SAS, the expander to which the SATA device is attached assigns it a SAS address. An advanced expander could send IDENTIFY PACKET DEVICE to retrieve the World Wide Name and use that as the SAS address, but this is not required in SAS- 1. 1. c) It cannot serve as the logical unit name, because the PACKET command payload might include a SCSI- 2 compatible LOGICAL UNIT NUMBER field in the CDB (byte 1 bits 7 : 5), and each logical unit name must be unique. For IDENTIFY PACKET DEVICE, the World Wide Name needs to be optional, because ATAPI DVD drives, which are the most common packet devices, have little need. ATAPI tape drives are more likely to implement it...|$|R
40|$|Abstract—Understanding {{source code}} <b>identifiers,</b> by {{identifying}} <b>words</b> composing them, {{is a necessary}} step for many program comprehension, reverse engineering, or redocumentation tasks. To this aim, researchers have proposed several identifier splitting and expansion approaches such as Samurai, TIDIER and more recently GenTest. The ultimate goal of such approaches is to help disambiguating conceptual information encoded in compound (or abbreviated) identifiers. This paper presents TRIS, TRee-based Identifier Splitter, a two-phases approach to split and expand program identifiers. First, TRIS pre-compiles transformed dictionary words into a tree representation, associating a cost to each transformation. In a second phase, it maps the identifier splitting/expansion problem into a minimization problem, i. e., the search of the shortest path (optimal split/expansion) in a weighted graph. We apply TRIS {{to a sample of}} 974 identifiers extracted from JHotDraw, 3, 085 from Lynx, and to a sample of 489 identifiers extracted from 340 C programs. Also, we compare TRIS with GenTest on a set of 2, 663 mixed Java, C and C++ identifiers. We report evidence that TRIS split (and expansion) is more accurate than state-of-the-art approaches and that it is also efficient in terms of computation time...|$|R
40|$|In {{this paper}} we are proposing a text {{clustering}} method {{based on a}} well-known Word Sense Disambiguation (WSD) algorithm, the Lesk algorithm, to classify textual data by doing highly accurate Word Sense Disambiguation. The clustering of text data is thus primarily based on the context or meaning of the words used for clustering. The Lesk algorithm is used to return the sense <b>identifiers</b> for the <b>words</b> used to classify the text files by looking up the senses of a word in a Knowledge-Base similar to the English WordNet (enriched with more informative columns or fields for each synset [synonym set] of the English WordNet database), so as to greatly increase the chances of contextual overlap, thereby resulting in high accuracy of proper sense or context identification of the words. The proposed scheme has been tested {{on a number of}} heterogeneous text document datasets. The clustering results and accuracies, obtained using the proposed scheme, have been compared with the results obtained using the K-means clustering algorithm on the Vector Space Models generated for all the heterogeneous textual datasets. Experimental results show that our algorithm performs much better than the Vector Space Model (VSM) and K-means based approach. The technique will thus help the users much better in searching for meaningful contextual information from a highly diversified collection of textual information, which is a key task of the information overload problem...|$|R
40|$|Wirth {{uses his}} own meta {{language}} to define its own syntax (and {{serve as an}} example of its use) : grammar = { production}. production = identifier "= " expression ". ". expression = term { "| " term}. term = factor { factor}. factor = identifier | literal | "(" expression ") " | "[" expression "] " | "{ " expression "}". literal = """ " character { character} """". The <b>word</b> <b>identifier</b> is used to denote a nonterminal symbol, and literal denotes a terminal symbol. For brevity, identifier and character are not further defined. Repetition is denoted by curly braces, i. e., { a} denotes: empty, a, aa, [...] Optionality is expressed by square brackets, i. e., [a] denotes a or empty. Parentheses merely serve for grouping, i. e., (a | b) c stands for: a c | b c. Terminal symbols are either literals, i. e., are enclosed in quote marks or are identifiers which do not appear on the left hand side of the metasymbol =. If a quote mark appears a a literal itself, then it is written twice (as is common in many programming languages). As a machine readable form, I have added the following additional properties to Wirth BNF grammars: Each production must start on a new line and may not have leading spaces. Each symbol, whether meta, terminal, or nonterminal, must be separated from all other symbols by spaces, except the terminating period. Productions may be freely continued on a new line; for readability these lines are often indented. Grammars may contain comments, which are lines which begin with a #, followed by a space. The remainder of the line is ignored. Grammars may contain blank lines to improve readability. Note that the spacing permits convenient processing by simple awk scripts...|$|R
40|$|Session - Orthography: no. 2 Purpose The {{present study}} aimed at {{identifying}} the indicators of persistent reading difficulties among Chinese readers in junior elementary grades. Method A battery of cognitive-linguistic and reading measures {{were administered to}} three groups of Chinese children with different reading trajectories ('persistent poor readers', 'improved poor readers' and 'normal readers') from Grade 1 {{to the beginning of}} Grade 4 in a 3 -year longitudinal study. The three groups were classified according to their performance in a standardized Chinese word reading test in Grade 1 and Grade 4. Results Results of ANOVA on the reading-related measures in Grades 1 and 2 among the three groups revealed that rapid naming was the most important <b>identifier</b> of early <b>word</b> reading difficulty. Additional difficulties in other domains, including orthographic skills, morphological awareness, and syntactic skills, were needed for reading problems to persist. Chinese persistent poor readers did not differ significantly from normally-achieving readers on measures of oral language skills, phonological awareness, or phonological memory. Conclusions The present findings show that poor rapid naming is an early indicator of reading difficulties in Chinese and this may help developing early identification tool for reading difficulty in Chinese. The present findings are also in line with the suggestion about the predominant role of the 'semantic pathway' in the 'triangle model' in reading Chinese words in contrast with the less significant role of the 'phonological pathway'. In terms of educational implications, intervention programmes may include training of orthographic, morphological, and syntactic skills to enhance word reading and reading comprehension of young Chinese poor readers. link_to_OA_fulltextThe 18 th Annual Meeting of the Society for the Scientific Study of Reading (SSSR 2011), St. Pete Beach, FL., 13016 July 2011...|$|R
40|$|Ip Chun Wah Timmy. Thesis (M. Phil.) [...] Chinese University of Hong Kong, 2000. Includes bibliographical {{references}} (leaves 113 - 120). Abstracts in English and Chinese. Chapter 1 [...] - Introduction [...] - p. 1 Chapter 1. 1 [...] - Topic Detection and Tracking [...] - p. 2 Chapter 1. 1. 1 [...] - What is a Topic? [...] - p. 3 Chapter 1. 1. 2 [...] - What is Topic Tracking? [...] - p. 4 Chapter 1. 2 [...] - Research Contributions [...] - p. 4 Chapter 1. 2. 1 [...] - Named Entity Tagging [...] - p. 5 Chapter 1. 2. 2 [...] - Handling Unknown Words [...] - p. 6 Chapter 1. 2. 3 [...] - Named-Entity Approach in Topic Tracking [...] - p. 7 Chapter 1. 3 [...] - Organization of Thesis [...] - p. 7 Chapter 2 [...] - Background [...] - p. 9 Chapter 2. 1 [...] - Previous Developments in Topic Tracking [...] - p. 10 Chapter 2. 1. 1 [...] - BBN's Tracking System [...] - p. 10 Chapter 2. 1. 2 [...] - CMU's Tracking System [...] - p. 11 Chapter 2. 1. 3 [...] - Dragon's Tracking System [...] - p. 12 Chapter 2. 1. 4 [...] - UPenn's Tracking System [...] - p. 13 Chapter 2. 2 [...] - Topic Tracking in Chinese [...] - p. 13 Chapter 2. 3 [...] - Part-of-Speech Tagging [...] - p. 15 Chapter 2. 3. 1 [...] - A Brief Overview of POS Tagging [...] - p. 15 Chapter 2. 3. 2 [...] - Transformation-based Error-Driven Learning [...] - p. 18 Chapter 2. 4 [...] - Unknown Word Identification [...] - p. 20 Chapter 2. 4. 1 [...] - Rule-based approaches [...] - p. 21 Chapter 2. 4. 2 [...] - Statistical approaches [...] - p. 23 Chapter 2. 4. 3 [...] - Hybrid approaches [...] - p. 24 Chapter 2. 5 [...] - Information Retrieval Models [...] - p. 25 Chapter 2. 5. 1 [...] - Vector-Space Model [...] - p. 26 Chapter 2. 5. 2 [...] - Probabilistic Model [...] - p. 27 Chapter 2. 6 [...] - Chapter Summary [...] - p. 28 Chapter 3 [...] - System Overview [...] - p. 29 Chapter 3. 1 [...] - Segmenter [...] - p. 30 Chapter 3. 2 [...] - TEL Tagger [...] - p. 31 Chapter 3. 3 [...] - Unknown <b>Words</b> <b>Identifier</b> [...] - p. 32 Chapter 3. 4 [...] - Topic Tracker [...] - p. 33 Chapter 3. 5 [...] - Chapter Summary [...] - p. 34 Chapter 4 [...] - Named Entity Tagging [...] - p. 36 Chapter 4. 1 [...] - Experimental Data [...] - p. 37 Chapter 4. 2 [...] - Transformational Tagging [...] - p. 41 Chapter 4. 2. 1 [...] - Notations [...] - p. 41 Chapter 4. 2. 2 [...] - Corpus Utilization [...] - p. 42 Chapter 4. 2. 3 [...] - Lexical Rules [...] - p. 42 Chapter 4. 2. 4 [...] - Contextual Rules [...] - p. 47 Chapter 4. 3 [...] - Experiment and Result [...] - p. 49 Chapter 4. 3. 1 [...] - Lexical Tag Initialization [...] - p. 50 Chapter 4. 3. 2 [...] - Contribution of Lexical and Contextual Rules [...] - p. 52 Chapter 4. 3. 3 [...] - Performance on Unknown Words [...] - p. 56 Chapter 4. 3. 4 [...] - A Possible Benchmark [...] - p. 57 Chapter 4. 3. 5 [...] - Comparison between TEL Approach and the Stochas- tic Approach [...] - p. 58 Chapter 4. 4 [...] - Chapter Summary [...] - p. 59 Chapter 5 [...] - Handling Unknown Words in Topic Tracking [...] - p. 62 Chapter 5. 1 [...] - Overview [...] - p. 63 Chapter 5. 2 [...] - Person Names [...] - p. 64 Chapter 5. 2. 1 [...] - Forming possible named entities from OOV by group- ing n-grams [...] - p. 66 Chapter 5. 2. 2 [...] - Overlapping [...] - p. 69 Chapter 5. 3 [...] - Organization Names [...] - p. 71 Chapter 5. 4 [...] - Location Names [...] - p. 73 Chapter 5. 5 [...] - Dates and Times [...] - p. 74 Chapter 5. 6 [...] - Chapter Summary [...] - p. 75 Chapter 6 [...] - Topic Tracking in Chinese [...] - p. 77 Chapter 6. 1 [...] - Introduction of Topic Tracking [...] - p. 78 Chapter 6. 2 [...] - Experimental Data [...] - p. 79 Chapter 6. 3 [...] - Evaluation Methodology [...] - p. 81 Chapter 6. 3. 1 [...] - Cost Function [...] - p. 82 Chapter 6. 3. 2 [...] - DET Curve [...] - p. 83 Chapter 6. 4 [...] - The Named Entity Approach [...] - p. 85 Chapter 6. 4. 1 [...] - Designing the Named Entities Set for Topic Tracking [...] - p. 85 Chapter 6. 4. 2 [...] - Feature Selection [...] - p. 86 Chapter 6. 4. 3 [...] - Integrated with Vector-Space Model [...] - p. 87 Chapter 6. 5 [...] - Experimental Results and Analysis [...] - p. 91 Chapter 6. 5. 1 [...] - Notations [...] - p. 92 Chapter 6. 5. 2 [...] - Stopword Elimination [...] - p. 92 Chapter 6. 5. 3 [...] - TEL Tagging [...] - p. 95 Chapter 6. 5. 4 [...] - Unknown <b>Word</b> <b>Identifier</b> [...] - p. 100 Chapter 6. 5. 5 [...] - Error Analysis [...] - p. 106 Chapter 6. 6 [...] - Chapter Summary [...] - p. 108 Chapter 7 [...] - Conclusions and Future Work [...] - p. 110 Chapter 7. 1 [...] - Conclusions [...] - p. 110 Chapter 7. 2 [...] - Future Work [...] - p. 111 Bibliography [...] - p. 113 Chapter A [...] - The POS Tags [...] - p. 121 Chapter B [...] - Surnames and transliterated characters [...] - p. 123 Chapter C [...] - Stopword List for Person Name [...] - p. 126 Chapter D [...] - Organization suffixes [...] - p. 127 Chapter E [...] - Location suffixes [...] - p. 128 Chapter F [...] - Examples of Feature Table (Train {{set with}} condition D 410) [...] - p. 12...|$|R
40|$|Lexical Simplification is {{the process}} of {{replacing}} complex words in texts to create simpler, more easily comprehensible alternatives. It has proven very useful as an assistive tool for users who may find complex texts challenging. Those who suffer from Aphasia and Dyslexia are among the most common beneficiaries of such technology. In this thesis we focus on Lexical Simplification for English using non-native English speakers as the target audience. Even though they number in hundreds of millions, there are very few contributions that aim {{to address the needs of}} these users. Current work is unable to provide solutions for this audience due to lack of user studies, datasets and resources. Furthermore, existing work in Lexical Simplification is limited regardless of the target audience, as it tends to focus on certain steps of the simplification process and disregard others, such as the automatic detection of the words that require simplification. We introduce a series of contributions to the area of Lexical Simplification that range from user studies and resulting datasets to novel methods for all steps of the process and evaluation techniques. In order to understand the needs of non-native English speakers, we conducted three user studies with 1, 000 users in total. These studies demonstrated that the number of words deemed complex by non-native speakers of English correlates with their level of English proficiency and appears to decrease with age. They also indicated that although words deemed complex tend to be much less ambiguous and less frequently found in corpora, the complexity of words also depends on the context in which they occur. Based on these findings, we propose an ensemble approach which achieves state-of-the-art performance in identifying words that challenge non-native speakers of English. Using the insight and data gathered, we created two new approaches to Lexical Simplification that address the needs of non-native English speakers: joint and pipelined. The joint approach employs resource-light neural language models to simplify words deemed complex in a single step. While its performance was unsatisfactory, it proved useful when paired with pipelined approaches. Our pipelined simplifier generates candidate replacements for complex words using new, context-aware word embedding models, filters them for grammaticality and meaning preservation using a novel unsupervised ranking approach, and finally ranks them for simplicity using a novel supervised ranker that learns a model based on the needs of non-native English speakers. In order to test these and previous approaches, we designed LEXenstein, a framework for Lexical Simplification, and compiled NNSeval, a dataset that accounts for the needs of non-native English speakers. Comparisons against hundreds of previous approaches as well as the variants we proposed showed that our pipelined approach outperforms all others. Finally, we introduce PLUMBErr, a new automatic error identification framework for Lexical Simplification. Using this framework, we assessed the type and number of errors made by our pipelined approach throughout the simplification process and found that combining our ensemble complex <b>word</b> <b>identifier</b> with our pipelined simplifier yields a system that makes up to 25 % fewer mistakes compared to the previous state-of-the-art strategies during the simplification process...|$|R

