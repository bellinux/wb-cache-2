51|108|Public
2500|$|Architectural {{experiments}} are continuing {{in a number}} of directions, e.g. the Cyclops64 system uses a [...] "supercomputer on a chip" [...] approach, in a direction away from the use of massive distributed processors. Each 64-bit Cyclops64 chip contains 80 processors, and the entire system uses a globally addressable memory architecture. The processors are connected with non-internally blocking crossbar switch and communicate with each other via global <b>interleaved</b> <b>memory.</b> There is no data cache in the architecture, but half of each SRAM bank {{can be used as a}} scratchpad memory. Although this type of architecture allows unstructured parallelism in a dynamically non-contiguous memory system, it also produces challenges in the efficient mapping of parallel algorithms to a many-core system.|$|E
50|$|If {{the cells}} are added as {{floating}} cells (no <b>interleaved</b> <b>memory)</b> {{they can also}} be warm added and removed from the partition.|$|E
5000|$|With <b>interleaved</b> <b>memory,</b> memory {{addresses}} are allocated to each memory bank in turn. For example, in an interleaved system with two memory banks (assuming word-addressable memory), if logical address 32 belongs to bank 0, then logical address 33 would belong to bank 1, logical address 34 would belong to bank 0, and so on. An <b>interleaved</b> <b>memory</b> {{is said to}} be n-way interleaved when there are [...] banks and memory location [...] resides in bank [...]|$|E
5000|$|C V Ramamoorthy and Benjamin W Wah. An Optimal Algorithm for Scheduling Requests on <b>Interleaved</b> <b>Memories</b> for a Pipelined Processor. IEEE Trans. Computers (...) , 30(10):787- 800, 1981.|$|R
40|$|In <b>interleaved</b> <b>memories,</b> {{interference}} between concurrently active vector streams {{results in}} memory bank conflicts and reduced bandwidth. In this paper, we present two schemes for reducing inter-vector interference. First, we propose a memory module partitioning technique in which disjoint access sets are created {{for each of}} the concurrent vectors. Various properties of the involved address mapping are presented. Then we present an interlaced data placement scheme, where the simultaneously accessed vectors are interlaced and stored to the memory. Performance of the two schemes are evaluated by trace driven simulation. It is observed that the schemes have significant merit in reducing the interference in <b>interleaved</b> <b>memories</b> and increasing the effective memory bandwidth. The schemes are applicable to memory systems for superscalar processors, vector supercomputers and parallel processors. Keywords: Address Distribution, Conflict-free access, Vector Interference, Memory Storage Patterns [...] ...|$|R
30|$|Fetching variable-sized {{instructions}} from the method cache can be performed in a single cycle. The method cache is split into two <b>interleaved</b> <b>memories</b> banks. Each of the two cache memories needs a read port wide enough for a maximum-sized instruction. Accessing both memories concurrently with a clever address calculation overcomes the boundary issue for variable-sized instruction access.|$|R
50|$|Early {{research}} into <b>interleaved</b> <b>memory</b> was performed at IBM in the 60s and 70s {{in relation to}} the IBM 7030 Stretch computer, but development went on for decades improving design, flexibility and performance to produce modern implementations.|$|E
50|$|The {{processors}} will {{be connected}} with a 96 port, 7 stage non-internally blocking crossbar switch. They will {{communicate with each other}} via global <b>interleaved</b> <b>memory</b> (memory that can be written to and read by all threads) in the SRAM.|$|E
50|$|It is {{different}} from multi-channel memory architectures, primarily as <b>interleaved</b> <b>memory</b> is not adding more channels between the main memory and the memory controller. However, channel interleaving is also possible, for example in freescale i.MX6 processors, which allow interleaving to be done between two channels.|$|E
50|$|The SIMMs are two-way {{interleaved}} {{using the}} low-order method, where even and odd memory addresses {{are treated as}} separate banks of <b>memory.</b> <b>Interleaving</b> the <b>memory</b> subsystem doubles the bandwidth of a non-interleaved memory subsystem using the same DRAMs, allowing the Model 200 Series to achieve an effective maximum bandwidth of 100 MB/s.|$|R
40|$|This paper {{presents}} an industrial evaluation of tests for intra-word faults in word oriented memories, applied to big arrays with bit-interleaved organization {{as well as}} to small arrays with bit-adjacent organization, in order to investigate the influence of the memory organization on the intraword faults. The test results show that the intra-word faults are also significantly important for <b>interleaved</b> <b>memories,</b> even though that the cells within a single word are not physically adjacent. ...|$|R
50|$|<b>Memory</b> <b>interleaving</b> {{is a way}} to {{distribute}} individual addresses over memory modules. Its aim is to keep the most of modules busy as computations proceed. With <b>memory</b> <b>interleaving,</b> the low-order k bits of the memory address generally specify the module on several buses.|$|R
50|$|<b>Interleaved</b> <b>memory</b> {{results in}} {{contiguous}} reads (which are common both in multimedia {{and execution of}} programs) and contiguous writes (which are used frequently when filling storage or communication buffers) actually using each memory bank in turn, instead of using the same one repeatedly. This results in significantly higher memory throughput as each bank has a minimum waiting time between reads and writes.|$|E
50|$|In computing, <b>interleaved</b> <b>memory</b> is {{a design}} made to {{compensate}} for the relatively slow speed of dynamic random-access memory (DRAM) or core memory, by spreading memory addresses evenly across memory banks. That way, contiguous memory reads and writes are using each memory bank in turn, resulting in higher memory throughputs due to reduced waiting for memory banks to become ready for desired operations.|$|E
50|$|The four {{models of}} the 360/85 are: I85 (512K), J85 (1M), K85 (2M) and L85 (4M), {{configured}} with 2 2365 Processor Storage units, 4 2365 units, an IBM 2385 Processor Storage unit Model 1 (=2M), or an IBM 2385 Processor Storage unit Model 2 (=4M) respectively. The I85 includes two-way <b>interleaved</b> <b>memory</b> while the others provide four-way interleaving of memory access.|$|E
30|$|Our {{architecture}} {{features a}} special 32 -bit mode to address memories over the AXI bus in a linear or interleaved fashion. Time interleavers (DAB, DVB-SH) cannot {{be stored in}} the local <b>interleaving</b> <b>memory</b> and require an external SDRAM. The MMFIC does not support such interleavers, whereas our architecture allows for tight integration of time interleavers and their neighboring interleaving functions. For example, DVB-SH requires a chain of symbol de-interleaving, depuncturing, time de-interleaving and bit de-interleaving. Our architecture implements this in two iterations over its local memory and a single iteration over an SDRAM.|$|R
40|$|In this paper, a <b>memory</b> <b>interleaving</b> and {{interlacing}} VLSI architecture for deblocking filter in H. 264 /AVC is proposed. Many literatures and {{the results}} of the chip implementation show that the memory organization dominates the hardware cost, the throughput rate, and the external memory bandwidth of the deblocking filter. Hence, we also discuss three different levels of the data-reuse scheme for deblocking filter in this paper. In order to increase the throughput, we propose the <b>memory</b> <b>interleaving</b> techniques to arrange data in the on-chip memory and access the data in both horizontal and vertical filters efficiently. We also utilize the hybrid schedule for 2 -D processing order and the memory interlacing configuration to reduce the total on-chip memory size and to accomplish the Level B data-reuse scheme. According to proposed <b>memory</b> <b>interleaving</b> organization, <b>memory</b> interlacing configuration and hybrid schedule of the 2 -D process order, our architecture only utilizes a half of the traditional memory size to boost the throughput and reduce the bus memory bandwidth(1) ...|$|R
5000|$|A space-efficient {{implementation}} of a sparse trie, in which the descendants of each node may be <b>interleaved</b> in <b>memory.</b> (The name is suggested by a similarity to a closed hash table.) ...|$|R
5000|$|... 6000-series {{systems were}} said to be [...] "memory oriented"— a system {{controller}} in each memory module had eight ports for communication with other system components, with an interrupt cell for each port. Memory modules contained 128 K words of 1.2 μs 36-bit words; a system could support one or two memory modules for a maximum of 256 K words (1 MB of 9-bit bytes). Each module provided two-way <b>interleaved</b> <b>memory.</b>|$|E
50|$|The Cray-1 {{was built}} as a 64-bit system, a {{departure}} from the 7600/6600, which were 60-bit machines (a change was also planned for the 8600). Addressing was 24-bit, with a maximum of 1,048,576 64-bit words (1 megaword) of main memory, where each word also had 8 parity bits for a total of 72 bits per word. There were 64 data bits and 8 check bits. Memory was spread across 16 <b>interleaved</b> <b>memory</b> banks, each with a 50 ns cycle time, allowing up to four words to be read per cycle. Smaller configurations could have 0.25 or 0.5 megawords of main memory.|$|E
5000|$|Architectural {{experiments}} are continuing {{in a number}} of directions, e.g. the Cyclops64 system uses a [...] "supercomputer on a chip" [...] approach, in a direction away from the use of massive distributed processors. Each 64-bit Cyclops64 chip contains 80 processors, and the entire system uses a globally addressable memory architecture. The processors are connected with non-internally blocking crossbar switch and communicate with each other via global <b>interleaved</b> <b>memory.</b> There is no data cache in the architecture, but half of each SRAM bank {{can be used as a}} scratchpad memory. Although this type of architecture allows unstructured parallelism in a dynamically non-contiguous memory system, it also produces challenges in the efficient mapping of parallel algorithms to a many-core system.|$|E
40|$|In this paper, we {{investigate}} {{the costs and}} benefits of implementing <b>memory</b> <b>interleaving</b> in software. As our main contribution, we compare software <b>memory</b> <b>interleaving</b> to row-major allocation and logarithmic broadcasting. Our analysis demonstrates the clear su-periority of software interleaving over row-major al-location in the presence of memory contention. Our analysis also indicates that the choice between soft-ware interleaving and logarithmic broadcasting is less clear, as it depends both on the type of synchroniza-tion used and the number of processors. We conclude that, on large-scale multiprocessors, software <b>memory</b> <b>interleaving</b> and lock-based synchronization is the most effective combination for reducing memory contention in matrix computations. ...|$|R
2500|$|One more {{approach}} is to [...] "pack" [...] the trie. Liang describes a space-efficient implementation of a sparse packed trie applied to automatic hyphenation, in which the descendants of each node may be <b>interleaved</b> in <b>memory.</b>|$|R
5000|$|Furthermore, {{independent}} storage sections provided two-way (H75) or four-way (I75, J75) <b>interleaving</b> of <b>memory</b> access. Even {{with only}} two-way interleaving, [...] "an effective sequential access rate of 400 nanoseconds per double word (eight bytes) is possible." ...|$|R
40|$|Interleaved {{memories}} {{are often used}} to provide the high bandwidth needed by multiprocessors and high performance uniprocessors such as vector and VLIW processors. The manner in which memory locations are distributed across the memory modules has a significant influence on whether, and for which types of reference patterns, the full bandwidth of the memory system is achieved. The most common <b>interleaved</b> <b>memory</b> architecture is the sequentially <b>interleaved</b> <b>memory</b> in which successive memory locations are assigned to successive memory modules. Although such an architecture is the simplest to implement and provides good performance with strides that are odd integers, it can degrade badly {{in the face of}} even strides, especially strides that are a power of two. In a pseudo-randomly <b>interleaved</b> <b>memory</b> architecture, memory locations are assigned to the memory modules in some pseudo-random fashion in the hope that those sequences of references, which are likely to occur in practice, will end up being evenly distributed across the memory modules. The notion of polynomial interleaving modulo an irreducible polynomial is introduced as a way of achieving pseudo-random interleaving with certain attractive and provable properties. The theory behind this scheme is developed and the results of simulations are presented. Kev words: supercomputer memory, parallel memory, <b>interleaved</b> <b>memory,</b> hashed memory, pseudo-random interleaving, memory buffering. 1...|$|E
40|$|We propose an <b>interleaved</b> <b>memory</b> {{organization}} supporting multi-pattern parallel accesses in twodimensional (2 D) addressing space. Our proposal targets computing {{systems with}} high memory bandwidth demands such as vector processors, multimedia accelerators, etc. We substantially extend prior research on <b>interleaved</b> <b>memory</b> organizations introducing 2 D-strided accesses along with additional parameters, which define a large variety of 2 D data patterns. The proposed scheme guarantees minimum memory latency and efficient bandwidth utilization for arbitrary configuration {{parameters of the}} data pattern. We provide mathematical descriptions and proofs of correctness for the proposed addressing schemes. The design complexity and the critical paths are evaluated using technology independent resource count...|$|E
40|$|Interleaved {{memories}} {{are often used}} to provide the high bandwidth needed by multi- processors and high performance uniprocessors. The manner in which memory locations are distributed across the memory modules has a significant influence on whether, and for which types of reference patterns, the full bandwidth of the memory system is achieved. The most common <b>interleaved</b> <b>memory</b> architecture is the sequentially <b>interleaved</b> <b>memory</b> in which successive memory locations are assigned to successive memory modules. Although such an architecture is the simplest to implement and provides good performance with strides that are odd integers, it can degrade badly {{in the face of}} even strides, especially strides that are a power of two. This happens because all the memory references are concentrated on a subset of the memory modules. Pseudo...|$|E
5000|$|One more {{approach}} is to [...] "pack" [...] the trie. Liang describes a space-efficient implementation of a sparse packed trie applied to automatic hyphenation, in which the descendants of each node may be <b>interleaved</b> in <b>memory.</b>|$|R
40|$|With {{growing demand}} of {{machine to machine}} (M 2 M) communication, {{wireless}} communication systems request simultaneous connections for many terminals to cope with thus increasing communication throughput. We focus on interleave division multiple access (EDMA) that has superior user detection performance and describe a VLSI design of an interference canceller that performs user detection in QPSK OFDM-IDMA systems. A conventional interference canceller has an issue of degradation in <b>interleave</b> <b>memory</b> throughput. We propose a new architecture of dual-frame processing in an interference canceller by making use of an OFDM-DDMA frame structure. In FPGA implementation, the proposed architecture has shown fewer hardware resources compared with the conventional architecture...|$|R
5000|$|... #Caption: <b>Memory</b> <b>interleaving</b> example with 4 banks. Red {{banks are}} {{refreshing}} and can't be used.|$|R
40|$|Memory {{interleaving}} is a cost-efficient {{approach to}} increase bandwidth. Improving data access locality and reducing memory access conflicts are two important aspects to achieve high efficiency for <b>interleaved</b> <b>memory.</b> In this paper, we introduce a design framework that integrates these two optimizations, {{in order to}} find out minimal memory banks and channels required in the embedded system under performance restriction. Several important techniques, loop and data layout transformations for data access locality, extracting data streams, conflict cache miss reduction as well as data placement and optimally reordered access for interleaved memories, are incorporated in the design framework. Experiments show that our co-design method results in substantially less hardware requirement compared to the implementations without optimization. Keywords <b>Interleaved</b> <b>memory</b> systems, data access locality, memory access conflict, in-dimension-stride vector, extracted data stream, optimally reordered access. 1...|$|E
40|$|On many {{commercial}} supercomputers, several vector register processors share {{a global}} highly <b>interleaved</b> <b>memory</b> in a MIMD mode. When all the processors {{are working on}} a single verctor loop, {{a significant part of the}} potential memory throughput may be wasted due to the asynchronism of the processors. In order to limit loss of memory throughput, a SIMD synchronization mode for vector accesses to memory may be used. But an important part of the memory bandwith may be wasted when accessing vectors with an even stride. In this paper, we present IPS, an interleaved parallel scheme, which ensures an equitable distribution of elements on a highly <b>interleaved</b> <b>memory</b> for a wide range a vector strides. We show how to organize access to memory, such that unscrambling of vectors from memory to the vector register processors requires a minimum number of passes through the interconnection network...|$|E
40|$|Many compute-intensive {{applications}} generate single result {{values by}} accessing clusters of nearby points in grids of one, two, or more dimensions. Often, {{the performance of}} FGPA implementations of such algorithms would benefit from concurrent, non-interfering access to all points in each cluster. When clusters contain dozens of points and access patterns are irregular, multiported memories are infeasible and vector-oriented approaches are inapplicable. Instead, the grid points may be distributed across multiple <b>interleaved</b> <b>memory</b> banks so that, when accessing any cluster, each point comes from a different memory bank. We present a general technique based on the application’s multidimensional indexing rather than linearized memory addresses. This technique maps the cluster structure into a custom, <b>interleaved</b> <b>memory</b> using the FPGA’s multiple on-chip RAMs and configurable data paths. Case studies demonstrate rectangular and non-rectangular grids of different dimensions, including performance vs. resource tradeoffs when cluster sizes are not powers of two. 1...|$|E
30|$|Another unique {{capability}} of our architecture {{is that it}} supports conflict resolution of read accesses. As mentioned before, conflict resolution is essential when handling streams at high-speed. Read-access conflict resolution allows data <b>interleaving</b> during <b>memory</b> read-out at high-speed. Being able to both write data interleaved to and read data <b>interleaved</b> from the <b>memory</b> allows implementation of two interleaving functions in a single iteration over the memory. Also complex interleavers or demultiplex/interleave scenarios can be supported at high-speeds. The use case of 4 simultaneous DVB-T radios significantly benefits from this capability, combining symbol and bit de-interleaving.|$|R
40|$|Abstract—Parallel memory modules {{can be used}} to {{increase}} memory bandwidth and feed a processor with only necessary data. Arbitrary stride access capability with <b>interleaved</b> <b>memories</b> is described in previous research where the skewing scheme is changed at run time according to the currently used stride. This paper presents the improved schemes which are adapted to parallel memories. The proposed novel parallel memory architecture allows conflict free accesses with all the constant strides which has not been possible in prior application specific parallel memories. Moreover, the possible access locations are unrestricted and the data patterns have equal amount of accessed data elements as the number of memory modules. The complexity is evaluated with resource counts. I...|$|R
50|$|The /W32i {{revision}} featured an <b>interleaved</b> 32-bit <b>memory</b> bus (with 2MB+ of memory) {{to improve}} memory throughput. It supports {{a maximum of}} 4 MB of video memory, though most boards featuring the chip typically offer a maximum expansion of 2MB or less.|$|R
