15|10000|Public
40|$|This thesis {{presents}} a <b>generic</b> <b>neural</b> <b>network</b> analysis method that utilizes domain-specific basic functions {{that are easy}} to interpret by the user and that can furthermore be used to optimize neural network systems. In general, the analysis consists in describing the internal functionality of the neural network in terms of domain-specific basic functions, functions that can be considered basic in the application domain of the neural network...|$|E
40|$|Neural Networks {{are widely}} used in pattern recognition, {{security}} applications and data manipulation. We propose a novel hardware architecture for a <b>generic</b> <b>neural</b> <b>network,</b> using Network on Chip (NoC) interconnect. The proposed architecture allows for expandability, mapping {{of more than one}} logical unit onto a single physical unit, and dynamic reconfiguration based on application-specific demands. Simulation results show that this architecture has significant performance benefits over existing architectures...|$|E
40|$|This article {{presents}} an XML[2] based language for the specication {{of objects in}} the Soft Computing area. The design promotes reuse and takes a compositional approach in which more complex constructs are built from simpler ones; it is also independent of implementation details as the denition of the language only states the expected behaviour of every possible imple-mentation. Here the basic structures for the specication of concepts in the Fuzzy Logic area are described and a simple construct for a <b>generic</b> <b>neural</b> <b>network</b> model is introduced...|$|E
30|$|Before {{applying}} the {{general theory of}} Section  2, let us make several comments about this <b>generic</b> model of <b>neural</b> <b>network</b> with learning. This model is a non-autonomous, stochastic, non-linear slow-fast system.|$|R
40|$|This paper {{presents}} a new <b>generic</b> multitopographic <b>neural</b> <b>network</b> model whose main {{area of application}} is clustering and knowledge extraction tasks on documentary data. The most powefull features of this model are its generalization mechanism and its mechanism of communication between topographies. This paper shows how these mechanisms can be exploited {{within the framework of}} the SOM and NG models. An evaluation of the generalization mechanism based on original quality and propagation coherency measures is also proposed. A secondary result of this evaluation is to proof that the generalization mechanism could significantly reduce the wellknown border effect of the SOM map...|$|R
40|$|AbstractThis paper {{presents}} a computational framework, the <b>Generic</b> Programmable <b>Neural</b> <b>Network</b> (GPNN), for efficient implementation of Back-Propagation based neural learning algorithms running on multi-core machines. GPNN has three components: parallelization of neural learning, abstraction of network components, and compile-time generalization. Together these computational components make GPNN an efficient framework for fast implementation of back-propagation based neural learning algorithms, and provide flexibility and reusability for modifying <b>neural</b> <b>network</b> topologies. The GPNN {{was applied to}} four different neural learning algorithms: classic back-propagation (BP), quick propagation (QP), resilient propagation (RP) and Levenberg-Marquardt (LM) algorithm. Experiments were conducted {{to evaluate the effectiveness}} of GPNN, and results show that the neural learning algorithms implemented in GPNN are more efficient than their respective functions provided by Matlab...|$|R
40|$|International audienceWe {{consider}} sequential {{decision making}} in the case where a generative model and a parametric policy are available. Such a framework is naturally tackled with Direct Policy Search, i. e. parametric op-timisation over simulations. We propose a simple method that combines this parametric policy with a more <b>generic</b> <b>neural</b> <b>network,</b> where all parameters are trained simultaneously. As such, our approach doesn't require any computational overhead. We show that the resulting policy significantly outperforms both the domain specific policies and the neural network on a unit commitment test problem...|$|E
40|$|The MediaEval 2012 Affect Task {{challenged}} {{participants to}} automatically find violent scenes {{in a set}} of Hollywood movies. We propose to first predict a set of mid-level concept annotations from low-level visual and auditory features, then fuse the concept predictions and features to detect violent content. Instead of engineering features suitable for the task, we deliberately restrict ourselves to simple generalpurpose features with limited temporal context and a <b>generic</b> <b>neural</b> <b>network</b> classifier, setting a baseline for more sophisticated approaches. On 3 test movies, our system detects 49 % of violent frames at a precision of 28 %, outperforming all other submissions. 1...|$|E
40|$|Rapid {{progress}} in deep reinforcement learning {{has made it}} increasingly feasible to train controllers for high-dimensional humanoid bodies. However, methods that use pure reinforcement learning with simple reward functions tend to produce non-humanlike and overly stereotyped movement behaviors. In this work, we extend generative adversarial imitation learning to enable training of <b>generic</b> <b>neural</b> <b>network</b> policies to produce humanlike movement patterns from limited demonstrations consisting only of partially observed state features, without access to actions, even when the demonstrations come from a body with different and unknown physical parameters. We leverage this approach to build sub-skill policies from motion capture data and show {{that they can be}} reused to solve tasks when controlled by a higher level controller...|$|E
40|$|Flexible {{behaviors}} are organized by complex <b>neural</b> <b>networks</b> in the prefrontal cortex. Recent {{studies have suggested}} that such networks exhibit multiple dynamical states, and can switch rapidly from one state to another. In many complex systems such as the brain, the early-warning signals that may predict whether a critical threshold for state transitions is approaching are extremely difficult to detect. We hypothesized that increases in firing irregularity are a crucial measure for predicting state transitions in the underlying neuronal circuits of the prefrontal cortex. We used both experimental and theoretical approaches to test this hypothesis. Experimentally, we analyzed activities of neurons in the prefrontal cortex while monkeys performed a maze task that required them to perform actions to reach a goal. We observed increased firing irregularity before the activity changed to encode goal-to-action information. Theoretically, we constructed theoretical <b>generic</b> <b>neural</b> <b>networks</b> and demonstrated that changes in neuronal gain on functional connectivity resulted in a loss of stability and an altered state of the networks, accompanied by increased firing irregularity. These results suggest that assessing the temporal pattern of neuronal fluctuations provides important clues regarding the state stability of the prefrontal network. We also introduce a novel scheme that the prefrontal cortex functions in a metastable state near the critical point of bifurcation. According to this scheme, firing irregularity in the prefrontal cortex indicates that the system is about to change its state and the flow of information in a flexible manner, which is essential for executive functions. Thi...|$|R
40|$|Speaker {{authentication}} {{has been}} developed rapidly {{in the last few}} decades. This research work attempts to extract the hidden features of human voice that is able to simulate human auditory system characteristics in speaker authentication. The hidden features are then presented as inputs to a Multi-Layer Perceptron <b>Neural</b> <b>Network</b> and <b>Generic</b> Self-organizing Fuzzy <b>Neural</b> <b>Network</b> to verify the speakers with high accuracy. Based on the experimental results, the two networks are able to verify speakers using two method in extracting hidden features from the recorded voice sources. r 2005 Elsevier B. V. All rights reserved...|$|R
40|$|Nonlinear {{methods such as}} Deep <b>Neural</b> <b>Networks</b> (DNNs) are {{the gold}} {{standard}} for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting <b>generic</b> multilayer <b>neural</b> <b>networks</b> by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empiricall y on the MNIST and ILSVRC data sets...|$|R
40|$|The {{excellent}} {{performance of}} deep neural networks has {{enabled us to}} solve several automatization problems, opening an era of autonomous devices. However, current deep net architectures are heavy with millions of parameters and require billions of floating point operations. Several works {{have been developed to}} compress a pre-trained deep network to reduce memory footprint and, possibly, computation. Instead of compressing a pre-trained network, in this work, we propose a <b>generic</b> <b>neural</b> <b>network</b> layer structure employing multilinear projection as the primary feature extractor. The proposed architecture requires several times less memory as compared to the traditional Convolutional Neural Networks (CNN), while inherits the similar design principles of a CNN. In addition, the proposed architecture is equipped with two computation schemes that enable computation reduction or scalability. Experimental results show the effectiveness of our compact projection that outperforms traditional CNN, while requiring far fewer parameters. Comment: 10 pages, 3 figure...|$|E
40|$|Neural {{networks}} {{are commonly used}} in protein secondary structure prediction. However, neural networks {{tend to be very}} computationally expensive and time consuming because the training algorithms have to determine the values of thousands of variables that define the networks. In this paper, we suggest a simple information theoretic method of pre-processing the training set before feeding it to a <b>generic</b> <b>neural</b> <b>network,</b> which should theoretically reduce the complexities of the neural network significantly, thus saving valuable computational time. Due to insufficient time, we did not actually physically implement a neural network with a pre-processing stage. Instead, we shall only demonstrate the feasibility of our suggestion by pre-processing the family of leucine zippers and show that it achieves the aim of obtaining improved inputs to the network. Hopefully, our future work will involve implementing such a neural network with a pre-processing stage to determine the actual improvement in computational time. 2 Introduction to Neural Networks and Structure Prediction The three-dimensional structure of a protein is very important to the function of...|$|E
40|$|This article {{presents}} a learning model that simulates {{the control of}} an anthropomorphic arm kinematics motion. The objective is to reach and grasp a static prototypic object placed behind different kinds of obstacle in size and position. The network, composed of two <b>generic</b> <b>neural</b> <b>network</b> modules, learns to combine multi-modal arm-related information (trajectory parameters) as well as obstacle-related information (obstacle size and location). Our simulation {{was based on the}} notion of Via Point, which postulates that the motion planning that is divided into specific successive position of the arm. In order to determine these special points, an experimental protocol has been built and pertinent parameters have been integrated to the model. According to these studies, we propose an original method that takes into account the previous learning modules to determine the entire trajectory of the wrist in order to reach the same object placed behind two successive obstacles. The aim of this approach is to understand better the impact of experience in a task realisation and show that learning can be performed from previous initiation. Some results (applied to obstacle avoidance task) show the efficiency of the proposed method...|$|E
40|$|A novel {{mechanism}} is proposed for pulse-based logarithmic computation that is scalable across multi-core neuromorphic processors. At {{the core of}} the proposed method is an integrate-and-fire margin-propagation (IFMP) unit that facilitates spike-based computation in the logarithmic domain. We first show that a network of three IF neurons implement threshold and rectification operations to compute nonlinear functions. The functions used in the computational algorithms are mapped into log-sum-exp function and approximated using margin propagation algorithm. Then, we show that a <b>generic</b> pulse <b>neural</b> <b>network</b> is implemented by pipelining and cascading multiple IFMP units. Because all computations are performed in log-domain, multiplication of the pre-synaptic pulse-train by synaptic weights is translated into a simple addition between the two input spike-trains. We demonstrate the scalability of the proposed approach by designing an IFMP-based support vector machine (SVM) for a separable and non separable classification tasks...|$|R
40|$|In {{this report}} we {{investigate}} the storage capacity of an abstract <b>generic</b> attractor <b>neural</b> <b>network</b> {{model of the}} mammalian cortex. This model network has a diluted connection matrix and a fixed activity level that is independent of network size. We develop an analytical model of the storage capacity {{for this type of}} networks when they are trained with both the Willshaw and Hopfield learning-rules. Experimentally we investigate three different learning-rules, the two mentioned and the BCPNN. We propose a new method for coding arbitrarily sparse patterns into this network, which allows some of the hypercolumns to be silent, i. e. they do not send any information to other hypercolumns. We find that silent hypercolumns cannot be used together with the Hopfield learning-rule. We show that silent hypercolumns increases the storage capacity and flexibility of this type of networks. ...|$|R
40|$|We propose {{associative}} domain adaptation, a novel {{technique for}} end-to-end domain adaptation with <b>neural</b> <b>networks,</b> {{the task of}} inferring class labels for an unlabeled target domain based on the statistical properties of a labeled source domain. Our training scheme follows the paradigm {{that in order to}} effectively derive class labels for the target domain, a network should produce statistically domain invariant embeddings, while minimizing the classification error on the labeled source domain. We accomplish this by reinforcing associations between source and target data directly in embedding space. Our method can easily be added to any existing classification network with no structural and almost no computational overhead. We demonstrate the effectiveness of our approach on various benchmarks and achieve state-of-the-art results across the board with a <b>generic</b> convolutional <b>neural</b> <b>network</b> architecture not specifically tuned to the respective tasks. Finally, we show that the proposed association loss produces embeddings that are more effective for domain adaptation compared to methods employing maximum mean discrepancy as a similarity measure in embedding space. Comment: In IEEE International Conference on Computer Vision (ICCV), 201...|$|R
40|$|We {{present the}} first public release of our <b>generic</b> <b>neural</b> <b>network</b> {{training}} algorithm, called SkyNet. This efficient and robust machine learning tool is able to train large and deep feed-forward neural networks, including autoencoders, {{for use in a}} wide range of supervised and unsupervised learning applications, such as regression, classification, density estimation, clustering and dimensionality reduction. SkyNet uses a `pre-training' method to obtain a set of network parameters that has empirically been shown to be close to a good solution, followed by further optimisation using a regularised variant of Newton's method, where the level of regularisation is determined and adjusted automatically; the latter uses second-order derivative information to improve convergence, but without the need to evaluate or store the full Hessian matrix, by using a fast approximate method to calculate Hessian-vector products. This combination of methods allows for the training of complicated networks that are difficult to optimise using standard backpropagation techniques. SkyNet employs convergence criteria that naturally prevent overfitting, and also includes a fast algorithm for estimating the accuracy of network outputs. The utility and flexibility of SkyNet are demonstrated by application to a number of toy problems, and to astronomical problems focusing on the recovery of structure from blurred and noisy images, the identification of gamma-ray bursters, and the compression and denoising of galaxy images. The SkyNet software, which is implemented in standard ANSI C and fully parallelised using MPI, is available at [URL] 19 pages, 21 figures, 7 tables; this version is re-submission to MNRAS in response to referee comments; software available at [URL]...|$|E
40|$|Designing {{object-oriented}} software is hard, and designing reusable {{object-oriented software}} is even harder. This task {{is even more}} daunting for a developer of computational intelligence applications, as optimising one design objective tends to make others inefficient or even impossible. Classic examples in computer science include ‘storage vs. time’ and ‘simplicity vs. flexibility. ’ Neural network requirements are {{by their very nature}} very tightly coupled – a required design change in one area of an existing application tends to have severe effects in other areas, making the change impossible or inefficient. Often this situation leads to a major redesign of the system and in many cases a completely rewritten application. Many commercial and open-source packages do exist, but these cannot always be extended to support input from other fields of computational intelligence due to proprietary reasons or failing to fully take all design requirements into consideration. Design patterns make a science out of writing software that is modular, extensible and efficient as well as easy to read and understand. The essence of a design pattern is to avoid repeatedly solving the same design problem from scratch by reusing a solution that solves the core problem. This pattern or template for the solution has well understood prerequisites, structure, properties, behaviour and consequences. CILib is a framework that allows developers to develop new computational intelligence applications quickly and efficiently. Flexibility, reusability and clear separation between components are maximised through the use of design patterns. Reliability is also ensured as the framework is open source and thus has many people that collaborate to ensure that the framework is well designed and error free. This dissertation discusses the design and implementation of a <b>generic</b> <b>neural</b> <b>network</b> framework that allows users to design, implement and use any possible neural network models and algorithms {{in such a way that}} they can reuse and be reused by any other computational intelligence algorithm in the rest of the framework, or any external applications. This is achieved by using object-oriented design patterns in the design of the framework. Dissertation (MSc) [...] University of Pretoria, 2008. Computer Scienceunrestricte...|$|E
40|$|We propose {{associative}} learning models that integrate spike-time dependent plasticity (STDP) and firing rate in two semi-supervised paradigms, Pavlovian and reinforcement learning. Through the Pavlovian approach, the learning rule associates paired stimuli (stimulus-stimulus) {{known as the}} predictor-choice pair. Synaptic plasticity {{is dependent on the}} timing and the rate of pre- and post synaptic spikes within a time window. The contribution of our learning model {{can be attributed to the}} implementation of the proposed learning rules using integration of STDP and firing rate in spatio-temporal neural networks, with Izhikevich's spiking neurons. There is no such model yet found in the literature. The model has been tested in recognition of real visual images. As a result of learning, synchronisation of activity among inter- and intra-subpopulation neurons demonstrates association between two stimulus groups. As an improvement to the stimulus-stimulus (S-S) association model, we extend the algorithm for stimulus-stimulus- response (S-S-R) association using a reinforcement approach with reward-modulated STDP. In the later model, firing rate in response groups determines a reward signal that modulates synaptic changes derived from STDP processes. The S-S-R model has been successfully tested in a visual recognition task with real images and simulation of the colour word Stroop effect. The learning algorithm is able to perform pair-associate learning as well as to recognise the sequence of the presented stimuli. Unlike other existing gradient-based learning models, the S-S-R model implements temporal sequence learning in more natural way through reward-based learning whose protocol follows a behavioural experiment from a psychology study. The key novelty of our S-S-R model can be ascribed to its lateral inhibition mechanism through a minimal anatomical constraint that enables learning in high competitive environments (e. g. temporal logic AND and XOR problems). The S-S model models for example the retrospective and prospective activity in the brain, whilst the S-S-R model exhibits reward acquisition behaviour in human learning. Furthermore, we have proven than, a goal directed learning can be implemented via a <b>generic</b> <b>neural</b> <b>network</b> with rich realistic dynamics based on neurophysiological data. Hence the loose dependency between the model's anatomical properties and functionalities could offer a wide range of applications especially in complex learning environments. Keywords: spiking neural network, spike timing dependent plasticity, associate learning, reinforcement learning, cognitive modellingEThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|Removing {{perspective}} distortion from hand held camera captured document images {{is one of}} the primitive tasks in document analysis, but unfortunately, no such method exists that can reliably remove the {{perspective distortion}} from document images automatically. In this paper, we propose a convolutional <b>neural</b> <b>network</b> based method for recovering homography from hand-held camera captured documents. Our proposed method works independent of document's underlying content and is trained end-to-end in a fully automatic way. Specifically, this paper makes following three contributions: Firstly, we introduce a large scale synthetic dataset for recovering homography from documents images captured under different geometric and photometric transformations; secondly, we show that a <b>generic</b> convolutional <b>neural</b> <b>network</b> based architecture can be successfully used for regressing the corners positions of documents captured under wild settings; thirdly, we show that L 1 loss can be reliably used for corners regression. Our proposed method gives state-of-the-art performance on the tested datasets, and has potential to {{become an integral part of}} document analysis pipeline. Comment: 10 pages, 8 figure...|$|R
40|$|We propose an {{associative}} learning model using reward mod- ulated spike-time dependent plasticity in reinforcement learning paradigm. The task of learning is to associate a stimulus pair, {{known as the}} predictor− choice pair, to a target response. In our model, a <b>generic</b> architecture of <b>neural</b> <b>network</b> has been used, with minimal assumption about the network dynamics. We demonstrate that stimulus-stimulus-response as- sociation can be implemented in a stochastic way within a noisy setting. The network has rich dynamics resulting from its recurrent connectiv- ity and background activity. The algorithm can learn temporal sequence detection and solve temporal XOR problem...|$|R
40|$|A <b>neural</b> <b>network</b> {{construction}} {{method for}} problems specified for data sets with in- and/or output {{values in the}} continuous or discrete domain is described and evaluated. This approach {{is based on a}} Boolean approximation of the data set and is <b>generic</b> for various <b>neural</b> <b>network</b> architectures. The construction method takes advantage of a construction method for Boolean problems without increasing the dimensions of the in- or output vectors, which is a strong advantage over approaches which work on a binarized version of the data set with an increased number of in- and output elements. Further, the networks are pruned in a second phase in order to obtain very small networks. Keywords: construction of networks, pruning, generalization, optimality criteria, high order perceptrons, backpropagation <b>neural</b> <b>networks.</b> 1 Introduction A major problem in applying of <b>neural</b> <b>networks</b> is the choice of a (minimal) topology [5]: a considerable amount of architectures and methods for the construction o [...] ...|$|R
40|$|Over {{the past}} decade, Deep Neural Networks (DNNs) {{have become very}} popular models for {{processing}} large amounts of data because of their successful application {{in a wide variety}} of fields. These models are layered, often containing parametrized linear and non-linear transformations at each layer in the network. At this point, however, we do not rigorously understand why DNNs are so effective. In this thesis, we explore one way to approach this problem: we develop a generic mathematical framework for representing neural networks, and demonstrate how this framework can be used to represent specific neural network architectures. In chapter 1, we start by exploring mathematical contributions to neural networks. We can rigorously explain some properties of DNNs, but these results fail to fully describe the mechanics of a <b>generic</b> <b>neural</b> <b>network.</b> We also note that most approaches to describing neural networks rely upon breaking down the parameters and inputs into scalars, as opposed to referencing their underlying vector spaces, which adds some awkwardness into their analysis. Our framework strictly operates over these spaces, affording a more natural description of DNNs once the mathematical objects that we use are well-defined and understood. We then develop the generic framework in chapter 3. We are able to describe an algorithm for calculating one step of gradient descent directly over the inner product space in which the parameters are defined. Also, we can represent the error backpropagation step in a concise and compact form. Besides a standard squared loss or cross-entropy loss, we also demonstrate that our framework, including gradient calculation, extends to a more complex loss function involving the first derivative of the network. After developing the generic framework, we apply it to three specific network examples in chapter 4. We start with the Multilayer Perceptron, the simplest type of DNN, and show how to generate a gradient descent step for it. We then represent the Convolutional Neural Network (CNN), which contains more complicated input spaces, parameter spaces, and transformations at each layer. The CNN, however, still fits into the generic framework. The last structure that we consider is the Deep Auto-Encoder, which has parameters that are not completely independent at each layer. We are able to extend the generic framework to handle this case as well. In chapter 5, we use some of the results from the previous chapters to develop a framework for Recurrent Neural Networks (RNNs), the sequence-parsing DNN architecture. The parameters are shared across all layers of the network, and thus we require some additional machinery to describe RNNs. We describe a generic RNN first, and then the specific case of the vanilla RNN. We again compute gradients directly over inner product spaces...|$|E
40|$|The {{development}} of models of dynamical systems behaviour {{is a fundamental}} activity {{in science and engineering}} disciplines. This thesis examines the problem of modelling a class of dynamical systems using neural networks. Existing research reveals that neural network models have been developed for lumped parameter dynamical systems; that is, systems where the variables of interest vary only over the timedomain. However, there are no adequate neural network models for distributed parameter dynamical systems; that is, systems where the variables of interest vary over some other domain, e. g. the spatial domain, in addition to the time domaln. The main goal of this research is to develop a neural network architecture for modelling distributed dynamical systems where one has limited and incomplete knowledge about the underlying behaviour of the system. The result of this research is a <b>generic</b> <b>neural</b> <b>network</b> architecture - the Interacting Neural Network (INN) architecture - that is capable of modelling a wide range of distributed dynamical systems. The fundamental problem associated with distributed systems which the INN architecture addresses is that of scaling. The scaling problem manifests itself when the complexity of a model increases in a manner which is unmanageable as the problem size increases. The INN architecture solves the scaling problem by using the philosophy of interacting subsystems which is a general methodology for managing complexity. The underlying principle of this methodology is to view the system as a combination of many small subsystems and to focus the modelling effort at the subsystem level rather than at the system level. The resulting models are relatively simpler, but when allowed to interact, the complex behaviour of the original system can be retrieved. The capabilities of the INN architecture are investigated by comparing its performance with other architectures on two distributed systems. First, investigations are carried out in modelling non-linear heat flow which serves as a case study to expound the capabilities of the INN architecture. Secondly, the architecture is applied to an aquifer problem to illustrate its capabilities on modelling practical problems. It is shown that the INN architecture captures the underlying behaviour of both systems, and more significantly, that the trained network can generalise spatially, wherein the same trained network can be applied to different instances of a given system. The spatial generalisation capabilities of the INN architecture is a unique and powerful result, which when used appropriately can significantly extend the usefulness of neural network models. Finally, two major factors that affect the generalisation ability of the INN architecture are investigated: (i) the effect of changing the geometry of a given system and (ii) the effect of the amount of training data available. New relationships are deduced for both factors...|$|E
40|$|Since {{the early}} {{development}} of artificial neural networks, researchers {{have tried to}} analyze trained neural networks {{in order to gain}} insight into their behavior. For certain applications and in certain problem domains this has been successful, for example by the development of so-called rule extraction and other methods. For many other problem domains the existing knowledge extraction methods do not offer satisfactory solutions. Another key factor involved in the knowledge extractability is formed by the neural network architecture, as some network architectures (e. g., self-organizing maps) offer more direct insight into the stored knowledge than others (e. g., feedforward nets). This thesis therefore presents a <b>generic</b> <b>neural</b> <b>network</b> analysis method that utilizes domain-specific basic functions that are easy to interpret by the user and that can furthermore be used to optimize neural network systems. In general, the analysis consists in describing the internal functionality of the neural network in terms of domain-specific basic functions, functions that can be considered basic in the application domain of the neural network. This means that users who may not be familiar with artificial neural networks, but who are familiar with basic functions that are often used in their problem domain, can gain insight in the way the neural network solves their problem. For such users, this is often an important factor in deciding to apply artificial neural networks to a problem that may be difficult to solve otherwise. Traditionally, artificial neural network systems are monolithic, single-tier systems. In such systems, the process knowledge is distributed among all elements of the system. This makes it very difficult to extract the contained knowledge. It is much easier to locate knowledge in a system if it is based on a 3 -tier model. Basic domain knowledge is stored in the elementary functions that are found in the bottom tier. It can be easily identified or analyzed. The middle tier basically describes how this knowledge is used in the system, and the application results, that are acquired on the base of the system knowledge, are presented in the top tier. Therefore, it would be beneficial in many aspects if systems based on a single-tier model could be transformed into equivalent systems based on the 3 -tier model. This thesis proposes a generic method that {{can be used as a}} first step to achieve this for monolithic neural network systems. Whereas in general the system knowledge is stored in the neural network in a distributive manner, this thesis shows that it is possible to create a foundation tier on which the whole system is apparently based. The method presented in this thesis breaks the internal system knowledge into identifiable basic foundation blocks. These foundation blocks, or basic functions, depend on the application domain in which the system is operational, and so do the methods to extract those basic functions. However, the domain-dependent methods are all based on a single generic domain-independent idea, namely the analysis of the neural network in terms of (generic) domain-dependent basic functions. This thesis first gives a brief overview of the enormous variety of neural networks and applications that have been developed since the first mathematical model of human nerve cells was described. This is followed by a review of existing methods for the analysis of neural networks. This includes a discussion to what extent these existing methods are successful in retrieving complete and comprehensive knowledge from the network. A study is presented which identifies those application domains for which existing knowledge extraction techniques produce insufficient results. This is then used as a starting point for exploring the suitability of existing and new methods for neural network analysis in those domains. This is then further expanded by outlining the new theory of the analysis of trained neural networks in terms of domain-dependent basic functions, the main topic in this thesis. Based on this, suggestions for basic functions are given for a range of application domains. Two of these suggestions are then worked out in more detail and applied to some typical applications in the respective problem areas. One example application that is worked out is edge detection, from the digital image processing domain. The stored domain knowledge is translated into sets of gradient filters, which are commonly used in image processing. Another one is a feedforward network for classification of character images. Class knowledge stored in this network is extracted in the form of class prototypes. It is expected that these examples offer good illustrations of the benefits of the new method for neural network analysis presented in this thesis. The method presented in this thesis offers a much wider applicability than existing methods, some of which can actually be defined as specific cases of the generic method that forms the central theme in this thesis...|$|E
40|$|ISBN : 978 - 1 - 4673 - 6239 - 9 International audienceThis paper {{presents}} a methodology for monitoring {{quality of service}} in multimedia networks. The proposal consists {{in the use of}} a simple and <b>generic</b> artificial <b>neural</b> <b>networks</b> (ANN) architecture that enables predicting the video quality. The main purpose is to develop automatic means for generating a numerical score that quantifies objectively the human assessing of video streams. The challenge is to create a video quality measurement tool (VQMT) assessing the video quality directly from the available measurements, by building a nonlinear correlation map between the measurements and the human rating mean opinion score (MOS). Promising results are obtained using the ANN for nonlinear modeling combined with fundamental measurable metrics, namely packet loss rate, peak signal to noise ratio, spatial indexes and temporal indexes. A statistical analysis is provided comparing this solution's performance with data-sets obtained through subjective human rating...|$|R
40|$|Many {{important}} NLP {{problems can}} be posed as dual-sequence or sequence-to-sequence modeling tasks. Recent advances in building end-to-end neural architectures have been highly successful in solving such tasks. In this work we propose a new architecture for dual-sequence modeling {{that is based on}} associative memory. We derive AM-RNNs, a recurrent associative memory (AM) which augments <b>generic</b> recurrent <b>neural</b> <b>networks</b> (RNN). This architecture is extended to the Dual AM-RNN which operates on two AMs at once. Our models achieve very competitive results on textual entailment. A qualitative analysis demonstrates that long range dependencies between source and target-sequence can be bridged effectively using Dual AM-RNNs. However, an initial experiment on auto-encoding reveals that these benefits are not exploited by the system when learning to solve sequence-to-sequence tasks which indicates that additional supervision or regularization is needed. Comment: To appear in RepL 4 NLP at ACL 201...|$|R
40|$|This paper {{presents}} a <b>generic</b> modular <b>neural</b> <b>network</b> architecture built up of Adaptive Resonance Theory (ART) networks. The network {{has a more}} powerful knowledge representation than its component ART networks {{in that it is}} capable of representing class hierarchies as opposed to simply partitioning the input data set. At the same time, the network retains the main properties of ART networks like fast and stable incremental learning, parallel search and so on. Three different implementations of the generic network (SMART, HART-J and HART-S) are described, each being capable of unsupervised learning of hierarchical clusterings of arbitrary sequences of input patterns. Each hierarchical ART network produces slightly different clusterings due to the differences in the interactions between the component ART modules and the input each module receives. Two examples, produced by computer simulations, illustrate the developed 2 and 3 -level clusterings on a machine learning benchmark dataset. The [...] ...|$|R
40|$|We combine Riemannian {{geometry}} {{with the}} mean field theory of high dimensional chaos {{to study the}} nature of signal propagation in <b>generic,</b> deep <b>neural</b> <b>networks</b> with random weights. Our results reveal an order-to-chaos expressivity phase transition, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth but not width. We prove this generic class of deep random functions cannot be efficiently computed by any shallow network, going beyond prior work restricted {{to the analysis of}} single functions. Moreover, we formalize and quantitatively demonstrate the long conjectured idea that deep networks can disentangle highly curved manifolds in input space into flat manifolds in hidden space. Our theoretical analysis of the expressive power of deep networks broadly applies to arbitrary nonlinearities, and provides a quantitative underpinning for previously abstract notions about the geometry of deep functions. Comment: Fixed equation reference...|$|R
40|$|Recent {{developments}} in neuromorphic hardware engineering make mixed-signal VLSI <b>neural</b> <b>network</b> models promising candidates for neuroscienti&# 64257;c research tools and massively parallel computing devices, especially for tasks which exhaust the computing power of software simulations. Still, like all analog hardware systems, neuromorphic models su&# 64256;er from a constricted con&# 64257;gurability and production-related &# 64258;uctuations of device characteristics. Since also future systems, involving ever-smaller structures, will inevitably exhibit such inhomogeities {{on the unit}} level, self-regulation properties become a crucial requirement for their successful operation. By applying a cortically inspired self-adjusting network architecture, we show that the activity of <b>generic</b> spiking <b>neural</b> <b>networks</b> emulated on a neuromorphic hardware system can be kept within a biologically realistic &# 64257;ring regime and gain a remarkable robustness against transistor-level variations. As a &# 64257;rst approach of this kind in engineering practice, the short-term synaptic depression and facilitation mechanisms implemented within an analog VLSI model of I&F neurons are functionally utilized {{for the purpose of}} network level stabilization. We present experimental data acquired both from the hardware model and from comparative software simulations which prove the applicability of the employed paradigm to neuromorphic VLSI devices...|$|R
40|$|Recurrent <b>Neural</b> <b>Networks</b> (RNNs) {{have been}} a {{prominent}} concept within artificial intelligence. They are inspired by Biological <b>Neural</b> <b>Networks</b> (BNNs) and provide an intuitive and abstract representation of how BNNs work. Derived from the more <b>generic</b> Artificial <b>Neural</b> <b>Networks</b> (ANNs), the recurrent ones {{are meant to be}} used for temporal tasks, such as speech recognition, because they are capable of memorizing historic input. However, such networks are very time consuming to train {{as a result of their}} inherent nature. Recently, Echo State Networks and Liquid State Machines have been proposed as possible RNN alternatives, under the name of Reservoir Computing (RC). RCs are far more easy to train. In this paper, Cellular Automata are used as reservoir, and are tested on the 5 -bit memory task (a well known benchmark within the RC community). The work herein provides a method of mapping binary inputs from the task onto the automata, and a recurrent architecture for handling the sequential aspects of it. Furthermore, a layered (deep) reservoir architecture is proposed. Performances are compared towards earlier work, in addition to its single-layer version. Results show that the single CA reservoir system yields similar results to state-of-the-art work. The system comprised of two layered reservoirs do show a noticeable improvement compared to a single CA reservoir. This indicates potential for further research and provides valuable insight on how to design CA reservoir systems...|$|R
40|$|Multiple <b>neural</b> <b>network</b> {{systems have}} been {{demonstrated}} to improve performance on tasks such as classification and regression when compared to single <b>neural</b> <b>networks.</b> Whilst there has been significant focus on understanding the theoretical properties of certain types of these multi-net systems, such as ensembles, {{there has been little}} theoretical work on understanding the properties of the generic combination of networks. In this paper we provide an abstract, formal framework in which the <b>generic</b> combination of <b>neural</b> <b>networks</b> can be described, and in which the properties of the system can be rigorously analyzed. We achieve this by describing multi-net systems in terms of partially ordered sets and state transition systems. By way of example, we explore an abstract version of backpropagation applied to a generic multi-net system that can combine an arbitrary number of networks in sequence and in parallel. By using the framework we show with a constructive proof that if it is possible to train the generic system, then training can be achieved by an abstract version of backpropagation...|$|R
40|$|Gaze {{estimation}} methods {{play an important}} role in a gaze tracking system. A novel 2 D gaze estimation method based on the pupil-glint vector is proposed in this paper. First, the circular ring rays location (CRRL) method and Gaussian fitting are utilized for pupil and glint detection, respectively. Then the pupil-glint vector is calculated through subtraction of pupil and glint center fitting. Second, a mapping function is established according to the corresponding relationship between pupil-glint vectors and actual gaze calibration points. In order to solve the mapping function, an improved artificial <b>neural</b> <b>network</b> (DLSR-ANN) based on direct least squares regression is proposed. When the mapping function is determined, gaze estimation can be actualized through calculating gaze point coordinates. Finally, error compensation is implemented to further enhance accuracy of gaze estimation. The proposed method can achieve a corresponding accuracy of 1. 29 °, 0. 89 °, 0. 52 °, and 0. 39 ° when a model with four, six, nine, or 16 calibration markers is utilized for calibration, respectively. Considering error compensation, gaze estimation accuracy can reach 0. 36 °. The experimental results show that gaze estimation accuracy of the proposed method in this paper is better than that of linear regression (direct least squares regression) and nonlinear regression (<b>generic</b> artificial <b>neural</b> <b>network).</b> The proposed method contributes to enhancing the total accuracy of a gaze tracking system...|$|R
40|$|The {{ground state}} {{potential}} energy and dipole moment surfaces for CS 2 have been determined at the CASPT 2 /C:cc-pVTZ,S:aug-cc-pV(T+d) Z level of theory. The potential energy surface has been fit to a sum-of-products form using the <b>neural</b> <b>network</b> method with exponential neurons. A <b>generic</b> interface between <b>neural</b> <b>network</b> potential energy surface fitting and the Heidelberg MCTDH software package is demonstrated. The potential energy surface {{has also been}} fit using the potfit procedure in MCTDH. For fits to the low-energy regions of the potential, the <b>neural</b> <b>network</b> method requires fewer parameters than potfit to achieve high accuracy; global fits are comparable between the two methods. Using these potential energy surfaces, the vibrational energies have been computed for the four most abundant CS 2 isotopomers. These results are compared to experimental and previous theoretical data. The current potential energy surfaces are shown to accurately reproduce the low-lying vibrational energies within a few wavenumbers. Hence, the potential energy and dipole moments surfaces will be useful for future study on the control of quantum dynamics in CS 2. 9 2012 American Chemical Society. Peer reviewed: YesNRC publication: Ye...|$|R
