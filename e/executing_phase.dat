14|212|Public
2500|$|Although {{the present}} system was planned in the 1970s and the 1980s, the project was taken up for {{implementation}} by the Government of India, Ministry of Railways in 1983–84 at an estimated cost of [...] 534.6 million, it took nearly a decade for the first phase to construct and begin operation. Construction began in earnest in 1991. After many delays, the first phase was operational from Beach up to Chepauk in 1995 as the first elevated railway line in the country, and it was extended to Thirumyilai station in Mylapore in 1997. When the first phase between Chennai Beach and Thirumyilai opened in 1997, the project cost [...] 2,690 million, compared to the initial estimate of [...] 550 million in 1984. The patronage of the first phase {{turned out to be}} way lesser than the over-reckoned value of 603,000 passengers per day. In 1998, the Railway Board accorded sanction of <b>executing</b> <b>Phase</b> II of the project from Thirumayilai to Velachery.|$|E
50|$|All {{three games}} {{share the same}} concept; turns are divided into a {{planning}} and an <b>executing</b> <b>phase.</b> While the planning phase can, in single player mode, {{last as long as}} the player needs to give orders to all their units, the <b>executing</b> <b>phase</b> always lasts 60 seconds of real-time. Both sides, either computer or another human, enter their orders before the execution phase takes place. This is known as the Wego system. During the execution phase, units carry out their orders, but the player cannot influence the result and is limited to watch, replay and move the camera. All games offer to play individual battles (ranging from 15 to 60 turns, or 120 turns in Barbarossa to Berlin and Afrika Korps) or operations, linking a series of battles. See Scenarios below for more information.|$|E
5000|$|Battle Dex {{employs a}} [...] "Wego" [...] {{simultaneous}} execution system where turns {{are divided into}} a planning and <b>executing</b> <b>phase.</b> While the planning phase can, in single player mode, {{last as long as}} the player needs to give orders to all their units, turns in multiplayer games are usually limited to two minutes. Both sides, either computer or another human, enter their orders during the planning phase. During the execution phase, units carry out their orders, but the player cannot influence the result and is limited to watch, replay and move the camera.|$|E
2500|$|When {{the user}} clicks the [...] "Install" [...] button {{in a typical}} MSI {{installation}} wizard, installation proceeds to the <b>Execute</b> <b>phase,</b> in which software components are actually installed. The <b>Execute</b> <b>phase</b> makes system changes, {{but it does not}} display any user interface elements.|$|R
30|$|At the <b>execute</b> <b>phase</b> {{we have a}} Firewall rule {{application}} engine component, {{which is}} responsible for effecting the new firewall rules on the servers. This component must take in consideration mechanisms for guaranteeing secure communication with each server, and configuration management techniques.|$|R
50|$|Step 4 of the Instruction Cycle is the Execute Cycle. Here, the {{function}} of the instruction is performed. If the instruction involves arithmetic or logic, the Arithmetic Logic Unit is utilized. This is the only stage of the instruction cycle that is useful {{from the perspective of the}} end user. Everything else is overhead required to make the <b>execute</b> <b>phase</b> happen.|$|R
5000|$|Although {{the present}} system was planned in the 1970s and the 1980s and the project was taken up for {{implementation}} by the Government of India, Ministry of Railways in 1983-84 at an estimated cost of [...] 534.6 million, it took nearly a decade for the first phase to construct and begin operation. Construction began in earnest in 1991. After many delays, the first phase was operational from Beach up to Chepauk in 1995 as the first elevated railway line in the country, and it was extended to Thirumyilai station in Mylapore in 1997. When the first phase between Chennai Beach and Thirumyilai opened in 1997, the project cost [...] 2,690 million, compared to the initial estimate of [...] 550 million in 1984. The patronage of the first phase {{turned out to be}} way lesser than the over-reckoned value of 603,000 passengers per day. In 1998, the Railway Board accorded sanction of <b>executing</b> <b>Phase</b> II of the project from Thirumayilai to Velachery.|$|E
30|$|After {{training}} a TNFN, the <b>executing</b> <b>phase</b> {{of the proposed}} image alignment system merely consists of computing the WGOH descriptor and then feeding it into the DMELA-trained TNFN to get a scaling factor s, a rotation angle θ, and translation parameters (Δx, Δy). About this, the proposed system is simple and efficient.|$|E
40|$|Abstract — Armed forces conduct {{operations}} within operational environments {{characterized by}} complexity, uncertainty, and continuous change. Military planners use planning and decision-making processes {{to cope with}} this confusion. This paper asserts that with changing operational environment {{there is a need to}} separate the planning in advance and planning in crises and <b>executing</b> <b>phase.</b> Our study states that the analytical process of decision-making does not provide the necessary means to respond effectively to crises response planning and decision-making and offers the implementation of intuitive process with some modifications...|$|E
50|$|Pipelining, {{in which}} {{different}} hardware in the CPU <b>executes</b> different <b>phases</b> of multiple instructions simultaneously.|$|R
40|$|While {{developing}} {{a large scale}} distributed system aimed at automating supply chain distribution facility the author came across the problem of enabling sharing unique equipment between two simultaneously <b>executed</b> <b>phases</b> of the project [...] test and production. This article summarises the experience gained by the author {{in the area of}} integration heterogeneous distributed large-scale systems being constrained by time and some limitations of the systems to be integrated such as predefined synchronous model of communication...|$|R
50|$|The {{construction}} of the new terminal, as well as runway expansion {{marked the end of}} Phase I in the project. AAI officials have announced that they are prepared to <b>execute</b> <b>Phase</b> 2 of the Kolkata Airport expansion plan. This primarily involves around the {{construction of}} an 157-meter ATC Tower to provide controllers with a better view of the planes at the new terminal. The building will be accompanied by a sprawling 4-storey office complex.|$|R
40|$|Dual-task {{paradigm}} {{was used}} to examine effects of task features (input, output and central processing) on integrated or separated operations. Results indicated that: (1) Performance of single task could be predicted by the principle of S-C-R compatibility, while performance of dual-task be predicted by the multiple resources theory of attention in tracking and spatial memory tasks. (2) Integration of the <b>executing</b> <b>phase</b> of the dual-task had little interference on performance compared to the condition in which the two tasks were executed separately. Possible mechanisms of the findings and effects of handedness on performance were discussed. IUPsy...|$|E
40|$|Different {{organizations}} have different backgrounds {{of culture and}} knowledge which might lead to potential conflicts in the modern interorganizational workflow. Another fact in this research area is that each organization wants to keep some parts of its local workflow secret to other organizations. Considering the above features, this thesis proposes an approach of modeling the self-centered interorganizational workflow based on previous research of loosely coupled interorganizational workflow. In the self-centered interorganizational workflow model, each organization has its own view of modeling the workflow based on its local cultural and knowledge background instead of sharing a pre-defined common global workflow with other organizations. Further, we design the multiagent cooperative negotiation mechanisms for the self-centered interorganizational workflow model both in the workflow modeling phase and in the workflow <b>executing</b> <b>phase.</b> Finally, {{the evaluation of the}} multiagent negotiation mechanisms will be discussed and analyzed...|$|E
40|$|According {{to formal}} project {{management}} methodology, projects go through three key phases. Out of the three, the <b>executing</b> <b>phase</b> occupies the largest {{portion of the}} project life span in {{which most of the}} work needed to achieve the objectives of the project is actually done. In software projects, executing is the software engineering process. Several methodologies of engineering processes have been established, each having a set of advantages and disadvan-tages based on factors such as the size of the project, complexity level, team competence, etc. The single focus these methodologies often have – such as traditional or agile, present a major challenge for software project managers. This paper proposes a framework that enables the project manager to harness the best of known engineering processes in an agile, but dis-ciplined, manner. The framework provides an effective balance between the need for certainty and the need for agility in software project management...|$|E
50|$|HEFT <b>executes</b> in two <b>phases.</b>|$|R
30|$|As {{previously}} mentioned, we {{employ the}} Puppet configuration management language for describing servers’ configurations. Puppet provides tools for applying configurations, and for obtaining {{the current status}} of a host. A Puppet agent component runs on each host, and reports to (and receive commands from) the puppet-master component, which stores servers description into the Puppet catalog. Hence, Puppet agents fulfill the roles of sensor and effector of servers, while the Puppet master is responsible for the monitor and <b>execute</b> <b>phases</b> of the MAPE-K.|$|R
40|$|Introduction The {{most natural}} way of {{programming}} parallel machines is data-parallelism where programs <b>execute</b> <b>phases</b> of computations and communications on {{different sets of}} data and no overlap exists between communications and computations. Moreover, communication phases are synchronous, i. e. every processor <b>executes</b> these <b>phases</b> {{at the same time}} and waits until the last processor completes his communication phase. From the perspective of program correctness, these data-parallel programs are much more easier to prove than asynchronous CSP-based parallel programs [Hoa 85]. Unfortunately, performances of such programs are bounded by the communications cost. To avoid this problem, the solution is to overlap communications by computations. This is not always possible because of the dependences within the code. If data dependences prevent the use of simple overlap, a solution consists in using a pipelined data parallel algorithm by decreasing the grain of computations, and overlapp...|$|R
40|$|Abstract Title Cashews by SMS – an {{implementation}} in Mozambique Problem Innovation {{is described by}} Tidd, Bessant and Pavitt (2005) as the core process within organisations associated with renewal and as generic activity associated with survival and growth. Yet many organisations fail to realise the benefits of adopting an innovation. Which the theory will show this is most likely due to a problem with one certain phase in the innovation process: the implementation. Purpose The purpose with this academic paper is by a practical example illustrate the risks and problems one can come across in an implementation {{and the consequences of}} this. We also intend to give suggestion on how it is possible to restart an implementation process when the process once has failed. Research questions Why has marketAlerts failed to be implemented in Mozambique? How should IPEX resume the implementation of marketAlerts? Methodology Ethnographical approach. Conclusion Our conclusion is that the Institute for Export Promotion (IPEX) has managed to adopt marketAlerts but has failed to implement it in their daily work mainly {{due to the fact that}} they only completed the acquiring phase. The failure is due to a combination of hierarchy, lack of interest and absents of routines for sending marketAlerts. In order for IPEX to make the best use of marketAlerts we believe that they have to go back and start from the <b>executing</b> <b>phase</b> and implement the service once again...|$|E
40|$|A Project Management Office (PMO) {{is a group}} {{within a}} {{business}} that outlines and preserves standards for project management within an organization. It strives to regulate and introduce economies of repetition when executing projects. It is documentation, guidance and metrics {{that are used in}} project management. In this paper, a PMO for the Microsoft Retail enterprise is built. It is a virtual PMO that aims at being supportive, controlling and directive towards project management in this recently added area of Microsoft. Since Microsoft is so new to the retail industry, started in 2009, {{there are a lot of}} ways that projects can be improved and standardized therefore a great opportunity to create a PMO in order to help improve project management. In this PMO, there are three phases: establishing, implementing and executing. The establishing phase is about the PMO justification, the implementing phase involves defining the PMO in different areas of the business and the <b>executing</b> <b>phase</b> defines the phases of deploying the PMO which turns into the operations side of the business. This last phase is going to be ongoing. The tools recommended for the planning phase are: scope definition, work breakdown structure, responsibility matrix, Gantt chart, project charter, cost estimate, risk response plan, critical chain diagram and Monte Carlo analysis. The tools recommended for the execution phase are: progress report: Gantt update, cost estimate update, risk evaluation update, earned value analysis and schedule crashing. For the close out phase the recommended tools are the following: postmortem, Gantt update, cost estimate update, risk evaluation update, progress report and earned value analysis. Having the right tools helps keep the schedule on time, the cost within budget and a good performance, it is crucial that the Project Manager ensures that everything is being addressed and used in the right way...|$|E
40|$|In Large Transportation Projects (LTPs) {{estimation}} of duration provided in tender and initiating phase are often contradicted in <b>executing</b> <b>phase,</b> {{due to a}} mix of technical and contractual problems arising during the project execution. This paper present a research that apply a statistic approach to this problem and, determining the key features of a LTP {{and a set of}} hi storic data coming from the railway field, present a quantitative solution. The first part of the paper makes an analysis of existing literature on this topic, then the methodology is discussed. In the second part, a statistical model is presented, to show how the set of identified relevant variables are used to estimate the duration according to project features avoiding other goals such as political goals of cutting the project duration. In fact, the common sense of technical advisers is that the duration of a project is often given by non -technical goals, but the drivers are more referred to a specific deadline such as elections or large public events (World Cups, Universal Exposition etc.), asking to cut and cut time and, as a result, projects become more expensive, hard to manage, and face technical problems in the operating phase. In the third part, this method is calibrated on a set of input coming from real railway LTPs of the last ten years. The aim {{of this paper is to}} let the decision makers know that, given a new project, the past experience says that it will have a given duration according to its features, and to ask for a shorter project duration would be risky and cause project failure, or let the project cost rise. It also allow to define a guideline to determine which contractual form is the best according to t he project...|$|E
5000|$|In September 1983, Chamberlain and Burchfield <b>executed</b> <b>Phase</b> V, [...] "Primary Light Documentation,” {{requiring}} 65 designated participants (plus backup) obtained 13 {{vehicles and}} a 30,000-watt generator, towed by a 40-foot flatbed truck. They moved this entire caravan down Laguna Canyon Road from 6 PM to 6 AM, while escorted by three different police agencies and Caltrans officials. The resulting images were printed onto a single print 3.5 inches wide by 516 feet long, depicting {{the entire length}} of the Northeast side of the road in kaleidoscopic color.|$|R
40|$|Many {{scientific}} applications {{can benefit}} from pipelining computation and communication. Our aim is to provide compiler and runtime support for High Performance Fortran applications that could benefit from these techniques. This paper describes the integration of a library for pipelined computations in the runtime system. Results on some application kernels are given. 1 Introduction With the introduction of High Performance Fortran (HPF) [KLS + 94], {{it is possible to}} use the data parallel programming paradigm in a very convenient way for scientific applications. With current compilation technology, these programs will <b>execute</b> <b>phases</b> of computations and communications on differents sets of data and no overlap exists between communications and computations. Moreover, communication phases are synchronous, i. e. each processor <b>executes</b> these <b>phases</b> {{at the same time and}} waits until the last processor completes his communication phase. An important task of the HPF compiler is to detect th [...] ...|$|R
40|$|Metamodel {{approaches}} to building visual environments are becoming {{common in the}} field of domain specific visual languages, mainly focusing on the definition of visual editors and of simulation environments. Recent efforts tackle the generation of complex interaction management both in the editing and in the <b>executing</b> <b>phases.</b> We present an approach to interaction specification which takes into account metamodel information both on the objects that can be manipulated and on the spatial relations among them. Interaction dynamics are defined through a visual, declarative and formal notation based on graph grammar...|$|R
40|$|The {{research}} is fore grounded by writer’s concern {{on the quality}} of sportivity spirit in Indonesia which is regarded to be very low. This is resulted from some factors, one of which is the failure of model implementation of physics instruction from elementary level to university level. One of the problems arises in education especially physical education teaching and learning in elementary school is the low quality of physical instruction in terms of process and result. In terms of process, it is found that teacher’s instruction toward the students develops less sportivity values; mean while in terms of result, student’s mastery on basic motor ability is still poor. Physic’s instructional learning in elementary level tends to formulate basic motor ability yet ignore the formulation of sportivity values which is also a part of the goal. The {{research is}} aimed at developing physical instructional model with sportivity value based system for students in elementary school as one alternative of effective and meaningful instruction. Some effects of this research were also considered to be beneficial in terms that it also revealed the weaknesses and strengths of physical teaching and learning models that have been applied by physics teachers of elementary schools in Sumedang District throughout years. The research applies R&D approach with pre-survey analysis on 6 th graders resulted in the systemic model of learning started from planning, implementing, and evaluating the result. In the planning phase, there are pre activity, main activity and close activity; in implementing / <b>executing</b> <b>phase,</b> sportivity values are inserted in every step. Evaluating phase included the process and result of the teaching and learning. The result shows that this model gives positive impact on students’ scores and significant influence on students’ mastery toward the subject. Having gained the results, it is recommended to physical education teachers, principal, Education Board and Education Institutions that the result is significantly distributed and discussed through Elementary School Teachers Training continuously and periodically...|$|E
40|$|Absract: Information {{logistics}} is {{a question}} about moving information between different individuals or systems to that place {{in time and space}} where a demand for the information arise. It is utilized trough different sources of bearer that is distributed trough different channels in an optimal way. How to make it without distorting the information in the interface between different participants? This report is exactly about that. The report has resulted in the Information Quality Model (IQM) and the Information Quality Process (IQP). IQM is a model for evaluation of information that explains which characteristics that affects the efficiency of information. IQM is built on Aspects - Characteristics – Questions. If we for example have right information and we can deliver to right place in right time but with a poor layout the efficiency still can be low. This leads to that we can state that all the stated aspects with its belonging characteristics must be paid attention to get a good quality in the in the interfaces. IQP Is a process showing how to handle and build up processes for collecting and processing of information with a method that will secure the quality. The method is built on an analyze phase where frames are created first for what should be collected and how it should be done and an <b>executing</b> <b>phase</b> where information is filled in, in accordance to to the rules that was set up in the analyze phase. IQM and IQP form a solution that together illuminates the whole process of the information delivery in a unique way. The research method is mainly based on cases built on the authors empirical experiences. Sammanfattning: Informationslogistik handlar om att flytta information mellan olika individer eller system till den plats i tid och rum som informationsbehovet uppstår. Detta sker genom att man utnyttjar olika former av bärare som distribueras via olika kanaler på ett optimalt sätt. Hur gör man detta utan att information förvanskas i gränssnitten mellan olika aktörer? Just detta handlar detta X-jobb om. X-jobbet har resulterat i The Information Quality Model (IQM) och The Information Quality Process (IQP). IQM är en informationsvärderingsmodell som förklarar vilka egenskaper som påverkar verkningsgraden hos information. IQM bygger på Aspekter – Egenskaper – Frågor. Om exempelvis vi har rätt information och vi kan leverera till rätt plats i rätt tid men i en usel layout så kan verkningsgraden bli låg. Detta leder till att vi kan konstatera att alla redovisade aspekter med tillhörande egenskaper måste beaktas för att få en bra kvalitet i gränssnitten. IQP är en process som visar hur man hanterar och bygger upp processer för insamling och bearbetning av information med en kvalitetssäkrande metod. Metoden bygger på en analysfas där man skapar ramar för vad som skall samlas in och hur det skall ske samt en utförande fas där man fyller på med information uppställd enligt de regler man satt i analysfasen. IQM och IQP bildar tillsammans en helhet som belyser hela processen av informationsöverlämning på ett unikt sätt. Forskningsmetoden är i huvudsak case baserad och bygger på författarnas empiriska erfarenheter...|$|E
40|$|This paper {{describes}} the features {{and implementation of}} our automatic data distribution research tool. The tool (DDT) accepts programs written in Fortran 77 and generates HPF directives and executable statements. DDT works by identifying a set of computational phases (procedures and loops). The algorithm builds a search space of candidate solutions for these phases which is explored looking for their combination that minimize the overall cost; this cost includes movement cost and computation cost. The data movement cost includes the cost of <b>executing</b> each <b>phase</b> with a given mapping and the remapping costs {{that have to be}} paid in order to <b>execute</b> each <b>phase</b> with the mapping selected. The computation cost includes the cost of <b>executing</b> each <b>phase</b> in parallel according to the mapping selected and the owner computes rule. Control flow information is used to identify how phases are sequenced during the execution of the application. 1 Introduction Data distribution is one of the topics of [...] ...|$|R
50|$|As explained, the {{possibility}} to <b>execute</b> different <b>phases</b> of the implementation process iteratively enables the process to be executed by incrementally aligning the product to be implemented with the end-user (organization).|$|R
40|$|AbstractThis paper {{presents}} {{results of}} {{an analysis of the}} most relevant urban guided transportation projects of the last decades, to understand root causes of failures and relevant variances from previously estimated parameters. We collected information from all side of the project – history of the transportation network, social environment, technical choices – reading documentation and interviewing stakeholders. Major problems emerging during the <b>executing</b> <b>phases</b> turn out to be managerial and referable to project management areas. So we addressed to the modern theory of project management based on complexity, which seemed to be fit for urban transportation projects...|$|R
40|$|The end of Dennard scaling is {{expected}} to shrink the range of DVFS in future nodes, limiting the energy savings of this technique. This paper evaluates how much we can increase the effectiveness of DVFS by using a software decoupled access-execute approach. Decoupling the data access from execution allows us to apply optimal voltage-frequency selection for each phase and therefore improve energy efficiency over standard coupled execution. The underlying insight of our work is that by decoupling access and execute we {{can take advantage of}} the memory-bound nature of the access phase and the compute-bound nature of the <b>execute</b> <b>phase</b> to optimize power efficiency, while maintaining good performance. To demonstrate this we built a task based parallel execution infrastructure consisting of: (1) a runtime system to orchestrate the execution, (2) power models to predict optimal voltage-frequency selection at runtime, (3) a modeling infrastructure based on hardware measurements to simulate zero-latency, per-core DVFS, and (4) a hardware measurement infrastructure to verify our model's accuracy. Based on real hardware measurements we project that the combination of decoupled access-execute and DVFS has the potential to improve EDP by 25 % without hurting performance. On memory-bound applications we significantly improve performance due to increased MLP in the access phase and ILP in the <b>execute</b> <b>phase.</b> Furthermore we demonstrate that our method can achieve high performance both in presence or absence of a hardware prefetcher. LPGPU FP 7 -ICT- 288653 UPMAR...|$|R
40|$|Energy-efficiency plays a {{significant}} role given the battery lifetime constraints in embedded systems and hand-held devices. In this work we target the ARM big. LITTLE, a heterogeneous platform that is dominant in the mobile and embedded market, which allows code to run transparently on different microarchitectures with individual energy and performance characteristics. It allows to se more energy efficient cores to conserve power during simple tasks and idle times and switch over to faster, more power hungry cores when performance is needed. This proposal explores the power-savings and the performance gains that can be achieved by utilizing the ARM big. LITTLE core in combination with Decoupled Access-Execute (DAE). DAE is a compiler technique that splits code regions into two distinct phases: a memory-bound Access phase and a compute-bound <b>Execute</b> <b>phase.</b> By scheduling the memory-bound phase on the LITTLE core, and the compute-bound phase on the big core, we conserve energy while caching data from main memory and perform computations at maximum performance. Our preliminary findings show that applying DAE on ARM big. LITTLE has potential. By prefetching data in Access we can achieve an IPC improvement of up to 37 % in the <b>Execute</b> <b>phase,</b> and manage to shift {{more than half of the}} program runtime to the LITTLE core. We also provide insight into advantages and disadvantages of our approach, present preliminary results and discuss potential solutions to overcome locking overhead. Comment: Presented at HIP 3 ES, 201...|$|R
40|$|This work {{demonstrates}} {{the potential of}} hardware and software optimization to improve theeffectiveness of dynamic voltage and frequency scaling (DVFS). For software, we decouple data prefetch (access) and computation (execute) to enable optimal DVFS selectionfor each phase. For hardware, we use measurements from state-of-the-art multicore processors to accurately model the potential of per-core, zero-latency DVFS. We demonstrate that the combinationof decoupled access-execute and precise DVFS {{has the potential to}} decrease EDP by 25 - 30 % without reducing performance. The underlying insight in this work is that by decoupling access and execute we can take advantageof the memory-bound nature of the access phase and the compute-bound nature of the <b>execute</b> <b>phase</b> to optimize power efficiency. For the memory-bound access phase, where we prefetch data into the cachefrom main memory, we can run at a reduced frequency and voltage without hurting performance. Thereafter, the <b>execute</b> <b>phase</b> can run much faster, thanks to the prefetching of the access phase, and achieve higher performance. This decoupled program behavior allows us to achieve more effective use of DVFS than standard coupled executions which mix data access and compute. To understand the potential of this approach, we measure application performance and power consumption on a modern multicore system across a range of frequencies and voltages. From this data we build a model that allows us to analyze the effects of per-core, zero-latency DVFS. The results of this work demonstrate the significant potential for finer-grain DVFS in combination with DVFS-optimized software...|$|R
2500|$|Gas {{in there}} and <b>execute</b> the {{following}} <b>phases</b> of the project have been considered: ...|$|R
40|$|ABB Combustion Engineering, Inc. is one {{of three}} {{contractors}} <b>executing</b> <b>Phases</b> 1, 2 and 3 of the Department of Energy project entitled Engineering Development of Advanced Coal-Fired Low-Emission Boiler Systems (LEBS). Phase 1 has been completed and Phase 2 is scheduled for completion on September 30, 1996. The following major activities are being carried out in parallel in Phase 2 and this paper is a status report on this work: (1) in-furnace NOx reduction; (2) catalytic filter optimization; (3) add Kalina cycle to POCTF; and (4) POCTF design and licensing. The in-furnace NOx reduction work has been completed and, therefore, a description of this work comprises the major part of this paper...|$|R
40|$|Abstract Multi-processor {{system-on-chip}} (MPSoC) simulators {{are many}} {{orders of magnitude}} slower than the hardware they simulate due to increasing architectural com-plexity. In this paper, we propose a new application sampling technique to accelerate the simulation of MPSoC design space exploration (DSE). The proposed technique dy-namically combines simultaneously <b>executed</b> <b>phases,</b> thus generating a sampling unit. This technique accelerates the simulation by allowing the repeated combinations of parallel phases to be skipped. A complementary technique, called cluster synthesis, is also proposed to improve the simulation acceleration {{when the number of}} possible phase combinations increases. Our experimental results show that this technique can accelerate the simulation up to a factor of 800 with a relatively small estimation error. ...|$|R
50|$|Like the Hohmann {{transfer}}, both transfer orbits {{used in the}} bi-elliptic transfer constitute {{exactly one}} half of an elliptic orbit. This means that {{the time required to}} <b>execute</b> each <b>phase</b> of the transfer is half the orbital period of each transfer ellipse.|$|R
