74|143|Public
5|$|Two-page book embeddings with a <b>fixed</b> <b>partition</b> of {{the edges}} into pages can be {{interpreted}} as a form of clustered planarity, in which the given graph must be drawn {{in such a way that}} parts of the graph (the two subsets of edges) are placed in the drawing in a way that reflects their clustering. Two-page book embedding has also been used to find simultaneous embeddings of graphs, in which two graphs are given on the same vertex set and one must find a placement for the vertices in which both graphs are drawn planarly with straight edges.|$|E
25|$|The {{conversion}} {{was handled}} via the United States foreign military sales program, {{which in turn}} contracted McDonnell Douglas. Costs for the conversion were initially estimated at $89.5million (FY 1994). The aircraft was to be equipped with both a boom and a probe and drogue system. However, because McDonnell Douglas {{did not have any}} experience with the requested Remote Aerial Refueling Operator (RARO) system, and because the third aircraft differed from the original two, the program could not be completed at budget. By omitting the probe and drogue system and a <b>fixed</b> <b>partition</b> wall between the cargo and passenger, the cost could be limited at $96million. To make up for the cost increase McDonnell Douglas hired Dutch companies to do part of the work. The actual converting of the aircraft was done by KLM. Conversion of the aircraft was done from October 1994 to September 1995 for the first aircraft and from February to December 1995 for the second. This was much longer than planned, mostly because McDonnell Douglas delivered the parts late. This would have again increased the cost, but in the contract for the AH-64 Apaches which the Royal Netherlands Air Force also bought from McDonnell Douglas, the price was agreed to be kept at $96million.|$|E
5000|$|For a <b>fixed</b> <b>partition</b> [...] of S, random {{variables}} [...] form a multinomial distribution with event probabilities ...|$|E
40|$|In this paper, we derive {{an upper}} bound on the (n − 1) -star {{reliability}} in an Sn using the probability fault model. Approximate (n − 1) -star reliability results are also obtained using the <b>fixed</b> <b>partitioning.</b> The numerical {{results show that the}} (n − 1) -star reliabilities under the probability fault model and the <b>fixed</b> <b>partitioning</b> are in good agreement especially for the low value of the node reliability. The numerical results are also shown to be consistent with and close to the simulation results. Conservative comparisons are made where possible between the reliability of similar size star graphs and hypercubes...|$|R
50|$|Despite its {{simplicity}} executive was, for the time, quite powerful, allocating memory to programs as needed (rather than the <b>fixed</b> <b>partitions</b> provided by OS/360). This was possible because the FP6000 design contained hardware to aid multi-programming, datum and limit registers which made programs address independent and avoided one program accessing the memory allocated to another.|$|R
40|$|Recent work by Nesterov and Stich {{showed that}} {{momentum}} {{can be used}} to accelerate the rate of convergence for block Gauss-Seidel in the setting where a <b>fixed</b> <b>partitioning</b> of the coordinates is chosen ahead of time. We show that this setting is too restrictive, constructing instances where breaking locality by running non-accelerated Gauss-Seidel with randomly sampled coordinates substantially outperforms accelerated Gauss-Seidel with any <b>fixed</b> <b>partitioning.</b> Motivated by this finding, we analyze the accelerated block Gauss-Seidel algorithm in the random coordinate sampling setting. Our analysis captures the benefit of acceleration with a new data-dependent parameter which is well behaved when the matrix sub-blocks are well-conditioned. Empirically, we show that accelerated Gauss-Seidel with random coordinate sampling provides speedups for large scale machine learning tasks when compared to non-accelerated Gauss-Seidel and the classical conjugate-gradient algorithm. Comment: Presented at the 34 th International Conference on Machine Learning (ICML 2017...|$|R
50|$|The LMOS {{database}} is {{a proprietary}} file system, designed with 11 access methods (variable index, index, hash tree, <b>fixed</b> <b>partition</b> file, etc.). This is highly tuned {{for the various}} pieces of data used by LMOS.|$|E
50|$|Several {{variants}} {{of the model}} exist. One minor tweak allocates vertices to communities randomly, according to a categorical distribution, {{rather than in a}} <b>fixed</b> <b>partition.</b> More significant variants include the censored block model and the mixed-membership block model.|$|E
50|$|On set S = {x1,x2,...,xn} of {{variables}} {{there is a}} <b>fixed</b> <b>partition</b> A of k classes A1,A2,...,Ak, and player P1 knows every variable, except those in Ai, for i = 1,2,...,k. The players have unlimited computational power, and they communicate {{with the help of}} a blackboard, viewed by all players.|$|E
40|$|The paper studies a {{class of}} tests based on disparities between the real-valued data and {{theoretical}} models resulting either from <b>fixed</b> <b>partitions</b> of the observation space, or from the partitions by the sample quantiles of fixed orders. In both cases there are considered the goodness-of-fit tests of simple and composite hypotheses. All tests are shown to be consistent, and their power is evaluated at the nonlocal {{as well as local}} alternatives...|$|R
50|$|The GECOS-II {{operating}} system {{was developed by}} General Electric for the 36-bit GE-635 in 1962-1964. It bore a close resemblance architecturally to IBSYS on the IBM 7094 and less to DOS/360 on the System/360. However, the GE-635 architecture {{was very different from}} the IBM System/360 and GECOS was more ambitious than DOS/360. GECOS-II supported both time-sharing (TSS) and batch processing, with dynamic allocation of memory (IBM had <b>fixed</b> <b>partitions,</b> at that time), making it a true second-generation {{operating system}}.|$|R
40|$|This paper compares <b>fixed</b> <b>partitioning</b> and salient points {{schemes for}} {{dividing}} an image into patches, {{in combination with}} low-level MPEG- 7 visual descriptors to represent the patches with particular patterns. A clustering technique is applied to construct a compact representation by grouping similar patterns into a cluster codebook. The codebook will then be used to encode the patterns into visual keywords. In order to obtain high-level information about the relational context of an image, a correlogram is constructed from the spatial relations between visual keyword indices in an image. For classifying images a k-nearest neighbors (k-NN) and a support vector machine (SVM) algorithm are used and compared. The techniques are compared to other methods on two well-known datasets, namely Corel and PASCAL. To measure {{the performance of the}} proposed algorithms, average precision, a confusion matrix, and ROC-curves are used. The results show that the cluster correlogram outperforms the cluster histogram. The saliency based scheme performs similarly to the <b>fixed</b> <b>partitioning</b> scheme and the SVM significantly outperforms the k-NN classifier. Finally, we demonstrate the robustness to noise, photometric, and geometric distortions. (C) 2009 Elsevier Ltd. All rights reserved...|$|R
50|$|If {{the system}} is not overloaded, a {{partition}} that is allocated (for example) 10% of the processor bandwidth, can, in fact, use more than 10%, as it will borrow from the spare budget of other partitions (but will be required to pay it back later). This is very useful for the non real-time subsystems that experience variable load, since these subsystems can make use of spare budget from hard real-time partitions in order to make more forward progress than they would in a <b>fixed</b> <b>partition</b> scheduler such as ARINC-653, but without impacting the hard real-time subsystems' deadlines.|$|E
50|$|Two-page book embeddings with a <b>fixed</b> <b>partition</b> of {{the edges}} into pages can be {{interpreted}} as a form of clustered planarity, in which the given graph must be drawn {{in such a way that}} parts of the graph (the two subsets of edges) are placed in the drawing in a way that reflects their clustering. Two-page book embedding has also been used to find simultaneous embeddings of graphs, in which two graphs are given on the same vertex set and one must find a placement for the vertices in which both graphs are drawn planarly with straight edges.|$|E
50|$|PATCO {{operates}} 121 67 ft cars {{which were}} acquired {{in two separate}} orders, labeled PATCO I and PATCO II. The original PATCO I cars were designed and manufactured by Budd of Philadelphia, PA in 1968. Cars numbered 101-125 are single units, and cars numbered 201-250 are in permanently coupled married pairs. The PATCO II cars were delivered in 1980 (in parallel {{with the opening of}} the Woodcrest Park and ride facility) and consisted of married pairs numbered 251-296. The PATCO II cars were manufactured by Vickers Canada under a license from Budd, but are nearly indistinguishable from the PATCO I's, the only differences being that the PATCO II cars have a <b>fixed</b> <b>partition</b> behind the operator's booth and lack a stainless steel shroud below the door line to ease access to traction components.|$|E
50|$|Local page {{replacement}} assumes {{some form}} of memory partitioning that determines how many pages are to be assigned to a given process {{or a group of}} processes. Most popular forms of <b>partitioning</b> are <b>fixed</b> <b>partitioning</b> and balanced set algorithms based on the working set model. The advantage of local page replacement is its scalability: each process can handle its page faults independently, leading to more consistent performance for that process. However global page replacement is more efficient on an overall system basis.|$|R
50|$|Other {{replacements}} include fixed jalousies on {{the sides}} of the hall, new ceilings, <b>fixed</b> cement <b>partitions</b> (collapsible wooden partitions before), and clay and wood tiles.|$|R
40|$|In {{this paper}} we {{describe}} a new method to combine visual MPEG- 7 descriptors with spatial information, {{by the use}} of cluster correlograms. We employ two approaches for dividing up an images for region-based image retrieval and categorization. We compare <b>fixed</b> <b>partitioning</b> and salient points schemes for dividing an image into patches, and low-level MPEG- 7 visual descriptors are used to represent the patches with particular patterns. A clustering technique is applied to construct a compact representation by grouping similar patterns into a cluster codebook. The codebook will then be used to encode th...|$|R
50|$|The Friends Meetinghouse {{is located}} in a rural area of central Casco, {{on the west side of}} Quaker Ridge Road, about 0.75 mi south of Maine State Route 11. It is a modest single-story wood frame structure, with a gable roof and narrow {{clapboard}} siding. A shed-roofed entrance vestibule projects from the left side of the building, its roof extending from roughly the mid-point of the main roof slope. The front facade has single sash windows at the main and attic levels. The interior of the main space is divided roughly in two, with a <b>fixed</b> <b>partition</b> wall that has openable shutters on its upper half; this was so that the sexes would be divided, according to Quaker custom, while allowing communication between the sides to take place. The space is furnished with high-backed bench pews and a small parlor organ.|$|E
50|$|The {{conversion}} {{was handled}} via the United States foreign military sales program, {{which in turn}} contracted McDonnell Douglas. Costs for the conversion were initially estimated at $89.5 million (FY 1994). The aircraft was to be equipped with both a boom and a probe and drogue system. However, because McDonnell Douglas {{did not have any}} experience with the requested Remote Aerial Refueling Operator (RARO) system, and because the third aircraft differed from the original two, the program could not be completed at budget. By omitting the probe and drogue system and a <b>fixed</b> <b>partition</b> wall between the cargo and passenger, the cost could be limited at $96 million. To make up for the cost increase McDonnell Douglas hired Dutch companies to do part of the work. The actual converting of the aircraft was done by KLM. Conversion of the aircraft was done from October 1994 to September 1995 for the first aircraft and from February to December 1995 for the second. This was much longer than planned, mostly because McDonnell Douglas delivered the parts late. This would have again increased the cost, but in the contract for the AH-64 Apaches which the Royal Netherlands Air Force also bought from McDonnell Douglas, the price was agreed to be kept at $96 million.|$|E
40|$|AbstractWe {{consider}} the Inventory-Routing Problem where n geographically dispersed retailers must be supplied by a central facility. The retailers experience {{demand for a}} product at a deterministic rate and incur holding costs for keeping inventory. Distribution is performed by a fleet of capacitated vehicles. The objective is to minimize the average transportation and inventory costs per unit time over the infinite horizon. In this paper, {{we focus on the}} set of <b>fixed</b> <b>partition</b> policies. In a <b>fixed</b> <b>partition</b> policy, the retailers are partitioned into disjoint and collectively exhaustive sets. Each set of retailers is served independently of the others and at its optimal replenishment rate. We derive a deterministic (O(n)) lower bound on the cost of the optimal <b>fixed</b> <b>partition</b> policy. A probabilistic analysis of the performance of this bound demonstrates that it is asymptotically 98. 5 %-effective. That is, as the number of retailers increases, the lower bound is very close to the cost of the optimal <b>fixed</b> <b>partition</b> policy...|$|E
50|$|Due to {{the rules}} {{governing}} memory allocation, more computer memory is sometimes allocated than is needed. For example, memory can only be provided to programs in chunks divisible by 4, 8 or 16, {{and as a result}} if a program requests perhaps 23 bytes, it will actually get a chunk of 32 bytes. When this happens, the excess memory goes to waste. In this scenario, the unusable memory is contained within an allocated region. This arrangement, termed <b>fixed</b> <b>partitions,</b> suffers from inefficient memory use - any process, no matter how small, occupies an entire partition. This waste is called internal fragmentation.|$|R
30|$|Overall, {{the most}} {{commonly}} followed strategy to combine global and local information usually relies on some late fusion method that severely slows down the retrieval process. <b>Fixed</b> <b>partitioning</b> of images and region-based image segmentation are also presented, but when applied, they not only add {{a new level of}} complexity but also tend to suffer in domain-specific tasks, where background information and foreground are not easily dissociated. Our proposed implementation of localized MPEG- 7 descriptors is designed around the fact that CBIR tasks employ a large number of images for indexing and retrieval. Thus, efficiency, low complexity and compactness of the final representation are of great importance.|$|R
40|$|AbstractIt is {{well known}} that the group of all nonsingular lower block-triangular p×p {{matrices}} acts transitively on the cone P∗ of all positive definite p×p matrices. This result has been applied to obtain several major results in multivariate statistical distribution theory and decision theory. Here a converse is established: if a matrix group acts transitively on P∗, then its group algebra must be (similar to) the algebra of all lower block-triangular p×p matrices with respect to a <b>fixed</b> <b>partitioning.</b> This implies the nonexistence of multivariate normal linear statistical models with unrestricted covariance structure that admit a transitive group action, other than those classical models invariant under a Full block-triangular group...|$|R
40|$|International audienceWe {{consider}} the infinite horizon inventory routing {{problem in a}} three-level distribution system with a vendor, a warehouse and multiple geographically dispersed retailers. In this problem, each retailer faces a demand at a deterministic, retailer-specific rate for a single product. The demand of each retailer is replenished either from the vendor through the warehouse or directly from the vendor. Inventories are kept at both the retailers and the warehouse. The objective is to determine a combined transportation (routing) and inventory strategy minimizing a long-run average system-wide cost while meeting the demand of each retailer without shortage. We present a decomposition solution approach based on a <b>fixed</b> <b>partition</b> policy where the retailers are partitioned into disjoint and collectively exhaustive sets and each set of retailers is served on a separate route. Given a <b>fixed</b> <b>partition,</b> the original problem is decomposed into three sub-problems. Efficient algorithms are developed for the sub-problems by exploring important properties of their optimal solutions. A genetic algorithm is proposed to find a near-optimal <b>fixed</b> <b>partition</b> for the problem. Computational results show {{the performance of the}} solution approach. 2010 Elsevier B. V. All rights reserved...|$|E
40|$|Objective: Cerebral {{blood flow}} (CBF) {{estimation}} with C 15 O 2 PET usually assumes a single tissue compartment model and a fixed brain-blood partition coefficient of water. However, the partition coefficient may change in pathological conditions. The {{purpose of this}} study was to investigate the changes of partition coefficient of water in pathological regions and its effect on regional CBF assessment. Methods: Study protocol included 22 patients with occlusive cerebrovascular disease to compare partition coefficients among 3 regions (infarction, non-infarct hypoperfusion, and contralateral) in the pathologic brain (analysis A), and to compare CBF estimated using <b>fixed</b> <b>partition</b> coefficient (CBF fixed) and CBF estimated using floating partition coefficients (CBF float) (analysis B). Results: Partition coefficient in the infarction (0. 55 ± 0. 07 ml/g) was lower than that in contralateral normal cortex (0. 68 ± 0. 05 ml/g), whereas non-infarct hypoperfusion did not show a significant change (0. 67 ± 0. 06 ml/g). As a result, use of a <b>fixed</b> <b>partition</b> coefficient of normal volunteers (0. 70 ml/g) resulted in an underestimation in rCBF by 12 % in infarction area (P< 0. 05), while estimation errors were smaller and induced no significant difference in non-infarct hypoperfusion area or in contralateral areas. Conclusions: Partition coefficient is stable except for the infarction, and CBF estimation using a <b>fixed</b> <b>partition</b> coefficient of normal volunteers provides clinically appreciable information in patients with cerebrovascular disease...|$|E
40|$|In {{this paper}} three models for data {{generated}} by different linear regression distributions with Gaussian errors are discussed: Finite mixture models with random and fixed covariates and a <b>fixed</b> <b>partition</b> model. The models are compared {{with respect to}} the adequacy of their assumptions for various data situations. The interpretation of parameters is discussed. The emphasis is put on the identifiability of the parameters. Identifiability is a necessary condition for the existence of consistent estimators. It turns out that the models treated here cause other identifiability problems than simple Gaussian mixtures. This was ignored up to now and thus there are no satisfying consistency proofs in this area. Counterexamples and sufficient conditions for identifiability are given. The identifiability concept is used for <b>fixed</b> <b>partition</b> models {{for the first time in}} this paper. The concept is generalized to "partial identifiability", i. e. identifiability of only a part of the parameters. 1. INTR [...] ...|$|E
40|$|Abstract. We present new serial and {{parallel}} algorithms for multilevel graph partitioning. Our algorithm has coarsening, partitioning and uncoarsening phases like other multilevel partitioning methods. However, we choose fixed nodes which {{are at least}} a specified distance {{away from each other}} and coarsen them with their neighbor nodes in the coarsening phase using various heuristics. Using this algorithm, it is possible to obtain theoretically and experimentally much more balanced partitions with substantially decreased total edge costs between the partitions than other algorithms. We also developed a parallel method for the <b>fixed</b> centered <b>partitioning</b> algorithm. It is shown that parallel <b>fixed</b> centered <b>partitioning</b> obtains significant speedups compared to the serial case. …...|$|R
40|$|Fully {{utilizing}} {{the power of}} modern heterogeneous systems requires judiciously dividing work across all of the available computational devices. Existing approaches for partitioning work require offline training and generate <b>fixed</b> <b>partitions</b> that fail to respond to fluctuations in device performance that occur at run time. We present a novel dynamic approach to work partitioning that requires no offline training and responds automatically to performance variability to provide consistently good performance. Using six diverse OpenCL TM applications, we demonstrate the effectiveness of our approach in scenarios both with and without run-time performance variability, {{as well as in}} more extreme scenarios in which one device is non-functional. Categories andSubjectDescriptor...|$|R
40|$|Pólya trees <b>fix</b> <b>partitions</b> and use random probabilities {{in order}} to {{construct}} random probability measures. With quantile pyramids we instead fix probabilities and use random partitions. For nonparametric Bayesian inference, there are two candidate likelihood functions, based {{on the need to}} work with a finite set of partitions. Both likelihood functions factorise in precisely the same way as for the quantile pyramid priors. While analytic summaries of posterior distributions are too complicated, updating with Markov chain Monte Carlo methods is quite straightforward. Among special cases of quantile pyramids we have the Dirichlet process. We give conditions securing the existence of an absolute continuous quantile process, and discuss consistency of the sequence of posterior distributions. Illustrations are included...|$|R
40|$|A Binary Space Partition (BSP) is {{a scheme}} for recursively {{dividing}} a configuration of objects by hyperplanes until all objects are separated. Objects {{can be cut}} into fragments by this process. We present some results on optimal size Binary Space Partitions in two dimensions. In this thesis we show that we cannot always get an optimal BSP {{for a set of}} disjoint line segments if we only use <b>fixed</b> <b>partition</b> lines, which are partition lines that go through at least two endpoints of fragments. We also proof that the best BSP that only uses <b>fixed</b> <b>partition</b> lines cuts at most 3 times as much line segments as the optimal BSP. We provide an algorithm that computes an optimal BSP for a rectangular subdivision in O(n 5) time – using dynamic programming – and discuss some heuristics to improve this. We generalize the algorithm so it works for a larger class of optimality criteria, including size and depth. We give experimental results based on randomly generated rectangula...|$|E
40|$|An {{important}} {{problem in}} transportation planning is the modeling of patterns of trip-making [...] -especially trips from home (origin) to work (destination) within a fixed geographic area. The standard tool {{used to study}} such OD flows is the so-called "gravity model", a Poisson log-linear regression model for studying the number of trips from origins within one element of a <b>fixed</b> <b>partition</b> area (traffic analysis zone, for example) to destinations within another...|$|E
40|$|AbstractFor fixed n and a <b>fixed</b> <b>partition</b> α of k<n we give an {{explicit}} formula {{for the number}} N(n;α) of standard skew Young tableaux with n squares and shape λ/α for some λ. From this formula the entire asymptotic expansion of N(n;α) as n→∞ can in principle be computed, generalizing recent work of McKay, Morse, and Wilf. We also give asymptotic formulas for the number fλ/α of standard skew Young tableaux of shape λ/α for α fixed and λ “large. ...|$|E
40|$|Polya trees <b>fix</b> <b>partitions</b> and use random probabilities {{in order}} to {{construct}} random probability measures. With quantile pyramids we instead fix probabilities and use random partitions. For nonparametric Bayesian inference we use a prior which supports piecewise linear quantile functions, based {{on the need to}} work with a finite set of partitions, yet we show that the limiting version of the prior exists. We also discuss and investigate an alternative model based on the so-called substitute likelihood, Both approaches factorize in a convenient way leading to relatively straightforward analysis via MCMC, since analytic summaries of posterior distributions are too complicated. We give conditions securing the existence of an absolute continuous quantile process, and discuss consistency and approximate normality for the sequence of posterior distributions. Illustrations are included...|$|R
40|$|We {{study the}} {{stationary}} points {{of what is}} known as the lattice Landau gauge fixing functional in one-dimensional compact U(1) lattice gauge theory, or as the Hamiltonian of the one-dimensional random phase XY model in statistical physics. An analytic solution of all stationary points is derived for lattices with an odd number of lattice sites and periodic boundary conditions. In the context of lattice gauge theory, these stationary points and their indices are used to compute the gauge <b>fixing</b> <b>partition</b> function, making reference in particular to the Neuberger problem. Interpreted as stationary points of the one-dimensional XY Hamiltonian, the solutions and their Hessian determinants allow us to evaluate a criterion which makes predictions on the existence of phase transitions and the corresponding critical energies in the thermodynamic limit. © 2011 Elsevier Inc. Articl...|$|R
40|$|Abstract—The {{essence of}} fractal image {{denoising}} is {{to predict the}} fractal code of a noiseless image from its noisy observation. From the predicted fractal code, one can generate {{an estimate of the}} original image. We show how well fractal-wavelet denoising predicts parent wavelet subtress of the noiseless image. The per-formance of various fractal-wavelet denoising schemes (e. g., <b>fixed</b> <b>partitioning,</b> quadtree partitioning) is compared to that of some standard wavelet thresholding methods. We also examine the use of cycle spinning in fractal-based image denoising for the purpose enhancing the denoised estimates. Our experimental results show that these fractal-based image denoising methods are quite com-petitive with standard wavelet thresholding methods for image de-noising. Finally, we compare the performance of the pixel- and wavelet-based fractal denoising schemes. Index Terms—Fractal image coding, fractals, image denoising, image restoration. I...|$|R
