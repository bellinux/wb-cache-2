11|25|Public
40|$|This report {{describes}} how a file system level log-based technique {{can improve the}} write performance of many-to-one write checkpoint workload typical for high performance computations. It is shown that a simple log-based organization can provide for substantial improvements in the write performance while retaining the convenience of a single flat file abstraction. The improvement of the write performance comes {{at the cost of}} degraded read performance however. Techniques to alleviate the read performance penalty, such as <b>file</b> <b>reconstruction</b> on the first read, are discussed. I...|$|E
40|$|Abstract:- Embedding {{a hidden}} stream of bits in a file is called Digital {{watermarking}}. The file {{could be an}} image, audio, video or text. Nowadays, a digital watermarking has many applications such as broadcast monitoring, owner identification, proof of ownership, transaction tracking, content authentication, copy control, device control and <b>file</b> <b>reconstruction.</b> It is intended to complement cryptographic processes. It is a visible or preferably invisible, identification code that is permanently embedded in the data and remains present within the data after any decryption process. The focus of this paper will detail digital watermarking for multimedia applications and covered by definition of digital watermarking, purpose, techniques and types of watermarking attacks briefly discussed...|$|E
30|$|The {{communications}} {{within a}} wireless storage network can be modeled as a multiple-access channel with additional inter-source communication links. Motivated by this observation, we have proposed three physical layer transmission schemes based on different time-sharing and relaying {{strategies that are}} suitable for the given channel model. In contrast to the state-of-the-art MAC DMT optimal algebraic space-time codes, our schemes are efficiently sphere-decodable with {{only one or two}} antennas. Their DMT performance reaches between the time-sharing DMT and the optimal MAC DMT—the one for conventional MIMO-MAC having no inter-source links—in the high-multiplexing gain regime. When the desired multiplexing gain is low, the schemes even outperform the optimal MAC DMT. Naturally, the schemes are also applicable to DSS <b>file</b> <b>reconstruction,</b> as well as to any MAC communications with inter-source links.|$|E
50|$|A {{scheduled}} (January 3, 2012) foreclosure auction was averted {{due to a}} Chapter 11 Bankruptcy <b>filing</b> by LaGrave <b>Reconstruction</b> Company Several months later, {{the debt}} holder Amegy Bank of Houston finally completed foreclosure and became the landlord of the Cats.|$|R
50|$|Basic archivers {{just take}} a list of files and {{concatenate}} their contents sequentially into archives. The archive files need to store metadata, at least the names and lengths of the original <b>files,</b> if proper <b>reconstruction</b> is possible. More advanced archivers store additional metadata, such as the original timestamps, file attributes or access control lists.|$|R
50|$|In early 2012, {{the club}} owner {{announced}} that the financial situation needed to be reviewed. As a result, the Redhawks <b>filed</b> for economic <b>reconstruction</b> on 11 April 2012. The filing was granted the following day. The club re-negotiated the most expensive player contracts {{in order to reduce}} the total salary cap from 30 million SEK down to 18 million.|$|R
40|$|AbstractThe {{demand for}} rapid use of Internet has {{increased}} many folds in distributed environments. It {{is easy for}} the custodian of the digital data to transfer images of classification across computers via internet. The information transmitted is not secured. Conventional encryption and decryption algorithms are vulnerable from the thefts of the digital data and do not protect the data from copying after the data is transmitted. The data prone to copyright thefts should be protected against manipulation, misuse and security breaks. First the image is classified using maximum likelihood classification and fuzzy model. Second, for transmission, with copyright information, image is secured, by image bit depth plane digital image watermarking. In this method, image is read and separated into n-bit planes. The accuracy is tested using visual interpretation and assessment. The application of this method is identification of ownership, claim of ownership, online transaction, digital content authorization, <b>file</b> <b>reconstruction.</b> This method is robust against intentional and unintentional attacks of malicious users...|$|E
40|$|Visualisation is {{becoming}} increasingly important for understanding information, such as investigative data (for example: computing, medical and crime scene evidence) and analysis (for example, network capability assessment, data <b>file</b> <b>reconstruction</b> and planning scenarios). Investigative data visualisation is used to reconstruct a scene or item and is used to assist the viewer (who {{may well be a}} member of the general public with little or no understanding of the subject matter) to understand what is being presented. Analysis visualisations, on the other hand, are usually developed to review data, information and assess competing scenario hypotheses for those who usually have an understanding of the subject matter. Courtroom environments are morphing into cinematic display environments, the media consumed by an audience who are increasingly visually literate and media savvy (Heintz, 2002). There are a number of fundamental implications inherent in the shift from oral to visual mediation and a number of facets of thi...|$|E
40|$|X-ray Computerized Tomography (CT) {{has emerged}} as a powerful, {{nondestructive}} tool to study and quantify the three-dimensional internal structure of HMA. Studies, in conjunction with modeling and computational techniques, have focused on characterization of HMA internal air-void distribution, internal structure evolution during laboratory compaction, damage evolution in specimens during laboratory tests, segregation analysis, forensic investigation of pavements using cores, and the quantification of HMA microstructural properties. The Center for Nondestructive Evaluation (CNDE) at Iowa State University (ISU) has an in-house built high-resolution CT system with customized software for data acquisition, volumetric <b>file</b> <b>reconstruction,</b> and visualization. Preliminary studies were conducted at the CNDE to investigate the capabilities and resolution levels of the imaging systems in studying asphalt materials. Researchers at both ISU and Iowa Department of Transportation are currently using the advanced imaging facilities available at the CNDE and the latest developments in image analysis techniques to develop {{a deeper understanding of the}} HMA internal structure, develop and optimize the various parameters that describe the internal structure and relate them to the performance of pavements in a scientific way. This will provide the foundations to building more durable and long-lasting pavements...|$|E
40|$|Abstract: This paper {{introduced}} a new design method for mechanical and electrical products, for example electric drill handle. By using Geomagic Studio Software, the cloud data about scanning electric drill was preprocessed, the three-dimension surface was reconfigured according to the preprocessed data, the surface <b>reconstruction</b> <b>file</b> was imported into UG Software for substantialization. Finally, the model was improved and optimized according to individual customer needs. ...|$|R
40|$|This paper {{presents}} {{the design and}} implementation of the recovery scheme in Calypso. Calypso is a cluster-optimized, distributed file system for UNIX clusters. As in Sprite and AFS, Calypso servers are stateful and scale well to a large number of clients. The recovery scheme in Calypso is non-disruptive, meaning that open files remain open, client modified data is saved, and in-flight operations are properly handled across server recovery. The scheme uses distributed state among the clients to reconstruct the server state on a backup node if disks are multi-ported or on the rebooted server node. It guarantees data consistency during recovery and provides congestion control. Measurements show that the state reconstruction can be quite fast: for example, in a 32 -node cluster, when an average node contains state for about 420 <b>files,</b> the <b>reconstruction</b> time is about 3. 3 seconds. However, the time to update a file system after a failure can be {{a major factor in the}} overall recovery time, [...] ...|$|R
30|$|In many cases, {{a number}} of similar samples are scanned so {{it is useful to}} use a GUI to {{optimize}} the reconstruction workflow and then apply the same parameters to a series of datasets. Considering this common scenario, the current version of the software is designed to retain the last adopted parameters. However, it would be desirable to support users with a batch execution, i.e., the preparation of a parameter <b>file</b> describing the <b>reconstruction</b> protocol combined with an automatic application of this protocol to a sequence of datasets without further user support. This feature will be included in future versions of the software.|$|R
40|$|Storing {{multiple}} {{copies of}} files {{is crucial for}} ensuring quality of service for data storage in mobile networks. This paper proposes a new scheme, called the K-out-of-N file distribution scheme, for the placement of files. In this scheme files are splitted, and Reed-Solomon codes or other maximum distance seperable (MDS) codes are used to produce file segments containing parity information. Multiple copies of the file segments are stored on gateways in the network {{in such a way}} that every gateway can retrieve enough file segments from itself and its neighbors within a certain amount of hops for reconstructing the orginal files. The goal is to minimize the maximum number of hops it takes for any gateway to get enough file segments for the <b>file</b> <b>reconstruction.</b> We formulate the K-out-of-N file distribution scheme as a coloring problem we call diversity coloring. A diversity coloring is defined to be optimal if it uses the smallest number of colors. Upper and lower bounds on the performance of diversity coloring for general graphs are studied. Diversity coloring algorithms for several special classes of graphs - trees, rings and tori - are presented, all of which have linear time complexity. Both the algorithm for trees and the algorithm for rings output optimal diversity colorings. The algorithm for tori guarantees to output optimal diversity coloring when the sizes of tori are sufficiently large...|$|E
40|$|Visualisation is {{becoming}} increasingly important for understanding information, such as investigative data (for example: computing, medical and crime scene evidence) and analysis (for example: network capability assessment, data <b>file</b> <b>reconstruction</b> and planning scenarios). Investigative data visualisation is used to reconstruct a scene or item and is used to assist the viewer (who {{may well be a}} member of the general public with little or no understanding of the subject matter) to understand what is being presented. Analysis visualisations, on the other hand, are usually developed to review data, information and assess competing scenario hypotheses for those who usually have an understanding of the subject matter. Â  Visualisation represents information that has been digitally recorded (for example: pictures, video and sound), hand written and/or spoken data, to show what may have, could have, did happen or is believed to have happened. That is why visualising data is an important development in the analysis and investigation realms, as visualisation explores the accuracies, inconsistencies and discrepancies of the collected data and information. Â  This paper presents introduces some of the various graphical techniques and technology used to display digital information in a courtroom. The advantages and disadvantages involved in the implementation of this technology are also discussed. This paper is part one of a two part series that aims to describe the use of, and provide guidelines for, the use of graphical displays in courtrooms. </p...|$|E
40|$|Abstract: Storing {{multiple}} {{copies of}} files {{is crucial for}} ensuring quality of service for data storage in mobile networks. This paper proposes a new scheme, called the K-out-of-N file distribution scheme, for the placement of files. In this scheme files are splitted, and Reed-Solomon codes or other maximum distance seperable (MDS) codes are used to produce file segments containing parity information. Multiple copies of the file segments are stored on gateways in the network {{in such a way}} that every gateway can retrieve enough file segments from itself and its neighbors within a certain amount of hops for reconstructing the orginal files. The goal is to minimize the maximum number of hops it takes for any gateway to get enough file segments for the <b>file</b> <b>reconstruction.</b> We formulate the K-out-of-N file distribution scheme as a coloring problem we call diversity coloring. A diversity coloring is defined to be optimal if it uses the smallest number of colors. Upper and lower bounds on the performance of diversity coloring for general graphs are studied. Diversity coloring algorithms for several special classes of graphs—trees, rings and tori—are presented, all of which have linear time complexity. Both the algorithm for trees and the algorithm for rings output optimal diversity colorings. The algorithm for tori guarantees to output optimal diversity coloring when the sizes of tori are sufficiently large. Index Terms: Data storage, diversity coloring, file assignment problem (FAP), graph coloring, K-out-of-N scheme, maximum distance seperable (MDS) codes, mobile computing, Quality of Servic...|$|E
40|$|The {{proliferation}} of mobile communication and computing devices, in particular smart mobile phones, is almost paralleled {{with the increasing}} number of mobile device forensics tools in the market. Each mobile forensics tool vendor, on one hand claims to have a tool that is best in terms of performance, while on the other hand each tool vendor seems to be using different standards for testing their tools and thereby defining what support means differently. To overcome this problem, a testing framework based on a series of tests ranging from basic forensics tasks such as <b>file</b> system <b>reconstruction</b> up to more complex ones countering antiforensic techniques is proposed. The framework, which is an extension of an existing effort done in 2010, prescribes a method to clearly circumscribe the term support into precise levels. It also gives an idea of the standard to be developed and accepted by the forensic community that {{will make it easier for}} forensics investigators to quickly select the most appropriate tool for a particular mobile device...|$|R
50|$|During October 2004, {{residents}} and council members opposing the <b>reconstruction</b> <b>filed</b> another petition signed {{by more than}} 2700 residents which {{was approved by the}} county's administration on November 30, 2004. The city council sued against the approval of the petition, thus preventing a second binding poll which was to be conducted on February 20, 2005, the same day on which the state parliament elections took place. The group opposing the <b>reconstruction</b> subsequently <b>filed</b> a request for an injunction against the city council to prevent them from immediately commencing the reconstruction by awarding a binding and definitive construction license to a company bidding for the tender.|$|R
40|$|Instant {{messenger}} {{programs such}} as ICQ are often used by hackers and criminals for illicit purposes and consequently the log files from such programs are of interest in a forensic investigation. This paper outlines research {{that has resulted in}} the development of a tool for the extraction of ICQ log <b>file</b> entries. Detailed <b>reconstruction</b> of data from log files was achieved with a number of different ICQ software. There are several limitations with the current design including timestamp information not adjusted for the time zone, data could be altered, and conversations must be manually reconstructed. Future research will aim to address these and other limitations as pointed out in this paper. </p...|$|R
40|$|This paper {{describes}} IDtrace, {{a binary}} instrumentation tool which produces execution traces for the ix 86 instruction set architecture. Long execution traces {{can be generated}} quickly and easily for input {{to a wide variety}} of performance evaluation tools. Issues involved in the construction of such a tool are listed along with illustrations of the uses of different generated traces. One example observes the behavior of a branch prediction technique and another compares the cache behavior of the i 486 with that of the MIPS R 3000. 1 Description of IDtrace Trace driven simulation plays an important role in the design and tuning of computer architectures. Thus tools which can produce long traces quickly and easily are valued by system designers. This paper discusses IDtrace[1], a software tracing tool for the i 486 which produces input data for a wide variety of performance evaluation tools including code profilers and branch prediction and cache simulators. IDtrace instruments program code so that traces are generated during the program’s execution. Thus long traces can be produced quickly and easily without additional hardware. Software tracing tools can instrument programs at the source, object, or executable level. Modifying the executable, called late code modification, requires text disassembly, code modification and relocation, and binary <b>file</b> <b>reconstruction.</b> While this approach is the most difficult, instrumentation does not require the source files, the library code is automatically instrumented, and the tool is almost trivial to use. IDtrace uses late code modification to instrument Unix SysV R 4 ELF statically-linked binaries created by Intel/ AT&T and USL CCS C compilers. No sources or symbol tables are needed. Applications can be instrumented to output profile, memory reference, or full execution traces. The resulting binary is about twelve times larger than the original and execution time for the instrumented program ranges from two times that of the original for a profile trace to about twelve times for a full execution trace...|$|E
40|$|The aim of {{this article}} is to present a {{developed}} method that decomposes the autofluorescence spectrum into the spectra of naturally occurring biochemical components of biotissue. It requires knowledge of detailed spectrum behaviour of different endogenous fluorophores. We have studied the main bio-markers in human tissue and proposed a simple modelling algorithm for their spectra shapes. The empirical method was tested theoretically by quantum-mechanical calculations of the spectra in the unharmonic Morse potential approach. Comment: 13 pages, 7 figures, submitted to "Journal of Medical Physics", 2005. As the supplemental material, 4 animation <b>files</b> illustrating the <b>reconstruction</b> procedures and properties of fluorophores spectra, by the proposed line-shape model. To view them with comments, - use link: [URL]...|$|R
40|$|Abstract. We {{address the}} issue of {{recovering}} a both safe and precise approximation of the Control Flow Graph (CFG) of a program given as an executable file. The problem is tackled in an original way, with a refinement-based static analysis working over finite sets of constant values. Requirement propagation allows the analysis to automatically adjust the domain precision only where it is needed, resulting in precise CFG recovery at moderate cost. First experiments, including an industrial case study, show that the method outperforms standard analyses in terms of precision, efficiency or robustness. Motivation. Automatic analysis of programs from their executable files has many potential applications in safety and security, for example: automatic analysis of mobile code and malware, security testing or worst case execution time estimation. We address the problem of (safe) CFG reconstruction, i. e. constructing a both safe and precise approximation of the Control Flow Graph (CFG) of a program given as an executable <b>file.</b> CFG <b>reconstruction</b> is a cornerstone of safe binary-level analysis: if the recovery is unsafe, subsequent analyses will be unsafe too; if it is too rough, they will be blurre...|$|R
40|$|Source <b>files</b> and <b>reconstructions</b> for "Simple 3 D {{compressed}} sensing {{scheme for}} faster and less phototoxic fluorescence microscopy imaging" The source files {{are to be}} used with the code on [URL] (also archived in [URL] 	The files prefixed with "VIZ" are high resolution TIF visualizations. 	The files come from three experiments on two different setups: 	 		A lattice light sheet microscope (LLSM) : beads sample (filed termed "lattice-beads" and actin-labelled mESCs (files termed "lattice-phalloidin") 		An epifluorescence microscope: beads sample (files termed "epifluorescence") 	 	 	The acquisitions were either performed using an identity measurement matrix (mimicking the plane-by-plane acquisition mode of a traditional z-stack) : files termes "reference" or with a Fourier measurement matrix (described in the code mentioned above) with a compression ratio of 2 (files termed "compressed". 	The reconstructions were performed as described in the paper with the code mentioned above. Several reconstructions were computed from the same compressed images by simulating increasing compression ratios. To do so, reconstructions were performed by selecting a subset of the acquired planes (number indicated as "**frames") 	Reconstructions were sparsified using a 2 D PSF model computed for our epifliuorescence setup and the LLSM (files termed "PSF_model"). These are provided as numpy arrays...|$|R
40|$|A 3 D {{model of}} lumbar {{structures}} of anesthetic interest was reconstructed from human magnetic resonance (MR) images and {{embedded in a}} Portable Document Format (PDF) file, which can be opened by freely available software and used offline. The MR images were analyzed using a specific 3 D software platform for biomedical data. Models generated from manually delimited volumes of interest and selected MR images were exported to Virtual Reality Modeling Language format and were presented in a PDF document containing JavaScript-based functions. The 3 D file and the corresponding instructions and license files can be downloaded freely at [URL] The 3 D PDF interactive <b>file</b> includes <b>reconstructions</b> of the L 3 -L 5 vertebrae, intervertebral disks, ligaments, epidural and foraminal fat, dural sac and nerve root cuffs, sensory and motor nerve roots of the cauda equina, and anesthetic approaches (epidural medial, spinal paramedial, and selective nerve root paths); it also includes a predefined sequential educational presentation. Zoom, 360 rotation, selective visualization, and transparency graduation of each structure and clipping functions are available. Familiarization requires no specialized informatics knowledge. The ease with which the document can be used could make it valuable for anatomical and anesthetic teaching and demonstration of patient information...|$|R
40|$|Estimating the {{potential}} benefit of advanced safety systems by simulation {{has become increasingly}} important during the last years. All over the world OEMs and suppliers carry out benefit estimations by simulations via computer models. Such simulations should, of course, be based on real world scenario such as the pre-crash phase of real world accidents. Several methodologies for building up accident scenarios have been developed in the past. This paper shows a new method for generating pre-crash scenarios directly from {{the reconstruction of the}} accident by using the software PC-Crash 1. The new method was developed by the Medical University Hannover (MHH) and the Fraunhofer Institute for Transportation Dresden (Fraunhofer IVI). It is based on transferring all information (participant-, vehicle-, environment- and motion-data) from the <b>reconstruction</b> <b>file</b> into a scenario-database...|$|R
40|$|In recent years, {{the problem}} of {{modeling}} and predicting a user’s surfing behavior on a web-site has obtained {{a lot of research}} interest. Markov models& its variations have also been used to analyze web navigation behavior of users. A user's web link transition on a particular website can be modeled using first, second-order or higher-order Markov models and {{can be used to make}} predictions regarding future navigation and to personalize the web page for an individual user. As it can be used to improve the web cache performance, recommend related pages, improve search engines, understand and influence buying patterns, and personalize the browsing experience. Modeling user web navigation data is a challenging task that is continuing to gain importance as the size of the web and its user-base increases. Data characterizing web navigation can be collected from the server or client- based log <b>files,</b> enabling the <b>reconstruction</b> of user navigation sessions...|$|R
40|$|The {{projection}} {{images of}} implosion-pellet captured by framing cameras or pinhole cameras are two-dimensional in the inertia confinement fusion experiments. Since the two-dimensional images are {{lack of the}} depth information, therefore they are hardly used to diagnose the compression symmetry of the implosion-pellet, the 3 D image of the implosion-pellet reconstructed from their two-dimensional projection images can overcome these problems. As the iterative algorithms can reconstruct the original 3 D image from just a few projections with good noise suppression, three iterative algorithms which are commonly applied in CT image <b>reconstruction</b> <b>filed</b> are utilized to the reconstruct 3 D image of implosion-pellet. The numerical simulations show that the algebraic reconstruction technique algorithm performs best {{under the condition that}} the projection images are ‘incomplete’ and noise free or with not so heavy noise. When there are heavy noise in the projection images the simultaneous iterative reconstruction technique surpasses the other algorithms, which is proved to be more competent for the 3 D image reconstruction of implosion-pellet in inertia confinement fusion experiment...|$|R
50|$|The type was classed {{with the}} trireme, {{and had two}} and a half files of oarsmen on each side. Judging from the Lindos relief and the famous Nike of Samothrace, both of which are thought to {{represent}} trihemioliai, the two upper files would have been accommodated in an oarbox, with the half-file located beneath them in the classic thalamitai position of the trireme. The Lindos relief also includes a list of the crews of two trihemioliai, allowing us to deduce that each was crewed by 144 men, 120 of whom were rowers (hence a full <b>file</b> numbered 24). <b>Reconstruction</b> based on the above sculptures shows that the ship was relatively low, with a boxed-in superstructure, a displacement of ca. 40 tonnes, and capable of reaching speeds comparable with those of a full trireme. The trihemiolia was a very successful design, and was adopted by the navies of Ptolemaic Egypt and Athens among others. Despite being classed as lighter warships, they were sometimes employed in a first-line role, for instance at the Battle of Chios.|$|R
40|$|International audienceMITICS {{is a new}} {{software}} developed for MALDI imaging. We tried to render this software compatible with all types of instruments. MITICS is divided in two parts: MITICS control for data acquisition and MITICS Image for data processing and images reconstruction. MITICS control is available for Applied BioSystems MALDI-TOF instruments and MITICS Image for both Applied BioSystems and Bruker Daltonics ones. MITICS Control provides an interface to the user for setting the acquisition parameters for the imaging sequence, namely set instruments acquisition parameters, create the raster of acquisition and control post-acquisition data processing, and provide this settings to the automatic acquisition software of the MALDI instrument. MITICS Image ensures image <b>reconstruction,</b> <b>files</b> are first converted to XML files before being loaded in a database. In MITICS image we have chosen to implement different data representations and calculations for image reconstruction. MITICS Image uses three different representations that have shown to ease extraction of information from the whole data set. It also offers image reconstruction base either on the maximum peak intensity or the peak area. Image reconstruction is possible for single ions but also by summing signals of different ions. MITICS was validated on biological cases...|$|R
40|$|Digital {{reconstruction}} of neuronal arborizations {{is an important}} step in the quantitative investigation of cellular neuroanatomy. In this process, neurites imaged by microscopy are semi-manually traced through the use of specialized computer software and represented as binary trees of branching cylinders (or truncated cones). Such form of the <b>reconstruction</b> <b>files</b> is efficient and parsimonious, and allows extensive morphometric analysis as well as the implementation of biophysical models of electrophysiology. Here, we describe Neuron_ Morpho, a plugin for the popular Java application ImageJ that mediates the digital {{reconstruction of}} neurons from image stacks. Both the executable and code of Neuron_ Morpho are freely distributed (www. maths. soton. ac. uk/staff/D'Alessandro/morpho or www. krasnow. gmu. edu/L-Neuron), and are compatible with all major computer platforms (including Windows, Mac, and Linux). We tested Neuron_Morpho by reconstructing two neurons from each of the two preparations representing different brain areas (hippocampus and cerebellum), neuritic type (pyramidal cell dendrites and olivar axonal projection terminals), and labeling method (rapid Golgi impregnation and anterograde dextran amine), and quantitatively comparing the resulting morphologies to those of the same cells reconstructed with the standard commercial system, Neurolucida. None of the numerous morphometric measures that were analyzed displayed any significant or systematic difference between the two reconstructing systems...|$|R
2500|$|Johnson's {{interpretations of}} Lincoln's {{policies}} prevailed until the Congressional elections of 1866 in the North, which enabled the Radicals {{to take control}} of policy, remove former Confederates from power, and enfranchise the freedmen. A Republican coalition came to power in nearly all the southern states and set out to transform the society by setting up a free labor economy, using the U.S. Army and the Freedmen's Bureau. The Bureau protected the legal rights of freedmen, negotiated labor contracts, and set up schools and churches for them. Thousands of Northerners came south as missionaries, teachers, businessmen and politicians. Some also entered politics. Hostile whites called them [...] "carpetbaggers". In early 1866, Congress passed the Freedmen's Bureau and Civil Rights Bills and sent them to Johnson for his signature. The first bill extended the life of the bureau, originally established as a temporary organization charged with assisting refugees and freed slaves, while the second defined all persons born in the United States as national citizens with equality before the law. After Johnson vetoed the bills, Congress overrode his veto, making the Civil Rights Act the first major bill {{in the history of the}} United States to become law through an override of a presidential veto. The Radicals in the House of Representatives, frustrated by Johnson's opposition to Congressional <b>Reconstruction,</b> <b>filed</b> impeachment charges. The action failed by one vote in the Senate.|$|R
40|$|We {{study the}} data {{reliability}} {{problem for a}} community of devices forming a mobile cloud storage system. We consider the application of regenerating codes for file maintenance within a geographically-limited area. Such codes require lower bandwidth to regenerate lost data fragments compared to <b>file</b> replication or <b>reconstruction.</b> We investigate threshold-based repair strategies where data repair is initiated after a threshold number of data fragments have been lost due to node mobility. We show that at a low departure-to-repair rate regime, a lazy repair strategy in which repairs are initiated after several nodes have left the system outperforms eager repair in which repairs are initiated after a single departure. This optimality is reversed when nodes are highly mobile. We further compare distributed and centralized repair strategies and derive the optimal repair threshold for minimizing the average repair cost per unit of time, {{as a function of}} underlying code parameters. In addition, we examine cooperative repair strategies and show performance improvements compared to non-cooperative codes. We investigate several models for the time needed for node repair including a simple fixed time model that allows for the computation of closed-form expressions and a more realistic model that takes into account the number of repaired nodes. We derive the conditions under which the former model approximates the latter. Finally, an extended model where additional failures are allowed during the repair process is investigated. Overall, our results establish the joint effect of code design and repair algorithms on the maintenance cost of distributed storage systems. Comment: 23 pages, 11 figure...|$|R
40|$|The Caribbean {{region has}} a complex tectonic history that {{resulted}} from the interplay of the North and South American, the Caribbean, and (Paleo-) Pacific plates. Being largely surrounded by long-lived subduction zones and transform boundaries, reconstructing Caribbean plate motion into the global plate circuit cannot be done using marine magnetic anomalies. Here, I present a fully quantitative, kinematically consistent tectonic reconstruction, back to 200 Ma, using the Atlantic plate circuit as boundary condition. This reconstruction is made in GPlates freeware and all <b>reconstruction</b> <b>files</b> are made available. To restore Caribbean plate motion between the American continents, I use a reconstruction hierarchy based on strike-slip and thrust belt records, using regionally extensive geological phenomena such as the Great Arc of the Caribbean, the Caribbean Large Igneous Province (CLIP) and the Caribeana high-pressure belt as correlation markers. The resulting model restores the Caribbean plate back along the Cayman Trough and strike-slip faults in Guatemala, offshore Nicaragua, offshore Belize and along the Northern Andes towards its position of origin, west of the North and South American continents. Two plate kinematic scenarios for {{the origin of the}} Caribbean plate lithosphere are evaluated; an origin from Proto-Caribbean/Atlantic spreading, or from spreading within the Panthalassa domain: I conclude that the latter can provide a simpler explanation. Placing our reconstruction in the most recent mantle reference frames shows that the CLIP erupted 2 - 3000 km east of, and is probably not the result of the plume head stage of the Galápagos hotspot. Finally, our reconstruction suggests that all modern subduction zones surrounding the Caribbean plate probably formed by inversion of transform faults, two of these (along the southern Mexican and NW South American margins) strongly diachronously as a result of migrating trench-trench-transform triple junctions...|$|R
40|$|Abstract Background Until today, {{analysis}} of 16 S ribosomal RNA (rRNA) sequences {{has been the}} de-facto gold standard {{for the assessment of}} phylogenetic relationships among prokaryotes. However, the branching order of the individual phlya is not well-resolved in 16 S rRNA-based trees. In search of an improvement, new phylogenetic methods have been developed alongside with the growing availability of complete genome sequences. Unfortunately, only a few genes in prokaryotic genomes qualify as universal phylogenetic markers and almost all of them have a lower information content than the 16 S rRNA gene. Therefore, emphasis has been placed on methods that are based on multiple genes or even entire genomes. The concatenation of ribosomal protein sequences is one method which has been ascribed an improved resolution. Since there is neither a comprehensive database for ribosomal protein sequences nor a tool that assists in sequence retrieval and generation of respective input <b>files</b> for phylogenetic <b>reconstruction</b> programs, RibAlign has been developed to fill this gap. Results RibAlign serves two purposes: First, it provides a fast and scalable database that has been specifically adapted to eubacterial ribosomal protein sequences and second, it provides sophisticated import and export capabilities. This includes semi-automatic extraction of ribosomal protein sequences from whole-genome GenBank and FASTA files as well as exporting aligned, concatenated and filtered sequence files that can directly be used in conjunction with the PHYLIP and MrBayes phylogenetic reconstruction programs. Conclusion Up to now, phylogeny based on concatenated ribosomal protein sequences is hampered by the limited set of sequenced genomes and high computational requirements. However, hundreds of full and draft genome sequencing projects are on the way, and advances in cluster-computing and algorithms make phylogenetic reconstructions feasible even with large alignments of concatenated marker genes. RibAlign is a first step in this direction and may be particularly interesting to scientists involved in whole genome sequencing of representatives of new or sparsely studied eubacterial phyla. RibAlign is available at [URL] </p...|$|R
40|$|Abstract- In {{this work}} we present an audio data {{compression}} software package {{which uses a}} Discrete Wavelet Transform based compression procedure. The used wavelet function can be chosen from the well known Daubechies class of compactly supported wavelets. Our implementation uses an adaptive manner for the wavelet domain threshold value computation. The package has two major components: a compression software which reads a standard audio data <b>file</b> and the <b>reconstruction</b> software which reads a compressed file and generates a standard audio file. Experimental results are also presented. I. DATA COMPRESSION WITH WAVELETS Data compression is a very largely used procedure for data storage purposes. There exist a large variety of compression algorithms, many of them standardised {{and each of them}} having its advantages and its backdraws. They offers speed, high compression ratio, portability etc., but {{there is not a single}} algorithm which best fit to all kind of applications. They were classified in two major categories, the first includes the ones for compression without loss and the second one includes those for compression with loss. All the algorithms have the same purpose, to reduce the information redundancy from the considered data set, but those from the first class allow a perfect reconstruction while those from the second class does not. Those procedures which allow small information losses can achieve higher compression ratio, but they can not be utilised in applications where the data set integrity is primordial (for file compression for example). This second class includes various voice and image compression algorithms. In this work we present a complete audio compression/decompression software package, which implements an algorithm from this second class, which exploits a very popular orthogonal transform named the Discrete Wavelet Transform (DWT), [1], [2]. The topics of the wavelet transforms and their properties had been studied very extensively by the authors in the context of various data compression and signal to noise (SNR) enhancemen...|$|R

