7|10000|Public
40|$|It is {{becoming}} feasible and practical {{to monitor the}} generic computer usage of students for extended periods, recording low level actions such as mouse clicks, typing and window changes. This paper presents a case study on the deployment of GRUMPS technology {{during a period of}} six weeks when 4. 7 million such actions were collected from 141 first year university students learning Ada programming. We suggest our approach can be characterised as REDDI, Rapidly Evolving Digitally-Derived Investigations. Data preparation and cleaning is noted as a bottleneck in <b>generic</b> <b>data</b> <b>collection,</b> but seems less so as techniques are developed and understood...|$|E
40|$|Abstract. Quantifying time-on-task is {{important}} in many areas, but is of considerable importance to researchers considering the relationships between exposure and injury risk. This paper describes the method used to develop a <b>generic</b> <b>data</b> <b>collection,</b> storage and retrieval program for measuring time-on-task. The program uses hypertext markup language, javaScript and Perl to provide an easily navigated computer interface for recording exposure variables on a tablet style computer (laptop). This method allows for data collection {{in the field and}} provides several benefits over traditional methods of data capture. This utility may have application in a range of research areas including education and skill acquisition, in addition to providing a precise measure of exposure to high-risk environments...|$|E
40|$|This chapter aims at {{providing}} {{a clear and}} concise picture of data collection for intrusion detection. It provides a detailed explanation of <b>generic</b> <b>data</b> <b>collection</b> mechanism components and the interaction with the environment, from initial triggering to output of log data records. Taxonomies of mechanism characteristics and deployment considerations are provided and discussed. Furthermore, guidelines and hints for mechanism selection and deployment are provided. Finally, this chapter presents a set of strategies for determining what data to collect, and it also discusses some of the challenges in the field. An appendix {{providing a}} classification of 50 studied mechanisms is also provided. This chapter aims at assisting intrusion detection system developers, designers, and operators in selecting mechanisms for resource efficient data collection...|$|E
40|$|The file {{attached}} to this record is the author's final peer reviewed version. The Publisher's final version can be found by following the DOI link. Mobile crowd photographing (MCP) is an emerging area of interest for researchers as the built-in cameras of mobile devices are {{becoming one of the}} commonly used visual logging approaches in our daily lives. In order to meet diverse MCP application requirements and constraints of sensing targets, a multifacet task model should be defined for a <b>generic</b> MCP <b>data</b> <b>collection</b> framework. Furthermore, MCP collects pictures in a distributed way in which a large number of contributors upload pictures whenever and wherever it is suitable. This inevitably leads to evolving picture streams. This paper investigates the multiconstraint-driven data selection problem in MCP picture aggregation and proposes a pyramid-tree (PTree) model which can efficiently select an optimal subset from the evolving picture streams based on varied coverage needs of MCP tasks. By utilizing the PTree model in a <b>generic</b> MCP <b>data</b> <b>collection</b> framework, which is called CrowdPic, we test and evaluate the effectiveness, efficiency, and flexibility of the proposed framework through crowdsourcing-based and simulation-based experiments. Both the theoretical analysis and simulation results indicate that the PTree-based framework can effectively select a subset with high utility coverage and low redundancy ratio from the streaming data. The overall framework is also proved flexible and applicable {{to a wide range of}} MCP task scenarios...|$|R
40|$|The {{importance}} of a deep knowledge of {{the circumstances of the}} occurrence of dangerous events threatening the air transportation system in order to prevent further analogous events is stressed. An effective data reporting system in the aviation maintenance domain, either mandatory or voluntary and complying with the EC regulations, is discussed. A prototypical procedure, developed according to a bottom-up approach and making use of ADREP 2000 taxonomy, that could serve as guideline for the development of more complex and <b>generic</b> instruments for <b>data</b> <b>collection</b> and analysis, is described. JRC. G. 4 -Maritime affair...|$|R
40|$|Standardized schemas, databases, {{and public}} data {{repositories}} {{are needed for}} the studies of malaria vectors that encompass a remarkably diverse array of designs and rapidly generate large data volumes, often in resource-limited tropical settings lacking specialized software or informatics support. Data from the majority of mosquito studies conformed to a <b>generic</b> schema, with <b>data</b> <b>collection</b> forms recording the experimental design, sorting of collections, details of sample pooling or subdivision, and additional observations. Generically applicable forms with standardized attribute definitions enabled rigorous, consistent data and sample management with generic software and minimal expertise. Forms use now includes 20 experiments, 8 projects, and 15 users at 3 research and control institutes in 3 African countries, resulting in 11 peer-reviewed publications. We have designed <b>generic</b> <b>data</b> schema {{that can be used}} to develop paper or electronic based <b>data</b> <b>collection</b> forms depending on the availability of resources. We have developed paper-based <b>data</b> <b>collection</b> forms {{that can be used to}} collect data from majority of entomological studies across multiple study areas using standardized data formats. Data recorded on these forms with standardized formats can be entered and linked with any relational database software. These informatics tools are recommended because they ensure that medical entomologists save time, improve data quality, and data collected and shared across multiple studies is in standardized formats hence increasing research outputs...|$|R
40|$|International audienceMobile crowd {{photography}} (MCP) is {{a widely}} used technique in crowd sensing. In MCP, a picture stream is generated when delivering intermittently to the backend server by participants. Pictures contributed later in the stream may be semantically or visually relevant to previous ones, which can result in data redundancy. To meet diverse constraints (e. g., spatiotemporal contexts, single or multiple shooting angles) on the data to be collected in MCP tasks, a data selection process is needed to eliminate data redundancy and reduce network overhead. This issue has little been investigated in existing studies. To address this requirement, we propose a <b>generic</b> <b>data</b> <b>collection</b> framework called PicPick. It first presents a multifaceted task model that allows for varied MCP task specification. A pyramid tree (PTree) method is further proposed to select an optimal set of pictures from picture streams based on multi-dimensional constraints. Experimental results on two real-world datasets indicate that PTree can effectively reduce data redundancy while maintaining the coverage requests, and the overall framework is flexibl...|$|E
40|$|Citizen Science (CS) is {{collaboration}} between scientists and citizens to expand opportunities for scientific {{data collection and}} problem solving. Recent advancements such as the Internet, social networks and smart devices have created a technological platform for CS to engage more citizens {{to work on a}} wide range of scientific problems. Due to technical, financial and management resource constraints many organisations struggle to develop effective tools to collect scientific data in CS projects. A robust web and mobile interface for scientific data collection will ensure collection of higher quality scientific data. While web and mobile applications have been developed for some CS projects many CS projects are hindered by the complexity and intrinsic costs of implementing these applications. This thesis describes a web-based model for CS data collection suitable for both small CS communities and larger scientific organisations. Offering features commonly used in CS projects, this model reduces costs associated with software implementation and management in CS. A CS campaign is undertaken as a case study that validates our model in a real world scenario. Overall the <b>generic</b> <b>data</b> <b>collection</b> framework presented will empower communities and organisations to engage and use CS in more ways and on large scales...|$|E
40|$|The {{health-related}} {{quality of}} life (HR-QOL) {{of children and adolescents}} is increasingly considered a relevant topic for research. Instruments to assess {{quality of life}} in children and adolescents of a generic as well as disease- or condition-specific nature are being developed and applied in epidemiological surveys, clinical studies, quality assurance and health economics. This paper attempts to give an overview {{on the state of the}} art of HR-QOL assessment in children as it relates to methodological and conceptual challenges. Instruments available in international or cross-cultural research to assess HR-QOL in generic terms were identified and described according to psychometric data provided and the width of application. In an initial literature search, several challenges in the assessment of child and adolescent HR-QOL were identified, ranging from conceptual and methodological to practical aspects. Seven specific major issues were considered: (i) What are the dimensions of HR-QOL relevant for children and adolescents, and do suitable instruments for their measurement exist? (ii) Can these dimensions be collected in a cross-culturally comparable way? (iii) What advantages and disadvantages do self-rated versus externally evaluated HR-QOL measurements of children and adolescents have? (iv) How can HR-QOL be assessed in an age-appropriate way? (v) What are the advantages and disadvantages of disease-specific and <b>generic</b> <b>data</b> <b>collection?</b> (vi) What advantages and disadvantages do profile and index instruments have? (vii) How can HR-QOL be connected with utility- preference values? In a second literature search we identified nine generic HR-QOL instruments and four utility health state classification systems that complied with the prespecified inclusion criteria. It was concluded that (i) HR-QOL instruments are available to assess the dimensions of the construct relevant to children and adolescents; (ii) provided that an instrument was constructed in an appropriate way, the dimensions of HR-QOL can be measured in an interculturally comparable manner; (iii) the HR-QOL of children and adolescents can and should be ascertained by self-rating; (iv) the measurement instruments used have to consider maturity and cognitive development; (v) only generic quality-of-life instruments allow for an assessment of HR-QOL in both healthy and chronically ill children and adolescents; (vi) the representation of HR-QOL achieved through a singular index value is connected to strict psychometric conditions: the index instrument has to be tailored to these psychometric conditions; (vii) how far utility measures are employable with children and adolescents has to be investigated in further studies. The problem aspects identified indicate the necessity for further research. Nevertheless, instruments for assessing the HR-QOL of children and adolescents can be identified that meet the requirements mentioned above. Adolescents, Children, Quality-of-life...|$|E
40|$|International audienceGames with {{a purpose}} other than {{entertainment}} can be called Serious Games. In this paper, we describe a <b>generic</b> event-based <b>Data</b> <b>Collection</b> Engine (DCE) that has been developed for Serious Games on the Unity Game Engine. Further, we describe a framework that allows for the manipulation and feedback of the collected data back into the game in real-time. The player experiences the visuals, sounds and the game itself that is streamed over the web. The player engages with an enriching, multimedia experience allowing him/her to be immersed in the game. By suitably designing the serious game we could determine {{the behavior of the}} player in real world under the given scenario or other scenarios. The DCE is optimized to collect relevant data streamed online without affecting the performance of the game. Also, the DCE is highly flexible and can be setup to collect data for any game developed on the Unity Engine...|$|R
40|$|Existing stream {{processing}} systems {{are designed for}} clustered deployments, and cannot adequately meet the scalability and adaptivity requirements of Internet-scale monitoring applications. Furthermore, these systems commonly optimize for a specific QoS metric, which may limit their applicability to diverse applications and environments. This paper presents XFlow, a <b>generic</b> distributed <b>data</b> <b>collection,</b> processing, and dissemination system that addresses these limitations. XFlow integrates a pub-sub model with data flows for {{stream processing}}. The underlying pub-sub model decouples sources and clients, {{as well as the}} processing operators, leading to a loosely-coupled architecture that can gracefully scale, adapt to churn in system membership and workload, and facilitate sophisticated optimizations. We first provide an overview of XFlow’s architecture. We then describe XFlow’s optimization model that changes the placement and implementation of operators to meet application-specific performance goals and constraints. Finally, we demonstrate the flexibility and the effectiveness using real-world streams and experimental results obtained from our PlanetLab deployment. 1...|$|R
40|$|Background {{and purpose}} EpiData and Epi Info {{are often used}} {{together}} by public health agencies around the world, particularly in developing countries, {{to meet their needs}} of low-cost public health data management; however, the current open source data management technology lacks a mobile component {{to meet the needs of}} mobile public health data collectors. The goal of this project is to explore the opportunity of filling this gap through developing and trial of a personal digital assistant (PDA) based data collection/entry system. It evaluated whether such a system could increase efficiency and reduce data transcription errors for public surveillance <b>data</b> <b>collection</b> in developing countries represented by Fiji. Methods A <b>generic</b> PDA-based <b>data</b> <b>collection</b> software eSTEPS was developed. The software and the data collected using it directly interfaces with EpiData. A field trial was conducted to test the viability of public health surveillance <b>data</b> <b>collection</b> using eSTEPS. The design was a randomised, controlled trial with cross-over design. 120 participants recruited from the Fiji School of Medicine were randomly assigned to be interviewed by one of six interviewers in one of the two ways: (1) paper-based survey followed by PDA survey and (2) PDA survey followed by paper-based survey. Data quality was measured by error rates (logical range errors/inconsistencies, skip errors, missing values, date or time field errors and incorrect data type). Work flow and cost were evaluated in three stages of the survey process: (1) preparation of <b>data</b> <b>collection</b> instrument, (2) <b>data</b> <b>collection</b> and (3) <b>data</b> entry, validation and cleaning. User acceptance was also evaluated in the two groups of participants: (1) data collectors and (2) survey participants. Results None of the errors presented in 20. 8...|$|R
40|$|Selective isotope {{labeling}} of methyl groups allows for atomic-resolution {{insight into the}} structures and dynamics of high molecular mass proteins by nuclear magnetic resonance (NMR) spectroscopy. Widespread application of the methodology has been limited due to the challenges and costs associated with assigning 1 H- 13 C resonances to particular amino acids within the protein. Here, I present a novel structure-based automatic assignment strategy, Methyl Assignment by Graph MAtching (MAGMA), which relates experimentally measured methyl-methyl connectivity (NOESY/DREAM) to inter-methyl distances extracted from a high-resolution protein structure. MAGMA features exact algorithms for graph-subgraph isomorphism and maximal common edge subgraph that can sample all theoretically possible methyl resonance assignments. MAGMA was applied to a benchmark of eight proteins for which high-resolution structures, NOESY or DREAM data, and methyl NMR assignments were available. On this benchmark, MAGMA provides a minimum of 31 % and a maximum of 89 % methyl residue assignments, with complete accuracy. On larger proteins in the benchmark MAGMA outperforms alternative automated methyl resonance assignment programs in both accuracy and coverage. I assessed the influence of different input structures on the accuracy of MAGMA assignments, and concluded that joining the assignments results from multiple structures guarantees accuracy {{in cases where the}} structural form in solution is unknown. Finally, I demonstrate the utility of MAGMA in two novel applications on molecular chaperones. In a ligand binding study involving the N-domain of human HSP 90 α, MAGMA confidently assigned 38 % of the input methyl residues when the results were joined over two protein conformations. MAGMA's assignments were then combined with inter-molecular (ligand-methyl) NOEs and HADDOCK to generate models of the protein-ligand complex. The generated models, while featuring the correct ligand binding site, were of lower quality (0. 5 - 2. 5 Ã higher ligand RMSD) when compared to models generated with previously published methyl assignments. Incorporating inter-molecular NOE restraints in the calculation enabled MAGMA to discriminate between certain ligand binding modes, and led to {{an increase in the number}} of confident assignments, demonstrating the potential for using such restraints in the future. Finally, a <b>generic</b> <b>data</b> <b>collection</b> protocol for MAGMA was established in a de novo assignment application on the 400 kDa archaeal small heat shock protein Hsp 16. 5. The protocol involves measurement of inter-methyl NOEs, determination of intra-residue NOEs between Leu-δ 1 /δ 2 and Val-γ 1 /γ 2 groups, and discrimination between Leu and Val resonances by preparation of an exclusively Val labelled protein sample. A high sparsity of inter-methyl NOE data obtained for this system led to predominantly ambiguous methyl resonance assignments, which can be used for guiding further experimental efforts. </p...|$|E
40|$|There {{seems to}} be paucity in the {{research}} into the <b>collection</b> of <b>data</b> for use in simulation. This is rather unfortunate since data quality and availability {{are two of the}} most challenging issues in many simulation projects. This paper discusses how simulation practitioners identify and collect data for simulation projects. The data was collected from ORH Ltd, a management consultancy company using participant observations method and an experiment using a mock case scenario. From the observation, we produce a <b>generic</b> <b>data</b> identification and <b>collection</b> method. The method is evaluated using a real project conducted for a UK Ambulance Service. The experiment reveals variations in the data identification process that are influenced by the role and the level of experience of modellers...|$|R
40|$|BackgroundStandardized schemas, databases, {{and public}} data {{repositories}} {{are needed for}} the studies of malaria vectors that encompass a remarkably diverse array of designs and rapidly generate large data volumes, often in resource-limited tropical settings lacking specialized software or informatics support. ResultsData from the majority of mosquito studies conformed to a <b>generic</b> schema, with <b>data</b> <b>collection</b> forms recording the experimental design, sorting of collections, details of sample pooling or subdivision, and additional observations. Generically applicable forms with standardized attribute definitions enabled rigorous, consistent data and sample management with generic software and minimal expertise. Forms use now includes 20 experiments, 8 projects, and 15 users at 3 research and control institutes in 3 African countries, resulting in 11 peer-reviewed publications. ConclusionWe have designed <b>generic</b> <b>data</b> schema {{that can be used}} to develop paper or electronic based <b>data</b> <b>collection</b> forms depending on the availability of resources. We have developed paper-based <b>data</b> <b>collection</b> forms {{that can be used to}} collect data from majority of entomological studies across multiple study areas using standardized data formats. Data recorded on these forms with standardized formats can be entered and linked with any relational database software. These informatics tools are recommended because they ensure that medical entomologists save time, improve data quality, and data collected and shared across multiple studies is in standardized formats hence increasing research outputs. Electronic supplementary materialThe online version of this article (doi: 10. 1186 /s 13029 - 016 - 0050 - 1) contains supplementary material, which is available to authorized users. 2016 - 03 - 28 T 00 : 00 : 00 Z 27022408 PMC 480902...|$|R
40|$|Corresponding author. One {{manifestation of}} {{globalization}} is medical tourism. As its implications remain largely unknown, we reviewed claimed benefits and risks. Driven by high health-care costs, long waiting periods, {{or lack of}} access to new therapies in developed countries, most medical tourists (largely from the United States, Canada, and Western Europe) seek care in Asia and Latin America. Although individual patient risks may be offset by credentialing and sophistication in (some) destination country facilities, lack of benefits to poorer citizens in developing countries offering medical tourism remains a <b>generic</b> equity issue. <b>Data</b> <b>collection,</b> measures, and studies of medical tourism all need to be greatly improved if countries are to assess better both the magnitude and potential health implications of this trade...|$|R
40|$|Abstract — Existing stream {{processing}} systems are optimized {{for a specific}} metric, which may limit their applicability to diverse applications and environments. This paper presents XFlow, a <b>generic</b> <b>data</b> stream <b>collection,</b> processing, and dissemination system that addresses this limitation efficiently. XFlow can express and optimize a variety of optimization metrics and constraints by distributing {{stream processing}} queries across a wide-area network. It uses metric-independent decentralized algorithms that work on localized, aggregated statistics, while avoiding local optima. To facilitate light-weight dynamic changes on the query deployment, XFlow relies on a loosely-coupled, flexible architecture consisting of multiple publish-subscribe overlay trees that can gracefully scale and adapt to changes to network and workload conditions. Based on the desired performance goals, the system progressively refines the query deployment, {{the structure of the}} overlay trees, as well as the statistics collection process. We provide an overview of XFlow’s architecture and discuss its decentralized optimization model. We demonstrate its flexibility and the effectiveness using real-world streams and experimental results obtained from XFlow’s deployment on PlanetLab. The experiments reveal that XFlow can effectively optimize various performance metrics in the presence of varying network and workload conditions. I...|$|R
40|$|Existing stream {{processing}} systems {{are designed for}} clustered deployments, and cannot adequately meet the scalability and adaptivity requirements of Internet-scale monitoring applications. Furthermore, these systems are optimized for a specific metric, which may limit their applicability to diverse applications and environments. This paper presents XFlow, a <b>generic</b> distributed <b>data</b> <b>collection,</b> processing, and dissemination system that addresses these limitations. XFlow can express and optimize a variety of global optimization metrics and constraints. It uses decentralized algorithms that work on localized, aggregated views of the system, while avoiding local optima. The system progressively refines the query deployment, {{the structure of the}} underlying overlay network, as well as the statistics collection process, based on the desired objectives. To facilitate light-weight dynamic changes, XFlow uses a publish-subscribe model to decouple sources and clients, as well as processing operators. The result is a loosely-coupled, flexible architecture consisting of multiple overlay trees that can gracefully scale and adapt to churn in system membership and workload. We provide an overview of XFlow’s architecture and discuss in detail its decentralized optimization model. We demonstrate the flexibility and the effectiveness of XFlow using real-world streams and experimental results obtained from deployment on PlanetLab. The experiments reveal that XFlow can effectively optimize various performance metrics in the presence of varying network and workload conditions. ...|$|R
40|$|The file {{attached}} to this record is the author's final peer reviewed version. The Publisher's final version can be found by following the DOI link. This paper proposes a <b>generic</b> task-driven <b>data</b> <b>collection</b> framework, named as Crowd Pic, for Mobile Crowd Photographing (MCP) - a widely used technique in crowd sensing. In order to meet diverse MCP application requirements (e. g. Spatio-temporal contexts, single or multiple shooting angles to a sensing target), a multifaceted task model with collection constraints is provided in Crowd Pic. Meanwhile, a pre-selection process is necessary to prevent mobile clients from uploading redundant pictures so as to reduce the overhead traffic and maintain the sensing quality. To address this issue, we developed a pyramid-tree (PTree) model which can select maximum diversified subset from the evolving picture streams based on multiple coverage requirements and constraints defined in MCP tasks by data requesters. Crowd sourcing-based and simulation-based methods are both {{used to evaluate the}} effectiveness, efficiency and flexibility of the proposed framework. The experimental results indicate that the PTree method can efficiently assess redundant pictures and effectively select minimal subset with high coverage from the streaming picture according to various coverage needs, and the whole framework is applicable {{to a wide range of}} use scenarios...|$|R
40|$|Research which {{integrates}} {{data from}} multiple data platforms must of course merge on samples processed in parallel on the platforms. However, exploiting the full biological {{significance of the}} data depends on merging on the respective features as well. The features have platform-specific biological identifiers, so identifier mapping is critical to this merging. The IdMappingRetrieval package allows initial acquisition of biological identifier mappings (ID maps) from online bioinformatics services, with caching in localdata repositories for subsequent fast retrieval. An ID map is a one-to-many map from one ID type (called the primary key) to another (called the secondary key). The services currently supported are NetAffx, DAVID, and Ensembl. The package employs a unified interface for accessing these services, so that the local repositories will be easy to create, update, and use. Aside from identifier maps themselves, the the service’s complete annotation data sets are also accessible through the same mechanism [...] Therefore, although ID mapping is the primary goal, secondarily the package performs as a <b>generic</b> annotation <b>data</b> <b>collection</b> tool if desired. The objects produced by this package are specifically suited {{for use by the}} package IdMappingAnalysis, currently in preparation for Bioconductor. The purpose of IdMappingAnalysis is to characterize and compare two or more ID maps...|$|R
40|$|This {{study is}} {{librarian}} analysis which collects {{the data from}} books and journal articles. Those articles and books chapters discuss about introductions {{in terms of their}} <b>generic</b> structures. The <b>data</b> <b>collections</b> were applied by reading, taking notes for their generic structures, and analyzing them in term of their generic structure models. Then, the data were reported using qualitative approach by describing their patterns. The results showed that there two big categories for their models, involving pattern models and free models. Pattern models dominated in this study because they have been applied seven experts, while free models have been applied by only two people. Therefore, this study suggests that if the journal, which you direct to publish your articles, doesn’t have pattern, you are recommended to apply the patterns as the following: CARS model from Swales, IPS model from Adnan (2011), Slatcher & Pennebaker, and Ball & Vincent...|$|R
40|$|Background and purpose: EpiData and Epi Info {{are often}} used {{together}} by public health agencies around the world, particularly in developing countries, {{to meet their needs}} of low-cost public health data management; however, the current open source data management technology lacks a mobile component {{to meet the needs of}} mobile public health data collectors. The goal of this project is to explore the opportunity of filling this gap through developing and trial of a personal digital assistant (PDA) based data collection/entry system. It evaluated whether such a system could increase efficiency and reduce data transcription errors for public surveillance <b>data</b> <b>collection</b> in developing countries represented by Fiji. Methods: A <b>generic</b> PDA-based <b>data</b> <b>collection</b> software eSTEPS was developed. The software and the data collected using it directly interfaces with EpiData. A field trial was conducted to test the viability of public health surveillance <b>data</b> <b>collection</b> using eSTEPS. The design was a randomised, controlled trial with cross-over design. 120 participants recruited from the Fiji School of Medicine were randomly assigned to be interviewed by one of six interviewers in one of the two ways: (1) paper-based survey followed by PDA survey and (2) PDA survey followed by paper-based survey. Data quality was measured by error rates (logical range errors/inconsistencies, skip errors, missing values, date or time field errors and incorrect data type). Work flow and cost were evaluated in three stages of the survey process: (1) preparation of <b>data</b> <b>collection</b> instrument, (2) <b>data</b> <b>collection</b> and (3) <b>data</b> entry, validation and cleaning. User acceptance was also evaluated in the two groups of participants: (1) data collectors and (2) survey participants. Results: None of the errors presented in 20. 8 % of the paper questionnaires was found in the data set collected using PDA. Sixty two percent of the participants perceived that the PDA-based questionnaire took less time to complete. Data entry, validation and cleaning for the PDA-based <b>data</b> <b>collection</b> from 120 participants took a total of 1. 5 hours, a 93. 26 % reduction of time from 20. 5 hours required using paper and pen. The cost is also significantly reduced with PDA-based protocol. Both data collectors and participants prefer to use PDA instead of paper for <b>data</b> <b>collection.</b> The trial results prove that eSTEPS is a feasible solution for public health surveillance <b>data</b> <b>collection</b> in the field. Several deficiencies of the software were also identified and would be addressed in the next version. Conclusion: eSTEPS offers the potential to meet the need for an effective mobile public health <b>data</b> <b>collection</b> tool for use in the field. The eSTEPS field trial proves that PDA was more efficient than paper for public health survey <b>data</b> <b>collection.</b> It also significantly reduced errors in data entry. The later benefit was derived from the software providing its users with the flexibility of building their own constraints to control the data type, range and logic of data entry...|$|R
40|$|The {{quest to}} provide {{computing}} services to resource-constrained environments {{in developing countries}} is becoming a reality due to the wide use of mobile phones and penetration of mobile networks. Nowadays, many organisations use Mobile <b>Data</b> <b>Collection</b> (MDC) tools to enable the collection and digitalisation of data at source, hence improving quality and increasing efficiencies. Mobile devices and environments present challenges to computing and application design {{that need to be}} overcome. Beyond mere digitalisation of data, MDC tools need to consider the process-related aspects of <b>data</b> <b>collection</b> used in paper-based routines expressed through paper trails. This lack of process-related support hinders the adoption of MDC routines in cases where great attention is paid to the <b>data</b> <b>collection</b> process. In conventional information systems, process-related features are implemented using workflows which may be embedded in an application or separately defined using Workflow Management Systems. This has {{led to the development of}} Process-Aware Information Systems (PAISs), which are software systems for managing and executing operational processes involving people, applications, and/or information sources on the basis of process models. PAISs facilitate the inclusion of processrelated activities which include the ordering of various tasks undertaken to achieve a business goal (control flow), the collaboration among various entities, and the allocation and provision as well the exchange of relevant information necessary for decision making. The use of mobile devices to carry out tasks is not the most preferable choice due to hardware limitations. Mobile-based systems should integrate with existing desktopbased solutions to provide a multiple access platform for work execution. This calls for integrating workflow systems with <b>generic</b> mobile <b>data</b> <b>collection</b> tools, which would require modifications in approach, methods and architecture to cater for device and environmental constraints in order to enable the mobile devices to be used appropriately. This thesis proposes a range of techniques that can be used to enable workflow support for mobile <b>data</b> <b>collection.</b> The overall goal is to minimise changes in workflow systems architecture, since these are based on widely agreed standards. Therefore, we propose an approach for online execution of work, for scenarios where network connection is readily available, and offline execution of work controlled by a workflow engine, when the connection is not available. A workflow adapter is proposed to enable matching of forms for <b>data</b> <b>collection</b> and workflow specifications. A distributed architecture for offline <b>data</b> <b>collection</b> based on partitioning a process model into fragments for distributed execution is also proposed. The methods proposed have been implemented with the OpenXdata MDC suite used for <b>data</b> <b>collection</b> and YAWL workflow management system. The OpenXdata and YAWL platforms adhere to commonly agreed standards for mobile <b>data</b> <b>collection</b> and workflow management and thus provide generalizable concepts within the domain of process-aware mobile <b>data</b> <b>collection.</b> Experiments were carried out on foundational concepts in order to determine that all relevant workflow-related constraints are observed. In addition, artefacts developed from the application of these methods were implemented in real life projects. The findings and results of these applications were used to validate the methods and frameworks suggested. </p...|$|R
40|$|This {{project is}} to provide the U. S. Army Corps of Engineers with a risk {{analysis}} that evaluates the non-routine closure of water flow through the turbines of powerhouses along the Columbia and Snake Rivers. The project is divided into four phases. Phase 1 efforts collected and analyzed relevant plant failure data for hydroelectric generating stations in the United States and Canada. Results from the Phase 1 efforts {{will be used to}} assess the risk (probability times consequences) associated with non-routine shut down of hydroelectric stations, which will be performed in the remaining phases of the project. Results of this project may be used to provide policy recommendations regarding operation and maintenance of hydroelectric stations. The methodology used to complete the Phase 1 of the project is composed of <b>data</b> <b>collection</b> and analysis activities. <b>Data</b> <b>collection</b> included performing site visits, conducting a data survey of hydroelectric stations, conducting an expert panel workshop, and reviewing and tabulating failure <b>data</b> from <b>generic</b> sources. <b>Data</b> analysis included estimating failure rates obtained from the survey data, expert judgment elicitation process, <b>generic</b> <b>data,</b> and combining these failure rates to produce final failure rate parameters. This paper summarizes the <b>data</b> <b>collection</b> analysis, results and discussions for the Phase 1 efforts...|$|R
40|$|High quality {{input data}} is a {{necessity}} for successful Discrete Event Simulation (DES) applications, and there are available methodologies for <b>data</b> <b>collection</b> in DES projects. However, in contrast to standalone projects, using DES as a day-to-day engineering tool requires high quality production data to be constantly available. Unfortunately, there are no detailed guidelines that describes how to achieve this. Therefore, this paper presents such a methodology, based on three concurrent engineering projects within the automotive industry. The methodology explains the necessary roles, responsibilities, meetings, and documents to achieve a continuous quality assurance of production data. It also specifies an approach to input data management for DES using the <b>Generic</b> <b>Data</b> Management Tool (GDM-Tool). The expected effects are increased availability of high quality production data and reduced lead time of input data management, especially valuable in manufacturing companies having advanced automated <b>data</b> <b>collection</b> methods and using DES on a daily basis...|$|R
40|$|Our aim is {{to detect}} {{mechanistic}} interaction between the effects of two causal factors on a binary response, {{as an aid to}} identifying situations where the effects are mediated by a common mechanism. We propose a formalization of mechanistic interaction which acknowledges asymmetries of the kind "factor A interferes with factor B, but not viceversa". A class of tests for mechanistic interaction is proposed, which works on discrete or continuous causal variables, in any combination. Conditions under which these tests can be applied under a <b>generic</b> regime of <b>data</b> <b>collection,</b> be it interventional or observational, are discussed in terms of conditional independence assumptions within the framework of Augmented Directed Graphs. The scientific relevance of the method and the practicality of the graphical framework are illustrated with the aid of two studies in coronary artery disease. Our analysis relies on the "deep determinism" assumption that there exists some relevant set V - possibly unobserved - of "context variables", such that the response Y is a deterministic function of the values of V and of the causal factors of interest. Caveats regarding this assumption in real studies are discussed. Comment: 20 pages including the four figures, plus two tables. Submitted to "Biostatistics" on November 24, 201...|$|R
40|$|An object-based {{software}} to support distributed databases and data reporting from combined fisheries and environmental surveys is presented. The key abstraction {{of this system}} is expressed as a <b>generic</b> station (<b>data)</b> object. The station object consists of a nested data structure, to store a master record and a matrix of data cycles, and a behavior to capture this data structure {{from a variety of}} input data formats. The station objects are {{not a part of the}} main system, but are implemented as separate runtime plug-ins or user-defined with XML scripts. This makes this system easily adaptable to particular <b>data</b> <b>collection</b> needs of a given survey. Once the data are stored in the database, they are accessible through a set of <b>generic</b> <b>data</b> protocols. Through these protocols, implemented with the COM technology, the data are easily integrated with the user-end software on Windows, including GIS and numerical computing environments. Keywords: Marine Data Management, Object-based technolog...|$|R
40|$|This paper {{addresses}} {{the problem of}} using proactive cryptosystems for <b>generic</b> <b>data</b> storage and retrieval. Proactive cryptosystems provide high security and confidentiality guarantees for stored data, and are capable of withstanding attacks that may compromise all the servers in the system over time. However, proactive cryptosystems are unsuitable for <b>generic</b> <b>data</b> storage uses for two reasons. First, proactive cryptosystems are usually used to store keys, which are rarely updated. On the other hand, <b>generic</b> <b>data</b> could be actively written and read. The system must therefore be highly available for both write and read operations. Second, existing share renewal protocols (the critical element to achieve proactive security) are expensive in terms of computation and communication overheads, and are time consuming operations. Since <b>generic</b> <b>data</b> will be voluminous, the share renewal process will consume substantial system resources and cause {{a significant amount of}} system downtime. Two schemes are proposed that combine Byzantine quorum systems and proactive secret sharing techniques to provide high availability and security guarantees for stored data, while reducing the overhead incurred during the share renewal process. Several performance metrics {{that can be used to}} evaluate proactively-secure <b>generic</b> <b>data</b> storage schemes are identified. The proposed schemes are thus shown to render proactive systems suitable for confidential <b>generic</b> <b>data</b> storage...|$|R
5000|$|<b>Generic</b> <b>data</b> {{structure}} for multi-layer network was introduced ...|$|R
40|$|Recent {{technology}} breakthroughs {{have enabled}} <b>data</b> <b>collection</b> of unprecedented scale, rate, variety and complexity {{that has led}} to an explosion in data management requirements. Existing theories and techniques are not adequate to fulfil these requirements. We endeav-our to rethink the way data management research is being conducted and we propose to work towards modular data management that will allow for unification of the expression of data management problems and systematization of their solution. The core of such an approach is the novel notion of a datom, i. e. a data management atom, which encapsulates <b>generic</b> <b>data</b> management provision. The datom is the foundation for comparison, customization and re-usage of data man-agement problems and solutions. The proposed approach can signal a revolution in data management research and a long anticipated evo-lution in data management engineering. 1...|$|R
5000|$|One {{approach}} to <b>generic</b> <b>data</b> modeling has the following characteristics: ...|$|R
30|$|L. van den Haak contributed the following: {{project development}}, <b>data</b> <b>collection,</b> <b>data</b> analysis, and {{manuscript}} writing. J.P.T. Rhemrev contributed the following: project development, <b>data</b> <b>collection,</b> <b>data</b> analysis, and manuscript writing. M.D. Blikkendaal contributed the following: <b>data</b> <b>collection,</b> <b>data</b> analysis, and manuscript writing. A.C.M. Luteijn contributed the following: <b>data</b> <b>collection,</b> and <b>data</b> analysis. J.J. van den Dobbelsteen contributed the following: project development and data analysis. S.R.C. Driessen contributed the following: <b>data</b> <b>collection,</b> <b>data</b> analysis, and manuscript writing. F.W. Jansen contributed the following: project development, <b>data</b> <b>collection,</b> <b>data</b> analysis, and manuscript writing.|$|R
5000|$|A {{reusable}} ALSP Interface {{consisting of}} <b>generic</b> <b>data</b> exchange message protocols; and ...|$|R
5000|$|<b>GENERIC</b> <b>data</b> types can be {{used for}} procedures, {{functions}} and abstract entities.|$|R
50|$|<b>Generic</b> <b>data</b> {{models are}} generalizations of {{conventional}} data models. They define standardized general relation types, {{together with the}} kinds of things that may be related by such a relation type. The definition of <b>generic</b> <b>data</b> model is similar to the definition of a natural language. For example, a <b>generic</b> <b>data</b> model may define relation types such as a 'classification relation', being a binary relation between an individual thing and a kind of thing (a class) and a 'part-whole relation', being a binary relation between two things, one with the role of part, the other with the role of whole, regardless the kind of things that are related.|$|R
40|$|The Phase 1 Characterization Sampling and Analysis Plan (CSAP) {{provides}} {{details about}} environmental <b>data</b> <b>collection</b> {{that will be}} taking place to support Phase 1 decommissioning activities described in the Phase 1 Decommissioning Plan for the West Valley Demonstration Project, Revision 2 (Phase I DP; DOE 2009). The four primary purposes of CSAP <b>data</b> <b>collection</b> are: (1) pre-design <b>data</b> <b>collection,</b> (2) remedial support, (3) post-remediation status documentation, and (4) Phase 2 decision-making support. <b>Data</b> <b>collection</b> to support these four main objectives is organized into two distinct <b>data</b> <b>collection</b> efforts. The first is <b>data</b> <b>collection</b> that will take place prior to the initiation of significant Phase 1 decommissioning activities (e. g., the Waste Management Area [WMA] 1 and WMA 2 excavations). The second is <b>data</b> <b>collection</b> that will occur during and immediately after environmental remediation in support of remediation activities. Both <b>data</b> <b>collection</b> efforts {{have a set of}} well-defined objectives that encompass the data needs of the four main CSAP <b>data</b> <b>collection</b> purposes detailed in the CSAP. The main body of the CSAP describes the overall <b>data</b> <b>collection</b> strategies that will be used to satisfy <b>data</b> <b>collection</b> objectives. The details of pre-remediation <b>data</b> <b>collection</b> are organized by WMA. The CSAP contains an appendix for each WMA that describes the details of WMA-specific pre-remediation <b>data</b> <b>collection</b> activities. The CSAP is intended to expand upon the <b>data</b> <b>collection</b> requirements identified in the Phase 1 Decommissioning Plan. The CSAP is intended to tightly integrate with the Phase 1 Final Status Survey Plan (FSSP). <b>Data</b> <b>collection</b> described by the CSAP is consistent with the FSSP where appropriate and to the extent possible...|$|R
