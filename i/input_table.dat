21|295|Public
5000|$|The E or Extension spec(s) are next, and {{describe}} arrays and tables, {{which may be}} prefetched from disk files (an <b>Input</b> <b>table),</b> drawn from constants placed {{at the end of}} the source between ** and /* symbols, or built from calculations.|$|E
5000|$|... #Caption: Maurice Wilkes (right) {{with the}} Meccano {{differential}} analyser in the Cambridge University Mathematics Laboratory, c1937. A.F. Devonshire (left) co-authored {{a number of}} papers on melting and disorder with the Laboratory's first director, John Lennard-Jones. The winner of the 1937 Mayhew Prize, J. Corner, is operating the <b>input</b> <b>table</b> (centre).|$|E
40|$|This {{document}} {{describes a}} convention for compressing FITS binary tables that is {{modeled after the}} FITS tiled-image compression method (White et al. 2009) {{that has been in}} use for about a decade. The <b>input</b> <b>table</b> is first optionally subdivided into tiles, each containing an equal number of rows, then every column of data within each tile is compressed and stored as a variable-length array of bytes in the output FITS binary table. All the header keywords from the <b>input</b> <b>table</b> are copied to the header of the output table and remain uncompressed for efficient access. The output compressed table contains the same number and order of columns as in the input uncompressed binary table. There is one row in the output table corresponding to each tile of rows in the <b>input</b> <b>table.</b> In principle, each column of data can be compressed using a different algorithm that is optimized for the type of data within that column, however in the prototype implementation described here, the gzip algorithm is used to compress every column. Comment: Proposed FITS Convention: [URL] v 1. 0, 28 October 2010, 6 page...|$|E
30|$|The {{rock and}} fluid {{properties}} are {{specified in the}} PROPS section. The term “fluid property” refers {{to a set of}} <b>input</b> <b>tables</b> that effectively defines the phase behavior of each phase. The term “rock property” refers to sets of <b>input</b> <b>tables</b> of relative permeability and capillary pressure versus saturation. Effectively, this defines the connect, critical, and maximum saturation of each phase, supplies information for defining the transition zone, and defines the conditions of flow of phases relative to one another. This strongly affects the ratios of produced phases, that is, water cuts and gas oil ratios (GORs). The REGIONS section subdivides the reservoir.|$|R
30|$|Finally, {{using the}} Expert Mode the user can edit <b>input</b> <b>tables</b> and/or the hypernet {{to control the}} {{impedances}} used in the model. The Expert Mode is an optional feature for advanced editing of the database values before running the model.|$|R
25|$|The app {{also allows}} users to <b>input</b> inline <b>tables.</b>|$|R
40|$|KLIFS KNIME nodes v 1. 0. 14 Some bug fixes, code cleaning, and {{inclusion}} of a testflow for all KLIFS KNIME nodes. Changed Added KNIME testflow Client is now based on ok[URL] libraries to prevent conflicts within KNIME Project is linked to Codacy and Travis-CI Fixed When and empty <b>input</b> <b>table</b> is provided the nodes will now return an emtpy list instead of polling the KLIFS webservice. Cleaned code based on Codacy flag...|$|E
40|$|Theoretical and {{practical}} aspects of classification systems and classification learning are considered. Analysis of subject area learning sets {{and analysis of}} classification schemes raises a number of nonstandard questions, such as relations between categorization/metadata and logic-combinatorial structuring/clustering of the descriptive part of the <b>input</b> <b>table.</b> Several such questions are treated in this paper. Software environment of testing and evaluation is the extended system PaGaNe, completed by research methodology of well known logic-combinatorial scheme of pattern recognition...|$|E
40|$|A 'MageBuilder ' object takes {{a set of}} 'MageMap' {{objects and}} a set of data streams as input, and {{produces}} a MAGEstk object representation, which is then serialized as MAGE-ML. A 'MageMap' object encapsulates the rules of how data records from an input stream relate to one MAGE object. Each input &quot;stream &quot; is an anonymous subroutine that supplies records whose fields represent columns in the <b>input</b> <b>table.</b> The input tables can be delimited text files, database queries, or essentially any source that can be coerced into a set of records with fixed fields...|$|E
5000|$|Join method: Given two {{tables and}} a join condition, {{multiple}} algorithms can produce the result {{set of the}} join. Which algorithm runs most efficiently depends on the sizes of the <b>input</b> <b>tables,</b> the number of rows from each table that match the join condition, and the operations required {{by the rest of}} the query.|$|R
40|$|A {{tool for}} {{managing}} {{large numbers of}} APEX runs, handling data input and output. Program Components: ACCESS Database: Contains <b>input</b> <b>tables</b> used by i_APEX to construct APEX runs and output tables to organize APEX output. Graphical User Interface: Allows for single runs and ranges of runs and permits editing of input data as well as selection of output variables and output files...|$|R
40|$|When a cube {{designer}} {{is working}} on an OLAP cube structure, several decisions should be made: � Cube granularity � Content and structure of dimensions � What data facts from <b>input</b> <b>tables</b> are relevant to your analysis and what calculations {{do you need to}} apply to this data. In this paper, we focus on the last decision. SAS OLAP Server provides various means to customize analysis of input data facts. Calculations can be created via...|$|R
40|$|This release fixes {{energy density}} output, minor openPMD issues, corrects a broken species {{manipulator}} to derive density weighted particle distributions, fixes a rounding issue in ionization routines {{that can cause}} simulation corruption for very small particle weightings and allows the moving window to start immediately with timestep zero. For ionization input, we now verify {{that the number of}} arguments in the <b>input</b> <b>table</b> matches the ion species' proton number. All fixes are backported to work in C++ 98 (and C++ 11) mode. Please refer to our ChangeLog for a full list of features, fixes and user interface changes. Thanks to Axel Huebl, René Widera, Richard Pausch, Alexander Debus, Marco Garten, Heiko Burau and Thomas Kluge for spotting the issues and providing fixes...|$|E
40|$|Monitoring of {{soil quality}} and health {{provides}} critical {{insights into the}} performance of ecosystems. Nematodes are useful indicators of soil condition because they are ubiquitous, represent different trophic levels of a soil food web and are convenient to work with. Several quantitative analyses of nematode assemblages have been developed and used in monitoring programs and by individual researchers. However, the calculations of the metrics involved are quite complicated. Since they are done manually using spreadsheet software, the calculations are time-consuming and error-prone and usually involve a significant learning curve for the user. We have developed an R code to perform these calculations. The code is compiled in html and deployed over the web. It is and will remain freely accessible and has a user-friendly interface. It requires only an <b>input</b> <b>table</b> with taxonomic inventory data and provides output within a few seconds...|$|E
40|$|Abstract—In this paper, we {{describe}} a NetFlow visualization tool, NVisionIP, which provides network administrators increased situational {{awareness of the}} state of networked devices within an IP address space. It does this by providing three increasingly detailed views {{of the state of}} devices in an entire IP address space to subnets to individual machines. Operators may use NVisionIP to transparently view NetFlow traffic without filtering or may selectively filter and interactively query NVisionIP for unique views given experience or relevant clues. the modules independently. The Data Retrieval Module reads in the NetFlow files, preprocesses them, and places them in a table structure for the Computation module to use. For every IP address in the <b>input</b> <b>table,</b> the Computation Module calculates various statistics as shown in Figure 2. These statistics are then passed to the Visualization Module that presents information to a user. Index Terms — NetFlows, Visualization, Network Security I...|$|E
30|$|The {{indicators}} of technical/technological performances for both ship categories and two ports have been already given as <b>inputs</b> in <b>Table</b> 1.|$|R
40|$|Until recently, {{the use of}} {{graphics}} processing units (GPUs) for query processing {{was limited}} {{by the amount of}} memory on the graphics card, a few gigabytes at best. Moreover, <b>input</b> <b>tables</b> had to be copied to GPU memory before they could be processed, and after computation was completed, query results had to be copied back to CPU memory. The newest generation of Nvidia GPUs and development tools introduces a common memory address space, which now al-lows the GPU to access CPU memory directly, lifting size limitations and obviating data copy operations. We confirm that this new technology can sustain 98 % of its nominal rate of 6. 3 GB/sec in practice, and exploit it to process database hash joins at the same rate, i. e., the join is processed“on the fly”as the GPU reads the <b>input</b> <b>tables</b> from CPU memory at PCI-E speeds. Compared to the fastest published results for in-memory joins on the CPU, this represents more than half an order of magnitude speed-up. All of our results include the cost of result materialization (often omitted in earlier work), and we investigate the implications of changing join predicate selectivity and table size. 1...|$|R
40|$|Spatial join is an {{important}} yet costly operation in spatial databases. In order {{to speed up the}} execution of a spatial join, the <b>input</b> <b>tables</b> are often indexed based on their spatial attributes. The quadtree index structure is a well-known index for organizing spatial database objects. It has been implemented in several database management systems, e. g., in Oracle Spatial and in PostgreSQL (via SP-GiST). Queries typically involve multiple pipelined spatial join operators that fit together in a query evaluation plan. In order to extend the applicability of these spatial joins, they are optimized so that upon receiving sorted input, they produce sorted output for the spatial join operators in the upperlevels of the query evaluation pipeline. This paper investigates the use of quadtree-based spatial join algorithms and how they can be adapted to answer queries that involve multiple pipelined spatial joins in a query evaluation plan. The paper investigates several adaptations to pipelined spatial join algorithms and their performance for the cases when both <b>input</b> <b>tables</b> are indexed, when only one of the tables is indexed while the second table is sorted, and when both tables are sorted but are not indexed...|$|R
40|$|Every day, {{millions}} of computer end-users need to perform tasks over large, tabular data, yet lack the programming knowledge {{to do such}} tasks automatically. In this work, we present an automatic technique that takes from a user {{an example of how}} the user needs to transform a table of data, and provides to the user a program that implements the transformation described by the example. In particular, we present a language of programs TableProg that can describe transformations that real users require. We then present an algorithm ProgFromEx that takes an example input and output table, and infers a program in TableProg that implements the transformation described by the example. When the program is applied to the example input, it reproduces the example output. When the program is applied to another, potentially larger, table with a “similar” layout as the example <b>input</b> <b>table,</b> then the program produce...|$|E
40|$|The {{purpose of}} this paper is to give a brief {{overview}} of TabVer, a front-end component that verbalizes large tables generated by a commercial expert system. TabVer's aim is to highlight the essential information that is contained in possibly very large tables by generating a small text. This text gives a short summary of appropriate communicative and pragmatic acceptability, supresses all unneccessary information and also provides some additional information, which is not directly expressed in the table. The output text is generated from basic templates by a ‘planner” which operates on a special plan language according to the A*-algorithm. All basic and higher order templates are encoded in this language, which is a cost-based version of Prolog. Using all these plans (including control plans), the planner searches for the best way to verbalize the <b>input</b> <b>table</b> and (via A*) finds the optimal method first...|$|E
40|$|The Best Estimate Flux {{value-added}} product (VAP) processes data started on March 22, 1997, when {{data from the}} three central facility (CF) radiometer systems, Solar Infrared Station (SIRS) E 13, C 1, and baseline surface radiation network (BSRN) (sgpsirs 1 duttE 13. c 1, sgpsirs 1 duttC 1. c 1, and sgpbsrn 1 duttC 1. c 1), were all available. In 2001, the diffuse shortwave (SW) instruments were switched to shaded black and white instruments, and the name BSRN was switched to broadband radiometer station (BRS). Before that time, this VAP uses corrected diffuse SW from the DiffCorr 1 Dutt VAP as <b>input.</b> <b>Table</b> 1 lists the fields being calculated in this VAP and the input platforms involved. The 1 -minute input data are compared to decide which {{will be used for}} averaging to get the best estimate. The output data are saved in two NetCDF files containing the best estimate values, quality control (QC) flags, and the difference fields. Figure 1 shows the beflux 1 long VAP logic flow. Table 1. Best estimate fields and input platforms...|$|E
40|$|Abstract In this paper, we {{introduce}} {{a concept of}} Annotation Based Query Answer, and a method for its computation, which can answer queries on relational databases that may violate a set of functional dependencies. In this approach, inconsistency {{is viewed as a}} property of data and described with annotations. To be more precise, every piece of data in a relation can have zero or more annotations with it and annotations are propagated along with queries from the source to the output. With annotations, inconsistent data in both <b>input</b> <b>tables</b> and query answers can be marked out but preserved, instead of being filtered in most previous work. Thus this approach can avoid information loss, a vital and common deficiency of most previous work in this area. To calculate query answers on an annotated database, we propose an algorithm to annotate the <b>input</b> <b>tables,</b> and redefine the five basic relational algebra operations (selection, projection, join, union and difference) so that annotations can be correctly propagated as the valid set of functional dependency changes during query processing. We also prove the soundness and completeness of the whole annotation computing system. Finally, we implement a prototype of our system, and give some performance experiments, which demonstrate that our approach is reasonable in running time, and excellent in information preserving...|$|R
40|$|We {{describe}} a technique and a tool called Qex for generating <b>input</b> <b>tables</b> and parameter values {{for a given}} parameterized SQL query. The evaluation semantics of an SQL query is translated into a specific background theory for a satisfiability modulo theories (SMT) solver {{as a set of}} equational axioms. Symbolic evaluation of a goal formula together with the background theory yields a model from which concrete tables and values are extracted. We use the SMT solver Z 3 in the concrete implementation of Qex and provide an evaluation of its performance. ...|$|R
40|$|GENRFQ is a {{computer}} code {{for the design of}} Radio-Frequency-Quadrupole accelerators and is used to prepare input files for the multiparticle tracking code PARMTEQ. The original GENRFQ was written by Yamada at LBL. This note describes the version of GENRFQ presently used at TRIUMF, and its use in a VAX/VMS environment. TRIUMF GENRFQ – code outline The Fortran program GENRFQ generates <b>input</b> <b>tables</b> for the particle tracking code PARMTEQ. The RFQ design is determined when three independent functions a(z), m(z) and φs(z) are given. GENRFQ,as its output,produces tables of these functions suitable for PARMTEQ. Sometime...|$|R
40|$|Spatial {{data mining}} {{requires}} {{the analysis of}} the interactions in space. These interactions can be materialized using distance tables, reducing spatial data mining to multi-table analysis. However, conventional data mining algorithms consider only one <b>input</b> <b>table</b> where each row is an observation to analyze. Simple relational joins between these tables does not resolve the problem and mislead the results because of the multiple counting of observations. We propose three alternatives of multi-table data mining in the context of spatial data mining. The first makes a hard modification in the conventional algorithm in order to consider those tables. The second is an optimization of the first approach. It pre-computes all join operations and adapts the conventional algorithm. The third re-organizes data into a unique table by completing -not joining- the target table using the existing data in the other tables, then applies any standard data mining algorithm without modification. This article presents these three alternatives. It describes their implementation for classification algorithms and compares their performances. Pages: 127 - 14...|$|E
40|$|Abstract. This Big Data Track {{submission}} {{demonstrates how}} the BTC 2014 dataset, Microdata annotations {{from thousands of}} websites, as well as millions of HTML tables are used to extend local tables with additional columns. Ta-ble extension is a useful operation within {{a wide range of}} application scenarios: Imagine you are an analyst having a local table describing companies and you want to extend this table with the headquarter of each company. Or imagine you are a film enthusiast and want to extend a table describing films with attributes like director, genre, and release date of each film. The Mannheim Search Joins Engine automatically performs such table extension operations based on a large data corpus gathered from over a million websites that publish structured data in various formats. Given a local table, the Mannheim Search Joins Engine searches the corpus for additional data describing the entities of the <b>input</b> <b>table.</b> The dis-covered data are then joined with the local table and their content is consolidated using schema matching and data fusion methods. As result, the user is presented with an extended table and given the opportunity to examine the provenance o...|$|E
40|$|Apatite (U–Th) /He and fission-track dates, {{as well as}} ^ 4 He/^ 3 He and fission-track length data, provide rich {{thermal history}} information. However, {{numerous}} choices and assumptions are required on the long road from raw data and observations to potentially complex geologic interpretations. This paper outlines a conceptual framework for this path, {{with the aim of}} promoting a broader understanding of how thermochronologic conclusions are derived. The tiered structure consists of thermal history model inputs at Level 1, thermal history model outputs at Level 2, and geologic interpretations at Level 3. Because inverse thermal history modeling {{is at the heart of}} converting thermochronologic data to interpretation, for others to evaluate and reproduce conclusions derived from thermochronologic results it is necessary to publish all data required for modeling, report all model inputs, and clearly and completely depict model outputs. Here we suggest a generalized template for a model <b>input</b> <b>table</b> with which to arrange, report and explain the choice of inputs to thermal history models. Model inputs include the thermochronologic data, additional geologic information, and system- and model-specific parameters. As an example we show how the origin of discrepant thermochronologic interpretations in the Grand Canyon can be better understood by using this disciplined approach...|$|E
40|$|Efficient {{processing}} of skyline queries {{has been an}} area of growing interest. Most existing techniques assume that the skyline query is applied to a single data table. Unfortunately, {{this is not true}} in many applications where, due to the complexity of the schema, the skyline query may involve attributes belonging to multiple tables. Recently, various hybrid skyline-join algorithms have been proposed. However, the current proposals suffer from several drawbacks: they often need to scan the <b>input</b> <b>tables</b> exhaustively in order to obtain the set of skyline-join results; moreover, the pruning techniques employed to eliminate the tuples are largely based on expensive pairwise tupleto-tuple comparisons. In this paper, we aim to address these shortcomings by proposing two novel skyline-join algorithms, namely skyline-sensitive join (S 2 J) and symmetric skyline-sensitive join (S 3 J), to process skyline queries over multiple tables. Our approaches compute the results using a novel layer/region pruning technique (LR-pruning) that prunes the join space in blocks as opposed to individual data points, thereby avoiding excessive pairwise point-topoint dominance checks. Furthermore, the S 3 J algorithm utilizes an early stopping condition in order to successfully compute the skyline results by accessing only a subset of the <b>input</b> <b>tables.</b> We report extensive experimental results that confirm the advantages of the proposed algorithms over the state-of-the-art skyline-join techniques...|$|R
40|$|In {{the field}} of radiotherapy, Monte Carlo (MC) {{particle}} transport calculations are recognized for their superior accuracy in predicting dose and fluence distributions in patient geometries compared to analytical algorithms which are generally used for treatment planning due to their shorter execution times. In this work, a newly developed MC-based treatment planning (MCTP) tool for proton therapy is proposed to support treatment planning studies and research applications. It allows for single-field and simultaneous multiple-field optimization in realistic treatment scenarios {{and is based on}} the MC code FLUKA. Relative biological effectiveness (RBE) -weighted dose is optimized either with the common approach using a constant RBE of 1. 1 or using a variable RBE according to radiobiological <b>input</b> <b>tables.</b> A validated reimplementation of the local effect model was used in this work to generate radiobiological <b>input</b> <b>tables.</b> Examples of treatment plans in water phantoms and in patient-CT geometries together with an experimental dosimetric validation of the plans are presented for clinical treatment parameters as used at the Italian National Center for Oncological Hadron Therapy. To conclude, a versatile MCTP tool for proton therapy was developed and validated for realistic patient treatment scenarios against dosimetric measurements and commercial analytical TP calculations. It is aimed to be used in future for research and to support treatment planning at state-of-the-art ion beam therapy facilities. © 2013 Institute of Physics and Engineering in Medicine...|$|R
2500|$|Additionally, the {{following}} special characters {{are not allowed}} in the first, fourth, fifth and eight character of a filename, as they conflict with the host command processor (HCP) and <b>input</b> sequence <b>table</b> build file names: ...|$|R
30|$|We consider, in this paper, a graph pattern p to be a feature, {{a set of}} graph {{patterns}} in a generation to be a set of features, and a positive or negative of an example to be a class label C= 1 or 0, respectively. If a pattern p matches an example, then we set the value of feature p as 1, otherwise 0. Table  2 shows an <b>input</b> <b>table</b> to CWC, obtained by the matching relation between bpo-graph patterns and outerplanar graphs. For feature F_i which corresponds to a graph pattern p and a variable C for class labels which corresponds to a finite set D {{of positive and negative}} graph data, we use the correlation measures: Mutual Information MI (F_i, C) (denoted by MI (p)), Symmetric Uncertainty SU (F_i, C) (denoted by SU (p)), Inconsistency Rate ICR (F_i, C) (denoted by ICR (p)), and Matthews Correlation Coefficient MCC (F_i, C) (which coincides with MCC (p). We define ICR ^*(p)= 1 -ICR (p). Also we define MCC ^*(p)=(MCC (p)+ 1)/ 2, according to [10]. For binary classification by a graph pattern p, the values of MI (p), SU (p), ICR ^*(p) and MCC ^*(p) are between 0 and 1. The value MI (p), SU (p), ICR ^*(p), MCC ^*(p) of 1 means that the correlation of a graph pattern p and a variable C for class labels is maximum.|$|E
40|$|We {{consider}} {{the problem of}} constructing decision trees for entity identification from a given relational table. The input is a table containing information about a set of entities over a fixed set of attributes and a probability distribution over the set of entities that specifies {{the likelihood of the}} occurrence of each entity. The goal is to construct a decision tree that identifies each entity unambiguously by testing the attribute values such that the average number of tests is minimized. This classical problem finds such diverse applications as efficient fault detection, species identification in biology, and efficient diagnosis in the field of medicine. Prior work mainly deals with the special case where the <b>input</b> <b>table</b> is binary and the probability distribution over the set of entities is uniform. We study the general problem involving arbitrary input tables and arbitrary probability distribution over the set of entities. We consider a natural greedy algorithm and prove an approximation guarantee of O(rK · log N), where N is the number of entities, K is the maximum number of distinct values of an attribute, and rK is a suitably defined Ramsey number. In addition, our analysis indicates a possible way of resolving a Ramsey theoretic conjecture by Erdös. We also show that it is NP-hard to approximate the general version of the problem within a factor of Ω(log N) ...|$|E
40|$|The {{potential}} {{exists in}} a nuclear reactor core melt severe accident for molten core debris to be dispersed under high pressure into the containment building. If this occurs, the set of phenomena that result in the transfer of energy to the containment atmosphere and its surroundings {{is referred to as}} direct containment heating (DCH). Because of the potential for DCH to lead to early containment failure, the U. S. Nuclear Regulatory Commission (USNRC) has sponsored an extensive research program consisting of experimental, analytical, and risk integration components. An important element of the analytical research has been the development and assessment of direct containment heating models in the CONTAIN code. This report documents the DCH models in the CONTAIN code. DCH models in CONTAIN for representing debris transport, trapping, chemical reactions, and heat transfer from debris to the containment atmosphere and surroundings are described. The descriptions include the governing equations and input instructions in CONTAIN unique to performing DCH calculations. Modifications made to the combustion models in CONTAIN for representing the combustion of DCH-produced and pre-existing hydrogen under DCH conditions are also described. <b>Input</b> <b>table</b> options for representing the discharge of debris from the RPV and the entrainment phase of the DCH process are also described. A sample calculation is presented to demonstrate the functionality of the models. The results show that reasonable behavior is obtained when the models are used to predict the sixth Zion geometry integral effects test at 1 / 10 th scale...|$|E
40|$|The {{purpose of}} this {{calculation}} is to document the creation of. <b>tables</b> for <b>input</b> into Integrated Probabilistic Simulator for Environmental Systems (RIP) version 5. 19. 01 (Golder Associates 1998) from Waste Package Degradation (WAPDEG) version 3. 09 (CRWMS M&O 1998 b. ''Software Routine Report for WAPDEG'' (Version 3. 09)) simulations. This calculation details {{the creation of the}} RIP <b>input</b> <b>tables</b> (representing waste package corrosion degradation over time) for the License Application Design Selection (LADS) analysis of the effects of continuous pre-closure ventilation. Ventilation during the operational phase of the repository could remove considerable water from the system, as well as reduce temperatures. Pre-closure ventilation is LADS Design Feature 7...|$|R
40|$|This paper {{presents}} a multimedia join operator that {{is carried out}} through the method of the nearest neighbor search. In contrast to related approaches that utilizes a similarity function to perform a join between two instances of the <b>input</b> <b>tables,</b> we adopt the more flexible and widely used nearest neighbor method. First, we introduce a simple nearest neighbor search algorithm based on an nested-loop execution strategy, second an optimized version is proposed which takes advantage of query point clustering in a hypersphere. Several experiments are performed to demonstrate the e#ciency of the optimized algorithm over the simple one for different datasets, datasizes and dimensions. Key words: Multimedia Databases, Processing a Multimedia Joins, Nearest Neighbor Search...|$|R
40|$|The present work aims {{to propose}} a {{systematic}} study and interpretation of a variable response in relation to three factors, using a model of Joint Table Analysis, the Tucker 3 model, {{as well as the}} joint biplot graph. The proposed method seems efficient and suitable for separating standard technical response, and the pattern of noise contained in a three <b>inputs</b> <b>table,</b> as well as allows its interpretation. The joint plot graph facilitates the study and interpretation of the data structure and provides additional information on these. In our application the aim is to identify the combinations of genotypes, locations and years that contribute or not to a high yield of bean cultivars...|$|R
