66|110|Public
50|$|LMS, {{sometimes}} implemented {{as a part}} of a major {{network management}} system, views the local loop as an active network element. It speeds pre-qualification and reduces <b>fault</b> <b>correction</b> time.|$|E
50|$|MHI employs new {{production}} {{methods such as}} integral wing stringers, unusually tight tolerances, shot peening of curved surfaces, and {{vacuum assisted resin transfer}} molding, intended to increase quality and thus reduce expensive <b>fault</b> <b>correction</b> to keep price competitive.|$|E
5000|$|Reduction of {{priority}} of <b>fault</b> <b>correction.</b> Even if the operator {{is aware of}} the fault, having a fault-tolerant system is likely to reduce the importance of repairing the fault. If the faults are not corrected, this will eventually lead to system failure, when the fault-tolerant component fails completely or when all redundant components have also failed.|$|E
50|$|Beyond {{the many}} {{technical}} innovations in the MCP design, the Burroughs Large Systems had many management innovations {{now being used}} by the internet community at large. The system software was shipped to customers inclusive of source code and all the editing and compilation tools to generate new versions of MCP for customers. Many customers developed niche expertise on {{the inner workings of the}} MCP, and customers often sent in the 'patches' (fragment pieces of source code with sequence numbers) as suggestions of new enhanced features or <b>fault</b> <b>corrections</b> (FTR - field trouble reports). Many of the suggested patches were included by the systems developers and integrated into the next version of the MCP release. Including a community of voluntary, self-professed experts, into mainstream technical work, is now widely practised and is the essence of Open Innovation. This management innovation of community development dated back to the '70s.|$|R
40|$|In {{software}} development life cycle (SDLC) testing {{is very important}} step. One of the key elements of software quality is testing. Fault detection and removal process is also very important when we are doing testing. In the last 30 years numerous software reliability growth models where developed for <b>fault</b> detection and <b>correction</b> process. Majority of models where developed under static condition. The main goal {{of this article is}} to examine the resource allocation plan for <b>fault</b> detection and <b>correction</b> process of the software to control the cost during testing and operational phase. For achieving this we developed a model for <b>fault</b> detection and <b>correction</b> process. For solving this model we use Pontryagain’s Maximum principle. For optimally resource allocation we use Differential Evolution (DE). Differential Evolution (DE) is an optimization algorithm. A numerical example is also explained for resource allocation for fault detection and removal process...|$|R
40|$|An {{empirical}} {{study on}} estimation and prediction of faults, prediction of <b>fault</b> detection and <b>correction</b> effort, and reliability assessment in the Software Engineering Laboratory environment (SEL) is presented. Fault estimation using empirical relationships and fault prediction using curve fitting method are investigated. Relationships between debugging efforts (<b>fault</b> detection and <b>correction</b> effort) in different test phases are provided, {{in order to}} make an early estimate of future debugging effort. This study concludes with the fault analysis, application of a reliability model, and analysis of a normalized metric for reliability assessment and reliability monitoring during development of software...|$|R
50|$|In {{software}} development, test effort {{refers to}} the expenses for (still to come) tests. There is a relation with test costs and failure costs (direct, indirect, costs for <b>fault</b> <b>correction).</b> Some factors which influence test effort are: maturity of the software development process, quality and testability of the testobject, test infrastructure, skills of staff members, quality goals and test strategy.|$|E
40|$|This {{paper is}} a {{continuation}} of the work reported in [SCH 01]. We found that {{the most important factor in}} <b>fault</b> <b>correction</b> (i. e., maintenance action) modeling is the delay between failure detection and <b>fault</b> <b>correction.</b> We modeled this factor, using prediction equations and the concept of a <b>fault</b> <b>correction</b> queuing system, with exponentially distributed delay. This distribution was a highly statistically significant empirical result for the NASA Space Shuttle. In some cases, <b>fault</b> <b>correction</b> is not performed immediately upon detecting a failure. For example, the Shuttle software developer might postpone <b>fault</b> <b>correction</b> because the failures were classified as non-critical on the current release and were not considered critical for one to three releases in the future. In contrast, our original model was based on keeping the software updated with corrections in the current release. As part of this research, we will investigate whether fault corrections are postponed for the NASA Goddard Space Flight Center (GSFC) <b>fault</b> <b>correction</b> process and, if so, whether we can model this delay. Our approach to solving this problem is to modify and augment the model in [SCH 01] to allow for both the delay of correcting a fault, once the correction process has started, and the delay due to postponing the start of <b>fault</b> <b>correction.</b> Because there are now two components to the wait time in the revised <b>fault</b> <b>correction</b> model, we provide new and modified prediction equations and a revised maintenance action queuing system (see Figure 1) to accommodate these change...|$|E
40|$|In this paper, we {{investigate}} software fault detection and <b>fault</b> <b>correction</b> processes based on infinite server queuing model which incorporate testing effort functions. Some researches {{proposed in the}} literature to study fault detection and <b>fault</b> <b>correction</b> processes. However, {{most of them do}} not consider the amount of resources consumed during fault detection and <b>fault</b> <b>correction</b> processes. The consumption amount of resources is usually depicted by testing effort functions which can largely influence fault detection speed and the time to correct a fault detected. Therefore, we will show that new models incorporate testing effort functions into the fault detection and <b>fault</b> <b>correction</b> processes. In additional, we study how to use queuing models to explain the fault detection and <b>fault</b> <b>correction</b> processes during software development. Parameters are estimated and experiments on actual fault data sets are illustrated. The results show that the proposed models in this paper can estimated the number of initial faults better than the model without testing effort functions. ...|$|E
40|$|This paper {{presents}} a possible approach {{for developing a}} violin teaching aid based on violin pedagogy, sound analysis and comparison of beginner and good player recordings. This teaching aid is targeted at students who have difficulty listening attentively to the sounds they produce. It aims to draw {{their attention to the}} sound of a <b>fault,</b> offer <b>correction</b> and to train the user 2 ̆ 7 s ear to actively listen...|$|R
500|$|Oldenburg was {{intended}} to be a fifth member of the [...] of sortie corvettes. The design for the ship was radically altered, between 1879 and 1881, for a variety of reasons. The German Navy was largely dissatisfied with the Sachsen class ships, and a number of design <b>faults</b> required <b>correction.</b> Budgetary constraints also limited the design of the ship, forcing the design staff to work within a displacement some [...] lower than that of the Sachsens.|$|R
40|$|The {{study of}} {{operator}} performance in manual and automated versions of dynamic decision tasks is proposed. The two microcomputer paradigms of simple and complex, dynamic scheduling tasks are described. Error detection accuracy and latency of assignment, and <b>fault</b> detection and <b>correction</b> {{for the two}} tasks are to be analyzed...|$|R
40|$|Context. <b>Fault</b> <b>correction</b> {{process is}} one of the two main {{activities}} in software evolution model. As it is very important for software maintainability, software industry especially large-scaled global companies, aim to have mature <b>fault</b> <b>correction</b> processes that detect faults and correct them in a continuous and efficient way. Considerable amount of effort is needed and some measures should be taken in order to be successful. This master thesis is mainly related with <b>fault</b> <b>correction</b> and finding possible solutions for better process. Objectives. The main aim {{of this study is to}} investigate and identify influential factors having affects over undesired <b>fault</b> <b>correction</b> outcomes. This study has three main stages: 1) to identify factors from company data that have affects over target factors, 2) to elicit influential factors from interviews and literature review, 3) to prioritize influential factors based on their significance. Based on the outcomes, giving recommendations to company and software industry is the other aim of this master thesis. Methods. This study mainly reflects the empirical research of software <b>fault</b> <b>correction</b> process and undesired outcomes of it. In this master thesis, both quantitative and qualitative data analysis were performed. Case study was conducted with Ericsson AB that data analysis was made with the archival data by using several methods including Machine Learning and Apriori. Also, surveys and semi-structured interviews were used for data collection instruments. Apart from this, literature review was performed in order to collect influential factors for <b>fault</b> <b>correction</b> process. Prioritization of the influential factors was made by using hierarchical cumulative voting. Results. Throughout the case study, quantitative data analysis, interviews and literature review was conducted and totally 45 influential factors were identified. By using these factors prioritization was performed with 26 practitioners (4 internal and 22 external) in order to find which factors are most a) significant and b) relevant in undesired <b>fault</b> <b>correction</b> outcomes. Based on the outcomes of prioritization, cause-effect diagram was drawn which includes all the important factors. Conclusions. This research showed that there are lots of factors influencing <b>fault</b> <b>correction</b> process. The practitioners mostly complained about the lack of analysis of deeply including correction of faults are not resulted the new requirements and they are not used for process improvement. Also, limited resources (such as work force, vacations and sickness), unbalanced <b>fault</b> <b>correction</b> task assignment and too much fault reports at the same time cause problems. Moreover, priorities of faults and customers affect the lead time of <b>fault</b> <b>correction</b> process as the critical faults are fixed at first. + 90 533 769878...|$|E
40|$|Existing input domain-based {{reliability}} models do {{not account}} for software reliability growth, {{because they do not}} consider fault corrections. This paper proposes an input domain-based reliability growth model with <b>fault</b> <b>correction</b> history being taken into account. Both partition and random testing can be used to generate input cases for test runs. It is generally considered in the model that input case generation, fault detection and <b>fault</b> <b>correction</b> can all be imperfect. As an application of the proposed input domain-based reliability growth model, the efficiency of random and partition testing in terms of reliability growth is studied and compared analytically. The impacts on the efficiency of testing strategies due to the number of faults in the program, the distribution of fault in the input domain of the program, as well as the imperfections of input case generation, fault detection and <b>fault</b> <b>correction</b> are studied. Through sophisticated analysis we obtain some new results which [...] ...|$|E
40|$|Since both cost/quality and {{production}} environments differ, this study presents an approach for customizing a characteristic set of software metrics to an environment. The approach is {{applied in the}} Software Engineering Laboratory (SEL), a NASA Goddard production environment, to 49 candidate process and product metrics of 652 modules from six (51, 000 to 112, 000 lines) projects. For this particular environment, the method yielded the characteristic metric set (source lines, <b>fault</b> <b>correction</b> effort per executable statement, design effort, code effort, number of I/O parameters, number of versions). The uses examined for a characteristic metric set include forecasting the effort for development, modification, and <b>fault</b> <b>correction</b> of modules based on historical data...|$|E
30|$|Robotics and sensors, {{together}} with their associated control systems have become important elements in industrial manufacturing. They offer several advantages, such as improved weld quality, increased productivity, reduced weld costs, increased repeatable consistency of welding, and minimized human input for selection of weld parameters, path of robotic motion, and <b>fault</b> detection and <b>correction.</b>|$|R
50|$|The {{aim of the}} {{institution}} is to impart religious, mental, moral, social and physical information based {{on the principles of}} Christian faith, to members of the Christian community. The method of education followed in the school was practiced by St. John Bosco, based on fear of God, on conviction, prevention of <b>faults</b> and paternal <b>correction.</b>|$|R
30|$|If we {{estimate}} the station corrections from PGVobs/PGV 700 exp, including {{the data from}} stations at large distances from the <b>fault,</b> the station <b>corrections</b> of the latter may be larger than the real values. Therefore, we excluded the data of larger fault distances at sites at which observed seismic intensity is < 2.5 (Fig. 4). We used earthquakes that had at least five seismic intensity observations.|$|R
40|$|Abstract: In this paper, the physical-layer {{redundancy}} {{method is}} {{proposed for the}} fault-tolerant industrial network. The proposed method consists of the fault detection method and the correction method. The fault detection method uses events created by the state transition in the IEEE 802. 4 MAC sublayer and the periodic status frame check {{for the detection of}} the fault. The <b>fault</b> <b>correction</b> method corrects the fault with the automatic physical layer switching to the stand-by physical layer due to the event created by the fault detection method. The proposed method is realized with dual physical layers, the Dual Channel Manager for switching and the Redundancy Management Module that has the Fault Detection Sub-module and the <b>Fault</b> <b>Correction</b> Sub-module. As results of the practical implementation, the proposed method guarantees high reliablility and fast fault-correction in PICNET...|$|E
40|$|Code reuse is good, e. g., “modules reused without {{revision}} had {{the fewest}} faults, fewest faults per source line, and lowest <b>fault</b> <b>correction</b> effort ” [2] Codebase defines {{the organization and}} the market for some legacy systems Open source code ✧ A vehicle for innovation through reuse ✧ A common platform for multiple participant...|$|E
40|$|Design, {{evaluation}} and demonstration of advanced instrumentation concepts for improving performance of manned spacecraft environmental control and {{life support systems}} were successfully completed. Concepts to aid maintenance following fault detection and isolation were defined. A computer-guided <b>fault</b> <b>correction</b> instruction program was developed and demonstrated in a packaged unit which also contains the operator/system interface...|$|E
50|$|Qorivva {{is a line}} of Power Architecture-based {{microcontrollers}} from Freescale {{built around}} one or more Power Architecture e200 cores. Within this line {{are a number of}} products specifically targeted for Functional Safety applications. The hardware-based <b>fault</b> detection and <b>correction</b> features found within this line include dual cores that may run in lock-step, full path ECC, automated self-testing of memory and logic, peripheral redundancy, and monitor/checker cores.|$|R
40|$|We {{present the}} Telecommunications {{protocol}} processing subsystem using Reconfigurable Interoperable Gate Arrays (TRIGA), {{a novel approach}} that unifies <b>fault</b> tolerance, error <b>correction</b> coding and interplanetary communication protocol off-loading to implement CCSDS File Delivery Protocol and Datalink layers. The new reconfigurable architecture offers more than one order of magnitude throughput increase while reducing footprint requirements in memory, command and data handling processor utilization, communication system interconnects and power consumption...|$|R
40|$|We {{provide a}} paper-and-pencil {{specification}} of a benchmark suite for computational grids. It {{is based on}} the NAS (NASA Advanced Supercomputing) Parallel Benchmarks (NPB) and is called the NAS Grid Benchmarks (NGB). NGB problems are presented as data flow graphs encapsulating an instance of a slightly modified NPB task in each graph node, which communicates with other nodes by sending/receiving initialization data. Like NPB, NGB specifies several different classes (problem sizes). In this report we describe classes S, W, and A, and provide verification values for each. The implementor has the freedom to choose any language, grid environment, security model, <b>fault</b> tolerance/error <b>correction</b> mechanism, etc., as long as the resulting implementation passes the verification test and reports the turnaround time of the benchmark...|$|R
40|$|Fault {{detection}} process (FDP) and <b>Fault</b> <b>correction</b> process (FCP) {{are important}} phases of software development life cycle (SDLC). It {{is essential for}} software to undergo a testing phase, during which faults are detected and corrected. The main goal {{of this article is}} to allocate the testing resources in an optimal manner to minimize the cost during testing phase using FDP and FCP under dynamic environment. In this paper, we first assume there is a time lag between fault detection and <b>fault</b> <b>correction.</b> Thus, removal of a fault is performed after a fault is detected. In addition, detection process and correction process are taken to be independent simultaneous activities with different budgetary constraints. A structured optimal policy based on optimal control theory is proposed for software managers to optimize the allocation of the limited resources with the reliability criteria. Furthermore, release policy for the proposed model is also discussed. Numerical example is given in support of the theoretical results...|$|E
40|$|AbstractOver {{the past}} three decades, many {{software}} reliability growth models (SRGMs) have been proposed, {{and they can be}} used to predict and estimate software reliability. One common assumption of these conventional SRGMs is to assume that detected faults will be removed immediately. In reality, this assumption may not be reasonable and may not always occur. During debugging, developers need time to reproduce the failure, identify the root causes of faults, fix them, and then re-run the software. From some experiments or observations, the <b>fault</b> <b>correction</b> rate may not be a constant and could be changed at certain points as time proceeds. Consequently, in this paper, we will investigate and study how to apply queueing models to describe the fault detection and correction processes during software development. We propose an extended infinite server queueing model with multiple change-points to predict and assess software reliability. Experimental results based on real failure data show that the proposed model can depict the change of <b>fault</b> <b>correction</b> rates and predict the behavior of software development more accurately than traditional SRGMs...|$|E
40|$|The {{challenge}} {{posed by}} the need for autonomous repair of unmanned spacecraft in unattended environments in outer space is addressed. The redundancy philosophy which forms a fundamental design driver for recent JPL interplanetary spacecraft is reviewed, and the requirements for autonomous spacecraft operations are described. Autonomous fault detection and correction is discussed, and in-flight experiences with <b>fault</b> <b>correction</b> during the VO and VGR missions are described. The lessons learned from these experiences are reviewed...|$|E
40|$|Many {{signals of}} {{interest}} are corrupted by faults {{of an unknown}} type. We propose an approach that uses Gaussian processes and a general "fault bucket" to capture a priori uncharacterised faults, along with an approximate method for marginalising the potential faultiness of all observations. This gives rise to an efficient, flexible algorithm for the detection and automatic <b>correction</b> of <b>faults.</b> Our method is deployed {{in the domain of}} water monitoring and management, where it is able to solve several <b>fault</b> detection, <b>correction,</b> and prediction problems. The method works well {{despite the fact that the}} data is plagued with numerous difficulties, including missing observations, multiple discontinuities, nonlinearity and many unanticipated types of fault. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved...|$|R
40|$|Targeting at {{the problem}} of finding the best {{demodulation}} band when applying envelope analysis in rolling bearing fault diagnosis, this paper proposes a novel Fast Kurtogram Demodulation Method (FKDM) to solve the problem. FKDM is established based on the theory of spectrum kurtosis and the short-time Fourier Transform. It determines the best demodulation band firstly, which {{is also known as}} the central frequency and frequency resolution. Then, the fault signals can be demodulated in the obtained frequency band by using envelope demodulation algorithm. The FKDM method ensures the <b>fault</b> diagnosis <b>correction</b> by solving the problem of demodulation band selection. Applied FKDM in rolling bearing fault diagnosis and compared with conventional envelope analysis, the results demonstrate FKDM can achieve a better performance...|$|R
5|$|The {{computer}} command subsystem (CCS) {{controls the}} cameras. The CCS contains fixed computer {{programs such as}} command decoding, <b>fault</b> detection and <b>correction</b> routines, antenna pointing routines, and spacecraft sequencing routines. This computer is an improved version of {{the one that was}} used in the 1970s Viking orbiters. The hardware in both custom-built CCS subsystems in the Voyagers is identical. There is only a minor software modification for one of them that has a scientific subsystem that the other lacks.|$|R
40|$|In this paper, {{we propose}} an optimal {{fault-tolerant}} algorithm for distributed event detection in wireless sensor networks. Two important problems are addressed: 1. How to handle both the noise-related measurement error and sensor fault simultaneously in fault-tolerant detection? 2. How {{to choose a}} proper neighborhood size n for a sensor node in <b>fault</b> <b>correction</b> such that the maximum energy could be conserved? Both theoretical analysis and experimental results confirm the effectiveness and efficiency of the proposed algorithm. 1...|$|E
40|$|This report {{presents}} the results of Task 323 - 08, Hardware and Software Reliability. Although hardware and software differ, they share a sufficient number of similarities that the mathematics used in hardware reliability modeling have been applied to software reliability modeling. This task examines those models and describes how they may be practical for application to projects at Goddard Space Flight Center. The task also resulted in improvements to one model to allow for <b>fault</b> <b>correction.</b> Application_and_Improvement_of_SW_Reliability_Models. doc i...|$|E
40|$|Abstract-In this paper, the {{modeling}} of fault-correction process {{from the viewpoint}} of correction time is first discussed. By proposing a new cost model, further analysis on the optimal release time determination is presented, which is also based on the model incorporating both fault detection and correction processes. The experiment results show it is more suitable to the reality. Finally, we propose a new solution to the assignment of testing resource. The approach is also illustrated with an actual data set from a software development project. Index Terms- fault detection process, <b>fault</b> <b>correction</b> process, cost model, optimal release time 1...|$|E
40|$|Robotic {{assistance}} {{is the new}} dimension of minimally invasive surgery. Despite being the state-of-the-art technology, newer technical problems still occur during robotic surgeries which are not addressed in the trouble shooting manual. We report one such problem being encountered with the tip cover accessory of monopolar scissors. In the current report, we discuss the technical <b>fault</b> and its <b>correction.</b> We feel that this problem needs to be registered into the trouble-shooting manual to prevent such incidents in future...|$|R
50|$|The {{computer}} command subsystem (CCS) {{controls the}} cameras. The CCS contains fixed computer {{programs such as}} command decoding, <b>fault</b> detection and <b>correction</b> routines, antenna pointing routines, and spacecraft sequencing routines. This computer is an improved version of {{the one that was}} used in the 1970s Viking orbiters. The hardware in both custom-built CCS subsystems in the Voyagers is identical. There is only a minor software modification for one of them that has a scientific subsystem that the other lacks.|$|R
40|$|A fault {{tolerant}} adder implemented using Kogge-stone configuration can correct the error due to inherent redundancy in the carry tree but no error detection is possible. This proposed design {{is based on}} {{fault tolerant}} adder [1] that uses Sparse kogge-stone adder {{that is capable of}} both <b>fault</b> detection and <b>correction.</b> <b>Fault</b> tolerance is achieved by using two additional ripple carry adders that form the basis of triple mode redundancy adder. Triple mode redundancy {{is one of the most}} common methods used to create fault tolerant designs in both ASIC and FPGA implementations. The latency will be increased because of the voter in the circuit’s critical path. More advanced fault tolerant methods exist including roving and graceful degradation approaches. Allowing fault tolerance to operate at different levels of abstraction might facilitate a more cost-effective design. Several enhancements are introduced in the design; the error recovery time is reduced by using a 16 -bit register, error <b>correction</b> due to <b>fault</b> in multiple ripple carry adders is included which improves the reliability of the circuit. The power analysis and the timing analysis for the estimation of setup time and hold time is also performed...|$|R
