903|4042|Public
5000|$|... #Caption: [...] Human ATRNL1 (Paralog to MEGF8) Protein <b>Feature</b> <b>Map</b> ...|$|E
5000|$|... #Subtitle level 3: General theorem for kernel {{approximation}} for a <b>feature</b> <b>map</b> ...|$|E
5000|$|For a <b>feature</b> <b>map</b> [...] with {{associated}} kernel : equality [...] also follows {{by replacing}} [...] {{by the operator}} [...] such that , , [...] , and [...] by the operator [...] such that , , [...] Once again, a simple inspection shows that the <b>feature</b> <b>map</b> is only needed in the proof while the end result only depends on computing the kernel function.|$|E
5000|$|The above {{reasoning}} directly generalizes to any finite {{number of}} dictionaries, or <b>feature</b> <b>maps.</b> It {{can be extended}} to <b>feature</b> <b>maps</b> inducing infinite dimensional hypothesis ...|$|R
40|$|Handset {{and channel}} {{mismatch}} degrades {{the performance of}} automatic speaker recognition systems significantly. This pa-per enhances the <b>feature</b> <b>mapping</b> technique by proposing an iterative clustering approach to context model generation which offers an improvement {{in the performance of}} <b>feature</b> <b>mapping</b> trained on labelled data and offers the potential to train <b>feature</b> <b>mapping</b> in the absence of correctly labelled background data. The performance of the clustered <b>feature</b> <b>mapping</b> models is demonstrated on an expanded version of the NIST 2003 Ex-tended Data Task (EDT) protocol. 1...|$|R
30|$|After <b>feature</b> <b>mapping,</b> {{we use the}} dereverberated {{features}} for acoustic modeling. We employed a hybrid DNN framework to perform joint training of DNNs for both <b>feature</b> <b>mapping</b> and acoustic modeling. In the hybrid DNN, we directly stack the acoustic modeling layers {{on top of the}} <b>feature</b> <b>mapping</b> layers. The output layer of <b>feature</b> <b>mapping</b> becomes the input layer for acoustic modeling, which is also a linear hidden layer of the whole network. Using the same object function as the back-end DNN, namely the cross entropy (CE) criterion, all weights are retrained. After joint training, the hybrid DNN can yield a better recognition performance which can be explained as the <b>feature</b> <b>mapping</b> network is refined to enable a better phone classification instead of optimizing the original MMSE criterion.|$|R
5000|$|As stated earlier, p.d. kernels can be {{constructed}} from inner products. This fact {{can be used to}} connect p.d. kernels with another interesting object that arises in machine learning applications, namely the <b>feature</b> <b>map.</b> Let [...] be a Hilbert space, and [...] the corresponding inner product. Any map [...] is called a <b>feature</b> <b>map.</b> In this case we call [...] the feature space. It is easy to see [...] that every <b>feature</b> <b>map</b> defines a unique p.d. kernel byIndeed, positive definiteness of [...] follows from the p.d. property of the inner product. On the other hand, every p.d. kernel, and its corresponding RKHS, have many associated feature maps. For example: Let , and [...] for all [...] Then , by the reproducing property.This suggests a new look at p.d. kernels as inner products in appropriate Hilbert spaces, or in other words p.d. kernels can be viewed as similarity maps which quantify effectively how similar two points [...] and [...] are through the value [...] Moreover, through the equivalence of p.d. kernels and its corresponding RKHS, every <b>feature</b> <b>map</b> can be used to construct a RKHS.|$|E
5000|$|In {{the finite}} {{dimensional}} case, every RKHS {{can be described}} in terms of a <b>feature</b> <b>map</b> [...] such that ...|$|E
5000|$|... where [...] form an {{orthonormal}} {{basis for}} , and [...] If feature maps is defined [...] with components , {{it follows that}} [...] This demonstrates that any kernel {{can be associated with}} a <b>feature</b> <b>map,</b> and that RLS generally consists of linear RLS performed in some possibly higher-dimensional feature space. While Mercer's theorem shows how one <b>feature</b> <b>map</b> that can be associated with a kernel, in fact multiple feature maps can be associated with a given reproducing kernel. For instance, the map [...] satisfies the property [...] for an arbitrary reproducing kernel.|$|E
3000|$|... [...]) for static images. Centre and {{surround}} {{scales are}} obtained using dyadic Gaussian pyramids with nine levels. Then, Centre-Surround Differences (CSD) [27] are computed as the pointwise differences across pyramid levels; and then, six <b>feature</b> <b>maps</b> for CSD network are computed {{for each of}} the seven features, yielding a total of 42 <b>feature</b> <b>maps.</b> Finally, all <b>feature</b> <b>maps</b> are integrated into the unique scalar image saliency [...]...|$|R
40|$|Non-linear kernel {{methods can}} be {{approximated}} by fast linear ones using suitable explicit <b>feature</b> <b>maps</b> allowing their application to large scale problems. To this end, explicit <b>feature</b> <b>maps</b> of kernels for vectorial data have been extensively studied. As many real-world data is structured, various kernels for complex data like graphs have been proposed. Indeed, many of them directly compute <b>feature</b> <b>maps.</b> However, the kernel trick is employed {{when the number of}} features is very large or the individual vertices of graphs are annotated by real-valued attributes. Can we still compute explicit <b>feature</b> <b>maps</b> efficiently under these circumstances? Triggered by this question, we investigate how general convolution kernels are composed from base kernels and construct corresponding <b>feature</b> <b>maps.</b> We apply our results to widely used graph kernels and analyze for which kernels and graph properties computation by explicit <b>feature</b> <b>maps</b> is feasible and actually more efficient. In particular, we derive <b>feature</b> <b>maps</b> for random walk and subgraph matching kernels and apply them to real-world graphs with discrete labels. Thereby, our theoretical results are confirmed experimentally by observing a phase transition when comparing running time with respect to label diversity, walk lengths and subgraph size, respectively. Moreover, we derive approximative, explicit <b>feature</b> <b>maps</b> for state-of-the-art kernels supporting real-valued attributes including the GraphHopper and Graph Invariant kernels. In extensive experiments we show that our approaches often achieve a classification accuracy close to the exact methods based on the kernel trick, but require only a fraction of their running time...|$|R
40|$|This paper {{presents}} {{a new approach}} for face recognition based on the fusion of tensors of census transform histograms from Local Gaussian <b>features</b> <b>maps.</b> Local Gaussian <b>feature</b> <b>maps</b> encode {{the most relevant information}} from Gaussian derivative features. Census Transform (CT) histograms are calculated and concatenated to form a tensor for each class of Gaussian map. Multi-linear Principal Component Analysis (MPCA) is applied to each tensor {{to reduce the number of}} dimensions as well as the correlation between neighboring pixels due to the Census Transform. We then train Kernel Discriminative Common Vectors (KDCV) to generate a discriminative vector using the results of the MPCA. Results of recognition using MPCA of tensors-CT histograms from Gaussian <b>features</b> <b>maps</b> with KDCV is shown to compare favorably with competing techniques that use more complex <b>features</b> <b>maps</b> like for example Gabor <b>features</b> <b>maps</b> in the FERET and Yale datasets. Additional experiments were done in the Yale B+ extended Yale B Faces dataset to show the performance of Gaussian <b>features</b> <b>map</b> with hard illumination changes...|$|R
5000|$|Structured {{sparsity}} regularization extends and generalizes {{the variable}} selection problem that characterizes sparsity regularization. Consider the above regularized empirical risk minimization {{problem with a}} general kernel and associated <b>feature</b> <b>map</b> [...] with [...]|$|E
5000|$|Let [...] - {{samples of}} data, [...] - a {{randomized}} <b>feature</b> <b>map</b> (maps a single vector to a vector of higher dimensionality) {{so that the}} inner product between a pair of transformed points approximates their kernel evaluation: ...|$|E
5000|$|Finally, we {{can replace}} the dot {{product in the}} dual {{perceptron}} by an arbitrary kernel function, to get {{the effect of a}} <b>feature</b> <b>map</b> [...] without computing [...] explicitly for any samples. Doing this yields the kernel perceptron algorithm: ...|$|E
30|$|The Itti {{algorithm}} exploits three low-level semantic {{features of}} an image: color, orientation and intensity. These features are {{extracted from the}} image to establish <b>feature</b> <b>maps.</b> Finally, the saliency map is computed from these <b>feature</b> <b>maps</b> after normalization and pooling.|$|R
3000|$|... (2) We extract deep {{convolutional}} activations from <b>feature</b> <b>maps</b> as <b>features,</b> {{and employ}} the proposed vertical pooling to aggregate deep convolutional activations across all <b>feature</b> <b>maps,</b> {{so that we}} can extract completed features and preserve the spatial information of images.|$|R
40|$|To predict where {{subjects}} {{look under}} natural viewing conditions, biologically inspired saliency models decompose visual input into {{a set of}} <b>feature</b> <b>maps</b> across spatial scales. The output of these <b>feature</b> <b>maps</b> are summed to yield the final saliency map. We studied the integration of bottom-up <b>feature</b> <b>maps</b> across multiple spatial scales by using eye movement data from four recent eye tracking datasets. We use AdaBoost as the central computational module {{that takes into account}} feature selection, thresholding, weight assignment, and integration in a principled and nonlinear learning framework. By combining the output of <b>feature</b> <b>maps</b> via a series of nonlinear classifiers, the new model consistently predicts eye movements better than any of its competitors...|$|R
5000|$|For example, we can trivially take [...] and [...] for all [...] Then (...) is {{satisfied}} by the reproducing property. Another classical {{example of a}} <b>feature</b> <b>map</b> relates to the previous section regarding integral operators by taking [...] and [...]|$|E
50|$|In general, {{under the}} kernel machine setting, the vector of {{covariates}} is first mapped into a high-dimensional (potentially infinite-dimensional) feature space {{characterized by the}} kernel function chosen. The mapping so obtained {{is known as the}} <b>feature</b> <b>map</b> and each of its coordinates, also known as the feature elements, corresponds to one feature (may be linear or non-linear) of the covariates. The regression function is then assumed to be a linear combination of these feature elements. Thus, the underlying regression model in the kernel machine setting is essentially a linear regression model with the understanding that instead of the original set of covariates, the predictors are now given by the vector (potentially infinite-dimensional) of feature elements obtained by transforming the actual covariates using the <b>feature</b> <b>map.</b>|$|E
5000|$|This {{connection}} between kernels and feature maps {{provides us with}} {{a new way to}} understand positive definite functions and hence reproducing kernels as inner products in [...] Moreover, every <b>feature</b> <b>map</b> can naturally define a RKHS by means of the definition of a positive definite function.|$|E
40|$|Integrating {{generative}} {{models and}} discriminative models in a hybrid scheme has shown {{some success in}} recognition tasks. In such scheme, generative models are used to derive <b>feature</b> <b>maps</b> for outputting a set of fixed length features that are used by discriminative models to perform classification. In this paper, we present a method, called posterior divergence, to derive <b>feature</b> <b>maps</b> from the log likelihood function implied in the incremental expectationmaximization algorithm. These <b>feature</b> <b>maps</b> evaluate a sample in three complementary measures: (1) how much the sample affects the model; (2) how well the sample fits the model; (3) how uncertain the fit is. We prove that the linear classification error rate using the outputs of the derived <b>feature</b> <b>maps</b> {{is at least as}} low as that of plug-in estimation. We present efficient algorithms for computing these <b>feature</b> <b>maps</b> for semi-supervised learning and supervised learning. We evaluate the proposed method on three typical applications, i. e. scene recognition, face and non-face classification and protein sequence analysis, and demonstrate improvements over related methods. 1...|$|R
5000|$|... #Subtitle level 3: Geographic <b>features</b> <b>mapped</b> by the {{expedition}} ...|$|R
30|$|The <b>feature</b> <b>maps</b> (the middle rows in Fig.  7) are in size of 28 × 28, {{indicating}} {{they are}} the <b>feature</b> <b>maps</b> serving as the input of the first FC layer of the network. Same <b>feature</b> <b>maps</b> are plotted in the bottom rows, where each of them has several missing pixels. Those missing pixels consist of the facial features, such as eyes, noses, mouths, eyebrows, and cheeks, which are discovered by the backtracking process. It is clearly {{to see that the}} <b>feature</b> <b>maps</b> in facial recognition have much more missing pixels than the ones in the emotion recognition. In fact, the average number of the missing pixels is 811 for the facial recognition and 188 for the emotion recognition. This fact suggests that there are more pixels (or facial features) contributed to the classification for the facial recognition than the emotion recognition.|$|R
5000|$|Kernel ICA {{is based}} on the idea that {{correlations}} between two random variables can be represented in a reproducing kernel Hilbert space (RKHS), denoted by , associated with a <b>feature</b> <b>map</b> [...] defined for a fixed [...] The -correlation between two random variables [...] and [...] is defined as ...|$|E
5000|$|A <b>feature</b> <b>map</b> is a map , where [...] is a Hilbert space {{which we}} will call the feature space. The first {{sections}} presented the connection between bounded/continuous evaluation functions, positive definite functions, and integral operators and in this section we provide another representation of the RKHS in terms of feature maps.|$|E
5000|$|Determine {{the weight}} vector that {{is closest to}} the input vector mapped to the current <b>feature</b> <b>map</b> (winner), using Euclidean {{distance}} (similar to the SOM). This step can be summarized as: find [...] such that [...] where , [...] are the input and weight vectors respectively, [...] is the position vector for nodes and [...] is the set of natural numbers.|$|E
40|$|A {{saliency}} attention {{model for}} predicting eye direction is proposed in this paper. This work {{is inspired by}} the success of the topological structure and Earth Mover's Distance (EMD) approach. Firstly, we extract visual saliency features such as color contrast, intensity contrast, orientation, and texture. Then, we eliminate disconnected regions in the <b>feature</b> <b>maps</b> to keep topological structure. Secondly, we calculate center surround difference using across-scale EMD between different scales <b>feature</b> <b>maps,</b> rather than utilizing the Difference of Gaussian (DoG), which is used in many other saliency attention models. Thirdly, we across-scale fuse the <b>feature</b> <b>maps</b> in different scale and same feature. Lastly, we take advantage of competition function to calculate <b>feature</b> <b>maps</b> in same <b>feature</b> to form a saliency map, which is use to predict eye direction. Experimental results demonstrated the proposed model outperformed the state-of-the-art schemes in eye direction prediction community...|$|R
30|$|In {{the first}} {{convolutional}} layer, the input image patch is continuously filtered with 48 <b>feature</b> <b>maps</b> of 3 × 11 × 11 and 3 × 7 × 7 kernels with a stride of 2. The second convolutional layer continuously filters {{the output of}} the first convolutional layer with 128 <b>feature</b> <b>maps</b> of 3 × 9 × 9 and 3 × 5 × 5 kernels. The third convolutional layer filters {{the output of the}} second convolutional layer with 128 <b>feature</b> <b>maps</b> of 3 × 7 × 7 and 3 × 3 × 3 kernels, sequentially. The following fully connected layer has 512 neurons.|$|R
30|$|In this subsection, we {{visualize}} the facial features {{that lead to}} the miss-classifications in both tasks. To do this, we apply the process of deconvolution, which provides top–down projections by <b>mapping</b> activations in <b>feature</b> <b>maps</b> generated in intermediate layers back to the input pixel space, showing the patterns that were learned by the <b>feature</b> <b>maps.</b>|$|R
5000|$|In {{this section}} {{it will be}} shown how to extend RLS {{to any kind of}} {{reproducing}} kernel K. Instead of linear kernel a <b>feature</b> <b>map</b> is considered for some Hilbert space , called the feature space. In this case the kernel is defined as: The matrix [...] is now replaced by the new data matrix [...] , where [...] , or the [...] -th component of the [...]|$|E
50|$|Since <b>feature</b> <b>map</b> size {{decreases}} with depth, layers {{near the}} input layer {{will tend to}} have fewer filters while higher layers can have more. To equalize computation at each layer, the feature x pixel position product is kept roughly constant across layers. Preserving {{more information about the}} input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.|$|E
5000|$|Shared weights: In CNNs, each filter is {{replicated}} {{across the}} entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a <b>feature</b> <b>map.</b> This means that all the neurons in a given convolutional layer respond to the same feature (within their specific response field). Replicating units in this way allows for features to be detected regardless of {{their position in the}} visual field, thus constituting the property of translation invariance.|$|E
5000|$|... #Subtitle level 2: Connection with {{reproducing}} kernel Hilbert {{spaces and}} <b>feature</b> <b>maps</b> ...|$|R
40|$|Introduction Data Mining aims to {{discover}} so far unknown knowledge in large datasets. The most important step thereby is {{the transition from}} subsymbolic to symbolic knowledge. Self-Organizing <b>Feature</b> <b>Maps</b> are very helpful in this task. If appropriately used, they exhibit the ability of emergence. I. e. using the cooperation of many neurons, Emergent <b>Feature</b> <b>Maps</b> are able to build structures on a new, higher level. The UMatrix -method visualizes these structures corresponding to structures of the high-dimensional input-space that otherwise would be invisible. A knowledge conversion algorithm transforms the recognized structures into a symbolic description of the relevant properties of the dataset. In chapter two we shortly introduce our approach to Data Mining and Knowledge Discovery, chapter three clarifies the use of Self-Organizing <b>Feature</b> <b>Maps</b> for Data Mining. Chapter four clarifies the use of <b>Feature</b> <b>Maps</b> {{in order to obtain}} emergence. In the chapters five...|$|R
40|$|Title of Dissertation: A Motor Control Model Based on Self-organizing <b>Feature</b> <b>Maps</b> Yinong Chen, Doctor of Philosophy, 1997 Dissertation {{directed}} by: Professor James A. Reggia Department of Computer Science Self-organizing <b>feature</b> <b>maps</b> {{have become}} important neural modeling methods {{over the last}} several years. These methods have not only shown great potential in application fields such as motor control, pattern recognition, optimization, etc, but have also provided insights into how mammalian brains are organized. Most past work developing self-organizing <b>features</b> <b>maps</b> has focused on systems with a single map that is solely sensory in nature. This research develops and studies a model which has multiple self-organizing <b>feature</b> <b>maps</b> in a closed-loop control system, and that involves motor output as well as proprioceptive and/or visual sensory input. The model is driven by a simulated arm that moves in 3 D space. By applying initial activations at randomly selected motor cortex regions, the [...] ...|$|R
