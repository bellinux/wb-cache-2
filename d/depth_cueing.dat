32|921|Public
5000|$|Features: Texture mapping, Gouraud shading, {{transparency}} effects, <b>depth</b> <b>cueing,</b> 16.7 million colors, 240,000 polygons/second ...|$|E
5000|$|Viewing {{features}} include Quick OpenGL previews and LiveRay photo-real previews, familiar interface, split view, camera view, multiple views, <b>depth</b> <b>cueing,</b> image/movie backdrop, and spotlight views.|$|E
50|$|The System 22 was {{designed}} by Namco with assistance from graphics & simulation company Evans & Sutherland. Graphical features include texture mapping, Gouraud shading, transparency effects, and <b>depth</b> <b>cueing,</b> thanks to the Evans & Sutherland 'TR3' chip/chipset, which stands for: Texture Mapping, Real-Time, Real-Visual, Rendering System. The main CPU provides a scene description to the TR3 graphics processing unit and a bank of DSP chips which perform 3D calculations.|$|E
40|$|International audienceThe {{perception}} of depth in images and video sequences {{is based on}} diﬀerent <b>depth</b> <b>cues.</b> Studies have considered depth perception threshold {{as a function of}} viewing distance (Cutting & Vishton, 1995), the combination of diﬀerent monocular <b>depth</b> <b>cues</b> and their quantitative relation with binocular <b>depth</b> <b>cues</b> and their diﬀerent possible type of interactions (Landy, 1995). But these studies only consider artiﬁcial stimuli and none of them attempts to provide a quantitative contribution of monocular and binocular <b>depth</b> <b>cues</b> compared {{to each other in the}} speciﬁc context of natural images. This study targets this particular application case. The evaluation of the strength of diﬀerent <b>depth</b> <b>cues</b> compared to each other using a carefully designed image database to cover as much as possible diﬀerent combinations of monocular (linear perspective, texture gradient, relative size and defocus blur) and binocular <b>depth</b> <b>cues.</b> The 200 images were evaluated in two distinct subjective experiments to evaluate separately perceived depth and diﬀerent monocular <b>depth</b> <b>cues.</b> The methodology and the description of the deﬁnition of the diﬀerent scales will be detailed. The image database is also released for the scientiﬁc community...|$|R
40|$|When full <b>depth</b> <b>cues</b> are available, size judgements are {{dominated}} by physical size. However, with reduced <b>depth</b> <b>cues,</b> size judgements are less influenced by physical size and more influenced by projected size. This study reduces <b>depth</b> <b>cues</b> further than previous size judgement studies, by manipulating monocularly presented pictorial <b>depth</b> <b>cues</b> only. Participants were monocularly presented with two shapes {{against a background of}} zero (control), one, two or three pictorial <b>depth</b> <b>cues.</b> Each cue was added progressively in the following order: height in the visual field, linear perspective, and texture gradient. Participants made a „same-different? judgement regarding the projected size of the two shapes, i. e. ignoring any <b>depth</b> <b>cues.</b> As expected, accuracy increased and response times decreased as the ratio between the projected size of the two shapes increased (range of projected size ratios, 1 : 1 to 1 : 5). In addition, {{with the exception of the}} larger size ratios (1 : 4 and 1 : 5), detection of projected size difference was poorer as <b>depth</b> <b>cues</b> were added. One-cue and two-cue conditions had the most weighting in this performance decrement, with little weighting from the three-cue condition. We conclude that even minimal depth information is difficult to inhibit. This indicates that depth perception requires little focussed attention...|$|R
40|$|AbstractThe conjugacy of saccades {{is rapidly}} {{modified}} if {{the images are}} made unequal for the two eyes. Disconjugacy persists {{even in the absence}} of disparity which indicates learning. Binocular visual disparity is a major <b>cue</b> to <b>depth</b> and is believed to drive the disconjugacy of saccades to aniseikonic images. The goal of the present study was to test whether monocular <b>depth</b> <b>cues</b> can also influence the disconjugacy of saccades. Three experiments were performed in which subjects were exposed for 15 – 20 min to a 10 % image size inequality. Three different images were used: a grid that contained a single monocular <b>depth</b> <b>cue</b> strongly indicating a frontoparallel plane; a random-dot pattern that contained a less prominent monocular <b>depth</b> <b>cue</b> (absence of texture gradient) which also indicates the frontoparallel plane; and a complex image with several overlapping geometric forms that contained a variety of monocular <b>depth</b> <b>cues.</b> Saccades became disconjugate in all three experiments. The disconjugacy was larger and more persistent for the experiment using the random-dot pattern that had the least prominent monocular <b>depth</b> <b>cues.</b> The complex image which had a large variety of monocular <b>depth</b> <b>cues</b> produced the most variable and less persistent disconjugacy. We conclude that the monocular <b>depth</b> <b>cues</b> modulate the disconjugacy of saccades stimulated by the disparity of aniseikonic images...|$|R
50|$|Koronis Rift {{was one of}} {{two games}} in Lucasfilm Games' second wave (December 1985). The other was The Eidolon. Both {{enhanced}} the fractal technology developed for Rescue on Fractalus!. In Koronis Rift, the Atari 8-bit family's additional colors (over those of the Commodore 64) allowed the programmers to gradually fade in the background rather than it suddenly popping in as in Rescue, an early example of <b>depth</b> <b>cueing</b> in a computer game.|$|E
5000|$|In the mid 70s {{until the}} end of the 80s, E&S {{produced}} the Picture System 1, 2 and PS300 series. These unique [...] "calligraphic" [...] (vector) color displays had <b>depth</b> <b>cueing</b> and could draw large wireframe models and manipulate (rotate, shift, zoom) them in real time. They were mainly used in chemistry to visualize large molecules such as enzymes or polynucleotides. The end of the Picture System line came in the late 80s, when raster devices on workstations could render anti-aliased lines faster.|$|E
5000|$|Motion {{parallax}} - When {{an observer}} moves, the apparent relative motion of several stationary objects {{against a background}} gives hints about their relative distance. If information about the direction and velocity of movement is known, motion parallax can provide absolute depth information. This effect can be seen clearly when driving in a car nearby things pass quickly, while distant objects appear stationary. Some animals that lack binocular vision because of the wide placement of the eyes employ parallax more explicitly than humans for <b>depth</b> <b>cueing</b> (e.g., some types of birds, which bob their heads to achieve motion parallax, and squirrels, which move in lines orthogonal to an object of interest to do the same).1 ...|$|E
40|$|The current {{eye-tracking}} study {{explored the}} relative impact of object size and <b>depth</b> <b>cues</b> on 8 -month-old infants' visual attention processes. A series of slides containing 3 objects of either different or same size were displayed on backgrounds with varying <b>depth</b> <b>cues.</b> The distribution of infants' first looks (a measure of initial attention switch) and infants' looking durations (a measure of sustained attention) at the objects were analyzed. Results {{revealed that the}} large objects captured infants' attention first, that is, {{most of the times}} infants directed their visual attention first to the largest object in the scene regardless of <b>depth</b> <b>cues.</b> For sustained attention, infants preferred maintaining their attention to the largest object also, but this occurred only when <b>depth</b> <b>cues</b> were present. These findings suggest that infants' initial attention response is driven mainly by object size, while infants' sustained attention is more the product of combined figure and background processing, where object sizes are perceived as a function of <b>depth</b> <b>cues...</b>|$|R
40|$|We {{examined}} {{the contribution of}} two important <b>depth</b> <b>cues,</b> occlusion and disparity, {{on the performance of}} a simulated telerobotic task. We have simulated a three-axis tracking task that is viewed under four different levels of realism. We hoped to determine if the combined presentation of the <b>depth</b> <b>cues</b> has a more beneficial effect on performance than either <b>depth</b> <b>cue</b> presented singularly. Results showed similar performance improvements with the presentation of occlusion or disparity individually. When both cues were present together, a somewhat larger performance improvement was measured...|$|R
40|$|This study investigates how {{frames of}} {{reference}} are chosen in a dynamic navigational task. Participants issued verbal instructions to an animated robot and were provided with one of three views for navigating the animated robot around a virtual world. The different views included a flat two-dimensional (2 D) North-up map, a three-dimensional (3 D) robot’s eye view of the world, and a 3 D view from behind the robot (3 D-Camera) in which <b>depth</b> <b>cues</b> were manipulated. Our results show people adopt an egocentric frame of reference when <b>depth</b> <b>cues</b> are salient and an exocentric reference frame when <b>depth</b> <b>cues</b> are absent. The results suggest the absence or presence of <b>depth</b> <b>cues</b> is a critical component in choosing a reference frame. We discuss the extension of Bryant and Tversky’s (1999) theoretical framework to a dynamic environment, such as navigation...|$|R
40|$|Most illustrative {{techniques}} used in non-photorealistic rendering to date apply a rendering style to objects or a scene and alter {{the appearance of}} this style by employing illumination or <b>depth</b> <b>cueing.</b> However, for generating high quality illustrations this approach relies almost entirely on smart placement of light sources...|$|E
40|$|Frequency domain volume {{rendering}} is {{a technique}} which allows projections of n 3 sized volume data to be generated in O(n 2 log n) time. This is achieved by exploiting the projection-slice theorem which states that a projection in spatial domain {{can be achieved by}} extracting a slice in frequency domain. This paper first gives an overview of the theoretical principles, whereas it also concentrates on the Hartley transform as a means of changing between spatial and frequency domain. The Hartley transform is more suitable in this context as the more familiar Fourier transform, since it generates real output for real data. As an extension, <b>depth</b> <b>cueing</b> to improve the spatial perception is described. Finally, some results achieved by an actual implementation are given. Keywords: volume rendering, Fourier transform, Hartley transform, projection-slice theorem, scientific visualization, <b>depth</b> <b>cueing</b> 1 Introduction Direct volume rendering {{is a technique}} that produces pictures of the vol [...] ...|$|E
40|$|This report {{presents}} a comparative analysis of different multimodal rendering methods proposed in [FPT 02]. It shows how relevant {{features of a}} property as well as relationships between data can be outlined by choosing an appropriate fusion modality. In addition, it analyses the visual clues that can be provided by using different shading models and by enabling rendering parameters such as <b>depth</b> <b>cueing</b> and light source attenuation. The simulations are performed on the software Hipo whose design is described in [PTF 02]. Postprint (published version...|$|E
40|$|Various {{visual cues}} provide {{information}} about depth and shape in a scene. When several of these cues are simultaneously available in a single location in the scene, the visual system attempts to combine them. In this paper, we discuss three key issues relevant to the experimental analysis of <b>depth</b> <b>cue</b> combination in human vision: cue promotion, dynamic weighting of cues, and robustness of cue combination. We review recent psychophysical studies of human <b>depth</b> <b>cue</b> combination {{in light of these}} issues. We organize the discussion and review as the development of a model of the <b>depth</b> <b>cue</b> combination process termed modified weak fusion (MWF). We relate the MWF framework to Bayesian theories of cue combination. We argue that the MWF model is consistent with previous experimental results and is a parsimonious summary of these results. While the MWF model is motivated by normative considerations, it is primarily intended to guide experimental analysis of <b>depth</b> <b>cue</b> combination in human vision. We describe experimental methods, analogous to perturbation analysis, that permit us to analyze <b>depth</b> <b>cue</b> combination in novel ways. In particular these methods allow us to investigate the key issues we have raised. We summarize recent experimental tests of the MWF framework that use these methods. <b>Depth</b> Multiple <b>cues</b> Sensor fusio...|$|R
30|$|Comparing {{the results}} in Figure 8 shows that both {{techniques}} like other motion-based depth estimation approaches fail to estimate the depth maps of static objects (e.g., the table in Figure 8 (b)). According to the human visual system, which integrates different <b>depth</b> <b>cues</b> to perceive the depth, one can expect the integration of <b>depth</b> <b>cues</b> to provide more sufficient means for depth map estimation techniques [5, 6]. Improvement on retrieving depth information of static objects may require integration of our approach with other <b>depth</b> <b>cues,</b> such as sharpness, and it is recommended path for future research.|$|R
40|$|Binocular unmasking {{refers to}} the {{improved}} detection performance of noise-masked simple visual patterns (e. g. grating) in the presence compared {{to the absence of}} 3 -D <b>cues</b> (stereoscopic <b>depth</b> <b>cues).</b> Here we investigated whether binocular unmasking is also important when perceiving noise-masked real-life objects. Specifically, we measured the detection, categorization, and identification thresholds of real-life objects in the presence and absence of stereoscopic <b>depth</b> <b>cues.</b> We found that the detection, categorization, and identification of objects was significantly better in the presence than in the absence of stereoscopic <b>depth</b> <b>cues</b> using hypothesis tests. Hence binocular unmasking seems important for object perception...|$|R
40|$|Structures of {{biological}} macromolecules deter-mined by transmission cryoelectron microscopy (cryo-TEM) and three-dimensional image recon-struction are often displayed as surface-shaded rep-resentations with <b>depth</b> <b>cueing</b> along the viewed direction (Z cueing). <b>Depth</b> <b>cueing</b> to indicate dis-tance {{from the center}} of virus particles (radial-depth cueing, or R cueing) has also been used. We have found that a style of R cueing in which color is applied in smooth or discontinuous gradients using the IRIS Explorer software is an informative tech-nique for displaying the structures of virus particles solved by cryo-TEM and image reconstruction. To develop and test these methods, we used existing cryo-TEM reconstructions of mammalian reovirus particles. The newly applied visualization tech-niques allowed us to discern several new structural features, including sites in the inner capsid through which the viral mRNAs may be extruded after they are synthesized by the reovirus transcriptase com-plexes. To demonstrate the broad utility of the meth-ods, we also applied them to cryo-TEM reconstruc-tions of human rhinovirus, native and swollen forms of cowpea chlorotic mottle virus, truncated core of pyruvate dehydrogenase complex from Saccharomy-ces cerevisiae, and flagellar filament of Salmonella typhimurium. We conclude that R cueing with color gradients is a useful tool for displaying virus par-ticles and other macromolecules analyzed by cryo-TEM and image reconstruction. r 1997 Academic Pres...|$|E
40|$|Non-photorealistic {{rendering}} {{can be used}} {{to illustrate}} subtle spatial relationships that might not be visible with more realistic rendering techniques. We present a parallel hardware-accelerated rendering technique, making extensive use of multi-texturing and paletted textures, for the interactive non-photorealistic visualization of scalar volume data. With this technique, we can render a 512 512 512 volume using non-photorealistic techniques that include tone-shading, silhouettes, gradient-based enhancement, and color <b>depth</b> <b>cueing,</b> as shown in the images on the color plate, at multiple frames second. The interactivity we achieve with our method allows for the exploration of a large visualization parameter space for the creation of effective illustrations...|$|E
40|$|This paper {{presents}} a new algorithm and technique for rendering triangular surfaces in pen-and-ink edge-based strokes. Our technique integrates two very important illustration strategies for depicting shape features: selection of drawing direction {{and the use}} of light. Drawing direction is given by four stroke directional fields. For lighting, we introduce the idea of “spotlight silhouettes ” for fast illumination computation, with target tone matched by adaptive stroke length adjustment. Stroke style is achieved by path perturbation and noise-based weight control. Our technique also allows visual effects of reverse tone values and <b>depth</b> <b>cueing.</b> Examples with models from anatomy and archeology demonstrate the capabilities of our system. 1...|$|E
40|$|We tested seven visual <b>depth</b> <b>cues</b> ({{relative}} brightness, relative size, relative height, linear perspective, foreshortening, texture gradient, and stereopsis) at viewing {{distances of}} one and two meters to answer two questions. First, which <b>cues</b> provide effective <b>depth</b> information (i. e., only a small change in the <b>depth</b> <b>cue</b> results in a noticeable change in perceived depth). Second, how does {{the effectiveness of these}} <b>depth</b> <b>cues</b> change {{as a function of the}} viewing distance? Six college-aged subjects were tested with each <b>depth</b> <b>cue</b> at both viewing distances. They were tested using a method of constant stimuli procedure and a modified Wheatstone stereoscopic display. Accuracies for perceptual match settings for all cues were very high (mean constant errors were near zero), and no cues were significantly more or less accurate than any others. Effectiveness of the perspective cues (linear perspective, foreshortening, and texture gradient) was superior to that of other <b>depth</b> <b>cues,</b> while effectiveness of relative brightness was vastly inferior. Moreover, stereopsis, among the more effective cues at one meter, was significantly less so at two meters. These results have theoretical implications for models of human spatial perception and practical implications for the design and development of 3 D virtual environments...|$|R
40|$|In two {{experiments}} with 162 participants, {{we examined the}} role of stereoscopic <b>depth</b> <b>cues</b> in the memory for objects and scenes. Stereoscopic <b>depth</b> <b>cues</b> are formed by disparities in the locations of objects in the images formed on the two eyes, and are naturally used by most observers to recover the distances to objects in the world. Using naturalistic pictures, we determined that stereo <b>depth</b> <b>cues</b> do persist in memory, {{as long as the}} observer is given several seconds to view the image and extract the depth relations from the scene. However, we did not find direct evidence that stereoscopic <b>depth</b> <b>cues</b> increase the rate of information extraction from scenes. One implication of the present work is that objects that will be recognized in the real world should be initially presented using stereoscopic <b>depth</b> <b>cues.</b> STEREO PROJECT REPORT PAGE 2 The world around us has three dimensions, although we often represent scenes using flat, 2 D representations such as photographs or movies. However, by virtue of the geometry of space and the separation of our two eyes by several inches, we perceive two slightl...|$|R
5000|$|... #Caption: The Necker cube: a {{wire frame}} cube with no <b>depth</b> <b>cues.</b>|$|R
40|$|Spot {{noise is}} a simple and {{effective}} algorithm to visualize vector fields. Originally proposed for visualizing two-dimensional flows or flows on surfaces, it is also easily extendible to three dimensions. However, images resulting from a straightforward extension of spot noise to 3 D are rather cluttered and confusing, they do not show the underlying flow too well and are also missing any indication of flow orientation. The latter is also an inherent drawback of two-dimensional spot noise. In this paper we outline several strategies to improve visualizations of both two- and three-dimensional vector fields with spot noise. These include orientation indication by anisotropic spots, sorted spots and <b>depth</b> <b>cueing</b> for better depth perception, and colour coding strategies...|$|E
40|$|We {{present a}} {{technique}} for the illustrative rendering of 3 D line data at interactive frame rates. We create depth-dependent halos around lines to emphasize tight line bundles while less structured lines are de-emphasized. Moreover, the depth-dependent halos combined with <b>depth</b> <b>cueing</b> via line width attenuation increase depth perception, extending techniques from sparse line rendering to the illustrative visualization of dense line data. We demonstrate how the technique can be used, in particular, for illustrating DTI fiber tracts but also show examples from gas and fluid flow simulations and mathematics {{as well as}} describe how the technique extends to point data. We report on an informal evaluation of the illustrative DTI fiber tract visualizations with domain experts in neurosurgery and tractography who commented positively about the results and suggested a number of directions for future work. ...|$|E
40|$|Most texture-based 3 D flow {{visualization}} {{techniques such}} as volume Line Integral Convolution (LIC) are limited to steady fields. Time-varying dense volume flow visualization is still an open problem due to intensive computation, demanding temporal-spatial coherence, and ad-hoc volume rendering. Our work {{is based on an}} enhanced Unsteady Flow LIC (UFLIC) algorithm, Accelerated UFLIC (AUFLIC), which uses a flow-driven seeding strategy and a dynamic seeding controller to reuse pathlines in the value scattering process to achieve fast time-dependent flow visualization. We first extend AUFLIC to time-varying volume flow fields for accelerated texture generation. To address occlusion, lack of <b>depth</b> <b>cueing,</b> and poor perception of flow directions within a dense volumetric texture, we employ a magnitude-based color-opacity mapping scheme in ray casting to clearly show flow structures in each frame and the flow evolution by means of a smooth animation...|$|E
40|$|The {{present study}} employs a {{stereoscopic}} manipulation to present sentences {{in three dimensions}} to subjects as they read for comprehension. Subjects read sentences with (a) no <b>depth</b> <b>cues,</b> (b) a monocular <b>depth</b> <b>cue</b> that implied the sentence loomed out of the screen (i. e., increasing retinal size), (c) congruent monocular and binocular (retinal disparity) <b>depth</b> <b>cues</b> (i. e., both implied the sentence loomed out of the screen) and (d) incongruent monocular and binocular <b>depth</b> <b>cues</b> (i. e., the monocular cue implied the sentence loomed out of the screen and the binocular cue implied it receded behind the screen). Reading efficiency was mostly unaffected, suggesting that reading in three dimensions is similar to reading in two dimensions. Importantly, fixation disparity was driven by retinal disparity; fixations were significantly more crossed as readers progressed through the sentence in the congruent condition and significantly more uncrossed in the incongruen...|$|R
40|$|Over {{recent years}} {{much has been}} learned about {{the way in which}} <b>depth</b> <b>cues</b> are {{combined}} (e. g. Landy et al., 1995). The majority of this work has used subjective measures, a rating scale or a point of subjective equality, to deduce the relative contributions of different cues to perception. We have adopted a very different approach by using two interval forced-choice (2 IFC) performance measures and a signal processing framework. We performed summation experiments for <b>depth</b> <b>cue</b> increment thresholds between pairs of pictorial <b>depth</b> <b>cues</b> in displays depicting slanted planar surfaces made from arrays of circular 'contrast' elements. Summation was found to be ideal when size-gradient was paired with contrast-gradient {{for a wide range of}} depth-gradient magnitudes in the null stimulus. For a pairing of size-gradient and linear perspective, substantial summation (> 1. 5 dB) was found only when the null stimulus had intermediate depth gradients; when flat or steeply inclined surfaces were depicted, summation was diminished or abolished. Summation was also abolished when one of the target cues was (i) not a <b>depth</b> <b>cue,</b> or (ii) added in conflict. We conclude that vision has a depth mechanism for the constructive combination of pictorial <b>depth</b> <b>cues</b> and suggest two generic models of summation to describe the results. Using similar psychophysical methods, Bradshaw and Rogers (1996) revealed a mechanism for the <b>depth</b> <b>cues</b> of motion parallax and binocular disparity. Whether this is the same or a different mechanism from the one reported here awaits elaboration...|$|R
30|$|With {{the need}} for <b>depth</b> <b>cues,</b> shape understanding, and {{avoidance}} of projective ambiguity, coordinating the locations of mechanical, electrical, and plumbing pipes is a demanding task. On one hand, <b>depth</b> <b>cues</b> and shape understanding require a 3 D display, while a standard 3 D display presents issues of projective ambiguity. The issue is averted in a physical model where subjects benefit from <b>depth</b> <b>cues</b> and shape understanding of a 3 D display and avoiding projective ambiguity from a true three dimensional, haptic output. However, 77 % of practitioners preferred a 3 D computer model, while 15 % and 8 % chose 2 D drawings and a physical model respectively.|$|R
40|$|After {{analyzing}} {{several possible}} illumination methods, we selected the maximum intensity projection as the method giving {{the most accurate}} results. In order to visualize small structures (thin veins), we apply {{the principles of the}} sampling theory in order to supply a gapless covering of the vein volume and to avoid sampling errors, while in the same time minimizing the computational effort. Line-oriented filtering is used to enhance the vein contrast. Visual cues like stereo, depth-of-field, <b>depth</b> <b>cueing,</b> prespective, rotation & parallex are employed to significantly increase the depth perception. As an alternative, the usual slice-oriented presentation of the dataset can be presented on the workstation screen, if desired by the physician. Last not least, our approach is fast enough to provide volumetric images of high quality within a few seconds of computation on a commercial modern workstation...|$|E
40|$|This paper {{presents}} an interactive technique for the dense texture-based visualization of unsteady 3 D flow, {{taking into account}} issues of computational efficiency and visual perception. High efficiency is achieved by a novel 3 D GPU-based texture advection mechanism that implements logical 3 D grid structures by physical memory {{in the form of}} 2 D textures. This approach results in fast read and write access to physical memory, independent of GPU architecture. Slice-based direct volume rendering is used for the final display. A real-time computation of gradients is employed to achieve volume illumination. Perception-guided volume shading methods are included, such as halos, cool/warm shading, or color-based <b>depth</b> <b>cueing.</b> The problems of clutter and occlusion are addressed by supporting a volumetric importance function that enhances features of the flow and reduces visual complexity in less interesting regions...|$|E
40|$|This paper {{describes}} an {{implementation of a}} tool for visualizing and interacting with huge information hierarchies, and some preliminary empirical evaluation of the tool's efficacy. Existing systems for visualizing huge hierarchies using cone trees "break down" once the hierarchy to be displayed exceeds roughly 1000 nodes, due to increasing visual clutter. This paper describes a system called fsviz which visualizes arbitrarily large hierarchies while retaining user control. This is accomplished by augmenting cone trees with several graphical and interaction techniques: usage-based filtering, animated zooming, hand-coupled rotation, fish-eye zooming, coalescing of distant nodes, texturing, effective use of colour for <b>depth</b> <b>cueing,</b> and the applications of dynamic queries. The fsviz system also improves upon earlier cone tree visualization systems through a more elaborate node layout algorithm. This algorithm enhances the usefulness of cone tree visualization for large hierarchies by all [...] ...|$|E
40|$|This study {{investigated}} the effect of different monocular <b>depth</b> <b>cues</b> on the understanding of a tactical situation in 3 D visual displays. The <b>depth</b> <b>cue</b> conditions were target symbol, target symbol with drop-shadow, and target symbol with drop-line. The tactical understanding {{was defined as the}} accuracy in perception of 3 D bearing (measured through Azimuth and Elevation) between own-ship and target symbols. Perhaps the most important result from these experiments with respect to design issues is that the differences between the <b>depth</b> <b>cue</b> conditions are minimal. That means that the need for drop-shadows or drop-lines is limited, which is good from a cluttering perspective...|$|R
40|$|The aim of {{this work}} is to {{identify}} the <b>depth</b> <b>cues</b> that provide intuitive depth-ordering when used to visualize abstract data. In particular {{we focus on the}} <b>depth</b> <b>cues</b> that are effective on a high-dynamic-range (HDR) display: contrast and brightness. In an experiment participants were shown a visualization of the volume layers at different depths with a single isolated monocular cue as the only indication of depth. The observers were asked to identify which slice of the volume appears to be closer. The results show that brightness, contrast and relative size are the most effective monocular <b>depth</b> <b>cues</b> for providing an intuitive depth ordering...|$|R
40|$|Introduction Computer {{graphics}} {{is confined}} chiefly to flat images. Images may look three-dimensional (3 -D), and sometimes {{create the illusion}} of 3 -D when displayed, for example, on a stereoscopic display [1 - 3]. Nevertheless, when viewing an image on most display systems, the human visual system (HVS) sees a flat plane of pixels. Volumetric displays can create a 3 -D computer graphics image, but fail to provide many visual <b>depth</b> <b>cues</b> (e. g., shading, texture gradients) and cannot provide the powerful <b>depth</b> <b>cue</b> of overlap (occlusion). Discrete parallax displays (such as lenticular displays) promise to create 3 -D images with all of the <b>depth</b> <b>cues,</b> but are limited by achievable resolution. Only a real-time electronic holographic ("holovideo") display [4 - 12] can create a truly 3 -D computer graphics image with all of the <b>depth</b> <b>cues</b> (motion parallax, ocular accommodation, occlusion, etc.) and resolution sufficient to provide extreme realism [2]. Holovideo displays promise to enhance numerous...|$|R
