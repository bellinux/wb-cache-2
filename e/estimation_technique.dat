3788|6079|Public
25|$|The Theil–Sen {{estimator}} is {{a simple}} robust <b>estimation</b> <b>technique</b> that chooses {{the slope of the}} fit line to be the median of the slopes of the lines through pairs of sample points. It has similar statistical efficiency properties to simple linear regression but is much less sensitive to outliers.|$|E
25|$|Least {{absolute}} deviation (LAD) regression is a robust <b>estimation</b> <b>technique</b> {{in that it}} is less sensitive to the presence of outliers than OLS (but is less efficient than OLS when no outliers are present). It is equivalent to maximum likelihood estimation under a Laplace distribution model for ε.|$|E
25|$|In physics or {{engineering}} education, a Fermi problem, Fermi quiz, Fermi question, Fermi estimate, {{or order}} estimation is an estimation problem {{designed to teach}} dimensional analysis, approximation, and such a problem is usually a back-of-the-envelope calculation. The <b>estimation</b> <b>technique</b> is named after physicist Enrico Fermi as he {{was known for his}} ability to make good approximate calculations with little or no actual data. Fermi problems typically involve making justified guesses about quantities and their variance or lower and upper bounds.|$|E
40|$|Abstract- In this paper, various channel <b>estimation</b> <b>techniques</b> for {{iterative}} receivers {{are compared}} for OFDM-IDMA system. Pilot –assisted based, semi-blind estimation, blind estimation and Decision –directed channel <b>estimations</b> <b>techniques</b> are considered and there comparisons are presented. The considered <b>estimation</b> <b>techniques</b> differ {{in terms of}} complexity, as well as performance. The main contribution {{of this paper is}} to give an overview of different channel <b>estimation</b> <b>techniques</b> OFDM-IDMA systems. There is no single channel estimator providing the best tradeoff and our comparison shows the system load and SNR influence the estimator choice...|$|R
5000|$|Dugan has {{developed}} bankruptcy prediction model limitations and accounting-based <b>estimation</b> <b>techniques</b> for corporate risk [...] His assessments reveal limitations in {{the accuracy of}} some widely accepted bankruptcy prediction models. The <b>estimation</b> <b>techniques</b> provide measures of degrees of risk inherent in corporate stock.|$|R
50|$|Noninvasive <b>estimation</b> <b>techniques</b> {{have been}} proposed.|$|R
500|$|The {{extensive}} {{reliance on}} the money-printing press to finance the war {{contributed significantly to the}} high inflation the South experienced {{over the course of the}} war, although fiscal matters and negative war news also played a role. Estimates of the extent of inflation vary by source, method used, <b>estimation</b> <b>technique,</b> and definition of the aggregate price level. According to a classic study by Eugene Lerner in 1956, a standard price index of commodities rose from 100 at the beginning of the war to more than 9200 by the war's de facto end in April 1865. By October 1864, the price index was at 2800, which implies that a very large portion of the rise in prices occurred in the last six months of the war. This drop in the demand for money, the corresponding increase in [...] "velocity of money" [...] (see next paragraph) and the resulting rapid increase in the price level has been attributed to the loss of confidence in Southern military victory or the success of the South's bid for independence.|$|E
50|$|Australian National Pollutant Inventory Emissions <b>Estimation</b> <b>Technique</b> Manuals.|$|E
5000|$|We {{can also}} derive {{software}} testing project size and effort using Delphi Technique or Analogy Based <b>Estimation</b> <b>technique.</b>|$|E
40|$|Abstract—This paper {{presents}} performance {{comparison of}} three <b>estimation</b> <b>techniques</b> used for peak load forecasting in power systems. The three optimum <b>estimation</b> <b>techniques</b> are, genetic algorithms (GA), least error squares (LS) and, least absolute value filtering (LAVF). The problem is formulated as an estimation problem. Different forecasting models are considered. Actual recorded data {{is used to}} perform the study. The performance of the above three optimal <b>estimation</b> <b>techniques</b> is examined. Advantages of each algorithms are reported and discussed. Keywords—Forecasting, Least error squares, Least absolute Value, Genetic algorithms...|$|R
40|$|Software {{inspections}} {{have established}} an impressive track record for early defect detection and correction. To increase their benefits, recent research efforts {{have focused on}} two different areas: systematic reading techniques and defect content <b>estimation</b> <b>techniques.</b> While reading techniques are to provide guidance for inspection participants on how to scrutinize a software artifact in a systematic manner, defect content <b>estimation</b> <b>techniques</b> aim at controlling and evaluating the inspection process by providing {{an estimate of the}} total number of defects in an inspected document. Although several empirical studies have been conducted to evaluate the accuracy of defect content <b>estimation</b> <b>techniques,</b> only few consider the reading approach as an influential factor. In this paper we examine the impact of two specific reading techniques - a scenario-based reading technique and checklist-based reading - on the accuracy of different defect content <b>estimation</b> <b>techniques.</b> The examination is based on data that were collected in a large experiment with students of the Vienna University of Technology. The results suggest that the choice of the reading technique has little impact on the accuracy of defect content <b>estimation</b> <b>techniques.</b> Although more empirical work is necessary to corroborate this finding, it implies that practitioners can use defect content <b>estimation</b> <b>techniques</b> without any consideration of their current reading technique...|$|R
40|$|The {{variety of}} {{software}} projects {{being carried out}} today is enormous. Some of them succeed while others fail. One key reason for failure of projects is lack of proper estimation {{or the use of}} inappropriate <b>estimation</b> <b>techniques.</b> While there are many <b>estimation</b> <b>techniques</b> developed for projects each of the...|$|R
50|$|A similar {{damping factor}} appears in Tikhonov regularization, {{which is used}} to solve linear ill-posed problems, as well as in ridge regression, an <b>estimation</b> <b>technique</b> in statistics.|$|E
5000|$|S. Skaff, A. Rizzi, H. Choset, P.-C. Lin. A Context-Based State <b>Estimation</b> <b>Technique</b> for Hybrid Systems. In Proc. IEEE Int. Conf. Robotics and Automation (ICRA), pp3935-3940, Barcelona, Spain, April 2005 ...|$|E
50|$|Theil is {{most famous}} for his {{invention}} of 2-stage least squares. This <b>estimation</b> <b>technique</b> greatly simplified estimation of simultaneous equation models {{of the economy and}} came into widespread use for this purpose.|$|E
5000|$|... "Cost <b>Estimation</b> <b>Techniques</b> for Web Projects", Emilia Mendes, IGI Publishing, ...|$|R
40|$|Abstract [...] - High-level <b>estimation</b> <b>techniques</b> are of {{paramount}} importance for design decisions like hardware/softwarepartition-ing or design space explorations. In both cases an appropriate compromise between accuracy and computation time determines about the feasibility of those <b>estimation</b> <b>techniques.</b> In this paper we present high [...] level <b>estimation</b> <b>techniques</b> for hardware effort and hardware/software communication time. Our techniques deliver fast results at sufficient accuracy. Furthermore, it is shown in which way these techniques are applied in order to cope with contradictory design goals like performance constraints and hardware effort constraints. As a solution, we present a cost function {{for the purpose of}} hardware/software partitioning that offers a dynamic weighting of its components. The conducted experiments show that the usage of our <b>estimation</b> <b>techniques</b> in conjunction with their efficient combination leads to reasonable hardware/software implementations as opposed to approaches that consider single constraints only. I...|$|R
40|$|In {{this paper}} {{the problem of}} TOA {{estimation}} in multipath channels are discussed. Brief introductions of TOA <b>estimation</b> <b>techniques</b> {{that can be used}} to improve the performance in multipath channels are presented. Some simulation results based on channel measurement data in indoor environment is presented to demonstrate and compare the performance of different TOA <b>estimation</b> <b>techniques...</b>|$|R
50|$|A commercially {{successful}} but specialized computer vision-based articulated body pose <b>estimation</b> <b>technique</b> is optical motion capture. This approach involves placing markers on {{the individual}} at strategic locations to capture the 6 degrees-of-freedom of each body part.|$|E
50|$|The Theil-Sen {{estimator}} is {{a simple}} robust <b>estimation</b> <b>technique</b> that chooses {{the slope of the}} fit line to be the median of the slopes of the lines through pairs of sample points. It has similar statistical efficiency properties to simple linear regression but is much less sensitive to outliers.|$|E
5000|$|In {{the above}} {{mathematical}} <b>estimation</b> <b>technique,</b> the function H(q) contains information about averaged generalized volatilities at scale [...] (only q = 1, 2 {{are used to}} define the volatility). In particular, the H1 exponent indicates persistent (H1 > ½) or antipersistent (H1 < ½) behavior of the trend.|$|E
25|$|Some of {{the more}} common <b>estimation</b> <b>techniques</b> for linear {{regression}} are summarized below.|$|R
5000|$|Many {{other popular}} <b>estimation</b> <b>techniques</b> can be cast {{in terms of}} GMM optimization: ...|$|R
30|$|Royle and Bezdan 2001 have {{demonstrated}} the comparison of shear wave velocity <b>estimation</b> <b>techniques.</b>|$|R
5000|$|Least {{absolute}} deviation (LAD) regression is a robust <b>estimation</b> <b>technique</b> {{in that it}} is less sensitive to the presence of outliers than OLS (but is less efficient than OLS when no outliers are present). It is equivalent to maximum likelihood estimation under a Laplace distribution model for ε.|$|E
50|$|The three-point <b>estimation</b> <b>technique</b> {{is used in}} {{management}} and information systems applications {{for the construction of}} an approximate probability distribution representing the outcome of future events, based on very limited information. While the distribution used for the approximation might be a normal distribution, this is not always so and, for example a triangular distribution might be used, depending on the application.|$|E
5000|$|Reverberation mapping is an astrophysical {{technique}} {{for measuring the}} structure of the broad emission-line region (BLR) around a supermassive black hole at the center of an active galaxy, and thus estimating the hole's mass. It is considered a [...] "primary" [...] mass <b>estimation</b> <b>technique,</b> i.e., the mass is measured directly from the motion that its gravitational force induces in the nearby gas.|$|E
30|$|In the following, we expose some head pose <b>estimation</b> <b>techniques</b> for {{monitoring}} driver vigilance state.|$|R
40|$|A {{model of}} the {{determinants}} of relative house prices was formulated and used to obtain estimates {{of the importance of}} various aspects of the dwelling and its environment. Two <b>estimation</b> <b>techniques</b> were employed: multiple regression analysis and the method of principal components. The results of the two exercises are compared and the relative efficiency of the <b>estimation</b> <b>techniques</b> is assessed in relation to the problem of multicollinearity. ...|$|R
40|$|This paper {{investigates the}} {{characteristics}} of different beta <b>estimation</b> <b>techniques</b> in infrequently traded and inefficient stock markets. These markets are artificially created from actual stock market data by removing return observations and by delaying the information transfer from market returns to individual stocks. Alternative beta <b>estimation</b> <b>techniques</b> are reported to behave differently in different types of markets. estimation beta estimation inefficient stock markets infrequently traded stock markets...|$|R
5000|$|Anderson and Hsiao {{were the}} first to propose an <b>estimation</b> <b>technique</b> for dynamic panel data models by {{exploiting}} the lagged structure for choosing appropriate instrumental variables within a GMM framework. More specifically, they propose First Differencing Δyit = Δxit b + Δeit, after which variables xi,t−1, xi,t−2, ... all provide valid instruments zi,t. They pass both the relevance and exclusion condition. One can then apply a two-step procedure for instrumental variables.|$|E
5000|$|The {{presence}} of the lagged dependent variable violates strict exogeneity, that is, endogeneity may occur. The fixed effect estimator and the first differences estimator both rely on the assumption of strict exogeneity. Hence, if [...] {{is believed to be}} correlated with one of the dependent variables, an alternative <b>estimation</b> <b>technique</b> must be used. Instrumental variables or GMM techniques are commonly used in this situation, such as the Arellano-Bond estimator.|$|E
50|$|In econometrics, {{the method}} of {{simulated}} moments (MSM) (also called simulated method of moments) is a structural <b>estimation</b> <b>technique</b> introduced by Daniel McFadden. It extends the generalized method of moments to cases where theoretical moment functions cannot be evaluated directly, such as when moment functions involve high-dimensional integrals. MSM's earliest and principal applications have been to research in industrial organization, after its development by Ariel Pakes, David Pollard, and others, though applications in consumption are emerging.|$|E
40|$|Several {{ways for}} {{detection}} {{and assessment of}} collinearity in measured data are discussed. Because data collinearity usually results in poor least squares estimates, two <b>estimation</b> <b>techniques</b> which can limit a damaging effect of collinearity are presented. These two techniques, the principal components regression and mixed estimation, belong to a class of biased <b>estimation</b> <b>techniques.</b> Detection and assessment of data collinearity and the two biased <b>estimation</b> <b>techniques</b> are demonstrated in two examples using flight test data from longitudinal maneuvers of an experimental aircraft. The eigensystem analysis and parameter variance decomposition {{appeared to be a}} promising tool for collinearity evaluation. The biased estimators had far better accuracy than the results from the ordinary least squares technique...|$|R
3000|$|..., {{which is}} {{inspired}} by spatial spectrum <b>estimation</b> <b>techniques.</b> Then we evaluate the estimation accuracy of F [...]...|$|R
40|$|M. Com. (Econometrics) The {{objective}} {{of this study is}} to evaluate different <b>estimation</b> <b>techniques</b> that can be used to estimate the coefficients of a model. The <b>estimation</b> <b>techniques</b> were applied to empirical data drawn from the South African economy. The Monte Carlo studies are unique in that data was statistically generated for the experiments. This approach was due to the fact that actual observations on economic variables contain several econometric problems, such as autocorrelation and MUlticollinearity, simultaneously. However, the approach in this study differs in that empirical data is used to evaluate the <b>estimation</b> <b>techniques.</b> The <b>estimation</b> <b>techniques</b> evaluated are : • Ordinary least squares method • Two stage least squares method • Limited information maximum likelihood method • Three stage least squares method • Full information maximum likelihood method. The estimates of the different coefficients are evaluated on the following criteria : • The bias of the estimates • The variance of the estimates • t-values of the estimates • The root mean square error. The ranking of the <b>estimation</b> <b>techniques</b> on the bias criterion is as follows : 1 Full information maximum likelihood method. 2 Ordinary least squares method 3 Three stage least squares method 4 Two stage least squares method 5 Limited information maximum likelihood method The ranking of the <b>estimation</b> <b>techniques</b> on the variance criterion is as follows : 1 Full information maximum likelihood method. 2 Ordinary least squares method 3 Three stage least squares method 4 Two stage least squares method 5 Limited information maximum. likelihood method All the <b>estimation</b> <b>techniques</b> performed poorly with regard to the statistical significance of the estimates. The ranking of the <b>estimation</b> <b>techniques</b> on the t-values of the estimates is thus as follows 1 Three stage least squares method 2 ordinary least squares method 3 Two stage least squares method and the limited information maximum likelihood method 4 Full information maximum likelihood method. The ranking of the <b>estimation</b> <b>techniques</b> on the root mean square error criterion is as follows : 1 Full information maximum likelihood method and the ordinary least squares method 2 Two stage least squares method 3 Limited information maximum likelihood method and the three stage least squares method The results achieved in this study are very similar to those of the Monte Carlo studies. The only exception is the ordinary least squares method that performed better on every criteria dealt with in this study. Though the full information maximum likelihood method performed the best on two of the criteria, its performance was extremely poor on the t-value criterion. The ordinary least squares method is shown, in this study, to be the most constant performer...|$|R
