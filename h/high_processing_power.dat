110|9896|Public
50|$|Processing power. Compression {{algorithms}} require different {{amounts of}} processing power to encode and decode. Some high compression algorithms require <b>high</b> <b>processing</b> <b>power.</b>|$|E
5000|$|Dmitry Dibenko (Дмитрий Дибенко) - {{creator of}} the deep-program. Uses a {{prototype}} 886 processor, give him extremely <b>high</b> <b>processing</b> <b>power</b> and speed. For his accomplishments, {{he was given a}} special medal that gives him access to every Deeptown system, no matter how secure. His usual avatar has its face covered in mist.|$|E
50|$|Minicomputers {{were also}} known as {{midrange}} computers. They grew to have relatively <b>high</b> <b>processing</b> <b>power</b> and capacity. They were used in manufacturing process control, telephone switching and to control laboratory equipment. In the 1970s, they were the hardware {{that was used to}} launch the computer-aided design (CAD) industry and other similar industries where a smaller dedicated system was needed.|$|E
50|$|The game {{features}} graphical improvements {{over its}} predecessors {{in light of}} the PlayStation Vita's <b>higher</b> <b>processing</b> <b>power</b> compared to the PlayStation Portable. The models in the game are also based on the PlayStation Portable predecessors rather than the Dreamy Theater models, which have a slightly different art style. There are also improvements in the animations of the characters, such as smoother movements for Miku's hair and Rin's ribbons; the game also features improved facial expressions and lighting with real-time lighting computation being added into the game.|$|R
40|$|This work reports an {{overview}} about the contributions that photonics can add for {{the evolution of}} Data Centers, and it is pointed out how the combination of new photonic devices and optical communications can be considered as a valid recipe for high performance Data Center and their integration in the telecommunication network. In particular optical communication systems can satisfy all the aspects regarding the increasingly intelligent, low-latency and high-bandwidth connections among data centers. Furthermore, inside the data center rack, multiple processing cores are being integrated into a single photon-based chip to achieve <b>higher</b> <b>processing</b> <b>power</b> and energy consumption reduction...|$|R
40|$|In {{the last}} decades the rising of <b>higher</b> <b>processing</b> <b>power</b> computers, {{together}} with more sophisticated robot actuators gave an impulse {{to the field of}} autonomous robots in robotics. The need to explore dirty, dangerous and difficult terrains is a suitable task for a robot, sparing a human from hazards of the environment. Even though wheeled robots have been used in great scale for explorations, its configuration has the downside of obstacle impediment and depending on the terrain its wheels can get stuck. The legged robot presents more versatility allowing him to surpass such obstacles in some cases. This article presents the continuation of the development of an quadruped robot with biomorphic insect leg...|$|R
50|$|Alim {{is cheaper}} and has less {{maintenance}} cost than normal radars because it doesn't have transmitter and movable mechanical parts. Although it needs <b>high</b> <b>processing</b> <b>power</b> {{because it has}} to do six levels of processing to track a target. Other advantages include the ability to detect low RCS targets in low altitudes. Also as it doesn't transmit anything, {{it is not possible to}} destroy it using anti radiation missiles such as American AGM-88 HARM thus it can be deployed near the enemy lines.|$|E
5000|$|Technology scaling {{has enabled}} to design very {{powerful}} and versatile computing systems on smaller chips. As the smaller feature size allows more functions {{to be implemented}} in the same area, there is an escalation in current density and the associated power dissipation. As the demands for <b>high</b> <b>processing</b> <b>power</b> and clock-rates are ever increasing due to the higher integration, the power consumption and thermal performance of Integrated circuit is emerging as the limiting factor for high performance processor systems. Also the need for reducing the self-heating of the processor chips operating in automotive temperature environments enhances the need for solutions that reduce power dissipation [...]A lot of techniques {{can be used to}} transform a high-performance system into a low power system.However,minimizing power consumption in digital CMOS circuits requires a lot of design effort at system, architectural, circuit and device levels. Supply Voltage reduction technique {{is one of the most}} effective ways of reducing the power consumption.Nevertheless, one direct consequence of static supply voltage scaling is performance degradation.To maintain the desired throughput, Dynamic voltage scaling systems are used to adjust the supply voltage according to the throughput requirements [...] But there is still a large amount of power margin wasted in the Open-loop DVS systems and to overcome this AVS systems are being examined.|$|E
5000|$|It has a {{solid steel}} dark green body with an opto-based pedal {{on the right and}} red LED display on the top left. It uses the Variable Architecture Modeling System (V.A.M.S [...] ) technology. In general, there are three main {{internal}} modules: for drive, for modulation and for delay. The unit allows a high level of distortion customization by using specialized software. It also allows the use of external distortion. The technology used in the unit does not allow full reordering of the effects but allows some of modulation effects like wah and phaser to be connected before or after the drive module. The drive module implements dynamic related effects like compressor, overdrive, distortion and fuzz. After the drive module, the noise gate module called ZNR (ZOOM Nose Reduction) is connected, followed by a parametric equalizer (presence, treble, middle, bass). The amp simulation module is connected next and allows various types of guitar amplifier simulations. The modulation module implements effects like wah, phaser, chorus, ring modulator, tremolo, vibrato, flanger and pitch shifter. The delay module is used to implement delay and reverb effects. Effects that require <b>high</b> <b>processing</b> <b>power</b> use modulation and delay module together. One such effect is the jam play effect, which allows, for example, a guitar player to play a rhythm guitar part and then play a solo part over it. The unit design is oriented toward ease of use by providing more knobs than usually found on such units, thus making the unit look more like a chain of effect boxes instead of the typical effect processor with a [...] "few knobs many functions" [...] type design. The unit has mature MIDI capabilities, allowing both control from an external sequencer or using the unit as a MIDI controller. The MIDI OUT can be configured to act as MIDI THRU.|$|E
40|$|Abstract — The paper {{investigate}} {{the concept of}} developing an error robust encoding mechanism with the minimum level of redundancy. The traditional prediction based video coding architectures have high susceptibility for channel errors. Most of these techniques fail to deliver acceptable error recovery at the receiving end at high error rate transmission networks. However, the parity based Wyner-Ziv paradigm exhibits acceptable level of error recovery in highly error prone environments, {{at the expense of}} <b>higher</b> <b>processing</b> <b>power.</b> The proposed method tries to integrate all of the advantages of these two techniques in order to mitigate the disadvantages associated with each and every technique while providing high error robustness to the transmitted bit stream. Keywords-component; Motion Vectors; WZ coding; Error resilience; I...|$|R
2500|$|Physical {{modelling}} synthesis is {{the synthesis}} of sound by using a set of equations and algorithms to simulate each sonic characteristic of [...] an instrument, starting with the harmonics {{that make up the}} tone itself, then adding the sound of the resonator, the instrument body, etc., until the sound realistically approximates the desired instrument. When an initial set of parameters is run through the physical simulation, the simulated sound is generated. Although physical modeling was not a new concept in acoustics and synthesis, {{it was not until the}} development of the Karplus-Strong algorithm and the increase in DSP power in the late 1980s that commercial implementations became feasible. The quality and speed of physical modeling on computers improves with <b>higher</b> <b>processing</b> <b>power.</b>|$|R
40|$|We {{present a}} uniform {{construct}} of parallel programming {{for a set}} of image processing tasks based on our Distributed Computing Primitive #DCP# concept. Our target architecture is a heterogeneous computing network system consisting of various high performance workstations connected through a local area network. We show that DCP has advantages over non-primitive PVM-based parallel approaches in three aspects: easeof -use, automation, and optimization. 1. INTRODUCTION Most image processing algorithms are computation intensive and require signi#cant computing power that cannot be provided by a uniprocessor. Parallel architectures contribute <b>higher</b> <b>processing</b> <b>power</b> by several orders of magnitude. However, a parallel system is not a cost e#ectiveway to improve the performance of parallel applications. Heterogeneous systems widely exist in industrial and academic computing environments offering a wealth of underutilized resources for high performance computing. Network Of Workstation #NOW# [...] ...|$|R
30|$|Though GA and {{the other}} {{evolutionary}} algorithms are very robust optimization algorithms, but since {{they are based on}} exhaustive search paradigms, they are well known for their long execution time, need for <b>high</b> <b>processing</b> <b>power</b> due to their computational complexity and sometimes inefficiency as they get cut up in some local optima (Bies et al. 2006).|$|E
30|$|The low data storage, low {{processing}} power and high energy consumption are the challenging problems of WSN’s sensor and these sensors {{are used in}} WBAN to monitor the sensitive organs of humans [75]. The manufacturers and designers are suggested to enhance the performance of sensors in terms to minimize the energy consumption, provide high data storage and <b>high</b> <b>processing</b> <b>power.</b>|$|E
30|$|There are {{different}} types of BMSs used to monitor different vital signs such as heartbeat rate, respiratory rate, EEG, ECG, glucose, temperature, blood pressure. The BMSs transmit the patient’s data in different data rates and frequencies which require <b>high</b> <b>processing</b> <b>power,</b> high storage and high energy to transmit to the body coordinator [68]. Hence, the researchers ought to design MAC protocol that fulfills {{the requirements of the}} dedicated data rates for BMSs.|$|E
5000|$|To be used efficiently, all {{computer}} software needs certain hardware components or other software resources {{to be present}} on a computer. These prerequisites are known as (computer) system requirements and are often used as a guideline {{as opposed to an}} absolute rule. Most software defines two sets of system requirements: minimum and recommended. With increasing demand for <b>higher</b> <b>processing</b> <b>power</b> and resources in newer versions of software, system requirements tend to increase over time. Industry analysts suggest that this trend plays a bigger part in driving upgrades to existing computer systems than technological advancements. A second meaning of the term of System requirements, is a generalisation of this first definition, giving the requirements to be met in the design of a system or sub-system. Typically an organisation starts with a set of Business requirements and then derives the System requirements from there.|$|R
40|$|Adaptive Optics Real-Time Control {{systems for}} next {{generation}} ground-based telescopes demand significantly <b>higher</b> <b>processing</b> <b>power,</b> memory bandwidth and I/O capacity on the hardware platform {{than those for}} existing control systems. We present a FPGA based high-performance computing platform that is developed at Dominion Radio Astrophysical Observatory and is very suitable for the applications of Adaptive Optics Real-Time Control systems. With maximum of 16 computing blades, 110 TeraMAC/s <b>processing</b> <b>power,</b> 1. 8 Terabyte/s memory bandwidth and 19. 5 Terabit/s I/O capacity, this ATCA architecture platform has enough capacity to perform pixel processing, tomographic wave-front reconstruction and deformable mirror fitting for first and second generation AO systems on 30 +-meter class telescopes. As an example, we demonstrate that with only one computing blade, the platform can handle the real time tomography needs of NFIRAOS, the Thirty-Meter Telescope first light facility Multi-Conjugate Adaptive Optics system. The High-Performance FPGA platform is integrated with Board Software Development Kit to provide a complete and fully tested set of interfaces to access the hardware resources. Therefore the firmware development can be focused on unique, user-specific applications. 9 2012 SPIE. Peer reviewed: YesNRC publication: Ye...|$|R
40|$|Wireless {{vision sensor}} network is an {{emerging}} field which combines image sensor, on board computation and communication links. Compared {{to the traditional}} wireless sensor networks which operate on one dimensional data, wireless vision sensor networks operate on two dimensional data which requires both <b>higher</b> <b>processing</b> <b>power</b> and communication bandwidth. The research focus {{within the field of}} wireless vision sensor network has been based on two different assumptions involving either sending data to the central base station without local processing or conducting all processing locally at the sensor node and transmitting only the final results. In this paper we focus on determining an optimal point for intelligence partitioning between the sensor node and the central base station and by exploring compression methods. The lifetime of the visual sensor node is predicted by evaluating the energy consumption for different levels of intelligence partitioning at the sensor node. Our results show that sending compressed images after segmentation will result in a longer life for the sensor node...|$|R
40|$|Paper {{describes}} the {{theoretical and practical}} aspects of the proposed model that uses distributed computing to a global network of Internet communication. Distributed computing are widely used in modern solutions such as research, where the requirement is very <b>high</b> <b>processing</b> <b>power,</b> {{which can not be}} placed in one centralized point. The presented solution is based on open technologies and computers to perform calculations provided mainly by Internet users who are volunteers. Comment: 6 pages, 10 figure...|$|E
40|$|International audienceManycore {{architectures}} {{correspond to}} a main evolution of computing systems {{due to their}} <b>high</b> <b>processing</b> <b>power.</b> Many applications can be executed in parallel which provides users with a very efficient technology. Cloud computing {{is one of the}} many domains where manycore architectures will play a major role. Thus, building secure manycore architectures is a critical issue. However a trusted platform based on manycore architectures is not available yet. In this paper we discuss the main challenges and some possible solutions to enhance manycore architectures with cryptoprocessor...|$|E
30|$|The {{prototype}} for the robot agent FAMPER (depicted in Fig. 2) has 4 expandable one-segment tilted caterpillars. The robot has attached an RFID reader/writer {{to collect information}} stored at the tag level, a chemical tank and sprayer for actuation purposes, two Li-ion batteries for 1 -h life, and a CCD camera for creating high-quality images related to inspection. It has <b>high</b> <b>processing</b> <b>power,</b> large memory, and several sensing functions. In addition, the robot has four extendable external interfaces to add different modules for pipeline inspection demands as shown in Fig. 7.|$|E
40|$|The {{purpose of}} setting up this {{environment}} {{is to provide a}} tool for solving problems arising from engineering and science using parallel computing methods and achieving high performances in these solutions. In general, these kinds of problems require <b>high</b> <b>processing</b> <b>powers.</b> This environment is based on Beowulf Clusters which are widely used and cost-effective parallel computer architectures. A Beowulf Cluster is a parallel computer that is constructed of commodity hardware running commodity software. For this environment, eight computers with Celeron®CPU's have been used. On each of these computers one of the popular distributions of Linux operating system, Fedora 11 has been installed. Then a local area network has been set up using a gigabit Ethernet switch. As the Message Passing Interface standard the high performance implementation of this standard, MPICH 2 has been installed. This was the final step of installation. Finally, this environment has been tested with some parallel programs solving simple numerical analysis problems. The environment must be tested with more serious problems...|$|R
40|$|AbstractThe non-thermal {{processing}} method of dense phase carbon dioxide (DPCD) for heat sensitive foods preserves quality, nutrients while inactivating microorganisms and enzymes in liquid foods. There is increasing interest in solid foods. Challenges associated with processing solids with DPCD will be discussed. Use of gases other than CO 2 is possible. Finally, {{the combination of}} other non-thermal technologies such as pulsed electric fields, <b>high</b> pressure <b>processing,</b> <b>high</b> <b>power</b> ultrasound, and irradiation with DPCD opens new possibilities of more reduction of microorganisms, better inactivation of enzymes at milder process conditions, while offering more effective preservation of quality and nutrients...|$|R
40|$|Multimedia {{support on}} {{hand-held}} devices is growing rapidly. The introduction of 3 D videos in cinemas and on home entertainment systems {{has made it}} appealing on mobile environment devices. MVC (Multiview Video Coding) provides a compressed representation of multiple views in a single scene. However, handheld devices have hardware constraints that makes it challenging to support highdefinition content and increases {{the complexity of the}} algorithms used. Due to this, an advanced processor with the capabilities of low power consumption and high instruction computation is needed. Compression standards such as H. 264 /MPEG- 4 and its amendment, AVC (Advanced Video Coding), have had great success in video compression. However, more complex algorithms and significantly <b>higher</b> <b>processing</b> <b>power</b> are necessary. As an extension to this standard, MVC was introduced for 3 D stereo video support. Conventional processors contain coprocessors that support Single Instruction and Multiple Data technology. Some arithmetic operations are also supported on the main processor. Therefore these features can reduce the decoding time of the MVC decoding process...|$|R
40|$|The {{exponential}} growth of mobile data traffic still remains an important {{challenge for the}} mobile network operators. In response, the 5 G scene needs to couple fast connectivity and optimized spectrum usage with cloud networking and <b>high</b> <b>processing</b> <b>power,</b> optimally combined in a converged environment. In this paper, we investigate two 5 G research projects; SESAME [1] and COHERENT [2]. We consider the proposed 5 G architectures and the corresponding key network components, in order to highlight the common aspects towards the 5 G architecture design. Peer ReviewedPostprint (published version...|$|E
30|$|To achieve total confidentiality, the naïve {{solution}} is to encrypt the whole dataset and send only the encrypted data to the cloud service provider. During the query phase, the TU retrieves the entire encrypted data from the server, decrypts it and searches for the required data points. This makes ideal security achievable, but it is clearly not practical in real-time applications as the resulting data communication cost would be high, especially if {{only a small portion}} of the data is queried. Furthermore, the <b>high</b> <b>processing</b> <b>power</b> of the cloud environment would not be utilized in this case.|$|E
30|$|We {{are living}} in a world of data where data pervades and {{controls}} almost every aspect of our lives. In order to keep up with growing data demands, there is a never-ending need to establish quality resources. For maintaining the quality of resources we require <b>high</b> <b>processing</b> <b>power</b> and high end equipments that are sometimes expensive and unavailable. In order to meet the requirements, most of the end users and organizations have been led to the deployment of Cloud Computing which offers affordability, mobility, agility, and effective use of high priced infrastructure resources with reduced cost.|$|E
40|$|Compared {{to their}} predecessors, the current {{generation}} handheld mobile devices possess <b>higher</b> <b>processing</b> <b>power,</b> increased memory and new multi-homing capabilities. These features combined with the widespread acceptance and use of these devices result {{in a situation where}} mobile devices are no longer merely data consumers, but also able to act as data producers. Nomadic mobile services make this data available to the clients on the Internet and the contextawareness provides the capability to the nomadic mobile services to select suitable interface for data transfer. In this paper, we conduct a performance analysis of nomadic mobile services prototyped in the remote patient tele-monitoring domain and hosted on a multi-homed handheld mobile device. The experimentation aims at analyzing the suitability of a particular network for the data transfer, the effect of multi-homing on the remote patient tele-monitoring application and the resource utilization on the mobile device. The performance analysis provides us useful insights, which will be exploited in future work regarding policy based network interface selection mechanisms for nomadic mobile services...|$|R
40|$|Recent {{technology}} {{advances in}} integrated electronics offer {{the ability to}} add more and more transistors into modern chips. Chip Multiprocessors (CMPs) are architectures that feature multiple processing cores on a single chip. They result in <b>higher</b> <b>processing</b> <b>power,</b> easier design scalability, and greater performance/power ratio. CMPs appear {{to be one of}} the dominating architectural approaches for the years to come in the area of high performance architectures. The purpose of this work is to design and implement a shared memory multi-core system that matches the needs of future CMPs. Specifically, an FPGA-based prototype has been implemented, which constitutes a two-node processing system. The design takes advantage of the two PowerPC cores that are embedded in the FPGA fabric. We have implemented external coherent caches equipped with a MESI protocol, and a bus-based coherent memory interconnect to connect the two processors. Shared memory resides in external DDR memory accessible through the interconnect and the DDR controller. We find that the area overhead of our coherent memory system is 33. 4 % of a medium...|$|R
40|$|For {{the next}} {{generation}} of onboard computer in space application a <b>high</b> demand on <b>processing</b> <b>power</b> exists. Currently used onboard system didn't provide the necessary computing power. The in the paper describe tasking frame work is the core processing model for the OBC-NG (On Board Computer - Next Generation) operating system with an reactive computing model...|$|R
30|$|Heterogeneous {{resource}} management: User can {{maximize the}} fine-grained parallelism of image processing task {{given to a}} group by allocating heterogeneous resources in each worker node of the group. According {{to the type of}} task, user can choose the optimal memory capacity, the number of CPU cores and/or GPU. For example, in case the task contains many conditional and branching statements, multicore may be advantageous over GPU, and hence, the workers in the group may be configured into multicores for higher performance, while in case the task requires <b>high</b> <b>processing</b> <b>power,</b> they may be configured into GPUs.|$|E
40|$|Abstract- MolDynGrid virtual {{laboratory}} {{has been established}} for collaborative research in computational biology and bioinformatics that requires <b>high</b> <b>processing</b> <b>power</b> and huge storage space. This research involves molecular dynamics (MD) simulations of biological macromolecules, such as proteins, nucleic acids and their complexes. Also, a comparison of MD simulation results with experimental NMR spectroscopy data is proposed. The main aim of MolDynGrid {{virtual laboratory}} is to provide an efficient infrastructure for automation of MD job processing in Grid. It will assist research workers to use Grid for their needs without having them {{to be familiar with}} highly complicated internals of the Grid...|$|E
40|$|Computer {{systems for}} {{real-time}} multimedia processing require <b>high</b> <b>processing</b> <b>power.</b> Problems {{that depend on}} <b>high</b> <b>processing</b> <b>power</b> are usually solved by using parallel or distributed computing techniques; however, {{the combination of the}} difficulties of both real-time and parallel programming has led the development of applications for real-time multimedia processing for general purpose computer systems to be based on centralized and single-processor systems. In several systems for multimedia processing, {{there is a need for}} low latency during the interaction with the user, which reinforces the tendency towards single-processor development. In this work, we implemented a mechanism for synchronous and distributed audio processing with low latency on a local area network which makes the use of a low cost distributed system for this kind of processing possible. The main goal is to allow the use of distributed systems for recording and editing of musical material in home and small studios, bypassing the need for high-cost equipment. The system we implemented is made of two parts: the first, generic, implemented as a middleware for synchronous and distributed processing of continuous media with low latency; and the second, based on the first, geared towards audio processing and compatible with legacy applications based on the standard LADSPA interface. We expect that future research and applications that share the needs of the system developed here make use of the middleware we developed, both for other kinds of audio processing as well as for the processing of other media forms, such as video. ...|$|E
40|$|The TTF-III power coupler adopted for the ILC {{baseline}} cavity {{design has}} shown a tendency to have long initial <b>high</b> <b>power</b> <b>processing</b> time. A possible cause for the long processing time {{is believed to be}} multipacting in various regions of the coupler. To understand performance limitations during <b>high</b> <b>power</b> <b>processing,</b> SLAC has built a flexible high-power coupler test stand. The plan is to test individual sections of the coupler, which includes the cold and warm coaxes, the cold and warm bellows, and the cold window, using the test stand to identify problematic regions. To provide insights for the high power test, detailed numerical simulations of multipacting for these sections will be performed using the 3 D multipacting code Track 3 P...|$|R
40|$|Complex {{computer}} simulations are {{a class of}} applications that demands <b>high</b> performance <b>processing</b> <b>power</b> {{in order to be}} realized in a feasible time. To achieve this <b>processing</b> <b>power,</b> networks composed of non-dedicated machines are increasingly being investigated. An efficient scheduling scheme {{is one of the most}} important issues to make a better use of these resources. In this paper we present an architecture for scheduling complex {{computer simulations}} aimed at heterogeneous non-dedicated machines which relies on information provided by the models that are being simulated. Furthermore, a case study demonstrates how the proposed architecture can assist in the execution of complex simulations applied to the protein structure prediction problem, which is one of the most important current challenges in structural bioinformatics. 1...|$|R
40|$|Smartphones have {{transformed}} people approach towards technology. It not only empowers {{them to use}} it for communication but also for tracking and maintaining health-fitness via mobile applications. With the increasing number of complex mobile applications, the mobile, with its limited resources (in terms of the computational power and storage capacity) cannot efficiently run these applications autonomously. Similarly our e-health mobile application (Eat Healthy Stay Healthy) requires a platform to run highly computational intensive algorithms like the deep learning (for recognizing food images) and calorie measurement would need <b>higher</b> <b>processing</b> <b>power</b> to perform optimally. We propose a cloud based virtualization model that provides our e-health application with the required computational power that it needs to perform efficiently {{and at the same time}} would also give it the flexibility to make use of the various cloud resources. Our model comprises of concepts like virtual swap between various mobile sessions that assist the system for faster processing and intelligent decision mechanism for distributing the task of image processing to cloud servers. By implementing intelligent decision mechanism, the final calorie computation significantly improved by 20. 5 % while implementing deep learning in cloud...|$|R
