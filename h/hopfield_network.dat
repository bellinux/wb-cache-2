411|880|Public
25|$|Episodic {{memories}} can {{be stored}} in autoassociative neural networks (e.g., a <b>Hopfield</b> <b>network)</b> if the stored representation includes information on the spatiotemporal context in which an item was studied.|$|E
2500|$|Using a {{different}} annealing technology based on {{nuclear magnetic resonance}} (NMR), a quantum <b>Hopfield</b> <b>network</b> was implemented in 2009 that mapped the input data and memorized data to Hamiltonians, allowing the use of adiabatic quantum computation. NMR technology also enables universal quantum computing, and it {{was used for the}} first experimental implementation of a quantum support vector machine to distinguish hand written number ‘6’ and ‘9’ on a liquid-state [...] quantum computer in 2015. The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors to represent the images as the states of a qubit. The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image. Once the vectors are defined on the feature space, the quantum support vector machine was implemented to classify the unknown input vector. The readout avoids costly quantum tomography by reading out the final state in terms of direction (up/down) of the NMR signal.|$|E
5000|$|... #Caption: Energy Landscape of a <b>Hopfield</b> <b>Network,</b> {{highlighting}} {{the current state}} of the network (up the hill), an attractor state to which it will eventually converge, a minimum energy level and a basin of attraction shaded in green. Note how the update of the <b>Hopfield</b> <b>Network</b> is always going down in Energy.|$|E
5000|$|... #Subtitle level 3: Hebbian {{learning}} rule for <b>Hopfield</b> <b>networks</b> ...|$|R
30|$|Our next {{demonstration}} is {{an exact}} setting for weights in these <b>Hopfield</b> <b>networks.</b>|$|R
40|$|To {{predict the}} {{aircraft}} collision unsafe event effectively, a feasible modeling and predicting method based on <b>Hopfield</b> neural <b>network</b> is proposed. First, {{the basic idea}} of <b>Hopfield</b> neural <b>network</b> is introduced, and then the principle of <b>Hopfield</b> neural <b>network</b> is expatiated. The simulation indicates that the proposed <b>Hopfield</b> neural <b>network</b> model has a good prediction on aircraft collision unsafe event...|$|R
50|$|Her great uncle is John Hopfield, {{originator}} of the <b>Hopfield</b> <b>network.</b>|$|E
50|$|The Boltzmann {{machine is}} a Monte Carlo {{version of the}} <b>Hopfield</b> <b>network.</b>|$|E
5000|$|Updates in the <b>Hopfield</b> <b>network</b> can be {{performed}} in two different ways: ...|$|E
5000|$|Bidirectional {{networks}} {{are similar to}} <b>Hopfield</b> <b>networks,</b> with the special case that the matrix [...] is a block matrix.|$|R
40|$|In this paper, {{we propose}} a {{continuous}} hysteresis neurons (CHN) <b>Hopfield</b> neural <b>network</b> architecture for efficiently solving crossbar switch problems. A <b>Hopfield</b> neural <b>network</b> architecture with continuous hysteresis and its collective computational properties are studied. It is proved theoretically and confirmed by simulating the randomly generated <b>Hopfield</b> neural <b>network</b> with CHN. The network architecture {{is applied to}} a crossbar switch problem and results of computer simulations are presented and used to illustrate the computation power of the network architecture. The simulation {{results show that the}} <b>Hopfield</b> neural <b>network</b> architecture with CHN is much better than the binary hysteresis <b>Hopfield</b> neural <b>network</b> architecture for crossbar switch problem in terms of both the computation time and the solution quality. Key words: Network architecture, crossbar switch problem, continuous hysteresis, <b>Hopfield</b> neural <b>network</b> 1...|$|R
40|$|A {{model of}} neurons with CHN (Continuous Hysteresis Neurons) for the <b>Hopfield</b> neural <b>networks</b> is studied. We prove {{theoretically}} that the emergent collective {{properties of the}} original <b>Hopfield</b> neural <b>networks</b> also {{are present in the}} <b>Hopfield</b> neural <b>networks</b> with continuous hysteresis neurons. The network architecture is applied to the N-Queens problem and results of computer simulations are presented and used to illustrate the computation power of the network architecture. The simulation results show that the <b>Hopfield</b> neural <b>network</b> with CHN is much better than other algorithms for N-Queens problem in terms of both the computation time and the solution quality. Key words: <b>Hopfield</b> neural <b>network,</b> hysteresis, collective properties, N...|$|R
50|$|The <b>Hopfield</b> <b>network</b> is a RNN {{in which}} all {{connections}} are symmetric. It requires stationary inputs and is thus not a general RNN, as it does not process sequences of patterns. It guarantees that it will converge. If the connections are trained using Hebbian learning then the <b>Hopfield</b> <b>network</b> can perform as robust content-addressable memory, resistant to connection alteration.|$|E
5000|$|Every pair {{of units}} i and j in a <b>Hopfield</b> <b>network</b> have a {{connection}} that {{is described by}} the connectivity weight [...] In this sense, the <b>Hopfield</b> <b>network</b> can be formally described as a complete undirected graph , where [...] {{is a set of}} McCulloch-Pitts neurons and [...] is a function that links pairs of nodes to a real value, the connectivity weight.|$|E
50|$|In {{feedback}} networks the Willshaw network {{as well as}} the <b>Hopfield</b> <b>network</b> {{are able}} to learn instantaneously.|$|E
5000|$|The Hebbian rule is {{both local}} and incremental. For the <b>Hopfield</b> <b>Networks,</b> it is {{implemented}} in the following manner, when learning binary patterns: ...|$|R
40|$|AbstractBased on the {{principle}} of discrete <b>Hopfield</b> neural <b>network,</b> the paper proposes a cascade <b>Hopfield</b> neural <b>network</b> controller model and applied in a miniature inchworm robot locomotion process. According to the robot moving modes in one cycle, the cascade <b>Hopfield</b> neural <b>network</b> model with three neural nodes was set up, the weight factors and thresholds of the networks had been designed. The convergence results prove the cascade <b>Hopfield</b> neural <b>network</b> controller is suitable for the orderly continuous moving process of an inchworm robot...|$|R
40|$|Abstract — In this paper, {{we propose}} a {{continuous}} hysteresis neurons <b>Hopfield</b> neural <b>network</b> architecture for efficiently solving crossbar switch problems. A <b>Hopfield</b> neural <b>network</b> architecture with continuous hysteresis and its collective computational properties are studied. It is proved theoretically and confirmed by simulating the randomly generated <b>Hopfield</b> neural <b>network</b> with continuous hysteresis neurons. The network architecture {{is applied to}} a crossbar switch problem and results of computer simulations are presented and used to illustrate the computation power of the network architecture. The simulation {{results show that the}} <b>Hopfield</b> neural <b>network</b> architecture with continuous hysteresis neurons is much better than the previous works including the original <b>Hopfield</b> neural <b>network</b> architecture, maximum neural <b>network</b> and <b>Hopfield</b> neural <b>network</b> with hysteresis binary neurons for crossbar switch problem in terms of both the computation time and the solution quality...|$|R
50|$|The <b>Hopfield</b> <b>network</b> (like similar attractor-based networks) is of {{historic}} interest {{although it is}} not a general RNN, as it is not designed to process sequences of patterns. Instead it requires stationary inputs. It is an RNN in which all connections are symmetric. It guarantees that it will converge. If the connections are trained using Hebbian learning the <b>Hopfield</b> <b>network</b> can perform as robust content-addressable memory, resistant to connection alteration.|$|E
5000|$|Updating {{one unit}} (node in the graph {{simulating}} the artificial neuron) in the <b>Hopfield</b> <b>network</b> {{is performed using}} the following rule: ...|$|E
50|$|Episodic {{memories}} can {{be stored}} in autoassociative neural networks (e.g., a <b>Hopfield</b> <b>network)</b> if the stored representation includes information on the spatiotemporal context in which an item was studied.|$|E
40|$|AbstractThe {{discrete}} delayed <b>Hopfield</b> neural <b>networks</b> is {{an extension}} of the discrete <b>Hopfield</b> neural <b>networks.</b> In this paper, the convergence of discrete delayed <b>Hopfield</b> neural <b>networks</b> is mainly studied, and some results on the convergence are obtained by using Lyapunov function. Several new sufficient conditions for the delayed networks converging towards a limit cycle with period at most 2 are proved in parallel updating mode. Also, some conditions for the delayed networks converging towards a limit cycle with 2 -period are investigated in parallel updating mode. All results established in this paper extend the previous results on the convergence of both the discrete <b>Hopfield</b> neural <b>networks,</b> and the discrete delayed <b>Hopfield</b> neural <b>networks</b> in parallel updating mode...|$|R
50|$|Initialization of the <b>Hopfield</b> <b>Networks</b> is done {{by setting}} {{the values of the}} units to the desired start pattern. Repeated updates are then {{performed}} until the network converges to an attractor pattern. Convergence is generally assured, as Hopfield proved that the attractors of this nonlinear dynamical system are stable, not periodic or chaotic as in some other systems. Therefore, in the context of <b>Hopfield</b> <b>Networks,</b> an attractor pattern is a final stable state, a pattern that cannot change any value within it under updating.|$|R
40|$|Abstract—A {{model of}} neurons with {{hysteresis}} (or hysteresis binary neurons) for the <b>Hopfield</b> neural <b>networks</b> is studied. We prove theoretically that the emergent collective {{properties of the}} original <b>Hopfield</b> neural <b>networks</b> also {{are present in the}} <b>Hopfield</b> neural <b>networks</b> with hysteresis binary neurons. As an example, the networks are also applied to the maximum cut problem and results of computer simulations are presented and used to illustrate the computation power of the networks. The simulation results show that the <b>Hopfield</b> neural <b>networks</b> with hysteresis binary neurons are much better than other existing neural network methods for solving the maximum cut problem in terms of both the computation time and the solution quality. Keywords—maximum cut problem, <b>Hopfield</b> neural <b>network,</b> hysteresis, collective properties, NP-complete problem 1...|$|R
5000|$|This energy {{function}} {{is analogous to}} that of a <b>Hopfield</b> <b>network.</b> As in general Boltzmann machines, probability distributions over hidden and/or visible vectors are defined in terms of the {{energy function}}: ...|$|E
50|$|John Joseph Hopfield (born July 15, 1933) is an American {{scientist}} {{most widely}} {{known for his}} invention of an associative neural network in 1982. It is now more {{commonly known as the}} <b>Hopfield</b> <b>Network.</b>|$|E
5000|$|There {{are various}} {{different}} learning rules {{that can be}} used to store information in the memory of the <b>Hopfield</b> <b>Network.</b> It is desirable for a learning rule to have both of the following two properties: ...|$|E
40|$|Most {{scheduling}} {{problems have}} been demonstrated to be NP-complete problems. The <b>Hopfield</b> neural <b>network</b> is commonly applied to obtain an optimal solution in various different scheduling applications, such as the traveling salesman problem (TSP), a typical discrete combinatorial problem. <b>Hopfield</b> neural <b>networks,</b> although providing rapid convergence to the solution, require extensive effort to determine coefficients. A competitive learning rule provides a highly effective means of attaining a sound solution and can reduce the effort of obtaining coefficients. Restated, the competitive mechanism reduces the network complexity. This important feature {{is applied to the}} <b>Hopfield</b> neural <b>network</b> to derive a new technique, i. e. the competitive <b>Hopfield</b> neural <b>network</b> technique. This investigation employs the competitive <b>Hopfield</b> neural <b>network</b> to resolve a multiprocessor problem with no process migration, time constraints (execution time and deadline), and limited resources. Simulation results demonstrate that the competitive <b>Hopfield</b> neural <b>network</b> imposed on the proposed energy function ensures an appropriate approach to solving this class of scheduling problems...|$|R
30|$|<b>Hopfield</b> neural <b>network</b> {{was applied}} in the {{boundary}} detection of wood images. We designed a novel cost function for a <b>Hopfield</b> neural <b>network</b> to detect a defect boundary as solving an optimization problem. After the boundary initiation using Canny edge algorithm, a slight adjustment {{can be made to}} seek the actual boundary which will be implemented by a <b>Hopfield</b> neural <b>network</b> with the cost function. Those points that decreased the network energy were detected as boundary points. Taking advantage of the collective computational ability and energy convergence capability of the <b>Hopfield</b> neural <b>network,</b> the experiment received a good result. As shown in the Figures 6 – 14, the method based on <b>Hopfield</b> neural <b>network</b> in detecting boundary of wood defects was effective; the noises were effectively removed. We can get a more noiseless and vivid wood defect boundary. Thus, a promising method of wood boundary detection based on <b>Hopfield</b> neural <b>network</b> with a novel cost function is provided. All the courses of image processing and building a <b>Hopfield</b> neural <b>network</b> in this paper were implemented using the tools of Matlab. The tools of Matlab are well done in the study of images.|$|R
50|$|Some {{examples}} of neural network training techniques are backpropagation, quick propagation, conjugate gradient descent, projection operator, Delta-Bar-Delta etc. Some unsupervised network architectures are multilayer perceptrons, Kohonen <b>networks,</b> <b>Hopfield</b> <b>networks,</b> etc.|$|R
50|$|The Boltzmann machine can {{be thought}} of as a noisy <b>Hopfield</b> <b>network.</b> It is one of the first neural {{networks}} to demonstrate learning of latent variables (hidden units). Boltzmann machine learning was at first slow to simulate, but the contrastive divergence algorithm speeds up training for Boltzmann machines and Products of Experts.|$|E
5000|$|A Boltzmann machine, like a <b>Hopfield</b> <b>network,</b> is {{a network}} of units with an [...] "energy" [...] defined for the overall network. Its units produce binary results. Unlike Hopfield nets, Boltzmann machine units are stochastic. The global energy, , in a Boltzmann machine is {{identical}} in form {{to that of a}} Hopfield network: ...|$|E
5000|$|... where [...] is {{the weight}} of the {{connection}} from neuron [...] to neuron , [...] is the number of training patterns, and [...] the th input for neuron [...] This is learning by epoch (weights updated after all the training examples are presented). Again, in a <b>Hopfield</b> <b>network,</b> connections [...] are set to zero if [...] (no reflexive connections).|$|E
50|$|Various {{types of}} attractors {{may be used}} to model {{different}} types of network dynamics. While fixed-point attractor networks are the most common (originating from <b>Hopfield</b> <b>networks),</b> other types of networks are also examined.|$|R
40|$|After the {{original}} work of Hopfield and Tank, {{a lot of}} modified <b>Hopfield</b> neural <b>network</b> models have been proposed for combinatorial optimization problems. Recently, a positively selffeedbacked <b>Hopfield</b> neural <b>network</b> architecture was proposed by Li et al. and successfully applied to crossbar switching problem. In this paper, we analysis {{the dynamics of the}} positively self-feedbacked <b>Hopfield</b> neural <b>network,</b> then show the role of the self-feedback and point out where the good performance comes from. Based on the theoretical analysis, we get better simulation results for crossbar switching problem by selecting suitably positive self-feedback value of the network. Key words: positively self-feedbacked <b>Hopfield</b> neural <b>network,</b> crossbar switching problem, combinatorial optimization problems. 1...|$|R
30|$|In a <b>Hopfield</b> neural <b>network,</b> a neuron can {{not only}} be used for an input neuron, but also an output neuron. Every <b>Hopfield</b> neural <b>network</b> has a {{so-called}} cost function (or an energy function), which is used for measuring stability of a <b>Hopfield</b> neural <b>network.</b> Signals were circularly transmitted in the whole network. The operation course {{can be regarded as}} a recovered and strengthened processing for an input signal. In the course, the network approach gradually to a stable state when the cost function is minimized. If a problem can be mapped to the task of minimizing a cost function, the <b>Hopfield</b> neural <b>network</b> will be implemented to obtain an optimal (or near optimal) solution.|$|R
