0|714|Public
40|$|We {{present a}} method for monocular {{kinematic}} pose estimation and activity recognition from video for movement imitation. Learned vocabularies of kinematic motion primitives are used emulate the function of hypothesized neuroscientific models for spinal fields and mirror neurons {{in the process of}} imitation. For imitation, we assume the movement of a demonstrator is produced through a “virtual trajectory ” specifying desired body poses over time. Each pose represents a decomposition into mirror neuron firing coefficients that specify the attractive dynamics to this configuration through a linear combination of primitives. Each primitive is a nonlinear dynamical system that predicts expected motion with respect to an underlying activity. Our aim is to invert this process by estimating a demonstrator’s <b>virtual</b> trajectory from <b>monocular</b> <b>image</b> observations in a bottom-up fashion. At our lowest level, pose estimates are inferred in a modular fashion {{through the use of a}} particle filter with each primitive. We hypothesize the likelihood of these pose estimates over time emulate the firing of mirror neurons from the formation of the virtual trajectory. We present preliminary results our method applied to video composed of multiple activities performed at various speeds and viewpoints...|$|R
40|$|For {{the online}} control of movement, it is {{important}} to respond fast. The extent to which cues are effective in guiding our actions might therefore depend on how quickly they provide new information. We compared the latency to alter a movement when monocular and binocular cues indicated that the surface slant had changed. We found that subjects adjusted their movement in response to three types of information: information about the new slant from the <b>monocular</b> <b>image,</b> information about the new slant from binocular disparity, and information about the change in slant from the change in the <b>monocular</b> <b>image.</b> Responses to changes in the <b>monocular</b> <b>image</b> were approximately 40 ms faster than responses to a new slant estimate from binocular disparity and about 90 ms faster than responses to a new slant estimate from the <b>monocular</b> <b>image.</b> Considering these delays, adjustments of ongoing movements to changes in slant will usually be initiated by changes in the <b>monocular</b> <b>image.</b> The response will later be refined on the basis of combined binocular and monocular estimates of slant. © ARVO...|$|R
40|$|When one <b>monocular</b> <b>image</b> {{contains}} a red square partly occluding a green square, {{and the other}} <b>monocular</b> <b>image</b> is the same except that the green square is partly occluding the red one, the two images resemble each other's amodal completion. Observers typically perceive two complete squares as if the red and green surfaces are transparent or penetrating each other at their overlapping image location, which never appears yellow. With this example, we introduce dichoptic completion as a perception with the following characteristics. (1) Similar to binocular rivalry, it is evoked by dichoptic stimuli with <b>monocular</b> <b>images</b> so disparate that they cannot arise from physical scenes; however, (2) it occurs when objects inferred from one <b>monocular</b> <b>image</b> are identified with, or do not conflict with, objects inferred from the other; and, consequently, (3) it {{is a form of}} perceptual superposition, distinct from the result of binocular summation or rivalry...|$|R
40|$|Error {{concealment}} is {{an important}} field of research in image processing. Many methods were applied to conceal block losses in <b>monocular</b> <b>images.</b> In this paper we present a concealment strategy for block loss in stereoscopic image pairs. Unlike the error concealment techniques used for <b>monocular</b> <b>images,</b> the information of the associated image is utilized, i. e., {{by means of a}} projective transformation model, pixel values from the associated stereo image are warped to their corresponding positions in the lost block. The stereoscopic depth perception is much less affected in our approach than using monoscopic error concealment techniques...|$|R
30|$|IMM is a {{database}} consisting of 240 annotated <b>monocular</b> <b>images</b> of 40 different human faces. Points of correspondence {{are placed on}} each image so the dataset can be readily used for building statistical models of shape.|$|R
40|$|This paper {{presents}} {{a new approach}} towards the reconstruction of human posture from <b>monocular</b> video <b>images</b> that contain unrestricted human posture and human movement. It is a way towards low cost motion capture {{and at the same}} time avoids many limitations of the classical motion capture methods. A parameterized human skeleton model based on anatomy is used with the angular constraints encoded in the joints. Criterion Function is defined to represent the residuals between feature points in the <b>monocular</b> <b>image</b> and the corresponding points resulted from projecting the human model to the projection plane. A procedure is developed to recovery the whole body posture, and angle constraints are considered to obtain more natural human postures. Our model makes it feasible to reconstruct any possible human postures, and hence possible to generate highly realistic human animation. The experimental results from many <b>monocular</b> <b>images</b> are satisfactory...|$|R
5000|$|Fusional vergence is the {{movement}} of both eyes that enables the fusion of <b>monocular</b> <b>images</b> producing binocular vision. It is especially important when a person has heterophoria. Premotor cells for fusional vergence {{are located in the}} mesencephalon near the oculomotor nucleus.|$|R
40|$|We {{present an}} {{important}} aspect of our human-robot communication interface which is being developed in the context of our long-term research framework PERSES dealing with highly interactive mobile companion robots. Based on a multi-modal people detection and tracking system, we present a hierarchical neural architec- ture that estimates a target point at the floor indicated by a pointing pose, thus enabling a user to navigate a mo- bile robot to a specific target position in his local surroundings by means of pointing. In this context, we were especially interested in determining whether it is possible to accomplish such a target point estimator using only <b>monocular</b> <b>images</b> of low-cost cameras. The estimator has been implemented and experimentally investigated on our mobile robotic assistant HOROS. Although only <b>monocular</b> <b>image</b> data of relatively poor quality were util- ized, the estimator accomplishes a good estimation performance, achieving an accuracy better than that of a hu- man viewer on the same data. The achieved recognition results demonstrate that it is in fact possible to realize a user-independent pointing direction estimation using <b>monocular</b> <b>images</b> only, but further efforts are necessary to improve the robustness of this approach for everyday application...|$|R
40|$|We {{describe}} a model-based object tracking system that updates the configuration parameters {{of an object}} model based upon information gathered from a sequence of <b>monocular</b> <b>images.</b> Realistic object and imaging models are {{used to determine the}} expected visibility of object features, and to determine the expected appearance of all visible features. We formulate the tracking problem as one of parameter estimation from partially observed data, and apply the Extended Kalman Filtering (EKF) algorithm. The models are also used to determine what point feature movement reveals about the configuration parameters of the object. This information is used by the EKF to update estimates for parameters, and for the uncertainty in the current estimates, based on observations of point features in <b>monocular</b> <b>images.</b> ...|$|R
40|$|Automated {{detection}} of postures {{and actions of}} humans from image/video data is aluablc for many applications. Some examples are automated surveillance. effective Human- Computer Interaction and Entertainment such as computer games. An essential step ill performing this task is to detect {{the presence of a}} human in a scene and acquire parameters of a predefined body model. Most of the existing research uses multiple cameras to obtain a full view of the human body for modelling. Where <b>monocular</b> <b>images</b> are used. using markers for identifying different parts or joints of the body is common. In most svsterns based on <b>monocular</b> <b>images.</b> it is assumed that the human body is occluded onlv b itself Marker-less automated human body model acquisition using monocular video in the presence of occlusion is still a challenging task...|$|R
40|$|Abstract. This paper {{provides}} {{an evaluation of}} two methods of shape descriptors (Compact Fourier Descriptors and Relevance Vector Regression) for human pose estimation. The first method recovers human poses in <b>monocular</b> <b>images</b> using Compact Fourier Descriptors [8]. In this method {{the performance of the}} Fourier Descriptors is measured with two sampling methods and with a range [8 - 128] of coefficients. This approach is evaluated with the recovery of upper body poses from the descriptors. The second method [1] provides a learning-based approach to recover 3 D human body pose from single and <b>monocular</b> <b>images.</b> Both the methods are compared {{on the basis of the}} evaluation results provided by the authors. In this course of the evaluation the strength and weaknesses of each method are analyzed, and it is analyzed that both the methods performs well under different parameters...|$|R
40|$|We {{consider}} {{the task of}} depth estimation from a single <b>monocular</b> <b>image.</b> We take a supervised learning approach to this problem, in which we begin by collecting a training set of <b>monocular</b> <b>images</b> (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap {{as a function of}} the image. Depth estimation is a challenging problem, since local features alone are insufficient to estimate depth at a point, and one needs to {{consider the}} global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps. ...|$|R
40|$|International audienceWe will {{describe}} our ongoing work on learning-based methods for recovering 3 D human body pose and motion from single <b>images</b> and from <b>monocular</b> <b>image</b> sequences. The methods work directly with raw image observations and require neither an explicit 3 D body model nor a prior labelling of body {{parts in the}} image...|$|R
40|$|An {{approach}} to road parameter recognition in <b>monocular</b> <b>image</b> sequences under egomotion is described. Two stabilization mechanisms are {{integrated into the}} basic recursive estimation procedure: a short sequence averaging of individual measurements and a scalar filter for long-term stabilization. Besides the bottom-up measurements a second type of road measurements is provided- synthetic measurements derived from model-based road stripe tracking. ...|$|R
40|$|The {{problem of}} depth-from-motion using a <b>monocular</b> <b>image</b> {{sequence}} is considered. A pixel-based model is developed for direct depth estimation within a Kaiman filtering framework. A method is proposed for incorporating local surface structure into the Kaiman filter. Experimental results are provided {{to illustrate the}} effect of structural information on depth estimation. © 1999 IEEE. published_or_final_versio...|$|R
40|$|Abstract — In this paper, {{we present}} a neural {{architecture}} {{that is capable of}} estimating a target point from a pointing gesture, thus enabling a user to command a mobile robot to a specific position in his local surroundings by means of pointing. In this context, we were especially interested to determine whether it is possible to implement a target point estimator using only <b>monocular</b> <b>images</b> of low-cost webcams. The feature extraction is also quite straightforward: We use a gabor jet to extract the feature vector from the normalized camera images; and a cascade of Multi Layer Perceptron (MLP) Classifiers as estimator. The System was implemented and tested on our mobile robotic assistant HOROS. The results indicate that it is in fact possible to realize a pointing estimator using <b>monocular</b> <b>image</b> data, but further efforts are necessary to improve the accuracy and robustness of our approach. I...|$|R
40|$|We {{present a}} two stage {{technique}} for estimation of rigid body motion and scene structure from <b>monocular</b> <b>image</b> sequences. The robustness of motion recovery is considerably increased by motion dependent distortion of rectangular measurement windows. In the first stage, we estimate {{a set of}} candidate motion parameter sets from which the optimum is determined in a second step evaluating motion based image content deformation together with corresponding depth values. Experimental {{results show that the}} motion parameter estimation error can be reduced significantly. 1. INTRODUCTION A vast variety of applications require the reconstruction of a 3 -dimensional scene from a <b>monocular</b> <b>image</b> sequence. Model-based video communication systems attempt to extract information about the 3 -dimensional structure of the scene to be transmitted. The control of autonomously moving vehicles requires the reconstruction of the surrounding scene to avoid collisions. Future computer-based movie and TV productions [...] ...|$|R
40|$|Abstract. We {{present an}} {{approach}} for joint inference of 3 D scene struc-ture and semantic labeling for monocular video. Starting with <b>monocular</b> <b>image</b> stream, our framework produces a 3 D volumetric semantic + occu-pancy map, {{which is much}} more useful than a series of 2 D semantic label images or a sparse point cloud produced by traditional semantic segmen-tation and Structure from Motion(SfM) pipelines respectively. We derive a Conditional Random Field (CRF) model defined in the 3 D space, that jointly infers the semantic category and occupancy for each voxel. Such a joint inference in the 3 D CRF paves the way for more informed priors and constraints, which is otherwise not possible if solved separately in their traditional frameworks. We make use of class specific semantic cues that constrain the 3 D structure in areas, where multiview constraints are weak. Our model comprises of higher order factors, which helps when the depth is unobservable. We also make use of class specific semantic cues to reduce either the degree of such higher order factors, or to approximately model them with unaries if possible. We demonstrate improved 3 D struc-ture and temporally consistent semantic segmentation for difficult, large scale, forward moving <b>monocular</b> <b>image</b> sequences. Fig. 1. Overview of our system. From <b>monocular</b> <b>image</b> sequence, we first obtain 2 D semantic segmentation, sparse 3 D reconstruction and camera poses. We then build a volumetric 3 D map which depicts both 3 D structure and semantic labels. ...|$|R
40|$|In this article, we {{tackle the}} problem of depth {{estimation}} from single <b>monocular</b> <b>images.</b> Compared with depth estimation using multiple images such as stereo depth perception, depth from <b>monocular</b> <b>images</b> is much more challenging. Prior work typically focuses on exploiting geometric priors or additional sources of information, most using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) set new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimations can be naturally formulated as a continuous conditional random field (CRF) learning problem. Therefore, here we present a deep convolutional neural field model for estimating depths from single <b>monocular</b> <b>images,</b> aiming to jointly explore the capacity of deep CNN and continuous CRF. In particular, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework. We then further propose an equally effective model based on fully convolutional networks and a novel superpixel pooling method, which is ∼ 10 times faster, to speedup the patch-wise convolutions in the deep model. With this more efficient model, {{we are able to}} design deeper networks to pursue better performance. Experiments on both indoor and outdoor scene datasets demonstrate that the proposed method outperforms state-of-the-art depth estimation approaches. Comment: Appearing in IEEE T. Pattern Analysis and Machine Intelligence. Journal version of arXiv: 1411. 6387. Test code is available at [URL]...|$|R
40|$|This paper {{presents}} the method for detecting pedestrian recently implemented on the ARGO vehicle. The {{perception of the}} environment is performed through the sole processing of images acquired from a vision system installed on board of the vehicle: the analysis of a <b>monocular</b> <b>image</b> delivers a first coarse detection, while a distance refinement is performed thanks to a stereo vision technique...|$|R
40|$|Abstract We {{propose a}} robust {{methodology}} for 3 D model-based markerless tracking of textured objects in <b>monocular</b> <b>image</b> sequences. The technique {{is based on}} mutual infor-mation maximization, a widely known criterion for multi-modal image registration, and employs an efficient multires-olution strategy {{in order to achieve}} robustness while keeping fast computational time, thus achieving near real-time per-formance for visual tracking of complex textured surfaces...|$|R
40|$|We {{present an}} {{approach}} for automatic 3 D human pose reconstruction from <b>monocular</b> <b>images,</b> {{based on a}} discriminative formulation with latent segmentation inputs. We advance the field of structured prediction and human pose reconstruction on several fronts. First, by working with a pool of figure-ground segment hypotheses, the prediction problem is formulated in terms of combined learning and inference over segment hypotheses and 3 D human articular configurations. Besides constructing tractable formulations for the combined segment selection and pose estimation problem, we propose new augmented kernels that can better encode complex dependencies between output variables. Furthermore, we provide primal linear re-formulations based on Fourier kernel approximations, in order to scale-up the non-linear latent structured prediction methodology. The proposed models are shown to be competitive in the HumanEva benchmark and are also illustrated in a clip collected from a Hollywood movie, where the model can infer human poses from <b>monocular</b> <b>images</b> captured in complex environments. 1...|$|R
40|$|Moving {{vehicles}} are detected and tracked automatically in <b>monocular</b> <b>image</b> sequences from road traffic scenes recorded by a stationary camera. In order {{to exploit the}} a priori knowledge about shape and motion of vehicles in traffic scenes, a parameterized vehicle model is used for an intraframe matching process and a recursive estimator based on a motion model is used for motion estimation. An interpretation cycle supports the intraframe matching process with a state MAP-update step. Initial model hypotheses are generated using an image segmentation component which clusters coherently moving image features into candidate representations of images of a moving vehicle. The inclusion of an illumination model allows taking shadow edges of the vehicle into account during the matching process. Only such an elaborate combination of various techniques has enabled us to track vehicles under complex illumination conditions and over long (over 400 frames) <b>monocular</b> <b>image</b> sequences. Results on vari ous real-world road traffic scenes are presented and open problems as well as future work are outlined...|$|R
40|$|An {{approach}} for estimating 3 D head orientation in a <b>monocular</b> <b>image</b> sequence is proposed. The approach employs recently developed image-based parameterized tracking for face and face features {{to locate the}} area in which a sub-pixel parameterized shape estimation of the eye's boundary is performed. This involves tracking of five points (four at the eye corners and the fifth is the tip of the nose). We describe an approach that relies on the coarse structure of the face to compute orientation relative to the camera plane. Our approach employs projective invariance of the cross-ratios of the eye corners and anthropometric statistics to estimate the head yaw, roll and pitch. Analytical and experimental results are reported. 1 Introduction We present an algorithm for estimating the orientation of a human face from a single <b>monocular</b> <b>image.</b> The algorithm takes advantage of the geometric symmetries of typical faces to compute the yaw and roll components of orientation, and anthropometric mo [...] ...|$|R
40|$|In video {{telephone}} scenes to be transmitted at low data rates, non [...] diffuse illumination {{has to be}} expected. Therefore, in this work the scene illumination is automatically estimated from a <b>monocular</b> <b>image</b> sequence in addition to shape and motion of moving objects. Image analysis uses the source model "moving rigid 3 D objects with nonuniform local illumination". The irradiance is modelled by a discrete 3 D irradiance map. Experiments report an interframe prediction error gain over a former method ignoring illumination. This gain depends {{on the amount of}} object motion and the nonuniform portion of scene illumination. A first experiment analysing a real image sequence states an interframe prediction gain of about 1 dB. 1 Introduction In Object [...] Based Analysis [...] Synthesis Coding (OBASC) [2], three dimensional information has to be automatically extracted from two dimensional projections of a real 3 D scene containing moving 3 D objects. Working on a <b>monocular</b> <b>image</b> sequence, a 3 D model based [...] ...|$|R
40|$|The person {{detection}} module can be decomposed into three submodules (see figure I), the image-This {{article focuses on}} a new approach for three- sequence preprocessor, the modelling part and the dimensional people detection in <b>monocular</b> <b>image</b> {{detection module}} (the hypothesis generator), which sequences taken from stationary cameras with fixed are explained in the following. focal lengths. u The main contribution of the new algorithm is {{the combination of the}} following four (Sequence] parts. An abstract articulated geometrical person model, a scene model including a platform and a camera model, a hypothesis generator for people positions based on the CONDENSATION algorithm, and a fast clustering approach for the people detection. This approach allows the robust detection of several persons on the platform in realtime. This approach demonstrates that even with <b>monocular</b> <b>image</b> processing algorithms it is possible to detect people in realtime and to obtain information on their three-dimensional locations. Figure 1 : Program Structur...|$|R
40|$|The {{human brain}} {{seems to have}} the ability of {{inferring}} shape from the binocular fusion of some kinds of <b>monocular</b> <b>images.</b> Recently, we have observed that photometric stereo (PS) <b>images,</b> which are <b>monocular</b> <b>images</b> obtained under different illuminations, produce a vivid impression of depth, when viewed under a stereoscope. Lately, we have found that {{the same is true of}} pairs of images obtained in different spectral bands. Employing an optical-flow based photometric stereo algorithmon a type of "colour separated images", whichhave been so produced as to emulate the kinds of records generated by the photosensitivecells in the human retina, we have been able to obtain depth estimates from them. This has led us to speculate on the possibility that a process similar to PS could work on the human visual system. Here we present our claim for a natural photometric stereo process, invoking some physical and biological arguments, along with experimental results, that could support it...|$|R
40|$|Lunar Digital Elevation Model (DEM) is {{important}} for lunar successful landing and exploration missions. Lunar DEMs are typically generated by photogrammetry or laser altimetry approaches. Photogrammetric methods require multiple stereo images of the region of interest {{and it may not}} be applicable in cases where stereo coverage is not available. In contrast, reflectance based shape reconstruction techniques, such as shape from shading (SfS) and shape and albedo from shading (SAfS), apply <b>monocular</b> <b>images</b> to generate DEMs with pixel-level resolution. We present a novel hierarchical SAfS method that refines a lower-resolution DEM to pixel-level resolution given a <b>monocular</b> <b>image</b> with known light source. We also estimate the corresponding pixel-wise albedo map in the process and based on that to regularize the shape reconstruction with pixel-level resolution based on the low-resolution DEM. In this study, a Lunar-Lambertian reflectance model is applied to estimate the albedo map. Experiments were carried out using <b>monocular</b> <b>images</b> from the Lunar Reconnaissance Orbiter Narrow Angle Camera (LRO NAC), with spatial resolution of 0. 5 - 1. 5. m per pixel, constrained by the Selenological and Engineering Explorer and LRO Elevation Model (SLDEM), with spatial resolution of 60. m. The results indicate that local details are well recovered by the proposed algorithm with plausible albedo estimation. The low-frequency topographic consistency depends on the quality of low-resolution DEM and the resolution difference between the image and the low-resolution DEM. Department of Land Surveying and Geo-Informatics 2016 - 2017 > Academic research: refereed > Publication in refereed journalbcr...|$|R
40|$|In {{the present}} paper we {{address the problem of}} {{computing}} structure and motion, given a set point and/or line correspondences, in a <b>monocular</b> <b>image</b> sequence, when the camera is not calibrated. We first describe an algebraic approach of the problem and then relate the different equations to a specific geometric construction which allows to solve the motion problem. An implementation is proposed and experimental results are provided...|$|R
40|$|In {{this paper}} {{the problem of}} obtaining 3 D models from image {{sequences}} is addressed. The presented method deals with uncalibrated <b>monocular</b> <b>image</b> sequences. No prior knowledge about the scene or about the camera is necessary to build the 3 D models. This approach is very flexible, even allowing the camera zoom to be used, and has no restriction {{on the size of}} the scenes...|$|R
40|$|BACKGROUND: How {{the visual}} system {{combines}} {{information from the}} two eyes to form a unitary binocular representation of the external world is a fundamental question in vision science {{that has been the}} focus of many psychophysical and physiological investigations. Ding & Sperling (2006) measured perceived phase of the cyclopean image, and developed a binocular combination model in which each eye exerts gain control on the other eye's signal and over the other eye's gain control. Critically, the relative phase of the monocular sine-waves plays a central role. METHODOLOGY/PRINCIPAL FINDINGS: We used the Ding-Sperling paradigm but measured both the perceived contrast and phase of cyclopean images in three hundred and eighty combinations of base contrast, interocular contrast ratio, eye origin of the probe, and interocular phase difference. We found that the perceived contrast of the cyclopean image was independent of the relative phase of the two monocular gratings, although the perceived phase depended on the relative phase and contrast ratio of the <b>monocular</b> <b>images.</b> We developed a new multi-pathway contrast-gain control model (MCM) that elaborates the Ding-Sperling binocular combination model in two ways: (1) phase and contrast of the cyclopean images are computed in separate pathways, although with shared cross-eye contrast-gain control; and (2) phase-independent local energy from the two <b>monocular</b> <b>images</b> are used in binocular contrast combination. With three free parameters, the model yielded an excellent account of data from all the experimental conditions. CONCLUSIONS/SIGNIFICANCE: Binocular phase combination depends on the relative phase and contrast ratio of the <b>monocular</b> <b>images</b> but binocular contrast combination is phase-invariant. Our findings suggest the involvement of at least two separate pathways in binocular combination...|$|R
40|$|We {{present a}} novel method for vision-based {{recovery}} of three-dimensional structures through simultaneous model reconstruction and camera position tracking from <b>monocular</b> <b>images.</b> Our approach does {{not rely on}} robust feature detecting schemes (such as SIFT, Good Features to Track etc.), but works directly on intensity values in the captured images. Thus, it is well-suited for reconstruction of surfaces that exhibit only little texture due to partial homogeneity of the surfaces. ...|$|R
40|$|An {{approach}} for estimating the pose and motion of a known moving object {{in three dimensions}} from a sequence of <b>monocular</b> <b>images</b> is considered. The principle is to obtain initial estimates of the pose and motion parameters and to update them by using feature location measurements made from subsequent monocular ima. ge frames. The {{ultimate goal is to}} use the obtained estimates for controlling the movements of a robot arm...|$|R
40|$|Nonuniform {{irradiance}} {{is estimated}} from a <b>monocular</b> <b>image</b> sequence. Image analysis uses the source model " 3 D rigid object with 3 D motion and local illumination" allowing for 3 D motion and nonuniform colour. The irradiance is modelled by a reflection map. First experimental results of estimating and compensating nonuniform irradiance using a synthesized image sequence are given. 1 Introduction In object [...] oriented analysis [...] synthesis coding [5][6] three dimensional information {{has to be}} extracted from two dimensional projections of a real 3 D scene containing moving 3 D objects. Working on a <b>monocular</b> <b>image</b> sequence, a 3 D model based image analysis algorithm was presented [2]. The images are segmented into static background and moving objects. Each moving real object is modelled by a 3 D model object. A model object is defined by its shape, motion and colour. The shape is rigid and represented by control points connected by a mesh of triangles, the motion is represented by six 3 D motion p [...] ...|$|R
40|$|Augmenting {{cloth in}} real video is a {{challenging}} task because cloth performs complex motions and deformations and produces complex shading on the surface. Therefore, for a realistic augmentation of cloth, parameters describing both deformation {{as well as}} shading properties are needed. Furthermore, objects occluding the real surface {{have to be taken}} into account as on the one hand they affect the parameter estimation and on the other hand should also occlude the virtually textured surface. This is especially challenging in <b>monocular</b> <b>image</b> sequences where a 3 -dimensional reconstruction of complex surfaces is difficult to achieve. In this paper, we present a method for cloth retexturing in <b>monocular</b> <b>image</b> sequences under external occlusions without a reconstruction of the 3 -dimensional geometry. We exploit direct image information and simultaneously estimate deformation and photometric parameters using a robust estimator which detects occluded pixels as outliers. Additionally, we exploit the estimated parameters to establish an occlusion map from local statistical color models of texture surface patches that are established during tracking. With this information we can produce convincing augmented results...|$|R
40|$|Abstract. We {{propose a}} {{hierarchical}} process for inferring the 3 D pose {{of a person}} from <b>monocular</b> <b>images.</b> First we infer a learned view-based 2 D body model from a single image using non-parametric belief propagation. This approach integrates information from bottom-up body-part proposal processes and deals with self-occlusion to compute distributions over limb poses. Then, we exploit a learned Mixture of Experts model to infer a distribution of 3 D poses conditioned on 2 D poses. This approach is more general than recent work on inferring 3 D pose directly from silhouettes since the 2 D body model provides a richer representation that includes the 2 D joint angles and the poses of limbs that may be unobserved in the silhouette. We demonstrate the method in a laboratory setting where we evaluate {{the accuracy of the}} 3 D poses against ground truth data. We also estimate 3 D body pose in a <b>monocular</b> <b>image</b> sequence. The resulting 3 D estimates are sufficiently accurate to serve as proposals for the Bayesian inference of 3 D human motion over time. ...|$|R
