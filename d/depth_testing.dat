24|743|Public
2500|$|The Mark 14 torpedo {{tended to}} run some [...] too deep for several reasons. The first {{was that it}} was tested with an {{exercise}} warhead that was more buoyant than the warhead; that was a precaution made to avoid losing an expensive torpedo. A light exercise head made the torpedo positively buoyant, so it would float to the surface {{at the end of its}} run. The live warhead contained more mass, so it reached equilibrium at a lower depth. Also, the depth mechanism was designed prior to the warhead's explosive charge being increased, making the torpedo even heavier overall. [...] "Testing conditions became more and more unrealistic, obscuring the effect of the heavier warhead on depth performance." [...] Furthermore, the <b>depth</b> <b>testing</b> device used by NTS to verify the torpedo's running depth (the depth and roll recorder) had the same measuring port placement error as the Mark 14's depth control port, so both were off by the same amount in the same direction and gave the impression that the torpedo was running at the desired depth when it was actually much deeper. After hearing of the deep-running torpedo problem, most submarine skippers simply set their torpedoes' running depth to zero, which risked broaching the torpedo.|$|E
2500|$|Discovery later {{rolled out}} from the VAB to Launch Complex 39A on 4 August 2009, in a slow drive {{on the top of}} the Crawler-transporter. The [...] rollout began at 02:07 EDT, and ended with the launch {{platform}} secured in place at about 13:50 EDT. The move took longer than expected due to adverse weather conditions, which included lightning warnings. The crawler also had to pause occasionally so mud could be removed from its treads and bearings. Technicians then quickly prepared the shuttle to host the crew’s countdown dress rehearsal known as the Terminal Countdown Demonstration Test (TCDT). Discovery’s seven astronauts flew to Kennedy on 5 August 2009 for the training activity which concludes later in the week with a complete practice countdown, minus liftoff, involving the crew and the launch team. Meanwhile, in an unprecedented operation, modifications were made to the left Solid rocket booster on the pad. The modifications involved replacement of a check valve filter assembly in the booster which was found to have broken. In a potentially delaying factor, in <b>depth</b> <b>testing</b> of the external tank with X-ray revealed voids in the foam which might have formed during the injection molding of the foam. This has also been decided as a suspect factor in the foam shedding during STS-127. The air in the voids could have expanded due to the high temperatures generated during ascent thus breaking the foam. The reviews considered a rollback as an option since the defect could not be set right in the pad. Later, the tank was cleared for launch as is without any additional inspections.|$|E
50|$|She was not {{completed}} {{because of the}} end of the war, however the hull was used for crush <b>depth</b> <b>testing</b> before arrival in June 1950 to Messrs. Smith & Houston of Port Glasgow, Scotland to be broken up.|$|E
40|$|Shadow mapping is a {{technique}} for doing real-time shadowing. Recent work has shown that shadow mapping hardware {{can be used as}} a second <b>depth</b> <b>test</b> in addition to the z-test. In this paper, we explore the computational power provided by this second <b>depth</b> <b>test</b> by examining the problem of rendering objects described as CSG (Constructive Solid Geometry) expressions. We provide an algorithm that asymptotically improves the number of rendering passes required to display a CSG object by a factor of n by exploiting the two-sided <b>depth</b> <b>test.</b> Interestingly, a matching lower bound can be proved demonstrating that our algorithm is optimal...|$|R
40|$|Shadow mapping is a {{technique}} for doing real-time shadowing. Recent work has shown that shadow mapping hardware {{can be used as}} a second <b>depth</b> <b>test</b> in addition to the z-test. In this paper, we explore the computational power provided by this second <b>depth</b> <b>test,</b> by demonstrating its utility in two separate applications. We firs...|$|R
30|$|Three 150  ×  150  ×  150  mm cubes for {{the water}} {{penetration}} <b>depth</b> <b>test.</b>|$|R
5000|$|The website Digital Photography Review {{rated the}} D50 highly and noted its {{improved}} noise performance over the D70s {{it was largely}} derived from, although it had fewer hobbyist features (making the D50 the first de-specified entry-level DSLR camera thus lowering the MSRP) and a lower price than the D70s. In <b>depth</b> <b>testing</b> by the Images Resources website and the October 2005 [...] "Hoshi Nabi" [...] (Star Navigator) Magazine, indicated that the D50 achieved its low noise through exceptionally good performance at the sensor level. Testing by Roger Clark confirmed that the D50 had higher gain and lower noise than the D70. Tests of the D50 at all ISO levels indicated that it did offer low noise without loss of contrast or detail.|$|E
5000|$|The Mark 14 torpedo {{tended to}} run some 10 ft too deep for several reasons. The first {{was that it}} was tested with an {{exercise}} warhead that was more buoyant than the warhead; that was a precaution made to avoid losing an expensive torpedo. The live warhead contained more mass, and it reached buoyancy equilibrium at a lower depth. Also, the depth mechanism was designed prior to the warhead's explosive charge being increased, making the torpedo even heavier overall. [...] "Testing conditions became more and more unrealistic, obscuring the effect of the heavier warhead on depth performance." [...] Furthermore, the <b>depth</b> <b>testing</b> device used by NTS to verify the torpedo's running depth (the depth and roll recorder) had the same measuring port placement error as the Mark 14's depth control port, so both were off by the same amount in the same direction and gave the impression that the torpedo was running at the desired depth when it was actually much deeper. After hearing of the deep-running torpedo problem, most submarine skippers simply set their torpedoes' running depth to zero, which risked broaching the torpedo.|$|E
50|$|Discovery later {{rolled out}} from the VAB to Launch Complex 39A on 4 August 2009, in a slow drive {{on the top of}} the Crawler-transporter. The 3.4 mi rollout began at 02:07 EDT, and ended with the launch {{platform}} secured in place at about 13:50 EDT. The move took longer than expected due to adverse weather conditions, which included lightning warnings. The crawler also had to pause occasionally so mud could be removed from its treads and bearings. Technicians then quickly prepared the shuttle to host the crew’s countdown dress rehearsal known as the Terminal Countdown Demonstration Test (TCDT). Discovery’s seven astronauts flew to Kennedy on 5 August 2009 for the training activity which concludes later in the week with a complete practice countdown, minus liftoff, involving the crew and the launch team. Meanwhile, in an unprecedented operation, modifications were made to the left Solid rocket booster on the pad. The modifications involved replacement of a check valve filter assembly in the booster which was found to have broken. In a potentially delaying factor, in <b>depth</b> <b>testing</b> of the external tank with X-ray revealed voids in the foam which might have formed during the injection molding of the foam. This has also been decided as a suspect factor in the foam shedding during STS-127. The air in the voids could have expanded due to the high temperatures generated during ascent thus breaking the foam. The reviews considered a rollback as an option since the defect could not be set right in the pad. Later, the tank was cleared for launch as is without any additional inspections.|$|E
5000|$|ISA 400 {{talks about}} the [...] "walk through testing" [...] or {{auditing}} in <b>depth</b> <b>test.</b>|$|R
50|$|The {{source code}} is hosted on GitHub. An {{integration}} server runs tests at each commit and in <b>depth</b> <b>tests</b> at regular intervals.|$|R
5000|$|The <b>depth</b> <b>test</b> is performed, {{fragments}} {{that pass}} will get {{written to the}} screen and might get blended into the frame buffer.|$|R
40|$|The Sequenced Convex Subtraction (SCS) {{algorithm}} is a hardware based multi-pass image-space algorithm for general purpose Constructive Solid Geometry (CSG) Rendering. Convex objects combined by volumetric intersection, difference and union are rendered in real-time without b-rep pre-processing. OpenGL stencil and <b>depth</b> <b>testing</b> {{is used to}} determine the visible surface for each pixel on the screen...|$|E
40|$|The {{compatibility}} of the Multimission Modular Spacecraft (MMS) Ground Support Software System (GSSS), currently operational on a ModComp IV/ 35, {{with the}} VAX 11 / 780 system is discussed. The compatibility is examined in various key {{areas of the}} GSSS through the results of in <b>depth</b> <b>testing</b> performed on the VAX 11 / 780 and ModComp IV/ 35 systems. The compatibility of the GSSS with the ModComp CLASSIC is presented based upon projections from ModComp supplied literature...|$|E
40|$|Testing to {{determine}} the significance as a State Archaeological Landmark and {{to determine}} eligibility to the National Register of Historic Places was undertaken in June 1984 of Site 41 MX 23, Morris County, Texas, on FM 3421. The test shows {{that portion of the}} site within the project’s right-of-way to be sparse in cultural material. The deposits containing the material vary from very shallow to moderate in <b>depth.</b> <b>Testing</b> indicates that insufficient cultural materials exist within the area to be impacted to warrant further investigations...|$|E
50|$|Enable the <b>depth</b> map <b>test,</b> and {{render the}} scene lit. Areas where the <b>depth</b> map <b>test</b> fails {{will not be}} overwritten, and remain shadowed.|$|R
30|$|The thick PHCS {{specimens}} over 315  mm in <b>depth</b> <b>tested</b> in {{this study}} showed very brittle shear failure modes, but their web-shear capacities were not reduced by the size effect.|$|R
30|$|Retail {{tests are}} “experiments, called tests, in which {{products}} are {{offered for sale}} under carefully controlled conditions in {{a small number of}} stores” (Fisher & Rajaram 2000). Such a test is used to test customer reaction to variables such as price, product placement or store design. If the test is used to predict season sales for a product it is called a <b>depth</b> <b>test</b> (Fisher & Rajaram 2000). In a <b>depth</b> <b>test</b> the test outlets are usually oversupplied in order to avoid stock-outs which usually distorts the forecast. The forecast is then used for the total season demand, which is ordered from a supplier {{before the start of the}} selling period.|$|R
40|$|What {{the experts}} {{have to say}} about Model-Based Testing for Embedded Systems: "This book is exactly what is needed at the exact right time in this fast-growing area. From its {{beginnings}} over 10 years ago of deriving tests from UML statecharts, model-based testing has matured into a topic with both breadth and <b>depth.</b> <b>Testing</b> embedded systems is a natural application of MBT, and this book hits the nail exactly on the head. Numerous topics are presented clearly, thoroughly, and concisely in this cutting-edge book. The authors are world-class leading experts in this area and teach us well-use...|$|E
40|$|This report {{documents}} the present practices involved in completing a wake survey at the National Research Council Canada (Institute for Ocean Technology). To begin the report, background information {{is provided in}} defining a wake survey. The introduction also provides a very brief synopsis of the history behind wake survey procedures, comparing past and present equipment used in implementation. Following this, preparation methods used on the wake probe during this work term; both pitot tube bleeding and water <b>depth</b> <b>testing</b> are addressed. Within water <b>depth</b> <b>testing</b> is explanation of the three different tests: Flow samples, step samples, and drift samples. Then there is a brief discussion on the testing and data analysis stages of wake surveying. Unfortunately though, these stages were never reached during this work term. This was due to various behavioural problems with particular pressure transducers which showed tendencies of unresponsiveness after lengthy submersion in water. However, {{this was not the}} only obstacle causing difficulty during this semester. Instruments used in transmitting pressure signals have also acted inadequately. To learn more about the performance of all equipment used in the wake survey process, various troubleshooting methods have been used and are documented. These include testing for relationships between wire impedance, voltage, and applied hydrostatic pressure in each transducer. The results and conclusions of these tests close the discussion. To complete the report, three recommendations are made which may assist in guiding any future work completed on improving wake survey equipment. Peer reviewed: NoNRC publication: Ye...|$|E
40|$|The Sequenced Convex Subtraction(SCS) {{algorithm}} is a hardware based multi-pass image-space lgorithm for general purpose Constructive Solid Geometry (CSG) Rendering. Convex objects combined by volumetric intersection, difference and union are rendered in real-time without b-rep re-processing. OpenGL stencil and <b>depth</b> <b>testing</b> {{is used to}} determine the visible surface for each pixel on the screen. This paper introduces a specialised algorithm for CSG Rendering of intersected convex objects,we call SCS-Intersect. This new technique requires linear time with respect to the number of intersections. SCS-Intersect is primarily of interest as an optimisation to the SCS algorithm for rendering CSG trees of convex objects. A revised formulation of the SCS CSG Rendering {{algorithm is}} presented in this pape...|$|E
50|$|During the {{rendering}} of computer graphics, the rasterization step takes a primitive, described by its vertex coordinates with associated {{color and texture}} information, and converts it into a set of fragments. These fragments then undergo a series of processing steps, e.g. scissor <b>test,</b> alpha <b>test,</b> <b>depth</b> <b>test,</b> stencil test, blending, texture mapping and so on. These steps are collectively referred to as fragment processing.|$|R
40|$|We {{present a}} {{real-time}} algorithm for rendering volumetric 3 D Magic Lenses# having arbitrary convex shapes. During fragment processing the algorithm performs a second <b>depth</b> <b>test</b> using a shadow map. Exploiting the second <b>depth</b> <b>test</b> {{we are able}} to classify each fragment, with respect to its position relative to the lens volume. Using this classification we first render the geometry behind the lens volume, then the geometry intersecting the lens volume using a different visual appearance and finally the parts in front of the lens volume. Regardless of the shape of the lens volume just two additional rendering passes are needed. Furthermore there are no limitations to the choice of visual appearance used to enhance expressiveness of the virtual world. We will describe theoretical and practical aspects of the algorithm and our implementation, which is accelerated by current graphics hardware...|$|R
5000|$|... 2. Stencil Test Function passes/Depth Test Function fails: [...] If say func is GL_ALWAYS, the stencil {{test will}} always pass, but <b>depth</b> <b>test</b> may fail. Neither Color/Depth buffer are {{modified}}. Stencil buffer is modified as per glStencilOp zfail. If say glStencilOp(GL_KEEP, GL_INCR, GL_KEEP) then GL_INCR takes place and stencilValue = (stencilValue+1) // will become 1 ...|$|R
40|$|Figure 1 : Triple Depth Culling {{introduces}} controllable early <b>depth</b> <b>testing</b> {{for highly}} complex scenes with arbitrary shader operations. Virtual worlds feature increasing geometric and shading complexi-ties, {{resulting in a}} constant need for effective solutions to avoid ren-dering objects invisible for the viewer. This observation is partic-ularly true {{in the context of}} real-time rendering of highly occluded environments such as urban areas, landscapes or indoor scenes. This problem has been intensively researched in the past decades, resulting in numerous optimizations building upon the well-known Z-buffer technique. Among them, extensions of graphics hardware such as early Z-culling [Morein 2000] efficiently avoid shading most of invisible fragments. However, this technique is not applica-ble when the fragment shader discards fragments or modifies their depth value, or if alpha testing is enabled [nVidia 2008]. We introduce Triple Depth culling for fast and controllable per-pixel visibility at the fragment shading stage using multiple dept...|$|E
40|$|This {{document}} provides supplemental {{material to}} simplify implementing volumetric shadows using our technique {{described in the}} paper Volumetric Shadows using Polygonal Light Volumes [BSA 10]. 1 Implementation Details To summarize, our shafts of light algorithm consists of the following steps: 1. Create the shadow map by rendering the scene into a depth buffer (see Figure 1). 2. Render scene from the camera with diffuse lighting, attenuating incoming and outgoing light due to absorption and scattering in media (see code listing below). Hard shadows on surfaces can be added in this step using standard shadow mapping. 3. Construct the mesh from the shadow map, with or without adaptive tesselation. The simplest is to skip adaptive tessellation, which is fast {{as long as the}} shadow map resolution is not very large (less than 4 k × 4 k). 4. Render the mesh with <b>depth</b> <b>testing</b> disabled and additive blending enabled. Th...|$|E
40|$|Interests {{have been}} a central focus of {{counselling}} psychology (and vocational psychology in particular) for over 100 years. The awareness of professional interests increases self-knowledge and provides occupational information. In career counselling, vocational interests are assessed more frequently than any other vocational construct, though early evaluations (before 13 years old) of professional interests are very rare. The aim {{of this research is}} to examine the 3 IP construct (Iconographic Professional Interests Inventory; an inventory composed of 65 stylised pictures that represent people in the act of performing a job) in <b>depth,</b> <b>testing</b> more models in addition to the 19 vocational areas proposed in the 3 IP manual. Results show that most of the vocational areas can be grouped into 4 second-level areas (“things”, “people”, “leisure”, and “culture”). Moreover, Holland’s RIASEC model is tested; an accurate selection of items reveals that this model works well using 24 specific jobs. The research concludes that the inventory has good psychometric qualities which can grow further by mean of the increasing, in a targeted way, of the number of jobs...|$|E
5000|$|... 3. Stencil Function passes/Depth Function passes: [...] If say func is GL_ALWAYS, the stencil {{test will}} always pass. If <b>depth</b> <b>test</b> also passes. Both Color/Depth buffer are {{modified}}. Stencil buffer is modified as per glStencilOp zpass. If say, glStencilOp(GL_KEEP, GL_KEEP, GL_KEEP) then Stencil values are not changed, only Color and Depth buffers are modified.|$|R
50|$|In the {{simplest}} case, the stencil buffer {{is used to}} limit the area of rendering (stenciling). More advanced usage of the stencil buffer makes use of the strong connection between the depth buffer and the stencil buffer in the rendering pipeline. For example, stencil values can be automatically increased/decreased for every pixel that fails or passes the <b>depth</b> <b>test.</b>|$|R
50|$|The simple {{combination}} of <b>depth</b> <b>test</b> and stencil modifiers make {{a vast number}} of effects possible (such as stencil shadow volumes, Two-Sided Stencil, compositing, decaling, dissolves, fades, swipes, silhouettes, outline drawing or highlighting of intersections between complex primitives) though they often require several rendering passes and, therefore, can put a heavy load on the graphics hardware.|$|R
40|$|We {{present a}} method to utilize the Shadow Volume Algorithm by Crow and Williams without using a stencil buffer. We show that the shadow mask can be {{generated}} in the alpha channel {{or even in the}} screen buffer, if a hardware accelerated stencil buffer is not available. In comparison to the original stencil buffer method, a small speed up can be achieved, if the shadow mask is computed in the alpha buffer. The method using the screen buffer requires the scene to be rendered a second time after the shadow mask has been computed. Both methods are less restrictive with respect to hardware requirements, since we use only standard color blending and <b>depth</b> <b>testing.</b> In general, rasterization bandwidth is the main bottle neck when generating the shadow mask at high screen resolutions. In order to overcome this bottle neck we propose a way to compute the shadow mask at a resolution that is lower than the resolution of the screen buffer. Then the shadow mask is applied to the scene by utilizing texture mapping. The latter method might be reasonable especially in interactive entertainment, where rendering speed is traded in favour of image quality...|$|E
40|$|An {{innovative}} X-ray {{imaging sensor}} with intrinsic digital characteristics is presented. It {{is based on}} Chromatic Photon Counting technology. The detector is able to count individually the incident X-ray photons and to separate them according to their energy (two 'color' images per exposure). The energy selection occurs in real time and at radiographic imaging speed (GHz global counting rate). Photon counting, color mode and a very high spatial resolution (more than 10 l. p. /mm at MTF 50) allow to obtain an optimal ratio between image quality and absorbed dose. The individual block of the imaging system is a two-side buttable semiconductor radiation detector made of a thin pixellated CdTe crystal (the sensor) coupled to a large area VLSI CMOS pixel ASIC. 1, 2, 4, 8 tile units have been built. The 8 tiles unit has 25 cm x 2. 5 cm sensitive area. Results and images obtained from in <b>depth</b> <b>testing</b> of several configurations of the system are presented. The X-Ray imaging system is the technological platform of PIXIRAD Imaging Counters s. r. l., a recently constituted INFN spin-off company. Comment: 8 pages, 5 figures, 3 table...|$|E
40|$|Road slope {{surface erosion}} can be {{addressed}} {{through a variety of}} methods, one method is to utilize the plant media. Plants can affect both to reduce erosion slope surface, because the beads of rain that falls can be mitigated through the leaves of the plant. Some herbs have diverse uses, environmentally friendly, effective and easy to maintain as Vetiver grass. Vetiver System is a simple, low cost technology by utilizing live vetiver plant for soil and water conservation and environmental protection. Vetiver System is very practical, inexpensive, easy to maintain, and extremely effective in controlling soil erosion and sedimentation, water conservation, and stabilization and rehabilitation. The test is divided into two parts, namely testing for the native land and land that has been planted with Vetiver grass. On native soil to test the physical and mechanical properties of the soil. While on land that has been planted with Vetiver grass testing the mechanical properties only. Each soil sample on testing the mechanical properties will be given depth variation, hat is a depth of 0 - 30 cm and 30 - 60 cm. Each <b>depth</b> <b>testing</b> direct shear and unconfined compression test. Based on test results, the Vetiver grass crops can improve soil physical and mechanical properties...|$|E
5000|$|The Correspondence School {{started in}} 1891 as a Question and Answer {{column in the}} pages of the mining journal titled [...] "Colliery Engineer and Metal Miner" [...] {{published}} by Mr Foster. This column was a response to the in <b>depth</b> <b>tests</b> required of miners and inspectors by the [...] "New" [...] Pennsylvania Mine Safety Act of 1885.|$|R
50|$|One {{aircraft}} {{converted in}} 1961 {{for use in}} SK-1 nuclear <b>depth</b> charge <b>tests.</b>|$|R
40|$|We {{present a}} novel {{approach}} using rasterization hardware to perform the following query: Given a collection of convex polytopes in 3 D, find the closest point from some given point inside the polytopes {{to the surface of}} the union of the polytopes. The algorithm takes advantage of multi-pass rendering, clipping and <b>depth</b> <b>tests.</b> We also demonstrate its application to penetration depth computation...|$|R
