9|26|Public
50|$|The {{values of}} TP-DCS are defined in GSM {{recommendation}} 03.38. Messages sent via this encoding can be encoded in the default GSM 7-bit alphabet, the 8-bit <b>data</b> <b>alphabet,</b> and the 16-bit UCS-2 alphabet.|$|E
50|$|In mobile {{telephony}} GSM 03.38 or 3GPP 23.038 {{is a character}} set used in the Short Message Service of GSM based cell phones. It is defined in GSM recommendation 03.38. Messages sent via this encoding can be encoded in the default GSM 7-bit alphabet, the 8-bit <b>data</b> <b>alphabet,</b> and the 16-bit UCS-2 alphabet. Support of the GSM 7-bit alphabet is mandatory for GSM handsets and network elements, but characters in languages such as Arabic, Chinese, Korean or Japanese languages must be encoded using the 16-bit UCS-2 character encoding or an extended national language shift table.|$|E
5000|$|The # is {{a marker}} used {{to show that}} the end of the message has been reached. There are thus 26 symbols in the {{plaintext}} alphabet (the 26 capital letters A through Z), and the # character represents a stop code. We arbitrarily assign these the values 1 through 26 for the letters, and 0 for '#'. (Most flavors of LZW would put the stop code after the <b>data</b> <b>alphabet,</b> but nothing in the basic algorithm requires that. The encoder and decoder only have to agree what value it has.) ...|$|E
40|$|This report {{contains}} {{about the program}} that has been completed made and then give the title character ”the introduction of Javanese Alphabet using canvas html 5 and php”. The program is made with based on a fact that occurs, especially {{in the world of}} education on Javanese Alphabet. This lessons are simply moribund, in education for now, Javanese Alphabet very little tangent. Many students will gain well lessons, while Javanese Alphabet is one way to preserve culture, moreover culture java. This makes writer interested in some the introduction of Javanese Alphabet, against the background of this writer would like to lessons not forgotten and with the goal of to help students in understanding lessons. In making this program to write not just looking for of reference Javanese Alphabet but also to the memory writer about java a script. Then writer continue to make the design to be made, from here writer start with programmed first one is the introduction of Javanese Alphabet, program that made with need data coordinates to form Javanese Alphabet in a canvas. In making the second random the programm to display character Javanese Alphabet in random, where in this program make a switch to display <b>data</b> Javanese <b>alphabet</b> randomly. And last program programmed to write Javanese alphabet, in making this third program writers are looking for <b>data</b> Javanese <b>Alphabet</b> script to make a character Javanese alphabet with sandhangan. Of any of the stages already raised writer make 3 menu, where the first is the introduction of java script, the second is the interactive, where the was featuring java randomly script, and last program made writer is writing java script into a sentence. The program is made to help learning to ease...|$|R
40|$|XML is fast {{becoming}} the intergalactic <b>data</b> speak <b>alphabet</b> for <b>data</b> and information exchange that hides the heterogeneity among the components of Loosely-coupled, distributed systems and provides the glue that allows the individual components {{to take part in}} the loosely integrated system. Since much of this data is currently stored in relational database systems, simplifying the transformation of this data from and to XML in general and from and to the agreed upon exchange schema specifically is an important feature that should improve the productivity of the programmer and the efficiency of this process. This article provides an overview over the features that are needed to provide access via HTTP and XML and presents the approach taken in Microsoft SQL Server. Keywords: Loosely-coupled, distributed system architectures, XML, relational database systems...|$|R
40|$|This report {{evaluates the}} {{performance}} of uncompressed and compressed substring indexes on build time, space usage and search performance. It is shown how the structures react to increasing <b>data</b> size, <b>alphabet</b> size and repetitiveness in the data. The main contribution is the strong relationship shown between time performance and locality in the data structures. As an example, it is shown that for a large alphabet, suffix tree construction can be speeded up by a factor 16, and query lookup by a factor 8, if dynamic arrays are used to store the lists of children for each node instead of linked lists, {{at the cost of}} using about 20 % more space. And for enhanced suffix arrays, query lookup is up to twice as fast if the data structure is stored as an array of structs instead of a set of arrays, at no extra space cost. ...|$|R
5000|$|Transmission {{of short}} {{messages}} between the SMSC and the handset is done whenever using the Mobile Application Part (MAP) of the SS7 protocol. [...] Messages are sent with the MAP MO- and MT-ForwardSM operations, whose payload length {{is limited by}} the constraints of the signaling protocol to precisely 140 bytes (140 bytes * 8 bits / byte = 1120 bits). Short messages can be encoded using a variety of alphabets: the default GSM 7-bit alphabet, the 8-bit <b>data</b> <b>alphabet,</b> and the 16-bit UCS-2 alphabet. Depending on which alphabet the subscriber has configured in the handset, this leads to the maximum individual short message sizes of 160 7-bit characters, 140 8-bit characters, or 70 16-bit characters. GSM 7-bit alphabet support is mandatory for GSM handsets and network elements, but characters in languages such as Arabic, Chinese, Korean, Japanese, or Cyrillic alphabet languages (e.g., Russian, Ukrainian, Serbian, Bulgarian, etc.) must be encoded using the 16-bit UCS-2 character encoding (see Unicode). Routing data and other metadata is additional to the payload size.|$|E
40|$|This {{application}} uses to displaying searching name, {{in search}} of name all data fill in the linked list, linked list consisting of nim, name, address and phone. Linked list itself has the advantage in the allocation of memory, so the data can be accommodated {{as much as we}} want. This application also use Hash table for to accelerate the search time. Hash Table also can accommodate <b>data</b> <b>alphabet</b> to match the existing data in the linked list...|$|E
40|$|We {{extend a}} {{previously}} introduced space-time modulation scheme (which we call matrix modulation) to the practically important cases of rank-deficient channels and multiple users. In particular, we prove an identifiability result and present an efficient demodulation method for these cases. Our matrix modulation technique {{does not require}} channel state information at the transmitter or receiver side. Its structure {{is strong enough to}} permit joint data and channel estimation without knowledge of the <b>data</b> <b>alphabet.</b> In the multiuser case, matrix modulation yields a flexible transmission scheme that can easily be adapted to different data-rate requirements of individual users. Numerical simulations demonstrate the good performance of the matrix modulation/demodulation technique. 1...|$|E
40|$|This paper {{provides}} a security method {{which can be}} used for <b>data</b> that contains <b>alphabets,</b> numerals and some special symbols during their transmission. A discussion about cryptology and the existing Polybius cipher is made. The existing Polybius cipher is based on the use of a 5 X 5 matrix of letters constructed using numbers from 1 to 5. This square can allow the text that contains alphabets only. For this reason, we have proposed an improvement to the existing Polybius cipher, in which an 8 X 8 matrix can be constructed...|$|R
40|$|The {{theme of}} this {{research}} is to provide security for the <b>data</b> that contains <b>alphabets</b> numerals and special characters during its transmission. However because of the drawbacks inherent in the classical Playfair cipher which adversely affects the security, this research proposed 3 D-Playfair Cipher (4 X 4 X 4 Playfair cipher) which works on trigraph rather than using digraph which eliminates the fact that a diagram and its reverse will encrypt in a similar fashion. 3 D-Playfair cipher supports all 26 alphabets A-Z, 10 digits 0 - 9 and 28 special characters ! “ #...|$|R
40|$|Abstract — Cryptography {{has been}} through {{numerous}} phases of evolution. Early ciphers in cryptography were designed to allow encryption and decryption to take place by hand, while those which are developed and used today are only possible due to the high computational performance of modern machines [1]. Conceptually cryptographic technique divided into two categories substitution and transposition methods. Transposition Ciphers are a bit different to Substitution Ciphers. Whereas Substitution ciphers replace each letter with a different letter or symbol to produce the cipher text, in a Transposition cipher, the letters are just moved around. There are numerous number of algorithms are proposed in both techniques. But combination of these techniques rare in the cryptographic trends. So this paper is providing combination of caesar cipher and railfence cipher are proposed. Also the caesar cipher key generation new way to proposed using automatic key generation techniques. Our algorithm supports security for the <b>data</b> containing <b>alphabets</b> with case sensitive, numbers and special characters. The proposed method {{can be used to}} simply encode the message for preserving privacy. It is difficult to understand the cipher text. ...|$|R
40|$|For fixed k ≥ 2 {{and fixed}} <b>data</b> <b>alphabet</b> of {{cardinality}} m, the hierarchical type class of a data string of length n = kj for some j ≥ 1 is formed by permuting the string in all possible ways under permutations {{arising from the}} isomorphisms of the unique finite rooted tree of depth j which has n leaves and k children for each non-leaf vertex. Suppose the data strings in a hierarchical type class are losslessly encoded via binary codewords of minimal length. A hierarchical entropy function is a function {{on the set of}} m-dimensional probability distributions which describes the asymptotic compression rate performance of this lossless encoding scheme as the data length n is allowed to grow without bound. We determine infinitely many hierarchical entropy functions which are each self-affine. For each such function, an explicit iterated function system is found such that the graph of the function is the attractor of the system...|$|E
40|$|ITC/USA 2007 Conference Proceedings / The Forty-Third Annual International Telemetering Conference and Technical Exhibition / October 22 - 25, 2007 / Riviera Hotel & Convention Center, Las Vegas, NevadaShaped offset {{quadrature}} {{phase shift}} keying (SOQPSK) is a highly bandwidth efficient modulation technique used widely in military and aeronautical telemetry standards. It can be classified as a form of continuous phase modulation (CPM), but its major distinction from other CPM schemes is that it has a constrained (correlated) ternary <b>data</b> <b>alphabet.</b> CPM-based detection models for SOQPSK have been developed only recently. One roadblock {{standing in the way of}} these detectors being adopted is that existing symbol timing recovery techniques for CPM are not always applicable since the data symbols are correlated. We investigate the performance of one CPM-based timing error detector (TED) that can be used with SOQPSK, and apply it to the versions of SOQPSK used in military (MIL-STD SOQPSK) and telemetry group (SOQPSK-TG) standards. We derive the theoretical performance limits on the accuracy of timing recovery for SOQPSK, as given by the modified Cramer-Rao bound (MCRB), and show that the proposed TED performs close to these bounds in computer simulations and is free of false-lock points. We also show that the proposed scheme outperforms a non-data aided TED that was recently developed for SOQPSK. These results show that the proposed scheme has great promise in a wide range of applications due to its low complexity, strong performance, and lack of false-lock points...|$|E
40|$|In {{all of our}} {{preceding}} compression algorithms, we rst partitioned our datavector into {{blocks and}} then encoded each block into a string of codebits. Arithmetic coding presents a whole new philosophy of coding. In an arithmetic code, as we process the data samples in a datavector (X 1;X 2;:::;Xn) from left to right, we do not replace each sample Xi {{with a string of}} codebits|instead, we assign to each Xi a subinterval Ii of the unit interval [0; 1] so that I 1 I 2 ::: In and so that Ii is determined from the previously processed samples X 1;:::;Xi, 1 in conjunction with the current sample Xi. When the nal interval In is determined, then a codeword (B 1;B 2;:::;BL) is assigned to the entire datavector so that the rational number:B 1 B 2 :::BL given by:B 1 B 2 :::BL = B 1 = 2 +B 2 = 4 +B 3 = 8 +:::+ BL= 2 L is a point inIn. We will show that if the correct procedure for choosing the intervals I 1;I 2;:::;In is used, then arithmetic coding yields a compression rate at least as good as the entropy of the datavector, provided that the datavector is su ciently long. Thus, arithmetic codes outperform Hu man codes in compressing large data les. 6. 1 Description We consider the <b>data</b> <b>alphabet</b> A = f 0; 1; 2;:::;k, 1 g of size k. To arithmetically encode every datavector over A, the encoder and decoder agree in advance that a certain subinterval I(U) of[0; 1] is assigned to each nite sequence U of samples from A, so that the following rules are obeyed: (i) The intervals I(0);I(1);:::;I(k, 1) are nonoverlapping and their union is [0; 1]. (ii) For each possible U, the intervals I(Ui) (i = 0;:::;k, 1) are nonoverlapping and their union is I(U). (iii) Every interval I(U) is of positive length. (That is, intervals which degenerate to a single point are not allowed.) Here is the most common way in which the intervals I(U) are chosen: (iv) A probability vector [p(0);p(1);:::;p(k, 1) ] with strictly positive entries is chosen. Then, I(0) = [0;p(0) ] I(i) = [p(0) +:::+ p(i, 1);p(0) +:::+ p(i) ]; i = 1;:::;k, 1 (Note that the length of I(i) isp(i). ...|$|E
40|$|Abstract — This paper {{presents}} a comparative study to evaluate experimental results for approximate string matching algorithms {{on the basis}} of edit distance. We compare the algorithms {{in terms of the number}} of character comparisons and the running time for molecular <b>data,</b> binary <b>alphabets</b> English alphabets etc. The terms like word processors, web search engine, molecular sequence, DNA sequence analysis and natural language processing have lead to the development of many algorithms in the field of pattern matching in a string. Amongst the various string searching algorithms being used, here the focus is mainly approximate implementation of pattern matching algorithms such as Knuth-Morris-Pratt, Boyer-Moore, Raita, Horspool based on PHP. The comparison between these algorithms is done with the help of Levenshtein distance. It also describes the importance of design of efficient “Approximate Pattern Search Algorithms in molecular database, binary alphabets, English alphabets and so on”. This approach is advantageous from all other string-pattern matching algorithms in terms of time complexity. Therefore this procedure improves the efficiency of approximate string matching and gives the near-optimal results...|$|R
40|$|Abstract. Data {{trees and}} data words {{have been studied}} {{extensively}} in connection with XML reasoning. These are trees or words that, in addition to labels from a finite alphabet, carry labels from an infinite <b>alphabet</b> (<b>data).</b> While in general logics such as MSO or FO are undecidable for such extensions, decidablity results for their fragments have been obtained recently, most notably for the two-variable fragments of FO and existential MSO. The proofs, however, are very long and nontrivial, {{and some of them}} come with no complexity guarantees. Here we give a much simplified proof of the decidability of two-variable logics for data words with the successor and data-equality predicates. In addition, the new proof provides several new fragments of lower complexity. The proof mixes database-inspired constraints with encodings in Presburger arithmetic. ...|$|R
40|$|Recently data {{trees and}} data words have {{received}} {{considerable amount of}} attention in connection with XML reasoning and system verification. These are trees or words that, in addition to labels from a finite <b>alphabet,</b> carry <b>data</b> values from an infinite <b>alphabet</b> (<b>data).</b> In general it is rather hard to obtain logics for data words and trees that are sufficiently expressive, but still have reasonable complexity for the satisfiability problem. In this paper we extend and study the notion of Büchi automata for omega-words with data. We prove that the emptiness problem for such extension is decidable in elementary complexity. We then apply our result to show the decidability of two kinds of logics for omega-words with data: the two-variable fragment of first-order logic and some extensions of classical linear temporal logic for omega-words with data...|$|R
40|$|This paper {{reviews the}} {{development}} of active learning {{in the last decade}} under the perspective of treating of data, a major source of undecidability, and therefore a key problem to achieve practicality. Starting with the first case studies, in which data was completely disregarded, we revisit different steps towards dealing with data explicitly in active learning: We discuss Mealy Machines as a model for systems with (<b>data)</b> output, automated <b>alphabet</b> abstraction refinement as a two-dimensional extension of the partition-refinement based approach of active learning for inferring not only states but also optimal alphabet abstractions, and Register Mealy Machines, which can be regarded as programs restricted to data-independent data processing as it is typical for protocols or interface programs. We are convinced that this development has the potential to transform active automata learning into a technology of high practical importance...|$|R
40|$|A {{universal}} lossless {{data compression}} code called the multilevel pattern matching code (MPM code) is introduced. In processing a finite <b>alphabet</b> <b>data</b> string of length n, the MPM code operates at O(log log n) levels sequentially. At each level, the MPM code detects matching {{patterns in the}} input data string (substrings of the data appearing in two or more nonoverlapping positions). The matching patterns detected at each level are of a fixed length which decreases by a constant factor from level to level, until this fixed length becomes one at the final level. The MPM code represents information about the matching patterns at each level as a string of tokens, with each token string encoded by an arithmetic encoder. From the concatenated encoded token strings, the decoder can reconstruct the data string via several rounds of parallel substitutions. A O(1 = log n) maximal redundancy/sample upper bound is established for the MPM code with respect to any class of finite state sources of unifo [...] ...|$|R
40|$|Abstract — In today‘s {{electronics}} world human {{machine interface}} is important part. Pen with inbuilt inertial sensors devices capture human handwriting or drawing motions in real-time and use the sensor data for recognition. An inertial sensor based Inertial pen consist of an inertial sensor MPU 9150 (accelerometer gyroscope and magnetometer), microcontroller, and a wireless transmission module, for sensing and collecting movement <b>data</b> for writing <b>alphabet.</b> The sensor <b>data</b> is received and processed for alphabets, recognition. The recognition algorithm composes of the steps of sensor data acquisition, signal pre-processing, feature generation, feature selection, and classification. KNN Classifiers for classification among 26 capital alphabets classes is built. The project aims at to validate {{the effectiveness of the}} inertial pen based motion data acquisition and recognition of class of test sample from among 26 classes. The recognition accuracy achieved is 82 %. The recognition accuracy of 93 % is achieved for recognition of four gestures...|$|R
40|$|Human motion {{monitoring}} and activity classification, {{specifically in the}} free-living environment, are becoming increasingly important as preventative, diagnostic and rehabilitative measures in health and wellness applications. In contrast to gait analysis, wearable sensor-based evaluation of upper body activities is not well studied. The work in this thesis tends to explore a novel system for upper limb activity monitoring and classification. The system focuses specifically on the application of motion classification to a complex task of automating rehabilitation evaluation, such as the Wolf Motor Function Test. The presented system consists of a novel wearable motion sensor platform that integrates accelerometers, gyroscopes and flex-sensors, and classification algorithms that convert motion <b>data</b> into an <b>alphabet</b> representation and form a string of primitives. String expressions are then derived for each test item and a regular expression based searching method is developed. We present results from the successful application of the proposed system to upper limb activity characterization {{in the context of}} the Wolf Motor Function Test...|$|R
40|$|This paper {{reports on}} a variety of {{compression}} algorithms developed {{in the context of a}} project to put all the data files for a full-text retrieval system on a CD-Rom. In the context of inexpensive preprocessing, a text compression algorithm is presented that is based on Markov-modeled Huffman coding on an extended <b>alphabet.</b> <b>Data</b> structures are examined for facilitating random access into the compressed text. In addition, new algorithms are presented for compression of word indices, both the dictionaries (word lists), and the text pointers (concordances). 1 Introduction In this paper we discuss the problems of compressing a large textual database, and the auxiliary files necessary for convenient access, for storage on a CD-Rom. Given the remarkable advances being made in computer mass storage technology, the growing interest in data compression, as evidenced in the recent spurt of literature in this area, may be surprising (see, for example, This paper is a revision of [5] y Cent [...] ...|$|R
30|$|Identifying {{malicious}} software executables is made difficult {{by the constant}} adaptations introduced by miscreants in order to evade detection by antivirus software. Such changes are akin to mutations in biological sequences. Recently, high-throughput methods for gene sequence classification have been developed by the bioinformatics and computational biology communities. In this paper, we apply methods designed for gene sequencing to detect malware in a manner robust to attacker adaptations. Whereas most gene classification tools are optimized for and restricted to an alphabet of four letters (nucleic acids), we have selected the Strand gene sequence classifier for malware classification. Strand’s design can easily accommodate unstructured <b>data</b> with any <b>alphabet,</b> including source code or compiled machine code. To demonstrate that gene sequence classification tools are suitable for classifying malware, we apply Strand to approximately 500 GB of malware data provided by the Kaggle Microsoft Malware Classification Challenge (BIG 2015) used for predicting nine classes of polymorphic malware. Experiments show that, with minimal adaptation, the method achieves accuracy levels well above 95 % requiring {{only a fraction of}} the training times used by the winning team’s method.|$|R
40|$|Genomic DNA {{sequences}} {{have been}} represented as long text <b>data</b> composed of <b>alphabets</b> A, C, T and G and these sequences present visualization challenges due to {{massive amount of}} discrete and multi-dimensional data. In this paper, we proposed a visual technique called VBP (Visualization by Pentahedrons) algorithm to visualize and analyze similarity of DNA sequences. Here, Markov chain model is employed to calculate the state transition probabilities of DNA sequences and map them into four different pentahedrons. The top pinnacle serves as the VBP walk origin, and the other four bottom points is the destination when the next read symbol {{is one of the}} alphabet A, C, T, and G. Therefore, a three-dimensional trajectory can be drawn for visualization. Since this is a first order Markov model, four pentahedrons are totally available for the query sequence. When the target sequence is very close to the query sequence, the VBP walk of the both sequences in the three-dimensional space are very close to each other. While the case when both sequences are far away from each other, the walk traces will be even more apart since four pentahedons are based only on th...|$|R
40|$|International audienceData trees {{provide a}} {{standard}} abstraction of XML documents with data values: they are trees whose nodes, {{in addition to}} the usual labels, can carry labels from an infinite <b>alphabet</b> (<b>data).</b> Therefore, one is interested in decidable formalisms for reasoning about data trees. While some are known - such as the two-variable logic - they tend to be of very high complexity, and most decidability proofs are highly nontrivial. We are therefore interested in reasonable complexity formalisms as well as better techniques for proving decidability. Here we show that many decidable formalisms for data trees are subsumed - fully or partially - by the power of tree automata together with set constraints and linear constraints on cardinalities of various sets of data values. All these constraints can be translated into instances of integer linear programming, giving us an NP bound on the complexity of the reasoning tasks. We prove that this bound, as well as the key encoding technique, remain very robust, and allow the addition of features such as counting of paths and patterns, and even a concise encoding of constraints, without increasing the complexity. We also relate our results to several reasoning tasks over XML documents, such as satisfiability of schemas and data dependencies and satisfiability of the two-variable logic...|$|R
40|$|Abstract: Problem statement: The {{study of}} Malaysian Arabic phoneme is rarely found which make the {{references}} work difficult. Specific guideline on Malaysian subject is not found {{even though a}} lot of acoustic and phonetics {{research has been done}} on other languages such as English, French and Chinese. Approach: This study discussed about the correct and simplest way of Arabic phonemes pronunciation in Malay accent. The International Phonetic Alphabet of Arabic chart was considered as the reference of every recorded speech samples using Malaysian children for their sound localization (makhraj point) of every alphabet. The recorded sound was analysed to determine the origin of each <b>alphabet</b> <b>data</b> by measuring its formant frequencies. The consonants of Standard Arabic (SA) phonemes were studied and the appropriate place of articulation of every phoneme was measured through its formant. Results: Only seven out of 25 consonants of SA phonemes of the children’s samples did not give the appropriate formants value. The formants are /kof/, [ق], /zo/, [ظ], /kho/, [خ], /gheyn/, [غ], /ha/, [ح], /ain/, [ع] & /ha/, [] which consider as the difficult SA to utter among Malaysian children. Conclusion/Recommendations: The values obtained are used as the reference of the database for our recognition system...|$|R
40|$|Data trees {{provide a}} {{standard}} abstraction of XML documents with data values: they are trees whose nodes, {{in addition to}} the usual labels, can carry labels from an infinite <b>alphabet</b> (<b>data).</b> Therefore, one is interested in decidable formalisms for reasoning about data trees. While some are known – such as the two-variable logic – they tend to be of very high complexity, and most decidability proofs are highly nontrivial. We are therefore interested in reasonable complexity formalisms as well as better techniques for proving decidability. Here we show that many decidable formalisms for data trees are subsumed – fully or partially – by the power of tree automata together with set constraints and linear constraints on cardinalities of various sets of data values. All these constraints can be translated into instances of integer linear programming, giving us an NP bound on the complexity of the reasoning tasks. We prove that this bound, as well as the key encoding technique, remain very robust, and allow the addition of features such as counting of paths and patterns, and even a concise encoding of constraints, without increasing the complexity. We also relate our results to several reasoning tasks over XML documents, such as satisfiability of schemas and data dependencies and satisfiability of the two-variable logic...|$|R
40|$|We {{consider}} {{the problem of}} estimating functions of distributed data using a distributed algorithm over a network. The extant literature on computing functions in distributed networks such as wired and wireless sensor networks and peer-to-peer networks deals with computing linear functions of the distributed <b>data</b> when the <b>alphabet</b> size of the data values is small, O(1). We describe a distributed randomized algorithm to estimate a class of non-linear functions of the distributed data which is over a large alphabet. We consider three types of networks: point-to-point networks with gossip based communication, random planar networks in the connectivity regime and random planar networks in the percolating regime both of which use the slotted Aloha communication protocol. For each network type, we estimate the scaled k-th frequency moments, for k ≥ 2. Specifically, for every k ≥ 2, we give a distributed randomized algorithm that computes, with probability (1 -δ), an ϵ-approximation of the scaled k-th frequency moment, F_k/N^k, using time O(M^ 1 - 1 /k- 1 T) and O(M^ 1 - 1 /k- 1 N (δ^- 1) /ϵ^ 2) bits of transmission per communication step. Here, N {{is the number of}} nodes in the network, T is the information spreading time and M=o(N) is the alphabet size...|$|R
40|$|Problem statement: The {{study of}} Malaysian Arabic phoneme is rarely found which make&# 13; the {{references}} work difficult. Specific guideline on Malaysian subject is not found {{even though a}} lot of&# 13; acoustic and phonetics {{research has been done}} on other languages such as English, French and&# 13; Chinese. Approach: This study discussed about the correct and simplest way of Arabic phonemes&# 13; pronunciation in Malay accent. The International Phonetic Alphabet of Arabic chart was considered as&# 13; the reference of every recorded speech samples using Malaysian children for their sound localization&# 13; (makhraj point) of every alphabet. The recorded sound was analysed to determine the origin of each&# 13; <b>alphabet</b> <b>data</b> by measuring its formant frequencies. The consonants of Standard Arabic (SA) &# 13; phonemes were studied and the appropriate place of articulation of every phoneme was measured&# 13; through its formant. Results: Only seven out of 25 consonants of SA phonemes of the childrens&# 13; samples did not give the appropriate formants value. The formants are /kof/, [&# 1602;], /zo/, [&# 1592;], /kho/, [,[&# 1582;&# 13; /gheyn/, [&# 1594;], /ha/, [&# 1581;], /ain/, [&# 1593;] & /ha/, [&# 1581;] which consider as the difficult SA to utter among Malaysian&# 13; children. Conclusion/Recommendations: The values obtained are used as the reference of the&# 13; database for our recognition system. </div...|$|R
40|$|Data is {{compressible}} by presuming {{a priori}} knowledge {{known as a}} data model, and applying an appropriate encoding to produce a shorter description. The two aspects of compression data modeling and coding - however are not always conceived as distinct, nor implemented as such in compression systems, leading to difficulties of an architectural nature. For example, how would one make improvements upon a data model whose specific form has been standardized into the encoding and decoding processes? How would one design coding for new types of data such as in biology and finance, without creating a new system in each case? How would one compress data that has been encrypted when the conventional encoder requires data-in-the-clear to extract redundancy? And how would mobile acquisition devices obtain good compression with lightweight encoders? These and many other challenges can be tackled by an alternative compression architecture. This work contributes a complete "model-code separation" system architecture for compression, based on a core set of iterative message-passing algorithms over graphical models representing the modeling and coding aspects of compression. Systems following this architecture resolve the challenges posed by current systems, and stand to benefit further from future advances {{in the understanding of}} data and the algorithms that process them. In the main portion of this thesis, the lossless compression of binary sources is examined. Examples are compressed under the proposed architecture and compared against some of the best systems today and to theoretical limits. They show that the flexibility of model-code separation does not incur a performance penalty. Indeed, the compression performance of such systems is competitive with and sometimes superior to existing solutions. The architecture is further extended to diverse situations of practical interest, such as mismatched and partially known models, different <b>data</b> and code <b>alphabets,</b> and lossy compression. In the process, insights into model uncertainty and universality, <b>data</b> representation and <b>alphabet</b> translation, and model-quantizer separation and low-complexity quantizer design are revealed. In many ways, the proposed architecture is uniquely suitable for understanding and tackling these problems. Throughout, a discourse is maintained over architectural and complexity issues, with a view toward practical implementability. Of interest to system designers, issues such as rate selection, doping, and code selection are addressed, and a method similar to EXIT-chart analysis is developed for evaluating when compression is possible. Suggestions for system interfaces and algorithmic factorization are distilled, and examples showing compression with realistic data and tasks are given to complete the description of a system architecture accessible to broader adoption. Ultimately, this work develops one architecturally principled approach toward flexible, modular, and extensible compression system design, with practical benefits. More broadly, it represents the beginning of many directions for promising research at the intersection of data compression, information theory, machine learning, coding, and random algorithms. by Ying-zong Huang. Thesis: Ph. D., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2015. Cataloged from PDF version of thesis. Includes bibliographical references (pages 133 - 142) and index...|$|R
40|$|The {{main purpose}} of our {{research}} is to provide security for the <b>data</b> that contains <b>alphabets</b> and integer values during the transmission, when data is transmitted form sender to receiver. As we know that playfair technique if best for multiple letter encryption, which treats the plain text as single units and translates these units into cipher text. It is highly difficult to the attacker to understand or to decrypt the cipher text. The existing playfair technique {{is based on the}} use of a 5 X 5 matrix of letters constructed using a keyword. This algorithm can only allow the text that contains alphabets only. But many algorithms have been proposed that allow text which contains alphabets, integers as well as special symbols using 6 * 6 matrix and 10 * 9 matrix etc. In playfair technique a groups of 2 letters in the plain text is converted to cipher text during encryption using a key. Similarly on other hand during decryption cipher text are converted to plain text using the same key. Some time it may be possible for the attacker to understand the plaintext. To overcome this problem we proposed an algorithm that extends the security of playfair technique using excess 3 code and ceasar cipher technique where first each alphabets and integer is converted into binary number and then its equivalent excess 3 code and after that with the help of key encryption process will be apply. In our proposed technique we are using 6 * 6 matrix which contain alphabets and integers only...|$|R
40|$|We derive {{competitive}} {{tests and}} estimators for several properties of discrete distributions, {{based on their}} i. i. d. sequences. We focus on symmetric properties that depend only on the multiset of probability values in the distributions and not on specific symbols of the alphabet that assume these values. Many applications of probability estimation, statistics and machine learning involve such properties. Our method of probability estimation, called profile maximum likelihood (PML), involves maximizing the likelihood of observing {{the profile of the}} given sequences, i. e., the multiset of symbol counts in the sequences. It has been used successfully for universal compression of large <b>alphabet</b> <b>data</b> sources, and has been shown empirically to perform well for other probability estimation problems like classification and distribution multiset estimation. We provide competitive estimation guarantees for the PML method for several such problems. For testing closeness of distributions, i. e., finding whether two given i. i. d. sequences of length n are generated by the same distribution or by two different ones, our schemes have an error probability of at most sqrt(delta) * exp(7 n̂(2 / 3)) whenever the best possible error probability is delta 0 with error probability at most delta <= exp(- 6 n̂(1 / 2)), then the PML estimator is within a distance of 2 * epsilon with error probability at most delta * exp(6 n̂(1 / 2)). Equivalently, the PML estimator approximates distributions to within a distance of 2 * epsilon with error probability delta using sequences of length n' = O(n̂ 2 /loĝ 2 (1 / 4 delta),n). Thus, this estimator is competitive with other estimators, including the one by Valiant et al. that approximates distributions of superlinear support size k = O(epsilon̂(2. 1) * n * log(n)) to within a relative earthmover distance of epsilon and whose error probability can be shown to be at most exp(-n̂(0. 9)). However, unlike the case of closeness testing, we do not yet have efficient schemes for computing the PML distribution. We extend the results for PML for distribution multiset estimation to two related problems of estimating the parameter multiset of multiple distributions or processes. These include the problems of estimating the multiset of success probabilities of Bernoulli processes, and the multiset of means of Poisson distribution...|$|R
40|$|Given {{a set of}} integer {{keys from}} a bounded {{universe}} along with associated data, the dictionary problem asks to answer two queries: membership and retrieval. Membership has to tell whether a given element is in the dictionary or not; retrieval has to return the data associated with the searched key. This paper studies three well-established relaxations of this basic problem: (Compressed) Static functions, Approximate membership and Relative membership. (Compressed) Static functions. In this relaxation, also known as retrieval-only dictionaries, we are given a set S ⊆ U of n integers {{and each of them}} has associated <b>data</b> from an <b>alphabet</b> of size σ. The problem asks to build a dictionary that, given a key x∈ S, returns its associate data. Notice that, whenever x ∈ S, arbitrary data is returned. This problem has been widely studied in the past. Solutions to this problem have to carefully organize associated data, so that, they can be retrieved in constant time without the need of storing keys in S. Compressed static functions move a step forward: not only do not store the keys, but also achieve space complexities bounded in term of the entropy H_ 0 of the associated data. Such kind of solutions are very interesting mainly for two reasons. Firstly, being nH_ 0 at most nσ, these results are always at least as good as the uncompressed ones. Moreover, since the associated data often follow a skewed distribution, nH_ 0 could even become sublinear in n. The current best solutions are by Porat and Hreinsson et al. The first one requires nσ +o(n) bits of space, while the second one uses (1 +δ) nH_ 0 +n ·(p_ 0 + 0. 086, 1. 82 (1 -p_ 0)) bits of space, where p_ 0 is the probability of the most frequent symbol and δ is a constant greater than 0. Thus, the space complexities of these solutions are incomparable: the former has a sublinear overhead but is not compressed, while the latter is suboptimal due to the factor (1 +δ) to multiply H_ 0 and has an overhead that may be Θ(n) depending on p_ 0. Our optimal scheme achieves the best of the two being the first known solution obtaining simultaneously constant query time, compressed space (nH_ 0), and sublinear overhead (o(n)). We strongly believe that these characteristics makes the use of static functions significantly more appealing for many applications. Approximate membership. The approximate membership problem has been studied for decades and the Bloom filter data structure is probably the most popular and widely used technique solving it. With Bloom filters we can represent a set of n integers by using n 1 /ϵ e bits of space with false positive probability (fpp) ϵ. Both its space and time complexities are non-optimal: space is a constant factor away from optimal, and query time is logarithmic in 1 /ϵ. Constant time approximate membership data structures are able to achieve optimal n 1 /ϵ +o(n) bits of space only if 1 /ϵ is a power of two. The current best solution requires an O(n) bits overhead in addition to the optimal space, for an arbitrary fpp. Our optimal scheme is the first known solution having o(n) overhead, for any value of ϵ such that 1 /ϵ = o(n / n), namely, any reasonable choice of ϵ in practice. Relative membership. This problem asks to solve a further relaxation of the membership query. Given two sets S and R with S ⊂ R, we must be able to distinguish whether a key belongs to S or R∖ S. Our compressed static functions are used to obtain a constant time solution for the problem achieving an optimal space complexity (up to lower order term). ...|$|R

