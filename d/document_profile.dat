18|341|Public
40|$|This chapter {{introduces}} {{an approach}} of deriving taxonomy from documents using a novel <b>document</b> <b>profile</b> model that enables document representations with the semantic information systematically generated at the document sentence level. A frequent word sequence method is proposed {{to search for}} the salient semantic information and has been integrated into the <b>document</b> <b>profile</b> model. The experimental study of taxonomy generation using hierarchical agglomerative clustering has shown a significant improvement in terms of Fscore based on the <b>document</b> <b>profile</b> model. A close examination reveals that the integration of semantic information has a clear contribution compared to the classic bag-of-words approach. This study encourages us to further investigate the possibility of applying <b>document</b> <b>profile</b> model over a wide range of text based mining tasks. Department of Industrial and Systems Engineerin...|$|E
40|$|To {{characterize}} a user's {{preference and}} the social summary of a document, the user profile and the general <b>document</b> <b>profile</b> are widely adopted in the existing personalized ranking functions. However, in many real world situations, using these two profiles can not primely personalize the search results on the social Web because (i) different people usually have different perceptions on a same document and (ii) {{the information contained in}} the user profile is usually not comprehensive enough to characterize a user's preference. Therefore, we propose a dual personalized ranking (D-PR) function to improve the personalized search on the social Web by introducing two novel profiles: the extended user profile and the personalized <b>document</b> <b>profile,</b> to better characterize a user's preference and better summarize his personal perception on a document. Instead of using a same general <b>document</b> <b>profile</b> for all users, for each of the documents, our method computes each individual user a personalized <b>document</b> <b>profile</b> to characterize his perception on this document; while the extended user profile is defined as the sum of all of the user's personalized document profiles. Moreover, how to obtain the personalized <b>document</b> <b>profile</b> is a challenge, we propose a method to estimate it utilizing the perception similarities between users. A method used to quantify the perception similarity is also presented. The experimental results show that our D-PR ranking function achieves better personalized ranking on the social Web than the state-of-the-art method...|$|E
40|$|Handheld {{devices have}} {{recently}} {{gained access to}} the Internet. The devices differ a lot from traditional desktop computers what comes to, for instance, browsing the Web. Several parties have defined specifications, which facilitates creating Web documents for handheld devices. In this paper, we have defined <b>document</b> <b>profile,</b> which combines many document formats targeted for handheld devices. The result is compound <b>document</b> <b>profile,</b> through which can be realized multimedia presentations in handhelds. The profile has two configurations, from which one can select depending on target device’s resources. ...|$|E
40|$|This site {{provides}} quick {{facts and}} figures, cultural policy profiles and updates, key <b>documents,</b> <b>profiles</b> {{of the national}} experts, partner resources on 41 countries within Europe. 41 countries currently participate in the Compendium Community. Our aim is to include all 49 member states co-operating {{within the context of}} the European Cultural Convention...|$|R
5000|$|State in {{a formal}} <b>document</b> how <b>profiles</b> and {{accounts}} are handled ...|$|R
40|$|Copyright Notice Copyright © Open Grid Forum (2008). All Rights Reserved. This <b>document</b> <b>profiles</b> the File staging {{capabilities}} of the Job Submission Description Language (JSDL) for use by HPC Basic Profile-compliance services. It includes clarifications, refinements, interpretations and amplifications of JSDL which promote interoperability. 1 GWD-R (- 00) 2 / 26 / 08 Content...|$|R
40|$|Abstract. Social {{networks}} provide rich {{information about}} user interests and activities representing a valuable source for search personalization. However, social information is typically large and dynamic making its exploitation to obtain relevant search results a very challenging task. This work presents a PhD project plan that investigates Social Infor-mation Retrieval. The goal is threefolds: (1) create confidence area for information search by community detection based on tags similarity (2) {{introduce a new}} notion of Social <b>Document</b> <b>Profile</b> based on user activi-ties, and (3) propose a novel ranking model based on social relevance...|$|E
40|$|The World Wide Web {{contains}} a huge quantity of text that is notoriously inefficient to use. This work aims {{to apply a}} text processing technique based on thesaurally derived lexical chains to improve Internet Information Retrieval where a lexical chain a set of words in a text that are related by both proximity, and by relations derived from an external lexical knowledge source such as WordNet, Roget's Thesaurus, LDOCE, and so on. Finding Information on the Internet is notoriously hard, even when users have a clear focus to their queries. This situation is exacerbated when users only have vague notions about the topics they wish to explore. This could be remedied using Exemplar Texts, where an Exemplar Text is the ideal model result for Web searches. Our problem is now transformed into one of identifying similar texts. The Generic <b>Document</b> <b>Profile</b> is designed to allow the comparison of document similarity whilst being independent of terminology and document length. It is simply a set of semantic categories derived from Roget's thesaurus with associated weights. These weights are based on lexical chain length and strength. A Generic <b>Document</b> <b>Profile</b> {{can be compared to}} another using a Case Based Reasoning approach. Case Based Reasoning (CBR) is a problem solving method that seeks to solve existing problems by reference to previous successful solutions. Here our Exemplar Texts count as previous solutions (and in thes...|$|E
40|$|Abstract. This paper {{presents}} {{an extension of}} prior work by Michael D. Lee on psychologically plausible text categorisation. Our approach utilises Lee’s model as a pre-processing filter to generate a dense representation for a given text document (a <b>document</b> <b>profile)</b> and passes that on to an arbitrary standard propositional learning algorithm. Similarly to standard feature selection for text classification, the dimensionality of instances is drastically reduced this way, which in turn greatly lowers the computational load for the subsequent learning algorithm. The filter itself is very fast as well, as it basically is just an interesting variant of Naive Bayes. We present different variations of the filter and conduct an evaluation against the Reuters- 21578 collection that shows performances comparable to previously published results on that collection, but at a lower computational cost. ...|$|E
50|$|In {{architectural}} conservation, {{they are}} used to <b>document</b> the <b>profiles</b> of decorative moldings.|$|R
40|$|RAP (Remote Assistant for Programmers) is a Web and multi-agent based {{system to}} support remote {{students}} and programmers during common projects or activities {{based on the}} Java programming language. RAP helps users to solve problems proposing information extracted from dedicated repositories and forwarding answers received from other users, recommended as experts. Its peculiar characteristic is {{the integration of the}} agent technology with the semantic Web technology. In fact, in order to improve filtering and recommendation techniques, RAP takes advantage of an ontological approach to user and <b>document</b> <b>profiling.</b> A RAP system is not a closed system, instead it is based on a dynamic network of RAP platforms managing groups of geographically localized users and documents. Therefore, recommendations should take into account of the accessible experts and documents. At this purpose, RAP users and <b>documents</b> <b>profile</b> management subsystems provide a mechanism that dynamically adapts the relevance of each profile. An initial prototype of the RAP System is under development by using JADE...|$|R
50|$|<b>Profile</b> <b>documents</b> can {{be stored}} on the agent's own Web server, and access thereto may be {{partially}} or wholly constrained to specific agent identities via the use of access controls, to preserve {{the privacy of the}} <b>profile</b> <b>document's</b> subject.|$|R
40|$|In {{order to}} assist {{researchers}} in addressing time constraint and low relevance in using scientific articles, an automatic tailored multi-paper summarization (TMPS) is proposed. In this paper, we extend Teufel’s tailored summary {{to deal with}} multi-papers and more flexible representation of user information needs. Our TMPS extracts Rhetorical <b>Document</b> <b>Profile</b> (RDP) from each paper and presents a summary based on user information needs. Building Plan Language (BPLAN) is introduced as a formalization of Teufel’s building plan and used to represent summary specification, which is more flexible representation of user information needs. Surface repair is embedded within the BPLAN for improving the readability of extractive summary. Our experiment shows that the average performance of RDP extraction module is 94. 46 %, which promises high quality of extracts for summary composition. Generality evaluation shows that our BPLAN is flexible enough in composing various forms of summary. Subjective evaluation provides evidence that surface repair operators can improve the resulting summary readability...|$|E
40|$|In  order  to  assist  researchers  in  addressing  time  constraint  and  low relevance  in  using  scientific  articles,  an  automatic  {{tailored}}  multi-paper summarization  (TMPS)   is  proposed.   In  this  paper,  we  extend  Teufel’s  tailored summary  to  deal  with  multi-papers  and  more  flexible  representation  of  {{user information}} needs. Our TMPS extracts Rhetorical <b>Document</b> <b>Profile</b> (RDP) from each paper and  presents a summary based on user information needs.   Building Plan  Language  (BPLAN)   is  introduced  as  a  formalization  of  Teufel’s  building plan  and  used  to  represent summary  specification,  which  is  more  flexible representation user information needs. Surface repair is embedded within the BPLAN  for  improving  the  readability  of  extractive summary.   Our  experiment {{shows that the}} average performance of RDP extraction module is 94. 46 %, which promises  high  quality  of  extracts  for  summary  composition.   Generality evaluation  shows  that  our  BPLAN  is  flexible  enough  in  composing  various forms  of summary.   Subjective  evaluation  provides evidence that  surface repair operators can improve the resulting summary readability. </p...|$|E
40|$|The {{importance}} of finding relevant information {{for business and}} decision making is imperative for both individuals as well as enterprises. In this paper, we present an approach {{for the development of}} a fuzzy information retrieval (IR) system. The approach provides a new mechanism for constructing and integrating three relevancy profiles comprising of: a task profile, user profile and <b>document</b> <b>profile,</b> into a unified index, through the use of relevance feedback and fuzzy rule based summarisation. Experiments were performed from which relevance feedback and user queries were captured from 35 users on 20 predefined simulated enterprise search tasks. The captured data set was used to develop the three types of profiles and train the fuzzy system. The system shows 86 % performance accuracy in correctly classifying document relevance. The overall performance of the system was evaluated based on standard precision and recall which shows significant improvements in retrieving relevant documents based on user queries...|$|E
50|$|The token formats and {{semantics}} {{are defined}} in the associated <b>profile</b> <b>documents.</b>|$|R
40|$|The {{sliding window}} concept {{is a common}} method for {{computing}} a <b>profile</b> of a <b>document</b> with unknown structure. This paper outlines an experiment with stylometric word-based feature {{in order to determine}} an optimal size of the sliding window. It was conducted for a vocabulary richness method called ‘average word frequency class’ using the PAN 2015 source retrieval training corpus for plagiarism detection. The paper shows {{the pros and cons of}} the stop words removal for the sliding window <b>document</b> <b>profiling</b> and discusses the utilization of the selected feature for intrinsic plagiarism detection. The experiment resulted in the recommendation of setting the sliding windows to around 100 words in length for computing the text profile using the average word frequency class stylometric feature. The sliding window concept is a common method for computing a <b>profile</b> of a <b>document</b> with unknown structure. This paper outlines an experiment with stylometric word-based feature in order to determine an optimal size of the sliding window. It was conducted for a vocabulary richness method called ‘average word frequency class’ using the PAN 2015 source retrieval training corpus for plagiarism detection. The paper shows the pros and cons of the stop words removal for the sliding window <b>document</b> <b>profiling</b> and discusses the utilization of the selected feature for intrinsic plagiarism detection. The experiment resulted in the recommendation of setting the sliding windows to around 100 words in length for computing the text profile using the average word frequency class stylometric feature...|$|R
40|$|Digital Avionics {{activities}} {{played an}} important role in the advancements made in civil aviation, military systems, and space applications. This <b>document</b> <b>profiles</b> advances made in each of these areas by the aerospace industry, NASA centers, and the U. S. military. Emerging communication technologies covered in this document include Internet connectivity onboard aircraft, wireless broadband communication for aircraft, and a mobile router for aircraft to communicate in multiple communication networks over the course of a flight. Military technologies covered in this document include avionics for unmanned combat air vehicles and microsatellites, and head-up displays. Other technologies covered in this document include an electronic flight bag for the Boeing 777, and surveillance systems for managing airport operations...|$|R
40|$|This paper {{describes}} the preliminary {{results of an}} efficient language classifier using an ad-hoc Cumulative Frequency Addition of N-grams. The new classification technique is simpler than the conventional Naïve Bayesian classification method, but it performs similarly in speed overall and better in accuracy on short input strings. The classifier is also 5 - 10 times faster than N-gram based rank-order statistical classifiers. Language classification using N-gram based rank-order statistics {{has been shown to}} be highly accurate and insensitive to typographical errors, and, as a result, this method has been extensively researched and documented in the language processing literature. However, classification using rank-order statistics is slower than other methods due to the inherent requirement of frequency counting and sorting of N-grams in the test <b>document</b> <b>profile.</b> Accuracy and speed of classification are crucial for a classier to be useful in a high volume categorization environment. Thus, it is important to investigate the performance of the N-gram based classification methods. In particular, if it is possible to eliminate the counting and sorting operations in the rank-order statistics methods, classification speed could be increased substantially. The classifier described here accomplishes that goal by using a new Cumulative Frequency Addition method. ...|$|E
40|$|With {{the advent}} of various {{services}} and applications of Semantic Web, semantic annotation had emerged as an important research area. The use of semantically annotated ontology had been evident in numerous information processing and retrieval tasks. One of such tasks is utilizing the semantically annotated ontology in product design which is able to suggest many important applications that are critical to aid various design related tasks. However, ontology development in design engineering remains a time consuming and tedious task that demands tremendous human efforts. In the context of product family design, management of different product information that features efficient indexing, update, navigation, search and retrieval across product families is both desirable and challenging. This paper attempts {{to address this issue}} by proposing an information management and retrieval framework based on the semantically annotated product family ontology. Particularly, we propose a <b>document</b> <b>profile</b> (DP) model to suggest semantic tags for annotation purpose. Using a case study of digital camera families, we illustrate how the faceted search and retrieval of product information can be accomplished based on the semantically annotated camera family ontology. Lastly, we briefly discuss some further research and application in design decision support, e. g. commonality and variety, based on the semantically annotated product family ontology...|$|E
40|$|This article {{presents}} a new algorithm for content-oriented search in P 2 P networks that avoids flooding and thus ensures scalability. It {{is based on}} the concept of small worlds: peers are enabled to actively influence network structure by choosing their neighbours. Different strategies for neighbour selection are possible, the most important being the one called ”cluster strategy”, which consists in peers choosing other peers that offer content similar to their own. Thus, peers will organize into clusters of semantic similarity. This will (as has to be shown) result in a small world network structure. This structure can then be exploited for implementing an efficient search algorithm: each peer forwards incoming queries to just one of its neighbours (the one whose <b>document</b> <b>profile</b> best matches the query). Because paths are short in small worlds and because there are semantic clues for finding them, {{it will be possible to}} quickly redirect queries to the right clusters of peers. After giving a detailed overview of related ideas and introducing the exact algorithm, a model of a peer-to-peer network will be presented that makes some simplifying assumptions about the world and thus allows us to build a simulation of our algorithm. The experimental setup of this simulation will be explained in detail and simulation results will be given and thoroughly discussed. ...|$|E
40|$|This <b>document</b> <b>profiles</b> {{certificate}} enrollment {{for clients}} using Certificate Management over CMS (CMC) messages over a secure transport. This profile, called Enrollment over Secure Transport (EST), describes a simple, yet functional, certificate management protocol targeting Public Key Infrastructure (PKI) clients {{that need to}} acquire client certificates and associated Certification Authority (CA) certificates. It also supports client-generated public/private key pairs as well as key pairs generated by the CA. Status of This Memo This is an Internet Standards Track document. This document {{is a product of}} the Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by th...|$|R
40|$|Objectives o Characterize color {{rendering}} for new monitor technologies (especially LCDs) {{as discussed in}} VESA committees and <b>documents</b> Conventional <b>Profiling</b> + more than full-screen +> 1 primary at a time Quantify viewing-angle dependencies (color reversal) Color-gamut metric Verify colors from bits to ligh...|$|R
40|$|This {{paper is}} {{concerned}} with the use of linguistically motivated phrases as indexing terms in Information Retrieval applications. Apart from the conventional noun phrases, we propose to use verb phrases as index terms for text classification. Techniques for phrase matching through syntactic normalization and semantical matching are described. In particular, we show how to perform syntactic normalization of phrases in order to enhance recall. Semantical normalization is based on lexico-semantical relations, taking into account certain properties of the classification algorithms used. The ideas described here are being implemented in the Document Routing system DORO, in which statistical learning algorithms are applied to <b>document</b> <b>profiles</b> consisting of phrases. This paper describes the rationale behind work in progress, rather than presenting final results...|$|R
40|$|WSDM' 2009 ACM Workshop on Exploiting Semantic Annotations in Information Retrieval, ESAIR 2009, Barcelona, 9 February 2009 With {{the advent}} of various {{services}} and applications of Semantic Web, semantic annotation had emerged as an important research area. The use of semantically annotated ontology had been evident in numerous information processing and retrieval tasks. One of such tasks is utilizing the semantically annotated ontology in product design which is able to suggest many important applications that are critical to aid various design related tasks. However, ontology development in design engineering remains a time consuming and tedious task that demands tremendous human efforts. In the context of product family design, management of different product information that features efficient indexing, update, navigation, search and retrieval across product families is both desirable and challenging. This paper attempts {{to address this issue}} by proposing an information management and retrieval framework based on the semantically annotated product family ontology. Particularly, we propose a <b>document</b> <b>profile</b> (DP) model to s ggest semantic tags for annotation purpose. Using a case study of digital camera families, we illustrate how the faceted search and retrieval of product information can be accomplished based on the semantically annotated camera family ontology. Lastly, we briefly discuss some further research and application in design decision support, e. g. commonality and variety, based on the semantically annotated product family ontology. Department of Industrial and Systems EngineeringRefereed conference pape...|$|E
40|$|Collecting design {{rationale}} (DR) {{and making}} it available in a well-organized manner will better support product design, innovation and decision-making. Many DR systems {{have been developed to}} capture DR since the 1970 s. However, the DR capture process is heavily human involved. In addition, with the increasing amount of DR available in archived design documents, it has become an acute problem to research a new computational approach that is able to capture DR from free textual contents effectively. In our previous study, we have proposed an ISAL (issue, solution and artifact layer) model for DR representation. In this paper, we focus on algorithm design to discover DR from design documents according to the ISAL modeling. For the issue layer of the ISAL model, we define a semantic sentence graph to model sentence relationships through language patterns. Based on this graph, we improve the manifold-ranking algorithm to extract issue-bearing sentences. To discover solution-reason bearing sentences for the solution layer, we propose building up two sentence graphs based on candidate solution-bearing sentences and reason-bearing sentences respectively, and propagating information between them. For artifact information extraction, we propose two term relations, i. e. positional term relation and mutual term relation. Using these relations, we extend our <b>document</b> <b>profile</b> model to score the candidate terms. The performance and scalability of the algorithms proposed are tested using patents as research data joined with an example of prior art search to illustrate its application prospects. Department of Industrial and Systems Engineerin...|$|E
40|$|AbstractMotivationAlthough full-text {{articles}} {{are provided by}} the publishers in electronic formats, it remains a challenge to find related work beyond the title and abstract context. Identifying related articles based on their abstract is indeed a good starting point; this process is straightforward and does not consume as many resources as full-text based similarity would require. However, further analyses may require in-depth understanding of the full content. Two articles with highly related abstracts can be substantially different regarding the full content. How similarity differs when considering title-and-abstract versus full-text and which semantic similarity metric provides better results when dealing with full-text {{articles are}} the main issues addressed in this manuscript. MethodsWe have benchmarked three similarity metrics – BM 25, PMRA, and Cosine, {{in order to determine}} which one performs best when using concept-based annotations on full-text documents. We also evaluated variations in similarity values based on title-and-abstract against those relying on full-text. Our test dataset comprises the Genomics track article collection from the 2005 Text Retrieval Conference. Initially, we used an entity recognition software to semantically annotate titles and abstracts as well as full-text with concepts defined in the Unified Medical Language System (UMLS®). For each article, we created a <b>document</b> <b>profile,</b> i. e., a set of identified concepts, term frequency, and inverse document frequency; we then applied various similarity metrics to those document profiles. We considered correlation, precision, recall, and F 1 in order to determine which similarity metric performs best with concept-based annotations. For those full-text articles available in PubMed Central Open Access (PMC-OA), we also performed dispersion analyses in order to understand how similarity varies when considering full-text articles. ResultsWe have found that the PubMed Related Articles similarity metric is the most suitable for full-text articles annotated with UMLS concepts. For similarity values above 0. 8, all metrics exhibited an F 1 around 0. 2 and a recall around 0. 1; BM 25 showed the highest precision close to 1; in all cases the concept-based metrics performed better than the word-stem-based one. Our experiments show that similarity values vary when considering only title-and-abstract versus full-text similarity. Therefore, analyses based on full-text become useful when a given research requires going beyond title and abstract, particularly regarding connectivity across articles. AvailabilityVisualization available at ljgarcia. github. io/semsim. benchmark/, data available at [URL]...|$|E
40|$|Purpose: In this contribution, we want {{to detect}} the <b>document</b> type <b>profiles</b> of the three prestigious {{journals}} Nature, Science, and Proceedings of the National Academy of Sciences of the United States (PNAS) with regard to two levels: journal and country. Design/methodology/approach: Using relative values based on fractional counting, we investigate the distribution of publications across document types at both the journal and country level, and we use (cosine) <b>document</b> type <b>profile</b> similarity values to compare pairs of publication years within countries. Findings: Nature and Science mainly publish Editorial Material, Article, News Item and Letter, whereas the publications of PNAS are heavily concentrated on Article. The shares of Article for Nature and Science are decreasing slightly from 1999 to 2014, while the corresponding shares of Editorial Material are increasing. Most studied countries focus on Article and Letter in Nature, but on Letter in Science and PNAS. The <b>document</b> type <b>profiles</b> {{of some of the}} studied countries change to a relatively large extent over publication years. Research limitations: The main limitation of this research concerns the Web of Science classification of publications into document types. Since the analysis of the paper is based on document types of Web of Science, the classification in question is not free from errors, and the accuracy of the analysis might be affected. Practical implications: Results show that Nature and Science are quite diversified with regard to document types. In bibliometric assessments, where publications in Nature and Science play a role, other document types than Article and Review might therefore be taken into account. Originality/value: Results highlight the importance of other document types than Article and Review in Nature and Science. Large differences are also found when comparing the country <b>document</b> type <b>profiles</b> of the three journals with the corresponding profiles in all Web of Science journals. ...|$|R
40|$|Even {{though the}} HIV {{epidemic}} {{is concentrated in}} urban India, there is a rising incidence of infection in rural areas. Existing studies <b>document</b> the <b>profiles</b> of people attending testing centres in urban areas or at Government clinics but very few studies profile those attending private and rural clinics...|$|R
5000|$|A WebID is an HTTP URI that denotes ("refers to" [...] or [...] "names)" [...] {{an agent}} on HTTP based {{networks}} e.g., the Web or an enterprise Intranet. In line with Linked Data principles, when you de-reference ("look up") a WebID, it resolves to a <b>profile</b> <b>document</b> that describes its referent (what it denotes). This <b>profile</b> <b>document</b> consists of RDF model based structured data, constructed initially using terms from the FOAF vocabulary, but now often including terms from other vocabularies.|$|R
40|$|Motivation Although full-text {{articles}} {{are provided by}} the publishers in electronic formats, it remains a challenge to find related work beyond the title and abstract context. Identifying related articles based on their abstract is indeed a good starting point; this process is straightforward and does not consume as many resources as full-text based similarity would require. However, further analyses may require in-depth understanding of the full content. Two articles with highly related abstracts can be substantially different regarding the full content. How similarity differs when considering title-and-abstract versus full-text and which semantic similarity metric provides better results when dealing with full-text {{articles are}} the main issues addressed in this manuscript. Methods We have benchmarked three similarity metrics – BM 25, PMRA, and Cosine, {{in order to determine}} which one performs best when using concept-based annotations on full-text documents. We also evaluated variations in similarity values based on title-and-abstract against those relying on full-text. Our test dataset comprises the Genomics track article collection from the 2005 Text Retrieval Conference. Initially, we used an entity recognition software to semantically annotate titles and abstracts as well as full-text with concepts defined in the Unified Medical Language System (UMLS®). For each article, we created a <b>document</b> <b>profile,</b> i. e., a set of identified concepts, term frequency, and inverse document frequency; we then applied various similarity metrics to those document profiles. We considered correlation, precision, recall, and F 1 in order to determine which similarity metric performs best with concept-based annotations. For those full-text articles available in PubMed Central Open Access (PMC-OA), we also performed dispersion analyses in order to understand how similarity varies when considering full-text articles. Results We have found that the PubMed Related Articles similarity metric is the most suitable for full-text articles annotated with UMLS concepts. For similarity values above 0. 8, all metrics exhibited an F 1 around 0. 2 and a recall around 0. 1; BM 25 showed the highest precision close to 1; in all cases the concept-based metrics performed better than the word-stem-based one. Our experiments show that similarity values vary when considering only title-and-abstract versus full-text similarity. Therefore, analyses based on full-text become useful when a given research requires going beyond title and abstract, particularly regarding connectivity across articles. Availability Visualization available at ljgarcia. github. io/semsim. benchmark/, data available at [URL] authors acknowledge the support from the members of Temporal Knowledge Bases Group at Universitat Jaume I. Funding: LJGC and AGC are both self-funded, RB is funded by the “Ministerio de Economía y Competitividad” with contract number TIN 2011 - 24147...|$|E
40|$|This thesis {{discusses}} {{relevance feedback}} including implicit parameters, explicit parameters and user query {{and how they}} {{could be used to}} build a recommender system to enhance the search performance in the enterprise. It presents an approach for the development of an adaptive fuzzy logic based recommender system for enterprise search. The system is designed to recommend documents and people based on the user query in a task specific search environment. The proposed approach provides a new mechanism for constructing and integrating a task, user and document profiles into a unified index thorough the use of relevance feedback and fuzzy rule based summarisation. The three profiles are fuzzy based and are created using the captured relevance feedback. In the task profile, each task was modelled as a sequence of weighted terms which were used by the users to complete the task. In the user profile, the user was modelled as a sequence of weighted terms which were used to search for the required information. In the <b>document</b> <b>profile</b> the document was modelled as a group of weighted terms which were used by the users to retrieve the document. Fuzzy sets and rules were used to calculate the term weight based on the term frequency in the user queries. An empirical research was carried out to capture the relevance feedback from 35 users on 20 predefined simulated enterprise search tasks and to investigate the correlation between the implicit and explicit relevance feedback. Based on the results, an adaptive linear predictive model was developed to estimate the document relevancy from the implicit feedback parameters. The predicted document relevancy was then used to train the fuzzy system which created and integrated the three profiles, as briefly described above. The captured data set was used to develop and train the fuzzy system. The proposed system achieved 89 % accuracy performance classifying the relevant documents. With regard to the implementation, Apache Sorl, Apache Tikka, Oracle 11 g and Java were used to develop a prototype system. The overall retrieval accuracy performance of the proposed system was tested by carrying out a comparative retrieval accuracy performance evaluation based on Precision (P), Recall (R) and ranking analysis. The values of P and R of the proposed system were compared with two other systems being the standard inverted index based Solr system and the semantic indexing based lucid system. The proposed system enhanced the value of P significantly where the average of P value has been increased from 0. 00428 to 0. 064 as compared with the standard Sorl and from 0. 0298 to 0. 064 compared with Lucid. In other words, the proposed system has managed to decrease the number of irrelevant documents in the search result which means that the ability of the system to show the relevant document has been enhanced. The proposed system has also enhanced the value of R. The average value of R has been increased significantly (doubling) from 0. 436 to 0. 828 as compared with the standard Solr and from 0. 76804 to 0. 828 as compared with Lucid. This means that the ability of the system to retrieve the relevant document has also been enhanced. Furthermore the ability of the system to rank higher the relevant documents has been improved as compared with the other two systems...|$|E
40|$|This paper uses a seminonparametric {{model and}} Consumer Expenditure Survey data to {{estimate}} life cycle profiles of consumption, controlling for demographics, cohort and time e. ects. In addition to <b>documenting</b> <b>profiles</b> for total and nondurable consumption, we devote {{special attention to}} the age expenditure pattern for consumer durables. We find hump-shaped paths over the life cycle for total, for nondurable and for durable expenditures. Changes in household size account for roughly half of these humps. The other half remains unaccounted for by the standard complete markets life cycle model. Our results imply that households do not smooth consumption over their lifetimes. This is especially true for services from consumer durables. Bootstrap simulations suggest that our empirical estimates are tight and sensitivity analysis indicates that the computed profiles are robust to a large number of different specifications. ...|$|R
40|$|This <b>document</b> <b>profiles</b> 125 fishing {{communities}} in Washington, Oregon, California, and other U. S. states, with basic information {{on social and}} economic characteristics. Various federal statutes, including the Magnuson-Stevens Fishery Conservation and Management Act and the National Environmental Policy Act, among others, require federal agencies to examine {{the social and economic}} impacts of policies and regulations. These profiles can serve as a consolidated source of baseline information for assessing community impacts in these states. The profiles are given in a narrative format that includes four sections: People and Place, Infrastructure, Involvement in West Coast Fisheries, and Involvement in North Pacific Fisheries. People and Place includes information on location, demographics (including age and gender structure of the population, racial and ethnic make up), education, housing, and local history. Infrastructure covers current economic activity, governance (including city classification, taxation, and proximity to fisherie...|$|R
5000|$|ISO 16610 is {{composed}} of two families of <b>documents,</b> one for <b>profiles</b> (open and closed) and one for surfaces. A general introduction is provided in: ...|$|R
50|$|The {{system is}} based on the XDS (Cross Enterprise <b>Document</b> Sharing) <b>Profile</b> {{published}} by Integrating the Healthcare Enterprise (IHE). However, the usual IHE Patient Management system (PIX/PDQ) has been replaced by the National Health Identity (HI) Service. In addition the usual authentication and security IHE profiles have been replaced by, or significantly modified to work with, existing infrastructure.|$|R
