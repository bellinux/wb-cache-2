101|339|Public
2500|$|Exadata <b>Database</b> <b>Machine</b> – hardware/software {{integrated}} storage ...|$|E
2500|$|... 2008: Smart scans in {{software}} improve query-response in HP Oracle <b>Database</b> <b>Machine</b> / Exadata storage ...|$|E
2500|$|The IBM System i, {{then known}} as the AS/400, was the {{continuation}} of the System/38 <b>database</b> <b>machine</b> architecture (announced by IBM in October 1978 and delivered in August 1979). The AS/400 removed capability-based addressing. The AS/400 added source compatibility with the System/36 combining the two primary computers manufactured by the IBM Rochester plant. The System/36 was IBM's most successful mini-computer but the architecture had reached its limit. [...] The first AS/400 systems (known by the development code names Silverlake and Olympic) were delivered in 1988 under the tag line [...] "Best of Both Worlds" [...] and the product line has been refreshed continually since then. Guy Dehond from Inventive Designers was one of the beta-testers of Silverlake. The programmers who worked on OS/400, the operating system of the AS/400, did not have a UNIX background. Dr Frank Soltis, the chief architect, says that this is the main difference between this and any other operating system.|$|E
40|$|A {{great deal}} of {{research}} {{has been focused on}} the development of <b>database</b> <b>machines.</b> In parallel to this work some vendors have developed general purpose <b>machines</b> with <b>database</b> function built directly into the machine architecture. The IBM AS/ 400 is one of the principle examples of this approach. Designed with a strong object orientation and the basis functions of the relational database model integrated into it's architecture, the AS/ 400 {{has proved to be a}} commerical success. In the present work we look at the database component of the AS/ 400. 1. Introduction A great deal has been written about the utilization of <b>database</b> <b>machines</b> in or for data processing applications [HuMP 89,HMPE 89]. However, at this point the market remains software based with only limited penetration by machines that satisfy the typical definition given for <b>database</b> <b>machines.</b> * Department of Computer Science, Iowa State University, Ames, Iowa 50011 #International Business Machines Corporation, Rochester, Minnesota [...] ...|$|R
40|$|There {{are many}} {{software}} database management systems available on many general-purpose computers ranging from micros to super-mainframes. <b>Database</b> <b>machines</b> as backened computers can offload the database management {{work from the}} mainframe {{so that we can}} retain the same mainframe longer. However, the database backend must also demonstrate lower cost, higher performance, and newer functionality. Some of the fundamental architecture issues in the design of high-performance and great-capacity <b>database</b> <b>machines</b> are addressed. Solutions towards resolving these design issues are articulated. This article is written for the New York University's Symposium in New Directions of Database Systems {{on the basis of a}} lecture given at Microelectronic and Computer Technology, Inc. (MCC) Prepared for: Chief of Naval Research[URL]...|$|R
40|$|Optimizing the two {{computing}} resources of any computing system - {{time and space}} - has al-ways {{been one of the}} priority objectives of any database. A current and effective solution in this respect is the computer database. Optimizing computer applications by means of <b>database</b> <b>machines</b> has been a steady preoccupation of researchers since the late seventies. Several information technologies have revolutionized the present information framework. Out of these, those which have brought a major contribution to the optimization of the databases are: efficient handling of large volumes of data (Data Warehouse, Data Mining, OLAP – On Line Analytical Processing), the improvement of DBMS – Database Management Systems facilities through the integration of the new technologies, the dramatic increase in computing power and the efficient use of it (computer networks, massive parallel computing, Grid Computing and so on). All these information technologies, and others, have favored the resumption of the research on <b>database</b> <b>machines</b> and the obtaining {{in the last few years}} of some very good practical results, as far as the optimization of the {{computing resources}} is concerned. <b>Database</b> Optimization, <b>Database</b> <b>Machines,</b> Data Warehouse, OLAP – On Line Analytical Processing, OLTP – On Line Transaction Processing, Parallel Processing...|$|R
5000|$|Exadata <b>Database</b> <b>Machine</b> - hardware/software {{integrated}} storage ...|$|E
50|$|The {{most recent}} Oracle Exadata <b>Database</b> <b>Machine</b> is the X6 {{generation}} introduced in April, 2016.|$|E
5000|$|... 2008: Smart scans in {{software}} improve query-response in HP Oracle <b>Database</b> <b>Machine</b> / Exadata storage ...|$|E
30|$|A {{designed}} visualization {{program is}} a software product that utilizes many advanced features and large <b>database</b> <b>machines.</b> It {{is not necessary to}} use robust database servers such as MS SQL or Oracle. Compact and small databases may offer many benefits, in particular during distribution.|$|R
40|$|ThistutorialbringstogetherperspectivesonERfromavariety of fields, {{including}} <b>databases,</b> <b>machine</b> learning, {{natural language}} processing and information retrieval, to provide, in one setting, a survey of {{a large body of}} work. We discuss both the practical aspects and theoretical underpinnings of ER. We describe existing solutions, current challenges, and open research problems. 1...|$|R
40|$|Research in {{the area}} of <b>Database</b> <b>Machines</b> is {{surveyed}} with a focus on the effects of synchronization, communication and I/O burdens on the performance of proposed architectures. A number of machines described in the literature are used 10 illustrate the potential problems and the approaches that have been offered to address those problems...|$|R
5000|$|A better {{approach}} {{is known as}} the [...] "grace hash join", after the GRACE <b>database</b> <b>machine</b> for which it was first implemented.|$|E
50|$|In {{addition}} to adding storage servers into an Exadata <b>Database</b> <b>Machine</b> base configuration, storage servers {{may also be}} acquired with or added to Exadata Storage Expansion racks.|$|E
5000|$|W. W. Armstrong, A. S. Mohamed, A {{mixed-flow}} query processing {{strategy for}} a multiprocessor <b>database</b> <b>machine,</b> 5th Int'l Conf. on Distributed Computing Systems, Denver, 292 - 299, IEEE CS, 1985.|$|E
50|$|File {{translation}} grid: A columnar grid {{arrangement of}} the source and target languages for translating text, supported by other information panes such as a preview, difference highlighting with similar information in reference sources and matches with various information sources such as translation memories, stored reference files, terminology <b>databases,</b> <b>machine</b> translation suggestions and external sources.|$|R
40|$|Software {{has always}} ruled {{database}} engines, and commodity processors riding Moore’s Law doomed <b>database</b> <b>machines</b> of the 1980 s from the start. However, today’s hardware landscape is very different, and moving in directions that make <b>database</b> <b>machines</b> increasingly attractive. Stagnant clock speeds, looming dark silicon, availability of reconfigurable hardware, {{and the economic}} clout of cloud providers all align to make custom database hardware economically viable or even necessary. Dataflow workloads (business intelligence and streaming) already benefit from emerging hardware support. In this paper, we argue that control flow workloads—with their corresponding latencies—are another feasible target for hardware support. To make our point, we outline a transaction processing architecture that offloads much of its functionality to reconfigurable hardware. We predict a convergence to fully “bionic ” database engines that implement nearly all key functionality directly in hardware and relegate software to a largely managerial role. 1...|$|R
40|$|In this paper, {{we discuss}} issues {{involving}} temporal data fragmentation, temporal query processing, and query optimization in multiprocessor <b>database</b> <b>machines.</b> We propose parallel processing strategies, {{which are based}} on partitioning of temporal relations on timestamp values, for multi-way joins (e. g., complex temporal pattern queries) and optimization al-ternatives. We analyze the proposed schemes quanti-tatively, and show their advantages in computing complex temporal joins...|$|R
50|$|Oracle {{announced}} the Oracle Big Data Appliance on October 3, 2011 at Oracle OpenWorld.It {{was similar to}} the Oracle Exadata <b>Database</b> <b>Machine</b> and announced with the Oracle Exalytics Business Intelligence Machine.|$|E
50|$|Jim Starkey {{graduated}} from University of Wisconsin at Madison, Wisconsin, with a Bachelor of Arts in Mathematics. After graduating, Starkey worked at Computer Corporation of America on {{a research project}} to build a <b>database</b> <b>machine</b> for ARPAnet.|$|E
5000|$|The Oracle Exadata <b>Database</b> <b>Machine</b> is a {{combined}} compute and storage system marketed for running Oracle Database software. Exadata debuted in 2008 {{as the first}} in Oracle Corporation's family of [...] "Engineered Systems". [...] Oracle Corporation releases new generations of Exadata roughly once a year.|$|E
40|$|Relational {{database}} {{systems have}} provided end users and application programmers with an improved working environment over older hierarchial and networked database systems. End users now use interactive query languages to inspect and manage their data. And application programs {{are easier to}} write and maintain due to the separation of physical data storage information from the application program itself. These and other benefits do not come without a price however. System resource consumption {{has long been the}} perceived problem with relational systems. The additional resource demands usually force computing sites to upgrade existing systems or add additional facilities. One method of protecting the current investment in systems is to use specialized hardware designed specifically for relational database processing. 2 ̆ 7 <b>Database</b> <b>Machines</b> 2 ̆ 7 provide that alternative. Since the commercial introduction of <b>database</b> <b>machines</b> in the early 19802 ̆ 7 s, both software and hardware vendors of relational database systems have claimed superior performance over competing products. Without a STANDARD performance measurement technique, the database user community has been flooded with benchmarks and claims from vendors which are immediately discarded by some competitors as being biased towards a particular system design. This thesis discusses the issues of relational database performance measurement with an emphasis on <b>database</b> <b>machines,</b> however; these performance issues are applicable to both hardware and software systems. A discussion of hardware design, performance metrics, software and database design is included. Also provided are recommended guidelines to use in evaluating relational database systems in lieu of a standard benchmark methodology...|$|R
40|$|We {{present a}} Model Driven Engineering {{approach}} to explain, verify, build, and test dataflow or streaming software architectures that are parallelized for performance or availability. Component-connector models are incrementally elaborated by transformations that refine or optimize architectural designs. We re-engineered two significant case studies {{to illustrate the}} generality of our work: (1) recoverable crash fault tolerant servers and (2) join parallelizations in <b>database</b> <b>machines...</b>|$|R
40|$|This paper {{presents}} {{a new strategy}} for increasing the availability of data in multi-processor, shared-nothing <b>database</b> <b>machines.</b> This technique, termed chained declustering, is demonstrated to provide superior performance {{in the event of}} failures while maintaining a very high degree of data availability. Furthermore, unlike most earlier replication strategies, the implementation of chained declustering requires no special hardware and only minimal modifications to existing software...|$|R
50|$|In the 1970s and 1980s, {{attempts}} were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. <b>database</b> <b>machine.</b>|$|E
50|$|WeDo Technologies’ Enterprise Business Assurance RAID 6.3 software, {{running on}} Oracle Exadata <b>Database</b> <b>Machine,</b> {{achieved}} a processing throughput {{of up to}} 1 billion call records per hour during loading enrichment and aggregation, reducing space requirements for storing detailed call data by 13-fold when compared with raw data, and by 8-fold when compared with Oracle standard compression.|$|E
50|$|A {{database}} machines or {{back end}} processor {{is a computer}} or special hardware that stores and retrieves data from a database. It is specially designed for database access and is coupled to the main (front-end) computer(s) by a high-speed channel. The <b>database</b> <b>machine</b> is tightly coupled to the main CPU, whereas the database server is loosely coupled via the network. Database machines can transfer large packets of data to the mainframe using hundreds to thousands of microprocessors with database software. The front end processor receives the data and displays it. The back end processor {{on the other hand}} analyzes and stores the data from the front end processor. Back end processors result in higher performance, increasing host main memory, increasing database recovery and security, and decrease of cost to manufacture. The <b>database</b> <b>machine</b> contrasts with a database server, which is a computer in a local area network that holds a database.|$|E
40|$|Association rule is wildly used {{in most of}} {{the data}} mining technologies. Apriori {{algorithm}} is the fundamental association rule mining algorithm. FP-growth tree algorithm improves the performance by reduce the generation of the frequent item sets. Simplex algorithm is a advanced FP-growth algorithm by using bitmap structure with the simplex concept in geometry. The bitmap structure implementation is particular designed for storing the data in <b>database</b> <b>machines</b> to support parallel computing the association rule mining...|$|R
5000|$|The Internet Pinball <b>Machine</b> <b>Database</b> {{identifies}} 225 <b>machines</b> {{designed by}} Krynski, including [...] "300", 2001, 4 Square, The Amazing Spider-Man, El Dorado City of Gold, Genie, Spirit of 76, Central Park, Royal Flush, Big Shot, and Sing Along.|$|R
40|$|Abstract. The {{need for}} {{learning}} from databases has increased {{along with their}} number and size. The new eld of Knowledge Discovery in Databases (KDD) develops methods that discover relevant knowledge in very large <b>databases.</b> <b>Machine</b> learning, statistics, and database methodology contribute to this exciting eld. In this paper, the discovery of knowledge {{in the form of}} Horn clauses is described. A case study of directly coupling an inductive logic programming (ILP) algorithm with a database system is presented. ...|$|R
50|$|The service {{lets the}} users join in and sign upfor free. The user {{does not need}} to {{download}} a software rather join via the website and connect it to his financial accounts. The system runs algorithms against financial transactions and generates insightful <b>database.</b> <b>Machine</b> learning suggests personalized behavior “nudges” based on aggregated data. All communication is done through texting with a basic chatbox.|$|E
50|$|Oracle Enterprise Manager 12c (EM) manages Oracle {{software}} and hardware, including the Exadata <b>Database</b> <b>Machine.</b> EM integrates with the built-in Exadata management tooling, {{as well as}} with customer’s existing systems management and helpdesk tools. Exadata plug-in for EM provides an integrated view of compute servers, storage servers, switches, and topology. In addition, it also provides discovery, monitoring and alerting capability for Exadata systems management.|$|E
5000|$|The Exadata <b>Database</b> <b>Machine</b> {{provides}} high-speed {{networks for}} {{internal and external}} connectivity. A 40 Gb/Sec InfiniBand network is used for internal connectivity between compute and storage servers and 10Gb/Sec and 1Gb/Sec Ethernet ports are included for data center connectivity. The InfiniBand network is also used as the cluster interconnect between compute servers. Exadata has a [...] "direct-to-wire" [...] protocol that allows the database to talk directly to the InfiniBand hardware, minimizing operating system overhead.|$|E
50|$|Another {{approach}} to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized <b>database</b> <b>machines</b> {{could not keep}} pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However this idea is still pursued for certain applications by some companies like Netezza and Oracle (Exadata).|$|R
40|$|Contents 1 Introduction 11 1. 1 The Need for High Performance Databases........ 11 1. 2 Conventional Databases.................. 13 1. 3 Distributed Databases................... 13 1. 4 Multidatabases....................... 14 1. 5 Data Servers......................... 14 1. 6 Parallel Data Servers.................... 15 1. 7 <b>Database</b> <b>Machines.....................</b> 17 1. 8 Overview of Some Data Servers.............. 18 1. 9 Current Trends....................... 21 1. 10 Conclusions......................... 21 2 Properties of Structures for Servers 23 2. 1 The Problem........................ 23 2. 2 Scalability.......................... 24 2. 3 Distribution......................... 25 2. 4 Availabili...|$|R
30|$|Similar to DFIS (Qin et al. 2012), {{we tested}} DFIS and ADFIS for four data sets from UCI {{benchmark}} <b>database</b> (UCI <b>Machine</b> Learning Repository 2013).|$|R
