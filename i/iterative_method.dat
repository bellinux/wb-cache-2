5703|7170|Public
25|$|Subgradient methods - An <b>iterative</b> <b>method</b> {{for large}} locally Lipschitz {{functions}} using generalized gradients. Following Boris T. Polyak, subgradient–projection methods {{are similar to}} conjugate–gradient methods.|$|E
25|$|Ellipsoid method: An <b>iterative</b> <b>method</b> {{for small}} {{problems}} with quasiconvex objective functions and of great theoretical interest, particularly {{in establishing the}} polynomial time complexity of some combinatorial optimization problems. It has similarities with Quasi-Newton methods.|$|E
25|$|Most often, {{the known}} wave {{parameters}} are the wave height H, mean water depth h, gravitational acceleration g, and either the wavelength λ {{or else the}} period τ. Then the above relations for λ, c and τ are used to find the elliptic parameter m. This requires numerical solution by some <b>iterative</b> <b>method.</b>|$|E
50|$|When David Young {{first began}} his {{research}} on <b>iterative</b> <b>methods</b> in the late 1940s, there was some skepticism {{with the idea of}} using <b>iterative</b> <b>methods</b> onthe new computing machines to solve industrial-size problems.Ever since Young's ground-breaking Ph.D. thesis, <b>iterative</b> <b>methods</b> have been used {{on a wide range of}} scientific and engineeringapplications with a variety of new <b>iterative</b> <b>methods</b> having been developed.|$|R
40|$|We {{present an}} {{investigation}} of the Hessenberg matrices that arise from polynomial <b>iterative</b> <b>methods</b> such as the conjugate gradients method, in particular of their various factorisations. Application of the results to <b>iterative</b> <b>methods,</b> including a proof that (quasi-) minimising <b>iterative</b> <b>methods</b> can always be written as a two-term residual smoothing method, is given. 1 Introduction Many studies of <b>iterative</b> <b>methods</b> mention Hessenberg matrices. Since these studies are mostly concerned with minimisation and orthogonalisation properties of the <b>iterative</b> <b>methods,</b> it is never quite clear how much the properties of the Hessenberg matrices depend on such properties of the sequences they relate to. This paper brings together various facts on these Hessenberg matrices, in particular, various theorems on their factorisations. At first, the presentation is isolated from the <b>iterative</b> <b>methods</b> the matrices arise from, but the results are then applied to the <b>iterative</b> <b>methods,</b> thus stressing the fu [...] ...|$|R
50|$|The {{modeling}} {{strategy of}} relaxation {{should not be}} confused with <b>iterative</b> <b>methods</b> of relaxation, such as successive over-relaxation (SOR); <b>iterative</b> <b>methods</b> of relaxation are used in solving problems in differential equations, linear least-squares, and linear programming. However, <b>iterative</b> <b>methods</b> of relaxation have been used to solve Lagrangian relaxations.|$|R
25|$|Several further results {{follow from}} this, such as Fermi's golden rule, which relates {{the rate of}} {{transitions}} between quantum states to the density of states at particular energies; or the Dyson series, obtained by applying the <b>iterative</b> <b>method</b> to the time evolution operator, {{which is one of}} the starting points for the method of Feynman diagrams.|$|E
25|$|Iterative {{methods are}} more common than direct methods in {{numerical}} analysis. Some methods are direct in principle but are usually used {{as though they were}} not, e.g. GMRES and the conjugate gradient method. For these methods the number of steps needed to obtain the exact solution is so large that an approximation is accepted {{in the same manner as}} for an <b>iterative</b> <b>method.</b>|$|E
25|$|Truncation {{errors are}} {{committed}} when an <b>iterative</b> <b>method</b> is terminated or a mathematical procedure is approximated, and the approximate solution {{differs from the}} exact solution. Similarly, discretization induces a discretization error because {{the solution of the}} discrete problem does not coincide with the solution of the continuous problem. For instance, in the iteration in the sidebar to compute the solution of , after 10 or so iterations, we conclude that the root is roughly 1.99 (for example). We therefore have a truncation error of 0.01.|$|E
30|$|This {{work has}} {{developed}} a scheme to unify various sixth-order <b>iterative</b> <b>methods.</b> Comparison among <b>iterative</b> <b>methods,</b> by using the basins of attraction and also through numerical computations, is also presented. Ideas presented in this work can be further developed and extended to include <b>iterative</b> <b>methods</b> of higher orders such as seven or eight.|$|R
30|$|Noor [4] {{suggested}} {{and analyzed}} several three-step <b>iterative</b> <b>methods,</b> {{which are also}} known as Noor iterations, for solving variational inequalities. It {{has been shown that}} three-step <b>iterative</b> <b>methods</b> are more efficient than two-step and one-step <b>iterative</b> <b>methods.</b> In addition, it is known that the convergence analysis of three-step can be proved under much weaker conditions.|$|R
40|$|The aim of {{this paper}} is to {{describe}} formulation of the 2, 3 and 4 Point Modified Explicit Group (MEG) <b>iterative</b> <b>methods</b> scheme with finite element approximation to solve the one dimensional diffusion equation. It had shown tha the MEG <b>iterative</b> <b>methods</b> is faster than the halfsweeps or fullsweeps Explicit Group (EG) <b>iterative</b> <b>methods...</b>|$|R
500|$|Newton's method, an <b>iterative</b> <b>method</b> {{to solve}} {{equations}} approximately, {{can also be}} used to calculate the logarithm, because its inverse function, the exponential function, can be computed efficiently. Using look-up tables, CORDIC-like methods can be used to compute logarithms if the only available operations are addition and bit shifts. Moreover, the binary logarithm algorithm calculates [...] recursively based on repeated squarings of x, taking advantage of the relation ...|$|E
500|$|The {{original}} {{proof of}} the Shapley–Folkman lemma established only {{the existence of the}} representation, but did not provide an algorithm for computing the representation: Similar proofs have been given by Arrow and Hahn, Cassels, and Schneider, among others. An abstract and elegant proof by Ekeland has been extended by Artstein. Different proofs [...] have appeared in unpublished papers, also. In1981, Starr published an <b>iterative</b> <b>method</b> for computing a representation of a given sum-point; however, his computational proof provides a weaker bound than does the original result. An elementary {{proof of the}} Shapley–Folkman lemma in finite-dimensional space {{can be found in the}} book by Bertsekas ...|$|E
500|$|Church {{explained}} that {{the core of the}} project was its [...] "dynamic creation". He noted that the team had [...] "no set of rules [...] or pre-written plan", but rather worked organically toward the general idea of creating a [...] "dungeon simulation". Church believed that the game's Ultima series heritage was extremely helpful, as it gave the team an anchor for their experiments. According to Church, because the team was young and inexperienced, they were [...] "improvising almost the whole time". He said that they would [...] "just write something" [...] that seemed interesting, but would then [...] "get it half done, and we'd say, 'Eh? That's not working.'" [...] He believed that this <b>iterative</b> <b>method</b> was useful overall, but that it entailed an abnormally large workload: it resulted in the creation of [...] "four movement systems before we were done, several combat systems, and so forth". Certain failed experiments meant that the team created [...] " [...] code for many ideas {{which turned out to be}} largely irrelevant to the actual gameplay".|$|E
40|$|Range {{restricted}} <b>iterative</b> <b>methods</b> {{based on}} the Arnoldi process are attractive for the solution of large nonsymmetric linear discrete ill-posed problems with error-contaminated data (right-hand side). Several derivations {{of this type of}} <b>iterative</b> <b>methods</b> are compared in A. Neuman, L. Reichel, and H. Sadok: Implementations of range restricted <b>iterative</b> <b>methods</b> for linear discrete ill-posed problems, Linear Algebra Appl., in press. We describe MATLAB codes for the best of these implementations. MATLAB codes for range restricted <b>iterative</b> <b>methods</b> for symmetric linear discrete ill-posed problems are also presented...|$|R
40|$|Abstract. This paper {{deals with}} {{discrete}} monotone <b>iterative</b> <b>methods</b> for solving semilinear singularly perturbed problems of elliptic and parabolic types. The monotone <b>iterative</b> <b>methods</b> solve only linear discrete systems at each iterative {{step of the}} iterative process. Uniform convergence of the monotone <b>iterative</b> <b>methods</b> are investigated and rates of convergence are estimated. Numerical experiments complement the theoretical results...|$|R
30|$|Many {{problems}} {{in science and}} engineering require solving a nonlinear scalar equation f(x)= 0 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]. As a result, solving nonlinear equations {{is an important part of}} scientific computing. There exist various <b>iterative</b> <b>methods</b> for solving nonlinear scalar equations. We are interested in sixth-order <b>iterative</b> <b>methods,</b> and their dynamics, to find a simple zero, that is f(γ)= 0 and f′(γ)≠ 0, of a nonlinear equation f(x)= 0. There exist many sixth-order <b>iterative</b> <b>methods</b> (see, e.g., [5, 6, 7, 12, 13, 15]) for solving nonlinear scalar equations. The paper develops a scheme for constructing sixth-order <b>iterative</b> <b>methods</b> or family of methods. The scheme unifies existing sixth-order <b>iterative</b> <b>methods.</b> It is shown that various existing sixth-order <b>iterative</b> <b>methods</b> can be generated by the scheme through a proper choice of the independent parameters.|$|R
2500|$|A general <b>iterative</b> <b>method</b> {{involves}} {{a process that}} converts the system of linear equations ...|$|E
2500|$|PageRank can be {{computed}} either iteratively or algebraically. The <b>iterative</b> <b>method</b> can {{be viewed}} as the ...|$|E
2500|$|Bundle {{method of}} descent: An <b>iterative</b> <b>method</b> for small–medium-sized {{problems}} with locally Lipschitz functions, particularly for convex minimization problems. [...] (Similar to conjugate gradient methods) ...|$|E
50|$|In {{the case}} of a system of linear equations, the two main classes of <b>iterative</b> <b>methods</b> are the {{stationary}} <b>iterative</b> <b>methods,</b> and the more general Krylov subspace methods.|$|R
5000|$|Preconditioned <b>iterative</b> <b>methods</b> for [...] are, in most cases, mathematically {{equivalent}} to standard <b>iterative</b> <b>methods</b> {{applied to the}} preconditioned system [...] For example, the standard Richardson iteration for solving [...] is ...|$|R
50|$|Time domain {{methods can}} be further divided into one step methods (time domain sensitivities) and <b>iterative</b> <b>methods</b> (shooting methods). One step methods require {{derivatives}} to compute the steady state; whenever those are not readily available at hand, <b>iterative</b> <b>methods</b> come into focus.|$|R
2500|$|... where g {{is a given}} column vector, can {{be solved}} {{directly}} for the vector x. [...] If (...) represents a regular splitting of A, then the <b>iterative</b> <b>method</b> ...|$|E
2500|$|The {{most common}} <b>iterative</b> <b>method</b> of square root {{calculation}} by hand {{is known as}} the [...] "Babylonian method" [...] or [...] "Heron's method" [...] after the first-century Greek philosopher Heron of Alexandria, who first described it.|$|E
2500|$|It can {{be shown}} that if A1 > 0, then [...] < 1, where [...] {{represents}} the spectral radius of D, and thus D is a convergent matrix. [...] As a consequence, the <b>iterative</b> <b>method</b> (...) is necessarily convergent.|$|E
2500|$|<b>Iterative</b> <b>methods,</b> such as {{conjugate}} gradient method and GMRES utilize fast computations of matrix-vector products , where matrix [...] is sparse. The use of preconditioners can significantly accelerate convergence of such <b>iterative</b> <b>methods.</b>|$|R
30|$|This {{special issue}} {{contains}} 37 articles accepted on different topics {{in the area}} of fixed point theory and EPs. Ten articles are devoted to general fixed point theory, four are on <b>iterative</b> <b>methods</b> for solving variational inequalities, two on hierarchical variational inequalities, one is on some <b>iterative</b> <b>methods</b> for solving systems of variational inequalities, six are devoted to different kinds of <b>iterative</b> <b>methods</b> for solving EPs, seven are on <b>iterative</b> <b>methods</b> for systems of EP, and one is on the existence of solutions of system of generalized vector quasi-EPs. There are six more articles on different topics from nonlinear analysis.|$|R
40|$|<b>Iterative</b> <b>methods</b> {{for solving}} {{large-scale}} linear {{systems have been}} gaining popularity {{in many areas of}} scientific computing, whereas direct methods are sometimes preferred to <b>iterative</b> <b>methods</b> because of the slow convergence of <b>iterative</b> <b>methods.</b> However, this problem to some extent can be handled by preconditioning, which {{plays an important role in}} the success of <b>iterative</b> <b>methods.</b> In this report, preconditioned Conjugate Gradient (PCG) method is implemented with three preconditioners – Kronecker product preconditioner, incomplete Cholesky factorization preconditioner and T. Chan’s preconditioner. The background theory for each preconditioner is explained and numerical results are shown in the end...|$|R
2500|$|Combining the Householder {{transformation}} {{with the}} LU decomposition {{results in an}} algorithm with better convergence than the QR algorithm. For large Hermitian sparse matrices, the Lanczos algorithm {{is one example of}} an efficient <b>iterative</b> <b>method</b> to compute eigenvalues and eigenvectors, among several other possibilities.|$|E
2500|$|... the {{computation}} of {{the square}} root of a positive number {{can be reduced to}} that of a number in the range [...] This simplifies finding a start value for the <b>iterative</b> <b>method</b> that is close to the square root, for which a polynomial or piecewise-linear approximation can be used.|$|E
2500|$|The only {{difference}} with the cnoidal wave {{solution of the}} KdV equation is in the equation for the wavelength λ. [...] For practical applications, usually the water depth h, wave height H, gravitational acceleration g, and either the wavelength λ, or—most often—the period (physics) τ are provided. Then the elliptic parameter m has to be determined from the above relations for λ, c and τ through some <b>iterative</b> <b>method.</b>|$|E
40|$|AbstractIn this paper, {{we suggest}} and analyze two new two-step <b>iterative</b> <b>methods</b> for solving {{the system of}} {{nonlinear}} equations using quadrature formulas. We prove that these new methods have cubic convergence. Several numerical examples are given to illustrate the efficiency {{and the performance of}} the new <b>iterative</b> <b>methods.</b> These new <b>iterative</b> <b>methods</b> may be viewed as an extension and generalizations of the existing methods for solving the system of nonlinear equations...|$|R
30|$|<b>Iterative</b> <b>methods</b> {{are quite}} robust against {{quantization}} and additive noise. In fact, we {{can prove that}} the <b>iterative</b> <b>methods</b> approach the pseudo-inverse (least squares) solution for a noisy environment; specially, when the matrix is ill-conditioned [50].|$|R
40|$|We present another {{simple way}} of {{deriving}} several <b>iterative</b> <b>methods</b> for solving nonlinear equations numerically. The presented approach of deriving these methods {{is based on}} exponentially fitted osculating straight line. These methods are the modifications of Newton's method. Also, we obtain well-known methods as special cases, for example, Halley's method, super-Halley method, Ostrowski's square-root method, Chebyshev's method, and so forth. Further, new classes of third-order multipoint <b>iterative</b> <b>methods</b> free from a second-order derivative are derived by semidiscrete modifications of cubically convergent <b>iterative</b> <b>methods.</b> Furthermore, a simple linear combination of two third-order multipoint <b>iterative</b> <b>methods</b> is used for designing new optimal methods of order four...|$|R
