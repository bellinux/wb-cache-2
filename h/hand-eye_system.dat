20|16|Public
40|$|The Stanford <b>hand-eye</b> <b>system</b> is {{implemented}} as several separate tasks, each executing under a timesharing executive. Development of {{a programming language}} (SAIL) and augmentation of the timesharing system were required to provide the necessary data sharing and control flow among the tasks. The SAIL language provides facilities for &quot;associative processing, &quot; and is extended to serve the data sharing and communication needs of the <b>hand-eye</b> <b>system.</b> Several user facilities are designed to aid running and debugging the system. Keywords associative processing, programming languages, symbolic processing, timesharing, segmentation, interprocess communication, debugging. *This work {{was supported by the}} Advanced Research Projects Agency (SD- 183) ...|$|E
40|$|The {{computer}} simulation of detecting fruit by visual feedback on <b>hand-eye</b> <b>system</b> {{was done in}} order to investigate detecting accuracy under the various conditions. From the result, it was obtained as follows : 1. The smaller {{the distance from the}} visual sensor to the fruit was smaller, the error by visual feedback became smaller proportionally. 2. The error depended on the fruit diameter. 3. The bigger the picture element number was, the smaller the scattering. 4. The bigger the manipulator speed was, the bigger the picture element number recognizing fruit became, and the error became bigger. 5. It was not suitable that visual feedback on <b>hand-eye</b> <b>system</b> was independently used for detecting fruit whose diameter had the scattering fairly...|$|E
40|$|Distribution of this {{document}} is unlimited. z 7. DSPPLUTMENTANTESET(fteasrc nee nBok 2. I ifrn rmRpr) 1718 None 19. KEY WORDS (Continue an fevers * side It necessary and identify by block number) Photometric stereo Bin-picking Puma arm Visual guidance <b>Hand-eye</b> <b>system</b> 8 extended Gaussian Image C 920. ABSTRACT (Continue on reverse old * Ifneceery mnd Identify by block nlumber) LLJ This paper describes a <b>hand-eye</b> <b>system</b> we developed {{to perform the}} bin-picking task. Two basic tools are employed: the photometric stereo method and the ex-tended Gaussian image. The photometric stereo method generates the surface no]-mal distribution of a scene. The extended Gaussian image allows us to determine {{the attitude of the}} object based on the normal distribution. Visual analysis of an image consists of two stages. The first stage segmelts the image into regions and determines the target region. The photometric ster...|$|E
40|$|This {{proposal}} discusses practical {{aspects of}} our project to produce a replicable research tool for development of real-world computer-controlled <b>hand-eye</b> <b>systems.</b> If this proposal is read out of context, it will not seem very sophisticated because it is concerned mainly with the practical aspects of putting together an engineering system. The theoretical and conceptual context is described more thoroughly in the memo, supplementary to our main ARPA contract proposal, that describes in detail robotics reasearch at the MIT A. I. Laboratory...|$|R
40|$|Abstract—This paper {{proposes a}} {{standard}} method {{to approach the}} uncalibrated robotic hand-eye coordination problem that is system configuration-and task-independent. The unknown hand-eye relationship is first modeled as the modeling errors of a dynamic system. An extended state observer is then implemented to estimate summation of the system’s modeling error and the system’s external disturbances. With the estimation results as the compensation, the system control is accomplished from a nonlinear combination of the system state errors. A universal framework of controller design is provided for decoupled and coupled <b>hand-eye</b> <b>systems</b> of different configurations to execute dynamic tracking task. Index Terms—Calibration-free, dynamic tracking, hand/eye coordination, state observer. I...|$|R
40|$|Abstract — A new {{image-based}} visual servoing, {{based on}} the epipolar geometry, is presented. The estimation of the epipoles position, obtained by points correspondences extracted from two different images, allows to control the <b>hand-eye</b> robotic <b>system.</b> The camera-robot motion is achieved from the observation of the epipoles coordinates. Experimental results are presented. I...|$|R
40|$|This paper {{describes}} a <b>hand-eye</b> <b>system</b> we developed {{to perform the}} binpicking task. Two basic tools are employed: the photometric stereo method and the extended Gaussian image. The photometric stereo method generates the surface normal distribution of a scene. The extended Gaussian image allows us to determine {{the attitude of the}} object based on the normal distribution...|$|E
40|$|Human {{grasping}} still outshines its robotical counterparts {{with respect}} to accuracy, speed, robustness, and flexibility. When {{trying to develop a}} robotical <b>hand-eye</b> <b>system,</b> it is therefore only natural to examine the results of neuroscience. In this paper, we examine the human <b>hand-eye</b> <b>system</b> concerning motion planning and control using robotical categories and strategies. From the results, we derive a system concept for an anthropomorphic robotical hand-eye coordination. 1. INTRODUCTION Using sensor information to control robots has become a very popular field of research, since it promises to lead to the design of autonomous robots. In contrast to their preprogrammed industrial counterparts, autonomous robots must {{be able to deal with}} unexpected events such as obstacles or misplaced objects. This is especially important for personal robots because they operate in an environment that is not adapted to the needs of machines. Nowadays, vision is by far the most commonly used sensor beca [...] ...|$|E
40|$|In {{the field}} of {{visually}} guided grasping, humans still outshine their robotic counterparts with respect to accuracy, speed, robustness, and flexibility. We therefore examined current neuroscientific models for the control of human reach-to-grasp movements and, based on one of them, developed a novel visual motion control strategy. This control strategy was integrated into a complete <b>hand-eye</b> <b>system,</b> including modules for the determination of suitable 3 D grasping positions on unknown objects from the images of a stereo camera system. The modules were implemented and tested on the experimental <b>hand-eye</b> <b>system</b> MinERVA. 1 Introduction The ability to grasp arbitrary objects {{will be an important}} component of future autonomous service robots. Humans acquire this ability very early; they achieve a performance that still outshines that of their robotic counterparts with respect to accuracy, speed, robustness, and flexibility. In the past, robotic approaches mostly used precisely calibrated [...] ...|$|E
40|$|Over {{the last}} several years, {{there has been an}} {{increasing}} appreciation of the impact of control architecture on the accuracy of visual servoing systems. In particular, it is generally acknowledged that so-called image-based methods provide the highest guarantees of accuracy on inaccurately calibrated <b>hand-eye</b> <b>systems.</b> Less clear is the impact of the control architecture on the set of tasks which the system can perform. In this article, we present a formal analysis of control architectures for hand-eye coordination. Specifically, we first state a formal characterization of what makes a task performable under three possible encoding methods. Then, for the specific case of cameras modeled using projective geometry, we relate this characterization to notions of projective invariance and demonstrate the limits of achievable performance in this regard. 1 Introduction In the design of vision-based control systems, two families of control architectures dominate [4, 11]. The first family, ch [...] ...|$|R
40|$|We {{propose a}} <b>hand–eye</b> {{coordination}} <b>system</b> for a humanoid robot that supports bimanual reaching. The system combines endpoint closed-loop and open-loop visual servo control. The closed-loop component moves the eyes, head, arms, and torso, {{based on the}} position of the target and the robot’s hands, as seen by the robot’s head-mounted cameras. The open-loop component uses a motor–motor mapping that is learnt online to support movement when visual cues are not available...|$|R
40|$|This paper {{proposes a}} visual servoing {{algorithm}} for <b>hand-eye</b> robotic <b>system</b> based on epipolar geometry. The control law {{is based on}} the estimation of the epipoles position obtained by points correspondences extracted from the current and target images. The camera-robot motion is computed from the observation of the epipoles coordinates. Only the principal camera point is assumed to be known but not the other intrinsic parameters. Experimental results are reported to validate the visual servoing algorithm proposed...|$|R
40|$|The {{stabilization}} to {{a desired}} pose of a nonholonomic mobile robot, based on visual data from a <b>hand-eye</b> <b>system</b> mounted on it, is considered. Instances of this problem occur in practice during docking or parallel parking maneuvers of such vehicles. In this paper, we investigate the use of visual servoing techniques for their control. After briefly presenting the relevant visual servoing framework, we point out some problems encountered when it is considered for nonholonomic mobile robots. In particular, simple velocity control schemes using visual data as feedback cannot be applied anymore. We show how, by using the extra degrees of freedom provided by the <b>hand-eye</b> <b>system,</b> we can design controllers capable of accomplishing the desired task. A first approach, allows to perform a visual servoing task dened in the camera frame without explicitly controlling the pose of the nonholonomic mobile basis. A second approach based on continuous timevarying state feedback techniques allows to stabil [...] ...|$|E
40|$|Original {{scientific}} paper In {{order to}} determine the position and orientation of an object in the wrist frame for robot, transform relation of <b>hand-eye</b> <b>system</b> should be estimated, which is described as rotational matrix and translational vector. A new approach integrating neural network and particle swarm optimization algorithm with crossover and mutation operation for robot sense calibration is proposed. First the neural network with rotational weight matrix is structured, where the weights are the elements of rotational part of homogeneous transform of the <b>hand-eye</b> <b>system.</b> Then the particle swarm optimization algorithm is integrated into the solving program, where the inertia weight factor and mutation probability are tuned self-adaptively according to the motion trajectory of particles in longitudinal direction and lateral direction. When the termination criterion is satisfied, the rotational matrix is obtained from the neural network’s stable weights. Then the translational vector is solved, so the position and orientation of camera frame with respect to wrist frame is achieved. The proposed approach provides a new scheme for robot sense calibration with self-adaptive technique, which guarantees the orthogonality of solve...|$|E
40|$|What {{viewpoint}} control {{strategies are}} important for exploring the shape of unknown, curved objects? We argue that strategies that control viewpoint on the tangent plane of automatically-selected points projecting to the occluding contour can be very useful for local, global, qualitative, as well as quantitative shape recovery. This paper studies the design and implementation of an uncalibrated <b>hand-eye</b> <b>system</b> for tangential viewpoint control. We show that tangential viewpoint control can be accurately performed in real time using an uncalibrated camera to control a robot manipulator with at least three degrees of freedom. The only requirement is that at least four points can be identified on the robot’s end-effector and can be tracked across frames. ...|$|E
40|$|In <b>hand-eye</b> <b>systems</b> for {{advanced}} robotic {{applications such as}} assembly, degrees-of-freedom of the vision sensor should be increased appropriately to cope with unstable scene conditions. Particularly, in case of using a simple vision sensor, an intelligent use of the sensor is essential to compensate for its inability to adapt to changing environment. This paper describes a vision sensor set-up planning system which automatically generates plans, based on the environmental models, of illumination positions to achieve reliable tasks. A typical vision task in which edges on an object are measured to determine the position and the orientation is assumed for the sensor set-up planning. In this context, the system is able to generate plans for camera position, illumination position, {{and a set of}} edges to be measured. The system determines the best plans of illumination for stationary or moving objects by evaluating scene conditions for reliable measurement of the object position such as edge length, contrast, and relative angles based on the model of the object and the task environment. Automatic vision sensor set-up planning functions as shown in this paper will play an important role not only for autonomous robotic systems but also for teleoperation systems in assisting advanced tasks...|$|R
40|$|This paper {{proposes a}} hand-eye LRF-based (laser range finder) welding plane-detection method for {{autonomous}} robotic welding {{in the field}} of shipbuilding. The <b>hand-eye</b> LRF <b>system</b> consists of a 6 DOF manipulator and an LRF attached to the wrist of the manipulator. The welding plane is detected by the LRF with only the wrist's rotation to minimize a mechanical error caused by the manipulator's motion. A position on the plane is determined as an average position of the detected points on the plane, and a normal vector to the plane is determined by applying PCA (principal component analysis) to the detected points. In this case, the accuracy of the detected plane is analysed by simulations with respect to the wrist's angle interval and the plane angle. As a result of the analysis, an iterative plane-detection method with the manipulator's alignment motion is proposed to improve the performance of plane detection. For verifying the feasibility and effectiveness of the proposed plane-detection method, experiments are carried out with a prototype of the <b>hand-eye</b> LRF-based <b>system,</b> which consists of a 1 DOF wrist's joint, an LRF system and a rotatable plane. In addition, the experimental results of the PCA-based plane detection method are compared with those of the two representative plane-detection methods, based on RANSAC (RANdom SAmple Consensus) and the 3 D Hough transform in both accuracy and computation time's points of view...|$|R
40|$|Specifically, we {{consider}} automatic assembly systems with real-time visual sensing {{for the back}} shells of cellular phones. Typically, industrial assembly tasks are accomplished using either the look-then-move open-loop or the look-and-move closed-loop control approach. For either approach, successful assembly requires that issues concerned with task accuracy must be considered based on camera calibration parameters. For the <b>hand-eye</b> robotic <b>system</b> to operate in real-time, one must adopt an appropriate control structure to maximize task efficiency. Simple and repetitive assembly tasks can be performed quickly through look-then-move open-loop controls. However, relatively slower look-and-move closed-loop control approaches are better suited for complex tasks or those requiring greater flexibility. To accomplish automatic assembly tasks with real-time visual sensing, either eye-to-hand or eye-in-hand vision must be employed. The proposed vision-based control approaches for back shell assembly tasks {{are likely to have}} real potential in industrial manufacturing applications...|$|R
40|$|During {{the next}} decade it will become {{practical}} to use more and more sophisticated techniques of automation [...] we shall call this "robotics" [...] both in established industries and in new areas. The rate at which these techniques become available will depend {{very much on the}} way research programs are organized to pursue them. The issues involved are rather large and touch not only on technical matters but also on aspects of national economic policy and attitudes toward world trade positions. The project herein proposed is concerned with the development of two particular aspects of Robotics, namely; 1.) Development of a miniature <b>hand-eye</b> <b>system</b> 2.) Development of remote, ARPA-NETWORK style operation of robotic systems, in which simple jobs are handled locally while more complex computations are done on a larger scale...|$|E
40|$|Human {{grasping}} still outshines its robotical counterparts {{with respect}} to accuracy, speed, robustness, and flexibility. When {{trying to develop a}} robotical <b>hand-eye</b> <b>system,</b> it therefore suggests itself to examine the results of neuroscience. In this paper, we describe a model for visually guided reach-to-grasp movements that unifies the two robotical strategies look-then-move and visual servoing, thereby compensating the problems that each strategy shows when used alone. This model was developed by analyzing and extending current models for the control of human reach-to-grasp movements. 1 INTRODUCTION Using sensor information to control robots has become a very popular field of research, since it promises to lead to the design of autonomous robots. In contrast to their preprogrammed industrial counterparts, autonomous robots must {{be able to deal with}} unexpected events such as obstacles or misplaced objects. Nowadays, vision is by far the most commonly used sensor because CCD cameras ar [...] ...|$|E
40|$|A {{system is}} {{proposed}} {{for the development of}} new techniques for the control and monitoring of a mechanical arm-hand. The use of visual feedback is seen to provide new interactive capabilities in a machine <b>hand-eye</b> <b>system.</b> The proposed system explores the use of visual feedback in such operations as the pouring and stirring of liquids, the location of objects for grasping, and the simple rote learning of new arm motions. This paper reproduces a thesis proposal of the same title submitted to the Dept. of Electrical Engineering for the degree of Master of Science. Work reported herein was conducted at the Artificial Intelligence Laboratory, a Massachusetts Institute of Technology research program supported in part by the Advanced Research Projects Agency of the Department of Defense and monitored by the Office of Naval Research under Contract Number N 00014 - 70 -A- 0362 - 0003. Vision flashes are informal papers intended for internal use. MIT Artificial Intelligence Laboratory Robotics Section Department of Defense Advanced Research Projects Agenc...|$|E
40|$|This {{investigation}} {{employed a}} transfer of training paradigm and a simple simulation of an endoscopic task to explore {{the nature of the}} relationships across different components of the simulation task. The simulation disrupted the participant 2 ̆ 7 s normal hand-eye coordination and created a loss of binocular stereopsis information about the relative depth of the objects in the task space. The goal of the investigation was to attempt to understand the <b>hand-eye</b> coordination <b>system</b> by manipulating the constraints under which it operated. The hand-eye coupling that was established during the training phase of the experiment seemed to be an important factor for supporting the transfer of skilled performance. The static form of the visual stimulus that was provided for performance feedback and the particular hand movements that were required to accomplish the simulation task were not as important for supporting the transfer of performance in this task...|$|R
40|$|Abstract—In {{this paper}} we tackle a Fish-Catching task under a visual {{feedback}} <b>hand-eye</b> robotic <b>system</b> with a catching net. As {{the time of}} tracking and catching process flows, the fish can somewhat get accustomed to the net motion pattern and gradually find out new strategies on how {{to escape from the}} bothering net. For the sake of such innate ability being widely existed in animal behavior, the catching operation becomes tough and some effective intelligent method needs to be conceived to go beyond the fish intelligence. The {{purpose of this paper is}} to construct intelligent system to be able to exceed the fish intelligence in order to track and catch the fish successfully. Then we embed chaotic and random motion into the net motion to realize a kind of robotic intelligence, and we shown the chaotic and random net motion is effective to overcome the fish escaping strategies. The effectiveness of the chaotic and random motion is confirmed through successive fish catching experiment. I...|$|R
40|$|This paper {{addresses}} {{issues related}} to the design and implementation of robotic assembly tasks. Specifically, we consider automatic assembly systems with real-time visual sensing for the back shells of cellular phones. Typically, industrial assembly tasks are accomplished using either the look-then-move open-loop or the look-and-move closed-loop control approach. For either approach, successful assembly requires that issues concerned with task accuracy must be considered based on camera calibration parameters. For the <b>hand-eye</b> robotic <b>system</b> to operate in real-time, one must adopt an appropriate control structure to maximize task efficiency. Simple and repetitive assembly tasks can be performed quickly through look-then-move open-loop controls. However, relatively slower look-and-move closed-loop control approaches are better suited for complex tasks or those requiring greater flexibility. To accomplish automatic assembly tasks with real-time visual sensing, either eye-to-hand or eye-in-hand vision must be employed. The proposed vision-based control approaches for back shell assembly tasks are likely to have real potential in industrial manufacturing applications. <br /...|$|R
40|$|Real world {{manipulation}} tasks vary {{in their}} demands for precision and freedoms controlled. In particular, during any one task the complexity may vary with time. For a robotic <b>hand-eye</b> <b>system,</b> precise tracking and control of full pose is computationally expensive and less robust than rough tracking of {{a subset of the}} pose parameters (e. g. just translation). We present an integrated vision and control system in which the vision component provides (1) the continuous, local feedback at the required complexity for robot manipulation and (2) the discrete state information needed to switch between control modes of differing complexity. 1 Introduction In robotic hand-eye tasks (which we will also refer to as visual servoing or visionbased manipulation), the rote motions of classical industrial robotics are replaced by a flexible control strategy which is robust to deviations in camera positioning, robot calibration, and placement of objects in the robot workspace. In large part, this robus [...] ...|$|E
40|$|In this paper, {{we discuss}} both {{theoretical}} and implementation issues of a vision based control approach {{applied to a}} mobile robot. After having briefly presented a visual servoing framework based on the task function approach, we point out some problems of its application {{to the case of}} mobile nonholonomic robots. We will show how using additional degrees of freedom provided by a manipulator arm, allows to overpass these difficulties by introducing redundancy with respect to the task. The second part of the paper deals with the development of an experimental testbed specially designed to rapidly implement and validate reactive vision based tasks. It is constituted by a mobile robot carrying a <b>hand-eye</b> <b>system</b> using a dedicated vision hardware based on VLSI's and DSP 96002 's processors, which allows to perform image processing at video rate. Programming aspects are fully taken into account from tasks level specification up to real time implementation by means of powerfull software tools [...] ...|$|E
40|$|In {{this paper}} {{we present a}} general {{framework}} by which a robotic <b>hand-eye</b> <b>system</b> can perform learned tasks by recalling the action sequences that it has learned. In the training phase, the system learns {{the relationship between the}} sensors and the actuators from a series of training examples supplied interactively by a system trainer. The system automatically builds a recursive partition tree (RPT) which approximates the mapping from the input to the output. Each node of the RPT represents a cell of the space which is further partitioned by its children via a Voronoi tessellation. Each leaf node corresponds to a training sample and stores the corresponding output. In the performance phase, given an input, the RPT is used to retrieve the desired output by interpolating among all the leaf nodes that are good matches to the input. The RPT results in a logarithmic average time complexity in the number of stored training samples. Such a mechanism is used to accomplish major components of the [...] ...|$|E
40|$|Abstract—This paper {{develops}} a novel calibration method for si-multaneously calibrating the intrinsic parameters {{of a camera}} and the hand–eye–workspace relationships of an eye-to-hand system using a line laser module. Errors in the parameters of a <b>hand-eye</b> coordination <b>system</b> lead to errors in the position targeting in the control of robots. To solve these problems, the proposed method uti-lizes a line laser module that is mounted on the hand to project laser beams onto the working plane. As well as calibrating the system parameters, the proposed method is effective when the eye cannot see the hand and {{eliminates the need for}} a precise calibration pat-tern or object. The collected laser stripes in the images must satisfy nonlinear constraints at each hand pose. A closed-form solution is derived by decoupling nonlinear relationships based on the homo-geneous transform and parallel plane/line constraints. A nonlinear optimization, which considers all parameters simultaneously with-out error propagation problem, is to refine the closed-form solution. This two-stage process can be executed automatically without man-ual intervention. The computer simulation and experiment verify the effectiveness of the proposed method and reveal that using a line laser is more accurate than using a single-point laser. Index Terms—Camera calibration, hand–eye calibration, laser, machine vision. I...|$|R
40|$|In this paper, we {{demonstrate}} how a new interactive 3 D desktop metaphor based on two-handed 3 D direct manipulation registered with head-tracked stereo viewing {{can be applied}} to the task of constructing animated characters. In our configuration, a six degree-of-freedom head-tracker and CrystalEyes shutter glasses are used to produce stereo images that dynamically follow the user head motion. 3 D virtual objects can be made to appear at a fixed location in physical space which the user may view from different angles by moving his head. To construct 3 D animated characters, the user interacts with the simulated environment using both hands simultaneously: the left hand, controlling a Spaceball, is used for 3 D navigation and object movement, while the right hand, holding a 3 D mouse, is used to manipulate through a virtual tool metaphor the objects appearing in front of the screen. In this way, both incremental and absolute interactive input techniques are provided by the <b>system.</b> <b>Hand-eye</b> coo [...] ...|$|R
40|$|This thesis {{investigates the}} use of neural {{networks}} and nonlinear estimation in robotic motor learning. It presents a detailed experimental investigation of the performance and parametric sensitivity of resource-allocating neural networks along with a new learning algorithm that offers rapid adaptation and excellent accuracy. It also includes an appendix that relates feed-forward neural networks to familiar mathematical ideas. In addition, it presents two learning <b>hand-eye</b> calibration <b>systems,</b> one based on neural networks {{and the other on}} nonlinear estimation. The network-based system learns to correct robot positioning errors arising from {{the use of}} nominal system kinematics, while the estimation-based system identifies the robot's kinematic parameters. Both systems employ the same two-link robot with stereo vision, and include noise and various other error sources. The network-based system is robust to all error sources considered, though noise naturally limits performance. The estimation-based system has significantly better performance when the robot and vision systems are well modeled, but is extremely sensitive to unmodeled error sources and noise. Finally, it presents a robot control system based on neural networks that learns to catch balls perfectly without requiring explicit programming or conventional controllers. It uses only feed-forward pursuit motions learned through practice, and is initially incapable of even moving its arm in response to external stimuli. It learns to identify and control its pursuit movements, to identify and predict ball behavior, and, with the aid of advice from a critic, to modify its movement commands to improve catching success. The system, which incorporates information from visual, arm state, and drive force sensors, characterizes control situations using input/response pairs. This allows it to learn and respond to plant variations without requiring parametric models or parameter identification. It achieves robust execution by comparing predicted and observed behavior, using inconsistencies to trigger learning and behavioral change. The architectural approach, which involves both declarative and analog knowledge as well as short- and long-term memory, can be extended to learning other sensor-motor skills like mechanical assembly and synchronizing motor actions with external processes...|$|R
40|$|In tele-assistance a robot {{operates}} partly {{guided by}} a human and partly autonomously. Tele-assistance thus fills in the gap between full autonomy, and direct control by a human tele-operator. Instead of directly controlling the motor actions of a robot, in tele-assistance the human interacts with the robot using a deictic high level language. Pook and Ballard have implemented such a system using a hand sign language, sensed by an exoskeleton[26]. In this work we present a method in which the robot is instructed through visual pointing. The human operator instructs the robot by pointing out (currently using a mouse and images displayed on a computer screen) desired manipulations. The tele-assistance system transforms the coarse task descriptions supplied by the human into a visual (camera) space task and trajectory plan. The manipulation is carried out using visual servoing on an uncalibrated <b>hand-eye</b> <b>system.</b> For this we have developed an integrated model estimation, control and visual [...] ...|$|E
40|$|This paper {{describes}} a <b>hand-eye</b> <b>system</b> we developed {{to perform the}} bin-picking task. Two basic tools are employed: the photometric stereo method and the extended Gaussian image. The photometric stereo method generates the surface normal distribution of a scene. The extended Gaussian image allows us to determine {{the attitude of the}} object based on the normal distribution. Visual anaIysis of an image consist, ^. of two stages. The first stage segments t,he image into regions and determines the target region. The photometric stereo system provides the surface normal distribution of the scene. The system segments the scene into isolated regions using the surface normal distribution rather than the brightness distribution. The second stage determines the object attitude and position by comparing the surface normal distribution with the extended-Gaussian-image. Fingers, with LED sensor, mounted on the PUMA arm can successfully pick an object from a pile based on the information from the vision part...|$|E
40|$|This paper {{describes}} a simple method of converting visual coordinates to arm coordinates {{which does not}} require knowledge {{of the position of}} the camera(s). Comparisons are made to other methods and two camera, three dimensional extensions are discusssed. The single camera method for converting points on a tabletop is used by Marc Raibert and Glen Speckert in a working <b>hand-eye</b> <b>system</b> which recognizes objects and picks them up under visual guidance. This was implemented on the MIT Micro-Automation PDP 11 / 45 using a low speed vidicon and a Scheinman arm. This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research. contract N 88814 - 75 C- 8643 - 8885. MIT Artificial Intelligence Laboratory Department of Defense Advanced Research Projects Agenc...|$|E
40|$|In this paper, we {{demonstrate}} a new interactive 3 D desktop metaphor based on two-handed 3 D direct manipulation registered with head-tracked stereo viewing. In our configuration, a six-degree-of-freedom head-tracker and CrystalEyes shutter glasses {{are used to}} produce stereo images that dynamically follow the user head motion. 3 D virtual objects {{can be made to}} appear at a fixed location in physical space which the user may view from different angles by moving his head. The user interacts with the simulated 3 D environment using both hands simultaneously. The left hand, controlling a Spaceball, is used for 3 D navigation and object movement, while the right hand, holding a 3 D mouse, is used to manipulate through a virtual tool metaphor, the objects appearing in front of the screen because of negative parallax. In this way, both incremental and absolute interactive input techniques are provided by the <b>system.</b> <b>Hand-eye</b> coordination is made possible by registration between virtual and physical space, allowing a variety of complex 3 D tasks to be performed more easily and more rapidly than is possible using traditional interactive techniques. The system has been tested using both Polhemus Fastrak and Logitech ultrasonic input devices for tracking the head and 3 D mouse. 197 - 206 Pubblicat...|$|R
40|$|Systems {{that use}} visual {{information}} for manipulator control usually fall {{into one of}} two groups: Classical open-loop methods which localise objects in video images and then guide the robot accordingly, or visual servo systems that close the control loop with a continuous feedback of visual information. We propose a hybrid <b>hand-eye</b> <b>system</b> that combines both methods {{in such a way that}} the required accuracy of the subsystems can be reduced, an approach which is inspired by the human "hand-eye system". 1 Introduction In the last years, using sensor information to control robots has become a very popular field of research, as it promises to lead to the design of autonomous robots, which, in contrast to their preprogrammed industrial counterparts, can deal with "the unexpected", for example obstacles or misplaced objects. This is especially important for "personal robots" that operate in the everyday world, that cannot and should not be adapted to the needs of machines. When it comes to se [...] ...|$|E
40|$|Abstract. Real world {{manipulation}} tasks vary {{in their}} demands for precision and freedoms controlled. In particular, during any one task the complexity may vary with time. For a robotic <b>hand-eye</b> <b>system,</b> precise tracking and control of full pose is computationally expensive and less robust than rough tracking of {{a subset of the}} pose parameters (e. g. just translation). We present an integrated vision and control system in which the vision component provides (1) the continuous, local feedback at the required complexity for robot manipulation and (2) the discrete state information needed to switch between control modes of differing complexity. 1 Introduction In robotic hand-eye tasks (which we will also refer to as visual servoing or visionbased manipulation), the rote motions of classical industrial robotics are replaced by a flexible control strategy which is robust to deviations in camera positioning, robot calibration, and placement of objects in the robot workspace. In large part, this robustness is due to feedback from vision systems which observe a robot action in progress, allowing for continual adaptation of robot motion. Despite the importance of vision in these tasks, vision systems which support hand-eye robot coordination have generally been developed ad hoc, designed specifically for particular tasks. In this paper, we present a principled approach to vision and control system design which supports complex vision-based manipulation...|$|E
