827|2|Public
25|$|GPU-based {{implementations}} of {{this approach}} won many pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of neuronal structures in EM stacks challenge, the <b>ImageNet</b> Competition, and others.|$|E
25|$|This {{killed off}} {{the market for a}} {{dedicated}} physics accelerator, and superseded Cell in video game consoles, and eventually led to their use in running convolutional neural networks such as AlexNet (which exhibited leading performance the <b>ImageNet</b> Large Scale Visual Recognition Challenge).|$|E
2500|$|Currently, {{the best}} {{algorithms}} for such tasks {{are based on}} convolutional neural networks. [...] An illustration of their capabilities is given by the <b>ImageNet</b> Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and hundreds of object classes. [...] Performance of convolutional neural networks, on the <b>ImageNet</b> tests, is now close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. [...] Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.|$|E
50|$|The <b>ImageNet</b> {{project is}} a large visual {{database}} designed for use in visual object recognition software research. As of 2016, over ten million URLs of images have been hand-annotated by <b>ImageNet</b> to indicate what objects are pictured; {{in at least one}} million of the images, bounding boxes are also provided. The database of annotations of third-party image URL's is freely available directly from ImageNet; however, the actual images are not owned by <b>ImageNet.</b> Since 2010, the <b>ImageNet</b> project runs an annual software contest, the <b>ImageNet</b> Large Scale Visual Recognition Challenge (ILSVRC), where software programs compete to correctly classify and detect objects and scenes.|$|E
5000|$|ImageNets is an {{open source}} and {{platform}} independent (Windows & Linux) framework for rapid prototyping of Machine Vision algorithms. With the GUI <b>ImageNet</b> Designer, no programming knowledge is required to perform operations on images. A configured <b>ImageNet</b> can be loaded and executed from C++ code {{without the need for}} loading the <b>ImageNet</b> Designer GUI to achieve higher execution performance. Originally, ImageNets was developed for the Care-Providing Robot FRIEND [...] but it can be used {{for a wide range of}} computer vision applications.|$|E
5000|$|Image analytics: widgets {{for working}} with images and <b>ImageNet</b> embeddings ...|$|E
5000|$|<b>ImageNet</b> (Alex Berg, Jia Deng, Fei-Fei Li, Wei, Liu, Olga Russakovsky and team) ...|$|E
50|$|Among her best-known {{work is the}} <b>ImageNet</b> project, {{which has}} {{revolutionized}} the field of large-scale visual recognition.|$|E
5000|$|<b>ImageNet</b> crowdsources its {{annotation}} process. Image-level annotations {{indicate the}} presence or absence of an object class in an image, such as [...] "there are tigers in this image" [...] or [...] "there are no tigers in this image". Object-level annotations provide a bounding box around the (visible part of the) indicated object. <b>ImageNet</b> uses a variant of the broad WordNet schema to categorize objects, augmented with 120 categories of dog breeds to showcase fine-grained classification.|$|E
50|$|SenseTime and AMAX Co-Developed Deep Learning Engine, SenseBox, wins 1st Place in two {{categories}} at the 6th annual <b>ImageNet</b> Large Scale Visual Recognition Challenge (ILSVRC2015).|$|E
50|$|GPU-based {{implementations}} of {{this approach}} won many pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of neuronal structures in EM stacks challenge, the <b>ImageNet</b> Competition and others.|$|E
50|$|One of {{the most}} famous {{datasets}} in computer vision is <b>ImageNet</b> which is used to assess the problem of object level Image classification. <b>ImageNet</b> {{is one of the largest}} annotated datasets available and has over one million images. The other important vision task is object detection and localisation which refers to detecting the object instance in the image and providing the bounding box coordinates around the object instance or segmenting the object. The most popular dataset for this task is the Pascal dataset. Similarly there are other datasets for specific tasks like the H3D dataset for human pose detection, Core dataset to evaluate the quality of detected object attributes such as colour, orientation, and activity.|$|E
50|$|This {{killed off}} {{the market for a}} {{dedicated}} physics accelerator, and superseded Cell in video game consoles, and eventually led to their use in running convolutional neural networks such as AlexNet (which exhibited leading performance the <b>ImageNet</b> Large Scale Visual Recognition Challenge).|$|E
5000|$|<b>ImageNet</b> is {{an image}} {{database}} organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images. Currently it has an average of over five hundred images per node.|$|E
5000|$|The DeepDream {{software}} {{originates in}} a deep convolutional network codenamed [...] "Inception" [...] after the film of the same name, was developed for the <b>ImageNet</b> Large-Scale Visual Recognition Challenge (ILSVRC) in 2014 and released in July 2015.The software is designed to detect faces and other patterns in images, {{with the aim of}} automatically classifying images.|$|E
5000|$|AlexNet is {{the name}} of a {{convolutional}} neural network, originally written CUDA to run with GPU support, which competed in the <b>ImageNet</b> Large Scale Visual Recognition Challenge in 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points ahead of the runner up. AlexNet was designed by the SuperVision group, consisting of Alex Krizhevsky, Geoffrey Hinton, and Ilya Sutskever.|$|E
50|$|Currently, {{the best}} {{algorithms}} for such tasks {{are based on}} convolutional neural networks. An illustration of their capabilities is given by the <b>ImageNet</b> Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and hundreds of object classes. Performance of convolutional neural networks, on the <b>ImageNet</b> tests, is now close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.|$|E
50|$|While {{all these}} advancements were being made, the {{community}} {{felt the need}} to have standardised datasets and evaluation metrics so the performances can be compared. This led to the emergence of challenges like the Pascal VOC challenge and the <b>ImageNet</b> challenge. The availability of standard evaluation metrics and the open challenges gave directions to the research. Better algorithms were introduced for specific tasks like object detection and classification.|$|E
50|$|Before the competition, {{training}} data was released for detection from the <b>ImageNet</b> Large-Scale Visual Recognition Challenge (ILSVRC). Source {{code of the}} referee system was {{released to the public}} in March 2015. For the competition, an intranet was established for the contestants to retrieve provided image files from and return answers to the competition's referee system. Teams were given 10 minutes to process images, which were ranked by detection accuracy and energy usage.|$|E
50|$|Gym aims {{to provide}} an easy-to-setup general-intelligence {{benchmark}} {{with a wide variety}} of different environments (somewhat akin to, but broader than, the <b>ImageNet</b> Large Scale Visual Recognition Challenge used in supervised learning research), and that hopes to standardize the way in which environments are defined in AI research publications, so that published research becomes more easily reproducible. The project claims to provide the user with a simple interface. As of June2017, the gym can only be used with Python.|$|E
50|$|The <b>ImageNet</b> Large Scale Visual Recognition Challenge is a {{benchmark}} in object classification and detection, {{with millions of}} images and hundreds of object classes. In the ILSVRC 2014, a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. As of that performance of convolutional neural networks on the <b>ImageNet</b> tests was close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.|$|E
50|$|Following the 2005 {{paper that}} {{established}} {{the value of}} GPGPU for machine learning, several publications described more efficient ways to train convolutional neural networks using GPUs. In 2011, they were refined and implemented on a GPU, with impressive results. In 2012, Ciresan et al. significantly improved on the best performance in the literature for multiple image databases, including the MNIST database, the NORB database, the HWDB1.0 dataset (Chinese characters), the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images), and the <b>ImageNet</b> dataset.|$|E
5000|$|Further {{acquisitions}} followed, {{with the}} purchase in 2004 of <b>image.net</b> for US$20 million. On February 9, 2006, the microstock photo website iStockphoto {{was acquired by}} Getty Images for US$50 million. In 2007, Getty successfully purchased its largest competitor, MediaVast, for $207 million. The acquisition meant that Getty Images gained control of WireImage (Entertainment, creative, and sports photography), FilmMagic (fashion and red carpet photography), Contour Photos (portrait and studio photography). Getty Images also acquired {{a host of other}} subsidiaries including [...] "Master Delegates" [...] who include: Isifa Image Service in Prague, Laura Ronchi in Italy.|$|E
5000|$|Li {{also led}} the Scalable Display Wall project which explores {{how to build}} and use a high-resolution, wall-size display system to {{visualize}} massive datasets. Recently, {{he has been working}} with colleagues at Stanford on a large well-labelled image dataset called <b>ImageNet</b> [...] to help computer vision community develop object recognition and classification methods for large image data. More recently, he has been working with colleagues at Princeton on developing methods to efficiently manage and analyze the vast amount of data generated by increasingly sophisticated research in fields ranging from genomics to neuroscience.|$|E
50|$|Significant {{additional}} impacts in {{image or}} object recognition were felt from 2011-2012. Although CNNs trained by backpropagation {{had been around}} for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs with max-pooling on GPUs {{in the style of}} Ciresan and colleaguesg were needed to progress on computer vision. In 2011, this approach achieved for the first time superhuman performance in a visual pattern recognition contest. Also in 2011, it won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest. Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In October 2012, a similar system by Krizhevsky and Hinton won the large-scale <b>ImageNet</b> competition by a significant margin over shallow machine learning methods. In November 2012, Ciresan et al.'s system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic. In 2013 and 2014, the error rate on the <b>ImageNet</b> task using deep learning was further reduced, following a similar trend in large-scale speech recognition. The Wolfram Image Identification project publicized these improvements. Image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs. Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.|$|E
50|$|By 1991 {{such systems}} {{were used for}} {{recognizing}} isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng et al. suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron, a method for performing 3-D object recognition directly from cluttered scenes. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron automatically learned an open number of unsupervised features in each layer, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. <b>ImageNet</b> tests), was first used in Cresceptron to reduce the position resolution {{by a factor of}} (2x2) to 1 through the cascade for better generalization.|$|E
5000|$|Since 2010, {{the annual}} <b>ImageNet</b> Large Scale Visual Recognition Challenge (ILSVRC) is a {{competition}} where research teams evaluate their algorithms on the given data set, and compete to achieve higher accuracy on several visual recognition tasks. The ILSVRC aims to [...] "follow in the footsteps" [...] of the smaller-scale PASCAL VOC challenge, established in 2005, which contained only about 20,000 images and twenty object classes. The 2010s saw dramatic progress in image processing. Around 2011, a good ILSVRC classification error rate was 25%. In 2012, a deep convolutional neural net achieved 16%; {{in the next}} couple of years, error rates fell to a few percent. By 2015, researchers reported that software exceeded human ability at the narrow ILSVRC tasks. However, as one of the challenge's organisers, Olga Russakovsky, pointed out in 2015, the programs only have to identify images as belonging to one of a thousand categories; humans can recognize a larger number of categories, and also (unlike the programs) can judge the context of an image.|$|E
40|$|The {{original}} <b>ImageNet</b> dataset is {{a popular}} large-scale benchmark for training Deep Neural Networks. Since the cost of performing experiments (e. g, algorithm design, architecture search, and hyperparameter tuning) on the original dataset might be prohibitive, we propose to consider a downsampled version of <b>ImageNet.</b> In contrast to the CIFAR datasets and earlier downsampled versions of <b>ImageNet,</b> our proposed <b>ImageNet</b> 32 × 32 (and its variants <b>ImageNet</b> 64 × 64 and <b>ImageNet</b> 16 × 16) contains exactly {{the same number of}} classes and images as <b>ImageNet,</b> with the only difference that the images are downsampled to 32 × 32 pixels per image (64 × 64 and 16 × 16 pixels for the variants, respectively). Experiments on these downsampled variants are dramatically faster than on the original <b>ImageNet</b> and the characteristics of the downsampled datasets with respect to optimal hyperparameters appear to remain similar. The proposed datasets and scripts to reproduce our results are available at [URL] and [URL]...|$|E
30|$|Tiny <b>ImageNet</b> is {{a subset}} of the <b>ImageNet</b> {{challenge}} (ILSVRC). It contains 200 different categories. Each class has 500 training images, 50 validation images, and 50 test images. In addition, the images are resized to 64 [*]×[*] 64 pixels (256 [*]×[*] 256 pixels in standard <b>ImageNet).</b>|$|E
40|$|To compare disc {{measurements}} {{obtained by}} indirect ophthalmoscopy, the Heidelberg Retina Tomograph (HRT), stereoscopic slide viewing (SSV) of color transparencies, and the Topcon <b>ImageNet</b> System (<b>ImageNet).</b> Population-based cross-sectional study. From the Rotterdam Study, 324 subjects (567 eyes) were nonselectively included. All underwent a full ophthalmologic examination in mydriasis. Vertical cup/disc ratios (VCDRs) were compared between all four methods and disc area (mm(2)), neural rim area (mm(2)), cup area (mm(2)), and cup volume (mm(3)) between HRT and <b>ImageNet.</b> Mean VCDR for ophthalmoscopy was 0. 25 (standard error [SE], 0. 007), for HRT 0. 42 (SE, 0. 008), for SSV 0. 39 (SE, 0. 010), and for <b>ImageNet</b> 0. 50 (SE, 0. 006). The correlation for VCDR between ophthalmoscopy, the two devices, and SSV was 0. 42, respectively 0. 57; between <b>ImageNet</b> and HRT 0. 75. The 97. 5 th percentiles of the VCDR for ophthalmoscopy, HRT, SSV, and <b>ImageNet</b> were 0. 80, 0. 73, 0. 80, and 0. 73, respectively; the 99. 5 th percentiles thus were 0. 90, 0. 79, 0. 86, and 0. 79. The mean disc area, rim area, cup area, and cup volume were 2. 08, 1. 63, 0. 45 mm(2) and 0. 09 mm(3) for HRT, and 2. 39, 1. 77, 0. 61 mm(2) and 0. 16 mm(3) for <b>ImageNet,</b> respectively. The corresponding correlations for these four parameters were 0. 67, 0. 42, 0. 81, and 0. 82. Different techniques lead to considerable differences in disc morphometric values. <b>ImageNet</b> produced higher mean values compared with HRT and ophthalmoscopy. Ophthalmoscopy showed the lowest correlations and SSV the highest {{ones with the}} two semiautomated devices. Between <b>ImageNet</b> and HRT the correlation for all parameters was high except for the neural rim are...|$|E
40|$|In {{a recent}} decade, <b>ImageNet</b> {{has become the}} most notable and {{powerful}} benchmark database in computer vision and machine learning community. As <b>ImageNet</b> has emerged as a representative benchmark for evaluating the performance of novel deep learning models, its evaluation tends to include only quantitative measures such as error rate, rather than qualitative analysis. Thus, there are few studies that analyze the failure cases of deep learning models in <b>ImageNet,</b> though there are numerous works analyzing the networks themselves and visualizing them. In this abstract, we qualitatively analyze the failure cases of <b>ImageNet</b> classification results from recent deep learning model, and categorize these cases according to the certain image patterns. Through this failure analysis, we believe that it can be discovered what the final challenges are in <b>ImageNet</b> database, which the current deep learning model is still vulnerable to. Comment: Poster presented at CVPR 2017 Scene Understanding Worksho...|$|E
30|$|<b>ImageNet</b> 100. This dataset {{contains}} the 100 largest classes from <b>ImageNet</b> (183, 116 images with size 23.6  GB). In each class, we sample 50  % images {{for training and}} 50  % images for testing (with random guess 1  %). We also construct BoW of every image using dense SIFT descriptor and 5, 000 codewords. For feature mapping, we use the same method as we do with <b>ImageNet</b> 10. The final size of training data is 8  GB.|$|E
30|$|On <b>ImageNet,</b> {{the results}} show that the {{proposed}} attack can be generalized to large size images and fool the corresponding larger neural network. Note that the <b>ImageNet</b> results are done with the same settings as CIFAR- 10 while the resolution of images we use for the <b>ImageNet</b> test is 227 × 227, which is 50 times larger than CIFAR- 10 (32 × 32). However, confidence results on CIFAR- 10 dataset is comparatively much higher than <b>ImageNet.</b> In each successful attack, the probability label of the target class (selected by the attack) is the highest. Therefore, the average confidence on <b>ImageNet</b> is relatively low but tell us that the remaining 999 classes are even lower such that the output becomes an almost uniform soft label distribution. To sum it up, the attack can break the confidence of AlexNet to a nearly uniform soft label distribution. The results indicate the large images can be less vulnerable than mid-sized images.|$|E
30|$|At the <b>ImageNet</b> Computer Vision Competition, Hinton et al. [17] {{demonstrated}} an approach using Deep Learning and Convolutional Neural Networks which outperformed other existing approaches for image object recognition. Using the <b>ImageNet</b> dataset, {{one of the}} largest for image object recognition, Hinton’s team showed the importance of Deep Learning for improving image searching. Dean et al. [38] demonstrated further success on <b>ImageNet</b> by using a similar Deep Learning modeling approach with a large-scale software infrastructure for training an artificial neural network.|$|E
40|$|Continuing {{advances}} in high speed computer network and distributed database technology have made possible new applications. One such application is <b>ImageNet,</b> a color image database and interconnecting network designed {{on a national}} scale. The concept of <b>ImageNet</b> includes several Database Nodes (DBNs) dispersed throughout the nation and interconnected via a high speed communications network. This work includes the design and implementation of a distributed database for color image retrieval and storage {{in the context of}} the <b>ImageNet</b> project. The system design for <b>ImageNet</b> and its distributed database are presented, and an implementation on multiple workstations was performed. In addition, a simple performance evaluation of the DBN implementation is shown, and suggestions for improving the next version of the DBN are given...|$|E
30|$|Step 1 : Train the {{off-the-shelf}} deep {{model on}} a large-scale database, i.e., <b>ImageNet.</b>|$|E
