2|7|Public
40|$|Abstract: In {{this paper}} {{we present a}} routing scheme which is {{extremely}} suited for use in massively parallel systems. The routing algorithm is fault-tolerant so that network failures will not stop the system. For reasons of scalability, the routing information is extremely compact, also when the network is injured. The wormhole routing technique guarantees very low routing latency. Moreover the routing is guaranteed to be deadlock-free {{without the need for}} <b>dedicated</b> <b>buffering.</b> Key words: compact routing, wormhole rousting, deadlock-free routing, fault tolerance, massively parallel systems Nowadays challenge in supercomputing Is to reach the teraflop goal to deal with the grand challenge applications. Despite the ever increasing performance of the processors, massively parallel systems must still be used. Thousands of processors must work in parallel to obtain this enormous computational power. Not only more processors should be used, the performance of the communication network should also scale. Small delays ar...|$|E
40|$|Abstract. The large-capacity, {{high-speed}} and {{low power}} consumption become the new requirements for the data storage systems. In this paper, a high-performance storage module based on multiple NAND flash memory chips is presented to real-time massive data acquisition system. In order to achieve the miniaturization dimension and the high-speed data storage design requirements, the paper presents a small size and high-speed storage unit based on NAND flash, where {{the dimensions of the}} module can reach 33 mm× 33 mm and the maximum rate is up to 60 MB/s. Ensuring continuous and reliable operation requires a <b>dedicated</b> <b>buffering</b> for the data transmission. We analyze the elements and peculiarities of the flash memory chip and propose a multi-way architecture to speed up data access. The design of a multilevel high-speed buffer structure based on the field programmable gate array (FPGA) technology is introduced in the paper. The proposed system can be applicable to some portable digital equipment...|$|E
40|$|There is {{increasing}} evidence that multimedia traffic {{in a variety}} of networks including the Internet, exhibits properties of self-similarity such as long-range dependence and a high-degree of correlation between arrivals. The demands of self-similar traffic impose unique new requirements in switch design, especially as far as the buffering strategy is concerned. In this paper, we study buffer sharing in the presence of self-similar traffic generated using the fractional ARIMA model. Our results quantify the loss in performance under self-similar traffic as opposed to Poisson traffic, using shared and <b>dedicated</b> output <b>buffering</b> strategies. We show that, with self-similar traffic, shared output buffering in comparison with <b>dedicated</b> output <b>buffering,</b> provides higher throughput and lower cell loss probability, but at the cost of higher cell delay. These results have important implications in the design of switch-based networked multimedia systems. ...|$|R
40|$|Advantageous optical {{interconnect}} technology {{was chosen for}} the projection mask-less lithography application to transmit the exposure data to the blanking plate electronics inside a high-voltage vacuum area. Ensuring continuous and reliable operation requires a <b>dedicated</b> preparation and <b>buffering</b> of the transmission data. This paper presents the implementation aspects and {{the design of a}} high-speed buffer system based on the field programmable gate array (FPGA) technology. The high data rates and the highly parallelized system operation require a specific architecture and careful signal integrity design for proper functionality...|$|R
40|$|This thesis {{deals with}} a 3 D {{visualization}} of the Earth´s surface in real time. A significant part is <b>dedicated</b> to the <b>buffering</b> of {{the large amount of}} data (a height map and textures of Earth) and its effective reading from hard disk directly to the memory of a graphic card. Individual levels of detail of the Earth´s surface are made by GeoMipMapping. The algorithms "horizon" and "frustum culling" are used, which cut off the hidden parts of the planet. The application of the height data is based on a custom algorithm calculating the data directly in the graphic card, in assistance with its vertex and geometry shaders...|$|R
40|$|Maskless {{lithography}} {{is one of}} {{the possible}} solutions to manage the escalating mask costs and demands for faster production cycles. One of the major issues with the maskless lithography technology however is the management and transfer of the enormous data volumes required to define the chip structures. Ensuring competitive and reliable operation requires <b>dedicated</b> preparation and <b>buffering</b> of the lithography data to be transmitted to the exposure unit. An optimized dedicated architecture and careful signal integrity design for proper functionality are needed due to the high data rates and the highly parallelized system operation. This paper presents the implementation aspects and the design of a high-speed transmission system solution for maskless lithography systems. The introduced solution treats a field programmable gate array (FPGA) based implementation for a latency-sensitive high speed lithography system...|$|R
40|$|Asynchronous Transfer Mode (ATM) {{networks}} {{have emerged as}} a promising technology in realization of future broadband integrated services digital networks (B-ISDN). Multipoint communication services are considered a basic functionality in ATM switches to meet the future multimedia applications. In fact, multicasting functionality is an important criterion in judging the powerfulness and extendibility of ATM switches. In this dissertation, a new architecture has been proposed to implement the ATM multicast switch. The switch architecture consists of a stack of binary trees and a bundle of modified Batcher's sorting networks. The basic building blocks are small, and re used to construct the large-scale ATM switches. This modular architecture provides multicast services and priority sorting functions. The input and output switching modules are completely partitioned, and this partitioned switch fabric provides a flexible distributed architecture. To perform the multicast functionality in binary trees, a new addressing scheme called Binary Partition Addressing Scheme (BiPAS) has been proposed. Another important characteristic in the design is the employment of a buffering scheme called hybrid shared memory and <b>dedicated</b> output <b>buffering</b> scheme which manage to reduce the memory requirement in the switch drastically. The performance of the proposed switch is evaluated in terms of switch throughput, average cell delay, and cell loss probability. The results in numerical analysis indicate a satisfactory approximation to our simulation results. The new architecture is evaluated in other aspects, which include overhead, complexity, and scalability issue. Subsequently, our results are compared with some architecture achieves a satisfactory throughput and offers the advantage of lesser cell loss in the switch. Such contribution is significant particularly for data services with low cell loss requirement and high latency-constraint...|$|R
40|$|The planned {{upgrades}} of {{the experiments}} at the Large Hadron Collider at CERN will require higher bandwidth networks for their data acquisition systems. The network congestion problem arising from the bursty many-to-one communication pattern, typical for these systems, will become more demanding. It is questionable whether commodity TCP/IP and Ethernet technologies in their current form will be still able to effectively adapt to the bursty traffic without losing packets due to the scarcity of buffers in the networking hardware. We continue our study {{of the idea of}} lossless switching in software running on commercial-off-the-shelf servers for data acquisition systems, using the ATLAS experiment as a case study. The flexibility of design in software, performance of modern computer platforms, and buffering capabilities constrained solely by the amount of DRAM memory are a strong basis for building a network dedicated to data acquisition with commodity hardware, which can provide reliable transport in congested conditions. In this paper we extend the popular software switch, Open vSwitch, with a <b>dedicated,</b> throughput-oriented <b>buffering</b> mechanism for data acquisition. We compare the performance under heavy congestion of typical Ethernet switches to a commodity server acting as a switch, equipped with twelve 10 Gbps Ethernet interfaces providing a total bandwidth of 120 Gbps. Preliminary results indicate that software switches with large packet buffers perform significantly better, reaching maximum bandwidth, and completely avoiding throughput degradation typical for hardware switches that suffer from high packet drop counts. Furthermore, we evaluate the scalability of the system when building a larger topology of interconnected software switches, highlighting aspects such as management, port density, load balancing, and failover. In this context, we discuss the usability of software-defined networking technologies, Open vSwitch Database and OpenFlow protocols, to centrally manage and optimize a data acquisition network. We build an IP-only leaf-spine network consisting of eight software switches running on separate physical servers as a demonstrator. We intend to show in this paper that building a high bandwidth lossless network based on software switches dedicated for data acquisition is feasible and can be considered as a viable solution for future small- and large-scale systems based on commodity TCP/IP and Ethernet...|$|R
40|$|The bursty many-to-one {{communication}} pattern, {{typical for}} data acquisition systems, is particularly demanding for commodity TCP/IP and Ethernet technologies. The problem arising from {{this pattern is}} widely known in the literature as incast and can be observed as TCP throughput collapse. It {{is a result of}} overloading the switch buffers, when a specific node in a network requests data from multiple sources. This will become even more demanding for future upgrades of the experiments at the Large Hadron Collider at CERN. It is questionable whether commodity TCP/IP and Ethernet technologies in their current form will be still able to effectively adapt to bursty traffic without losing packets due to the scarcity of buffers in the networking hardware. This thesis provides an analysis of TCP/IP performance in data acquisition networks and presents a novel approach to incast congestion in these networks based on software-based packet forwarding. Our first contribution lies in confirming the strong analogies between the TCP behaviour in data acquisition and datacenter networks. We also provide experimental evaluation of different proposals from the datacenter environment for application in data acquisition to improve performance and reduce buffer requirements. The second contribution lies in the design and experimental evaluation of a data acquisition network that is based on software switches. Performance has traditionally been the challenge of this approach, but this situation changes with modern server platforms. High performance load balancers, proxies, virtual switches and other network functions can be now implemented in software and not limited to specialised commercial hardware, thus reducing cost and increasing the flexibility. We first design and optimise a software-based switch with a <b>dedicated,</b> throughput-oriented <b>buffering</b> mechanism for data acquisition. Our experimental results indicate that it performs significantly better than some typical Ethernet switches under heavy congestion. The optimised software switch with large packet buffer reaches maximum bandwidth and completely avoids throughput degradation typical for hardware switches that suffer from high packet drop counts. Furthermore, we evaluate the scalability of the system when building a larger topology of interconnected software switches. We highlight aspects such as management, costs, port density, load balancing, and failover. In this context, we discuss the usability of software-defined networking technologies, Open vSwitch Database and OpenFlow, to centrally manage and optimise a data acquisition network. We have built an IP-only parallel leaf-spine network consisting of eight software switches running on separate physical servers as a demonstrator...|$|R

