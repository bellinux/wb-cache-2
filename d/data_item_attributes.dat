0|5024|Public
40|$|This paper {{describes}} an algorithm {{that is an}} extension of mixture-modelling to supervised clustering. It is demonstrated to be as accurate as current state-of-the-art machine learning algorithms across various data sets, and significantly more accurate than distance-based supervised clustering algorithms. Most significantly, it combines the classification itself with the calculation of rich information about the probabilities of class membership, the significance of attributes in relation to a classification, and the data space described by the <b>data</b> <b>items</b> and <b>attributes...</b>|$|R
40|$|Although {{traditional}} {{databases and}} data warehouses have been exploited widely to manage persistent data, {{a large number}} of applications from sensor network need functional supports for transient data in the continuous data stream. One of the crucial functions is to summarize the <b>data</b> <b>items</b> within a sliding window. A sliding window contains a fixed width span of data elements. The <b>data</b> <b>items</b> are implicitly deleted from the sliding window, when it moves out of the window scope. Several one-dimensional histograms have been proposed to store the succinct time information in a sliding window. Such histograms, however, only handle the <b>data</b> <b>items</b> with <b>attribute</b> values in unary domains. In this paper, we explore the problem of extending the value to a multi-valued domain. A two-dimensional histogram, the hybrid histogram, is proposed to support sliding window queries on a practical multi-valued domain. The basic building block of the hybrid histogram is the exponential histogram. The hybrid histogram is maintained to capture the changes of data distribution. To further compress the exponential histograms, we propose a condensed exponential histogram without losing the error bound. Results of an extensive experimental study are included to evaluate the benefits of the proposed technique. ...|$|R
40|$|In recent years, {{there has}} been much focus on skyline queries that {{incorporate}} and provide more flexible query operators that return <b>data</b> <b>items</b> which are dominating other <b>data</b> <b>items</b> in all <b>attributes</b> (dimensions). Several techniques for skyline have been proposed in the literature. Most of the existing skyline techniques aimed to find the skyline query results by supposing that the values of dimensions are always present for every <b>data</b> <b>item.</b> In this paper we aim to evaluate the skyline preference queries in which some dimension values are missing. We proposed an approach for answering preference queries in a database by utilizing the concept of skyline technique. The skyline set selected for a given query operation is then optimized so that the missing values are replaced with some approximate values that provide a skyline answer with complete data. This will significantly reduce the number of comparisons between <b>data</b> <b>items.</b> Beside that, the number of retrieved skyline <b>data</b> <b>items</b> is reduced and this guides the users to select the most appropriate <b>data</b> <b>items</b> from the several alternative complete skyline <b>data</b> <b>items...</b>|$|R
40|$|Association rules mining is {{a popular}} data mining {{modeling}} tool. It discovers interesting associations or correlation relationships among a large set of <b>data</b> <b>items,</b> showing <b>attribute</b> values that occur frequently together in a given dataset. Despite their great potential benefit, current association rules modeling tools are far from optimal. This article studies how visualization techniques {{can be applied to}} facilitate the association rules modeling process, particularly what visualization elements should be incorporated and how they can be displayed. Original designs for visualization of rules, integration of data and rule visualizations, and visualization of rule derivation process for supporting interactive visual association rules modeling are proposed in this research. Experimental results indicated that, compared to an automatic association rules modeling process, the proposed interactive visual association rules modeling can significantly improve the effectiveness of modeling, enhance understanding of the applied algorithm, and bring users greater satisfaction with the task. The proposed integration of data and rule visualizations can significantly facilitate understanding rules compared to their nonintegrated counterpart. 1...|$|R
40|$|Abstract—Mobile nodes in some {{challenging}} network scenarios {{suffer from}} intermittent connectivity and frequent partitions e. g. battlefield and disaster recovery scenarios. Disruption Tolerant Network (DTN) technologies {{are designed to}} enable nodes in such environments to communicate with one another. In an earlier work, we studied information retrieval schemes for singleattribute queries in DTNs. Our schemes disseminate replicated data copies and queries to local-neighborhood. However, <b>data</b> <b>items</b> often have multiple attributes and not every node can be trusted to store replicated data or queries. Thus, in this paper, we study the scenario where the queries have multiple attributes. In addition, we compare the effectiveness of using opportunistically encountered nodes or specially deployed index and storage points (ISPs) for storing replicated <b>data</b> <b>items</b> or indices of the <b>data</b> <b>items.</b> Specifically, we conduct extensive simulation studies to evaluate three information retrieval schemes namely (a) the Predetermined ISP advertisement (PISA) scheme where ISPs advertise indices of <b>data</b> <b>items</b> with <b>attribute</b> values that fall within some pre-determined ranges, (b) the Opportunistic Regular Node Advertisement with Index Duplication (ORNA-ID) scheme which uses opportunistically encountered nodes for advertising replicated indices, and (c) the Opportunistic Regular Node Advertisement with Data Duplication (ORNA-DD) scheme which uses opportunistically encountered nodes for storing replicated <b>data</b> <b>items.</b> Our {{results indicate that the}} ORNA-ID scheme and the PISA scheme achieve similar performance. This shows that either architecture can be used. In addition, the ORNA-DD scheme provides 91 % more query success rate and 128 % more overall success rate in the sparsest network scenario (40 nodes distributed over 4000 x 4000 m 2). Index Terms—disruption tolerant networking, content-based routing, multi-attribute queries, information retrieval. I...|$|R
40|$|AbstractPreference query has {{received}} high interest {{due to its}} great benefits over various types of database applications. This type of query provides more flexible query operators that retrieve <b>data</b> <b>items</b> which are not dominated by the other <b>data</b> <b>items</b> in all <b>attributes</b> (dimensions). Many preference techniques for preference queries have been introduced including top-k, skyline, multi-objective skyline, top-k dominating, k-dominance, ranked skyline, and k-frequency. All of these preference techniques aimed at finding the “best” result that meets the user preferences. This paper aims at evaluating {{the performance of the}} five well-known preference evaluation techniques, namely: top-k, skyline, top-k dominating, k-dominance and k-frequency; in a real database application when high number of dimensions is the main concern. To achieve this, a recipe searching application with maximum number of 60 dimensions has been developed which assists users to identify the most desired recipes that fulfill their preferences. Several analyses have been carried out, where execution time is the main measurement used to evaluate each preference technique...|$|R
40|$|A {{database}} for receptors on cell membrane has been developed. The system can collect <b>data</b> <b>items</b> such as <b>attributes</b> of proteins from distributed data sources on the Internet. Such sources include internationally standard biological databases {{such as the}} updated genetic database of PIR, Swiss Prot, PDB, GenBank, EMBL and GDB. The system provides various viewing tools that e ectively displays di erent types of receptor data; DNA sequences, amino acids sequences, DNA binding sites, ligand binding sites, gene and disease information, and the protein structural information. It can also display three dimensional images using a freeware program RASMOL. DNA binding sites, ligand binding sites and active sites are classi ed by coloring the sequences. PDB matching sites are classi ed by italicization. CSNDB (Cell Signaling Networks Database), which is a {{database for}} cellular signal transduction of human is also linked in the system. The database may be useful for quick reference for ligand- membrane receptors and signal transduction in the drug design. ...|$|R
40|$|One of the {{important}} problems in data mining [SAD + 93] is the classification-rule learning. The classification-rule learning involves finding rules or decision trees that partition given data into predefined classes. For any realistic problem domain of the classification [...] rule learning, the set of possible decision trees is too large to be searched exhaustively. In fact, the computational complexity of finding an optimal classification decision tree is NP-hard. All of the existing algorithms, like C 4 : 5 [Qui 93], CDP [AIS 93] and SLIQ [MAR 96], use local heuristics to handle the computational complexity. The computational complexity of these algorithms ranges from O(AN logN) to O(AN(logN) 2) with N training <b>data</b> <b>items</b> and A <b>attributes.</b> These algorithms are fast enough for application domains where N is relatively small. However, in the data mining domain where millions of records {{and a large number}} of attributes are involved, the execution time of these algorithms can become [...] ...|$|R
40|$|ABSTRACT: Rating {{scales were}} {{developed}} to assess the biodata dimensions of-fered by Mael (1991). Biodata items assessing conscientiousness were adminis-tered under honest-responding and faking-good conditions. <b>Item</b> <b>attributes</b> were examined to determine their value in predicting item validity for honest respon-dents and item validity for faking respondents. Analyses were also conducted {{to determine whether the}} degree of item faking was related to <b>item</b> <b>attributes.</b> <b>Item</b> <b>attributes</b> associated with <b>item</b> validity for honest respondents {{are not the same as}} the <b>item</b> <b>attributes</b> indicative of <b>item</b> validity for the faking respondents. We suggest that this makes it very difficult to develop a biodata questionnaire which will be equally valid for both honest and faking respondents. KEY WORDS: biodata; item attributes; faking; validity. The topic of accuracy in self-report information has been an issu...|$|R
40|$|Abstract—Collaborative {{recommender}} is {{the most}} popular recommendation technique nowadays and it mainly employs the user <b>item</b> rating <b>data</b> set. Traditional collaborative filtering approaches compute a similarity value between the target user and each other user by computing the relativity of their ratings, and they only consider the ratings information. User attribute information associated with a user's personality and <b>item</b> <b>attribute</b> information associated with an item's inside are rarely considered in the collaborative filtering recommendation process. In this paper, a hybrid collaborative filtering recommender is proposed which employs the user attribute information and the <b>item</b> <b>attribute</b> information. This approach combines the user rating similarity and the user attribute similarity in the user based collaborative filtering process and then it combines the item rating similarity and the <b>item</b> <b>attribute</b> similarity in the item based collaborative filtering process to produce recommendations. The collaborative filtering recommender employs the user <b>attribute</b> and <b>item</b> <b>attribute</b> can alleviate the sparsity issue in the recommender systems. Index Terms—recommender system, collaborative filtering, rating similarity, user <b>attribute</b> similarity, <b>item</b> <b>attribute</b> similarity I...|$|R
40|$|A {{method of}} {{representing}} {{a group of}} <b>data</b> <b>items</b> comprises, for each of a plurality of <b>data</b> <b>items</b> in the group, determining the similarity between said <b>data</b> <b>item</b> and each of a plurality of other <b>data</b> <b>items</b> in the group, assigning a rank to each pair {{on the basis of}} similarity, wherein the ranked similarity values for each of said plurality of <b>data</b> <b>items</b> are associated to reflect the overall relative similarities of <b>data</b> <b>items</b> in the group...|$|R
50|$|In a list, {{the order}} of <b>data</b> <b>items</b> is significant. Duplicate <b>data</b> <b>items</b> are permitted. Examples of {{operations}} on lists are searching for a <b>data</b> <b>item</b> in the list and determining its location (if it is present), removing a <b>data</b> <b>item</b> from the list, adding a <b>data</b> <b>item</b> to the list at a specific location, etc. If the principal operations on the list are to be the addition of <b>data</b> <b>items</b> {{at one end and}} the removal of <b>data</b> <b>items</b> at the other, it will generally be called a queue or FIFO. If the principal operations are the addition and removal of <b>data</b> <b>items</b> at just one end, it will be called a stack or LIFO. In both cases, <b>data</b> <b>items</b> are maintained within the collection in the same order (unless they are removed and re-inserted somewhere else) and so these are special cases of the list collection. Other specialized operations on lists include sorting, where, again, {{the order of}} <b>data</b> <b>items</b> is of great importance.|$|R
50|$|In COBOL, union <b>data</b> <b>items</b> {{are defined}} in two ways. The first uses the RENAMES (66 level) keyword, which {{effectively}} maps a second alphanumeric <b>data</b> <b>item</b> {{on top of}} the same memory location as a preceding <b>data</b> <b>item.</b> In the example code below, <b>data</b> <b>item</b> PERSON-REC is defined as a group containing another group and a numeric <b>data</b> <b>item.</b> PERSON-DATA is defined as an alphanumeric <b>data</b> <b>item</b> that renames PERSON-REC, treating the data bytes continued within it as character data.|$|R
40|$|This study investigates item {{characteristics}} in psychological scale that susceptible to faking response. We examine the difference scores of Self Description form between two conditions. In the first condition we instruct subject {{to complete the}} form honestly (honest condition). On the other hand, in the second condition we instruct subject complete the form to make they look as good as possible (faking condition). Result suggest that in the <b>item</b> <b>attributes</b> that susceptible to faking, the score on the faking condition was higher than honest condition because subject were enable to improve their scores. We found that <b>item</b> <b>attributes</b> that verifiable, continuous, actual, controllable, second‐third sources, and future oriented were persistent to faking response. However <b>item</b> <b>attributes</b> that non‐verifiable, dichotomous, perceptual, first hand source and recent oriented, were vulnerable to faking response...|$|R
40|$|The paper {{proposes a}} family of item-response models that allow the {{separate}} and independent specification of three orthogonal components: <b>item</b> <b>attribute,</b> person covariate, and local item dependence. Special interest lies in extending the linear logistic test model, which is commonly used to measure <b>item</b> <b>attributes,</b> to tests with embedded item clusters. The problem of local item dependence arises in item clusters. Existing methods for handling such dependence, however, often fail to satisfy the property of invariant marginal interpretation of the <b>item</b> <b>attribute</b> parameters. While such a property may not be necessary for applications that focus on predictive analysis, it is critical for linear logistic test models. To achieve the marginal property, we implement an iterative estimation method, which is illustrated using data collected from an inventory on verbal aggressiveness...|$|R
25|$|Configuration {{change control}} {{is a set of}} {{processes}} and approval stages required to change a configuration <b>item's</b> <b>attributes</b> and to re-baseline them.|$|R
40|$|To improve data {{accessibility}} in ad hoc networks, {{in our previous}} work we proposed three methods of replicating <b>data</b> <b>items</b> by considering the data access frequencies from mobile nodes to each <b>data</b> <b>item</b> and the network topology. In this paper, we extend our previously proposed methods to consider the correlation among <b>data</b> <b>items.</b> Under these extended methods, the data priority of each <b>data</b> <b>item</b> is defined based on the correlation among <b>data</b> <b>items,</b> and <b>data</b> <b>items</b> are replicated at mobile nodes with the data priority. We employ simulations {{to show that the}} extended methods are more efficient than the original ones. ...|$|R
50|$|In a tree, {{which is}} {{a special kind of}} graph, a root <b>data</b> <b>item</b> has {{associated}} with it some number of <b>data</b> <b>items</b> which in turn have associated with them some number of other <b>data</b> <b>items</b> in what is frequently viewed as a parent-child relationship. Every <b>data</b> <b>item</b> (other than the root) has a single parent (the root has no parent) and some number of children, possibly zero. Examples of operations on trees are the addition of <b>data</b> <b>items</b> so as to maintain a specific property of the tree to perform sorting, etc. and traversals to visit <b>data</b> <b>items</b> in a specific sequence.|$|R
40|$|Abstract—Recommender {{systems are}} web based systems that aim at {{predicting}} a customer's interest on available {{products and services}} by relying on previously rated products {{and dealing with the}} problem of information and product overload. Collaborative filtering is the most popular recommendation technique nowadays and it mainly employs the user <b>item</b> rating <b>data</b> set. Traditional collaborative filtering approaches compute a similarity value between the target user and each other user by computing the relativity of their ratings, which is the set of ratings given on the same items. Based on the ratings of the most similar users, commonly referred to as neighbors, the algorithms compute recommendations for the target user. They only consider the ratings information. User attribute information associated with a user's personality and <b>item</b> <b>attribute</b> information associated with an item's inside are rarely considered in the collaborative filtering recommendation process. In this paper, a new collaborative filtering personalized recommendation algorithm is proposed which employs the user attribute information and the <b>item</b> <b>attribute</b> information. This approach combines the user rating similarity and the user attribute similarity in the user based collaborative filtering process to fill the vacant ratings where necessary, and then it combines the item rating similarity and the <b>item</b> <b>attribute</b> similarity in the item based collaborative filtering process to produce recommendations. The hybrid collaborative filtering employs the user <b>attribute</b> and <b>item</b> <b>attribute</b> can alleviate the sparsity issue in the recommender systems. Index Terms—personalized services, collaborative filtering, rating similarity, user <b>attribute</b> similarity, <b>item</b> <b>attribute</b> similarity, sparsity, mean absolute error I...|$|R
40|$|Abstract. 4 -ary vector {{expression}} {{was defined by}} 3 -ary vector to describe subject and <b>data</b> <b>item</b> in dataspace. Association method between subject and <b>data</b> <b>item</b> was represented. Correlation of <b>data</b> <b>item</b> was defined by 4 -ary vector. Correlation and association way between <b>data</b> <b>items</b> were represented by 4 -ary vector. The validity of these methods was verified in technical document library...|$|R
5000|$|The [...] "Point" [...] message defines two {{mandatory}} <b>data</b> <b>items,</b> x and y. The <b>data</b> <b>item</b> {{label is}} optional. Each <b>data</b> <b>item</b> has a tag. The tag is defined after the equal sign. For example, x has the tag 1.|$|R
50|$|In a multiset (or bag), {{like in a}} set, {{the order}} of <b>data</b> <b>items</b> does not matter, {{but in this case}} {{duplicate}} <b>data</b> <b>items</b> are permitted. Examples of operations on multisets are the addition and removal of <b>data</b> <b>items</b> and determining how many duplicates of a particular <b>data</b> <b>item</b> are present in the multiset. Multisets can be transformed into lists by the action of sorting.|$|R
30|$|We {{found that}} the {{proposed}} algorithm is presently in the broadcast structure. The wireless broadcast scheduling has been considered the <b>data</b> <b>item</b> frequency of the fixed and it has an unreasonable supposition. The <b>data</b> <b>item</b> frequency would be {{the request of the}} client for a change under the factual dynamic environments. Each of the <b>data</b> <b>item</b> has a frequency value itself and the each frequency of <b>data</b> <b>item</b> should been computed for its weight value and adjusted for dynamic broadcast adaptive so the frequency of <b>data</b> <b>item</b> has no fixed probability value.|$|R
50|$|In a set, {{the order}} of <b>data</b> <b>items</b> does not matter (or is undefined) but {{duplicate}} <b>data</b> <b>items</b> are not permitted. Examples of operations on sets are the addition and removal of <b>data</b> <b>items</b> and searching for a <b>data</b> <b>item</b> in the set. Some languages support sets directly. In others, sets can be implemented by a hash table with dummy values; only the keys are used in representing the set.|$|R
500|$|<b>Data</b> <b>items</b> in COBOL are {{declared}} hierarchically {{through the}} use of level-numbers which indicate if a <b>data</b> <b>item</b> is part of another. An item with a higher level-number is subordinate to an item with a lower one. Top-level <b>data</b> <b>items,</b> with a level-number of 1, are called [...] Items that have subordinate aggregate data are called those that do not are called [...] Level-numbers used to describe standard <b>data</b> <b>items</b> are between 1 and 49.|$|R
40|$|In {{this paper}} we {{consider}} data freshness and overload handling in embedded systems. The requirements on data management and overload handling {{are derived from}} an engine control software. <b>Data</b> <b>items</b> need to be up-to-date, and to achieve this data dependencies must be considered, i. e., updating a <b>data</b> <b>item</b> requires other <b>data</b> <b>items</b> are upto-date. We also note that a correct result of a calculation can in some cases be calculated using {{a subset of the}} inputs. Hence, data dependencies can be divided into required and not required <b>data</b> <b>items,</b> e. g., only a subset of <b>data</b> <b>items</b> affecting the fuel calculation in an engine control needs to be calculated during a transient overload {{in order to reduce the}} number of calculations. Required <b>data</b> <b>items</b> must always be up-to-date, whereas not required <b>data</b> <b>items</b> can be stale. We describe an algorithm that dynamically determines which <b>data</b> <b>items</b> need to be updated taking workload, data freshness, and data relationships into consideration. Performance results show that the algorithm suppresses transient overloads better than (m, k) - and skipover scheduling combined with established algorithms to update <b>data</b> <b>items.</b> The performance results are collected from an implementation of a real-time database on the realtime operating system µC/OS-II. To investigate whether the system is occasionally overloaded an offline analysis algorithm estimating period times of updates is presented. ...|$|R
50|$|A <b>data</b> <b>item</b> {{describes}} an atomic state {{of a particular}} object concerning a specific property {{at a certain time}} point. A collection of <b>data</b> <b>items</b> for the same object at the same time forms an object instance (or table row). Any type of complex information can be broken down to elementary <b>data</b> <b>items</b> (atomic state). <b>Data</b> <b>items</b> are identified by object (o), property (p) and time (t), while the value (v) is a function of o, p and t: v = F(o,p,t).|$|R
50|$|The {{simplest}} processors are scalar processors. Each instruction {{executed by}} a scalar processor typically manipulates {{one or two}} <b>data</b> <b>items</b> at a time. By contrast, each instruction executed by a vector processor operates simultaneously on many <b>data</b> <b>items.</b> An analogy {{is the difference between}} scalar and vector arithmetic. A superscalar processor is a mixture of the two. Each instruction processes one <b>data</b> <b>item,</b> but there are multiple execution units within each CPU thus multiple instructions can be processing separate <b>data</b> <b>items</b> concurrently.|$|R
3000|$|... {{age of a}} <b>data</b> <b>item,</b> {{calculated}} by taking {{the difference between the}} current time, t_curr, and the measurement time of that <b>data</b> <b>item</b> t(d); [...]...|$|R
50|$|For example, an {{abstract}} stack, {{which is a}} last-in-first-out structure, could be defined by three operations: push, that inserts a <b>data</b> <b>item</b> onto the stack; pop, that removes a <b>data</b> <b>item</b> from it; and peek or top, that accesses a <b>data</b> <b>item</b> {{on top of the}} stack without removal. An abstract queue, which is a first-in-first-out structure, would also have three operations: enqueue, that inserts a <b>data</b> <b>item</b> into the queue; dequeue, that removes the first <b>data</b> <b>item</b> from it; and front, that accesses and serves the first <b>data</b> <b>item</b> in the queue. There would be no way of differentiating these two data types, unless a mathematical constraint is introduced that for a stack specifies that each pop always returns the most recently pushed item that has not been popped yet. When analyzing the efficiency of algorithms that use stacks, one may also specify that all operations take the same time no matter how many <b>data</b> <b>items</b> have been pushed into the stack, and that the stack uses a constant amount of storage for each element.|$|R
40|$|Although data {{broadcast}} {{has been}} shown to be an efficient data dissemination technique for mobile computing systems, many issues such as selection of broadcast data and caching strategies at the clients are still active research areas. In this paper, by examining the dynamic properties of the <b>data</b> <b>items</b> in mobile computing systems, we define the validity of a <b>data</b> <b>item</b> by its absolute validity interval (avi). Based on the avi of the <b>data</b> <b>items,</b> we propose different broadcast algorithms in which the selection of <b>data</b> <b>items</b> for broadcast will be based on the avi of the <b>data</b> <b>items</b> and their access frequencies. The purpose of the AVI algorithms is to increase the client cache hit probability so that the access delay for a <b>data</b> <b>item</b> will be much reduced. Simulation experiments have been conducted to compare the AVI algorithms with the algorithm which only considers the popularity of the <b>data</b> <b>items.</b> The results indicate that the AVI algorithms can significantly improve the mean response time and reduce the deadline missing requests. ...|$|R
40|$|Technologies {{such as the}} World Wide Web have {{resulted}} in the access of information over a global network. Data can be text, images, video, or sound. Current network limitations and the large size of <b>data</b> <b>items</b> result in a high response time - the time taken for an image to be transmitted from a remote site to the user. In this paper we focus on reducing this response time for images. Traditionally in an information system exact copies of text <b>data</b> <b>items</b> had to be retrieved for the user but multimedia <b>data</b> <b>items</b> such as video and images can be represented by an equivalent <b>data</b> <b>item</b> which is an approximation of the original <b>data</b> <b>item.</b> Such an equivalent <b>data</b> <b>item</b> might satisfy an application equally well and also have a lower response time. Based on the assumption that such an approximation of a <b>data</b> <b>item</b> is sufficient for several applications, we have developed an architecture where an image equivalent to the original can be retrieved instead of the original image itself. Our architectu [...] ...|$|R
40|$|The {{rapid growth}} of data is inevitable, and {{retrieving}} the best results that meet the user’s preferences is essential. To achieve this, skylines were introduced in which <b>data</b> <b>items</b> that are not dominated by the other <b>data</b> <b>items</b> in the database are retrieved as results (skylines). In most of the existing skyline approaches, the databases {{are assumed to be}} static and complete. However, in real world scenario, databases are not complete especially in multidimensional databases in which some dimensions may have missing values. The databases might also be dynamic in which new <b>data</b> <b>items</b> are inserted while existing <b>data</b> <b>items</b> are deleted or updated. Blindly performing pairwise comparisons on the whole <b>data</b> <b>items</b> after the changes are made is inappropriate as not all <b>data</b> <b>items</b> need to be compared in identifying the skylines. Thus, a novel skyline algorithm, DInSkyline, is proposed in this study which finds the most relevant <b>data</b> <b>items</b> in dynamic and incomplete databases. Several experiments have been conducted and the results show that DInSkyline outperforms the previous works by reducing the number of pairwise comparisons in the range of 52...|$|R
50|$|DataBlitz also {{provides}} higher-layer interfaces for grouping related <b>data</b> <b>items,</b> and performing scans {{as well as}} associative access (via indices) on <b>data</b> <b>items</b> in a group...|$|R
30|$|An {{alternative}} {{data dissemination}} mechanism is the broadcast disks scheme, which permits <b>data</b> <b>items</b> to be broadcast with different frequencies [5]. This algorithm first divides <b>data</b> <b>items</b> {{into a few}} groups (i.e., disks) such that <b>data</b> <b>items</b> with similar popularity are assigned to the same disks. Afterwards, it determines the rotation speed of each disk according to the popularity of <b>data</b> <b>items.</b> In this way, one can construct a broadcast program that adjusts the trade-off between the access time of hot data and that of cold data.|$|R
40|$|Applications {{that make}} use of very large {{scientific}} datasets have become an increasingly important subset of scientific applications. In these applications, datasets are often multi-dimensional, i. e., <b>data</b> <b>items</b> are associated with points in a multi-dimensional attribute space, and access to <b>data</b> <b>items</b> is described by range queries. The basic processing involves mapping input <b>data</b> <b>items</b> to output <b>data</b> <b>items,</b> and some form of aggregation of all the input <b>data</b> <b>items</b> that project to the each output <b>data</b> <b>item.</b> We have developed an infrastructure, called the Active Data Repository (ADR), that integrates storage, retrieval and processing of multi-dimensional datasets on distributed-memory parallel architectures with multiple disks attached to each node. In this paper we address efficient execution of range queries on distributed memory parallel machines within ADR framework. We present three potential strategies, and evaluate them under different application scenarios and machine co [...] ...|$|R
