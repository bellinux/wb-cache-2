4|4|Public
40|$|The {{endogenous}} orienting {{of spatial}} {{attention has been}} studied with both informative central cues and <b>informative</b> <b>peripheral</b> cues. Central cues studies are difficult to compare with studies that have used uninformative peripheral cues due to the differences in stimulus presentation. Moreover, <b>informative</b> <b>peripheral</b> cues attract both endogenous and exogenous attention, thus {{making it difficult to}} disentangle the contribution of each process to any behavioural results observed. In the present study, we used an <b>informative</b> <b>peripheral</b> cue (either tactile or visual) that predicted that the target would appear (in different blocks of trials) on either the same or opposite side as the cue. By using this manipulation, both expected and unexpected trials could either be exogenously cued or uncued, thus making it possible to isolate expectancy effects from cuing effects. Our aim was to compare the endogenous orienting of spatial attention to tactile (Experiment 1) and to visual targets (Experiment 2) under conditions of intramodal and crossmodal spatial cuing. The results suggested that the endogenous orienting of spatial attention should not be considered as being a purely supramodal phenomenon, given that significantly larger expectancy effects were observed in the intramodal cuing conditions than in the crossmodal cuing conditions in both experiments...|$|E
40|$|Objective: This study {{examined}} the effectiveness of using <b>informative</b> <b>peripheral</b> visual and tactile cues to support task switching and interruption management. Background: Effective support for the allocation of limited attentional resources is needed for operators who must cope with numerous competing task demands and frequent interruptions in data-rich, event-driven domains. One prerequisite for meeting this need is to provide information {{that allows them to}} make informed decisions about, and before, (re) orienting their attentional focus. Method: Thirty participants performed a continuous visual task. Occasionally, they were presented with a peripheral visual or tactile cue that indicated the need to attend to a separate visual task. The location, frequency, and duration parameters of these cues represented the domain, importance, and expected completion time, respectively, of the interrupting task. Results: The findings show that the informative cues were detected and interpreted reliably. Information about the importance (rather than duration) of the task was used by participants to decide whether to switch attention to the interruption, indicating adherence to experimenter instructions. Erroneous task-switching behavior (nonadherence to experimenter instructions) was mostly caused by misinterpretation of cues. Conclusion: The effectiveness of <b>informative</b> <b>peripheral</b> visual and tactile cues for supporting interruption management was validated in this study. However, the specific implementation of these cues requires further work and needs to be tailored to specific domain requirements. Application: The findings from this research can inform the design of more effective notification systems for a variety of complex event-driven domains, such as aviation, medicine, or process control...|$|E
40|$|In {{the present}} study, we used event-related {{potentials}} (ERPs) and behavioral measurements in a peripherally cued line-orientation discrimination task {{to investigate the}} underlying mechanisms of orienting and focusing in voluntary and involuntary attention conditions. <b>Informative</b> <b>peripheral</b> cue (75 % valid) with long stimulus onset asynchrony (SOA) {{was used in the}} voluntary attention condition; uninformative peripheral cue (50 % valid) with short SOA was used in the involuntary attention condition. Both orienting and focusing were affected by attention type. Results for attention orienting in the voluntary attention condition confirmed the "sensory gain control theory," as attention enhanced the amplitude of the early ERP components, P 1 and N 1, without latency changes. In the involuntary attention condition, compared with invalid trials, targets in the valid trials elicited larger and later contralateral P 1 components, and smaller and later contralateral N 1 components. Furthermore, but only in the voluntary attention condition, targets in the valid trials elicited larger N 2 and P 3 components than in the invalid trials. Attention focusing in the involuntary attention condition resulted in larger P 1 components elicited by targets in small-cue trials compared to large-cue trials, whereas in the voluntary attention condition, larger P 1 components were elicited by targets in large-cue trials than in small-cue trials. There was no interaction between orienting and focusing. These results suggest that orienting and focusing of visual-spatial attention are deployed independently regardless of attention type. In addition, the present results provide evidence of dissociation between voluntary and involuntary attention during the same task...|$|E
40|$|This paper {{builds on}} {{two pieces of}} {{previous}} work. One presents a rich set of design preferences and requirements {{based on interviews with}} participants who were deaf, culminating in a qualitative study of two displays exploring functional requirements such as sound recognition and history (Matthews et al. 2005). The other compares two other displays quantitatively along the dimensions of distraction and awareness, looking at the importance of different sound characteristics (Ho-Ching et al. 2003). This paper provides additional details about these past studies, and builds on them by presenting a set of design guidelines derived from both projects, shedding light on how to create a more <b>informative</b> and reliable <b>peripheral</b> display of sound that incorporates both computer and end-user interpretation of sound...|$|R
40|$|The aim of {{this work}} is the {{creation}} of a completely automatic method for the extraction of <b>informative</b> parameters from <b>peripheral</b> signals recorded through a sensorized T-shirt. The acquired data belong to patients affected from bipolar disorder, and consist of RR series, body movements and activity type. The extracted features, i. e. linear and non-linear HRV parameters in the time domain, HRV parameters in the frequency domain, and parameters indicative of the sleep quality, profile and fragmentation, are of interest for the automatic classification of the clinical mood state. The analysis of this dataset, which is to be performed online and automatically, must address the problems related to the clinical protocol, which also includes a segment of recording in which the patient is awake, and {{to the nature of the}} device, which can be sensitive to movements and misplacement. Thus, the decision tree implemented in this study performs the detection and isolation of the sleep period, the elimination of corrupted recording segments and the checking of the minimum requirements of the signals for every parameter to be calculated...|$|R
40|$|In {{the present}} series of experiments, <b>peripheral</b> <b>informative</b> cues {{were used in}} order to dissociate {{endogenous}} and exogenous orienting of spatial attention using {{the same set of}} stimuli. For each block of trials, the cue predicted either the same or the opposite location of target appearance. Crucially, using this manipulation, both expected and unexpected locations could be either cued or uncued. If one accepts the hypothesis that inhibition of return (IOR) is an attentional effect that inhibits the returning of attention to a previously attended location (Posner & Cohen, 1984), one would not predict an IOR effect at the expected location, since attention should not disengage from the location predicted by the cue. Detection and discrimination tasks were used to examine any potential difference in the mechanism responsible for IOR {{as a function of the}} task at hand. Two major results emerged: First, IOR was consistently observed at the expected location, where, according to the traditional "reorienting" hypothesis, IOR is not supposed to occur. Second, a different time course of cueing effects was found in detection versus discrimination tasks, even after controlling for the orienting of attention. We conclude that IOR cannot be accounted for solely by the "reorienting of attention" hypothesis. Moreover, we argue that the observed time course differences in cueing effects between detection and discrimination tasks cannot be explained by attention disengaging from cues later in discrimination than in detection tasks, as proposed by Klein (2000). The described endogenous-exogenous dissociation is consistent with models postulating that endogenous and exogenous attentional processes rely on different neural mechanisms...|$|R
40|$|Objectives. Here {{we aim at}} {{testing the}} {{activity}} and expression levels of a restricted number of molecules in peripheral blood cells from patients with Huntington Disease (HD). These molecules were chosen because they are affected in cells or mice expressing mutant huntingtin. Other hereditary neurological diseases both associated and non- associated to polyglutamine (Spinocerebellar Ataxias, SCA, and Friedreich Ataxia) will be considered to evaluate whether similar peripheral alterations are present. Background/rationale. HD {{is part of a}} group of hereditary late-onset neurodegenerative diseases that share a similar genetic mutation, characterized by an expansion of glutamine tract in specific proteins. Although genetic test can easily assess the presence of the mutation, no other tests are currently available to monitor disease onset and progression. The identification of reliable peripheral indicators of disease would be especially important in clinical therapeutic trials. Recently, members of this network have shown that alterations in A 2 A receptor profile can be detected both in experimental HD models and also in lymphocytes from HD patients. Description of the project. On these bases, we plan to establish whether the observed adenosine A 2 A receptor alterations are also present in other polyglutamine disorders, such as SCAs and to verify if the changes correlate with disease progression and clinical severity: In addition, we will study a group of other biological indicators, including endocannabinoid receptors (CB 1 and non-CB 1), brain-derived-neurotrophic factor (BDNF) mRNA and protein levels, mRNA levels of cholesterol enzyme biosynthetic pathway, in blood of HD and SCA patients. Anticipated output. To make available <b>informative</b> <b>peripheral</b> biomarkers that may implement clinical monitoring of disease progression in future clinical trials evaluating the efficacy of potential therapeutic interventions...|$|E
40|$|AbstractApproaches to the {{measurement}} of lymphohematopoietic chimerism have evolved from laboratory research to important clinical tools. However, {{there has been no}} logical, consistent, and uniform set of recommendations for {{the measurement}} of chimerism in clinical transplantation. The National Marrow Donor Program and the International Bone Marrow Transplant Registry (IBMTR) sponsored a workshop to discuss the use of chimerism analysis after allogeneic transplantation. The workshop was organized {{in an effort to make}} reasonable recommendations regarding laboratory techniques, the types of specimens to be studied, and the frequency of analysis. The panel recommended the following guidelines: 1. Chimerism analysis should use sensitive, informative techniques. At present, short tandem repeats (STR) or variable number tandem repeats (VNTR) analysis is the approach most likely to give reproducible <b>informative</b> data. 2. <b>Peripheral</b> blood cells are generally more useful than bone marrow cells for chimerism analysis. 3. Lineage-specific chimerism should be considered the assay of choice in the setting of nonmyeloablative and reduced-intensity conditioning. 4. The use of T-cell depletion, nonmyeloablative or reduced-intensity conditioning, or novel graft-versus-host disease (GVHD) prophylactic regimens warrants chimerism analysis at 1, 3, 6, and 12 months, because interventions such as donor lymphocyte infusions may depend on chimerism status. 5. In nonmyeloablative transplantation, the early patterns of chimerism may predict either GVHD or graft loss. Therefore, more frequent (every 2 - 4 weeks) peripheral blood analysis may be warranted. 6. For nonmalignant disorders, chimerism generally should be measured 1, 2, and 3 months after transplantation. Interventions to enhance donor engraftment must be considered on a disease-specific basis in relation to concurrent GVHD and, ultimately, clinical rationale. Biol Blood Marrow Transplant 2001; 7 (9) : 473 - 85...|$|R

