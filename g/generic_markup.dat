14|6|Public
50|$|Some {{have said}} that the Star was ahead of its time, that few outside of a small circle of {{developers}} really understood the potential of the system, considering that IBM introduced their 8088-based IBM PC running the comparatively primitive PC DOS the same year that the Star was brought to market. However, comparison with the IBM PC may be irrelevant: well before it was launched, buyers in the Word Processing industry were aware of the 8086-based IBM Displaywriter, the full-page portrait black-on-white Xerox 860 page display system and the 120 page-per-minute Xerox 9700 laser printer. Furthermore, the design principles of Smalltalk and modeless working had been extensively discussed in the August 1981 issue of Byte magazine, so Xerox PARC's standing and the potential of the Star can scarcely have been lost on its target (office systems) market, who would never have expected IBM to position a mass-market PC to threaten far more profitable dedicated WP systems. Unfortunately, the influential niche market of pioneering players in electronic publishing such as Longman were already aligning their production processes towards <b>generic</b> <b>markup</b> languages such as SGML (forerunner of HTML and XML) whereby authors using inexpensive offline systems could describe document structure, making their manuscripts ready for transfer to computer to film systems that offered far higher resolution than the then maximum of 360 dpi laser printing technologies.|$|E
40|$|This paper {{consists}} of three parts. In a first part, {{it is argued that}} <b>generic</b> <b>markup</b> languages such as SGML and XML, that have been developed primarily for modeling documents are also convenient for modeling general data. In a second part, XML and concepts related to XML are introduced. A third part presents a logic programming perspective to data modeling with <b>generic</b> <b>markup</b> languages...|$|E
40|$|Die Arbeit mit Texten im Computer hebt sich dann von der elektronischen Schreibmaschine ab und beginnt zur Datenverarbeitung zu werden, wenn im Mittelpunkt des Interesses nicht mehr der Ausdruck von Papier steht, sondern auch der Zugriff auf die Textdaten für Fragen der Analyse, Statistik und inhaltlichen Auswertung berücksichtigt wird. Der vorliegende Beitrag beabsichtigt eine praktische Einführung in die Idee und Anwendung der sachlich orientierten Textauszeichnung (<b>generic</b> <b>markup),</b> die die Grundlage für eine Textdatenverarbeitung bietet. Ausführlich wird das Programmpaket TUSTEP als Werkzeug für den Umgang mit dieser Art von Textdaten für Druckausgabe und Weiterverarbeitung dargestellt. " (Autorenreferat) "Working with {{computer}} becomes real data processing in opposite to simply {{working with a}} typewriter, when the user does not face only the printout but faces the data to be processed for several purposes: quantitative analysis, grammatical analysis, statistical approaches, retrieval for information. The success of this purposes needs not only powerful tools but a good method of data management. The paper gives a practical introduction into the generic text markup. Generic text markup means here markup {{with regard to the}} content of a text and not only to the typography in opposite to the specific markup for specific formatters. In a first part the paper describes the idea of <b>generic</b> <b>markup.</b> It stresses the advantages of <b>generic</b> <b>markup</b> in the daily work. The second part shows the four fundamental steps in working with <b>generic</b> <b>markup.</b> This steps are explained with regard to SGML. The third part gives examples. They present TUSTEP as a tool for working with SGML-like tags. TUSTEP-programs are demonstrated for parsing texts with <b>generic</b> <b>markup.</b> These examples include not only different layout programs, but also analytical work with tagged texts. " (author's abstract...|$|E
40|$|Abstract. In {{the same}} way as the “static ” Semantic Web deals with data model and {{language}} heterogeneity and semantics that lead to RDF and OWL, there is language heterogeneity and the need for a semantical account concerning Web dynamics. Thus, <b>generic</b> rule <b>markup</b> has to bridge these discrepancies, i. e., allow for composition of component languages, retaining their distinguished semantics and making them accessible e. g. for reasoning about rules. In this paper we analyze the basic concepts for a general language for evolution and reactivity in the Semantic Web. We propose an ontology based on the paradigm of Event-Condition-Action (ECA) rules including an XML markup. In this framework, different languages for events (including languages for composite events), conditions (queries and tests) and actions (including complex actions) can be composed to define highlevel rules for describing behavior in the Semantic Web. ...|$|R
40|$|Abstract There is a {{need for}} tools that {{facilitate}} the systematic exploration of novel theoretical neural network models. Existing neural network simulation environments, neural network specification languages, and genetic encoding of neural networks fall short of providing the tools needed for this task. We suggest that a useful approach to the design of such tools may be the use of a grammar to capture neural design principles as well as structural and behavioral elements, and mechanisms to automatically translate the parse trees of the grammar into complete neural network specifications in a generic format. We present the Attribute Grammar Encoding (AGE) as a specific example of our approach that uses attribute grammars to create descriptions of neural network solutions in an XML-based format termed the <b>Generic</b> Neural <b>Markup</b> Language (GNML). Lessons learned from the development of this system are presented to identify and address the issues of a broader application of this approach to other specification formats and other grammar encoding approaches. I...|$|R
40|$|Applications {{that use}} {{directory}} services or relational databases {{operate in a}} client–server model, where a client requests information from a server, and the server returns {{a response to the}} client. These client–server applications typically have a specific message protocol that is unique to that application. Systems with multiple client–server applications require that there are separate client programs that individually communicate with their respective server programs. A need exists to access information from heterogeneous systems in a standard message request–response format. A <b>generic</b> eXtensible <b>Markup</b> Language (XML) model was developed to obtain data from diverse measurement systems. The objective {{of this paper is to}} describe the XML model that abstracts the differences in the underlying heterogeneous client–server message formats and provides a common XML message interface. The XML messages are parsed through a common XML gateway that decides to which application server to forward the messages. The generic XML messages are translated to the correct application server format before being sent to the application server...|$|R
40|$|Several {{documentation}} systems incorporate as a text formatter, but use a <b>generic</b> <b>markup</b> {{language as}} a front-end. This means rn is hidden from the users, and usually has been modified or extended. This paper discusses {{the advantages and disadvantages}} of such systems, as regards international standards, macro packages, and 'I$ $ itself. 1...|$|E
40|$|In this paper, we {{introduce}} the new concept of interactive automata by proposing that {{states are not}} virtual elements but real objects containing useful information. They are used as models for structured data and system architecture, and then implemented in two developing environments, the XML environment and the client-server environment. These implementations produce, respectively, a <b>generic</b> <b>markup</b> language called Automata Markup Language (AML) and a client-server system, which together realize a homogeneous and uniform platform. An example is given in the E-learning domain where AML is a questionnaire description language and the automata client-server system {{plays the role of}} an assessment platform...|$|E
40|$|AbstractSysML is a standardized, <b>generic</b> <b>markup</b> {{language}} for modeling technical systems. The authors’ work considers evaluating SysML in research projects {{in cooperation with}} industrial partners. In this paper, system of interest is a high voltage battery system for automotive applications. Observations in this project underlined the need for multi-view modeling. The central thesis {{of this paper is}} that multiple, thematic cross-cutting views of a system will need optimized thematic structures of system architecture. They introduce a human-centric approach: Enabling thematic views but committing it to the responsibility and capability of the engineer to apply them as needed. As a conclusion, optimized thematic structures provide an optimized thematic view. One holistic systems model is created which represents both the system as a whole and the different views of stakeholders in an integrated way, but tool implementation has to be taken into account...|$|E
30|$|GSDML (<b>Generic</b> Station Description <b>Markup</b> Language) and FDCML (Field Device Markup Language) [5] on the {{contrary}} are device description languages based on XML. GSDML is primarily established in the Profinet I/O domain and is again used for the configuration and commissioning of systems by engineering tools. FDCML {{on the other hand}} is a metalanguage for describing automation devices from different views, such as communication, functionality, diagnosis and mechanics. Its primary usage is to provide (human-readable) product data sheets and to enable a tool-based commissioning. Its application mainly focuses on INTERBUS components. FDCML is flexible for extensions such as manufacturer-specific attributes, but which on {{the contrary}} inhibits the comparability of devices due to a nonuniform vocabulary.|$|R
40|$|International audienceIn {{this paper}} we {{describe}} a markup language for semantically annotating raw texts. We define a formal representation of text documents written in natural language {{that can be}} applied for the task of Named Entities Recognition and Spatial Role Labeling. The proposal relies on a multi-layer annotation process based on a core generic layer, which can be freely adapted into more specific layers depending on the intended goal. Our markup language is based on the TEI Guidelines 1 to propose a <b>generic</b> and extensible <b>markup</b> language. This language is particularly dedicated for the text mining task and ready to use to be layered with more semantic relationships between elements of the text. We show the feasibility of this proposal from a generic annotation of texts describing itineraries toward a geospatial semantic annotation...|$|R
40|$|Despite some {{development}} {{in the area of}} DL architectures and systems, there is still little support for the complete life cycle of DL development, including requirements gathering, conceptual modeling, rapid prototyping, and code generation and reuse. Even when partially supported, those activities are uncorrelated within the current systems, which can lead to inconsistencies and incompleteness. Moreover, the current few existing approaches are not supported by comprehensive and formal foundations and theories, which brings problems of interoperability and makes it extremely difficult to adapt and tailor systems to specific societal preferences and needs of the target community. In this paper, having the 5 S formal theoretical framework as support, we present an architecture and a family of tools that allow rapid modeling, prototyping, and generation of digital libraries. 5 S stands for Streams, Structures, Spaces, Scenarios, and Societies and is our formal theory for DLs. 5 SL is a domain-specific, declarative language for DL conceptual modeling. 5 SGraph is a visual modeling tool that helps designers to model a digital library without knowing the theoretical foundations and the syntactical details of 5 SL. Furthermore, 5 SGraph maintains semantic constraints specified by a 5 S metamodel and enforces these constraints over the instance model to ensure semantic consistency and correctness. 5 SGraph also enables component reuse to reduce the time and efforts of designers. 5 SLGen is a DL generation tool that takes specifications in 5 SL and a set of component pools and generates portions of a running DL system. The outputs of 5 SLGen include user interface prototypes, in a <b>generic</b> UI <b>markup</b> language, for validation of services behavior and workflow representations of the running system, generated to support the desired scenarios...|$|R
40|$|Both L A T E X and HTML are {{languages}} {{that can}} express {{the structure of}} a document, and similarities between these two systems are shown. A detailed study is made of the LaTeX 2 HTML program, written by Nikos Drakos, that is today the most complete utility for translating L A T E X code into HTML, providing a quasi-automatic translation for most elements. A discussion of a few other tools for translating between HTML and L A T E X concludes the article. 1 Similarities between L A T E X and HTML HTML and L A T E X are both <b>generic</b> <b>markup</b> systems, and a comparison between tags for structural elements in both cases is shown in Table 1. In most cases the differences are trivial, seeming to indicate that, at first approximation, translating between these two systems should not prove too difficult. The translation programs described in this article use these similarities, but in order to exploit the richness of the L A T E X language as compared to HTML (especially HTML 2, whic [...] ...|$|E
40|$|The {{editors of}} the journal {{describe}} why they and the publisher decided to start the journal, and what they hope it will accomplish. When embarking on such an uncertain project as {{the creation of a}} new journal in what is not exactly a recognized academic field — a print journal, no less, for the discussion of systems for electronic text markup — it is difficult to avoid the temptation to explain the rationale and goals of the journal. We have neither avoided nor attempted to resist that temptation, and this essay is the result of our surrender to it. Why a new journal, on this subject matter, now? In the past few years, the steady trickle of work on the theory and practice of marking up texts for machine processing has become a stream, and then a river, and now looks likely to take on torrential proportions. <b>Generic</b> <b>markup,</b> especially in the form of HTML, has become ubiquitous — or, at least, about as widespread as the Internet itself: though not quite truly ubiquitous even in the most highly developed countries, the Net and HTML are much more widesprea...|$|E
40|$|The Extensible Markup Language (XML) is {{extremely}} popular as a <b>generic</b> <b>markup</b> language for text documents with an explicit hierarchical structure. The {{different types of}} XML data found in today’s document repositories, digital libraries, intranets and on the web range from flat text with little meaningful structure to be queried, over truly semistructured data with a rich and often irregular structure, to rather rigidly structured documents with little text that would also fit a relational database system (RDBS). Not surprisingly, various ways of storing and retrieving XML data have been investigated, including native XML systems, relational engines based on RDBSs, and hybrid combinations thereof. Over the years a number of native XML indexing techniques have emerged, the most important ones being structure indices and labelling schemes. Structure indices represent the document schema (i. e., the hierarchy of nested tags {{that occur in the}} documents) in a compact central data structure so that structural query constraints (e. g., path or tree patterns) can be efficiently matched without accessing the documents. Labelling schemes specify ways to assign unique identifiers, or labels, to the document nodes so that specific relations (e. g., parent/child...|$|E
40|$|The {{concepts}} of hypertext and hypermedia {{have created a}} flurry of activity {{both inside and outside}} the world of computer science but much of the work undertaken so far has been concerned with creating hypertext interfaces that allow an author to create documents in a hypertext format, incorporating buttons and links in the ways permitted by the particular hypertext system. This work is of fundamental importance to our understanding of hypertext systems and the nature of the structures they allow us to create. It is also vital, however, that hypertext systems provide interfaces to existing text-processing systems so that documents created in this way can easily and perhaps automatically be incorporated into a hypertext format. At the same time, this would provide a hypertext interface for viewing, accessing and cross-referencing such documents. This paper describes a project being undertaken in Southampton to provide a hypertext interface for texts written using a <b>generic</b> <b>markup</b> language, such as LaTeX. The author creates an ordinary LaTeX document using normal facilities for cross-referencing, indexing etc., and this generic information is used to generate tags for a hypertext interface, which allows the document to be viewed section by section and provides buttons for accessing tables, diagrams, figures, references and cross-references to other documents...|$|E
40|$|Both LAT E X and HTML are {{languages}} {{that can}} express {{the structure of}} a document, and similarities between these two systems are shown. A detailed study is made of the La-TeX 2 HTML program, written by Nikos Drakos, that is today the most complete utility for translating LAT E X code into HTML, providing a quasi-automatic translation for most elements. A discussion of a few other tools for translating between HTML and LATEX concludes the article. 1 Similarities between LATEX and HTML HTML and LATEX are both <b>generic</b> <b>markup</b> systems, and a comparison between tags for structural elements in both cases is shown in Table 1. In most cases the di erences are trivial, seeming to indicate that, at rst approximation, translating between these two systems should not prove too di cult. The translation programs described in this article use these similarities, but in order to exploit the richness of the LAT EX language as compared to HTML (especially HTML 2, which has no support for tables or mathematics), an ad hoc approach has to be adopted. To handle correctly LATEX commands that have no equivalent in HTML, such elements can either be transformed into bitmap or PostScript pictures (an approach taken by LaTeX 2 HTML), or the user can specify how the given element should be handled in the target language...|$|E
40|$|This International Standard is {{designed}} to specify the processing of valid SGML documents. DSSSL defines the semantics, syntax, and processing model of two languages for the specification of document processing: a) The transformation language for transforming SGML documents marked up in accordance {{with one or more}} DTDs into other SGML documents marked up in accordance with other DTDs. The specification of this transformation process is fully defined by this International Standard. b) The style language, where the result is achieved by applying a set of formatting characteristics to portions of the data, and the specification is, therefore, as precise as the application requires, leaving some formatting decisions, such as line-end and column-end decisions, to the composition and layout process. The DSSSL style language is intended to be used {{in a wide variety of}} environments with typographic requirements ranging from simple single-column layouts to complex multiplecolumn layouts. This International Standard does not standardize a formatter nor does it standardize composition or other processing algorithms. Rather, it provides the means whereby an implementation may externalize ‘style characteristics ’ and other techniques for associating style information with an SGML document. DSSSL provides a mechanism for specifying the use of ‘external processes ’ to manipulate data. The nature of these processes is outside the scope of DSSSL, but may include typical data management functions, such as sorting and indexing; typical composition functions, such as hyphenation algorithms; and graphics or multimedia processes for non-SGML data. Documents that have already been formatted or do not contain any hierarchical structural information or <b>generic</b> <b>markup</b> are not within the field of application of this Internationa...|$|E
40|$|The Standard Generalized Markup Language (SGML) {{has been}} the International Organization of Standardization (ISO) {{published}} standard for text interchange for nearly a decade. Since 1986, SGML based publishing has been successfully implemented in many fields, notably those industries with massive and mission-critical publishing operations such as the military, legal, medical, and heavy industries. SGML based publishing differs from the WYSIWYG paradigm of desktop publishing in that an SGML document contains descriptive, structural markup rather than specific formatting markup. Specific markup describes {{the appearance of a}} document and is usually a proprietary code which makes the document difficult to re-use or interchange to different systems. The structurally <b>generic</b> <b>markup</b> codes in an SGML document allow the fullest exploitation of the information. An SGML document exhibits more re-usability than a document created and stored in a proprietary formatting code. In many cases, workflow and production are greatly improved by the implementation of SGML based publishing. Historical and anecdotal case studies of many applications clearly delineate the benefits of an SGML based publishing system. And certainly, the boom in Web publishing has spurred interest in enabling a publishing system with multi-output functionality. However, implementation is associated with high costs. The acquisition of new tools and new skills is a costly investment. A careful cost-benefit analysis must determine that the current publishing needs would be satisfied by moving to SGML. Increased productivity is the measure by which SGML is adopted. The purpose of this thesis project is to investigate the relative benefits and requirements of a simple SGML based publishing implementation. The graduate thesis for most of the School of Printing Management and Sciences at the Rochester Institute of Technology was used as an example. The author has expanded the requirements for the publication process of a graduate thesis with factors which do not exist in reality. The required output has been expanded from mere print output to include publishing on the World Wide Web (WWW) in the Hypertext Markup Language (HTML), and to some proprietary electronic browser such as Folio Views for inclusion in a searchable collection of graduate theses on CD-ROM. A proposed set of tools and methods are discussed in order to clarify the requirements of such an SGML implementation...|$|E
40|$|The Extensible Markup Language (XML) is {{extremely}} popular as a <b>generic</b> <b>markup</b> language for text documents with an explicit hierarchical structure. The {{different types of}} XML data found in today’s document repositories, digital libraries, intranets and on the web range from flat text with little meaningful structure to be queried, over truly semistructured data with a rich and often irregular structure, to rather rigidly structured documents with little text that would also fit a relational database system (RDBS). Not surprisingly, various ways of storing and retrieving XML data have been investigated, including native XML systems, relational engines based on RDBSs, and hybrid combinations thereof. Over the years a number of native XML indexing techniques have emerged, the most important ones being structure indices and labelling schemes. Structure indices represent the document schema (i. e., the hierarchy of nested tags {{that occur in the}} documents) in a compact central data structure so that structural query constraints (e. g., path or tree patterns) can be efficiently matched without accessing the documents. Labelling schemes specify ways to assign unique identifiers, or labels, to the document nodes so that specific relations (e. g., parent/child) between individual nodes can be inferred from their labels alone in a decentralized manner, again without accessing the documents themselves. Since both structure indices and labelling schemes provide compact approximate views on the document structure, we collectively refer to them as structural summaries. This work presents new structural summaries that enable highly efficient and scalable XML retrieval in native, relational and hybrid systems. The key contribution of our approach is threefold. (1) We introduce BIRD, a very efficient and expressive labelling scheme for XML, and the CADG, a combined text and structure index, and combine them as two complementary building blocks of the same XML retrieval system. (2) We propose a purely relational variant of BIRD and the CADG, called RCADG, that {{is extremely}} fast and scales up to large document collections. (3) We present the RCADG Cache, a hybrid system that enhances the RCADG with incremental query evaluation based on cached results of earlier queries. The RCADG Cache exploits schema information in the RCADG to detect cached query results that can supply some or all matches to a new query with little or no computational and I/O effort. A main-memory cache index ensures that reusable query results are quickly retrieved even in a huge cache. Our work shows that structural summaries significantly improve the efficiency and scalability of XML retrieval systems in several ways. Former relational approaches have largely ignored structural summaries. The RCADG shows that these native indexing techniques are equally effective for XML retrieval in RDBSs. BIRD, unlike some other labelling schemes, achieves high retrieval performance with a fairly modest storage overhead. To the best of our knowledge, the RCADG Cache is the only approach to take advantage of structural summaries for effectively detecting query containment or overlap. Moreover, no other XML cache we know of exploits intermediate results that are produced as a by-product during the evaluation from scratch. These are valuable cache contents that increase the effectiveness of the cache at no extra computational cost. Extensive experiments quantify the practical benefit of all of the proposed techniques, which amounts to a performance gain of several orders of magnitude compared to various other approaches...|$|E

