133|61|Public
25|$|<b>Entropy</b> <b>encoding</b> – a {{coding scheme}} that assigns codes to symbols {{so as to}} match code lengths with the probabilities of the symbols.|$|E
25|$|Markov {{chains are}} used {{throughout}} information processing. Claude Shannon's famous 1948 paper A Mathematical Theory of Communication, {{which in a}} single step created the field of information theory, opens by introducing the concept of entropy through Markov modeling of the English language. Such idealized models can capture many of the statistical regularities of systems. Even without describing the full structure of the system perfectly, such signal models can make possible very effective data compression through <b>entropy</b> <b>encoding</b> techniques such as arithmetic coding. They also allow effective state estimation and pattern recognition. Markov chains also {{play an important role}} in reinforcement learning.|$|E
5000|$|FLIF (Free Lossless Image Format) - a {{work-in-progress}} lossless {{image format}} which claims to outperform PNG, lossless WebP, lossless BPG and lossless JPEG2000 {{in terms of}} compression ratio. It uses the MANIAC (Meta-Adaptive Near-zero Integer Arithmetic Coding) <b>entropy</b> <b>encoding</b> algorithm, {{a variant of the}} CABAC (context-adaptive binary arithmetic coding) <b>entropy</b> <b>encoding</b> algogithm.|$|E
5000|$|... #Caption: The <b>entropy</b> <b>encode</b> {{from the}} VCE ASIC can be {{utilized}} {{with the help}} of AMD APP SDK.|$|R
30|$|The {{quantized}} {{elements are}} <b>entropy</b> <b>encoded</b> either independently, conditionally, or jointly. In either case, {{it is done}} in practice by look-up tables and is therefore of low complexity, i.e., O(N).|$|R
3000|$|DCT transformed; the low-frequency DCT {{coefficients}} {{are then}} properly quantized, and parity bits are calculated {{on the two}} least significant bits of the quantization indices. A cyclic redundancy check (CRC) is also computed for each block of the low-frequency coefficients and transmitted separately. Higher-frequency DCT coefficients are <b>entropy</b> <b>encoded</b> with a classic run-amplitude code, and, possibly, some uncorrelated refinement bits for the low-frequency coefficients {{are added to the}} stream to achieve higher reconstruction quality.|$|R
5000|$|Arithmetic coding—a form of variable-length <b>entropy</b> <b>encoding</b> for {{efficient}} {{data compression}} ...|$|E
5000|$|... #Caption: The <b>entropy</b> <b>encoding</b> {{block of}} the VCE ASIC is also {{separately}} accessible, enabling [...] "hybrid mode". In [...] "hybrid mode" [...] {{most of the}} computation {{is done by the}} 3D engine of the GPU. Using AMD's Accelerated Parallel Programming SDK and OpenCL developers can create hybrid encoders that pair custom motion estimation, inverse discrete cosine transform and motion compensation with the hardware <b>entropy</b> <b>encoding</b> to achieve faster than real-time encoding.|$|E
5000|$|Golomb coding - <b>entropy</b> <b>encoding</b> {{invented by}} Prof. Solomon W. Golomb that is optimal for alphabets {{following}} geometric distributions ...|$|E
40|$|We {{present an}} {{approach}} based on Support Vector Machines (SVM) and quad-tree decomposition for compressing still images. Unlike JPEG,themethod applies the discrete cosine transform (DCT) to regions of variable size, re-scale {{them to a}} unique block size, and before the coefficients are <b>entropy</b> <b>encoded,</b> they are approximated by a SVM model. The method was applied to grey scale and colour images and the obtained results improved JPEG's performance at medium and low bit rates...|$|R
50|$|First a wavelet {{transform}} is applied. This produces as many coefficients {{as there are}} pixels in the image (i.e., there is no compression yet since {{it is only a}} transform). These coefficients can then be compressed more easily because the information is statistically concentrated in just a few coefficients. This principle is called transform coding. After that, the coefficients are quantized and the quantized values are <b>entropy</b> <b>encoded</b> and/or run length encoded.|$|R
40|$|Tech ReportTri-state delta {{modulation}} (TSDM) is introduced {{as an extension}} of two-state adaptive {{delta modulation}}. Its advantages for picture transmission are discussed as well as its implementaiton. Statistics from computer simulations of the TSDM on slow scan video signals show that the source messages (+ 1, 0, - 1) can be <b>entropy</b> <b>encoded</b> for bandwidth reduction. In particular run-length coding of 0 's gives a rate of less than 1 bit/pel in some cases. NAS...|$|R
50|$|Rice coding {{is used as}} the <b>entropy</b> <b>encoding</b> {{stage in}} a number of {{lossless}} image compression and audio data compression methods.|$|E
50|$|By {{employing}} AMD APP SDK, {{available for}} Linux and Microsoft Windows, developers can create hybrid encoders that pair custom motion estimation, inverse discrete cosine transform and motion compensation with the hardware <b>entropy</b> <b>encoding</b> to achieve faster than real-time encoding. In hybrid mode, only the <b>entropy</b> <b>encoding</b> block of the VCE unit is used, while the remaining computation is offloaded to the 3D engine (GCN) of the GPU, so the computing scales {{with the number}} of available compute units (CUs).|$|E
50|$|<b>Entropy</b> <b>encoding</b> - a {{coding scheme}} that assigns codes to symbols {{so as to}} match code lengths with the probabilities of the symbols.|$|E
40|$|This paper {{studies the}} use of the Tsallis Entropy versus the classic Boltzmann-Gibbs-Shannon entropy for {{classifying}} image patterns. Given a database of 40 pattern classes, the goal is to determine the class of a given image sample. Our experiments show that the Tsallis <b>entropy</b> <b>encoded</b> in a feature vector for different $q$ indices has great advantage over the Boltzmann-Gibbs-Shannon entropy for pattern classification, boosting recognition rates by a factor of 3. We discuss the reasons behind this success, shedding light on the usefulness of the Tsallis entropy...|$|R
40|$|Abstract. This paper {{presents}} a novel algorithm for rate-complexity scalable multi-view image coding using an adaptive disparity-compensated (DC) wavelet lifting scheme. First, image regions of multi-view images are prioritized by counting matching points. The proposed algorithm selects either Haar, 5 / 3, or our proposed multiple picture reference DC wavelet lifting adaptively. The selection criterion {{is based on}} the bit budget constraint, the complexity budget constraint, and the priorities of image regions. Then, the low-pass and high-pass subbands, obtained from the DC wavelet lifting, are further decomposed by a spatial wavelet transform. The resulting wavelet coefficients are <b>entropy</b> <b>encoded</b> with the SPIHT codec. Experimental results show that the proposed algorithm provides an efficient adaptive framework for multi-view image coding...|$|R
40|$|An {{audio decoder}} for {{providing}} a decoded audio {{information on the}} basis of an <b>entropy</b> <b>encoded</b> audio information comprises a context-based entropy decoder configured to decode the entropy-encoded audio information in dependence on a context, which context is based on a previously-decoded audio information in a non-reset state-of-operation. The context-based entropy decoder is configured to select a mapping information, for deriving the decoded audio information from the encoded audio information, in dependence on the context. The context-based entropy decoder comprises a context resetter configured to reset the context for selecting the mapping information to a default context, which default context is independent from the previously-decoded audio information, in response to a side information of the encoded audio information...|$|R
50|$|Data {{compression}} which explicitly {{tries to}} minimize {{the average length of}} messages according to a particular assumed probability model is called <b>entropy</b> <b>encoding.</b>|$|E
5000|$|In {{information}} theory an [...] <b>entropy</b> <b>encoding</b> is a lossless data compression scheme that {{is independent of}} the specific characteristics of the medium.|$|E
5000|$|Context-adaptive binary {{arithmetic}} coding (CABAC) {{is a form}} of <b>entropy</b> <b>encoding</b> used in the H.264/MPEG-4 AVC and High Efficiency Video Coding (HEVC) standards. It is a lossless compression technique, although the video coding standards in which it is used are typically for lossy compression applications. CABAC is notable for providing much better compression than most other <b>entropy</b> <b>encoding</b> algorithms used in video encoding, and {{it is one of the}} key elements that provides the H.264/AVC encoding scheme with better compression capability than its predecessors.|$|E
40|$|The bit {{allocation}} procedure of JPEG 2000 {{is based on}} a rate-distortion curve directly computed from quantized and encoded wavelet coeffcients. Thus, JPEG 2000 exploits at best the efficiency of its bit plane context-based arithmetic coder. However, encoding data which will not be saved in the final bit stream introduces com- plexity and JPEG 2000 {{bit allocation}} requires a lot of tests. We propose a new compression scheme using a low complexity model-based bit allocation followed by scalar quantizers with optimized deadzone sizes and the EBCOT bit plane coder to <b>entropy</b> <b>encode</b> the quantized subbands. The resulting compression scheme provides the same performances as JPEG 2000 with less complexity and simpler hardware implementation...|$|R
40|$|We {{present a}} method for {{improving}} {{the overall quality of}} motion compensated video codecs by using a variable number of motion objects to alter the bandwidth allocated to the motion description. The proposed algorithm takes as its input an existing motion vector field from which it extracts groups of vectors. Control is obtained by only encoding the largest N of these groups. Those blocks which do not belong to the N groups are reassigned to one based upon a block comparison metric. The resulting list of vectors and a mapping showing group membership are then <b>entropy</b> <b>encoded.</b> Experimental results demonstrate that by choosing a value of N that matches the type and complexity of motion in the sequence, the overall compression performance is improve...|$|R
40|$|Video object coding {{is one of}} {{the most}} {{important}} functionalities proposed by MPEG 4. In this paper, we propose a new wavelet method to encode the texture of an arbitrarily shaped object, both for the still and for the moving object. The method uses the shape adaptive wavelet transform (SA-DWT) in MPEG 4 still object coding, but with a computationally more efficient lifting implementation. The transformed object coefficients are then quantized and <b>entropy</b> <b>encoded</b> with a partial bitplane embedded coder, which greatly improves the coding efficiency. We denote the coding algorithm as video object wavelet (VOW) coder. Experimental results show that VOW significantly outperforms MPEG 4 in still object coding, and achieves a comparable performance in video object coding in terms of PSNR. Moreover, the VOW decoded object looks better subjectively, with much less annoying blocking artifacts than that of MPEG 4. 1...|$|R
5000|$|The name of {{the project}} is {{inspired}} by the MPEG video standards group, together with [...] "FF" [...] for [...] "fast forward". The logo uses a zigzag pattern that shows how MPEG video codecs handle <b>entropy</b> <b>encoding.</b>|$|E
50|$|Golomb was the {{inventor}} of Golomb coding, a form of <b>entropy</b> <b>encoding.</b> Golomb rulers, used in astronomy and in data encryption, are also named for him, as {{is one of the}} main generation techniques of Costas arrays, the Lempel-Golomb generation method.|$|E
50|$|Adaptive Binary Optimization, (ABO), is a {{supposed}} lossless image compression algorithm by MatrixView Ltd. It uses a patented method to compress the high correlation found in digital content signals and additional compression with standard <b>entropy</b> <b>encoding</b> algorithms such as Huffman coding.|$|E
40|$|Critical {{dynamics}} {{have been}} postulated {{as an ideal}} regime for neuronal networks in the brain, considering optimal dynamic range and information processing. Herein, we focused on how information <b>entropy</b> <b>encoded</b> in spatiotemporal activity patterns may vary in critical networks. We employed branching process based models to investigate how entropy can be embedded in spatiotemporal patterns. We determined that the information capacity of critical networks may {{vary depending on the}} manipulation of microscopic parameters. Specifically, the mean number of connections governed the number of spatiotemporal patterns in the networks. These findings are compatible with those of the real neuronal networks observed in specific brain circuitries, where critical behavior is necessary for the optimal dynamic range response but the uncertainty provided by high entropy as coded by spatiotemporal patterns is not required. With this, we were able to reveal that information processing can be optimized in neuronal networks beyond critical states...|$|R
40|$|Abstract: In this paper, {{we present}} a novel video coding scheme based on the {{adaptive}} non-uniform bitplane modeling of video sequences in wavelet domain and the generalized finite automata (GFA) representation. Unlike the traditional block-based motion compensation coding, where a video sequence in GoPs is arranged as I-, P- and B-frames and a motion estimate is searched in {{one or a few}} reference frames at the fixed or predefined block sizes at a spatial domain metric, in the proposed scheme, a video sequence is represented in GoPs as an overall binary image by bitplane modeling the significant coefficients of the video sequence within subbands. The inter-frame, inter-level and inter-bitplane similarities inhabited in the binary image are then optimally explored, leading to a compact GFA representation of the bitplane modeled video sequence. Finally, all the transitions in the GFA representation are <b>entropy</b> <b>encoded</b> into a scalable bitstream. The proposed scheme significantly outperforms the H. 26 X series coding schemes in rate-distortion performance. It could achieve bitrate ranges at 4 - 5 Kbps and 15 - 18 Kbps for QCIF 10 Hz and QCIF 30 Hz sequences, respectively, a target unachievable by even the newly emerged H. 264 standard. 1...|$|R
40|$|This paper {{presents}} {{a new technique}} for the compression of multispectral images, which relies on the segmentation of the image into regions of approximately homogeneous land cover. The rationale behind this approach is that, within regions of the same land cover, the pixels have stationary statistics and are characterized by mostly linear dependency, contrary to what usually happens for unsegmented images. Therefore, by applying conventional transform coding techniques to homogeneous groups of pixels, the proposed algorithm is able to effectively exploit the statistical redundancy of the image, thereby improving the rate distortion performance. The proposed coding strategy consists of three main steps. First, each pixel is classified by vector quantizing its spectral response vector, so that both a reliable classification and a minimum distortion encoding of each vector are obtained. Then, the classification map is <b>entropy</b> <b>encoded</b> and sent as side information, Finally, the residual vectors are grouped according to their classes and undergo Karhunen-Loeve transforming in the spectral domain and discrete cosine transforming in the spatial domain. Numerical experiments on a six-band thematic mapper image show that the proposed technique outperforms the conventional transform coding technique by 1 to 2 dB at all rates of interest...|$|R
50|$|The {{handling}} of video data involves computation of data compression algorithms and possibly of video processing algorithms. As the template Compression methods shows, lossy video compression algorithms involve the steps: Motion estimation (ME), Discrete cosine transform (DCT), and <b>entropy</b> <b>encoding</b> (EC).|$|E
50|$|AMD intends {{developers}} to employ AMD APP SDK to utilize Video Coding Engine hybrid mode to create hybrid encoders that pair custom motion estimation, inverse discrete cosine transform and motion compensation with the hardware <b>entropy</b> <b>encoding</b> to achieve faster than real-time encoding.|$|E
50|$|In this example, all common substrings {{with four}} or more {{characters}} were eliminated by the compression process. More common compressors can compress this better. Unlike compression methods such as gzip and bzip2, there is no <b>entropy</b> <b>encoding</b> used to pack alphabet into the bit stream.|$|E
40|$|Entropy is a {{fundamental}} concept in equilibrium statistical mechanics, yet its origin in the non-equilibrium dynamics of isolated quantum systems is not fully understood. A strong consensus is emerging around {{the idea that the}} stationary thermodynamic entropy is the von Neumann entanglement entropy of a large subsystem embedded in an infinite system. Also motivated by cold-atom experiments, here we consider the generalisation to Renyi entropies. We develop a new technique to calculate the diagonal Renyi entropy in the quench action formalism. In the spirit of the replica treatment for the entanglement entropy, the diagonal Renyi entropies are generalised free energies evaluated over a thermodynamic macrostate which depends on the Renyi index and, in particular, it is not the same describing the von Neumann entropy. The technical reason for this, maybe surprising, result is that the evaluation of the moments of the diagonal density matrix shifts the saddle point of the quench action. An interesting consequence is that different Renyi <b>entropies</b> <b>encode</b> information about different regions of the spectrum of the post-quench Hamiltonian. Our approach provides a very simple proof of the long-standing issue that, for integrable systems, the diagonal entropy is half of the thermodynamic one and it allows us to generalise this result to the case of arbitrary Renyi entropy. Comment: 8 pages, 1 figure, to appear in PR...|$|R
40|$|Abstract – In this paper, I have {{described}} Genetic Algorithm for combinatorial data leading to establishment of mathematical modeling for Information Theory. The paper describes GA (Genetic Algorithm) {{in light of}} information theory and then derives Mathematical Framework covering {{but not limited to}} Big Data. This framework is a boon to any domain that has to harness data explosion like neurobiology, statistical inference, quantum computation, <b>entropy</b> management, <b>encoding</b> information theory (cryptography) etc. The efficiency calculated here describes incremental probabilistic approach to reach a solution which is highly stable, evolutionary and self adjusting henceforth invents a extremely new approach for computational analytics...|$|R
40|$|Using Matrix theory, {{we propose}} a {{technique}} {{on how to}} compute the entangle- ment entropy between a supergravity probe and modes on a spherical membrane. We demonstrate that a membrane stretched between the probe and the sphere entangles these modes {{and can lead to}} an the entanglement <b>entropy</b> that <b>encodes</b> information about local gravitational geometry seen by the probe. Comment: 36 pages, no figures; conclusion and interpretation revised. Readers are directed instead to a significantly expanded version of this computation that can be found in arXiv: 1705. 01128; this paper is being removed to avoid unnecessary duplicatio...|$|R
