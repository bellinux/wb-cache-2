5|10000|Public
50|$|From 1961 to 1967, Price was {{employed}} by IBM {{as a consultant}} on <b>graphic</b> <b>data</b> <b>processing.</b> In 1966 he was treated for thyroid cancer, but the operation to remove the tumour left his shoulder partially paralysed and left him reliant on thyroxine medication. With the money from his medical insurance, {{he moved to the}} United Kingdom {{to start a new life}} in November 1967.|$|E
40|$|The main aim of {{this thesis}} is a {{creation}} of a technical map of Kraví hora area in scale of 1 : 500. The thesis describes all the stages {{of the process of}} creating this map - reconnaissance, surveying network, detailed measurements, data processing of these measurements, data analysis of the accuracy to <b>graphic</b> <b>data</b> <b>processing.</b> This thesis also deals with a comparison of geodetic directives obtained from public utility commissioners and a creation of a thematic map in digital form according to the attributes in tables contained in these directives. The main result of this thesis is a technical map which contains planimetry, altimetry and map lettering according to the CSN 01 3410 and CSN 01 3411 standards...|$|E
40|$|This diploma paper, {{from the}} {{thematic}} cartography field, is showing {{the course of}} making an outline scheme, an editorial plan with the description of map-making technology and also map-making schemes by using the GEOS 6 program. The specialty of this diploma is the main cartographic source, the digital cadastral plan (DKN), which was chosen because of the rare use of such digital graphic data for other purposes rather then land cadastre data maintenance. The digital cadastral plan is useful for the digital basis of a map as a vector picture and therefore becomes its main cartographic source. However the DKN alone does not suffice {{and because of this}} fact, another two auxiliary cartographic sources are being used. Those are the ortho-photo-plan and the basic topographic plan at a 1 : 5000 (TTN- 5) scale. Therefore the main cartographic source (DKN) is supplemented with two additional cartographic sources and with additionally added data from field examinations. The cartographic basis was processed with the GEOS 6 (version 4. 2) computer graphic program, which is adjusted to digital <b>graphic</b> <b>data</b> <b>processing</b> in the exact same form in which it is being maintained and managed by Slovenia’s geodesic administration...|$|E
40|$|Multiprocessor {{system for}} <b>graphics</b> <b>data</b> <b>processing,</b> whereby several {{processors}} (microprocessors or signal processors) perform the geometric processing of geometrical objects, and then display the transformed and clipped coordinates (results) in a rendering {{area in a}} set of pixels (dataset), said set being stored in an image store, whereby the processors also perform the rendering task, whereby all the processors simultaneously retain the results of the geometrical object for processing by rendering, and the set of the pixels determined in each of the processors is stored {{in a part of the}} distributed image store allocated to each processor...|$|R
50|$|Lingo {{can be used}} {{to build}} user interfaces, to {{manipulate}} raster graphics, vector graphics and 3D computer <b>graphics,</b> and other <b>data</b> <b>processing</b> tasks. Lingo supports specialized syntax for image processing and 3D object manipulation. 3D meshes can also be created on the fly using Lingo.|$|R
40|$|The aim of {{this thesis}} is to design and {{implement}} framework for geolocation games on Android platform. Its use is then demonstrated on sample application. In this text are also described some geolocation games and ways of creating 3 D <b>graphics,</b> <b>processing</b> <b>data</b> from sensors and use of augmented reality elements on Android platform...|$|R
40|$|Structural {{strength}} analysis depends mainly on {{finite element method}} and experiments. For complex structural system, a rather large error {{can be caused by}} some uncertain factors, such as load distributions, boundary conditions and constitutive relations in numerical analysis. At the same time, owing to the limitation of measuring and testing techniques, the strength and stiffness of key components can not be estimated by using the limited test data. To simulate stresses accurately under complex static environment, improve man-machine interactive system, and make the best use of fore- and post-processing function in <b>graphic</b> <b>data</b> <b>processing,</b> the combine numerical analysis with experimental technique and have developed the semi-object emulation technique to analyze the nonlinear problem of structure statics. The modern optical measuring techniques and image processing techniques are firstly used for the method to acquire displacement data of the vessel surface, and the data are used for the boundary condition to determine the geometrical size of disfigurement in the wall of vessel and the stress level. The experimental verification of a given test model show that these adverse problem can be solved by using semi-object emulation technolog...|$|E
40|$|The computeroince its {{development}} in the late 1940 's, has created an era'of unlimited potential and change. A 'transformation of the world abont us and of the processes and methods with which we are accustomed is taking place at' an accelerating pace. Computer technology is making significant contributions to architecture and to the processes through which the architectural solution is produced. This is evidenced in its application to accounting procedures, critical path planning, specifications writing, cost estimating, and <b>graphic</b> <b>data</b> <b>processing.</b> Architects will see develop a new 'interprofessional and interdisciplinary dialogue' which will permit them to receive engineering data and evaluation of functional characteristics almost instantly, at any stage in the design process. Working drawings. and specifications will be produced with great rapidity using computerized technology. But the computer {{is more than just}} a tool for management or a more efficient means of producing construction documents. It can become a useful tool In determining criteria for the design process and through an analytic-synthesis and evaluation of such information can establish precise functional and environmental criteria, determine suitable engineering and architectural systems and solutions and make routine design decisions based on logical quantitative data. The architect, today, cannot depend solely on traditional methods used in design development. In the search for form, we begin to uncover a very complex structure of elements of which it consists. It becomes evident that because of this underlying complexity that the sheer speed and accuracy of the computer can be an aid to the designer. In the design development phase of the architectural process, the architect dissects his problem into its various component needs and by determining the relative importance of each aspect in his system of analysis creates a hierarchy of design criteria. Then, choosing from an information source a particular system or systems to fit the needs indicated or set forth in the criteria, he sets up a working model of combinations of these systems to approximate the correct three dimensional architectural solution to his specific problem. He then tests this model and its alternates until the best solution is found, that is, the combination of elements which best satisfies all the design criteria in its proper order of relative importance. The computer, applied to such a task, would give man the opportunity to set up and investigate many more alternates to the architectural solution in a more sophisticated manner. This in no way makes the architect less responsible for the final solution. In truth, it requires a deeper understanding of the architectural problem, a logical approach in solving it, and offers more opportunity for the architect to make use of his intuitive judgment on those problems which cannot be solved by computer technology...|$|E
40|$|Abstract. The {{computing}} power and programmability of graphics processing units (GPUs) {{has been successfully}} exploited for calculations unrelated to <b>graphics,</b> such as <b>data</b> <b>processing,</b> numerical algorithms, and secret key cryptography. In this paper, a new variant of the Montgomery exponentiation algorithm that exploits the processing power and parallelism of GPUs is designed and implemented. Furthermore, performance tests are conducted and the suitability of the proposed algorithm for accelerating public key encryption is discussed. ...|$|R
40|$|Abstract] Recently, {{there has}} been {{increasing}} interest in developing long term Structure Health Monitoring (SHM) system for civil infrastructure. The extremely large amount of data collected during the long term structural monitoring is a great challenge for <b>data</b> <b>processing</b> and interpretation. In this paper, a web based scalable data management system is proposed, which allows for automatic database updating, advanced statistical and structural data interpretation, and web-based data searching. Open source tools, Apache web server, PHP, and MySQL are employed to implement the system. Matlab is integrated as the <b>graphic</b> and <b>data</b> <b>processing</b> engine. The system is developed dedicatedly for the E- 12 th Street Bridge project on I- 235 in Des Moines, IA. However, it could be extended to other projects very conveniently. [Keywords] long term Structure Health Monitoring (SHM) system; scalable web-based data management system; open source tools; PHP; MySQL; Apache...|$|R
30|$|Discussion of {{interdisciplinary}} approach and framework {{used in this}} study This project featured an interdisciplinary approach in consultation with numismatic experts. The project team was formed of twelve specialists, some with expertise in two or more disciplines, including: archaeology, art history and museology, conservation and heritage science, chemistry and electrochemistry, mechatronics and engineering, metrology, computer science, computer <b>graphics</b> and <b>data</b> <b>processing.</b> This joint panel of experts produced holistic knowledge about two ancient Roman coins, with a great expenditure of time and care with the aim to produce active guidelines for end users in numismatic collections [31]. Discussion and feedback was sought in a team discussion with four numismatic experts. All methods used in this case study were presented and discussed to check that they met requirements in real world numismatic collections [3].|$|R
30|$|The R {{language}} (R Development Core Team 2008) {{is one of}} {{the leading}} statistical programming languages, and was referenced a significant number of times in the dataset. R was created as a free and open source implementation of the S statistical programming language with influences from Scheme. R focuses on ease of use, tight integration with publication quality <b>graphics</b> and charts, <b>data</b> <b>processing,</b> and modular extensions to go beyond the core functionality. It has its own mathematical formula expression language, like LaTeX, and provides users convenient tools converting formulas into executable code.|$|R
40|$|This diploma thesis {{deals with}} {{comparison}} of {{insurance companies and}} insurance groups and their mobile applications for non-life insurance. Part of diploma thesis is survey between users of smartphones and mobile applications. Another part is interview with experts connected with mobile applications of insurance companies. Based on acquired data and information is developed set of steps useful for effective marketing for any mobile application for non-life insurance, especially for Pojišťovna application. The diploma thesis includes tables, pictures and <b>graphics</b> <b>processing</b> <b>data...</b>|$|R
40|$|Figure 1 : 3 example {{applications}} with Xflow. Each {{screen shot}} shows a web page that uses XML 3 D and Xflow to render real-time 3 D <b>graphics</b> with advanced <b>data</b> <b>processing.</b> The left picture uses multiple instances of mesh interpolation for animation. The middle picture shows a dynamically generated noise texture. The right screen demonstrates multiple instances of skeletal animations. The Web {{evolved from a}} simple information medium to an inter-active application platform featuring advanced 2 D layouts, videos, and audio. At the same time, support for hardware-accelerated 3 D graphics improves continuously even among mobile devices. Hence, there is {{a renewed interest in}} adding interactive 3 D graph-ics to the overall Web experience and therefore a need for high-performance <b>data</b> <b>processing</b> beyond DOM modifications through JavaScript. A challenge in adding this functionality to Web tech-nologies is to close the deep gap between low-level hardware pro-gramming (e. g. using WebGL or WebCL) and high-level Web de-velopment with HTML, CSS and JavaScript...|$|R
40|$|This thesis {{deals with}} the general mapping and {{describing}} the project of the implementation {{of changes in the}} process of Change Management in insurance company Česká pojišťovna with the final evaluation including a final report and analysis of important moments. From this project was obtained various outputs, which together with the final evaluation are compared with procedures reported in the theoretical background (IPMA;, Methodology of Project Management of Česká pojišťovna). The conclusion is devoted to determining the level of maturity of the Change Management process after the conclusion of the project. The thesis includes pictures, diagrams, tables and <b>graphics</b> <b>processing</b> <b>data</b> needed for clarity...|$|R
40|$|Wavelets and multiresolution {{analysis}} are instrumental for developing efficient methods for representing, storing and manipulating functions {{at various levels}} of detail. Although alternative methods such as hierarchical quadtrees or pyramidal models {{have been used to}} that effect as well, wavelets have picked up increasing popularity in recent years due to their energy compactness, efficiency, and speed. Wavelet representations have achieved a great success {{in a wide variety of}} applications, including <b>graphics,</b> <b>data</b> compression, signal <b>processing,</b> physical simulation, hierarchical optimization, and numerical analysis, among others. This paper gives an overview of wavelets and their construction, as well as some applications to graphics and 3 -D mesh processing at multiple levels of detail. The emphasis is on meshes of arbitrary topology and their multiresolution analysis by means of subdivision wavelets and their generalizations. In particular, both the traditional (first generation) wav [...] ...|$|R
40|$|The fast {{developments}} in the information technology reflects also on the GIS community and new opportunities give new challenges to {{the research and development}} groups. As a result of these developments, 3 D data can already be processed efficiently and better methods are being explored increasingly. At the same time, the Internet has matured in many senses since it was first publicized and it is being actively used in the developed and developing world {{despite the fact that it}} has a number of disadvantages mainly regarding the data traffic. Regardless of its disadvantages, it can be said that WWW is the most popular and powerful networked information system to date. [1] It is evident that a combination of the 3 D <b>graphics,</b> geospatial <b>data</b> <b>processing</b> or GIS and internet is demanded at this stage of the technological development. More clearly, an Online 3 D GIS is at demand. Yet there still are a number of limitations in existing technology and, to solve them, new approaches are being sought continuously. In this paper you will find a discussion of whether it is worth to try to utilize VRML/X 3 D as an interface for an online GIS system or not. The discussion is based on a literature survey...|$|R
40|$|The World Wide Web is {{amidst a}} {{transition}} from interactive websites to web applications. An increasing number of users perform their daily computing tasks entirely within the web browser — turning the Web into an important platform for application development. The Web as a platform, however, lacks the computational performance of native applications. This problem has motivated the inception of Microsoft Xax and Google Native Client (NaCl), two independent projects that fa-cilitate the development of native web applications. Native web applications allow the extension of conventional web applications with compiled native code, while maintaining operating system portability. This dissertation determines the bene-fits and drawbacks of native web applications. It also addresses the question how the performance of JavaScript web applications compares to that of native appli-cations and native web applications. Four application benchmarks are introduced that focus on different performance aspects: number crunching (serial and parallel), 3 D <b>graphics</b> performance, and <b>data</b> <b>processing.</b> A performance analysis is under...|$|R
40|$|Modern {{university}} education is very strongly based on computer techniques, which provide nec-essary computational power to develop specialized software. Very intuitive graphical programs be-came indispensable tools for word and <b>graphics</b> <b>processing,</b> <b>data</b> storage, and advanced computational tasks. An increasing role of information availability {{from the data}} stored on web pages and network accessible data bases can be observed. This immense potential source of information requires appro-priate searching tools and algorithms. In this situation a well-scheduled track through computer sci-ence courses, for students of various specializations, is an essential issue. This problem is especially acute in education in Biomedical Engineering since application of computer methods to medicine, computational biology, and bioinformatics requires an approach that differs from classical methods. We discuss concepts and techniques in teaching computer science courses, comparing three speciali...|$|R
40|$|Real-time {{graphics}} processing on {{the cloud}} poses significant challenges {{in terms of}} <b>processing</b> capability, <b>data</b> transmission, and the management of latency. The rendering of large and complex <b>graphics</b> <b>data</b> requires large <b>processing</b> power and significant storage that low-powered machines are unlikely to handle capably. In addition, the transmission of graphics may introduce considerable delays, leading to poor interactivity. Numerous works {{have been carried out}} taking these issues into account, most of which being based on level of detail (LOD) and image based rendering (IBR) techniques. However, there are many tradeoffs that need to be carefully studied in order to realize some of the benefits of cloud computing for three dimensional (3 D) networked graphics. In this project, we explore {{the state of the art}} remote rendering, or in other words, moving the rendering of complex <b>graphics</b> <b>data</b> into a cloud system. A networked rendering paradigm based on our proposed pipeline-splitting method is introduced to facilitate a remote-rendering system with the aim of partitioning the rendering workload between the client and server. We also propose a visibility streaming method for networked applications to reduce the transmission capacity required. One of the main advantages of our proposed methods is that it is easy to scale up at the server side by distributing the workload to be handled in different machines, leading to a significant improvement at the server side in terms of performance...|$|R
40|$|Department of Biomedical EngineeringOver {{the several}} decades, {{there have been}} {{clinical}} needs that requires advanced technologies in medicine. Optical coherence tomography (OCT), one of the newly emerged medical imaging devices, provides non-invasive cross-sectional images in high resolution which is mainly used in ophthalmology. However, due to the limited penetration depth of 1 - 2 mm in bio-samples, {{there is a limit}} to be widely used. In order to easily integrate with existing medical tools and be convenient to users, it is necessary that the sample unit of OCT should be compact and simple. In this study, we developed high-speed swept-source OCT (SS-OCT) for advanced screening of otolaryngology. Synchronized signal sampling with a high-speed digitizer using a clock signal from a swept laser source, its trigger signal is also used to synchronize with the movement of the scanning mirror. The SS-OCT system can reliably provide high-throughput images, and two-axis scanning of galvano mirrors enables real-time acquisition of 3 D <b>data.</b> <b>Graphic</b> <b>processing</b> unit (GPU) can performs high-speed <b>data</b> <b>processing</b> through parallel programming, and can also implement perspective projection 3 D OCT visualization with optimal ray casting techniques. In the Clinical Study of Otolaryngology, OCT was applied to identify the microscopic extrathyroidal extension (mETE) of papillary thyroid cancer (PTC). As a result to detect the mETE of around 60...|$|R
40|$|Abstract. Geological {{disasters}} practical work requires accurate extracted {{data and}} convenient cartography. This paper couples the essential features of AutoCAD and MAPGIS. The multi-analysis {{is supposed to}} pay attention to both <b>graphic</b> <b>data</b> characteristics and the <b>graphic</b> <b>data</b> switching process. This process and method elaborates and transforms AutoCAD DXF and DWG <b>graphic</b> <b>data</b> and the MAPGIS MPJ <b>graphic</b> <b>data,</b> in order to enable the existing geography information resource obtain the full usage of <b>graphic</b> <b>data</b> coupling...|$|R
40|$|Recent {{years have}} seen a {{powerful}} shift towards data-rich environments throughout society. This has extended {{to a change in}} how the artifacts and products of scientific knowledge production can be analyzed and understood. Bottom-up approaches are on the rise that combine access to huge amounts of academic publications with advanced computer <b>graphics</b> and <b>data</b> <b>processing</b> tools, including natural language processing. Knowledge domain visualization is one of those multi-technology approaches, with its aim of turning domain-specific human knowledge into highly visual representations in order to better understand the structure and evolution of domain knowledge. For example, network visualizations built from co-author relations contained in academic publications can provide insight on how scholars collaborate with each other in one or multiple domains, and visualizations built from the text content of articles can help us understand the topical structure of knowledge domains. These knowledge domain visualizations need to support interactive viewing and exploration by users. Such spatialization efforts are increasingly looking to geography and GIS as a source of metaphors and practical technology solutions, even when non-georeferenced information is managed, analyzed, and visualized. When it comes to deploying spatialized representations online, web mapping and web GIS can provide practical technology solutions for interactive viewing of knowledge domain visualizations, from panning and zooming to the overlay of additional information. This thesis presents a novel combination of advanced natural language processing - in the form of topic modeling - with dimensionality reduction through self-organizing maps and the deployment of web mapping/GIS technology towards intuitive, GIS-like, exploration of a knowledge domain visualization. A complete workflow is proposed and implemented that processes any corpus of input text documents into a map form and leverages a web application framework to let users explore knowledge domain maps interactively. This workflow is implemented and demonstrated for a data set of more than 66, 000 conference abstracts...|$|R
40|$|With rapid {{development}} of hardware devices and software programs, {{a large amount}} of <b>graphic</b> <b>data</b> has been brought to or generated in digital domain, and become increasingly more widely used in our everyday life. Due to the ease of editing and distributing <b>graphic</b> <b>data</b> in the digital domain, protecting <b>graphic</b> <b>data</b> from such fraudulent operations as malicious tampering and unauthorized copying is becoming a major concern. The primary motivation of this dissertation research is to develop novel forensic techniques for digital <b>graphic</b> <b>data</b> to facilitate its proper distribution, authentication, and usage. We investigate two complementary mechanisms for performing forensic analysis on <b>graphic</b> <b>data,</b> namely, the extrinsic and intrinsic approaches. In the extrinsic approaches, we seamlessly embed into <b>graphic</b> <b>data</b> extrinsic watermarks/fingerprints, which shall later be extracted for verifying authenticityor tracing leak of the <b>graphic</b> <b>data.</b> By utilizing such extrinsic techniques via data embedding, we have studied robust digital fingerprinting for curve-based graphics such as topographic maps and drawings, in which a unique ID referred to as a digital fingerprint is robustly embedded for tracing traitors. Through proper transformation...|$|R
40|$|The main {{thrust of}} our work {{in the third year}} of {{contract}} NAG 8 - 759 was the development and analysis of various <b>data</b> <b>processing</b> techniques that may be applicable to residual acceleration data. Our goal is the development of a <b>data</b> <b>processing</b> guide that low gravity principal investigators can use to assess their need for accelerometer data and then formulate an acceleration data analysis strategy. The work focused on the flight of the first International Microgravity Laboratory (IML- 1) mission. We are also developing a data base management system to handle large quantities of residual acceleration data. This type of system should be an integral tool in the detailed analysis of accelerometer data. The system will manage a large <b>graphics</b> <b>data</b> base in the support of supervised and unsupervised pattern recognition. The goal of the pattern recognition phase is to identify specific classes of accelerations so that these classes can be easily recognized in any data base. The data base management system is being tested on the Spacelab 3 (SL 3) residual acceleration data...|$|R
40|$|This paper investigates how new {{database}} technologies, in particular, complex object databases, object-oriented databases, and deductive databases, {{can be used}} {{to store}} object-oriented <b>graphic</b> <b>data</b> and discusses the related problems. Keywords: <b>graphic</b> <b>data</b> structures and databases, scene and object modeling 1 Introduction <b>Graphic</b> <b>data</b> is generally divided into two kinds: bit-mapped graphics and objectoriented graphics [18]. Bit-mapped graphics are usually generated with scanning or painting software, while object-oriented graphics are usually created with drawing software using primitive objects such as points, lines, shapes, texts etc. How to effectively store <b>graphic</b> <b>data</b> in a database system presents a challenge to the traditional relational database technology. <b>Graphic</b> <b>data</b> is usually handled by various file systems, rather than database systems. As a result, the graphics can only be displayed or printed rather than be queried. For example, we cannot ask where a specific object [...] ...|$|R
40|$|One of the {{advantages}} of automated cartography is that map data stored in the digital computer can be plotted or displayed at any scale or projection by recomputing the coordinates of the data. This is especially easy in the case of vector (<b>graphics)</b> <b>data</b> {{but in the case of}} digital image (raster) data, remapping is a more difficult operation. Examples of the remapping of digital imagery would include rectification of a LANDSAT MSS to an orthographic or Mercator projection, warping of one image to register with another, or rotation, scale, or aspect changes of a digital image. Use of general purpose computers and array processors for this task will be covered. <b>Data</b> <b>processing</b> error will be discussed for each modelling/warping approach...|$|R
40|$|Thesis, (M. S.) Geological Sciences Includes bibliographical {{references}} (pages 51 - 53) Recent {{years have}} seen a powerful shift towards data-rich environments throughout society. This has extended {{to a change in}} how the artifacts and products of scientific knowledge production can be analyzed and understood. Bottom-up approaches are on the rise that combine access to huge amounts of academic publications with advanced computer <b>graphics</b> and <b>data</b> <b>processing</b> tools, including natural language processing. Knowledge domain visualization is one of those multi-technology approaches, with its aim of turning domain-specific human knowledge into highly visual representations in order to better understand the structure and evolution of domain knowledge. For example, network visualizations built from co-author relations contained in academic publications can provide insight on how scholars collaborate with each other in one or multiple domains, and visualizations built from the text content of articles can help us understand the topical structure of knowledge domains. These knowledge domain visualizations need to support interactive viewing and exploration by users. Such spatialization efforts are increasingly looking to geography and GIS as a source of metaphors and practical technology solutions, even when non-georeferenced information is managed, analyzed, and visualized. When it comes to deploying spatialized representations online, web mapping and web GIS can provide practical technology solutions for interactive viewing of knowledge domain visualizations, from panning and zooming to the overlay of additional information. This thesis presents a novel combination of advanced natural language processing [...] in the form of topic modeling [...] with dimensionality reduction through self-organizing maps and the deployment of web mapping/GIS technology towards intuitive, GIS-like, exploration of a knowledge domain visualization. A complete workflow is proposed and implemented that processes any corpus of input text documents into a map form and leverages a web application framework to let users explore knowledge domain maps interactively. This workflow is implemented and demonstrated for a data set of more than 66, 000 conference abstract...|$|R
50|$|Earth {{sciences}} {{graphics software}} includes {{the capability to}} read specialized data formats such as netCDF, HDR and GRIB. Such software is sometimes able to access the data from remote data centers. Examples of applications include satellite <b>data</b> <b>processing,</b> analysis of output from complex meteorological models and display of time series of <b>data.</b> <b>Graphics</b> capabilities range from simple line plots, to complex three-dimensional visualizations.|$|R
5000|$|Ikaruga XBLA (2008) - <b>Graphic</b> <b>Data</b> Conversion, Additional <b>Graphics</b> ...|$|R
40|$|With rapid {{development}} of hardware devices and software programs, {{a large amount}} of <b>graphic</b> <b>data</b> has been brought to or generated in digital domain, and become increasingly more widely used in our everyday life. Due to the ease of editing and distributing <b>graphic</b> <b>data</b> in the digital domain, protecting <b>graphic</b> <b>data</b> from such fraudulent operations as malicious tampering and unauthorized copying is becoming a major concern. The primary motivation of this dissertation search is to develop novel forensic techniques for digital <b>graphic</b> <b>data</b> to facilitate its proper distribution, authentication, and usage. We investigate two complementary mechanisms for performing forensic analysis on <b>graphic</b> <b>data,</b> namely, the extrinsic and intrinsic approaches. In the extrinsic approaches, we seamlessly embed into <b>graphic</b> <b>data</b> extrinsic watermarks/fingerprints, which shall later be extracted for verifying authenticity or tracing leak of the <b>graphic</b> <b>data.</b> By utilizing such extrinsic techniques via data embedding, we have studied robust digital fingerprinting for curve-based graphics such as topographic maps and drawings, in which a unique ID referred to as a digital fingerprint is robustly embedded for tracing traitors. Through proper transformations between 2 -D contour curves and 3 -D digital elevation maps, we have also developed an effective fingerprinting technique for digital elevation maps. In order to authenticate such <b>graphic</b> <b>data</b> as critical document and signature images, we have investigated high-payload watermark embedding for binary images, whose authenticity shall be decided through verifying integrity of the hidden watermark. In the intrinsic approaches, since scanners are a major kind of apparatus to capture <b>graphic</b> <b>data,</b> we develop a new technique of utilizing intrinsic sensor noise features for non-intrusive scanner forensics to verify the acquisition source and integrity of digital scanned images. We extract statistical features of scanning noise from scanned image samples, and construct a robust scanner identifier to determine the model of the scanner used to capture a scanned image. We further broaden the scope of acquisition forensics to differentiating scanned images from camera taken images and computer generated images, as well as perform integrity forensic analysis on scanned images using the proposed noise features, including detecting post-processing operations after scanning, and implementing steganalysis on scanned images...|$|R
50|$|Universal 3D (U3D) is a {{compressed}} {{file format}} standard for 3D computer <b>graphics</b> <b>data.</b>|$|R
5000|$|Interprets the Playfield <b>graphics</b> <b>data</b> stream from ANTIC {{to apply}} {{color to the}} display.|$|R
5000|$|... #Caption: Satellite {{image of}} Congo, {{generated}} from raster <b>graphics</b> <b>data</b> supplied by The Map Library.|$|R
5000|$|The {{ability for}} games to {{allocate}} offscreen memory and to store <b>graphics</b> <b>data</b> in offscreen memory.|$|R
40|$|<b>Data</b> <b>Processing</b> {{discusses}} the principles, practices, and associated tools in <b>data</b> <b>processing.</b> The book {{is comprised of}} 17 chapters that are organized into three parts. The first part covers the characteristics, systems, and methods of <b>data</b> <b>processing.</b> Part 2 deals with the <b>data</b> <b>processing</b> practice; this part {{discusses the}} data input, output, and storage. The last part discusses topics related to systems and software in <b>data</b> <b>processing,</b> which include checks and controls, computer language and programs, and program elements and structures. The text will be useful to practitioners of computer-re...|$|R
