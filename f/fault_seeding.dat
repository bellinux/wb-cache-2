16|118|Public
50|$|Bebugging (or <b>fault</b> <b>seeding</b> or error seeding) is {{a popular}} {{software}} engineering technique used in the 1970s to measure test coverage. Known bugs are randomly added to a program source code and the programmer is tasked to find them. The percentage of the known bugs not found gives {{an indication of the}} real bugs that remain.|$|E
5000|$|An early {{application}} of bebugging was Harlan Mills's <b>fault</b> <b>seeding</b> approach [...] which was later refined by stratified fault-seeding. These techniques worked {{by adding a}} number of known faults to a software system {{for the purpose of}} monitoring the rate of detection and removal. This assumed {{that it is possible to}} estimate the number of remaining faults in a software system still to be detected by a particular test methodology.|$|E
40|$|Abstract-A {{number of}} {{analytical}} {{models have been}} proposed during the past 15 years for assessing the reliability of a software system. In this paper we present {{an overview of the}} key modeling approaches, provide a critical analysis of the underlying assumptions, and assess the limitations and applicability of these models during the software development cycle. We also propose a step-by-step procedure for fitting a model and illustrate it via an analysis of failure data from a mediumsized real-time command and control software system. Index Terms-Estimation, failure count models, <b>fault</b> <b>seeding,</b> input domain models, model fitting, NHPP, software reliability, times between failures...|$|E
40|$|Abstract — Model {{checking}} is {{very effective}} at finding out even subtle faults in system designs. A counterexample is usually generated by model checking algorithms when a {{system does not}} satisfy the given specification. However, a counterexample is not always helpful in explaining and isolating faults in a system when the counterexample is very long, which is usually the case for large scale systems. As such, there is a pressing need to develop fault explanation and isolation techniques. In this paper, we present a new approach for the fault explanation and isolation in discrete event systems with LTL (linear-time temporal logic) specifications. The notion of <b>fault</b> <b>seed</b> is introduced to characterize {{the cause of a}} fault. The identification of the <b>fault</b> <b>seed</b> is further reduced to a model checking problem. An algorithm is obtained for the <b>fault</b> <b>seed</b> identification. An example is provided to demonstrate the effectiveness of the approach developed. I...|$|R
30|$|For {{both the}} labeled and unlabeled data sets, fault signal was {{recorded}} from all four {{positions of the}} sensor. For the labeled data set, the acoustic fault data are recorded for three different types of <b>seeded</b> <b>faults</b> and one normal operation, as shown in Table 1. For the unlabeled data set, three different types of <b>faults</b> are <b>seeded</b> randomly. These <b>seeded</b> <b>faults</b> for the unlabeled data set {{are different from the}} <b>faults</b> <b>seeded</b> for the labeled data set. Each position of the sensor on the setup represents a data distribution D_i. The unlabeled data from distribution D_ 1 to D_ 4 are merged to form the unlabeled data set x_ul. This common unlabeled data set x_ul is used by RBM 1 to compute unlabeled weights W_ 1 and bias a_ 1 and b_ 1. These unlabeled weights and bias are then used as the initial weight and bias of the RBM 2. This technique is tested separately for each sensor position, with their respective labeled data sets and with this common unlabeled data set.|$|R
40|$|Abstract. The {{induction}} motor {{is the most}} common driver in industry and has been previously proposed as a means of inferring the condition of an entire equipment train, predominantly through the measurement and processing of power supply parameters. This has obvious advantages in terms of being non-intrusive or remote, less costly to apply and improved safety. This paper describes the use of the {{induction motor}} current to identify and quantify a number of common <b>faults</b> <b>seeded</b> on a two-stage reciprocating compressor. An analysis of the compressor working cycle leads to current signal the components that are sensitive to the common <b>faults</b> <b>seeded</b> to compressor system, and second- and third-order signal processing tools are used to analyse the current signals. It is shown that the developed diagnostic features: the bispectral peak value from the amplitude modulation bispectrum and the kurtosis from the current gives rise to reliable fault classification results. The low feature values can differentiate the belt looseness from other fault cases and valve leakage and inter-cooler leakage can be separated easily using two linear classifiers. This work provides a novel approach to the analysis stator current data for the diagnosis of motor drive faults...|$|R
40|$|Abstract—With {{the growth}} of {{complexity}} in modern auto-motive infotainment systems, graphical user interfaces {{become more and more}} sophisticated, and this leads to various chal-lenges in software testing. Due to the enormous amount of possible interactions, test engineers have to decide, which test aspects to focus on. In this paper, we examine what types of failures can be found in graphical user interfaces of automotive infotainment systems, and how frequently they occur. In total, we have analyzed more than 3, 000 failures, found and fixed during the development of automotive infotainment systems at Audi, Bosch, and Mercedes-Benz. We applied the Orthog-onal Defect Classification for categorizing these failures. The difficulties we faced when applying this classification led us to formulating requirements for an own classification scheme. On this basis, we have developed a hierarchical classification scheme for failures grounded on common concepts in software engineering, such as Model-View-Controller and Screens. The results of the application of our classification show that 62 % of the reports describe failures related to behavior, 25 % of the reports describe failures related to contents, 6 % of the reports describe failures related to design, and 7 % of the reports describe failures to be categorized. An outlined capability of the results is the support for <b>fault</b> <b>seeding</b> approaches which leads to the challenge of tracing the found failures to the correspondent faults. Keywords-domain specific failures; GUI based software; in-vehicle infotainment system; failure classification; <b>fault</b> <b>seeding.</b> I...|$|E
40|$|Abstract—Mutation {{analysis}} is an effective, yet often time-consuming and difficult-to-use method {{for the evaluation}} of testing strategies. In response to these and other challenges, this paper presents MAJOR, a <b>fault</b> <b>seeding</b> and mutation analysis tool that is integrated into the Java Standard Edition compiler as a non-invasive enhancement for use in any Java-based development environment. MAJOR reduces the mutant generation time and enables efficient mutation analysis. It has already been successfully applied to large applications with up to 373, 000 lines of code and 406, 000 mutants. Moreover, MAJOR’s domain specific language for specifying and adapting mutation operators also makes it extensible. Due to its ease-of-use, efficiency, and extensibility, MAJOR is an ideal platform for the study and application of mutation analysis. I...|$|E
40|$|Though {{mutation}} testing {{has been widely}} studied for more than thirty years, the prevalence and properties of equiv-alent mutants remain largely unknown. We report on the causes and prevalence of equivalent mutants and their rela-tionship to stubborn mutants (those that remain undetected by a high quality test suite, yet are non-equivalent). Our re-sults, based on manual analysis of 1, 230 mutants from 18 programs, reveal a highly uneven distribution of equivalence and stubbornness. For example, the ABS class and half UOI class generate many equivalent and almost no stubborn mu-tants, while the LCR class generates many stubborn and few equivalent mutants. We conclude that previous test ef-fectiveness studies based on <b>fault</b> <b>seeding</b> could be skewed, while developers of {{mutation testing}} tools should prioritise those operators that we found generate disproportionately many stubborn (and few equivalent) mutants...|$|E
40|$|A "seeded fault test" {{in support}} of a {{rotorcraft}} condition based maintenance program (CBM), is an experiment in which a component is tested with a known fault while health monitoring data is collected. These tests are performed at operating conditions comparable to operating conditions the component would be exposed to while installed on the aircraft. Performance of <b>seeded</b> <b>fault</b> tests is one method used to provide evidence that a Health Usage Monitoring System (HUMS) can replace current maintenance practices required for aircraft airworthiness. Actual in-service experience of the HUMS detecting a component fault is another validation method. This paper will discuss a hybrid validation approach that combines in service-data with <b>seeded</b> <b>fault</b> tests. For this approach, existing in-service HUMS flight data from a naturally occurring component fault will be used to define a component <b>seeded</b> <b>fault</b> test. An example, using spiral bevel gears as the targeted component, will be presented. Since the U. S. Army has begun to develop standards for using <b>seeded</b> <b>fault</b> tests for HUMS validation, the hybrid approach will be mapped to the steps defined within their Aeronautical Design Standard Handbook for CBM. This paper will step through their defined processes, and identify additional steps that may be required when using component test rig fault tests to demonstrate helicopter CI performance. The discussion within this paper will provide the reader with a better appreciation for the challenges faced when defining a <b>seeded</b> <b>fault</b> test for HUMS validation...|$|R
40|$|The {{planetary}} gearbox {{is a critical}} mechanism in helicopter transmission systems. Tooth failures in planetary gear sets will cause great risk to helicopter operations. A gear pitting damage level estimation methodology has been devised in this paper by integrating a physical model for simulation signal generation, a three-step statistic algorithm for feature selection and damage level estimation for grey relational analysis. The proposed method was calibrated firstly with <b>fault</b> <b>seeded</b> test data and then validated with the data of other tests from a planetary gear set. The estimation results of test data coincide with the actual test records, showing the effectiveness and accuracy of the method in providing a novel way to model based methods and feature selection and weighting methods for more accurate health monitoring and condition prediction...|$|R
40|$|In recent years, Service Oriented Architecture (SOA) {{has been}} {{increasingly}} adopted to develop {{applications in the}} context of Internet. To develop reliable SOA-based applications, an important issue is how to ensure the quality of Web services. In this paper, we propose a dynamic random testing (DRT) technique for Web services which is an improvement of the widely practiced random testing. We examine key issues when adapting DRT to the context of SOA and develop a prototype for such an adaptation. Empirical studies are reported where DRT is used to test two real-life Web services and mutation analysis is employed to measure the effectiveness. The experimental results show that DRT can save up to 24 % test cases in terms of detecting the first <b>seeded</b> <b>fault,</b> and up to 21 % test cases in terms of detecting all <b>seeded</b> <b>faults,</b> both with the cases of uniformed mutation analysis and distribution-aware mutation analysis, which refer to <b>faults</b> being <b>seeded</b> in an even or clustered way, respectively. The proposed DRT and the prototype provide an effective approach to testing Web Services. © 2012 IEEE. IEEE; IEEE Computer SocietyIn recent years, Service Oriented Architecture (SOA) has been increasingly adopted to develop applications {{in the context of}} Internet. To develop reliable SOA-based applications, an important issue is how to ensure the quality of Web services. In this paper, we propose a dynamic random testing (DRT) technique for Web services which is an improvement of the widely practiced random testing. We examine key issues when adapting DRT to the context of SOA and develop a prototype for such an adaptation. Empirical studies are reported where DRT is used to test two real-life Web services and mutation analysis is employed to measure the effectiveness. The experimental results show that DRT can save up to 24 % test cases in terms of detecting the first <b>seeded</b> <b>fault,</b> and up to 21 % test cases in terms of detecting all <b>seeded</b> <b>faults,</b> both with the cases of uniformed mutation analysis and distribution-aware mutation analysis, which refer to <b>faults</b> being <b>seeded</b> in an even or clustered way, respectively. The proposed DRT and the prototype provide an effective approach to testing Web Services. © 2012 IEEE...|$|R
40|$|The {{presented}} research {{resulted in}} a generic component taxonomy, a generic code-fault taxonomy, and an approach to tailoring the generic taxonomies into domain-specific as well as project-specific taxonomies. Also, a means to identify fault links was developed. Fault links represent relationships between the types of code-faults {{and the types of}} components being developed or modified. For example, a fault link has been found to exist between Controller modules (that forms a backbone for any software via. its decision making characteristics) and Control/Logic faults (such as unreachable code). The existence of such fault links can be used to guide code reviews, walkthroughs, testing of new code development, as well as code maintenance. It {{can also be used to}} direct <b>fault</b> <b>seeding.</b> The results of these methods have been validated. Finally, we also verified the usefulness of the obtained fault links through an experiment conducted using graduate students. The results were encouraging...|$|E
40|$|Regression {{test suite}} {{prioritization}} techniques reorder a test suite {{with the goal}} of ensuring that the reorganized test suite finds faults faster than the initial ordering. It is challenging to empirically evaluate the effectiveness of a new test case arrangement because existing metrics (i) require <b>fault</b> <b>seeding</b> or (ii) ignore test case costs. This paper presents a coverage effectiveness (CE) metric that (i) obviates the need to seed faults into the program under test and (ii) incorporates available data about test case execution times. A test suite is awarded a high CE value when it quickly covers the test requirements. It is possible to calculate coverage effectiveness regardless of the coverage criterion that is chosen to evaluate test case quality. The availability of an open source CE calculator enables future case studies and controlled experiments to use coverage effectiveness when evaluating different approaches to test suite prioritization...|$|E
40|$|The AH- 64 {{helicopter}} {{tail rotor}} gearbox is a grease lubricated right-angle transmission which frequently leaks through both its {{input and output}} seals. An experiment was designed to create a worst-case scenario for a leaking output seal on three different high-life gearboxes, which were to be run for 500 hours in a seeded fault condition. The test objective was to demonstrate that aircraft with leaking output seals could continue to operate until a scheduled phase maintenance which occurs every 250 hours. Although previously considered impossible, during the study it became evident that grease freely moves from the main gear compartment into the static mast. As a result, the output seal leaks caused lubricant starvation on the gear mesh surfaces, ultimately leading to catastrophic failures of the input gear teeth. The three gearboxes tested survived 490, 487, and 573 hours after <b>fault</b> <b>seeding,</b> and numerous vibration and thermal observations were recorded as the gearboxes approached failure...|$|E
40|$|Validation {{has emerged}} as a {{significant}} problem in the development of knowledgebased systems (KBS). Verification of KBS correctness and completeness has been cited {{as one of the most}} difficult aspects of validation. A number of software tools have been developed to perform such verification, but none of these are in widespread use. One of the reasons for this is that little quantitative evidence exists to demonstrate the effectiveness of the tools. This paper presents an experimental study of three KBS verification tools: a consistency checker, a completeness checker, and a testing tool (for correctness). The tools are evaluated on their ability to reveal plausible <b>faults</b> <b>seeded</b> into a complex, realistic KBS application. The cost of using the tools is also measured. It is shown that each tool is independently effective at detecting certain kinds of fault, and that the capabilities of the tools are complementary [...] - a result not revealed by previous studies...|$|R
6000|$|MRS. KARNEGIE was a {{woman of}} feeble {{intelligence}} and violent temper; prompt to take offense, and not, for the most part, easy to appease. But Mrs. Karnegie being--as we all are in our various degrees--a compound of many opposite qualities, possessed a character {{with more than one}} side to it, and had her human merits as well as her human <b>faults.</b> <b>Seeds</b> of sound good feeling were scattered away in the remoter corners of her nature, and only waited for the fertilizing occasion that was to help them to spring up. The occasion exerted that benign influence when the cab brought Mr. Crum's client back to the hotel. The face of the weary, heart-sick woman, as she slowly crossed the hall, roused all that was heartiest and best in Mrs. Karnegie's nature, and said to her, as if in words, [...] "Jealous of this broken creature? Oh, wife and mother is there no appeal to your common womanhood here?" ...|$|R
40|$|Controlled {{experiments}} in web application testing use <b>seeded</b> <b>faults</b> {{to evaluate the}} effectiveness of the testing technique. However, the classes of <b>seeded</b> <b>faults</b> are not always experimentally supported by real-world fault data. In this paper, we conduct an exploratory study on two large open source web systems to identify a fault classification that is representative of and supported by real world faults. Through our study we provide support to several categories of an existing web application fault classification, and identify one new fault category and six new sub-categories. Researchers and experimenters will find the proposed fault classification useful when evaluating techniques for testing web applications...|$|R
40|$|The {{increasing}} demand for reliable Web applications gives {{a central role}} to Web testing. Most of the existing works {{are focused on the}} definition of novel testing techniques, specifically tailored to the Web. However, no attempt was carried out so far to understand the specific nature of Web faults. This is of fundamental importance to assess the effectiveness of the proposed Web testing techniques. In this paper, we describe the process followed in the construction of a Web fault taxonomy. After the initial, top-down construction, the taxonomy was subjected to four iterations of empirical validation aimed at refining it and at understanding its effectiveness in bug classification. The final taxonomy is publicly available for consultation and editing on a Wiki page. Testers can use it in the definition of test cases that target specific classes of Web faults. Researchers can use it to build <b>fault</b> <b>seeding</b> tools that inject artificial faults which resemble the real ones...|$|E
40|$|Automated {{verification}} {{tools are}} capable of detecting subtle errors in models of complex software systems. Unfortunately, {{it can be difficult}} to use these tools effectively. Input models must correctly represent essential system behavior. Models must also enable efficient verification in terms of their time and memory requirements. Satisfying these two requirements can be challenging even for experts. But, given a correct model, can a user be confident that the verification results are correct? One way to assess correctness of verification results is to provide translation tools from a modeling language to the input languages of diverse verification tools. Our experiments with automatic translators from the SCR modeling language to the input languages of various verification tools show that verification results produced from automatically translated models are not always consistent. In the tradition of fault identification and fault tolerance provided through diversity, we argue that various verification tools need to be used in concert. Our <b>fault</b> <b>seeding</b> experiments indicate that the results of different model verification tools can be compared, their discrepancies analyzed and modeling faults corrected. Our experiments further imply that “ensemble verification ” should be a viable paradigm for the assessment of high assurance systems in the future. 1...|$|E
40|$|Fault links {{represent}} {{relationships between}} the types of code faults, or defects, {{and the types of}} components in which faults are detected. For example, our prior work validated that a fault link exists between Controller components and Control/Logic faults (such as unreachable code). Fault link information can guide code reviews, walkthroughs, testing, maintenance, and can advise <b>fault</b> <b>seeding.</b> In this paper, we use fault links to augment code reviews. Two experiments were undertaken to evaluate the usefulness of fault links, one with 26 Computer Science students and another with 24 software engineering professionals. The first experiment showed that fault link information assisted in finding more total defects and more ‘hard to detect ’ defects, in the same amount of time, in a Java component of an online course management application. The experiment was repeated with professionals, adding a second Java component from the same application. For the second experiment, more total defects were found by the participants using fault link information for one of the two components and more hard to detect defects were found, in the same amount of time, in both Java components. The group using fault link information for code walkthroughs found, on average, 1. 7 – 2 times more faults and 2 – 3 times more hard faults than the contro...|$|E
40|$|Nowadays, Service Oriented Architecture (SOA) {{has become}} one {{mainstream}} paradigm for developing distributed applications. As the basic unit in SOA, Web services can be composed to construct complex applications. The quality of Web services and their compositions {{is critical to the}} success of SOA applications. Testing, as a major quality assurance technique, is confronted with new challenges in the context of service compositions. In this paper, we propose a scenario-oriented testing approach that can automatically generate test cases for service compositions. Our approach is particularly focused on the service compositions specified by Business Process Execution Language for Web Services (WS-BPEL), a widely recognized executable service composition language. In the approach, a WS-BPEL service composition is first abstracted into a graph model; test scenarios are then derived from the model; finally, test cases are generated according to different scenarios. We also developed a prototype tool implementing the proposed approach, and an empirical study was conducted to demonstrate the applicability and effectiveness of our approach. The experimental results show that the automatic scenario-oriented testing approach is effective in detecting many types of <b>faults</b> <b>seeded</b> in the service compositions...|$|R
40|$|Some of {{the common}} faults {{associated}} with suspension components are damaged or leaking shock absorbers, spring weakness, wearing down of the pivot and bushing and damage to the main support member assembly. To investigate these problems, a seven degree-of-freedom (7 - DOF) model has been developed, for a full vehicle, using MATLAB. In the simulation, the suspension faults have been considered via the damage caused to the shock absorbers (dampers) and the <b>faults</b> were <b>seeded</b> by reducing the damper coefficient by 25...|$|R
40|$|Boolean {{expressions}} are extensively used {{in software}} specifications. It {{is important to}} generate a small-sized test set for Boolean expressions without sacrificing the fault-detection capability. MUMCUT is an efficient test case generation strategy for Boolean expressions in Irreducible Disjointed Normal Form (IDNF). In the real world, however, Boolean expressions written by a software designer or programmer are not normally in IDNF. In this paper, we apply MUMCUT to generate test cases for general Boolean expressions and develop a mutation-based empirical evaluation {{on the effectiveness of}} this application. The experimental data show that MUMCUT can still detect single <b>seeded</b> <b>faults</b> in up to 98. 20 % of general Boolean expressions. We also analyze patterns where test cases generated by MUMCUT cannot detect the <b>seeded</b> <b>faults...</b>|$|R
40|$|With {{the growth}} of {{complexity}} in modern automotive infotainment systems, graphical user interfaces {{become more and more}} sophisticated, and this leads to various challenges in software testing. Due to the enormous amount of possible interactions, test engineers have to decide, which test aspects to focus on. In this paper, we examine what types of failures can be found in graphical user interfaces of automotive infotainment systems, and how frequently they occur. In total, we have analyzed more than 3, 000 failures, found and fixed during the development of automotive infotainment systems at Audi, Bosch, and Mercedes-Benz. We applied the Orthogonal Defect Classification for categorizing these failures. The difficulties we faced when applying this classification led us to formulating requirements for an own classification scheme. On this basis, we have developed a hierarchical classification scheme for failures grounded on common concepts in software engineering, such as Model-View-Controller and Screens. The results of the application of our classification show that 62 % of the reports describe failures related to behavior, 25 % of the reports describe failures related to contents, 6 % of the reports describe failures related to design, and 7 % of the reports describe failures to be categorized. An outlined capability of the results is the support for <b>fault</b> <b>seeding</b> approaches which leads to the challenge of tracing the found failures to the correspondent faults...|$|E
40|$|There {{are many}} {{challenges}} in testing of Graphical User Interface (GUI) applications {{due to its}} event driven nature and infinite input domain. Testing each and every possible combination of input require creating number of test cases to satisfy the adequacy criteria of GUI testing. It {{is not possible to}} test each and every test case within specified time frame. Therefore it is important to assign higher priority to test cases which have higher fault revealing capability than other test cases. Various methods are specified in literature for test suite prioritization of GUI based software {{and some of them are}} based on interaction coverage and weight of events. Weight based methods are defined namely fault prone weight based method, random weight based method and equal weight based method in which fault prone based method is most effective. In this paper we have proposed Event-Coverage and Weight based Method (EC-WBM) which prioritizes GUI test cases according to their event coverage and weight value. Weight value will be assigned based on unique event coverage and fault revealing capability of events. Event coverage based method is used to evaluate the adequacy of test cases. EC-WBM is evaluated for 2 applications one is Notepad and another is Calculator. <b>Fault</b> <b>seeding</b> method is used to create number of versions of application and these faults are evaluated using APFD (Average percentage of fault detection). APFD for prioritized test cases of Notepad is 98 % and APFD for non-prioritized test cases is 62 %...|$|E
40|$|Web {{testing is}} {{assuming}} {{an increasingly important}} role in Web engineering, {{as a result of}} the quality demands put onto modern Web-based systems and of the complexity of the involved technologies. Most of the existing works in Web testing are focused on the definition of novel testing techniques, while only limited effort was devoted to understanding the specific nature of Web faults. However, the performance of a new Web testing technique is strictly dependent on the classes of Web faults it addresses. In this paper, we describe the process followed in the construction of a Web fault taxonomy. We used an iterative, mixed top-down and bottom-up approach. An initial taxonomy was defined by analyzing the high level characteristics of Web applications. Then the taxonomy was subjected to several iterations of empirical validation. During each iteration the taxonomy was refined by analyzing real faults and mapping them onto the appropriate categories. Metrics collected during this process were used to ensure that in the final taxonomy bugs distribute quite evenly among fault categories; fault categories are not-too-big, not-too-small and not ambiguous. Testers can use our taxonomy to define test cases that target specific classes of Web faults, while researchers can use it to build <b>fault</b> <b>seeding</b> tools, to inject artificial Web faults into benchmark applications. The final taxonomy is publicly available for consultation: since it is organized as a Wiki page, it is also open to external contributions. We conducted a case study in which test cases have been derived from the taxonomy for a sample Web application. The case study indicates that the proposed taxonomy is very effective in directing the testing effort toward those test scenarios that have higher chances of revealing Web specific faults...|$|E
40|$|This study {{investigated}} the method of identifying injector faults in a JCB 444 T 2 diesel engine using acoustic emission (AE) technique. Different kinds of injector <b>faults</b> were <b>seeded</b> in the four-cylinder, four-stroke, and turbo-engine. The AE signals recorded from the tests were processed in the angular domain, frequency and joint angular-frequency domain. The results showed that AE could clearly monitor the combustion process of diesel engine because high frequency AE signal measured from engine cylinder head has very high signal-to-noise ratio. Using features in the AE signal, faults of injector can be identified during {{the operation of the}} engine...|$|R
40|$|Access control {{policies}} are often specified in declarative languages. In this paper, we propose a novel approach, called mutation verification, {{to assess the}} quality of properties specified for a policy and, in doing so, {{the quality of the}} verification itself. In our approach, given a policy and a set of properties, we first mutate the policy to generate various mutant policies, each with a single <b>seeded</b> <b>fault.</b> We then verify whether the properties hold for each mutant policy. If the properties still hold for a given mutant policy, then the quality of these properties is determined to be insufficient in guarding against the <b>seeded</b> <b>fault,</b> indicating that more properties are needed to augment the existing set of properties to provide higher confidence of the policy correctness. We have implemented Mutaver, a mutation verification tool for XACML, and applied it to policies and properties from a real-world software system. 1...|$|R
40|$|Early {{detection}} of diesel engine faults {{is essential in}} order to take early correction actions and avoid costly repair. Injection faults due to defects in a fuel pump, fuel lines and injectors affect the power of the engine, increase the polluting particles in the exhausted gas and reduce the life cycle of the engine. High frequency AE signal measured from engine cylinder head has a very high signal-to-noise ratio and can be used to monitor the condition of engines. Three injection <b>faults</b> were <b>seeded</b> (injector pressure decrease, increase and injector blocked) in a four-stroke, four-cylinder diesel engine in the experimental study to investigate the potential of AE diagnostic technology...|$|R
40|$|The Vehicle Integrated Propulsion Research (VIPR) Phase III {{project was}} {{executed}} at Edwards Air Force Base, California, by the National Aeronautics and Space Administration and several industry, academic, and government {{partners in the}} summer of 2015. One of the research objectives was to use external radial acoustic microphone arrays to detect changes in the noise characteristics produced by the research engine during volcanic ash ingestion and <b>seeded</b> <b>fault</b> insertion scenarios involving bleed air valves. Preliminary results indicate the successful acoustic detection of suspected degradation as a result of cumulative exposure to volcanic ash. This detection is shown through progressive changes, particularly in the high-frequency content, as a function of exposure to greater cumulative quantities of ash. Additionally, detection of the simulated failure of the 14 th stage stability bleed valve and, to a lesser extent, the station 2. 5 stability bleed valve, to their fully-open fail-safe positions was achieved by means of spectral comparisons between nominal (normal valve operation) and <b>seeded</b> <b>fault</b> scenarios...|$|R
40|$|In {{this thesis}} we {{developed}} a new fault localization process to localize faults in object oriented software. The process is built upon the 2 ̆ 2 Encapsulation 2 ̆ 72 ̆ 7 principle and aims to locate state-dependent discrepancies in the software 2 ̆ 7 s behavior. We experimented with the proposed process on 50 <b>seeded</b> <b>faults</b> in 8 subject programs, {{and were able to}} locate the faulty class in 100...|$|R
40|$|Detecting <b>seeded</b> <b>faults</b> on a {{full-scale}} helicopter transmission {{is the focus}} of this work. Methods to isolate the dynamics of an individual sun gear, in an effort to as-sess its condition, are developed and validated on an OH- 58 helicopter transmission’s planetary reduction stage. This area {{has been shown to be}} challenging because the planetary system does not allow for direct measurements of the sun gear. Instead, special measurement and data processing techniques are needed to filter out the ef-fects of the planet gears, bearings, input spiral bevel stage, and other components in and around the gearbox. Planetary indexing is used to geometrically synchronize dy-namic measurements with the meshing tooth’s position along its pressure line. This provides the opportunity for source/signal mapping that can lead to increased sensi-tivity, allowing faults to be detected early and thus increasing the available time for corrective action. Accelerometers mounted along the transmission housing, acoustic transducers distributed about the test cell, and an oil debris monitoring system are all used to analyze three <b>seeded</b> <b>fault</b> cases. Transmission components, (two sun gears and...|$|R
40|$|This study {{focuses on}} {{investigation}} of the method of identifying injector faults in a JCB 444 T 2 diesel engine using acoustic emission (AE) technique. Different kinds of injector <b>faults</b> were <b>seeded</b> in the four-cylinder, four-stroke, and turbo-engine. Then, faulty injectors are tested to evaluate AE based injection fault detection. The AE signals recorded from the tests were processed in the angular, frequency and joint angular-frequency domain. The results from joint angular-frequency analysis have shown that AE can clearly monitor {{the changes in the}} combustion process due to its high signal to noise ratio, where other vibro-acoustic sources have little influence. Using features in the AE signal, faults of injector can be identified during the operation of the engine. </p...|$|R
40|$|To date, the {{majority}} of existing Condition Indicators for gears are based on various statistical moments of a recorded time history. A supplementary analysis proposed in this study, shall suggest an approach that may, in the future, enable the identification of faulty gearwheel and possibly fault type in the system. In this work, a combined analytical and empiric approach is applied. This approach {{is based on the}} assumption that reliable dynamic models can be utilized to predict the effects of faults on vibrational patterns. Dynamic model generated signatures are used to verify experimental findings. Moreover, discrepancies between simulated and actual results, combined with understanding of the assumptions and omissions of the model, are helpful in understanding and explaining the experimental results. A spur gear transmission setup was used for experiments, along with an electric AC motor and a friction belt loading device. The experimental runs were conducted at varying speed settings. Two types of faults, a tooth face fault and a tooth root <b>fault,</b> were <b>seeded</b> in the experimental transmission and into the model. The effect on extracted signal features is examined. The purpose of this study is to evaluate fault detection capabilities of proposed diagnostic tools at the presence of two <b>seeded</b> <b>faults</b> of varying severity, verified by a dynamic model. Observed differences between examined fault types and their manifestation will be discussed. A basis for future work on prognostics capabilities is laid by a varying degree of tooth root fault...|$|R
40|$|Self-adaptive {{software}} systems {{monitor their}} state and then adapt when certain conditions are met, {{guided by a}} global utility function. In prior work we developed algorithms and conducted a post-hoc analysis demonstrating the possibil-ity of adapting to software failures by judiciously changing configurations. In this paper we present the REFRACT framework that realizes this idea in practice by building on the self-adaptive Rainbow architecture. REFRACT ex-tends Rainbow with new components and algorithms tar-geting failure avoidance. We use REFRACT in a case study running four independently executing Firefox clients with 36 passing test cases and 7 <b>seeded</b> <b>faults.</b> The study show that workarounds for {{all but one of}} the <b>seeded</b> <b>faults</b> are found and the one that is not found never fails – it is guarded from failing by a related workaround. Moreover, REFRACT finds workarounds for eight configuration-related unseeded failures from tests that were expected to pass (and did under the default configuration). Finally, the data show that when a failure and its workaround are found, configuration guards prevent the failure from appearing again. In a simulation lasting 24 hours we see over 150 guard activations and no failures with workarounds remaining beyond 16 hours...|$|R
