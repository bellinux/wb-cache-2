103|1064|Public
30|$|By {{comparing}} {{the results from}} Figs.  9 and 10, the using of the instantaneous outputting voltage feedback method effectively inhibits the harmonic circulating current at different frequencies. When the outputting voltage is 4  Hz, the harmonic <b>distortion</b> <b>rate</b> of the outputting voltage is 5.05  % under the open-loop state, but the harmonic <b>distortion</b> <b>rate</b> is 2.32  % of the controlled by the instantaneous outputting voltage feedback. When the frequency increases, the harmonic distortion decreases. The harmonic <b>distortion</b> <b>rate</b> reaches the minimum value at frequency 50  Hz.|$|E
30|$|In the {{established}} simulation model of parallel inverter system, the harmonic <b>distortion</b> <b>rate</b> at the outputting voltage frequency values of 4, 10, 20, 41 and 50  Hz are calculated to verify inhibition {{effects of the}} outputting circulation current by the close-loop controller of the instantaneous voltage feedback. The harmonic <b>distortion</b> <b>rate</b> are all low than 2.32  % of the controlled by the instantaneous outputting voltage feedback.|$|E
40|$|Abstract This paper {{deals with}} the Harmonics on the power lines in the {{computer}} room. The voltage and current at each phase of electric power from UPS to the load must be ensured less than 5 % distortion rates according to the regulation terms from KEPC(Korea Electric Power Corporation) and the KSC 4310 Uninterruptible Power Supply regulation. However, measurements show that the voltage <b>distortion</b> <b>rate</b> conforms those regulations but the current <b>distortion</b> <b>rate</b> exceeds much the limit. This current distortion is stabilized fro...|$|E
30|$|There are {{overlaps}} {{between different}} frequency ranges {{that correspond to}} different selected scales. In these areas, a higher scale means higher compression ratios and higher <b>distortion</b> <b>rates,</b> whereas a lower scale means lower compression ratios and lower <b>distortion</b> <b>rates.</b> It is difficult to decide which choice is better. Therefore, both linear models that correspond to both scales {{should be used to}} select the wavelets. The better choice can be selected based on the criterion of the minimum CDCI.|$|R
30|$|The {{selection}} of a decomposition scale is the more critical factor in determining the compression performance and reconstruction accuracy. On a scale of 1 to 4, the compression ratios increase {{by a factor of}} 2 per level and can almost reach the limits. The stable values of the <b>distortion</b> <b>rates</b> are quite low. On the contrary, the stable value of <b>distortion</b> <b>rates</b> at scale 5 is much higher, but the compression ratios are not much higher than those of scale 4. The result is consistent with the choice made by the CDCI. The explanation is as follows. On a scale of 1 to 4, WCs are considered to consist only of trivial information, which is eliminated by thresholding. By contrast, the valuable information of the oscillation is kept in SCs without being destroyed. However, at scale 5, the valuable information is mixed in WCs and then destroyed by thresholding, resulting in high distortion. Moreover, the amplitude of the WCs caused by the oscillation is too large to be zero. As a result, the <b>distortion</b> <b>rates</b> are quite low, and the compression ratios can reach the limits on a scale of 1 to 4, whereas the <b>distortion</b> <b>rates</b> are high and the compression ratios cannot reach the limits at scale 5.|$|R
3000|$|The Symlets wavelet family {{shows the}} higher signal <b>distortion</b> <b>rating</b> (rating between: 3 – 4) {{indicating}} the fairly natural speech signal quality {{compared to other}} proposed and state-of-the art sensing matrices [...]...|$|R
3000|$|... by {{adjusting}} the number of data points, which {{is the number of}} image blocks by random sampling in our case, and the <b>distortion</b> <b>rate</b> [...]...|$|E
30|$|Indicators {{including}} the <b>distortion</b> <b>rate</b> (DR), marginal calibration, sharpness, and the continuous ranked probability score (CRPS) {{are applied to}} evaluate the distribution forecast performance of the proposed approach.|$|E
40|$|International audienceThis paper revisits {{earlier work}} on rate {{distortion}} behavior of sparse sources, namely it highlights {{the fact that}} a graphical sparsity characterization proposed in Weidmann: 00 b is a Lorenz curve, a tool for summarizing income inequality that has been used by economists for over a century. The Lorenz curve associated to a memoryless source can be used to obtain upper bounds on the <b>distortion</b> <b>rate</b> function, thus characterizing source compressibility. It is shown that an order relation on Lorenz curves induces an analogous relation on <b>distortion</b> <b>rate</b> upper bounds. This can be used to characterize the compressibility of certain parametric families of source distributions, for which an order on the parameters induces an order on Lorenz curves...|$|E
40|$|Quantum random variable, {{distortion}} operator {{are introduced}} based on canonical operators. As the lower bound of <b>rate</b> <b>distortion,</b> the entanglement information <b>rate</b> <b>distortion</b> {{is achieved by}} Gaussian map for Gaussian source. General Gaussian maps are further reduced to unitary transformations and additive noises from the physical meaning of distortion. The entanglement information <b>rate</b> <b>distortion</b> function then are calculated for one mode Gaussian source. The <b>rate</b> <b>distortion</b> is accessible at zero distortion point. For pure state, the <b>rate</b> <b>distortion</b> function is always zero. In contrast to the distortion defined via fidelity, our definition of the distortion {{makes it possible to}} calculate the entanglement information <b>rate</b> <b>distortion</b> function for Gaussian source...|$|R
40|$|Quantum random variable, {{distortion}} operator {{are introduced}} based on canonical operators. As the lower bound of <b>rate</b> <b>distortion,</b> the entanglement information <b>rate</b> <b>distortion</b> {{is achieved by}} Gaussian map for Gaussian source. General Gaussian maps are further reduced to unitary transformations and additive noises from the physical meaning of distortion. The entanglement information <b>rate</b> <b>distortion</b> function then are calculated for one mode Gaussian source. The <b>rate</b> <b>distortion</b> is accessible at zero distortion point. For pure state, the <b>rate</b> <b>distortion</b> function is always zero. In contrast to the distortion defined via fidelity, our definition of the distortion {{makes it possible to}} calculate the entanglement information <b>rate</b> <b>distortion</b> function for Gaussian source. Comment: 6 page, 1 figur...|$|R
30|$|The db 5, db 9, db 10, coif 3, coif 4, coif 5 and Symlets wavelet {{families}} {{shows the}} good background <b>distortion</b> <b>rating</b> (between rating: 2 – 3) indicating noticeable noise, but not intrusive and are close comparable to state-of-the art sensing matrices.|$|R
40|$|One of {{the many}} {{advantages}} of low pressure carburizing (LPC) {{is that it can}} be combined with high-pressure gas quenching. This makes it possible to achieve workpieces with pure metallic surfaces, less distortion hardening, and, above all, it allows a more reliable and repeatable treatment than conventional oil quenching. This article presents a study of the <b>distortion</b> <b>rate</b> of workpieces carburized at low pressure then quenched in nitrogen at 1. 4 MPa. By comparing the distortion which takes place during the carburizing stage only and the carburizing combined with post-carburizing heat treatment {{it will be possible to}} assess the <b>distortion</b> <b>rate</b> and its causes at the different stages of the heat treatment process...|$|E
40|$|This paper derives bounds on the <b>distortion</b> <b>rate</b> {{function}} for quantization on the complex projective space denoted as CP n− 1. In essence {{the problem of}} quantization in an Euclidean space with constraints can be posed as an unconstrained problem on an appropriate manifold. CP n− 1 is a non-linear manifold that represents the constraints that arise {{in areas such as}} communication with multiple antennas at the transmitter and receiver. Due to the constraints, the <b>distortion</b> <b>rate</b> analysis developed for Euclidean spaces cannot be applied directly. The special structure of CP n− 1 and the distortion measures that are defined on it differentiate this problem from traditional vector quantization in Euclidean spaces. I...|$|E
40|$|This paper {{characterizes the}} effect of finite rate channel state {{feedback}} on the sum rate of a multi-access multiple-input multiple-output (MIMO) system. We propose to control the users jointly, specifically, we first choose the users jointly and then select the corresponding beamforming vectors jointly. To quantify the sum rate, this paper introduces the composite Grassmann manifold and the composite Grassmann matrix. By characterizing the <b>distortion</b> <b>rate</b> function on the composite Grassmann manifold and calculating the logdet function of a random composite Grassmann matrix, a good sum rate approximation is derived. According to the <b>distortion</b> <b>rate</b> function on the composite Grassmann manifold, the loss due to finite beamforming decreases exponentially as the feedback bits on beamforming increases. Comment: 10 pages. In Proc. Allerton Conf. on Commun., Control, and Computing, 200...|$|E
40|$|With the {{tremendous}} growth in imaging applications {{and the development}} of filmless radiology, the need for compression techniques which can achieve high compression ratios with user specified <b>distortion</b> <b>rates</b> become necessary. Boundaries and edges in the tissue structures are vital for detection of lesions and tumors, which in turn requires the preservation of edges in the image. Unlike existing lossy transform-based compression techniques such as FFT and DCT, edge preservation is addressed in this new compression scheme. The proposed Edge Preserving Image Compressor (EPIC) combines lossless compression of edges with neural network compression techniques based on Dynamic Associative Neural Networks (DANN), to provide high compression ratios with user specified <b>distortion</b> <b>rates</b> in an adaptive compression system well-suited to parallel implementations. Improvemen [...] ...|$|R
40|$|Although Shannon {{introduced}} {{the concept of}} a <b>rate</b> <b>distortion</b> function in 1948, only in the last decade has the methodology for developing <b>rate</b> <b>distortion</b> function lower bounds for real-world sources been established. However, these recent results have not been fully exploited due to some confusion about how these new <b>rate</b> <b>distortion</b> bounds, once they are obtained, should be interpreted and should be used in source codec performance analysis and design. We present the relevant <b>rate</b> <b>distortion</b> theory and show how this theory can be used for practical codec design and performance prediction and evaluation. Examples for speech and video indicate exactly how the new <b>rate</b> <b>distortion</b> functions can be calculated, interpreted, and extended. These examples illustrate the interplay between source models for <b>rate</b> <b>distortion</b> theoretic studies and the source models underlying video and speech codec design. Key concepts include the development of composite source models per source realization and the application of conditional <b>rate</b> <b>distortion</b> theory...|$|R
40|$|We {{show that}} the Lagrange dual {{problems}} of the channel capacity problem with input cost and the <b>rate</b> <b>distortion</b> problem are simple geometric programs. Upper bounds on channel capacity and lower bounds on <b>rate</b> <b>distortion</b> can be efficiently generated from their duals. For channel capacity, the geometric programming dual characterization is shown to be equivalent to the minmaxKL characterization in [9, 13]. For <b>rate</b> <b>distortion,</b> the geometric programming dual is extended to <b>rate</b> <b>distortion</b> with two-sided state information. A 'duality&apos...|$|R
30|$|From the {{performance}} experiment results, {{the accuracy of}} the method for detecting and identifying English in natural scenes is 99.3 %, which is less than 100 %. Therefore, further research and improvement are needed. From the current status quo, compared with the current advanced genetic neural network algorithm recognition results, the algorithm leads the genetic neural network algorithm in recognition speed, accuracy, image sharpness, image de-drying rate, and image <b>distortion</b> <b>rate.</b> First of all, in the recognition speed, the algorithm is far ahead of the genetic neural network algorithm, and secondly, it is close to 100 % accuracy, which can be initially applied to practice. In addition, the algorithm can ensure the image <b>distortion</b> <b>rate</b> is low after recognition, ensure the image has certain clarity, and effectively eliminate the noise in image recognition. Therefore, it is a good image recognition algorithm for images.|$|E
30|$|The rules {{indicated}} in Fig.  5 {{are similar to}} those in Fig.  3. ① There is little difference between the performance of dbN and symN. ② The selection of a decomposition scale is more important than that of the wavelet function. On a scale of 3 to 5, the distortion rates are too high to accept. By contrast, at scale 2, the distortion rates are low, and the compression ratios are high enough. This result is consistent with the choice made by the CDCI. ③ The order of the wavelet has very little effect on the performance if the <b>distortion</b> <b>rate</b> reaches the stable value. At scale 2, the <b>distortion</b> <b>rate</b> reaches the stable value at about order 8 or higher, whereas the performance difference between order 7 to order 10 is small. The result is consistent with the choice made by the CDCI.|$|E
40|$|This paper {{presents}} a preliminary study on biometric watermarking by using offline handwritten signature. Specifically, offline handwritten signature is discritized into binary bit string as hidden biometric watermark. It is reconstructed subsequently {{with the intention}} to authenticate the claimed source of the digital document In this paper, {{we focus on the}} study in examining the strength and robustness of three selected biometric watermarking schemes, namely Least Significant Bit (LSB) substitution, CDMA spread spectrum in spatial domain and CDM-A spread spectrum in Discrete Wavelet Transform (DWT), against JPG compression. We check the <b>distortion</b> <b>rate</b> for the original and the extracted biometric bit string after JPG compression. The performance of the selected biometric watermarking schemes is validated based on human visual inspection, Peak Signal to Noise Ratio (PSNR) and <b>distortion</b> <b>rate</b> (normalized Hamming distance). Experiment results show that CDMA spread spectrum in DWT is the most promising scheme in biometric watermarking...|$|E
40|$|Quantum random variable, {{distortion}} operator {{are introduced}} based on canonical operators. As the lower bound of <b>rate</b> <b>distortion,</b> the entanglement information <b>rate</b> <b>distortion</b> {{is achieved by}} Gaussian map for Gaussian source. General Gaussian maps are further reduced to unitary transformations and additive noises from the physical meaning of distortion. The entanglement information <b>rate</b> <b>distortion</b> function then are calculated for one mode Gaussian source. The <b>rate</b> <b>distortion</b> is accessible at zero distortion point. For pure state, the <b>rate</b> <b>distortion</b> function is always zero. In contrast to the distortion defined via fidelity, our definition of the distortion {{makes it possible to}} calculate the entanglement information <b>rate</b> <b>distortion</b> function for Gaussian source. Distortion operator: Two major parts in classical information theory are channel capacity and <b>rate</b> <b>distortion</b> theory. They concern respectively with the reliability and effectiveness of information transmission. In quantum information theory, channel capacity has been widely investigated, but little effort has been put into developing quantum rate-distortion theory[1][2]. It was proven [1] that the quantum rate-distortion function R(D) is lower bounded by entanglement information rate-distortion function R I (D). For a given source R I (D) is defined by R I (D) = mi...|$|R
5000|$|... #Caption: An {{example of}} the {{modifiable}} areal unit problem and the <b>distortion</b> of <b>rate</b> calculations ...|$|R
40|$|No. {{of pages}} in text: 51. With the {{tremendous}} growth in imaging applications {{and the development}} of filmless radiology, the need for compression techniques which can achieve high compression ratios with user specified <b>distortion</b> <b>rates</b> become necessary. Boundaries and edges in the tissue structures are vital for detection of lesions and tumors, which in turn requires the preservation of edges in the image. Unlike existing lossy transform-based compression techniques such as FFT and DCT, edge preservation is addressed in this new compression scheme. The proposed Edge Preserving Image Compressor (EPIC) combines lossless compression of edges with neural network compression techniques based on Dynamic Associative Neural Networks (DANN), to provide high compression ratios with user specified <b>distortion</b> <b>rates</b> in an adaptive compression system well-suited to parallel implementations. Improvements to DANN-based training {{through the use of a}} variance classifier for controlling a bank of neural networks speed convergence and allow the use of higher compression ratios for “simple ” patterns. The adaptation and generalization capabilities inherent in EPIC also facilitate progressive transmission of images throug...|$|R
30|$|To sum up, the {{proposed}} piecewise linear model-based method is {{more suitable for}} the compression of oscillations in storage than the ESDC method. Specifically, the amount of computation of the ESDC method is much {{less than that of}} {{the proposed}} method, whereas the <b>distortion</b> <b>rate</b> of the proposed method is lower than that of the ESDC method. The compression ratios of the two techniques are close to each other. The ESDC method was proposed for real-time applications, thus requiring a small amount of computation. By contrast, the proposed method is used for storage, and thus can afford a heavy computational burden. Therefore, the performance of the proposed method is better than that of the ESDC method in applications of data storage owing to the low <b>distortion</b> <b>rate.</b> Moreover, the parameters of the ESDC method are set based on experience. By contrast, the wavelet and scale can be directly selected based on the oscillation frequency in the proposed method.|$|E
40|$|A pruning {{algorithm}} of Chou et al. (1989) {{for designing}} optimal tree structures identifies only those codebooks which {{lie on the}} convex hull of the original codebook's operational <b>distortion</b> <b>rate</b> function. The authors introduce {{a modified version of}} the original algorithm, which identifies a large number of codebooks having minimum average distortion, under the constraint that, in each step, only modes having no descendents are removed from the tree. All codebooks generated by the original algorithm are also generated by this algorithm. The new algorithm generates a much larger number of codebooks in the middle- and low-rate regions. The additional codebooks permit operation near the codebook's operational <b>distortion</b> <b>rate</b> function without time sharing by choosing from the increased number of available bit rates. Despite the statistical mismatch which occurs when coding data outside the training sequence, these pruned codebooks retain their performance advantage over full search vector quantizers (VQs) for a large range of rates...|$|E
40|$|Abstract — This paper proposes an {{approach}} for pixel {{transformation of the}} displayed image to increase the potential energy saving of the backlight scaling method. The proposed approach takes advantage of human visual system characteristics and tries to minimize distortion between the perceived brightness values of the individual pixels in the original image {{and those of the}} backlight-scaled image. This is in contrast to previous backlight scaling approaches which simply match the luminance values of the individual pixels in the original and backlight-scaled images. Moreover, the proposed dynamic backlight scaling approach, which is based on tone mapping, is amenable to highly efficient hardware realization because it does not need information about the histogram of the displayed image. Experimental results show that the dynamic tone mapping for backlight scaling method results in about 35 % power saving with an effective <b>distortion</b> <b>rate</b> of 5 % and 55 % power saving for a 20 % <b>distortion</b> <b>rate...</b>|$|E
40|$|We {{show that}} the Lagrange dual {{problems}} of the channel capacity problem with input cost and the <b>rate</b> <b>distortion</b> problem are simple geometric programs. Upper bounds on channel capacity and lower bounds on <b>rate</b> <b>distortion</b> can be efficiently generated from their duals. For channel capacity, the geometric programming dual characterization is shown to be equivalent to the minmax Kullback–Leibler (KL) characterization in [10], [14]. For <b>rate</b> <b>distortion,</b> the geometric programming dual is extended to <b>rate</b> <b>distortion</b> with two-sided state information. A “duality by mapping ” is then given between the Lagrange dual problems of channel capacity with input cost and <b>rate</b> <b>distortion,</b> which resolves several apparent asymmetries between their primal problems in the familiar form of mutual information optimization problems. Both the primal and dual problems can be interpreted in a common framework of free energy optimization from statistical physics...|$|R
40|$|A {{spontaneous}} symmetry breaking {{argument is}} applied to the problem of protein form, via a <b>Rate</b> <b>Distortion</b> analysis of the relation between genome coding and the final condensation of the protein &#x 27;molten globule&#x 27;. The <b>Rate</b> <b>Distortion</b> Function, under coding constraints, serves as a temperature analog, so that low values act to drive proteins to simple symmetries. The <b>Rate</b> <b>Distortion</b> Function itself is significantly constrained by the availability of metabolic free energy. This work extends Tlusty&#x 27;s (2007) elegant exploration of the evolution of the genetic code, suggesting that <b>rate</b> <b>distortion</b> considerations may play a critical role across a broad spectrum of molecular expressions of evolutionary process...|$|R
40|$|In this paper, {{we propose}} a {{simultaneous}} rate control and video de-noising algorithm based on <b>rate</b> <b>distortion</b> optimization. According to our previous works [1] [2], video de-noising {{can be performed}} by using <b>rate</b> <b>distortion</b> optimization with a lower bound quantization parameter (QP) constraint, where the lower bound QP {{is determined by the}} noise variance. Then, we find that the macroblock level rate control method in H. 264 can be seen as an approximate solution of a <b>rate</b> <b>distortion</b> optimization problem with a specified <b>rate</b> <b>distortion</b> function. Based on these two studies, we integrate the video de-noising problem and rate control problem to a <b>rate</b> <b>distortion</b> optimization problem. We show the convexity of the problem and derive the optimal solution. To reduce the complexity, we propose to use a suboptimal solution based on simply thresholding. Some experiments are conducted to demonstrate the efficiency and effectiveness of the proposed method...|$|R
40|$|AbstractThe {{relationships}} between a slip {{system in the}} parent lattice and its transform by twinning shear are considered in regards to tangential continuity conditions on the plastic <b>distortion</b> <b>rate</b> at twin/parent interface. These conditions are required at coherent interfaces like twin boundaries, which can be assigned zero surface-dislocation content. For two adjacent crystals undergoing single slip, relations between plastic slip rates, slip directions and glide planes are accordingly deduced. The fulfillment of these conditions is investigated in hexagonal lattices {{at the onset of}} twinning in a single slip deforming parent crystal. It is found that combinations of slip system and twin variant verifying the tangential continuity of the plastic <b>distortion</b> <b>rate</b> always exist. In all cases, the Burgers vector belongs to the interface. Certain twin modes are only admissible when slip occurs along an 〈a〉 direction of the hexagonal lattice, and some others only with a 〈c+a〉 slip. These predictions are in agreement with EBSD orientation measurements in commercially pure Ti sheets after plane strain compression...|$|E
40|$|ABSTRACT- In this paper, {{a method}} is {{proposed}} for finding a pixel transformation function that maximizes backlight dimming {{while maintaining a}} pre-specified image distortion level for a liquid crystal display. This is achieved by finding a pixel transformation function, which maps the original image histogram to a new histogram with lower dynamic range. Next the contrast of the transformed image is enhanced so as to compensate for brightness loss that would arise from backlight dimming. The proposed approach relies on an accurate definition of the image distortion which takes into account both the pixel value differences and {{a model of the}} human visual system and is amenable to highly efficient hardware realization. Experimental results show that the histogram equalization for backlight scaling method results in about 45 % power saving with an effective <b>distortion</b> <b>rate</b> of 5 % and 65 % power saving for a 20 % <b>distortion</b> <b>rate.</b> This is significantly higher power savings compared to previously reported backlight dimming approaches. ...|$|E
40|$|Abstract- This paper proposes an {{approach}} for pixel {{transformation of the}} displayed image to increase the potential energy saving of the backlight scaling method. The proposed approach takes advantage of human visual system characteristics and tries to minimize distortion between the perceived brightness values of the individual pixels in the original image {{and those of the}} backlight-scaled image. This is in contrast to previous backlight scaling approaches which simply match the luminance values of the individual pixels in the original and backlight-scaled images. Moreover, the proposed dynamic backlight scaling approach, which is based on tone mapping, is amenable to highly efficient hardware realization because it does not need information about the histogram of the displayed image. Experimental results show that the dynamic tone mapping for backlight scaling method results in about 35 % power saving with an effective <b>distortion</b> <b>rate</b> of 5 % and 55 % power saving for a 20 % <b>distortion</b> <b>rate...</b>|$|E
40|$|Abstract—It is {{well-known}} {{that the information}} bottleneck method and <b>rate</b> <b>distortion</b> theory are related. Here it is described how the information bottleneck {{can be considered as}} <b>rate</b> <b>distortion</b> theory for a family of probability measures where information divergence is used as distortion measure. It is shown that the information bottleneck method has some properties that are not shared with <b>rate</b> <b>distortion</b> theory based on any other divergence measure. In this sense the information bottleneck method is unique. I...|$|R
40|$|Abstract—This paper {{discusses}} optimal wavelet packet basis selection within JPEG 2000. Three algorithms for <b>rate</b> <b>distortion</b> optimal wavelet packet basis {{selection in}} JPEG 2000 are presented. The first approach considers the JPEG 2000 packet body {{data in the}} <b>rate</b> <b>distortion</b> optimization only, while the other techniques additionally integrate packet header data. The algorithms are evaluated {{on a wide range}} of highly textured image data. Results demonstrate that inclusion of header data information into <b>rate</b> <b>distortion</b> optimization leads to superior compression results. For the first time the maximum performance gains of custom isotropic wavelet packets in JPEG 2000 can be assessed. Index Terms—Image compression, JPEG 2000, wavelet packet bases, <b>rate</b> <b>distortion</b> optimization I...|$|R
40|$|Thesis (M. S.) [...] Wichita State University, College of Engineering, Dept. of Electrical and Computer EngineeringIncludes bibliographic {{references}} (leaves 28 - 31) <b>Rate</b> <b>Distortion</b> {{analysis is}} {{a branch of}} information theory that predicts the tradeoffs between <b>rate</b> and <b>distortion</b> in source coding. In this thesis, we present the <b>rate</b> <b>distortion</b> analysis for conditional motion estimation, a process that estimates motion based on a criterion that affects coding rate, complexity of coding scheme {{and quality of the}} reconstructed video. In order to guide the <b>rate</b> <b>distortion</b> analysis, we use a conditional motion estimation scheme that estimates motion for certain blocks selected based on significant changes. We begin by explaining the conditional motion estimation technique and the effect of decision criteria on the technique. We then model the motion vectors as Gaussian-Markov process and study the <b>rate</b> <b>distortion</b> tradeoffs in the video encoding scheme. The <b>rate</b> <b>distortion</b> bound derived in this manner is also validated with a practical approach...|$|R
