1271|10000|Public
2500|$|In some {{implementations}} {{an extra}} 'sentinel' or 'dummy' node may be added {{before the first}} <b>data</b> <b>record</b> or after the last one. This convention simplifies and accelerates some list-handling algorithms, by ensuring that all links can be safely dereferenced and that every list (even one that contains no data elements) always has a [...] "first" [...] and [...] "last" [...] node.|$|E
2500|$|The term leaf is also inconsistent. [...] {{considered}} the leaf level {{to be the}} lowest level of keys, but Knuth {{considered the}} leaf level to be one level below the lowest keys [...] There are many possible implementation choices. In some designs, the leaves may hold the entire data record; in other designs, the leaves may only hold pointers to the <b>data</b> <b>record.</b> Those choices are not fundamental {{to the idea of a}} B-tree.|$|E
5000|$|Cancel <b>Data</b> <b>Record</b> Packet - This message {{orders the}} CGF to remove {{one or more}} <b>Data</b> <b>Record</b> Packet from the CGF [...] "possibly duplicated" [...] pending queue.|$|E
30|$|Large {{quantities}} of angiosperm <b>data</b> <b>records</b> were utilized to analyze floristic relationships and to evaluate geographical distribution ranges of plants. The angiosperm <b>data</b> <b>records</b> {{of all the}} countries in East and South Asia were downloaded from the Global Biodiversity Information Facility (GBIF, [URL] More than 4.5 millions <b>data</b> <b>records</b> were collected. The <b>data</b> <b>records</b> of angiosperms were mapped by Diva-GIS (Hijmans et al. 2001). <b>Data</b> <b>records</b> with incorrect values of latitude and longitude or without scientific names were dropped out from the analysis.|$|R
5000|$|...Notes:*: <b>Data</b> <b>recorded</b> is from January to August 2013*: <b>Data</b> <b>recorded</b> is from January to July 2013 ...|$|R
30|$|Figures  4 and 5 show a {{comparison}} of QL(predicted) versus QL(measured) for each data point training subset (127 randomly selected <b>data</b> <b>records)</b> and testing subset (55 randomly selected <b>data</b> <b>records),</b> respectively, of the Reshadat oil field dataset (total of 182 wellhead test <b>data</b> <b>records).</b>|$|R
50|$|The <b>Data</b> <b>Record</b> Transfer Response {{acknowledges}} {{receipt of}} one or more <b>Data</b> <b>Record</b> Transfer messages; responses can be grouped for reasons of efficiency but must be sent more frequently than the sending CDFs timeout.|$|E
5000|$|Pricing (first-year cost, ongoing cost, custom {{programming}} charges, <b>data</b> <b>record</b> storage fees); ...|$|E
50|$|The {{creations of}} these {{functions}} can be automated by Haskell's <b>data</b> <b>record</b> syntax.|$|E
40|$|A {{large amount}} of {{information}} on the Web is contained in regularly structured objects, which we call <b>data</b> <b>records.</b> Such <b>data</b> <b>records</b> are important because they often present the essential information of their host pages, e. g., lists of products or services. It is useful to mine such <b>data</b> <b>records</b> in order to extract information from them to provide value-added services. Existing automatic techniques are not satisfactory because of their poor accuracies. In this paper, we propose a more effective technique to perform the task. The technique is based on two observations about <b>data</b> <b>records</b> on the Web and a string matching algorithm. The proposed technique is able to mine both contiguous and noncontiguous <b>data</b> <b>records.</b> Our experimental results show that the proposed technique outperforms existing techniques substantially. Categories and Subject Descriptors I. 5 [Pattern Recognition]: statistical and structural H. 2. 8 [Database Applications]: data mining Keywords Web <b>data</b> <b>records,</b> Web mining, Web information integration 1. ...|$|R
30|$|The {{angiosperm}} <b>data</b> <b>records</b> of GBIF were downloaded by {{the countries}} in Asia. The countries in continental Asia include Russia, Mongolia, Korea, China, India, countries in Indochina, and Malaysia. The temperate islands belong to Japan and tropical islands include Philippine, Sumatra, Java, Borneo, Sulawesi, and New Guinea. More than 4.5 millions of angiosperm <b>data</b> <b>records</b> were downloaded from the GBIF. The boundaries of countries in Asia are {{different from that of}} ecoregions. In order to make angiosperm lists of ecoregions, angiosperm <b>data</b> <b>records</b> within an ecoregion were extracted from different countries. The angiosperm records extracted from different countries were manipulated by ArcMap software (ESRI, Redlands, USA). The extracted <b>data</b> <b>records</b> were incorporated into an ecoregion. The incorporated <b>data</b> <b>records</b> of ecoregions were mapped, checked and revised by Diva-GIS (Hijmans et al. 2001) and were utilized to generate angiosperm lists of ecoregions. Numbers of <b>data</b> <b>records</b> and numbers of angiosperm lists of ecoregions are listed in Table  1.|$|R
30|$|The {{extent of}} the {{collected}} content <b>data</b> <b>records</b> {{is related to the}} honeypot’s level of interaction. Low-interaction honeypots capture and collect smaller amounts of content <b>data</b> <b>records</b> than medium-interaction and high-interaction honeypots.|$|R
5000|$|Each <b>Data</b> <b>Record</b> Transfer Request message {{can contain}} {{a message of}} one of four types: ...|$|E
5000|$|Determine the {{variability}} of ocean circulation at decadal time scales from combined <b>data</b> <b>record</b> of TOPEX/Poseidon and Jason-1 ...|$|E
50|$|To {{be able to}} {{recognize}} a person by biometric characteristics and derived biometric features, a learning phase must first take place. The procedure is called enrolment and comprises the creation of an enrolment <b>data</b> <b>record</b> of the biometric data subject (the person to be enrolled) and its storage in a biometric enrolment database. The enrolment <b>data</b> <b>record</b> comprises one or multiple biometric references and arbitrary non-biometric data such as a name or a personnel number.|$|E
5000|$|... #Caption: The {{worldwide}} {{heat map}} from the NSA's data visualisation tool BOUNDLESSINFORMANT, showing that during a 30-day period, 97 billion internet <b>data</b> <b>records</b> (DNI) and 124 billion telephony <b>data</b> <b>records</b> (DNR) were collected.|$|R
5000|$|Another {{burst from}} the source was later found in <b>data</b> <b>recorded</b> September 28, 2003, and a weaker burst was found in <b>data</b> <b>recorded</b> March 20, 2004. [...] As of January 2007, no other bursts have been found.|$|R
50|$|<b>Data</b> <b>Records</b> is a {{subsidiary}} record label of Ministry of Sound based in London. <b>Data</b> <b>Records,</b> along with Ministry sub-labels Sound of Ministry and Rulin Records since 1999, have been distributed in Australia by Ministry of Sound Australia.|$|R
50|$|Pseudonymization is a {{procedure}} {{by which the}} most identifying fields within a <b>data</b> <b>record</b> are replaced {{by one or more}} artificial identifiers, or pseudonyms. There can be a single pseudonym for a collection of replaced fields or a pseudonym per replaced field. The purpose is to render the <b>data</b> <b>record</b> less identifying and therefore lower customer or patient objections to its use. Data in this form is suitable for extensive analytics and processing.|$|E
5000|$|Merge <b>data</b> <b>record</b> {{directly}} with SQL statements (several Databases and data Sources are supported in native, other can be add using plug-ins).|$|E
50|$|TSB {{disclosed}} at a {{news conference}} on 26 March that the flight <b>data</b> <b>record</b> indicated that oil pressure was lost, but that there was no anomaly other than the broken stud to explain that loss. The aircraft descended at 1000 ft/min m/s. The aircraft lost electrical power, interrupting the <b>data</b> <b>record.</b> Damage analysis indicated that it struck the water belly-down and tail first with an acceleration of 20 g (20 times that of the Earth's gravity).|$|E
50|$|In most cases, {{an index}} {{is used to}} quickly locate the <b>data</b> <b>record(s)</b> from which the {{required}} data is read. In other words, the index is only used to locate <b>data</b> <b>records</b> in the table and not to return data.|$|R
25|$|The link fields {{need not}} be {{physically}} part of the nodes. If the <b>data</b> <b>records</b> are stored in an array and referenced by their indices, the link field may be stored in a separate array with the same indices as the <b>data</b> <b>records.</b>|$|R
40|$|This paper {{studies the}} problem of extracting data from a Web page that {{contains}} several structured <b>data</b> <b>records.</b> The objective is to segment these <b>data</b> <b>records,</b> extract <b>data</b> items/fields from them and put the data in a database table. This problem has been studied by several researchers. However, existing methods still have some serious limitations. The first class of methods is based on machine learning, which requires human labeling of many examples from each Web site that one is interested in extracting data from. The process is time consuming due to {{the large number of}} sites and pages on the Web. The second class of algorithms is based on automatic pattern discovery. These methods are either inaccurate or make many assumptions. This paper proposes a new method to perform the task automatically. It consists of two steps, (1) identifying individual <b>data</b> <b>records</b> in a page, and (2) aligning and extracting data items from the identified <b>data</b> <b>records.</b> For step 1, we propose a method based on visual information to segment <b>data</b> <b>records,</b> which is more accurate than existing methods. For step 2, we propose a novel partial alignment technique based on tree matching. Partial alignment means that we align only those data fields in a pair of <b>data</b> <b>records</b> that can be aligned (or matched) with certainty, and make no commitment {{on the rest of the}} data fields. This approach enables very accurate alignment of multiple <b>data</b> <b>records.</b> Experimental results using a large number of Web pages from diverse domains show that the proposed two-step technique is able to segment <b>data</b> <b>records,</b> align and extract data from them very accurately...|$|R
50|$|The <b>Data</b> <b>Record</b> Transfer {{messages}} {{are used to}} reliably transport CDRs {{from the point of}} generation (SGSN/GGSN) to non-volatile storage in the CGF.|$|E
50|$|Scientists {{consider}} the 15-plus-year climate <b>data</b> <b>record</b> that this mission will extend critical understanding how ocean circulation {{is linked to}} global climate change.|$|E
5000|$|This script above will add, to each <b>data</b> <b>record</b> {{passing through}} its component, a {{property}} named Hello containing the string [...] "Hello World!".|$|E
40|$|<b>Data</b> <b>recording</b> {{system in}} the {{assembling}} area which still done manually in many cases, is deemed to be ineffective because {{it will take a}} longer time to process the data. Moreover, the process of <b>recording</b> <b>data</b> manually tends to have a higher potential of error, because the process relies heavily on the accuracy of the operators who perform the <b>data</b> <b>recording.</b> Therefore, it is necessary to process the <b>data</b> <b>recording</b> automatically. The purpose of the implementation of this research is to design a model of a simplification of the real {{system in the}} manufacturing industry into a laboratory-scale system Department of Industrial Engineering University of Muhammadiyah Surakarta, identifying the system requirements for the application of automated <b>data</b> <b>recording</b> system (automatic <b>data</b> capture) using RFID, as well as designing activities simulation <b>data</b> <b>recording</b> automatically (automatic <b>data</b> capture) using RFID, which is carried out at the Laboratory of Industrial Engineering Department of the University of Muhammadiyah Surakarta. Research on automatic <b>data</b> <b>recording</b> system is implemented by modeling the system. Modeling process begins by conceptualizing the model, namely the inductive approach, by observing a number of symptoms individually, and then formulates it in draft form. Similarly, with this study, in which symptoms occur in assembling activities in a manufacturing industry becomes a reference of the model to be created. After successfully model the state of the real system, then the stages of verification and validation of the model. Results of this study were successfully modeled 4 types of automated <b>data</b> <b>recording</b> system. Such as determining the type of major components, determining the type of supporting components, determining the number of components in the store, as well as error detecting while input the component. Fourth system is then designed into a program using RFID automatic <b>data</b> <b>recording.</b> Keywords...|$|R
40|$|A {{large amount}} of {{information}} on the Web is contained in regularly structured objects, which we call <b>data</b> <b>records.</b> Such <b>data</b> <b>records</b> are important because they often present the essential information of their host pages, e. g., lists of products and services. It is useful to mine such <b>data</b> <b>records</b> in order to extract information from them to provide value-added services. Existing approaches to solving this problem mainly include the manual approach, supervised learning, and automatic techniques. The manual method is not scalable to a large number of pages. Supervised learning needs manually prepared positive and negative training data and thus also require substantial human effort. Current automatic techniques are still unsatisfactory because of their poor performances. In this paper, we propose a much more effective automatic technique to perform the task. This technique is based on two important observations about <b>data</b> <b>records</b> on the Web and a string matching algorithm. The proposed technique is able to mine both contiguous and noncontiguous <b>data</b> <b>records.</b> By non-contiguous <b>data</b> <b>records,</b> we mean that two or more <b>data</b> <b>records</b> intertwine in terms of their HTML codes. When they are displayed on a browser, each of them appears contiguous. Existing techniques are unable to mine such records. Our experimental results show that the proposed technique outperforms existing techniques substantially. Categories and Subject Descriptors I. 5 [Pattern Recognition]: statistical and structural. H. 2. 8 [Database Applications]: data minin...|$|R
40|$|Abstract. With {{the rapid}} {{increasing}} of web data, deep web {{is the fastest}} growing web data carrier. Therefore, the research of deep web, especially on extracting <b>data</b> <b>records</b> from Result pages, has already become an urgent task. We present a <b>data</b> <b>records</b> extraction based on Global Schema method, which automatically extracts the query result records from web pages. This method first analyzes the Query interface and result records instances to build a Global Schema by ontology. Then, the Global Schema {{is used in the}} process of extracting <b>data</b> <b>records</b> from result pages and storing these data in a table. Experimental results indicate that this method is accurate to extract <b>data</b> <b>records,</b> as well as to save in a table with a Global Schema. 1...|$|R
50|$|CHR/CTR {{acronyms}} {{should not}} be confused with CKD, which refers to Count Key Data, the layout of an addressable <b>data</b> <b>record</b> on a CTR.|$|E
5000|$|Send <b>Data</b> <b>Record</b> Packet - This message {{contains}} zero or more CDRs. CDRs may be {{encoded in}} ASN.1 using BER or, less commonly, PER.|$|E
5000|$|Send {{possibly}} duplicated <b>Data</b> <b>Record</b> Packet - This message contains {{one or more}} CDRs, {{and this}} message has previously been sent to another CGF.|$|E
5000|$|She {{developed}} the [...] "Limb by Limb" [...] method of <b>data</b> <b>recording,</b> a standardised methodology for clinicians providing data to epidemiologists. Before this method was described, different interpretations of commonly used terms (e.g. diplegia, hemiplegia and quadriplegia) meant that <b>data</b> <b>recorded</b> in studies {{were not always}} able to be assessed correctly at an epidemiological level. The [...] "Limb by Limb" [...] method stimulated a debate among other researchers about the best format for <b>data</b> <b>recording</b> in epidemiological studies, providing a basis from which other recording methodologies could be derived.|$|R
40|$|Social workers {{operate in}} an {{increasingly}} informational context (Parton, 2008, 2009). In this context, registration and <b>data</b> <b>recording</b> are becoming an important condition for the funding of social work organisations (Jones, 2001). In this article, we argue that the debate on <b>data</b> <b>recording</b> and social work mainly focuses {{on the question of}} whether <b>data</b> <b>recording</b> is a threat to or an opportunity for the relational aspects of social work practice. Based on an analysis of the data-recording system of victim-offender mediation for adult offenders in Flanders (Belgium), the article shows that recognising the social dimension of social work demands that social work approaches <b>data</b> <b>recording</b> not only as a way to show the activity of the social workers, but also as an instrument to reflect critically on how social work intervenes in-and therefore defines-social problems...|$|R
40|$|The method {{involves}} transmitting <b>data</b> <b>records</b> in {{the form}} of data packets from a transmitter to a receiver without feedback by the receiver regarding error occurrences in the transferred <b>data</b> <b>records.</b> The <b>data</b> <b>records</b> are divided into records of higher priority and records of lower priority, and the records of higher priority are sent at a high transfer rate, the data packets are transmitted only once or {{with a small amount of}} repetitions. The <b>data</b> <b>records</b> of lower priority are sent at a low transfer rate. The data packets are transmitted with a larger amount of repetitions. USE/ADVANTAGE - Esp. for broadcast transmission in computer network, e. g. e-mail. Enables large amount of receivers, and assures that valid data is present at receiver after predetermined amount of time...|$|R
