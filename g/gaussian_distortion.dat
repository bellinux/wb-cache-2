3|23|Public
30|$|In {{continuous}} spectrum sensing with full-duplex communication, {{the consideration of}} self-interference is particulary important since the self-interference can affect the sensing outcome and degrade SUs performance. Although the SU self-interference signal can have non-zero mean, it has been been assumed {{in the majority of}} related works to have a zero mean. For instance in [43], the authors mentioned that in practical full-duplex systems, the self-interference cannot be completely canceled, such that the signals received at each node is a combination of the signal transmitted by the other source, the residual self-interference (RSI), and the noise. They also assume that the RSI can be typically modeled as zero-mean additive white Gaussian noise (AWGN). The work reported in [42] assumed that the <b>Gaussian</b> <b>distortion</b> and noise follows central chi-square distribution in the absence of PU signals but potentially including RSI and noncentral chi-square distribution when PU signal is present.|$|E
40|$|Data hiding using {{quantization}} {{has revealed}} {{as an effective}} way of taking into account side information at the encoder. When quantizing more than one host signal samples there are two choices: 1) using the Cartesian product of several one-dimensional quantizers, as made in Scalar Costa Scheme (SCS); or 2) performing vectorial quantization. The second option seems better, as rate-distortion theory affirms that higher dimensional quantizers yield improved performance due to better sphere-packing properties. Although the embedding problem does resemble that of rate-distortion, no attacks or host signal characteristics are usually considered when designing the quantizer in this way. We show that attacks worsen {{the performance of the}} a priori optimal lattice quantizer through a counterexample: the comparison under <b>Gaussian</b> <b>distortion</b> of hexagonal lattice quantization against bidimensional Distortion-Compensated Quantized Projection (DC-QP), a data hiding alternative based in quantizing a linear projection of the host signal. Apart from empirical comparisons, theoretical lower bounds on the probability of decoding error of hexagonal lattices under Gaussian host signal and attack are provided and compared to the already analyzed DC-QP method...|$|E
40|$|Frequency flat, fast Rayleigh fading may be {{considered}} the most critical disturbance in a wireless communication system. In its most general form, it is modeled as a multiplicative time continuous random (zero mean complex <b>Gaussian)</b> <b>distortion</b> of the transmitted signal. In order to achieve an efficient communication here, {{each part of the}} communication link must be carefully designed based on the properties of the time continuous channel. Such an approach is distinct from most previous works, which have used strategies developed for the additive white Gaussian noise channel (AWGN) as starting points. Crucial in this development is {{how to deal with the}} deep fades and the rapid fluctuations of the received signal power. To reduce the influence of the deep fades on the error probability, diversity techniques must be used. Coded interleaved modulation can be regarded one such strategy, where the diversity effect arises as a result of nearby encoder output symbols being subjected to statistically independent fading. Central in achieving this independence is the interleaver, which spreads the symbols in time. A higher diversity order is obtained if the encoder output bits, instead of symbols, are interleaved. By also using codes of lower rates, the diversity is increased even further. To avoid bandwidth expansion here, the channel symbol constellation must be expanded accordingly. The resulting system is referred to as bit-interleaved channel symbol expansion diversity (CSED) and is the best low-complexity coded modulation scheme presented so far for the Rayleigh fading channel. On the fast fading channel, coding is only part of the solution, however. Here, matched filter detectors originally developed for AWGN are unable to efficiently handle the rapid (compared with the signaling rate) fluctuations of the received signal power. More sophisticated solutions are needed. It is essential to make the transition from the time continuous received signal to a discrete representation in the receiver, without losing too much accuracy. Both an adequate number of discrete observables and a sufficiently long observation interval are required. When this is fulfilled, error probability curves with steeper slopes and considerably lower error floors than those of matched filter based receivers are obtained. The steeper slopes are results of the implicit diversity of the random message component in the received signal, which is captured when multiple discrete observables (per symbol interval) are used. This effect is more pronounced the faster the fading is, therefore uncoded signaling has a potentially better performance at fast fading than in slow fading. Use of observation intervals larger than the symbol interval in the receiver, immediately leads to detectors based on sequence detection. The MAP (maximum aposteriori) sequence detector becomes prohibitively complex, consequently simpler suboptimal detectors are required. A near optimal detector needs to maintain only a small fixed number of candidate symbol sequences as the length of the received message increases. This is in contrast to the MAP sequence detector, where the number of sequences grows exponentially in the length of the message. By minor modifications, the suboptimal detector can also produce the bitwise soft outputs needed in the decoder part of a bit-interleaved CSED system...|$|E
50|$|Upon {{reception}} of the signal, the demodulator examines the received symbol, {{which may have been}} corrupted by the channel or the receiver (e.g. additive white <b>Gaussian</b> noise, <b>distortion,</b> phase noise or interference). It selects, as its estimate of what was actually transmitted, that point on the constellation diagram which is closest (in a Euclidean distance sense) to that of the received symbol. Thus it will demodulate incorrectly if the corruption has caused the received symbol to move closer to another constellation point than the one transmitted.|$|R
40|$|In this paper, {{we propose}} a {{performance}} evaluation of orientation field smoothing {{according to the}} application of two different filters; average filter and Gaussian filters. Each is investigated at different sizes. The finding is that by applying the filters size of 2 - 4 times of ridges spacing, the orientation field can be smoothed with less <b>distortion.</b> <b>Gaussian</b> filter seems to offer slightly better performance compared to an average one. Success in core point detection using Poincare technique is {{used to measure the}} filtering performance...|$|R
40|$|The semiclassical Herman–Kluk {{initial value}} {{approximation}} of the quantum-mechanical propagator, which is heavily used in dynamical studies of atomic and molecular systems, {{takes into account}} nonlinearity of the underlying classical dynamics by using multiple classical initial value solutions. In applications to the propagation of <b>Gaussian</b> wavepackets non-Gaussian <b>distortions</b> {{in the course of}} time and therefore a realistic description of the quantum dynamics are possible. This is in contrast to the single-trajectory approximations investigated in a recent paper by Baranger et al, one of which was erroneously termed Herman–Kluk approximation. PACS number: 03. 65. Sq 1...|$|R
40|$|The {{following}} topics were dealt with: Shannon theory; universal lossless source coding; CDMA; turbo codes; {{broadband networks}} and protocols; signal processing and coding; coded modulation; information theory and applications; universal lossy source coding; algebraic geometry codes; modelling analysis {{and stability in}} networks; trellis structures and trellis decoding; channel capacity; recording channels; fading channels; convolutional codes; neural networks and learning; estimation; <b>Gaussian</b> channels; rate <b>distortion</b> theory; constrained channels; 2 D channel coding; nonparametric estimation and classification; data compression; synchronisation and interference in communication systems; cyclic codes; signal detection; group codes; multiuser systems; entropy and noiseless source coding; dispersive channels and equalisation; block codes; cryptography; image processing; quantisation; random processes; wavelets; sequences for synchronisation; iterative decoding; optical communications...|$|R
40|$|Quantum random variable, {{distortion}} operator {{are introduced}} based on canonical operators. As the lower bound of rate distortion, the entanglement information rate distortion {{is achieved by}} Gaussian map for Gaussian source. General Gaussian maps are further reduced to unitary transformations and additive noises from the physical meaning of distortion. The entanglement information rate distortion function then are calculated for one mode Gaussian source. The rate distortion is accessible at zero distortion point. For pure state, the rate distortion function is always zero. In contrast to the distortion defined via fidelity, our definition of the distortion {{makes it possible to}} calculate the entanglement information rate <b>distortion</b> function for <b>Gaussian</b> source. <b>Distortion</b> operator: Two major parts in classical information theory are channel capacity and rate distortion theory. They concern respectively with the reliability and effectiveness of information transmission. In quantum information theory, channel capacity has been widely investigated, but little effort has been put into developing quantum rate-distortion theory[1][2]. It was proven [1] that the quantum rate-distortion function R(D) is lower bounded by entanglement information rate-distortion function R I (D). For a given source R I (D) is defined by R I (D) = mi...|$|R
40|$|In this paper, we {{consider}} the implementation of robust watermarking scheme for non i. i. d. <b>Gaussian</b> signals and <b>distortion</b> based on perceptual metrics. We consider this problem as a communication problem and formulated it as a game between an attacker and an embedder {{in order to establish}} its theoretical performance. We first show that known parallel Gaussian channels technique does not lead to valid practical implementation, and then propose a new scheme based on Wide Spread Spectrum and Side Information. Theoretical performances of this scheme are established and shown to be very close to the upper bound on capacity defined by Parallel Gaussian channels. Practical implementation of this scheme is then presented and influence of the di#erent parameters on performance is discussed. Finally, experimental results for image watermarking are presented and validate the proposed scheme...|$|R
40|$|Abstract—In {{this paper}} we find {{properties}} that are shared between two seemingly unrelated lossy source coding setups with side-information. The first setup {{is when the}} source and side-information are jointly <b>Gaussian</b> and the <b>distortion</b> measure is quadratic. The second setup is when the side-information is an erased version of the source. We begin with the observation that in both these cases the Wyner-Ziv and conditional rate-distortion functions are equal. We further find {{that there is a}} continuum of optimal strategies for the conditional rate distortion problem in both these setups. Next, we consider the case when there are two decoders with access to different side-information sources. For the case when the encoder has access to the side-information we establish bounds on the rate-distortion function and a sufficient condition for tightness. Under this condition, we find a characterization of the rate-distortion function for physically degraded side-information. This characterization holds for both the Gaussian and erasure setups. I...|$|R
40|$|Roughness of a {{processed}} surface {{has to be}} filtered {{to remove}} form errors and waviness. The mostcommon filter, the <b>Gaussian</b> filter, introduces <b>distortions</b> when used on some wood surfaces, whereas theRobust Gaussian Regression Filter (RGRF) does not. Unfortunately, the computation time when using theRGRF is increased significantly because the filter works iteratively and involves all profile data points in theevaluation. A modified algorithm that reduces the number of datapoints in the weighting window is proposedin this paper. The effect of the RGRF with a truncated window is {{compared to that of}} the RGRF withouttruncation on profiles of sanded wood and plastic and evaluated as an absolute error of Ra and Rtroughness parameters. Various weighting windows were tested and it was found that a window equivalent to 1. 25 l (l being the filter cut-off length), gave negligible errors. The computation time is reduced significantlywhen the number of data points is limited by this weighting window...|$|R
3000|$|It is {{well known}} that {{conventional}} quality metrics, such as MSE, SNR and PSNR do not always correlate with image visual quality [10, 14, 18, 41]. This resulted in starting intensive design of other full-reference fidelity metrics. Current situation in the field of designing full-reference fidelity metrics is well characterized by the title of invited talk of M. Pedersen at Color Imaging Symposium in Gjovik (Norway, 2009) [...] " 111 Full-Reference Image Quality Metrics and Still not Good Enough?" [...] Thus, choice of a proper visual quality metric for analysis and comparisons is always a problem and can be argued. Because of this, we relied on data and conclusions presented in the paper [17], where a comparison of 18 visual quality metrics have been performed for several subsets of distortion types for the database TID 2008 (available at [URL] A subset Safe allows considering the following seven types of distortions: additive Gaussian white noise, spatially correlated noise, high frequency noise, impulse noise, <b>Gaussian</b> blur, <b>distortions</b> due to lossy compression by JPEG and JPEG 2000. The best performance (in the sense of largest Spearman and Kendall correlation of a quality metric with mean opinion score) for this subset was achieved by the metrics PSNR-HVS-M, PSNR-HVS, and WSNR. Another subset of distortion types called JPEG takes into account only images compressed by JPEG and JPEG 2000. For this subset, the best results have been provided by PSNR-HVS-M, PSNR-HVS, and MSSIM. WSNR was the fourth best. Thus, we have decided to use these four visual quality metrics.|$|R
40|$|Abstract—In this paper, {{the impact}} of {{clipping}} noise on optical wireless communication (OWC) systems employing orthogonal frequency division multiplexing (OFDM) is investigated. The two existing optical OFDM (O-OFDM) transmission schemes, asymmetrically clipped optical OFDM (ACO-OFDM) and direct-current-biased optical OFDM (DCO-OFDM), are studied. Time domain signal clipping generally results from direct current (DC) biasing and/or from physical limitations of the transmitter front-end. These include insufficient forward biasing and the maximum power driving limit of the emitter. The clipping noise can be modeled according to the Bussgang theorem and the central limit theorem (CLT) as attenuation of the data-carrying subcarriers at the receiver and addition of zero-mean complex-valued Gaussian noise. Analytical expressions for the attenuation factor and the clipping noise variance are determined in closed-form and employed in the derivation of the electrical signal-to-noise ratio (SNR). The validity of the model is verified through a Monte Carlo bit-error ratio (BER) simulation. Finally, the BER performance of ACO-OFDM with DCO-OFDM is compared for different clipping levels and multi-level quadrature amplitude modulation (M-QAM) schemes. Index Terms—Wireless communication, optical devices, OFDM, <b>Gaussian</b> processes, non-linear <b>distortion.</b> I...|$|R
40|$|Abstract—Digital {{fingerprinting}} is {{a technology}} for tracing {{the distribution of}} multimedia content and protecting them from unauthorized redistribution. Unique identification information is embedded into each distributed copy of multimedia signal {{and serves as a}} digital fingerprint. Collusion attack is a cost-effective attack against digital fingerprinting, where colluders combine several copies with the same content but different fingerprints to remove or attenuate the original fingerprints. In this paper, we investigate the average collusion attack and several basic nonlinear collusions on independent Gaussian fingerprints, and study their effectiveness and the impact on the perceptual quality. With unbounded <b>Gaussian</b> fingerprints, perceivable <b>distortion</b> may exist in the fingerprinted copies as well as the copies after the collusion attacks. In order to remove this perceptual distortion, we introduce bounded Gaussian-like fingerprints and study their performance under collusion attacks. We also study several commonly used detection statistics and analyze their performance under collusion attacks. We further propose a preprocessing technique of the extracted fingerprints specifically for collusion scenarios to improve the detection performance. Index Terms—Digital forensics, multimedia fingerprinting, nonlinear collusion attacks, spread spectrum embedding, traitor tracing. I...|$|R
40|$|Image {{and video}} quality {{evaluation}} is very important. In applications involving signal transmission, the Reduced- or No-Reference quality metrics {{are generally more}} practical than the Full-Reference metrics. Digital watermarking based quality evaluation emerges as a potential Reduced- or No-Reference quality metric, which estimates signal quality by assessing the degradation of the embedded watermark. Since the watermark contains {{a small amount of}} information compared to the cover signal, performing accurate signal quality evaluation is a challenging task. Meanwhile, the watermarking process causes signal quality loss. To address these problems, in this thesis, a framework for image and video quality evaluation is proposed based on semi-fragile and adaptive watermarking. In this framework, adaptive watermark embedding strength is assigned by examining the signal quality degradation characteristics. The "Ideal Mapping Curve" is experimentally generated to relate watermark degradation to signal degradation so that the watermark degradation can be used to estimate the quality of distorted signals. With the proposed framework, a quantization based scheme is first implemented in DWT domain. In this scheme, the adaptive watermark embedding strengths are optimized by iteratively testing the image degradation characteristics under JPEG compression. This iterative process provides high accuracy for quality evaluation. However, it results in relatively high computational complexity. As an improvement, a tree structure based scheme is proposed to assign adaptive watermark embedding strengths by pre-estimating the signal degradation characteristics, which greatly improves the computational efficiency. The SPIHT tree structure and HVS masking are used to guide the watermark embedding, which greatly reduces the signal quality loss caused by watermark embedding. Experimental results show that the tree structure based scheme can evaluate image and video quality with high accuracy in terms of PSNR, wPSNR, JND, SSIM and VIF under JPEG compression, JPEG 2000 compression, Gaussian low-pass filtering, <b>Gaussian</b> noise <b>distortion,</b> H. 264 compression and packet loss related distortion...|$|R
40|$|Currently, {{the most}} {{successful}} approach to steganography in empirical objects, such as digital media, is to cast the embedding problem as source coding with a fidelity constraint. The sender specifies the costs of changing each cover element and then embeds a given payload by minimizing the total embedding cost. Since efficient practical codes exist that embed near the rate–distortion bound, the remaining task left to the steganographer is the fidelity measure – {{the choice of the}} costs. In the past, the costs were obtained either in an ad hoc manner or determined from the effects of embedding in a chosen feature space. In this paper, we adopt a different strategy in which the cover is modeled as a sequence of independent but not necessarily identically distributed quantized Gaussians and the embedding change probabilities are derived to minimize the total KL divergence within the chosen model for a given embedding operation and payload. Despite the simplicity of the adopted model, the resulting stegosystem exhibits security that is comparable to current state-of-the-art methods methods across a wide range of payloads. Index Terms — Steganography, multivariate <b>Gaussian</b> cover, additive <b>distortion</b> function, syndrome-trellis codes, steganalysis 1...|$|R
40|$|Traditional radial lens {{distortion}} models {{are based on}} the physical construction of lenses. However, manufacturing defects and physical shock often cause the actual observed distortion to be different from what can be modeled by the physically motivated models. In this work, we initially propose a <b>Gaussian</b> process radial <b>distortion</b> model {{as an alternative to the}} physically motivated models. The non-parametric nature of this model helps implicitly select the right model complexity, whereas for traditional distortion models one must perform explicit model selection to decide the right parametric complexity. Next, we forego the radial distortion assumption and present a completely non-parametric, mathematically motivated distortion model based on locally-weighted homographies. The separation from an underlying physical model allows this model to capture arbitrary sources of distortion. We then apply this fully non-parametric distortion model to a zoom lens, where the distortion complexity can vary across zoom levels and the lens exhibits noticeable non-radial distortion. Through our experiments and evaluation, we show that the proposed models are as accurate as the traditional parametric models at characterizing radial distortion while flexibly capturing non-radial distortion if present in the imaging system...|$|R
40|$|This article {{presents}} two algorithms for spatial processing of low seeding density PIV (particle image velocimetry) images {{which lead to}} sub-pixel precision in particle positioning. The particle centres are estimated to accuracies {{of the order of}} 0. 1 pixel, yielding 1 % error in velocity calculation. The first algorithm discriminates valid particles {{from the rest of the}} image and determines their centres in Cartesian coordinates by using a two-dimensional Gaussian fit, The second algorithm performs local correlation between particle pairs and determines instantaneous two-dimensional velocities. The methods have been applied initially to simulated data, <b>Gaussian</b> noise and <b>distortion</b> has then been added to simulate experimental conditions, It is shown that, in comparison with conventional methods, the new algorithms offer up to an order of magnitude higher accuracy for particle centre estimation, Finally, the Gaussian fit approach has been used to map an experimental transonic flow field from the stator trailing edge wake region of a cascade with an estimated error of 1 %. The experimental results are found to be in good agreement with previous theoretical steady-state viscous calculations. Copyright (C) 1996 Elsevier Science Ltd...|$|R
40|$|We study {{forward link}} {{performance}} of a multi-user cellular wireless network. In our proposed cellu-lar broadcast model, the receiver population is partitioned into smaller mutually exclusive subsets called cells. In each cell an autonomous transmitter with average transmit power constraint communicates to all receivers in its cell by broadcasting. The broadcast signal is a multiplex of independent information from many remotely located sources. Each receiver extracts its desired information from the composite signal, which consists of a distorted version of the desired signal, interference from neighboring cells and additive white <b>Gaussian</b> noise. Waveform <b>distortion</b> is caused by time and frequency selective linear time-variant channel that exists between every transmitter-receiver pair. Under such system and design constraints, and a fixed bandwidth for the entire network, we show that the most efficient resource allocation policy for each transmitter based on information theoretic measures such as channel capacity, simultaneously achievable rate regions and sum-rate is superposition coding with successive interference cancellation. The optimal policy dominates over its sub-optimal al-ternatives at {{the boundaries of the}} capacity region. By taking into account practical constraints such as finite constellation sets, frequency translation via carrier modulation, pulse shaping and real-time sig...|$|R
40|$|International audienceThe present paper {{advances}} {{a robust}} video fingerprinting system for tracking the visual content subjected to live recording. The methodological novelty {{of the system}} relies in creating synergies between architectural modules, designed so as to offer: (1) local visual feature representations, invariant with respect to scale, orientation and affine transformations; (2) scalable global feature representations invariant with respect to photometric transformations and (3) time-variant jitter synchroniza-tion. The system is tested on a reference database of 14 hours of cinematographic content and on a query dataset of 28 hours of video related to two use cases: a) computer generated <b>distortions</b> (<b>Gaussian</b> filtering, sharpening, rotations with 2 °, conver-sion to grayscale, contrast changes, brightness changes, geometric random bending) and b) live camera recording. The for-mer use case resulted in ideal rate of false alarm, probability of missed detection of 0. 02 and F 1 score of 0. 97. However, the applicative novelty is given by solving the latter use case: experimental values of the false alarm rate lower than 0. 01, prob-ability of missed detection of 0. 04 and F 1 score equal to 0. 94 were obtained for content live recorded from theatres’ and PC screens...|$|R
40|$|Abstract—We propose {{computationally}} efficient encoders and decoders for {{lossy compression}} using a Sparse Regression Code. The codebook {{is defined by}} a design matrix and codewords are structured linear combinations of columns of this matrix. The proposed encoding algorithm sequentially chooses columns of the design matrix to successively approximate the source sequence. It is shown to achieve the optimal distortion-rate function for i. i. d Gaussian sources under the squared-error distortion criterion. For a given rate, {{the parameters of the}} design matrix can be varied to trade off distortion performance with encoding complexity. An example of such a trade-off {{as a function of the}} block length n is the following. With computational resource (space or time) per source sample of O((n / logn) 2), for a fixed distortion-level above the Gaussian distortion-rate function, the probability of excess distortion decays exponentially in n. The Sparse Regression Code is robust in the following sense: for any ergodic source, the proposed encoder achieves the optimal distortion-rate function of an i. i. d Gaussian source with the same variance. Simulations show that the encoder has good empirical performance, especially at low and moderate rates. Index Terms—Lossy compression, computationally efficient encoding, squared error <b>distortion,</b> <b>Gaussian</b> rate-distortion, sparse regression, compressed sensing I...|$|R
40|$|We {{present an}} {{achievable}} rate-distortion for an un-reliable sensor network problem. Noisy measurements of some source X made by n remote sensors needto be communicated {{to a central}} decoder over unreliable channels. Limited power resources of the sen-sors constrain the resolution with which the noisy sensor readings can be transmitted to a rate R bits persource sample. The central decoder is guaranteed to reliably receive readings from a certain minimum num-ber k < = n of sensors. The goal of the central de-coder is to estimate X from the received transmis-sions where a specified distortion metric provides an objective measure of estimation quality. In this work,we present an information-theoretic achievable rate- distortion region for this multiterminal source codingproblem when the sensor measurement noise statistics is symmetric {{with respect to the}} sensors. In the spe-cial case of a Gaussian sensor network, i. e., when the source X is i. i. d. Gaussian, the sensor noise processesare additive i. i. d. <b>Gaussian,</b> and the <b>distortion</b> metric is mean squared error, we have the following inter-esting result. When any k out of n unreliable sensortransmissions are received, the central decoder's estimation quality can be as good as the best reconstruc-tion quality that can be achieved by deploying only kreliable sensors and the central decoding unit is able to receive transmissions from all k sensors. Furthermore,when more than k out of the n sensor transmissionsare received, the estimation quality strictly improves...|$|R
40|$|International audienceNonspecular {{effects for}} {{reflection}} from a lossy dielectric are studied under the paraxial approximation. It is shown that lateral shifts larger than 100 wavelengths are obtained for an {{angle of incidence}} close to the Brewster angle. Furthermore, it is shown that a three-dimensional Gaussian beam with a circular cross section becomes on reflection near the Brewster angle a Gaussian beam with an elliptical cross section. In addition, {{an evaluation of the}} accuracy of the paraxial approximation is done by comparison with an exact calculation. Nonspecular reflection effects have been studied for total reflection after the pioneering work of Goos and Hanchen. Experimental evidence of a lateral shift of a finite beam was first reported in the context of acoustic waves. Many theoretical approaches have been developed; among them, the use of an angular spectral approach has proved to be useful in showing that a variety of nonspecular effects may be defined. 3 These effects are easily understood {{in the context of an}} angular spectrum approach based on the following expansion of the incident monochro-matic p-polarized beam Ej: Ei(xjL,zi) = Wk^ f exp(W 'k 2 Si 2) u(vi) X exp(ikisi * x 1 ji + ikicizi) dsi (1) in the (xi, yi, zi) coordinate system (see Fig. 1), where the time dependence exp(-iwst) has been omitted. The beam waist is w, ki = wic, v is the unit vector [s, c = (1 -S 2) v 2] of the wave vector, and u is the unit vector specifying the polarization, given by us zAv/IzAvI, (2 a) up vAu, (2 b) for s and p polarization, respectively. The reflected field is then expressed as w 2 k. 2 (wki 2 s 2 Er(XllrZr) = 8 v! R(sr) exp-r IU(Vr) 47 T f 4 / x exp[+iki(sr * Xj 1 r + CrZr) ]dSr (3) in the (Xr, Yr, Zr) coordinate system, where R(Sr) is the reflection factor for p polarization. It is seen from Eq. (3) that the dependence of the reflection factor R(Sr) on the direction of the wave vector Sr introduces a distortion of the beam compared with the symmetric beam. If we assume that the reflected beam remains <b>Gaussian,</b> the <b>distortion</b> can be accounted for by introducing variations of the parameters of the beam. These modifications, which appear as nonspecular effects, are lateral and focal shifts 8 A and 3, of the p...|$|R
40|$|We {{present a}} {{measurement}} of the linear growth rate of structure, f from the Sloan Digital Sky Survey III (SDSS III) Baryon Oscillation Spectroscopic Survey (BOSS) Data Release 12 (DR 12) using Convolution Lagrangian Perturbation Theory (CLPT) with <b>Gaussian</b> Streaming Redshift-Space <b>Distortions</b> (GSRSD) to model the two point statistics of BOSS galaxies in DR 12. The BOSS-DR 12 dataset includes 1, 198, 006 massive galaxies spread over the redshift range 0. 2 < z < 0. 75. These galaxy samples are categorized in three redshift bins. Using CLPT-GSRSD in our analysis of the combined sample of the three redshift bins, we report measurements of f σ_ 8 for the three redshift bins. We find f σ_ 8 = 0. 430 ± 0. 054 at z_ eff = 0. 38, f σ_ 8 = 0. 452 ± 0. 057 at z_ eff = 0. 51 and f σ_ 8 = 0. 457 ± 0. 052 at z_ eff = 0. 61. Our {{results are consistent with}} the predictions of Planck ΛCDM-GR. Our constraints on the growth rates of structure in the Universe at different redshifts serve as a useful probe, which can help distinguish between a model of the Universe based on dark energy and models based on modified theories of gravity. This paper is part of a set that analyses the final galaxy clustering dataset from BOSS. The measurements and likelihoods presented here are combined with others in Alam et al. 2016 to produce the final cosmological constraints from BOSS. Comment: This is one of the companion papers of the SDSS DR 12 final cosmological result. Submitted to MNRAS. 15 pages, 7 figures, 2 tables. The present draft is a reviewed version submitted to MNRAS after getting reviewer's comment...|$|R
40|$|We derive {{the optimal}} second-order coding region and {{moderate}} deviations constant for successive refinement source coding with a joint excess-distortion probability constraint. We consider two scenarios: (i) a discrete memoryless source (DMS) and arbitrary distortion measures at the decoders and (ii) a Gaussian memoryless source (GMS) and quadratic distortion measures at the decoders. For a DMS with arbitrary distortion measures, we prove an achievable second-order coding region, using type covering lemmas by Kanlis and Narayan and by No, Ingber and Weissman. We prove the converse using the perturbation approach by Gu and Effros. When the DMS is successively refinable, the expressions for the second-order coding {{region and the}} moderate deviations constant are simplified and easily computable. For this case, we also obtain new insights on the second-order behavior compared to the scenario where separate excess-distortion proabilities are considered. For example, we describe a DMS, for which the optimal second-order region transitions from being characterizable by a bivariate Gaussian to a univariate <b>Gaussian,</b> as the <b>distortion</b> levels are varied. We then consider a GMS with quadratic distortion measures. To prove the direct part, we {{make use of the}} sphere covering theorem by Verger-Gaugry, together with appropriately-defined Gaussian type classes. To prove the converse, we generalize Kostina and Verdú's one-shot converse bound for point-to-point lossy source coding. We remark that this proof is applicable to general successively refinable sources. In the proofs of the moderate deviations results for both scenarios, we follow a strategy similar to that for the second-order asymptotics and use the moderate deviations principle. Comment: Part of this paper has beed presented at ISIT 2016. Submitted to IEEE Transactions on Information Theory in Jan, 2016. Revised in Aug. 201...|$|R
40|$|The {{field of}} digital data {{communications}} has experienced an explosive {{growth in the}} last three decade with the growth of internet technologies, high speed and efficient data transmission over communication channel has gained significant importance. The rate of data transmissions over a communication system is limited due to the effects of linear and nonlinear distortion. Linear distortions occure in from of inter-symbol interference (ISI), co-channel interference (CCI) and adjacent channel interference (ACI) in the presence of additive white <b>Gaussian</b> noise. Nonlinear <b>distortions</b> are caused due to the subsystems like amplifiers, modulator and demodulator along with nature of the medium. Some times burst noise occurs in communication system. Different equalization techniques are used to mitigate these effects. Adaptive channel equalizers are used in digital communication systems. The equalizer located at the receiver removes the effects of ISI, CCI, burst noise interference and attempts to recover the transmitted symbols. It has been seen that linear equalizers show poor performance, where as nonlinear equalizer provide superior performance. Artificial neural network based multi layer perceptron (MLP) based equalizers have been used for equalization in the last two decade. The equalizer is a feed-forward network consists of one or more hidden nodes between its input and output layers and is trained by popular error based back propagation (BP) algorithm. However this algorithm suffers from slow convergence rate, {{depending on the size of}} network. It has been seen that an optimal equalizer based on maximum a-posterior probability (MAP) criterion can be implemented using Radial basis function (RBF) network. In a RBF equalizer, centres are fixed using K-mean clustering and weights are trained using LMS algorithm. RBF equalizer can mitigate ISI interference effectively providing minimum BER plot. But when the input order is increased the number of centre of the network increases and makes the network more complicated. A RBF network, to mitigate the effects of CCI is very complex with large number of centres. To overcome computational complexity issues, a single neuron based chebyshev neural network (ChNN) and functional link ANN (FLANN) have been proposed. These neural networks are single layer network in which the original input pattern is expanded to a higher dimensional space using nonlinear functions and have capability to provide arbitrarily complex decision regions. More recently, a rank based statistics approach known as Wilcoxon learning method has been proposed for signal processing application. The Wilcoxon learning algorithm has been applied to neural networks like Wilcoxon Multilayer Perceptron Neural Network (WMLPNN), Wilcoxon Generalized Radial Basis Function Network (WGRBF). The Wilcoxon approach provides promising methodology for many machine learning problems. This motivated us to introduce these networks in the field of channel equalization application. In this thesis we have used WMLPNN and WGRBF network to mitigate ISI, CCI and burst noise interference. It is observed that the equalizers trained with Wilcoxon learning algorithm offers improved performance in terms of convergence characteristic and bit error rate performance in comparison to gradient based training for MLP and RBF. Extensive simulation studies have been carried out to validate the proposed technique. The performance of Wilcoxon networks is better then linear equalizers trained with LMS and RLS algorithm and RBF equalizer in the case of burst noise and CCI mitigations...|$|R

