30|12|Public
2500|$|In quantum mechanics, the {{uncertainty}} principle, {{also known as}} Heisenberg's uncertainty principle or Heisenberg's <b>indeterminacy</b> <b>principle,</b> is any {{of a variety of}} mathematical inequalities [...] asserting a fundamental limit to the precision with which certain pairs of physical properties of a particle, known as complementary variables, such as position x and momentum p, can be known.|$|E
5000|$|William James {{said that}} {{philosophers}} (and scientists) have an [...] "antipathy to chance." [...] Absolute chance, a possible implication {{of quantum mechanics}} and the <b>indeterminacy</b> <b>principle,</b> implies a lack of causality. This possibility often disturbs those who assume {{there must be a}} causal and lawful explanation for all events.|$|E
5000|$|In quantum mechanics, the {{uncertainty}} principle, {{also known as}} Heisenberg's uncertainty principle or Heisenberg's <b>indeterminacy</b> <b>principle,</b> is any {{of a variety of}} mathematical inequalities [...] asserting a fundamental limit to the precision with which certain pairs of physical properties of a particle, known as complementary variables, such as position x and momentum p, can be known.|$|E
40|$|In this paper, the {{interaction}} and transmission time of quantum density solitons waves representing particles passing through finite barrier potentials is investigated. Using {{the conservation of}} energy and of quantum density, it is first demonstrated that these waves have finite de Broglie wavelength and represent particles in quantum theory. The passage of the quantum density solitons (particles) through barriers of finite energies is then shown {{to lead to the}} phenomena of resonant tunneling and, in Josephson-like configurations, to the quantization of magnetic flux. A precise general measure for barrier tunneling time is derived which is found to give a new interpretation of the quantum <b>indeterminacy</b> <b>principles...</b>|$|R
40|$|Alma-Ata), who had {{repeated}} E. Rutherford’s {{experiment on}} nuclear scattering of α-particles, conducted diffraction analysis of that experiment results. The {{analysis showed that}} nuclei of all chemical elements feature quasi-crystal structures [1]. In other words, they obtained experimental evidences {{of the fact that}} the smallest indivisible quanta and <b>indeterminacy</b> <b>principles</b> are not objective factors of the material World. Further analysis proved the conclusion of principal impossibility to apply the classical physics techniques in the microworld theory to be wrong. That conclusion resulted from the false conclusion of the absence of material ether in Nature that could interact with microscopic objects and resist their motion, as well as from mistaken negligence of magnetic interactions between microobjects in the microworld theory. Application of methods of the theory of non-linear oscillations [2] allowed building adequate structural mathematical models of atomic nucleus and atom with consideration for ether and magnetic interactions among microscopic objects. Using those adequate structural models made it possible to obtain new, detailed data on structure of microobjects unavailable in quantum physics. [3 ÷ 12]...|$|R
40|$|USSR (Alma-Ata) {{published}} the experimental {{evidence of the}} fact that the smallest indivisible quanta and <b>indeterminacy</b> <b>principles</b> are not objective factors of the material world [1]. The concepts of the smallest indivisible quanta of energy emerged after physicists had adequate structural models of atoms because of the improper negligence of ether and its resistance to motion of microscopic objects and absurd refusal to account for magnetic interactions among microobjects. Consideration of these factors would allow for building adequate structural models of microscopic objects and obtaining new, detailed data on properties of microobjects unavailable to quantum theory. Could these findings be used to learn something new concerning the issue of “black holes”? [2]. According to [2], a “black hole ” can occur in our Universe as a result of a massive star having burnt to a metal core wherein no nuclear reactions take place with the release of heat. If the core mass exceeds the critical value a gravitational collapse happens to the core, i. e. a “black hole ” occurs. Herewith the collapsed matter density can increase by a factor of 15 ÷ 16 [2]. The structure of primitive objects of the collapsed matter will hardly be preserved in suc...|$|R
5000|$|Science {{generally}} {{attempts to}} eliminate vague definitions, causally inert entities, and indeterminate properties, via further observation, experimentation, characterization, and explanation. Occam's razor tends to eliminate causally inert entities from functioning models of quantifiable phenomena, but some quantitative models, such as quantum mechanics, actually imply certain indeterminacies, {{such as the}} relative indeterminacy of quantum particles' positions to the precision with which their momenta can be measured (and vice versa). (See Heisenberg's <b>indeterminacy</b> <b>principle.)</b> ...|$|E
50|$|Kane is one {{of several}} {{philosophers}} and scientists to propose a two-stage model of free will. The American philosopher William James was the first (in 1884). Others include the French mathematician and scientist Henri Poincaré (about 1906), the physicist Arthur Holly Compton (1931, 1955), the philosopher Karl Popper (1965, 1977), the physicist and philosopher Henry Margenau (1968, 1982), the philosopher Daniel Dennett (1978), the classicists A. A. Long and David Sedley (1987), the philosopher Alfred Mele (1995), and most recently, the neurogeneticist and biologist Martin Heisenberg (2009), son of the physicist Werner Heisenberg, whose quantum <b>indeterminacy</b> <b>principle</b> lies at the foundation of indeterministic physics.|$|E
5000|$|Causal {{determinism}} has {{a strong}} relationship with predictability. Perfect predictability implies strict determinism, but lack of predictability does not necessarily imply lack of determinism. Limitations on predictability could be caused by factors such {{as a lack of}} information or excessive complexity. [...] Laplace's Demon is a supreme intelligence who could completely predict the one possible future given the Newtonian dynamical laws of classical physics and perfect knowledge of the positions and velocities of all the particles in the world. [...] In experimental physics, there are always observational errors determining variables such as positions and velocities. So perfect prediction is practically impossible. Moreover, in modern quantum mechanics, Werner Heisenberg's <b>indeterminacy</b> <b>principle</b> puts limits on the accuracy with which such quantities can be known. So such perfect predictability is also theoretically impossible.|$|E
40|$|We {{developed}} a sticky-price model that introduces the factors of (a) the non-separability of consumption and {{labor in the}} utility function and (b) a technological change induced by the investment of profits, to analyze the determinacy of equilibrium. We found that while engaging in inflation targeting increases the probability of determinacy, engaging in share-price targeting decreases the probability of determinacy in a standard sticky-price model; engaging in both inflation targeting and share-price targeting can increase the probability of determinacy in our model. Taylor <b>principle</b> <b>Indeterminacy</b> Share-price targeting New Keynesian Phillips curve...|$|R
40|$|In a sticky-price {{model with}} labor market search and {{matching}} frictions, forecast-based interest rate policy almost always induces indeterminacy {{when it is}} strictly inflation targeting and satisfies the Taylor <b>principle.</b> <b>Indeterminacy</b> {{is due to a}} vacancy channel of monetary policy that makes inflation expectations self-fulfilling. The effect of this channel strengthens as the sluggishness of the adjustment of employment relative to that of consumption increases. When this relative sluggishness is high, the Taylor principle fails to ensure determinacy, regardless of whether the policy is forecast-based or outcome-based, whether it is strictly or flexibly inflation targeting, or contains policy rate smoothing. ...|$|R
40|$|This essay aims {{to redress}} the {{contention}} that epistemic possibility cannot be {{a guide to the}} principles of modal metaphysics. I argue that the interaction between the multi-dimensional intensional framework and intensional plural quantification enables epistemic possibilities to target the haecceitistic properties of individuals. I outline the elements of plural logic, and I specify, then, a multi-dimensional intensional formula encoding the relation between the epistemic possibility of haecceity comprehension and its metaphysical possibility. I conclude by addressing objections from the <b>indeterminacy</b> of ontological <b>principles</b> relative to the space of epistemic possibilities, and from the consistency of epistemic modal space...|$|R
5000|$|As a boy, {{he visited}} Paris {{with his father}} and was {{introduced}} to Louis Pasteur. Pasteur laid his hand on Simpson's head and exclaimed: [...] "Travaillez, mon ami, travaillez!" [...] my friend, work! Turning to the father, he said [...] "A-t-il dit, Oui?" [...] he said, yes? Simpson seems to have implemented Pasteur's injunction throughout his life. In his writings, his dominant interest lay in showing the connection between science and religion. In his view, there is no contradiction between these, and he views Christianity as the natural outcome of man's evolutionary progress. Jesus Christ is [...] "the fulfilment of all that went before[...] [...] He is the Alpha and Omega of strictly human history." [...] and so on. In a later book, Nature: Cosmic, Human and Divine (1929), Simpson argues that religion results from the confrontation of Mind with the Infinite Energy of the universe as suggested by Heisenberg's <b>indeterminacy</b> <b>principle.</b>|$|E
40|$|We revisit Heisenberg <b>indeterminacy</b> <b>principle</b> in {{the light}} of the Galois-Grothendieck theory for the case of finite abelian Galois extensions. In this {{restricted}} framework, the Galois-Grothendieck duality between finite K-algebras split by a Galois extension L and finite Gal(L:K) -sets can be reformulated as a Pontryagin-like duality between two abelian groups. We then define a Galoisian quantum theory in which the Heisenberg <b>indeterminacy</b> <b>principle</b> between conjugate canonical variables can be understood as a form of Galoisian duality: the larger the group of automorphisms H (a subgroup of G) of the states in a G-set O = G/H, the smaller the "conjugate" observable algebra that can be consistently valuated on such states. We then argue that this Galois <b>indeterminacy</b> <b>principle</b> can be understood as a particular case of the Heisenberg <b>indeterminacy</b> <b>principle</b> formulated in terms of the notion of entropic indeterminacy. Finally, we argue that states endowed with a group of automorphisms H can be interpreted as squeezed coherent states, i. e. as states that minimize the Heisenberg indeterminacy relations...|$|E
40|$|International audienceAs a {{consequence}} of Heisenberg <b>indeterminacy</b> <b>principle,</b> quantum states are defined by half the number of variables required in classical mechanics. The main claim {{of this paper is}} that this " reduction " in the number of variables required to completely describe a physical system can be understood as {{a consequence}} of the same formalism underlying the reduction procedure used in gauge theories, namely the Mardsen-Weinstein symplectic reduction. This fact points towards a gauge-theoretical interpretation of the <b>indeterminacy</b> <b>principle</b> in quantum mechanics...|$|E
40|$|I analyze two {{connections}} between neoclassical and classical economics. First, I consider the indeterminacy that arises for both schools: in the neoclassical theories of overlapping generations and of factor pricing and in Sraffa’s price theory. Neoclassical indeterminacy occurs only in environments where relative prices can change through time; otherwise, determinacy obtains. Although these results challenge the Sraffian position on <b>indeterminacy,</b> the classical <b>principle</b> that current economic activity {{is embedded in}} the past proves to be a powerful insight: it establishes the robustness of factor-price indeterminacy and casts doubt on the importance of overlapping-generations indeterminacy. Second, I argue that recent claims that capital-theoretic paradoxes arise in intertemporal general equilibrium modes, not just in aggregative theory, cannot be validated...|$|R
40|$|The {{problem of}} {{initializing}} phase in a quantum computing system is considered. The initialization of phases {{is a problem}} when the system is initially present in an entangled state {{and also in the}} application of the quantum gate transformations since each gate will introduce phase uncertainty. The accumulation of these random phases will render the recently proposed quantum computing schemes ineffective for all but toy problems. I think I can safely say that nobody understands quantum mechanics. [...] Richard Feynman 1 Introduction A quantum state is undetermined with respect to its phase. This <b>indeterminacy</b> is in <b>principle</b> irremovable[6]. The uncertainty of phase, together with superposition, is responsible for the power of quantum mechanics. It also compels us to speak of information in positivist terms [...] -with respect to an observation rather than in an absolute sense. Since phase indeterminacy is fundamental to quantum description, it is relevant to examine its implications for quantu [...] ...|$|R
40|$|The {{intention}} {{of this paper}} is not to build quantum metaphysics but to justify the possibility of a second game through interpreting the ideas, concepts and visions from actual physics with ideas and visions borrowed from physics of the Orient. We are aware that quantum metaphysics is far from Abhinavagupta’s metaphysics. After Culianu we introduce both visions in a deforming grid to build a new ideational universe. This ideational universe is a transmodern construction, which tends to become topical in the context of transdisciplinarity and multidisciplinarity prevalence. Characterization of thinking as theory and determination of knowledge as a theoretical approach takes place in a inwardly beginning of the technical interpretation of thinking. There is an attempt to keep a sense of reason independently towards action. Philosophy is always in the critical situation to justify its existence to science. The safest {{way to do this is}} by self-establishing itself as science. But this effort represents the sacrifice of the essence of knowledge: “in the technical interpretation of thinking, the being as self element of thinking is sacrificed (Hidegger, 1988 : 314) ”. To look positively to philosophy as a dodge of science from metaphysics proves not only destructive to philosophy but also harmful to science. Absolute objectivity claims are increasingly refuted by the development of the cognitive approach. Not only social and human sciences require subjectivity as a dimension of objective knowledge, but physics itself considered the hard part of science reaches to formulate <b>indeterminacy</b> (uncertainty) <b>principles.</b> A new perspective that might be required in the philosophy of science is to look at it as to a semiotics of cognitive act. Quantum Metaphysics, David Bohm, transmodern discourse, philosophy of religions...|$|R
40|$|Abstract. We revisit Heisenberg <b>indeterminacy</b> <b>principle</b> in {{the light}} of the Galois-Grothendieck theory for the case of finite abelian Galois extensions. In this {{restricted}} framework, the Galois-Grothendieck duality between finite K-algebras split by a Galois extension L and finite Gal(L: K) -sets can be reformulated as a Pontryagin-like duality between two abelian groups. We then define a Galoisian quantum theory in which the Heisenberg <b>indeterminacy</b> <b>principle</b> between conjugate canonical variables can be understood as a form of Galoisian duality: the larger the group of au-tomorphisms H ⊆ G of the states in a G-set O ≃ G/H, the smaller the “conjugate ” observable algebra that can be consistently valuated on such states. We then argue that this Galois indeter-minacy principle can be understood as a particular case of the Heisenberg <b>indeterminacy</b> <b>principle</b> formulated in terms of the notion of entropic indeterminacy. Finally, we argue that states endowed with a group of automorphisms H can be interpreted as squeezed coherent states, i. e. as states that minimize the Heisenberg indeterminacy relations. 1...|$|E
40|$|By {{examining}} two counterexamples to {{the existing}} theory, it is shown, with mathematical rigor, {{that as far as}} scattered particles are concerned the true distribution function is in principle not determinable (<b>indeterminacy</b> <b>principle</b> or uncertainty principle) while the average distribution function over each predetermined finite velocity solid-angle element can be calculated. ...|$|E
40|$|We discuss price {{variations}} distributions {{in foreign}} exchange markets, characterizing {{them both in}} calendar and business time frameworks. The price dynamics {{is found to be}} the result of two distinct processes, a multi-variance diffusion and an error process. The presence of the latter, which dominates at short time scales, leads to <b>indeterminacy</b> <b>principle</b> in finance. Furthermore, dynamics does not allow for a scheme based on independent probability distributions, since volatility exhibits a strong correlation even at the shortest time scales. ...|$|E
40|$|This paper re-examines the {{empirical}} {{evidence on the}} price puzzle and proposes a new theoretical interpretation. Using structural VARs and two different identification strategies based on zero restrictions and sign restrictions, {{we find that the}} positive response of price to a monetary policy shock is historically limited to the sub-samples associated with a weak central bank response to inflation. These sub-samples correspond to the pre-Volcker period for the US and the pre-inflation targeting regime for the UK. Using a micro-founded DSGE sticky price model of the US economy, we then show that the structural VARs are capable of reproducing the price puzzle on artificial data only when monetary policy is passive and hence multiple equilibria arise. In contrast, the DSGE model never generates on impact a positive inflation response to a policy shock. The omission in the VARs of a variable capturing the high persistence of expected inflation under indeterminacy is found to account for the price puzzle observed on actual data. Price puzzle, DSGE model, Taylor <b>principle,</b> <b>Indeterminacy,</b> SVARs...|$|R
40|$|This PhD {{dissertation}} {{regards the}} limitations to constitutional amendments, and the limitations to constitutional changes deriving {{from the process}} of European integration. Both these two categories are often conceived under a comprehensive notion called 'supreme principles of the legal order'. The investigation of this category has been the object of vast attention by the legal scholarship, but {{the results of the}} best efforts have been significantly diverse. Moving from this fact, the dissertation applied an innovative empirical methodology to this subject, shifting the main focus on the epiphanies of the supreme principles of the legal order in the constitutional law “in action”. This route led to recognising a deep normative <b>indeterminacy</b> of those <b>principles</b> in action, and a sort of logical independence of the concept from the prescription eventually disposing the limitations to constitutional amendments of changes. The next step consisted in building a re-constructive hypothesis, in order to highlight the supreme principles' essential role: that of closing the circle of a constitutional State where no player has the right to say the last word...|$|R
40|$|Paper {{presented}} {{during the}} Annual Conference of the Philosophical Society of Southern Africa, 16 - 18 January 2008. Hosted by the Department of Philosophy, University of Pretoria. ABSTRACT: It is a foundational premise of classical logic that the Principle of Bivalence obtains. This paper will see an exposition {{of how it}} can come {{to be that the}} truth value of some statements is indeterminate, in other words, that it cannot always be presumed that the truth value of a statement will be either true or false. The paper will investigate two premises for an argument for <b>indeterminacy</b> (counter the <b>Principle</b> of Bivalence) : (1) If the metaphysical state of affairs is such that no evidence is available to serve as truth conditions for a claim, that claim is indeterminate and not merely false. (2) If truth is regarded an epistemic notion (evidentially constrained) then many of our statements remain indeterminate in truth value. The conclusion forwarded will be that premise (1) is more problematic than premise (2) but can still serve, if adequately defended, an argument for indeterminacy. If, however, both premises are accepted they would labour interdependently for such a conclusion. Accepting that certain statements remain indeterminate will mean that the unconditional acceptance of the Principle of Bivalence is wrong...|$|R
40|$|Recent {{involvement}} in designing {{and developing a}} simulation model to allow interaction between biophysical, economic and social processes has also led to interest in uncertainty and error propagation in models. This uncertainty exists {{in each of the}} biophysical, economic and social domains. With regard to the hydrologic processes there appears to be an <b>indeterminacy</b> <b>principle</b> that makes up-scaling difficult. When this is combined with the uncertainty in the other aspects of the model it would suggest that caution is needed in interpreting output from such models...|$|E
40|$|There {{are reasons}} to doubt that {{making sense of the}} wave {{function}} (other than as a probability algorithm) will help with the project of making sense of quantum mechanics. The consistency of the quantum-mechanical correlation laws with the existence of their correlata is demonstrated. The demonstration makes use of the fact (which is implied by the <b>indeterminacy</b> <b>principle)</b> that physical space is not partitioned "all the way down," and it requires that the eigenvalue-eigenstate link be replaced by a different interpretive principle, whose implications are explored. Comment: 12 pages, contribution to an online workshop on the meaning of the wave function, slightly revise...|$|E
40|$|We {{argue that}} the {{classical}} description of a symplectic manifold endowed with a Hamiltonian action of an abelian Lie group G and the corresponding quantum theory {{can be understood as}} different aspects of the unitary representation theory of G. To do so, we propose a conceptual analysis of formal tools coming from symplectic geometry (notably, Souriau's moment map and the Mardsen-Weinstein symplectic reduction formalism) and group representation theory (notably Kirillov's orbit method). The proposed argumentative line strongly relies on the conjecture proposed by Guillemin and Sternberg according to which "quantization commutes with (symplectic) reduction". By using the generalization of this conjecture to non-zero coadjoint orbits, we argue that phase invariance in quantum mechanics and gauge invariance have a common geometric underpinning, namely the symplectic reduction formalism. This fact points towards a gauge-theoretic interpretation of Heisenberg <b>indeterminacy</b> <b>principle...</b>|$|E
40|$|The {{holographic}} indeterminacy {{resulting from}} the quantization of spacetime leads to an inherent uncertainty (lpL) 1 / 2 in the relative positions of two events, separated by a distance L, in a direction transverse to a null ray connecting the events, where lP is the Planck length. The new <b>indeterminacy</b> <b>principle</b> leads to a critical condition in which the holographic uncertainty in the relative transverse positions of two diametrically opposed particles on the surface a body becomes greater than the average distance between particles in the body. The Chandrasekhar mass and the characteristic nuclear density emerge as the minimum mass and density of a baryonic body that could meet the critical criteria. Neutron stars are therefore identified as a class of bodies in which holographic indeterminacy may have physical consequences. Comment: 3 pages, no figures, no table...|$|E
40|$|Abstract. We {{argue that}} the {{classical}} description of a symplectic manifold endowed with a Hamil-tonian action of an abelian Lie group G and the corresponding quantum theory {{can be understood as}} different aspects of the unitary representation theory of G. To do so, we propose a conceptual analysis of formal tools coming from symplectic geometry (notably, Souriau’s moment map and the Mardsen-Weinstein symplectic reduction formalism) and group representation theory (notably Kirillov’s orbit method). The proposed argumentative line strongly relies on the conjecture pro-posed by Guillemin and Sternberg according to which “quantization commutes with (symplectic) reduction”. By using the generalization of this conjecture to non-zero coadjoint orbits, we argue that phase invariance in quantum mechanics and gauge invariance have a common geometric un-derpinning, namely the symplectic reduction formalism. This fact points towards a gauge-theoretic interpretation of Heisenberg <b>indeterminacy</b> <b>principle.</b> Key words: quantum mechanics; gauge theories; moment map; symplectic reduction. 1...|$|E
40|$|Modern {{developments}} of atomic theory have required alterations {{in some of}} the most fundamental physical ideas. This has resulted in its being usually easier to discover the equations that describe some particular phenomenon than just how the equations are to be interpreted. The quantum mechanics of Heisenberg and Schrodinger was first worked out for a number of simple examples, from which a general mathematical scheme was constructed, and afterwards people were led to the general physical principles governing the interpretation, such as the superposition of states and the <b>indeterminacy</b> <b>principle.</b> In this way a satisfactory nonrelativistic quantum mechanics was established. In extending the theory to make it relativistic, the developments needed in the mathematical scheme are easily worked out, but difficulties arise in the interpretation. If one keeps to the same basis of interpretation as in the non-relativistic theory, one finds that particles have states of negative kinetic energy as well as their usual states of positive energy...|$|E
40|$|Some {{notes and}} {{questions}} about the concept of time are exposed. Particular reference {{is given to the}} problem in quantum mechanics, in connection with the <b>indeterminacy</b> <b>principle.</b> PACS: 03. 65. -w Quantum mechanics. 03. 65. Bz Foundations, theory of measurement, miscellaneous theories. “How much time”, “It took quite some time”, “There is plenty of time”. These are some examples only of our common way to think about time: an interval between two instants. In physics as well time is seen as an interval. Asher Peres stressed that the measurement of time is the observation of a dynamical variable, which law of motion is known and it is uniform and constant in time [1]. There is a kind of self–reference in this definition and nothing is said about time. On the other hand, time is considered simply as a parameter and, according to this, the above definition is completely satisfactory. Moreover, time is sometimes neglected (e. g. steady state phenomena) ...|$|E
40|$|Complex is {{a special}} {{attribute}} we can give to many kinds of systems. In a broad sense, it can be taken as synonym of unpredictable. Human systems are affected by several sources of complexity, belonging to three classes: logical, gnosiological and computational, in order of descending restrictivity. Systems belonging to first class are not predictable at all, those belonging to the second class are predictable only through an infinite computational capacity, and those belonging to the third class are predictable only through a trans-computational capacity. Using (also) a dialectical criterion of demarcation, we can distinguish a precise and useful {{meaning of the word}} "complexity",different from that of "difficulty". In first class (logical complexity) are two sources of complexity: the pure logical complexity, directly deriving from self-reference and Gödel incompleteness theorems, and the relational complexity, resulting in a sort of <b>indeterminacy</b> <b>principle</b> occurring in social systems. In the second class (gnosiological complexity) are four sources of complexity: pure gnosiological complexity, which consists in the variety of possible perceptions; the evolutionary complexity, which derives from the genuine notion of evolution; the semiotic complexity...|$|E
40|$|The {{uncertainty}} principle {{can be understood}} as a condition of joint indeterminacy of classes of properties in quantum theory. The mathematical expressions most closely associated with this principle have been the uncertainty relations, various inequalities exemplified by the well known expression regarding position and momentum introduced by Heisenberg. Here, recent work involving a new sort of “logical” <b>indeterminacy</b> <b>principle</b> and associated relations introduced by Pitowsky, expressable directly in terms of probabilities of outcomes of measurements of sharp quantum observables, is reviewed and its quantum nature is discussed. These novel relations are derivable from Boolean “conditions of possible experience” of the quantum realm and have been considered both as fundamentally logical and as fundamentally geometrical. This work focuses on the relationship of indeterminacy to the propositions regarding the values of discrete, sharp observables of quantum systems. Here, reasons for favoring each of these two positions are considered. Finally, with an eye toward future research related to indeterminacy relations, further novel approaches grounded in category theory and intended to capture and reconceptualize the complementarity characteristics of quantum propositions are discussed in relation to the former...|$|E
40|$|Keywords:Uncertainty, hydrology, modelling, social systems, {{economic}} systems Recent involvement in designing {{and developing a}} simulation model to allow interaction between biophysical, economic and social processes has also led to interest in uncertainty and error propagation in models. This uncertainty exists {{in each of the}} biophysical, economic and social domains. With regard to the hydrologic processes there appears to be an <b>indeterminacy</b> <b>principle</b> that makes up-scaling difficult. When this is combined with the uncertainty in the other aspects of the model it would suggest that caution is needed in interpreting output from such models. The uncertainty in biophysical, social and {{economic systems}} is a combination of uncertainty in; data input and model structure. As we build models where non-linearities due to the model structure are incorporated the relative uncertainty in the outputs will grow rapidly. Using a simple model where data input errors are either added or multiplied together we can see the consequences for the relative error in the output (figure 1). Suppose we have a process that results in a local (cell) parameter yi with standard deviation σi and the area of the plot is ai. We will assume that the same value and standard deviation occur in n other plots such that: and o...|$|E
40|$|Through {{a series}} of {{brillant}} paper publications dating from the late 1940 s, Dennis Gabor developed a new conceptual and operational framework {{for the analysis of}} sound signals, based on a quantum-oriented view of acoustical phenomena. In the present essay, I try to illustrate the shaping up of Gabor’s quantum analysis of sound, especially as delineated in two papers from 1946 (Theory of communication) and 1947 (Acoustical quanta and the theory of hearing), and to overview its legacy in scientific research as well as in audio and musical applications. After some introductory remarks, I follow a hybrid path, between history of science and history of audio technology, sketching a “genealogy” of Gabor’s quantum view of sound (i. e. as connected to Heisenberg’s <b>indeterminacy</b> <b>principle,</b> to Mach’s analysis of sensation, etc.), and relating it to an empiricist tradition of modern science. I eventually situate his research in the context of contemporary research preoccupations shared by other, at the time, and I finally discuss some of the earliest audio devices that seem to tie back to Gabor’s own practical experiments (beside his theoretical framework) and that revealed of primary interest to pioneers in analog and digital music technologies and related creative practices...|$|E
40|$|We {{resort to}} a logical {{phenomenon}} related to paradoxes and possibly to other logical facts, like limitation theorems and transfinite set theory, {{to shed light}} upon the meaning of Heisenberg’s <b>Indeterminacy</b> <b>Principle.</b> Our aim is to show how critical realism as opposed to naïve realism might be consistent with this principle and its empirical and theoretical consequences. Call (1) the sentence-token “(1) expresses no true proposition”. (1) has no truth value, so it describes no state-of-affairs. We partly state this {{by means of the}} sentence-token (2) which says “(1) expresses no true proposition”. (2) describes an actual state-of-affairs and is true. This is possible only because (1) and (2) are uttered in different logical contexts. Since (2) is based upon a previous assessment of (1), we say that (2) stands in a logically posterior instant. There is a state-of-affairs that is an available state-of-affairs in the logical instant to which (2) belongs but is not yet such in the logical instant in which (1) stands. Only the introduction of this kind of logical temporality makes the set of available states-of-affairs relative to the logical context, thereby rendering possible the solution to the paradox. This reveals that some logical objects are distributed along some logical temporality. The reason of this relativity resides in a fact that can be couched in the terms of Husserl’...|$|E
