39|271|Public
25|$|The hypercycle {{concept has}} been {{continuously}} studied since its origin. Shortly after Eigen and Schuster published their main work regarding hypercycles, John Maynard Smith raised an objection that the catalytic {{support for the}} replication given to other molecules is altruistic. Therefore, it cannot be selected and maintained in a system. He also underlined hypercycle vulnerability to parasites, as they are favoured by selection. Later on, Josef Hofbauer and Karl Sigmund indicated that in reality, a hypercycle can maintain only fewer than five members. In agreement with Eigen and Schuster's principal analysis, they argued that systems with five or more species exhibit limited and unstable cyclic behaviour, because some species can die out due to stochastic events and break the positive feedback loop that sustains the hypercycle. The extinction of the hypercycle then follows. It was also emphasized that a hypercycle size of up to four {{is too small to}} maintain the amount of information sufficient to cross the <b>information</b> <b>threshold.</b>|$|E
50|$|There {{are also}} {{communities}} in Australia (see Cohousing Australia), the United Kingdom (see UK Cohousing Network http://www.cohousing.org.uk for <b>information,</b> <b>Threshold</b> Centre Cohousing Community http://www.thresholdcentre.org.uk/ offers training), {{and other parts}} of the world.|$|E
50|$|The {{directive}} on ozone {{in ambient}} air came into force in 2002. The long-term {{objective is to}} limit the number of days with average ozone concentrations above 120 µg/m3 to less than 25 days a year (see table below). The objectives of the directive are in line with the World Health Organizations guidelines for ozone.The directive requires national authorities to inform the public - hourly or daily - about any incidence of ozone pollution above 180 µg/m3 (<b>information</b> <b>threshold).</b> Also, Member States should report monthly to the European Commission on all exceedances of the information and alert thresholds (see table below). Each year they must provide additional information on ozone pollution, especially concerning exceedances of the long-term objective.|$|E
40|$|Herd {{behaviour}} {{in financial}} markets is a recurring phenomenon that exacerbates asset price volatility, {{and is considered}} a possible contributor to market fragility. While numerous studies investigate herd behaviour in financial markets, it is often considered {{without reference to the}} pricing of financial instruments or other market dynamics. Here, a trader interaction model based upon informational cascades in the presence of <b>information</b> <b>thresholds</b> is used to construct a new model of asset price returns that allows for both quiescent and herd-like regimes. Agent interaction is modelled using a stochastic pulse-coupled network, parametrised by <b>information</b> <b>thresholds</b> and a network coupling probability. Agents may possess either one or two <b>information</b> <b>thresholds</b> that, in each case, determine the number of distinct states an agent may occupy before trading takes place. In the case where agents possess two thresholds (labelled as the finite state-space model, corresponding to agents' accumulating information over a bounded state-space), and where coupling strength is maximal, an asymptotic expression for the cascade-size probability is derived and shown to follow a power law when a critical value of network coupling probability is attained. For a range of model parameters, a mixture of negative binomial distributions is used to approximate the cascade-size distribution. This approximation is subsequently used to express the volatility of model price returns in terms of the model parameter which controls the network coupling probability. In the case where agents possess a single pulse-coupling threshold (labelled as the semi-infinite state-space model corresponding to agents' accumulating information over an unbounded state-space), numerical evidence is presented that demonstrates volatility clustering and long-memory patterns in the volatility of asset returns. Finally, output from the model is compared to both the distribution of historical stock returns and the market price of an equity index option...|$|R
40|$|Abstract-Random dot {{techniques}} {{were used to}} investigate the human visual system’s sensitivity to sinusoidal depth modulations specified by motion parallax <b>information.</b> <b>Thresholds</b> for perceiving depth {{were found to be}} smallest when the spatial frequency of the depth corrugations was between 0. 2 and 0. 5 c/deg visual angle. These data were compared with the equivalent thresholds for perceiving depth corrugations specified by binocular disparity using similar apparatus and psychophysical procedures. The similarity between the sensitivity functions is suggestive of a closer relationship between the two systems than has previously been thought...|$|R
40|$|This paper {{proposes a}} method for {{unsupervised}} selection of features for detecting important events in a surveillance context. While traditional feature selection requires manually annotated ground truth to choose the best features, we examine the possibility of exploiting the redundancy between a pair of independent data sources for selecting good detection features. Building on our prior work on mutual <b>information</b> <b>thresholding</b> [2], we show that strong agreement between data sources indicates strong detection performance. Experimental tests, combining visual and audio data, show that the best performing features can be automatically selected {{by taking advantage of}} the common information shared by the sensors...|$|R
50|$|The hypercycle {{concept has}} been {{continuously}} studied since its origin. Shortly after Eigen and Schuster published their main work regarding hypercycles, John Maynard Smith raised an objection that the catalytic {{support for the}} replication given to other molecules is altruistic. Therefore, it cannot be selected and maintained in a system. He also underlined hypercycle vulnerability to parasites, as they are favoured by selection. Later on, Josef Hofbauer and Karl Sigmund indicated that in reality, a hypercycle can maintain only fewer than five members. In agreement with Eigen and Schuster's principal analysis, they argued that systems with five or more species exhibit limited and unstable cyclic behaviour, because some species can die out due to stochastic events and break the positive feedback loop that sustains the hypercycle. The extinction of the hypercycle then follows. It was also emphasized that a hypercycle size of up to four {{is too small to}} maintain the amount of information sufficient to cross the <b>information</b> <b>threshold.</b>|$|E
30|$|Based {{on these}} studies, in the {{following}} experiments, we use 0.7 as the mutual <b>information</b> <b>threshold</b> and 10 web pages as the source data for the AutoTL by default.|$|E
40|$|Eco-evolutionary dynamics, coding {{structure}} and the <b>information</b> <b>threshold</b> Folkert K de Boer *, Paulien Hogeweg Background: The amount of {{information that can be}} maintained in an evolutionary system of replicators is limited by genome length, the number of errors during replication (mutation rate) and various external factors that influence the selection pressure. To date, this phenomenon, known as the <b>information</b> <b>threshold,</b> has been studied (both genotypically and phenotypically) in a constant environment and with respect to maintenance (as opposed to accumulation) of information. Here we take a broader perspective on this problem by studying the accumulation of information in an ecosystem, given an evolvable coding structure. Moreover, our setup allows for individual based as well as ecosystem based solutions. That is, all functions can be performed by individual replicators, or complementing functions can be performed by different replicators. In this setup, where both the ecosystem and the individual genomes can evolve their structure, we study how populations cope with high mutation rates and accordingly how the <b>information</b> <b>threshold</b> might be alleviated. Results: We observe that the first response to increased mutation rates is a change in coding structure. A...|$|E
30|$|In the {{discrete}} cosine transform, {{the first}} coefficient {{is basically the}} average of all pixel values within a given block [17]. Therefore, for a precise inverse transforming result, the accurate DC coefficient is crucial. In {{order to make sure}} that the inverse transformation result is the product of vital block information, our proposed method does not apply hard thresholding on the DC coefficient. Instead, it only applies hard thresholding on AC coefficients. The AC coefficients carry various block frequency information [1, 7, 17]. This information varies from low-frequency to high-frequency <b>information.</b> <b>Thresholding</b> all AC coefficients in the same manner might lead to losing some significant information, while preserving some other insignificant information.|$|R
40|$|The {{ability to}} detect faces in images is of {{critical}} ecological significance. It is a pre-requisite for other important face perception tasks such as person identification, gender classification and affect analysis. Here we {{address the question of}} how the visual system classifies images into face and non-face patterns. We focus on face detection in impoverished images, which allow us to explore <b>information</b> <b>thresholds</b> required for different levels of performance. Our experimental results provide lower bounds on image resolution needed for reliable discrimination between face and non-face patterns and help characterize the nature of facial representations used by the visual system under degraded viewing conditions. Specifically, they enable an evaluation of the contribution of luminance contrast, image orientation and local context on face-detection performance...|$|R
40|$|Teaching <b>Information</b> Literacy <b>Threshold</b> Concepts: Lesson Plans for Librarians is a {{collection}} designed by instruction librarians to promote critical thinking and engaged learning. It provides teaching librarians detailed, ready-to-use, and easily adaptable lesson ideas to help students understand and be transformed by <b>information</b> literacy <b>threshold</b> concepts. The lessons in this book, created by teaching librarians across the country, are categorized according to the six information literacy frames identified in the ACRL Framework for Information Literacy in Higher Education (2015). This volume offers concrete and specific ways of teaching the threshold concepts that {{are central to the}} ACRL Framework and is suitable for all types of academic libraries, high school libraries, as well as a pedagogical tool for library and information schools...|$|R
30|$|There are two {{important}} factors that would impact {{the performance of}} the AutoTL: the mutual <b>information</b> <b>threshold</b> ε of determining whether two features are correlated and the number of web pages selected as the source data. Hence, we first run two sets of experiments to study how these two factors impact the performance and then figure out the right one as the default setting in the following experiments.|$|E
40|$|In the Republic of Croatia, {{according}} to the Air Protection Act, air pollution assessment is obligatory on the whole State territory. For individual regions and populated areas in the State a network has been established for permanent air quality monitoring. The State network consists of stations for measuring background pollution, regional and cross-border remote transfer and measurements as part of international government liabilities, then stations for measuring air quality in areas of cultural and natural heritage, and stations for measuring air pollution in towns and industrial zones. The exceeding of alert and <b>information</b> <b>threshold</b> levels of air pollutants are related to emissions from industrial plants, and accidents. Each excess represents a threat to human health in case of short-time exposure. Monitoring of alert and <b>information</b> <b>threshold</b> levels is carried out at stations {{from the state and}} local networks for permanent air quality monitoring {{according to}} the Air Quality Measurement Program in the State network for permanent monitoring of air quality and air quality measurement programs in local networks for permanent air quality monitoring. The State network for permanent air quality monitoring has a developed automatic system for reporting on alert and <b>information</b> <b>threshold</b> levels, whereas many local networks under the competence of regional and local self-governments still lack any fully installed systems of this type. In case of accidents, prompt action at all responsibility levels is necessary in order to prevent crisis and this requires developed and coordinated competent units of State Administration as well as self-government units. It is also necessary to be continuously active in improving the implementation of legislative regulations in the field of crises related to critical and alert levels of air pollutants, especially at local levels...|$|E
40|$|Abstract Background The {{amount of}} {{information}} that can be maintained in an evolutionary system of replicators is limited by genome length, the number of errors during replication (mutation rate) and various external factors that influence the selection pressure. To date, this phenomenon, known as the <b>information</b> <b>threshold,</b> has been studied (both genotypically and phenotypically) in a constant environment and with respect to maintenance (as opposed to accumulation) of information. Here we take a broader perspective on this problem by studying the accumulation of information in an ecosystem, given an evolvable coding structure. Moreover, our setup allows for individual based as well as ecosystem based solutions. That is, all functions can be performed by individual replicators, or complementing functions can be performed by different replicators. In this setup, where both the ecosystem and the individual genomes can evolve their structure, we study how populations cope with high mutation rates and accordingly how the <b>information</b> <b>threshold</b> might be alleviated. Results We observe that the first response to increased mutation rates is a change in coding structure. At moderate mutation rates evolution leads to longer genomes with a higher diversity than at high mutation rates. Thus, counter-intuitively, at higher mutation rates diversity is reduced and the efficacy of the evolutionary process is decreased. Therefore, moderate mutation rates allow for more degrees of freedom in exploring genotype space during the evolutionary trajectory, facilitating the emergence of solutions. When an individual based solution cannot be attained due to high mutation rates, spatial structuring of the ecosystem can accommodate the evolution of ecosystem based solutions. Conclusions We conclude that the evolutionary freedom (eg. the number of genotypes that can be reached by evolution) is increasingly restricted by higher mutation rates. In the case of such severe mutation rates that an individual based solution cannot be evolved, the ecosystem can take over and still process the required information forming ecosystem based solutions. We provide a proof of principle for species fulfilling the different roles in an ecosystem when single replicators can no longer cope with all functions simultaneously. This could be a first step in crossing the <b>information</b> <b>threshold.</b> </p...|$|E
40|$|As the {{performance}} of long-term projects is not observable {{in the short run}} politicians may pander to public opinion. To solve this problem, we propose a triple mechanism involving political <b>information</b> markets, reelection <b>threshold</b> contracts, and democratic elections. An information market is used to predict the long-term performance of a policy, while threshold contracts stipulate a price level on the political information market that a politician must reach {{to have the right to}} stand for reelection. Reelection thresholds are offered by politicians during campaigns. We show that, on balance, the triple mechanism increases social welfare. democracy; elections; <b>information</b> markets; <b>threshold</b> contracts; triple mechanism...|$|R
40|$|The {{theory of}} global games {{has shown that}} {{coordination}} games with multiple equilibria may have a unique equilibrium if certain parameters of the payoff function are private information instead of common knowledge. We report {{the results of an}} experiment designed to test the predictions of this theory. Comparing sessions with common and private information, we observe only small differences in behavior. For common information, subjects coordinate on threshold strategies that deviate from the global game solution towards the payoff-dominant equilibrium. For private <b>information,</b> <b>thresholds</b> are closer to the global game solution than for common information. Variations in the payoff function affect behavior as predicted by comparative statics of the global game solution. Predictability of coordination points is about the same for both information conditions. Copyright The Econometric Society 2004. ...|$|R
40|$|Politicians may pander {{to public}} opinion and may renounce {{undertaking}} beneficial long-term projects. To alleviate this problem, we introduce a triple mechanism involving political <b>information</b> markets, reelection <b>threshold</b> contracts, and democratic elections. An information market is used to predict the long-term performance of a policy, while threshold contracts stipulate a price level on the political information market that a politician must reach {{to have the right}} to stand for reelection. Reelection thresholds are offered by politicians during campaigns. We show that, on balance, the triple mechanism increases social welfare. Finally, we suggest several ways to avoid the manipulation of information markets and we discuss possible pitfalls of the mechanism. democracy, elections, <b>information</b> markets, <b>threshold</b> contracts and triple mechanism...|$|R
30|$|From these results, we obtain two insights. First, {{selecting}} different mutual <b>information</b> <b>threshold</b> {{to determine}} whether two features are correlated impact the performance. Second, AutoTL achieves a better performance when the threshold is set around 0.7, while the performance decreases when the threshold is set too small or too large. For example, the performance of point 0.2 and 0.8 is worse than that of 0.7. This is within expectation, as a small or large threshold would either result in too many unrelated features or too less correlated features which all lead to a worse learning result.|$|E
40|$|In 2011, Thailand {{witnessed}} {{its worst}} flooding catastrophe {{in half a}} century. In this study, we explored social media as a new and promising weapon to address the physical and morale challenges caused by the natural disaster. A case {{study was conducted in}} the context of crisis response, which investigated the use of social media to contribute to the collective cultural repertoire during the natural disaster. By investigating two paths toward the cultural repertoire construction considering different social groups, this study also identified the roles of social media as an information market and an <b>information</b> <b>threshold</b> in the crisis response...|$|E
40|$|We {{addressed}} {{the evolution of}} complexity {{by looking at the}} possibilities to accumulate, contain and/or use more information in replicators through either a division of labor or a ‘smart’ coding. We have shown that both these mechanisms can evolve in relatively simple evolutionary models and that both these mechanisms can be identified in biotic systems. Multiple coding, is widely observed in biotic systems: genetic information is often used to code for distinct biological roles and many different gene-products are produced by post-transcriptional and post-translational modifications. Moreover, the genotype-phenotype mapping may be more important than the coding of information itself, as supported by experimental data, which shows that over different organisms, protein coding sequences are highly conserved, while the relative amount of non-protein-coding sequence increases consistently with complexity. On {{the other side of the}} spectrum, an intriguing parallel with our ecosystem based solutions comes from the recently developing field of meta-genomics, where rather than individual based diversity, the number of species and population diversity are responsible for the molecular functional composition of these communities. We only studied strict vertical inheritance, however in the light of an ecosystem as a ‘parts-list’, including a mechanism for horizontal gene transfer would allow for an interesting mix of population based diversity and individual based diversity. That is, there are organisms, which have been identified to use horizontal gene transfer as an important way of information accumulation and an overrepresentation of transfers of defense mechanisms and transcription-regulation genes is linked with more ecological interactions in species-rich communities. This ecological, as opposed to vertical, signal of genetic information, suggests that ecosystems, consisting of species with distinct roles, might ‘choose’ which genes are shared, and which are not. Moreover, it is the ecosystem as a whole which evolves, and it has been suggested that the high level of novelty required to evolve cell designs is a product of ecosystems, through horizontal gene transfer, rather than intralineage variation. In early replicators, such horizontal gene transfer could lead to the rapid spread of new genes and allow the build-up of larger, fitter genomes than could be achieved by purely vertical inheritance. The question if we ‘solved’ the problem of the <b>information</b> <b>threshold,</b> unfortunately can only be answered with a rather unsatisfactory “Yes and no”. The error threshold, in the sense of “a critical size, beyond which lethal numbers of deleterious mutations accumulate and information in subsequent generations will be destroyed”, still limits genomic size, as the mutational load which replicators can sustain, did not increase. However, we showed that both a population based as an individual based solution exists to increase the information processing potential, despite high mutation rates: even under severe mutation rates information may still be processed, either by a division of information or by enabling multiple coding through the use of a primitive form of RNA-modification (not to be confused with a repair mechanism). Thus, if we consider the <b>information</b> <b>threshold</b> as “the maximum amount of information, which can be maintained on a genome of critical size given a certain mutation rate”, the boundaries are clearly shifted. Concluding, rather than solving the problem of the <b>information</b> <b>threshold,</b> our results show that the <b>information</b> <b>threshold</b> should be considered as a dynamic property of evolving systems, shaping the structure of information and by including enough degrees of freedom (degrees which are supposed to be present in biological evolution), more is possible in ecosystems and/or with small-sized genomes...|$|E
40|$|This paper {{considers}} information criteria as model evaluation {{tools for}} nonlinear threshold models. Results concerning {{the consistency of}} information criteria in selecting the lag order of linear autoregressive models are extended to nonlinear autoregressive threshold models. Extensive Monte Carlo evidence of the small sample performance {{of a number of}} criteria is presented. Nonlinearity, Model selection, <b>Information</b> criteria, <b>Threshold</b> models...|$|R
40|$|An {{innovative}} hierarchical {{image segmentation}} scheme is {{reported in this}} research communication. Unlike static / spatially divided sub-images, the current innovation concentrates on object level hierarchy for segmentation of gray scale or color images into constituent component / sub-parts. As for example, a gray scale document image may be segmented (binarized in case of two-level segmentation) into connected foreground components (text / graphics) and background component by hierarchically applying a gray level threshold selection algorithm in the object-space. In any hierarchy, constituent objects are identified as connected foreground pixels, as classified by the gray scale threshold selection algorithm. To preserve the global <b>information,</b> <b>thresholds</b> for each object in any hierarchy are estimated as a weighted aggregate of the current and previous thresholds relevant to the object. The developed technique may be customized as a general purpose hierarchical information clustering algorithm {{in the domain of}} pattern analysis, data mining, bioinformatics etc...|$|R
40|$|In this paper, de-anonymizing {{internet}} users by actively querying their group memberships in social networks is considered. In this problem, an anonymous victim visits the attacker's website, and the attacker uses the victim's browser history to query her social media activity {{for the purpose}} of de-anonymization using the minimum number of queries. A stochastic model of the problem is considered where the attacker has partial prior knowledge of the group membership graph and receives noisy responses to its real-time queries. The victim's identity is assumed to be chosen randomly based on a given distribution which models the users' risk of visiting the malicious website. A de-anonymization algorithm is proposed which operates based on <b>information</b> <b>thresholds</b> and its performance both in the finite and asymptotically large social network regimes is analyzed. Furthermore, a converse result is provided which proves the optimality of the proposed attack strategy...|$|R
40|$|A digital {{literacy}} and cultural workshop which encourages twenty young inmates {{to create and}} publish blogs is being {{carried out in the}} Juvenile Detention Facility of Barcelona since July 2006. It's a collaboration between the prison library and Òmnia point, and the University of Barcelona Faculty of Library and Information Science. The aims of this activity were to promote inmates' education and raise their <b>information</b> <b>threshold,</b> to encourage them to read and write more, to expand their contact with the outer environment, to arouse or to strengthen their informational abilities and to develop their {{digital literacy}}. A critical assessment is made about Internet access as a necessary right that can favor inmates' reintegration, and about prison libraries, understood as tools that can have influence on the future inmates' reintegration into society and the labour market...|$|E
40|$|In this study, a {{time series}} {{from the ground}} level ozone {{monitoring}} station for Rijeka (Croatia) {{as well as the}} associated meteorological conditions were investigated. In the summer, during 13 to 19 August 2000, the afternoon hourly ozone measurements were consistently higher than the 180 μg m- 3 which represents an <b>information</b> <b>threshold</b> of pollutant concentrations according to the national standards. During the night the ozone concentrations were unusually very high as well, above 100 μg m- 3. An analysis of the observations was performed to identify characteristics of the low-level wind in the Rijeka Bay and the results of a nonhydrostatic mesoscale meteorological model WRF were compared with the observations with respect to these characteristics. The contribution of the distant pollution sources to observed concentrations was evaluated by the atmospheric chemical model EMEP...|$|E
40|$|We apply our {{recently}} developed information-theoretic measures for the characterisation and comparison of protein– protein interaction networks. These measures {{are used to}} quantify topological network features via macroscopic statistical properties. Network differences are assessed based on these macroscopic properties as opposed to microscopic overlap, homology information or motif occurrences. We present {{the results of a}} large–scale analysis of protein–protein interaction networks. Precise null models are used in our analyses, allowing for reliable interpretation of the results. By quantifying the methodological biases of the experimental data, we can define an <b>information</b> <b>threshold</b> above which networks may be deemed to comprise consistent macroscopic topological properties, despite their small microscopic overlaps. Based on this rationale, data from yeast–two–hybrid methods are sufficiently consistent to allow for intra–species comparisons (between different experiments) and inter–species comparisons, while data from affinity–purification mass–spectrometry method...|$|E
3000|$|... where x is {{the input}} data and μ* and Σ* {{are the two}} {{estimated}} mixture parameters. Since the segmentation method is completely unsupervised and not input-dependent, the two resulting distributions will be slightly different from subject to subject in terms of count rate, thus avoiding the need to specify a priori <b>information</b> or <b>threshold</b> values that could be subject-specific or scanner-specific.|$|R
40|$|This paper {{presents}} a new neural network based method for the segmentation of sky from video {{data collected from}} a vehicle with a fixed camera position. Using a combination of spatial information, a neural network and thresholding, {{a high degree of}} success has been achieved with the images tested. Having the approximate location of the sky allows for an initial starting point for segmentation to be determined. By training aneural network on various sky pixel data, it is possible to find starting locations for thresholding despite the effects of different lighting conditions which significantly affect the colour of the sky in an image. Using this <b>information,</b> <b>thresholds</b> based on colour difference can be employed to discover sky connected pixels. Due to the similar colour of poles to sky, these must then be subtracted from the discovered sky pixels using an edge detection algorithm. The results have been compared with both an SVM and exclusive thresholding technique and comparative analysis is presented in this paper...|$|R
40|$|The {{objective}} of the research was to scrutinize probabilistic causalities between integration, <b>information</b> <b>thresholds,</b> and arrangement in mind dynamic. Data obtained from videotaped sessions with structured observation. The participants had to accomplish four tasks. The participants were 39 females, and 83 males. Reliability and validity assessed as probabilities. The frequencies converted into probability matrices, and sampling without replacement was necessary. Thereafter, a causal state space originated, and maintained through Householder matrices. The Bayes formula with joint distributions in a matrix form applied to result in the start matrix for the causal dynamic. The reduced start array matrix powered from 1 to 6. There are the probabilistic causalities between the integration, the thresholds, and the arrangement. Theoretic results show. It is the entire mind of the persons strives to form patterns for the causal functioning, continuously. Furthermore, the whole mind conveys mental contents under the same patterns. The patterns remain but {{the contents of the}} processes differ during the mindamic...|$|R
40|$|The {{spread of}} disease through a physical-contact {{network and the}} spread of {{information}} about the disease on a communication network are two intimately related dynamical processes. We investigate the asymmetrical interplay between the two types of spreading dynamics, each occurring on its own layer, by focusing on the two fundamental quantities underlying any spreading process: epidemic threshold and the final infection ratio. We find that an epidemic outbreak on the contact layer can induce an outbreak on the communication layer, and information spreading can effectively raise the epidemic threshold. When structural correlation exists between the two layers, the <b>information</b> <b>threshold</b> remains unchanged but the epidemic threshold can be enhanced, making the contact layer more resilient to epidemic outbreak. We develop a physical theory to understand the intricate interplay between the two types of spreading dynamics. Comment: 29 pages, 14 figure...|$|E
40|$|A {{comparison}} {{study has}} been performed with neural networks (NNs) and multiple linear regression models to forecast the next day's maximum hourly ozone concentration in the Athens basin at four representative monitoring stations that show very different behavior. All models use 11 predictors (eight meteorological and three persistence variables) and are developed and validated between April and October from 1992 to 1999. Performance results based on a wide set of forecast quality measures indicate that the NNs provide better estimates of ozone concentrations at the monitoring sites, whilst the more often used linear models are less efficient at accurately forecasting high ozone concentrations. The violation of the European <b>information</b> <b>threshold</b> of 180 mug/m(3) is successfully predicted by the NN in 72 % of the cases on average. Results at all stations are consistent with similar ozone forecast studies using NNs in other European cities. (C) 2003 Elsevier Science B. V. All rights reserved...|$|E
40|$|Work on {{single tone}} {{frequency}} estimation {{has focused on}} finding frequency estima-tors for the case where the signal is sampled equidistantly. The maximal estimation accuracy one can achieve {{is limited by the}} Cramér-Rao bound (CRB), and there are estimators which achieve the CRB asymptotically, i. e. for an infinite number of sampling points. To overcome the limit set by the CRB, one can employ improved sampling designs which have lower CRB. This article offers sampling designs which are specifically optimized for a small number of sampling points. The optimal design problem is approached by numer-ical optimization where the weights of predefined sampling points are varied. The problem of ambiguity, i. e. the fact that a set of observations can be explained by more than one frequency estimate, is considered in the numerical optimization. De-pending on the optimization criterion, the CRB of the optimal design can fall below the CRB for equidistant sampling by a factor of more than two. Key words: experimental design, Fisher <b>information,</b> <b>threshold</b> effect of nonlinear estimatiors, frequency scanning laser interferometr...|$|E
40|$|The {{literature}} {{acknowledges the}} relationship between the study of biological systems and that of social and technological systems. However, while {{the relationship between}} the two seems to offer valuable insights, there has been little literature on how insights from the research on information behavior of cells can inform the research of organizational IS. Reflecting this literature gap, the objective of this preliminary Research-in-Progress paper is to raise an awareness of possible new insights from biological IS, as well as to demonstrate how such insights might be incorporated into organizational IS research. In this paper we suggest a few directions for progress that may be interest to IS researchers and illustrate it by exploring one specific example: information overload, which is an issue faced by both biological and organizational IS. By drawing on the similarities and differences between biological and organizational IS, potential information practices to address information overload are explored, using the concept of <b>Information</b> <b>Thresholds...</b>|$|R
40|$|A {{system for}} {{producing}} vehicle routes such as aircraft flight {{plans in the}} presence of weather and other hazards defines static and moving hazards with polygons drawn on a display containing graphic hazard regions. Different hazard types and intensities are displayed differently. Both lateral and vertical geographic depictions are displayed, and hazards can be displayed temporally as well. Users input <b>information</b> and <b>thresholds</b> for hazards...|$|R
30|$|Considering that mostly, {{we did not}} cut beyond 35 % {{harvested}} basal area, {{the maximum}} established in Chilean regulations, research in selection silviculture should also evaluate an ample range of harvesting intensities using a relatively ample range of initial and residual basal areas. This would allow a more robust <b>information</b> on <b>thresholds</b> to conserve {{in the best possible}} manner “old-growthness” (sensu Bauhus et al. (2009)) in managed forest ecosystems.|$|R
