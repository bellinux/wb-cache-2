9|75|Public
40|$|Rigorous {{engineering}} of safety-critical Cyber- Physical Systems (CPS) requires {{integration of}} heterogeneous modeling methods from different disciplines. It is often necessary to view this integration {{from the perspective}} of analyses – algorithms that read and change models. Although analytic integration supports formal contract-based verification of model evolution, it suffers from the limitation of analytic dependency loops. Dependency loops between analyses cannot be resolved based on the existing contract-based verification. This paper makes a step towards using rich architectural description to resolve circular analytic dependencies. We characterize the <b>dependency</b> <b>loop</b> problem and discuss three algorithmic approaches to resolving such loops: analysis iteration, constraint solving, and genetic search. These approaches take advantage of information in multi-view architectures to resolve analytic <b>dependency</b> <b>loop...</b>|$|E
40|$|Dependence between {{software}} packages is {{of importance to}} influence the extendibility and stability of system. In existing programs, dependence mainly manifests for class and component. Here, it has the important guiding sense to our system construction and programming. This paper analyzed the dependency between {{software packages}}, and designed the algorithm for detection existence of the package <b>dependency</b> <b>loop,</b> based on defined dependency, stability, no-responsibility and stability of the System; and then elevated the package design principles. To validate our design methodology in software development, which is valid and can be helpful for the programmers, we developed a software to analyze the dependencies between the software packages and use a graphical method to express this dependency. </p...|$|E
40|$|Physical Systems (CPS) {{requires}} {{integration of}} heterogeneous modeling methods from different disciplines. It is often necessary to view this integration {{from the perspective}} of analyses – algorithms that read and change models. Although such analytic integration supports formal contract-based verification of model evolution, it suffers from the limitation of analytic dependency loops. Dependency loops between analyses cannot be resolved based on existing contract-based verification. This paper makes a step towards using rich architectural descriptions to resolve circular analytic dependencies. We characterize the <b>dependency</b> <b>loop</b> problem and discuss three algorithmic approaches to resolv-ing such loops: analysis iteration, constraint solving, and genetic search. These approaches take advantage of information in multi-view architectures to resolve analytic dependency loops. Keywords—Analytical models, Component architectures, Em-bedded software, Systems engineering and theor...|$|E
40|$|Let Let us now review {{some of the}} {{previous}} research done Since compiler {{spend most of the}} time in executing loop nests because of the dependences, minimizing dependences across loops is vital for compiler optimization. This paper explores two methods, sums of data dependencies and dominant data dependencies for eliminating <b>dependencies</b> in multi-dimensional <b>loops.</b> The first method eliminates dependences by combining other dependencies and method two eliminates dependences by making use of the distance of the dependence. To conclude we provide an algorithm for minimizing the number of dependences in multi-dimensional loops. Index Terms: multi-dimensional <b>dependencies,</b> <b>loop</b> carried dependences. ...|$|R
40|$|Dr. Chien-Chung Chan Since {{processors}} {{spend most}} of the time in executing loop nests because of the dependencies, minimizing <b>dependencies</b> across <b>loops</b> is vital for compiler optimization. This paper explores two methods, sums of data dependencies and dominant data dependencies for eliminating <b>dependencies</b> in multi-dimensional <b>loops.</b> The first method eliminates dependencies by combining other dependencies and method two eliminates dependencies by making use of the distance of the dependence. We also present how to apply loop transformation techniques to these methods to explore much better results. To conclude we provide an algorithm for minimizing the number of <b>dependencies</b> in multi-dimensional <b>loops...</b>|$|R
40|$|Mindflux is a {{collective}} of electronic artists based in Adelaide, an 'amorphous blob' comprising several (de) central conspirators {{and a network}} of cohorts from around the world. Dedicated to exploring the <b>dependency</b> <b>loops</b> of virtual/actual, stasis/evolution, human/machine, the group assemble interactive spaces that are, according to artist and programmer Nik Gaffney, "a primeval soup of possible memes". Cross-fertilising the mutant streams of evolutionary biology, chaos theory and electronic art in a 'nomadic' fashion, Mindflux write themselves into {{the space between the}} virtual and the (virtually receding) actual...|$|R
40|$|In this paper, we {{consider}} a concurrent model of computation called dataflow, where components (actors) communicate via streams of data tokens. Dataflow semantics {{has been adopted}} by experimental and production languages used to design embedded systems. The execution of a dataflow actor is enabled by the availability of its input data. One important question is whether a dataflow model will deadlock (i. e., actors cannot execute due to a data <b>dependency</b> <b>loop).</b> Deadlock in many cases can be determined, although it is generally not decidable. We develop a causality interface for dataflow actors based on the general framework we introduced in [1] and show how this causality information can be algebraically composed so that composition of components acquire causality interfaces that are inferred from their components and the interconnections. We illustrate {{the use of these}} causality interfaces to statically analyze for deadlock. Categories and Subject Descriptor...|$|E
40|$|Abstract—Wireless ad hoc {{networks}} (WANETs) offer commu-nications over {{a shared}} wireless channel without any pre-existing infrastructure. Forming peer-to-peer security associations in self-organized WANETs is more challenging than in conventional networks {{due to the}} lack of central authorities. In this paper, we propose a generic model to evaluate the relationship of connectivity, memory size, communication overhead and security in fully self-organized WANETs. Based on some reasonable as-sumptions on node deployment and mobility, we show that when the average number of authenticated neighbors of each node is Θ(1), with respect to the network size n, most of the nodes can be securely connected, forming a connected secure backbone, i. e., the secure network percolates. This connected secure backbone can be utilized to break routing-security <b>dependency</b> <b>loop,</b> and provide enough derived secure links connecting isolated nodes with the secure backbone in a multi-hop fashion, which leads to the secure connectivity of the whole network. I...|$|E
40|$|High-performance superscalar {{processors}} {{examine a}} large pool of speculative instructions, called the dynamic instruction window, to exploit instruction-level parallelism (ILP). Scaling performance requires {{a larger and}} more accurate dynamic instruction window, which, in turn, requires a more accurate branch predictor. Achieving higher accuracy typically requires increasing {{the size of the}} branch predictor. Unfortunately, this may cause an increase in the processor's cycle time. A fast cycle time can be preserved by pipelining the branch prediction logic. This is not straightforward, however. The branch predictor uses the program counter (PC) of the current fetch block (among other context) to produce information that is needed to predict the PC of the next fetch block. This means that the inputs to the branch prediction logic depend on its outputs. This <b>dependency</b> <b>loop</b> within the branch prediction logic renders naive pipelining ineffective. Accordingly, if we want to use a larger branch predictor without increasing cycle time, then we need to apply sophisticated approaches to effectively pipeline the next PC loop. Several approaches to effectively pipeline the branch prediction logic hav...|$|E
50|$|The {{problem of}} {{computing}} <b>dependencies</b> within <b>loops,</b> {{which is a}} significant and nontrivial problem, is tackled by loop dependence analysis, which extends the dependence framework given here.|$|R
5000|$|In {{compiler}} {{theory of}} computer science, A Greatest common divisor Test is the test used in study of loop optimization and loop dependence analysis {{to test the}} <b>dependency</b> between <b>loop</b> statements.|$|R
5000|$|... #Caption: The <b>dependencies</b> of , before <b>loop</b> skewing. The red dot {{corresponds}} to the purple dot {{corresponds to}} [...]|$|R
40|$|A {{scientist}} once gave {{a public}} lecture describing how the Earth orbits {{around the sun}} and how the sun, in turn, orbits around {{the center of a}} collection of stars called our galaxy. At the end of the lecture, a little old lady {{at the back of the}} room got up and said: “What you have told us is rubbish. The world is really a flat plate supported on the back of a giant tortoise. ” The scientist gave a superior smile before replying, “What is the tortoise standing on?” “You’re very clever, young man, very clever, ” said the old lady, “but it’s turtles all the way down!” Current trusted computing technologies allow computing devices to verify each other, but in a networked world, there is no reason to trust one computing device any more than another. Treating these devices as turtles, the user who seeks a trustworthy system from which to verify others quickly realizes that it’s “turtles all the way down” because of the endless loop of trust dependencies. We need to provide the user with one initial turtle (the iTurtle) which is axiomatically trustworthy, thereby breaking the <b>dependency</b> <b>loop.</b> In this paper, we present some of the research challenges involved in designing and using such an iTurtle. ...|$|E
40|$|Turtles All The Way Down:Research Challenges in User-Based Attestation * Jonathan M. McCune Adrian Perrig Arvind Seshadri Leendert van DoornCMU/CyLab CMU/CyLab CMU/CyLab AMD A {{scientist}} once gave {{a public}} lecture describing howthe Earth orbits {{around the sun}} and how the sun, in turn, orbits around {{the center of a}} collection of stars called ourgalaxy. At the end of the lecture, a little old lady at the backof the room got up and said: &quot;What you have told us is rubbish. The world is really a flat plate supported on theback of a giant tortoise. &quot; The scientist gave a superior smile before replying,&quot;What is the tortoise standing on? &quot; &quot;You're very clever, young man, very clever, &quot; said theold lady, &quot;but it's turtles all the way down!&quot; Abstract Current trusted computing technologies allow comput-ing devices to verify each other, but in a networked world, {{there is no reason to}} trust one computing device any morethan another. Treating these devices as turtles, the user who seeks a trustworthy system from which to verify oth-ers quickly realizes that it's &quot;turtles all the way down &quot; because of the endless loop of trust dependencies. Weneed to provide the user with one initial turtle (the iTurtle) which is axiomatically trustworthy, thereby breakingthe <b>dependency</b> <b>loop.</b> In this paper, we present some of the research challenges involved in designing and usingsuch an iTurtle...|$|E
40|$|This {{special issue}} {{comprises}} of 22 selected papers from the 2012 International Conference on Information, Computing and Telecommunications (ICICT 2012). The conferences received 1660 paper submissions from 11 countries and regions, of which 810 {{were selected for}} presentation after a rigorous review process. From these 810 research papers, through two rounds of reviewing, the guest editors selected 22 as the best papers on the software track of the Conference. The candidates of the Special Issue are all the authors, whose papers have been accepted and presented at the ICICT 2012, with the contents not been published elsewhere before. Information, Computing and Telecommunications are very hot and important research topics and gain more attention by economy as well as society in recent years. The 2012 International Conference on Information, Computing and Telecommunications (ICICT 2012) will be held from Jan 7 ~ 8, 2012 in Harbin, P. R. China. This conference is co-sponsored by Harbin University of Science and Technology and International Science and Engineering Research Center, and it is technical co-sponsored by Harbin Engineering University, Northeast Forestry University, Harbin Normal University, Heilongjiang University, Northeast Petroleum University and Harbin University. “DREET: A Data-Reliable, Energy-Efficient Transport Layer Protocol”, by Gang Han, Jia Lu, Xinbiao Gan, Huanzhong Li and Wenhua Dou, proposes the metric of data-reliability to measure the loss of sensor data when transporting and a transport protocol based on this metric, which is proven to be data-reliable and energy-efficient by simulation. “The Meso-level Numerical Experiment Research of the Mechanics Properties of Recycled”, by Aijiu Chen, Xiaozhou Xia,Qing zhang， ming wu, proposes with the meso finite element model just generated, the damage distribution and evolution processes of the RC are simulated and tracked. and which corresponds to the CT scanning pictures. “Model Checking and Verification of the Internet Payment System with SPIN”, by Wei Zhang, Wen-ke Ma, Hui-ling Shi, Fu-qiang Zhu, proposes a model checking method to verify the security and reliability of the Internet Payment Systems, including the PROMELA modeling process, the system properties description and the initial results on the actual verification with SPIN. “Service Selection Based on Behavior Matching”, by Mingyue Jiang, Zuohua Ding, Jing Liu, proposes a service selection technique based on behavior matching, thus this technique can protect us from obtaining “bad” service during service composition. By integrating the selection algorithm to IBM WebSphere Process Server, the web service binding can be automatically completed in the run time. “Research of the Electro-hydraulic Servo System Based on RBF Fuzzy Neural Network Controller”, by Huaizhong Chen, proposes a kind of control algorithm about electro-hydraulic speed governor based on fuzzy neural networks, and simulation study proves that this control system has a better adaptability and control effect. “Lip AUs detection by Boost-SVM and Gabor”, by Xianmei Wang, Yuyu Liang, Xiujie Zhao, Zhiliang Wang, proposes a solution for the identification of five types of lip AUs by non-cascaded Adboost framework with SVM as the weak classifiers and Gabor features. “Robust Text-Independent Speaker Identification in a Time-Varying Noisy Environment”, by Yaming Wang, Fuqian Tang, Junbao Zheng, proposes a new MFCC based speaker identification system that cover important aspects for application, such as how accurate it can be, how well it is in avoiding time-varying noise.  “Design and Implementation of Radio Function Module of an Embedded Car Audio System Based on ITRON Standard”, by Di Wu, Chenxi Hou, Limin Sun, Jiangchuan Liu, introduces the architectural design and implementation details for the XM function module in an embedded car audio system based on ITRON standard. “Blank Nodes in RDF”, by Lei Chen, Haifei Zhang, Ying Chen, Wenping Guo, proposes some methods to eliminate the inconsistencies caused by blank nodes, {{such as how to}} use entailment to make RDF graphs leaner and how to transform blank nodes to URI references. “An Object Tracking Algorithm Based on the "current" Statistical Model and the Multi-Feature Fusion”, by Jinhua Wang, JieCao, Di Wu,Yabing Yu, proposes an object tracking algorithm. This paper mainly to improve the performance of the target tracking by the way of feature fusion and the improved algorithm, such as accuracy，real-time and anti-jamming ability in complex background。 “Structure Design of Twin-Spirals Scroll Compressor Based on 3 C”, by Bin Peng, Hongsheng Zhang, Rong-zhen Zhao, Li Zhang, proposes CAD/CAM/CAE method to the structure of twin-spirals scroll compressor from conceptual design to finished product. The mathematical model, performance prediction and optimization, CAE analysis, virtual model of TSSC are developed. “An Ontology-Based Framework of Requirements Evolvement Management”, by Hongyue He, Zhixue Wang, Ying Zhang, Weizhong Zhang, proposes an ontology-based framework of Requirements Evolvement Management, it describes that how to use ontology to record the information about Requirements Evolvement(RE) and how to analyze the consistency and influence of RE. “Formal Description of Simulation Runtime Support Platfrom Architecture with XYZADL”, by Liyang SUN, Shaojie Mao,Zhong LIU, processes architecture of NCS-RSP by adopting dual software architecture description framework XYZ/ADL through graphics language and formal language respectively. Then this paper decomposes and refines the core service layer during the construction of CoST, which not only expresses the architecture graphics and behavioral abstraction of NCS-RSP from visual viewpoint, but also validates the correctness and completeness of architecture design from formal view. and this is a new attempt of formal description in military simulation domain and provides a guide for the composition and reuse of NCS-RSP service. “Improved Attitude Algorithm for Fiber-Optic-Gyro Strapdown INS”, by Qian Li, Yueyang Ben, Fen Sun, Fei Yu, Wei Gao, proposes a set of improved attitude algorithms for fiber-optic-gyro strapdown INS using angular rate, which are developed and evaluated under classical coning motion. “The Design and Implementation of Composite Collaborative Filtering Algorithm for Personalized Recommendation”, by Liang Hu, Wenbo Wang, Feng Wang, Xiaolu Zhang, Kuo Zhao, proposes a composite collaborative filtering algorithm for personalized recommend to solve the original Collaborative Filtering algorithm problem such as “None of User Starting” and “Data Sparsity”.  “Generating Executable Capability Models for Requirements Validation”, by Wei-zhong Zhang, Zhi-xue Wang, Wen Zhao, Ying-ying Yang, Xin Xin, proposes a formalization definition of Application activity diagrams, representing dynamic behaviors of systems in capability requirements models, and a mapping algorithm of translating them into executable models for simulation and validation. “A Hybrid Algorithm of Raster Conversion for Circle Based on Pattern Analysis”, by Haiwen Feng, Lianqiang Niu, Bowen Fu, Ling Zhong, proposes a hybrid algorithm which combines multi-point movement, pixel movement and run-length technique in order to improve the speed of raster converion for circle arc. “Software Reliability Test Based on Markov Usage Model”, by Kuanjiu Zhou, Xiaolong Wang, Gang Hou, Jie Wang and Shanbin Ai, proposes an efficient method to generate test cases based on Markov usage model, which is built from an improved STM, and then implements a MTCG tool with verified high efficiency. “UML Modeling and Parametric Design for Cross Shaft Universal Coupling CAD System”, by Yongming Wang, Jiangtao Li, proposes its functional framework of cross shaft universal coupling CAD system, establishes three UML models and develops its parametric design software. “An Online Kernel Learning Algorithm based on Orthogonal Matching Pursuit”, by ShiLei Zhao, Peng Wu, YuPeng Liu, proposes a kind of online kernel learning algorithm which utilizes “kernels trick” and “orthogonal matching pursuit” not only to estimate the nonlinear target function but also to keep control of the sparsity of the solution. “Laplacian Meshes Deformation Based on the Offset of Sketching”, by Sha Chenming, Zhang Xiaojing, Yue Yajie proposes a new Laplacian meshes deformation based-on the offset of sketching to solve the drawback that Laplacian coordinates are not invariant under rotation. “Research on Measurement of Software package dependency based on Component”, by Guang-yi TANG, Hong-wei XUAN, proposes a method to detection existence of the package <b>dependency</b> <b>loop,</b> and developed software system to analyze the dependencies between the software packages and use a graphical method to express this dependency. We would like to take this opportunity to thank the authors for the efforts they put in the preparation of the manuscripts and for their valuable contributions. We wish to express our deepest gratitude to the program committee members for their help in selecting papers for this issue and especially the referees of the selected papers for their thorough reviews under a tight time schedule. Last, but not least, our thanks go to the editorial board of the “Journal of Software” for the exceptional effort they did throughout this process. In closing, we sincerely hope that you will enjoy reading this special issue. </p...|$|E
40|$|There {{has been}} some {{ambiguity}} about the growth of attractors in Kauffman networks with network size. Some recent work has linked this to the role and growth of circuits or loops of boolean variables. Using numerical methods we have investigated the growth of structural circuits in Kauffman networks and suggest that the exponential {{growth in the number}} of structural circuits places a lower bound on the complexity of the growth of boolean <b>dependency</b> <b>loops</b> and hence of the number of attractors. We use a fast and exact circuit enumeration method that does not rely on sampling trajectories. We also explore the role of structural self-edges, or self-inputs in the NK-model, and how they affect the number of structural circuits and hence of attractors...|$|R
5000|$|... #Caption: The <b>dependencies</b> of , after <b>loop</b> skewing. The array {{elements}} will {{be processed}} {{in the order}} gray, red, green, blue, yellow...|$|R
30|$|With the {{proposed}} algorithm, <b>loops</b> <b>dependencies</b> do not enable their execution in parallel. Several for-loop strategies can be implemented, {{but are not}} studied herein.|$|R
40|$|Using {{parallel}} processing systems to compute scientific applications {{is one of}} the most common solutions for achieving more efficient computing performance. In some applications such as fluid mechanics, structural analysis, solid state simulations, the dependencies across iterations (loop-carried dependencies) of the computation of array elements may be constants (uniform) or functions of array indices (non-uniform). Traditional scheduling can efficiently generate an optimized schedule for applications with any uniform dependencies. However, non-uniform dependencies of array elements in those applications may cause the scheduler to produce an inefficient result. This paper presents a new scheduling methodology for both uniform and non-uniform <b>dependency</b> <b>loops,</b> considering a limited number of target processors. A loop pipelining technique, called rotation scheduling, is used as the basis for our algorithm to overlap operations in the loops, taking into account the various forms of loop- [...] ...|$|R
40|$|We {{define the}} Value State Dependence Graph (VSDG). The VSDG {{is a form}} of the Value Dependence Graph (VDG) {{extended}} by the addition of state dependence edges to model sequentialised computation. These express store <b>dependencies</b> and <b>loop</b> termination <b>dependencies</b> of the original program. We also exploit them to express the additional serialization inherent in producing final object code...|$|R
40|$|Using {{parallel}} processing systems to execute scientific applications {{is one of}} the most common solutions for achieving more efficient computing performance. However, those applications may contain <b>loops</b> with non-uniform <b>dependencies</b> which may cause a compiler to produce sequential codes. This paper presents a new scheduling methodology for non-uniform <b>dependency</b> <b>loops,</b> considering a limited number of target processors. A loop pipelining technique, called rotation scheduling, is used as the basis for a new algorithm that systematically computes a new schedule table. New mathematical models are derived to handle the difficulties of the non-uniform dependencies. 1 Introduction Several scientific applications are based on modeling a problem by a grid of elements. Applications such as fluid mechanics, structural analysis and solid state simulations may require changes in the granularity of such grids due to areas of the problem presenting characteristics of rapid change on the values of the [...] ...|$|R
40|$|Nested loops are {{normally}} {{the most time}} intensive tasks in computer algorithms. These loops often include multiple dependencies between arrays that impose communication constraints when used in multiprocessor systems. These dependencies may be between dependent arrays (<b>loop</b> <b>dependencies),</b> or between independent arrays (data dependencies). In this paper, reducing the communication caused by data and <b>loop</b> <b>dependencies</b> for perfect nested loops is explored. It is shown that for a given partition data dependencies may {{be treated as a}} specialized form of <b>loop</b> <b>dependencies.</b> Once this is done, previous results on scalable loop tiling can be used to calculate the final total communication. Next, the effects of changing the partition for both loop and data communication are examined. Using these results, the optimal partition for a number of cases are examined. Results are shown which illustrate the efficiency of the system as well as the savings achieved. 1. INTRODUCTION Applications such as d [...] ...|$|R
40|$|We present GraSSP, a novel {{approach}} to perform automated parallelization relying on recent advances in formal verification and synthesis. GraSSP augments an existing sequential program with an additional functionality to decompose data <b>dependencies</b> in <b>loop</b> iterations, to compute partial results, and to compose them together. We show that for some classes of the sequential prefix sum problems, such parallelization can be performed efficiently. Comment: In Proceedings SYNT 2016, arXiv: 1611. 0717...|$|R
40|$|<b>Dependencies</b> between <b>loop</b> {{iterations}} cannot {{always be}} characterized during program compilation. Doacross loops typically {{make use of}} a-priori knowledge of inter-iteration dependencies to carry out required synchronizations. A type of doacross loop is proposed that allows the scheduling of iterations of a loop among processors without advance knowledge of inter-iteration dependencies. The method proposed for loop iterations requires that parallelizable preprocessing and postprocessing steps be carried out during program execution...|$|R
40|$|Loops that {{synchronize}} parallel processors {{at the end}} of each iteration {{are compared}} with loops that do not synchronize their iterations. In the presence of data <b>dependencies,</b> <b>loop</b> synchronization cannot always be removed [...] -the purpose here is to estimate the additional costs incurred when synchronization is necessary. Suppose there are n parallel processors each executing k iterations. Under the assumption that each iteration of the loop body runs for a time controlled by an independent identically distributed random variable X with mean µ and variance s 2, it is shown here that the ratio of the expected time taken with synchronized loops to the expected time taken with unsynchronized loops is asymptotically µ EX (n) + e(n) ############# (n, k ®) (*) where EX (n) is the expected maximum of n independent random variables with with the distribution of X, and e(n) is the (deterministic) time to fork n processes. Since EX (n) grows with n in a way that can be estimated for many par [...] ...|$|R
40|$|Most of the {{reported}} work in the Parallelizing Compilers literature focuses on analyzing program characteristics such as the <b>dependencies,</b> <b>loop</b> structures, memory reference patterns etc. to optimize the generated parallel code. Unfortunately, parallelizing compilers have very little or {{no knowledge of the}} actual run time behavior of the synthesized code on the underlying hardware due to the complex behavior of the underlying hardware and software subsystems. This interaction could significantly affect the performance of the generated code and must be considered during program partitioning phases of the compiler. In this paper, we present an efficient and accurate performance model based program partitioning approach for parallel architectures. We introduce the concept of behavioral edges for capturing the interactions between computation and communication through parametric functions. We present an efficient algorithm to identify behavioral edges, modify costs using the behavioral edges and adapt the schedule to improve schedule length. The program partitioning phase uses the static estimates computed using the behavioral edges and partitioning is iteratively performed using the ordering on PDG based on computed intervals. A significant performance improvement (factor of 10 in many cases) is demonstrated by using our framework...|$|R
40|$|As design progresses, {{artefact}} {{and process}} knowledge often evolve together. However, {{there is very}} limited knowledge on {{the true nature of}} the dependencies between these two elements of knowledge. This paper presents the first attempt to clearly define 'creation' dependencies, which cause a change in design knowledge. Three data analyses were used to identify the dependencies: two were in-depth protocol analyses of a single student product design project and a senior ship designer’s daily work, and a third was a quantitative questionnaire analysis involving seven experienced complex system designers from industry. The analyses revealed a set of 52 previously unknown creation dependencies between artefact and design process knowledge with commonality found in only 5, with additional dependencies being identified that were specific to the design being studied. Different frequencies of dependency occurrence and particular <b>dependency</b> <b>loops</b> were identified. In addition, the importance and role of domain knowledge were explicitly revealed. The described research highlights the need for further work to provide a more comprehensive definition of the nature of evolutionary artefact and design process knowledge dependencies. Identification of these dependencies offers a significant opportunity to develop tools and techniques with an enhanced ability to support 'what–if' analyses during design...|$|R
40|$|Loop fusion {{is widely}} used to exploit the instruction-level {{parallelism}} by transforming separate loops into one loop for applications of embedded systems. Loop fusion, however, is not always applicable because {{of the existence of}} the fusion-prevention <b>dependencies</b> among <b>loops.</b> Therefore, techniques for eliminating the fusion-prevention dependencies are necessary for fully exploiting the benefits of the loop fusion. In this paper we present an efficient loop fusion technique based on <b>loop</b> <b>dependency</b> graph model and multi-dimensional retiming concept. Legalizing fusion theorems are derived for loops to be legally fused. Polynomial-time legalizing fusion algorithms are developed to solve the loop fusion problems for 1 -level loops and 2 -level nested loops. Our loop fusion techniques are carefully designed to consider multiple optimization objectives, such as minimizing the code size and the critical path of the fused loop. The resultant code size can be accurately computed. The experimental results show that our loop fusion technique always significantly reduces the schedule length...|$|R
50|$|DOACROSS {{parallelism}} is a parallelization {{technique used}} to perform Loop-level parallelism by utilizing synchronisation primitives between statements in a loop. This technique is used when a loop cannot be fully parallelized by DOALL parallelism due to data <b>dependencies</b> between <b>loop</b> iterations, typically loop-carried dependencies. The {{sections of the}} loop which contain loop-carried dependence are synchronized, while treating each section as a parallel task on its own. Therefore, DOACROSS parallelism {{can be used to}} complement DOALL parallelism to reduce loop execution times.|$|R
50|$|If GCD(x,y) divides (m-k) {{then there}} may exist some <b>dependency</b> in the <b>loop</b> {{statement}} s1 and s2. If GCD(x,y) does not divide (m-k) then both statements are independent {{and can be}} executed at parallel. Similarly this test is conducted for all statements present in a given loop.|$|R
40|$|Abstract — We {{consider}} the resource-constrained scheduling of <b>loops</b> with interiteration <b>dependencies.</b> A <b>loop</b> is modeled as a data flow graph (DFG), where edges are labeled {{with the number}} of iterations between dependencies. We design a novel and flexible technique, called rotation scheduling, for scheduling cyclic DFG’s using loop pipelining. The rotation technique repeatedly transforms a schedule to a more compact schedule. We provide a theoretical basis for the operations based on retiming. We propose two heuristics to perform rotation scheduling and give experimental results showing that they have very good performance. Index Terms — High-level synthesis, loop pipelining, parallel compiler, retiming, scheduling...|$|R
50|$|Approaches that cannot {{represent}} symbolic terms (such as {{the loop}}-invariant quantity N {{in the loop}} bound and subscript) cannot reason about <b>dependencies</b> in this <b>loop.</b> They will either conservatively refuse to run it in parallel, {{or in some cases}} speculatively run it completely in parallel, determine that this was invalid, and re-execute it sequentially.|$|R
40|$|Memristive Devices (MDs) {{are usually}} {{described}} by their associated hysteresis loops. Their distinctive memory properties stem from this unusual characteristic, which depends on both stimulus frequency and amplitude. Understanding {{the behavior of}} these devices {{is of paramount importance}} for a multitude of different applications. This paper investigates the <b>dependency</b> of <b>loop</b> area and length on stimulus amplitude of MDs. The characterization methodology follows the morphological approach, introduced by the authors in [10], for frequency characterization. An example, considering thin film TiO 2 MDs reveals that the peak amplitude (amplitude where the loop has maximum area) of the device depends strongly on device dimensions and physical properties...|$|R
3000|$|All {{the above}} support {{technologies}} {{have to be}} aware of the following: If the temporal and the spatial computing are separated (hybrid computing with only combinational logic in the spatial part), and if latency is traded for better performance (maximal acceleration even for the most sophisticated types of data <b>dependencies</b> between different <b>loop</b> iterations), benefits occur only if the support technologies are able to utilize them! [...]...|$|R
40|$|The {{purpose of}} this {{dissertation}} is to develop an instrument to measure the quality, quality changes and the efficiency of a service system with <b>dependency</b> <b>loops</b> {{on an ongoing basis}} in order to provide timely feedback for decision-makers and to set the basis for a continuous improvement cycle. This instrument is developed using an engineering educational system as the prime example. The first outcome has been a data driven Strengths and Weakness (SW) analysis. It consists of four steps - data collection, data summarization, display of proportions (data aggregated into positive, neutral and negative perceptions), and the construction of a SW table by using a set of heuristic rules that reflects the decision-maker's desired level of sensitivity for the methodology. The core of the method resides in selecting the category with the largest proportion for a finite population where each element is classified into exactly one of k mutually exclusive categories. The heuristic rules used for classification are justified using the concepts of statistical ranking and selection procedures. Applications of the SW table in cross-sectional and longitudinal analyses are given. Special graphs, e. g. the one-dimensional and two-dimensional arrows that help the analysis have been constructed so as to provide aid to the decision makers in the engineering educational system. The second outcome provides a scheme for the evaluation of the relative efficiency of processes within this type of service system. Data Envelopment Analysis has been used iteratively to evaluate the efficiency of levels and programs within an engineering educational service system. This is used to chart the changes in students' perceptions as they progress during their career from the freshmen to the senior level...|$|R
40|$|A trace is {{a record}} of the {{execution}} of a computer program, showing the sequence of operations executed. A trace may be obtained through static or dynamic analysis. An object trace contains only those operations that relate to a particular object. Traces can be very large for longer system executions. Moreover, they lack structure because they do not show the control dependencies and completely unfold loops. Object process graphs are a finite concise description of dynamic object traces. They offer the advantage of representing control <b>dependencies</b> and <b>loops</b> explicitly. This paper describes a new technique to extract object process graphs through dynamic analysis and discusses several applications. A case study is described that illustrates and demonstrates use and feasibility of the technique...|$|R
