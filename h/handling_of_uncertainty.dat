102|10000|Public
40|$|The KDD process aims at the {{discovery}} and extraction of "useful" knowledge (such as interesting patterns, classification, rules etc) from large data repositories. A widely recognized requirement {{is that the}} patterns discovered must be valid and ultimately comprehensible (i. e., to be easily understood by analysts). Another requirement that is under-addressed in KDD process is the reveal and the <b>handling</b> <b>of</b> <b>uncertainty</b> in the main data mining processes of clustering, classification and association rules extraction...|$|E
40|$|Sustainability has {{recently}} {{emerged as a}} key issue in process systems engineering (PSE). Mathematical programming techniques offer a general modeling framework for including environmental concerns in the synthesis and planning of chemical processes. In this paper, we review major contributions in process synthesis and supply chain management, highlighting the main optimization approaches that are available, including the <b>handling</b> <b>of</b> <b>uncertainty</b> and the multi-objective optimization of economic and environmental objectives. Finally, we discuss challenges and opportunities identified in the area...|$|E
40|$|With {{the coming}} of post-industrial society, organisations will {{increasingly}} rely on intellectual technology to aid problem-solving. But those technologies that deal explicitly with uncertainty, such as decision analysis, face special problems hampering their institutionalisation. The main argument {{of this paper is}} that success or failure in implementing decision analysis is in part determined by an organisation's structure. Yet, uncertainty is one factor that determines the structure of an organisation, and some structures facilitate an organisation's <b>handling</b> <b>of</b> <b>uncertainty,</b> while others do not. In general, intellectual technologies that deal with uncertainty will be difficult to implement in organisations whose structures do not facilitate the <b>handling</b> <b>of</b> <b>uncertainty.</b> Strong centralisation of decision making, often found in large organisations, creates special problems in implementing decision analysis. Organisational structures that allow the level and type of uncertainty facing individuals to be matched to their personal levels of capability to handle it, will facilitate implementation of decision analysis. Within large organisations, creating specialist programming and technical functions at appropriate levels or introducing matrix structures, which combine vertical and lateral structures, may facilitate the introduction of technologies that deal with uncertainty...|$|E
30|$|The {{evolution}} models {{described above}} {{can be used}} for trajectory prediction in various ways, the main difference being in the <b>handling</b> <b>of</b> <b>uncertainties.</b>|$|R
40|$|Abstract. This paper {{discusses}} how {{the early}} phases of developing embedded electronic {{systems can be}} improved by enhanced modelling of cost and performance that includes explicit <b>handling</b> <b>of</b> <b>uncertainties.</b> The approach is to add cost information to existing UML models, capture uncertainties using probability distributions, and use Monte Carlo simulation to analyze the risk of not reaching the cost targets. It is demonstrated how the information obtained can be used when evaluating different architecture alternatives, while including both development and product cost as well as risk in the trade-off...|$|R
40|$|In a video {{sequence}} with a 3 D rigid object moving, changing {{shapes of the}} 2 D projections provide interrelated spatio-temporal cues for incremental 3 D shape reconstruction. This paper describes a probabilistic approach for intelligent view-integration to build 3 D model of vehicles from traffic videos collected from an uncalibrated static camera. The proposed Bayesian net framework allows the <b>handling</b> <b>of</b> <b>uncertainties</b> in a systematic manner. The performance is verified with several types of vehicles in different videos. Index Terms – Learning, 3 D shape from video 1...|$|R
40|$|Fuzzy {{set theory}} is making many inroads into the <b>handling</b> <b>of</b> <b>uncertainty</b> in {{various aspects of}} image {{processing}} and computer vision. High level computer vision {{is a place that}} holds great potential for fuzzy sets because of its natural linguistic capabilities. Scene description, i. e., the languagebased representation of regions and their relationships, for either humans or higher automated reasoning provides an excellent opportunity. In this paper we discuss aspects of scene interpretation involving linguistic descriptions of spatial relations between image objects. 1...|$|E
40|$|We {{present an}} {{approach}} and an associated computer program (ATT-Meta) which can interpret novel uses of familiar metaphors. ATT-Meta performs sophisticated reasoning with uncertainty {{of a kind}} that is required for reasoning {{about the content of}} metaphorical utterances. ATT-Meta’s <b>handling</b> <b>of</b> <b>uncertainty</b> is qualitative and handles potential conflicts between different lines of reasoning or arguments. It also uses a particular approach to specificity, which involves a complex examination of the complete argument structures supporting the conflicting hypotheses/arguments, as a powerful tool for comparing arguments...|$|E
40|$|A new exemplar-based {{probabilistic}} approach for face recognition in video sequences is presented. The approach has two stages: First, Exemplars, which are selected {{representatives from the}} raw video, are automatically extracted from gallery videos. The exemplars are used to summarize the gallery video information. In the second part, exemplars are then used as centers for probabilistic mixture distributions for the tracking and recognition process. Probabilistic methods are attractive in this context as they allow a systematic <b>handling</b> <b>of</b> <b>uncertainty</b> and an elegant way for fusing temporal information. Contrary t...|$|E
40|$|Solving {{decision-making}} problems requires efficient <b>handling</b> <b>of</b> <b>uncertainties.</b> This task {{has been}} usually performed {{by means of}} expert systems {{which are based on}} classical logic and, therefore, need special methods such as heuristic approaches, probability theory, possibility theory, and fuzzy theory. The later approach, fuzzy reasoning and logic, offers a more natural way <b>of</b> <b>handling</b> <b>uncertainty</b> since it is similar to human logical reasoning. In this paper, we develop a fuzzy logic model for assessment and prediction of country risk. This fuzzy method provides a systematic approach to analyzing a target country. By its nature, the decision making for global market involves various uncertain criteria; therefore, the fuzzy approach is suitable for this kind of analysis. The advantages of the approach are inclusion of economic data, consideration of political/social factors, and the ability to handle exact and fuzzy data...|$|R
40|$|This study {{deals with}} the <b>handling</b> <b>of</b> <b>uncertainties</b> in the seismic {{analysis}} of complex structures. As a case example of a complex structure, a long span suspension bridge is considered, whose main span is 3300 m long. This is, without doubt, a very complex structure for the nonlinearities, uncertainties and interactions involved in its behavior. This paper focuses the attention on {{the definition of the}} seismic input (artificial accelerograms) and the consequent dynamical analyses which are developed in presence <b>of</b> <b>uncertainties.</b> In order to treat these uncertainties, an analysis is developed based on the fuzzy theory. Therefore, the procedure performed in order to reproduce the fuzzy response of the bridge is illustrated...|$|R
40|$|A {{common problem}} in the {{optimization}} of structures is the <b>handling</b> <b>of</b> <b>uncertainties</b> in the parameters. If the parameters appear in the constraints, the uncer-tainties can lead to {{an infinite number of}} constraints. Usually the constraints have to be approximated by finite expressions to generate a computable problem. Here, using the example of the topology optimization of a truss, a method is proposed to deal with such un-certainties by using robust optimization techniques, leading to an approach without the necessity of any approximation. With adequately chosen load cases, the final expression is equivalent to the multiple load case. Simple numerical examples of typical problems illustrate the application of the method. ...|$|R
40|$|International audienceThis paper {{deals with}} the <b>handling</b> <b>of</b> <b>uncertainty</b> {{in the context of}} {{cost-effectiveness}} analyses, through the building of confidence regions for the incremental cost-effectiveness ratio (ICER), to provide a statistical descriptive tool for helping medico-economic decision-making. However, the confidence region for the ICER, mathematically provided, cannot be directly usable for decision-making, because two identical ICERs can be associated with two totally opposite findings. To solve this problem, algorithms are developed in this paper, to provide on the one hand a directed confidence region usable for decision-making, {{and on the other hand}} a decision directly from the data...|$|E
40|$|Model Fusion is {{a working}} {{technology}} that is producing excellent results. The projects that have been completed successfully to date have clearly demonstrated something of the real potential {{and power of the}} technology. However, to achieve the full potential of the vision there are substantial challenges to be addressed. These include ontology and semantics, scale, the <b>handling</b> <b>of</b> <b>uncertainty,</b> ready access to appropriate datasets, managing intrusion, and culture change to name but a few. These challenges are best address by the developing model fusion community working together to clearly specify the challenge and agree collective approaches...|$|E
40|$|McESE (McMaster Expert System Environment) is a {{software}} tool for building problem-specific shells and creating {{expert system applications}} in a particular programming language. Developed and implemented by the authors at McMaster University, {{it is designed to}} allow the user to deal with imprecise and incomplete knowledge, customize the shell’s <b>handling</b> <b>of</b> <b>uncertainty,</b> allow hierarchical partitioning of knowledge bases, and fast prototyping. Specialized software of McESE is written in C and facilitates handling of all aspects of dealing with rule-based knowledge bases. Some of practical and theoretical aspects of McESE are discussed in this paper. For more details see [FB], [FB 1], [F], [L]...|$|E
40|$|A {{number of}} {{different}} definitions of artificial intelligence is considered and their possible significance for the domain of reconstruction of man-made objects from aerial images is discussed. A useful property of AI is its inclination towards exploratory work which has helped to launch many new research areas. It is shown that contributions of AI {{can be found in}} various subdomains which are important for the field of man-made object reconstruction as a whole, namely perceptual organization, modeling and knowledge representation, control, <b>handling</b> <b>of</b> <b>uncertainties,</b> to name but the most important ones. As the field is far from mature at this very moment, AI can and will further contribute to its advancement...|$|R
40|$|Shallow lake {{eutrophication}} {{has received}} less attention than deep lake eutrophication, {{yet it is}} a problem far from being solved. Unlike other books on the subject, usually approaching the field from a disciplinary level, this book offers an approach to harmonize scientific research and the needs of practical management. New and original contributions in the fields of hydrodynamics, phytoplankton dynamics, water-sediment interaction, nutrient load dynamics, as well as modeling techniques are integrated in a model-based framework of aggregating the pertinent scientific information for use on the management level. Preservation of what is really important including the <b>handling</b> <b>of</b> <b>uncertainties</b> is a major feature of the approach developed in this book...|$|R
30|$|Data mining {{techniques}} such as classification and clustering {{play a vital role}} in the development of medical decision support systems contributing to improved healthcare quality. The medical decision-making problems inherently involve complexities and uncertainties, and thus, the researchers have advocated the integration of fuzzy methodologies in medical data interpretation. The <b>handling</b> <b>of</b> <b>uncertainties</b> by capturing <b>of</b> knowledge using fuzzy sets and rules together with an interpretability offered by simple linguistic if-then rules are two most important features of fuzzy methodologies. The fuzzy approaches are commonly applied to medical data classification problems. In our previous work [9], we suggested to represent multi-dimensional medical data by means of an optimal fuzzy membership function, and a carefully designed data model is introduced in a completely deterministic framework where uncertain variables are characterized by fuzzy membership functions.|$|R
40|$|Distributed {{cooperative}} {{systems that}} use event notifica-tion for communication {{can benefit from}} event correlation within the notification network. In the presence of uncer-tain data, however, correlation results easily become unreli-able. The <b>handling</b> <b>of</b> <b>uncertainty</b> is therefore an important challenge for event correlation in distributed event notifica-tion systems. In this paper, we present a generic correlation model that is aware of uncertainty. We propose uncertainty constraints that event correlation can take into account and show how they can lead to higher confidence in the corre-lation result. We demonstrate that the application of this model allows to obtain a qualitative description of event correlation. 1...|$|E
40|$|In {{this work}} we present an appearance-based 3 -D Face Recognition {{approach}} that is able to recognize faces in video sequences, independent from face pose. For this we combine eigen light-fields with probabilistic propagation over time for evidence integration. Eigen light-fields allow us to build an appearance based 3 -D model of an object; probabilistic methods for evidence integration are attractive in this context as they allow a systematic <b>handling</b> <b>of</b> <b>uncertainty</b> and an elegant way for fusing temporal information. Experiments demonstrate the effectiveness of our approach. We tested this approach successfully on more than 20 testing sequences, with 74 different individuals...|$|E
40|$|Quantification and {{reduction}} of uncertainty associated to {{decision making is}} one of the primary functions of modeling and monitoring targeted to assist decision making in reservoir, river, and lake water quality management. In many practical activities such as environmental impact assessment, the inference is bound to be based primarily on subjective, expert judgments, supported by empirical data and models. A bulk of analytical approaches is presently available for modeling purposes. The paper discusses selected decision analytic approaches to the <b>handling</b> <b>of</b> <b>uncertainty</b> associated to information available, uncertainty as a decision criterion, and as a component influencing the model structure. Computational solutions based on experience on six case studies are reviewed...|$|E
30|$|The study {{verified}} {{the applicability}} <b>of</b> FP-KLE in <b>handling</b> <b>uncertainties</b> <b>of</b> flood modeling {{in a more}} efficient manner; further test with multiple inputs of random fields is desired.|$|R
30|$|When {{the future}} motion {{of a vehicle}} is {{represented}} by a probability distribution on sample trajectories (which is typically the case with approaches relying on Monte Carlo simulations or Gaussian Processes), {{it is possible to}} compute risk as the “probability of a collision in the future” by integrating over all the possible future trajectories and detecting collisions between each possible pair. This approach provides a lot of flexibility in the <b>handling</b> <b>of</b> <b>uncertainties.</b> For example, for a Maneuver-based motion model the calculation can either sum over both the maneuvers and their executions, or assume that the maneuvers are known and sum on the possible executions only [[25]]. Furthermore, depending on the final application one can compute the risk of colliding with a specific vehicle or sum over all the vehicles and obtain a global collision risk [[25],[52]].|$|R
40|$|Abstract. This paper {{addresses}} {{two problems}} with toponym extrac-tion and disambiguation. First, almost no existing works examine the extraction and disambiguation interdependency. Second, existing disam-biguation techniques mostly take as input extracted toponyms without considering the <b>uncertainty</b> and imperfection <b>of</b> the extraction process. It is {{the aim of}} this paper to investigate both avenues and to show that explicit <b>handling</b> <b>of</b> the <b>uncertainty</b> <b>of</b> annotation has much potential for making both extraction and disambiguation more robust. ...|$|R
40|$|Both {{market and}} policy {{failures}} {{can lead to}} environmental degradation. Considerable {{progress has been made}} in the area of project evaluation and the valuation of environmental effect. On the other hand the analytical tools to study the effects of policy failures -the impact of taxes, prices, exchange rate and incentives- are more limited. The correct <b>handling</b> <b>of</b> <b>uncertainty</b> also remains a major issue. The analysis of intemational environmental problems, such as acid rain or CO 2 buildup, may benefit from the application of game theory approaches and the use of revelation mechanisism designed for public goods. Economic Theory and Environmental Degradation: A Survey of Some Problem...|$|E
40|$|Abstract A {{variety of}} cluster {{analysis}} techniques prerequisite to cluster objects having similar characteristics in data mining. But the clustering of those algorithms {{have lots of}} difficulties in dealing with categorical data within the databases. The imprecise <b>handling</b> <b>of</b> <b>uncertainty</b> within categorical data in the clustering process stems from the only algebraic logic of rough set, resulting in the degradation of stability and effectiveness. This paper proposes a information-theoretic rough entropy(RE) by {{taking into account the}} dependency of attributes and proposes a technique called min-mean-mean roughness(MMMR) for selecting clustering attribute. We analyze and compare the performance of the proposed technique with K-means, fuzzy techniques and other standard deviatio...|$|E
40|$|A clear {{bidirectional}} path {{exists between}} everyday {{social and cultural}} life and the formal notions of uncertainty. If in recent times {{there has been a}} resurgence of the contribution of the formal notions of fuzziness and vagueness to disciplines such as aesthetics, medicine and more generally humanities, {{it is also true that}} concepts and phenomena from everyday reality are continually and usefully reused in order to build formal definitions that are more akin to the essence of things. In this paper, some notions connected with the <b>handling</b> <b>of</b> <b>uncertainty,</b> and particularly with FST, are outlined with the aim of benefitting the spontaneous emergence of new paths toward an unified theory of uncertainty...|$|E
25|$|Bayesian Quadrature is a {{statistical}} {{approach to the}} numerical problem of computing integrals and falls under the field of probabilistic numerics. It can provide a full <b>handling</b> <b>of</b> the <b>uncertainty</b> over {{the solution of the}} integral expressed as a Gaussian Process posterior variance. It is also known to provide very fast convergence rates which can be up to exponential in the number of quadrature points n.|$|R
40|$|Abstract In {{assessing}} the cost-effectiveness of an intervention, the interpretation and <b>handling</b> <b>of</b> <b>uncertainties</b> <b>of</b> the traditional summary measure, the Incremental Cost Effectiveness Ratio (ICER), can be problematic. This is particularly {{the case with}} strategies towards universal health coverage in which the decision makers are typically concerned with coverage and equity issues. We explored the feasibility and relative advantages of the net-benefit framework (NBF) (compared to the more traditional Incremental Cost-Effectiveness Ratio, ICER) in presenting results of cost-effectiveness analysis of a community based health insurance (CBHI) scheme in Nouna, a rural district of Burkina Faso. Data were collected from April to December 2007 from Nouna’s longitudinal Demographic Surveillance System on utilization of health services, membership of the CBHI, covariates, and CBHI costs. The incremental cost of a 1 increase in utilization of health services by household members of the CBHI was 433, 000 XOF ($ 1000 approximately). The incremental cost varies significantly by covariates. The probability of the CBHI achieving a 1 % increase in utilization of health services, when the ceiling ratio is $ 1, 000, is barely 30 % for households in Nouna villages compared to 90 % for households in Nouna town. Compared to the ICER, the NBF provides more useful information for policy making. </p...|$|R
40|$|Abstract 5 Dealing {{consistently}} {{with risk}} and uncertainty across the IPCC reports is a difficult challenge. Huge practical difficulties arise from the Panel's scale and interdisciplinary context, {{the complexity of the}} climate change issue and its political context. The key question of this paper is if the observed differences in the <b>handling</b> <b>of</b> <b>uncertainties</b> by the three IPCC Working Groups can be clarified. To address this question, the paper reviews a few key issues on the foundations <b>of</b> <b>uncertainty</b> analysis, and summarizes the history of the treatment <b>of</b> <b>uncertainty</b> by the IPCC. One of the key findings is that there is reason to agree to disagree: the fundamental differences between the issues covered by the IPCC’s three interdisciplinary Working Groups, between the type of information available, and between the dominant paradigms of the practitioners, legitimately lead to different approaches. We argue that properly using the IPCC’s Guidance Notes for Lead Authors for addressing uncertainty, adding a pedigree analysis for key findings, and particularly communicating the diverse nature <b>of</b> <b>uncertainty</b> to the users of the assessment would increase the quality of the assessment. This approach would provide information about the nature <b>of</b> the <b>uncertainties</b> in addition to their magnitude and the confidence assessors have in their findings...|$|R
40|$|We {{present an}} exemplar-based {{probabilistic}} approach for face recognition in video data. The approach has two stages: First, Exemplars, which are selected {{representatives from the}} raw video, are automatically extracted from gallery videos. The exemplars are used to summarize the gallery video information. In the second part, these exemplars are then used as centers for probabilistic mixture distributions for the tracking and recognition process. Here, we exploit the fact, that tracking and recognition are related problems. The use of probabilistic methods are attractive because they allow a systematic <b>handling</b> <b>of</b> <b>uncertainty</b> and an elegant way for fusing temporal information. We tested our approach on more than 100 training and testing sequences, with 25 different individuals. ...|$|E
40|$|Ad hoc {{techniques}} and inference methods used in expert systems are often logically inconsistent. On the other hand, among properties and assertions concerning <b>handling</b> <b>of</b> <b>uncertainty,</b> those which {{turns out to}} be well founded can be in general easily deduced from probability laws. Relying on the general concept of event as a proposition and starting from a few conditional events of initial interest, a gradual and coherent assignment of conditional probabilities is possible by resorting to de Finetti's theory of coherent extension of subjective probability. Moreover, even when numerical probabilities can be easily assessed, a more general approach is obtained introducing an ordering among conditional events by means of a coherent qualitative probability...|$|E
40|$|This paper {{introduces}} a new cost-model for robot systems with cognitive features {{for the use}} in small and medium sized enterprises (SME). The model combines approaches from activity-based costing and life-cycle costing to reflect machine and production-centric aspects {{of the use of}} robot systems. Key contribution of this paper is the <b>handling</b> <b>of</b> <b>uncertainty</b> in the input parameters of the cost-model by using methods from interval analysis. This approach allows to compute explicitly best and worst case scenarios without manual variation of parameters. The cost-model is tested for an application scenario for robotic deburring and shows the economic performance of the robot system in best and worst case in an intuitive way...|$|E
40|$|Reliable {{predictions}} {{are essential}} for managing software projects with respect to cost and quality. Several {{studies have shown that}} hybrid prediction models combining causal models with Monte Carlo simulation are especially successful in addressing the needs and constraints of today's software industry: They deal with limited measurement data and, additionally, make use of expert knowledge. Moreover, instead of providing merely point estimates, they support the <b>handling</b> <b>of</b> estimation <b>uncertainty,</b> e. g., estimating the probability of falling below or exceeding a specific threshold. Although existing methods do well in terms <b>of</b> <b>handling</b> <b>uncertainty</b> <b>of</b> information, we can show that they leave uncertainty coming from imperfect modeling largely unaddressed. One of the consequences is that they probably provide over-confident uncertainty estimates. This paper presents a possible solution by integrating bootstrapping into the existing methods. In order to evaluate whether this solution does not only theoretically improve the estimates but also has a practical impact {{on the quality of the}} results, we evaluated the solution in an empirical study using data from more than sixty projects and six estimation models from different domains and application areas. The results indicate that the <b>uncertainty</b> estimates <b>of</b> currently used models are not realistic and can be significantly improved by the proposed solution...|$|R
40|$|Abstract Part 2 of this 3 part series {{continues}} a rebuttal to Kenneth Arrow’s famous {{argument that}} health care is special and free market economic principles do not apply. The rebuttal is based on concepts of Austrian Economics. Part 1 of the series framed the debate and discussed general concepts. Part 2 discusses specific examples of how health care is special and does not behave according to market principles. <b>Uncertainty</b> <b>of</b> demand and <b>uncertainty</b> <b>of</b> outcome are discussed in detail. Information asymmetry is a special form <b>of</b> <b>uncertainty</b> that Kenneth Arrow claimed was somewhat unique to health care. Free market solutions to these problems are discussed in general with specific examples provided. The conclusions are that free market insurance (as opposed to subsidy) <b>handles</b> <b>uncertainty</b> <b>of</b> demand, branding <b>handles</b> <b>uncertainty</b> <b>of</b> outcome, and the free market for specialized information handles information asymmetry...|$|R
40|$|International audienceThis paper {{presents}} a cognitive vision approach for video event recognition able <b>of</b> <b>handling</b> the <b>uncertainty</b> <b>of</b> the recognition process. The recognition task is complex because of image noise, of segmentation and classification issues. In this work, we extend the event recognition algorithm (crisp algorithm) proposed in [1] by proposing a geometric method which <b>handles</b> the <b>uncertainty</b> <b>of</b> the recognition process. This method consists in computing {{the precision of}} the 3 D information of the mobile objects evolving in the scene for each frame of the video sequence. We use the computed information to calculate the probability of the event. The proposed method is tested with videos of everyday activities of elderly people. Events of interest have been modeled {{with the help of}} medical experts (i. e. gerontologists). The experimental results show that the proposed approach improves significantly the process of recognition and can characterize the likelihood of the recognized events...|$|R
