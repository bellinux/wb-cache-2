40|143|Public
25|$|In some algorithms, an {{intermediate}} result is computed {{in a larger}} precision, then must be rounded to the final precision. Double rounding can be avoided by choosing an adequate rounding for the <b>intermediate</b> <b>computation.</b> This consists in avoiding to round to midpoints for the final rounding (except when the midpoint is exact). In binary arithmetic, {{the idea is to}} round the result toward zero, and set the least significant bit to 1 if the rounded result is inexact; this rounding is called sticky rounding. Equivalently, it consists in returning the intermediate result when it is exactly representable, and the nearest floating-point number with an odd significand otherwise; this is why it is also known as rounding to odd.|$|E
5000|$|Memory (typically some {{region of}} virtual memory); which {{includes}} the executable code, process-specific data (input and output), a call stack (to keep track of active subroutines and/or other events), and a heap to hold <b>intermediate</b> <b>computation</b> data generated during run time.|$|E
50|$|The central ideas {{incorporated}} in Rameyev's {{invention of the}} electronic computer included: storing programs in computer memory, using binary code, utilizing external devices, and deploying electronic circuits and semiconductor diodes. The first publication about similar technology outside of the USSR appeared in 1949-1950. Rameyev also suggested that <b>intermediate</b> <b>computation</b> data be automatically printed on punched tape and sent into the computer's arithmetic device for subsequent processing, meaning that the processing of commands would be performed in the computer’s arithmetic device; this is usually {{referred to as the}} Von Neumann architecture.|$|E
50|$|The {{board and}} <b>intermediate</b> <b>computations</b> now look like this.|$|R
5000|$|... {{is defined}} as IEEE 754 double {{extended}} or quad precision if available. Using higher precision than required for <b>intermediate</b> <b>computations</b> can minimize round-off error (the typedef double_t {{can be used for}} code that is portable under all FLT_EVAL_METHODs).|$|R
40|$|AbstractA {{real number}} x {{is said to}} be {{effective}} if there exists an algorithm which, given a required tolerance ɛ∈Z 2 Z, returns a binary approximation x˜∈Z 2 Z for x with |x˜-x|<ɛ. Effective real numbers are interesting in areas of numerical analysis where numerical instability is a major problem. One key problem with effective real numbers is to perform <b>intermediate</b> <b>computations</b> at the smallest precision which is sufficient to guarantee an exact end-result. In this paper we first review two classical techniques to achieve this: a priori error estimates and interval analysis. We next present two new techniques: “relaxed evaluations” reduce the amount of re-evaluations at larger precisions and “balanced error estimates” automatically provide good tolerances for <b>intermediate</b> <b>computations...</b>|$|R
50|$|In some algorithms, an {{intermediate}} result is computed {{in a larger}} precision, then must be rounded to the final precision. Double rounding can be avoided by choosing an adequate rounding for the <b>intermediate</b> <b>computation.</b> This consists in avoiding to round to midpoints for the final rounding (except when the midpoint is exact). In binary arithmetic, {{the idea is to}} round the result toward zero, and set the least significant bit to 1 if the rounded result is inexact; this rounding is called sticky rounding. Equivalently, it consists in returning the intermediate result when it is exactly representable, and the nearest floating-point number with an odd significand otherwise; this is why it is also known as rounding to odd.|$|E
30|$|Our {{approach}} {{is similar to}} that of Langlais and Mandea (2000) which they used to propose a candidate SV model for IGRF- 08. However, whereas Langlais and Mandea (2000) computed SV models by first difference of successive MF models, we directly computed annual SV models from the difference of observatory annual means, bypassing in fact the <b>intermediate</b> <b>computation</b> of MF.|$|E
40|$|This paper {{presents}} {{implementation and}} comparison of two multiplication methods which {{are currently being}} used. The multiplication is carried out between single precision floating point numbers. There is significant reduction in number of <b>intermediate</b> <b>computation</b> using Radix- 4 Booth multiplication algorithm. Comparison of both the methods is done on basis of number of registers and LUTs used for designing. The proposed design is implemented using VHDL on Xilinx ISE...|$|E
50|$|Since JVM 1.2, <b>intermediate</b> <b>computations</b> are {{not limited}} to the {{standard}} 32 bit and 64 bit precisions. On platforms that can handle other representations e.g. 80-bit double extended on x86 or x86-64 platforms, those representations can be used, helping to prevent round-off errors and overflows, thereby increasing precision.|$|R
2500|$|If, however, <b>intermediate</b> <b>computations</b> are all {{performed}} in extended precision (e.g. by setting line [...] to C99 long double), then up to full precision {{in the final}} double result can be maintained. Alternatively, a numerical analysis of the algorithm reveals that if the following non-obvious change to line [...] is made: ...|$|R
30|$|Semi-honest attack model: In this adversary model, {{all parties}} follow the {{protocol}} but dishonest party may be curious to violate others privacy by keeping {{a record of}} all its <b>intermediate</b> <b>computations</b> and messages. This is weak attack model in comparison with malicious attack model. Semi-honest adversaries are also called honest-but-curious or passive attackers.|$|R
40|$|There {{are many}} {{computer}} applications {{that can be}} made incremental. After a small perturbation to the computation at hand, intermediate values of a previous evaluation can be used to obtain the result of the new computation. This requires less time than reevaluating the entire computation. We propose the use of a directed graph to represent computations that we wish to make incremental. This graph, called a dependency graph, represents an <b>intermediate</b> <b>computation</b> at each vertex. Edges between vertices represent the dependence of intermediate computations on other intermediate computations. A change to the computation can be represented as a change in the dependency graph...|$|E
30|$|We {{assume that}} all parties are semi-honest. A semi-honest party truthfully follows a {{protocol}} and sends correct inputs to others, except that he may record all <b>intermediate</b> <b>computation</b> and try to derive other parties’ private inputs from the record. Goldreich has proved that, a protocol which can privately compute a functionality f in the semi-honest model can be complied, by introducing a bit commitment macro, into another protocol which can compute the functionality f in the malicious model. The semi-honest model {{is not only an}} important methodological tool but may also provide a good model in many settings. It suffices to prove that a protocol is secure in the semi-honest model.|$|E
40|$|To {{determine}} {{the value of}} perfect information in an influence diagram, one needs first to modify the diagram to reflect the change in information availability, and then to compute the optimal expected values of both the original diagram and the modified diagram. The value of perfect information {{is the difference between}} the two optimal expected values. This paper is about how to speed up the computation of the optimal expected value of the modified diagram by making use of the <b>intermediate</b> <b>computation</b> results obtained when computing the optimal expected value of the original diagram. Comment: Appears in Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence (UAI 1993...|$|E
40|$|A fast radi- 2 -based median {{filtering}} {{algorithm is}} proposed. The median is determined bit-by-bit successively {{by eliminating the}} samples whose previous bits are different {{to that of the}} median. The <b>intermediate</b> <b>computations</b> of the algorithm do not involve any array computation, nor any memory. The worst-case computational complexity of the algorithm is O(w) for w samples...|$|R
40|$|International audienceThis paper {{presents}} {{a method for}} the automatic generation of high-performance and low-power arithmetic operators based on polynomial approximations. It deals with the bit-level representation of the polynomial coefficients, the <b>intermediate</b> <b>computations</b> width, the approximation and the rounding errors. The generated operators are small, fast and numerically validated at design time. Some examples have been implemented on FPGAs...|$|R
40|$|AbstractThis paper {{presents}} an algorithm for computing algebraically relative resolvents which enhances an existing algorithm by avoiding {{the accumulation of}} superfluous powers in the <b>intermediate</b> <b>computations.</b> The superfluous power generated at each step is predetermined over a certain quotient ring. As a byproduct, an efficient algorithm for extracting an n-th root of a univariate polynomial is obtained...|$|R
40|$|How {{do people}} choose between options? At one extreme, the ‘value-first’ {{view is that}} the brain computes the value of {{different}} options and simply favours options with higher values. An intermediate position, taken by many psychological models of judgment and decision making, is that values are computed but that the resulting choices depend heavily on the context of available options. At the other extreme, the ‘comparison-only’ view argues that choice depends directly on comparisons, with or even without any <b>intermediate</b> <b>computation</b> of value. In this paper, we place past and current psychological and neuroscientific theories on this spectrum, and review empirical data that have led to an increasing focus on comparison rather than value as the driver of choice...|$|E
40|$|Since {{the last}} decade, {{computing}} systems turn to large scale parallel platforms composed {{of thousands of}} processors. Many actual applications run on such systems for long duration, up to several days or weeks. Recently, statistic studies about failures on high performance computing platforms emphasize that the {{mean time between failures}} may not exceed few hours. Thus, it is necessary to develop effcient strategies providing a safe and reliable completion of applications. This may be achieved through redundancy or by storing <b>intermediate</b> <b>computation</b> states on reliable external devices. Saved states are then used to restart computations from the last checkpoint. This last approach called checkpointing {{is one of the most}} popular fault tolerance technique in parallel systems...|$|E
40|$|Colloque sur invitation. internationale. International audienceThis talk {{presents}} new algorithms {{for solving}} polynomial systems {{and in particular}} two new efficient algorithms for computing Gröbner bases. To avoid {{as much as possible}} <b>intermediate</b> <b>computation,</b> the F 4 algorithm computes successive truncated Gröbner bases and replaces the classical polynomial reduction found in the Buchberger algorithm by the simultaneous reduction of several polynomials. This powerful reduction mechanism is achieved by means of a symbolic precomputation and by extensive use of sparse linear algebra methods. A second algorithm, called F 5, eliminate the Buchberger criteria so that there is no reduction to zero during the computation when the input system is a regular sequence. In a second part of the talk we review different applications solved by these algorithms/programs (Robotic, Cryptography, N body problem, Coding theory, [...] .) ...|$|E
40|$|We {{propose a}} unified coded {{framework}} for distributed computing with straggling servers, by introducing a tradeoff between "latency of computation" and "load of communication" for some linear computation tasks. We {{show that the}} coded scheme of [1]-[3] that repeats the <b>intermediate</b> <b>computations</b> to create coded multicasting opportunities to reduce communication load, and the coded scheme of [4], [5] that generates redundant <b>intermediate</b> <b>computations</b> to combat against straggling servers {{can be viewed as}} special instances of the proposed framework, by considering two extremes of this tradeoff: minimizing either the load of communication or the latency of computation individually. Furthermore, the latency-load tradeoff achieved by the proposed coded framework allows to systematically operate at any point on that tradeoff to perform distributed computing tasks. We also prove an information-theoretic lower bound on the latency-load tradeoff, which is shown to be within a constant multiplicative gap from the achieved tradeoff at the two end points. Comment: a shorter version to appear in NetCod 201...|$|R
40|$|A {{classification}} {{problem is}} proposed for supersymmetric evolutionary PDE that satisfy {{the assumptions of}} nonlinearity, nondegeneracy, and homogeneity. Four classes of nonlinear coupled boson-fermion systems are discovered under the weighting as-sumption jf j = jbj = jDtj = 1 2. The syntax of the Reduce package SsTools, which was used for <b>intermediate</b> <b>computations,</b> and the applicability of its procedures to the calculus of super-PDE are described...|$|R
40|$|International audienceThis paper {{presents}} {{some recent}} works on hardware evaluation of functions. A method for the automatic generation of high-performance arithmetic operators based on polynomial approximations is described. It {{deals with the}} bit-level representation of the polynomial coefficients, the <b>intermediate</b> <b>computations</b> width, the approximation and the rounding errors. The generated operators are small, fast and numerically validated at design time. Some examples have been implemented on FPGAs...|$|R
40|$|Although {{triangle}} meshes {{are used}} pervasively in 3 D graphics applications and there exist highly efficient mesh representations, almost all existing 3 D graphics processors {{are based on}} the assumption that individual triangles are processed completely independently of one another. Consequently, none of these graphics processors are able to exploit triangle mesh's vertex/edge sharing property. This paper describes a meshoriented 3 D graphics architecture called Heresy, which treats meshes as first-class objects, and significantly reduces the computation and communication costs of rendering triangle meshes by reusing <b>intermediate</b> <b>computation</b> results and eliminating data redundancy. The central architectural feature of Heresy is a highly efficient triangle mesh representation based on breadth-first mesh traversal, which is applied throughout the entire 3 D graphics pipeline, from geometric transformation, clipping, to rasterization, and thus enables aggressive exploitation of vertex/edge sh [...] ...|$|E
30|$|Efficient {{systems for}} the {{lossless}} compression of image data require a decorrelation step which maps the integer input samples to integer output values. In wavelet-based compression systems (see [1] for an overview), this is achieved by using the lifting implementation of a discrete wavelet transform (DWT) [2] {{in combination with the}} rounding of <b>intermediate</b> <b>computation</b> results, which is called integer wavelet transform (IWT) [3]. Beginning with initial investigations on the IWT, which were also motivated by the standardization of the new image compression system JPEG 2000 [4], and its application in the JPEG 2000 framework [5], this topic has received growing attention. The idea of integer transforms relates back to the so-called S transform [6], the improved version called S + P transform [7], and the reversible TS transform [8]. Since then, several integer wavelet transforms have been analysed in terms of their performance in image compression systems [9].|$|E
40|$|Abstract. We {{propose a}} method that uses {{off-the-shelf}} binary classifiers and combines them in real-time to achieve quality, time, and cost characteristics that satisfy user-provided constraints; moreover, it shares <b>intermediate</b> <b>computation</b> (e. g., features, classifiers, kernels) between classes to reduce time and cost. An important distinction between this approach and others is that {{the primary focus of}} this work is on improving flexibility, operation {{on a wide range of}} operating points, as opposed to quality or speed; however, we show that the proposed approach achieves performance comparable to the state-of-the-art on the scene recognition task for the SUN 397 [1] where we attain a mean AUC of 0. 9625 compared to 0. 9573 [1], 0. 8426 [2], 0. 9227 [3], and 0. 9343 (based on [4]). Additionally, we show that human annotators can be naturally integrated into the approach to produce hybrid human/algorithmic classifiers. ...|$|E
40|$|The "classical " PSO {{version is}} very simple but the user have to define some {{parameters}} (swarm size, neighbourhoods, some coefficients). An adaptive version like TRIBES [1, 2] does not have this drawback. It is also often more effective when the criterion takes into account only the relation "best value vs number of fitness evaluations". However it is more time consuming, for it performs some <b>intermediate</b> <b>computations</b> in order t...|$|R
40|$|In this paper, we {{revisit the}} {{previous}} multi-precision multiplication techniques including “operand-scanning”, “hybrid-scanning”, “operand-caching”, “consecutive operand-caching” and “product-scanning. ” Particularly, the former four methods execute an <b>intermediate</b> result <b>computation</b> which is process for updating the results with a newly computed result by computing {{a number of}} addition operations. This operations is expensive, so efficient implementation is required to boost the performance. For this reason, we propose a novel method, i. e., “Carry-Once”, which reduces the number of <b>intermediate</b> result <b>computation</b> by size of result accumulation. The main idea is gathering carry values and updating the values at once. This method improves all multi-precision multiplication techniques having <b>intermediate</b> result <b>computation</b> and show performance enhancement in terms of speed by up to 2. 5 %, compared with best known results...|$|R
40|$|We propose an {{efficient}} algorithm for computing triangular decompositions of algebraic varieties. It {{is based on}} an incremental process and produces components in order of decreasing dimension. The combination of these two major features is obtained by means of lazy evaluation techniques and a lifting property for calculations modulo regular chains. This allows a good management of the <b>intermediate</b> <b>computations,</b> as confirmed by several implementations and applications of this work. Our algorithm is also well suited for parallel execution...|$|R
40|$|The classic {{approach}} to structure from motion entails a clear separation between motion estimation and structure estimation and between 2 D and 3 D information. For {{the recovery of}} the rigid transformation between different views only 2 D image measurements are used. To have available enough information, most existing techniques are based on the <b>intermediate</b> <b>computation</b> of optical flow which, however, poses a problem at the locations of depth discontinuities. If we knew where depth discontinuities were, we could (using a multitude of approaches based on smoothness constraints) accurately estimate flow values for image patches corresponding to smooth scene patches; but to know the discontinuities requires solving the structure from motion problem first. This paper introduces a novel {{approach to}} structure from motion which addresses the processes of smoothing, 3 D motion and structure estimation in a synergistic manner. It provides an algorithm for estimating the transformation between two v [...] ...|$|E
40|$|Segment model (SM) is {{a family}} of methods by using {{segmental}} distribution rather than frame-based features (e. g. HMM) to represent the underlying characteristics of observation sequence. It has been {{proved to be more}} precise than that of HMM. However, the high complexity prevents these models from practical system. In this paper we present a framework to reduce the computational complexity of segment model by fixing the number of the basic unit in the segment to share the <b>intermediate</b> <b>computation</b> results. Our work is twofold. First, we compared the complexity of SM with HMM and proposed a fast SM framework based on the comparison. Second we use two examples to illustrate this framework. The fast SMs have better performance than the system based on HMM, and at the mean time, we successfully keep the computation complexity of SM at the same level of HMM [...] ...|$|E
40|$|International audienceThis paper {{introduces}} a new efficient algorithm for computing Gröbner bases. To avoid as much <b>intermediate</b> <b>computation</b> as possible, the algorithm computes successive truncated Gröbner bases and it replaces the classical polynomial reduction {{found in the}} Buchberger algorithm by the simultaneous reduction of several polynomials. This powerful reduction mechanism is achieved {{by means of a}} symbolic precomputation and by extensive use of sparse linear algebra methods. Current techniques in linear algebra used in Computer Algebra are reviewed together with other methods coming from the numerical field. Some previously untractable problems (Cyclic 9) are presented as well as an empirical comparison of a first implementation of this algorithm with other well known programs. This comparison pays careful attention to methodology issues. All the benchmarks and CPU times used in this paper are frequently updated and available on a Web page. Even though the new algorithm does not improve the worst case complexity it is several times faster than previous implementations both for integers and modulo p computations...|$|E
40|$|Abstract. Given {{a number}} of Dempster-Shafer belief {{functions}} there are different architectures which allow to do a compilation of the given knowledge. These architectures are the Shenoy-Shafer Architecture, the Lauritzen-Spiegelhalter Architecture and the HUGIN Architecture. We propose a new architecture called ”Fast-Division Architecture ” {{which is similar to}} the former two. But there are two important advantages: (i) results of <b>intermediate</b> <b>computations</b> are always valid Dempster-Shafer belief functions and (ii) some operations can often be performed much more efficiently. ...|$|R
40|$|In exact {{computing}} environments such as Maple and Mathematica problems {{often have}} symbolic parameters. As such a typical domain for computation {{is an integral}} domain (such as Q[a 1, [...] ., ak]) rather than a field. In such environments growth of coefficients in <b>intermediate</b> <b>computations</b> are a central concern. For methods that involve elimination intermediate growth can be controlled by removing greatest common divisors at each step. Fraction-free computation is an elimination process which controls coefficient growth in <b>intermediate</b> <b>computations</b> {{while at the same}} time avoids expensive greatest common divi-sor computations. In this talk we give a new, fast algorithm for solving the simultaneous Pade ́ approxi-mation problem. The algorithm is fraction-free and is intended for computation in exact arithmetic. The algorithm gives significant improvement on previous fraction-free meth-ods, in particular when solved via the use of vector Hermite-Pade ́ approximation using the FFFG order basis algorithm previously done by the authors. The improvements are both in terms of bit complexity and in reduced size of the intermediate quantities. The primary technique takes advantage of certain duality properties of Hermite-Pade ́ and Simultaneous-Pade ́ approximation problems...|$|R
40|$|In {{this paper}} we give a new, fast {{algorithm}} for solving the simultaneous Padé approximation problem. The algorithm is fraction-free and {{is suitable for}} computation in domains where growth of coefficients in <b>intermediate</b> <b>computations</b> are a central concern. The algorithm gives significant improvement on previous fraction-free methods, in particular when solved via the use of vector Hermite-Padé approximation using the FFFG order basis algorithm previously done by the authors. The improvements are {{both in terms of}} bit complexity and in reduced size of the intermediate quantities. 1...|$|R
