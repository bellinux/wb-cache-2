1011|5377|Public
50|$|PTPicker: Java {{front end}} to {{panorama}} stitcher and other tools. It provides a graphical interface for <b>feature</b> <b>point</b> selection and position optimization.|$|E
5000|$|So the {{retrieval}} function {{could be}} formed {{based on such}} optimal classifier. For new query , the retrieval function first projects all elements of the database to the feature space. Then it orders these feature points by the values of their inner products with the optimal vector. And the rank of each <b>feature</b> <b>point</b> is the rank of the corresponding element of database for the query [...]|$|E
50|$|The big {{football}} {{events on}} the global sporting calendar always feature such as African Cup of Nations, World Cup and European Championships. As do the multi-sport events like the Olympics, Commonwealth and Asian Games. The regular {{events on the}} annual calendar are a <b>feature</b> <b>point</b> of the year such as the Grand Slam's in Tennis, the Golfing Majors, Test cricket, Formula One, Diamond League Athletics, the NBA, the Baseball World Series and Moto GP.|$|E
40|$|Abstract: <b>Feature</b> <b>points</b> {{can be used}} {{to match}} images. Candidate <b>feature</b> <b>points</b> are {{extracted}} through SIFT firstly. Then <b>feature</b> <b>points</b> are selected from candidate points through singular value decomposing. Distance between <b>feature</b> <b>points</b> sets is computed According to theory of invariability of <b>feature</b> <b>points</b> set, images are matched if the distance is less than a threshold. Experiment showed that this algorithm is available...|$|R
40|$|Abstract: A {{method to}} {{determine}} the stationery probability of regions or <b>feature</b> <b>points</b> in a video sequence is proposed in this paper. This is done by identifying <b>feature</b> <b>points</b> using the Harris corner detector, finding descriptors for the <b>feature</b> <b>points</b> and then tracking the <b>feature</b> <b>points.</b> The information gained from tracking the <b>feature</b> <b>points</b> is then used {{to determine the}} stationery probability of these features [...] ...|$|R
40|$|A {{method to}} {{determine}} the stationery probability of regions or <b>feature</b> <b>points</b> in a video sequence is proposed in this paper. This is done by identifying <b>feature</b> <b>points</b> using the Harris corner detector, finding descriptors for the <b>feature</b> <b>points</b> and then tracking the <b>feature</b> <b>points.</b> The information gained from tracking the <b>feature</b> <b>points</b> is then used {{to determine the}} stationery probability of these features. This method is shown to successfully identify probable stationery and moving regions in video sequences...|$|R
50|$|B company under Lieutenant O.P. Balhara {{operated}} along Mandi towards Gagariam and C company under Captain Kalam Singh {{moved to}} capture <b>feature</b> <b>Point</b> 8677 {{and the area}} northeast of it. B company occupied all the posts vacated earlier by 7th Battalion, the Sikh Regiment. It also brought all the weapons left behind by 7th Battalion, The Sikh Regiment including 3 inch mortars and medium machine guns and were handed over to them. Ammunition could not be brought back by the company and was destroyed in location.|$|E
5000|$|Ordon {{made her}} film debut in 2002 in Cédric Anger's Novela. [...] She then portrayed {{one of the}} main {{characters}} in the 2003 movie Inquiétudes by director Gilles Bourdos (released with the title [...] "A Sight for Sore Eyes" [...] in the United States). A supporting role in the 2004 independent <b>feature</b> <b>Point</b> & Shoot directed by Shawn Regruto led to the steamy opener in bed with Daniel Craig in 2008's Flashbacks of a Fool. She was also filmed in Indian Bollywood romantic comedy drama picture Befikre as [...] "Christine".|$|E
5000|$|Their single [...] "Jersey Strong" [...] {{was written}} by Strevens while he had no power {{in the wake of}} Hurricane Sandy in October 2012. It has become a crowd {{favorite}} and a staple in every show. It has been named the official anthem of New Jersey by the organization Strong Than The Storm [...] and was a <b>feature</b> <b>point</b> on Discovery's Destination America Red, White and You. It has put them in the national media spotlight, in which they are currently doing a media tour in support of the song.|$|E
30|$|Based on {{the study}} of SIFT {{algorithm}}, a SURF (Speeded-Up Robust Features) algorithm is proposed in document [15]. The algorithm improves the speed and stability of finding <b>feature</b> <b>points.</b> In order to achieve the purpose of acceleration, it integrates the original image and uses the Haar wavelet derivative instead of the Gaussian filter. The Hessian matrix is used for the robust row algorithm to increase the <b>feature</b> <b>points.</b> The main implementation process includes detection of <b>feature</b> <b>points,</b> description of <b>feature</b> <b>points,</b> and matching of <b>feature</b> <b>points</b> [16].|$|R
40|$|This paper {{proposes a}} method for line {{matching}} based on invariance of <b>feature</b> <b>points.</b> Firstly, <b>feature</b> <b>points</b> are roughly matched by Normalized Cross Correlation (NNC) and Average of Square Difference (ASD). Additionally, <b>feature</b> <b>points</b> obtained from the two views are grouped into matched point pairs. Finally, curve segments between matched point pairs are matched by dynamic programming algorithm with edge potential functions (EPF) taken as the measure. The proposed method makes full use of <b>feature</b> <b>points,</b> the relationship between <b>feature</b> <b>points</b> and the curve, and space information of the gray image.  </p...|$|R
40|$|This paper {{introduces}} {{an object}} shape representation using Kernel Density <b>Feature</b> <b>Points</b> Estimator (KDFPE). In this method we obtain {{the density of}} <b>feature</b> <b>points</b> within defined rings around the centroid of the image. The Kernel Density <b>Feature</b> <b>Points</b> Estimator is then applied to the vector of the image. KDFPE is invariant to translation, scale and rotation. This method of image representation shows improved retrieval rate when compared to Density Histogram <b>Feature</b> <b>Points</b> (DHFP) method. Analytic analysis is done to justify our method and we compared our results with object shape representation by the Density Histogram of <b>Feature</b> <b>Points</b> (DHFP) to prove its robustness...|$|R
50|$|Gord became {{involved}} in the film industry in Toronto, Ontario in the early 1970s. He produced the ultra-low budget Dream On The Run in 1973 and was production manager on another no-budget Canadian <b>feature</b> <b>Point</b> of No Return. In 1977, he produced the low-budget sci-fi film Starship Invasions, which was distributed by Warner Bros. and in 1979, The High Country for Crown International. He continued to production manage and/or line produce other low-budget films and television shows. Some examples include Deadly Eyes, Loose Screws, Recruits, Busted Up, Mr. Nice Guy, The Housekeeper, The Edison Twins and The Brain, through the 1980s.|$|E
5000|$|They {{formed in}} Monmouth County NJ in 2006, {{although}} various members have played together in other bands for over 10 years. The band currently divides it's time between New Jersey and Nashville TN, but perform shows {{all over the}} country. They play a large repertoire of their own material written mainly by head songwriter Strevens, but also include collaborations with Kelly and Burlett. 2010 saw the independent release of their debut album [...] "Hillbilly Clubhouse" [...] to critic’s raves with the title track receiving airplay both nationally and internationally. In 2012 [...] "Burning Down The Farm" [...] the 2nd single from that album stayed on the Music Row Charts for 32 weeks hitting #72. The single was also included on the Sony Music distributed compilation CD [...] "Country Mix USA" [...] pairing them alongside other country artists Craig Morgan, Due West, and Sammy Kershaw. Early 2013 the band participated in the Hard Rock Cafe's Hard Rock Rising competition, which was a worldwide battle of the bands that started with over 12.000 bands around the world. After The Reign won the New York City market, in which they represented {{the area in the}} competition. Over-all they ranked in the top 20 of the 12,000 bands. They released their sophomore album [...] "Almost Famous" [...] in the summer of 2013, produced by Marc Muller, mixed by Billy Decker, and mastered by Richard Dodd. Their single [...] "Jersey Strong" [...] featured on their second album has been named the official anthem of New Jersey by the organization Strong Than The Storm and was a <b>feature</b> <b>point</b> on Discovery's Destination American Red, White and You. It has put them in the national media spotlight, in which they completed a media tour in support of the song. In 2014 the band suffered a setback with the sudden death of longtime drummer Herb Van Note. For a moment the band was unsure if it could continue, but with the help of replacement drummer Greg Annunziata the band hit the road again in April 2014. 2014-2015 the band performed headlining theater shows in New Jersey and Virginia. In 2016 After The Reign was named a top 10 Best Bar Band At the Jersey Shore by 105.7 The Hawk's fan voted competition. In late 2016, drummer Greg Annunziata announced his departure from the band. Through the end of 2016 into 2017 the band searched for a new drummer. In February 2017 ATR welcomed John Carretta as the band's new drummer. The band also renewed their contract with Bar Anticipation for the 2017 year for the fifth year in a row.|$|E
5000|$|... '"The KING {{has been}} graciously pleased {{to approve the}} posthumous award of the Victoria Cross to Second-Lieutenant Moana-Nui-a-Kiwa Ngarimu."During the action at the Tebaga Gap on 26 March 1943, 2nd Lieutenant Ngarimu {{commanded}} a platoon in an attack upon the vital hill <b>feature,</b> <b>Point</b> 209. He was given the task of attacking and capturing an under-feature forward of Point 209 itself and held in considerable strength by the enemy. He led his men with great determination straight up {{the face of the}} hill, undeterred by the intense mortar and machine-gun fire, which caused considerable casualties. Displaying courage and leadership of the highest order, he was himself first on the hill crest, personally annihilating at least two enemy machine-gun posts. In the face of such a determined attack the remainder of the enemy fled, but further advance was impossible as the reverse slope was swept by machine-gun fire from Point 209 itself."Under cover of a most intense mortar barrage the enemy counter-attacked, and 2nd Lieutenant Ngarimu ordered his men to stand to and engage the enemy man or man. This they did with such good effect that the attackers were virtually mown down, 2nd Lieutenant Ngarimu personally killing several. He was twice wounded, once by rifle fire in the shoulder and later by shrapnel in the leg, and though urged by both his company and battalion commanders to go out, he refused to do so, saying that he would stay a little while with his men. He stayed until he met his death the following morning."Darkness found this officer and his depleted platoon lying on the rock face of the forward slope of the hill feature, with the enemy in a similar position on the reverse slope about twenty yards distant. Throughout the night the enemy repeatedly launched fierce attacks in an attempt to dislodge 2nd Lieutenant Ngarimu and his men, but each counter-attack was beaten off by 2nd Lieutenant Ngarimu's inspired leadership. During one of these counter-attacks the enemy, using hand grenades, succeeded in piercing a certain part of the line. Without hesitation this officer rushed to the threatened area, and those of the enemy he did not kill he drove back with stones and with his tommy-gun."During another determined counter-attack by the enemy, part of his line broke. Yelling orders and encouragement, he rallied his men and led them in a fierce onslaught back into their old positions. All through the night, between attacks, he and his men were heavily harassed by machine-gun and mortar fire, but 2nd Lieutenant Ngarimu watched his line very carefully, cheering his men on and inspiring them by his personal conduct. Morning found him still in possession of the hill feature but only he and two unwounded other ranks remained. Reinforcements were sent up to him. In the morning the enemy again counter-attacked and it was during this attack that 2nd Lieutenant Ngarimu was killed. He was killed on his feet defiantly facing the enemy with his tommy-gun at his hip. As he fell he came to rest almost on top of those of the enemy who had fallen, the number of whom survived testified to his outstanding courage and fortitude." ...|$|E
40|$|Tliis paper {{presents}} the method {{to determine the}} order of <b>feature</b> (attention) <b>points</b> for face detection. The order of <b>feature</b> <b>points</b> is determined {{in terms of the}} classification ability to face and non-face images (cross validation). The matching is performed in the order of the selected <b>feature</b> <b>points.</b> It is confirmed that high rec:ognition rate is obtained by using {{only a small number of}} <b>feature</b> <b>points.</b> By ordering of the featuro points, we can reject the input image as non-face by systemat,ically checking the <b>feature</b> <b>points.</b> This can speed up the face detection. ...|$|R
5000|$|Find the model's <b>feature</b> <b>points.</b> Assume that 5 <b>feature</b> <b>points</b> {{are found}} in the model image with the {{coordinates}} , see the picture.|$|R
3000|$|Suppose {{that there}} are m <b>feature</b> <b>points</b> in the first group and n <b>feature</b> <b>points</b> in the second group. Consider a matrix [...]...|$|R
30|$|Where m(x,y) {{denotes the}} {{gradient}} modulus {{value of the}} <b>feature</b> <b>point</b> (x,y), θ(x,y) denotes the gradient direction of the <b>feature</b> <b>point,</b> and L(x, y) denotes the Gaussian image of the scale on which the <b>feature</b> <b>point</b> is located. For each <b>feature</b> <b>point,</b> a histogram {{can be used to}} count the gradient distribution of the pixel gray values in the neighborhood of the pixel center, that is, determine the main direction of the <b>feature</b> <b>point.</b>|$|E
3000|$|... 4. A {{key point}} {{descriptor}} is constructed. Through {{the calculation of}} the above three steps, each <b>feature</b> <b>point</b> detected contains three messages ((x,[*]y),[*]σ,[*]θ). That is, position, scale, and direction. Because the descriptor of the <b>feature</b> <b>point</b> {{is related to the}} scale of the <b>feature</b> <b>point,</b> the generation of the <b>feature</b> <b>point</b> descriptor needs to be carried out in the Gaussian pyramid space of the corresponding scale [14]. First, the neighborhood centered on the <b>feature</b> <b>point</b> is divided into BP[*]×[*]BP sub-blocks, and the edge size of each sub-block is mσ pixels. The construction process of <b>feature</b> <b>point</b> descriptors is as follows: first, taking the <b>feature</b> <b>point</b> as the center, the rotation angle of the image in the neighborhood of the ([...] mσ(B_p+ 1)√(2)× mσ(B_p+ 1)√(2)) [...] of the <b>feature</b> <b>point</b> is θ to the main direction of the <b>feature</b> <b>point.</b> Then, take the <b>feature</b> <b>point</b> as the center, select the mσBp[*]×[*]σBp size image block, and divide the interval into the BP[*]×[*]BP sub block, then use the gradient histogram to calculate the gradient accumulating value of all pixels in each sub block in eight directions, and form the seed point. A 128 dimensional feature vector is formed. In addition, in the process of constructing <b>feature</b> <b>point</b> descriptors, all the pixels in the neighborhood of the <b>feature</b> <b>point</b> need to be weighted by Gaussian, and all the pixels in the neighborhood range of the feature points need to be normalized two times in order to remove the influence of illumination and other factors [15].|$|E
30|$|Comparative {{analysis}} of BRISK and ORB algorithm {{in the same}} scale rotating 90 ° image <b>feature</b> <b>point</b> extraction and different scale scaling 50 % and rotating 90 ° image <b>feature</b> <b>point</b> extraction time, <b>feature</b> <b>point</b> extraction number, accuracy, etc.|$|E
40|$|In {{this paper}} we address the {{initialization}} problem for model-based coding system. A semi-automatic scheme based on using <b>feature</b> <b>points</b> is proposed. It has three features: 1) uses personal <b>feature</b> <b>points,</b> 2) user’s assistance is necessary, and 3) it is a global optimal solution. The minimum spanning tree (MST) technique is used to organize <b>feature</b> <b>points</b> into an ordered path and a dynamic programming based matching technique is developed and used to localize the defined <b>feature</b> <b>points.</b> Promising results are obtained and reported...|$|R
40|$|This paper {{presents}} {{a method to}} automatically locate facial <b>feature</b> <b>points</b> under large variations in pose, illumination and facial expressions. First we propose a method to calculate probabilistic-like output for each pixel of image. This probabilistic-like output describes {{the possibility of the}} pixel {{to be the center of}} specified object. A Gaussian Mixture Model is used to approximate the distribution of probabilistic-like output. The centers of these Gaussians are assigned with a probabilistic-like measure and they are considered as candidate <b>feature</b> <b>points.</b> There might be one or more candidate <b>feature</b> <b>points</b> in each facial region. A 3 D model of facial <b>feature</b> <b>points</b> is built to enforce constraints on the localization results of <b>feature</b> <b>points.</b> Compared with Active Shap...|$|R
3000|$|The {{number of}} {{extracted}} <b>feature</b> <b>points</b> in the UR-SIFT was determined based on 0.4 % of image size. The 2, 500 <b>feature</b> <b>points</b> {{for the first}} dataset and 4, 000 <b>feature</b> <b>points</b> for the second dataset are adequate to ensure proper working of the proposed method. However, in some cases of challenging image pairs, 4, 500 <b>feature</b> <b>points</b> result in an expected working of the proposed method. The size of each cell in the grid is 200 [*]×[*] 200 pixels. The optimum values of W [...]...|$|R
30|$|Figure  17 {{showed the}} first test results {{from top to bottom}} from 7 to 35  m. There were 7 obvious feature points in the figure in total. <b>Feature</b> <b>point</b> 1 was located on {{abscissa}} 20 – 40  ns and ordinate 7 – 10  m, <b>feature</b> <b>point</b> 2 was located on abscissa 50 – 72  ns and ordinate 8 – 16  m, <b>feature</b> <b>point</b> 3 was located on abscissa 75 – 85  ns and ordinate 7 – 14  m, <b>feature</b> <b>point</b> 4 was located on abscissa 95 – 110  ns and ordinate 14 – 21  m, <b>feature</b> <b>point</b> 5 was located on abscissa 22 – 60  ns and ordinate 28 – 34  m, <b>feature</b> <b>point</b> 6 was located on abscissa 79 – 91  ns and ordinate 23 – 31  m, and <b>feature</b> <b>point</b> 7 was located on abscissa 110 – 140  ns and ordinate 20 – 28  m.|$|E
40|$|Abstract. This paper {{presents}} {{a method for}} detecting feature points from an image and locating their matching correspondence points across images. The proposed method leverages a novel rapid LBP <b>feature</b> <b>point</b> detection to filter out texture-less SURF feature points. The detected feature points, also known as Non-Uniform SURF feature points, are used to match corresponding feature points from other frame images to reliably locate positions of moving objects. The proposed method consists of two processing modules: <b>Feature</b> <b>Point</b> Extraction (FPE) and <b>Feature</b> <b>Point</b> Mapping (FPM). First, FPE extracts salient feature points with Feature Transform and <b>Feature</b> <b>Point</b> Detection. FPM is then applied to generate motion vectors of each <b>feature</b> <b>point</b> with Feature Descriptor and <b>Feature</b> <b>Point</b> Matching. Experiments are conducted on both artificial template patterns and real scenes captured from moving camera at different speed settings. Experimental {{results show that the}} proposed method outperforms the commonly-used SURF <b>feature</b> <b>point</b> detection and matching approach...|$|E
40|$|<b>Feature</b> <b>point</b> {{detection}} {{is generally}} {{the first step}} in model-based approaches to sketch recognition. <b>Feature</b> <b>point</b> detection in free-hand strokes is a hard problem because the input has noise from digitization, from natural hand tremor, and from lack of perfect motor control during drawing. Existing <b>feature</b> <b>point</b> detection methods for free-hand strokes require hand-tuned thresholds for filtering out the false positives. In this paper, we present a threshold-free <b>feature</b> <b>point</b> detection method using ideas from the scale-space theory...|$|E
3000|$|... {{correlation}} {{window of}} the two <b>feature</b> <b>points</b> before orientation normalization. Figures 3 (e) and 3 (f) show the correlation {{window of the}} two <b>feature</b> <b>points</b> after orientation normalization.|$|R
40|$|Abstract. A {{new model}} for {{tracking}} of <b>feature</b> <b>points</b> in dynamic image is proposed. The model {{is represented in}} a form of nonlinear state space model having state variables with positions of <b>feature</b> <b>points,</b> velocities for each object, and object labels that specify the associations between the <b>feature</b> <b>points</b> and the objects. We use particle filters with Rao-Blackwellization to estimate {{the state of the}} nonlinear model. By estimating the state, we obtain the tracking result of <b>feature</b> <b>points,</b> which consists of positions of <b>feature</b> <b>points</b> and velocities for each object, as well as the classification result of the <b>feature</b> <b>points</b> into objects from the estimate of the associations. 3 D reconstruction is also dealt with in this framework by introducing a camera projection model into observation equation of the state space model. Experiments using real images for 2 D tracking and 3 D reconstruction show the efficiency of the model and the method...|$|R
40|$|Automatic cell {{segmentation}} has various application potentials in cytometry and histometry. In this paper, {{an automatic}} cluster (touching) cell segmentation approach using the dominant contour <b>feature</b> <b>points</b> has been presented. Dominant <b>feature</b> <b>points</b> are {{the locations of}} indentation on the contour of the cluster. First, dominant <b>feature</b> <b>points</b> on the contour of the cluster are detected by distance profile. Next, using shape features of the cells, these <b>feature</b> <b>points</b> are selected for segmentation. We compared {{the results of the}} proposed method with manual segmentation and observed that the method has an overall accuracy about to 82 %...|$|R
30|$|Feature {{description}} vector generation. On {{the scale}} {{image of the}} <b>feature</b> <b>point,</b> {{the orientation of the}} coordinate axis and the orientation of the <b>feature</b> <b>point</b> are adjusted to be the same, and then the <b>feature</b> <b>point</b> is taken as the center. Similar to SIFT method of constructing the feature description vector, a 64 -dimensional feature vector is obtained.|$|E
3000|$|... {{stands for}} the {{similarity}} measure between the i th <b>feature</b> <b>point</b> in the first group and the j th <b>feature</b> <b>point</b> in the second group. If [...]...|$|E
40|$|Facial <b>feature</b> <b>point</b> {{tracking}} is {{a research}} area {{that can be used}} in human-computer interaction (HCI), facial expression analysis, fatigue detection, etc. In this paper, a statistical method for facial <b>feature</b> <b>point</b> tracking is proposed. <b>Feature</b> <b>point</b> tracking is a challenging topic in case of uncertain data because of noise and/or occlusions. With this motivation, a graphical model that incorporates not only temporal information about <b>feature</b> <b>point</b> movements, but also information about the spatial relationships between such points is built. Based on this model, an algorithm that achieves <b>feature</b> <b>point</b> tracking through a video observation sequence is implemented. The proposed method is applied on 2 D gray scale real video sequences taken in a vehicle environment and the superiority of this approach over existing techniques is demonstrated...|$|E
40|$|The identification, tracking, and {{statistical}} analysis of tropical convective complexes using satellite imagery is explored {{in the context of}} identifying <b>feature</b> <b>points</b> suitable for tracking. The <b>feature</b> <b>points</b> are determined based on the shape of complexes using the distance transform technique. This approach has been applied to the determination <b>feature</b> <b>points</b> for tropical convective complexes identified in a time series of global cloud imagery. The <b>feature</b> <b>points</b> are used to track the complexes, and from the tracks statistical diagnostic fields are computed. This approach allows the nature and distribution of organized deep convection in the Tropics to be explored...|$|R
30|$|In the {{computer}} image processing, the <b>feature</b> <b>points</b> {{play a very}} important role in characteristic recognition. <b>Feature</b> <b>points,</b> such as angular point, tangency point and inflection point, are the basic units to characterize a specific shape. They can be applied to senior visual processing such as pattern recognition, shape matching and dimension measurement, etc. For the spline shaft measuring, the <b>feature</b> <b>points</b> are the tooth roots and tooth crests.|$|R
3000|$|If the {{reflectance}} or colour images {{associated with}} the range scans are available we can extract the corresponding <b>feature</b> <b>points</b> (e.g., SIFT or SURF <b>feature</b> <b>points</b> [32]) associated with each 3 -D point of [...]...|$|R
