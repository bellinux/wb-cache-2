1360|1818|Public
5|$|The {{idea of an}} {{associative}} array {{that allows}} data to be accessed by its value rather than by its address {{dates back to the}} mid-1940s in the work of Konrad Zuse and Vannevar Bush, but <b>hash</b> <b>tables</b> were not described until 1953, in an IBM memorandum by Hans Peter Luhn. Luhn used a different collision resolution method, chaining, rather than linear probing.|$|E
5|$|For {{implementing}} associative arrays, <b>hash</b> <b>tables,</b> a {{data structure}} that maps keys to records using a hash function, are generally faster than binary search on a sorted array of records; most implementations require only amortized constant time on average. However, hashing is not useful for approximate matches, such as computing the next-smallest, next-largest, and nearest key, {{as the only}} information given on a failed search is that the target is not present in any record. Binary search is ideal for such matches, performing them in logarithmic time. In addition, all operations possible on a sorted array can be performed—such as finding the smallest and largest key and performing range searches.|$|E
25|$|The first {{generation}} peer-to-peer file sharing networks, such as Napster, {{relied on a}} central database to co-ordinate look ups on the network. Second generation peer-to-peer networks, such as Gnutella, used flooding to locate files, searching every node on the network. Third generation peer-to-peer networks use Distributed <b>hash</b> <b>tables</b> to look up files in the network. Distributed <b>hash</b> <b>tables</b> store resource locations throughout the network. A major criterion for these protocols is locating the desired nodes quickly.|$|E
50|$|When the hash {{function}} {{is used to}} store values in a <b>hash</b> <b>table</b> that outlives {{the run of the}} program, and the <b>hash</b> <b>table</b> needs to be expanded or shrunk, the <b>hash</b> <b>table</b> is referred to as a dynamic <b>hash</b> <b>table.</b>|$|R
50|$|Linear hashing is a <b>hash</b> <b>table</b> {{algorithm}} {{that permits}} incremental <b>hash</b> <b>table</b> expansion. It is implemented using a single <b>hash</b> <b>table,</b> but with two possible lookup functions.|$|R
30|$|Some {{researchers}} use various data {{structures to}} improve the efficiency of association rule mining algorithms. Singh [35] tries to use a <b>hash</b> <b>table,</b> <b>hash</b> trie and <b>hash</b> <b>table</b> trie for candidate storage in Apriori MapReduce-based implementation. They find that <b>hash</b> <b>table</b> trie is most efficient than others in MapReduce context {{while it is not}} much efficient in a sequential approach.|$|R
25|$|Some tries {{can require}} more space than a hash table, as memory may be {{allocated}} for each {{character in the}} search string, rather than a single chunk of memory for the whole entry, as in most <b>hash</b> <b>tables.</b>|$|E
25|$|Tries can be slower in {{some cases}} than <b>hash</b> <b>tables</b> for looking up data, {{especially}} if the data is directly accessed on a hard disk drive or some other secondary storage device where the random-access time is high compared to main memory.|$|E
25|$|Different {{kinds of}} data {{structures}} are suited to {{different kinds of}} applications, and some are highly specialized to specific tasks. For example, databases use B-tree indexes for small percentages of data retrieval and compilers and databases use dynamic <b>hash</b> <b>tables</b> as look up tables.|$|E
25|$|A <b>hash</b> <b>table</b> may use linked lists {{to store}} the chains of items that hash to the same {{position}} in the <b>hash</b> <b>table.</b>|$|R
25|$|Looking up {{data in a}} trie {{is faster}} in the worst case, O(m) time (where m is {{the length of a}} search string), {{compared}} to an imperfect <b>hash</b> <b>table.</b> An imperfect <b>hash</b> <b>table</b> can have key collisions. A key collision is the hash function mapping of different keys to the same position in a <b>hash</b> <b>table.</b> The worst-case lookup speed in an imperfect <b>hash</b> <b>table</b> is O(N) time, but far more typically is O(1), with O(m) time spent evaluating the hash.|$|R
5000|$|Once the <b>hash</b> <b>table</b> is built, {{scan the}} larger {{relation}} {{and find the}} relevant rows from the smaller relation by looking in the <b>hash</b> <b>table.</b>|$|R
25|$|Because conses {{and lists}} are so {{universal}} in Lisp systems, it {{is a common}} misconception that they are Lisp's only data structures. In fact, all but the most simplistic Lisps have other data structures, such as vectors (arrays), <b>hash</b> <b>tables,</b> structures, and so forth.|$|E
25|$|Exploiting {{distributed}} <b>hash</b> <b>tables</b> (DHT): This attack exploits {{the fact}} that distributed hash table (DHT) connections through Tor are impossible, so an attacker is able to reveal a target's IP address by looking {{it up in the}} DHT even if the target uses Tor to connect to other peers.|$|E
25|$|Distributed <b>hash</b> <b>tables</b> use a more {{structured}} key-based routing {{in order to}} attain both the decentralization of Freenet and gnutella, and the efficiency and guaranteed results of Napster. One drawback is that, like Freenet, DHTs only directly support exact-match search, rather than keyword search, although Freenet's routing algorithm can be generalized to any key type where a closeness operation can be defined.|$|E
50|$|In {{order to}} meet the {{requirements}} of speed generally demanded by a computer game, files are indexed in a <b>hash</b> <b>table</b> using a quick, low-collision hashing algorithm. The index of a specific file within the <b>hash</b> <b>table</b> is the <b>hash</b> of the lowercased filename modulo the size of the <b>hash</b> <b>table,</b> allowing for quick verification of a file's existence within the archive. If multiple files within the archive have the same hash, colliding entries will follow each other in increasing index order (forming a colliding hash cluster). In order to identify the exact entry for the requested file within a colliding hash cluster, each <b>hash</b> <b>table</b> entry stores 2 additional hashes of the lowercased filename, each using the same hashing algorithm but with a different seed value, as well as a locale code and platform code. The end of a colliding hash cluster is detected either by encountering an empty <b>hash</b> <b>table</b> entry or by traversing the entire <b>hash</b> <b>table</b> (including the modulo loopback) back to the initial <b>hash</b> <b>table</b> index.|$|R
5000|$|First {{prepare a}} <b>hash</b> <b>table</b> {{of the smaller}} relation. The <b>hash</b> <b>table</b> entries consist of the join {{attribute}} and its row. Because the <b>hash</b> <b>table</b> is accessed by applying a hash function to the join attribute, {{it will be much}} quicker to find a given join attribute's rows by using this table than by scanning the original relation.|$|R
30|$|Step 4 : Build up <b>HASH</b> <b>table</b> {{according}} to the prefix (the first one to two characters) information about the pattern string of each network data, and take subscript of the first pattern string started with such prefix as the <b>table</b> item in <b>HASH</b> <b>table.</b> While building up <b>HASH</b> <b>table,</b> create a prefix array to store the number of pattern strings having the same prefix.|$|R
25|$|Since DOS 3.3 the {{operating}} system provides means to improve the performance of file operations with FASTOPEN by {{keeping track of the}} position of recently opened files or directories in various forms of lists (MS-DOS/PCDOS) or <b>hash</b> <b>tables</b> (DR-DOS), which can reduce file seek and open times significantly. Before DOS 5.0 special care must be taken when using such mechanisms in conjunction with disk defragmentation software bypassing the file system or disk drivers.|$|E
25|$|The {{best way}} {{to speed up the}} baby-step giant-step {{algorithm}} is to use an efficient table lookup scheme. The best in this case is a hash table. The hashing is done on the second component, and to perform the check in step 1 of the main loop, γ is hashed and the resulting memory address checked. Since <b>hash</b> <b>tables</b> can retrieve and add elements in O(1) time (constant time), this does not slow down the overall baby-step giant-step algorithm.|$|E
25|$|In June 2008, The Pirate Bay {{announced}} that their servers would support SSL encryption {{in response to}} Sweden's new wiretapping law. On 19 January 2009, The Pirate Bay launched IPv6 support for their tracker system, using an IPv6-only version of Opentracker. On 17 November 2009, The Pirate Bay shut off its tracker service permanently, stating that centralised trackers are no longer needed since distributed <b>hash</b> <b>tables</b> (DHT), peer exchange (PEX), and magnet links allow peers to find each other and content in a decentralised way.|$|E
50|$|The {{problem of}} optimal static hashing was first solved in general by Fredman, Komlós and Szémeredi. In their 1984 paper, they detail a two-tiered <b>hash</b> <b>table</b> scheme {{in which each}} bucket of the (first-level) <b>hash</b> <b>table</b> {{corresponds}} to a separate second-level <b>hash</b> <b>table.</b> Keys are <b>hashed</b> twice—the first hash value maps to a certain bucket in the first-level hash table; the second hash value gives the position of that entry in that bucket's second-level <b>hash</b> <b>table.</b> The second-level table is guaranteed to be collision-free (i.e. perfect hashing) upon construction. Consequently, the look-up cost is guaranteed to be O(1) in the worst-case.|$|R
40|$|In {{this thesis}} we {{introduce}} CPHash − a scalable fixed size <b>hash</b> <b>table</b> that supports eviction using an LRU list, and CPServer − a scalable in memory key/value cache server that uses CPHash to implement its <b>hash</b> <b>table.</b> CPHash uses computation migration to avoid transferring data between cores. Experiments on a 48 core ma-chine show that CPHash has 2 to 3 times higher throughput than a <b>hash</b> <b>table</b> implemented using scalable fine-grained locks. CPServer achieves 1. 2 to 1. 7 times higher throughput than a key/value cache server {{that uses a}} <b>hash</b> <b>table</b> with scalable fine-grained locks and 1. 5 to 2. 6 times higher throughput than Memcached...|$|R
50|$|The Content Addressable Network (CAN) is a distributed, {{decentralized}} P2P {{infrastructure that}} provides <b>hash</b> <b>table</b> functionality on an Internet-like scale. CAN {{was one of}} the original four distributed <b>hash</b> <b>table</b> proposals, introduced concurrently with Chord, Pastry, and Tapestry.|$|R
25|$|For a long time, {{number theory}} in general, {{and the study}} of prime numbers in particular, was seen as the {{canonical}} example of pure mathematics, with no applications outside of the self-interest of studying the topic with the exception of use of prime numbered gear teeth to distribute wear evenly. In particular, number theorists such as British mathematician G. H. Hardy prided themselves on doing work that had absolutely no military significance. However, this vision was shattered in the 1970s, when it was publicly announced that prime numbers could be used {{as the basis for the}} creation of public key cryptography algorithms. Prime numbers are also used for <b>hash</b> <b>tables</b> and pseudorandom number generators.|$|E
25|$|Cuckoo hashing, another {{technique}} for implementing <b>hash</b> <b>tables,</b> guarantees constant time per lookup (regardless of the hash function). Insertions into a cuckoo hash table may fail, causing the entire table to be rebuilt, but such failures are sufficiently {{unlikely that the}} expected time per insertion (using either a truly random hash function or a hash function with logarithmic independence) is constant. With tabulation hashing, on the other hand, the best bound known on the failure probability is higher, high enough that insertions cannot be guaranteed to take constant expected time. Nevertheless, tabulation hashing is adequate to ensure the linear-expected-time construction of a cuckoo hash table for a static set of keys that does not change as the table is used.|$|E
500|$|Linear probing [...] is {{a scheme}} in {{computer}} programming for resolving collisions in <b>hash</b> <b>tables,</b> data structures for maintaining {{a collection of}} key–value pairs and looking up the value associated with a given key. It was invented in 1954 by Gene Amdahl, Elaine M. McGraw, and Arthur Samuel and first analyzed in 1963 by Donald Knuth.|$|E
5000|$|Without any pre-processing, a query can be {{answered}} by inserting the elements of Si into a temporary <b>hash</b> <b>table</b> and then checking for each element of Sk whether {{it is in the}} <b>hash</b> <b>table.</b> The query time is [...]|$|R
50|$|Print the <b>hash</b> <b>table.</b>|$|R
5000|$|Some <b>hash</b> <b>table</b> implementations, {{notably in}} {{real-time}} systems, cannot {{pay the price}} of enlarging the <b>hash</b> <b>table</b> all at once, because it may interrupt time-critical operations. If one cannot avoid dynamic resizing, a solution is to perform the resizing gradually: ...|$|R
500|$|Since their work, {{even better}} {{algorithms}} have been developed. For instance, by repeatedly applying the Kirkpatrick–Reisch range reduction technique until the keys are {{small enough to}} apply the Albers–Hagerup packed sorting algorithm, {{it is possible to}} sort in time however, the range reduction part of this algorithm requires either a large memory (proportional to [...] ) or randomization in the form of <b>hash</b> <b>tables.</b>|$|E
500|$|A Van Emde Boas tree {{may be used}} as a {{priority}} queue to sort a set of [...] keys, each in the range from [...] to , in time [...] This is a theoretical improvement over radix sorting when [...] is sufficiently large. However, in order to use a Van Emde Boas tree, one either needs a directly addressable memory of [...] words, or one needs to simulate it using a hash table, reducing the space to linear but making the algorithm be randomized. Another priority queue with similar performance (including the need for randomization in the form of <b>hash</b> <b>tables)</b> is the Y-fast trie of [...]|$|E
500|$|A random {{bipartite graph}} with n {{vertices}} {{on each side}} of its bipartition, and with cn edges chosen independently at random from each of the n2 possible pairs of vertices, is a pseudoforest with high probability whenever c is a constant strictly less than one. This fact {{plays a key role in}} the analysis of cuckoo hashing, a data structure for looking up key-value pairs by looking in one of two <b>hash</b> <b>tables</b> at locations determined from the key: one can form a graph, the [...] "cuckoo graph", whose vertices correspond to hash table locations and whose edges link the two locations at which one of the keys might be found, and the cuckoo hashing algorithm succeeds in finding locations for all of its keys if and only if the cuckoo graph is a pseudoforest.|$|E
50|$|In computing, a <b>hash</b> <b>table</b> (<b>hash</b> map) is a data {{structure}} which implements an associative array abstract data type, {{a structure that}} can map keys to values. A <b>hash</b> <b>table</b> uses a <b>hash</b> function to compute an index into an array of buckets or slots, from which the desired value can be found.|$|R
50|$|Linear hashing is {{a dynamic}} <b>hash</b> <b>table</b> {{algorithm}} invented by Witold Litwin (1980), and later popularized by Paul Larson. Linear hashing allows {{for the expansion of}} the <b>hash</b> <b>table</b> one slot at a time.The frequent single slot expansion can very effectively control the length ofthe collision chain. The cost of <b>hash</b> <b>table</b> expansion is spread out across eachhash table insertion operation, as opposed to being incurred all at once. Linear hashing is therefore well suited for interactive applications.|$|R
50|$|A {{number of}} data {{structures}} {{have been proposed}} to solve anagrams in constant time. Two {{of the most commonly}} used data structures are the Alphabetic map and the Frequency map. The Alphabetic map maintains a <b>hash</b> <b>table</b> of all the possible words that can be in the language (this {{is referred to as the}} lexicon). For a given input sting, sort the letters in alphabetic order. This sorted string maps onto a word in the <b>hash</b> <b>table.</b> Hence finding the anagram requires sorting the letters and a looking up the word in the <b>hash</b> <b>table.</b> The sorting can be done in linear time by the counting sort and <b>hash</b> <b>table</b> look up can be done in constant time.|$|R
