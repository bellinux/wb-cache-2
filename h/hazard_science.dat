4|40|Public
50|$|A 2011 {{study of}} fire {{toxicity}} of insulating materials at the University of Central Lancashire's Centre for Fire and <b>Hazard</b> <b>Science</b> studied PIR and other commonly used materials under more realistic and wide-ranging conditions {{representative of a}} wider range of fire hazard, observing that most fire deaths resulted from toxic product inhalation. The study evaluated the degree to which toxic products were released, looking at toxicity, time-release profiles, and lethality of doses released, in a range of flaming, non-flaming, and poorly ventilated fires, and concluded that PIR generally released a considerably higher level of toxic products than the other insulating materials studied (PIR > PUR > EPS > PHF; glass and stone wools also studied). In particular, hydrogen cyanide is recognised as a significant contributor to the fire toxicity of PIR (and PUR) foams.|$|E
40|$|In {{the risk}} classification, road {{transport}} of hazardous materials {{is one of}} the major technological risks. In fact, their transport exposes the population to pollution, explosion and fire which could lead to wildlife catastrophic impacts. The aim of the current study is to identify the least risky routes in Oran urban areas, and define first a mapping of dangerous roads, based on a decision support system. Also, using complementary methods issued from systemic approach of <b>hazard</b> <b>science,</b> we analyse the security degree of these roads. geographic information systems; GIS; decision support systems; DSS: application: itinerary modification; road transport; hazardous materials; scenario; systemic approach; risk management; risk assessment; Algeria; routing; route modification; security...|$|E
40|$|This is a {{draft of}} a chapter that has been {{accepted}} for publication by Oxford Oxford Research Encyclopedia of Natural <b>Hazard</b> <b>Science</b> by/edited by Susan Cutter published in 2017. Flood losses in the United States have increased dramatically {{over the course of}} the past century, averaging US$ 7. 96 billion in damages per year for the 30 -year period ranging from 1985 to 2014. In terms of human fatalities, floods are the second largest weather-related hazard in the United States, causing an average of 82 deaths per year between 1986 and 2015. Given the wide-reaching impacts of flooding across the United States, the evaluation of flood-generating mechanisms and of the drivers of changing flood hazard are two areas of active research. Flood events can be driven by a variety of physical mechanisms, including rain and snowmelt, frontal systems, monsoons, intense tropical cyclones, and more generic cyclonic storms. However, flood frequency analysis has traditionally been based on statistical analyses of the observed flood distributions that rarely distinguish among these physical flood-generating processes. In reality, flood frequency distributions are often characterized by ‘mixed populations’ arising from multiple flood-generating mechanisms, which can be challenging to disentangle. Temporal changes in the frequency and magnitude of flooding have also been the subject of a large body of work in recent decades. The science has moved from a focus on the detection of trends and shifts in flood peak distributions towards the attribution of these changes, with particular emphasis on climatic and anthropogenic factors, including urbanisation and changes in agricultural practices. A better understanding of these temporal changes in flood peak distributions, as well as of the physical flood-generating mechanisms, will enable us to move forward with the estimation of future flood design values in the context of both climatic and anthropogenic change...|$|E
50|$|FINKEL, M. P. (1958). Radiation <b>Hazards.</b> <b>Science.</b> 128(3338): 1580-1585.|$|R
2500|$|On {{occasion}} the AGU Council issues position {{statements on}} matters affecting public policy {{that are related}} to geophysics. These include biological evolution, natural <b>hazards,</b> <b>science</b> education and funding, and climate change. The AGU adopted its first position statement on climate change in December 1998. That statement began ...|$|R
5000|$|Independent Response of Complex Urban Infrastructures Subjected to Multiple <b>Hazards,</b> National <b>Science</b> Foundation, October 2007 - October 2010 ...|$|R
40|$|Interest in {{the toxic}} effects of fires has been rapidly increasingly. This book {{will raise the}} {{important}} issues {{of the types of}} toxic effluents that fires produce and the different methods that exist to assess fire toxicity, using animal exposure studies, laboratory scale or large scale generation of fire effluents, as well as the effects from actual fires. Further chapters will discuss the effects of toxicity on its victims, international standards and how different materials and the fire environment can influence the generation of toxic products. Fire toxicity will be relevant for professionals in the fire community, including fire fighters, fire investigators, regulators, fire safety engineers and fire-safe material formulators. It will also be suitable for researchers in industry or academia. Key Features: investigates the controversial subject of toxic effluents as the cause of the majority of fire deaths and injuries; describes the different types of toxic effluents and the specific fires that they produce, their physiological effects and methods for generation; provides an overview of national and international fire safety regulations including current and proposed regulations such as a standardized framework for prediction of fire gas toxicity. Contents: Part 1 Introduction: Introduction to fire toxicity; Fire Scenarios and combustion conditions. Part 2 Harmful effects of fire effluents: Hazards from smoke and irritants; Asphyxiant components of fire effluents; Effects of fire effluents on fire victims. Part 3 Biological assessment of fire toxicity: Experimental methods in combustion toxicology; Animal exposure studies; Application of human animal exposure studies to human fire safety; In vitro biological toxicity assessments for fire combustion products; A Combined fire smoke and lung model test equipment. Part 4 Toxicity assessment using chemical analysis: Sampling and measurement of toxic fire effluents; Bench-scale generation of fire effluents; Large scale generation and characterisation of fire effluents; Effects of the material and fire conditions on toxic product yields; Estimation of toxicity during burning of common materials. Part 5 National and international fire safety regulations: Prescriptive regulations and tests considering the toxicity of fire effluents; An international standardised framework for prediction of fire gas toxicity. Part 6 Numerical simulation of fires and their hazards: Computer simulation of fire hazards and evacuation; Toxic hazard calculation models for use with fire effluent data; Modelling fire growth and toxic gas formation. About the Author Dr. Stec and Prof. Hull have worked together on fire toxicity for the last 6 years, currently in the Centre for Fire and <b>Hazard</b> <b>Science</b> at University of Central Lancashire, UK. Anna Stec is a lecturer in Fire Chemistry and Toxicology. Her work has focused on quantification of toxic hazards in fires, understanding the factors that affect fire gas toxicity, and the relationship between the physiological effects of the concentration and dose of different toxicants. She has conducted detailed comparisons on the yields of fire gases and other combustion products using a selection of different small-scale fire models in order to predict the effects of fire exposures to humans. Dr Stec has been designated as the UK s Principal Expert on fire chemistry, and an active participant at the ISO TC 92 Fire threat to people and environment meetings. Richard Hull is a Professor of Chemistry and Fire Science. His work has focused on fire retardant mechanisms, fire effluent toxicity and fire science. With another contributor, Prof David Purser, he has been pivotal {{in the development of the}} steady state tube furnace as the first internationally recognised standard for the assessment of fire gas toxicity (ISO TS 19700); as one of the only methods capable of replicating real fire conditions on a bench-scale, this has led to its growing acceptance, particularly for performance-based fire assessment...|$|E
5000|$|... Establishment and {{application}} of geotechnical earth <b>science</b> <b>hazard</b> database.|$|R
40|$|Acknowledgments The {{workshop}} was {{an activity}} of COST Action TD 1209 : ALIEN Challenge. COST (European Cooperation in Science and Technology) is a pan-European intergovernmental framework. The mission of COST is to enable {{scientific and technological}} developments leading to new concepts and products and thereby contribute to strengthening Europe's research and innovation capacities. Additional support for HR, HH, and BVP was received from the NERC Centre for Ecology & Hydrology National Capability allocation (Project NEC 05100 HARM: Frameworks for Horizon-scanning And Risk Mitigation of pathogens and invasive alien species (IAS) in changing UK and European environments) under the Natural <b>Hazards</b> <b>Science</b> Area. JP was partly supported by long-term research development project RVO 67985939 (The Czech Academy of Sciences). Peer reviewedPublisher PD...|$|R
5000|$|Prof Willy Aspinall CMG, Cabot Professor in Natural <b>Hazards</b> and Risk <b>Science</b> since 2009 at the University of Bristol ...|$|R
2500|$|For humankind, {{the factor}} of {{technology}} is a distinguishing and critical consideration, both as an enabler and an additional source of byproducts. Short of survival, human concerns include the range from quality of life to health <b>hazards.</b> Since <b>science</b> holds experimental demonstration to be definitive, modern treatment of toxicity or environmental harm involves defining a level at which an effect is observable. Common examples of fields where practical measurement is crucial include automobile emissions control, industrial exposure (e.g. Occupational Safety and Health Administration (OSHA) PELs), toxicology (e.g. [...] ), and medicine (e.g. medication and radiation doses).|$|R
40|$|Uncertainties are {{pervasive}} in natural hazards, {{and it is}} crucial to develop robust and meaningful approaches to characterize and communicate uncertainties to inform modeling efforts. In this monograph we provide a broad, cross-disciplinary overview of issues relating to uncertainties faced in natural hazard and risk assessment. We introduce some basic tenets of uncertainty analysis, discuss issues related to communication and decision support, and offer numerous examples of analyses and modeling approaches that vary by context and scope. Contributors include scientists from across the full breath of the natural hazard scientific community, from those in real-time analysis of natural hazards to those in the research community from academia and government. Key themes and highlights include: Substantial breadth and depth of analysis in terms of the types of natural hazards addressed, the disciplinary perspectives represented, and the number of studies included. Targeted, application-centered analyses with a focus on development and use of modeling techniques to address various sources of uncertainty. Emphasis on the impacts of climate change on natural hazard processes and outcomes. Recommendations for cross-disciplinary and science transfer across natural <b>hazard</b> <b>sciences.</b> This volume will be an excellent resource for those interested in the current work on uncertainty classification/quantification and will document common and emergent research themes to allow all to learn from each other and build a more connected but still diverse and ever growing community of scientists...|$|R
40|$|This module is {{designed}} to help emergency managers prepare their communities for tsunamis. Lessons include basic tsunami <b>science,</b> <b>hazards</b> produced by tsunamis, the tsunami warning system, the importance of public education activities, and how to craft good emergency messages and develop tsunami response plans. The module also contains links to extensive Reference and Resources sections. Educational levels: Middle school, High school, General public...|$|R
40|$|Earthquake {{prediction}} {{efforts in}} the United States are heavily concentrated in California. Near the community of Parkfield, for example, scientists from the United States Geological Survey (USGS) —the federal agency with primary responsibility for earthquake prediction research in this country—and other agencies are conducting studies that aim to improve earthquake prediction and recent earthquake experiences, California residents are generally aware that an earthquake <b>hazard</b> exists. National <b>Science</b> Foundatio...|$|R
40|$|This Special Issue of Nanomaterials {{examines}} {{the potential for}} engineered nanomaterials to negatively impact biological systems and highlights some advances in evaluating key areas of their <b>hazard</b> potential. Nanomaterial <b>science</b> is evolving rapidly with the generation of more complex nanostructures with exciting potential applications. Keeping modern toxicology abreast of this innovation {{to the point that}} it guides a safer nanotechnology presents an equally exciting and eminently worthwhile challenge. [ [...] . ...|$|R
40|$|The {{discipline}} of emergency management has been evolving {{in scope and}} priority at an accelerating rate over the past decade. But the educational opportunities and focus areas have not necessarily kept pace with this change. While the volume of higher education curriculum has increased, three key thematic areas must be addressed as the baseline of knowledge for emergency management professionals: 1) <b>hazard</b> and threat <b>science,</b> 2) sociological and psychological considerations, and 3) prevention/mitigation principles...|$|R
40|$|Safety hazards {{that are}} {{frequently}} found in science classrooms are {{addressed in this}} digest which updates and supplements the 1980 ERIC/SMEAC information bulletin "Safety in the Science Classroom. " Information obtained from journal articles and safety guides {{is presented in the}} categories oft (1) risks involved in <b>science</b> activities; (2) <b>hazards</b> in chemistry classrooms; (3) hazards in physics classrooms; (4) hazards in biology classrooms; (5) fire; and (6) resources for teachers. Various publications related to safety and safety <b>hazards</b> in <b>science</b> that are available in the ERIC database are identified and described. Guide questions that assist in an assessment of a teacher's familiarity with safety in the science classroom are also provided. A list of 20 references concludes the digest. (ML) i Reproductions supplied by EDRS are the best that can be made * * from the original document. ...|$|R
40|$|Better {{professional}} {{practices and}} academic research support hands-on, process and inquiry-based laboratory and field investigations {{as well as}} hands-on activities to promote deep conceptual understanding of science by students. To ensure a safer and effective science teaching/learning environment, the following recommendations are derived from recognized reliable sources, legal safety standards, and best professional safety practices. The recommendations represent the best professional standards and practices on safety {{as it relates to}} overcrowding. However, it cannot be assumed that all <b>hazards</b> in <b>science</b> classrooms are ameliorated by simply reducing overcrowding. Other factors affecting safety, may include facilities design, engineering controls, appropriate personal protective equipment, standard operating procedures, and/or safety training of students and teachers. These additional factors, which can be linked with science accidents, must also be attended to as well as meeting the requirements of any legal safety code or regulation or law of any state, municipality or other jurisdiction...|$|R
40|$|Measuring {{resilience}} {{to natural}} hazards {{is a central}} issue in the <b>hazard</b> mitigation <b>sciences.</b> This paper applied a confirmatory factor methodology to operationalize the biophysical, built-environment, and socioeconomic resilience dimensions for local jurisdictions in large urban metropolitan areas in South Korea. Mapping the factor scores of the dimensions revealed great spatial variations. The factor covariances showed a trade-off relationship between natural infrastructure and human activities. A hierarchical cluster {{analysis was used to}} classify the localities into heterogeneous groups with respect to the identified resilience dimensions. Densely developed and affluent urban areas tend to lack biophysical resilience. Some local governments, sorted into the same groups, turn out to be located in different metropolitan areas. The spatial variation and inequality in the resilience dimensions suggest the necessity of integrated and flexible governance for sustainable hazard mitigation. Korea (South). Ministry of Education (National Research Foundation of Korea. Basic Science Research Program NRF- 2013 R 1 A 1 A 2013676...|$|R
40|$|This thesis {{presents}} a technique for performing vulnerability assessments, using measures of exposure, sensitivity, and adaptive capacity. Historically, vulnerability assessments {{have focused on}} analyzing the hazard itself, absent information on its causes and mitigations. The Vulnerability Assessment Method (VAM), presented herein, acquires data and heuristics from affected stakeholders to assess not only the hazard, but also the causes of vulnerability, potential for adaptation, previous impacts, and ways to mitigate future impacts. We apply the VAM to {{a case study of}} Washington State, assessing drought vulnerability across 34 sub-sectors. Results indicate highest vulnerability for dryland farmers, farmers with junior water rights, select fisheries, ski area operators, and the green industry. Through validation exercises, we demonstrate the VAM's internal consistency and broader applicability. Contributions of the VAM include its incorporation of stakeholder data, quantitative assessments of underlying components, and applicability to other areas and types of <b>hazards.</b> National <b>Science</b> Foundation; Washington Department of Ecology; Washington Department of Community, Trade, and Economic Developmen...|$|R
40|$|The {{events of}} September 11 th shocked {{the nation and}} painfully {{illustrated}} our vulnerability to international terrorist attacks. Despite {{some of the most}} sophisticated models, monitoring systems, and science in the world, officials were unable to anticipate and predict these cascading events. The collective scientific ability to geographically represent environmental threats,map exposures, andmap consequences is relatively straightforward when the threats are recognized. But what happens when we cannot recognize threats or some of their unintended consequences? This article examines the twin issues of the inadequacies in our currentmodes of understanding (the vulnerability of science) and the need for more integrative approaches in understanding and responding to environmental <b>hazards</b> (vulnerability <b>science).</b> Key Words: geographical understanding, hazards, September 11 th, vulnerability. The organized complexity of modern existence is a new phenomenon in [hu]man’s experience. Considering {{what has happened in the}} United States during the last century, one is tempted to ask whether we are living in a moment of great progress or of great aberration in the human adventure...|$|R
40|$|The {{objective}} {{of this study was}} to look at ways of strengthening future planning and decision-making to take better account of <b>hazard</b> information and <b>science</b> knowledge. The intention was to produce an agenda for encouraging improvements to planning and decision-making processes. This report outlines the main findings of the first phase of an assessment of how well natural hazard knowledge is being developed and utilised in an evolving climate of community and government expectations. The report does not provide an exhaustive assessment of all aspects of the topic, rather it brings forward the key factors and considerations believed to be important to good decision-making...|$|R
40|$|During {{the past}} several decades, the public has given {{increasing}} attention to environmental problems and increased their demands that these problems be solved. During this period, the difficulty and costs of solving the problems have become apparent. Environmental abatement programs must be effective in achieving the desired goals and efficient in achieving them at low cost. Risk analysis is necessary to quantify the benefits of proposed solutions {{in order to make}} them effective and efficient. The necessity for performing risk assessment stems from a presidential executive order, from a Supreme Court Decision, and from the public 2 ̆ 7 s demand for information about the extent of possible danger from a hazard, rather than knowing simply that it is a <b>hazard.</b> The <b>science</b> of risk analysis is in an early stage and so there are many uncertainties concerning interpretation of the estimates. This approach has sharpened the scientific questions and hastened improvements in scientific understanding. Risk analysis is most helpful when the analysis reflects the science, without intrusion of the risk assessor 2 ̆ 7 s values or attempts to force a risk management outcome by skewing the risk analysis...|$|R
30|$|Finally, it {{is argued}} that {{participatory}} approaches are great opportunities for knowledge exchange between vulnerable groups and the scientific community (Kelman and Glantz 2014). Communities around the world use various sources of information to develop their own warning systems (Maguire and Hagan 2007) and often have an extensive knowledge of their own vulnerabilities and personal, often long-term, experience in managing risks (Tozier de la Poterie and Baudoin 2015). Local knowledge provides a wealth of locally relevant indicators for natural <b>hazards.</b> Coupled with <b>science</b> and technology, this knowledge can contribute to augmenting and confirming risk prediction (Hall 2007; Kelman and Glantz 2014), as well as to building trust among local communities, scientists, and practitioners through the way local knowledge is valued (Archer 2003; Baudoin et al. 2014).|$|R
40|$|International audienceThe Educational & Outreach Group (E&O Group) of the Istituto Nazionale di Geofisica e Vulcanologia (INGV) {{designed}} a portable museum {{to bring on}} the road educational activities focused on seismology, seismic <b>hazard</b> and Earth <b>science.</b> This project was developed for {{the first edition of}} the Science Festival organized in Genoa, Italy, in 2003. The museum has been mainly focused to school students of all ages and explains the main topics of geophysics through posters, movie and slide presentations, and exciting interactive experiments. This new INGV museum has been remarkably successful, being visited by more than 8000 children and adults during the 10 days of the Science Festival. It is now installed at the INGV headquarters in Rome and represents the main attraction during the visits of the schools all year round...|$|R
40|$|This work {{describes}} a methodology to assess ¿risk to disaster¿ due to natural hazards, particularly in data poor communities. It {{is to be}} used by (1) international organizations and donors to size development programs aiming to reduce risk to disasters and (2) by local authorities as a disaster management tool for implementing risk reduction, mitigation and preparedness programs. The methodology provides the guidelines to assemble a disaster risk information system that incorporates knowledge on natural <b>hazards,</b> construction <b>science</b> and disaster dynamics and is aimed for use by decision makers with the support of technical staff. The methodology is based on Geographical Information System (GIS) technology {{for the development of a}} database of disaster related information including built-up infrastructure, population, vulnerability and the occurrence of natural hazards. It integrates Earth Observation (EO) and information collected in situ for generating essential information such as building stock and indirectly population distribution in hazard affected areas. The database can also be used for generating damage assessment in the immediate aftermath of a disaster based on information on the hazard location and its intensity. Damage information can in turn improve the information content of the database to support more accurate risk assessments in the future. The information layers could then become important information that supports the development and urban planning projects. JRC. G. 2 -Global security and crisis managemen...|$|R
40|$|In view of {{the rapid}} {{development}} of economy and technology in Taiwan, perilous meteorological and geological conditions often cause devastating disasters and result in severe loss of human lives and properties. To promote seismic hazard mitigation in an integrated and systematic approach, the Haz-Taiwan program started in 1998. It is known as analysis framework and application software for seismic hazard analysis, risk assessment and socioeconomic loss estimation. The objectives of Haz-Taiwan program are not only to simulate the seismic hazard scenarios, but also to provide a decision support system for government agencies after the occurrence of natural hazards. It emphasizes on the implementation of research accomplishment in the National Disaster Management System. This paper briefly describes the framework and current progresses in Haz-Taiwan program, {{especially those in the}} analysis of potential earth <b>science</b> <b>hazards</b> (PESH) and risk analysis of civil infrastructures. 1...|$|R
40|$|We propose an international, {{collaborative}} surface-ship-based {{effort to}} survey the world 2 ̆ 7 s ocean floors in all the areas not yet thoroughly surveyed by swath bathymetric sonar, sidescan sonar and routine geophysics (magnetics, gravity, subbottom profiler) to precise standards of data quality and navigation accuracy. These new bathymetric and associated measurements will gradually replace the existing non-uniform coverage of surface vessel track and area data, currently combined with uniform but low resolution altimeter-derived depths. The objective is to produce a world ocean database at least {{an order of magnitude}} better in resolution than what exists over much of the world ocean. Currently, the only established international structure with a worldwide mandate for compiling bathymetric data is GEBCO (General Bathymetric Chart of the Oceans, jointly sponsored by the Intergovernmental Oceanographic Commission of UNESCO and the International Hydrographic Organization). GEBCO continually updates its world ocean maps and grids of depths by incorporating regional maps produced in the IBC (International Bathymetric Chart), localized maps resulting from research projects, data assembled by scientific data groups such as LDEO 2 ̆ 7 s Marine Geoscience Data System, track data submitted to world and national data centers, and inshore data collected by Hydrographic Offices for navigational purposes. Further multibeam data is expected to come from Coastal States who have undertaken mapping of their juridical Continental Shelf to satisfy the requirements of UNCLOS Article 76. This acoustic data is combined with satellite altimetry data to produce maps that have a horizontal resolution that is in places hundreds of meters but over extensive areas no better than approximately 28 km. In other words, the fabric of the world ocean floor is known and portrayed to highly variable resolution. This means that while the first-order plate tectonic/volcanic seafloor fabric (fracture zones, rift valleys, large seamounts and igneous plateaus) is captured for the entire ocean, second order features like abyssal hills, fault-controlled escarpments, lava flows, propagating rift tips, volcanic rift zones, deep-sea channels, mud volcanoes, emergent diapirs, landslide scars and debris flows, pockmarks, and iceberg plowmarks are know only in a few limited areas. Although vigorous efforts have produced maps of high resolution in some areas, there is at present no coherent plan to collect data that would bring the rest of the ocean up to the level of knowledge that is achievable. We solicit interest in organizing an international workshop that would produce a long-term plan to measure depths and associated data in a systematic fashion in the vast oceanic regions sparsely surveyed by surface ships. To meet this ambitious objective, this international project will need to be carefully designed to optimize return of information balanced against economical use of ship time. Societal benefits will include improved assessment of seafloor/subbottom geological resources, geohazard risk (earthquakes, tsunamis, volcanism and submarine landslides), fin and shellfish habitats, submarine cable routing, and navigational <b>hazards.</b> <b>Science</b> will gain understanding of the complex processes that shape and modify the ocean floor and subbottom, including the influence of bottom topography on circulation and the seafloor 2 ̆ 7 s role as benthic biosphere host. Exciting discoveries will attract follow-up academic studies, e. g. with AUVs, sampling, and sea-floor observatories...|$|R
40|$|In this {{research}} we sought {{answers to the}} question: What are the social characteristics and conditions of human communities that promote adaptive capacity for wildfire? The Quadrennial Fire Review (USDA and USDI 2009) promotes a goal of “achieving fire-adapted communities” in the wildland urban interface (WUI), and identifies metrics for determining whether a community is fire-adapted. While these metrics {{address some of the}} biophysical conditions necessary for fire-adapted human communities, they offer little insight into the social elements that promote or sustain adaptive capacity. Adaptive capacity refers to the individual and collective resources, capabilities, and actions that alleviate the risk or impacts of disturbances such as wildland fire, and support individual and community adaptive behaviors in response to changing conditions (Adger and Vincent 2005). More succinctly, adaptive capacity is a community’s ability to mobilize resources with a goal of adapting to change driven by events such as wildland fire (Nelson et al. 2007). In this project we improved our understanding of how the notion of adaptive capacity can be fruitfully applied to the problem of at-risk WUI communities. We sought advice from emergency managers, local stakeholders, and our colleagues working in the natural resources and <b>hazards</b> social <b>sciences.</b> We found that adaptive capacity is composed of a set of overt and latent characteristics that are mobilized by catalysts to adapt to disturbances, including wildland fire. We developed a model that begins to identify the social characteristics of adaptive capacity for wildfire. Finally, we suggest that {{more research is needed to}} (1) define social elements that are consistent across locales and disturbances, (2) understand how structure impedes or facilitates adaptive capacity, (3) integrate social characteristics of adaptive capacity into tools to assess the impacts of wildland fire, and (4) identify catalysts of adaptive capacity and the potential roles of different actors in adapting to living with wildland fire...|$|R
40|$|This {{dissertation}} {{consists of}} three essays on urban structure, housing, and environment. The first paper contributes to the existing debate on the co-location hypothesis by devising a proximity measure and controlling {{for a set of}} other urban form measures. Multiple regression analysis revealed that job-worker proximity leads to shorter commuting time. In addition, results from subareas suggested that the impact of job-worker imbalance and the impact of job-worker mismatch on the commuting time are both greater in the suburb in comparison with the city center. The second paper examines the impact of the LIHTC construction on nearby housing prices in the Boston metropolitan area by using the AITS-DID method. The paper found that the price gap between the LIHTC micro-neighborhood and the area beyond is reduced by approximately 16. 5 percent points after the LIHTC construction. The segmentation of the analysis by sub-region showed spatially heterogeneous results. The findings from this research are contrary to the conventional perception that subsidized housing developments lead to neighborhood decline persistently. Measuring resilience to natural hazards is a central issue in the <b>hazard</b> mitigation <b>sciences.</b> The third paper applied a confirmatory factor methodology to operationalize the biophysical, built environment, and socioeconomic resilience dimensions for local jurisdictions in large urban metropolitan areas in South Korea. The factor covariances showed a trade-off relationship between natural infrastructure and human activities. Densely developed and affluent urban areas tend to lack biophysical resilience. Some local governments, sorted into the same groups, turn out to be located in different metropolitan areas. The spatial variation and inequality in the resilience dimensions suggest the necessity of integrated and flexible governance for sustainable hazard mitigation. by Chun Il Kim. Thesis: Ph. D. in Urban and Regional Planning, Massachusetts Institute of Technology, Department of Urban Studies and Planning, 2017. Cataloged from PDF version of thesis. Includes bibliographical references...|$|R
40|$|The {{effects of}} blowing dust on {{transport}} operations are often mentioned {{as one of}} the significant impacts of aeolian processes on human welfare. However, few studies have been presented to demonstrate this impact. This research examined official air traffic incident reports in Australia for inclusively 1969 - 2010 to characterise the hazard of blowing dust to aviation in the country, the first such study of its kind. For the 42 year record, 61 incidents were identified (mean 1. 4 per annum), with the large majority occurring {{in the first half of}} the 1970 s. Only 20 % of incidents occurred from 1984 onwards. Australian dust activity has not decreased over time, and the reduction in incidents is partly explained by improvements in aviation technology. The centralisation of Air Traffic Control operations to major coastal cities may however have reduced pilot reporting of dust-induced aviation incidents. By type of dust activity, dust storms were associated with nearly half of the reported incidents and dust hazes produced around a quarter. Only 5 % of incidents resulted in any physical damage to aircraft and only one case involving personal injury was reported. The majority of the adverse effects on aviation due to dust (nearly 60 % of reported incidents) were related to difficulties for navigation and completion of scheduled journey. Since aircraft damage and bodily harm were rare, the impact of dust in Australia is mostly that of inconvenience and associated raised economic costs. From 1990, the temporal pattern of incidents does not show any significant increase despite several intensely dusty years associated with recent droughts. This suggests that Australian aviation safety may be relatively resistant to the adverse effects of atmospheric dust as a <b>hazard.</b> Griffith <b>Sciences,</b> Griffith School of EnvironmentNo Full Tex...|$|R
40|$|International audienceThis paper {{examines}} the philosophy {{and evolution of}} volcanological science in recent years, particularly {{in relation to the}} growth of volcanic <b>hazard</b> and risk <b>science.</b> It uses the lens of Science and Technology Studies to examine the ways in which knowledge generation is controlled and directed by social forces, particularly during eruptions, which constitute landmarks in the development of new technologies and models. It also presents data from a survey of volcanologists carried out during late 2008 and early 2009. These data concern the felt purpose of the science according to the volcanologists who participated and their impressions of the most important eruptions in historical time. It demonstrates that volcanologists are motivated both by the academic science environment and by a social concern for managing the impact of volcanic hazards on populations. Also discussed are the eruptions that have most influenced the discipline and the role of scientists in policymaking on active volcanoes. Expertise in volcanology can become the primary driver of public policy very suddenly when a volcano erupts, placing immense pressure on volcanologists. In response, the epistemological foundations of volcanology are on the move, with an increasing volume of research into risk assessment and management. This requires new, integrated methodologies for knowledge collection that transcend scientific disciplinary boundaries...|$|R
40|$|Operating robotic space {{missions}} via time-based command sequences {{has become}} a limiting factor in the exploration, defense, and commercial sectors. Command sequencing was originally designed for comparatively simple and predictable missions, with safe-mode responses for most faults. This approach has been increasingly strained to accommodate today’s more complex missions, which require advanced capabilities like autonomous fault diagnosis and response, vehicle mobility with <b>hazard</b> avoidance, opportunistic <b>science</b> observations, etc. Goal-based operation changes the fundamental basis of operations from imperative command sequences to declarative specifications of operational intent, termed goals. Execution based on explicit intent simplifies operator workload by focusing {{on what to do}} rather than how to do it. The move toward goal-based operations, which has already begun in some space missions, involves changes and opportunities in several places: operational processes and tools, human interface design, planning and scheduling, control architecture, fault protection, and verification and validation. Further, the need for future interoperation among multiple goal-based systems suggests that attention be given to areas for standardization. This overview paper defines the concept of goal-based operations, reviews a history of steps in this direction, and discusses the areas of change and opportunity through comparison with the prevalent operational paradigm of command sequencing. I...|$|R
40|$|There is {{increasing}} use of virtual environments to provide scenario-based training in hazardous environments, where real-world training would be difficult, dangerous or have ethical implications. One specific example is fire evacuation training {{and the use of}} virtual fire drills. Virtual environments can be used to present simulated buildings and fire hazards where evacuation drills can be safely practised. However, realism in the virtual environment, e. g. visual and sound effects, and the interactions with simulated entities, e. g. fire, smoke and other building evacuees, can impact the training experience. Also building such complex environments can be time consuming and error prone. One solution is the reuse of components that make up the virtual environment, in this case, the building layouts, the interactive environment itself, and the realistic simulation of fire <b>hazards</b> and fire <b>science</b> impact on human behaviour. This paper describes work to extend a virtual environment development pipeline for building virtual fire evacuation training systems. The pipeline integrates 3 D building models and fire egress behaviour from fire evacuation simulations into a game engine. The behaviour of autonomous agents representing human evacuees, extracted from the fire simulations, is validated against the target virtual environment. Consistent agent behaviour was found across egress time, agent population scalability, the addition of fire hazards and in extended building complexity...|$|R
40|$|This thesis {{describes}} {{the development and}} testing of a methodology to use information collected by volunteers and technicians {{on the status of}} hydraulic structures, applicable for day-to-day risk management. The study was performed in the northeastern Italian Alps of the Friuli Venezia Giulia region. A decision support method was developed and implemented by means of a prototype Web-GIS application. The methodology ensures that technicians in charge can systematically evaluate inspection reports coming from either volunteers or technicians. As such, the study deals with the fields of citizen science and risk management of hydro-meteorological <b>hazards.</b> Concerning citizen <b>science,</b> the thesis studied the quality of visual inspections by volunteers of hydraulic structures that protect society against debris flows. Debris flows are flows of water, sediment and debris down a stream channel that can occur suddenly. Concerning risk management, this thesis looked at collaboration between managing organizations and local volunteers in collecting data on the status of protecting structures such as check-dams. In the Friuli Venezia Giulia region, protection against debris flows is particularly complex due to the abundance and variety of protecting structures. Furthermore, the budget for risk management in general is decreasing while the need is only increasing, due to environmental change. To facilitate collaboration, trade-offs between the motivations of volunteers, scientific goals and management goals will have to be made. It is recommended to team up volunteers with technicians in an extensive training period. In this way, volunteers can become capable inspectors of hydraulic structure...|$|R
30|$|Since 1900, around 90, 000 {{people have}} {{lost their lives in}} 76 {{earthquakes}} in Turkey, with a total affected population of around 7 million and direct losses of around 25 billion USD. Based on a time-dependent model that includes coseismic and post-seismic effects of the 1999 Kocaeli earthquake with moment magnitude Mw[*]=[*] 7.4, Parsons (J Geophys Res. 109, 2004) concluded that the probability of an earthquake with Mw[*]>[*] 7 in the Sea of Marmara near Istanbul is 35 to 70  % in the next 30  years. According to a 2011 study, an earthquake with Mw[*]=[*] 7.25 on the Main Marmara Fault is expected to heavily damage or destroy 2 to 4  % of around 1, 000, 000 buildings in Istanbul with a population around 13 million, with 9 to 15  % of the buildings receiving medium damage and 20 to 34  % of the buildings damaged lightly (Erdik, Science 341 : 72, 2013). In the absence of adequate post-earthquake assembly areas especially in the heavily urbanized Istanbul, it is evident that after a major earthquake, especially in the coastal parts of the city, citizens would be storming to landfill assembly and recreational areas. Besides earthquakes, around 30 tsunamis have been reported by Altınok et al. (Natural <b>Hazards</b> Earth System <b>Science</b> 11 : 273 – 293, 2011) in the Marmara Sea. Among those, catastrophic earthquakes such as 1509, 1766, and 1894 resulted in considerable tsunamis and some damage. The latest tsunami observed in Marmara was due to a triggered submarine landslide of the 1999 Mw[*]=[*] 7.4 Kocaeli earthquake which led to reported run-up heights of 1 – 3  m in most places (Tinti et al., Marine Geology 225 : 311 – 330, 2006). In this study, I propose a design for a tsunami warning system specific for the Marmara region that is strongly coupled with the earthquake early warning system (due to the short arrival times of tsunami) and stakeholders of the tsunami mitigation activities, such as local and regional components of disaster and emergency management and civil protection units, to ensure that the citizens would remain away from the coastline in case of a large earthquake, while discussing associated challenges such as decoupled earthquake and tsunami mitigation activities in the Marmara region.|$|R
