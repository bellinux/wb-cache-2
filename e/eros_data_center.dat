65|10000|Public
5000|$|Another {{pioneering}} aspect {{was using}} the Sun GRID Engine to perform CPU intensive operations, particularly color balancing and JPEG2000 compression, on its server farm. This was {{in keeping with the}} design philosophy of establish a [...] "commercial DAAC" [...] , {{on a par with the}} USGS <b>EROS</b> <b>data</b> <b>center,</b> that could perform distributed batch processing across federated supercomputing sites using the Globus framework.|$|E
50|$|After his NASA career, Gibson held {{marketing}} and program management positions with Booz Allen Hamilton and TRW {{in the areas}} of space and energy development. He was the President of the Oregon Museum of Science and Industry and his own consulting company, Gibson International. Currently he is Senior vice president, Science Applications International Corporation, and manager of its <b>EROS</b> <b>Data</b> <b>Center</b> Operation in Sioux Falls, South Dakota.|$|E
50|$|As a Senator, Mundt {{served on}} the Senate's Appropriations Committee, Foreign Relations Committee, Government Operations Committee, and Permanent Investigations Subcommittee, and he {{represented}} the Senate on the Intergovernmental Relations Advisory Commission. In 1954, he chaired the Senate Subcommittee on Investigations for the Army-McCarthy Hearings. His accomplishments as a Senator included obtaining support for Missouri River projects, establishment of the <b>EROS</b> <b>Data</b> <b>Center</b> in Sioux Falls, South Dakota, agriculture programs, and Interstate highway construction in South Dakota.|$|E
50|$|<b>Data</b> <b>center</b> {{personnel}} - All <b>data</b> <b>center</b> personnel {{should be}} authorized {{to access the}} <b>data</b> <b>center</b> (key cards, login ID’s, secure passwords, etc.). <b>Data</b> <b>center</b> employees are adequately educated about <b>data</b> <b>center</b> equipment and properly perform their jobs. Vendor service personnel are supervised when doing work on <b>data</b> <b>center</b> equipment. The auditor should observe and interview <b>data</b> <b>center</b> employees to satisfy their objectives.|$|R
40|$|Abstract. Cloud {{computing}} <b>data</b> <b>centers</b> can {{be called}} cloud computing centers. It has put forward newer and higher demands for <b>data</b> <b>centers</b> {{with the development of}} cloud computing technologies. This paper will discuss what are cloud computing <b>data</b> <b>centers,</b> cloud computing <b>data</b> <b>center</b> construction, cloud computing <b>data</b> <b>center</b> architecture, cloud computing <b>data</b> <b>center</b> management and maintenance, and the relationship between cloud computing <b>data</b> <b>centers</b> and clouds...|$|R
50|$|Modular <b>data</b> <b>centers</b> {{typically}} come in {{two types}} of form factors. The more common type, referred to as containerized <b>data</b> <b>centers</b> or portable modular <b>data</b> <b>centers,</b> fits <b>data</b> <b>center</b> equipment (servers, storage and networking equipment) into a standard shipping container, which is then transported to a desired location. Containerized <b>data</b> <b>centers</b> typically come outfitted with their own cooling systems. Cisco makes {{an example of this}} type of <b>data</b> <b>center,</b> called the Cisco Containerized <b>Data</b> <b>Center.</b>|$|R
50|$|He {{had held}} {{research}} and teaching appointments at the Federal University of Technology Minna in Nigeria, Colorado State University in Fort Collins, Colorado and the Earth Resources Observation Systems (<b>EROS)</b> <b>Data</b> <b>Center,</b> in Sioux Falls, South Dakota. He was also formerly the Executive Director of the Council for Scientific and Industrial Research (CSIR) in Natural Resources & Environment (NRE) Division, in Pretoria, South Africa. He also served as the Director of the Applied Center for Climate & Earth Systems Science (ACCESS), a Center of Excellence (CoE) of the South Africa Department of Science and Technology (DST) Global Change Grande Challenge (GCGC) program.|$|E
50|$|Taranik's {{career began}} in 1971 at the Iowa Geological Survey, where {{he founded the}} Iowa Remote Sensing Laboratory. While in Iowa, he also taught at the University of Iowa, {{including}} pioneering aerospace remote sensing courses. He next served as Principal Remote Sensing Scientist at the <b>EROS</b> <b>Data</b> <b>Center</b> in Sioux Falls, South Dakota, {{under the auspices of}} the United States Geological Survey (USGS), from 1975 to 1979. In 1979, he became branch chief of the Non-Renewable Resources section at NASA headquarters in Washington, DC, where he managed NASA's programs for engineering development and flight of aerospace technology for solid earth applications. He was the Program Scientist for the first set of scientific instruments sent up on the Space Shuttle as cargo in 1981, having served in that role for both of the first two Space Shuttle launches. He was awarded NASA's Exceptional Scientific Achievement Medal in 1982.|$|E
5000|$|After {{leaving the}} Sioux Falls area, Interstate 29 {{continues}} north toward Brookings. The highway serves the <b>EROS</b> <b>Data</b> <b>Center</b> and United States Geological Survey near Baltic. The highway then continues north and intersects the northern terminus of SD 115 west of Dell Rapids. This is I-29's last exit before leaving Minnehaha County and entering Moody County. The highway continues due north to an interchange with SD 34 near Madison. Just 5 mi north of here, the route shares an interchange with SD 32, a highway that serves nearby Flandreau, South Dakota. The highway has a rest stop {{north of the}} Flandreau exit before entering Brookings County. The highway's first exit in Brookings County, serves SD 324. After this interchange, Interstate 29 enters Brookings and has two exits in the city. The first is an interchange with US 14 at exit 132. This exit is also a signed business spur of I-29. Exit 133 serves the business route of US 14. This exit is signed on the northbound route as [...] "US 14B." [...] After these exits, the highway continues north toward Watertown.|$|E
40|$|<b>Data</b> <b>centers</b> now play an {{important}} role in modern IT infrastructures. Although much research effort has been made in the field of green <b>data</b> <b>center</b> computing, performance metrics for green <b>data</b> <b>centers</b> have been left ignored. This paper is devoted to identify and implement energy efficiency and green computing performance metrics in <b>data</b> <b>centers.</b> The metrics helps <b>data</b> <b>center</b> managers to measure and implement cost and power savings in <b>data</b> <b>centers.</b> A metrics based energy efficiency model for categorizing <b>data</b> <b>center</b> into measurable units is proposed which divides <b>data</b> <b>center</b> into four measurable areas and maps metrics to measure their efficiency and performance. The results generated after applying Power Usage Effectiveness metrics clearly demonstrate poor performance of <b>data</b> <b>center</b> with PUE value of 3. 2, which indicates very inefficient <b>data</b> <b>center...</b>|$|R
50|$|<b>Data</b> <b>Center</b> Design Consultant (DCDC)Established in 2011, <b>Data</b> <b>Center</b> Design Consultants {{demonstrate}} {{knowledge in}} <b>data</b> <b>center</b> design.|$|R
50|$|The Telecommunications Industry Association's Telecommunications Infrastructure Standard for <b>Data</b> <b>Centers</b> {{specifies}} {{the minimum}} requirements for telecommunications infrastructure of <b>data</b> <b>centers</b> and computer rooms including single tenant enterprise <b>data</b> <b>centers</b> and multi-tenant Internet hosting <b>data</b> <b>centers.</b> The topology proposed {{in this document}} {{is intended to be}} applicable to any size <b>data</b> <b>center.</b>|$|R
30|$|Hardy 98. The Hardy 98 {{vegetation}} cover and fuel loading map provides a spatially consistent map of fuel loadings at 1 km resolution {{for the western}} United States (Hardy et al. 1998). In this map, {{vegetation cover}} types comprise 18 broad categories created using an <b>EROS</b> <b>Data</b> <b>Center</b> LAND Characterization Class (USDI Geological Survey, <b>EROS</b> <b>Data</b> <b>Center,</b> Sioux Falls, South Dakota, USA) product. The fuel loadings by vegetation cover type are presented for live shrubs, live herbaceous fuels, downed woody fuels, litter, and duff.|$|E
30|$|All {{images were}} {{acquired}} by LAND-SAT-Thematic Mapper (TM) and geometrically registered using terrain correction algorithms (Level 1 T) by the United States Geological Survey (USGS) Earth Resources Observation Systems (<b>EROS)</b> <b>Data</b> <b>Center.</b> We converted all satellite data to at-sensor-reflectance (Chander et al. 2009).|$|E
40|$|Figure 13 - Map of China and {{neighboring}} countries showing {{the distribution of}} ten of eleven Palaearctic Limnobaris species. See Yoshihara and Morimoto (1994) for distribution of Limnobaris babai in Japan. Yellow line demarks southern limit of Palaeartic Realm according to Hoffmann (2001). Topographic relief in 1000 m increments calculated from GTOPO 30 data set of US Geological Survey, <b>EROS</b> <b>data</b> <b>center...</b>|$|E
30|$|A {{similarity}} between cloud and traditional <b>data</b> <b>center</b> {{is that both}} {{can be used for}} data storage. Cloud is an example of off-premises computing, whereas <b>data</b> <b>centers</b> are being used on premise storing system. Nowadays, <b>data</b> <b>centers</b> are effectively being utilized in cloud computing. Cloud services are now being provided through <b>data</b> <b>centers,</b> which house cloud services and cloud-related resources. Cloud service providers also own <b>data</b> <b>centers,</b> which is located in different geographical location, for provisioning of uninterrupted services in case of outage and unpredictable situations. IaaS (Infrastructure as a service), which provides facilities like virtual machines, storage and load balancing maintains a large pool of resources in <b>data</b> <b>centers.</b> <b>Data</b> <b>centers,</b> which are largely being used for cloud computing are called cloud <b>data</b> <b>center.</b> Lately the demarcation of the terms has disappeared and all are referred as <b>data</b> <b>centers.</b> Existing <b>data</b> <b>centers</b> are often restructured with modern equipment so that it can take advantage of greater performance and energy efficient facilities of cloud computing. The entire process of modernization of <b>data</b> <b>center</b> is called <b>data</b> <b>center</b> transformation [15].|$|R
40|$|This work {{describes}} two <b>data</b> <b>center</b> efficiency metrics: Power Usage Effectiveness (PUE) and Compute Power Efficiency (CPE). PUE {{characterizes the}} {{fraction of the total}} <b>data</b> <b>center</b> power used for IT work. CPE characterizes the overall <b>data</b> <b>center</b> efficiency, considering IT equipment utilization as well as how power is used in the <b>data</b> <b>center.</b> The PUE results from three <b>data</b> <b>center</b> studies are presented here. The data suggests that a carefully designed and managed <b>data</b> <b>center</b> has a PUE of 2. 0. More studies are required to determine the range of values for the typical <b>data</b> <b>center.</b> A <b>data</b> <b>center</b> infrastructure and energy cost model is presented to compare hardware costs to infrastructure and energy costs. The impact of PUE on these costs is examined to illustrate the impact of <b>data</b> <b>center</b> efficiency on the total cost of operating a <b>data</b> <b>center...</b>|$|R
5000|$|Modules - <b>data</b> <b>center</b> modules are purpose-engineered modules and {{components}} to offer scalable <b>data</b> <b>center</b> capacity. They typically use standardized components, which make them easily added, integrated or retrofitted into existing <b>data</b> <b>centers,</b> and cheaper {{and easier to}} build. In a colocation environment, the <b>data</b> <b>center</b> module is a <b>data</b> <b>center</b> within a <b>data</b> <b>center,</b> with its own steel walls and security protocol, and its own cooling and power infrastructure. “A number of colocation companies have praised the modular approach to <b>data</b> <b>centers</b> to better match customer demand with physical build outs, and allow customers to buy a <b>data</b> <b>center</b> as a service, paying only for what they consume.” ...|$|R
40|$|The Landsat Science Data Processing System, {{developed}} by NASA for the Landsat 7 Project provides science data handling infrastructure {{used at the}} <b>EROS</b> <b>Data</b> <b>Center</b> Landsat 7 Data Handling Facility of the USGS Department of Interior. This paper presents an overview the designs, architectures, and details of the various systems used in the processing of the Landsat 7 Science Data...|$|E
40|$|This {{data set}} {{is based on}} the GTOPO 30 Digital Elevation Model (DEM) {{produced}} by the United States Geological Survey <b>EROS</b> <b>Data</b> <b>Center</b> (USGS EDC). The BOReal Ecosystem-Atmosphere Study (BOREAS) region (1, 000 km x 1000 km) was extracted from the GTOPO 30 data and reprojected by BOREAS staff into the Albers Equal-Area Conic (AEAC) projection. The pixel size of these data is 1 km. The data are stored in binary, image format files...|$|E
40|$|The {{author has}} {{identified}} the following significant results. Daily weather service satellite {{photographs of the}} midwest {{were found to be}} of great assistance before ordering <b>EROS</b> <b>Data</b> <b>Center</b> products. These weather satellite images are a quick and inexpensive record of the location of cloud masses, which supplements the percent of cloud catalogues. Savings of time and money were made because the location of cloud cover was known before any imagery was ordered...|$|E
50|$|There {{are three}} World <b>Data</b> <b>Centers</b> for Oceanography:World <b>Data</b> <b>Center,</b> Silver Spring, Maryland, United States,World <b>Data</b> <b>Center,</b> Moscow, Russia, andWorld <b>Data</b> <b>Center,</b> Tianjin, People's Republic of China.They {{are part of}} the World <b>Data</b> <b>Center</b> System {{initiated}} in 1957 to provide a mechanism for data exchange, and they operate under guidelines issued by the International Council of Scientific Unions (ICSU).|$|R
50|$|Generally, <b>data</b> <b>center</b> {{services}} {{fall into}} two categories: services provided to a <b>data</b> <b>center</b> or services provided from a <b>data</b> <b>center.</b>|$|R
50|$|A network-neutral <b>data</b> <b>center</b> (or carrier-neutral <b>data</b> <b>center)</b> is a <b>data</b> <b>center</b> (or carrier hotel) {{which allows}} {{interconnection}} between multiple telecommunication carriers and/or colocation providers. Network-neutral <b>data</b> <b>centers</b> exist {{all over the}} world and vary in size and power.|$|R
40|$|The U. S. Geological Survey <b>EROS</b> <b>Data</b> <b>Center</b> {{evaluated}} {{the utility of}} LANDSAT multispectral scanner (MSS) and Thematic Mapper (TM) data for natural resource assessment and land cover information, emphasizing manual interpretation and digital classification of the data for U. S. Department of the Interior applications. In most cases, substantially more information was derived from TM data than from MSS data. The test areas included Washington, D. C. and prairie regions of South Dakota and Kansas...|$|E
40|$|Content: “Everything Is Connected to Everything Else. ” by Dave Gosselin, NESEN Director New and Improved NESEN Web Page by Mark Mesarch NESEN at NATS 1999 STEDII Weather Is Persistent for Five Years by Mark Mesarch Virtual Nebraska Update – by Rick Perk Video Programs Available {{from the}} USGS <b>EROS</b> <b>Data</b> <b>Center</b> - by Duane Mohlman Perilous Beauty [...] The Hidden Dangers of Mount Rainier – by Duane Mohlman Earth Science in the Community New Publication from the Conservation and Survey Division – by Duane Mohlman 1999 NESEN Summer Workshops Dave Gosselin Receives Catalyst Awar...|$|E
40|$|The <b>EROS</b> <b>Data</b> <b>Center,</b> {{supported}} by the U. S. Geological Survey (USGS), provides an extensive selection of imagery and datasets. These materials may be accessed {{through a variety of}} management systems and interfaces, including the USGS Global Visualization Viewer, Earth Explorer, Photo Finder, Map Finder, and the EOS Data Gateway. Products include aerial and satellite photos, multispectral imagery, maps, elevation datasets, and land cover imagery and datasets. Some products are free; others must be ordered at cost. Educational levels: High school, Undergraduate lower division, Undergraduate upper division, Graduate or professional...|$|E
40|$|<b>Data</b> <b>Center</b> as a Service (DCaaS) {{facilitates}} {{to clients}} as alternate outsourced physical <b>data</b> <b>center,</b> {{the expectations of}} business community to fully automate these <b>data</b> <b>centers</b> to run smoothly. Geographically distributed <b>data</b> <b>centers</b> and its connectivity has major role in next generation <b>data</b> <b>centers.</b> In order to deploy the reliable connections between distributed <b>data</b> <b>centers</b> the SDN based security and logical firewalls are attractiveand enviable. We present the middleware security framework for software defined <b>data</b> <b>centers</b> inter-connectivity, the proposed security framework {{will be based on}} some learning processes,which will reduce the complexity and manage very large number of secure connections in real-world <b>data</b> <b>centers.</b> In this paper we will focus on two main objectives; (1) proposing simple and yet scalable techniques for security and analysis, (2) Implementing and evaluating these techniques on real-world <b>data</b> <b>centers...</b>|$|R
50|$|<b>Data</b> <b>Center</b> Interconnect (DCI): <b>Data</b> <b>center</b> {{interconnect}} {{solutions are}} intended to extend the benefits of multi-tenant private clouds across multiple <b>data</b> <b>centers.</b>|$|R
30|$|Conceptually, CloudSim on {{one side}} offers classes {{representing}} <b>data</b> <b>centers,</b> physical hosts, virtual machines, services to be executed in the <b>data</b> <b>centers,</b> users of cloud services, internal <b>data</b> <b>center</b> networks, and energy consumption of physical hosts and <b>data</b> <b>centers</b> elements. On the other side, CloudSim supports dynamic insertion of simulation elements and provides message-passing application and <b>data</b> <b>center</b> network topology [9, 18].|$|R
40|$|This paper {{describes}} {{a method for}} cloud cover assessment using computerbased analysis of multi-band Landsat images. The objective is to accurately determine the percentage of cloud cover in an efficient manner. The “correct ” value is determined by an expert’s visual assessment. Acceptable error rates are ± 10 % from the visually-determined coverage. This research improves upon an existing algorithm developed {{for use by the}} <b>EROS</b> <b>Data</b> <b>Center</b> several years ago. The existing algorithm uses threshold values in bands 3, 5, and 6 (red, middle infrared, and thermal, respectively) based on the expected frequency response for clouds in each band. While this algorithm is reasonably fast, the accuracy is often unsatisfactory. The dataset used in developing the new method contained 329 subsampled, 7 -band Landsat browse images with wide geographic coverage and a variety of cloud types. This dataset, provided by the <b>EROS</b> <b>Data</b> <b>Center,</b> also specifies the visual cloud cover assessment and the cloud cover assessment using the current automated algorithm. Mask images, separating cloud and non-cloud pixels, were developed for a subset of these images. 1 The new approach is statistically based, developed from a multi-dimensional histogram analysis of a training subset. Images from a disjoint test set were then classified. Initial results are significantly more accurate than the existing automated algorithm. Key words: remote sensing, image analysis, automated cloud cover assessment, multi-spectral analysis, environmental sensing. ...|$|E
40|$|A semiautomated {{method for}} objectively {{interpreting}} and extracting the land-water interface has been devised and used successfully to generate multiple shoreline {{data for the}} test States of Louisiana and Delaware. The method {{is based on the}} application of tasseled cap transformation coefficients derived by the <b>EROS</b> <b>Data</b> <b>Center</b> for Landsat 7 Enhanced Thematic Mapper Data, and is used in conjunction with ERDAS Imagine software. Shoreline data obtained using this method are cost effective compared with conventional mapping methods for State, regional, and national coastline applications. Attempts to attribute vector shoreline data with orthometric elevation values derived from tide observation stations, however, proved unsuccessful...|$|E
40|$|The {{objective}} {{of this study was}} to determine if the quality of the ERTS imagery could be improved by averaging successive passes over the same physical area. The U. S. G. S. designated five passes over two areas of study: Washington, D. C. and Phoenix, Ariz., with the requirement that optimized imagery be derived directly from the <b>EROS</b> <b>Data</b> <b>Center</b> Bulk CCT 2 ̆ 7 s. The Mead system goes directly from the Bulk CCT 2 ̆ 7 s to hard-copy print-out, thereby by-passing any intermediate electronic, electro-optical, or photographic processes which may lead to image deterioration...|$|E
50|$|The Telecommunications Industry Association (TIA) ANSI/TIA-942-A Telecommunications Infrastructure Standard for <b>Data</b> <b>Centers</b> is an American National Standard (ANS) that {{specifies}} {{the minimum}} requirements for telecommunications infrastructure of <b>data</b> <b>centers</b> and computer rooms including single tenant enterprise <b>data</b> <b>centers</b> and multi-tenant Internet hosting <b>data</b> <b>centers.</b> The topology {{proposed in the}} standard {{was intended to be}} applicable to any size <b>data</b> <b>center.</b>|$|R
40|$|The {{cloud based}} {{distributed}} <b>data</b> <b>center</b> uses virtualization technology {{to share the}} resources {{to the outside world}} through a virtual machine. Cloud administrator selects the <b>data</b> <b>center</b> to access virtual machines by using administrative and dynamic policies. Every <b>data</b> <b>center</b> has multiple virtual machines. Selection of <b>data</b> <b>center</b> is an important task which affects on performance as well as cost effectiveness of the <b>data</b> <b>center.</b> This problem can be solved by centralized as well as distributed <b>data</b> <b>center.</b> In Logistics Company, Centralized <b>data</b> <b>center</b> faces bottleneck in operations like virtual machine migration, creation, deletion, and needs to contact central administrator which can increase the negligible amount of network traffic. The paper presents comparison of distributed and centralized <b>data</b> <b>center</b> and strategies of distributed <b>data</b> <b>center</b> for reducing the latency and cost of selection of <b>data</b> <b>center</b> over the cloud by proposing an algorithm distributed service broker policy (DSBP) for logistics information system...|$|R
40|$|The {{number of}} <b>Data</b> <b>Centers</b> and the servers present in {{them has been}} on the rise over the last decade with the advent of cloud computing, social networking, Big data {{analytics}} etc. This has eventually led to the increase in the power consumption of the <b>Data</b> <b>Center</b> due to the power hungry interconnection fabric which consists of switches and routers. The scalability of the <b>data</b> <b>center</b> has also become a problem due to the interconnect cabling complexity which is also responsible for the increase in the energy used for cooling the <b>data</b> <b>center</b> as these bundles of wires reduce the air flow in the <b>data</b> <b>center.</b> The maintenance costs of the <b>data</b> <b>center</b> is high due to this reason. This brings the challenge of reducing the power consumption as well as improving the scalability of the <b>data</b> <b>center.</b> There is a lot of cost involved in the establishment of a network in a <b>data</b> <b>center</b> and this network {{is one of the main}} source of power consumption. Therefore, there is a need to accurately characterize the <b>data</b> <b>center</b> network before its construction which requires the simulation of the <b>data</b> <b>center</b> models. For the simulation of <b>data</b> <b>center</b> models, we require the traffic which is identical to that of an actual <b>data</b> <b>center</b> so that the results will be similar to a real time <b>data</b> <b>center.</b> Traditional <b>data</b> <b>center</b> networks have a wired communication fabric, which is not scalable and contributes largely to the power consumption. This has led to the investigation of other methods. There have been transceivers designed that can support the unlicensed 60 GHz spectrum, supporting high bandwidth similar to the wired network present in traditional <b>data</b> <b>centers.</b> These wireless links have spatial reusability and the <b>data</b> <b>centers</b> can make use of this communication medium to meet the high bandwidth demands and also reduce the use of cable thereby bringing down the cost and the power consumption. This thesis studies the previous traffic models used in the simulation of a <b>data</b> <b>center</b> network. Traffic collected from ten different <b>data</b> <b>centers</b> is then characterized and modelled based on various probability distributions. The implementation of the model tries to generate traffic similar to that of an actual <b>data</b> <b>center.</b> The <b>Data</b> <b>Center</b> Network is then simulated using the traffic generated and the performance of the wired <b>data</b> <b>center</b> is quantified in terms of metrics like throughput, latency and the power consumption of the <b>data</b> <b>center</b> networks...|$|R
