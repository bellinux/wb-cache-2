95|212|Public
5000|$|MURI (<b>Dependent</b> <b>failure</b> models & Collaborative backup for {{withstanding}} network catastrophes) ...|$|E
5000|$|Phoenix, S. L., and Tierney, L.-J. (1983) “A {{statistical}} {{model for the}} time <b>dependent</b> <b>failure</b> of unidirectional composite materials under local elastic load-sharing among fibers.” Engrg. Fract. Mech. 18 (1), pp. 193-215.|$|E
50|$|The failure {{mechanisms}} are divided in orientation dependent and orientation independent. Orientation <b>dependent</b> <b>failure</b> mechanisms {{depend on the}} orientation of the slope with respect to {{the orientation of the}} discontinuities in the rock mass, i.e. sliding (plane and wedge sliding) and toppling failure. Orientation independent relates to the possibility that a slope fails independently from its orientation, e.g. circular failure completely through newly formed discontinuities in intact rock blocks, or failing partially following existing discontinuities and partially new discontinuities.|$|E
30|$|In {{most of the}} {{aforementioned}} researches, <b>dependent</b> <b>failures</b> are ignored. A few studies have merely hinted at a single kind of dependent variables (e.g., CCF or common cause shock failure). However, there are many redundant systems which are exposed to cascading failure as well as CCF. <b>Dependent</b> <b>failures</b> and reparability are among the features that {{should be taken into}} consideration in evaluation of MTBF of redundant systems in order to obtain realistic results. Hence, it is important to develop a method which incorporates these features into the MTBF function. Furthermore, it was demonstrated in this paper how <b>dependent</b> <b>failures</b> affect and reduce redundant system MTBF. If <b>dependent</b> <b>failures</b> are not incorporated into reliability analyses, reliability parameters are not correctly evaluated creating misleading results about the redundant systems. Some other studies have been silent regarding fuzzy failure rates, whereas engineers designing redundant systems would prefer the failure rates expressed as linguistic terms that can be effectively modeled as fuzzy numbers. Therefore, this research presents a method to evaluate the MTBF of a 2 -out-of- 3 redundant repairable system with independent failure, CCF, cascading failure, and fuzzy parameters.|$|R
40|$|<b>Dependent</b> <b>failures</b> are {{extremely}} important in reliability analysis {{and must be}} given adequate treatment so as to minimize gross underestimation of reliability. German regulatory guidance documents for PSA stipulate that model parameters used for calculating frequencies should be derived from operating experience in a transparent manner. Progress has been made with the process oriented simulation (POS) model for common cause failure (CCF) quantification. A number of applications are presented for which results obtained from established CCF models are available, focusing on cases with high degree of redundancy and small numbers of observed events. 1. Common cause failure analysis {{in the frame of}} probabilistic safety assessment Design, operation and maintenance of systems are performed to minimize potential failures such as random, systematic and <b>dependent</b> <b>failures.</b> <b>Dependent</b> <b>failures</b> comprise secondary failures caused, e. g., by violation of operational conditions and so-called commanded failures like component fails due to violation of interface conditions. The residual part of the group of commanded failures is called common cause failures (CCF). To identify <b>dependent</b> <b>failures,</b> approaches have been extended to encompass potential interpendencies between systems or components. Secondary and commanded failures are supposed to be modelled explicitly as far a...|$|R
40|$|Parametric {{failures}} – deviations {{in control}} (e. g. line width) result in functional failures or quality-loss performance degradation • Random failures – uncorrelated random failure in some element – example: individual via <b>failures</b> • Area <b>dependent</b> <b>failures</b> – failures {{related to the}} area of opportunity for failure – example: “killer defect ” particle...|$|R
40|$|In this paper, a {{new method}} is {{developed}} to model <b>dependent</b> <b>failure</b> behavior among failure mechanisms. Unlike the existing methods, the developed method models {{the root cause}} of the dependency explicitly, so that a deterministic model, rather than a probabilistic one, can be established. Three steps comprise the developed method. First, physics-of-failure (PoF) models are utilized to model each failure mechanism. Then, interactions among failure mechanisms are modeled as a combination of three basic relations, competition, superposition and coupling. This is the reason why the method is referred to as “compositional method”. Finally, the PoF models and the interaction model are combined to develop a deterministic model of the <b>dependent</b> <b>failure</b> behavior. As a demonstration, the method is applied on an actual spool and the developed failure behavior model is validated by a wear test. The result demonstrates that the compositional method is an effective way to model <b>dependent</b> <b>failure</b> behavior...|$|E
40|$|A coupled elastoplastic-damage {{constitutive}} model with Lode angle <b>dependent</b> <b>failure</b> criterion for high strain and ballistic applications is presented. A Lode angle dependent function {{is added to}} the equivalent plastic strain to failure definition of the Johnson–Cook failure criterion. The weakening in the elastic law and in the Johnson–Cook-like constitutive relation implicitly introduces the Lode angle dependency in the elastoplastic behaviour. The material model is calibrated for precipitation hardened Inconel 718 nickel-base superalloy. The combination of a Lode angle <b>dependent</b> <b>failure</b> criterion with weakened constitutive equations is proven to predict fracture patterns of the mechanical tests performed and provide reliable results. Additionally, the mesh size dependency on the prediction of the fracture patterns was studied, showing that was crucial to predict such pattern...|$|E
40|$|This paper {{advocates a}} third approach: 3. Use the same o-line {{analysis}} to identify how processes fail in a correlated manner. Represent this using our abstraction for dependent failures, and replicate {{in a way}} that satis es our replication requirement and that minimizes the number of replicas. Instantiate the appropriate <b>dependent</b> <b>failure</b> protoco...|$|E
30|$|The {{reliability}} {{of the system is}} obtained by summing up states’ probabilities. This approach is conceptually general and flexible and makes it possible to take into account various <b>dependent</b> <b>failures</b> (Dr Nahman 2002; Anderson 1998; Billinton and Allan 1996).|$|R
40|$|Field data {{provide a}} rich source of {{information}} about the <b>dependent</b> <b>failures</b> whose omission from existing models can result in underestimation of the reliability of repairable systems, A graphical technique has been developed to highlight these events. This involves comparing the observed number of failures with the expected pattern under a null model of no common cause dependence, a non-homogeneous Poisson process with Weibull rate function. The derivation of the graph is outlined and its use as a screening tool is illustrated by applications to field data. The <b>dependent</b> <b>failures</b> identified are described and their engineering implications are discussed. The statistical power of the technique is evaluated for a range of alternative models of dependence, including simple shock models...|$|R
40|$|In {{this report}} the {{state-of-the-art}} regarding <b>dependent</b> <b>failures</b> is compiled and commented on. Among others the following recommendations are infered: The term "common mode failures" should {{be restricted to}} failures of redundant, similar components; the generic term is "dependent failures" with the subsets "causal failures" and "common cause failures". In risk studies, <b>dependent</b> <b>failures</b> should be covered {{as far as possible}} by "explicit methods". Nevertheless an uncovered rest remains, which should be accounted for by sensitivity analyses using "implicit methods". For this the homogeneous Marshall-Olkin-model is recommended. Because the available reports on operating experiences only record "common mode failures" systematically, it is recommended to additionally apply other methods, e. g. carry out a "precursor study"...|$|R
30|$|The {{reliability}} {{model of}} the transformer is very similar with that of transmission lines proposed in Sect. 3. The main {{difference is that the}} transformer do not have multiple segments. Its healthiness <b>dependent</b> <b>failure</b> model is built using the model proposed in [23]. The paper will not go into specific on the reliability {{model of the}} transformers.|$|E
40|$|A {{mechanical}} component may fail in many modes {{that are usually}} not independent. There is generally not a {{joint probability density function}} to describe these correlated failure modes. Thus, it is difficult to compute the reliability when considering the correlations between the failure modes. It is supposed that three or more failure modes arise synchronously to be a very small probability event. The relationship between ultimate state functions in different failure modes is established by utilizing linear regression method. A double integration model for reliability of {{mechanical component}}s with <b>dependent</b> <b>failure</b> modes is built according to stress-strength interference model. In case of square, cube, or exponential relationship between two ultimate state functions, a linear transformation is made. An example of pin that may fail in shear fracture, bruise, or both is discussed. The reliability is compared with that obtained by using Monte Carlo method, which represents that the reliability model with <b>dependent</b> <b>failure</b> modes proposed is correct...|$|E
40|$|Problem statement: Many {{authors have}} studied k-out of-n {{repairable}} system with <b>dependent</b> <b>failure</b> and standby support. The question was raised whether the repair and standby units support increase {{the reliability of}} the system. Determine the efficiency of repair and standby support on {{the reliability of the}} system. Approach: In this study the statistical analysis of k-out of-n repairable system with <b>dependent</b> <b>failure</b> and standby support were discussed. Several reliability characteristics are obtained by using Kolmogorov's forward equations method. After the model is developed a particular case study is discussed to validate the theoretical results, a numerical computation are derived. Tables and graphs have been also given in the end. Results: The results indicated that the system with repair and standby support is better than the system without repair and standby support. Conclusion: These results indicated that the system with repair of its failed parts and standby redundancy facility increased the reliability of the system...|$|E
40|$|Failure models characterise the {{expected}} component failures in fault-tolerant computing. In {{the context of}} distributed systems, a failure model usually consists of two parts: a functional part specifying in what way individual processing entities may fail and a structural part specifying the potential scope of failures within the system. Such models must be expressive enough to cover all relevant practical situations, but must also be simple enough to allow uncomplicated reasoning about fault-tolerant algorithms. Usually, an increase in expressiveness complicates formal reasoning, but enables more accurate models that allow to improve the assumption coverage and resilience of solutions. In this paper, we introduce the structural failure model class DiDep that allows to specify directed <b>dependent</b> <b>failures,</b> which, for example, occur {{in the area of}} intrusion tolerance and security. DiDep is a generalisation of previous classes for undirected <b>dependent</b> <b>failures,</b> namely the general adversary structures, the fail-prone systems, and the core and survivor sets, which we show to be equivalent. We show that the increase in expressiveness of DiDep does not significantly penalise the simplicity of corresponding models by giving an algorithm that transforms any Consensus algorithm for undirected <b>dependent</b> <b>failures</b> into a Consensus algorithm for a DiDep model. We characterise the improved resilience obtained with DiDep and show that certain models even allow to circumvent the famous FLP impossibility result. ∗ This work is supported by the German Research Foundation (DFG) ...|$|R
30|$|The network {{approach}} {{when applicable}} usually provides a shorter route to solution. The network approach {{is usually not}} suitable when <b>dependent</b> <b>failures</b> or repairs are involved (common cause failures, restricted repairs, warm standby unit, etc.). It {{is not necessary to}} assume the event independence in this approach, but dependent events can greatly increase the algebra of the computations.|$|R
40|$|The use of knowledge-base {{architecture}} and planning control; mechanisms {{to perform an}} intelligent monitoring task in the flight domain is addressed. The route level, the trajectory level, {{and parts of the}} aerodynamics level are demonstrated. Hierarchical planning and monitoring conceptual levels, functional-directed mechanism rationalization, and using deep-level mechanism models for diagnoses of <b>dependent</b> <b>failures</b> are discussed...|$|R
40|$|The {{reliability}} analysis of complex systems may often become unmanageable, especially when state or time <b>dependent</b> <b>failure</b> rates, repair facilities and standby operations {{are present in}} a system. This paper describes the possible use of a simulation approach {{and the development of}} a reliability, availability and maintainability simulator which may be used to alleviate some of the disadvantages inherent in the traditional analytical approach...|$|E
40|$|From {{operational}} {{records of the}} years 1977 to 1986, survival distributions of Helium valves in gas circuits of the AVR experimental nuclear power plant were determined. Results are constant failure rates {{in the range from}} 3 to 6 · 10 ^- 6 /h and, for some populations, indications of time <b>dependent</b> <b>failure</b> rates. Nonparametric methods showed only limited efficiency. For a Bayesian approach the necessary prior information was missing. Furthermore, the main failure causes could be determined...|$|E
40|$|A {{numerical}} {{and experimental}} study of ballistic impacts at various temperatures on precipitation hardened Inconel 718 nickel-base superalloy plates has been performed. A coupled elastoplastic-damage constitutive model with Lode angle <b>dependent</b> <b>failure</b> criterion has been implemented in LS-DYNA non-linear finite element code to model the mechanical behaviour of such an alloy. The ballistic impact tests {{have been carried out}} at three temperatures: room temperature (25 °C), 400 °C and 700 °C. The numerical study showed that the mesh size is crucial to predict correctly the shear bands detected in the tested plates. Moreover, the mesh size convergence has been achieved for element sizes on the same order that the shear bands. The residual velocity as well as the ballistic limit prediction has been considered excellent for high temperature ballistic tests. Nevertheless, the model has been less accurate for the numerical simulations performed at room temperature, being though in reasonable agreement with the experimental data. Additionally, the influence that the Lode angle had on quasi-static failure patterns such as cup-cone and slanted failure has been studied numerically. The study has revealed that the combined action of weakened constitutive equations and Lode angle <b>dependent</b> <b>failure</b> criterion has been necessary to predict the previously-mentioned failure pattern...|$|E
50|$|Flavio Junqueira and Keith Marzullo. Synchronous Consensus for <b>Dependent</b> Process <b>Failures.</b> ICDCS 2003, to appear.|$|R
30|$|Redundancy is a {{well-known}} and widely used approach to enhancement of failure-sensitive systems which are subject to both <b>dependent</b> and independent <b>failures.</b> In most reliability analyses and {{mean time between failures}} (MTBF) evaluation models for redundant systems, components are considered independent of one another with respect to failure. This results in an incorrect and inaccurate evaluation of system features. Therefore, it is highly crucial to identify and consider <b>dependent</b> <b>failures</b> in the evaluation of reliability and MTBF of systems. Common cause failure (CCF) {{is one of the most}} important <b>dependent</b> <b>failures</b> in redundant systems, in which an intrinsic factor leads to the propagation of failure in all components, resulting in the simultaneous failure of components in the redundant system (Kančev and Čepin 2012). Failing to incorporate CCF into MTBF of redundant systems leads to irreversible damage. In March 22, 1975, negligence of CCF led to a fire that occurred in a nuclear power plant located in the state of Alabama, USA (Mortazavi et al. 2016). After this event, in order to prevent the recurrence of such incidents, extensive research was conducted on CCF, resulting in the development of various standards (Mosleh et al. 1988, 1998). As another example, one may refer to the failure of all four engines of the Boeing 747 in fight BA 009 on 24 June 1982 over the Indian Ocean (Tootell 1985). The engineers estimated the likelihood of all four engines failing during the same flight to be negligible; however, a CCF of volcano ash proved otherwise. Other catastrophic incidents resulted by ignoring the likelihood of <b>dependent</b> <b>failures</b> in redundant systems have been reported, e.g., the hydraulic pumps failure due to engine explosion in United Airlines DC- 10 in July 1989 (Galison 2000).|$|R
40|$|The {{methods for}} {{evaluation}} of the electric equipment reliability taking <b>dependent</b> <b>failures</b> and external actions being characteristic for agriculture into consideration have been developed; the principial-new system of the dufferential equations for dynamics of the electric equipment reliability taking <b>dependent</b> <b>failures,</b> wear and substitution of the failured elemetns into consideration has been obtained; the methods and computer program on {{the evaluation of the}} mutual influence and intensity level of the failures in the elements of the electric equipment have been developed; the mathematical models for reliability in the electric equipment of the electric drives for agricultural machines have been constructed, verivied according to the results of statistical tests and investigated by the methods of the developed theory; on their base the reliability evaluations and parameters of the external actions regularized; the kinetic functions of the reliability for the induction motors have been found; the growth functions and limit values of their failure intensity have been determined. Available from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|The {{anelastic}} {{deformation of}} solids is often treated using continuum damage mechanics. An alternative {{approach to the}} brittle failure of a solid {{is provided by the}} discrete fiber-bundle model. Here we show that the continuum damage model can give exactly the same solution for material failure as the fiber-bundle model. We compare both models with laboratory experiments on the time <b>dependent</b> <b>failure</b> of chipboard and fiberglass. The power-law scaling obtained in both models and in the experiments is consistent with the power-law seismic activation observed prior to some earthquakes...|$|E
40|$|AbstractIn {{order to}} {{describe}} the mechanism of system parts occurring <b>dependent</b> <b>failure,</b> extending the definition of stress and strength to wider range which based on the component's failure stress-strength interference theory. Using hypergragh theory {{to carry on the}} modeling of related failure system and calculate its component's related failure rate, then obtaining the real failure rate of the system. Using component's failure data of RIAC Automated Databook to calculate system failure rate, then comparing with corresponding system failure data of OREDA to demonstrate the efficiency of this method...|$|E
40|$|Abstract: In {{order to}} improve {{reliability}} of automobile transmission system and its running stability, this paper raises a reliability assessment calculation method. This method considers <b>dependent</b> <b>failure</b> of the parts, material strength degradation with increasing load times and uncertainty of load times in micro and macro scopes to explore main factors affecting the system reliability. The feasibility of this method has been validated by the example analysis. As a result, material properties of each gear parts, torques and load times play key parts in reliability of the system. ...|$|E
40|$|Cyber-induced {{failures}} affect {{power system}} reliability {{and thus are}} important {{to be considered in}} composite system reliability evaluation. This dissertation extends the scope of bulk power system reliability modeling and analysis with the consideration of cyber elements. A novel methodology by introducing the concept of Cyber-Physical Interface Matrix (CPIM) is proposed. The failure modes of cyber components and their impact on transmission line tripping behaviors are modeled and numerically analyzed as an example to illustrate the construction and utility of the CPIM. The methodology is then enhanced and implemented on an extended Roy Billinton Test System (RBTS) with its applicability for large systems illustrated. The results clearly show the impact of cyber-induced failures on system-wide reliability indices. The CPIM is the critical idea in the proposed methodology. It decouples the analysis of the cyber part from the physical part and provides the means of performing the overall analysis in a tractable fashion. The overall methodology proposed in this dissertation also provides a scalable option for reliability evaluation of large cyber-physical power systems. The efficiency of the overall methodology can be further improved with the use of non-sequential Monte Carlo techniques. However, the failure and repair processes in cyber-induced events are inherently sequential involving <b>dependent</b> <b>failures,</b> making it difficult to utilize non-sequential sampling methods as simply as when the components are independent. In this dissertation, the difficulties of using sampling when there are <b>dependent</b> <b>failures</b> are thoroughly explored. An approach is proposed to overcome the difficulties by generating a representative state space and its probabilities from which states can be sampled. The proposed approach not only preserves the sequential and dependent features of cyber-induced events but also improves the efficiency, which is very beneficial for reliability evaluation of large power systems in the presence of cyber-induced <b>dependent</b> <b>failures...</b>|$|R
30|$|Considering the {{significant}} impact of <b>dependent</b> <b>failures</b> (CCF and cascading failure) on reliability of redundant systems, {{these types of}} failures substantially affect MTBF of such systems. Neglecting the role that these types of failures play may result in significantly misleading MTBF evaluation models which in turn lead to incorrect MTBF value. Therefore, this paper addresses these two important types of failures and incorporates those into a proposed model for MTBF evaluation of a 2 -out-of- 3 repairable redundant system.|$|R
40|$|ABSTRACT. In this paper, we {{have studied}} a k-out of-n system with <b>dependent</b> <b>failures</b> and standbys. The failure of any working unit affects the system {{reliability}} and the standby support increases {{the reliability of}} the system. To improve the grade of service we have studied two standby support options for degraded systems. The expressions for reliability and expected operational time are being derived. To validate the analytical results, illustrative examples with numerical results are also facilitated. 1...|$|R
40|$|ABSTRACT. Multivariate failure time data {{arise when}} the sample {{consists}} of clusters and each cluster contains several possibly <b>dependent</b> <b>failure</b> times. The Clayton-Oakes model (Clayton, 1978; Oakes, 1982) for multivariate failure times characterizes the intracluster dependence parametrically but allows arbitrary specification {{of the the}} marginal distributions. In this paper, we discuss estimation in the Clayton-Oakes model when the marginal distributions are modeled to follow the Cox (1972) propor-tional hazards regression model. Parameter estimation {{is based on an}} approximate generalized maximum likelihood estimator. We illustrate the model’s application with example datasets. Some key words: frailty model, multivariate failure time, nonparametric maximum likelihood 1...|$|E
40|$|The {{application}} NUREG/CR- 6850 EPRI/NRC fire PRA methodology to DOE facility presented several challenges. This paper {{documents the}} process and discusses several insights gained during development of the fire PRA. A brief review of the tasks performed is provided with particular focus on the following: • Tasks 5 and 14 : Fire-induced risk model and fire risk quantification. A key lesson learned was to begin model development and quantification {{as early as possible}} in the project using screening values and simplified modeling if necessary. • Tasks 3 and 9 : Fire PRA cable selection and detailed circuit failure analysis. In retrospect, it would have been beneficial to perform the model development and quantification in 2 phases with detailed circuit analysis applied during phase 2. This would have allowed for development of a robust model and quantification earlier in the project and would have provided insights into where to focus the detailed circuit analysis efforts. • Tasks 8 and 11 : Scoping fire modeling and detailed fire modeling. More focus should be placed on detailed fire modeling and less focus on scoping fire modeling. This was the approach taken for the fire PRA. • Task 14 : Fire risk quantification. Typically, multiple safe shutdown (SSD) components fail during a given fire scenario. Therefore <b>dependent</b> <b>failure</b> analysis is critical to obtaining a meaningful fire risk quantification. <b>Dependent</b> <b>failure</b> analysis for the fire PRA presented several challenges which will be discussed in the full paper...|$|E
40|$|A {{method for}} {{lifetime}} or durability predictions for laminated fiber reinforced plastics is given. The procedure {{is similar to}} but {{not the same as}} the well known time-temperature-superposition principle for polymers. The method is better described as an analytical adaptation of time-stress-super-position methods. The analytical constitutive modeling is based upon a nonlinear viscoelastic constitutive model developed by Schapery. Time <b>dependent</b> <b>failure</b> models are discussed and are related to the constitutive models. Finally, results of an incremental lamination analysis using the constitutive and failure model are compared to experimental results. Favorable results between theory and predictions are presented using data from creep tests of about two months duration...|$|E
30|$|System {{failure in}} dynamic {{environment}} is another variety of <b>dependent</b> <b>failures</b> (XiaoFei and Min 2014). To evaluate the MTBF of redundant systems, {{it is assumed}} that the components operate in a static environment. Therefore, the models developed under such assumptions may not be appropriate for redundant systems operating in dynamic conditions. It is recommended that future studies develop a model to evaluate the MTBF of redundant system, incorporating CCF, cascade failure in dynamic environments. The dynamic model may also incorporate uncertainty of the real-world problems.|$|R
40|$|Abstract. To {{establish}} lower bounds on {{the amount}} of replication, there is a common partition argument used to construct indistinguishable executions such that one violates some property of interest. This violation leads {{to the conclusion that the}} lower bound on process replication is of the form n> ⌊kt/b⌋, where t is the maximum number of process failures in any of these executions and k, b are positive integers. In this paper, we show how this argument can be extended to give bounds on replication when <b>failures</b> are <b>dependent.</b> We express these bounds in terms of our model of cores and survivors sets using set properties instead of predicates of the form n> ⌊kt/b⌋. We give two different properties that express the same requirement for k> 1 and b = 1. One property comes directly from the argument, and the other is more useful when designing an algorithm that takes advantage of <b>dependent</b> <b>failures.</b> We also consider a somewhat unusual replication bound of n> ⌊ 3 t/ 2 ⌋ that arises in the Leader Election problem for synchronous receive-omission failures. We generalize the replication bound for <b>dependent</b> <b>failures,</b> and develop an algorithm that shows that this generalized replication bound is tight. ...|$|R
40|$|International audienceIn {{condition-based}} maintenance area, mean residual life function {{is recognized as}} a promising reliability indicator for maintenance decision-making. However, very few maintenance strategies based on the mean residual life have been developed. This paper therefore analyzes a predictive maintenance strategy based on the mean residual life given the degradation level collected periodically in different inspection times. This strategy is developed for systems which are subject to competing and <b>dependent</b> <b>failures</b> due to degradation and traumatic shocks. Numerical examples are given to illustrate the analytical results of the strategy...|$|R
