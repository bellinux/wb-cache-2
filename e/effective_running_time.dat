11|10000|Public
5000|$|There is {{also another}} group os {{algorithms}} that adopt different selection criteria. For example, the algorithms LDIS [...] and CDIS [...] select the densest instances {{in a given}} arbitrary neighborhood. The selected instances can include both, border and internal instances. The LDIS and CDIS algorithms are very simple and select subsets that are very representative of the original dataset. Besides that, since they search by the representative instances in each class separately, they are faster (in terms of time complexity and <b>effective</b> <b>running</b> <b>time)</b> than other algorithms, such as DROP3 and ICF.|$|E
40|$|Many galactic and extragalactic astrophysical {{sources are}} {{currently}} considered promising candidates as high energy neutrino emitters. Astrophysical neutrinos {{can be detected}} as upward-going muons produced in charged-current interactions with the medium surrounding the detector. The expected neutrino fluxes from various models start to dominate on the atmospheric neutrino background at neutrino energies above some tens of TeV. We present {{the results of a}} search for an excess of high energy upward-going muons among the sample of data collected by MACRO during ~ 5. 8 years of <b>effective</b> <b>running</b> <b>time.</b> No significant evidence for this signal was found. As a consequence, an upper limit on the flux of upward-going muons from high-energy neutrinos was set at the level of 1. 7 10 ^(- 14) cm^(- 2) s^(- 1) sr^(- 1). The corresponding upper limit for the diffuse neutrino flux was evaluated assuming a neutrino power law spectrum. Our result was compared with theoretical predictions and upper limits from other experiments. Comment: 19 pages, 8 figures, 2 table...|$|E
40|$|A {{simulator}} for parallel applications with dynamically varying {{compute node}} allocation Dynamically allocating computing nodes to parallel applications is a promising technique {{for improving the}} utilization of cluster resources. We introduce the concept of dynamic efficiency which expresses the resource utilization efficiency {{as a function of}} time. We propose a simulation framework which enables predicting the dynamic efficiency of a parallel application. It relies on the DPS parallelization framework to which we add direct execution simulation capabilities. The high level flow graph description of DPS applications enables the accurate simulation of parallel applications without needing to modify the application code. Thanks to partial direct execution, simulation times and memory requirements may be reduced. In simulations under partial direct execution, the application's parallel behavior is simulated thanks to direct execution, and the duration of individual operations is obtained from a performance prediction model or from prior measurements. We verify the accuracy of our simulator by comparing the <b>effective</b> <b>running</b> <b>time,</b> respectively the dynamic efficiency, of parallel program executions with the running time, respectively the dynamic efficiency, predicted by the simulator. These comparisons are performed for an LU factorization application under different parallelization and dynamic node allocation strategies. 1...|$|E
40|$|The {{influence}} of network construction on graphbased semi-supervised learning (SSL) and their related applications have only received limited study despite its critical impact on accuracy. We introduce four variants for network construction for SSL that adopt different network topology: 1) S-kNN (Sequential k-Nearest Neighbors) that generates regular networks; 2) GBILI (Graph Based on the informativeness of Labeled Instances) and 3) RGCLI (Robust Graph that Considers Labeled Instances), which exploit the labels available generating scale-free networks; 4) GBLP (Graph Based on Link Prediction), {{which are based}} on link prediction measures and creates smallworld networks. Comprehensive experimental results using several benchmark datasets show that it can achieve or outperform existing state-of-the-art results. Furthermore, it is confirmed to be more <b>effective</b> in <b>running</b> <b>time.</b> São Paulo Research Foundation (FAPESP) (grant: 2011 / 21880 - 3...|$|R
5000|$|... "That's the dilemma. The run-and-shoot is {{not going}} to be real {{effective}} if you're not going to have an <b>effective</b> <b>running</b> back. If you have an <b>effective</b> <b>running</b> back, like Detroit, it's going to create problems." [...] - Philadelphia Eagles Defensive Coordinator Jeff Fisher in 1990 ...|$|R
5000|$|Providing {{services}} and facilities {{to ensure the}} efficient and <b>effective</b> <b>running</b> of Parliament ...|$|R
40|$|In {{this paper}} we discuss {{ways to reduce the}} {{execution}} time of a software Global Navigation Satellite System (GNSS) receiver that is meant for offline operation in a cloud environment. Client devices record satellite signals they receive, and send them to the cloud, to be processed by this software. The goal of this project is for each client request to be processed as fast as possible, but also to increase total system throughput by making sure as many requests as possible are processed within a unit of time. The characteristics of our application provided both opportunities and challenges for increasing performance. We describe the speedups we obtained by enabling the software to exploit multi-core CPUs and GPGPUs. We mention which techniques worked for us and which did not. To increase throughput, we describe how we control the resources allocated to each invocation of the software to process a client request, such that multiple copies of the application can run at the same time. We use the notion of <b>effective</b> <b>running</b> <b>time</b> to measure the system's throughput when running multiple instances at the same time, and show how we can determine when the system's computing resources have been saturated...|$|E
40|$|Dynamically allocating {{computing}} nodes to parallel applications is {{a promising}} technique {{for improving the}} utilization of cluster resources. We introduce the concept of dynamic efficiency which expresses the resource utilization efficiency {{as a function of}} time. We propose a simulation framework which enables predicting the dynamic efficiency of a parallel application. It relies on the DPS parallelization framework to which we add direct execution simulation capabilities. The high level flow graph description of DPS applications enables the accurate simulation of parallel applications without needing to modify the application code. Thanks to partial direct execution, simulation times and memory requirements may be reduced. In simulations under partial direct execution, the application's parallel behavior is simulated thanks to direct execution, and the duration of individual operations is obtained from a performance prediction model or from prior measurements. We verify the accuracy of our simulator by comparing the <b>effective</b> <b>running</b> <b>time,</b> respectively the dynamic efficiency, of parallel program executions with the running time, respectively the dynamic efficiency, predicted by the simulator. These comparisons are performed for an LU factorization application under different parallelization and dynamic node allocation strategies. 1...|$|E
40|$|High {{statistics}} calorimetric {{measurements of}} the beta spectrum of 187 Re are being performed with arrays of silver perrhenate crystals operated at low temperature. After a modification of the experimental set-up, which allowed to substantially reduce the background of spurious counts and therefore to increase the sensitivity on the electron antineutrino mass, a new measurement with 10 silver perrhenate microbolometers is running since July 2002. The crystals have masses between 250 and 350 micrograms and their average FWHM energy resolution, constantly monitored by means of fluorescence X-rays, is of 28. 3 eV at the beta end-point. The Kurie plot collected during 4485 hours x mg <b>effective</b> <b>running</b> <b>time</b> has an end-point energy of 2466. 1 +/- 0. 8 {stat} +/- 1. 5 {syst} eV, while the half lifetime of the decay {{is found to be}} 43. 2 +/- 0. 2 {stat} +/- 0. 1 {syst} Gy. These values are the most precise obtained so far for 187 Re. From the fit of the Kurie plot we can deduce a value for the squared electron antineutrino mass m(nu) ^ 2 of 147 +/- 237 {stat} +/- 90 {syst} eV^ 2. The corresponding 90 % C. L. upper limit for m(nu) is 21. 7 eV. Comment: 3 pages, 3 figures. Submitted to Phys. Rev. Let...|$|E
5000|$|Planning for the {{structure}} and facilities for the <b>effective</b> <b>running</b> of the University, ...|$|R
5000|$|Episodes airing on Monday had a <b>running</b> <b>time</b> of two hours. Episodes airing on Tuesday & Sunday had a <b>running</b> <b>time</b> of one hour. Episodes airing on Monday had a <b>running</b> <b>time</b> of two hours. Episodes airing on Tuesday & Sunday had a <b>running</b> <b>time</b> of one hour.|$|R
50|$|There were a {{range of}} paid and {{voluntary}} staff positions to ensure the smooth and <b>effective</b> <b>running</b> of the system.|$|R
40|$|Dynamically allocating {{computing}} nodes to parallel applications is {{a promising}} technique {{for improving the}} utilization of cluster resources. Detailed simulations can help identify allocation strategies and problem decomposition parameters that increase the efficiency of parallel applications. We describe a simulation framework supporting dynamic node allocation which, given a simple cluster model, predicts the running time of parallel applications taking CPU and network sharing into account. Simulations {{can be carried out}} without needing to modify the application code. Thanks to partial direct execution, simulation times and memory requirements are reduced. In partial direct execution simulations, the application's parallel behavior is retrieved via direct execution, and the duration of individual operations is obtained from a performance prediction model or from prior measurements. Simulations may then vary cluster model parameters, operation durations and problem decomposition parameters to analyze their impact on the application performance and identify the limiting factors. We implemented the proposed techniques by adding direct execution simulation capabilities to the Dynamic Parallel Schedules parallelization framework. We introduce the concept of dynamic efficiency to express the resource utilization efficiency as a function of time. We verify the accuracy of our simulator by comparing the <b>effective</b> <b>running</b> <b>time,</b> respectively the dynamic efficiency, of parallel program executions with the running time, re...|$|E
40|$|Ultra-endurance {{exercise}} {{is associated with}} stresses that mechanically damage muscle cells and lead to injured proteins and organelles. Previous investigations provided evidence that ultra-endurance {{exercise is}} associated with a coordinated up-regulation of the ubiquitin-proteasome and autophagy-lysosomal proteolytic pathways (Jamart et al., 2012). The purpose of the present investigation was to assess the resulting modifications in the muscle proteome. Ten men, experienced ultra-endurance athletes ran for 24 h on a treadmill. Muscle biopsy samples were taken from the vastus lateralis muscle 2 h before starting and immediately after finishing exercise. Athletes ran 150 + 16 km with an <b>effective</b> <b>running</b> <b>time</b> of 18 h: 42 min (± 41 min). Label-free quantitative protein profiling (‘Shot-Gun’) was performed (Théron et al., 2014) to quantify and compare proteomes before and after ultra-endurance running. Shot-Gun proteomics of the Human muscle homogenate identified 633 proteins, and among them 96 were differentially expressed after ultra-endurance running. Most of the proteins were under-represented after exercise. Functional interaction networks indicated that ultra-endurance strongly altered the mitochondrial proteome suggesting enhanced mitophagy. Our results also revealed important modifications related to the cytoskeleton, cytodetoxification, proteostasis and membrane repair. This study describes the most extensive proteomic analysis of Human muscle adaptation to ultra-endurance exercise. Many potential biomarkers may represent novel starting points to elucidate the mechanisms of muscle adaptation to extreme exercis...|$|E
40|$|AbstractDynamically allocating {{computing}} nodes to parallel applications is {{a promising}} technique {{for improving the}} utilization of cluster resources. Detailed simulations can help identify allocation strategies and problem decomposition parameters that increase the efficiency of parallel applications. We describe a simulation framework supporting dynamic node allocation which, given a simple cluster model, predicts the running time of parallel applications taking CPU and network sharing into account. Simulations {{can be carried out}} without needing to modify the application code. Thanks to partial direct execution, simulation times and memory requirements are reduced. In partial direct execution simulations, the application's parallel behavior is retrieved via direct execution, and the duration of individual operations is obtained from a performance prediction model or from prior measurements. Simulations may then vary cluster model parameters, operation durations and problem decomposition parameters to analyze their impact on the application performance and identify the limiting factors. We implemented the proposed techniques by adding direct execution simulation capabilities to the Dynamic Parallel Schedules parallelization framework. We introduce the concept of dynamic efficiency to express the resource utilization efficiency as a function of time. We verify the accuracy of our simulator by comparing the <b>effective</b> <b>running</b> <b>time,</b> respectively the dynamic efficiency, of parallel program executions with the running time, respectively the dynamic efficiency, predicted by the simulator under different parallelization and dynamic node allocation strategies...|$|E
50|$|We {{can further}} improve upon this algorithm, by iteratively merging the two {{shortest}} arrays. It {{is clear that}} this minimizes the <b>running</b> <b>time</b> and can therefore not be worse than the strategy described in the previous paragraph. The <b>running</b> <b>time</b> is therefore in O(n log k). Fortunately, in border cases the <b>running</b> <b>time</b> can be better. Consider for example the degenerate case, where all but one array contain only one element. The strategy explained in the previous paragraph needs Θ(n log k) <b>running</b> <b>time,</b> while the improved one only needs Θ(n) <b>running</b> <b>time.</b>|$|R
40|$|We give {{improved}} algorithms {{for constructing}} minimum directed and undirected cycle bases in graphs. For general graphs, the new algorithms are Monte Carlo and have <b>running</b> <b>time</b> O(m ω), where ω is the exponent of matrix multiplication. The previous best algorithm had <b>running</b> <b>time</b> Õ(m² n). For planar graphs, the new algorithm is deterministic and has <b>running</b> <b>time</b> O(n²). The previous best algorithm had <b>running</b> <b>time</b> O(n² logn). A key ingredient to our improved <b>running</b> <b>times</b> is the insight {{that the search}} for minimum bases can be restricted {{to a set of}} candidate cycles of total length O(nm) ...|$|R
500|$|Algorithms with <b>running</b> <b>time</b> [...] are {{sometimes}} called linearithmic. Some examples of algorithms with <b>running</b> <b>time</b> [...] or [...] are: ...|$|R
40|$|ORGANIZING COMMITTEEChairs: Didier Attaix - Lydie Combaret - Daniel TaillandierDaniel Béchet - Agnès Claustre - Cécile Coudy-Gandilhon - Christiane Deval - Gérard Donadille - Cécile PolgeSCIENTIFIC COMMITTEEDidier Attaix - Lydie Combaret - Alfred L. Goldberg - Ron Hay - Germana Meroni - Marco Sandri - Daniel Taillandier - Keiji Tanaka - Simon S. WingSession 3 - AutophagyUltra-endurance {{exercise}} {{is associated with}} stresses that mechanically damage muscle cells and lead to injured proteins and organelles. Previous investigations provided evidence that ultra-endurance {{exercise is}} associated with a coordinated up-regulation of the ubiquitin-proteasome and autophagy-lysosomal proteolytic pathways (Jamart et al., 2012). The purpose of the present investigation was to assess the resulting modifications in the muscle proteome. Ten men, experienced ultra-endurance athletes ran for 24 h on a treadmill. Muscle biopsy samples were taken from the vastus lateralis muscle 2 h before starting and immediately after finishing exercise. Athletes ran 150 + 16 km with an <b>effective</b> <b>running</b> <b>time</b> of 18 h: 42 min (± 41 min). Label-free quantitative protein profiling (‘Shot-Gun’) was performed (Théron et al., 2014) to quantify and compare proteomes before and after ultra-endurance running. Shot-Gun proteomics of the Human muscle homogenate identified 633 proteins, and among them 96 were differentially expressed after ultra-endurance running. Most of the proteins were under-represented after exercise. Functional interaction networks indicated that ultra-endurance strongly altered the mitochondrial proteome suggesting enhanced mitophagy. Our results also revealed important modifications related to the cytoskeleton, cytodetoxification, proteostasis and membrane repair. This study describes the most extensive proteomic analysis of Human muscle adaptation to ultra-endurance exercise. Many potential biomarkers may represent novel starting points to elucidate the mechanisms of muscle adaptation to extreme exercis...|$|E
40|$|In this study, the coordinated {{activation}} of ubiquitin-proteasome pathway (UPP), autophagy-lysosomal pathway (ALP), and mitochondrial remodeling including mitophagy {{was assessed by}} measuring protein markers during ultra-endurance running exercise in human skeletal muscle. Eleven male, experienced ultra-endurance athletes ran for 24 h on a treadmill. Muscle biopsy samples {{were taken from the}} vastus lateralis muscle 2 h before starting and immediately after finishing exercise. Athletes ran 149. 8 ± 16. 3 km with an <b>effective</b> <b>running</b> <b>time</b> of 18 h 42 min (± 41 min). The phosphorylation state of Akt (- 74 ± 5 %; P < 0. 001), FOXO 3 a (- 49 ± 9 %; P < 0. 001), mTOR Ser 2448 (- 32 ± 14 %; P = 0. 028), and 4 E-BP 1 (- 34 ± 7 %; P < 0. 001) was decreased, whereas AMPK phosphorylation state increased by 247 ± 170 % (P = 0. 042). Proteasome β 2 subunit activity increased by 95 ± 44 % (P = 0. 028), whereas the activities associated with the β 1 and β 5 subunits remained unchanged. MuRF 1 protein level increased by 55 ± 26 % (P = 0. 034), whereas MAFbx protein and ubiquitin-conjugated protein levels did not change. LC 3 bII increased by 554 ± 256 % (P = 0. 005), and the form of ATG 12 conjugated to ATG 5 increased by 36 ± 17 % (P = 0. 042). The mitochondrial fission marker phospho-DRP 1 increased by 110 ± 47 % (P = 0. 003), whereas the fusion marker Mfn 1 and the mitophagy markers Parkin and PINK 1 remained unchanged. These results fit well with a coordinated regulation of ALP and UPP triggered by FOXO 3 and AMPK during ultra-endurance exercise...|$|E
40|$|We present {{improved}} <b>running</b> <b>times</b> for a {{wide range}} of approximate high dimensional proximity problems. We obtain subquadratic <b>running</b> <b>time</b> for each of these problems. These improved <b>running</b> <b>times</b> are obtained by reduction to Nearest Neighbour queries. The problems we consider in this paper are Approximate Diameter...|$|R
5000|$|The album {{lists the}} <b>running</b> <b>time</b> of [...] "I Want You" [...] as 4:07, {{but the actual}} <b>running</b> <b>time</b> is 3:44.|$|R
5000|$|Episodes airing on Monday had a <b>running</b> <b>time</b> of two hours. Episodes airing on Tuesday had a <b>running</b> <b>time</b> of one hour.|$|R
25|$|The <b>running</b> <b>time</b> of the {{algorithm}} {{and the space}} complexity is , {{much better than the}} O(n) <b>running</b> <b>time</b> of the naive brute force calculation.|$|R
50|$|Format. Each angle {{has its own}} title. The <b>running</b> <b>time</b> {{for each}} angle is 38 minutes. with a {{combining}} <b>running</b> <b>time</b> of 114 minutes.|$|R
5000|$|Episodes airing on Monday had a <b>running</b> <b>time</b> of two hours. Episodes airing on Tuesday & Sunday had a <b>running</b> <b>time</b> of one hour.|$|R
50|$|The <b>running</b> <b>time</b> of the {{algorithm}} {{and the space}} complexity is , {{much better than the}} O(n) <b>running</b> <b>time</b> of the naive brute force calculation.|$|R
25|$|The {{most direct}} {{competitor}} of quicksort is heapsort. Heapsort's <b>running</b> <b>time</b> is , but heapsort's average <b>running</b> <b>time</b> is usually considered slower than in-place quicksort. This result is debatable; some publications indicate the opposite. Introsort is {{a variant of}} quicksort that switches to heapsort when a bad case is detected to avoid quicksort's worst-case <b>running</b> <b>time.</b>|$|R
40|$|Pruhs and Woeginger {{prove the}} {{existence}} of FPTAS's for a general class of minimization and maximization subset selection problems. Without losing generality from the original framework, we prove how better asymptotic worst-case <b>running</b> <b>times</b> can be achieved if a ρ-approximation algorithm is available, and in particular we obtain matching <b>running</b> <b>times</b> between maximization and minimization subset selection problems. We directly apply this result to the Minimum Knapsack Problem, for which the original framework yields an FPTAS with <b>running</b> <b>time</b> O(n^ 5 /ϵ), where ϵ is the required accuracy and n {{is the number of}} items, and obtain an FPTAS with <b>running</b> <b>time</b> O(n^ 3 /ϵ), thus improving the <b>running</b> <b>time</b> by a quadratic factor in the worst case...|$|R
5000|$|... "The Psychiatrist" [...] is {{the longest}} Fawlty Towers episode, with a <b>running</b> <b>time</b> of 36:16. The other episodes have a <b>running</b> <b>time</b> of {{approximately}} 30 minutes.|$|R
50|$|The {{most direct}} {{competitor}} of quicksort is heapsort. Heapsort's <b>running</b> <b>time</b> is O(n log n), but heapsort's average <b>running</b> <b>time</b> is usually considered slower than in-place quicksort. This result is debatable; some publications indicate the opposite. Introsort is {{a variant of}} quicksort that switches to heapsort when a bad case is detected to avoid quicksort's worst-case <b>running</b> <b>time.</b>|$|R
50|$|Jakobsen & Knudsen's {{higher order}} {{differential}} cryptanalysis breaks KN-Cipher with only 512 chosen plaintexts and 241 <b>running</b> <b>time,</b> or with 32 chosen plaintexts and 270 <b>running</b> <b>time.</b>|$|R
5000|$|The <b>running</b> <b>time</b> of Schreier-Sims varies on the implementation. Let [...] {{be given}} by [...] generators. For the {{deterministic}} {{version of the}} algorithm, possible <b>running</b> <b>times</b> are: ...|$|R
5000|$|Most album {{configurations}} {{contain the}} hidden track [...] "Cosmic Christmas" [...] (<b>running</b> <b>time</b> 0:35) following [...] "Sing This All Together (See What Happens)" [...] (<b>running</b> <b>time</b> 7:58).|$|R
5000|$|The {{original}} {{issues of}} this album accidentally indicates the <b>running</b> <b>time</b> of [...] "Yesterday" [...] as being 1:04 in length. Later issues have the correct 2:04 <b>running</b> <b>time.</b>|$|R
30|$|As a result, in Figs.  9 and 10, a weak {{performance}} is delivered by Pig in every <b>running</b> <b>time</b> performance of Join benchmark compared to its high performance in web log processing. While comparing these MapReduce-based HLQL {{at the level}} of the <b>running</b> <b>time,</b> we find that the Join benchmark of the MR java, Hive and Pig take almost the same execution time (Fig.  10). On one hand, JAQL takes a long <b>running</b> <b>time.</b> On the other hand, Big SQL is characterized by less <b>running</b> <b>time</b> in increasing input size metric.|$|R
