69|10000|Public
25|$|Hemoglobin can {{be tracked}} noninvasively, {{to build an}} <b>individual</b> <b>data</b> <b>set</b> {{tracking}} the hemoconcentration and hemodilution effects of daily activities for better understanding of sports performance and training. Athletes are often concerned about endurance and intensity of exercise. Using the scientific technique of absorption spectroscopy with eight wavelengths of light. This method {{is similar to a}} pulse oximeter, which consists of a small sensing device that clips to the finger. The sensor uses light-emitting diodes that emit red and infrared light through the tissue to a light detector, which then sends a signal to a processor to calculate the absorption of light by the hemoglobin protein.|$|E
5000|$|... so that aT {{is now in}} {{a linear}} {{position}} with all other terms known, and thus can be analyzed by linear regression techniques. For more than one parameter the method extends in a direct manner. After checking that the model has been improved this process can be repeated until convergence. This approach has the advantages that it does not need the parameters q {{to be able to be}} determined from an <b>individual</b> <b>data</b> <b>set</b> and the linear regression is on the original error terms ...|$|E
50|$|MIDAWs allow ECKD channel {{programs}} {{to read and}} write to many storage locations using one channel command, which means fewer signals up and down the channel are required to transfer the same amount of data. This reduction is particularly noticeable for Extended Format data sets, accessed through Media Manager. Examples include Extended Format Sequential data sets, Extended Format VSAM data sets and certain types of DB2 tablespaces. While each of these data set organizations have alternatives, each has a distinct set of advantages, whether in the area of performance, space saving (through hardware-assisted data compression), or scalability (by allowing an <b>individual</b> <b>data</b> <b>set</b> to exceed 4 GiB).|$|E
40|$|Cognitive mapping {{involves}} {{the description of}} the way individuals store and process geographic information. Typically, cognitive mapping data are derived from individual responses and then analyzed in one of three ways: (1) the <b>individual</b> <b>data</b> <b>sets</b> are analyzed separately and only pooled for comparison (disaggregation); (2) the <b>individual</b> <b>data</b> <b>sets</b> are averaged and then analyzed (collective aggregation); or (3) the <b>individual</b> <b>data</b> <b>sets</b> are analyzed and the results averaged (individual aggregation). This paper compares the latter two aggregation strategies for analyzing cognitive mapping data using data collected in a large-scale study of students’ configurational knowledge of the city of Swansea in the United Kingdom. It is contended that the aggregation strategy adopted will have a fundamental effect upon the conclusions drawn from a study...|$|R
40|$|The {{analysis}} {{focuses on}} immigrants and native-born individuals because employers {{are likely to}} have less reliable signals of productivity for an immigrant than a native-born <b>individual.</b> Using multiple <b>data</b> <b>sets,</b> the paper presents a robust empirical finding that the wage gains associated with height are almost twice as large for immigrants than for native-born individuals. [Working Paper No. 289]. statistical discrimination, native-born, employers, productivity, immigrants, wage, height, <b>individuals,</b> <b>data</b> <b>sets,</b> economic research,...|$|R
40|$|Land {{evapotranspiration}} (ET) {{estimates are}} available from several global <b>data</b> <b>sets.</b> Here, Monthly Global Land et Synthesis Products, Merged from These <b>Individual</b> <b>Data</b> <b>Sets</b> over the Time Periods 1989 - 1995 (7 Yr) and 1989 - 2005 (17 Yr), Are Presented. the Merged Synthesis Products over the Shorter Period Are Based on A Total of 40 Distinct <b>Data</b> <b>Sets</b> while Those over the Longer Period Are Based on A Total of 14 <b>Data</b> <b>Sets.</b> in the <b>Individual</b> <b>Data</b> <b>Sets,</b> et Is Derived from Satellite And/or in Situ Observations (Diagnostic <b>Data</b> <b>Sets)</b> or Calculated Via Land-surface Models (LSMs) Driven with Observations-based Forcing or Output from Atmospheric Reanalyses. Statistics for Four Merged Synthesis Products Are Provided, One Including All <b>Data</b> <b>Sets</b> and Three Including only <b>Data</b> <b>Sets</b> from One Category Each (Diagnostic, LSMs, and Reanalyses). the Multi-annual Variations of et in the Merged Synthesis Products Display Realistic Responses. They Are Also Consistent with Previous Findings of A Global Increase in et between 1989 and 1997 (0. 13 Mm y...|$|R
50|$|Hemoglobin can {{be tracked}} noninvasively, {{to build an}} <b>individual</b> <b>data</b> <b>set</b> {{tracking}} the hemoconcentration and hemodilution effects of daily activities for better understanding of sports performance and training. Athletes are often concerned about endurance and intensity of exercise. Using the scientific technique of absorption spectroscopy with eight wavelengths of light. This method {{is similar to a}} pulse oximeter, which consists of a small sensing device that clips to the finger. The sensor uses light-emitting diodes that emit red and infrared light through the tissue to a light detector, which then sends a signal to a processor to calculate the absorption of light by the hemoglobin protein.|$|E
50|$|Cluster {{analysis}} {{itself is}} not one specific algorithm, but the general task to be solved. It {{can be achieved by}} various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances among the cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including values such as the distance function to use, a density threshold or the number of expected clusters) depend on the <b>individual</b> <b>data</b> <b>set</b> and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.|$|E
5000|$|A {{number of}} expert {{witnesses}} to the Digital Economy Bill Committee expressed concerns about the bill. Jerry Fishenden, co-chair of the Cabinet Office’s Privacy and Consumer Advisory Group until he resigned in protest on 2 May 2017, expressed the opinion that the bill {{was based on an}} [...] "obsolete" [...] model of data sharing. He commented: [...] "I find it surprising the bill doesn’t have definition of what data sharing is, both practically and legally… I’d like to see some precision around what’s meant by data sharing. The lack of detail is concerning." [...] He also said that the bill [...] "appears to weaken citizens’ control over their personal data", something that is [...] "likely to undermine trust in government and make citizens less willing to share their personal data". Jeni Tennison, CEO of the Open Data Institute, commented on the lack of transparency regarding existing public sector data sharing agreements and how the bill's measures fit with them. She spoke of her belief that the bill lacks the transparency needed to avoid the kind of problems that arose with NHS Digital's abandoned Care.data programme. Mike Bracken, chief digital officer at the Co-operative Group and former head of the Government Digital Service, expressed the opinion that [...] "the government relies on bulk data sets too often, instead of simply asking for the <b>individual</b> <b>data</b> <b>set</b> pertaining to the information needed". The civil liberties and privacy advocacy group Big Brother Watch told the committee said that bill overlooked the work of the Government Digital Service in setting up the GOV.UK Verify scheme, a model based on the government not centrally storing data.|$|E
5000|$|Another way is by {{distinguishing}} {{the techniques}} by G. Carlsson, one being {{the study of}} homological invariants of <b>data</b> one <b>individual</b> <b>data</b> <b>sets,</b> {{and the other is}} the use of homological invariants in the study of databases where the data points themselves have geometric structure.|$|R
5000|$|Most window {{functions}} afford {{more influence}} {{to the data}} {{at the center of}} the <b>set</b> than to <b>data</b> at the edges, which represents a loss of information. To mitigate that loss, the <b>individual</b> <b>data</b> <b>sets</b> are commonly overlapped in time (as in the above step).|$|R
5000|$|FlashCopy Version 2 {{introduced}} {{the ability to}} flash <b>individual</b> <b>data</b> <b>sets</b> and then added support for “consistency groups”. FlashCopy consistency groups {{can be used to}} help create a consistent point-in-time copy across multiple volumes, and even across multiple ESSs, thus managing the consistency of dependent writes.|$|R
40|$|The probit {{estimation}} using a large <b>individual</b> <b>data</b> <b>set</b> {{associated with}} the Korean economic crisis in 1997 shows that the suicidal ideation demonstrates strong response to economic crisis, depression and stress. The elasticity of income on suicide risk is greater for males than females and greater for youths and adults than olds. ...|$|E
30|$|One cohort was {{excluded}} at each time {{to investigate the}} influence of the <b>individual</b> <b>data</b> <b>set</b> on the overall results. The statistical significance of the overall results was not altered when any single study {{was excluded}}, confirming the stability of the results. However, P for Q test could reduce significantly when excluding the study of Li et al. (Li et al. 2013) and Song et al. (Song et al. 2013).|$|E
30|$|For {{exploration}} of heterogeneity, subgroup {{analyses were performed}} based on anatomic location, pathological subtype, ethnicity, source of control, sample size (≤ 1000 and > 1000 subjects), quality scores (score > 7 or ≤ 7) adjustment and statistical power. Sensitivity analyses were performed by deletion of a single study each time to reflect {{the influence of the}} <b>individual</b> <b>data</b> <b>set</b> on the pooled ORs. Begg’s funnel plots (Egger et al. 1997) were used to assess the publication bias. Statistical analyses were conducted using Review manager Version 5.1 (Copenhagen, The Nordic Cochrane Centre, The Cochrane Collaboration, 2011).|$|E
40|$|Thermal {{conductivity}} data acquired previously for {{the establishment}} of Standard Reference Material (SRM) 1450, Fibrous Glass Board, as well as subsequent renewals 1450 a, 1450 b, 1450 c, and 1450 d, are re-analyzed collectively and as <b>individual</b> <b>data</b> <b>sets.</b> Additional <b>data</b> <b>sets</b> for proto- 1450 material lots are also included in the analysis. The data cover 36 years of activity by the National Institute of Standards and Technology (NIST) in developing and providing thermal insulation SRMs, specifically high-density molded fibrous-glass board, to the public. Collectively, the <b>data</b> <b>sets</b> cover two nominal thicknesses of 13 mm and 25 mm, bulk densities from 60 kg∙m- 3 to 180 kg∙m- 3, and mean temperatures from 100 K to 340 K. The analysis repetitively fits six models to the <b>individual</b> <b>data</b> <b>sets.</b> The most general form of the nested set of multilinear models used is given in the following equation:...|$|R
40|$|We use {{observations}} of {{cosmic microwave background}} anisotropies, supernova luminosities and the baryon acoustic oscillation signal in the galaxy distribution to constrain the cosmological parameters in a simple interacting dark energy model with a time-varying equation of state. Using a Monte Carlo Markov Chain technique we determine the posterior likelihoods. Constraints from the <b>individual</b> <b>data</b> <b>sets</b> are weak, but {{the combination of the}} three <b>data</b> <b>sets</b> confines the interaction constant Γ to be less than 23...|$|R
40|$|Abstract — This paper {{presents}} {{a novel approach}} to data fusion for stochastic processes that model spatial data. It addresses the problem of data fusion {{in the context of}} large scale terrain modeling for a mobile robot. Building a model of large scale and complex terrain that can adequately handle uncertainty and incompleteness in a statistically sound manner is a very challenging problem. To obtain a comprehensive model of such terrain, typically, multiple sensory modalities as well as multiple <b>data</b> <b>sets</b> are required. This work uses Gaussian processes to model large scale terrain. The model naturally provides a multi-resolution representation of space, incorporates and handles uncertainties appropriately and copes with incompleteness of sensory information. Gaussian process regression techniques are applied to estimate and interpolate (to fill gaps in unknown areas) elevation information across the field. In this work, the GP modeling approach is extended to fuse multiple, multi-modal <b>data</b> <b>sets</b> to obtain a best estimate of the elevation given the <b>individual</b> <b>data</b> <b>sets.</b> The <b>individual</b> <b>data</b> <b>sets</b> are treated as different noisy samples of the same underlying terrain. Experiments performed on sparse GPS based survey data and dense laser scanner data taken at minesites are reported. I...|$|R
40|$|The {{rationale}} for the objective assessment of dose-response curves (DRCs) is presented. Using data derived from isoprenaline/heart rate responses studies, two new statistical methods of objectively defining the terminal linear segment of an incomplete DRC are presented. Using data derived from phenylephrine/diastolic blood pressure response studies, the parallel shift quadratic model of Sumner et al. (1982) has been extended to include {{a measure of the}} suitability of the quadratic model for each <b>individual</b> <b>data</b> <b>set</b> using the Akaike information criterion. A parallel shift Emax model is proposed for complete DRCs...|$|E
40|$|The Surface Ocean CO 2 Atlas (SOCAT), an {{activity}} of the international marine carbon research community, provides access to synthesis and gridded f CO 2 (fugacity of carbon dioxide) products for the surface oceans. Version 2 of SOCAT is an update of the previous release (version 1) with more data (increased from 6. 3 million to 10. 1 million surface water f CO 2 values) and extended data coverage (from 1968 – 2007 to 1968 – 2011). The quality control criteria, while identical in both versions, have been applied more strictly in version 2 than in version 1. The SOCAT website ([URL]) has links to quality control comments, metadata, <b>individual</b> <b>data</b> <b>set</b> files, and synthesis and gridded data products. Interactive online tools allow visitors to explore {{the richness of the}} data. Applications of SOCAT include process studies, quantification of the ocean carbon sink and its spatial, seasonal, year-to-year and longerterm variation, as well as initialisation or validation of ocean carbon models and coupled climate-carbon models. Data coverage Repository-References: <b>Individual</b> <b>data</b> <b>set</b> files and synthesis product: doi: 10. 1594 /PANGAEA. 811776 Gridded products: doi: 10. 3334 /CDIAC/OTG. SOCAT_V 2 _GRID Available at: [URL] Coverage: 79 ° S to 90 ° N; 180 ° W to 180 ° E Location Name: Global Oceans and Coastal Seas Date/Time Start: 16 November 1968 Date/Time End: 26 December 201...|$|E
40|$|This paper uses a large <b>individual</b> <b>data</b> <b>set</b> {{from the}} Euro Barometer Survey (ICPSR 1993) to {{estimate}} the influence of religious phenomena on self-perceived satisfaction of an individual, controlling for macroeconomic conditions, effects of his political stance, and other socio-economic variables. Our estimated ordered logit model results show that an individual's life satisfaction is positively related to measures of strong religious attachment {{in the sense of}} being willing to commit to attending religious services frequently. Our other findings include that no strong evidence exists for the hypothesis that leftists suffer more from income inequality. Religious phenomena, self-perceived satisfaction, Euro barometer survey, ordered logit model,...|$|E
40|$|Independently {{produced}} <b>data</b> <b>sets</b> {{that describe}} the four-dimensional temperature structure {{from the surface}} through the lower stratosphere provide different temperature trends. These differences are seen in varying degrees in comparisons of separate in situ (surface and weather balloon) <b>data</b> <b>sets,</b> in comparisons of separate space-based <b>data</b> <b>sets,</b> and in comparisons of <b>individual</b> <b>data</b> <b>sets</b> drawn from the different observational platforms and different trend analysis teams. This CCSP synthesis product will address the accuracy and consistency of these temperature records and outline steps necessary to reconcile differences between <b>individual</b> <b>data</b> <b>sets.</b> Understanding exactly how and why {{there are differences in}} temperature trends reported by several analysis teams using differing observation systems and analysis methods represents a necessary step in reducing the uncertainties that underlie current efforts focused on the detection and quantification of surface and tropospheric temperature trends. Consequently, this synthesis product promises to be of significant value to decisionmakers, and to the expert scientific and stakeholder communities. For example, we expect this assessment to be a major contributor to the IPCC (2007) Climate Assessment. In addition, we expect the information generated will be used by the Global Climate Observing System Atmospheric Observation Panel to help identify effective ways to reduce observational uncertainty...|$|R
40|$|This work is devoded to {{evolutionary}} algorithms and {{solution of}} global optimization problems, mainly the traveling salesman problem. The traveling salesman problem is analyzed in detail {{as well as}} its methods of solution, such as: graph theory, heuristics and evolutionary algorithms. The main optimization method of this work is a Inver - over operator. In conclusion are implemented selected methods and performed testing and evaluation of the <b>individual</b> <b>data</b> <b>sets...</b>|$|R
40|$|The {{invention}} {{relates to}} the analysis of successive <b>data</b> <b>sets.</b> A local intensity variation is formed from such successive <b>data</b> <b>sets,</b> that is, from data values in successive <b>data</b> <b>sets</b> at corresponding positions in each of the <b>data</b> <b>sets.</b> A region of interest is localized in the <b>individual</b> <b>data</b> <b>sets</b> {{on the basis of the}} local intensity variation. In particular the time derivative of the local intensity variation is used to localize the region of interest. The invention can be used notably for cardiological applications so as to separate the image of the myocardium from a sequence of 3 D magnetic resonance reconstruction images...|$|R
40|$|The {{importance}} of specific spectral regions to signature extension is explored. In the recent past, the signature extension task {{was focused on}} the development of new techniques. Tested techniques are now used to investigate this spectral aspect of the large area survey. Sets of channels were sought which, for a given technique, were the least affected by several sources of variation over four data sets and yet provided good object class separation on each <b>individual</b> <b>data</b> <b>set.</b> Using sets of channels determined as part of this study, signature extension was accomplished between data sets collected over a six-day period and over a range of about 400 kilometers...|$|E
30|$|The {{processing}} technique applied on <b>individual</b> <b>data</b> <b>set</b> is {{the same}} as for the candidate (Hamoudi et al. 2007) of the first WDMAM version. The reader should report to this publication for more details. The data quality over each data set is difficult to estimate as complete metadata are rarely available. Most provided compilations result from putting together smaller surveys that were flown at various altitudes and epochs. In some compilations, these individual panels were not properly upward continued to a common altitude. For other compilations, this altitude and epoch information is provided but as a general rule, the mean altitudes, or the mean terrain clearances, are not known. Furthermore, the panels inside each <b>individual</b> <b>data</b> <b>set</b> have been reduced with IGRF/DGRF-like models or alternatively with local polynomials; but in most cases, it is difficult to find out which model was used to reduce the data. However, we had no other choice but to estimate these altitudes and reference model and continue the data processing with the estimated values. Finally, the compilations are provided in different format, coordinate systems and projections, and we did our best to account for these. Overall, the final patch-worked compilations are prone to mismatch in anomaly shapes and strengths that may easily be confused with magnetic anomalies. Moreover, the lack of absolute reference makes it difficult to restore the large wavelengths: these should be regarded with caution. A complete processing was applied to each data set, except for Eurasia, India and Mexico that were provided partially processed.|$|E
30|$|In our experiments, {{we started}} with the {{smallest}} possible ε-distance between any two 1 -D projections of points in the given dataset. Considering the curse of dimensionality, the points become farther in high dimensional subspaces, so the user may intend to find clusters with larger ε values than that used by SUBSCALE for 1 -D subspaces. Most of the subspace clustering algoritms use heuristics to adjust these parameters in higher dimensional subspaces. Some authors [49, 50] have suggested the methods to adapt ε and τ parameters for the high dimensional subspaces. However, we {{would argue that the}} choice of these density parameters is highly subjective to the <b>individual</b> <b>data</b> <b>set</b> as well as the user requirements.|$|E
40|$|The {{problem of}} {{measurement}} and welfare implications of intergenerational transmission of inequality is studied. A possible decomposition between educational attainments {{and other factors}} is proposed and applied to three <b>individual</b> <b>data</b> <b>sets</b> regarding Germany, Italy and United States. The main result is that educational attainment is responsible of almost half of observed immobility. Whether increasing equality of opportunity in entering the educational system can result in less inequality in income distribution is considered...|$|R
40|$|A {{technique}} {{has been developed}} for the simultaneous analysis of several powder diffraction data {{on the basis of}} the Rietveld method. Counting rates from one specimen at a given temperature taken at neutron, synchrotron or X-ray powder diffractometers are joined to one single <b>data</b> <b>set</b> with weights given by the counting statistics. The structure is refined from this <b>data</b> <b>set</b> with a parameter field containing one structural model and individual zero points, scale factors and FWHM parameters for each of the methods and <b>data</b> <b>sets.</b> A new definition of the residuals is given. The residuals and goodness-of-fit values are calculated for all as well as for the <b>individual</b> <b>data</b> <b>sets...</b>|$|R
30|$|The {{localized}} Slepian model (S 360) fully honors both near-surface {{and satellite}} input data for improved perspectives {{on how the}} magnetic anomalies of the lithosphere may vary over the intervening altitudes. These perspectives are not available from standard downward or upward continuations of the <b>individual</b> <b>data</b> <b>sets.</b> However, these continuations represent inversion results that are not unique and subject to measurement and modeling errors so that their interpretation at locations lacking magnetic observations requires considerable care.|$|R
40|$|Previous {{research}} has shown an association between paternal age and neonatal mortality controlling for age of mother and other confounding factors. As this association is possibly of a biological nature, one would also expect to find {{a significant relationship between}} paternal age and foetal mortality, stillbirth in particular. The present research is based on a Czech <b>individual</b> <b>data</b> <b>set</b> for the period 1986 - 1990. The risk of stillbirth is examined using logistic regression, taking account of both parental ages, sex of the foetus, birth order, prematurity, and education of parents. It is shown that, in this data set, there is a significant relation between age of father and risk of stillbirth...|$|E
40|$|On {{the basis}} of an <b>individual</b> <b>data</b> <b>set</b> for 1998 / 99, this study {{examines}} whether the use of information technologies (IT) and organisational changes in companies influence the employees’ wage level. It becomes obvious that IT use is associated with an average wage effect of five to six per cent, with the effects occurring for different IT components being very heterogeneous. Organisational changes {{have a significant effect on}} wages only when they have a complementary relationship with IT use. In the case of outsourcing, the employees’ qualification level plays a significant role in addition. " (Author's abstract, IAB-Doku) ((en)) Lohnhöhe - Determinanten, Informationstechnik, organisatorischer Wandel, BIBB/IAB-Erhebung, erwerbstätige Männer, Westdeutschland, Bundesrepublik Deutschland...|$|E
40|$|Asymmetric {{information}} {{has become one}} of the most influential theoretical concepts in explaining competitive insurance market. In contract theory, asymmetric information is the situation when one party of an economic transaction possesses information that is not available to the other contractual party. In this perspective, this paper aims to investigate the presence of asymmetric information in Tunisian automobile insurance market. This investigation is made using an <b>individual</b> <b>data</b> <b>set</b> during the year 2009. For this purpose, we employ two empirical approaches; the bivariate probit model and the two stages approach of Richaudeau (1999). Hence, results show a strong evidence for the presence of asymmetric information for the begging drivers. Inversely, for the experienced drivers, no evidence of asymmetric information have been detected...|$|E
50|$|The DD {{statements}} for IEHMOVE, {{other than}} SYSPRINT and SYSIN, refer to DASD or tape volumes instead of <b>individual</b> <b>data</b> <b>sets.</b> However, referencing volumes can pose a problem, since specifying DISP=OLD gains exclusive {{access to a}} volume. Therefore, while your IEHMOVE job runs, that entire volume (and all datasets on it) is unavailable to other users. This is acceptable for private volumes, such as tape volumes or mountable volumes, but unacceptable public volumes, such as DASD volumes.|$|R
40|$|A {{recently}} developed, freely available, application {{specifically designed}} for the visualization of multimodal <b>data</b> <b>sets</b> is presented. The application allows multiple 3 D <b>data</b> <b>sets</b> such as CT (x-ray computer tomography), MRI (magnetic resonance imaging), PET (positron emission tomography), and SPECT (single photon emission tomography) of the same subject to be viewed simultaneously. This is done by maintaining synchronization of the spatial location viewed within all modalities, and by providing fused views of the data where multiple <b>data</b> <b>sets</b> are displayed as a single volume. Different options for the fused views are provided by plug-ins. Plug-ins typically used include color-overlays and interlacing, but more complex plug-ins such as those based on different color spaces, and component analysis techniques are also supported. Corrections for resolution differences and user preference of contrast and brightness are made. Pre-defined and custom color tables {{can be used to}} enhance the viewing experience. In addition to these essential capabilities, multiple options are provided for mapping 16 -bit <b>data</b> <b>sets</b> onto an 8 -bit display, including windowing, automatically and dynamically defined tone transfer functions, and histogram based techniques. The 3 D <b>data</b> <b>sets</b> can be viewed not only as a stack of images, but also as the preferred three orthogonal cross sections through the volume. More advanced volumetric displays of both <b>individual</b> <b>data</b> <b>sets</b> and fused views are also provided. This includes the common MIP (maximum intensity projection) both with and without depth correction for both <b>individual</b> <b>data</b> <b>sets</b> and multimodal <b>data</b> <b>sets</b> created using a fusion plug-in...|$|R
40|$|Natural <b>data</b> <b>sets,</b> such as {{precipitation}} records, often contain geometries {{that are}} too complex to model in their totality with classical stochastic methods. In the past years, we have developed a promising deterministic geometric procedure, the fractal-multifractal (FM) method, capable of generating patterns as projections that share textures and other fine details of <b>individual</b> <b>data</b> <b>sets,</b> {{in addition to the}} usual statistics of interest. In this paper, we formulate an extension of the FM method around the concept of “closing the loop ” by linking ends of two fractal §Corresponding author. 261 Fr ac ta l...|$|R
