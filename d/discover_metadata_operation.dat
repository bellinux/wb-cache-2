0|97|Public
5000|$|<b>Discover</b> <b>metadata</b> of {{the source}} database, {{including}} value patterns and distributions, key candidates, foreign-key candidates, and functional dependencies ...|$|R
30|$|Maximum {{number of}} file <b>metadata</b> <b>operations</b> per second (includes file creation, {{deletion}} or gathering of file information).|$|R
50|$|<b>Metadata</b> <b>operations</b> such as {{permitting}} or restricting {{access the}} a directory by various users {{or groups of}} users are usually included.|$|R
50|$|Bonnie++ {{allows you}} to {{benchmark}} how your file systems perform with respect to data read and write speed, the number of seeks that can be performed per second, {{and the number of}} file <b>metadata</b> <b>operations</b> that can be performed per second.|$|R
5000|$|The final CSV output {{includes}} the {{information related to}} data read and write speed, number of seeks that can be performed per second, and number of file <b>metadata</b> <b>operations</b> that can be performed per second and the CPU usage statistics for the below given tests: ...|$|R
40|$|Abstract—The {{demand for}} {{scalable}} I/O {{continues to grow}} rapidly as computer clusters keep growing. Much of the research in storage systems {{has been focused on}} improving the scale and performance of I/O throughput. Scalable file systems {{do a good job of}} scaling large file access bandwidth by striping or sharing I/O resources across many servers or disks. However, the same cannot be said about scaling file <b>metadata</b> <b>operation</b> rates. Most existing parallel filesystems choose to concentrate all the metadata processing load on a single server. This centralized processing can guarantee the correctness, but it severely hampers scalability. This downside is becoming more and more unacceptable as metadata throughput is critical for large scale applications. Distributing metadata processing load is critical to improve metadata scalability when handling huge number of client nodes. However, a solution to speed up <b>metadata</b> <b>operations</b> has to address two challenges simultaneously, namely the scalability and reliability. In this paper, we have designed a decentralized metadata service layer and evaluated its benefits and shortcomings that concern parallel filesystems. The main aim of this service layer is to maintain reliability and consistency in a distributed metadata environment. At the same time we also focus on improving the scalability of the <b>metadata</b> <b>operations,</b> and in turn, the scalability of the underlying parallel filesystem. As demonstrated by experiments, the approach presented in this paper achieves significant improvements over native parallel filesystems by large margin for all the major <b>metadata</b> <b>operations.</b> With 256 client processes, our decentralized metadata service outperforms Lustre and PVFS 2 by a factor of 1. 9 and 23, respectively, to create directories. With respect to stat() operation on files, our approach is 1. 3 and 3. 0 times faster than Lustre and PVFS. I...|$|R
40|$|Large HPC {{installations}} typically {{make use}} of parallel file systems that adhere to POSIX I/O conventions, and that implement a separation of data and metadata {{in order to maintain}} high performance. File systems such as GPFS and Lustre have evolved to enable an increase in data bandwidth that is primarily achieved by adding more disk drives behind an increasing number of disk controllers. Improvements in metadata performance cannot be achieved by just deploying a large volume of hardware, as the defining characteristics are the number of simultaneous operations that can be carried out and the latency of those operations. For highly scalable applications using parallel I/O libraries, the speed of <b>metadata</b> <b>operations,</b> such as opening a file on thousands of processes, has the potential to become the major bottleneck to improved I/O performance. This Metadata Wall has the ability to grow such that <b>metadata</b> <b>operations</b> can take much longer than the subsequent data operations, even on systems with very large amounts of I/O data bandwidth. We present results showing the performance of <b>metadata</b> <b>operations</b> with standard disk equipment and with solid state storage hardware, and extrapolate whether we expect the evolution in hardware alone will be sufficient to limit the effects of this I/O Metadata Wall. We also report challenges in making the metadata I/O measurements and subsequent analysis for parallel file systems...|$|R
40|$|We {{present the}} first {{systematic}} analysis of read, write, and space amplification in Linux file systems. While many researchers are tackling write amplification in key-value stores, IO amplification in file systems {{has been largely}} unexplored. We analyze data and <b>metadata</b> <b>operations</b> on five widely-used Linux file systems: ext 2, ext 4, XFS, btrfs, and F 2 FS. We find that data operations result in significant write amplification (2 - 32 X) and that <b>metadata</b> <b>operations</b> have a large IO cost. For example, a single rename requires 648 KB write IO in btrfs. We also find that small random reads result in read amplification of 2 - 13 X. Based on these observations, we present the CReWS conjecture {{about the relationship between}} IO amplification, consistency, and storage space utilization. We hope this paper spurs people to design future file systems with less IO amplification, especially for non-volatile memory technologies...|$|R
40|$|Abstract—The {{ever-increasing}} {{scale of}} modern highperformance computing (HPC) systems presents {{a variety of}} challenges to the parallel file system (PFS) based storage in these systems. The scalability of application checkpointing is a particularly important challenge because {{it is critical to}} the reliability of computing and it often dominates the I/Os in a HPC system. When a large number of parallel processes simultaneously perform checkpointing, the PFS metadata servers can become a serious bottleneck due to the large volume of concurrent <b>metadata</b> <b>operations.</b> This paper specifically addresses this PFS metadata management issue in order to support scalable application checkpointing in large HPC systems. It proposes a new technique named PFSdelegation which delegates the management of the PFS storage space used for checkpointing to applications, thereby relieving the load of <b>metadata</b> <b>operations</b> on the PFS during their checkpointing. This proposed technique is prototyped on PVFS 2, a widely used PFS implementation, and evaluated on a HPC cluster using a representative parallel I/O benchmark, IOR. Experiments with up to 128 parallel processes show that the PFS-delegation based checkpointing is significantly faster than the traditional shared-file and file-per-process based checkpointing methods (7 % and 10 % speedup when the underlying PVFS 2 uses a centralized metadata server; 22 % and 31 % speedup when using distributed metadata servers). The results also demonstrate that the PFS-delegation based checkpointing substantially reduces the total number of <b>metadata</b> <b>operations</b> handled by the metadata servers during the checkpointing...|$|R
40|$|Nowadays, Linux file {{systems have}} to manage {{millions}} of tiny files for different applications, and face with higher <b>metadata</b> <b>operations.</b> So {{how to provide}} such high metadata performance with such enormous number of files and large scale directories is a big challenge for Linux file system. We viewed that <b>metadata</b> lookup <b>operations</b> dominate <b>metadata</b> workload and incur low metadata performance. In this paper, we present a metadata cache to accelerate metadata access for Linux file system. Through this optimization, the Linux file system (such as EXT 2, EXT 4, BTRFS, etc.) can gain improvement in read rates as well as write rates. Comment: 4 pages, 8 figure...|$|R
40|$|In petabyte-scale {{distributed}} file {{systems that}} decouple {{read and write}} from <b>metadata</b> <b>operations,</b> behavior of the metadata server cluster will be critical to overall system performance. We examine aspects of the workload that {{make it difficult to}} distribute effectively, and present a few potential strategies to demonstrate the issues involved. Finally, we describe the advantages of intelligent metadata management and a simulation environment we have developed to validate design possibilities. ...|$|R
40|$|In petabyte-scale {{distributed}} file {{systems that}} decouple {{read and write}} from <b>metadata</b> <b>operations,</b> behavior of the metadata server cluster will be critical to overall system performance and scalability. We present a dynamic subtree partitioning and adaptive metadata management system designed to efficiently manage hierarchical metadata workloads that evolve over time. We examine {{the relative merits of}} our approach in the context of traditional workload partitioning strategies, and demonstrate the performance, scalability and adaptability advantages in a simulation environment. ...|$|R
5000|$|<b>Metadata</b> <b>operations</b> in XFS have {{historically}} been slower than with other file systems, resulting in, for example, poor performance with operations such as deletions {{of large numbers of}} files. However, a new XFS feature implemented by Dave Chinner and called delayed logging, available since version 2.6.39 of the Linux kernel mainline, is claimed to resolve this; performance benchmarks done by the developer in 2010 revealed performance levels to be similar to ext4 at low thread counts, and superior at high thread counts.|$|R
40|$|International audienceThe Hadoop Distributed File System (HDFS) scales {{to store}} tens of {{petabytes}} of data {{despite the fact}} that the entire file system’s metadata must fit on the heap of a single Java virtual machine. The size of HDFS’ metadata is limited to under 100 GB in production, as garbage collection events in bigger clusters result in heartbeats timing out to the metadata server (NameNode). In this paper, we address the problem of how to migrate the HDFS’ metadata to a relational model, so that we can support larger amounts of storage on a shared-nothing, in-memory, distributed database. Our main contribution is that we show how to provide at least as strong consistency semantics as HDFS while adding support for a multiple-writer, multiple-reader concurrency model. We guarantee freedom from deadlocks by logically organizing inodes (and their constituent blocks and replicas) into a hierarchy and having all <b>metadata</b> <b>operations</b> agree on a global order for acquiring both explicit locks and implicit locks on subtrees in the hierarchy. We use transactions with pessimistic concurrency control to ensure the safety and progress of <b>metadata</b> <b>operations.</b> Finally, we show how to improve performance of our solution by introducing a snapshotting mechanism at NameNodes that minimizes the number of roundtrips to the database...|$|R
40|$|The {{metadata}} {{service of the}} Ursa Minor distributed storage system scales metadata throughput as metadata servers are added. While doing so, it correctly handles <b>metadata</b> <b>operations</b> that involve items served by different metadata servers, consistently and atomically updating the items. Unlike previous systems, it does so by reusing existing metadata migration functionality to avoid complex distributed transaction protocols. It also assigns item IDs to minimize the occurrence of multiserver operations. Ursa Minor’s approach allows one to implement a desired feature with less complexity than alternative methods and with minimal performance penalty (under 1 % in non-pathological cases). ...|$|R
40|$|Striping is a {{technique}} that distributes file content over multiple storage servers and thereby enables parallel ac-cess. In {{order to be able}} to provide a consistent view across file data and <b>metadata</b> <b>operations,</b> the file system has to track the layout of the file and know where the file ends and where it contains gaps. In this paper, we present a light-weight protocol for maintaining a consis-tent notion of a file’s layout that provides POSIX seman-tics without restricting concurrent access to the file. In an evaluation, we show that the protocol scales and elicit its corner cases. ...|$|R
40|$|Abstract Prefetching is an {{effective}} technique for improving file access performance, which can reduce access latency for I/O systems. In distributed storage system, prefetching for metadata files is critical for the overall system performance. In this paper, an Affinitybased Metadata Prefetching (APM) scheme is proposed for metadata servers in large-scale distributed storage systems to provide aggressive metadata prefetching. Through mining useful information about metadata assesses from past history, AMP can <b>discover</b> <b>metadata</b> file affinities accurately and intelligently for prefetching. Compared with LRU {{and some of the}} latest file prefetching algorithms such as NEXUS and C-miner, trace-driven simulations show that AMP can improve the hit rates by up to 12 %, 4. 5 % and 4 %, respectively, whil...|$|R
40|$|Abstract — An efficient, {{accurate}} and distributed metadataoriented prefetching scheme {{is critical to}} the overall performance in large distributed storage systems. In this paper, we present a novel weighted-graph-based prefetching technique, built on successor relationship, to gain performance benefit from prefetching specifically for clustered metadata servers, an arrangement envisioned necessary for petabyte-scale distributed storage systems. Extensive trace-driven simulations show that by adopting our new prefetching algorithm, the hit rate for metadata access on the client site can be increased by up to 13 %, while the average response time of <b>metadata</b> <b>operations</b> can be reduced by up to 67 %, compared with LRU and an existing state of the art prefetching algorithm. I...|$|R
40|$|The lack of {{a broader}} {{acceptance}} of today's pure ODBMS (as represented by the ODMG standard) brought many questions concerning the necessary changes of their architecture. One of the issues worth reconsidering is {{the design of a}} metamodel that would make <b>metadata</b> <b>operations</b> simpler and the underlying data model more evolvable and extensible. In this paper we discuss the implications of proposed simplified generic metadata structure for an object database. The role of a query language in metadata management is emphasized. We also consider the issue of compliance with existing metamodels. We argue that the simplified structure not only improves database metamodel flexibility, but it also contributes to a more intuitive metadata access...|$|R
40|$|Prefetching is an {{effective}} technique for improving file access performance, which can reduce access latency for I/O systems. In distributed storage system, prefetching for metadata files is critical for the overall system performance. In this paper, an Affinity-based Metadata Prefetching (APM) scheme is proposed for metadata servers in large-scale distributed storage systems to provide aggressive metadata prefetching. Through mining useful information about metadata assesses from past history, AMP can <b>discover</b> <b>metadata</b> file affinities accurately and intelligently for prefetching. Compared with LRU {{and some of the}} latest file prefetching algorithms such as NEXUS and C-miner, trace-driven simulations show that AMP can improve the hit rates by up to 12 %, 4. 5 % and 4 %, respectively, while reduce the average response time by up to 60 %, 12 % and 8 %, respectively...|$|R
40|$|This project report {{focuses on}} book {{metadata}} practices at the University of British Columbia Press. Metadata management has become essential for publishers in recent decades, as book buying has moved online. This report details {{the significance of}} metadata, how publishers use it, how customers (both institutional and individual) benefit from it, and how (good) metadata increases sales. Metadata has become increasingly complex, with varying deadlines, standards, levels, and granularity putting immense pressure on publishers to keep current. This project report analyzes the University of British Columbia Press’ <b>metadata</b> <b>operations</b> to identify its challenges and successes. The report also draws on the current literature of metadata “best practices” for publishers. In tandem, these resources clarify optimal future directions and recommendations for the Press...|$|R
40|$|The Ceph {{distributed}} object-based storage system, {{developed at}} UC Santa Cruz, [1] uses CRUSH, a pseudo-random placement function, {{to decide which}} OSDs to store data on, instead of storing the placement information in a table. This technique offers multiple advantages. In particular, the amount of metadata stored per file is drastically reduced, reducing the load on the metadata servers and speeding up metadata accesses and clients need communicate with the metadata servers only for <b>metadata</b> <b>operations,</b> since they can directly calculate the correct data placement for read and write operations. [2] However, pseudorandom placement also brings challenges for load balancing, since data cannot be arbitrarily moved to other nodes. We identify two types of load imbalance: persistent imbalance and transient imbalance. Persistent imbalance is cause...|$|R
40|$|Distributed file {{systems are}} {{important}} building blocks in modern computing environments. The challenge of increasing I/O bandwidth to files {{has been largely}} resolved {{by the use of}} parallel file systems and sufficient hardware. However, determining the best means by which to manage large amounts of metadata, which contains information about files and directories stored in a distributed file system, has proved a more difficult challenge. The objective of this thesis is to analyze the role of metadata and present past and current implementations and access semantics. Understanding the development of the current file system interfaces and functionality is a key to understanding their performance limitations. Based on this analysis, a distributed metadata benchmark termed DMetabench is presented. DMetabench significantly improves on existing benchmarks and allows stress on <b>metadata</b> <b>operations</b> in a distributed file system in a parallelized manner. Both intranode and inter-node parallelity, current trends in computer architecture, can be explicitly tested with DMetabench. This {{is due to the fact}} that a distributed file system can have different semantics inside a client node rather than semantics between multiple nodes. As measurements in larger distributed environments may exhibit performance artifacts difficult to explain by reference to average numbers, DMetabench uses a time-logging technique to record time-related changes in the performance of <b>metadata</b> <b>operations</b> and also protocols additional details of the runtime environment for post-benchmark analysis. Using the large production file systems at the Leibniz Supercomputing Center (LRZ) in Munich, the functionality of DMetabench is evaluated by means of measurements on different distributed file systems. The results not only demonstrate the effectiveness of the methods proposed but also provide unique insight into the current state of metadata performance in modern file systems...|$|R
40|$|Abstract. Cloud {{storage is}} a hot topic in current research. Different from {{previous}} work, we {{emphasize the importance of}} metadata cache in the study of cloud storage. Because the efficiency of distributed file system has much effect on cloud storage The <b>metadata</b> <b>operation</b> accounts for more than 50 % of the total file operation. So the strategy of efficient metadata management is important. There are three parts in this paper. We start with a brief introduction of cloud storage. Then a metadata caching algorithm for cloud storage is proposed. An additional discussion of its performance is also provided. The prototype which incorporates the proposed metadata caching algorithm is realized on Luster to evaluate its performance. Comparing experimental results from this study conclude that the metadata caching subsystem can improve the performance of cloud storage...|$|R
40|$|We have {{developed}} Ceph, a distributed file system that provides excellent performance, reliability, and scala- bility. Ceph maximizes {{the separation between}} data and metadata management by replacing allocation ta- bles with a pseudo-random data distribution function (CRUSH) designed for heterogeneous and dynamic clus- ters of unreliable object storage devices (OSDs). We leverage device intelligence by distributing data replica- tion, failure detection and recovery to semi-autonomous OSDs running a specialized local object file system. A dynamic distributed metadata cluster provides extremely efficient metadata management and seamlessly adapts {{to a wide range}} of general purpose and scientific comput- ing file system workloads. Performance measurements under a variety of workloads show that Ceph has ex- cellent I/O performance and scalable metadata manage- ment, supporting more than 250, 000 <b>metadata</b> <b>operations</b> per second...|$|R
40|$|International audienceLarge-scale {{scientific}} {{applications are}} often expressed as workflows that help defining data dependencies between their different components. Several such workflows have huge storage and computation requirements, {{and so they}} need to be processed in multiple (cloud-federated) datacenters. It has been shown that efficient metadata handling {{plays a key role in}} the performance of computing systems. However, most of this evidence concern only single-site, HPC systems to date. In this paper, we present a hybrid decentralized/distributed model for handling hot metadata (frequently accessed metadata) in multisite architectures. We couple our model with a scientific workflow management system (SWfMS) to validate and tune its applicability to different real-life scientific scenarios. We show that efficient management of hot metadata improves the performance of SWfMS, reducing the workflow execution time up to 50 % for highly parallel jobs and avoiding unnecessary cold <b>metadata</b> <b>operations...</b>|$|R
40|$|Abstract—The {{trend in}} {{parallel}} computing toward clusters running thousands of cooperating processes per application {{has led to}} an I/O bottleneck that has only gotten more severe as the CPU density of clusters has increased. Current parallel file systems provide large amounts of aggregate I/O bandwidth; however, they do not achieve the high degrees of metadata scalability required to manage files distributed across hundreds or thousands of storage nodes. In this paper we examine the use of collective communication between the storage servers to improve the scalability of file <b>metadata</b> <b>operations.</b> In particular, we apply server-to-server communication to simplify consistency checking and improve the performance of file creation, file removal, and file stat. Our results indicate that collective communication is an effective scheme for simplifying consistency checks and significantly improving the performance for several real metadata intensive workloads. I...|$|R
40|$|Abstract. Modern file systems {{maintain}} extensive metadata about stored files. While {{this usually}} is useful, there are situations when the additional overhead {{of such a}} design becomes a problem in terms of performance. This {{is especially true for}} parallel and cluster file systems, because due to their design every <b>metadata</b> <b>operation</b> is even more expensive. In this paper several changes made to the parallel cluster file system PVFS are presented. The changes are targeted at the optimization of workloads with large numbers of small files. To improve metadata performance, PVFS was modified such that unnecessary metadata is not managed anymore. Several tests with a large quantity of files were done to measure the benefits of these changes. The tests have shown that common file system operations can be sped up by a factor of two even with relatively few changes. ...|$|R
40|$|Abstract. This paper {{describes}} {{the design and}} implementation of a decentral-ized storage cluster for file storage so as to provide self-organizing, available, scalable, and consistent data management capability for cluster platform. The self-organizing capability of CHT grants our storage cluster the potential to handle both failure and online provisioning gracefully. We use soft-state to manage membership changes, and evenly distribute data across nodes by adopting linear hash algorithm so as to achieve incremental scalability of throughput and storage capacity, thus making the system easy to scale {{as the number of}} nodes grows. In order to guarantee metadata and data strict consis-tency among multiple replicas, we adopt decentralized weighted voting scheme that allows data and <b>metadata</b> <b>operations</b> to be safely interleaved, with enabling the system to perform self-tuning. We also present the experiment re-sults to demonstrate the features and performance of our design...|$|R
40|$|Parallel NFS (pNFS) is {{touted as}} an {{emergent}} standard protocol for parallel I/O access in vari-ous storage environments. Several pNFS prototypes {{have been implemented}} for initial validation and protocol examination. Previous efforts have focused on realizing the pNFS protocol to expose the best bandwidth potential from underlying file and storage systems. In this presentation, we provide an ini-tial characterization of two pNFS prototype imple-mentations, lpNFS (a Lustre-based parallel NFS im-plementation) and spNFS (another reference imple-mentation from Network Appliance, Inc.). We show that both lpNFS and spNFS can faithfully achieve {{the primary goal of}} pNFS, i. e., aggregating I/O bandwidth from many storage servers. However, they both face the challenge of scalable metadata management. Particularly, the throughput of sp-NFS <b>metadata</b> <b>operations</b> degrades significanlty with an increasing number of data servers. Even for the better-performing lpNFS, we discuss its architecture and propose a direct I/O request flow protocol to improve its performance. ...|$|R
40|$|Modern {{parallel}} and cluster file systems provide {{highly scalable}} I/O bandwidth by enabling highly parallel {{access to file}} data. Unfortunately metadata access does not benefit from parallel data transfer, so metadata performance scaling is less common. To support metadata-intensive workloads, we offer a middleware design that layers on top of existing cluster file systems, adds support for load balanced and high-performance <b>metadata</b> <b>operations</b> without sacrificing data bandwidth. The core idea is to integrate a distributed indexing mechanism with a metadata optimized on-disk Log-Structured Merge tree layout. The integration requires several optimizations including cross-server split operations with minimum data migration, and decoupling of data and metadata paths. To demonstrate the feasibility of our approach, we implemented a prototype middleware layer GIGA+TableFS and evaluated it with a Panasas parallel file system. GIGA+TableFS improves metadata performance of PanFS by as much an order of magnitude, while still performing comparably on data-intensive workloads...|$|R
40|$|Libraries {{have long}} relied on OCLC’s WorldCat {{database}} {{as a way}} to cooperatively share bibliographic data and declare library holdings to support interlibrary loan services. As curator, OCLC has traditionally mediated all interactions with the WorldCat database through their various cataloging clients to control access to the information. As more and more libraries look for new ways to interact with their data and streamline <b>metadata</b> <b>operations</b> and workflows, these clients have become bottlenecks and an inhibitor of library innovation. To address some of these concerns, in early 2013 OCLC announced the release of a set of application programming interfaces (APIs) supporting read and write access to the WorldCat database. These APIs offer libraries their first opportunity to develop new services and workflows that directly interact with the WorldCat database, and provide opportunities for catalogers to begin redefining how they work with OCLC and their data...|$|R
40|$|As storage systems evolve, the block-based {{design of}} today’s disks is {{becoming}} inadequate. As an alternative, objectbased storage devices (OSDs) offer a view where the disk manages data layout and {{keeps track of}} various attributes about data objects. By moving functionality that is traditionally {{the responsibility of the}} host OS to the disk, it is possible to improve overall performance and simplify management of a storage system. The capabilities of OSDs will also permit performance improvements in parallel file systems, such as further decoupling <b>metadata</b> <b>operations</b> and thus reducing metadata server bottlenecks. In this work we present an implementation of the Parallel Virtual File System (PVFS) integrated with a software emulator of an OSD and describe an infrastructure for client access. Even with the overhead of emulation, performance is comparable to a traditional server-fronted implementation, demonstrating that serverless parallel file systems using OSDs are an achievable goal. 1...|$|R
40|$|An {{efficient}} and distributed scheme for file mapping or file lookup {{is critical in}} decentralizing metadata management within a group of metadata servers, here the technique used called HIERARCHICAL BLOOM FILTER ARRAYS (HBA) to map filenames to the metadata servers holding their metadata. The Bloom filter arrays with different levels of accuracies are used on each metadata server. The first one with low accuracy and used to capture the destination metadata server information of frequently accessed files. The other array is used to maintain the destination metadata information of all files. Simulation results show our HBA design to be highly effective and efficient in improving the performance and scalability of file systems in clusters with 1, 000 to 10, 000 nodes (or superclusters) and {{with the amount of}} data in the petabyte scale or higher. HBA is reducing <b>metadata</b> <b>operation</b> by using the single metadata architecture instead of 16 metadata server...|$|R
40|$|Introduction and Background The {{notion of}} a log {{structured}} file system (LFS) [6, 9] evolved from earlier efforts using similar techniques [8, 2] {{as a means to}} improve write performance of file systems. Other benefits include faster <b>metadata</b> <b>operations,</b> e. g. file create and delete. But there is controversy about the the utility of LFS for database systems, {{especially in light of the}} critique in [10]. This position paper argues that LFS has wonderful potential as the underpinning of a database system, solving a number of problems that are known to be quite vexing, and providing some additional important benefits. These include atomic writes, system management and scalability, storage efficiency, and recovery system performance. There are three inter-related ideas in LFS. LFS virtualizes the placement of files on disk. Every write to a file dynamically relocates the data being written. Thus, a write must also update the data structures involved in this relocation mapping. Becaus...|$|R
40|$|Although data {{prefetching}} algorithms {{have been}} extensively studied for years, {{there is no}} counterpart research done for metadata access performance. Existing data prefetching algorithms, either lack of emphasis on group prefetching, or bearing {{a high level of}} computational complexity, do not work well with metadata prefetching cases. Therefore, an efficient, accurate, and distributed metadata-oriented prefetching scheme is critical to leverage the overall performance in large distributed storage systems. In this paper, we present a novel weighted-graph-based prefetching technique, built on both direct and indirect successor relationship, to reap performance benefit from prefetching specifically for clustered metadata servers, an arrangement envisioned necessary for petabyte-scale distributed storage systems. Extensive trace-driven simulations show that by adopting our new metadata prefetching algorithm, the miss rate for metadata accesses on the client site can be effectively reduced, while the average response time of <b>metadata</b> <b>operations</b> can be dramatically cut by up to 67 percent, compared with legacy LRU caching algorithm and existing state-of-the-art prefetching algorithms...|$|R
