9037|5279|Public
5|$|It is {{the most}} {{sophisticated}} unclassified hyperspectral <b>imaging</b> <b>system</b> available, according to U.S. Government officials.|$|E
5|$|Airborne Real-time Cueing Hyperspectral Enhanced Reconnaissance, {{also known}} by the acronym ARCHER, is an aerial <b>imaging</b> <b>system</b> that {{produces}} ground images far more detailed than plain sight or ordinary aerial photography can.|$|E
5|$|Intermediate {{dispersion}} Spectrograph and <b>Imaging</b> <b>System</b> – medium resolution (R = 1,800-20,000) long-slit dual-beam optical spectrograph. Mounted at Cassegrain focus. ISIS {{was one of}} {{the original}} first generation of WHT instruments.|$|E
5000|$|The term [...] "forward looking" [...] {{is used to}} {{distinguish}} fixed forward-looking thermal <b>imaging</b> <b>systems</b> from sideways-tracking infrared systems, also known as [...] "push broom" [...] imagers, and other thermal <b>imaging</b> <b>systems</b> such as gimbal-mounted <b>imaging</b> <b>systems,</b> handheld <b>imaging</b> <b>systems</b> and the like. Pushbroom systems typically have been used on aircraft and satellites.|$|R
50|$|Optical <b>imaging</b> <b>systems</b> may {{be divided}} into {{diffusive}} and ballistic <b>imaging</b> <b>systems.</b>|$|R
50|$|In <b>imaging</b> <b>systems,</b> {{the phase}} {{component}} is typically not {{captured by the}} sensor. Thus, the important measure with respect to <b>imaging</b> <b>systems</b> is the MTF.|$|R
5|$|With its {{multiple}} arms, the starfish {{provides a}} popular metaphor for computer networks, companies and software tools. It {{is also the}} name of a seabed <b>imaging</b> <b>system</b> and company.|$|E
5|$|On November 15, 2008, a 34-kg probe made a {{hard landing}} near the crater. The moon impact probe (MIP) was {{launched}} from the Indian Chandrayaan-I spacecraft {{and reached the}} surface 25 minutes later. The probe carried a radar altimeter, video <b>imaging</b> <b>system,</b> and a mass spectrometer, which {{will be used to}} search for water.|$|E
5|$|The {{challenge}} for NASA's imaging team was that, as the mission progressed, the objects being photographed were getting {{farther away from}} the spacecraft, therefore appearing fainter and requiring longer exposures and even slewing (panning) of the cameras to achieve good quality. The telecommunication capability also decreased with distance, limiting the number of data modes that could be used by the <b>imaging</b> <b>system.</b>|$|E
40|$|In this paper, {{we present}} {{concepts}} of document <b>imaging</b> <b>systems</b> {{and also a}} survey on some of well known products in this field. To make easy understanding for the studied products, we show a comparative study based on the basics of <b>imaging</b> <b>systems</b> as well as strategies used by those products. This study is very useful to help organizations to choose appropriate document <b>imaging</b> <b>systems</b> tools based on their needs...|$|R
40|$|The {{phase of}} the optical {{transfer}} function is advocated as an important tool in the characterization of modern incoherent <b>imaging</b> <b>systems.</b> It is shown that knowledge of the phase transfer function (PTF) can benefit a diverse array of applications involving both traditional and computational <b>imaging</b> <b>systems.</b> Areas of potential benefits are discussed, and three applications are presented, demonstrating {{the utility of the}} {{phase of the}} complex frequency response in practical scenarios. In traditional <b>imaging</b> <b>systems,</b> the PTF is shown via simulation results to be strongly coupled with odd-order aberrations and hence useful in misalignment detection and correction. In computational <b>imaging</b> <b>systems,</b> experimental results confirm that the PTF can be successfully applied to subpixel shift estimation and wavefront codin...|$|R
40|$|Image resolution, {{image quality}} and data {{transmission}} rate of remote <b>imaging</b> <b>systems</b> continuously controlled by operator intervention. Set of levers allows operator to control independently frame rate, frame size and image quality in bits per pixel. <b>System</b> used for <b>imaging</b> <b>systems</b> in aircraft or remotely piloted vehicles...|$|R
5|$|Although many in NASA's Voyager {{program were}} {{supportive}} of the idea, there were concerns that taking a picture of Earth {{so close to the}} Sun risked damaging the spacecraft's <b>imaging</b> <b>system</b> irreparably. It was not until 1989 that Sagan's idea was put into practice, but then instrument calibrations delayed the operation further, and the personnel who devised and transmitted the radio commands to Voyager 1 were also being laid off or transferred to other projects. Finally, NASA Administrator Richard Truly interceded to ensure that the photograph was taken.|$|E
5|$|Some Tornado {{variants}} carry different avionics and equipment, {{depending on}} their mission. The Tornado ECR is devoted to Suppression of Enemy Air Defences (SEAD) missions, operated by Germany and Italy. The Tornado ECR is equipped with an emitter-locator system (ELS) to spot radar use. German ECRs have a Honeywell infrared <b>imaging</b> <b>system</b> for reconnaissance flights. RAF and RSAF Tornados have the Laser Range Finder and Marked Target Seekers (LRMTS) for targeting laser-guided munitions. In 1991, the RAF introduced TIALD, allowing Tornado GR1s to laser-designate their own targets.|$|E
5|$|The ARCHER <b>imaging</b> <b>system,</b> mounted {{aboard the}} GA8 Airvan, uses visible and {{near-infrared}} light {{to examine the}} surface of the Earth and find suspected crash sites, evaluate areas affected by disasters, or examine foliage from an airborne perspective in order to flag possible marijuana plantations. Both the SDIS and ARCHER systems were used to great success in the response to Hurricane Katrina; ARCHER may be used in coordination with the SDIS system.|$|E
5000|$|Image {{processing}} {{in digital}} cameras, computers and various <b>imaging</b> <b>systems</b> ...|$|R
3000|$|... 3 D <b>imaging</b> <b>systems</b> {{inserted}} with microlens array {{can be used}} {{to capture}} the light field information, but the spatial resolution is much lower compared with that of photos captured by 2 D camera. These 3 D <b>imaging</b> <b>systems</b> may be applied in the situation that the high spatial resolution is not required.|$|R
50|$|Most digital <b>imaging</b> <b>systems</b> display {{an image}} as a grid of tiny, square pixels. However, some <b>imaging</b> <b>systems,</b> {{especially}} those that must be compatible with standard-definition television motion pictures, display an {{image as a}} grid of rectangular pixels, in which the pixel width and height are different. Pixel Aspect Ratio describes this difference.|$|R
5|$|In {{addition}} to CAP's own corporate fleet, many member-owned aircraft are {{made available for}} official tasking by CAP's volunteers should the need arise. Aircraft on search missions are generally crewed by at least three qualified aircrew members: a Mission Pilot, responsible for the safe flying of the aircraft; a Mission Observer, responsible for navigation, communications and coordination of the mission as well as ground observation; and a Mission Scanner {{who is responsible for}} looking for crash sites and damage clues. Additionally, the mission scanner may double as a satellite digital <b>imaging</b> <b>system</b> (SDIS) operator. Larger aircraft may have additional scanners aboard, providing greater visual coverage. Because of the additional ARCHER equipment, the crew of a Civil Air Patrol GA8 Airvan may also include an operator of the ARCHER system, depending upon the requirements of the mission and the capabilities of the aircraft.|$|E
5|$|In {{order to}} give its astronauts {{experience}} before these programs started, the Blue Gemini program was proposed, which would have seen USAF astronauts fly on NASA missions in order to practice various techniques required for their own missions. This would have first seen cooperative missions between NASA and the US Air Force, with two missions flying with crews composed of one astronaut from NASA, and one from the USAF, followed by two missions with all-USAF crews, but performing missions for NASA. After these flights, the US Air Force would have flown a number of missions of its own. Firstly, it would have flown a two-man Agena rendezvous and docking mission, followed by two one-man scientific or technology research missions. Other proposed missions included tests of the Astronaut Mobility Unit {{which was designed to}} assist with EVAs, inertial navigation systems, and flying a radar <b>imaging</b> <b>system.</b>|$|E
5|$|The Twyman–Green {{interferometer}}, {{invented by}} Twyman and Green in 1916, is {{a variant of}} the Michelson interferometer widely used to test optical components. The basic characteristics distinguishing it from the Michelson configuration are the use of a monochromatic point light source and a collimator. It {{is interesting to note that}} Michelson (1918) criticized the Twyman-Green configuration as being unsuitable for the testing of large optical components, since the light sources available at the time had limited coherence length. Michelson pointed out that constraints on geometry forced by limited coherence length required the use of a reference mirror of equal size to the test mirror, making the Twyman-Green impractical for many purposes. Decades later, the advent of laser light sources answered Michelson's objections. (A Twyman-Green interferometer using a laser light source and unequal path length is known as a Laser Unequal Path Interferometer, or LUPI.) Fig.14 illustrates a Twyman-Green interferometer set up to test a lens. Light from a monochromatic point source is expanded by a diverging lens (not shown), then is collimated into a parallel beam. A convex spherical mirror is positioned so that its center of curvature coincides with the focus of the lens being tested. The emergent beam is recorded by an <b>imaging</b> <b>system</b> for analysis.|$|E
40|$|Abstract—We {{propose a}} novel continuous-time simulta-neous-readout scheme for active <b>imaging</b> <b>systems</b> based on {{orthogonal}} modulation of photodetector signals. The superim-posed-continuous-time approach presented here {{differs from the}} conventional scheduled-discrete-time scheme in that the photodetector signals are summed in a common bus and read concurrently. We show how that our proposed architecture may be advantageous, particularly in applications where bandwidth re-quirements for a time-multiplexed scheme are highly demanding. The active readout cell presented here is the kernel of the proposed orthogonal encoding architecture. We describe the cell operation principle, its properties and major design challenges. A 0. 5 - m CMOS test chip has been fabricated to demonstrate functionality of the readout architecture. Test results show {{it to be a}} viable option for highly-integrated active <b>imaging</b> <b>systems.</b> Index Terms—Active <b>imaging</b> readout circuits, focal plane arrays, laser radar <b>imaging</b> <b>systems,</b> three-dimensional (3 -D) <b>imaging</b> <b>systems.</b> I...|$|R
25|$|Crosfield Electronics – digital <b>imaging</b> <b>systems,</b> {{now part}} of FFEI Ltd.|$|R
5000|$|Crosfield Electronics - digital <b>imaging</b> <b>systems,</b> {{now part}} of FFEI Ltd.|$|R
25|$|Mastcam-Z, a {{stereoscopic}} <b>imaging</b> <b>system</b> {{with the}} ability to zoom.|$|E
25|$|Installing {{a hybrid}} OR is a {{challenge}} to standard hospital room sizes, as not only the <b>imaging</b> <b>system</b> requires some additional space, but there are also more people in the room as in a normal OR. A team of 8 to 20 people including anasthesiologists, surgeons, nurses, technicians, perfusionists, support staff from device companies etc. can work in such an OR. Depending on the <b>imaging</b> <b>system</b> chosen, a room size of 70 square meters including a control room but excluding a technical room and the preparation areas is recommended. Additional preparations of the room necessary are 2-3mm lead shielding and potentially enforcement of the floor or ceiling to hold the additional weight of the <b>imaging</b> <b>system</b> (approximately 650–1800kg).|$|E
25|$|Malin and {{his team}} helped build THEMIS (Thermal Emission <b>Imaging</b> <b>System)</b> on the 2001 Mars Odyssey mission.|$|E
5000|$|By {{virtue of}} the {{linearity}} property of optical <b>imaging</b> <b>systems,</b> i.e., ...|$|R
5000|$|It {{should be}} noted that active systems such as the L3 Provision™ and the Smiths eqo™ are {{actually}} mm-wave <b>imaging</b> <b>systems</b> rather than Terahertz <b>imaging</b> <b>systems</b> lite Millitech™ systems. These widely deployed systems do not display images, avoiding any privacy issues. Instead they display generic [...] "mannequin" [...] outlines with any anomalous regions highlighted.|$|R
5000|$|Aging biology, nano-bio <b>imaging,</b> <b>systems</b> {{and complex}} biology, bio-sustainability, {{biochemistry}} and biophysics ...|$|R
25|$|The Airy disk {{can be an}} {{important}} parameter in limiting the ability of an <b>imaging</b> <b>system</b> to resolve closely located objects.|$|E
25|$|The <b>imaging</b> <b>system</b> {{calls up}} the {{structure}} of polygons needed for the scene to be created from the database. This is transferred to active memory and finally, to the display system (screen, TV monitors etc.) so that the scene can be viewed. During this process, the <b>imaging</b> <b>system</b> renders polygons in correct perspective ready for transmission of the processed data to the display system. Although polygons are two-dimensional, through the system computer they are placed in a visual scene in the correct three-dimensional orientation.|$|E
25|$|Hot Spot: Infra-red <b>imaging</b> <b>system</b> {{that shows}} {{where the ball}} has {{been in contact with}} bat or pad. Improved cameras were {{introduced}} for the 2012 season.|$|E
40|$|The {{resolution}} limit of <b>imaging</b> <b>systems</b> is ultimately limited by diffraction. However, diffraction is often neglected {{in the analysis}} and design of both front and back illumination <b>imaging</b> <b>systems</b> {{in favor of the}} simpler ray tracing model. In many systems, paraxial optics provides a reasonable model for the design of systems with high resolution. This is certainly true for the majority of front-illuminated imaging systems; however, in back illuminated (shadowgraphic) <b>imaging</b> <b>systems</b> resolution is very strongly affected by diffraction. We present a detailed experimental comparison of imaging resolution differences between front and back illuminated <b>imaging</b> <b>systems</b> for non-scattering and scattering environments. Additionally, modeling results of both systems are compared with the experimental results and classical optical theory. Preliminary results and calculations show that physical optics creates a stronger effect on resolution in front illuminated systems in either scattering or non-scattering environments despite original predictions...|$|R
40|$|All-reflective <b>imaging</b> <b>systems</b> {{that are}} {{asymmetrical}} and eccentric {{have the advantage}} of providing more degrees of freedom to improve image quality. A disadvantage of these asymmetrical <b>imaging</b> <b>systems</b> is that they suffer from asymmetric mapping. This asymmetric mapping manifests itself mainly in the presence of keystone distortion and anamorphism. Due to the increase in degrees of freedom, the complexity of such systems escalates; thus, the designer is confronted with the difficult task of determining optimal starting points. This work addresses several first-order aspects of the design and characterisation of asymmetrical, all-reflective, aspherical, eccentric <b>imaging</b> <b>systems.</b> In contrast to the work of Stone and Forbes, which is based upon the theory of Hamiltonian optics and includes both the first- and second-order considerations, this work is based upon the theory of collineation. Because of the inherent simplicity of the collinear mapping, which is a projective transformation, we are able to present a simple but certainly not naive way of designing and characterising such asymmetrical all-reflective <b>imaging</b> <b>systems.</b> The simplicity of this proposition has the advantage that we can gain insights into asymmetrical mapping behaviour. Specifically, we apply the collinear mapping model on all-reflective asymmetrical <b>imaging</b> <b>systems</b> resulting in the description of how the mapping between conjugate planes may be described. First we will define keystone distortion and anamorphism. Then we will introduce and investigate the significance of the Cardinal points and planes, the Scheimpflug condition and the horizon planes and show how they are applied in the designing of <b>imaging</b> <b>systems</b> that are free of both keystone distortion and anamorphism. Having established a first-order layout of the optical system, we will then develop a process for converting the first-order layouts into <b>imaging</b> <b>systems</b> consisting of real aspheric surfaces...|$|R
5000|$|... 1996 - Mallinckrodt {{acquired}} {{maker of}} urology <b>imaging</b> <b>systems</b> and injectors, Liebel-Flarsheim Co.|$|R
