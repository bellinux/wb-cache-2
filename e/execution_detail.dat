6|242|Public
50|$|A century later, {{the living}} of St. Nicholas Cole Abbey {{was owned by}} Colonel Francis Hacker, a Puritan who {{commanded}} the <b>execution</b> <b>detail</b> of Charles I.|$|E
50|$|The name of {{the problem}} comes from an analogy with {{real-world}} firing squads: {{the goal is to}} design a system of rules according to which an officer can so command an <b>execution</b> <b>detail</b> to fire that its members fire their rifles simultaneously.|$|E
50|$|The Flash is {{hailed as}} a {{messenger}} spoken of in gorilla prophecy by all but Grodd who sees the interloper {{as a threat to}} his ascension and designs for world conquest. When told of the treachery planned by the Ape Elders from his most trusted general, Grodd led a personal <b>execution</b> <b>detail</b> in an attempt to kill Barry and assume the mantle of the Light Bringer. But The Flash foils Grodd's attempts by outmaneuvering him until King Grodd causes the caves containing Gorilla Cities history to collapse upon himself, knocking him unconscious, allowing the Flash to escape.|$|E
5000|$|The phrases [...] "speeds and feeds" [...] or [...] "feeds and speeds" [...] have {{sometimes}} been used metaphorically {{to refer to}} the <b>execution</b> <b>details</b> of a plan, which only skilled technicians (as opposed to designers or managers) would know.|$|R
50|$|The Linux Trace Toolkit (LTT) {{is a set}} {{of tools}} that is {{designed}} to log program <b>execution</b> <b>details</b> from a patched Linux kernel and then perform various analyses on them, using console-based and graphical tools. LTT has been mostly superseded by its successor LTTng (Linux Trace Toolkit Next Generation).|$|R
40|$|This {{handbook}} {{was created}} to transition event <b>execution</b> <b>details</b> utilizing project management best practices. It combines instruction methods, visual icons, and a newly designed structural relationship to project management/lean principles. The handbook is delivered in a modern style and to foster continual consideration of historical genesis that withstands leadership changes. Master's Applied Projec...|$|R
50|$|Casca {{was first}} {{introduced}} in Casca 1: The Eternal Mercenary. Little is known about his early life, although vague clues are given in various books in the series. He {{spent most of his}} adult life serving the Roman empire in the Legions, and there are some scraps of information about his family. He grew up in the hill country of Etruria, now known as Tuscany, to the north of Rome. The village name was Falerno. When he was a child, he saw the 10th Legion march through his village en route to Gaul. His uncle, Tontine, enlisted into the army under Julius Caesar. His family died of plague and the young Casca burned the family home afterward. He enlisted into the 7th Legion at either Messilia or Livorno. His first battle under the eagle of the 7th was on the German border against the Suevii when 15,000 tribesmen attacked them at dawn. Only 300 made it back into the forests of Germania. He joined the 10th Legion and was sent to Jerusalem where he was assigned to the <b>execution</b> <b>detail</b> for three prisoners, amongst whom was Jesus.|$|E
30|$|Event {{package is}} a program {{collection}} {{which is responsible for}} the MAC layer management message corresponding to the real world, whose main capability is to define the event types and the sub-procedure operation interfaces inside the event process procedures, and the code entity of <b>execution</b> <b>detail</b> is provided by the device instance. The system architecture of event package includes interface class, abstract class, and object class program, and the use of patterns has factory, bridge, chain of responsibility, and command pattern, where the UML diagram for event package is shown in Appendix  1 : Fig. 11.|$|E
40|$|Conventional system {{simulators}} {{are readily}} used by computer architects {{to design and}} evaluate their processor designs. These simulators provide reasonable levels of accuracy and <b>execution</b> <b>detail</b> but suffer from long simulation latencies and increased implementation complexity. In this work we propose iQ, a queue-based modeling technique that targets design space exploration and optimization studies at the core component level. iQ emulates processor elements by abstracting the implementation details into modular components composed of queue structures, delay parameters, probabilistic driven message generation and event control. Its easy reconfigurability makes iQ a highly flexible and powerful processor simulator. We have used iQ to build an Ivy Bridge and a Core 2 Duo processor model and have validated them against real hardware running SPEC CPU 2006 Int achieving average error rates of 9. 55 % and 8. 93 %. Peer ReviewedPostprint (author's final draft...|$|E
40|$|This paper {{describes}} a software architecture for mission-level control of autonomous unmanned air vehicles (UAVs). The architecture provides for sensor data fusion, worldview generation, and mission planning and <b>execution.</b> <b>Details</b> about the airborne platform and a high-level {{description of the}} control architecture are provided. As {{an example of the}} architecture’s versatility a formation flight behavior is described. I...|$|R
5000|$|Each {{and every}} {{programming}} language has an execution model, which determines {{the manner in}} which the units of work (that are indicated by program syntax) are scheduled for <b>execution.</b> <b>Detailed</b> examples of the specification of execution models of a few popular languages include those of Python, the execution model of the Unified Parallel C (UPC) programming language, ...|$|R
40|$|Abstract — Web {{services}} facilitate {{integration and}} interoperability of distributed heterogeneous application and components. It {{is a standard}} representation for information resource {{that can be used}} by other programs, regardless of the development platform, middleware, operating system and the hardware. During execution of a web service, {{there is a need to}} monitor its internal behavior to check for its availability and correctness, and to detect performance bottlenecks. This paper presents a Probebased Observability Mechanism required for the monitoring of the web services. Our mechanism facilitates observation of internal <b>execution</b> <b>details</b> of the web services during testing and execution. The mechanism defines the structure of log which is required to be inserted into the web service. During execution of the web service, our mechanism uses XML to create a log file. A reporting interface is provided for observing the internal <b>execution</b> <b>details</b> of the log file. We illustrate the mechanism with an example written using ASP. NET and C#. Index Terms—Web Services, Probe, Observabilit...|$|R
30|$|Data {{analysis}} software as {{a service}} (DASaaS). This is a higher-level model that offers to end users data mining algorithms, data analysis suites or ready-to-use knowledge discovery applications as Internet services that can be accessed and used directly through a Web browser. According to this approach, every data analysis software is provided {{as a service}}, avoiding end users to worry about implementation and <b>execution</b> <b>details.</b>|$|R
40|$|End-user {{software}} is executed billions of times daily, but the corresponding <b>execution</b> <b>details</b> (“by-products”) are discarded. We hypothesize that, if suitably cap-tured and aggregated, these by-products could substan-tially {{speed up the}} process of testing programs and prov-ing them correct. Ironically, both testing and debugging involve simulating real-world conditions and executions, in essence trying to recreate in the lab some of these (pre-viously available, but discarded) <b>execution</b> <b>details.</b> This position paper proposes a way to recoup the ex-ecution information that is lost during everyday software use, aggregate it, and automatically turn it into bug fixes and proofs. The goal is to enable software to improve it-self by “learning ” from past failures and successes, lever-aging the information-rich execution by-products that to-day are being wasted. We view every execution of a pro-gram as a test run and aggregate executions across the lifetime of a program into one gigantic test suite—i. e., we remove the distinction between software use and soft-ware testing and verification—with the purpose of sub-stantially reducing software bug density. ...|$|R
40|$|In service-oriented environments, {{services}} are {{put together in}} the form of a workflow with the aim of distributed problem solving. Capturing the <b>execution</b> <b>details</b> of the services' transformations is a significant advantage of using workflows. These <b>execution</b> <b>details,</b> referred to as provenance information, are usually traced automatically and stored in provenance stores. Provenance data contains the data recorded by a workflow engine during a workflow execution. It identifies what data is passed between services, which {{services are}} involved, and how results are eventually generated for particular sets of input values. Provenance information is of great importance and has found its way through areas in computer science such as: Bioinformatics, database, social, sensor networks, etc. Current exploitation and application of provenance data is very limited as provenance systems started being developed for specific applications. Thus, applying learning and knowledge discovery methods to provenance data can provide rich and useful information on workflows and services. Therefore, in this work, the challenges with workflows and services are studied to discover the possibilities and benefits of providing solutions by using provenance data. A multifunctional architecture is presented which addresses the workflow and service issues by exploiting provenance data. These challenges include workflow composition, abstract workflow selection, refinement, evaluation, and graph model extraction. The specific contribution of the proposed architecture is its novelty in providing a basis for taking advantage of the previous <b>execution</b> <b>details</b> of services and workflows along with artificial intelligence and knowledge management techniques to resolve the major challenges regarding workflows. The presented architecture is application-independent and could be deployed in any area. The requirements for such an architecture along with its building components are discussed. Furthermore, the responsibility of the components, related works and the implementation details of the architecture along with each component are presented...|$|R
50|$|The blood eagle is a ritualized {{method of}} <b>execution,</b> <b>detailed</b> in late skaldic poetry. According {{to the two}} {{instances}} mentioned in the Sagas, the victim (always {{a member of a}} royal family) was placed prone, the ribs severed from the spine with a sharp tool and the lungs pulled through the opening to create a pair of “wings”. There is a continuing debate about whether the ritual was a literary invention, a mistranslation of the original texts or an actual historical practice.|$|R
30|$|Using {{high-level}} scalable models, {{a programmer}} defines only the high-level logic of an application while hides the low-level details {{that are not}} essential for application design, including infrastructure-dependent <b>execution</b> <b>details.</b> A programmer is assisted in application definition and application performance depends on the compiler that analyzes the application code and optimizes its execution on the underlying infrastructure. On the other hand, low-level scalable models allow programmers to interact directly with computing and storage elements composing the underlying infrastructure and thus define the applications parallelism directly.|$|R
30|$|DG-SPARQL is {{designed}} to evaluate the sub-query plans in a parallel fashion. In particular, DG-SPARQL parallelizes {{the evaluation of the}} x relational-based sub-query plans by assigning the evaluation of each plan RQP_i into a distinct relational store of the underlying slave nodes (n). In addition, DG-SPARQL parallelizes the evaluation of the main memory query plans by relying on Bulk Synchronous Parallel-based (Valiant 1990) graph traversal operations and communication over the graph partitions (TP_n). In the following subsection, we present the query optimization and <b>execution</b> <b>details</b> of the DG-SPARQL query engine.|$|R
40|$|Abstract. We have {{designed}} and implemented an API for grid computing {{that can be}} used for developing grid-distributed parallel programs without leaving the level of the language in which the core application is written. Our software framework is able to utilize the information about heterogeneous grid environments in order to adapt the algorithmic structure of parallel programs to the particular situation. Since our solution hides low-level grid-related <b>execution</b> <b>details</b> from the application by providing an abstract execution model, it is able to eliminate some algorithmic challenges of nowadays grid programming. ...|$|R
40|$|Abstract. Declarative debuggers are {{semi-automatic}} debugging {{tools that}} abstract the <b>execution</b> <b>details</b> {{to focus on}} the program semantics. This paper presents a tool implementing this approach for the sequential subset of Erlang, a functional language with dynamic typing and strict evaluation. Given an erroneous computation, it first detects an erroneous function (either a “named ” function or a lambda-abstraction), and then continues the process to identify the fragment of the function responsible for the error. Among its features it includes support for exceptions, pre-defined and built-in functions, higher-order functions, and trusting and undo commands. ...|$|R
50|$|When {{the test}} cases are executed, {{we need to}} keep track of the <b>execution</b> <b>details</b> like when it is executed, who did it, how long it took, what is the result etc. This data must be {{available}} to the test leader and the project manager, along with all the team members, in a central location. This may be stored in a specific directory in a central server and the document must say clearly about the locations and the directories. The naming convention for the documents and files must also be mentioned.|$|R
40|$|The {{quality of}} the data {{provided}} {{is critical to the}} success of data warehousing initiatives. There is strong evidence that many organisations have significant data quality problems, and that these have substantial social and economic impacts. This paper describes a study which explores modeling of the dynamic parts of the data warehouse. This metamodel enables data warehouse management, design and evolution based on a high level conceptual perspective, which can be linked to the actual structural and physical aspects of the data warehouse architecture. Moreover, this metamodel is capable of modeling complex activities, their interrelationships, the relationship of activities with data sources and <b>execution</b> <b>details...</b>|$|R
40|$|AbstractOur {{framework}} is a programming method whose main idea consists in solving a problem statically and explicitly before taking some <b>execution</b> <b>details</b> into account. Within {{the framework of}} the methodology for constructing a program, we are often compelled to introduce intermediate values. We prove {{that in the case of}} sequential languages, and especially when they are sequence data types, those intermediates do not need to be effectively constructed. This aspect of the problem is obviously connected with efficiency. We suggest program transformation as a possible means of eliminating useless intermediates in the case of sequential languages. This paper follows [12] and aims at describing a program construction method, a set of transformations rules and their application with an example...|$|R
40|$|In the e-Science context, {{workflow}} technologies {{provide a}} problem-solving environment for scientists by facilitating {{the creation and}} execution of experiments {{from a pool of}} available data and computation services. We argue that in order to characterise scientific analysis we need to go beyond low-level service composition and <b>execution</b> <b>details</b> by capturing higherlevel description of the scientific process. The aim here is to make the experimental conditions and goals of the experiment transparent. Current workflow technologies do not incorporate any representation of these goals and conditions, which we call the scientist’s intent. Our hypothesis is that by extending workflow representation in this way, scientists (including social scientists) would be able to analyse, verify, execute, monitor and re-use workflows more efficiently...|$|R
40|$|In {{the frame}} of the Austrian Grid Phase 2, we have {{designed}} and implemented an API for grid computing {{that can be used for}} developing grid-distributed parallel programs without leaving the level of the language in which the core application is written. Our software framework is able to utilize the information about heterogeneous grid environments in order to adapt the algorithmic structure of parallel programs to the particular situation. Since our solution hides lowlevel grid-related <b>execution</b> <b>details</b> from the application by providing an abstract execution model, it is able to eliminate some algorithmic challenges of nowadays grid programming. In this paper, we present on the first feature-complete prototype of our topology-aware software system. ...|$|R
40|$|Autodesk Inventor. ???? ???????? ?? ???? ??????? Autodesk Inventor 2016 Professional. Inventor ???????? ????????? ?? ??????? ??????? ??????????? ??????? ???????????????? ????, ??????? ??? ??????? ?????????????????? ????????, ?? ?????????? ?????????????? ?????? ????????? ????? ???????? ??? ?????????? ??????????? ?????? ??????, ??????? ?? ???????. ????????? ??????? ?????? ? Inventor ????????? ????????? ??????????? ?? ????????????? ???????????? ???????????????? ????????? ? ???????????????? ????, ??? ????????? ???? ????????? ??????? ???? ?? ????????? ????? ??????????? ????????. ? ?????????? ????????? ?????????? ????????? ?????? ?????, ??????? ??????? ? ????????? ???????? ??????? ? ????????? ?????? ???? ?????????? ??????????? ???????? ? ??????. These {{guidelines}} {{are designed to}} familiarize students with computer aided design capabilities Autodesk Inventor. Description is {{made on the basis}} of system Autodesk Inventor 2016 Professional. Inventor is most common in Ukraine representative range CAD design, has a so-called "intelligent" means that allows designers to significantly reduce the number of operations in the design of three-dimensional models of parts of the object or structure. Getting experience in Inventor allow future graduates in the specialty ?Power engineering? work in design offices that create their own software projects is through this software. In guidance solutions of an algorithm, modeling and <b>execution</b> <b>details</b> of the drawings detailing all series of the operations and commands. ?????? ???????????? ???????? ????????????? ???????????? ????????? ? ????????? ????????????? ??????? ??????????????????? ?????????????? Autodesk Inventor. ???????? ????????? ?? ???? ??????? Autodesk Inventor 2016 Professional. Inventor ???????? ???????????????? ?? ?????????? ??????? ????????????? ??????? ??????????????? ????, ???????? ??? ??????????? ??????????????????? ??????????, ???????????? ???????????? ??????????? ????????? ????? ???????? ??? ?????????? ?????????? ?????? ??????, ??????? ??? ??????????. ????????? ????? ?????? ? Inventor ???????? ??????? ??????????? ?? ????????????? ??????????????? ??????????????? ???????? ? ??????????????? ????, ??????? ??????? ???? ??????????? ??????? ?????? ? ??????? ????? ???????????? ????????. ? ???????????? ????????? ??????????? ????????? ??????? ?????, ?????????? ??????? ? ?????????? ???????? ??????? ? ????????? ????????? ???? ??????????????? ??????????? ???????? ? ??????...|$|R
40|$|The {{exponential}} growth {{of information and}} proliferation of service offers on the Web require a robust platform which can not merely offer networked computing capabilities but also support intelligent data and service handling. Agent Grid Intelligence Platform (AGrIP) is such an Internet intelligent platform that enables greater access to both content and services. Based on agents, AGrIP re-organizes the global information into a well-ordered and semantic-rich space by using ontology techniques, structures dynamic aspects of Web services in a formal way by using Dynamic Description Logic, and allows users to query global information and invoke a global Web service without being aware of the site, structure, query language, <b>execution</b> <b>details</b> and service holder. AGrIP provides a collaborative e-Commerce scenarios for service offerings on the Semantic Web. 1...|$|R
5000|$|Mahdi Bohlouli is {{the project}} leader of Grid-HPA. The {{idea of this}} project has been started from his master thesis in 2007. Mahdi {{published}} the concept details and implementation results firstly in International Journal of Computer Science [...] after having successful and more accurate implementation results. Java {{has been used for}} implementation part and the project database is based on SQL-Server. Project database consists of to the history of former executed jobs and their <b>execution</b> <b>details</b> as well as used resources for their execution. The proposed algorithm of prediction can be used for every job in every environment. The measured accuracy of prediction in a centralized approach is better than a decentralized method. Both methods have some advantages and disadvantages.|$|R
40|$|AbstractThis paper {{proposes a}} {{programming}} method whose main {{idea is to}} give a simple static solution to a problem, taking <b>execution</b> <b>details</b> into account afterwards. The method proposes the following approach to the user: first, the result is defined concisely using a definition scheme, which introduces a number of intermediate objects. These intermediate objects characterise subproblems which have to be solved. This process is repeated recursively on these intermediate objects, until finally the data of the problem are introduced. The support language for the method is static and uses ‘sequences’ as a basic data type. In a second paper we tackle the problem which Jackson terms ‘structure clashes’. The same method is used, taking account of efficiency considerations. Such considerations lead {{to the introduction of}} program transformation rules...|$|R
40|$|Abstract. In the e-Science context, {{workflow}} technologies {{provide a}} problem-solving environment for researchers by facilitating {{the creation and}} execution of experiments {{from a pool of}} available data and computation services. Recent activities in the field of social simulation indicate the need to improve the scientific rigour of agent-based modelling by making simulation experiments more transparent. We argue that in order to characterise such experiments we need to go beyond low-level service composition and <b>execution</b> <b>details</b> by capturing higher-level descriptions of the scientific process making the experiment’s constraints and goals transparent. Current workflow technologies do not incorporate any representation of these goals and conditions, which we call the scientist’s intent. Our hypothesis is that by extending workflow representation in this way, researchers would be able to analyse, verify, execute, monitor and re-use scientific workflows more effectively...|$|R
40|$|Abstract. Late Launch, {{which is}} a kind of dynamic {{measurement}} technology proposed by both Intel and AMD, offers isolated execution environment for codes needed to be protected. However, since the specifications and documents of Late Launch have hundreds of pages, they are too long and complicated to be fully covered and analyzed. A model based on Horn clauses is presented to solve the problem that {{there is a lack of}} realistic models and of automated tools for the verification of security protocols based on Late Launch. A running example is taken to show the <b>execution</b> <b>details</b> of Late Launch. Based on the example, secrecy properties of Late Launch are verified. What’s more, the automatic theorem proving tool ProVerif is used to make the verification more fast and accurate...|$|R
40|$|In the e-Science context, {{workflow}} technologies {{provide a}} problem-solving environment for researchers by facilitating {{the creation and}} execution of experiments {{from a pool of}} available data and computation services. Recent activities in the field of social simulation indicate the need to improve the scientific rigour of agent-based modelling by making simulation experiments more transparent. We argue that in order to characterise such experiments we need to go beyond low-level service composition and <b>execution</b> <b>details</b> by capturing higher-level descriptions of the scientific process making the experiment’s constraints and goals transparent. Current workflow technologies do not incorporate any representation of these goals and conditions, which we call the scientist’s intent. Our hypothesis is that by extending workflow representation in this way, researchers would be able to analyse, verify, execute, monitor and re-use scientific workflows more effectively...|$|R
40|$|PROMESAS (S 0505 /TIC/ 0407), TIN 2008 - 06622 -C 03 - 01, S- 0505 /TIC/ 0407, and UCM-BSCH-GR 58 / 08 - 910502. Declarative {{debugging}} is a debugging {{technique that}} abstracts the <b>execution</b> <b>details,</b> {{that can be}} difficult to follow in general in declarative languages, to focus on results. It relies on a data structure called debugging tree, that represents the computation and is traversed by asking questions to the user about the correction of the computation steps related to each node. Thus, the complexity of the questions is an important factor regarding the applicability of the technique. In this paper we present a transformation for debugging trees for Maude specifications that ensures that any subterm occurring in a question has been previously replaced by the most reduced form that it has take...|$|R
40|$|Workflow {{technologies}} provide scientific {{researchers with}} a flexible problem-solving environment, by facilitating {{the creation and}} execution of experiments {{from a pool of}} available services. In this paper we argue that in order to better characterise such experiments we need to go beyond low-level service composition and <b>execution</b> <b>details</b> by capturing higher-level descriptions of the scientific process. Current workflow technologies do not incorporate any representation of such experimental constraints and goals, which we refer to as the scientistpsilas intent. We have developed a framework based upon use of a number of semantic Web technologies, including the OWL ontology language and the semantic Web rule language (SWRL), to capture scientistpsilas intent. Through the use of a social simulation case study we illustrate the benefits of using this framework in terms of workflow monitoring, workflow provenance and enrichment of experimental results...|$|R
40|$|Previous {{research}} has provided metadata models that enable the capturing of the static {{components of a}} Data Warehouse (DW) architecture, along with information on different quality factors over these components. This paper complements this work with the modeling of the dynamic parts of the DW, i. e., with a metamodel for DW operational processes. The proposed metamodel is capable of modeling complex activities, their interrelationships, {{and the relationship of}} activities with data sources and <b>execution</b> <b>details.</b> Finally, the metamodel complements proposed architecture and quality models in a coherent fashion, resulting in a full framework for DW metamodeling, capable of supporting the design, administration and evolution of a DW. We have implemented this metamodel using the language Telos and the metadata repository system ConceptBase. 1 Introduction Data Warehouses (DW) are complex and data-intensive systems that integrate data from multiple heterogeneous information sour [...] ...|$|R
40|$|Abstract. Declarative debuggers are {{semi-automatic}} debugging {{tools that}} abstract the <b>execution</b> <b>details</b> {{to focus on}} the program semantics. Erroneous computations are represented by suitable trees, which are tra-versed by asking questions to the user until a bug is found. This paper applies declarative debugging to the sequential subset of the language Erlang. The debugger takes the intermediate representation generated by Erlang systems, known as Core Erlang, and an initial error detected by the user, and locates an erroneous program function responsible for the error. In order to represent the erroneous computation, a semantic calculus for sequential Core Erlang programs is proposed. The debugger uses an abbreviation of the proof trees of this calculus as debugging trees, which allows us to prove the soundness of the approach. The technique has been implemented in a debugger tool publicly available...|$|R
