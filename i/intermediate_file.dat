38|116|Public
25|$|Link-time {{optimization}} optimizes across object file boundaries {{to directly}} improve the linked binary. Link-time optimization relies on an <b>intermediate</b> <b>file</b> containing the serialization of some Gimple representation {{included in the}} object file. The file is generated alongside the object file during source compilation. Each source compilation generates a separate object file and link-time helper file. When the object files are linked, the compiler is executed again and uses the helper files to optimize code across the separately compiled object files.|$|E
25|$|Croteam {{developed}} their own level editor, Serious Editor 2, and used it {{for the development of}} Serious Sam 2. The editor has significantly more features than the original Serious Editor that was used for the previous Serious Sam games. Serious Editor 2 allows geometry to be imported and exported to and from third-party 3D programs, such as 3D Studio Max, via an <b>intermediate</b> <b>file</b> format, thus allowing for plugins to be easily created for any 3D modeling program. Two methods of creating particle systems exist in the editor, and they can be created either as procedural particle systems or emitter systems. The editor also features its own interpreted language, similar to C++, which allows for relatively simple mod programming, and a script editor and debugger, enabling the level designer to control gameplay events more directly.|$|E
5000|$|Often {{there are}} several steps of program {{translation}} or minification between the original source code typed by a human and an executable program. While some, like the FSF, argue that an <b>intermediate</b> <b>file</b> [...] "is not real source code and does not count as source code", others find it convenient to refer to each <b>intermediate</b> <b>file</b> as the source code for the next steps.|$|E
5000|$|... a {{compiler}} {{that generated}} <b>intermediate</b> <b>files</b> {{which could be}} [...] "fast-loaded" ...|$|R
30|$|<b>Intermediate</b> <b>files</b> {{on local}} disks are {{required}} between {{each of the}} map, combiner, and reduce stages executed for traditional MapReduce.|$|R
5000|$|Compacting the input, <b>intermediate</b> <b>files,</b> {{and output}} can reduce {{time spent on}} I/O, but is not allowed in the Sort Benchmark.|$|R
50|$|SPOOL is set if {{the file}} is a spool file, {{such as an}} <b>intermediate</b> <b>file</b> used during printing.|$|E
50|$|Some {{relational}} databases {{can also}} be deployed on mass storage devices without an <b>intermediate</b> <b>file</b> system or storage manager. Oracle and MySQL, for example, can store table data directly on raw block devices.|$|E
5000|$|Another {{solution}} is to write out an <b>intermediate</b> <b>file</b> listing the offsets and other information from the compile stage, known as meta-data. The linker then uses this information to correct itself when the library is loaded into an application. Platforms such as [...]NET do this.|$|E
50|$|A {{motion picture}} film scanner is a device used in digital filmmaking to scan {{original}} film for storage as high-resolution digital <b>intermediate</b> <b>files.</b>|$|R
30|$|The map task {{from the}} two {{previous}} applications produces output files that are a little larger than the initial input. Therefore, the amount of data produced by mappers (and then sent to reducers) {{is similar to the}} volume received by mappers as input. On the other hand, for each 5 MB input, N-Gram’s map task creates around 30 MB of <b>intermediate</b> <b>files</b> which must be transferred to reducers. Therefore, N-Gram is helpful in assessing the performance of our system with applications with large <b>intermediate</b> <b>files.</b>|$|R
30|$|Second, the 158 <b>intermediate</b> <b>files</b> were merged, and the {{frequency}} of each keyword was counted. We developed a java program named merge.java (Additional file 1 : Appendix C) using Eclipse software. When this program was run, the <b>intermediate</b> <b>files</b> defined in the input parameters were opened, and the keywords and their counters were saved to a HashMap. Then the keywords were counted again with HashMap traversal algorithm: the counters of the same keywords were added. Then, the HashMap traversal result was saved to an array, sorted by the counters, and exported into a result file.|$|R
5000|$|Link-time {{optimization}} optimizes across object file boundaries {{to directly}} improve the linked binary. Link-time optimization relies on an <b>intermediate</b> <b>file</b> containing the serialization of some Gimple representation {{included in the}} object file. The file is generated alongside the object file during source compilation. Each source compilation generates a separate object file and link-time helper file. When the object files are linked, the compiler is executed again and uses the helper files to optimize code across the separately compiled object files.|$|E
50|$|Croteam {{developed}} their own level editor, Serious Editor 2, and used it {{for the development of}} Serious Sam 2. The editor has significantly more features than the original Serious Editor that was used for the previous Serious Sam games. Serious Editor 2 allows geometry to be imported and exported to and from third-party 3D programs, such as 3D Studio Max, via an <b>intermediate</b> <b>file</b> format, thus allowing for plugins to be easily created for any 3D modeling program. Two methods of creating particle systems exist in the editor, and they can be created either as procedural particle systems or emitter systems. The editor also features its own interpreted language, similar to C++, which allows for relatively simple mod programming, and a script editor and debugger, enabling the level designer to control gameplay events more directly.|$|E
50|$|When a file is {{exported}} by one CAD program into an <b>intermediate</b> <b>file</b> {{format and}} opened in another CAD program, {{it is not}} unusual for translation errors to occur. This inability to reliably transfer files between disparate programs is especially problematic with 3D solid modeling software, because of behind-the-scenes technical complexities that arise whenever complex surfaces abut or blend into each other; surfaces no longer align or some features do not translate due to the way CAD programs employ different approaches to handling certain object classes. To minimize translation errors, TransMagic typically—but not always—translates directly from one native CAD kernel to another. Still, “stitching errors” (gaps and overlaps) can occur while trying to import the file and reinterpret geometry. TransMagic’s “Auto Repair Wizard” corrects these flaws while translating the file.|$|E
30|$|We {{were able}} to {{conclude}} that VMR not only can perform better than VCS when running jobs with large <b>intermediate</b> <b>files,</b> but is also able to significantly remove {{the burden on the}} server’s network connection. It {{is important to note that}} the changes we introduced did not create any significant or visible overhead on the server side. Considering these results, and taking into consideration the typical compute-intensive nature of VC applications, we can determine that MapReduce jobs with large <b>intermediate</b> <b>files</b> are more suited (than others with smaller <b>intermediate</b> <b>files)</b> for VC. This can be explained by the reduced overhead on the server, since clients handle most of the heavy data communication among themselves. Furthermore, small input and input data can reduce the influence of server-client transfers. There are quite a few examples of real-world applications with these characteristics, such as image rendering [22], market basket analysis, and N-Gram based functions (e.g., biological sequence analysis [23]).|$|R
30|$|Therefore, we can {{conclude}} that VMR not only can perform better than VCS when running jobs with large <b>intermediate</b> <b>files,</b> but is also able to alleviate the server’s network connection.|$|R
30|$|Fourth, we {{developed}} a batch program named count.bat (Additional file 1 : Appendix E) to call the count.class with the input parameters “DE” and “ID”. All 158 text files were processed one by one. As a result, 158 <b>intermediate</b> <b>files</b> were created.|$|R
5000|$|Where batch {{processing}} remains in use, the outputs of separate stages (and input for the subsequent stage) are typically stored as files. This {{is often used}} for ease of development and debugging, as it allows intermediate data to be reused or inspected. For example, to process data using two program [...] and , one might get initial data from a file , and store the ultimate result in a file [...] Via {{batch processing}}, one can use an <b>intermediate</b> <b>file,</b> , and run the steps in sequence (Unix syntax):step1 < input > intermediatestep2 < intermediate > outputThis batch processing can be replaced with a flow: the intermediary file can be elided with a pipe, feeding output from one step to the next as it becomes available:step1 < input | step2 > output ...|$|E
5000|$|In the initramfs scheme (available {{since the}} Linux kernel 2.6.13), the image {{may be a}} cpio archive (optionally compressed). The archive is {{unpacked}} by the kernel into a special instance of a tmpfs that becomes the initial root file system. This scheme {{has the advantage of}} not requiring an <b>intermediate</b> <b>file</b> system or block drivers to be compiled into the kernel. Some systems use the dracut package to create an initramfs image. [...] In the initramfs scheme, the kernel executes [...] as its first process that is not expected to exit.. For some applications, initramfs can use the casper utility to create a writable environment using unionfs to overlay a persistence layer over a read-only root filesystem image. For example, overlay data can be stored on a USB flash drive, while a compressed SquashFS read-only image stored on a live CD acts as a root filesystem.|$|E
50|$|As each CAD {{system has}} its own method of {{describing}} geometry, both mathematically and structurally, there is always some loss of information when translating data from one CAD data format to another. One example is when the translation occurs between CAD systems using different geometric modeling kernels, in which the translation inconsistencies can lead to anomalies in the data. The <b>intermediate</b> <b>file</b> formats are also limited in what they can describe, {{and they can be}} interpreted differently by both the sending and receiving systems. It is, therefore, important when transferring data between systems to identify what needs to be translated. If only the 3D model is required for the downstream process, then only the model description needs to be transferred. However, there are levels of detail. For example: is the data wireframe, surface, or solid; is the topology (BREP) information required; must the face and edge identifications be preserved on subsequent modification; must the feature information and history be preserved between systems; and is PMI annotation to be transferred. With product models, retaining the assembly structure may be required. If drawings need to be translated, the wireframe geometry is normally not an issue; however text, dimensions and other annotation can be an issue, particularly fonts and formats. No matter what data is to be translated, there is also a need to preserve attributes (such as color and layer of graphical objects) and metadata stored within the files.|$|E
40|$|Many {{scientific}} computations can {{be expressed}} as Many-Task Computing (MTC) applications. In such scenarios, application processes communicate by means of <b>intermediate</b> <b>files,</b> in particular input, temporary data generated during job exe-cution (stored in a runtime file system), and output. In data-intensive scenarios...|$|R
30|$|Fifth, we {{developed}} another batch program named merge.bat (Additional file 1 : Appendix F) {{to call the}} merge.class with the input parameters, that is, the 158 <b>intermediate</b> <b>files,</b> to merge them. As a result, a final file was created, in which all keywords in 78, 986 articles were counted and sorted.|$|R
50|$|The Media Dispatch Protocol (MDP) was {{developed}} by the Pro-MPEG Media Dispatch Group to provide an open standard for secure, automated, and tapeless delivery of audio, video and associated data files. Such files typically range from low resolution content for the web to HDTV and high-resolution digital <b>intermediate</b> <b>files</b> for cinema production.|$|R
40|$|This {{document}} {{discusses the}} design {{and implementation of the}} Worstcase Execution Analyser (hereafter `the analyser'), specified in the Task 4 deliverable, Definition of the Tools[1]. ESTEC/Contract No. 9198 / 90 /NL/SF ESTEC TECHNICAL MANAGER: Mr F. Gomez-Molinero Ó York Software Engineering Limited, TABLE OF CONTENTS SECTION TITLE PAGE 1 INTRODUCTION 1 2 RESTRICTIONS ON ADA 2 2. 1 Pragma LOOPCOUNT 2 2. 2 Further Requirements of This Implementation 3 3 IMPLEMENTATION OVERVIEW 4 4 OBJECT CODE ANALYSIS 4 4. 1 Disassembler 4 4. 2 Basic Blocks and Flow Graphs 6 4. 3 Ada Structure Graph 7 4. 4 Instruction Timing Data 9 4. 5 Correlating Basic Blocks and Ada Blocks 10 4. 6 Output Files 10 4. 7 Program Size 10 5 SEGMENTS AND SEGMENT TREES 11 6 PROBLEMS PECULIAR TO THIS IMPLEMENTATION 12 6. 1 Name Mapping 13 6. 2 Code Generated Out of Order 14 7 EXAMPLE 15 REFERENCES 17 A FORMAT OF THE. B <b>INTERMEDIATE</b> <b>FILE</b> 18 B FORMAT OF THE. W <b>INTERMEDIATE</b> <b>FILE</b> 22 C FORMAT OF THE. S <b>INTERMEDIATE</b> <b>FILE</b> 25 D RESOLVER OUTPUT [...] ...|$|E
30|$|Strand’s map {{reduction}} aggregation methods {{take advantage}} of the parallelism constructs afforded by the MapReduce model while avoiding much of the overhead associated with <b>intermediate</b> <b>file</b> disk I/O, sorting, and inter-process communication between the master and worker processes located on different commodity hardware machines.|$|E
40|$|Often CAD models {{already exist}} for {{parts of a}} {{geometry}} being simulated using GEANT 4. Direct import of these CAD models into GEANT 4 however,may not be possible and complex components may be diffcult to define via other means. Solutions that allow for users to work around the limited support in the GEANT 4 toolkit for loading predefined CAD geometries have been presented by others, however these solutions require <b>intermediate</b> <b>file</b> format conversion using commercial software. Here within we describe a technique that allows for CAD models to be directly loaded as geometry {{without the need for}} commercial software and <b>intermediate</b> <b>file</b> format conversion. Robustness of the interface was tested using a set of CAD models of various complexity; for the models used in testing, no import errors were reported and all geometry was found to be navigable by GEANT 4. Funding source: Cancer Australia (Department of Health and Ageing) Research Grant 61421...|$|E
5000|$|A source {{package is}} created using the [...] tool or its wrapper [...] When invoked {{to create a}} source package, [...] calls the maintainer's rules to clean the source tree of any <b>intermediate</b> <b>files,</b> does various sanity checks, and finally, signs the [...] file with the packager's key using the [...] utility.|$|R
40|$|Abstract—PHCpack {{is a large}} {{software}} package for solving systems of polyno-mial equations. The executable phc is menu driven and file oriented. This paper describes the development of phcpy, a Python interface to PHCpack. Instead of navigating through menus, users of phcpy solve systems in the Python shell or via scripts. Persistent objects replace <b>intermediate</b> <b>files.</b> ...|$|R
40|$|This is a {{snapshot}} of the code used in the Injected Interstitial paper, used in an implementation of the radiation point defect balance and kinetics equations. It contains all the code used, <b>intermediate</b> <b>files</b> from SRIM, data files, input files, and output files used in generating the paper. It requires MOOSE, LibMesh, and PETSc to run. The git commit hash of the version of MOOSE used in this code is 59 daa 6 a 23 ec 1 b 222 e 83759 cea 4 a 64 ede 383397 fb, and the MOOSE repository can be found at [URL] This snapshot is provided after the initial round of paper reviews, and incorporates requested and other changes to the code, input parameters, and output files. It also includes all <b>intermediate</b> <b>files</b> used to generate the figures and results in the paper, and a README. txt file explaining how to use this code to generate this paper's results...|$|R
40|$|Typically {{used as a}} {{tool for}} Monte Carlo {{simulation}} of high energy physics experiments, GEANT 4 is increasingly being employed for the simulation of complex radiotherapy treatments. Often the specification of components within a clinical linear accelerator treatment head is provided in a CAD file format. Direct import of these CAD files into GEANT 4 may not be possible, and complex components such as individual leaves within a multi-leaf collimator may be difficult to define via other means. Solutions that allow for users to work around the limited support in the GEANT 4 toolkit for loading predefined CAD geometries has been presented by others, however these solutions require <b>intermediate</b> <b>file</b> format conversion using commercial software. Here within we describe a technique that allows for CAD models to be directly loaded as geometry without the need for commercial software and <b>intermediate</b> <b>file</b> format conversion. Robustness of the interface was tested using a set of CAD models of various complexity; for the models used in testing, no import errors were reported and all geometry was found to be navigable by GEANT 4...|$|E
40|$|Summary: Mass spectrometry-based {{proteomics}} {{stands to}} gain from additional analysis of its data, but its large, complex datasets make demands on speed and memory usage requiring special consideration from scripting languages. The software library ‘mspire’—developed in the Ruby programming language—offers quick and memory-efficient readers for standard xml proteomics formats, converters for <b>intermediate</b> <b>file</b> types in typical proteomics spectral-identification work flows (including the Bioworks. srf format), and modules for the calculation of peptide false identification rates...|$|E
40|$|At this time, X LATEX {{produces}} a final PDF output file {{but it gets}} this output {{by means of the}} transformation of a XDV (extended DVI) <b>intermediate</b> <b>file.</b> This excludes most of the possibilities offered by pdfLATEX that, since version 1. 40. 9 {{and with the help of}} an extension file pdfx. sty, can directly produce a PDF/A- 1 b compliant output. This paper explains how to overcome this by resorting to the ubiquitous Ghostscript program...|$|E
5000|$|Obash {{takes the}} input script and aes-256 encodes it, and also base64 encodes the AES cipertex {{so that it}} can be used to declare an {{unsigned}} char array.It then produces an <b>intermediate</b> c <b>file</b> which is basically the interpreter (see interpreter.c), functions, text array containing the cipher text, the optional raw key and iv for reusable binaries (not bound to the hardware) and the main. The <b>intermediate</b> c <b>file</b> is then compiled into an executable.The <b>intermediate</b> c <b>file</b> is built in the following manner (see mk_sh_c function in functions.c):includes block from interpreter.hcrypted_script variable containing the base64 aes-256 encoded scriptserial and uuid variables (empty if non reusable) functions block from interpreter.hmain_body block from interpreter.h ...|$|R
5000|$|XeTeX {{processes}} input in two stages. In {{the first}} stage XeTeX outputs an extended DVI (...) file, which is then converted to PDF by a driver. In the default operating mode the [...] output is piped directly to the driver without producing any user-visible <b>intermediate</b> <b>files.</b> It is possible to run just {{the first stage}} of XeTeX and save the , although [...] there are no viewers capable of displaying the intermediate format.|$|R
30|$|We use {{an initial}} input text file of 1 GB, divided into 100 chunks (one 10 MB chunk per map task), {{in the word}} count and {{inverted}} index experiments. For the N-Gram application, we also split the input into 100 chunks, but each chunk is only 5 MB in size, due to the larger size of <b>intermediate</b> <b>files.</b> The NAS EP benchmark receives as input a small file with a number range and an initial seed (for a random number generator).|$|R
