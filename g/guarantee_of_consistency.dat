10|10000|Public
500|$|At {{some point}} in the 1896 season, Hogan {{apparently}} secured his release from the Browns. Once again, however, his physical speed offered no <b>guarantee</b> <b>of</b> <b>consistency</b> on the playing field. [...] On July 21, 1896, the St. Paul (Minneapolis) Globe reported that Hogan had performed poorly in a contest between the Hoosiers and the local ball club. [...] "Marty Hogan...made a bad fumble, and then {{looked up at the sky}} to see if it had moved while he was locating the ball", the paper reported. [...] "It was a good bluff, but the crowd discovered Marty's weakness before the end of the game". [...] In January 1897, the Vindicator reported that the Hoosiers had sold Hogan to a club in Grand Rapids, Michigan. [...] The article called Hogan [...] "one of the fastest outfielders and baserunners in the Western League" [...] and predicted he would [...] "greatly strengthen the Grand Rapids outfield". [...] The following month, however, the paper described the previous report as a [...] "mistake", indicating instead that Hogan had signed a contract with baseball executive John T. Brush to play with the Hoosiers for another year. [...] The paper also reported that the contract granted Hogan [...] "the largest salary he has ever drawn". [...] Despite this lucrative contract, Hogan established and maintained his own advertising distribution agency in Indianapolis. [...] "He goes about the streets dressed much like an English costermonger", Sporting Life reported in January 1897. [...] "Marty can be seen with a little red wagon full of signs and advertising matter chasing up and down streets nailing the signs to buildings and convenient places and distributing advertising literature in the reel-dance portion of Indianapolis". [...] Less than four months later, in May 1897, he was released by the Indianapolis ball club. In June 1897, the Kansas City Journal indicated Hogan had moved on to the Dayton (Ohio) Old Soldiers, a team affiliated with the Class B Interstate League, where he was [...] "playing a sensational center field". In October of the same year, Sporting Life speculated Hogan would remain with Dayton during the upcoming 1898 season. [...] "Marty Hogan's contract with Dayton is such that he cannot be reserved, as are the rest of the players", the article stated. [...] "Unless Marty has a better offer to play with some other team it is safe to say that he will be with Dayton next year". [...] Further research is required to determine how long Hogan continued to play as an outfielder in the minor leagues. [...] (His obituary indicated that he also worked as a major league trainer.) [...] During his playing career, he apparently received at least one serious injury. [...] In February 1903, Sporting Life reported that the former baseball player's friends were [...] "anxious to get him appointed on the staff of American League umpires". [...] The article added, [...] "Hogan has suffered from operations to remove portions of his breast bone, which was injured in a collision during a baseball game".|$|E
50|$|The agency reviewers {{assign a}} ranking, but are given no {{guidelines}} {{about how to}} do this so there is no <b>guarantee</b> <b>of</b> <b>consistency</b> from one reviewer to the next or even for the same reviewer across different projects.|$|E
50|$|Oracle NoSQL Database is {{configurable}} to {{be either}} C/P or A/P in CAP. In particular, if writes are configured to be performed synchronously to all replicas, it is C/P in CAP i.e. a partition or node failure causes the system to be unavailable for writes. If replication is performed asynchronously, and reads are configured to be served from any replica, it is A/P in CAP i.e. the system is always available, {{but there is no}} <b>guarantee</b> <b>of</b> <b>consistency.</b>|$|E
50|$|Boosting {{refers to}} a family of {{algorithms}} in which a set of weak learners (learners that are only slightly correlated with the true process) are combined to produce a strong learner. It has been shown, for several boosting algorithms (including AdaBoost), that regularization via early stopping can provide <b>guarantees</b> <b>of</b> <b>consistency,</b> that is, that {{the result of the}} algorithm approaches the true solution as the number of samples goes to infinity.|$|R
40|$|This chapter surveys {{extended}} transaction models {{proposed to}} support long duration, interactive and/or cooperative {{activities in the}} context of multi-user software development and CAD/CAM environments. Many of these are variants of the checkout model, which addresses the long duration and interactive nature of the activities supported by environments but still isolates environment users, making {{it difficult for them to}} collaborate while their activities are in progress. However, a few cooperative transaction models have been proposed to facilitate collaboration, usually while maintaining some <b>guarantees</b> <b>of</b> <b>consistency...</b>|$|R
40|$|Abstract. Abstract State Machines (asms) offer a {{formalism}} {{for describing}} state transitions over relational structures. This makes them promising for modeling system {{features such as}} access control, especially {{in an environment where}} the policy’s outcome depends on the evolving state of the system. The current notions of modularity for asms, however, provide insufficiently strong <b>guarantees</b> <b>of</b> <b>consistency</b> in the face of parallel update requests. We present a real-world context that illustrates this problem, discuss desirable properties for composition in this context, describe an operator that exhibits these properties, formalize its meaning, and outline its implementation strategy. ...|$|R
5000|$|At {{some point}} in the 1896 season, Hogan {{apparently}} secured his release from the Browns. Once again, however, his physical speed offered no <b>guarantee</b> <b>of</b> <b>consistency</b> on the playing field. On July 21, 1896, the St. Paul (Minneapolis) Globe reported that Hogan had performed poorly in a contest between the Hoosiers and the local ball club. [...] "Marty Hogan...made a bad fumble, and then {{looked up at the sky}} to see if it had moved while he was locating the ball", the paper reported. [...] "It was a good bluff, but the crowd discovered Marty's weakness before the end of the game". [...] In January 1897, the Vindicator reported that the Hoosiers had sold Hogan to a club in Grand Rapids, Michigan. [...] The article called Hogan [...] "one of the fastest outfielders and baserunners in the Western League" [...] and predicted he would [...] "greatly strengthen the Grand Rapids outfield". [...] The following month, however, the paper described the previous report as a [...] "mistake", indicating instead that Hogan had signed a contract with baseball executive John T. Brush to play with the Hoosiers for another year. [...] The paper also reported that the contract granted Hogan [...] "the largest salary he has ever drawn". [...] Despite this lucrative contract, Hogan established and maintained his own advertising distribution agency in Indianapolis. [...] "He goes about the streets dressed much like an English costermonger", Sporting Life reported in January 1897. [...] "Marty can be seen with a little red wagon full of signs and advertising matter chasing up and down streets nailing the signs to buildings and convenient places and distributing advertising literature in the reel-dance portion of Indianapolis". [...] Less than four months later, in May 1897, he was released by the Indianapolis ball club. In June 1897, the Kansas City Journal indicated Hogan had moved on to the Dayton (Ohio) Old Soldiers, a team affiliated with the Class B Interstate League, where he was [...] "playing a sensational center field". In October of the same year, Sporting Life speculated Hogan would remain with Dayton during the upcoming 1898 season. [...] "Marty Hogan's contract with Dayton is such that he cannot be reserved, as are the rest of the players", the article stated. [...] "Unless Marty has a better offer to play with some other team it is safe to say that he will be with Dayton next year". [...] Further research is required to determine how long Hogan continued to play as an outfielder in the minor leagues. (His obituary indicated that he also worked as a major league trainer.) During his playing career, he apparently received at least one serious injury. [...] In February 1903, Sporting Life reported that the former baseball player's friends were [...] "anxious to get him appointed on the staff of American League umpires". [...] The article added, [...] "Hogan has suffered from operations to remove portions of his breast bone, which was injured in a collision during a baseball game".|$|E
40|$|In an {{experiment}} featuring nonlinear optics, delayed choice and EPRtype correlations, {{the possibility of}} faster–than–light communication appears not totally implausible. Attempts are put forward and discussed to refute this claim. Quantum theory and special relativity theory are the cornerstones of today’s physical perception. Obviously their intrinsic consistency {{as well as the}} consistency of a combined theory is of greatest relevance. Thus one of the most discomforting results of 20 th century mathematics and physics is the conclusion that with respect to consistency of theories, in general no affirmative answer can be given — It must be clearly spelled out that the mere introduction of “manifestly covariant ” entities, such as Lorentz invariant spinors and tensors in quantum electrodynamics, is an insufficient <b>guarantee</b> <b>of</b> <b>consistency.</b> Any attempt to “prove” consistency in nontrivial theory contexts, such as for instance Shirokov’s remarkable investigation [1], must inevitably be too specific or even misleading...|$|E
40|$|Accreditation {{requirements}} for undergraduate programs for professional engineers require final year students to complete capstone projects, but currently {{there is no}} gauge or <b>guarantee</b> <b>of</b> <b>consistency.</b> Practices differ greatly between universities and little work has been initiated that seeks to identify good practice. The literature shows {{that there are no}} definite or guaranteed assessment criteria for assessing the Final Year Engineering Projects (FYEPs) highlighting the need for the development of guidelines for the FYEPs and assessment criteria. This paper presents a review on the FYEPs learning and teaching methodologies as employed across several universities at national and international levels and is part of a wider Office for Learning and Teaching (OLT) commissioned study - Assessing FYEPs: Ensuring Learning and Teaching Standards and Australian Qualification Framework (AQF 8) Outcomes. This study is intended to promote quality practice amongst supervisors and academics involved in teaching and facilitating FYEPs in Australian universities. This preliminary literature review comprises one part of this wider study...|$|E
5000|$|ABC-M: Management. <b>Guarantees</b> <b>consistency</b> <b>of</b> {{databases}} {{among all}} Alcatel OmniPCXs. It {{is used to}} inform the whole network about configuration changes and provides centralized alarms.|$|R
50|$|Dataspaces {{shift the}} {{emphasis}} to a data co-existence approach providing base functionality over all data sources, {{regardless of how}} integrated they are. For example, a DataSpace Support Platform (DSSP) can provide keyword search over all of its data sources, similar to that provided by existing desktop search systems. When more sophisticated operations are required, such as relational-style queries, data mining, or monitoring over certain sources, then additional effort {{can be applied to}} more closely integrate those sources in an incremental fashion. Similarly, in terms <b>of</b> traditional database <b>guarantees,</b> initially a dataspace system can only provide weaker <b>guarantees</b> <b>of</b> <b>consistency</b> and durability. As stronger guarantees are desired, more effort can be put into making agreements among the various owners of data sources, and opening up certain interfaces (e.g., for commit protocols).|$|R
40|$|Workstations {{typically}} {{depend on}} remote servers accessed over a network for such services as mail, printing, storing files, booting, and time. The availability of these remote services {{has a major}} impact on the usability of the workstation. Availability can be increased by repli-cating the servers. In the Echo distributed file system at DEC SRC, two different replication techniques are employed, one at the upper levels of our hierarchical name space, the name service, and another at the lower levels of the name space, the file volume service. The two replication techniques provide different <b>guarantees</b> <b>of</b> <b>consistency</b> be-tween their replicas and, therefore, different levels of availability. Echo also caches data from the name service and file volume service in client machines (e. g., workstations), with the cache for each service having its own cache consistency guarantee that mimics the <b>guarantee</b> on the <b>consistency</b> <b>of</b> the replicas for that service. The replication and caching consistency guarantees provided by each service are appropriate for its intended use...|$|R
40|$|The CERN X-Window User Interface Management System (XUIMS) is a modular {{and highly}} {{configurable}} software development environment allowing the interactive design, prototyping, {{and production of}} OSF/Motif Human Computer Interfaces (HCI). Fully compliant with the X 11 R 5 and OSF/Motif industry standards, XUIMS covers complex software areas like the development of schematics, the visualization and on-line interactions with 2 D and 3 D scientific data, the display of relational database data, and the direct access to CERN SPS and LEP accelerators equipment. The <b>guarantee</b> <b>of</b> <b>consistency</b> across the applications and the encapsulation of complex functionality in re-usable and user-friendly components has also been implemented {{through the development of}} home made graphical objects (widgets) and templates. The XUIMS environment is built with commercial software products integrated in the CERN SPS and LEP controls infrastructure with a very limited home-made effort. Productivity and quality have been improved through less coding and better HCI prototyping...|$|E
40|$|Model {{selection}} {{is critical to}} least squares support vector machine (LSSVM). A major prob-lem of existing model selection approaches of LSSVM is that the inverse of the kernel matrix need to be calculated with O(n 3) complexity for each iteration, where n {{is the number of}} training examples. It is prohibitive for the large scale application. In this paper, we pro-pose an approximate approach to model selection of LSSVM. We use multilevel circulant matrices to approximate the kernel matrix so that the fast Fourier transform (FFT) can be applied to reduce the computational cost of matrix inverse. With such approximation, we first design an efficient LSSVM algorithm with O(n log(n)) complexity and theoretically analyze the effect of kernel matrix approximation on the decision function of LSSVM. We further show that the approximate optimal model produced with the multilevel circulant matrix is consistent with the accurate one produced with the original kernel matrix. Under the <b>guarantee</b> <b>of</b> <b>consistency,</b> we present an approximate model selection scheme, whose complexity is significantly lower than the previous approaches. Experimental results on benchmark datasets demonstrate the effectiveness of approximate model selection...|$|E
40|$|Rigid bodies, plastic impact, {{persistent}} contact, Coulomb friction, and massless limbs are ubiquitous simplifications {{introduced to}} reduce the complexity of mechanics models despite the obvious physical inaccuracies that each incurs individually. In concert, {{it is well known}} that the interaction of such idealized approximations can lead to conflicting and even paradoxical results. As robotics modeling moves from the consideration of isolated behaviors to the analysis of tasks requiring their composition, a mathematically tractable framework for building models that combine these simple approximations yet achieve reliable results is overdue. In this paper we present a formal hybrid dynamical system model that introduces suitably restricted compositions of these familiar abstractions with the <b>guarantee</b> <b>of</b> <b>consistency</b> analogous to global existence and uniqueness in classical dynamical systems. The hybrid system developed here provides a discontinuous but self-consistent approximation to the continuous (though possibly very stiff and fast) dynamics of a physical robot undergoing intermittent impacts. The modeling choices sacrifice some quantitative numerical efficiencies while maintaining qualitatively correct and analytically tractable results with consistency guarantees promoting their use in formal reasoning about mechanism, feedback control, and behavior design in robots that make and break contact with their environment. For more information: Kod*La...|$|E
40|$|This chapter surveys {{extended}} transaction models {{proposed to}} support long duration, interactive and/or cooperative {{activities in the}} context of multi-user software development and CAD/CAM environments. Many of these are variants of the checkout model, which addresses the long duration and interactive nature of the activities supported by environments but still isolates environment users, making {{it difficult for them to}} collaborate while their activities are in progress. However, a few cooperative transaction models have been proposed to facilitate collaboration, usually while maintaining some <b>guarantees</b> <b>of</b> <b>consistency.</b> To appear as a chapter in Won Kim (ed.), Modern Database Management: Object-Oriented and Multidatabase Technologies, ACM Press. Conventional database transactions guarantee atomicity in two senses: concurrency atomicity, for consistent concurrent access, and failure atomicity, for consistent persistence. Unfortunately, these atomicity properties severely limit the applicability of the transaction concept with respect to modern database applications such as software development and CAD/CAM environments, even though thes...|$|R
40|$|Abstract. Distributed {{database}} management protocols are usually {{based on the}} assumption that data consistency is paramount. However, this assumption is invalid for distributed real-time applications such as ship combat systems and air traffic control, where data availability is crucial and may need to be achieved at the expense <b>of</b> <b>guarantees</b> <b>of</b> data <b>consistency.</b> Two protocols designed for real-time combat systems which achieve availability by relaxing consistency restraints are ADDAM (AR...|$|R
40|$|An {{intelligent}} agent uses known facts, including statistical knowledge, to assign degrees of belief to assertions {{it is uncertain}} about. We investigate three principled techniques for doing this. All three are applications {{of the principle of}} indi erence, because they assign equal degree of belief to all basic &quot; consistent with the knowledge base. They di er because there are competing intuitions about what the basic situations are. Various natural patterns of reasoning, such as the preference for the most speci c statistical data available, turn out to follow from {{some or all of the}} techniques. This is an improvement over earlier theories, such aswork on direct inference and reference classes, which arbitrarily postulate these patterns without o ering any deeper explanations or <b>guarantees</b> <b>of</b> <b>consistency.</b> The three methods we investigate have surprising characterizations: there are connections to the principle of maximum entropy, a principle of maximal independence, and a of mass &quot; principle. There are also unexpected connections between the three, that help us understand why the speci c language chosen (for the knowledge base) is much more critical in inductive reasoning of the sort we consider than it is in traditional deductive reasoning. ...|$|R
40|$|Undergraduate {{engineering}} programs require {{final year}} students to complete capstone final year projects and {{demonstrate that they}} can integrate knowledge, skills and professional graduate attributes developed during the program at Australian Qualification Framework, level 8 (AQF 8) outcomes. Literature shows that currently there is no <b>guarantee</b> <b>of</b> <b>consistency</b> for curriculum, supervision and assessment practices of FYEPs. Practices differ greatly between universities and littlework has been initiated that seeks to identify good practice, highlighting {{the need for the}} development of guidelines for curriculum, supervision and assessment of FYEPs. This workshop is designed to share and disseminate the good practice guidelines that have been developed on curriculum, supervision and assessment of Final Year Engineering Projects as a part of phase 2 of the project ‘Assessing Final Year Engineering Projects (FYEPs) : Ensuring Learning and Teaching Standards and AQF 8 Outcomes’ funded by the Australian Office for Learning and Teaching (OLT) with people working in the area of FYEPs. The guidelines typically apply to four year undergraduate engineering degrees with embedded Honours and support achievement of AQF 8 learning outcomes. The project team has 7 partner Universities – Central Queensland University (the lead), University of Technology Sydney, University of Adelaide, Curtin University, Deakin University, University of Tasmania and RMIT University. Participants will be invited to reflect on and evaluate guidelines and findings derived from FYEP coordinators, supervisors and the wider literature and to consider the ways in which these findings might lead to improvements in their practice...|$|E
40|$|Support for {{ontology}} {{evolution is}} extremely important in ontology engineering and application of ontologies in dynamic environments. A core aspect in the evolution process is the to <b>guarantee</b> <b>consistency</b> <b>of</b> the ontology when changes occur. In this paper we discuss the consistent evolution of OWL ontologies...|$|R
40|$|This work {{presents}} an {{implementation of a}} distributed system building block that is formally speci ed as the Eventually-Serializable Data Service (ESDS) [7] proposed by Fekete et al. ESDS deals with replicated objects that allow the users of the service to relax consistency requirements in return for improved responsiveness, while providing <b>guarantees</b> <b>of</b> eventual <b>consistency</b> <b>of</b> the replicated data. The ESDS paper [7] includes a formal service speci cation and an abstract algorithm implementing the service. The algorithm is given in terms of I/O automata of Lynch and Tuttle [15]. An important consideration in formulating ESDS was {{that it could be}} employed in building real systems...|$|R
50|$|Aggregate: A {{collection}} {{of objects that}} are bound together by a root entity, otherwise known as an aggregate root. The aggregate root <b>guarantees</b> the <b>consistency</b> <b>of</b> changes being made within the aggregate by forbidding external objects from holding references to its members.|$|R
40|$|We {{consider}} the predictive problem of supervised ranking, where {{the task is}} to rank sets of candidate items returned in response to queries. Although there exist statistical procedures that come with <b>guarantees</b> <b>of</b> <b>consistency</b> in this setting, these procedures require that individuals provide a complete ranking of all items, which is rarely feasible in practice. Instead, individuals routinely provide partial preference information, such as pairwise comparisons of items, and more practical approaches to ranking have aimed at modeling this partial preference data directly. As we show, however, such an approach raises serious theoretical challenges. Indeed, we demonstrate that many commonly used surrogate losses for pairwise comparison data do not yield consistency; surprisingly, we show inconsistency even in low-noise settings. With these negative results as motivation, we present {{a new approach to}} supervised ranking based on aggregation of partial preferences, and we develop $U$-statistic-based empirical risk minimization procedures. We present an asymptotic analysis of these new procedures, showing that they yield consistency results that parallel those available for classification. We complement our theoretical results with an experiment studying the new procedures in a large-scale web-ranking task. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|Through the {{application}} to the risk based design of an Allyl Chloride production plant, {{the authors would like}} to illustrate the methodological approaches developed by two process safety pioneers, Norberto Piccinini and Remo Galvagni, both scientists and gentlemen, who devoted their professional lives to develop novel approaches to enhance accident prediction and prevention and to train young scientists. In particular, Norberto Piccinini has to be mentioned to have developed the Recursive Operability Analysis, a step ahead in the HazOp techniques, able to directly obtain the Logic Trees from the tables of the operability analysis, thus guaranteeing the coherence between the hazard identification step and the quantification step in the quantitative risk assessment. Remo Galvagni instead conceived and developed the Integrated Dynamic Decision Analysis (IDDA), an Event Tree empowered with conditionings, both logic and probabilistic. The tool, aimed to a correct and coherent application of probability theory according to the De Finetti's principles, allows the logical-probabilistic model to run integrated with a deterministic model of the plant to have <b>guarantees</b> <b>of</b> <b>consistency</b> and completeness in a risk assessment used as a basis for a proper plant design. Comparing the qualitative and quantitative results of the two methods applied to the same case study allows discussing their effectiveness in supporting the risk based decision-making...|$|R
40|$|Atomic actions {{represents}} {{a powerful tool}} for system structuring, controlling accesses to shared data implementing backward error recovery techniques. In this paper properties of atomic actions are analyzed, specifically referring to crash recovery problems. In order to <b>guarantee</b> <b>consistency</b> <b>of</b> shared data in spite of crashes, programs implementing atomic actions must satisfy some constraints...|$|R
40|$|Most {{existing}} {{group membership}} protocols such as Abbadi et al’s View Formation (VF) protocol {{assume that the}} network infrastructure is static and reliable. This, however, {{is not the case}} in ad hoc networks. VF makes an excessive use of aborts to deal with simultaneous attempts to change the group configuration with the objective <b>of</b> <b>guaranteeing</b> <b>consistency,</b> forcin...|$|R
30|$|Tunable {{consistency}} guarantees in {{big data}} stores help in achieving optimized consistency guarantees with improved performance. Commercial data stores offer tunable consistency guarantees at transaction level where the user specifies the desired level <b>of</b> <b>consistency</b> in terms <b>of</b> number of participating replicas in {{read and write}} consensus. Selective data consistency model applies strict consistency to a subset of data objects. The <b>consistency</b> <b>guarantees</b> <b>of</b> data attributes or objects are measured using an application independent metric called consistency index (CI). Our consistency model is predictive and helps in expression <b>of</b> data <b>consistency</b> {{as a function of}} known database design parameters, like workload characteristics and number of replicas of the data object. This work extends the causal relationships presented in our earlier work and presents adaptive <b>consistency</b> <b>guarantees</b> <b>of</b> this <b>consistency</b> model. The adaptive consistency guarantees are implemented with a consistency tuner, which probes the <b>consistency</b> index <b>of</b> an observed replicated data object in an online application. The tuner uses statistically derived threshold values of an optimum time gap, which, when padded in a workload stream, guarantees a desired value <b>of</b> <b>consistency</b> index for the observed data object. The tuner thus works like a workload scheduler of the replicated data object and pads only the required time delay between the requests {{in such a way that}} desired level <b>of</b> <b>consistency</b> is achieved with minimal effect on performance metrics like response time.|$|R
40|$|We {{present a}} tool for {{component}} system design in the UML. The tool supports an internal process model for graphical specifications. The model is used to <b>guarantee</b> <b>consistency</b> <b>of</b> different graphical views in the component specification. The model allows the tool to manage the graphical views composition, so that the composed specification inherits internal process models of components. 1...|$|R
40|$|A novel {{method for}} semantics-based {{reconciliation}} of long-lived transactions in concurrent engineering environments is described. The reconciliation {{is a key}} element of recent optimistic replication technologies facing the challenges of diverging replicas, conflicts between concurrent operations and disturbing consistency. The research presented addresses the general problem of semantically consistent and functionally meaningful reconciliation and enables significant simplification and formalization of its solution. The advantages of the method are strong <b>guarantees</b> <b>of</b> semantic <b>consistency</b> <b>of</b> the convergent representation, capabilities to use it in autonomous and user interactive modes as well as avoidance of combinatorial explosion peculiar to many other methods. The particular application is presented to illustrate how the method can be effectively applied for collaborative software engineering in the scope of the emerging UML-driven methodology. Pages: 542 - 54...|$|R
40|$|In this paper, {{we present}} a new {{paradigm}} that allows dynamically changing the topology of 2 -manifold polygonal meshes. Our new paradigm always <b>guarantees</b> topological <b>consistency</b> <b>of</b> polygonal meshes. Based on our paradigm, by simply adding and deleting edges, handles can be created and deleted, holes can be opened or closed, polygonal meshes can be connected or disconnected...|$|R
40|$|To prove {{difficult}} theorems in {{a mathematical}} field requires substantial knowledge of that field. In this paper a frame-based knowledge representation formalism is presented, which supports a conceptual representation {{and to a}} large extent <b>guarantees</b> the <b>consistency</b> <b>of</b> the built-up knowledge bases. We define a semantics of the representation by giving a translation into the underlaying logic...|$|R
40|$|Abstract. This paper {{describes}} {{a novel approach}} that <b>guarantees</b> <b>consistency</b> <b>of</b> structure-based configuration models in an evolution process using a knowledge representation based on description logics. We define the term consistency and discuss different notions <b>of</b> <b>consistency</b> that are domain-dependent or configuration-tool specific. Change operations are introduced as a formal means to capture changes to a configuration model. A set of invariants formally defines what consistency denotes for structure-based configuration and enables an adaptive approach to handle configuration models that rely on different notions <b>of</b> <b>consistency.</b> A model editor implementing this evolution process allows to execute change operations, identifies inconsistency in a configuration model after change execution and suggests repair operations for repairing inconsistent configuration models. ...|$|R
40|$|Geo-replicated {{databases}} often {{operate under}} the principle <b>of</b> eventual <b>consistency</b> to offer high-availability with low latency on a simple key/value store abstraction. Recently, some have adopted commutative data types to provide seamless reconciliation for special purpose data types, such as counters. Despite this, the inability to enforce numeric invariants across all replicas still remains a key shortcoming of relying on the limited <b>guarantees</b> <b>of</b> eventual <b>consistency</b> storage. We present a new replicated data type, called bounded counter, which adds support for numeric invariants to eventually consistent geo-replicated databases. We describe how this can be implemented on top of existing cloud stores without modifying them, using Riak as an example. Our approach adapts ideas from escrow transactions to devise a solution that is decentralized, fault-tolerant and fast. Our evaluation shows much lower latency and better scalability than the traditional approach <b>of</b> using strong <b>consistency</b> to enforce numeric invariants, thus alleviating the tension between consistency and availability...|$|R
40|$|Views is a {{user-interface}} {{system in}} which the user interface is a layer above applications, <b>guaranteeing</b> <b>consistency</b> <b>of</b> the interface, and with a data-layer implementing external object representations, allowing exchange of objects between applications without loss of structure. Although Views offers an architecture to deal with user-interface aspects on a high level, in this report is shown that also low level interaction can be modelled with the architecture provided. ...|$|R
40|$|This paper {{examines}} {{concurrency control}} in databases beginning with some basic terminology and {{issues that will}} be used throughout the paper, and an informal intuitive description of serializability. This paper later provides a comprehensive mathematical theory. This theory forms the basis {{for the development of the}} various algorithmic tests for serializability which <b>guarantee</b> the <b>consistency</b> <b>of</b> a database which permits concurrency...|$|R
40|$|Abstract. Both {{aggregates}} and arithmetic built-ins {{are widely}} used in current database query languages: Aggregates are second-order constructs such as COUNT and SUM of SQL; arithmetic built-ins include relational and other mathematical operators that apply to numbers, such as ≤ and +. These features are also {{of interest in the}} context of database integrity constraints: correct and efficient integrity checking is crucial, as, without any <b>guarantee</b> <b>of</b> data <b>consistency,</b> the answers to queries cannot be trusted. In this paper we propose a method of practical relevance {{that can be used to}} derive, at database design time, simplified versions of such integrity constraints that can be tested before the execution of any update. In this way, virtually no time is spent for optimization or rollbacks at run time. Both set and bag semantics are considered. ...|$|R
