1297|10000|Public
5|$|State-of-the-art <b>deep</b> <b>neural</b> <b>network</b> {{architectures}} {{can sometimes}} even rival human accuracy in fields like computer vision, specifically {{on things like}} the MNIST database, and traffic sign recognition.|$|E
50|$|In 2014, {{backpropagation}} {{was used}} to train a <b>deep</b> <b>neural</b> <b>network</b> for speech recognition.|$|E
5000|$|... 25.04.2017: Nvidia publishes it's paper on: [...] "Explaining How a <b>Deep</b> <b>Neural</b> <b>Network</b> Trained with End-to-End Learning Steers a Car" ...|$|E
40|$|The thesis {{addresses}} {{the topic of}} <b>Deep</b> <b>Neural</b> <b>Networks,</b> in particular the methods regar- ding the field of Deep Learning, {{which is used to}} initialize the weight and learning process s itself within <b>Deep</b> <b>Neural</b> <b>Networks.</b> The focus is also put to the basic theory of the classical <b>Neural</b> <b>Networks,</b> which is important to comprehensive understanding of the issue. The aim of this work is to determine the optimal set of optional parameters of the algori- thms on various complexity levels of image recognition tasks through experimenting with created application applying <b>Deep</b> <b>Neural</b> <b>Networks.</b> Furthermore, evaluation and analysis of the results and lessons learned from the experimentation with classical and <b>Deep</b> <b>Neural</b> <b>Networks</b> are integrated in the thesis...|$|R
40|$|In this paper, {{we propose}} gcForest, a {{decision}} tree ensemble approach with performance highly competitive to <b>deep</b> <b>neural</b> <b>networks.</b> In contrast to <b>deep</b> <b>neural</b> <b>networks</b> which require great effort in hyper-parameter tuning, gcForest {{is much easier}} to train. Actually, even when gcForest is applied to different data from different domains, excellent performance can be achieved by almost same settings of hyper-parameters. The training process of gcForest is efficient and scalable. In our experiments its training time running on a PC is comparable to that of <b>deep</b> <b>neural</b> <b>networks</b> running with GPU facilities, and the efficiency advantage may be more apparent because gcForest is naturally apt to parallel implementation. Furthermore, in contrast to <b>deep</b> <b>neural</b> <b>networks</b> which require large-scale training data, gcForest can work well even when there are only small-scale training data. Moreover, as a tree-based approach, gcForest should be easier for theoretical analysis than <b>deep</b> <b>neural</b> <b>networks.</b> Comment: IJCAI 201...|$|R
30|$|Various deep {{learning}} architectures such as <b>deep</b> <b>neural</b> <b>networks,</b> convolutional <b>deep</b> <b>neural</b> <b>networks,</b> <b>deep</b> belief <b>networks,</b> and recurrent <b>neural</b> <b>networks</b> {{have been applied}} to fields like computer vision, automatic speech recognition, natural language processing, audio recognition, and bioinformatics {{where they have been}} shown to produce state-of-the-art results on various tasks [5, 10].|$|R
50|$|Picas is free art photo editing {{application}} {{which uses}} <b>deep</b> <b>neural</b> <b>network</b> and artificial intelligence to automatically redraw photos to artistic effects.|$|E
5000|$|... (2015) A <b>deep</b> <b>neural</b> <b>network</b> {{architecture}} is successfully used, {{for the first}} time successfully, to optimally control simulated spacecraft and rocket descents.|$|E
50|$|State-of-the-art <b>deep</b> <b>neural</b> <b>network</b> {{architectures}} {{can sometimes}} even rival human accuracy in fields like computer vision, specifically {{on things like}} the MNIST database, and traffic sign recognition.|$|E
40|$|Fixed-point {{optimization}} of <b>deep</b> <b>neural</b> <b>networks</b> {{plays an}} important role in hardware based design and low-power implementations. Many <b>deep</b> <b>neural</b> <b>networks</b> show fairly good performance even with 2 - or 3 -bit precision when quantized weights are fine-tuned by retraining. We propose an improved fixedpoint optimization algorithm that estimates the quantization step size dynamically during the retraining. In addition, a gradual quantization scheme is also tested, which sequentially applies fixed-point optimizations from high- to low-precision. The experiments are conducted for feed-forward <b>deep</b> <b>neural</b> <b>networks</b> (FFDNNs), convolutional <b>neural</b> <b>networks</b> (CNNs), and recurrent <b>neural</b> <b>networks</b> (RNNs). Comment: This paper is accepted in ICASSP 201...|$|R
40|$|In this paper, {{we focus}} on fraud {{detection}} on a signed graph with only a small set of labeled training data. We propose a novel framework that combines <b>deep</b> <b>neural</b> <b>networks</b> and spectral graph analysis. In particular, we use the node projection (called as spectral coordinate) in the low dimensional spectral space of the graph's adjacency matrix as input of <b>deep</b> <b>neural</b> <b>networks.</b> Spectral coordinates in the spectral space capture the most useful topology information of the network. Due to the small dimension of spectral coordinates (compared with the dimension of the adjacency matrix derived from a graph), training <b>deep</b> <b>neural</b> <b>networks</b> becomes feasible. We develop and evaluate two <b>neural</b> <b>networks,</b> <b>deep</b> autoencoder and convolutional <b>neural</b> <b>network,</b> in our fraud detection framework. Experimental results on a real signed graph show that our spectrum based <b>deep</b> <b>neural</b> <b>networks</b> are effective in fraud detection...|$|R
40|$|Data {{inconsistency}} {{leads to}} a slow training process when <b>deep</b> <b>neural</b> <b>networks</b> are used for the inverse design of photonic devices, an issue that arises from the fundamental property of non-uniqueness in all inverse scattering problems. Here we show that by combining forward modeling and inverse design in a tandem architecture, one can overcome this fundamental issue, allowing <b>deep</b> <b>neural</b> <b>networks</b> to be effectively trained by data sets that contain non-unique electromagnetic scattering instances. This paves the way for using <b>deep</b> <b>neural</b> <b>networks</b> to design complex photonic structures that requires large training sets...|$|R
50|$|In machine learning, a deep belief network (DBN) is a {{generative}} graphical model, {{or alternatively}} {{a class of}} <b>deep</b> <b>neural</b> <b>network,</b> composed of multiple layers of latent variables ("hidden units"), with connections between the layers but not between units within each layer.|$|E
5000|$|The MLP {{consists}} of three or more layers (an input and an output layer {{with one or more}} hidden layers) of nonlinearly-activating nodes making it a <b>deep</b> <b>neural</b> <b>network.</b> Since MLPs are fully connected, each node in one layer connects with a certain weight [...] to every node in the following layer.|$|E
5000|$|With the 2016 {{introduction}} of Adobe Voco audio editing and generating software prototype {{slated to be}} part of the Adobe Creative Suite and the similarly enabled DeepMind WaveNet, a <b>deep</b> <b>neural</b> <b>network</b> based audio synthesis software from Google [...] speech synthesis is verging on being completely indistinguishable from a real human's voice.|$|E
40|$|Symbolic {{reasoning}} {{is difficult for}} <b>neural</b> <b>networks.</b> Especially, reasoning with variables can be a challenging task for them. In this paper, a symbolic reasoning method based on <b>deep</b> <b>neural</b> <b>networks</b> is proposed, and this method is applied to axiom discovery. This method makes use {{of the concept of}} “symbolic manipulation”. Specifically, it relies on the learning ability of the <b>deep</b> <b>neural</b> <b>networks</b> and the reasoning ability of a logical system: The logical system generates training examples, which indicate how to manipulate symbols, from given data, and then the <b>deep</b> <b>neural</b> <b>networks</b> try to learn these examples, score them and abstract possible axioms from them. In particular, this method enables the <b>deep</b> <b>neural</b> <b>networks</b> to realise simple reasoning with variables in predicate logic. In experiments, we demonstrate that the <b>deep</b> <b>neural</b> <b>networks</b> are able to learn to copy and generate symbols from a certain form of rules produced by the logical system. Moreover, we find that the more hidden layers usually mean the stronger learning ability of symbolic manipulation: An increasing number of hidden layers usually bring about a higher rule acceptance rate. Also, we find that the more hidden layers can bring about better results on axiom discovery tasks, and we show that the <b>deep</b> <b>neural</b> <b>networks</b> can discover some useful axioms in mathematics. No Full Tex...|$|R
2500|$|Earlier {{challenges}} in training <b>deep</b> <b>neural</b> <b>networks</b> were successfully addressed with {{methods such as}} unsupervised pre-training, while available computing power increased {{through the use of}} GPUs and distributed computing. <b>Neural</b> <b>networks</b> were deployed on a large scale, particularly in image and visual recognition problems. This became known as [...] "deep learning", although deep learning is not strictly synonymous with <b>deep</b> <b>neural</b> <b>networks.</b>|$|R
40|$|Abstract Great {{successes of}} <b>deep</b> <b>neural</b> <b>networks</b> have been {{witnessed}} in various real applications. Many algorithmic and implementation techniques have been developed; however, theoretical understanding of {{many aspects of}} <b>deep</b> <b>neural</b> <b>networks</b> is far from clear. A particular interesting issue is the usefulness of dropout, which was motivated from the intuition of preventing complex co-adaptation of feature detectors. In this paper, we study the Rademacher complexity {{of different types of}} dropouts, and our theoretical results disclose that for shallow <b>neural</b> <b>networks</b> (with one or none hidden layer) dropout is able to reduce the Rademacher complexity in polynomial, whereas for <b>deep</b> <b>neural</b> <b>networks</b> it can amazingly lead to an exponential reduction...|$|R
50|$|The {{platform}} utilizes proprietary clustering algorithms and a <b>deep</b> <b>neural</b> <b>network</b> to scan {{millions of}} candidate resumes {{to provide the}} right candidates to employers. In doing so, the technology has reduced the average time to hire a candidate to 12 days, compared with the 81 day industry average. Search Party is based in Sydney, Australia but has offices around the world.|$|E
50|$|A <b>deep</b> <b>neural</b> <b>network</b> (DNN) is an ANN with {{multiple}} hidden layers between the {{input and output}} layers. Similar to shallow ANNs, DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.|$|E
50|$|More fundamentally, many {{algorithms}} used in machine learning {{are not easily}} explainable. For example, the output of a <b>deep</b> <b>neural</b> <b>network</b> depends on many layers of computations, connected in a complex way, and no one input or computation may be a dominant factor. The field of Explainable AI seeks to provide better explanations from existing algorithms, and algorithms that are more easily explainable, {{but it is a}} young and active field.|$|E
40|$|With rapid {{progress}} and great successes {{in a wide}} spectrum of applications, deep learning is being applied in many safety-critical environments. However, <b>deep</b> <b>neural</b> <b>networks</b> have been recently found vulnerable to well-designed input samples, called adversarial examples. Adversarial examples are imperceptible to human but can easily fool <b>deep</b> <b>neural</b> <b>networks</b> in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying <b>deep</b> <b>neural</b> <b>networks</b> in safety-critical scenarios. Therefore, the attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples against <b>deep</b> <b>neural</b> <b>networks,</b> summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications and countermeasures for adversarial examples are investigated. We further elaborate on adversarial examples and explore the challenges and the potential solutions. Comment: 21 pages, 13 figures, Github: [URL]...|$|R
40|$|Great {{successes of}} <b>deep</b> <b>neural</b> <b>networks</b> have been {{witnessed}} in various real applications. Many algorithmic and implementation techniques have been developed, however, theoretical understanding of {{many aspects of}} <b>deep</b> <b>neural</b> <b>networks</b> is far from clear. A particular interesting issue is the usefulness of dropout, which was motivated from the intuition of preventing complex co-adaptation of feature detectors. In this paper, we study the Rademacher complexity {{of different types of}} dropout, and our theoretical results disclose that for shallow <b>neural</b> <b>networks</b> (with one or none hidden layer) dropout is able to reduce the Rademacher complexity in polynomial, whereas for <b>deep</b> <b>neural</b> <b>networks</b> it can amazingly lead to an exponential reduction of the Rademacher complexity. Comment: 20 page...|$|R
40|$|The {{literature}} [- 5]contains several reports {{evaluating the}} abilities of <b>deep</b> <b>neural</b> <b>networks</b> in text transfer learning. To our knowledge, however, {{there have been few}} efforts to fully realize the potential of <b>deep</b> <b>neural</b> <b>networks</b> in cross-domain product review sentiment classification. In this paper, we propose a two-layer convolutional <b>neural</b> <b>network</b> (CNN) for cross-domain product review sentiment classification (LM-CNN-LB). Transfer learning research into product review sentiment classification based on <b>deep</b> <b>neural</b> <b>networks</b> has been limited by the lack of a large-scale corpus; we sought to remedy this problem using a large-scale auxiliary cross-domain dataset collected from Amazon product reviews. Our proposed framework exhibits the dramatic transferability of <b>deep</b> <b>neural</b> <b>networks</b> for cross-domain product review sentiment classification and achieves state-of-the-art performance. The framework also outperforms complex engineered features used with a non-deep <b>neural</b> <b>network</b> method. The experiments demonstrate that introducing large-scale data from similar domains is an effective way to resolve the lack of training data. The LM-CNN-LB trained on the multi-source related domain dataset outperformed the one trained on a single similar domain...|$|R
50|$|Other deep {{learning}} working architectures, specifically those built for computer vision, {{began with the}} Neocognitron introduced by Fukushima in 1980. In 1989, LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970, to a <b>deep</b> <b>neural</b> <b>network</b> {{with the purpose of}} recognizing handwritten ZIP codes on mail. While the algorithm worked the training time was an impractical 3 days.|$|E
50|$|In 2014, Arthur Graves from Deepmind {{published}} {{a series of}} paper describing a novel <b>Deep</b> <b>Neural</b> <b>Network</b> structure called Neural Turing Machine able to read symbols on a tape and store symbols in memory. Relational Networks, another Deep Network module published by Deepmind are able to create object-like representations and manipulate them to answer complex questions. Relational Networks and Neural Turing Machines are yet another evidence that connectionism and computationalism need not be at odds.|$|E
50|$|Users {{can take}} or choose {{a picture and}} select {{different}} filteres to turn the picture into art effects. At launch, the app offers 45 filters, new filters updated each week. The app transforms pictures into artistic effects {{with the help of}} artificial intelligence and <b>deep</b> <b>neural</b> <b>network</b> algorithm on their server, and no photos will be saved as the developer stated. On 24 October 2016, the developer announced that the app is now support online photo editing.|$|E
5000|$|OpenNN—An {{open source}} C++ library which {{implements}} <b>deep</b> <b>neural</b> <b>networks</b> and provides parallelization with CPUs.|$|R
5000|$|... deep-pwning Metasploit {{for deep}} {{learning}} which currently has attacks on <b>deep</b> <b>neural</b> <b>networks</b> using Tensorflow ...|$|R
50|$|Most speech {{recognition}} researchers {{moved away from}} neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI conducted research on <b>deep</b> <b>neural</b> <b>networks</b> in speech and speaker recognition. Heck's speaker recognition team achieved the first significant success with <b>deep</b> <b>neural</b> <b>networks</b> in speech processing as demonstrated in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation and later {{published in the journal}} of Speech Communication. While SRI experienced success with <b>deep</b> <b>neural</b> <b>networks</b> in speaker recognition, they were unsuccessful in demonstrating similar success in {{speech recognition}}. One decade later, Hinton and Deng collaborated with each other and then with colleagues across groups at University of Toronto, Microsoft, Google and IBM, igniting a renaissance of <b>deep</b> feedforward <b>neural</b> <b>networks</b> in speech recognition.|$|R
5000|$|As of 2016, AlphaGo's {{algorithm}} uses {{a combination}} of machine learning and tree search techniques, combined with extensive training, both from human and computer play. It uses Monte Carlo tree search, guided by a [...] "value network" [...] and a [...] "policy network," [...] both implemented using <b>deep</b> <b>neural</b> <b>network</b> technology. A limited amount of game-specific feature detection pre-processing (for example, to highlight whether a move matches a nakade pattern) {{is applied to the}} input before it is sent to the neural networks.|$|E
50|$|A {{recursive}} {{neural network}} (RNN) {{is a kind}} of <b>deep</b> <b>neural</b> <b>network</b> created by applying the same set of weights recursively over a structure, to produce a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order. RNNs have been successful for instance in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding. RNNs have first been introduced to learn distributed representations of structure, such as logical terms.Models and general frameworks have been developed in further works since the 90s.|$|E
50|$|Brendan John Frey FRSC (born 29 August 1968) is a Canadian-born machine {{learning}} and genome biology researcher, known mainly {{for his work}} on factor graphs, the wake-sleep algorithm for deep learning, and using {{machine learning}} to model genome biology and understand genetic disorders. He founded Deep Genomics and is currently its CEO, and he is a Professor of Engineering and Medicine at the University of Toronto. He co-developed a new computational approach to identifying the genetic determinants of disease, {{was one of the first}} researchers to successfully train a <b>deep</b> <b>neural</b> <b>network,</b> and was a pioneer in the introduction of iterative message-passing algorithms.|$|E
50|$|<b>Deep</b> <b>neural</b> <b>networks</b> are {{generally}} interpreted {{in terms of}} the universal approximation theorem or probabilistic inference.|$|R
40|$|We {{present a}} study on {{automatic}} birdsong recognition with <b>deep</b> <b>neural</b> <b>networks</b> using the BIRDCLEF 2014 dataset. Through deep learning, feature hierarchies are learned that represent the data on several levels of abstraction. Deep learning has been applied with success to problems in fields such as music information retrieval and image recognition, but its use in bioacoustics is rare. Therefore, we investigate {{the application of a}} common deep learning technique (<b>deep</b> <b>neural</b> <b>networks)</b> in a classification task using songs from Amazonian birds. We show that various <b>deep</b> <b>neural</b> <b>networks</b> are capable of outperforming other classification methods. Furthermore, we present an automatic segmentation algorithm that is capable of separating bird sounds from non-bird sounds...|$|R
40|$|Adaptive {{gradient}} {{methods have}} become recently very popular, in particular {{as they have}} been shown to be useful in the training of <b>deep</b> <b>neural</b> <b>networks.</b> In this paper we have analyzed RMSProp, originally proposed for the training of <b>deep</b> <b>neural</b> <b>networks,</b> in the context of online convex optimization and show √(T) -type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of <b>deep</b> <b>neural</b> <b>networks.</b> Comment: ICML 2017, 16 pages, 23 figure...|$|R
