31|152|Public
5000|$|PKRS-1 (Password Authenticated <b>Key</b> <b>Retrieval</b> Scheme, version 1) ...|$|E
50|$|The first password-authenticated <b>key</b> <b>retrieval</b> {{methods were}} {{described}} by Ford and Kaliski in 2000.|$|E
50|$|This {{document}} {{includes a}} number of password-authenticated key agreement schemes, and a password-authenticated <b>key</b> <b>retrieval</b> scheme.|$|E
50|$|FIRST Steamworks {{is played}} on a field 27ft (823cm) by 54ft 4in (1656cm), covered in green carpet and bounded by {{transparent}} polycarbonate guardrails on the longer sides and the Alliance Walls on the shorter sides. It is divided into the Neutral Zone, alliance specific Launchpads, <b>Keys</b> and <b>Retrieval</b> Zones.|$|R
40|$|Different {{techniques}} {{which can}} be used to indicate the quality of the mixing ratios retrieved from spectral radiance measurements made by the CLAES instrument for 11 different atmospheric gases are discussed. A <b>key</b> to <b>retrieval</b> from noisy data is the Singular Value Decomposition (SVD), which is described. The use of the SVD with simulated data and radiance measurements from a balloon floating at 30 km is discussed...|$|R
40|$|We {{present an}} {{overview}} of the most recent developments in polyphonic music retrieval and an experiment in which we compare two harmonic similarity measures. In contrast to earlier work, in this paper we specifically focus on the symbolic chord description as the primary musical representation and the similarity between sequences of these descriptions. In the experiment we compare a geometrical and an alignment approach to harmonic similarity, and measure the effects of chord description detail and a priori <b>key</b> information on <b>retrieval</b> performance. For this experiment a large new chord sequence corpus is assembled. The results show that a computational costly alignment approach significantly outperforms a much faster geometrical approach in most cases, that a priori <b>key</b> information boosts <b>retrieval</b> performance, and that using a triadic chord representation yields significantly better results than using more simple or more complex chord representations...|$|R
50|$|On January 15, 2007 {{a website}} {{launched}} at HDKeys.com containing {{a database of}} HD DVD title keys. It also featured a modified copy of the BackupHDDVD software allowing for online <b>key</b> <b>retrieval</b> (the latter was later removed after a DMCA complaint).|$|E
50|$|Each side of {{the arena}} is called the Launchpad. The Launchpad is {{alliance}} specific, bounded by the Launchpad Lines and the Alliance Wall. Each Launchpad belongs to the alliance that has their Alliance Wall bounding it. Robots start the match contacting the Alliance Wall. The Launchpad contains the <b>Key,</b> <b>Retrieval</b> Zone and the Airship.|$|E
50|$|Password-authenticated <b>key</b> <b>retrieval</b> is {{a process}} in which a client obtains a static key in a password-based {{negotiation}} with a server that knows data associated with the password, such as the Ford and Kaliski methods. In the most stringent setting, one party uses only a password in conjunction with N (two or more) servers to retrieve a static key. This is completed in a way that protects the password (and key) even if N-1 of the servers are completely compromised.|$|E
40|$|Abstract — Keywords are the {{thematic}} words in any document. They represent topic of that document. Keywords {{are commonly used}} for search engines and document databases to locate information and determine if two pieces of test are related to each other. <b>Key</b> terms <b>retrieval</b> is also addressed as mining of words, words retrieval, recognition of words, or retrieval of glossary, is a small phase of retrieval of information. Overall motive of retrieval of terminology is retrieving relevant words automatically in a corpus. Moreover, techniques of automatic words retrieval mainly apply language techniques (automatic chunking of words phrases and tagging part of speech) fo...|$|R
40|$|In this paper, we {{introduce}} an optimum {{approach for}} querying similar images on large digital-image databases. Our work {{is based on}} RBIR (region-based image retrieval) method which uses multiple regions as the <b>key</b> to <b>retrieval</b> images. This method significantly improves the accuracy of queries. However, this also increases the cost of computing. To reduce this expensive computational cost, we implement binary signature encoder which maps an image to its identification in binary. In order to fasten the lookup, binary signatures of images are classified by the help of S-kGraph. Finally, our work is evaluated on COREL's images. Comment: 17 pages, 9 figure...|$|R
40|$|Software reuse {{is one of}} the {{promising}} ways to improve productivity, quality and reliability. Component retrieval {{is one of the}} important phases in the process of software reuse. Most of the existing retrieval methods are based on syntactic-based approach. This thesis work proposes a semantic-property-based retrieval method and integrates two existing methods that include type-based retrieval method and execution-based retrieval method to retrieving imperative programs, like C programs. In semantic-property-based retrieval, we use a number of semantic properties to describe the behaviors of components and be search <b>keys</b> on <b>retrieval.</b> The type-based retrieval consists of using type information as a search <b>key</b> to perform <b>retrieval.</b> Execution-based retrieval is to actually execute the components with user provided sample test data and retrieve components by verifying the results. We have developed a prototype system called WISER that integrates these three different retrieval methods. Paper copy at Leddy Library: Theses 2 ̆ 6 Major Papers - Basement, West Bldg. / Call Number: Thesis 1997. T 345. Source: Masters Abstracts International, Volume: 39 - 02, page: 0533. Adviser: Young Gil Park. Thesis (M. Sc.) [...] University of Windsor (Canada), 1998...|$|R
40|$|A trie {{structure}} can immediately {{determine whether}} a desired key is in a given key set or not, and can find its longest match easily [...] . this paper. Insertion and deletion operations, as well as <b>key</b> <b>retrieval</b> for this double-trie, are presented. The efficiency of this method is shown {{by the results of}} simulations for various key sets...|$|E
30|$|Key-binding schemes: Helper {{data are}} {{obtained}} by binding a chosen {{key to a}} biometric template. As {{a result of the}} binding process a fusion of the secret key and the biometric template is stored as helper data. Applying an appropriate <b>key</b> <b>retrieval</b> algorithm, keys are obtained from the helper data at authentication [8]. Since cryptographic keys are independent of biometric features these are revocable while an update of the key usually requires re-enrollment in order to generate new helper data.|$|E
40|$|Abstract. We {{develop a}} new {{methodology}} to assess cryptographic key strength using cloud computing, by calculating the true economic cost of (symmetric- or private-) <b>key</b> <b>retrieval</b> for the most common crypto-graphic primitives. Although the present paper gives both the current (2012) and last years (2011) costs, more importantly it provides the tools and infrastructure to derive new data points {{at any time in}} the future, while allowing for improvements such as of new algorithmic approaches. Over time the resulting data points will provide valuable insight in the selection of cryptographic key sizes. 3...|$|E
40|$|International audienceWe {{present an}} {{comparison}} between two recent {{approaches to the}} harmonic similarity of musical chords sequences. In contrast to earlier work, that mainly focuses on the similarity of musical scores or musical audio, {{in this paper we}} speci cally use on the symbolic chord description as the primary musical representation and the similarity between sequences of these descriptions. In an experiment we compare a geometrical and an alignment approach to harmonic similarity, and measure the e ects of chord description detail and a priori <b>key</b> information on <b>retrieval</b> performance. For this experiment a large new chord sequence corpus is assembled. The results show that a computational costly alignment approach signi cantly outperforms a much faster geometrical approach in most cases, that a priori <b>key</b> information boosts <b>retrieval</b> performance, and that using a triadic chord representation yields signi cantly better results than using more simple or more complex chord representations...|$|R
40|$|Abstract. We {{present a}} {{comparison}} between two recent approaches to the harmonic similarity of musical chords sequences. In contrast to earlier work that mainly focuses on the similarity of musical notation or musical audio, {{in this paper we}} specifically use on the symbolic chord description as the primary musical representation. For an experiment, a large chord sequence corpus was created. In this experiment we compare a geomet-rical and an alignment approach to harmonic similarity, and measure the effects of chord description detail and a priori <b>key</b> information on <b>retrieval</b> performance. The results show that an alignment approach sig-nificantly outperforms a geometrical approach in most cases, but that the geometrical approach is computationally more efficient than the align-ment approach. Furthermore, the results demonstrate that a priori <b>key</b> information boosts <b>retrieval</b> performance, and that using a triadic chord representation yields significantly better results than a simpler or more complex chord representation...|$|R
5000|$|Unordered storage {{typically}} stores {{the records}} in the order they are inserted. Such storage offers good insertion efficiency (...) , but inefficient retrieval times (...) [...] Typically these retrieval times are better, however, as most databases use indexes on the primary <b>keys,</b> resulting in <b>retrieval</b> times of [...] or [...] for keys that {{are the same as}} the database row offsets within the storage system.|$|R
30|$|The key {{advantages}} of the proposed (multi-)iris fuzzy vault scheme can be summarized in three terms: (1) {{in contrast to the}} vast majority of biometric cryptosystems, the proposed system maintains biometric performance obtained by the corresponding unprotected iris recognition system in a single- or multi-instance scenario; (2) the suggested fusion technique combines most discriminative information of two iris-codes to a single fuzzy vault and <b>key</b> <b>retrieval</b> is performed based on an analysis of suitable decoding strategies providing fast decoding times; (3) in contrast to existing proposals the presented (multi-)iris fuzzy vault scheme is designed to protect iris-codes, i.e. the protection of an iris biometric database does not require re-enrolment registered subjects.|$|E
40|$|In this work, two known {{probability}} distributions {{based on}} the replacement model, namely, the Maxwell-Boltzmann and Bose-Einstein ones, are examined. Each of these distributions is assigned to a specific case of query processing, which is known in the literature as batching on primary key values or secondary <b>key</b> <b>retrieval,</b> respectively. These distributions are applied in evaluating the expected distance traveled by the read/write heads in disk searching. Two new probability distributions are derived, {{based on the}} previous ones, which {{are involved in the}} evaluation of the expected number of the cylinder hits. Therefore, seek time evaluation is faced globally. Numerical results are given. 1...|$|E
40|$|In this {{technical}} report, {{we present}} a process algebra aimed at modelling PKI-based systems. The new language, SPIKY, extends the spi-calculus by adding primitives for the retrieval of certified/uncertified public keys as well as private keys belonging to users of the PKI-based system. SPIKY also formalises the notion of process ownership by PKI users, which is necessary in controlling the semantics of the <b>key</b> <b>retrieval</b> capabilities. We also construct a static analysis for SPIKY that captures the property of term substitutions resulting from message-passing and PKI/cryptographic operations. This analysis is shown {{to be safe and}} computable. Finally, we use the analysis to define the term secrecy and peer participation properties for a couple of examples of authentication protocols. ...|$|E
40|$|A case based reason {{application}} thatgen ates poetryversion {{of texts}} {{provided by the}} user is presen d. Cases con of a sen e of prose (used as <b>retrieval</b> <b>key)</b> associated with a correspon poem fragmen (used asstartin poin for the solution- Adaptation takes place by combinS phonSmetrical an lexical inSabout the wordsin the di#eren sources - the prose message, the retrieved case,an additionS vocabulary provided by the user...|$|R
40|$|A {{review of}} the Food and Drug Administration (FDA) Adverse Drug Reaction (ADR) file system. This paper {{describes}} the current operations for maintaining the on-line computerized data entry, storage and retrieval programs, and the microfilm retrieval system. Other technology developments discussed include data diskette exchange and laser disc storage and <b>retrieval.</b> <b>Key</b> Words: Adverse drug experience reports; I D A ADR system; Electronic submis sion; ADR data processin...|$|R
40|$|Abstract. Based on {{the shape}} of the image {{retrieval}} occupy an important position in the content-based image retrieval, and studied architecture, content-based image retrieval system, ie research-based image <b>retrieval</b> <b>key</b> technologies shape features for image noise in addition to the morphological processing; image segmentation; shape-based feature extraction and regional boundaries and description techniques and similarity measure techniques. The results show that the algorithm can effectively identify the characteristics of the image...|$|R
30|$|When {{designing}} a fuzzy vault-based cryptosystem, a practical decoding strategy is needed. Though, {{in the original}} fuzzy vault [23, 62], a Reed-Solomon decoder [72] has been proposed, its resulting error-correcting capabilities are not sufficient to achieve a practical implementation for single finger. As a consequence, {{the use of a}} Lagrange-based decoder has been proposed [64] and adopted for other single-finger implementations [65, 68]. A multi-finger implementation has been proposed in [71]. If a Lagrange-based decoder was chosen for the implementation, then the decoding complexity would become infeasible. A reasonable trade-off between decoding time and verification performance can be achieved using a Guruswami-Sudan-based decoder [71, 73] of which a Reed-Solomon decoder {{can be viewed as a}} special case. In this paper, these strategies are considered for <b>key</b> <b>retrieval</b> in Section 3.4.|$|E
40|$|We call changeware {{software}} that surreptitiously modifies resources of software applications, e. g., configuration files. Changeware is developed by malicious entities which gain profit if their changeware is executed by {{large numbers of}} end-users of the targeted software. Browser hijacking mal-ware is one popular example that aims at changing web-browser settings such as the default search engine or the home page. Changeware tends to provoke end-user dissat-isfaction with the target application, e. g. due to repeated failure of persisting the desired configuration. We describe a solution to counter changeware, to be employed by ven-dors of software targeted by changeware. It combines several protection mechanisms: white-box cryptography to hide a cryptographic key, software diversity to counter automated <b>key</b> <b>retrieval</b> attacks, and run-time process memory integrity checking to avoid illegitimate calls of the developed API...|$|E
40|$|Template {{protection}} targets {{privacy and}} security risks caused by unprotected storage of biometric data. Meeting properties of irreversibility and unlinkability template protection {{systems can be}} applied to secure existing records within biometric databases, i. e. without re-enrollment of registered subjects. The National Institute of Standards and Technology (NIST) demonstrated that iris recognition algorithms can maintain their accuracy and interoperability with compressed images. While template protection schemes are generally conceded highly sensitive to any sort of signal degradation, investigations on the impact of image compression on recognition accuracy have remained elusive. In this work a comprehensive study of different image compression standards applied to iris-biometric fuzzy commitment schemes is presented. It is demonstrated that compressed images, compact enough for transmission across global networks, do not drastically effect the <b>key</b> <b>retrieval</b> performance of a fuzzy commitment scheme. ...|$|E
40|$|Abstract. Legal text {{retrieval}} traditionally relies upon external knowledge {{sources such}} as thesauri and classification schemes, and an accurate indexing of the documents is often manually done. As a result not all legal documents can be effectively retrieved. However a number of current artificial intelligence techniques are promising for legal text retrieval. They sustain the acquisition of know-ledge and the knowledge-rich processing {{of the content of}} document texts and information need, and of their matching. Currently, techniques for learning information needs, learning concept attributes of texts, information extraction, text classification and clustering, and text summarization need to be studied in legal text retrieval because of their potential for improving retrieval and decreasing the cost of manual indexing. The resulting query and text representations are semantically much richer than a set of key terms. Their use allows for more refined retrieval models in which some reasoning can be applied. This paper gives an overview {{of the state of the}} art of these innovative techniques and their potential for legal text <b>retrieval.</b> <b>Key</b> words: case <b>retrieval</b> model, information discovery, legal text retrieval, machine learning. 1...|$|R
40|$|Ranking {{the tens}} of {{thousands}} of retrieved webpages for a user query on a Web search engine such that the most informative webpages are on the top is a <b>key</b> information <b>retrieval</b> technology. A popular ranking algorithm is the HITS algorithm which explores the mutual reinforcement between authority and hub webpages based on hyperlink structure of the Web; the SVD of the Web graph adjacency matrix contains the rankings. We provide an in-depth analysis of the HITS algorithm. Hubs are induced by co-reference while authorities are induced by co-citation. Solutions to HITS in average case are obtained in closed form, which provides useful insights to HITS. In particular, rankings by HITS are identical to rankings by in-degree and by out-degree...|$|R
40|$|Computers {{connected}} to the Internet store billions of images [...] {{many of which are}} neither categorized nor indexed. Considering the fact that the majority of these images are stored in JPEG compressed format, it would be highly desirable to have automated (language independent) image indexing conducted directly in compressed domain (currently, virtually all image databases use English lexicon <b>keys</b> for <b>retrieval).</b> In this paper, we present an algorithm to index images directly in the DCT domain for all JPEG compressed images. The results of an empirical study carried out to compare the performance of this approach with partially decoded approaches (which use pixel based extraction) are also reported. The new algorithm is applicable to applications where fast non-lexicon image key generation is required...|$|R
40|$|We {{present a}} new class of {{distributed}} key generation and recovery algorithms suitable for group communication sys-tems where the group membership is either static or slowly time-varying, and must be tightly controlled. The proposed key generation approach allows entities which may have only partial trust in each other to jointly generate a shared key without the aid of an external third party. The shared key is generated using strong one-way function of the group pa-rameter. This scheme also has perfect forward secrecy. The validity of key generation can be checked using verifiable se-cret sharing techniques. The <b>key</b> <b>retrieval</b> method does not require the keys to be stored in an external retrieval cen-ter. We note that many Internet-based applications may have these requirements. Fulfillment of these requirements is realized through the use of fractional keys—a distributed technique recently developed to enhance the security of dis-tributed systems in a non-cryptographic manner...|$|E
40|$|In {{the present}} work we analyse the key {{scheduling}} algorithm of the RC 4 stream cipher. The internal permutation {{generated by the}} algorithm is biased towards the secret key which can be utilized in the key recovery attempts. Multiple sources were combined to provide proven formulae for these biases. We have completed missing proofs, experimentally verified resulting probabilities and created examples to illustrate usage of the biases in key recovery attempts. This together with methods for extracting information about the secret key gives analysis of the key scheduling algorithm. We have also selected an efficient <b>key</b> <b>retrieval</b> algorithm and implemented an improved version which results in better success probabilities of the key recovery. We have also provided a tool for extracting the key from a permutation obtained by the key scheduling algorithm. Powered by TCPDF (www. tcpdf. org) Department of AlgebraKatedra algebryMatematicko-fyzikální fakultaFaculty of Mathematics and Physic...|$|E
40|$|We {{present the}} design and {{implementation}} of a compiler that automatically generates protocols that perform two-party computations. The input to our protocol is the specification of a computation with secret inputs (e. g., a signature algorithm) expressed using operations in the field Zq of integers modulo a prime q and in the multiplicative subgroup of order q in Z # p for q|p 1 with generator g. The output of our compiler is an implementation of each party in a two-party protocol to perform the same computation securely, i. e., so that both parties can together compute the function but neither can alone. The protocols generated by our compiler are provably secure, in that their strength {{can be reduced to}} that of the original cryptographic computation via simulation arguments. Our compiler can be applied to various cryptographic primitives (e. g., signature schemes, encryption schemes, oblivious transfer protocols) and other protocols that employ a trusted party (e. g., <b>key</b> <b>retrieval,</b> key distribution) ...|$|E
40|$|This thesis {{develops}} a general parameterized model that facilitates {{the comparison of}} different file organization techniques for a given multiple <b>key</b> information <b>retrieval</b> system. The model is based on minimizing the expected pro-cessing time of the data base in performing on-line retrieval and updating operations. The decision rules are {{a function of the}} relevant characteristics of the data base, the on-line queries, the storage devices, and the file organization tech-niques, as well as the relative breakdown of the processing requests between retrievals and various types of updating operations. To demonstrate the use of the model, detailed timing formulas are developed for the retrieval and updating opera-tions for three different file organizations: the Multilist system, the Inverted Index system, and the Cellular Seria...|$|R
40|$|This paper {{presents}} a probabilistic information retrieval {{framework in which}} the retrieval problem is formally treated as a statistical decision problem. In this framework, queries and documents are modeled using statistical language models, user preferences are modeled through loss functions, and retrieval is cast as a risk minimization problem. We discuss how this framework can unify existing retrieval models and accommodate systematic development of new retrieval models. As an example of using the framework to model nontraditional retrieval problems, we derive retrieval models for subtopic retrieval, which is concerned with retrieving documents to cover many different subtopics of a general query topic. These new models differ from traditional retrieval models in that they relax the traditional assumption of independent relevance of documents. <b>Key</b> words: <b>Retrieval</b> models, risk minimization, statistical language models, Bayesian decision theor...|$|R
40|$|In this paper, {{we apply}} {{possibilistic}} reasoning to information retrieval for documents endowed with similarity relations. On the one hand, {{it is used}} together with Boolean models for accommodating possibilistic uncertainty. The logical uncertainty principle is then interpreted in the possibilistic framework. On the other hand, possibilistic reasoning is integrated into description logic and applied to some information retrieval problems, such as query relaxation, query restriction, and exemplar-based <b>retrieval.</b> <b>Key</b> words: Possibilistic logic, Boolean models, Description logic, Similaritybased reasoning. ...|$|R
