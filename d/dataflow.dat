3829|102|Public
5|$|Mathematically, {{these models}} can be {{represented}} in several ways. Petri nets, which were introduced in Carl Adam Petri's 1962 doctoral thesis, were an early attempt to codify the rules of consistency models. <b>Dataflow</b> theory later built upon these, and <b>Dataflow</b> architectures were created to physically implement the ideas of <b>dataflow</b> theory. Beginning in the late 1970s, process calculi such as Calculus of Communicating Systems and Communicating Sequential Processes were developed to permit algebraic reasoning about systems composed of interacting components. More recent additions to the process calculus family, such as the π-calculus, have added the capability for reasoning about dynamic topologies. Logics such as Lamport's TLA+, and mathematical models such as traces and Actor event diagrams, have also been developed to describe the behavior of concurrent systems.|$|E
5|$|<b>Dataflow</b> {{programming}} languages describe {{systems of}} operations on data streams, and {{the connections between}} the outputs of some operations and the inputs of others. These languages can be convenient for describing repetitive data processing tasks, in which the same acyclically-connected collection of operations is applied to many data items. They can be executed as a parallel algorithm in which each operation is performed by a parallel process as soon as another set of inputs becomes available to it.|$|E
25|$|Adapteva epiphany is {{targeted}} as a coprocessor, featuring a network on a chip scratchpad memory model, {{suitable for a}} <b>dataflow</b> programming model, which should be suitable for many machine learning tasks.|$|E
40|$|We study an {{iteration}} operation {{modeling the}} repeated behavior of syn-chronous <b>dataflows</b> {{in which all}} partial results are issued and also memorized for the next computation step. The definition is {{by means of a}} fixpoint equation. We prove some equational properties of iteration, comparing them to the ones of the Elgot iteration. We also prove a normal form theorem for <b>dataflows</b> and show that equality is decidable for canonical <b>dataflows</b> associated to freely generated algebraic theories...|$|R
50|$|S5 {{conformance}} {{is established}} by its specified <b>dataflows.</b>|$|R
40|$|The {{problems}} and methods for adaptive control and multi-agent {{processing of information}} in global telecommunication and computer networks (TCN) are discussed. Criteria for controllability and communication ability (routing ability) of <b>dataflows</b> are described. Multi-agent model for exchange of divided information resources in global TCN has been suggested. Peculiarities for adaptive and intelligent control of <b>dataflows</b> in uncertain conditions and network collisions are analyzed...|$|R
25|$|The Department {{is engaged}} in {{research}} across Computer and Information Sciences, spanning Artificial Intelligence, Software Engineering, Information Retrieval, Mobile and Ubiquitous Interaction, Functional Programming, <b>Dataflow</b> Systems, Database Indexing and Information Science.|$|E
25|$|As such, as of 2016 GPUs {{are popular}} for AI work, and they {{continue}} to evolve in a direction to facilitate deep learning, both for training and inference in devices such as self-driving cars. - and gaining additional connective capability for the kind of <b>dataflow</b> workloads AI benefits from (e.g. Nvidia NVLink).|$|E
25|$|UniProtKB/TrEMBL {{contains}} high-quality computationally analyzed records, {{which are}} enriched with automatic annotation. It {{was introduced in}} response to increased <b>dataflow</b> resulting from genome projects, as the time- and labour-consuming manual annotation process of UniProtKB/Swiss-Prot could not be broadened to include all available protein sequences. The translations of annotated coding sequences in the EMBL-Bank/GenBank/DDBJ nucleotide sequence database are automatically processed and entered in UniProtKB/TrEMBL.|$|E
40|$|Data-intensive, {{interactive}} {{applications are}} an important class of metacomputing (Grid) applications. They are characterized by large <b>dataflows</b> between data providers and consumers, like scientific simulations and remote visualization clients of simulation output. Such <b>dataflows</b> vary at runtime, due to changes in consumers' data needs, changes {{in the nature of}} the data being transmitted, or changes in the availability of computing resources used by flows. The topi...|$|R
50|$|The flow of data (<b>dataflows)</b> can be {{superimposed}} on the diagrams to depict {{a sequence of}} elements required to support a business service.|$|R
30|$|In {{this section}} we present the {{architecture}} and design choices of the F l u χ middleware framework that {{is capable of}} managing <b>dataflows</b> following the model described in the previous section (Section 2). The F l u χ framework, is designed to be tightly coupled with a large-scale (NoSQL) data store, enabling the construction of quality-driven <b>dataflows</b> in which the triggering of processing steps (actions) may be delayed, but still complying with QoD requirements defined over the stored data.|$|R
25|$|The {{memory access}} pattern of AI {{calculations}} differs from graphics: a more predictable but deeper <b>dataflow,</b> benefiting {{more from the}} ability to keep more temporary variables on-chip (e.g. in scratchpad memory rather than caches); GPUs by contrast devote silicon to efficiently dealing with highly non-linear gather-scatter addressing between texture maps and frame-buffers, and texture filtering, as is needed for their primary role in 3D rendering.|$|E
25|$|In {{the early}} 1980s, Evans & Sutherland (E) {{decoupled}} their PS300 graphics processor/display, which contained its own display information transformable through a <b>dataflow</b> architecture. Complex graphical objects could be downloaded over a serial line (e.g. 9600, 56K baud) or Ethernet interface and then manipulated without {{impact on the}} host. The architecture was excellent for high performance display but very inconvenient for domain-specific calculations, such as electron-density fitting and energy calculations. Many crystallographers and modellers spent arduous months trying to fit such activities into this architecture. E designed a card for the PS-300 which had several calculation algorithms using a 100 bit wide finite state machine {{in an attempt to}} simplify this process but it was so difficult to program that it quickly became obsolete.|$|E
500|$|Similarly, topological orderings of DAGs {{can be used}} {{to order}} the {{compilation}} operations in a makefile. The program evaluation and review technique uses DAGs to model the milestones and activities of large human projects, and schedule these projects to use as little total time as possible. Combinational logic blocks in electronic circuit design, and the operations in <b>dataflow</b> programming languages, involve acyclic networks of processing elements. DAGs can also represent collections of events and their influence on each other, either in a probabilistic structure such as a Bayesian network or as a record of historical data such as family trees or the version histories of [...] distributed revision control systems. DAGs can also be used as a compact representation of sequence data, such as the directed acyclic word graph representation of a collection of strings, or the binary decision diagram representation of sequences of binary choices. More abstractly, ...|$|E
50|$|By {{employing}} the OBASHI framework, B&IT diagrams {{are able to}} accurately depict the complex inter-relationships and dependencies of business processes, IT resources and <b>dataflows</b> in an easy-to-understand visual format.|$|R
50|$|The OBASHI {{methodology}} {{provides a}} framework and method for capturing, illustrating and modeling the relationships, dependencies and <b>dataflows</b> between business and Information technology (IT) assets and resources in a business context.|$|R
40|$|Abstract. The {{advent of}} cloud {{computing}} technologies shows great promise for web engineering and facilitates {{the development of}} flexible, distributed, and scalable web applications. Data integration can notably benefit from cloud computing because integrating web data is usually an expensive task. This paper introduces CloudFuice, a data integration system that follows a mashup-like specification of advanced <b>dataflows</b> for data integration. CloudFuice’s task-based execution approach allows for an efficient, asynchronous, and parallel execution of <b>dataflows</b> in the cloud and utilizes recent cloud-based web engineering instruments. We demonstrate and evaluate CloudFuice’s applicability for mashup-based data integration in the cloud {{with the help of}} a first prototype implementation...|$|R
2500|$|Architectures {{such as the}} Cell {{microprocessor}} have exhibited features significantly {{overlapping with}} AI accelerators - in its support for packed low precision arithmetic, <b>dataflow</b> architecture, and prioritising 'throughput' over latency and [...] "branchy-int" [...] code. This was a move toward heterogeneous computing, {{with a number of}} throughput-oriented accelerators intended to assist the CPU with a range of intensive tasks: physics-simulation, AI, video encoding/decoding, and certain graphics tasks beyond its contemporary GPUs.|$|E
50|$|In {{computer}} programming, <b>dataflow</b> {{programming is}} a programming paradigm that models a {{program as a}} directed graph of the data flowing between operations, thus implementing <b>dataflow</b> principles and architecture. <b>Dataflow</b> programming languages share some features of functional languages, and were generally developed {{in order to bring}} some functional concepts to a language more suitable for numeric processing. Some authors use the term Datastream instead of <b>Dataflow</b> to avoid confusion with <b>Dataflow</b> Computing or <b>Dataflow</b> architecture, based on an indeterministic machine paradigm. <b>Dataflow</b> programming was pioneered by Jack Dennis and his graduate students at MIT in the 1960s.|$|E
50|$|Hardware {{architectures}} for <b>dataflow</b> {{was a major}} {{topic in}} computer architecture research in the 1970s and early 1980s. Jack Dennis of MIT pioneered the field of static <b>dataflow</b> architectures while the Manchester <b>Dataflow</b> Machine and MIT Tagged Token architecture were major projects in dynamic <b>dataflow.</b>|$|E
50|$|Tracing is {{essential}} for debugging, during which, a user can issue multiple tracing queries. Thus, {{it is important that}} tracing has fast turnaround times. Ikeda et al. can perform efficient backward tracing queries for MapReduce <b>dataflows,</b> but are not generic to different DISC systems and do not perform efficient forward queries. Lipstick, a lineage system for Pig, while able to perform both backward and forward tracing, is specific to Pig and SQL operators and can only perform coarse-grain tracing for black-box operators. Thus, {{there is a need for}} a lineage system that enables efficient forward and backward tracing for generic DISC systems and <b>dataflows</b> with black-box operators.|$|R
30|$|Resource Monitor This {{component}} {{is responsible for}} monitoring the resource utilization and load of the machines allocated to execute <b>dataflows.</b> It informs the QoD Engine about the computation loads at runtime in order to automatically tune the QoD constraints.|$|R
30|$|Therefore, we find F l u χ a {{compelling}} effort, within {{the current state}} of the art, to improve <b>dataflows</b> execution, in a performance-improved, resource efficient and correct manner and, thus, deliver higher QoS to end-users and drive costs of operation down.|$|R
5000|$|At second, it is {{a hybrid}} multithreaded <b>dataflow</b> runtime engine {{controlled}} by a von-Neumann front-end VM. The <b>dataflow</b> runtime engine executes tagged-token contextual parallel instructions (opposite to the restricted fork-join paradigm) while the von-Neumann front-end VM initializes contexts and feeds the <b>dataflow</b> runtime engine with marshaled clusters of instructions.|$|E
5000|$|At first, it is {{a hybrid}} <b>dataflow</b> {{emulator}} running multithreadedly on commodity SMP. The SMP ensures MIMD while <b>dataflow</b> exploits implicit parallelism.|$|E
5000|$|Meanwhile, {{there is}} a clash of terminology, since the term <b>dataflow</b> is used for a subarea of {{parallel}} programming: for <b>dataflow</b> programming.|$|E
30|$|Observer Mode This (pessimistic) mode {{assumes that}} <b>dataflows</b> {{are not the}} only {{entities}} performing changes on database objects. Therefore, it resorts to observers to scan the objects to detect modifications, since it is not guaranteed that every update passes through the Monitoring component.|$|R
50|$|Lineage {{systems for}} DISC <b>dataflows</b> {{must be able}} to capture {{accurate}} lineage across black-box operators to enable fine-grain debugging. Current approaches to this include Prober, which seeks to find the minimal set of inputs that can produce a specified output for a black-box operator by replaying the data-flow several times to deduce the minimal set, and dynamic slicing, as used by Zhang et al. to capture lineage for NoSQL operators through binary rewriting to compute dynamic slices. Although producing highly accurate lineage, such techniques can incur significant time overheads for capture or tracing, and it may be preferable to instead trade some accuracy for better performance. Thus, {{there is a need for}} a lineage collection system for DISC <b>dataflows</b> that can capture lineage from arbitrary operators with reasonable accuracy, and without significant overheads in capture or tracing.|$|R
40|$|In this report, some {{high-performance}} interconnection {{networks are}} briefly commented/evaluated against a specific radar signal processing system selected {{as a general}} representative system (see Figure 1). The computations in the shown signal processing chain are pipelined in three stages. All stages are therefore always kept busy and the transmissions of the <b>dataflows</b> between the stage...|$|R
50|$|In 2011, Vendini {{acquired}} <b>Dataflow</b> Workspace: {{a software}} that helps manage the backend aspects {{of music and}} arts festivals, built by <b>Dataflow</b> Enterprises, a company based in Knoxville, TN. As {{a part of the}} acquisition, Vendini brought on Robby Black, CEO and founder of <b>Dataflow</b> Enterprises, as Director of Festival Solutions.|$|E
50|$|There {{are many}} {{hardware}} architectures oriented toward the efficient implementation of <b>dataflow</b> programming models. MIT's tagged token <b>dataflow</b> architecture {{was designed by}} Greg Papadopoulos.|$|E
50|$|BMDFM dynamic {{scheduling}} subsystem performs an SMP emulation of Tagged-Token <b>Dataflow</b> Machine {{to provide}} the transparent <b>dataflow</b> semantics for the applications. No directives for parallel execution are required.|$|E
30|$|Before {{considering}} the four cases, {{we need to}} consider the designs of the following three FFT <b>dataflows.</b> Note that we only briefly discuss the approaches to remove redundancies of the FFTs with these three input patterns in this paper. Future work will be directed towards addressing the complete algorithms for generating canonic FFTs with these input patterns.|$|R
40|$|International audienceData-flow {{oriented}} embedded systems, such as automotive systems used {{to render}} HMI (e. g., instrument clusters, infotainments), are increasingly built from highly variable specifications while targeting different constrained hardware platforms configurable in a fine-grained way. These variabilities at two different levels {{lead to a}} huge number of possible embedded system solutions, which feasibility is extremely complex and tedious to predetermine. In this paper, we propose a tooled approach that capture high level specifications as variable <b>dataflows,</b> and targeted platforms as variable component models. <b>Dataflows</b> can then be mapped onto platforms to express a specification of such variability-intensive systems. The proposed tool support transforms this specification into structural and behav-ioral variability models and reuses automated reasoning techniques to explore and assess the feasibility of all variants in a single run. We also report on the application of the proposed approach to an industrial case study of automotive instrument cluster...|$|R
50|$|Apache NiFi is a {{software}} project from the Apache Software Foundation which enables the automation of data flow between systems. It {{can be described}} as data logistics. Similar to how parcel services move and track packages, Apache NiFi helps move and track data. The project is written using flow-based programming and provides a web-based user interface to manage <b>dataflows</b> in real time.|$|R
