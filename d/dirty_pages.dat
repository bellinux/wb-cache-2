59|38|Public
50|$|Additionally, {{changing}} {{the value of}} /proc/sys/vm/dirty_expire_centisecs can {{have an effect on}} the performance. It sets the flush interval when <b>dirty</b> <b>pages</b> are written to disk.|$|E
50|$|Pages in {{the page}} cache {{modified}} after being brought in are called <b>dirty</b> <b>pages.</b> Since non-dirty {{pages in the}} page cache have identical copies in secondary storage (e.g. hard disk drive or solid-state drive), discarding and reusing their space is much quicker than paging out application memory, and is often preferred over flushing the <b>dirty</b> <b>pages</b> into secondary storage and reusing their space. Executable binaries, such as applications and libraries, are also typically accessed through page cache and mapped to individual process spaces using virtual memory (this is done through the mmap system call on Unix-like operating systems). This not only means that the binary files are shared between separate processes, but also that unused parts of binaries will be flushed out of main memory eventually, leading to memory conservation.|$|E
5000|$|Checkpoint Record {{notes that}} a {{checkpoint}} has been made. These {{are used to}} speed up recovery. They record information that eliminates the need to read a long way into the log's past. This varies according to checkpoint algorithm. If all <b>dirty</b> <b>pages</b> are flushed while creating the checkpoint (as in PostgreSQL), it might contain: ...|$|E
30|$|<b>Page</b> <b>dirty</b> rate: It is {{also the}} major factor {{impacting}} migration behavior. The rate at which VM memory pages are updated by VM applications, {{it depends on the}} number of transferred pages in every pre-copy iteration [70]. If the dirty rate is higher than it increases data sent per iteration, leads in increasing total migration time and service downtime. <b>Dirty</b> <b>page</b> rate and migrating VM performance are not in a linear relationship. If the rate of <b>dirty</b> <b>page</b> generation is lower than link capacity results in lower total migration time and downtime because modified pages are sent frequently. Otherwise, migration performance degrades significantly.|$|R
50|$|From the DPT we can {{compute the}} minimal Sequence Number of a <b>dirty</b> <b>page.</b> From there {{we have to}} start redoing the actions until the crash, in case they weren't persisted already.|$|R
5000|$|... redoLSN: This is a {{reference}} to the first log record that corresponds to a <b>dirty</b> <b>page.</b> i.e. the first update that wasn't flushed at checkpoint time. This is where redo must begin on recovery.|$|R
50|$|During {{the same}} run we also fill the dirty page table {{by adding a}} new entry {{whenever}} we encounter a page that is modified and not yet in the DPT. This however only computes a superset of all <b>dirty</b> <b>pages</b> {{at the time of}} the crash, since we don't check the actual database file whether the page was written back to the storage.|$|E
50|$|Virtuoso is {{designed}} {{to take advantage of}} operating system threading support and multiple CPUs. It consists of a single process with an adjustable pool of threads shared between clients. Multiple threads may work on a single index tree with minimal interference with each other. One cache of database pages is shared among all threads and old <b>dirty</b> <b>pages</b> are written back to disk as a background process.|$|E
50|$|These older chips also {{generally}} allow 2 or 3 write cycles per page. YAFFS {{takes advantage}} of this: <b>dirty</b> <b>pages</b> are marked by writing to a specific spare area byte. Newer NAND flash chips have larger pages, first 2K pages (+ 64 bytes OOB), later 4K, with stricter write requirements. Each page within an erase block (128 kilobytes) must be written to in sequential order, and each page must be written only once.|$|E
5000|$|Usually the {{resulting}} logfile is stored on so-called [...] "stable storage", {{that is a}} storage medium that is assumed to survive crashes and hardware failures. To gather the necessary information for the logging two data structures have to be maintained: the <b>dirty</b> <b>page</b> table (DPT) and the transaction table (TT).|$|R
30|$|Similar to Jin et al. (2011), Liu et al. (2010) {{presents}} a slowdown scheduling algorithm. This algorithm reduces the dirty rate by adjusting CPU resources {{allocated to the}} migration domain. This results into decreasing CPU activity, and reducing <b>dirty</b> <b>page</b> rate. It improves migration performance but also affects the application performance running within the migration domain.|$|R
50|$|The <b>dirty</b> <b>page</b> table keeps {{record of}} all the pages that have been {{modified}} and not yet written back to disc and the first Sequence Number that caused that <b>page</b> to become <b>dirty.</b> The transaction table contains all transactions that are currently running and the Sequence Number of the last log entry they caused.|$|R
50|$|To {{deal with}} this situation, various precleaning {{policies}} are implemented. Precleaning is the mechanism that starts I/O on <b>dirty</b> <b>pages</b> that are (likely) to be replaced soon. The idea is {{that by the time}} the precleaned page is actually selected for the replacement, the I/O will complete and the page will be clean. Precleaning assumes {{that it is possible to}} identify pages that will be replaced next. Precleaning that is too eager can waste I/O bandwidth by writing pages that manage to get re-dirtied before being selected for replacement.|$|E
5000|$|After the warm-up phase, the VM will {{be stopped}} on the {{original}} host, the remaining <b>dirty</b> <b>pages</b> will be copied to the destination, and the VM will be resumed on the destination host. The time between stopping the VM on the original host and resuming it on destination is called [...] "down-time", and ranges from a few milliseconds to seconds according {{to the size of}} memory and applications running on the VM. There are some techniques to reduce live migration down-time, such as using probability density function of memory change.|$|E
50|$|To write file data, YAFFS {{initially}} {{writes a}} whole page (chunk in YAFFS terminology) {{that describes the}} file metadata, such as timestamps, name, path, etc. The new file is assigned a unique object ID number; every data chunk within the file will contain this unique object ID within the spare area. YAFFS maintains a tree structure in RAM memory of the physical location of these chunks. When a chunk is no longer valid (the file is deleted, or parts of the file are overwritten), YAFFS marks a particular byte in the spare area of the chunk as ‘dirty’. When an entire block (32 pages) is marked as dirty, YAFFS can erase the block and reclaim the space. When the filesystem's free space is low, YAFFS consolidates a group of good pages onto a new block. YAFFS then reclaims the space used by <b>dirty</b> <b>pages</b> {{within each of the}} original blocks.|$|E
30|$|In Fig. 2 (b) pre-copy {{approach}} [5], {{the minimal}} {{state of the}} processor is transferred to the target and followed by iterative push phase where the <b>dirty</b> (modified) <b>pages</b> are pushed to destination iteratively until the <b>dirty</b> <b>page</b> rate {{is less than the}} pages transferred rate. Then a small stop and copy phase is followed. In pre-copy approach, during iterative push phase, many zero pages which contains all zeros, identical pages (80 % above similar) and similar pages (60 % to 80 % similar) are transferred to the target host. These pages are not required for VM to resume [6] at the destination machine.|$|R
30|$|The {{comprehensive}} {{survey of}} state-of-the-art Live VM migration approaches {{are divided into}} two broad categories. We first discuss the models - which are theoretical phases. Then we discuss the frameworks - which are practical implementation. The live VM migration frameworks are further divided into three sub-categories like the type of migration, duplication based VM migration, and context aware VM migration. These categories {{are based on the}} (i) single or multiple VM migration, (ii) replication, duplication, redundancy and compression VM/VM’s memory pages, and (iii) dependency among VM’s, soft <b>page,</b> <b>dirty</b> <b>page</b> (<b>dirty</b> <b>page</b> rate) and page fault due to network of VM pages. The existing approaches of all the above sub-categories are compared based on performance metrics. Threats in live VM migration are discussed and categorize the possible attacks in three categories (control plane, data plane and migration module) based on the type of attack. Finally we mention some of the critical research challenges which require further research for improving the migration process and efficiency of CDC’s.|$|R
30|$|Few of {{existing}} live migration techniques {{can be applied}} to the delay-sensitive web services applications or a VM backup process that needs to be done in a specific time. Pre-copy migration technique requires frequently varied transfer bandwidth, which is a critical problem for network operators. The accurate prediction of migration time is also not possible. Zang et al. [117] theoretically analyze appropriate bandwidth that guarantees the total migration time and service downtime. Authors first assume the dirty distribution of VM memory pages that follow the deterministic distribution. Then the bandwidth is determined under the condition that dirty distribution obeys the bernoulli distribution. Authors assumed that the dirty frequency of each page is varied and the Cumulative Distribution Function (CDF) of the <b>dirty</b> <b>page</b> frequency is a reciprocal function. The experiment is conducted on two Servers (Dell, Linux kernel 2.6. 18.8 -xen, Xen 3.4. 3 hypervisor) connected by HP ProCurve 2910 al Ethernet. The observed results show that the reciprocal-based model characterize the <b>dirty</b> <b>page</b> rate well and also provides bounded delay guarantee.|$|R
40|$|Service can be {{delivered}} anywhere and anytime in cloud computing using virtualization. The main issue to handle virtualized resources is to balance ongoing workloads. The migration of virtual machines has two major techniques: (i) reducing <b>dirty</b> <b>pages</b> using CPU scheduling and (ii) compressing memory pages. The available techniques for live migration {{are not able to}} predict <b>dirty</b> <b>pages</b> in advance. In the proposed framework, time series based prediction techniques are developed using historical analysis of past data. The time series is generated with transferring of memory pages iteratively. Here, two different regression based models of time series are proposed. The first model is developed using statistical probability based regression model and it is based on ARIMA (autoregressive integrated moving average) model. The second one is developed using statistical learning based regression model and it uses SVR (support vector regression) model. These models are tested on real data set of Xen to compute downtime, total number of pages transferred, and total migration time. The ARIMA model is able to predict <b>dirty</b> <b>pages</b> with 91. 74 % accuracy and the SVR model is able to predict <b>dirty</b> <b>pages</b> with 94. 61 % accuracy that is higher than ARIMA...|$|E
30|$|Third Phase: This {{phase is}} termed as stop and copy phase. In this phase, {{remaining}} pages are transferred {{based on two}} stopping conditions. The first stopping condition {{is the same as}} the stopping condition in precopy method. The second stopping condition states that if the number of <b>dirty</b> <b>pages</b> in last iteration is larger than 1.5 times of the number of <b>dirty</b> <b>pages</b> in the previous iteration, the pages are transferred after compression. The compression method used is Run Length encoding. This condition is employed to ensure that the downtime should not be unbearable.|$|E
40|$|Abstract: Live VM (virtual machine) {{migration}} {{has become}} a research hotspot of virtualized cloud computing architecture. We present a novel migration algorithm which is called HMDC. Its main idea includes two parts. One is that it combines memory pulling copy with memory pushing copy to achieve hybrid memory copy. The other one is that it uses a delta compression mechanism during <b>dirty</b> <b>pages</b> copy, in which source host makes use of memory cache to get XOR delta pages, and then compresses delta pages which are easy to compress by the XOR binary RLE (run-length encoding) algorithm. Source host transmits delta compression pages instead of <b>dirty</b> <b>pages</b> to target host. HMDC increases throughput and decreases total migration data by using delta compression and thus to achieve <b>dirty</b> <b>pages</b> copy quickly. The experimental results demonstrate that HMDC evidently reduces total migration time, VM downtime and total migration data compared with Pre-copy and XBRLE algorithm. It makes the process of memory migration more high-effective and transparent...|$|E
30|$|Preparation Time: The time {{difference}} between initiation of migration and transferring the VM’s {{state to the}} target server, while continuing its execution and <b>dirtying</b> memory <b>pages.</b>|$|R
30|$|The {{categories}} {{are based on}} the inter-dependency among single or multi-VM pages, zero content memory pages, the frequency of <b>page</b> <b>dirty</b> and network/page fault aware.|$|R
50|$|According {{to their}} Myspace <b>page,</b> <b>Dirty</b> Looks {{reformed}} in 2007, and released a new album, Gasoline. Two more studio albums were released in 2008; Superdeluxe and California Free Ride.|$|R
30|$|The pre-copy {{technique}} is not performed well if running application is Write-intensive. Because write-intensive application frequently modifies {{a large number}} of pages that result in <b>dirty</b> <b>pages</b> transferred multiple time.|$|E
40|$|Most {{embedded}} systems {{are equipped with}} flash memory owing to its shock resistance, fast access, and low power consumption. However, some of its distinguishing characteristics, including out-of-place updates, an asymmetric read/write/erase speed, and {{a limited number of}} write/erase cycles, make it necessary to reconsider the existing system designs to explore its performance potential. For example, the buffer replacement policy of flash-based systems should not only consider the cache hit ratio, but also the relative heavy write and erase costs that are caused by flushing <b>dirty</b> <b>pages.</b> Most of the recent studies on buffer designs have focused on a Clean-First LRU strategy that evicts clean pages preferentially {{to reduce the number of}} writes to flash memory. However, each of them fails to distinguish <b>dirty</b> <b>pages,</b> which may have a different effect on the flash memory. In this paper, we propose a Lazy-Split LRU-based buffer management scheme that not only considers an imbalance of the read and write speeds but also different effects of different <b>dirty</b> <b>pages</b> and frequent changes of the B+-tree index structure caused by intensive overwrites. Specifically, it introduces a semi-clean state to further classify some <b>dirty</b> <b>pages</b> into clean part and dirty part and several efficient replacement policies to reduce the number of B+-tree splits. The experimental results show that our solution outperforms other algorithms including pure LRU and CFDC, and is effective and efficient for improving the performance of B+-tree on flash memory...|$|E
30|$|During the {{iterative}} {{phase of}} pre-copy live VM migration, the VM’s pages are sent over the network between corresponding servers. As the source VM is running during this process, its memory contents are constantly updated. Because memory bandwidth {{is higher than}} network bandwidth, {{there is a high}} risk of memory pages being dirtied at a faster rate than they can be transferred over the network. As a result, these <b>dirty</b> <b>pages</b> are transferred repeatedly while the amount of remaining <b>dirty</b> <b>pages</b> transfer does not decrease. This means that the migration process gets stuck in the iterative phase and as a result, the migration may have to be forced into the stop-and-copy phase with a large number of <b>dirty</b> <b>pages</b> remaining to transfer. As the VM is suspended during the stop-and-copy phase, this leads to extended migration downtime and a prolonged total migration time. Even in less severe cases, where the algorithm {{does not need to be}} forced to proceed stop-and-copy phase, total migration time and service downtime are still extended to some degree.|$|E
30|$|Patel et al. [89] {{measured}} the performance impact of prediction during live virtual machine migration using machine learning algorithms. Specifically, a time series prediction using historical analysis of past data relating <b>dirty</b> memory <b>pages</b> was performed using two prediction models; {{the first one}} using an autoregressive integrated moving average model, and the second one based on a learning model using the support vector regression technique.|$|R
40|$|Virtual machine {{migration}} {{is a useful}} and widely used workload management technique. However, the overhead of moving gigabytes of data across machines, racks, or even data centers limits its applicability. According {{to a recent study}} by IBM [7], the number of distinct servers visited by a migrating VM is small; often just two. By storing a checkpoint on each server, a subsequent incoming migration of the same VM must transfer less data over the network. Our analysis shows that for short migration intervals of 2 hours on average 50 % to 70 % of the checkpoint can be reused. For longer migration intervals of up to 24 hours still between 20 % to 50 % can be reused. In addition, we compared different methods to reduce the migration traffic. We find that content-based redundancy elimination consistently achieves better results than relying on <b>dirty</b> <b>page</b> tracking alone. Sometimes the difference is only a few percent, but can reach up to 50 % and more. Our empirical measurements with a QEMU-based prototype confirm the reduction in migration traffic and time...|$|R
40|$|International audienceLive-migration {{has become}} a common {{operation}} on virtualized infrastructures. Indeed, it is widely used by resource management algorithms to distribute the load between servers and to reduce energy consumption. Operators rely also on migrations to prepare production servers for critical maintenance by relocating their running VMs elsewhere. To apply new VM placement decisions, live-migrations must be scheduled by selecting for each migration the moment to start and the bandwidth to allocate. Long migrations violate SLAs and reduce the practical benefits of placement algorithms. The VMs should then be migrated as fast as possible. To do so, the migration scheduler {{must be able to}} predict accurately the migration durations and schedule them accordingly. Dynamic VM placement algorithms focus extensively on computing a placement of quality. Their practical reactivity is however lowered by restrictive assumptions that underestimate the migration durations. For example, Entropy supposes a non-blocking homogeneous network coupled with a null <b>dirty</b> <b>page</b> rate and we already demonstrated that the network topology but also the workload live memory usage are dominating factors. Recently, some migration models have been developed and integrated into simulators to evaluate VM placement algorithms properly. While these models reproduce migrations finely, they are only devoted to simulation purpose and not used to compute scheduling decisions. We propose here a migration scheduler that considers the network topology, the migration routes, the VM memory usage and the <b>dirty</b> <b>page</b> rates, to compute precise migration durations and infer better schedules. We implemented our scheduler on top of BtrPlace, an extensible version of Entropy that allows to enrich the scheduling decision capabilities through plug-ins. To assess the flexibility of our scheduler, we also implemented constraints to synchronize migrations, to establish precedence rules, to respect power budgets and an objective that minimizes energy consumption. We evaluated our model accuracy and its resulting benefits by executing migration scenarios on a real testbed including a blocking network, mixed VM memory workloads and collocation settings. Our model predicted the migration durations with a 94 % accuracy at minimum and an absolute error of 1 second while BtrPlace vanilla was only 30 % accurate. This gain of precision led to wiser scheduling decisions. In practice, the migrations completed on average 3. 5 time faster as compared to an execution based on BtrPlace vanilla. Thanks to a better control of migrations and power-switching actions we also reduced the power consumption of a server decommissioning scenario according to different power budgets...|$|R
40|$|Operating {{system memory}} {{managers}} fail {{to consider the}} population of read versus write pages in the buffer pool and the outstanding I/O requests when writing <b>dirty</b> <b>pages</b> to disk or network file systems. This leads to bursty I/O patterns, which stall processes reading data and reduce the efficiency of disk and file systems. We address these limitations by adaptively allocating memory between write buffering and read caching and by writing <b>dirty</b> <b>pages</b> to disk opportunistically before the operating system submits them for writeback. We implement and evaluate our methods within the Linux operating system and show performance gains of more than 30 % for mixed read/write workloads. ...|$|E
30|$|Effectiveness of Live Virtual Machine Migration {{technique}}: Pre-copy technique {{focus to}} keep downtime small by minimizing transferred VM’s state, so application service is running without interruption or VM transfer is seamless. But it increases total migration time {{due to the}} repeatedly transfer of <b>dirty</b> <b>pages.</b>|$|E
30|$|Preparation time: When {{migration}} is initiated, {{the time for}} transferring minimal state of the CPU to the destination. In the pre-copy approach, pages become dirty while VM on source host is running. This time includes the entire process of iteratively pushing the <b>dirty</b> <b>pages</b> to the destination host.|$|E
40|$|In this paper, {{we address}} {{how to reduce}} the amount of page updates in flash-based DBMS {{equipped}} with SSD (solid state drive). We propose a novel buffering scheme that evicts a <b>dirty</b> <b>page</b> X without flushing it into SSD, and restores the right image of X when X is requested for later access. The restoration of X having previous flushing-less eviction is performed through our online redo actions on X. We call this page-restoring online redo the on-the-fly redo. Although our on-the-fly redo mechanism has some overhead of increasing the number of page reads, this can be compensated by infrequent page updates. Additionally, since the proposed buffering scheme with the on-the-fly redo can easily support the no-steal policy in buffer management, we can enjoy the advantages of smaller logging overhead and faster recovery. Through the TPC-C benchmarks using a Berkeley DB, we show that our scheme shortens the transaction processing times by up to 53 %. © 2015 ACM. This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. NRF- 2014 R 1 A 2 A 1 A 10054151) ...|$|R
40|$|In most {{operating}} systems which are customized for disk-based storage system, the replacement algorithm concerns only {{the number of}} memory hits. However, flash memory has different read and write cost in the aspects {{of time and energy}} so the replacement algorithm with flash memory should consider not only the hit count but also the replacement cost caused by selecting dirty victims. The replacement cost of <b>dirty</b> <b>page</b> is higher than that of clean page with regard to both access time and energy consumption. In this paper, we propose the Clean-First LRU (CFLRU) replacement algorithm that exploits the characteristics of flash memory. CFLRU splits the LRU list into the working region and the clean-first region and adopts a policy that evicts clean pages preferentially in the clean-first region until the number of page hits in the working region is preserved in a suitable level. Using the trace-driven simulation, the proposed algorithm reduces the average replacement cost by 28. 4 % in swap system and by 26. 2 % in buffer cache, compared with LRU algorithm. We also implement the CFLRU algorithm in the Linux kernel and present some optimization issues...|$|R
30|$|For {{accurate}} {{prediction of}} migration performance, a model is proposed by Akoush et al. [70], which examines the service interruptions {{for a particular}} workload. Authors show network link capacity and memory dirty rate are the major factors that highly affect the migration behavior. The predicted value of migration time must be accurate to handle dynamic and intelligent VM placement without affecting application performance. Live VM migration behavior in pre-copy migration technique is investigated in Xen hypervisor platform. The link capacity and <b>page</b> <b>dirty</b> rate highly impact migration performance in a non-linear manner due to hard-stop conditions force migration in last stop-and-copy phase. Authors also implement Average <b>page</b> <b>dirty</b> rate (AVG) and History based <b>page</b> <b>dirty</b> rate (HIST) simulation models, used to predict the performance of pre-copy migration. Experiment is performed on 3 servers (2 Intel(R) Xeon(TM) E 5506 CPU’s 2.13 GHZ, 6 GB DDR 3 RAM, dual Gigabit Ethernet, Citrix Xenserver 5.5. 0 (Xen 3.3. 1), Ubuntu 2.6. 27 - 7 kernel). The results show that for high speed (10 Gbps) network links, Xen migration architecture does work well. Several optimization’s approaches increase the migration throughput by 125.5 % (from 3.2 Gbps to 7.12 Gbps). Both AVG and HIST models are more than 90 % accurate with respect to actual results.|$|R
