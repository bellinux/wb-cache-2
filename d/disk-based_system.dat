9|112|Public
50|$|RSX-11D - a {{multiuser}} <b>disk-based</b> <b>system.</b> Evolved into IAS.|$|E
50|$|PolyMorphic's <b>disk-based</b> <b>system</b> was the System 8813. It {{consisted}} of a larger chassis holding one, two, or three 5-inch minifloppy disk drives from Shugart Associates. The drives used single-sided, single-density storage on hard-sectored diskettes. Storage capacity was approximately 90K bytes per diskette.|$|E
50|$|In a single-user <b>disk-based</b> <b>system,</b> it is {{possible}} to scan the media file in real time to locate the independent frames, but in a network-based video-on-demand system, the bandwidth allocated to the client is fixed, so the server has to use pregenerated 'hint' or 'index' information to locate suitable frames, and then play them out at the selected fast-forward or rewind speed within the original bandwidth envelope. Alternatively, the server may pregenerate an entirely new video stream with at least one forward and backward speed and switch to it when requested.|$|E
50|$|The MT/ST became {{obsolete}} in the 1970s {{in favor}} of floppy <b>disk-based</b> <b>systems.</b> IBM discontinued support in 1983.|$|R
40|$|Abstract—In-memory {{database}} {{systems are}} becoming {{popular due to}} the availability and affordability of sufficiently large RAM and processors in modern high-end servers with the capacity to manage large in-memory database transactions. While fast and reliable in-memory systems are still being developed to overcome cache misses, CPU/IO bottlenecks and distributed transaction costs, disk-based data stores still serve as the primary persistence. In addition, with the recent growth in multi-tenancy cloud applications and associated security concerns, many organisations consider the trade-offs and continue to require fast and reliable transaction processing of <b>disk-based</b> database <b>systems</b> as an available choice. For these organizations, {{the only way of}} increasing throughput is by improving the performance of disk-based concurrency control. This warrants a hybrid database system with the ability to selectively apply an enhanced disk-based data management within the context of in-memory systems that would help improve overall throughput. The general view is that in-memory <b>systems</b> substantially outperform <b>disk-based</b> <b>systems.</b> We question this assumption and examine how a modified variation of access invariance that we call enhanced memory access, (EMA) can be used to allow very high levels of concurrency in the pre-fetching of data in <b>disk-based</b> <b>systems.</b> We demonstrate how this prefetching in <b>disk-based</b> <b>systems</b> can yield close to in-memory performance, which paves the way for improved hybrid database systems. This paper proposes a novel EMA technique and presents a comparative study between <b>disk-based</b> EMA <b>systems</b> and in-memory systems running on hardware configurations of equivalent power {{in terms of the number}} of processors and their speeds. The results of the experiments conducted clearly substantiate that when used in conjunction with all concurrency control mechanisms, EMA can increase the throughput of <b>disk-based</b> <b>systems</b> to levels quite close to those achieved by in-memory system. The promising results of this work show that enhanced <b>disk-based</b> <b>systems</b> facilitate in improving hybrid data management within the broader context of in-memory <b>systems.</b> Keywords—Concurrency control, <b>disk-based</b> databases, in-memory <b>systems,</b> enhanced memory access (EMA). I...|$|R
50|$|Hard <b>disk-based</b> <b>systems</b> include TiVo as well {{as other}} digital video {{recorder}} (DVR) offerings. These types of systems provide users with a no-maintenance solution for capturing video content. Customers of subscriber-based TV generally receive electronic program guides, enabling one-touch setup of a recording schedule. Hard <b>disk-based</b> <b>systems</b> allow for many hours of recording without user-maintenance. For example, a 120 GB system recording at an extended recording rate (XP) of 10 Mbit/s MPEG-2 can record over 25 hours of video content.|$|R
40|$|Abstract * In this paper, {{we discuss}} {{the design of a}} data {{normalization}} system that we term commonality factoring. A real-world implementation of a storage system based upon data normalization requires design of the data normalization itself, of the storage repository for the data, and of the protocols to be used between applications performing data normalization and the server software of the repository. Each of these areas is discussed and potential applications are presented. Building on research begun in 1999, Avamar Technologies has implemented an initial application of this technology to provide a nearline, <b>disk-based</b> <b>system</b> for backup of primary storage. 1...|$|E
40|$|This paper {{presents}} a new cluster architecture for low-power dataintensive computing. FAWN couples low-power embedded CPUs to {{small amounts of}} local flash storage, and balances computation and I/O capabilities to enable efficient, massively parallel access to data. The key contributions of this paper are {{the principles of the}} FAWN architecture and the design and implementation of FAWN-KV—a consistent, replicated, highly available, and high-performance keyvalue storage system built on a FAWN prototype. Our design centers around purely log-structured datastores that provide the basis for high performance on flash storage, as well as for replication and consistency obtained using chain replication on a consistent hashing ring. Our evaluation demonstrates that FAWN clusters can handle roughly 350 key-value queries per Joule of energy—two orders of magnitude more than a <b>disk-based</b> <b>system...</b>|$|E
40|$|Current {{systems for}} graph {{computation}} require a distributed computing cluster to handle very large real-world problems, such as analysis on social networks or the web graph. While distributed computational resources {{have become more}} accessible, developing distributed graph algorithms still remains challenging, especially to non-experts. In this work, we present GraphChi, a <b>disk-based</b> <b>system</b> for computing efficiently on graphs with billions of edges. By using a well-known method to break large graphs into small parts, and a novel parallel sliding windows method, GraphChi is able to execute several advanced data mining, graph mining, and machine learning algorithms on very large graphs, using just a single consumer-level computer. We further extend GraphChi to support graphs that evolve over time, and demonstrate that, on a single computer, GraphChi can process over one hundred thousand graph updates per second, while simultaneously performing computation. We show, through experiments and theoretical analysis, that GraphChi performs well on both SSDs and rotational hard drives. By repeating experiments reported for existing distributed systems, we show that, with only fraction of the resources, GraphChi can solve the same problems in very reasonable time. Our work makes large-scale graph computation available to anyone with a modern PC. ...|$|E
40|$|Currently, {{e-commerce}} {{is in its}} infancy, however its {{expansion is}} expected to be exponential and as it grows, so too will the demands for very fast real time online transaction processing systems. One avenue for meeting the demand for increased transaction processing speed is conversion from disk-based to in-memory databases. However, while in-memory systems are very promising, there are many organizations whose data is too large to fit in in-memory systems or who are not willing to undertake the investment that an implementation of an in-memory system requires. For these organizations an improvement in the performance of <b>disk-based</b> <b>systems</b> is required. Accordingly, in this thesis, we introduce two mechanisms that substantially improve the performance of <b>disk-based</b> <b>systems.</b> The first mechanism, which we call a contention-based scheduler, is attached to a standard 2 PL system. This scheduler determines each transaction's probability of conflict before it begins executing. Using this knowledge, the contention-based scheduler allows transactions into the system in both optimal numbers and an optimal mix. We present tests that show that the contention-based scheduler substantially outperforms standard 2 PL concurrency control {{in a wide variety of}} disk-based hardware configurations. The improvement though most pronounced in the throughput of low contention transactions extends to all transaction types over an extended processing period. We call the second mechanism that we develop to improve the performance of <b>disk-based</b> database <b>systems,</b> enhanced memory access (EMA). The purpose of EMA is to allow very high levels of concurrency in the pre-fetching of data thus bringing the performance of <b>disk-based</b> <b>systems</b> close to that achieved by in-memory systems. The basis of our proposal for EMA is to ensure that even when conditions satisfying a transaction's predicate change between pre-fetch time and execution time, the data required for satisfying transactions' predicates are still found in memory. We present tests that show that the implementation of EMA allows the performance of <b>disk-based</b> <b>systems</b> to approach the performance achieved by in-memory systems. Further, the tests show that the performance of EMA is very robust to the imposition of additional costs associated with its implementation...|$|R
5000|$|Software {{was also}} minimal for the non-disk drive {{versions}} of the C1P, C2P, and Superboard II. They contained an 8K Basic in ROM and used cassette tapes to load and store programs. <b>Disk-based</b> <b>systems</b> included a bare-bones [...] "Disk Operating System" [...] that was much handier than using compact cassettes at 1200 baud.|$|R
5000|$|... #Subtitle level 2: The ExaGrid <b>disk-based</b> backup <b>system</b> {{with data}} {{deduplication}} ...|$|R
40|$|The Mark 5 C disk-based VLBI data {{system is}} being {{developed}} as the third-generation Mark 5 <b>disk-based</b> <b>system,</b> increasing the sustained data-recording rate capability to 4 Gbps. It is built on the same basic platform as the Mark 5 A, Mark 5 B and Mark 5 B+ systems and will use the same 8 -disk modules as earlier Mark 5 systems, although two 8 -disk modules {{will be necessary to}} support the 4 Gbps rate. Unlike its earlier brethren, which use proprietary data interfaces, the Mark 5 C will accept data from a standard 10 Gigabit Ethernet connection and be compatible with the emerging VLBI Data Interchange Format (VDIF) standard. Data sources for the Mark 5 C system will be based on new digital backends now being developed, specifically the RDBE in the U. S. and the dBBC in Europe, as well as others. The Mark 5 C system is being planned for use with the VLBI 2010 system and will also be used by NRAO as part of the VLBA sensitivity upgrade program; it will also be available to the global VLBI community from Conduant. Mark 5 C system specification and development is supported by Haystack Observatory, NRAO, and Conduant Corporation. Prototype Mark 5 C systems are expected in early 2010...|$|E
40|$|Portable {{systems such}} as cell phones and {{portable}} media players commonly use non-volatile RAM (NVRAM) to hold all of their data and metadata, and larger systems can store metadata in NVRAM to increase file system performance by reducing synchronization and transfer overhead between disk and memory data structures. Unfortunately, wayward writes from buggy software and random bit flips may result in an unreliable persistent store. We introduce two orthogonal and complementary approaches to reliably storing file system structures in NVRAM. First, we reinforce hardware and operating system memory consistency by employing page-level write protection and error correcting codes. Second, we perform on-line consistency checking of the filesystem structures by replaying logged file system transactions on copied data structures; a structure is consistent if the replayed copy matches its live counterpart. Our experiments show that the protection mechanisms can increase fault tolerance by six orders of magnitude while incurring an acceptable amount of overhead on writes to NVRAM. Since NVRAM is much faster and consumes far less power than disk-based storage, the added overhead of error checking leaves an NVRAM-based system both faster and more reliable than a <b>disk-based</b> <b>system.</b> Additionally, our techniques can be implemented on systems lacking hardware support for memory management, allowing them to be used on lowend and embedded systems without an MMU...|$|E
40|$|Due to the {{presence}} of large and heterogeneous user workloads and concurrent I/O requests, it is important to guarantee isolation in cloud storage systems. This dissertation explores isolation in cloud storage systems and makes fundamental contributions that advance {{the state of the art}} in supporting such isolation. Specifically, this dissertation focuses on three key areas necessary for isolation in cloud storage systems: performance isolation, transactional isolation, and fine-grained consistency control. Regarding the first, performance isolation, resource contention in storage systems is often unavoidable under concurrent users and the contention allows a user to affect the performance experienced by other users. In particular, a single user can easily degrade the performance for all other users in a <b>disk-based</b> <b>system</b> because the disk performance is inherently susceptible to random I/O requests. Second, to maintain consistent data states under concurrent I/O requests, systems have implemented transactional isolation in high layers of the storage stack. However, different implementations in high layers of the storage stack make the support for transactional isolation redundant and transactions executed by different applications incompatible with each other. Thus, portable and compatible transactional isolation is required, as well as reconsideration of the layers of the storage stack in which transactional isolation should be placed. Finally, distributed systems often provide per-client views of the system by using client-centric consistency semantics to trade off consistency and performance. While cloud storage servers have tens of parallel storage devices and CPU cores, which make the server comparable to a distributed system, the potential trade-off between consistency and performance within a server has never been explored. We subsequently make three contributions embodied in approaches to addressing various isolation challenges. First, we present an approach that achieves performance isolation by resolving I/O contention using a chained-logging design. The chained-logging design retains at least one disk for sequentially logging without I/O contention even under garbage collection and systematically separates read and write operations to different disks. We implemented an instance of the approach in a system called Gecko. Second, we investigate an approach for block-level transactions that support portable and compatible transactional isolation. The block-level transaction facilitates transactional application designs in any layer of the storage stack and enables cross-application transactions. We implemented an instance of the approach in a system called Isotope. Finally, we define a new class of systems called StaleStore, which can trade off consistency and performance within a server using stale data, and we study the necessary functionality and interface to take advantage of this trade-off. Yogurt, an instance of StaleStore, explores different versions of data and estimates the access cost for each version under client-centric consistency semantics to trade off consistency and performance within a server. Together, these three approaches are important steps towards isolation in cloud storage systems...|$|E
40|$|Currently, {{e-commerce}} {{is in its}} infancy, however its {{expansion is}} expected to be exponential and as it grows, so too will the demands for very fast real time online transaction processing systems. One avenue for meeting the demand for increased transaction processing speed is conversion from disk-based to in-memory databases. However, while in-memory systems are very promising, there are many organizations whose data is too large to fit in in-memory systems or who are not willing to undertake the investment that an implementation of an in-memory system requires. For these organizations an improvement in the performance of <b>disk-based</b> <b>systems</b> is required. Accordingly, in this thesis, we introduce two mechanisms that substantially improve the performance of <b>disk-based</b> <b>systems.</b> The first mechanism, which we call a contention-based scheduler, is attached to a standard 2 PL system. This scheduler determines each transaction’s probability of conflict before it begins executing. Using this knowledge, the contention-based scheduler allow...|$|R
50|$|Quantum Corporation is a {{manufacturer}} of data storage devices and systems, including tape drive and <b>disk-based</b> <b>systems.</b> The company's headquarters is in San Jose, California. From its founding in 1980 until 2001, {{it was also a}} major disk storage manufacturer (usually second-place in market share behind Seagate), and was based in Milpitas, California. Quantum sold its hard disk drive business to Maxtor in 2001 and now focuses on integrated storage systems.|$|R
40|$|Abstract: Green {{computing}} {{or energy}} saving when processing information is primarily considered a task of processor development. However, this position paper advocates that a holistic approach {{is necessary to}} reduce power consumption to a minimum. We discuss the potential of integrating NAND flash memory into DB-based ar-chitectures and its support by adjusted DBMS algorithms governing IO processing. The goal is to drastically improve energy efficiency while comparable performance as is <b>disk-based</b> <b>systems</b> is maintained. ...|$|R
50|$|Memory {{was fitted}} {{in up to}} four banks of RAM, each of either 4 KB (4 × 1024 bytes) or 16 KB, {{although}} not every permutation was permitted. Typical configurations were 16 KB for cassette-based systems and 32, 48 or 64 KB of memory on <b>disk-based</b> <b>systems.</b> Main memory was not used by the text or graphics video cards, although memory on the video cards was bank switched into a dedicated 1.5 KB address block.|$|R
5000|$|Synthetic {{file system}} a {{hierarchical}} interface to non-file objects that {{appear as if}} they were regular files in the tree of a <b>disk-based</b> file <b>system</b> ...|$|R
50|$|Video {{and audio}} data are first {{captured}} to video servers, other hard <b>disk-based</b> <b>systems,</b> or other digital storage devices. The data are either {{direct to disk}} recording or are imported from another source (transcoding, digitizing, transfer). Once imported, the source material can be edited on a computer using application software, any {{of a wide range}} of video editing software. For a comprehensive list of available software, see List of video editing software, whereas Comparison of video editing software gives more detail of features and functionality.|$|R
40|$|Data-protection class workloads, {{including}} backup {{and long-term}} retention of data, {{have seen a}} strong industry shift from tape-based platforms to <b>disk-based</b> <b>systems.</b> But the latter are traditionally designed to serve as primary storage {{and there has been}} little published analysis of the characteristics of backup workloads as they relate to the design of <b>disk-based</b> <b>systems.</b> In this paper, we present a comprehensive characterization of backup workloads by analyzing statistics and content metadata collected from a large set of EMC Data Domain backup systems in production use. This analysis is both broad (encompassing statistics from over 10, 000 systems) and deep (using detailed metadata traces from several production systems storing almost 700 TB of backup data). We compare these systems to a detailed study of Microsoft primary storage systems [22], showing that backup storage differs significantly from their primary storage workload in the amount of data churn and capacity requirements as well as the amount of redundancy within the data. These properties bring unique challenges and opportunities when designing a disk-based filesystem for backup workloads, which we explore in more detail using the metadata traces. In particular, the need to handle high churn while leveraging high data redundancy is considered by looking at deduplication unit size and caching efficiency. ...|$|R
50|$|The System/3 (1969) ran a <b>disk-based</b> batch {{operating}} <b>system</b> called SCP (5702-SC1). IBM introduced for the S/3 {{an online}} program called CCP ("Communications Control Program.") which was {{started as a}} batch program. The IBM System/32 (1975) ran a <b>disk-based</b> operating <b>system</b> called SCP ("System Control Program.") The IBM System/38 (1978) ran an operating system called CPF ("Control Program Facility") that was much more advanced than SSP and not particularly similar.|$|R
40|$|Abstract- Spark is an {{open source}} cluster {{computing}} system that aims to make data analytics fast — both fast to run and fast to write. To run programs faster, Spark provides primitives for inmemory cluster computing: your job can load data into memory and query it repeatedly much quicker than with <b>disk-based</b> <b>systems</b> like Hadoop Map Reduce. To make programming faster, Spark integrates into the Scala language, letting you manipulate distributed datasets like local collections. You can also use Spark interactively to query big data from the Scala interpreter...|$|R
5000|$|Multi-tracking can be {{achieved}} with analogue recording, tape-based equipment (from simple, late-1970s cassette-based four track Portastudios, to eight track cassette machines, to 2" [...] reel-to-reel 24-track machines), digital equipment that relies on tape storage of recorded digital data (such as ADAT eight-track machines) and hard <b>disk-based</b> <b>systems</b> often employing a computer and audio recording software. Multi-track recording devices vary in their specifications, such {{as the number of}} simultaneous tracks available for recording at any one time; in the case of tape-based systems this is limited by, among other factors, the physical size of the tape employed.|$|R
40|$|Systems {{that provide}} {{powerful}} transaction mechanisms often rely on write-ahead logging (WAL) implementations {{that were designed}} with slow, <b>disk-based</b> <b>systems</b> in mind. The emerging class of fast, byte-addressable, non-volatile memory (NVM) technologies (e. g., phase change memories, spin-torque MRAMs, and the memristor), however, present performance characteristics very different from both disks and flash-based SSDs. This paper addresses the problem of designing a WAL scheme optimized for these fast NVM-based storage systems. We examine the features that a system like ARIES, a WAL algorithm popular for databases, must provide and separate them from the implementation decisions ARIES makes to optimize for <b>disk-based</b> <b>systems.</b> We design a new NVMoptimized WAL scheme (called MARS) in tandem with a novel SSD multi-part atomic write primitive that combine to provide the same features as ARIES does {{without any of the}} disk-centric limitations. The new atomic write primitive makes the log’s contents visible to the application, allowing for a simpler and faster implementation. MARS provides atomicity, durability, and high performance by leveraging the enormous internal bandwidth and high degree of parallelism that advanced SSDs will provide. We have implemented MARS and the novel visible atomic write primitive in a next-generation SSD. This paper demonstrates the overhead of the primitive is minimal compared to normal writes, and our hardware provides large speedups for transactional updates to hash tables, b-trees, and large graphs. MARS outperforms ARIES by up to 3. 7 ⇥ while reducing software complexity. ...|$|R
5000|$|Channel Gateway is {{a family}} of {{products}} that provide mainframe access to <b>disk-based</b> open <b>systems</b> storage by acting as a tape control unit and presenting the storage as [...] "virtual" [...] tape drives (emulating IBM 3490/3590) via FICON or ESCON channels.|$|R
40|$|Disk-oriented {{approaches}} to online storage {{are becoming increasingly}} problematic: they do not scale gracefully {{to meet the needs}} of large-scale Web applications, and improvements in disk capacity have far outstripped improvements in access latency and bandwidth. This paper argues for a new approach to datacenter storage called RAMCloud, where information is kept entirely in DRAM and large-scale systems are created by aggregating the main memories of thousands of commodity servers. We believe that RAMClouds can provide durable and available storage with 100 - 1000 x the throughput of <b>disk-based</b> <b>systems</b> and 100 - 1000 x lower access latency. The combination of low latency and large scale will enable a new breed of dataintensive applications. ...|$|R
40|$|The {{dream of}} {{replacing}} rotating mechanical storage, the disk drive, with solid-state, nonvolatile RAM {{may become a}} reality in the near future. Approximately ten new technologies—collectively called storage-class memory (SCM) —are currently under development and promise to be fast, inexpensive, and power efficient. Using SCM as a disk drive replacement, storage system products will have random and sequential I/O performance that is orders of magnitude better than that of comparable <b>disk-based</b> <b>systems</b> and require much less space and power in the data center. In this paper, we extrapolate disk and SCM technology trends to 2020 and analyze the impact on storage systems. The result is a 100 - to 1, 000 -fold advantage for SCM {{in terms of the}} data center space and power required...|$|R
5000|$|Digital Research's CP/M {{operating}} system was supplied with the 664 and 6128 <b>disk-based</b> <b>systems,</b> and the DDI-1 disk expansion unit for the 464. 64k machines shipped with CP/M 2.2 alone, while the 128k machines also include CP/M 3.1. The compact CP/M 2.2 implementation is largely stored {{on the boot}} sectors of a 3" [...] disk in what was called [...] "System format"; typing |CPM from Locomotive BASIC would load code from these sectors, making it a popular choice for custom game loading routines. The CP/M 3.1 implementation is largely in a separate file which is in turn loaded from the boot sector.Much public domain CP/M software was made available for the CPC, from word-processors such as VDE to complete bulletin board systems such as ROS.|$|R
50|$|The main {{operating}} {{systems for the}} 1700 were the Utility System, which usually {{took the form of}} several punched paper tapes (resident monitor plus utilities), a similar Operating System for larger configurations (often including punched cards and magnetic tape), and the Mass Storage Operating <b>System</b> (MSOS) for <b>disk-based</b> <b>systems.</b> A Fortran compiler was available. Pascal was also available, via a cross compiler on a CDC 6000 series host. The Cyber 18 series, exploiting the extended instruction set, ran a disk-based OS, the Interactive Terminal Oriented System (ITOS). This system supported Fortran, Cobol, and UCSD Pascal. ITOS was a foreground/background system with multiple users connected via serial CRT terminals; user tasks ran in the background while the operating system itself ran in the foreground.|$|R
40|$|Power {{consumption}} {{has become}} {{an important factor in}} modern storage system design. Power efficiency is particularly beneficial in <b>disk-based</b> backup <b>systems</b> that store mostly cold data, have significant idle periods, and must compete with the operational costs of tape-based backup. There are no prior published studies on power consumption in these systems, leaving researchers and practitioners to rely on existing assumptions. In this paper we present the first analysis of power consumption in real-world, enterprise, <b>disk-based</b> backup storage <b>systems.</b> We uncovered several important observations, including some that challenge conventional wisdom. We discuss their impact on future power-efficient designs. ...|$|R
40|$|Many applications, such as telecommunication, process control, {{and virtual}} reality, require {{real-time}} access to database. Main-memory DBMS, which becomes feasible {{with the increasing}} availability of large and relatively cheap memory, can provide better performance than <b>disk-based</b> <b>systems</b> for real-time applications. This paper presents an overall architecture of M 2 RT, a Main-Memory Real-Time DBMS, and an object-oriented design of its storage system called M 2 RTSS. M 2 RTSS provides classes that implement the core functionality of storage management, real-time transaction scheduling, and recovery. Implementation-specific information is encapsulated in these classes and extensions {{can be made by}} inheritance. With object-oriented features, M 2 RTSS can easily incorporate new development in application requirements and the result of ongoing research in realtime systems. Keywords: object-oriented design and implementation, extensibility, main-memory DBMS, real-time DBMS 1 Introdu [...] ...|$|R
50|$|The CT {{series was}} a lower-cost product for Chromatics, {{designed}} around the recently introduced NEC µPD7220 graphics display controller chip. It was their only product built using a single circuit board. It {{was also the}} only series {{which could not be}} configured with disk storage and a <b>disk-based</b> operating <b>system.</b>|$|R
40|$|This paper {{describes}} {{current and}} anticipated {{work at the}} University of MichiganÕs Center for Information Technology Integration (CITI) in developing and integrating mass storage with distributed file systems, specifically with the Andrew File System (AFS). After surveying existing mass storage and associated file systems, this paper presents one approach to integrating AFS with mass storage. We consider the mass store itself to be the file system, not a bag {{on the side of}} a <b>disk-based</b> file <b>system.</b> This unifying perspective distinguishes our approach from other large-scale file systems. Instead of developing a back-end server to manage the movement of data files between traditional <b>disk-based</b> storage <b>systems</b> (employed, in our case, by AFS) and magnetic-tape or optical-based mass storage systems (of which AFS has little or no knowledge), we envision the mass store as a first-class data repository. A traditional <b>disk-based</b> file <b>system</b> serves as a (very large) cache of the mass store system. On top of that is another, large, high-speed memory cache. All storage other than the mass store is used exclusively for caching. In this approach, cache management policies are of fundamental importance. Two main requirements for this work are that the AFS name space remain unchanged, and performance seen by users must not suffer. For example, users must not have to pre-stage files explicitly in order to achieve acceptable performance...|$|R
40|$|In {{the last}} fifteen years, lock {{managers}} for regular <b>disk-based</b> database <b>systems</b> have seen little change. This is not without reason, since traditional memory-resident lock managers have always been much faster than disk-based database storage managers and <b>disk-based</b> database <b>systems</b> had few alternative design options. However, the introduction of memory-resident database systems has created both new requirements and new opportunities for better lock managers. We present {{the design of a}} lock manager that exploits the special nature of the memory-resident storage component in the Starburst experimental database system. To achieve a performance advantage over traditional lock managers, our lock manager physically attaches concurrency control meta-data to the database data itself, thereby making the meta-data directly accessible, rather than indirectly accessible via a hash-table structure. Furthermore, our lock manager eliminates intention locks and changes the lock granularity of each r [...] ...|$|R
