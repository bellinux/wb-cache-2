3|10000|Public
40|$|In this work, {{we present}} {{the results of a}} {{systematic}} study to investigate the (commercial) benefits of automatic text summarization systems in a real world scenario. More specifically, we <b>define</b> <b>a</b> <b>use</b> <b>case</b> in the context of media monitoring and media response analysis and claim that even using a simple query-based extractive approach can dramatically save the processing time of the employees without significantly reducing the quality of their work...|$|E
40|$|Smartphones are {{becoming}} increasingly popular, {{with the result that}} customers prefer to carry out at least some customer services using an app on a mobile device. Among app users, smooth transfer to a live agent is seen as an important feature and this means that the company contact center need a solution to handle this as well as increasing numbers of interactions. The question this thesis tries to answer is "how can smartphone end user apps be included {{in the context of the}} company contact center"? To answer this question research was conducted regarding the possibilities of an Android smartphone, with the results of this research being used to <b>define</b> <b>a</b> <b>use</b> <b>case,</b> a state flow diagram and create a demonstration app. The thesis showed that it is possible to have an app as an online channel for customer service interactions. New possibilities in comparison to traditional telephony include that customer data such as topic, authentication, location and multimedia can be sent to the contact center before an actual interaction is started...|$|E
40|$|Reproducibility is a {{cornerstone}} of the scientific process. While the reproduction of an experiment can be extremely difficult, the ability to reproduce the (computational) {{analysis of the data}} that supported a certain conclusion (e. g. the validation of a hypothesis) should be a minimum requirement on every piece of published research. We call this type of reproducibility analytical reproducibility. The ability to reproduce the analytic results of a certain piece of research requires, as a minimum, that: i) the primary or secondary data is available, ii) the data is syntactically well-formed and ready-to-use, iii) the data is appropriately documented, iv) the analysis procedures (e. g. scripts) that were used to process or analyze the data are available, and v) these analytic procedures can be run on the data to reproduce the actual result published in a paper. Analytical reproducibility is often hampered by the fact that one of the above requirements is not met. The goal of this project is to extend the infrastructure available at Bielefeld University for the management of data and publications by a framework that supports researchers in meeting the above mentioned requirements and thus to make their work analytically reproducible. Departing from current practices where data and software is published {{at the end of a}} research project, if at all, we intend to move the hosting of data to the very beginning of the scientific process. Borrowing ideas from computer science and from continuous integration, we intend to implement a continuous quality control framework that from early on encourages researchers to publish their data and analytic procedures so that these can easily be re-used and verified. The way we understand quality of data in the project is thus in the sense of readiness to be re-used and validated. Towards this goal, we will interact with a selected group of researchers at Bielefeld University that have committed themselves to <b>define</b> <b>a</b> <b>use</b> <b>case,</b> provide requirements, implement pilots, and continuously work with the infrastructure and provide regular feedback. The researchers come from disciplines as varied as psychology, sports sciences, biology, chemistry, cognitive linguistics, computational linguistics, robotics as well as economics. By involving a varied set of disciplines, our goal is to identify common requirements on an infrastructure that supports data quality as a continuous process, and supports sharing and external validation of research results. Besides extending our infrastructure, the project can be expected to have an impact way beyond Bielefeld University. By sharing our experiences and requirements identified, we hope to inform other universities and policy makers on the trade-off between effort and return-on-investment and which policies to adopt to support higher transparency in research...|$|E
5000|$|Another {{description}} of the difference is by [...] that <b>defines</b> <b>a</b> <b>use</b> <b>case</b> as <b>a</b> completed sequence of actions which gives increased value to the user, one could <b>define</b> <b>a</b> misuse case as a completed sequence of actions which results in loss for the organization or some specific stakeholder.|$|R
5000|$|Incidence rate, where <b>cases</b> {{included}} are <b>defined</b> <b>using</b> <b>a</b> <b>case</b> definition ...|$|R
50|$|A SAML {{profile is}} a {{concrete}} manifestation of <b>a</b> <b>defined</b> <b>use</b> <b>case</b> <b>using</b> <b>a</b> particular combination of assertions, protocols and bindings.|$|R
50|$|A SAML profile {{describes}} in detail how SAML assertions, protocols, and bindings combine to support <b>a</b> <b>defined</b> <b>use</b> <b>case.</b> The most important SAML profile is the Web Browser SSO Profile.|$|R
40|$|Masteroppgave i informasjons- og kommunikasjonsteknologi 2005 - Høgskolen i Agder, GrimstadSeveral {{frameworks}} {{are available}} in 3 GPP networks to create mobile services, such as the Open Service Access (OSA) Application Programming Interface (API) and the native Session Initiation Protocol (SIP). Each of these frameworks has their own advantages and disadvantages. Therefore {{it is important for}} <b>a</b> service to <b>use</b> <b>a</b> framework which suits its own requirements as best as possible. In this thesis we have <b>defined</b> <b>a</b> <b>use</b> <b>case,</b> TMMS Service. This <b>use</b> <b>case</b> has been designed for four frameworks selected, which are: GPRS, IMS, OSA API and OSA Parlay X Web Services. We have then evaluated the design of these services against a set of evaluation criteria. The evaluation criteria cover security, usability, modifiability, reliability, interoperability and billability. Our evaluation has proved that none of the frameworks are superior in all areas. The best framework overall is IMS which provides a lot of end-to-end features and is also very extensible. One of the biggest disadvantages with IMS is the current lack of a specific set of documentation for application developers...|$|R
40|$|Several {{frameworks}} {{are available}} in 3 GPP networks to create mobile services, such as the Open Service Access (OSA) Application Programming Interface (API) and the native Session Initiation Protocol (SIP). Each of these frameworks has their own advantages and disadvantages. Therefore {{it is important for}} <b>a</b> service to <b>use</b> <b>a</b> framework which suits its own requirements as best as possible. In this thesis we have <b>defined</b> <b>a</b> <b>use</b> <b>case,</b> TMMS Service. This <b>use</b> <b>case</b> has been designed for four frameworks selected, which are: GPRS, IMS, OSA API and OSA Parlay X Web Services. We have then evaluated the design of these services against a set of evaluation criteria. The evaluation criteria cover security, usability, modifiability, reliability, interoperability and billability. Our evaluation has proved that none of the frameworks are superior in all areas. The best framework overall is IMS which provides a lot of end-to-end features and is also very extensible. One of the biggest disadvantages with IMS is the current lack of a specific set of documentation for application developers...|$|R
25|$|<b>Use</b> <b>cases</b> {{are widely}} <b>used</b> system {{analysis}} modeling tools for identifying and expressing the functional requirements of <b>a</b> system. Each <b>use</b> <b>case</b> is <b>a</b> business scenario or event {{for which the}} system must provide <b>a</b> <b>defined</b> response. <b>Use</b> <b>cases</b> evolved from object-oriented analysis.|$|R
40|$|<b>Use</b> <b>cases</b> are {{promising}} vehicles for specifying requirements. However, obtaining well-organized <b>use</b> <b>case</b> models is difficult during software evolution. The thesis proposes {{to address the}} issue by refactoring <b>use</b> <b>case</b> models. Refactoring is a program transformation approach for iterative software development. Its concept is introduced to <b>use</b> <b>case</b> models in Cascaded Refactoring. The thesis introduces major research involved in refactoring <b>use</b> <b>case</b> models. It <b>defines</b> <b>a</b> <b>use</b> <b>case</b> metamodel to formalize <b>use</b> <b>cases.</b> The three-level metamodel covers the environment or context of <b>a</b> <b>use</b> <b>case</b> model, the structure of <b>use</b> <b>cases</b> in terms of episodes, and the event or message passing details of a scenario. The thesis presents a process algebra semantics for the <b>use</b> <b>case</b> model. The episode semantics is provided from the literature. The semantics of <b>a</b> single <b>use</b> <b>case</b> is <b>defined</b> in terms of the episode model. The semantics of the <b>use</b> <b>case</b> model is defined in terms of the individual <b>use</b> <b>cases</b> and their relationships. The thesis identifies a list of properties that need to be preserved during refactoring. It defines fifty-three <b>use</b> <b>case</b> refactorings, which are described <b>using</b> <b>a</b> template covering the refactoring description, arguments, preconditions, postconditions and verification of behavior preservation. The thesis also introduces <b>a</b> tool for <b>use</b> <b>case</b> modeling and refactoring. The tool helps validate the <b>use</b> <b>case</b> metamodel and refactorings on two case studies, which demonstrate that refactoring <b>use</b> <b>case</b> models is feasible and practical. Based on these case studies, the thesis discusses the nature of <b>use</b> <b>case</b> evolution and provides some guidelines for the refactoring process...|$|R
40|$|BACKGROUND: Generally {{benefits}} and risks of vaccines {{can be determined}} from studies carried out as part of regulatory compliance, followed by surveillance of routine data; however there are some rarer and more long term events that require new methods. Big data generated by increasingly affordable personalised computing, and from pervasive computing devices is rapidly growing and low cost, high volume, cloud computing makes the processing of these data inexpensive. OBJECTIVE: To describe how big data and related analytical methods might be applied to assess the {{benefits and}} risks of vaccines. METHOD: We reviewed {{the literature on the}} use of big data to improve health, applied to generic vaccine <b>use</b> <b>cases,</b> that illustrate benefits and risks of vaccination. We <b>defined</b> <b>a</b> <b>use</b> <b>case</b> as the interaction between a user and an information system to achieve <b>a</b> goal. We <b>used</b> flu vaccination and pre-school childhood immunisation as exemplars. RESULTS: We reviewed three big data <b>use</b> <b>cases</b> relevant to assessing vaccine benefits and risks: (i) Big data processing using crowdsourcing, distributed big data processing, and predictive analytics, (ii) Data integration from heterogeneous big data sources, e. g. the increasing range of devices in the "internet of things", and (iii) Real-time monitoring for the direct monitoring of epidemics as well as vaccine effects via social media and other data sources. CONCLUSIONS: Big data raises new ethical dilemmas, though its analysis methods can bring complementary real-time capabilities for monitoring epidemics and assessing vaccine benefit-risk balance...|$|R
40|$|Language Abstract: <b>Use</b> <b>cases</b> are <b>a</b> {{method for}} {{describing}} interactions between humans and/or systems. However, despite their popularity, {{there is no}} agreed formal State Machine Language (AsmL) is an executable specification language developed at Microsoft Research. In this paper we <b>define</b> <b>an</b> encoding of <b>use</b> <b>cases</b> in AsmL and demonstrate the advantages by describing techniques to generate test cases and test oracles from the encoding...|$|R
40|$|<b>Use</b> <b>cases</b> are <b>a</b> {{method for}} {{describing}} interactions between humans and/or systems. However, despite their popularity, {{there is no}} agreed formal syntax and semantics of <b>use</b> <b>cases.</b> The Abstract State Machine Language (ASML) is an executable specification language developed at Microsoft Research. In this paper we <b>define</b> <b>an</b> encoding of <b>use</b> <b>cases</b> in ASML and demonstrate the advantages by describing techniques to generate test cases and test oracles from the encoding. ...|$|R
40|$|The {{proliferation}} of smartphones/mobile devices that support {{a wide range}} of broadband applications and services has driven the volume of mobile data traffic to an unprecedented high level, requiring a next generation mobile communication system, i. e., the fifth generation (5 G). Millimeter wave bands, due to the large available spectrum bandwidth, are considered {{as one of the most}} promising approaches to significantly boost the capacity. In this paper, we <b>define</b> <b>a</b> typical <b>use</b> <b>case</b> envisaged in the early stage of the 5 G system rollout, where users can experience 100 M+ data rate in the target area and at the same time enjoy services demanding extremely high data rates, such as virtual reality and ultra-high definition video. We then break down the <b>use</b> <b>case</b> into four different traffic types: web browsing, content sharing, virtual reality experience and ultra-high definition video, and derive and analyze the distributions of the required instantaneous data rates for each individual traffic type. Finally, we consider the case where multiple users with mixture of traffic types are simultaneously active and analyze the overall data rate and bandwidth requirements to support such a scenario. Comment: 7 pages, 7 figures, to appear in IEEE ICC 201...|$|R
40|$|As {{the volume}} of data and human-centered {{information}} available to decision-makers continues to increase at an ever-accelerating rate, the need to represent information in software-processable formats becomes more apparent. At the same time, the availability of information from diverse sources through the World Wide Web provides the opportunity to widen the scope of input to decision-support systems, if this information can be made accessible through automated means. Past approaches to information-centric interoperability {{have been based on}} the <b>use</b> of <b>a</b> shared static object model, but this becomes impractical when we consider the loosely-coupled decentralized nature of the Web. This paper discusses the motivations driving a change from static to dynamic information models. It <b>defines</b> <b>a</b> representative <b>use</b> <b>case,</b> and describes <b>a</b> service-based architecture that allows for extending existing information sources to allow programmatic access. The proposed architecture uses existing and emerging Web Service specifications, enhanced by an ontology definition language, to create an environment that does not require information service providers to use static shared models, while allowing information consumers to learn ontologies from the services themselves. Clients such as decision-support systems can thus build their own information context at run-time based on models received from multiple sources...|$|R
40|$|AbstractThe {{abundance}} of computing technologies and devices imply {{that we will}} live in a data-driven society in the next years. But this data-driven society requires radically new technologies in the data center to deal with data manipulation, transformation, access control, sharing and placement, among others. We advocate in this paper {{for a new generation}} of Software Defined Data Management Infrastructures covering the entire life- cycle of data. On the one hand, this will require new extensible programming abstractions and services for data-management in the data center. On the other hand, this also implies opening up the control plane to data owners outside the data center to manage the data life cycle. We present in this article the open challenges existing in data-driven software <b>defined</b> infrastructures and <b>a</b> <b>use</b> <b>case</b> based on Software Defined Protection of data...|$|R
40|$|Recent {{developments}} in Building Information Model (BIM) capable software are leading to increased interoperability among heterogeneous tools. The results are representing {{greater levels of}} data available for all stakeholders involved in the building industry. The increasing range of data within BIMs enables the reuse of data for downstream applications such as Building Energy Performance Simulation (BEPS). Current BEPS tools work well in many modeling scenarios, but fail to support innovative and flexible model configurations due to existing tool limitations. Modelica is an object-oriented, equation-based programming language used for detailed dynamic simulation purposes across different industries. The use of Modelica in the building industry is increasing {{and it is a}} promising and flexible tool to provide modeling solutions addressing the upcoming challenges in the building industry and beyond. This paper illustrates <b>a</b> method of <b>using</b> BIM based information as the primary data source for a flexible simulation application. It includes an implementation for <b>a</b> <b>defined</b> generic <b>use</b> <b>case...</b>|$|R
40|$|Many Internet {{applications}} have a {{need for}} object-based security mechanisms in addition to security mechanisms at the network layer or transport layer. For many years, the Cryptographic Message Syntax (CMS) has provided a binary secure object format based on ASN. 1. Over time, binary object encodings such as ASN. 1 have become less common than text-based encodings, such as the JavaScript Object Notation (JSON). This document <b>defines</b> <b>a</b> set of <b>use</b> <b>cases</b> and requirements for a secure object format encoded using JSON, drawn {{from a variety of}} application security mechanisms currently in development. Status of This Memo This document is not an Internet Standards Track specification; it is published for informational purposes. This document {{is a product of the}} Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by the Internet Engineering Steering Group (IESG). Not all documents approved by the IESG are a candidate for any level of Internet Standard; see Section 2 of RFC 5741. Information about the current status of this document, any errata, and how to provide feedback on it may be obtained a...|$|R
40|$|The {{participation}} {{of small and}} medium enterprises in inter-firm collaboration can enhance their market reach while maintaining production lean. The conventional centralised collaboration approach {{is believed to be}} unsustainable, in today’s complex environment. The research aimed to investigate manufacturing network collaborations, where manufacturers maintain control over their scheduling activities and participate in a market-based event, to decide which collaborations are retained. The work investigated two pairing mechanisms where the intention was to capture and optimise collaboration at the granular level and then build up a network from those intermediate forms of organisation. The research also looked at two bidding protocols. The first protocol involves manufacturers that bid for operations from the process plan of a job. The second protocol is concerned with networks that bid for a job in its entirety. The problem, <b>defined</b> by <b>an</b> industrial <b>use</b> <b>case</b> and operation research data sets, was modelled as decentralised flow shop scheduling. The holonic paradigm identified the problem solving agents that participated in agent-based modelling and simulation of the pairing and the bidding protocols. The protocols are strongly believed to achieve true decentralisation of scheduling, with good performance on scalability, conflict resolution and schedule optimisation, for the purpose of inter-firm collaboration...|$|R
40|$|The {{bioinformatics}} {{field has}} encountered a data deluge {{over the last}} years, due to increasing speed and decreasing cost of DNA sequencing technology. Today, sequencing the DNA of a single genome only takes about a week, and it can result in up to a terabyte of data. The sequencing data are usually stored in files, and specialized tools {{have been designed to}} analyze and manage them. Despite of these tools, bioinformaticians are still exposed to many data management hurdles when analyzing these files, which often leads to excessively time consuming tasks. In this thesis, we accurately map the needs of bioinformaticians by <b>defining</b> <b>a</b> set of <b>use</b> <b>cases</b> that reflect the everyday analysis that is applied on genetic data. We propose a modern-DBMS based approach, to analyze and manage genetic data file repositories. We identify {{the pros and cons of}} this method compared to the traditional file-based approach. Additionally, we experimented with a novel in-situ approach, where the DBMS applies Just-In-Time ETL (Extract-Transform-Load) on the original files instead of loading all data from these files up front. A major advantage of this approach is that it greatly reduces the data-to-query time, since not all data are loaded in the DBMS during initialization. Other advantages include the decrease in storage requirements and the reduced data duplication. With this project, we have taken the first step towards the adaptation of the state-of-the-art database technology to accelerate genetic data analytics. The preliminary results presented in this thesis are highly promising and they open up a plethora of new research opportunities...|$|R
40|$|<b>Use</b> <b>case</b> models {{capture and}} {{describe}} the functional requirements of a software system. <b>A</b> <b>use</b> <b>case</b> driven development process, where <b>a</b> <b>use</b> <b>case</b> model is the principal basis for constructing an object-oriented design, is recommended when applying UML. There are, however, some problems with <b>use</b> <b>case</b> driven development processes and alternative ways of applying <b>a</b> <b>use</b> <b>case</b> model have been proposed. One alternative is to apply the <b>use</b> <b>case</b> model in <b>a</b> responsibility-driven process {{as a means to}} validate the design model. We wish to study how <b>a</b> <b>use</b> <b>case</b> model best can be applied in an object-oriented development process and have conducted a pilot experiment with 26 students as subjects to compare <b>a</b> <b>use</b> <b>case</b> driven process against...|$|R
50|$|<b>A</b> <b>use</b> <b>case</b> {{controller}} {{should be}} used to deal with all system events of <b>a</b> <b>use</b> <b>case,</b> and may be used for more than one <b>use</b> <b>case</b> (for instance, for <b>use</b> <b>cases</b> Create User and Delete User, one can have a single UserController, instead of two separate <b>use</b> <b>case</b> controllers).|$|R
50|$|This {{walkthrough}} {{will set}} out {{one example of}} how to go about <b>a</b> <b>use</b> <b>case</b> analysis. There are many variations of how to develop <b>a</b> <b>use</b> <b>case</b> analysis, and finding the right method can take time.|$|R
50|$|With content {{based upon}} an action or event flow structure, {{a model of}} well-written <b>use</b> <b>cases</b> also serves as an {{excellent}} groundwork and valuable guidelines {{for the design of}} test cases and user manuals of the system or product, which is an effort-worthy investment up-front. There is obvious connections between the flow paths of <b>a</b> <b>use</b> <b>case</b> and its test cases. Deriving functional test <b>cases</b> from <b>a</b> <b>use</b> <b>case</b> through its scenarios (running instances of <b>a</b> <b>use</b> <b>case)</b> is straightforward.|$|R
40|$|In this lecture {{we cover}} how UML <b>Use</b> <b>Cases</b> can be <b>used</b> for {{requirements}} capture. We {{look at the}} anatomy of <b>a</b> <b>Use</b> <b>Case</b> Description, {{and the way in}} which <b>use</b> <b>cases</b> can be brought together in <b>a</b> <b>use</b> <b>case</b> diagram. We also look at the way that <b>use</b> <b>cases</b> can be derived from problems using noun verb analysis...|$|R
50|$|<b>A</b> <b>use</b> <b>case</b> diagram at its {{simplest}} is {{a representation}} of a user's interaction with the system that shows {{the relationship between the}} user and the different <b>use</b> <b>cases</b> in which the user is involved. <b>A</b> <b>use</b> <b>case</b> diagram can identify the different types of users of a system and the different <b>use</b> <b>cases</b> and will often be accompanied by other types of diagrams as well.|$|R
30|$|ERQ 2 : Which of the {{evaluated}} template structures requires {{less time}} to understand <b>a</b> <b>use</b> <b>case?</b> Answer: The Tags structure, represented by the template of Eriksson et al., requires {{less time to}} understand <b>a</b> <b>use</b> <b>case.</b> The subjects <b>using</b> this template structure had better results {{in terms of time}} spent.|$|R
40|$|Traditionally, natural {{language}} {{is used for}} writing <b>use</b> <b>cases.</b> While this makes <b>use</b> <b>cases</b> easily readable to users, it neither permits reasoning on requirement specifications (written as <b>use</b> <b>cases),</b> nor employing the <b>use</b> <b>cases</b> in deriving <b>an</b> initial design in an automated way. While employing linguistic tools to analyze <b>use</b> <b>cases</b> has already been considered, such attempts usually attempted to utilize all the information possibly contained in <b>a</b> <b>use</b> <b>case</b> specification, thus facing the complexity of {{natural language}}. Yet, in <b>a</b> <b>use</b> <b>case,</b> the sentence describing <b>a</b> <b>use</b> <b>case</b> step adheres to a simple prescribed structure, and describes an action, which is either a communication action (among entities involved in the <b>use</b> <b>case),</b> or <b>an</b> internal action. In this paper, we describe how the principal attributes of the action described by <b>a</b> <b>use</b> <b>case</b> step can be acquired from the parse tree of the sentence specifying the step; we employ readily available linguistic tools for obtaining the parse tree. Having identified the communication actions permits us to construct an (estimate of) behavior specification of the entity modeled by <b>a</b> <b>use</b> <b>case</b> model (in {{the form of a}} Behavior Protocol [21]), as well as collect the list of operations accepted and requested by the entity; this may aid with defining the entity’s interfaces in an initial design...|$|R
40|$|The {{design of}} a {{software}} system or component starts with specifying its requirements; traditionally, <b>use</b> <b>cases</b> written in natural language are used for this task. While this makes <b>use</b> <b>cases</b> easily readable, it neither permits reasoning on requirement specifications (written as <b>use</b> <b>cases),</b> nor employing the <b>use</b> <b>cases</b> in deriving <b>an</b> initial design in an automated way. While employing linguistic tools to analyze <b>use</b> <b>cases</b> has already been considered, such efforts usually attempted to utilize all the information possibly contained in <b>a</b> <b>use</b> <b>case</b> specification, thus facing the complexity of a natural language. Yet, in <b>a</b> <b>use</b> <b>case,</b> the sentence describing <b>a</b> <b>use</b> <b>case</b> step adheres to a simple prescribed structure, and describes an action, which is either a communication action (among entities involved in the <b>use</b> <b>case),</b> or <b>an</b> internal action. In this paper, we describe how the principal attributes of the action described by <b>a</b> <b>use</b> <b>case</b> step can be acquired from the parse tree of the sentence specifying the step; we employ readily available linguistic tools for obtaining the parse tree. Having identified the communication actions permits us to construct an (estimate of) behavior specification of the entity (component) modeled by <b>a</b> <b>use</b> <b>case</b> model, {{as well as to}} collect the list of operations accepted and requested by the entity; this may aid with defining the interfaces of a component...|$|R
40|$|<b>Use</b> <b>case</b> models {{capture and}} {{describe}} the functional requirements of a software system. <b>A</b> <b>use</b> <b>case</b> driven development process, where <b>a</b> <b>use</b> <b>case</b> model is the principal basis for constructing an object-oriented design, is recommended when applying UML. There are, however, some problems with <b>use</b> <b>case</b> driven development processes and alternative ways of applying <b>a</b> <b>use</b> <b>case</b> model have been proposed. One alternative is to apply the <b>use</b> <b>case</b> model in <b>a</b> responsibility-driven process {{as a means to}} validate the design model. We wish to study how <b>a</b> <b>use</b> <b>case</b> model best can be applied in an object-oriented development process and have conducted a pilot experiment with 26 students as subjects to compare <b>a</b> <b>use</b> <b>case</b> driven process against a responsibility-driven process in which <b>a</b> <b>use</b> <b>case</b> model is applied to validate the design model. Each subject was given detailed guidelines on one of the two processes, and used those to construct design models consisting of class and sequence diagrams. The resulting class diagrams were evaluated with regards to realism, that is, how well they satisfied the requirements, size and number of errors. The results show that the validation process produced more realistic class diagrams, but with a larger variation in the number of classes. This indicates that the <b>use</b> <b>case</b> driven process gave more, but not always more appropriate, guidance on how to construct a class diagram The experiences from this pilot experiment were also used to improve the experimental design, and the design of a follow-up experiment is presented...|$|R
40|$|This article {{describes}} a framework, {{with the name}} RealTimeTalk (RTT), for development of applications, which have hard real-time constraints. The framework consists of: an object model that <b>defines</b> how <b>an</b> application should be configured, an execution and synchronisation model, an analysis and design model and development tools. The RTT object model supports the design of an application with hierarchical decomposition. An application consists {{of a set of}} operational modes. Each mode is <b>defined</b> by <b>a</b> finite number of <b>use</b> <b>cases.</b> <b>A</b> <b>use</b> <b>case</b> <b>defines</b> <b>a</b> particular periodic activity in the system, for example controlling one axis of a robot. The temporal requirements for <b>a</b> <b>use</b> <b>case</b> are described in a precedence graph where deadlines and release times can be specified for methods operating on an object. To support the development of an application, based on the analysis, design and RTT object model, six tools have been developed. These tools are: an Application Editor, a Br [...] ...|$|R
50|$|The {{lifecycle}} of {{an object}} load can be controlled on <b>a</b> <b>use</b> <b>case</b> basis.|$|R
5000|$|Automatic test generation: <b>A</b> <b>use</b> <b>case</b> driven {{approach}} IEEE Trans on Software Engineering, 2006 ...|$|R
5000|$|Every Role of <b>a</b> <b>use</b> <b>case</b> {{is played}} by an object {{determined}} by the Context {{at the start of}} the <b>use</b> <b>case</b> enactment; ...|$|R
5000|$|It was {{documented}} in the 1992 book Object-Oriented Software Engineering: <b>A</b> <b>Use</b> <b>Case</b> Driven Approach, ...|$|R
