3|36|Public
40|$|Video Scout is a {{prototype}} retrieval application that allows Personal Video Recorders to actually watch the TV programs they record. By analyzing the visual, audio, and transcript <b>data,</b> <b>Scout</b> can segment and index TV programs, finding and recording specific video clips that match requests in users ’ profiles. For example: if users request information on Philips, Scout will watch news programs and capture any stories it finds on Philips. The Scout interface offers a familiar TV environment where users can interact with whole TV programs and video clips organized by topic. Scout also provides users with tools for managing their profiles. This paper captures the Video Scout interface design process, from concept sketches to user testing to final prototype design. ...|$|E
40|$|Scientists in all {{disciplines}} increasingly rely on simulations {{to develop}} {{a better understanding of}} the subject they are studying. For example the neuroscientists we collaborate with in the Blue Brain project have started to simulate the brain on a supercomputer. The level of detail of their models is unprecedented as they model details on the subcellular level (e. g., the neurotransmitter). This level of detail, however, also leads to a true data deluge and the neuroscientists have only few tools to efficiently analyze the data. This demonstration showcases three innovative spatial management techniques that have substantial impact on computational neuroscience and other disciplines in that they allow to build, analyze and simulate bigger and more detailed models. More particularly, we demonstrate a tool that integrates three spatial data management techniques that have enabled breakthroughs in neuroscience: FLAT that enables efficient querying of spatial <b>data,</b> <b>SCOUT</b> that allows for fast exploration of spatial data and TOUCH that makes efficient data discovery possible...|$|E
40|$|Indium Hydroxy Sulphide has {{demonstrated}} abundance in resources, low prices, nontoxic characteristics, radiation resistance, high temperature resistance, and chemical stability, {{and therefore it}} has become an extremely important photoelectric, photovoltaic, and light sensing thin film material. Some treatment on this material include thermal annealing which is a process used for intrinsic stress liberation, structural improving, and surface roughness to control its electro-optical properties. In a qualitative way, annealing modifies surface morphology, intrinsic parameters, and electron mobility with temperature and time. In this work, an explanation on the surface modification of In(OH) xSy thin films when subjected to an annealing process is discussed. Both electrical and optical effects caused by annealing were carried out and characterizations were performed at different annealing temperatures in nitrogen in the temperature range 373 – 573 K. Using optical measurements data and simulated <b>data,</b> <b>Scout</b> software was employed and the results showed that increasing annealing temperature causes a slight decrease in transmittance with a consequence of modifying the energy band gaps values between 2. 79 – 3. 32 eV. It was concluded that annealing influence optical transmittance and resistance of the film make the thin films potential for photovoltaic, and light sensing applications...|$|E
40|$|In 2011, the CMS {{collaboration}} introduced <b>Data</b> <b>Scouting</b> {{as a way}} {{to produce}} physics results with events that cannot be stored on disk, due to resource limits in the data acquisition and offline infrastructure. The viability of this technique was demonstrated in 2012, when 18 fb^- 1 of collision data at √(s) = 8 TeV were collected. The technique is now a standard ingredient of CMS and ATLAS data-taking strategy. In this talk, we present the status of <b>data</b> <b>scouting</b> in CMS and the improvements introduced in 2015 and 2016, which promoted <b>data</b> <b>scouting</b> to a full-fledged, flexible discovery tool for the LHC Run II...|$|R
40|$|In {{the year}} 2011, the CMS {{collaboration}} introduced the novel concept of <b>data</b> <b>scouting,</b> allowing to take data {{that otherwise would}} be rejected by the usual trigger filters. This special data flow, based on event-size reduction, was created to maintain sensitivity to new light resonances decaying to jets or muons, with very small online and offline resources allocated to it. The challenges implied by this new workflow and the solutions developed within the CMS experiment are shown. This technique is now a standard ingredient for CMS data-taking strategy. The present status of <b>data</b> <b>scouting</b> in CMS is presented. Comment: 5 pages, Conference proceeding, The Fifth Annual Conference on Large Hadron Collider Physics (LHCP 2017), Shanghai Jiao Tong University, Shanghai, Chin...|$|R
40|$|The Linguistic Data Consortium (LDC) {{creates a}} variety of {{linguistic}} resources – data, annotations, tools, standards and best practices – for many sponsored projects. The programming staff at LDC has created the tools and technical infrastructures to support the data creation efforts for these projects, creating tools and technical infrastructures for all aspects of data creation projects: <b>data</b> <b>scouting,</b> <b>data</b> collection, data selection, annotation, search, data tracking and workflow management. This paper introduces a number of samples of LDC programming staff’s work, with particular focus on the recent additions and updates to the suite of software tools developed b...|$|R
40|$|The LHC {{provides}} {{experiments with}} an unprecedented amount of data. Experimental collaborations {{need to meet}} storage and computing requirements {{for the analysis of}} this data: this is often a limiting factor in the physics program that would be achievable if the whole dataset could be analysed. In this talk, I will describe the strategies adopted by the LHCb, CMS and ATLAS collaborations to overcome these limitations and make the most of LHC data: <b>data</b> parking, <b>data</b> <b>scouting,</b> and real-time analysis...|$|R
50|$|Grevy's Zebra scouts are locals who {{are hired}} and trained to help monitor the herds and report back <b>data.</b> The <b>scouts</b> also help share {{the message of}} {{conservation}} in their own communities.|$|R
40|$|A {{search for}} {{resonances}} decaying to dijet final states is performed in pp collisions at a center-of-mass energy √(s) = 8 TeV. Using a loose online data selection and a reduced data format collected {{with a novel}} technique called <b>data</b> <b>scouting,</b> the search extends to low-mass values, testing previously unprobed low couplings. No evidence for a signal is found. The result is interpreted as upper limits on the production cross section {{as a function of}} the resonance mass. These limits are translated into an upper limit on the resonance coupling, allowing a comparison with results obtained in the same mass region at lower collision energies...|$|R
40|$|AbstractData {{recorded}} at the CMS experiment are funnelled into streams, integrated in the HLT menu, and further organised in a hierarchical structure of primary datasets and secondary datasets/dedicated skims. Datasets are defined {{according to the}} final-state particles reconstructed by the high level trigger, the data format and the use case (physics analysis, alignment and calibration, performance studies). During the first LHC run, new workflows {{have been added to}} this canonical scheme, to exploit at best the flexibility of the CMS trigger and data acquisition systems. The concepts of data parking and <b>data</b> <b>scouting</b> have been introduced to extend the physics reach of CMS, offering the opportunity of defining physics triggers with extremely loose selections (e. g. dijet resonance trigger collecting data at a 1 kHz). In this presentation, we review the evolution of the dataset definition during the LHC run I, and we discuss the plans for the run II...|$|R
40|$|Due to {{copyright}} restrictions, {{the access}} to {{the full text of}} this article is only available via subscription. Narrow resonances decaying into dijet final states are searched with the data obtained from proton-proton collisions at a center-of-mass energy of 8 TeV, corresponding to an integrated luminosity of 18. 8 f b− 1. The data were collected with the CMS detector using a novel technique called <b>data</b> <b>scouting.</b> This novel technique allows collecting the data at a rate of 1  kHz in which the events only containing certain properties of jets. The measured dijet mass spectrum shows no evidence of a narrow resonances. Upper limits on the resonance cross sections are given {{as a function of the}} resonance mass, and also compared with a variety of models predicting narrow resonances. These limits are then translated into upper limits on the coupling of a leptophobic resonance Z′B to quarks, improving on the results obtained by previous experiments for the mass range from 500 to 800 GeV...|$|R
40|$|A {{search for}} narrow {{resonances}} decaying into dijet final states is performed {{on data from}} proton-proton collisions at a center-of-mass energy of 8 TeV, corresponding to an integrated luminosity of 18. 8 [*][*]fb − 1. The data were collected with the CMS detector using a novel technique called <b>data</b> <b>scouting,</b> in which the information associated with these selected events is much reduced, permitting collection of larger data samples. This technique enables CMS to record events containing jets {{at a rate of}} 1 kHz, by collecting the data from the high-level-trigger system. In this way, the sensitivity to low-mass resonances is increased significantly, allowing previously inaccessible couplings of new resonances to quarks and gluons to be probed. The resulting dijet mass distribution yields no evidence of narrow resonances. Upper limits are presented on the resonance cross sections as a function of mass, and compared with a variety of models predicting narrow resonances. The limits are translated into upper limits on the coupling of a leptophobic resonance Z′ B to quarks, improving on the results obtained by previous experiments for the mass range from 500 to 800 GeV...|$|R
40|$|Submitted to Phys. Rev. Lett, see {{paper for}} full list of authorsInternational audienceA search for narrow {{resonances}} decaying into dijet final states is performed {{on data from}} proton-proton collisions at a center-of-mass energy of 8 TeV, corresponding to an integrated luminosity of 18. 8 inverse femtobarns. The data were collected with the CMS detector using a novel technique called <b>data</b> <b>scouting,</b> in which the information associated with these selected events is much reduced, permitting collection of larger data samples. This technique enables CMS to record events containing jets {{at a rate of}} 1 kHz, by collecting the data from the high-level-trigger system. In this way, the sensitivity to low-mass resonances is increased significantly, allowing previously inaccessible couplings of new resonances to quarks and gluons to be probed. The resulting dijet mass distribution yields no evidence of narrow resonances. Upper limits are presented on the resonance cross sections as a function of mass, and compared with a variety of models predicting narrow resonances. The limits are translated into upper limits on the coupling of a leptophobic resonance Z'[B] to quarks, improving on the results obtained by previous experiments for the mass range from 500 to 800 GeV...|$|R
40|$|Information {{useful for}} new and {{experienced}} growers using biological control in greenhouses has been collected and organized into a printed workbook and mobile app.   The mobile app also has an interactive aspect so that <b>scouting</b> <b>data</b> can be collected, stored and graphically displayed on a smart device. Both resources are nearing completion...|$|R
40|$|Pakistan is the world’s fifth largest cotton producer. To monitor cotton growth, {{different}} {{government departments}} and agencies in Pakistan have been recording pest scouting, agriculture and metrological data for decades. Coarse estimates {{of just the}} cotton pest <b>scouting</b> <b>data</b> recorded stands at around 1. 5 million records, and growing. The primary agro-met data recorded has never been digitized, integrated or standardized to give a complete picture, and hence cannot support decision making. In this paper, a complete life-cycle implementation of a novel Pilot Agriculture Extension Data Warehouse is discussed, followed by data analysis by querying the Data Warehouse and some interesting findings through data mining using an indigenous technique based on the crossing minimization paradigm. Actual cotton pest <b>scouting</b> <b>data</b> of 1, 500 + farmers for years 2001 and 2002 for the Multan district was processed and used in the pilot project...|$|R
50|$|CFG {{signed a}} {{collaboration}} agreement with Atletico Venezuela of the Venezuelan Primera Division in April 2017. The scouting agreement enabled {{both parties to}} share <b>scouting</b> <b>data,</b> and also included the provision of coaching support to Atletico. Venezuelan midfielder Yangel Herrera has signed for Manchester City from Atletico Venezuela and been loaned to New York City FC.|$|R
40|$|Canopy {{reflectance}} calculations for {{a spring}} type Mexican wheat, Penjamo, are compared with published <b>data</b> on <b>Scout</b> winter wheat. Good agreement exists between model calculations and experimental {{data in the}} spectral range, 500 nm to 750 nm, suggesting that the model parameters for wheat {{can be applied to}} different cultivars of wheat in the same growth stage. Wheat canopy reflectance is dependent upon surface soil type and this dependency is examined with the Suits' spectral model. In this particular growth stage wheat reflectance is shown to be nearly independent of soil reflectance in the visible wavelengths and progressively dependent at longer wavelengths in the infrared...|$|R
40|$|Western bean cutworm (WBC) is a corn pest {{that has}} been {{expanding}} it’s range across Iowa and into more eastern states. Beginning in 2003, Iowa State University set up a network of pheromone traps to monitor WBC range expansion and to provide moth emergence <b>data</b> to enhance <b>scouting</b> efforts. Pheromone trap cooperators included local seed corn dealers, private corn and soybean agronomists and others interested in pest management issues...|$|R
40|$|A {{search for}} narrow {{resonances}} decaying into dijet final states is performed {{on data from}} proton-proton collisions at a center-of-mass energy of 8 TeV, corresponding to an integrated luminosity of 18. 8 [*][*]fb− 1. The data were collected with the CMS detector using a novel technique called <b>data</b> <b>scouting,</b> in which the information associated with these selected events is much reduced, permitting collection of larger data samples. This technique enables CMS to record events containing jets {{at a rate of}} 1 kHz, by collecting the data from the high-level-trigger system. In this way, the sensitivity to low-mass resonances is increased significantly, allowing previously inaccessible couplings of new resonances to quarks and gluons to be probed. The resulting dijet mass distribution yields no evidence of narrow resonances. Upper limits are presented on the resonance cross sections as a function of mass, and compared with a variety of models predicting narrow resonances. The limits are translated into upper limits on the coupling of a leptophobic resonance Z′B to quarks, improving on the results obtained by previous experiments for the mass range from 500 to 800 GeV. BMWF; FWF; FNRS; FWO; CNPq; CAPES; FAPERJ; FAPESP; MEYS; CERN; CAS; MoST; NSFC; COLCIENCIAS; MSES; RPF; MoER; SF 0690030 s 09; ERDF; Academy of Finland; MEC; HIP; CEA; CNRS/IN 2 P 3; BMBF; DFG; HGF; GSRT; OTKA; NKTH; DAE; DST; IPM; SFI; INFN; NRF; WCU; LAS; CINVESTAV; CONACYT; SEP; UASLP-FAI; MSI; PAEC; MSHE; NSC; FCT; JINR; MON; RosAtom; RAS; RFBR; MSTD; SEIDI; CPAN; Swiss Funding Agencies; NSC; ThEPCenter; IPST; NSTDA; TUBITAK; TAEK; NASU; STFC; University of California Institute for Mexico and the United States; the Marie-Curie programme and the European Research Council and EPLANET; DOE and NSF...|$|R
40|$|Talent {{identification}} {{is an integral}} component of the ever-professionalised sporting landscape. However, to date, there is a dearth of high quality evidence upon which to conduct talent identification practice. This thesis represents the preliminary stages in the validation of a talent identification model for cricket. The thesis contains six chapters (four empirical), which examine varying methodological approaches to talent identification, and present initial evidence (cross sectional and longitudinal) of those attributes that {{may be related to}} elite success. Chapter 2 presents two pilot studies examining the reliability and discriminant validity of batting and pace bowling assessments. Varying levels of validity and reliability are found. Some evidence suggests that skill-based differences (between high and low ability groups) become more pronounced after familiarisation / practice. Chapter 3 presents two studies examining the discriminant validity of scouting, and the most appropriate methodologies through which scouting can be conducted. The analysis documents significant discriminant value in <b>scouting</b> <b>data.</b> Skill-based <b>scouting</b> parameters consistently discriminate between low and high ability groups. Psychological scouting variables are the only data that discriminate between high and very high ability groups. Chapter 4 presents a longitudinal analysis of performance statistics. Findings suggest that performance statistics may be a valuable talent identification tool. Performance statistics that represent non-traditional metrics (e. g., ability to adapt on entering a new environment) consistently correlated with subsequent performance and are worthy of further examination. Chapter 5 presents evidence examining relative age effect and maturation across the male and female England Cricket Pathway. The findings suggest that RAEs may be advantageous from a development perspective, with relatively young athletes who remain in the system, becoming overrepresented at later stages...|$|R
500|$|In 1978, geophysicists Glen Penfield and Antonio Camargo {{were working}} for the Mexican {{state-owned}} oil company Petróleos Mexicanos, or Pemex, {{as part of an}} airborne magnetic survey of the Gulf of Mexico north of the Yucatán peninsula. Penfield's job was to use geophysical <b>data</b> to <b>scout</b> possible locations for oil drilling. In the data, Penfield found a huge underwater arc with [...] "extraordinary symmetry" [...] in a ring [...] across. He then obtained a gravity map of the Yucatán made in the 1960s. A decade earlier, the same map suggested an impact feature to contractor Robert Baltosser, but he was forbidden to publicize his conclusion by Pemex corporate policy of the time. Penfield found another arc on the peninsula itself, the ends of which pointed northward. Comparing the two maps, he found the separate arcs formed a circle, [...] wide, centered near the Yucatán village Chicxulub; he felt certain the shape had been created by a cataclysmic event in geologic history.|$|R
50|$|Recent {{studies by}} {{agriculture}} researchers in Pakistan (one {{of the top}} four cotton producers of the world) showed that attempts of cotton crop yield maximization through pro-pesticide state policies {{have led to a}} dangerously high pesticide use. These studies have reported a negative correlation between pesticide use and crop yield in Pakistan. Hence excessive use (or abuse) of pesticides is harming the farmers with adverse financial, environmental and social impacts. By data mining the cotton Pest <b>Scouting</b> <b>data</b> along with the meteorological recordings it was shown that how pesticide use can be optimized (reduced). Clustering of data revealed interesting patterns of farmer practices along with pesticide use dynamics and hence help identify the reasons for this pesticide abuse.|$|R
50|$|To monitor cotton growth, {{different}} {{government departments}} and agencies in Pakistan have been recording pest scouting, agriculture and metrological data for decades. Coarse estimates {{of just the}} cotton pest <b>scouting</b> <b>data</b> recorded stands at around 1.5 million records, and growing. The primary agro-met data recorded has never been digitized, integrated or standardized to give a complete picture, and hence cannot support decision making, thus requiring an Agriculture Data Warehouse. Creating a novel Pilot Agriculture Extension Data Warehouse followed by analysis through querying and data mining some interesting discoveries were made, such as pesticides sprayed at the wrong time, wrong pesticides used {{for the right reasons}} and temporal relationship between pesticide usage and day of the week.|$|R
40|$|We {{propose a}} {{framework}} for obtaining statistical inferences from multi-modal and multisensor data. In particular, we consider a military battlefield scene and address problems that arise in tactical decision-making while using {{a wide variety of}} sensors (an infrared camera, an acoustic sensor array, a human scout, and a seismic sensor array). Outputs of these sensors vary widely, from 2 D images and 1 D signals to categorical reports. We propose novel statistical models for representing seismic sensor <b>data</b> and human <b>scout</b> reports while using standard models for images and acoustic data. Combining the joint likelihood function with a marked Poisson prior, we formulate a Bayesian framework and use a Metropolis-Hastings algorithm to generate inferences. We demonstrate this framework using experiments involving simulated data...|$|R
40|$|This work {{presents}} a rapidly deployable system for automated precision weeding with minimal human labeling time. This overcomes a limiting factor in robotic precision weeding {{related to the}} use of vision-based classification systems trained for species that may not be relevant to specific farms. We present a novel approach to overcome this problem by employing unsupervised weed scouting, weed-group labeling, and finally, weed classification that is trained on the labeled <b>scouting</b> <b>data.</b> This work demonstrates a novel labeling approach designed to maximize labeling accuracy whilst needing to label as few images as possible. The labeling approach is able to provide the best classification results of any of the examined exemplar-based labeling approaches whilst needing to label over seven times fewer images than full data labeling...|$|R
40|$|Recent {{studies by}} {{agriculture}} researchers in Pakistan {{have shown that}} attempts of crop yield maximization through pro-pesticide state policies {{have led to a}} dangerously high pesticide usage. These studies have reported a negative correlation between pesticide usage and crop yield in Pakistan. Hence excessive use (or abuse) of pesticides is harming the farmers with adverse financial, environmental and social impacts. In this work we have shown that how data mining integrated agricultural <b>data</b> including pest <b>scouting,</b> pesticide usage and meteorological recordings is useful for optimization (and reduction) of pesticide usage. The data used in this work has never been utilized in this manner ever before. We have performed unsupervised clustering of this data through Recursive Noise Removal (RNR) heuristic of Abdullah and Brobst (2003). These clusters reveal interesting patterns of farmer practices along with pesticide usage dynamics and hence help identify the reasons for this pesticide abuse...|$|R
40|$|Agricultural {{producers}} have shown interest in collecting detailed, accurate, and meaningful field <b>data</b> through field <b>scouting,</b> but scouting is labor intensive. They use yield monitor attachments to collect weed and other field data while driving equipment. However, distractions from using a keyboard or buttons while driving {{can lead to}} driving errors or missed data points. At Purdue University, researchers have developed an ASR system to allow equipment operators to collect georeferenced data while keeping hands and eyes on the machine during harvesting and to ease georeferencing of <b>data</b> collected during <b>scouting.</b> ^ A notebook computer retrieved locations from a GPS unit and displayed and stored data in Excel. A headset microphone with a single earphone collected spoken input while allowing the operator to hear outside sounds. One-, two-, or three-word commands activated appropriate VBA macros. Four speech recognition products were chosen based on hardware requirements and ability to add new terms. After training, speech recognition accuracy was 100 % for Kurzweil VoicePlus and Verbex Listen for the 132 vocabulary words tested, during tests walking outdoors or driving an ATV. ^ Scouting tests were performed by carrying the system in a backpack while walking in soybean fields. The system recorded a point or a series of points with each utterance. Boundaries of points showed problem areas {{in the field and}} single points marked rocks and field corners. Data were displayed as an Excel chart to show a real-time map as data were collected. The information was later displayed in a GIS over remote sensed field images. Field corners and areas of poor stand matched, with voice data explaining anomalies in the image. ^ The system was tested during soybean harvest by using voice to locate weed patches. A harvester operator with little computer experience marked points by voice when the harvester entered and exited weed patches or areas with poor crop stand. The operator found the system easy to use while driving. ^ The system provides an inexpensive, portable, non-obtrusive method of collecting field data while walking or driving that can provide needed information for precision agriculture. ...|$|R
40|$|In {{clinical}} MRI examinations, {{the geometry}} of diagnostic scans is defined in an initial planning phase. The operator plans the scan volumes (off-centre, angulation, field-of-view) with respect to patient anatomy in ‘scout’ images. Often multiple plans are required within a single examination, distracting attention from the patient waiting in the scanner. A novel and robust method is described for automated planning of neurological MRI scans, capable of handling strong shape deviations from healthy anatomy. The expert knowledge required to position scan geometries is learned from previous example plans, allowing site-specific styles to be readily taken into account. The proposed method first fits an anatomical model to the <b>scout</b> <b>data,</b> and then new scan geometries are positioned with respect to extracted landmarks. The accuracy of landmark extraction {{was measured to be}} comparable to the inter-observer variability, and automated plans are shown to be highly consistent with those created by expert operators using clinical data. The results of the presented evaluation demonstrate the robustness and applicability of the proposed approach, which has the potential to significantly improve clinical workflow...|$|R
40|$|Pesticides {{are used}} for {{controlling}} pests, {{but at the same}} time they have impacts on the environment as well as the product itself. Although cotton covers 2. 5 % of the world's cultivated land yet uses 16 % of the world's insecticides, more than any other single major crop [1]. Pakistan is the world's fourth largest cotton producer and a major pesticide consumer. Numerous state run organizations have been monitoring the cotton crop for decades through pest-scouting, agriculture surveys and meteorological data-gatherings. This non-digitized, dirty and non-standardized data is of little use for strategic analysis and decision support. An advanced intelligent Agriculture Decision Support System (ADSS) is employed in an attempt to harness the semantic power of that data, by closely connecting visualization and data mining to each other in order to better realize the cognitive aspects of data mining. In this paper, we discuss the critical issue of handling data anomalies of pest <b>scouting</b> <b>data</b> for the six year period: 2001 - 2006. Using the ADSS it was found that the pesticides were not sprayed based on the pests crossing the critical population threshold, but were instead based on centuries old traditional agricultural significance of the weekday (Monday), thus resulting in non optimized pesticide usage, that can potentially reduce yield...|$|R
40|$|Today’s {{scientists}} are quickly moving from in vitro to in silico experimentation: {{they no longer}} analyze natural phenomena in a petri dish, but instead they build models and simulate them. Managing and analyzing the massive amounts of data involved in simulations is a major task. Yet, they lack the tools to efficiently work with data of this size. One problem many scientists share is {{the analysis of the}} massive spatial models they build. For several types of analysis they need to interactively follow the structures in the spatial model, e. g., the arterial tree, neuron fibers, etc., and issue range queries along the way. Each query takes long to execute, and the total time for executing a sequence of queries significantly delays data analysis. Prefetching the spatial data reduces the response time considerably, but known approaches do not prefetch with high accuracy. We develop SCOUT, a structure-aware method for prefetching data along interactive spatial query sequences. SCOUT uses an approximate graph model of the structures involved in past queries and attempts to identify what particular structure the user follows. Our experiments with neuroscience <b>data</b> show that <b>SCOUT</b> prefetches with an accuracy from 71 % to 92 %, which translates to a speedup of 4 x- 15 x. SCOUT also improves the prefetching accuracy on datasets from other scientific domains, such as medicine and biology. 1...|$|R
40|$|Background: Hydrophobic {{interaction}} chromatography (HIC) {{most commonly}} requires experimental determination (i. e., scouting) {{in order to}} select an optimal chromatographic medium for purifying a given target protein. Neither a two-step purification of untagged green fluorescent protein (GFP) from crude bacterial lysate using sequential HIC and size exclusion chromatography (SEC), nor HIC column scouting elution profiles of GFP, have been previously reported. Methods and Results: Bacterial lysate expressing recombinant GFP was sequentially adsorbed to commercially available HIC columns containing butyl, octyl, and phenyl-based HIC ligands coupled to matrices of varying bead size. The lysate was fractionated using a linear ammonium phosphate salt gradient at constant pH. Collected HIC eluate fractions containing retained GFP were then pooled and further purified using high-resolution preparative SEC. Significant differences in presumptive GFP elution profiles were observed using in-line absorption spectrophotometry (A 395) and post-run fluorimetry. SDS-PAGE and western blot demonstrated that fluorometric detection was the more accurate indicator of GFP elution in both HIC and SEC purification steps. Comparison of composite HIC column <b>scouting</b> <b>data</b> indicated that a phenyl ligand coupled to a 34 mm matrix produced the highest degree of target protein capture and separation. Conclusions: Conducting two-step protein purification using the preferred HIC medium followed by SEC resulted in a final, concentrated product with. 98 % protein purity. In-line absorbance spectrophotometry was not as precise of an indicato...|$|R
40|$|Hydrophobic {{interaction}} chromatography (HIC) {{most commonly}} requires experimental determination (i. e., scouting) {{in order to}} select an optimal chromatographic medium for purifying a given target protein. Neither a two-step purification of untagged green fluorescent protein (GFP) from crude bacterial lysate using sequential HIC and size exclusion chromatography (SEC), nor HIC column scouting elution profiles of GFP, have been previously reported. Bacterial lysate expressing recombinant GFP was sequentially adsorbed to commercially available HIC columns containing butyl, octyl, and phenyl-based HIC ligands coupled to matrices of varying bead size. The lysate was fractionated using a linear ammonium phosphate salt gradient at constant pH. Collected HIC eluate fractions containing retained GFP were then pooled and further purified using high-resolution preparative SEC. Significant differences in presumptive GFP elution profiles were observed using in-line absorption spectrophotometry (A 395) and post-run fluorimetry. SDS-PAGE and western blot demonstrated that fluorometric detection was the more accurate indicator of GFP elution in both HIC and SEC purification steps. Comparison of composite HIC column <b>scouting</b> <b>data</b> indicated that a phenyl ligand coupled to a 34 µm matrix produced the highest degree of target protein capture and separation. Conducting two-step protein purification using the preferred HIC medium followed by SEC resulted in a final, concentrated product with > 98 % protein purity. In-line absorbance spectrophotometry was not as precise of an indicator of GFP elution as post-run fluorimetry. These findings demonstrate the importance of utilizing a combination of detection methods when evaluating purification strategies. GFP is a well-characterized model protein, used heavily in educational settings and by researchers with limited protein purification experience, and the data and strategies presented here may aid in development other of HIC-compatible protein purification schemes...|$|R
40|$|Field {{studies were}} {{designed}} to more clearly determine how adult western corn rootworm, Diabrotica virgifera virgifera LeConte, population distribution patterns are altered over time by changes and contrasts in corn (Zea mays L.) plant phenology using whole-plant beetle counts as the sampling tool. In 1994, studies were conducted in a model system consisting of a late-planted corn strip placed {{in the middle of}} an early-planted cornfield. The system was replicated over three fields. Large-scale variation was modeled using trend-surface regression analysis to describe the relationship between beetle counts and distance from the center of the late-planted strip. In each field, the beetle distribution became greatly skewed toward the late-planted strip when the strip was either in the tassel or silk stage and the surrounding field was ≥ blister stage. In 1995, studies were conducted in and at the interface of two adjacent cornfields that were planted 9 d apart. Count data were analyzed to quantify population density changes at different sampling locations within and among fields over time as crop phenology changed. Rapid positive or negative changes in beetle densities occurred within and among fields as contrasts in corn phenology changed. In both years, the stability of the beetle distribution was strongly influenced by the length of time that a contrast in plant phenology was maintained between adjacent patches of corn. Directional movements of beetles toward pollinating corn and associated semiochemicals could be inferred from both 1994 and 1995 data analyses. Data suggest that contrasts in crop phenology at the interface and among cornfields should be considered when developing beetle sampling programs and interpreting <b>scouting</b> <b>data</b> to improve the accuracy of rootworm management decisions...|$|R
40|$|The Lott Ranch 3 D seismic {{prospect}} {{located in}} Garza County, Texas {{is a project}} initiated in September of 1991 by the J. M. Huber Corp., a petroleum exploration and production company. By today's standards the 126 square mile project does not seem monumental, however {{at the time it}} was conceived it was the most intensive land 3 D project ever attempted. Acquisition began in September of 1991 utilizing GEO-SEISMIC, INC., a seismic data contractor. The field parameters were selected by J. M. Huber, and were of a radical design. The recording instruments used were GeoCor IV amplifiers designed by Geosystems Inc., which record the data in signed bit format. It would not have been practical, if not impossible, to have processed the entire raw volume with the tools available at that time. The end result was a dataset that was thought to have little utility due to difficulties in processing the field data. In 1997, Yates Energy Corp. located in Roswell, New Mexico, formed a partnership to further develop the project. Through discussions and meetings with Pinnacle Seismic, it was determined that the original Lott Ranch 3 D volume could be vastly improved upon reprocessing. Pinnacle Seismic had shown the viability of improving field-summed signed bit data on smaller 2 D and 3 D projects. Yates contracted Pinnacle Seismic Ltd. to perform the reprocessing. This project was initiated with high resolution being a priority. Much of the potential resolution was lost through the initial summing of the field data. Modern computers that are now being utilized have tremendous speed and storage capacities that were cost prohibitive when this data was initially processed. Software updates and capabilities offer a variety of quality control and statics resolution, which are pertinent to the Lott Ranch project. The reprocessing effort was very successful. The resulting processed data-set was then interpreted using modern PC-based interpretation and mapping software. Production data, log <b>data,</b> and <b>scout</b> ticket <b>data</b> were integrated with the 3 D interpretations to evaluate drilling opportunities resulting in an initial three well drilling program. Thousands of miles of signed bit data exist. Much of this data was processed during a time when software and hardware capabilities were either incapable or cost prohibitive to glean the full potential of the data. In fact in some circles signed bit gained an undeserved reputation for being less than optimum. As a consequence much of the older signed bit data sits on the shelf long forgotten or overlooked. With the high cost of new acquisition and permitting it might behoove other exploration companies to reconsider resurrecting perfectly viable existing volumes and have them reprocessed at a fraction of the cost of new acquisition...|$|R
40|$|A {{reliable}} prognosis {{of climate}} change in the earth’s atmosphere {{as well as the}} distinction of the relevant natural and anthropogenic influences is a prevailing scientific question and necessitates a detailed understanding of energetic, dynamic and chemical processes in the atmosphere, particularly in the tropopause region. The upper troposphere and lower stratosphere seems to proof highly sensitive to changes in atmospheric parameters such as temperature and trace gas distributions. Dynamic transport and exchange of chemical constituents between troposphere and stratosphere may cause significant changes in the atmosphere’s chemical and radiative budget. In spite of their great importance the involved processes are far from beeing well understood. To assess the major uncertainties concerning the distribution, chemistry and radiative forcing of water vapor, aerosols, ozone, the hydroxyl radical or the formation mechanisms of the various types of cirrus clouds, processes ranging from synoptical scale to microscale have to be investigated requiring different measurement techniques.      While hitherto global satellite measurements are limited either by their horizontal or vertical resolution, local in-situ instruments are limited by their little spatial coverage. Therefore since 2003 the Research Center Jülich and the University of Wuppertal have cooperated {{in the development of a}} new limb sounding infrared instrument aboard the high altitude research aircraft M 55 -Geophysika aiming at measurements in the tropopause region, combining high vertical resolution with reasonable regional coverage. The first mission of the new CRISTA-NF (CRyogenic Infrared Spectrometers and Telescope for the Atmosphere - New Frontiers) instrument took place during the extensive European SCOUT-O 3 (Stratosphere Climate Ozone links with emphasis on the UTLS) measurement campaign in Darwin, Australia, in November/December 2005.      Here, an overview of the instrument’s measurement technique is given, followed by descriptions of the instrument calibration, the process of data analysis and retrieval. Using these methods a set of preliminary <b>data</b> from the <b>SCOUT</b> campaign is analyzed concerning the distribution of clouds as well as the mixing ratios of water vapor and the chlorofluorocarbon CFCl 3. Simulations by the Chemical Lagrangian Model of the Stratosphere (CLaMS) are used for comparisons with the observed situation concerning processes of transport and mixing...|$|R
40|$|The {{objective}} of this diagnostic study was to evaluate the number of impacts per training session undergone by two amateur volleyball teams while performing spikes and blocks, and to relate the number of impacts {{to the number of}} injuries they suffered over the previous two years. The study recruited 24 athletes from the Universidade Federal de Santa Catarina first teams, 12 from the men’s and 12 from the women’s volleyball teams, sampled intentionally. Data were collected using a questionnaire, a video camera and assessment sheets fi lled out by a talent <b>scout.</b> <b>Data</b> were collected at the practice ground and presented in the form of descriptive statistics in means, standard deviations and simple frequencies and inferential statistics were applied in the form of Pearson’s correlation and Student’s t test, both to p≤ 0. 05. The results allowed for the conclusions that both teams exhibited similar duration of practice and that their training characteristics were similar in terms of duration and frequency. The ankles were most often affected by injuries and blocks were the most common causative mechanisms of injuries in both teams. The majority of athletes use protective equipment and the great majority underwent physiotherapy once injured. The number of impacts per training session, including spike and block jumps, was low when compared with high level teams. It appears that the number of impacts did not affect the number of injuries in either team. The two teams did not differ either {{in terms of the number}} of injuries or the number of impacts per training session. RESUMO Este estudo diagnóstico teve como objetivo avaliar o número de repetições de impacto por treino, em atletas de duas equipes amadoras de voleibol, realizando cortadas e bloqueios, bem como relacionar o número de impactos com o número de lesões sofridas pelos mesmos nos últimos dois anos. Participaram do estudo 24 atletas titulares, sendo 12 da equipe feminina e 12 da equipe masculina de voleibol da Universidade Federal de Santa Catarina, escolhidas de forma intencional. Como instrumento de medida, foi utilizado um questionário, uma filmadora e fi chas de avaliação de escalte técnico. Os dados foram coletados no local de prática e tratados mediante a estatística descritiva em termos de média, desvio padrão e freqüência simples e com a estatística inferencial, por meio da correlação de Pearson e teste “t” de Student, ambos a p≤ 0, 05. Os resultados obtidos permitem concluir que as equipes apresentam tempo de prática e características de treino similares em termos de duração e freqüência; o tornozelo foi o local mais afetado por lesões e os bloqueios foram os mecanismos mais causadores de lesões para ambas as equipes; a maioria dos atletas usa equipamentos de proteção e a grande maioria, após lesionada, fez fi sioterapia; o número de impactos por treino, incluindo saltos de cortadas e bloqueios, é pouco quando comparado com equipes de alto nível; parece que o número de impactos não interferiu no número de lesões nas duas equipes; as duas equipes não diferem tanto no número de lesões quanto no número de impactos por treino...|$|R
