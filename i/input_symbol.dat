242|448|Public
25|$|Put together: Given an <b>input</b> <b>symbol,</b> current state, and stack symbol, the {{automaton}} {{can follow}} {{a transition to}} another state, and optionally manipulate (push or pop) the stack.|$|E
25|$|An NFA, {{similar to}} a DFA, consumes a string of input symbols. For each <b>input</b> <b>symbol,</b> it {{transitions}} to a new state until all input symbols have been consumed.|$|E
25|$|Keep a set data {{structure}} of all states which the NFA might currently be in. On {{the consumption of}} an <b>input</b> <b>symbol,</b> unite {{the results of the}} transition function applied to all current states to get the set of next states; if ε-moves are allowed, include all states reachable by such a move (ε-closure). Each step requires at most s2 computations, where s is the number of states of the NFA. On the consumption of the last <b>input</b> <b>symbol,</b> if one of the current states is a final state, the machine accepts the string. A string of length n can be processed in time O(ns2), and space O(s).|$|E
5000|$|<b>Input</b> <b>symbols</b> Σ: {{a finite}} {{collection}} of <b>input</b> <b>symbols</b> or designators ...|$|R
3000|$|... is the {{variance}} of the zero mean (symmetrical constellation) and uncorrelated <b>input</b> <b>symbols.</b> The assumption of the uncorrelated <b>input</b> <b>symbols</b> can be justified because of coding and interleaving in practical symbols [21].|$|R
40|$|In {{this paper}} {{we show that}} for any {{positive}} integers μ, p and q there exist (1) a minimal nondegenerate (pq) μ-state μ-finite memory machine with p <b>input</b> <b>symbols</b> and q output symbols, (2) a minimal strongly connected pμ-state μ-input memory machine with p <b>input</b> <b>symbols</b> and q output symbols, and (3) a minimal strongly connected pμμ-state μ-output memory machine with p <b>input</b> <b>symbols</b> and q output symbols...|$|R
25|$|When {{the last}} <b>input</b> <b>symbol</b> is consumed, the NFA accepts if {{and only if}} there is some set of {{transitions}} that will take it to an accepting state. Equivalently, it rejects, if, no matter what transitions are applied, it would not end in an accepting state.|$|E
25|$|Unlike a DFA, it is non-deterministic, i.e., {{for some}} state and <b>input</b> <b>symbol,</b> the next state may be nothing or {{one or two}} or more {{possible}} states. Thus, in the formal definition, the next state {{is an element of}} the power set of the states, which is a set of states to be considered at once.|$|E
25|$|Nondeterministic finite {{automaton}} with ε-moves (NFA-ε) {{is a further}} generalization to NFA. This automaton replaces the transition function with the one that allows the empty string ε as a possible input. The transitions without consuming an <b>input</b> <b>symbol</b> are called ε-transitions. In the state diagrams, they are usually labeled with the Greek letter ε. ε-transitions provide a convenient way of modeling the systems whose current states are not precisely known.|$|E
30|$|Entropy {{encoding}} exploits a certain, non-uniform {{statistical distribution}} {{of a set}} of <b>input</b> <b>symbols</b> in a way that individual, frequent <b>input</b> <b>symbols</b> are mapped to codes allocating lesser bits and less frequent symbols are mapped to codes allocating more bits than the original <b>input</b> <b>symbols,</b> resulting in an overall data-size reduction compared with the set of input data. Depending on the chosen entropy coding scheme, there is not necessarily a one-to-one mapping of <b>input</b> <b>symbols</b> to codes. Arithmetic entropy coding schemes can map a number of <b>input</b> <b>symbols</b> to a single code-bit or to fractions of code-bits resulting in improved overall compression efficiency. JPEG, JPEG-lossless and JPEG-LS use a Huffman entropy coder which applies a one-to-one mapping of uniquely sized <b>input</b> <b>symbols</b> to output symbols of different sizes. JPEG 2000 is using a context adaptive binary arithmetic coder (CABAC), which encodes binary symbols using statistical distributions corresponding to the type of binary symbol to be encoded. A binary symbol representing, e.g. the sign of a numeric value, is encoded using the statistical distribution of all already encoded sign symbols, which is different to the statistical distribution used to encode numeric value bits. The encoder switches its contexts {{depending on the type of}} binary symbol to be encoded.|$|R
40|$|We {{implement}} a {{finite state machine}} by representing state, transition rules and <b>input</b> <b>symbols</b> with DNA components. Transitions between states are triggered by a clock signal which allows synchronized, parallel operation of two (or more) state machines. The state machine can be re-programmed by changing the <b>input</b> <b>symbols...</b>|$|R
30|$|Besides, {{due to the}} {{sensitiveness}} on <b>input</b> <b>symbols,</b> in low SNR region (e.g., < 0.2 dB) the decoding {{performance of}} PC scheme will be inferior to the other LDPC scheme. That is, the error propagation in successive cancelation process will be very serious, when the <b>input</b> <b>symbols</b> are seriously contaminated.|$|R
25|$|Interleaving is used {{to convert}} {{convolutional}} codes from random error correctors to burst error correctors. The basic idea behind the use of interleaved codes is to jumble symbols at the receiver. This leads to randomization of bursts of received errors which are closely located and we can then apply the analysis for random channel. Thus, the main function performed by the interleaver at transmitter is to alter the <b>input</b> <b>symbol</b> sequence. At the receiver, the deinterleaver will alter the received sequence to get back the original unaltered sequence at the transmitter.|$|E
2500|$|... {{reading an}} <b>input</b> <b>symbol</b> is {{required}} for each state transition.|$|E
2500|$|... {{each of its}} {{transitions}} {{is uniquely}} determined by its source state and <b>input</b> <b>symbol,</b> and ...|$|E
5000|$|... {{the set of}} {{terminal}} (<b>input)</b> <b>symbols</b> with {{a special}} end-of-input (EOI) symbol [...]|$|R
30|$|MTx {{larger than}} 1, e.g., MTx= 2 or 3, {{corresponds}} to faster-than-Nyquist signaling following the principle in [8, 9]. In this regard, a compression of channel <b>input</b> <b>symbols</b> in time is given, such that MTx channel <b>input</b> <b>symbols</b> are emitted {{in the unit}} time interval Ts. The compression of <b>input</b> <b>symbols</b> in time provides additional degrees of freedom which can be exploited for the waveform design. In order to avoid extensively complex trellis-based receivers, a transmit filter h(t) with short impulse response is favorable. In this context, different standard pulses (Gaussian pulse, cosine pulse, and rect pulse) will be examined {{in terms of the}} spectral efficiency for the considered channel.|$|R
40|$|Abstract—Rateless {{codes are}} {{designed}} to decode all the <b>input</b> <b>symbols</b> when {{a certain number of}} coded symbols have been received. However, it is possible to recover a subset of the <b>input</b> <b>symbols</b> from the actually received coded symbols: this process is called partial decoding and the number of recovered <b>input</b> <b>symbols</b> is termed the intermediate performance of rateless codes. In this paper we study the problem of the optimality of the partial decoding process: we say that a partial decoding algorithm is optimal if, given a rateless code, it is able to maximize the intermediate performance of the code, i. e. it is able to retreive the maximum number of <b>input</b> <b>symbols</b> when a certain number n of coded symbols have been received, for every n. We propose OPD, an optimal partial decoding algorithm for any rateless code, proving its optimality. The proposed algorithm is finally used to analyze the intermediate performance of LT codes. Index Terms—LT codes, Partial decoding, incremental decod-ing, rateless code...|$|R
2500|$|Machine {{transitions}} {{are based}} on the current state and <b>input</b> <b>symbol,</b> and also the current topmost symbol of the stack. [...] Symbols lower in the stack are not visible and have no immediate effect. Machine actions include pushing, popping, or replacing the stack top. [...] A deterministic pushdown automaton has at most one legal transition for the same combination of <b>input</b> <b>symbol,</b> state, and top stack symbol. This is where it differs from the nondeterministic pushdown automaton.|$|E
2500|$|Create {{multiple}} copies. For each n way decision, the NFA creates up to [...] {{copies of}} the machine. Each will enter a separate state. If, upon consuming the last <b>input</b> <b>symbol,</b> at least one copy of the NFA is in the accepting state, the NFA will accept. [...] (This, too, requires linear storage {{with respect to the}} number of NFA states, as there can be one machine for every NFA state.) ...|$|E
2500|$|A pushdown {{automaton}} reads a given input string {{from left}} to right. In each step, it chooses a transition by indexing a table by <b>input</b> <b>symbol,</b> current state, and the symbol {{at the top of}} the stack. [...] A pushdown automaton can also manipulate the stack, as part of performing a transition. The manipulation can be to push a particular symbol to the top of the stack, or to pop off the top of the stack. The automaton can alternatively ignore the stack, and leave it as it is.|$|E
3000|$|... repeat above three {{process until}} the {{destination}} receives enough check symbol to recover all <b>input</b> <b>symbols.</b>|$|R
5000|$|... and [...] are finite sets (whose {{elements}} are called states, stack <b>symbols,</b> and <b>input</b> <b>symbols,</b> respectively), ...|$|R
30|$|Choose d <b>input</b> <b>symbols</b> {{randomly}} and uniformly in k source symbols as {{neighbors of}} the encoding symbol.|$|R
5000|$|... {{reading an}} <b>input</b> <b>symbol</b> is {{required}} for each state transition.|$|E
50|$|There is a simple, {{intuitive}} way {{of understanding}} quantum finite automata. One begins with a graph-theoretic interpretation of deterministic finite automata (DFA). A DFA can be represented as a directed graph, with states as nodes in the graph, and arrows representing state transitions. Each arrow is labelled with a possible <b>input</b> <b>symbol,</b> so that, given a specific state and an <b>input</b> <b>symbol,</b> the arrow points at the next state. One way of representing such a graph is {{by means of a}} set of adjacency matrices, with one matrix for each <b>input</b> <b>symbol.</b> In this case, the list of possible DFA states is written as a column vector. For a given <b>input</b> <b>symbol,</b> the adjacency matrix indicates how any given state (row in the state vector) will transition to the next state; a state transition is given by matrix multiplication.|$|E
50|$|In automata theory, a {{transition}} that involves no shifting of an <b>input</b> <b>symbol.</b>|$|E
40|$|In this work, we {{consider}} a discrete-time stationary Rayleigh flat-fading channel with unknown channel state information at transmitter and receiver. The {{law of the}} channel is presumed to be known to the receiver. In addition, we assume the power spectral density (PSD) of the fading process to be compactly supported. For i. i. d. zero-mean proper Gaussian input distributions, we investigate the achievable rate. One of the main contributions is the derivation of two new upper bounds on the achievable rate with zero-mean proper Gaussian <b>input</b> <b>symbols.</b> The first one holds only for the special case of a rectangular PSD and depends on the SNR {{and the spread of}} the PSD. Together with a lower bound on the achievable rate, which is achievable with i. i. d. zero-mean proper Gaussian <b>input</b> <b>symbols,</b> we have found a set of bounds which is tight in the sense that their difference is bounded. Furthermore, we show that the high SNR slope is characterized by a pre-log of 1 - 2 f_d, where f_d is the normalized maximum Doppler frequency. This pre-log is equal to the high SNR pre-log of the peak power constrained capacity. Furthermore, we derive an alternative upper bound on the achievable rate with i. i. d. <b>input</b> <b>symbols</b> which is based on the one-step channel prediction error variance. The novelty {{lies in the fact that}} this bound is not restricted to peak power constrained <b>input</b> <b>symbols</b> like known bounds, e. g. in [1]. Therefore, the derived upper bound can also be used to evaluate the achievable rate with i. i. d. proper Gaussian <b>input</b> <b>symbols.</b> We compare the derived bounds on the achievable rate with i. i. d. zero-mean proper Gaussian <b>input</b> <b>symbols</b> with bounds on the peak power constrained capacity given in [1 - 3]. Finally, we compare the achievable rate with i. i. d. zero-mean proper Gaussian <b>input</b> <b>symbols</b> with the achievable rate using synchronized detection in combination with a solely pilot based channel estimation. Comment: submitted to the Transactions on Information Theor...|$|R
30|$|In a SC transmission, the PAPR of the {{transmitted}} {{sequence is}} defined after the Tx pulse-shape filter. The PP {{we see in}} the filter output depends on the maximum amplitude of the <b>input</b> <b>symbols</b> and on a portion of the absolute values of the filter coefficients, depending on the oversampling. Because we have fixed the Tx pulse-shape filter, only the maximum amplitudes of the <b>input</b> <b>symbols</b> effect the observed PAPR.|$|R
3000|$|... [...] "-blocks {{indicate}} the conversion into real-valued {{data from the}} real and imaginary parts of the complex-valued <b>input</b> <b>symbols</b> [...]...|$|R
5000|$|... {{consuming}} transitions are transitions of {{the form}} [...] and consume an <b>input</b> <b>symbol</b> , and ...|$|E
5000|$|... {{each of its}} {{transitions}} {{is uniquely}} determined by its source state and <b>input</b> <b>symbol,</b> and ...|$|E
5000|$|SWAP B →a C: consumes the <b>input</b> <b>symbol</b> a, {{and changes}} {{the state of}} the active thread: ...|$|E
50|$|However {{a higher}} degree of {{diffusion}} is achieved because each output symbol depends on 3 <b>input</b> <b>symbols</b> instead of two.|$|R
5000|$|... is {{the set of}} <b>input</b> <b>symbols,</b> that is, the set {{of symbols}} allowed {{to appear in the}} initial tape {{contents}} ...|$|R
30|$|Clearly, the {{traditional}} rateless codes select the <b>input</b> <b>symbols</b> {{at the same}} probability and have equal error protection for all information.|$|R
