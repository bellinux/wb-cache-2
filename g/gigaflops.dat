118|23|Public
25|$|On August 29, 2007, IBM {{announced}} the BladeCenter QS21. Generating a measured 1.05 giga–floating point {{operations per second}} (<b>gigaFLOPS)</b> per watt, with peak performance of approximately 460GFLOPS {{it is one of}} the most power efficient computing platforms to date. A single BladeCenter chassis can achieve 6.4 tera–floating point operations per second (teraFLOPS) and over 25.8 teraFLOPS in a standard 42U rack.|$|E
500|$|Though the Dreamcast launch {{had been}} successful, Sony still held 60 {{percent of the}} overall video game market share in North America with the PlayStation {{at the end of}} 1999. On March 2, 1999, in what one report called a [...] "highly publicized, vaporware-like announcement" [...] Sony {{revealed}} the first details of its [...] "next generation PlayStation", which Ken Kutaragi claimed would allow video games to convey unprecedented emotions. The center of Sony's marketing plan and the upcoming PlayStation 2 itself was a new CPU (clocked at 294MHz) jointly developed by Sony and Toshiba—the [...] "Emotion Engine"—which Kutaragi announced would feature a graphics processor with 1,000 times more bandwidth than contemporary PC graphics processors and a floating-point calculation performance of 6.2 <b>gigaflops,</b> rivaling most supercomputers. Sony, which invested $1.2 billion in two large-scale integration semiconductor fabrication plants to manufacture the PlayStation 2's [...] "Emotion Engine" [...] and [...] "Graphics Synthesizer", designed the machine to push more raw polygons than any video game console in history. Sony claimed the PlayStation 2 could render 75 million raw polygons per second with absolutely no effects, and 38 million without accounting for features such as textures, artificial intelligence, or physics. With such effects, Sony estimated the PlayStation 2 could render 7.5 million to 16 million polygons per second, whereas independent estimates ranged from 3 million to 20 million, compared to Sega's estimates of more than 3 million to 6 million for the Dreamcast. The system would also utilize the DVD-ROM format, which could hold substantially more data than the Dreamcast's GD-ROM format. Because it could connect to the Internet while playing movies, music, and video games, Sony hyped PlayStation 2 as the future of home entertainment. Rumors spread that the PlayStation 2 was a supercomputer capable of guiding missiles and displaying Toy Story-quality graphics, while Kutaragi boasted its online capabilities would give consumers the ability to [...] "jack into ‘The Matrix’!" [...] In addition, Sony emphasized that the PlayStation 2 would be backwards compatible with hundreds of popular PlayStation games. Sony's specifications appeared to render the Dreamcast obsolete months before its U.S. launch, although reports later emerged that the PlayStation 2 was not as powerful as expected and distinctly difficult to program games for. The same year, Nintendo announced that its next generation console would meet or exceed anything on the market, and Microsoft began development of its own console.|$|E
5000|$|... {{the fastest}} six-core PC {{processor}} reaches 109 <b>gigaFLOPS</b> (Intel Core i7 980 XE) in double precision calculations. GPUs are considerably more powerful. For example, Nvidia Tesla C2050 GPU computing processors perform around 515 <b>gigaFLOPS</b> in double precision calculations, and the AMD FireStream 9270 peaks at 240 <b>gigaFLOPS.</b> In single precision performance, Nvidia Tesla C2050 computing processors perform around 1.03 teraFLOPS and the AMD FireStream 9270 cards peak at 1.2 teraFLOPS. Both Nvidia and AMD's consumer gaming GPUs may reach higher FLOPS. For example, AMD's HemlockXT 5970 reaches 928 <b>gigaFLOPS</b> in double precision calculations with two GPUs {{on board and}} the Nvidia GTX 480 reaches 672 <b>gigaFLOPS</b> with one GPU on board.|$|E
5000|$|A {{so-called}} GigaCube was {{a module}} {{that was already}} a one <b>gigaflop</b> system; furthermore, it was the building block for greater systems.A module (i.e. cube in Parsytec's lingo) contained ...|$|R
40|$|Introduction Computation {{has become}} a major {{research}} tool for most scientific disciplines. 1 The role of high-performance computing in the physical sciences and engineering disciplines is well established. The quantitative definition of highperformance computing is somewhat arbitrary, and varies from one discipline to the next. In astrophysics, theoretical chemistry, or computational fluid dynamics, high-performance computing often implies <b>gigaflop</b> (1 <b>gigaflop</b> = 1 billion floating point operations per second) calculation speeds. In other disciplines, computation speeds available from current RISC workstations qualify fully as high-performance computing. In some cases, fundamental properties of matter can be calculated as accurately as they can be measured with stateof -the-art instrumentation. As early as 1968, Kolos and Wolniewicz performed extremely accurate quantum mechanical calculations for the dissociation energy of molecular hydrogen. Their result, in di...|$|R
50|$|He develops, {{tests and}} {{documents}} parallel analysis software to speed matrix equation solution to simulate physical & biological behavior on advanced-computer architectures (e.g. NASA's GPS solver based on prior Finite element machine and rapid parallel analysis of Space Shuttle SRB redesign earned Cray's 1st <b>GigaFLOP</b> Performance Award at Supercomputing '89).|$|R
50|$|NEC's SX-9 {{supercomputer}} was the world's first {{vector processor}} to exceed 100 <b>gigaFLOPS</b> per single core.|$|E
5000|$|Floating Point Operations: 230 <b>Gigaflops</b> (500MHz * 24 shaders * 2 ops per clock per ALU) ...|$|E
50|$|The first Lonestar {{system was}} built by Dell and {{integrated}} by Cray, using Dell PowerEdge 1750 servers and Myrinet interconnects, with a peak performance of 3672 <b>gigaFlops.</b> An upgrade in 2004 {{increased the number of}} processors to 1024 and the peak rate of 6338 <b>gigaflops.</b> The second iteration (Lonestar 2) in 2006 was deployed with Dell PowerEdge 1855 servers and Infiniband. (1300 processors, 2000 gigabytes memory, peak performance 8320 <b>gigaflops.)</b> Later that year, the cluster's third iteration was built from Dell PowerEdge 1955 servers; it was composed of 5200 processors and 10.4 TB memory. Lonestar 3 entered the Top500 list in November 2006 as the 12th fastest supercomputer, with 55.5 TFlops peak.|$|E
40|$|The {{methodology}} {{is described}} for converting a large, long-running applications code that executed {{on a single}} processor of a CRAY- 2 supercomputer to a version that executed efficiently on multiple processors. Although the conversion of every application is different, {{a discussion of the}} types of modification used to achieve <b>gigaflop</b> performance is included to assist others in the parallelization of applications for CRAY computers, especially those that were developed for other computers. An existing application, from the discipline of computational fluid dynamics, that had utilized over 2000 hrs of CPU time on CRAY- 2 during the previous year was chosen as a test case to study the effectiveness of multitasking on a CRAY- 2. The nature of dominant calculations within the application indicated that a sustained computational rate of 1 billion floating-point operations per second, or 1 <b>gigaflop,</b> might be achieved. The code was first analyzed and modified for optimal performance on a single processor in a batch environment. After optimal performance on a single CPU was achieved, the code was modified to use multiple processors in a dedicated environment. The results of these two efforts were merged into a single code that had a sustained computational rate of over 1 <b>gigaflop</b> on a CRAY- 2. Timings and analysis of performance are given for both single- and multiple-processor runs...|$|R
40|$|We {{present the}} {{preliminary}} results obtained on the 6 <b>Gigaflop</b> prototype {{which is the}} building block of the APE- 100 computer. Results are presented for pseudoscalar and vector mesons for the Wilson and the improved action with a statistics of about 100 configurations. Static heavy-light meson propagator have also been studied for different smeared operators. We find that large smearing (greater-than-or-equal-to 9) are bad projectors on the lightest pseudoscalar states...|$|R
40|$|We briefly discuss three {{algorithms}} used to process {{synthetic aperture radar}} (SAR) images: a polar reformatter that maps raw SAR data to a rectangular mesh, 2 -d FFTs that convert the data set to a real image, and phase gradient autofocusing which creates clear images by correcting blurs resulting from uncompensated phase errors. Implementation details and performance data for the algorithms are given for two parallel supercomputers, a nCUBE 2 hypercube and a CM- 2. Portions of the algorithms execute at <b>gigaflop</b> speeds, meaning that large SAR data sets (16, 384 x 65, 536) can be processed in tens of minutes instead of hours as on conventional supercomputers. Equally significantly, the gigabytes of internal memory on the parallel machines allow entire images to be processed in place. ...|$|R
5000|$|... 2002 Installation of the Earth Simulator, the world's fastest {{supercomputer}} from 2002 to 2004 {{reaching a}} speed of 35,600 <b>gigaflops</b> ...|$|E
50|$|The SX-3/44R was {{announced}} by NEC Corporation in 1989 {{and a year}} later earned the fastest in the world title with a 4 processor model. However, Fujitsu's Numerical Wind Tunnel supercomputer used 166 vector processors to gain the top spot in 1994. It had a peak speed of 1.7 <b>gigaflops</b> per processor. The Hitachi SR2201 on the other hand obtained a peak performance of 600 <b>gigaflops</b> in 1996 by using 2048 processors connected via a fast three-dimensional crossbar network.|$|E
50|$|Located at the {{supercomputing}} facility named Satish Dhawan Supercomputing Facility at Vikram Sarabhai Space Centre (VSSC), Thiruvananthapuram. it {{was built}} using commercially available hardware, open source software components and in house developments. The system uses 400 NVIDIA Tesla C2070 GPUs and 400 Intel Quad Core Xeon CPUs supplied by WIPRO. Each NVIDIA Tesla C2070 GPU is capable of delivering 515 <b>gigaflops</b> compared to the Xeon CPU’s more modest contribution of 50 <b>gigaflops.</b> The system cost about INR 140,000,000 to build. The system consumes only about 150 kW.|$|E
40|$|Accurate {{and rapid}} {{evaluation}} of radar signature for alternative aircraft/store configurations {{would be of}} substantial benefit {{in the evolution of}} integrated designs that meet radar cross-section (RCS) requirements across the threat spectrum. Finite-volume time domain methods offer the possibility of modeling the whole aircraft, including penetrable regions and stores, at longer wavelengths on today's <b>gigaflop</b> supercomputers and at typical airborne radar wavelengths on the teraflop computers of tomorrow. A structured-grid finite-volume time domain computational fluid dynamics (CFD) -based RCS code has been developed at the Rockwell Science Center, and this code incorporates modeling techniques for general radar absorbing materials and structures. Using this work as a base, the goal of the CFD-based CEM effort is to define, implement and evaluate various code development issues suitable for rapid prototype signature prediction...|$|R
40|$|Abstract. Static {{symbolic}} factorization {{coupled with}} supernode partitioning and asynchronous computation scheduling can achieve high <b>gigaflop</b> rates for parallel sparse LU factorization with partial pivoting. This paper studies properties of elimination forests and uses them to optimize supernode partitioning/amalgamation and execution scheduling. It also proposes supernodal matrix multiplication {{to speed up}} kernel computation by retaining the BLAS- 3 level efficiency and avoiding unnecessary arithmetic operations. The experiments show that our new design with proper space optimization, called S +, improves our previous solution substantially and can achieve up to 10 GFLOPS on 128 Cray T 3 E 450 MHz nodes. Key words. Gaussian elimination with partial pivoting, LU factorization, sparse matrices, elimination forests, supernode amalgamation and partitioning, asynchronous computation scheduling AMS subject classifications. 65 F 50, 65 F 05 PII. S 089547989833738...|$|R
40|$|HPTC) a step beyond {{conventional}} clusters. SiCortex {{concentrates on}} power-efficient design and simultaneous tuning of silicon, microcode, and system software to deliver outstanding application performance per dollar, per watt, and per square foot. The Company’s initial product offering includes: •The SC 5832, {{which is a}} 5. 8 Teraflop system with up to 8 Terabytes of memory. The SC 5832 fits into a single cabinet and draws 18 KW. The SC 648, which is a 648 <b>Gigaflop</b> system with up to 864 Gigabytes of memory. Two SC 648 systems fit in a single 19 ” rack with room to spare. A single SC 648 system draws 2 KW. This paper describes the SiCortex interconnect fabric, its software interface, and the communication software, including the Message Passing Interface (MPI), that uses it...|$|R
5000|$|The Blue Gene/P {{processor}} {{consists of}} four PowerPC 450 cores running at 850 MHz reaching 13.6 <b>gigaflops</b> in total. IBM is claiming very power efficient design compared to other supercomputer processors.|$|E
50|$|Another {{difference}} in the iWarp was that the systems were connected together as a n-by-m torus, instead of the more common hypercube. A typical system included 64 CPUs connected as an 8&times;8 torus, which could deliver 1.2 <b>gigaflops</b> peak.|$|E
50|$|An XK6 cabinet {{accommodates}} 24 blades (96 nodes). Each of the Tesla processors {{is rated}} at 665 double-precision <b>gigaflops</b> giving 63.8 teraflops per cabinet. The XK6 {{is capable of}} scaling to 500,000 Opteron cores, giving up to 50 petaflops total hybrid peak performance.|$|E
40|$|Multicomputers {{have the}} {{potential}} to deliver <b>Gigaflop</b> performance on many scientific applications. Initial implementations of parallel programs on these machines, however, are often inefficient and require significant optimization before they can harness the potential power of the machine. Performance prediction tools can provide valuable information on which optimizations will result in increased performance. This paper describes an analytical performance prediction model. The model is designed to provide performance data to compilers, programmers and system architects to assist them in making choices which will lead to more efficient implementations. Efficient performance prediction tools can provide information which will help programmers make better use of the power of multicomputers. 1 Introduction One of the most important advances in high performance computing is the increasing availability of commercial parallel computers. These machines promise to provide solutions to many p [...] ...|$|R
40|$|A new {{approach}} to autonomous vehicle perception is presented which solves the historically significant throughput problem at contemporary speeds through computational stabilization of the sensor sweep. This adaptive approach to perception {{has made it possible}} to achieve unprecedented autonomous vehicle speeds at little or no cost to other aspects of performance. In order to measure the local environment at sufficient resolution and sufficient rate, an autonomous vehicle requires computational throughput on the order of where is the vehicle reaction time and is the velocity. On the other hand, the traditional approach of nonadaptive range image processing requires throughput on the order of. The product is on the order of 10 for a conventional automobile so the difference between these two expressions is four orders of magnitude at 20 mph. Nonadaptive range image processing requires about 1 <b>gigaflop</b> in order to achieve 20 mph speeds whereas the algorithm presented here req [...] ...|$|R
40|$|Evolutionary {{algorithms}} {{have been}} gaining increased attention {{the past few}} years because of their versatility and are being successfully applied in several different fields of study. We group under this heading a family of new computing techniques rooted in biological evolution {{that can be used for}} solving hard problems. In this chapter we present a survey of genetic algorithms and genetic programming, two important evolutionary techniques. We discuss their parallel implementations and some notable extensions, focusing on their potential applications in the field of evolvable hardware. 1 Introduction The performance of modern computers is quite impressive; it seems fair to say that computers are far better than humans in many domains and that they comprise a powerful tool that is constantly changing our view of the world. On scientific and engineering number-crunching problems performance increases steadily and we are able to tackle so-called "grand challenge" problems with <b>gigaflop</b> [...] ...|$|R
50|$|PSSC Labs {{was founded}} in 1984 by Larry Lasser. In 1998, it {{manufactured}} the Aeneas Supercomputer for Dr. Herbert Hamber of the University of California, Irvine (the physics and astronomy department); {{it was based on}} Linux and had a maximum speed of 20.1 <b>Gigaflops.</b>|$|E
50|$|On October 25, 2007, NEC Corporation of Japan {{issued a}} press release announcing its SX series model SX-9, {{claiming}} {{it to be the}} world's fastest vector supercomputer. The SX-9 features the first CPU capable of a peak vector performance of 102.4 <b>gigaFLOPS</b> per single core.|$|E
50|$|A common way of {{building}} a PSC is syncing several computers with fast networking (commonly dual gigabit Ethernet switching per processor) linked by a gigabit network switch. Some PSCs use clustered GPUs or CPUs. For example, the TYANPSC uses 40 Xeon processors to achieve 256 <b>gigaflops.</b>|$|E
40|$|We have {{implemented}} a parallel computer architecture based entirely upon commodity personal computer components. Using 16 Intel Pentium Pro microprocessors and switched fast ethernet as a communication fabric, we have obtained sustained performance on scientific applications {{in excess of}} one <b>Gigaflop.</b> During one production astrophysics treecode simulation, we performed 1 : 2 Θ 10 15 floating point operations (1. 2 Petaflops) over a three week period, with one phase of that simulation running continuously for two weeks without interruption. We report {{on a variety of}} disk, memory and network benchmarks. We also present results from the NAS parallel benchmark suite, which indicate that this architecture is competitive with current commercial architectures. In addition, we describe some software written to support efficient message passing, as well as a Linux device driver interface to the Pentium hardware performance monitoring registers. Keywords: Beowulf, treecode, benchmarks 1 Intr [...] ...|$|R
40|$|Supernode {{partitioning}} for unsymmetric matrices {{together with}} complete block diagonal supernode pivoting and asynchronous computation can achieve high <b>gigaflop</b> rates for parallel sparse LU factorization on shared memory parallel computers. The progress in weighted graph matching algorithms helps to extend these concepts further and unsymmetric prepermutation of rows {{is used to}} place large matrix entries on the diagonal. Complete block diagonal supernode pivoting allows dynamical interchanges of columns and rows during the factorization process. The level- 3 BLAS efficiency is retained and an advanced two-level left–right looking scheduling scheme results in good speedup on SMP machines. These algorithms have been integrated into the recent unsymmetric version of the PARDISO solver. Experiments demonstrate that a wide set of unsymmetric linear systems can be solved and high performance is consistently achieved for large sparse unsymmetric matrices from real world applications. Key words: Computational sciences, numerical linear algebra, direct solver, unsymmetric linear system...|$|R
50|$|Based on the Finite Element Machine's {{success in}} demonstrating Parallel Computing viability, (alongside ILLIAC IV and Goodyear MPP), {{commercial}} parallel computers soon were sold. NASA Langley subsequently purchased a Flex/32 Multicomputer (and later Intel iPSC and Intel Paragon) to continue parallel finite element algorithm R&D. In 1989, the parallel equation solver code, first prototyped on FEM, and tested on FLEX was ported to NASA's first Cray YMP via Force (Fortran for Concurrent Execution) {{to reduce the}} structural analysis computation time for the space shuttle Challenger Solid Rocket Booster resdesign with 54,870 equations from 14 hours to 6 seconds. This research accomplishment was awarded the first Cray <b>GigaFLOP</b> Performance Award at Supercomputing '89. This code evolved into NASA's General-Purpose Solver (GPS) for Matrix Equations used in numerous finite element codes to speed solution time. GPS sped up AlphaStar Corporation's Genoa code 10X, allowing 10X larger applications for which the team received NASA's 1999 Software of the Year Award and a 2000 R&D100 Award.|$|R
50|$|In 1987, India {{decided to}} launch a {{national}} initiative in supercomputing to design, develop and deliver a supercomputer in the <b>gigaflops</b> range. Complementary projects were initiated in various labs, ANURAG being one of them. PACE was unveiled by then Prime Minister P.V. Narasimha Rao in April 1995.|$|E
5000|$|The APP was {{marketed as}} a [...] "matrix co-processor" [...] system and {{required}} a SPARC-based host system to operate, {{such as the}} Cray S-MP. Connection to the host system was via VMEbus or HiPPI. A fully configured APP had a peak performance of 6.7 (single-precision) <b>gigaflops.</b>|$|E
5000|$|To {{give some}} {{perspective}} to this, using Virginia Tech's System X {{with a maximum}} performance of 12.25 Teraflops, it would take approximately [...] seconds or about 3 weeks. Or for commodity processors at 2 <b>gigaflops</b> it would take 6,000 machines approximately {{the same amount of}} time.|$|E
40|$|We {{discuss the}} work of the QCDSP {{collaboration}} to build an inexpensive Teraflop scale massively parallel computer suitable for computations in Quantum Chromodynamics (QCD). The computer is a collection of nodes connected in a four dimensional toroidial grid with nearest neighbor bit serial communications. A node is composed of a Texas Instruments Digital Signal Processor (DSP), memory, and a custom made communications and memory controller chip. An 8192 node computer with a peak speed of 0. 4 Teraflops is being constructed at Columbia University for a cost of $ 1. 8 Million. A 12, 288 -node machine with a peak speed of 0. 6 Teraflops is being constructed for the RIKEN Brookhaven Research Center. Other computers have been built including a 50 <b>Gigaflop</b> version for Florida State University. Keywords: parallel, supercomputer, digital signal processor, QCD Introduction The atoms and nuclei of everyday matter are now known to be made up of still tinier particles known as quarks and leptons. [...] ...|$|R
40|$|Our {{long-term}} {{goal is to}} investigate the potential of multicomputers constructed of commercial, off-the-shelf components {{as a replacement for}} conventional parallel processors in high-performance computing. By commercial, off-the-shelf components, we mean commodity workstations and personal computers connected by readily available networking fabric, e. g. DEC Alpha and Intel PC boxes connected by gigabit ethernet. To understand the impact of clusters in this area, we are developing several layers of software to support traditional supercomputing applications such as climate and ocean modeling. OBJECTIVES We hope to demonstrate the efficacy of constructing medium-scale, clustered multicomputers for {{a fraction of the cost}} of traditional supercomputers. We are building a clustered system called Centurion. Using Centurion, we will demonstrate that we can outperform a cluster of C 90 machines, and at a fraction of the cost. This research can point the way to developing more effective, lower-cost computational engines for the Department of Defense. APPROACH While the original proposal was to build a 50 -Gigaflop machine, the resultant machine will have more than a 200 <b>Gigaflop</b> peak rate. We are building this multicomputer using off-the-shelf commodit...|$|R
40|$|Abstract: We {{present an}} {{overview}} of vectorization techniques for matrix algebra on the G 4 Velocity Engine. Though small matrices can be processed above <b>gigaflop</b> rates, our main emphasis herein is on very large matrices, for which the class of Strassen algorithms having superior asymptotic complexity apply. These large-matrix recursions use, as is natural, the very fast, small-matrix core multiply at recursion bottom. We invstigate the matrix operations of: multiplication, inversion, transposition; {{with a view to}} appropriate implementation variants with respect to G 4 architecture. For N × N matrix operands and 500 MHz. G 4, performance results are as follows. For sizes N ∼ 32 one can achieve well over 1 gigaflop/s for the core matrix multiply. For N ∼ 2500 the combination of G 4 and Strassen algorithm effects a full multiply in only 40 seconds. For such very-large matrices the Strassen algorithm is 7 times faster than the classical one. In fact the classical-Strassen breakover, though highly dependent on various conditions as we quantify, lies approximately at matrix size N × N = 128 × 128...|$|R
