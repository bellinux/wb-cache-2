2074|10000|Public
5|$|Convolution {{is used to}} add two <b>independent</b> <b>random</b> <b>variables</b> {{defined by}} {{distribution}} functions. Its usual definition combines integration, subtraction, and multiplication. In general, convolution is useful {{as a kind of}} domain-side addition; by contrast, vector addition is a kind of range-side addition.|$|E
25|$|Then Z0 and Z1 are <b>independent</b> <b>random</b> <b>variables</b> with a {{standard}} normal distribution.|$|E
25|$|The {{probability}} {{distribution of the}} sum of two <b>independent</b> <b>random</b> <b>variables</b> is the convolution of each of their distributions.|$|E
5000|$|Then, we sample N <b>independent</b> <b>random</b> <b>variable</b> [...] {{with common}} {{probability}} density [...] so that ...|$|R
30|$|When {{these are}} {{compared}} with the corresponding results of <b>independent</b> <b>random</b> <b>variable</b> sequences, there still remains much to be desired.|$|R
3000|$|... [...]) was {{obtained}} by Childs and Balakrishnan [7] by adding another <b>independent</b> <b>random</b> <b>variable</b> to the original n variables [...]...|$|R
25|$|Exactly {{the same}} method {{can be used}} to compute the {{distribution}} of other functions of multiple <b>independent</b> <b>random</b> <b>variables.</b>|$|E
25|$|This can {{be derived}} from a two-way change of {{variables}} involving Y=U+V and Z=V, similarly to the example below for the quotient of <b>independent</b> <b>random</b> <b>variables.</b>|$|E
25|$|Given two <b>independent</b> <b>random</b> <b>variables</b> U and V, each {{of which}} has a {{probability}} density function, the density of the product Y=UV and quotient Y=U/V can be computed by a change of variables.|$|E
3000|$|... is an <b>independent</b> <b>random</b> <b>variable</b> sequence, {{then the}} {{conclusion}} holds by [[5], Lemma  1, p. 444] or [[3], § 17.3, p. 248].|$|R
3000|$|..., j= 1, 2,…,n are <b>independent</b> <b>random</b> <b>variable</b> {{which follows}} any of two general form of {{distributions}} {{as defined in}} (2.1) and (2.2).|$|R
5000|$|The {{previous}} algorithm calculates [...] {{in order}} of [...] memory bits. Alon et al. in [...] simplified this algorithm using four-wise <b>independent</b> <b>random</b> <b>variable</b> with values mapped to [...]|$|R
25|$|Asymptotic normality, that is, {{convergence}} to {{the normal}} distribution after appropriate shift and rescaling, is a phenomenon much more general than the classical framework treated above, namely, sums of <b>independent</b> <b>random</b> <b>variables</b> (or vectors). New frameworks are revealed from time to time; no single unifying framework is available for now.|$|E
25|$|In {{probability}} theory, {{a distribution}} {{is said to}} be stable if a linear combination of two <b>independent</b> <b>random</b> <b>variables</b> with this distribution has the same distribution, up to location and scale parameters. A random variable {{is said to be}} stable if its distribution is stable. The stable distribution family is also sometimes referred to as the Lévy alpha-stable distribution, after Paul Lévy, the first mathematician to have studied it.|$|E
500|$|Another way to {{calculate}} [...] using probability {{is to start}} with a random walk, generated by a sequence of (fair) coin tosses: <b>independent</b> <b>random</b> <b>variables</b> [...] such that [...] with equal probabilities. [...] The associated random walk is ...|$|E
30|$|The main aim of {{this note}} is to {{establish}} some bounds in Poisson approximation for row-wise arrays of <b>independent</b> integer-valued <b>random</b> <b>variables</b> via the Trotter-Renyi distance. Some results related to <b>random</b> sums of <b>independent</b> integer-valued <b>random</b> <b>variables</b> are also investigated.|$|R
40|$|Let X,X 1,X 2, [...] . be <b>independent</b> identically {{distributed}} <b>random</b> <b>variables.</b> Then, Baum and Katz [1965. Convergence {{rates in}} the law of large numbers. Trans. Amer. Math. Soc. 120, 108 - 123] proved that for p> 1,if and only if and. We prove thatif and only if and is slowly varying as x [...] >[infinity]. Complete convergence <b>Independent</b> <b>random</b> <b>variable</b> Slowly varying function...|$|R
5000|$|A Bernoulli {{scheme is}} a discrete-time {{stochastic}} process where each <b>independent</b> <b>random</b> <b>variable</b> may take {{on one of}} N distinct possible values, with the outcome i occurring with probability , with i = 1, ..., N, and ...|$|R
500|$|OP-20-G {{then turned}} to the Japanese navy's [...] "Coral" [...] cipher, a key tool for the attack on which was the [...] "Gleason crutch", a form of Chernoff bound on tail {{distributions}} of sums of <b>independent</b> <b>random</b> <b>variables,</b> but predating Chernoff's work by a decade.|$|E
500|$|In {{addition}} to exact analytical expressions for representation of , there are stochastic techniques for estimating [...] [...] One such approach {{begins with an}} infinite sequence of <b>independent</b> <b>random</b> <b>variables</b> , ..., drawn from the uniform distribution on [...] Let [...] be the least number [...] such that {{the sum of the}} first [...] observations exceeds 1: ...|$|E
500|$|A {{symmetric}} {{random walk}} and a Wiener process (with zero drift) are both examples of martingales, respectively, in [...] discrete and continuous time. For {{a sequence of}} independent and identically distributed random variables [...] with zero mean, the stochastic process formed from the successive partial sums [...] is a discrete-time martingale. In this aspect, discrete-time martingales generalize the idea of partial sums of <b>independent</b> <b>random</b> <b>variables.</b>|$|E
50|$|Cramér's theorem is {{the result}} that if X and Y are <b>independent</b> real-valued <b>random</b> <b>variables</b> whose sum X + Y is a normal <b>random</b> <b>variable,</b> then both X and Y must be normal as well. By induction, if any finite sum of <b>independent</b> real-valued <b>random</b> <b>variables</b> is normal, then the summands must all be normal.|$|R
5000|$|... #Subtitle level 3: Learning Sums of <b>Independent</b> Integer <b>Random</b> <b>Variables</b> ...|$|R
3000|$|...k= 1, 2,…} {{is assumed}} to be “white” {{in the sense that the}} noise at {{distinct}} time points is an <b>independent</b> <b>random</b> <b>variable.</b> We also assume that noise process is independent of the initial state X 0 and the input sequence {u [...]...|$|R
500|$|Logarithms {{are used}} for maximum-likelihood {{estimation}} of parametric statistical models. For such a model, the likelihood function depends {{on at least one}} parameter that must be estimated. [...] A maximum of the likelihood function occurs at the same parameter-value as a maximum of the logarithm of the likelihood (the [...] "loglikelihood"), because the logarithm is an increasing function. The log-likelihood is easier to maximize, especially for the multiplied likelihoods for <b>independent</b> <b>random</b> <b>variables.</b>|$|E
2500|$|If [...] are independent, and , then [...] A {{converse}} is Raikov's theorem, {{which says}} that if the sum of two <b>independent</b> <b>random</b> <b>variables</b> is Poisson-distributed, then so are each of those two <b>independent</b> <b>random</b> <b>variables.</b>|$|E
2500|$|... {{the number}} of points in {{disjoint}} intervals are <b>independent</b> <b>random</b> <b>variables.</b>|$|E
5000|$|... whose {{coordinates}} are jointly distributed as two <b>independent</b> standardnormal <b>random</b> <b>variables.</b>|$|R
40|$|In this paper, {{we propose}} a novel method for {{increasing}} the entropy of a sequence of <b>independent,</b> discrete <b>random</b> <b>variables</b> with arbitrary distributions. The method uses an auxiliary table and a novel theorem that concerns the entropy of a sequence in which the elements are a bitwise exclusive-or sum of <b>independent</b> discrete <b>random</b> <b>variables...</b>|$|R
25|$|The normal {{distribution}} {{is an important}} example where the inverse transform method is not efficient. However, there is an exact method, the Box–Muller transformation, which uses the inverse transform to convert two <b>independent</b> uniform <b>random</b> <b>variables</b> into two <b>independent</b> normally distributed <b>random</b> <b>variables.</b>|$|R
2500|$|Let [...] and [...] be <b>independent</b> <b>random</b> <b>variables,</b> with , then we {{have that}} ...|$|E
2500|$|... {{the number}} of points in [...] {{disjoint}} Borel sets forms [...] <b>independent</b> <b>random</b> <b>variables.</b>|$|E
2500|$|A Bernoulli {{process is}} a finite or {{infinite}} sequence of <b>independent</b> <b>random</b> <b>variables</b> X1,X2,X3,..., such that ...|$|E
40|$|Let X be a <b>random</b> <b>variable.</b> We shall call an <b>independent</b> <b>random</b> <b>variable</b> Y to be a symmetrizer for X, if X+Y is {{symmetric}} around zero. If Y {{is independent}} copy of −X, {{it is obviously}} a symmetrizer. A <b>random</b> <b>variable</b> {{is said to be}} symmetry resistant if the variance of any symmetrizer Y, is never smaller than the varianc...|$|R
40|$|The {{arbitrary}} functions principle {{says that}} the fractional part of $nX$ converges stably to an <b>independent</b> <b>random</b> <b>variable</b> uniformly distributed on the unit interval, {{as soon as the}} <b>random</b> <b>variable</b> $X$ possesses a density or a characteristic function vanishing at infinity. We prove a similar property for <b>random</b> <b>variables</b> defined on the Wiener space when the stochastic measure $dB_s$ is crumpled on itself...|$|R
5000|$|... where [...] are {{pairwise}} <b>independent</b> integrable <b>random</b> <b>variables</b> {{with second}} moments and [...]|$|R
