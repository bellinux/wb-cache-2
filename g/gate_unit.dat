12|237|Public
5000|$|A {{good example}} of {{time-controlled}} noise gating is the well-known [...] "gated reverb" [...] effect heard on the drums on the Phil Collins hit single [...] "In the Air Tonight", created by engineer-producer Hugh Padgham, in which the powerful reverberation added to the drums is cut off by the noise gate after a few milliseconds, rather than being allowed to decay naturally. This can also be achieved by: sending the 'dry' snare signal to the reverb (or other process) unit, inserting a noise gate {{on the path of}} the reverb signal and connecting the snare sound to the side chain of the <b>gate</b> <b>unit.</b> With the <b>gate</b> <b>unit</b> set to 'external sidechain' (or 'external key'), the gate will respond to the snare signal level and 'cut off' when that has decayed below the threshold, not the reverberated sound.|$|E
50|$|At its root, the KLM scheme induces an {{effective}} interaction between photons by making projective measurements with photodetectors, which {{falls into the}} category of non-deterministic quantum computation. It is based on a non-linear sign shift between two qubits that uses two ancilla photons and post-selection. It is also based on the demonstrations that the probability of success of the quantum gates can be made close to one by using entangled states prepared non-deterministically and quantum teleportation with single-qubit operations Otherwise, without a high enough success rate of a single quantum <b>gate</b> <b>unit,</b> it may require an exponential amount of computing resources. Meanwhile, the KLM scheme is {{based on the fact that}} proper quantum coding can reduce the resources for obtaining accurately encoded qubits efficiently with respect to the accuracy achieved, and can make LOQC fault-tolerant for photon loss, detector inefficiency and phase decoherence. As a result, LOQC can be robustly implemented through the KLM scheme with a low enough resource requirement to suggest practical scalability, making it as promising a technology for QIP as other known implementations.|$|E
50|$|The Protoss {{receive a}} {{change to the}} Photon Overcharge ability, it can now be cast on Pylons and not Nexuses. Also, the Chrono Boost ability {{can no longer be}} cast on {{multiple}} structures, but instead focuses on a single structure infinitely, though it is limited to one per Nexus. The Oracle from Heart of the Swarm now has its Revelation and Envision abilities combined in one. It can also now cast a Stasis Ward, an invisible mine-like structure that, when detonated, traps units in a small radius in stasis, much alike the Stasis Field ability from the Arbiter of StarCraft: Brood War. The Warp Prism, the trademark Protoss air transport, now has the ability to pick up units at a longer, safe range, but still must get on point to unload any cargo or deploy its psionic field utilized to warp-in Protoss units. The Carrier now has the ability of launching interceptors at a longer range and safer distance. The Immortal loses its trademark Hardened Shields from HotS / WoL and instead receives the 'Barrier' ability, which mitigates damage temporarily. As for new units, the Protoss receive the Adept, a Gateway / Warp <b>Gate</b> <b>unit</b> which excels against light class units like the Terran Marine or Zerg Hydralisk and can use 'Psionic Transfer', creating a psionic copy of the unit to which the Adept teleports after a few seconds, favoring hit-and-run tactics. Another new unit is the Disruptor, built from the Robotics Facility and requiring the Robotics Bay. It attacks by discharging a ball of energy which can be player-controlled and directed at the opponent, dealing massive damage. The Disruptor is reminiscent of the Protoss Reaver from Brood War in that precise micromanagement is required to deliver greater damage.|$|E
50|$|The {{phase shift}} can be {{specified}} either in absolute terms (in delay chain <b>gate</b> <b>units),</b> or {{as a proportion of}} the clock period, or both.|$|R
40|$|We {{introduce}} a neural network architecture and a learning algorithm to produce factorized symbolic representations. We propose to learn these concepts by observing consecutive frames, letting {{all the components}} of the hidden representation except a small discrete set (<b>gating</b> <b>units)</b> be predicted from the previous frame, and let the factors of variation in the next frame be represented entirely by these discrete <b>gated</b> <b>units</b> (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3 D transformations and Atari 2600 games. Comment: Under review as a workshop paper for ICLR 201...|$|R
50|$|It is {{a common}} {{tradition}} for any military unit visiting the installation to paint a design {{on one of the}} large rock formations near the main <b>gate.</b> <b>Units</b> of all types and locations are represented.|$|R
40|$|The {{subspace}} Restricted Boltzmann Machine (subspaceRBM) is a third-order Boltzmann machine where multiplicative {{interactions are}} between one visible and two hidden units. There {{are two kinds}} of hidden units, namely, gate units and subspace units. The subspace units reflect variations of a pattern in data and the <b>gate</b> <b>unit</b> is responsible for activating the subspace units. Additionally, the <b>gate</b> <b>unit</b> {{can be seen as a}} pooling feature. We evaluate the behavior of subspaceRBM through experiments with MNIST digit recognition task, measuring reconstruction error and classification error. Comment: 7 page...|$|E
40|$|The {{purpose of}} this master thesis {{was to create a}} model of an IGBT in a single pulse test circuit and connect this model to a model of a <b>Gate</b> <b>Unit.</b> The IGBT model and the single pulse test circuit were both {{implemented}} in MATLAB and the <b>Gate</b> <b>Unit</b> was implemented in Simulink. The {{purpose of this}} model was to test the actions of the <b>gate</b> <b>unit,</b> so that the initial tuning could be done before going to the lab. Since no tests were performed in the lab, {{it was not possible to}} see how much of the testing that could have been done by simulations. However, the actions of the IGBT model much resembled the actions of the real component, even though some drawbacks were clear, such as the lack of tail current and tail voltage. These comparisons could be made between simulated characteristics and recordings from a previous test with the same component...|$|E
40|$|The use of {{simulation}} tools is {{of great}} value {{in the process of}} developing new power electronics devices and new converter topologies. The development time of power electronics and converter topologies can be shortened by the use of simulation tools. Sentaurus Device combines a good numerical model of power electronics devices and complex external circuits. It’s capable to simulate the IGBT switching behaviours applying different <b>gate</b> <b>unit</b> settings in HVDC Light™. The numerical model of the IGBT is studied in the thesis. The results of the IGBT fine-tuning are carried out. Implementing the IGBT model with its <b>gate</b> <b>unit</b> circuit using Sentaurus Device, the impact of various gate drive settings on the IGBT switching losses is demonstrated...|$|E
40|$|Recently {{recurrent}} {{neural networks}} (RNN) {{has been very}} successful in handling sequence data. However, understanding RNN and finding the best practices for RNN is a difficult task, partly {{because there are many}} competing and complex hidden units (such as LSTM and GRU). We propose a <b>gated</b> <b>unit</b> for RNN, named as Minimal <b>Gated</b> <b>Unit</b> (MGU), since it only contains one gate, which is a minimal design among all <b>gated</b> hidden <b>units.</b> The design of MGU benefits from evaluation results on LSTM and GRU in the literature. Experiments on various sequence data show that MGU has comparable accuracy with GRU, but has a simpler structure, fewer parameters, and faster training. Hence, MGU is suitable in RNN's applications. Its simple architecture also means that it is easier to evaluate and tune, and in principle it is easier to study MGU's properties theoretically and empirically...|$|R
40|$|We {{present a}} {{complimentary}} objective for training recurrent neural networks (RNN) with <b>gating</b> <b>units</b> that helps with regularization and interpretability of the trained model. Attention-based RNN models have shown success in many difficult sequence to sequence classification problems with {{long and short}} term dependencies, however these models are prone to overfitting. In this paper, we describe how to regularize these models through an L 1 penalty on the activation of the <b>gating</b> <b>units,</b> and show that this technique reduces overfitting {{on a variety of}} tasks while also providing to us a human-interpretable visualization of the inputs used by the network. These tasks include sentiment analysis, paraphrase recognition, and question answering. Comment: In review at NIP...|$|R
40|$|This paper {{introduces}} two recurrent {{neural network}} structures called Simple <b>Gated</b> <b>Unit</b> (SGU) and Deep Simple <b>Gated</b> <b>Unit</b> (DSGU), which are general structures for learning long term dependencies. Compared to traditional Long Short-Term Memory (LSTM) and <b>Gated</b> Recurrent <b>Unit</b> (GRU), both structures require fewer parameters and less computation time in sequence classification tasks. Unlike GRU and LSTM, which {{require more than}} one gates to control information flow in the network, SGU and DSGU only use one multiplicative gate to control the flow of information. We show that this difference can accelerate the learning speed in tasks that require long dependency information. We also show that DSGU is more numerically stable than SGU. In addition, we also propose a standard way of representing inner structure of RNN called RNN Conventional Graph (RCG), which helps analyzing the relationship between input units and hidden units of RNN. Comment: This paper has been withdrawn by the author due to lacking of enough experiment...|$|R
40|$|Recently, {{very deep}} {{convolutional}} neural networks (CNNs) have been attracting considerable attention in image restoration. However, as the depth grows, the long-term dependency problem is rarely realized for these very deep models, {{which results in}} the prior states/layers having little influence on the subsequent ones. Motivated {{by the fact that}} human thoughts have persistency, we propose a very deep persistent memory network (MemNet) that introduces a memory block, consisting of a recursive unit and a <b>gate</b> <b>unit,</b> to explicitly mine persistent memory through an adaptive learning process. The recursive unit learns multi-level representations of the current state under different receptive fields. The representations and the outputs from the previous memory blocks are concatenated and sent to the <b>gate</b> <b>unit,</b> which adaptively controls how much of the previous states should be reserved, and decides how much of the current state should be stored. We apply MemNet to three image restoration tasks, i. e., image denosing, super-resolution and JPEG deblocking. Comprehensive experiments demonstrate the necessity of the MemNet and its unanimous superiority on all three tasks over the state of the arts. Code is available at [URL] Accepted by ICCV 2017 (Spotlight presentation...|$|E
40|$|A new approach, using {{electromagnetic}} analysis, {{is proposed}} for field-effect transistor model scaling and monolithic-microwave integrated-circuit (MMIC) design. It {{is based on}} an empirical distributed modeling technique where the active device is described in terms of an external passive structure connected to a suitable number of internal active sections. On this basis, an equivalent admittance matrix per <b>gate</b> <b>unit</b> width is obtained which, as confirmed by experimental results provided in this paper, is consistent with simple scaling rules. The same technique can also be adopted for a “global approach” to MMIC design where complex electromagnetic phenomena are also taken into account. An example of application concerning this aspect is presente...|$|E
40|$|A novel {{diagnosis}} {{method has}} been developed based on transistor operating point analysis. The method is worked by the way of fault logic propagation trace based on voltage value and the way of operation time setting of each gate circuit. The former way is to detect penetration current net result from fault, replace the net with impedance net, calculate voltage value of each node of the impedance net by OHM’s low, and then sequentially trace the fault logic propagation. The latter way is to set a standardized operation time at each gate circuit, and then, the virtual terminal is prepared at output point of <b>gate</b> <b>unit.</b> The proposed method {{makes it possible to}} detect not only signal propagation of each gate in order of time, but oscillation phenomenon brought by feedback fault...|$|E
40|$|Recurrent neural {{networks}} with {{various types of}} hidden units {{have been used to}} solve a diverse range of problems involving sequence data. Two of the most recent proposals, <b>gated</b> recurrent <b>units</b> (GRU) and minimal <b>gated</b> <b>units</b> (MGU), have shown comparable promising results on example public datasets. In this paper, we introduce three model variants of the minimal <b>gated</b> <b>unit</b> (MGU) which further simplify that design by reducing the number of parameters in the forget-gate dynamic equation. These three model variants, referred to simply as MGU 1, MGU 2, and MGU 3, were tested on sequences generated from the MNIST dataset and from the Reuters Newswire Topics (RNT) dataset. The new models have shown similar accuracy to the MGU model while using fewer parameters and thus lowering training expense. One model variant, namely MGU 2, performed better than MGU on the datasets considered, and thus may be used as an alternate to MGU or GRU in recurrent {{neural networks}}. Comment: 5 pages, 3 Figures, 5 Table...|$|R
5000|$|MAESTRO-II: a multi-chip module {{approximately}} {{the size of}} a dime that serves as the hardware core of several other products. The module contains a 66 MHz ARM7 processor, 4 MB of flash, 8 MB of RAM, and a FPGA with 500,000 <b>gates.</b> <b>Unit</b> cost: $3-4K (in 2008). It replaces the previous generation modules which were based on the HC12 microcontroller.|$|R
50|$|A more major update was the MK-747, which {{added a new}} antenna from Raytheon, the Diplex <b>Gating</b> <b>Unit</b> (DGU), a {{bandpass}} filter and other modifications, to produce the AN/FPS-91 and 91A. The similar MK-748 applied to the -60 series resulted in the AN/FPS-64A, -65A, -66A and -67A. Canadian AN/FPS-87s were also converted, becoming AN/FPS-93 and 93A. These units were used with the SAGE system.|$|R
40|$|A {{digital signal}} {{generator}} includes an input unit configured to receive signal information {{of a target}} data signal, a controller configured to calculate at least two delay values {{and at least two}} data values, the at least two delay values and the at least two data values being used to generate a data signal corresponding to the signal information input through the input unit, a multi-phase clock generator configured to delay a reference clock signal based on the at least two delay values to generate at least two clock signals having different phases, a signal generator configured to generate at least two data signals by assigning the at least two data values to the at least two clock signals, and a logic <b>gate</b> <b>unit</b> configured to generate the data signal corresponding to the signal information input through the input unit based on the at least two data signals. Samsung Electronics Co., Ltd. Georgia Tech Research Corporatio...|$|E
40|$|An IGBT/diode {{model with}} more {{accurate}} characteristics than simple switchis required to serve for EMC issues from converter valve. The purpose of thismaster thesis {{is to develop}} an IGBT and diode model to achieve both accuratetransient behavior and fast simulation time during single pulse switchingtest circuit for the 4 : 5 kV and 2 : 0 kA StakPakTM IGBT module. A gate unitwhich resembles the ABB <b>gate</b> <b>unit</b> is implemented to obtain a good agreementbetween simulation and measurement. For demonstration and verication, theIGBT/diode model is applied in a simplied arm simulation of full scale ABBGeneration 4 HVDC-VSC converter station and capable of a half cell consistingof 8 series-connected IGBTs and their anti-paralleled diodes. The arm simulationresults are analyzed further for converter station EMC studies. Convergence issue {{is the most important}} problem in the whole process of modelimplementation and application. To guarantee the convergence in simulationsome characteristics such as the tail voltage at the end of turn-o is disregarded. But overall, the model is validated and adopted successfully. En IGBT-/diodmodell med mer exakta egenskaper an en enkel switch kravs foratt hantera EMC-problem fran omvandlarventilen. Syftet med denna magisteruppsatsar att utveckla en IGBT- och diodmodell for att uppna bade noggrantovergaende beteende och snabb simuleringstid under enkelpulsomkopplingstestkretsfor 4, 5 kV och 2, 0 kA-StakPak IGBT-modulen. En grindenhetsom liknar ABB-grindenheten implementeras for att fa god overensstammelsemellan simulering och matning. For demonstration och veriering, tillampasIGBT-/diodmodellen i en forenklad armsimulering av en fullskalig ABB Generation 4 HVDC-VSC-omvandlarstation och med kapacitet for en halvcell bestaendeav 8 seriekopplade IGBT och deras anti-parallellkopplade dioder. Resultatenfran armsimuleringen analyseras vidare for EMC-studier av omvandlarstationen. Konvergensfragan ar det viktigaste problemet i hela processen for modellimplementeringoch -tillampning. For att garantera konvergensen i simulering ignorerasvissa egenskaper sasom svansspanningen vid slutet av avstangning. Mentotalt sett, valideras och antas modellen framgangsrikt...|$|E
40|$|Recurrent neural {{networks}} (RNNs) have shown clear superiority in sequence modeling, particularly {{the ones with}} <b>gated</b> <b>units,</b> such as long short-term memory (LSTM) and <b>gated</b> recurrent <b>unit</b> (GRU). However, the dynamic properties behind the remarkable performance remain unclear in many applications, e. g., automatic speech recognition (ASR). This paper employs visualization techniques to study the behavior of LSTM and GRU when performing speech recognition tasks. Our experiments show some interesting patterns in the gated memory, {{and some of them}} have inspired simple yet effective modifications on the network structure. We report two of such modifications: (1) lazy cell update in LSTM, and (2) shortcut connections for residual learning. Both modifications lead to more comprehensible and powerful networks. Comment: ICASSP 201...|$|R
5000|$|The {{rectangular}} function (also {{known as}} the rectangle function, rect function, Pi function, <b>gate</b> function, <b>unit</b> pulse, or the normalized boxcar function) is defined as: ...|$|R
30|$|There {{are several}} {{improved}} RNN, such as simple RNN (SRNs), bidirectional RNN, deep (bidirectional) RNN, echo state networks, <b>Gated</b> Recurrent <b>Unit</b> RNNs, and clockwork RNN (CW-RNN.|$|R
50|$|Realizing that {{he needed}} more assets, the {{commander}} of 2nd Air Division, then Brig. Gen. Rollen Henry Anthis, asked for additional Air Force personnel and aircraft for Farm Gate use. Anthis wanted 10 more B-26s, five more T-28s, and two more SC-47s. McNamara reviewed the request, but he was cool {{to the idea of}} expanding Farm <b>Gate</b> <b>units</b> for combat use. His goal was to build up the VNAF so it could operate without American help. Still, McNamara approved the request for additional aircraft and also assigned two U-10s to Farm Gate.|$|R
40|$|The {{standard}} LSTM, {{although it}} succeeds in the modeling long-range dependences, {{suffers from a}} highly complex structure that can be simplified through modifications to its <b>gate</b> <b>units.</b> This paper was to perform an empirical comparison between the standard LSTM and three new simplified variants that were obtained by eliminating input signal, bias and hidden unit signal from individual gates, on the tasks of modeling two sequence datasets. The experiments show that the three variants, with reduced parameters, can achieve comparable performance with the standard LSTM. Due attention should be paid to turning the learning rate to achieve high accuraciesComment: 5 pages, 5 figure...|$|R
40|$|Abstract. We {{introduce}} low-overhead power optimization {{techniques to}} reduce leakage power in embedded processors. Our techniques improve previous work by a) {{taking into account}} idle time distribution for different execution units, and b) using instruction decode and control dependencies to wakeup the <b>gated</b> (but needed) <b>units</b> as soon as possible. We take into account idle time distribution per execution unit to detect an idle time period as soon as possible. This in turn results in increasing our leakage power savings. In addition, we use information already available in the processor to predict when a <b>gated</b> execution <b>unit</b> will be needed again. This results in early and less costly reactivation of <b>gated</b> execution <b>units.</b> We evaluate our techniques for a representative subset of MiBench benchmarks and for a processor using a configuration similar to Intels Xscale processor. We show that our techniques reduce leakage power considerably while maintaining performance. ...|$|R
40|$|In this work, {{we propose}} a novel {{recurrent}} neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global <b>gating</b> <b>unit</b> for {{each pair of}} layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and <b>gated</b> recurrent <b>units,</b> on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions. Comment: 9 pages, removed appendi...|$|R
40|$|We propose {{modeling}} {{time series}} by representing the transformations that take a frame {{at time t}} to a frame at time t+ 1. To this end we show how a bi-linear model of transformations, such as a gated autoencoder, {{can be turned into}} a recurrent net-work, by training it to predict future frames from the current one and the inferred transformation using backprop-through-time. We also show how stacking multi-ple layers of <b>gating</b> <b>units</b> in a recurrent pyramid makes it possible to represent the ”syntax ” of complicated time series, and that it can outperform standard recurrent neural networks in terms of prediction accuracy on a variety of tasks. ...|$|R
40|$|Recurrent Neural Networks (RNNs), {{which are}} a {{powerful}} scheme for modeling temporal and sequential data need to capture long-term dependencies on datasets and represent them in hidden layers with a powerful model to capture more information from inputs. For modeling long-term dependencies in a dataset, the gating mechanism concept can help RNNs remember and forget previous information. Representing the hidden layers of an RNN with more expressive operations (i. e., tensor products) helps it learn a more complex relationship between the current input and the previous hidden layer information. These ideas can generally improve RNN performances. In this paper, we proposed a novel RNN architecture that combine the concepts of gating mechanism and the tensor product into a single model. By combining these two concepts into a single RNN, our proposed models learn long-term dependencies by modeling with <b>gating</b> <b>units</b> and obtain more expressive and direct interaction between input and hidden layers using a tensor product on 3 -dimensional array (tensor) weight parameters. We use Long Short Term Memory (LSTM) RNN and <b>Gated</b> Recurrent <b>Unit</b> (GRU) RNN and combine them with a tensor product inside their formulations. Our proposed RNNs, which are called a Long-Short Term Memory Recurrent Neural Tensor Network (LSTMRNTN) and <b>Gated</b> Recurrent <b>Unit</b> Recurrent Neural Tensor Network (GRURNTN), are made by combining the LSTM and GRU RNN models with the tensor product. We conducted experiments with our proposed models on word-level and character-level language modeling tasks and revealed that our proposed models significantly improved their performance compared to our baseline models. Comment: Accepted at IJCNN 2016 URL : [URL]...|$|R
40|$|Many {{problems}} {{solved by}} multilayer neural networks (MLNNs) are reduced into pattern mapping. If the mapping includes several different rules, {{it is difficult}} to solve these problems by using a single MLNN with linear connection weights and continuous activation functions. In this paper, a structure trainable neural network has been proposed. The <b>gate</b> <b>units</b> are embedded, which can be trained together with the connection weights. Pattern mapping problems, which include several different mapping rules, can be realized using a single new network. Since, some parts of the network can be commonly used for different mapping rules, the network size can be reduced compared with the modular neural networks, which consists of several independent expert networks...|$|R
30|$|In this section, we extend {{conventional}} LSTM {{and investigate}} {{in more depth}} the <b>gated</b> recurrent <b>unit</b> (GRU) element introduced in our previous work [18]. We also propose a new element called a residual GRU (rGRU) to alleviate the vanishing gradient problem across multiple layers.|$|R
40|$|In this paper, a {{synthesis}} and learning method for the neural network with embedded <b>gate</b> <b>units</b> and a multi-dimensional input is proposed. When the input is multi-dimensional, gate functions are controlled in a multi-dimensional space. In this case, a hypersurface, {{on which the}} gate function is formed should be optimized. Furthermore, the switching points should be considered on the unit input. An initialization and a control methods for gate functions, which optimize the hypersurface, the switching point and the inclination, are proposed. The stabilization methods, already proposed, are further modified {{to be applied to}} the multi-dimensional environment. The gate functions can be trained together with the connection weights. Discontinuous function approximation is demonstrated to confirm usefulness of the proposed method...|$|R
40|$|Theoretical and {{empirical}} {{evidence indicates that}} the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive <b>gating</b> <b>units</b> to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures. Comment: 11 pages. Extends arXiv: 1505. 00387. Project webpage is at [URL] in Advances in Neural Information Processing Systems 201...|$|R
50|$|Also {{appearing}} in Europe, {{this one is}} commandeered by Kaiser Kikai himself and merged with a junked battleship on Dream Island. With the tank, Kaiser Kikai was able to defeat the combined (and overconfident) forces of <b>Gate</b> Robo <b>Units</b> 1-3, seriously injuring Jim Skylark in the process.|$|R
50|$|<b>Gated</b> {{recurrent}} <b>units</b> (GRUs) are a gating {{mechanism in}} recurrent neural networks, introduced in 2014. Their performance on polyphonic music modeling and speech signal modeling {{was found to}} be similar to that of long short-term memory. They have fewer parameters than LSTM, as they lack an output gate.|$|R
40|$|Analisis Kinerja Gate Pada Terminal Keberangkatan Domestik Di Bandar Udara Internasional Sultan HasanuddinAn {{increasing}} number of aircraft impact annually {{on the need to}} serve the needs of the aircraft gate at Sultan Hasanuddin International Airport Makassar in particular on the use of gates at the new airport. Gate is referred to here is the door that connects the terminal to the aircraft door through aviobridge. Gate on existing conditions amounted to 6 <b>units.</b> <b>Gate</b> has not been further developed for the new airport is operated. Though the growth that occurred in the plane of Sultan Hasanuddin airport as very rapidly. To analyze the needs of the gate on the existing condition, 5 and 10 years from the equation used by Robert Horonjeff and forecasting with the linear regression method. In the existing condition with 30 -minute service time for a single plane, it takes additional <b>gate</b> 14 <b>units.</b> In the five years {{to come up with a}} 30 minute service time for a single plane, it takes as many as 20 <b>units</b> of additional <b>gate.</b> While in the 10 years to come up with a 30 minute service time for a single plane, it takes additional <b>gate</b> 37 <b>units...</b>|$|R
