9|171|Public
500|$|...hack simulates an MMORPG; players {{assume the}} role of a {{participant}} in a fictional game called The World. The player controls the on-screen player character Kite from a third-person perspective but first-person mode is available. The player manually controls the viewing perspective using the game controller. Within the fictional game, players explore monster-infested fields and dungeons, and [...] "Root Towns" [...] that are free of combat. They can also log off from The World and return to a computer desktop interface which includes in-game e-mail, news, message boards, and desktop and background music customization options. The player may save the game to a memory card both from the desktop and within The World at a Save Shop. A <b>Data</b> <b>Flag</b> appears on the save file after the player completes the game, allowing the transfer of all aspects of the player character and party members to the next game in the series.|$|E
50|$|In 2010—the {{day after}} his final day as an IDF officer—Ouzan co-founded BillGuard with Yaron Samid. A spending-and-credit-tracking app, it allowed users to track their spending, monitor their credit, set up fraud alerts and, via crowd-sourced <b>data,</b> <b>flag</b> {{unauthorized}} charges. Ouzan served as BillGuard's chief technology officer and head of product development.|$|E
5000|$|...hack//Gift, a self-deprecating, tongue-in-cheek, OVA {{that was}} created as a [...] "gift" [...] {{for those who had}} bought and {{completed}} all four [...]hack video games. It was released under Project [...]hack. In Japan, it was available when the <b>Data</b> <b>Flag</b> on the memory card file in [...]hack//Quarantine was present, whereas the American version included Gift on the fourth Liminality DVD. It is predominantly a comedy that makes fun of everything that developed throughout the series, even the franchise's own shortcomings. Character designs are deliberately simplistic.|$|E
50|$|A common {{technique}} {{to deal with}} RFI within the observed frequency bandwidth, is to employ RFI detection in software. Such software can find samples in time, frequency or time-frequency space that are contaminated by an interfering source. These samples are subsequently ignored in further analysis of the observed data. This process {{is often referred to}} as <b>data</b> <b>flagging.</b> Because most transmitters have a small bandwidth and are not continuously present such as lightning or citizens' band (CB) radio devices, most of the data remains available for the astronomical analysis. However, <b>data</b> <b>flagging</b> can not solve issues with continuous broad-band transmitters, such as windmills, digital video or digital audio transmitters.|$|R
40|$|This {{document}} {{presents the}} basic definition and methodology for the PREF data collection. It covers basic definitions of funding streams and funding instruments, the thematic classifications, characterization of research funding organizations and umbrella public research organizations. It also provides guidelines concerning the data structure, data collection process, <b>data</b> <b>flagging</b> and collection {{and management of}} metadata. JRC. B. 7 -Knowledge for Finance, Innovation and Growt...|$|R
40|$|In this demonstration, we {{show-case}} {{a database}} management system extended with {{a new type of}} component that we call a Data Use Manager (DUM). The DUM enables DBAs to attach policies to data loaded into the DBMS. It then monitors how users query the <b>data,</b> <b>flags</b> potential policy violations, recommends possible fixes, and supports offline analysis of user activities related to data policies. The demonstration uses real healthcare data...|$|R
50|$|At {{the launch}} of Google Public DNS, it did not {{directly}} support DNSSEC. Although RRSIG records could be queried, the AD (Authenticated <b>Data)</b> <b>flag</b> was not set in the launch version, meaning the server was unable to validate signatures {{for all of the}} data. This was upgraded on 28 January 2013, when Google's DNS servers silently started providing DNSSEC validation information, but only if the client explicitly set the DNSSEC OK (DO) flag on its query. This service requiring a client-side flag was replaced on 6 May 2013 with full DNSSEC validation by default, meaning all queries will be validated unless clients explicitly opt out.|$|E
5000|$|...hack simulates an MMORPG; players {{assume the}} role of a {{participant}} in a fictional game called The World. The player controls the on-screen player character Kite from a third-person perspective but first-person mode is available. The player manually controls the viewing perspective using the game controller. Within the fictional game, players explore monster-infested fields and dungeons, and [...] "Root Towns" [...] that are free of combat. They can also log off from The World and return to a computer desktop interface which includes in-game e-mail, news, message boards, and desktop and background music customization options. The player may save the game to a memory card both from the desktop and within The World at a Save Shop. A <b>Data</b> <b>Flag</b> appears on the save file after the player completes the game, allowing the transfer of all aspects of the player character and party members to the next game in the series.|$|E
50|$|The 1802 is an 8-bit byte machine, {{with minimal}} support for 16-bit operations, except via 2-byte manipulation. The primary {{accumulator}} is the 8-bit 'D' register (Data register). The single bit carry flag is DF (<b>Data</b> <b>Flag).</b> Most operations use the D register, including arithmetic and logic and memory reference load and store instructions. Most 16-bit operations {{have to work}} on the lower byte and then the upper byte, via D, using the DF as carry and borrow as needed. Instructions allow the get and put of the upper and lower bytes of the sixteen 16-bit registers via D. However, the 16-bit registers can be incremented and decremented with single instructions, and a few instructions perform automatic increment and decrement, like LDA (load advance) and STXD (store via X and decrement). 16-bit register and value comparisons would also need to use the D register as a go-between, using multiple instructions to perform the operations.|$|E
30|$|Raw GPR data {{needs to}} be preprocessed and prescreened. Preprocessing {{includes}} ground-level alignment and signal and noise background removal. Prescreening is needed to focus attention and identify regions with subsurface anomalies. For this step, we use the adaptive least mean squares (LMS) prescreener [34]. The LMS flags locations of interest utilizing a computationally inexpensive algorithm so that more advanced algorithms can be applied only on the small subsets of <b>data</b> <b>flagged</b> by the prescreener.|$|R
3000|$|... [...]) {{and total}} scalar field values, along with various <b>data</b> quality <b>flag</b> {{indicators}} (e.g. missing data; star camera availability; spacecraft manoeuvres).|$|R
40|$|An {{experiment}} is reported which: extended {{the concepts of}} <b>data</b> <b>flagging</b> and tagging to the aerospace scientific and technical literature; generated experience with the assignment of data summaries and data terms by documentation specialists; and obtained real world assessments of data summaries and data terms in information products and services. Inclusion of data summaries and data terms improved users' understanding of referenced documents from a subject perspective {{as well as from}} a data perspective; furthermore, a radical shift in document ordering behavior occurred during the experiment toward proportionately more requests for data-summarized items...|$|R
5000|$|...hack//G.U. simulates a massively {{multiplayer}} online role-playing game; players {{assume the}} role of a participant in a fictional game called The World. While in The World, the player controls the on-screen player character, Haseo, from a third-person perspective (with optional first-person mode). The player may control the camera using the game controller's right analog stick. Within the fictional game, players explore monster-infested fields and dungeons as well as [...] "Root Towns" [...] that are free of combat. They also can [...] "log-off" [...] from the game and return to a computer desktop interface which includes in-game e-mail, news, and message boards, as well as desktop and background music customization options. In Reminisce, an optional card game called [...] "Crimson VS" [...] becomes available. The player may save the game to a memory card both from the desktop and within The World at a Save Shop. After the player completes the game, a <b>Data</b> <b>Flag</b> appears on the save file, which allows the transfer of all aspects of the player character and party members to the next game in the series. This can also be applied to previous games if the player first finished the Reminisce or Redemption.|$|E
40|$|This {{research}} {{focuses on}} the data quality control methods for evaluating the performance of Weigh-In-Motion (WIM) systems on Oregon highways. This research identifies and develops a new methodology and algorithm to explore the accuracy of each station 2 ̆ 7 s weight and spacing data at a corridor level, and further implements the Statistical Process Control (SPC) method, finite mixture model, axle spacing error rating method, and <b>data</b> <b>flag</b> method in published research to examine the soundness of WIM systems. This research employs the historical WIM data to analyze sensor health and compares the evaluation results of the methods. The results suggest the new triangulation method identified most possible WIM malfunctions that other methods sensed, and this method unprecedentedly monitors the process behavior with controls of time and meteorological variables. The SPC method appeared superior in differentiating between sensor noises and sensor errors or drifts, but it drew wrong conclusions when accurate WIM data reference was absent. The axle spacing error rating method cannot check the essential weight data in special cases, but reliable loop sensor evaluation results were arrived at by employing this multiple linear regression model. The results of the <b>data</b> <b>flag</b> method and the finite mixed model results were not accurate, thus they {{could be used as}} additional tools to complement the data quality evaluation results. Overall, these data quality analysis results are the valuable sources for examining the early detection of system malfunctions, sensor drift, etc., and allow the WIM operators to correct the situation on time before large amounts of measurement are lost...|$|E
40|$|A {{high-speed}} {{data acquisition}} based on FPGA and implemented in VHDL {{is presented in}} this paper. According to the requirement of a new radar system, several new technologies are adopted {{in the design and}} implementation such as Time Compression Storage and Memory Rewriting. As a result, the system performs well with low dissipation of power, simple circuit layout and high efficient utilization of memory. The acquisition system comprises four parts: ADC circuit, Data Package and Interface, Sampling Data Memory and <b>data</b> <b>Flag</b> Memory. To implement large circuit, FPGA is adopted in this data acquisition system with reconfigurable ability and constant delay feature [1]. Index Terms—Compression Sampling, field programmable gate array (FPGA), VHDL, flag, memory...|$|E
40|$|Scientific {{datasets}} {{collected by}} instruments usually include outliers which {{have to be}} <b>flagged</b> in <b>data</b> quality control process. OutlierFlag was developed to make this process accurate and simple by providing a suitable outlier <b>data</b> <b>flagging</b> algorithm and a user friendly GUI. The algorithm consists of three steps performed one by one: limitation check, error check and standard deviation check. Several parameters are configurable so the algorithm {{can be used for}} various datasets. OutlierFlag is an open source software written in Java and the MeteoInfo library was used for data plotting function...|$|R
5000|$|... a data file, being {{a list of}} {{recipients}} {{stored in}} a non-document, comma-delimited plain ASCII text file, typically named Clients.dat (although WordStar had no requirement for a specific file extension). Each subsequent line of text in the file would be dedicated to a particular client, with name and address details separated on the line dedicated to a client by commas, read left to right. For example: Mr., Michael, Smith, 7 Oakland Drive, ... WordStar would also access Lotus123 spreadsheet files (*.wk1) for this data and if the <b>data</b> contained <b>flags</b> to start and stop WordStar processing the <b>data</b> then <b>flags</b> could be set so that certain 'clients' are omitted from the output stream.|$|R
5000|$|Although the programme's {{playlist}} {{is almost}} exclusively rock, pop and soul from the 1970s onwards, until early 2008 its Radio <b>Data</b> System <b>flag</b> identified it as programme type 12, [...] "M.O.R." [...] It is now identified as programme type 10, [...] "Pop music." ...|$|R
50|$|By 2010, serious games {{had evolved}} to {{incorporate}} actual economies like Second Life, in which users can create actual business which deal in virtual commodities {{and services for}} Linden dollars which were exchangeable for US currency. In 2015, Project Discovery was launched as a serious game. Project Discovery was launched as a vehicle by which geneticists and astronomers with the University of Geneva could access the cataloging efforts of the gaming public via a mini-game contained within the Eve Online massively multiplayer online role-playing game (MMORPG). Players acting as citizen scientists categorize and assess actual genetic samples or astrological data. This data was then utilized and warehoused by researchers. Any <b>data</b> <b>flagged</b> as atypical was further investigated by trained scientists.|$|R
40|$|Explanations will {{be given}} of the various user-written {{routines}} required by the Monte Carlo detector-modeling program GEANT, developed by CERN, the European Organization for Nuclear Research. User-written routines must be linked with the CERN library to accomplish the researcher 2 ̆ 7 s intentions. Examples will illustrate how GEANT passes information to subprograms needed to model events. Various data structures used by GEANT library calls and included in each user routine, are similarly illustrated. Both computational-speed and memory-size limitations need to be factored into {{the construction of a}} simulation model. This will constrain the calls used in the user-written routines. Examples are provided of GEANT input <b>data</b> <b>flags,</b> defined by the user to determine simulation parameters and to control various testing choices in GEANT...|$|R
5000|$|Since 1970 {{researchers}} at the Communications Research Centre (CRC) in Ottawa {{had been working on}} a set of [...] "picture description instructions", which encoded graphics commands as a text stream. Graphics were encoded as a series of instructions (graphics primitives) each represented by a single ASCII character. Graphic coordinates were encoded in multiple 6 bit strings of XY coordinate <b>data,</b> <b>flagged</b> to place them in the printable ASCII range so that they could be transmitted with conventional text transmission techniques. ASCII SI/SO characters were used to differentiate the text from graphic portions of a transmitted [...] "page". In 1975, the CRC gave a contract to Norpak to develop an interactive graphics terminal that could decode the instructions and display them on a colour display, which was successfully up and running by 1977.|$|R
40|$|AbstractIn the Solar Radiation Resource Assessment (SRRA) {{project of}} the Ministry of New and Renewable Energy, India a network of 51 {{automatic}} solar radiation monitoring stations {{have been set up}} across India. Such a large number of high-quality solar radiation stations with sensitive instruments require efficient procedures for regularly controlling proper operation of each station, the quality of the measured data, and its overall performance. Following best practices for quality assessment tests, such routines are implemented at the SRRA archiving and processing center. Various quality control tests are applied that check the plausibility of data, differentiate trustworthy data from likely erroneous <b>data</b> and <b>flag</b> them accordingly. A <b>data</b> <b>flagging</b> system is implemented to identify, differentiate and quantify different types of errors. These quality-checked, <b>flagged</b> <b>data</b> are then used by routines to create monthly reports and data products. This paper describes the automated quality check system implemented and evaluates the performance of stations since their erection in 2011. This paper also describes first experiments to validate the functionality of the applied quality checks. The quality flag statistics of all 51 stations reveals that some stations are performing very well and others need more attention to improve. In the period from January 2012 to March 2013 on an average over all 51 stations, 92 % of the solar radiation data are classified as correct. Around 4 % of solar radiation data do not pass the coherence test. Tracking errors are observed during 0. 3  % of the time averaged over all 51 stations. This analysis helps to further improve the operation of this network and find ways for better-automatized quality checks...|$|R
40|$|This webcast {{features}} Dr. Robert Murphy of NASA {{discussing the}} <b>data</b> quality <b>flags</b> and distribution network {{for the initial}} data coming from the NPOESS Preparatory Project (NPP) satellite instruments. Dr. Murphy also provides contact points {{for more information or}} to receive the initial NPP data stream. Educational levels: Graduate or professional...|$|R
3000|$|The {{reported}} TII <b>data</b> quality <b>flag</b> {{is determined}} using a preliminary algorithm that evidently does not capture all outliers or problematic intervals. Therefore, each satellite pass was analyzed {{to identify and}} exclude unreliable data, which is often characterized by a double population of points in either the ion drift data or T [...]...|$|R
40|$|Cleaning {{organizational}} data of discrepancies in {{structure and content}} is important for data warehousing and Enterprise Data Integration (EDI). Current commercial solutions for data cleaning involve many iterations of time-consuming "data quality" analysis to find errors, and long-running transformations to fix them. Users need to endure long waits and often write complex transformation programs. We present an interactive framework for data cleaning that tightly integrates transformation and discrepancy detection. Users gradually build transformations by adding or undoing transforms, in a intuitive, graphical manner through a spreadsheet-like interface; {{the effect of a}} transform is shown at once on records visible on screen. In the background, the system incrementally searches for discrepancies on the latest transformed version of <b>data,</b> <b>flagging</b> them as they are found. This allows users to gradually construct a transformation as discrepancies are found, and clean the data with [...] ...|$|R
40|$|Since the mid- 1980 s, {{airborne}} {{and ground}} measurements {{have been widely}} used to provide comprehensive characterization of atmospheric composition and processes. Field campaigns have generated a wealth of insitu data and have grown considerably over the years in terms of both the number of measured parameters and the data volume. This can largely be attributed to the rapid advances in instrument development and computing power. The users of field data may face a number of challenges spanning data access, understanding, and proper use in scientific analysis. This tutorial is designed to provide an introduction to using data sets, with a focus on airborne measurements, for atmospheric research. The first part of the tutorial provides an overview of airborne measurements and data discovery. This will be followed by a discussion on the understanding of airborne data files. An actual data file will be used to illustrate how data are reported, including the use of <b>data</b> <b>flags</b> to indicate missing data and limits of detection. Retrieving information from the file header will be discussed, which is essential to properly interpreting the data. Field measurements are typically reported as a function of sampling time, but different instruments often have different sampling intervals. To create a combined data set, the data merge process (interpolation of all data to a common time base) will be discussed in terms of the algorithm, data merge products available from airborne studies, and their application in research. Statistical treatment of missing <b>data</b> and <b>data</b> <b>flagged</b> for limit of detection will also be covered in this section. These basic data processing techniques are applicable to both airborne and ground-based observational data sets. Finally, the recently developed Toolsets for Airborne Data (TAD) will be introduced. TAD (tad. larc. nasa. gov) is an airborne data portal offering tools to create user defined merged data products with the capability to provide descriptive statistics and the option to treat measurement uncertainty...|$|R
40|$|Effective solar {{radiation}} measurements {{for research and}} economic analyses require a strict protocol for maintenance, calibration, and documentation to minimize station downtime and data corruption. The National Renewable Energy Laboratory's Concentrating Solar Power: Best Practices Handbook for the Collection and Use of Solar Resource Data includes guidelines for operating a solar measurement station. This paper describes a suite of automated and semi-automated routines based on the best practices handbook as developed for the National Renewable Energy Laboratory Solar Resource and Meteorological Assessment Project. These routines allow efficient inspection and <b>data</b> <b>flagging</b> to alert operators of conditions that require immediate attention. Although the handbook is targeted for concentrating solar power applications, the quality-assessment procedures described are generic and should benefit many solar measurement applications. The routines use data in one-minute measurement resolution, {{as suggested by the}} handbook, but they could be modified for other time scales...|$|R
40|$|This {{document}} describes data products {{related to}} the reported planetary parameters and uncertainties for the Kepler Objects of Interest (KOIs) based on a Markov-Chain-Monte-Carlo (MCMC) analysis. Reported parameters, uncertainties and data products {{can be found at}} the NASA Exoplanet Archive. The codes used for this data analysis are available on the Github website (Rowe 2016). The relevant paper for details of the calculations is Rowe et al. (2015). The main differences between the model fits discussed here and those in the DR 24 catalogue are that the DR 25 light curves were used in the analysis, our processing of the MAST light curves took into account different <b>data</b> <b>flags,</b> the number of chains calculated was doubled to 200 000, and the parameters which are reported are based on a damped least-squares fit, instead of the median value from the Markov chain or the chain with the lowest 2 as reported in the past...|$|R
40|$|Related to the Fukushima Daiichi Nuclear Power Station Accident, the Japan Atomic Energy Agency Library has {{accumulated}} valuable information on Internet. In the Fifteenth International Conference on Grey Literature, we reported {{the development of}} the Fukushima Nuclear Accident Archive using the DSpace. We have encountered a new, challenging issue of grey literature. In many cases, Internet information contains valuable numerical data. However, identifying the existence of numerical data on Internet sites is difficult, and the metadata created for the Fukushima Nuclear Accident Archive cannot currently be used to distinguish whether such information contains numeric data. Therefore, we have considered a method to identify numerical data and have introduced a “data flagging” system that has been used in the International Atomic Energy Agency’s International Nuclear Information System. In this paper, we introduce the proposed <b>data</b> <b>flagging</b> system and discuss its application to the Fukushima Nuclear Accident Archive. Includes: Conference preprint, Powerpoint presentation, Abstract and Biographical notesXAInternationa...|$|R
40|$|We outline {{different}} methods to detect errors in automatically-parsed dependency corpora, by comparing so-called dependency rules to their {{representation in the}} training <b>data</b> and <b>flagging</b> anomalous ones. By comparing each new rule to every relevant rule from training, we can identify parts of parse trees which are likely erroneous. Even the relatively simple methods of comparison we propose show promise for speeding up the annotation process. ...|$|R
50|$|Data {{processing}} includes <b>data</b> normalization, <b>flagging</b> of the <b>data,</b> averaging {{the intensity}} ratio for replicates, clustering of similarly expressed genes, etc. Data {{also must be}} normalized before further analysis. Normalization removes non-biological variation between the samples. After normalization, the intensity ratio is calculated for each gene in the replicate. Based on the ratio, the level of gene expression is determined. Quality control can then be performed.|$|R
50|$|However, {{despite the}} {{existence}} of an ISO GDF standard, the nature of model abstractions as well as semantic interpretations and proprietary content extensions lead to interoperability issues between flavors of GDF map products from different vendors. In practice the GDF files are not fully interchangeable due to vendor specific extensions. To this end, GDF5.0 provides major improvements in terms of extended meta <b>data</b> and <b>flags</b> for signalling implementation choices.|$|R
5000|$|The Canadian Communications Research Centre (CRC), {{based in}} Ottawa, {{had been working}} on various {{graphics}} systems since the late 1960s, much of it led by Herb Bown. Through the 1970s they turned their attention to building out a system of [...] "picture description instructions", which encoded graphics commands as a text stream. Graphics were encoded as a series of instructions (graphics primitives) each represented by a single ASCII character. Graphic coordinates were encoded in multiple 6 bit strings of XY coordinate <b>data,</b> <b>flagged</b> to place them in the printable ASCII range so that they could be transmitted with conventional text transmission techniques. ASCII SI/SO characters were used to differentiate the text from graphic portions of a transmitted [...] "page". These instructions were decoded by separate programs to produce graphics output, on a plotter for instance. Other work produced a fully interactive version. In 1975, the CRC gave a contract to Norpak to develop an interactive graphics terminal that could decode the instructions and display them on a color display.|$|R
50|$|Control {{self-assessment}} {{creates a}} clear line of accountability for controls, {{reduces the risk}} of fraud (by examining <b>data</b> that may <b>flag</b> unusual patterns of transactions) and results in an organisation with a lower risk profile.|$|R
40|$|Quality {{control is}} a major {{prerequisite}} for using meteorological information. High quality data sources are vital to scientists, engineers and decision makers alike. Validation of meteorological data ensures that the information needed has been properly generated and that it identifies incorrect values and detects problems that require immediate maintenance attention. In this work, several quality assurance procedures based on different criteria are proposed and applied to meteorological data from the Agroclimatic Information Network of Andalusia (Southern Spain) to assess their integrity and quality. The procedures include validations of record structure data, range/limits, time and internal consistency, persistence and spatial consistency tests. Quality assurance tests consist of procedures or rules against which data are tested, setting <b>data</b> <b>flags</b> to provide guidance to end users. The proposed system is capable of identifying several types of errors and {{is used as a}} tool that allows one to make decisions such as sensor replacement and to remove data prior to their application. © 2011 Elsevier B. V. This work was supported by the Spanish National Institute of Agricultural Research (INIA) under Project RTA 04 - 063. Peer Reviewe...|$|R
50|$|A {{database}} (.DBF) file {{is composed}} of a header, <b>data</b> records, deletion <b>flags,</b> and an end-of-file marker. The header contains information about the file structure, and the records contain the actual data. One byte of each record is reserved for the deletion flag.|$|R
