14|10000|Public
40|$|AbstractUsing {{classical}} Weierstrass' result concerning polynomial zeros in {{a suitable}} interval form and some iterative method in floating-point arithmetic, a hybrid method for finding an isolated simple complex zero of a polynomial is established. This method possesses a high computational efficiency and {{the capability of}} automatic <b>determination</b> <b>of</b> <b>error</b> bounds of the approximate solution. A computationally verifiable test {{for the existence of}} a polynomial complex zeros in a given disk is also presented...|$|E
40|$|Bacteriophage T 4 genes 32, 41, 44, 45, 56, and 62 are {{essential}} to DNA replication. Amber mutants (suppressed by su+ 1, su+ 2, or su+ 3 bacteria) in these genes were examined for any mutator or antimutator effects on the reversion of a transition mutation. In every case except for mutations in gene 56, elevated or lowered error frequencies were observed. These results indicate the importance {{of all of the}} replicative proteins in the <b>determination</b> <b>of</b> <b>error</b> frequency...|$|E
40|$|This paper {{explores the}} {{possibility}} of using electrical resistance tomography for the identification of buried objects. The technique requires the solution of both direct and inverse problems, which for noncanonical geometries must be solved by numerical techniques like finite element methods and soft computing. The unknown is the perturbation field, {{a small fraction of the}} source field. Therefore, simulation and error estimation tools adopted in the solution process must be both accurate and reliable. This paper shows that complementary formulations readily allow <b>determination</b> <b>of</b> <b>error</b> bounds for global quantities like the electric resistance...|$|E
40|$|Optical data {{processing}} {{has a significant}} potential in future aerospace systems. In this paper, potential system applications are identified. One {{of the more important}} applications is the <b>determination</b> <b>of</b> <b>errors</b> <b>of</b> large antennas or reflector surfaces and the active control or compensation of the surface. Technological challenges to the application of optical {{data processing}} technology to aerospace systems are identified, and current NASA research efforts are discussed...|$|R
3000|$|Our main {{interest}} here is in <b>determination</b> <b>of</b> <b>errors</b> involving (1) height resolution and (2) wind-velocity accuracy. These {{are discussed in}} the following [...] "Height resolution" [...] and [...] "Velocity measurements" [...] sections, followed by some discussion in the [...] "Discussion" [...] section and finally conclusions in the [...] "Conclusions" [...] section.|$|R
40|$|Abstract − Methods {{enabling}} a <b>determination</b> <b>of</b> <b>errors</b> εI and δI {{for a high}} {{transformation ratio}} difference between a standard and tested instrument current transformers (ICT) are described in this article. A widespread method uses an automatic transformer test set for the measurement of a difference between a standard and tested ICT. A method transforming secondary currents to voltages and an indirect method using error measurement from magnetizing current are also described...|$|R
40|$|The paper {{deals with}} the problem of <b>determination</b> <b>of</b> <b>error</b> {{probability}} of cryptography and safety codes used within the safety-related railway applications with increasing safety integrity level (SIL). In the paper are also described requirements for cryptographic block code and safety linear block code in safety-related communications for railway application. The main part is oriented to the description of mathematical apparatus for the error probability of the cryptography and safety block codes for communication between two safety-related devices across GSM-R communication channel. The practical results are related to the quantitative evaluation of an average error probability of the cryptography and safety codes for several lengths of safety-related messages which are expanded about determination of the cryptography degradation with using GMSK modulation scheme...|$|E
40|$|The {{feedback}} coding {{problem for}} Gaussian systems {{in which the}} noise is neither white nor statistically independent between channels is formulated in terms of arbitrary linear codes at the transmitter and at the receiver. This new formulation is used to determine a number of feedback communication systems. In particular, the optimum linear code that satisfies an average power constraint on the transmitted signals is derived for a system with noiseless feedback and forward noise of arbitrary covariance. The noisy feedback problem is considered and signal sets for the forward and feedback channels are obtained with an average power constraint on each. The general formulation and results are valid for non-Gaussian systems in which the second order statistics are known, the results being applicable to the <b>determination</b> <b>of</b> <b>error</b> bounds via the Chebychev inequality. </p...|$|E
40|$|In {{this work}} we {{illustrate}} our novel quantitative simulation approach for dense amorphous polymer systems, {{as discussed in}} our previous work[Kulkarni et al., A Novel Approach for Lattice Simulations of Polymer Chains in Dense Amorphous Polymer Systems: Method Development and Validation with 2 -D Lattices, arXiV, 2008] in applications involving large lattice sizes and high energetic bias. We first demonstrate how the topology of the microstate ensemble in 2 -D lattices presents a serious challenge for the collection of accurate and reliable quantitative results (i. e., with simultaneous <b>determination</b> <b>of</b> <b>error</b> bars) for large lattices. This necessitates a further enhancement of our Monte Carlo simulation scheme to sample effectively a meaningful 2 -D lattice configurational subspace. Two techniques were investigated: simulated annealing and parallel tempering, to avoid trapping near a local free energy minimum in simulations at high energetic bias. Extensive results of the prediction of various chain conformation statistics and thermodynamic quantities, in the thermodynamic limit (i. e. infinite lateral sizes) are presented. Comment: 19 page...|$|E
40|$|The {{very low}} error {{probability}} obtained with long error-correcting codes {{results in a}} very small number <b>of</b> observed <b>errors</b> in simulation studies of practical size and renders the usual confidence interval techniques inapplicable to the observed error probability. A natural extension of the notion of a 'confidence interval' is made and applied to such <b>determinations</b> <b>of</b> <b>error</b> probability by simulation. An example is included to show the surprisingly great significance of as few as two decoding errors in {{a very large number of}} decoding trials...|$|R
40|$|One of {{the main}} {{problems}} when standard spur gears are in mesh is the appearance of edge contact on the gear tooth surfaces caused by misalignments. Those misalignments are caused partially by deflections of gear supporting shafts. As {{a result of an}} edge contact, a non-favorable condition of the bearing contact occurs, yielding high level of contact stresses. An intensive research and many practical solutions have been directed to modify the gear tooth surfaces in order to avoid edge contact. An innovative procedure is proposed here for: (1) <b>determination</b> <b>of</b> <b>errors</b> <b>of</b> alignment caused by shaft deflections, (2) compensation of predicted shaft deflections during generation of spur gears, and (3), obtaining a favorable function <b>of</b> transmission <b>errors</b> for the design load. A finite element model of a spur gear drive that comprises pinion and gear supporting shafts is used for the <b>determination</b> <b>of</b> <b>errors</b> <b>of</b> alignment along a cycle of meshing. Compensation of misalignments caused by shaft deflections in gear generation is then accomplished by modification of pinion tooth surfaces whereas the gear tooth surfaces are kept unmodified. Additional modifications of pinion tooth surfaces may be required for obtaining a favorable function <b>of</b> transmission <b>errors.</b> The effect <b>of</b> several misalignment compensations in the reduction of contact stresses has been investigated. Postprocessing of load intensity functions and loaded transmission errors is included. The developed approach is illustrated with numerical examples. The authors express their deep gratitude to the Spanish Ministry of Economy and Competitiveness (MINECO) for the financial support of research projects Refs. DPI 2010 - 20388 -C 02 - 01 (financed jointly by FEDER), DPI 2013 - 47702 -C 2 - 1, and DPI 2013 - 47702 -C 2 - 2...|$|R
40|$|The <b>determination</b> <b>of</b> an <b>error</b> {{criterion}} {{which will}} give a sampling rate for adequate performance of linear, time-invariant closed-loop, discrete-data control systems was studied. The proper modelling of the closed-loop control system for characterization <b>of</b> the <b>error</b> behavior, and the <b>determination</b> <b>of</b> an absolute <b>error</b> definition for performance of the two commonly used holding devices are discussed. The definition of an adequate relative error criterion {{as a function of}} the sampling rate and the parameters characterizing the system is established along with the <b>determination</b> <b>of</b> sampling rates. The validity of the expressions for the sampling interval was confirmed by computer simulations. Their application solves the problem of making a first choice in the selection of sampling rates...|$|R
40|$|Fusion {{techniques}} have received considerable attention for achieving lower error rates with biometrics. A fused classifier architecture based on sequential integration of multi-instance and multi-sample fusion schemes allows controlled trade-off between false alarms and false rejects. Expressions {{for each type}} of error for the fused system have previously been derived for the case of statistically independent classifier decisions. It is shown in this paper that the performance of this architecture can be improved by modelling the correlation between classifier decisions. Correlation modelling also enables better tuning of fusion model parameters, ‘N’, the number of classifiers and ‘M’, the number of attempts/samples, and facilitates the <b>determination</b> <b>of</b> <b>error</b> bounds for false rejects and false accepts for each specific user. Error trade-off performance of the architecture is evaluated using HMM based speaker verification on utterances of individual digits. Results show that performance is improved for the case of favourable correlated decisions. The architecture investigated here is directly applicable to speaker verification from spoken digit strings such as credit card numbers in telephone or voice over internet protocol based applications. It is also applicable to other biometric modalities such as finger prints and handwriting samples...|$|E
40|$|This paper {{deals with}} a {{prospective}} approach of modeling, design evaluation and error determination applied to pipelined A/D converter architecture. In contrast with conventional ADC modeling algorithms targeted to extract the maximum ADC non-linearity error, the innovative approach presented allows to decompose magnitudes of individual error sources from a measured or simulated response of an ADC device. Design Evaluation methodology was successfully applied to Nyquist rate cyclic converters in our works [13]. Now, we extend its principles to pipelined architecture. This qualitative decomposition can significantly contribute to the ADC calibration procedure performed on the production line in term of integral and differential nonlinearity. This is backgrounded {{by the fact that}} the knowledge of ADC performance contributors provided by the proposed method helps to adjust the values of on-chip converter components so as to equalize (and possibly minimize) the total non-linearity error. In this paper, the design evaluation procedure is demonstrated on a system design example of pipelined A/D converter. Significant simulation results of each stage of the design evaluation process are given, starting from the INL performance extraction proceeded in a powerful Virtual Testing Environment implemented in Maple™ software and finishing by an error source simulation, modeling of pipelined ADC structure and <b>determination</b> <b>of</b> <b>error</b> source contribution, suitable for a generic process flow...|$|E
40|$|In the {{information}} age, {{it is hard}} to imagine planning and production management without the support of information technologies, because the production process has become more complex and harder to control. Almost every company have a management information system for production monitoring, but for some companies, just monitoring is no longer sufficient, becouse they need more accurate <b>determination</b> <b>of</b> <b>error</b> causes and also need to track embedded products and materials The main objective of the thesis was to create an information solution for monitoring and tracking batches of products in discrete production, which would enable monitoring of input batches of materials and components, as well as other events, that occur in the production process. The module for tracking product lots is an upgrade of an existing information system for monitoring discrete production. Good knowledge of basic information system and manufacturing processes in the discrete production was therefore a prerequisite for making the module. Module for tracking lots is improving data capture and provides quality analysis of the production process. The practical part of the thesis was to design a solution for production tracking that enables data entry via terminals, and setting and reviewing them in a web application. Tracking lots in production makes it easier to discover the cause of bad products, consequently easier to eliminate these causes. Good tracking can eventually eliminate the negative factors of poor raw materials and components, and determine the origin of the material, which is the cause of bad lots. ...|$|E
40|$|Programs for PC's and compatibles provide nonparametric (NPEM) {{population}} pharmacokinetic modeling, BOXES for compartments {{and arrows}} for pathways, to make large kinetic and dynamic models, and clinical software for Bayesian adaptive control of drug dosage regimens, with D-optimal sampling strategies and explicit <b>determination</b> <b>of</b> assay <b>error</b> patterns...|$|R
40|$|Abstract—- Recurrent {{neural network}} (RNN) {{exhibits}} better performance in nonlinear channel equalization problem. In this present work a hybrid model of recurrent neural equalizer configuration {{has been proposed}} where a discrete cosine transform (DCT) block is embedded {{within the framework of}} a conventional RNN structure. The RNN module needs training and involves updation of the connection weights using the standard RTRL algorithm, which necessitates the <b>determination</b> <b>of</b> <b>errors</b> at the nodes of the RNN module. To circumvent this difficulty, an adhoc solution has been suggested to back propagate the output error through this heterogeneous configuration. Performance analysis of the proposed Recurrent Transform Cascaded (RTCS) equalizer for standard communication channel models show encouraging results...|$|R
40|$|The {{development}} of the digital signal processors and their implementation in measuring technique {{has led to the}} manufacturing of power analyzers used as multifunction meters in industry, automation, tests and laboratory activities, monitoring and control of processes, etc. The parameters of a three-phase system can be known if the phase currents, the phase voltages and the phase difference between them can be known. A power analyzer has six inputs for currents and voltages measuring signals. The paper presents a method <b>of</b> <b>determination</b> <b>of</b> <b>errors</b> and uncertainties <b>of</b> electrical quantities measurement using a power analyzer associated with external transducers. The best estimation of measured quantity and uncertainty of measurement are used to report the result of measurement process...|$|R
40|$|The {{techniques}} of Bayesian inference {{have been applied}} with great success to many problems in neural computing including evaluation of regression functions, <b>determination</b> <b>of</b> <b>error</b> bars on predictions, and the treatment of hyper-parameters. However, the problem of model comparison {{is a much more}} challenging one for which current techniques have signicant limitations. In this paper we show how an extended form of Markov chain Monte Carlo, called chaining, is able to provide eective estimates of the relative probabilities of dierent models. We present results from the robot arm problem and compare them with the corresponding results obtained using the standard Gaussian approximation framework. 1 Bayesian Model Comparison In a Bayesian treatment of statistical inference, our state of knowledge of the values of the parameters w in a modelM is described in terms of a probability distribution function. Initially this is chosen to be some prior distribution p(wjM), which can be combined with a likelihood function p(Djw;M) using Bayes ' theorem to give a posterior distribution p(wjD;M) in the form p(wjD;M) = p(Djw;M) p(wjM) p(DjM) (1) where D is the data set. Predictions of the model are obtained by performing integrations weighted by the posterior distribution. The comparison of dierent modelsM i is based on their relative probabilities, which can be expressed, again using Bayes ' theorem, in terms of prior probabilities p(M i to give p(M i jD) p(M j jD) p(Dj...|$|E
40|$|The Tank Safety Screening Data Quality Objective (DQO) {{will be used}} to {{classify}} 149 single shell tanks and 28 double shell tanks containing high-level radioactive waste into safety categories for safety issues dealing with the presence of ferrocyanide, organics, flammable gases, and criticality. Decision rules used {{to classify}} a tank as ``safe`` or ``not safe`` are presented. Primary and secondary decision variables used for safety status classification are discussed. The number and type of samples required are presented. A tabular identification of each analyte to be measured to support the safety classification, the analytical method to be used, the type of sample, the decision threshold for each analyte that would, if violated, place the tank on the safety issue watch list, and the assumed (desired) analytical uncertainty are provided. This is a living document that should be evaluated for updates on a semiannual basis. Evaluation areas consist of: identification of tanks that have been added or deleted from the specific safety issue watch lists, changes in primary and secondary decision variables, changes in decision rules used for the safety status classification, and changes in analytical requirements. This document directly supports all safety issue specific DQOs and additional characterization DQO efforts associated with pretreatment and retrieval. Additionally, information obtained during implementation can assist in resolving assumptions for revised safety strategies, and in addition, obtaining information which will support the <b>determination</b> <b>of</b> <b>error</b> tolerances, confidence levels, and optimization schemes for later revised safety strategy documentation...|$|E
40|$|Summary. The {{extraction}} of fructose, glucose, galactose, sucrose and lactose from aqueous salt solutions, hydrophilic solvents (aliphatic alcohols, alkyl acetates, ketones) of double and triple mixtures has been studied. Under identical conditions set quantitative characteristics extraction has been established. It {{was found that}} from the all studied carbohydrateы most fully extracted disaccharides lactose and sucrose. The conditions of concentration and almost complete recovery of carbohydrates from aqueous salt solutions has beenoptimized. The technique of extraction-potentiometric selective determination of carbohydrates in foods and beverages has been developed. As a titrant was used isopropanol solution of boric acid. The developed method allows to determine separately the mono- or disaccharides in milk, which include those contained 5 or less carbohydrates. The complex of photocolorimetric, polarimetric, potentiometric and chromatographic methods for determining carbohydrates in aqueous media and food (diabetic confectionery, juices, dairy products, honey) wasproposed. To determine the fructose, glucose and sucrose in natural juices us used optical methods (photoelectrocolorimeters, polarimetry). Method is express, does not require expensive equipment and reagents. Fructose and sucrose in diabetic confectionery was determined by ascending thin layer chromatography. Some diabetic products based on fructose, produced by Russian confectionery factorieshas beenanalyzed. Duration analysis, 50 - 60 minutes, selective <b>determination</b> <b>of</b> <b>error</b> within 5 - 7 %. Extracts from honey and milk were analyzed potentiometrically. We have developed a technique characterized by the following advantages compared with state standards: rapidity (analysis time 30 - 35 min), accuracy (relative error within 5 %), does not require expensive equipment and reagents, as well as dilution and filtration of milk stage sampling...|$|E
40|$|Verification of {{the current}} {{measurement}} transformers on the power frequency is taken up. The description for operation principles and constructions of precision current comparator are represented. Comparator provides measurement of relative currents difference of high precision. Measurement chain of device is equilibrated with help of inductive divider. Calculation of the result of measurement is operated with taking of final non-equilibrium signal that permits us to receive high precision of measurements during lessening of inductive divider. The device is elaborated for <b>determination</b> <b>of</b> current transformers <b>error</b> by method <b>of</b> standard transformer comparison. There is also measurement of relative voltage difference in the device. It helps in <b>determination</b> <b>of</b> windings <b>errors</b> <b>of</b> transformers by comparison with a reference winding located on the test transformer...|$|R
30|$|Wave-particle {{interactions}} in a collisionless plasma have been analyzed in several past space science missions but direct and quantitative {{measurement of the}} interactions has not been conducted. We here introduce the Wave-Particle Interaction Analyzer (WPIA) to observe wave-particle interactions directly by calculating the inner product between the electric field of plasma waves and of plasma particles. The WPIA has four fundamental functions: waveform calibration, coordinate transformation, time correction, and interaction calculation. We demonstrate the feasibility of One-chip WPIA (O-WPIA) using a Field Programmable Gate Array (FPGA) as a test model for future science missions. The O-WPIA is capable of real-time processing with low power consumption. We validate {{the performance of the}} O-WPIA including <b>determination</b> <b>of</b> <b>errors</b> in the calibration and power consumption.|$|R
40|$|A new {{approach}} for <b>determination</b> <b>of</b> machine-tool settings for {{spiral bevel gears}} is proposed. The proposed settings provide a predesigned parabolic function <b>of</b> transmission <b>errors</b> and the desired location and orientation of the bearing contact. The predesigned parabolic function <b>of</b> transmission <b>errors</b> is able to absorb piece-wise linear functions <b>of</b> transmission <b>errors</b> that are caused by the gear misalignment and reduce gear noise. The gears are face-milled by head cutters with conical surfaces or surfaces of revolution. A computer program for simulation of meshing, bearing contact and <b>determination</b> <b>of</b> transmission <b>errors</b> for misaligned gear has been developed...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedCurrent theories {{that attempt to}} explain the emission and reflection properties of metallic surfaces still provide some room for conjecture and alternative concepts. This is true particularly for processes in the visible portion of the electromagnetic spectrum. One relatively new theory that has recently received increases attention and support {{is that of the}} "Native cluster" model. The model proposes that metallic surfaces are populated with small groups of atoms that have been liberated from the crystalline lattice structure of the bulk metal. These colloids possess dielectric qualities that act to modify basic properties of the parent material, such as polarizability, electrical conductivity, thermal emission, and luminescence. While proof of luminescence from metallic surfaces would not significantly detract from existing free electron and quantum theory, it would tend to support the "Native cluster" model. Due to its reflectivity characteristics, copper was selected as the metal to be studied in this research. One instrument that is well suited for the collection of reflectivity and emission data is the Optical Multichannel Analyzer. Although a powerful tool for spectral research, the requirement of a significant initial investment of time necessary to gain sufficient user familiarity to become proficient with the equipment has resulted in the instrument being underutilized. Therefore, in addition to the primary aim of this research in evaluating the ability of a polished copper surface to luminescence, a secondary aim was to evaluate the characteristics and applicability of this instrument to support the luminescence research. The results of this research were the development of user friendly checklists for basic operations of the OMA III, a <b>determination</b> <b>of</b> <b>error</b> sources due to experimental equipment and procedures, the magnitude of those errors, substantiation of the results by reproducing known metallic reflectivity data, and the collection of data indicating the possible existence of luminescence from a copper surface. [URL] Commander, United States Nav...|$|E
40|$|The {{combined}} PSF of the BFI and the SOT onboard the Hinode spacecraft is investigated. Observations of the Mercury transit from November 2006 and {{the solar}} eclipse(s) from 2007 {{are used to}} determine the PSFs of SOT for the blue, green, and red continuum channels of the BFI. For each channel large grids of theoretical point spread functions are calculated by convolution of the ideal diffraction-limited PSF and Voigt profiles. These PSFs are applied to artificial images of an eclipse and a Mercury transit. The comparison of the resulting artificial intensity profiles across the terminator and the corresponding observed profiles yields a quality measure for each case. The optimum PSF for each observed image is indicated by the best fit. The observed images of the Mercury transit and the eclipses exhibit a clear proportional relation between the residual intensity and the overall light level in the telescope. In addition there is a anisotropic stray-light contribution. [...] . BFI/SOT operate close to the diffraction limit and have only a rather small stray-light contribution. The FWHM of the PSF is broadened by only ~ 1 % with respect to the diffraction-limited case, while the overall Strehl ratio is ~ 0. 8. In view of the large variations [...] best seen in the residual intensities of eclipse images [...] and the dependence on the overall light level and position in the FOV, a range of PSFs should be considered instead of a single PSF per wavelength. The individual PSFs of that range allow then the <b>determination</b> <b>of</b> <b>error</b> margins for the quantity under investigation. Nevertheless the stray-light contributions are here found to be best matched with Voigt functions with the parameters sigma = 0. " 008 and gamma = 0. " 004, 0. " 005, and 0. " 006 for the blue, green, and red continuum channels, respectively. Comment: 14 pages, 9 figures, accepted by A&...|$|E
40|$|In {{this paper}} we {{introduce}} a universal operator theoretic framework for quantum fault tolerance. This incorporates a top-down approach that implements a system-level criterion based on specification {{of the full}} system dynamics, applied at every level <b>of</b> <b>error</b> correction concatenation. This leads to more accurate <b>determinations</b> <b>of</b> <b>error</b> thresholds than could previously be obtained. This is demonstrated both formally and with an explicit numerical example. The basis for our approach is the Quantum Computer Condition (QCC), an inequality governing {{the evolution of a}} quantum computer. We show that all known coding schemes are actually special cases of the QCC. We demonstrate this by introducing a new, operator theoretic form of entanglement assisted quantum error correction, which incorporates as special cases all known error correcting protocols, and is itself a special case of the QCC. Comment: v 3 : 8 pages, 3 figures, explicit numerical example comparing OQFT to QFT adde...|$|R
30|$|For <b>determination</b> <b>of</b> {{the method}} <b>error,</b> ten {{randomly}} selected cephalograms were measured again {{within a week}} by the same operator. Random errors according to Dahlberg [18] and coefficients of reliability [19] were calculated.|$|R
40|$|Expediency of {{the usage}} of {{resistors}} instead of the measures <b>of</b> capacitance for <b>determination</b> <b>of</b> adaptive <b>error</b> components <b>of</b> phase measurements and tangents of loss angles in selective meters of electrical quantity complex at the mains frequency is substantiated. At {{the same time the}} value range of input current strength is extended and the capability <b>of</b> <b>error</b> estimation for combination of voltage and current at the device's inputs appears. The admissible resistance range of resistors is indicated. Recommendations on application of AC bridge for experimental <b>determination</b> <b>of</b> tangents of resistor phase angles at the mains frequency are give...|$|R
40|$|The CTEQ and MRS parton {{distributions}} {{involve a}} substantial number (~ 30) of parameters that are fit to a large number (~ 900) of data. Typically, these groups produce fits that represent a good fit to the data, {{but there is no}} substantial attempt to determine the errors associated with the fits. <b>Determination</b> <b>of</b> <b>errors</b> would involve consideration of the experimental statistical and systematic errors and also the errors in the theoretical formulas that relate the measured cross sections to parton distributions. We discuss the principles that would be needed in such an error analysis. These principles are standard. However, certain aspects of the principles appear counter-intuitive {{in the case of a}} large number of data. Accordingly, we strive to devote careful attention to the logic behind the methods. Comment: 15 pages, latex, no figure...|$|R
40|$|Abstract — The paper {{presents}} an analytic method capable {{to evaluate the}} transmission errors between two synchros. The method is validated by experimental tests. The system created in this way allows the <b>determination</b> <b>of</b> small <b>errors,</b> like seconds in the transmission of the angular position between a transmitter selsyn and a receiver selsyn. Index Terms — analytic method, selsyn, transmission error...|$|R
40|$|We {{present the}} results of a {{large-scale}} weakly-compressible homogeneous isotropic turbulence simulation. Analysis of the anomalous scaling exponents derived from the simulation are in excellent agreement with both model pre-diction and experimental results, and strongly suggest that the inertial range properties are insensitive to the detailed dissipation mechanism. In addition, we present the foundations <b>of</b> a rigorous <b>determination</b> <b>of</b> the <b>errors</b> <b>of</b> model prediction at finite sample size...|$|R
40|$|We {{explore the}} <b>determination</b> <b>of</b> motion <b>errors</b> <b>of</b> an {{aircraft}} platform from the signum-coded {{synthetic aperture radar}} raw signal (SC-SAR data). The SC-SAR has a one bit representation. In fact it only provides information about the sign of SAR recorded data. Displacement from the line-of-sight (LOS) direction and nominal forward velocity of SAR platform are estimated from this signal by using the reflectivity displacement method (RDM), which allows the motion error determination directly from SAR raw data...|$|R
40|$|The paper {{considers}} a hardware-software complex for research of characteristics of accuracy and noise immunity of a near navigation {{system based on}} pseudolites. The complex is implemented {{on the basis of}} the “National Instruments” hardware platform and “LabView” coding environment. It provides a simulated navigation field, the analysis of the received signals, the <b>determination</b> <b>of</b> the <b>errors</b> <b>of</b> measurement of navigation parameters for pseudolites signals, comparing the measured error with the characteristics of a standard GNSS receiver...|$|R
40|$|In case of {{coordinate}} {{machines that}} use CAA correction matrix, the issue <b>of</b> kinematic <b>errors</b> analysis may {{be based on}} the <b>determination</b> <b>of</b> residual <b>error</b> distribution. Temperature changes have an impact on CMM kinematic structure, which may cause the differences in the map <b>of</b> residual <b>errors.</b> As for today, the residual errors were analysed only for the reference temperature. No research was undertaken on the residual errors changes depending on the temperature variations. This paper presents the experiment aimed at residual errors analysis and resulting errors distributions for different temperatures...|$|R
40|$|The {{processing}} stage {{in which the}} restored values of the physical parameters are received is described. The following main steps are discussed: estimation {{of the state of}} the telemetry data, processing of the calibration data, and <b>determination</b> <b>of</b> the <b>errors</b> in the data; data decommutation and analysis of the structure of measurement cycles for each instrument; decoding, estimates of the reliability of the restored data, and their agreement with the models adopted for the measurement process; and analysis <b>of</b> <b>errors</b> due to deterministic and random factors. A block diagram of the method is presented...|$|R
