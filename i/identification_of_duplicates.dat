5|10000|Public
40|$|A {{major problem}} in the review and {{synthesis}} of citations resulting from multidatabase searching is overlap in coverage among the databases. The ability to identify and eliminate duplicate references from a multidatabase search provides an important service and significant time savings for the end user. The Upjohn Company's Corporate Technical Library has developed software to detect and eliminate duplicates from literature searches captured in electronic format. Methods for the <b>identification</b> <b>of</b> <b>duplicates</b> and the merging and sorting of unique citations are discussed, together with the library's procedures for electronic data capture...|$|E
40|$|Deduplication is {{the process}} of {{determining}} all categories of information within a data set that signify the same real life / world entity. The data gathered from various resources may have data high quality issues in it. The concept to identify duplicates by using windowing and blocking strategy. The objective is to achieve better precision, good efficiency and also to reduce the false positive rate all are in accordance with the estimated similarities of records. Various Similarity metrics are commonly used to recognize the similar field entries. So the main focus {{of this paper is to}} applying appropriate similarity measure on appropriate data to properly identifying the duplicates. De-duplication is a property which provides additional information of similarities between the two entities. Thus, in today‟s data centric environment there are huge numbers of defects in similarity measure. As a result to identify the duplicates is always been a challenging task. In this paper the primary focus is given on exact <b>identification</b> <b>of</b> <b>duplicates</b> in the database by applying concept of windowing & blocking. The objective is to achieve better precision, good efficiency and also to reduce the false positive rate all are in accordance with the estimated similarities of records...|$|E
40|$|Urbanization {{and growing}} {{individual}} mobility are globally active trends that intensify the needs for transportation in cities. In this context, parking space {{has become a}} scarce resource. Drivers searching for open parking spots cause about {{one third of the}} total traffic in urban areas. This creates significant fuel consumption, greenhouse gas emissions and time loss. Intelligent Transportation Systems with particular focus on parking are a promising approach to overcome the information asymmetry and lead drivers directly to available parking spots. This requires highly accurate occupancy data for parking areas on a geographically extended scale. ^ The ultimate goal of this thesis is to improve the modeling of parking occupancy by extraction of meaningful features from raw data in social media. The research focus is set to points of interest and public events in urban areas. First, robust methodologies are developed for the acquisition and benchmarking of largescale social media data. This includes exploratory data analysis and testing of Facebook as a leading platform against alternative online data sources. Here, a multistage approach for the <b>identification</b> <b>of</b> <b>duplicates</b> in heterogeneous data sources is applied. Secondly, a diverse set of feature extraction methodologies is developed that integrates a variety of secondary data sources and findings in the literature. This comprises the adjustment of online popularity attributes for social media objects based on external data and the extraction of parking-related attributes based on text mining. Additionally, historical parking events from Floating Car Data are cross-referenced to thematic similarities among objects and adequate feature sets are derived. This includes the category-specific transformation of historical parking patterns into characteristic time- and object-dependent features. Also, text-based topic modeling using Latent Dirichlet Allocation is applied on social media data to extract thematic object similarities as probabilistic input features for parking demand modeling. In the final evaluation phase, ground truth occupancy data for a selection of off- and on-street locations is used to compare machine learning models trained with varying input feature sets. A baseline and extended set are compared while the latter includes extracted social media features. These models account for the prediction of parking occupancy over different timeframes. Random forest learning machines that include social media features are found to outperform the tested baseline models for both off- and on-street parking demand modeling. Particularly event topic probabilities and category-specific parking events on an hourly basis are identified to be valuable. ...|$|E
40|$|Abstract: The paper {{considers}} a realization of a software application, which performs <b>identification</b> <b>of</b> <b>duplicated</b> records in a database, containing customer information. Some {{of the most}} important works in this direction are overviewed. The selected algorithm is discussed. The problems and appropriate solutions due to handling of multi-language information are listed. The developed application, built using CLIPS engine and rule scripts is debated...|$|R
40|$|Fast {{detection}} of string differences {{is a prerequisite}} for string clustering problems. An example of such a problem is the <b>identification</b> <b>of</b> <b>duplicate</b> information in the data cleansing stage of the data mining process. The relevant algorithms allow the application of large-scale clustering techniques in order to create clusters of similar strings. The vast majority of comparisons, in such cases...|$|R
40|$|This {{research}} {{presents a}} feature recognition algorithm for the automated <b>identification</b> <b>of</b> <b>duplicate</b> geometries in the CAD assembly. The duplicate geometry {{is one of}} the seven indicators of the lazy parts mass reduction method. The lazy parts method is a light weight engineering method that is used for analyzing parts with the mass reduction potential. The duplicate geometry is defined as any geometries lying equal to or within the threshold distance with the user-defined orientation between them and have the percentage similarity that is equal to or greater than the threshold value. The feature recognition system developed in this research for the <b>identification</b> <b>of</b> <b>duplicate</b> geometries is also extended to retrieve the weighted bipartite graph of part connections for the assembly time estimation. The weighted bipartite graph is used as input for the part connectivity based assembly time estimation method. The SolidWorks API software development kit is used in this research to develop a feature recognition system in SolidWorks CAD software package using C++ programming language. The feature recognition system built in the SolidWorks CA...|$|R
40|$|O feijoeiro (Phaseolous vulgaris L.) é uma cultura {{important}}e no aspecto socioeconômico, pois é fonte de proteínas e emprega em seu cultivo mão de obra da agricultura familiar. O Brasil é o maior produtor de grãos de feijão e no estado do Espírito Santo essa cultura assume o terceiro lugar em importância. A produtividade da cultura ainda é baixa, mas pode ser aumentada, corrigindo-se o manejo inadequado ainda adotado e utilizando sementes comerciais adaptadas a cada região. Nesse sentido, torna-se necessário conhecer a diversidade genética existente entre as cultivares selvagens e comerciais para subsidiar programas de melhoramento da cultura adaptada às condições climáticas de regiões específicas. Com objetivo de estimar a divergência genética entre 57 genótipos de feijoeiro foram realizados três experimentos, sendo um no ano agrícola de 2008 / 09 e dois no de 2009 / 2010. Do total de genótipos utilizados, 31 foram resgatados na localidade de Fortaleza, 20 fornecidos pela EMBRAPA e seis cultivares comerciais. Para estimar a diversidade genética foram utilizadas análises morfoagronômicas e moleculares. As análises morfoagronômicas foram feitas, utilizando 19 caracteres quantitativos e 27 qualitativos, por sua vez, nas moleculares, foram utilizados 16 primers de marcadores moleculares SSR e 11 primers ISSR. Os caracteres qualitativos, quantitativos e moleculares mostraram-se eficientes na diferenciação dos genótipos, comprovados pelo valor do coeficiente de correlação cofenética (CCC), sendo que todos foram significativos, a 1 % de probabilidade pelo teste t. O maior e o menor CCC foram observados, respectivamente, nas análises por SSR (0, 923) e ISSR (0, 833). Não foi observada concordância entre os agrupamentos feitos pelos três tipos de caracteres. Nas análises por marcadores SSR, observaram-se 14 grupos, pelas ISSR, 11 e pela combinação dos marcadores, 14. Já pelas análises dos caracteres quantitativos, observou-se a formação de 4 grupos e pelos qualitativos, 11. Sendo essa discordância no número de grupos formados esperada, devido à diferença de avaliação de cada caracter. Verificou-se ampla variabilidade entre os genótipos de feijoeiro estudados. Nos genótipos provenientes da comunidade Fortaleza (locais) foi observada uma significativa diversidade genética, mas entre os genótipos provenientes da EMBRAPA, a diversidade foi estreita. As análises por caracteres quantitativos mostraram que nas cultivares comerciais estudadas houve pequena dissimilaridade com menor valor (24, 96), entre as cultivares Iapar 81 e Iapar 44, representando 1, 08 %. Não houve a identificação de duplicatas genéticas e a correlação de apenas 0, 22 entre as matrizes de dissimilaridade, obtida pelos marcadores SSR e ISSR demonstram que a utilização desses marcadores em conjunto reflete na melhor eficiência no poder discriminatório dos genótipos. A combinação dos marcadores foi mais eficiente na separação dos genótipos, discriminando diferença entre os mesmos, por outro lado, as análises separadas classificaram esses genótipos como iguais geneticamente. A correlação moderada / alta (0, 57) entre os marcadores SSR e os caracteres quantitativos demonstra que a dissimilaridade por meio de marcadores SSR ligados a características de interesse agronômico pode substituir as análises por caracteres quantitativos, já que as análises por esses marcadores não são afetadas pelo ambiente, pois são fáceis, rápidas e requerem menor quantidade de mão de obra e espaço físicoThe bean (Phaseolous vulgaris L.) is {{an important}} crop in the socioeconomic aspect, it {{is a source of}} proteins and employed its cultivation, labor of family farming. The Brazil is the largest producer of beans and in the state, of the Espírito Santo that culture takes third place in importance. The yield is still low, but can be increased by correcting the improper management has adopted and using commercial seed adapted to each region. In this sense, it is necessary to know the genetic diversity between wild and commercial cultivars to support programs to improve the culture adapted to the conditions of specific regions. To estimate the genetic differences among 57 bean genotypes were performed three experiments, one in the agricultural year of 2008 / 09 and two in 2009 / 2010. Of the genotypes, 31 were rescued in the town of Fortaleza, 20 supplied by EMBRAPA and six cultivars. To estimate the genetic diversity were used morphological and molecular analysis. The morphological analysis were made using 19 quantitative characters and 27 qualitative, in turn, the molecular primers were used 16 primers of molecular markers SSR and 11 ISSR primers. The characters qualitative, quantitative and molecular techniques were effective in differentiating the genotypes, as evidenced by the value of cophenetic correlation coefficient (CCC), all of which were significant, at 1 % probability by t test. The highest and lowest CCC were observed, respectively, the analysis by SSR (0. 923) and ISSR (0. 833). There wasn t correlation between the groupings made by the three types of characters. In the analysis by SSR markers, there were 14 groups, the ISSR, 11 and the combination of markers, 14. In the analysis of quantitative characters, we observed the formation of 4 groups and the qualitative, 11. This discrepancy in the number of groups formed expected, due to the difference in valuation of each character. There was wide variation among bean genotypes studied. In the genotypes from community Fortaleza (local) was a significant genetic diversity, but among the genotypes from EMBRAPA, the diversity was narrow. The analysis of quantitative traits showed that the cultivars studied was small dissimilarity with lower value (24. 96) among the cultivars Iapar 81 and lapar 44, representing 1. 08 %. There wasn t <b>identification</b> <b>of</b> <b>duplicates</b> and genetic correlation of only 0. 22 between the dissimilarity matrices obtained by SSR and ISSR markers show that these markers together reflected in greater efficiency in discriminating power of the genotypes. The combination of markers was more efficient in the separation of the genotypes, distinguishing difference between them, on the other, the separate analysis classified the genotypes as genetically identical. The correlation moderate / high (0. 57) between the SSR markers and quantitative traits shows that the dissimilarity using SSR markers linked to desirable agronomic characteristics can replace the analysis of quantitative traits, since the analysis for these markers aren t affected by the environment as they are easy, fast and require less amount of manpower and physical spac...|$|E
40|$|While Brachypodium distachyon (Brachypodium) is an {{emerging}} model for grasses, no expression atlas or gene coexpression network is available. Such tools are of high importance to provide {{insights into the}} function of Brachypodium genes. We present a detailed Brachypodium expression atlas, capturing gene expression in its major organs at different developmental stages. The data were integrated into a large-scale coexpression database (www. gene 2 function. de), enabling <b>identification</b> <b>of</b> <b>duplicated</b> pathways and conserved processes across 10 plant species, thus allowing genome-wide inference of gene function. We highlight {{the importance of the}} atlas and the platform through the <b>identification</b> <b>of</b> <b>duplicated</b> cell wall modules, and show that a lignin biosynthesis module is conserved across angiosperms. We identified and functionally characterised a putative ferulate 5 -hydroxylase gene through overexpression of it in Brachypodium, which resulted in an increase in lignin syringyl units and reduced lignin content of mature stems, and led to improved saccharification of the stem biomass. Our Brachypodium expression atlas thus provides a powerful resource to reveal functionally related genes, which may advance our understanding of important biological processes in grasses...|$|R
5|$|Verma {{started his}} {{research}} career at G. B. Pant University of Agriculture and Technology, Pantnagar, {{where he worked}} on the DNA fingerprinting of Indian scented basmati rice for <b>identification</b> <b>of</b> <b>duplicate</b> accessions. In 1998, Verma was appointed as a scientist at the Centre for DNA Fingerprinting and Diagnostics (CDFD) where he continued his research on the DNA-based identification system, and in 1999, he received the Emerging Forensic Scientist Continental Award from the International Association of Forensic Sciences at the University of California, USA for his work on DNA microsatellite based <b>identification</b> <b>of</b> wild animals.|$|R
40|$|<b>Identification</b> <b>of</b> <b>duplicate</b> {{items in}} {{different}} bibliographic databases {{is one of}} the most important operations to construct large-scale databases for practical use. The PYAT identifiers are general terms for the techniques we developed for effective identification. P, Y, A, T are the components of the identifiers, such as publishing place, year, author name and title. The identification experiment clarified that components of identifiers give different effects on the identification efficiency: the success ratio in identifying duplicates by computer. Therefore, the identification efficiency thus acquired can be evaluated as useful data for composing a more effective identifier...|$|R
40|$|This {{demonstration}} will highlight {{several key}} steps {{in a digital}} curation workflow that incorporates digital forensics tools and methods. Using the open-source BitCurator environment, I will demonstrate several discrete tasks, how they can feed into each other, and considerations related to incorporating them into a larger set of curation practices within collecting institutions. A strong emphasis will be placed on features of the software that have been added or enhanced over the past year, including mounting and exporting of files from forensically packaged disk images, <b>identification</b> <b>of</b> <b>duplicate</b> files, generation <b>of</b> PREMIS metadata and initial steps toward redaction of potentially sensitive information...|$|R
40|$|Abstract- Duplicate bug report {{describes}} {{problems for}} which there is already a report in a bug repository. For many open source projects, the number <b>of</b> <b>duplicate</b> reports represents a significant percentage of the repository, so automatic <b>identification</b> <b>of</b> <b>duplicate</b> reports are very important and need let’s avoid wasting time a triager spends in searching for <b>duplicate</b> bug reports <b>of</b> any incoming report. In this paper we want to present a novel approach which it can help better <b>of</b> <b>duplicate</b> bug report <b>identification.</b> The proposed approach has two novel features: firstly, use n-gram features for the task <b>of</b> <b>duplicate</b> bug report detection. Secondly, apply cluster shrinkage technique to improve the detection performance. We tested our approach on three popular open source projects: Apache, Argo UML, and SVN. We have also conducted empirical studies. The experimental results show that the proposed scheme can effectively improve the detection performance compared with previous methods...|$|R
40|$|Minor update. Improves {{detection}} <b>of</b> <b>duplicate</b> reads with paired end reads, reduces {{run time}} with dedup [...] output-stats {{and a few}} simple debugs. Improved <b>identification</b> <b>of</b> <b>duplicate</b> reads from paired end reads - will now use {{the position of the}} FIRST splice junction in the read (in reference coords) (# 187) Speeds up dedup when running with [...] output-stats - (# 184) Fixes bugs: whitelist [...] set-cell-number [...] plot-prefix -> unwanted error dedup gave non-informative error when input contains zero valid reads/read pairs. Now raises a warning but exits with status 0 (# 190, # 195) count errored if gene identifier contained a ":" (# 198) Renames [...] whole-contig option to [...] buffer-whole-contig to avoid confusion with per-contig option. [...] whole-contig option will still work but will not be visible in documentation (# 196...|$|R
40|$|A {{critical}} task in data cleaning and integration is the <b>identification</b> <b>of</b> <b>duplicate</b> records representing the same real-world entity. A popular approach to duplicate identification employs similarity join to find pairs of similar records {{followed by a}} clustering algorithm to group together records that refer to the same entity. However, the clustering algorithm is strictly used as a post-processing step, which slows down the overall performance and only produces results {{at the end of}} the whole process. In this paper, we propose SjClust, a framework to integrate similarity join and clustering into a single operation. Our approach allows to smoothly accommodating a variety of cluster representation and merging strategies into set similarity join algorithms, while fully leveraging state-of-the-art optimization techniques...|$|R
40|$|The <b>identification</b> <b>of</b> <b>duplicated</b> and plagiarised {{passages}} of text {{has become an}} increasingly active area of research. In this paper we investigate methods for plagiarism detection that aim to identify potential sources of plagiarism from MEDLINE, particularly when the original text has been modified through the replacement of words or phrases. A scalable approach based on Information Retrieval is used to perform candidate document selection - the <b>identification</b> <b>of</b> a subset of potential source documents given a suspicious text - from MEDLINE. Query expansion is performed using the ULMS Metathesaurus to deal with situations in which original documents are obfuscated. Various approaches to Word Sense Disambiguation are investigated to deal with cases where there are multiple Concept Unique Identifiers (CUIs) for a given term. Results using the proposed IR-based approach outperform a state-of-the-art baseline based on Kullback-Leibler Distance...|$|R
40|$|Code cloning is {{not only}} assumed to inflate mainte-nance costs but also {{considered}} defect-prone as inconsistent changes to code duplicates can lead to unexpected behavior. Consequently, the <b>identification</b> <b>of</b> <b>duplicated</b> code, clone detection, {{has been a very}} active area of research in recent years. Up to now, however, no substantial investigation of the consequences of code cloning on program correctness has been carried out. To remedy this shortcoming, this pa-per presents the results of a large-scale case study that was undertaken to find out if inconsistent changes to cloned code can indicate faults. For the analyzed commercial and open source systems we not only found that inconsistent changes to clones are very frequent but also identified a significant number of faults induced by such changes. The clone de...|$|R
40|$|The aim of {{the present}} {{contribution}} is to merge bibliographic data for members of a bounded scientific community in order to derive a complete unified archive, with top-international and nationally oriented production, as a new basis to carry out network analysis on a unified co-authorship network. A two-step procedure is used {{to deal with the}} <b>identification</b> <b>of</b> <b>duplicate</b> records and the author name disambiguation. Specifically, for the second step we strongly drew inspiration from a well-established unsupervised disambiguation method proposed in the literature following a network-based approach and requiring a restricted set of record attributes. Evidences from Italian academic statisticians were provided by merging data from three bibliographic archives. Non-negligible differences were observed in network results in the comparison of disambiguated and not disambiguated data sets, especially in network measures at individual level...|$|R
40|$|Abstract—We {{present an}} {{approach}} to identify duplicate bug reports expressed in free-form text. Duplicate reports needs to be identified to avoid a situation where duplicate reports get assigned to multiple developers. Also, duplicate reports can contain complementary information which can be useful for bug fixing. Automatic <b>identification</b> <b>of</b> <b>duplicate</b> reports (from thousands of existing reports in a bug repository) can increase the productivity of a Triager by {{reducing the amount of}} time a Triager spends in searching for <b>duplicate</b> bug reports <b>of</b> any incoming report. The proposed method uses character N-gram-based model for the task <b>of</b> <b>duplicate</b> bug report detection. Previous approaches are word-based whereas this study investigates the usefulness of low-level features based on characters which have certain inherent advantages (such as natural-language independence, robustness towards noisy data and effective handling of domain specific term variations) over word-based features for the problem <b>of</b> <b>duplicate</b> bug report detection. The proposed solution is evaluated on a publicly-available dataset consisting of more than 200 thousand bug reports from the open-source Eclipse project. The dataset con-sists of ground-truth (pre-annotated dataset having bug reports tagged as duplicate by the Triager). Empirical results and evaluation metrics quantifying retrieval performance indicate that the approach is effective...|$|R
40|$|Information Retrieval (IR) {{approaches}} are nowadays {{used to support}} various software engineering tasks, such as feature location, traceability link recovery, clone detection, or refactoring. However, previous studies showed that inadequate instantiation of an IR technique and underlying process could significantly affect the performance of such approaches in terms of precision and recall. This paper proposes the use of Genetic Algorithms (GAs) to automatically configure and assemble an IR process for software engineering tasks. The approach (named GA-IR) determines the (near) optimal solution {{to be used for}} each stage of the IR process, i. e., term extraction, stop word removal, stemming, indexing and an IR algebraic method calibration. We applied GA-IR on two different software engineering tasks, namely traceability link recovery and <b>identification</b> <b>of</b> <b>duplicate</b> bug reports. The results of the study indicate that GA-IR outperforms approaches previously published in the literature, and that it does not significantly differ from an ideal upper bound that could be achieved by a supervised and combinatorial approach...|$|R
40|$|There {{have been}} many {{attempts}} to study {{the content of the}} Web, either through human or automatic agents. Describes five different previously used Web survey methodologies, each justifiable in its own right, but presents a simple experiment that demonstrates concrete differences between them. The concept of crawling the Web also bears further inspection, including the scope of the pages to crawl, the method used to access and index each page, and the algorithm for the <b>identification</b> <b>of</b> <b>duplicate</b> pages. The issues involved here will be well-known to many computer scientists but, with the increasing use of crawlers and search engines in other disciplines, they now require a public discussion in the wider research community. Concludes that any scientific attempt to crawl the Web must make available the parameters under which it is operating so that researchers can, in principle, replicate experiments or be aware of and take into account differences between methodologies. Also introduces a new hybrid random page selection methodology...|$|R
40|$|Romanian {{hazelnut}} (Corylus avellana) germplasm {{is held in}} {{a national}} collection at SCDP Valcea. A clear situation of the held accessions is necessary for an efficient management of the germplasm collection. In order to achieve this, the genetic variability of 43 accessions was assessed using 23 RAPD primers. The RAPD analysis was carried out as a screening test to confirm the genetic identity of some accessions. Based on the screening results, 12 accessions were selected for analysis using nine SSR primers. A high level of genetic diversity was observed (He= 0. 75, Ho= 0. 81, F=- 0. 061) among the analyzed samples. A genetic similarity matrix was constructed and the resulting UPGMA dendrogram revealed three major groups, corresponding to the geographical origin of the accessions. In order to increase the effectiveness of genebank management, the <b>identification</b> <b>of</b> <b>duplicate</b> and mislabeled accessions {{with the aid of}} molecular markers is of high interest, especially being the first one of this kind in a Romanian hazelnut germplasm collection...|$|R
30|$|The <b>identification</b> <b>of</b> <b>duplicate</b> ads is {{also very}} useful to {{estimate}} the price elasticity of demand. This is the relative difference in demand—in this case, the number of clicks—that {{is caused by a}} relative difference in price. (Clarification: a price revision for an ad and the price elasticity of demand are used as different concepts here. A price revision means that the ad was already online when the price change occurred. The price elasticity of demand is more of a thought experiment: had the ad been posted with a different price, what would the relative difference in clicks be?) The elasticity of demand is an extremely important concept for both businesses and policy. Many companies need to know how changing prices would affect the demand for their goods, and many institutions need causal understanding of the link between prices and demand to implement some policies. For example, a city council may want to start a program of housing subsidies. By subsidizing poor households this policy effectively decreases the price of houses for those households, and its success depends on the effect on demand.|$|R
40|$|Morphological {{characterisation}} of germplasm {{helps in}} the <b>identification</b> <b>of</b> <b>duplicate</b> accessions, unique traits {{and the development}} of core collection. Cryopreservation serves as a complementary conservation method to field and in-vitro methods. This study aimed at evaluating the diversity of Ipomoea batatas landrace collection maintained in tissue culture at the National Plant Genetic Resources Centre (NPGRC) of South Africa. The response of the selected sweet potato accessions to cryopreservation as the complementary conservation method was also tested. Vegetative and storage root characters of the 51 sweet potato accessions were characterized using International Potato Centre descriptors for sweet potato and analyzed with Numerical Taxonomy System-pc software. Encapsulation-vitrification and encapsulation-dehydration protocols were tested for cryopreservation on selected accessions. The unweighted pair-group method using an arithmetic average (UPGMA) defined 22 clusters at a distance coefficient of approximately one. Four Principal Components cumulatively explained 46 percent of the variation, whereas four Principal Coordinate axes, explained 50 percent. Preliminary results presented here show that the NPGRC of South Africa conserves high morphological diversity of sweet potato landraces in the field and tissue culture. But, the results need to be verified with replicates...|$|R
40|$|An {{international}} {{workshop on}} cassava genetic resources was recently held at CIAT. The main {{objectives of the}} meeting were to assess the present status of cassava germplasm conservation and use, and to examine the possibility of establishing a global network for cassava genetic resources. A global conservation strategy was discussed, based on the present status of national and international cassava genetic resources programs. Priorities were set for future germplasm collecting expeditions and sharing of conservation workload among institutions for both cassava and wild Manihot germplasm. The discussion on a global strategy for conservation of genetic resources involved areas such as: studies on genetic diversity, definition <b>of</b> core collections, <b>identification</b> <b>of</b> <b>duplicate</b> accessions, in-vitro and cryopreservation techniques, duplication of germplasm collections in other institutions, sexual seed and pollen gene collections, etc. The existing mechanisms for safe germplasm exchange were evaluated for an effective movement of genetic resources within the network. Other important subjects discussed during the meeting were: the development of data bases for cassava germplasm, human resource development, and {{the integration of the}} germplasm network with other networks (i. e. CBN) and its implications...|$|R
40|$|An {{increasing}} number of market- and technology-driven software development companies {{face the challenge of}} managing several thousands of requirements written in natural language. The large number of requirements causes bottlenecks in the requirements management process and calls for increased efficiency in requirements engineering. This thesis presents results from empirical investigations of using linguistic engineering techniques to alleviate three requirements management activities in large-scale software development: <b>identification</b> <b>of</b> <b>duplicate</b> requirements, linkage <b>of</b> related requirements, and consolidation of different sets of requirements. The activities rely on one common activity: finding requirements that are semantically similar, i. e., refer to the same underlying functionality. Three case studies are presented, in which three different companies, comprising three different requirements management challenges, are investigated. Simulation is used to explore process bottlenecks and two different sets of industrial requirements are used for evaluating suggested solutions. A controlled experiment is also presented, evaluating a new open source support tool for semiautomatic <b>identification</b> <b>of</b> similar requirements. The results show that, for the investigated activities, lexical similarity between requirements may be a sufficient approximation of their semantic similarity. It is also shown that automatic calculation of this similarity may support the activities and give valuable timesavings. The results from the presented research point in one direction: that simple, robust, and costefficient linguistic engineering techniques can give effective support to requirements management activities...|$|R
40|$|Antarctic notothenioids {{radiated}} over {{millions of}} years in subzero waters, evolving peculiar features, such as antifreeze glycoproteins and absence of heat shock response. Icefish, family Channichthyidae, also lack oxygen-binding proteins and display extreme modifications, including high mitochondrial densities in aerobic tissues. A genomic expansion accompanying the evolution of this fish was reported, but paucity of genomic information limits the understanding of notothenioid cold adaptation. We reconstructed and annotated the first skeletal muscle transcriptome of the icefish Chionodraco hamatus providing a new resource for icefish genomics ([URL] We exploited deep sequencing of this energy-dependent tissue to test the hypothesis of selective duplication of genes involved in mitochondrial function. We developed a bioinformatic approach to univocally assign C. hamatus transcripts to orthology groups extracted from phylogenetic trees of five model fish species. C. hamatus duplicates were recorded for each orthology group allowing the <b>identification</b> <b>of</b> <b>duplicated</b> genes specific to the icefish lineage. Significantly more duplicates were found in the icefish when transcriptome data were compared with whole genome data of model fishes species. Indeed, duplicated genes were significantly enriched in proteins with mitochondrial localization, involved in mitochondrial function and biogenesis. In cold conditions and without oxygen-carrying proteins, energy production is challenging. The combination of high mitochondrial densities and the maintenance <b>of</b> <b>duplicated</b> genes involved in mitochondrial biogenesis and aerobic respiration might confer a selective advantage by improving oxygen diffusion and energy supply to aerobic tissues. Our results provide new insights into the genomic basis of icefish cold adaptation...|$|R
40|$|International audienceGene and whole-genome duplications are {{widespread}} in plant nuclear genomes, resulting in sequence heterogeneity. <b>Identification</b> <b>of</b> <b>duplicated</b> genes {{may be particularly}} challenging in highly redundant genomes, especially {{when there are no}} diploid parents as a reference. Here, we developed a pipeline to detect the different copies in the ribosomal RNA gene family in the hexaploid grass Spartina maritima from next-generation sequencing (Roche- 454) reads. The heterogeneity of the different domains of the highly repeated 45 S unit was explored by identifying single nucleotide polymorphisms (SNPs) and assembling reads based on shared polymorphisms. SNPs were validated using comparisons with Illumina sequence data sets and by cloning and Sanger (re) sequencing. Using this approach, 29 validated polymorphisms and 11 validated haplotypes were reported (out of 34 and 20, respectively, that were initially predicted by our program). The rDNA domains of S. maritima have similar lengths as those found in other Poaceae, apart from the 5 ′-ETS, which is approximately two-times longer in S. maritima. Sequence homogeneity was encountered in coding regions and both internal transcribed spacers (ITS), whereas high intragenomic variability was detected in the intergenic spacer (IGS) and the external transcribed spacer (ETS). Molecular cytogenetic analysis by fluorescent in situ hybridization (FISH) revealed the presence of one pair of 45 S rDNA signals on the chromosomes of S. maritima instead of three expected pairs for a hexaploid genome, indicating loss <b>of</b> <b>duplicated</b> homeologous loci through the diploidization process. The procedure developed here may be used at any ploidy level and using different sequencing technologie...|$|R
40|$|International audienceReconstructing synteny blocks is an {{essential}} step in comparative genomics studies. Different methods were already developed to answer various needs such as genome (re-) annotation, <b>identification</b> <b>of</b> <b>duplicated</b> regions and whole genome duplication events or estimation of rearrangement rates. We present SynChro, a tool that reconstructs synteny blocks between pairwise comparisons of multiple genomes. SynChro {{is based on a}} simple algorithm that computes Reciprocal Best-Hits (RBH) to reconstruct the backbones of the synteny blocks and then automatically completes these blocks with non-RBH syntenic homologs. This approach has two main advantages: (i) synteny block reconstruction is fast (feasible on a desk computer for large eukaryotic genomes such as human) and (ii) synteny block reconstruction is straightforward as all steps are integrated (no need to run Blast or TribeMCL prior to reconstruction) and there is only one parameter to set up, the synteny block stringency. Benchmarks on three pairwise comparisons of genomes, representing three different levels of synteny conservation (Human/Mouse, Human/Zebra Finch and Human/Zebrafish) show that Synchro runs faster and performs at least as well as two other commonly used and more sophisticated tools (MCScanX and i-ADHoRe). In addition, SynChro provides the user with a rich set of graphical outputs including dotplots, chromosome paintings and detailed synteny maps to visualize synteny blocks with all homology relationships and synteny breakpoints with all included genetic features...|$|R
40|$|Leptin is a {{key factor}} in the {{regulation}} of food intake and {{is an important factor in}} the pathophysiology of obesity. However, more than a decade after the discovery of leptin in mouse, information regarding leptin in any nonmammalian species is still scant. We report the <b>identification</b> <b>of</b> <b>duplicate</b> leptin genes in common carp (Cyprinus carpio). The unique gene structure, the conservation of both cysteines that form leptin's single disulfide bridge, and stable clustering in phylogenetic analyses substantiate the unambiguous orthology of mammalian and carp leptins, despite low amino acid identity. The liver is a major yet not the only site of leptin expression. However, neither 6 d nor 6 wk of fasting nor subsequent refeeding affected hepatic leptin expression, although the carp predictably shifted from carbohydrate to lipid metabolism. Animals that were fed to satiation grew twice as fast as controls; however, they did not show increased leptin expression at the termination of the study. Hepatic leptin expression did, however, display an acute and transient postprandial increase that follows the postprandial plasma glucose peak. In summary, leptin mRNA expression in carp changes acutely after food intake, but involvement of leptin in the long-term regulation of food intake and energy metabolism was not evident from fasting for days or weeks or long-term feeding to satiation. These are the first data on the regulation of leptin expression in any nonmammalian specie...|$|R
40|$|As {{a result}} of {{improvements}} in genome assembly algorithms and the ever decreasing costs of high-throughput sequencing technologies, new high quality draft genome sequences are published at a striking pace. With well-established methodologies, larger and more complex genomes are being tackled, including polyploid plant genomes. Given the similarity between multiple copies of a basic genome in polyploid individuals, assembly of such data usually results in collapsed contigs that represent a variable number of homoeologous genomic regions. Unfortunately, such collapse is often not ideal, as keeping contigs separate can lead both to improved assembly and also insights about how haplotypes influence phenotype. Here, we describe {{a first step in}} avoiding inappropriate collapse during assembly. In particular, we describe ConPADE (Contig Ploidy and Allele Dosage Estimation), a probabilistic method that estimates the ploidy of any given contig/scaffold based on its allele proportions. In the process, we report findings regarding errors in sequencing. The method can be used for whole genome shotgun (WGS) sequencing data. We also show applicability of the method for variant calling and allele dosage estimation. Results for simulated and real datasets are discussed and provide evidence that ConPADE performs well as long as enough sequencing coverage is available, or the true contig ploidy is low. We show that ConPADE may also be used for related applications, such as the <b>identification</b> <b>of</b> <b>duplicated</b> genes in fragmented assemblies, although refinements are needed...|$|R
40|$|Reconstructing synteny blocks is an {{essential}} step in comparative genomics studies. Different methods were already developed to answer various needs such as genome (re-) annotation, <b>identification</b> <b>of</b> <b>duplicated</b> regions and whole genome duplication events or estimation of rearrangement rates. We present SynChro, a tool that reconstructs synteny blocks between pairwise comparisons of multiple genomes. SynChro {{is based on a}} simple algorithm that computes Reciprocal Best-Hits (RBH) to reconstruct the backbones of the synteny blocks and then automatically completes these blocks with non-RBH syntenic homologs. This approach has two main advantages: (i) synteny block reconstruction is fast (feasible on a desk computer for large eukaryotic genomes such as human) and (ii) synteny block reconstruction is straightforward as all steps are integrated (no need to run Blast or TribeMCL prior to reconstruction) and there is only one parameter to set up, the synteny block stringency [Formula: see text]. Benchmarks on three pairwise comparisons of genomes, representing three different levels of synteny conservation (Human/Mouse, Human/Zebra Finch and Human/Zebrafish) show that Synchro runs faster and performs at least as well as two other commonly used and more sophisticated tools (MCScanX and i-ADHoRe). In addition, SynChro provides the user with a rich set of graphical outputs including dotplots, chromosome paintings and detailed synteny maps to visualize synteny blocks with all homology relationships and synteny breakpoints with all included genetic features. SynChro is freely available under the BSD license at [URL]...|$|R
40|$|Abstract Background Concerted {{evolution}} {{occurs in}} multigene families and {{is characterized by}} stretches of homogeneity and higher sequence similarity between paralogues than between orthologues. Here we identify human gene pairs that have undergone concerted evolution, caused by ongoing gene conversion, {{since at least the}} human-mouse divergence. Our strategy involved the <b>identification</b> <b>of</b> <b>duplicated</b> genes with greater similarity within a species than between species. These genes were required to be present in multiple mammalian genomes, suggesting duplication early in mammalian divergence. To eliminate genes that have been conserved due to strong purifying selection, our analysis also required at least one intron to have retained high sequence similarity between paralogues. Results We identified three human gene pairs undergoing concerted evolution (BMP 8 A/B, DDX 19 A/B, and TUBG 1 / 2). Phylogenetic investigations reveal that in each case the duplication appears to have occurred prior to eutherian mammalian radiation, with exactly two paralogues present in all examined species. This indicates that all three gene duplication events were established over 100 million years ago. Conclusion The extended duration of concerted evolution in multiple distant lineages suggests that there has been prolonged homogenization of specific segments within these gene pairs. Although we speculate that selection for homogenization could have been utilized in order to maintain crucial homo- or hetero- binding domains, it remains unclear why gene conversion has persisted for such extended periods of time. Through these analyses, our results demonstrate additional examples of a process that plays a definite, although unspecified, role in molecular evolution. </p...|$|R
40|$|Data Cleansing is an {{activity}} involving {{a process of}} detecting and correcting the errors and inconsistencies in data warehouse. It deals with <b>identification</b> <b>of</b> corrupt and <b>duplicate</b> data inherent in the data sets of a data warehouse to {{enhance the quality of}} data. The research was directed at investigating some existing approaches and frameworks to data cleansing. That attempted to solve the data cleansing problem and came up with their strengths and weaknesses which led to the <b>identification</b> <b>of</b> gabs in those frameworks and approaches. A comparative analysis of the four frameworks was conducted and by using standard testing parameters a proposed feature was discussed to fit in the gaps...|$|R
40|$|Data {{cleansing}} is {{a process}} that deals with <b>identification</b> <b>of</b> corrupt and <b>duplicate</b> data inherent in the data sets of a data warehouse to enhance the quality of data. This paper aims to facilitate the data cleaning process by addressing the problem <b>of</b> <b>duplicate</b> records detection pertaining to the „name ‟ attributes of the data sets. It provides a sequence of algorithms through a novel framework for identifying duplicity in the „name ‟ attribute of the data sets of an already existing data warehouse. The key features of the research includes its proposal of a novel framework through a well defined sequence of algorithms and refining the application of alliance rules [1] by incorporating the use of previously existing and well defined similarity computation measures. The results depicted show the feasibility and validity of the suggested method...|$|R
30|$|We {{carried out}} {{comparative}} expression analysis {{of rice and}} Arabidopsis PRX family genes which suggests conserved or diversified roles between the two species, leading the <b>identification</b> <b>of</b> tandemly <b>duplicated</b> rice PRX genes in the 1 -CysPrx subgroup, Os 1 -CysPrxA and Os 1 -CysPrxB, differentially expressed in seeds. Os 1 -CysPrxB showed embryo- or root-preferential expression, while Os 1 -CysPrxA showed endosperm-preferential expression. Analysis of the cis-acting regulatory elements (CREs) revealed unique CREs responsible for embryo and root or endospermpreferential expression. In addition, the presence of leaf/shoot-preferential PRXs in rice suggests their evolutional requirement {{to survive in the}} growth environment with a higher light intensity when compared with that of Arabidopsis. Downregulation of two PRXs in the dxr mutant causing an albino phenotype implies that those genes have roles in processing ROS produced during photosynthesis. Predicted protein-protein network associated with four PRXs suggests useful regulatory model for further study.|$|R
30|$|All {{data from}} the {{included}} studies will be extracted using a data extraction form. The form will be pilot tested prior to commencing data extraction. After the <b>identification</b> <b>of</b> studies, <b>duplicates</b> will be removed and two independent reviewers will conduct three levels of screening. Level one of screening is to determine study relevance to the overall objective of the systematic review and this will include title screening. Level two screening will be a title and abstract screen based on the inclusion/exclusion criteria. Level three screening will include reviewing the final group of studies identified by both reviewers as meeting the inclusion criteria. A second reviewer will verify the accuracy of extracted data. Study data quality and study bias risk assessments will be performed. The two independent reviewers will assess the quality of included studies using the appropriate critical appraisal skills programme (CASP) checklist (Milne and Oliver 1996).|$|R
