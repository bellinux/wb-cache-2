84|34|Public
500|$|The game's sound {{team was}} {{composed}} of Kemal Amarasingham, Damin Djawadi and audio director Eric Brosius. According to Brosius, {{each member of the}} audio department did [...] "everything", without clear demarcations between roles. Like Thief, Thief II features a sound engine that simulates propagation in real-time. To achieve this effect, each level's geometry was input both to the level editor and to a [...] "separate [...] database", which mapped how sound would realistically propagate based on [...] "the physical room characteristics [...] how all the different rooms and areas are connected together". For example, noise travels freely through an open door but is blocked when the door is closed. The team used the new [...] "occlusion" [...] feature in EAX 2.0 to make Thief IIs sound environment more realistic and to allow the player to listen through doors. The game features more sound effects, music and speech than the original Thief. Thief IIs score, as with that of its predecessor, was designed to [...] "blur ambient [...] and music" [...] together. However, Brosius later stated that, while Thiefs soundtrack is composed of [...] "simple and hypnotic" [...] loops only a few seconds in length, Thief II features longer and [...] "more thoughtfully" [...] constructed pieces. He believed that this method had positive aspects, but that it resulted in a less <b>immersive</b> <b>audio</b> environment.|$|E
2500|$|... — <b>Immersive</b> <b>audio</b> experiences, SoundLink games {{provided}} narrated voice-acting gameplay to Nintendo {{fans for}} the first time. SoundLink games were often broadcast on strict timing schedules requiring players across the nation to begin {{at the same time}} and end at the same time. For this reason, competition events were quite common among SoundLink games, and prizes were often awarded for contest winners.|$|E
5000|$|Presentation: Audio and Video {{standards}} (to be determined), Ultra HD with High Definition {{and standard}} definition multicast, <b>Immersive</b> <b>Audio</b> ...|$|E
50|$|Auro-3D is an <b>immersive</b> 3D <b>audio</b> format {{developed}} by the Belgium-based company Auro Technologies.|$|R
50|$|Galaxy Studios is {{the home}} of the <b>immersive</b> 3D <b>audio</b> {{technology}} Auro 3D. The concept and formats were developed in 2005 by Wilfried Van Baelen, and in 2010 Auro Technologies was formed by The Van Baelen brothers together with their partner Alfred Schefenacker to develop and promote the technology.|$|R
40|$|This {{research}} investigates {{cognitive and}} behavioural responses that are {{triggered by the}} perceptual focusing within these constructed environments. Space Interface Redux is a live <b>Immersive</b> Visual <b>Audio</b> Jam at the IVT (Immersive Vision Theatre) Saturday 10 th September 7 pm. Collaboration between Eberhard Kranemann/Mathew Emmett + Mike Philips/Luke Christison. Creating poly sensory immersive reality for the fulldome architectural experience. The IVT is a transdisciplinary instrument for the manifestation of material and imaginary worlds...|$|R
50|$|Kyriakakis has {{authored}} and co-authored nearly 100 {{peer reviewed}} technical papers. In 2006 he co-authored the book <b>Immersive</b> <b>Audio</b> Signal Processing.|$|E
50|$|On January 5, 2017, the Ultra HD Forum {{announced}} that their guidelines had been updated to version 1.2 (including things like <b>immersive</b> <b>audio,</b> BT.2100, etc.) and that additional companies have joined which includes Google.|$|E
50|$|Kyriakakis' {{research}} has focused on the intersection of acoustics, psychoacoustics and audio signal processing. Together with his students he published papers on virtual microphones, <b>immersive</b> <b>audio</b> rendering and the characterization and correction of room acoustics.|$|E
5000|$|This [...] "Spatial Multimedia" [...] is the timely {{union of}} digital media {{including}} still photography, motion video, stereo, panoramic imagery sets, <b>immersive</b> media constructs, <b>audio,</b> {{and other data}} with location and date-time information from the GPS and other location designs.|$|R
50|$|FOX Sports Midwest Live!: A year-round {{marketplace}} and event-hosting venue with a retractable canopy. It features a 40-foot LED display with <b>immersive</b> video and <b>audio,</b> is targeted {{to be the}} centralized gathering place in Ballpark Village, and connects the Brew House and Cardinal Nation buildings.|$|R
40|$|Unlike their visual counterparts, <b>immersive</b> spatialized <b>audio</b> {{displays}} {{are highly}} sensitive to {{individual differences in}} the signal processing parameters associated with source placement in azimuth and elevation. We introduce Active Sensory Tuning (AST) as a general framework within which human observers can efficiently search through large design spaces. The application of AST to individualizing spatialized audio displays is demonstrated and its use in {{a broader range of}} auditory data processing and synthesis is discussed. Keywords Spatialized audio, active sensory tuning, individual fitting, exploratory data analysis, perceptual scalin...|$|R
50|$|In 2012, Nagel {{was cast}} in the Temple <b>Immersive</b> <b>Audio</b> AudioDrop {{production}} of Moonie the Starbabe based on Nick Cuti's classic underground comic character. The audio production is scheduled for release in April 2012. Nagel has appeared in other AudioDrop productions, including the popular show Red Colt.|$|E
50|$|Open Wonderland (originally Project Wonderland) is a Java {{open-source}} toolkit {{for creating}} collaborative 3D virtual worlds. Within those worlds, users {{can communicate with}} high-fidelity, <b>immersive</b> <b>audio,</b> share live desktop applications and documents and conduct real business. Open Wonderland is completely extensible; developers and graphic artists can extend its functionality to create entirely new worlds and add new features to existing worlds.|$|E
50|$|In 2004 Tetine {{performed}} at Sonar in São Paulo {{where they were}} also invited to take part at SONARAMA - Sonar' series of multimedia sound installations commissioned for Institute Tomie Ohtake in São Paulo. For SONARAMA Tetine created Turkish Bath - an <b>immersive</b> <b>audio</b> visual 'sauna' made up of four large scale projections depicting 16 stories told by 16 men of various backgrounds in bathing situations.|$|E
40|$|Whilst it is {{possible}} to create exciting, immersive listening experiences with current spatial audio technology, the required systems are generally difficult to install in a standard living room. However, in any living room there is likely to already be a range of loudspeakers (such as mobile phones, tablets, laptops, and so on). device orchestration" (MDO) is the concept of utilising all available devices to augment the reproduction of a media experience. In this demonstration, MDO is used to augment low channel count renderings of various programme material, delivering <b>immersive</b> three-dimensional <b>audio</b> experiences...|$|R
40|$|Presented of the 6 th International Conference on Auditory Display (ICAD), Atlanta, GA, April 2 - 5, 2000 Unlike their visual counterparts, <b>immersive</b> spatialized <b>audio</b> {{displays}} {{are highly}} sensitive to {{individual differences in}} the signal processing parameters associated with source placement in azimuth and elevation. We introduce Active Sensory Tuning (AST) as a general framework within which human observers can efficiently search through large design spaces. The application of AST to individualizing spatialized audio displays is demonstrated and its use in {{a broader range of}} auditory data processing and synthesis is discussed...|$|R
50|$|The Auro-3D {{technology}} {{consists of}} the Auro-3D Engine and a Creative Tool Suite. The engine comprises the Auro-Codec and the Auro-Matic upmixing algorithm to convert legacy content into the Auro-3D format. The Creative Tool Suite {{is a set of}} plugins {{that can be used to}} create native <b>immersive</b> 3D <b>audio</b> content. Auro-3D is fully compatible with all existing production processes and theatre systems, and the format also offers a host of compatibility features such as Single Inventory Distribution (multiple formats are combined in one PCM carrier) and full DCI compliancy.|$|R
50|$|SoundLink Games (サウンドリンクゲーム) — <b>Immersive</b> <b>audio</b> experiences, SoundLink games {{provided}} narrated voice-acting gameplay to Nintendo {{fans for}} the first time. SoundLink games were often broadcast on strict timing schedules requiring players across the nation to begin {{at the same time}} and end at the same time. For this reason, competition events were quite common among SoundLink games, and prizes were often awarded for contest winners.|$|E
50|$|Sol Rezza (born April 7, 1982, Buenos Aires, Argentina) {{is a sound}} artist, radio producer, sound {{designer}} for media and composer. She has been a notable advocate of radio art and radio experimentation in the radio scene in Latin America. While radio is her main medium, she has explored different approaches to spatial resonance via surround sound, performance, and live mixing, to create <b>immersive</b> <b>audio</b> experiences.|$|E
50|$|The Diamond Monster Sound MX300 {{was based}} on the Vortex2 audio ASIC from Aureal Semiconductor. It was a revolutionary step forward in gaming audio, with {{impressive}} 3D audio positioning and other innovative effects. Utilizing the then state-of-the-art Aureal A3D 2.0 3D audio API, the MX300 was capable of producing startlingly <b>immersive</b> <b>audio.</b> Its capability to turn simple stereo speakers into a 3D-audio experience was clearly ahead of the pack for the time, and is unique in its presentation compared to even the renowned and far newer Creative Audigy 2 series.|$|E
40|$|Where {{do we go}} from here? is a video {{installation}} by Atlanta-based artist James O’Donnell {{that attempts}} to provoke others into similar contemplation of that existential question through an <b>immersive</b> video and <b>audio</b> installation exploring the self through references to the internal and external; past and future; and connection and disconnection...|$|R
40|$|The present paper gives a {{practical}} example on how broadcast content {{can be produced}} for MPEG-H. Existing production workflows are investigated with the question in mind, what adaptations are {{required in order to}} make use of <b>audio</b> objects and <b>immersive</b> 3 D <b>Audio</b> provided by the new broadcast standard. After a short introduction to the features of MPEG-H, practical use cases are presented for immersive mixes and interactive personalized audio. In field tests, the tonmeisters who authored the paper attended two sports events and gathered original audio material. Recording methods were tested to see how much additional effort is needed {{to make use of the}} mentioned features. The results show that already existing TV production techniques can be used to provide enough audio material for interactive TV mixes. With little additional effort, <b>immersive</b> 3 D <b>Audio</b> environments can be created...|$|R
40|$|The WAVE project {{proposes a}} {{multidisciplinary}} investigation {{in order to}} create a model-prototype of a virtual <b>immersive</b> instrument using <b>audio,</b> visual technologies, and virtual reality. This model will open up new horizons for the processes involved in music making by dealing not only with relevant technological issues, but especially with meaningful research in theareas of human-machine interaction and sound...|$|R
50|$|Kyriakakis was {{appointed}} to the EE Systems faculty at USC in 1996 where he became the founding director of the USC <b>Immersive</b> <b>Audio</b> Laboratory. He was part of the original team of researchers that founded the Integrated Media Systems Center, a National Science Foundation engineering research center that was awarded to USC in 1996. He later served as the Director of the Computer Interfaces group. He became Deputy Director of IMSC in 2003 and worked closely with the IMSC Director, Adam Clayton Powell III, to achieve the research vision of the center.|$|E
50|$|The Integrated Media Systems Center (IMSC) is on {{the campus}} of the University of Southern California, United States. It was founded using a grant from the US National Science Foundation in 1996 for the study of Integrated Media Systems.The {{original}} mission was focused on the advancement the integration of digital video, <b>immersive</b> <b>audio,</b> text, animation and graphics to transform the way people work, communicate, learn, teach, entertain and play. By utilising cross-disciplinary programs of research and education, the Center has taken multimedia to a new level of technological sophistication.|$|E
5000|$|In 2012, it was {{announced}} that the comic miniseries story [...] "Moonie vs the Spider Queen"would be adapted by Temple of the Cave's Temple <b>Immersive</b> <b>Audio</b> as an AudioDrop, a short-form audio production. It was scheduled for release in April 2012. The production features Tom Nagel and Michael Cornacchia and is scripted and directed by Eric Paul Erickson from the original story by Cuti. Bill Black, a friend and fellow indie movie maker, had suggested to Cuti that instead of trying to produce a high budget version of Moonchild he should do it in episodes and use each episode as a means to finance the next episode. On September 12, 2012, Cuti began shooting the first episode of a three-part movie entitled Moonie and the Spider Queen, Episode One. The movie starred Nikoma DeMitro as Moonie, Anthony Wayne and William August as the space pilots. The shooting by cinematographer, Wheat, was completed on September 15. It was then edited by Randy Carter and composited with special effects by Stuart Scoon. The movie premiered on 16 August 2013 at the Fetish Convention, held in Tampa Bay and was put on sale on Amazon.com.|$|E
50|$|Open Wonderland {{features}} tight {{integration of}} <b>immersive,</b> high-fidelity stereo <b>audio.</b> Using the open source voice bridge, 'jVoiceBridge', {{not only do}} you hear recorded audio in stereo at CD-quality, but you also can hear other live people at this quality. jVoiceBridge adapts to allow remote users with lower bandwidth connections to use lower audio fidelities, including telephone-quality.|$|R
40|$|We {{present the}} i-Cone TM : a new projection-based {{panoramic}} display system for virtual environments. The i-Cone TM uses conical screen geometry, resulting in optimized projector placement {{to create an}} extended workspace for standing participants in afront projection curved screen display with a very large continuous field of view. Improved acoustical properties of the conical screen geometry enable the use of <b>immersive</b> spatial <b>audio.</b> Combined with the excellent visual quality and homogeneity of a curved screen display, these properties make the i-Cone TM an attractive display system for immersive virtual environments, suitable for larger audiences. Interaction paradigms and application examples for this new immersive display system are presented...|$|R
40|$|The Shared Reality Environment is {{an ongoing}} project that explores the use of virtual reality {{technologies}} to achieve realistic computer-mediated human-human interaction. The project integrates <b>immersive</b> displays, spatialized <b>audio,</b> haptics, and gesture recognition, through a minimal latency network architecture. As our primary goal is to provide distributed participants with a convincing sense of co-presence without inhibiting natural, spontaneous interaction, the environment must employ unobtrusive technology wherever possible...|$|R
5000|$|The game's sound {{team was}} {{composed}} of Kemal Amarasingham, Damin Djawadi and audio director Eric Brosius. According to Brosius, {{each member of the}} audio department did [...] "everything", without clear demarcations between roles. Like Thief, Thief II features a sound engine that simulates propagation in real-time. To achieve this effect, each level's geometry was input both to the level editor and to a [...] "separate sound database", which mapped how sound would realistically propagate based on [...] "the physical room characteristics and how all the different rooms and areas are connected together". For example, noise travels freely through an open door but is blocked when the door is closed. The team used the new [...] "occlusion" [...] feature in EAX 2.0 to make Thief IIs sound environment more realistic and to allow the player to listen through doors. The game features more sound effects, music and speech than the original Thief. Thief IIs score, as with that of its predecessor, was designed to [...] "blur ambient sound and music" [...] together. However, Brosius later stated that, while Thiefs soundtrack is composed of [...] "simple and hypnotic" [...] loops only a few seconds in length, Thief II features longer and [...] "more thoughtfully" [...] constructed pieces. He believed that this method had positive aspects, but that it resulted in a less <b>immersive</b> <b>audio</b> environment.|$|E
40|$|Numerous {{applications}} are currently envisioned for <b>immersive</b> <b>audio</b> systems. The principal function of such systems is to synthesize, manipulate, and render sound fields in real time. In this paper, we examine several fundamental and technological limitations that impede {{the development of}} seamless <b>immersive</b> <b>audio</b> systems. Such limitations stem from signal-processing requirements, acoustical considerations, human listening characteristics, and listener movement. We present a brief historical overview to outline the development of <b>immersive</b> <b>audio</b> technologies and discuss the performance and future research directions of <b>immersive</b> <b>audio</b> systems with respect to such limits. Last, we present a novel desktop audio system with integrated listener-tracking capability that circumvents several of the technological limitations faced by today’s digital audio workstations. Keywords—Acoustic signal processing, audio systems, auditory system, multimedia systems, signal processing. I...|$|E
40|$|BINCI - Binaural Tools for the Creative Industries There are {{multiple}} use cases {{as well as}} interpretations about <b>immersive</b> <b>audio.</b> As an aid for understanding BINCI approach of <b>immersive</b> <b>audio</b> we created this first 360 demo video with a basi introduction. You can download this video and watch and hear with Google Cardboard or even better, with Samsung Gear...|$|E
40|$|This paper aims to {{investigate}} the creative uses of scrolling as an interaction method for navigating through sound and music. Mainly focused {{on the use of}} granular synthesis, the paper explores the interaction model and technical chal-lenges and presents a prototype as proof of concept to demon-strate a case in which scrolling can be used to create an <b>immersive</b> and interactive <b>audio</b> experience on the web...|$|R
40|$|This paper {{discusses}} efficient {{digital signal}} processing algorithms for real-time synthesis of dynamically controllable, natural-sounding artificial reverberation. A general modular framework is proposed for configuring a spatial sound processing and mixing system according to the reproduction format or setup and the listening conditions, over loudspeakers or headphones. In conclusion, the implementation and applications of a spatial sound processing software are described, and approaches to control interface design and effective distance effects are reviewed. 1 Introduction <b>Immersive</b> real-time <b>audio</b> requires synthesizing and dynamically controlling directional panning and room reverberation effects over headphones or loudspeakers. Artificial reverberation is necessary to ensure a convincing degree of realism of the virtual sound scene and provide control over the perceived distance of virtual sound sources. Applications of spatial sound processing and artificial reverberation techniq [...] ...|$|R
40|$|Directional Audio Coding (DirAC) is an {{efficient}} technique {{to capture and}} reproduce spatial sound. The analysis step outputs a mono DirAC stream, comprising an omnidirectional microphone pressure signal and side information, i. e., direction of arrival and diffuseness of the sound field expressed in time-frequency domain. This contribution proposes a method to merge two or more mono DirAC streams for a joint playback at the reproduction side. This problem arises in applications such as <b>immersive</b> spatial <b>audio</b> teleconferencing. With respect to a trivial direct merging, the proposed method is more efficient as {{it does not require}} the synthesis step. From this follows the benefit that the loudspeaker setup at the reproduction side {{does not have to be}} known in advance. Simulations and informal listening tests confirm the absence of any artifacts and that the proposed method is practically indistinguishable from the ideal merging...|$|R
