166|281|Public
5000|$|Let [...] be a set {{called the}} <b>instance</b> <b>space</b> or the {{encoding}} {{of all the}} samples. In the character recognition problem, the <b>instance</b> <b>space</b> is [...] In the interval problem the <b>instance</b> <b>space,</b> , is the set of all bounded intervals in , where [...] denotes the set of all real numbers.|$|E
5000|$|Here [...] is a {{real number}} that is called the {{threshold}}. Together with the weights, the threshold defines a dividing hyperplane in the <b>instance</b> <b>space.</b> Good bounds are obtained if [...] (see below).|$|E
50|$|William Bardsley, physicist. His work at Malvern {{on growing}} {{crystals}} {{was published in}} a series of papers that have been referenced over 200 times through the time of writing (2010), in work on semiconductor devices and, in one <b>instance,</b> <b>space</b> science.|$|E
50|$|The {{idea of the}} {{practice}} is that the person reserves the space from which they have freed their vehicle for future parking during {{the remainder of the}} storm and as long as snow remains on the ground. It is generally a Lockean recognition that the effort of the physical exertion of digging provides an entitlement to the space where the vehicle was previously located. But in some <b>instances,</b> <b>spaces</b> get reserved in this fashion even before a snowstorm starts.|$|R
5000|$|Missions will {{be given}} out by NPCs but will not be static. What {{missions}} are available and even the access to the NPCs themselves are subject to how the battlefield is going. Some may be specific to control points that the player will need to reclaim from the Bane to gain access again. Missions are also to have multiple options to take. One example is destroying a dam to stop Bane forces that will also demolish a local village. A player can choose to just destroy or try to warn the village beforehand risking further advances by the Bane. Referred to as [...] "ethical parables" [...] they are to make up about 20% of all missions. The missions the player chooses to do and the choices made during them will change the way certain NPCs treat the player's character.Some missions will deliver the player's character to private <b>instanced</b> <b>spaces.</b> One design goal {{of the game is}} to use <b>instanced</b> <b>spaces</b> to create in-depth storytelling, with puzzles, traps, and NPCs, that would be more difficult in shared spaces.Some missions will be ethically challenging. The players will have to choose from different points of view and it can alter their future progress. [...] "Ethical and moral dilemmas are something we definitely wanted to incorporate into the design of Tabula Rasa from the very start. The entire goal is to give you pause and allow you to think about the choices that they make in order to accomplish a mission." ...|$|R
50|$|Landfill mining is also {{possible}} in countries where land is not available for new landfill sites. In this <b>instance</b> landfill <b>space</b> can be reclaimed by the extraction of biodegradable waste and other substances then refilled with wastes requiring disposal.|$|R
5000|$|Instance ranking {{also has}} the <b>{{instance}}</b> <b>space</b> [...] and label set [...] In this task, labels are defined to have a fixed order [...] and each instance [...] {{is associated with a}} label [...] Giving a set of instances as training data, the goal of this task is to find the ranking order for a new set of instances.|$|E
5000|$|The basic algorithm, Winnow1, is as follows. The <b>{{instance}}</b> <b>space</b> is , that is, {{each instance}} {{is described as}} a set of Boolean-valued features. The algorithm maintains non-negative weights [...] for , which are initially set to 1, one weight for each feature. When the learner is given an example , it applies the typical prediction rule for linear classifiers: ...|$|E
5000|$|In label ranking, {{the model}} has an <b>instance</b> <b>space</b> [...] and a finite set of labels [...] The {{preference}} information {{is given in}} the form [...] indicating instance [...] shows preference in [...] rather than [...] A set of preference information is used as training data in the model. The task of this model {{is to find a}} preference ranking among the labels for any instance.|$|E
30|$|Synergetic {{effect in}} photocatalysis, {{discussed}} {{at the beginning of}} this work, is interpreted by some authors [9] as a result of specific electronic band alignment of anatase and rutile crystals leading to, for <b>instance,</b> successful <b>space</b> separation of photoexcited electron-hole pairs.|$|R
40|$|We {{study the}} {{relationships}} between Gateaux, weak Hadamard and Frechet differentiability and their bornologies for Lipschitz and for convex functions. In particular, Frechet and weak Hadamard differentiabily coincide for all Lipschitz functions {{if and only if}} the space is reflexive (an earlier paper of the first two authors shows that these two notions of differentiability coincide for continuous convex functions if and only if the space does not contain a copy of ℓ_ 1). We also examine when Gateaux and weak Hadamard differentiability coincide for continuous convex functions. For <b>instance,</b> <b>spaces</b> with the Dunford-Pettis (Schur) property can be characterized by the coincidence of Gateaux and weak Hadamard (Frechet) differentiabilty for dual norms...|$|R
50|$|A typical royal diploma had {{a clause}} {{describing}} {{the boundaries of}} the territory {{that is the subject of}} the charter. There are also boundary descriptions in a number of leases and two wills. In the earliest examples, these boundary descriptions are short, in Latin and with few boundary points. In time, the descriptions became longer, more detailed and written in Old English. By the end of the 9th century, all boundary clauses were written in Old English. Many charters, particularly those that have survived in later copies, do not have boundary clauses. In some <b>instances,</b> <b>space</b> has been left for a boundary clause that was never copied. A few boundary descriptions survive that do not appear to be related to any surviving charter.|$|R
5000|$|Another {{example of}} {{parameter}} adjustment is hierarchical classification (sometimes {{referred to as}} <b>instance</b> <b>space</b> decomposition [...] ), which splits a complete multi-class problem into a set of smaller classiﬁcation problems. It serves for learning more accurate concepts due to simpler classiﬁcation boundaries in subtasks and individual feature selection procedures for subtasks. When doing classiﬁcation decomposition, the central choice is the order of combination of smaller classiﬁcation steps, called the classiﬁcation path. Depending on the application, it {{can be derived from}} the confusion matrix and, uncovering the reasons for typical errors and finding ways to prevent the system make those in the future. For example, on the validation set one can see which classes are most frequently mutually confused by the system and then the <b>instance</b> <b>space</b> decomposition is done as follows: firstly, the classification is done among well recognizable classes, and the difficult to separate classes are treated as a single joint class, and finally, as a second classification step the joint class is classified into the two initially mutually confused classes.|$|E
50|$|Ideally, the stamps used {{relate to}} the celebration (for <b>instance,</b> <b>space</b> stamps for a shuttle launch). Cancels are either {{obtained}} {{in the city of}} the event (such as Kitty Hawk for the Wright Brothers first flight anniversary) or, for larger quantities of envelopes, from a special cancellation unit maintained by the Postal Service in Kansas City, Missouri. If a special cancellation (one with a design) has been created for the event, you can generally only receive that cancel from the local post office.|$|E
50|$|Grammar {{induction}} (or grammatical inference) is {{the process}} in machine learning of learning a formal grammar (usually {{as a collection of}} re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the <b>instance</b> <b>space</b> consists of discrete combinatorial objects such as strings, trees and graphs.|$|E
40|$|No {{convenient}} internal {{characterization of}} spaces that are productively Lindelöf is known. Perhaps the best general result known is Alsterʼs internal characterization, under the Continuum Hypothesis, of productively Lindelöf spaces {{which have a}} basis of cardinality at most ℵ 1. It turns out that topological spaces having Alsterʼs property are also productively weakly Lindelöf. The weakly Lindelöf spaces form a much larger class of spaces than the Lindelöf <b>spaces.</b> In many <b>instances</b> <b>spaces</b> having Alsterʼs property satisfy a seemingly stronger version of Alsterʼs property and consequently are productively X, where X is a covering property stronger than the Lindelöf property. This paper examines the question: When is it the case that a space that is productively X is also productively Y, where X and Y are covering properties related to the Lindelöf property...|$|R
40|$|The {{fundamental}} question studied {{in this thesis}} is how to evaluate and analyse supervised learning algorithms and classifiers. As a first step, we analyse current evaluation methods. Each method is described and categorised according {{to a number of}} properties. One conclusion of the analysis is that performance is often only measured in terms of accuracy, e. g., through cross-validation tests. However, some researchers have questioned the validity of using accuracy as the only performance metric. Also, the number of instances available for evaluation is usually very limited. In order to deal with these issues, measure functions have been suggested as a promising approach. However, a limitation of current measure functions is that they can only handle two-dimensional <b>instance</b> <b>spaces.</b> We present the design and implementation of a generalised multi-dimensiona...|$|R
50|$|Nontraditional {{settings}} provide cultural {{access as}} well as access to multiple generations. These spaces are often seen as a bridge point where multiple generations gather and where cultural values and traditions are learned or passed on. In many <b>instances</b> these <b>spaces</b> are inclusive of non-English speaking members of the community.|$|R
50|$|Because of {{the high}} {{dimensionality}} of the new feature space {{and the cost of}} explicitly enumerating all APRs of the original <b>instance</b> <b>space,</b> GMIL-1 is inefficient both in terms of computation and memory. GMIL-2 was developed as a refinement of GMIL-1 in an effort to improve efficiency. GMIL-2 pre-processes the instances to find a set of candidate representative instances. GMIL-2 then maps each bag to a Boolean vector, as in GMIL-1, but only considers APRs corresponding to unique subsets of the candidate representative instances. This significantly reduces the memory and computational requirements.|$|E
5000|$|Another common {{approach}} is to consider the geometry of the bags themselves as metadata. This is the approach taken by the MIGraph and miGraph algorithms, which represent each bag as a graph whose nodes are the instances in the bag. There is an edge between two nodes if the distance (up to some metric on the <b>instance</b> <b>space)</b> between the corresponding instances is less than some threshold. Classification is done via an SVM with a graph kernel (MIGraph and miGraph only differ in their choice of kernel). Similar approaches are taken by MILES [...] and MInD. MILES represents a bag by its similarities to instances in the training set, while MInD represents a bag by its distances to other bags.|$|E
5000|$|The {{probably}} approximately correct (PAC) {{model was}} applied by D. Roth (2002) to solve computer vision problem {{by developing a}} distribution-free learning theory based on this model. This theory heavily relied {{on the development of}} feature-efficient learning approach. The goal of this algorithm is to learn an object represented by some geometric features in an image. The input is a feature vector and the output is 1 which means successfully detect the object or 0 otherwise. The main point of this learning approach is collecting representative elements which can represent the object through a function and testing by recognising an object from image to find the representation with high probability. The learning algorithm aims to predict whether the learned target concept [...] is belongs to a class, where X is the <b>instance</b> <b>space</b> consists with parameters and then test whether the prediction is correct.|$|E
40|$|AbstractWe study a {{model of}} {{probably}} exactly correct (PExact) learning that can be viewed either as the Exact model (learning from equivalence queries only) relaxed so that counterexamples to equivalence queries are distributionally drawn rather than adversarially chosen or as the probably approximately correct (PAC) model strengthened to require a perfect hypothesis. We also introduce {{a model of}} probably almost exactly correct (PAExact) learning that requires a hypothesis with negligible error and thus lies between the PExact and PAC models. Unlike the Exact and PExact models, PAExact learning is applicable to classes of functions defined over infinite <b>instance</b> <b>spaces.</b> We obtain a number of separation results between these models. Of particular note are some positive results for efficient parallel learning in the PAExact model, which stand {{in stark contrast to}} earlier negative results for efficient parallel Exact learning...|$|R
40|$|A {{new method}} for {{performing}} a nonlinear form of Principal Component Analysis is proposed. By {{the use of}} integral operator kernel functions, one can efficiently compute principal components in high [...] dimensional feature spaces, related to input space by some nonlinear map; for <b>instance</b> the <b>space</b> of all possible d [...] pixel products in images...|$|R
50|$|Contra-tiempo palmas {{is a way}} of {{clapping}} {{between the}} normal beats in a bar. For <b>instance,</b> filling the <b>space</b> between beats with another beat or clap.|$|R
40|$|AbstractWe prove a new {{combinatorial}} {{characterization of}} polynomial learnability from equivalence queries, and state {{some of its}} consequences relating the learnability of a class with the learnability via equivalence and membership queries of its subclasses obtained by restricting the <b>instance</b> <b>space.</b> Then we propose and study two models of query learning {{in which there is}} a probability distribution on the <b>instance</b> <b>space,</b> both as an application of the tools developed from the combinatorial characterization and as models of independent interest...|$|E
30|$|In {{this paper}} we {{describe}} {{and evaluate the}} performance of d-Confidence [19]. D-Confidence is an active learning approach that tends to explore unseen regions in <b>instance</b> <b>space,</b> thus selecting instances from unseen classes faster—with fewer queries—than traditional active learning approaches. D-Confidence selects queries based on a criterion that aggregates the posterior classifier confidence and the distance between queries and known classes. Confidence [4] and distance, farthest-first [23], are traditional active learning criteria. D-Confidence is biased towards instances that {{do not belong to}} known classes (low confidence) and that are located in unseen areas in <b>instance</b> <b>space</b> (high distance to known classes).|$|E
40|$|Constraint {{satisfaction}} {{problems are}} {{of special interest}} for the artificial intelligence and operations research community due to their many applications. Although heuristics involved in solving these problems have largely been studied in the past, {{little is known about}} the relation between instances and the respective performance of the heuristics used to solve them. This paper focuses on both the exploration of the <b>instance</b> <b>space</b> to identify relations between instances and good performing heuristics and how to use such relations to improve the search. Firstly, the document describes a methodology to explore the <b>instance</b> <b>space</b> of constraint satisfaction problems and evaluate the corresponding performance of six variable ordering heuristics for such instances in order to find regions on the <b>instance</b> <b>space</b> where some heuristics outperform the others. Analyzing such regions favors the understanding of how these heuristics work and contribute to their improvement. Secondly, we use the information gathered from the first stage to predict the most suitable heuristic to use according to the features of the instance currently being solved. This approach proved to be competitive when compared against the heuristics applied in isolation on both randomly generated and structured instances of constraint satisfaction problems...|$|E
50|$|Permanent {{bollards}} {{intended for}} traffic-control purposes may be mounted {{near enough to}} each other that they block ordinary cars, for <b>instance,</b> but <b>spaced</b> widely enough to permit special-purpose vehicles and bicycles to pass through. Bollards may {{also be used to}} enclose car-free zones. Bollards and other street furniture are used to control overspill parking onto sidewalks and verges.|$|R
50|$|The unique {{aspect of}} {{gameplay}} in Achron {{is the fact}} that the game proceeds not only in many <b>instances</b> of <b>space,</b> but also in many instances of time. Players can simultaneously and independently play in the past, present, or future. The player can only travel a certain distance into the past - after a while, the timeline becomes permanent.|$|R
40|$|The main {{objective}} {{of this study is}} to uncover the differences in the programming behavior between methodology designers and methodology users. We conducted an experiment with methodology designers who have invented one of the major object-oriented methodologies and programmers who have used the methodology for their projects. Concurrent verbal protocols were analyzed based on a theoretical framework which views programming as search in four problem spaces: representation, rule, instance, and paradigm. In programming, the main problem spaces are the representation and the rule spaces, while the paradigm and <b>instance</b> <b>spaces</b> are the supporting spaces. The results of the experiment show that differences in the supporting space produced different search behavior in the main problem spaces, which in turn resulted in different final programs and performance. 1. INTRODUCTION The world we live in today is much more a man-made, artificial, world than it is a natural world (Dasgupta, 1994; Sim [...] ...|$|R
30|$|To {{prevent this}} {{behavior}} and {{to direct the}} learner to low confidence instances but also to unexplored regions in <b>instance</b> <b>space,</b> the d-Confidence value of a point is high {{in the neighborhood of}} known instances decreasing with the distance to those (Fig.  3 b).|$|E
40|$|Two of {{the most}} popular {{approaches}} to induction are instance-based learning (IBL) and rule generation. Their strengths and weaknesses are largely comple-mentary. IBL methods are able to identify small details in the <b>instance</b> <b>space,</b> but have trouble with attributes that are relevant {{in some parts of the}} space but not oth-ers. Conversely, rule induction methods may overlook small exception regions, but are able to select different attributes in different parts of the <b>instance</b> <b>space.</b> The two methods have been unified in the RISE algorithm (Domingos 1995). RISE views instances as maximally specific rules, forms more general rules by gradually clustering instances of the same class, and classifies a test example by letting the nearest rule win. This approach potentially combines the advantages of rul...|$|E
30|$|These {{preliminary}} results were extended in [19]. This paper describes a systematic {{approach to the}} evaluation of d-Confidence. It is based on artificial data and focused on comparing the performance of d-Confidence to that of confidence w.r.t. {{the coverage of the}} <b>instance</b> <b>space.</b> Two-dimensional artificial datasets have been generated to exhibit a set of properties describing global dataset characteristics: cluster alignment, label distribution, cluster morphism and cluster separability. All these properties were defined as binary. Sixteen artificial datasets have been generated covering all the combinations of these four binary meta-descriptors expecting to simulate a wide range of real datasets’ structures arising in classification tasks. The empirical results showed that d-Confidence selects queries from remote regions—where the density of known (labeled) instances is sparse—more efficiently than confidence. <b>Instance</b> <b>space</b> is covered more efficiently when using d-Confidence, thus creating conditions to identify representative cases from unknown classes earlier. On average, a 100  % coverage of the <b>instance</b> <b>space</b> is achieved by d-Confidence with a fraction of the effort required by confidence. Regarding the global properties of the datasets, d-Confidence performed clearly better than confidence on “well behaved” datasets (balanced, collinear, isomorphic and separable). On not so well behaved datasets, d-Confidence also performs better than confidence but not as clearly, especially with respect to the classification error.|$|E
5000|$|... {{where the}} {{bilinear}} form on V is denoted [...] , [...] (for <b>instance,</b> in Euclidean <b>space</b> v, w = v ⋅ w is the dot product of v and w).|$|R
25|$|Several {{important}} {{spaces in}} functional analysis, for <b>instance</b> the <b>space</b> of all infinitely often differentiable functions R → R, or {{the space of}} all distributions on R, are complete but are not normed vector spaces and hence not Banach spaces. In Fréchet spaces one still has a complete metric, while LF-spaces are complete uniform vector spaces arising as limits of Fréchet spaces.|$|R
40|$|This paper {{develops}} a theory for learning scenarios where multiple learners co-exist {{but there are}} mutual coherency constraints on their outcomes. This is natural in cognitive learning situations, where "natural" constraints are imposed on the outcomes of classifiers so that a valid sentence, image or any other domain representation is produced. We formalize these learning situations, after a model suggested in (Roth & Zelenko, 2000) and study generalization abilities of learning algorithms under these conditions in several frameworks. We show that the mere existence of coherency constraints, even without the learner's awareness of them, deems the learning problem easier than predicted by general theories and explains the ability to generalize well from a fairly small number of examples. In particular, it is shown that within this model one can develop an understanding to several realistic learning situations such as highly biased training sets and low dimensional data that is embedded in high dimensional <b>instance</b> <b>spaces.</b> 1...|$|R
