88|10000|Public
25|$|In particular, fib(2) was {{calculated}} three times from scratch. In larger examples, many more values of fib, or subproblems, are recalculated, {{leading to an}} <b>exponential</b> <b>time</b> <b>algorithm.</b>|$|E
2500|$|Since an algorithm's {{performance}} time may vary with different inputs {{of the same}} size, one commonly uses the worst-case time complexity of an algorithm, denoted as T(n), which {{is defined as the}} maximum amount of time taken on any input of size n. Less common, and usually specified explicitly, is the measure of average-case complexity. Time complexities are classified {{by the nature of the}} function T(n). For instance, an algorithm with T(n) = O(n) is called a linear time algorithm, and an algorithm with [...] for some constant [...] is said to be an <b>exponential</b> <b>time</b> <b>algorithm.</b>|$|E
5000|$|There is {{an exact}} <b>exponential</b> <b>time</b> <b>algorithm</b> for the Grundy number that runs in time [...]|$|E
40|$|In {{this article}} we survey {{algorithmic}} lower bound results that have been obtained {{in the field of}} exact <b>exponential</b> <b>time</b> <b>algorithms</b> and pa-rameterized complexity under certain assumptions on the running <b>time</b> of <b>algorithms</b> solving CNF-Sat, namely <b>Exponential</b> <b>time</b> hypothesis (ETH) and Strong <b>Exponential</b> <b>time</b> hypothesis (SETH). ...|$|R
25|$|An {{algorithm}} {{is said to}} be <b>exponential</b> <b>time,</b> if T(n) is upper bounded by 2poly(n), where poly(n) is some polynomial in n. More formally, an <b>algorithm</b> is <b>exponential</b> <b>time</b> if T(n) is bounded by O(2n'k) for some constant k. Problems which admit <b>exponential</b> <b>time</b> <b>algorithms</b> on a deterministic Turing machine form the complexity class known as EXP.|$|R
5000|$|Iterative {{compression}} {{has been}} used successfully in many problems, for instance odd cycle transversal (see below) and edge bipartization, feedback vertex set, cluster vertex deletion and directed feedback vertex set. [...] It has also been used successfully for exact <b>exponential</b> <b>time</b> <b>algorithms</b> for independent set.|$|R
5000|$|In particular, [...] was {{calculated}} three times from scratch. In larger examples, many more values of , or subproblems, are recalculated, {{leading to an}} <b>exponential</b> <b>time</b> <b>algorithm.</b>|$|E
5000|$|Although {{very simple}} {{this method is}} very inefficient: for the simple example {{considered}} here, [...] integers (including [...] ) have to be checked for finding the solution [...] This is an <b>exponential</b> <b>time</b> <b>algorithm,</b> as {{the size of the}} input is, up to a constant factor, the number of digits of , and the average number of operations is of the order of [...]|$|E
5000|$|The Varshamov {{construction}} {{above is}} not explicit; that is, {{it does not}} specify the deterministic method to construct the linear code that satisfies the Gilbert-Varshamov bound. The naive {{way that we can}} do is to go over all the generator matrices [...] of size [...] over the field [...] and check if that linear code has the satisfied Hamming distance. That leads to the <b>exponential</b> <b>time</b> <b>algorithm</b> to implement it.|$|E
40|$|AbstractRecently, E. A. Emerson and C. S. Jutla (SIAM J. Comput., 1999), have {{successfully}} applied complexity of tree automata to obtain optimal deterministic <b>exponential</b> <b>time</b> <b>algorithms</b> for some important modal logics of programs. The running <b>time</b> of these <b>algorithms</b> corresponds, of course, to complexity functions which are potential functions and, thus, {{they do not}} belong, in general, to any dual p-complexity space. Motivated by these facts we here introduce and study a very general class of complexity spaces, which provides, in the dual context, a suitable framework {{to carry out a}} description of the complexity functions that generate <b>exponential</b> <b>time</b> <b>algorithms.</b> In particular, such spaces can be modelled as biBanach semialgebras which are isometrically isomorphic to the positive cone of the asymmetric normed linear space consisting of bounded sequences of real numbers endowed with the supremum asymmetric norm...|$|R
40|$|We {{present in}} this note a rather {{new way to}} cope with {{polynomial}} inapproximability of NP-hard problems. We study approximation of min coloring by moderately <b>exponential</b> <b>time</b> <b>algorithms,</b> able to achieve approximation ratios unachievable in polynomial time for min coloring by algorithms with provably upper complexity bounds, better than those of exact resolution...|$|R
40|$|Recent {{times have}} seen quite some {{progress}} {{in the development of}} "efficient" <b>exponential</b> <b>time</b> <b>algorithms</b> for NP-hard problems. These results are also tightly related to the so-called theory of fixed parameter tractability. In this incomplete, personally biased survey, we reflect on some recent developments and prospects in the field of fixed parameter algorithms...|$|R
5000|$|In some {{theoretical}} analyses [...] "difficult" [...] has {{a specific}} mathematical meaning, such as [...] "not solvable in asymptotic polynomial time". Such interpretations of difficulty {{are important in}} the study of provably secure cryptographic hash functions but do not usually have a strong connection to practical security. For example, an <b>exponential</b> <b>time</b> <b>algorithm</b> can sometimes still be fast enough to make a feasible attack. Conversely, a polynomial time algorithm (e.g., one that requires n20 steps for n-digit keys) may be too slow for any practical use.|$|E
50|$|Pollard {{gives the}} time {{complexity}} of the algorithm as , based on a probabilistic argument which follows from the assumption that f acts pseudorandomly. Note that when {{the size of the}} set {a, …, b} to be searched is measured in bits, as is normal in complexity theory, the set has size log(b&thinsp;&minus;&thinsp;a), and so the algorithm's complexity is , which is exponential in the problem size. For this reason, Pollard's lambda algorithm is considered an <b>exponential</b> <b>time</b> <b>algorithm.</b> For an example of a subexponential time discrete logarithm algorithm, see the index calculus algorithm.|$|E
5000|$|Since an algorithm's {{performance}} time may vary with different inputs {{of the same}} size, one commonly uses the worst-case time complexity of an algorithm, denoted as T(n), which {{is defined as the}} maximum amount of time taken on any input of size n. Less common, and usually specified explicitly, is the measure of average-case complexity. Time complexities are classified {{by the nature of the}} function T(n). For instance, an algorithm with T(n) = O(n) is called a linear time algorithm, and an algorithm with [...] for some constant [...] is said to be an <b>exponential</b> <b>time</b> <b>algorithm.</b>|$|E
40|$|Abstract—The {{field of}} exact <b>exponential</b> <b>time</b> <b>algorithms</b> for NP-hard {{problems}} has thrived {{over the last}} decade. While exhaustive search remains asymptotically the fastest known algorithm for some basic problems, difficult and non-trivial <b>exponential</b> <b>time</b> <b>algorithms</b> have been found for a myriad of problems, including GRAPH COLORING, HAMILTONIAN PATH, DOMINATING SET and 3 -CNF-SAT. In some instances, improving these algorithms further seems {{to be out of}} reach. The CNF-SAT problem is the canonical example of a problem for which the trivial exhaustive search <b>algorithm</b> runs in <b>time</b> O(2 n), where n is the number of variables in the input formula. While there exist non-trivial algorithms for CNF-SAT that run in time o(2 n), no algorithm was able to improve the growth rate 2 to a smaller constant, and hence it is natural to conjecture that 2 is the optimal growth rate. The stron...|$|R
40|$|The {{field of}} exact <b>exponential</b> <b>time</b> <b>algorithms</b> for NP-hard {{problems}} has thrived {{over the last}} decade. While exhaustive search remains asymptotically the fastest known algorithm for some basic problems, difficult and non-trivial <b>exponential</b> <b>time</b> <b>algorithms</b> have been found for a myriad of problems, including Graph Coloring, Hamiltonian Path, Dominating Set and 3 -CNF-Sat. In some instances, improving these algorithms further seems {{to be out of}} reach. The CNF-Sat problem is the canonical example of a problem for which the trivial exhaustive search <b>algorithm</b> runs in <b>time</b> O(2 ^n), where n is the number of variables in the input formula. While there exist non-trivial algorithms for CNF-Sat that run in time o(2 ^n), no algorithm was able to improve the growth rate 2 to a smaller constant, and hence it is natural to conjecture that 2 is the optimal growth rate. The strong <b>exponential</b> <b>time</b> hypothesis (SETH) by Impagliazzo and Paturi [JCSS 2001] goes a little bit further and asserts that, for every epsilo...|$|R
40|$|We {{present a}} new dynamic {{programming}} algorithm that solves minimum Steiner tree problems with k terminals in time O^*(c^k) for any c> 2. This improves the running {{time of the}} previously fastest <b>exponential</b> <b>time</b> <b>algorithms</b> (Dreyfus-Wagner cite{DW}) of order O^*(3 ^k) and the so-called ''full set dynamic programming" algorithm, cf. cite{FK}, solving rectilinear instances in time O^*(2. 38 ^k) ...|$|R
50|$|A better <b>exponential</b> <b>time</b> <b>{{algorithm}}</b> {{is known}} which runs in time O(2N/2). The algorithm splits arbitrarily the N elements into {{two sets of}} N/2 each. For each of these two sets, it stores {{a list of the}} sums of all 2N/2 possible subsets of its elements. Each of these two lists is then sorted. Using a standard comparison sorting algorithm for this step would take time O(2N/2N). However, given a sorted list of sums for k elements, the list can be expanded to two sorted lists with the introduction of a (k + 1)st element, and these two sorted lists can be merged in time O(2k). Thus, each list can be generated in sorted form in time O(2N/2). Given the two sorted lists, the algorithm can check if an element of the first array and an element of the second array sum up to s in time O(2N/2). To do that, the algorithm passes through the first array in decreasing order (starting at the largest element) and the second array in increasing order (starting at the smallest element). Whenever the sum of the current element in the first array and the current element in the second array is more than s, the algorithm moves to the next element in the first array. If it is less than s, the algorithm moves to the next element in the second array. If two elements with sum s are found, it stops. Horowitz and Sahni first published this algorithm in a technical report in 1972.|$|E
30|$|Clearly, the {{algorithm}} presented is an <b>exponential</b> <b>time</b> <b>algorithm</b> unless {{the number of}} clusters p is fixed.|$|E
40|$|The HOM problem {{questions}} whether {{the image of}} a given regular tree language through a given tree homomorphism is also regular. Decidability of HOM is an important theoretical question which was open for a long time. Recently, HOM has been proved decidable with a triple <b>exponential</b> <b>time</b> <b>algorithm.</b> In this paper we obtain an <b>exponential</b> <b>time</b> <b>algorithm</b> for this problem, and conclude that it is EXPTIME-complete. The proof builds upon previous results and techniques on tree automata with constraints. Postprint (published version...|$|E
50|$|Several {{researchers}} {{have studied the}} complexity of <b>exponential</b> <b>time</b> <b>algorithms</b> restricted to cubic graphs. For instance, by applying dynamic programming to a path decomposition of the graph, Fomin and Høie showed how to find their maximum independent sets in time 2n/6 + o(n). The travelling salesman problem in cubic graphs can be solved in time O(1.2312n) and polynomial space.|$|R
40|$|Exact <b>exponential</b> <b>time</b> <b>algorithms</b> for NP-hard {{problems}} have thrived {{over the last}} decade. Non-trivial <b>exponential</b> <b>time</b> <b>algorithms</b> have been found for a myriad of problems, including Graph Coloring, Hamiltonian Path, Dominating Set and 3 –CNF-Sat, that is, satisfiability of 3 -CNF formulas. For some basic problems, however, {{there has been no}} progress over their trivial solution. For others non-trivial solutions have been found, but improving these algorithms further seems to be out of reach. The CNF-Sat problem is the canonical example of a problem for which the brute force 2 n n O(1) <b>time</b> <b>algorithm</b> remains the best known. The assumption that k–CNF-Sat requires 2 n time in the worst case when k grows to infinity is known as the strong <b>exponential</b> <b>time</b> hypothesis (SETH) of Impagliazzo and Paturi. In this paper we reveal connections between well-studied problems, and show that improving over the currently best known algorithms for several of them would violate SETH. Specifically, we show that for every ɛ < 1, an O(2 ɛn) <b>time</b> <b>algorithm</b> for Hitting Set, Set Splitting or NAE-Sat would violate SETH. Here n is the number of elements (o...|$|R
40|$|Iterative Compression has {{recently}} {{led to a}} number of breakthroughs in parameterized complexity. Here, we show that the technique can also be useful in the design of exact <b>exponential</b> <b>time</b> <b>algorithms</b> to solve NP-hard problems. We exemplify our findings with algorithms for the Maximum Independent Set problem, a parameterized and a counting version of d-Hitting Set and the Maximum Induced Cluster Subgraph problem. ...|$|R
40|$|A computable real {{function}} F on [0, 1] {{is constructed}} such that there exists an <b>exponential</b> <b>time</b> <b>algorithm</b> {{for the evaluation}} of the function on [0, 1] on Turing machine but there does not exist any polynomial time algorithm {{for the evaluation of}} the function on [0, 1] on Turing machine (moreover, it holds for any rational point on (0, 1)) Comment: 5 page...|$|E
40|$|We {{present a}} {{moderately}} <b>exponential</b> <b>time</b> <b>algorithm</b> for the satis_ability of Boolean formulas over the full binary basis. For formulas of size at most cn, our algorithm runs in time 2 {(1 _μc) n} for some constant μc > 0. As {{a byproduct of}} the running time analysis of our algorithm, we obtain strong average-case hardness of a_ne extractors for linear-sized formulas over the full binary basis...|$|E
40|$|We design exact {{algorithms}} {{for several}} NP-complete graph-theoretic problems. The term exact algorithm is a shorthand for a fast <b>exponential</b> <b>time</b> <b>algorithm.</b> Our main result states that a minimum dominating {{set in a}} graph of order n {{can be found in}} time O(1. 8899 ^n). The list of considered problems includes MINIMUM DOMINATING SET, 3 -COLOURABILITY, MINIMUM CONNECTED DOMINATING SET (~MAXIMUM LEAF SPANNING TREE), MINIMUM EDGE DOMINATING SET, MINIMUM INDEPENDENT DOMINATING SET and MINIMUM MAXIMAL MATCHING...|$|E
40|$|Abstract. Iterative Compression has {{recently}} {{led to a}} number of breakthroughs in parameterized complexity. The main {{purpose of this paper is}} to show that iterative compression can also be used in the design of exact <b>exponential</b> <b>time</b> <b>algorithms.</b> We exemplify our findings with algorithms for the Maximum Independent Set problem, a counting version of k-Hitting Set and the Maximum Induced Cluster Subgraph problem. ...|$|R
40|$|Assuming P = NP, {{which is}} widely {{believed}} to be true, many important computational problems are not solvable in polynomial time. However, this {{does not imply that}} NP-hard problems are not exactly solvable at all. Both the concepts of moderately <b>exponential</b> <b>time</b> <b>algorithms</b> and parameterized complexity provide tools for solving many of these problems in reasonable time. In this thesis, we introduce the concept of intuitive algorithms. While intuitive algorithms can be either moderately <b>exponential</b> <b>time</b> <b>algorithms</b> or parameterized algorithms, we require that they follow an intuitive idea and are kept as simple as possible. When we analyze algorithms only in terms of a worst case runtime bound, this approach is disadvantageous, as it is sometimes much harder to prove good bounds for simpler algorithms. In some cases, this might even be impossible. However, we will show that there are several aspects of intuitive algorithms that makes the development of such algorithms worthwhile. For example...|$|R
40|$|International audienceIterative Compression has {{recently}} {{led to a}} number of breakthroughs in parameterized complexity. Here, we show that the technique can also be useful in the design of exact <b>exponential</b> <b>time</b> <b>algorithms</b> to solve NP-hard problems. We exemplify our findings with algorithms for the Maximum Independent Set problem, a parameterized and a counting version of d-Hitting Set and the Maximum Induced Cluster Subgraph problem...|$|R
40|$|We {{establish}} a characterization of alternating links {{in terms of}} definite spanning surfaces. We apply it to obtain a new proof of Tait's conjecture that reduced alternating diagrams of the same link have the same crossing number and writhe. We also deduce a result of Banks and Hirasawa-Sakuma about Seifert surfaces for special alternating links. The appendix, written by Juhász and Lackenby, applies the characterization to derive an <b>exponential</b> <b>time</b> <b>algorithm</b> for alternating knot recognition. Comment: 11 pages, 1 figur...|$|E
40|$|International audienceThe {{reachability}} {{analysis of}} recursive programs that communicate asynchro- nously over reliable Fifo channels calls for restrictions to ensure decidability. Our first result characterizes communication topologies with a decidable reachability problem re- stricted to eager runs (i. e., runs where messages are either received immediately after being sent, or never received). The problem is ExpTime-complete in the decidable case. The second {{result is a}} doubly <b>exponential</b> <b>time</b> <b>algorithm</b> for bounded context analysis in this setting, together with a matching lower bound...|$|E
40|$|Abstract. We {{consider}} a generalization {{of the classical}} minimum spanning tree problem called the generalized minimum spanning tree problem and denoted by GMST problem. The aim {{of this paper is}} to present an exact <b>exponential</b> <b>time</b> <b>algorithm</b> for the GMST problem as well three heuristic algorithms, two of them based on Prim’s and Kruskal’s algorithms for the minimum spanning tree problem and one based on the local global approach. These three algorithms are implemented and computational results are reported for many instances of the problem...|$|E
40|$|One {{fundamental}} problem in image recognition {{is to establish}} the resemblance of two images. This {{can be done by}} searching the best pixel to pixel mapping taking into account monotonicity and continuity constraints. We show that this problem is NPcomplete by reduction from 3 -SAT, thus giving evidence that the known <b>exponential</b> <b>time</b> <b>algorithms</b> are justi ed, but approximation algorithms or simpli cations are necessary...|$|R
40|$|The satisfiability {{problem of}} Boolean Formulae in 3 -CNF (3 -SAT) {{is a well}} known NP-complete problem and the {{development}} of faster (moderately <b>exponential</b> <b>time)</b> <b>algorithms</b> has received much interest in recent years. We show that the 3 -SAT problem can be solved by a probabilistic <b>algorithm</b> in expected <b>time</b> O(1; 3290). Our approach is based on Schöning's random walk algorithm for k-SAT, modified in two ways...|$|R
40|$|The Dagstuhl {{seminar on}} Moderately <b>Exponential</b> <b>Time</b> <b>Algorithms</b> took place from 19. 10. 08 to 24. 10. 08. The 54 {{participants}} came from 18 countries. There were 27 talks and 2 open problem sessions. Talks were complemented by intensive informal discussions, and many new research directions and open problems will result from these discussions. The warm and encouraging Dagstuhl atmosphere stimulated new research projects...|$|R
