1|3615|Public
40|$|This high-fidelity driving {{simulator}} study used a paired comparison design {{to investigate the}} effectiveness of 12 potential eco-driving interfaces. Previous work has demonstrated fuel economy improvements through the provision of in-vehicle eco-driving guidance using a visual or haptic interface. This study uses an eco-driving assistance system that advises {{the driver of the}} most fuel efficient accelerator pedal angle, in real time. Assistance was provided to drivers through a visual dashboard display, a multimodal visual dashboard and auditory tone combination, or a haptic accelerator pedal. The style of advice delivery was varied within each modality. The effectiveness of the eco-driving guidance was assessed via subjective feedback, and objectively through the pedal angle error between system-requested and participant-selected accelerator pedal angle. Comparisons amongst the six haptic systems suggest that drivers are guided best by a force feedback system, where a driver experiences a step change in force applied against their foot when they accelerate inefficiently. Subjective impressions also identified this system as more effective than a stiffness feedback system involving a more gradual change in pedal feedback. For interfaces with a visual component, drivers produced smaller pedal errors with an <b>in-vehicle</b> <b>visual</b> <b>display</b> containing second order information on the required rate of change of pedal angle, in addition to current fuel economy information. This was supported by subjective feedback. The presence of complementary audio alerts improved eco-driving performance and reduced visual distraction from the roadway. The results of this study can inform the further development of an in-vehicle assistance system that supports ‘green’ driving...|$|E
40|$|Memory {{retention}} of drivers was tested for traffic- and traveler-related messages displayed on an in-vehicle information system (IVIS). Three research questions were asked: (a) How does <b>in-vehicle</b> <b>visual</b> message format affect comprehension? (b) How does message format affect memory retention? and (c) What impact does driver age have on recall of <b>in-vehicle</b> <b>visual</b> messages? Nine younger (less than 30 years old) and nine older (65 {{years old or}} older) drivers participated in the experiment. As subjects steered the Battelle Automobile Simulator, an IVIS pre-sented traveler-related messages. Two types of messages, symbols and text, were presented. Message recognition was tested immediately or 50 sec after the message left the IVIS. Except for low comprehension symbols, driver recognition scores on both text and symbol messages were similar. Younger drivers scored higher than older drivers in iden-tifying the meaning of messages, particularly in the 50 -sec question delay condition. Latency {{to respond to the}} questions and confidence i...|$|R
40|$|Design and user {{evaluation}} of a multimodal interaction style for music programming is described. User requirements were instant usability and optional use of a <b>visual</b> <b>display.</b> The interaction style consists of a visual roller metaphor. User control of the rollers proceeds by manipulating a force feedback trackball. Tactual and auditory cues strengthen the roller impression and support use without a <b>visual</b> <b>display.</b> The evaluation investigated ask performance and procedural learning when performing music programming tasks with and without a <b>visual</b> <b>display.</b> No procedural instructions were provided. Tasks could be completed successfully with and without a <b>visual</b> <b>display,</b> though programming without a display needed more time to complete. Prior experience with a <b>visual</b> <b>display</b> did not improve performance without a <b>visual</b> <b>display.</b> When working without a display, procedures have to be acquired and remembered explicitly, as more procedures were remembered after working without a <b>visual</b> <b>display.</b> It is demonstrated that multimodality provides new ways to interact with music. Keywords multimodal interaction, nonvisual interaction, interfac...|$|R
40|$|<b>Visual</b> <b>displays</b> {{drive the}} human operator's highest {{bandwidth}} sensory input channel. Thus, no telemanipulation system is adequate {{which does not}} make extensive use of <b>visual</b> <b>displays.</b> Although an important use of <b>visual</b> <b>displays</b> is the presentation of a televised image of the work scene, <b>visual</b> <b>displays</b> are examined for presentation of nonvisual information (forces and torques) for simulation and planning, and for management {{and control of the}} large numbers of subsystems which make up a modern telemanipulation system...|$|R
40|$|The {{growth of}} common {{as well as}} {{emerging}} <b>visual</b> <b>display</b> technologies are surveyed. The major inference is that contemporary society is rapidly growing evermore reliant on <b>visual</b> <b>display</b> {{for a variety of}} purposes. Because of its unique mission requirements, the National Aeronautics and Space Administration has contributed in an important and specific way to the growth of <b>visual</b> <b>display</b> technology. These contributions are characterized by the use of computer-driven <b>visual</b> <b>displays</b> to provide an enormous amount of information concisely, rapidly and accurately...|$|R
40|$|Research on manual {{tracking}} with a kinesthetic-tactual (KT) display suggests that {{under certain conditions}} {{it can be an}} effective alternative or supplement to <b>visual</b> <b>displays.</b> In order to better understand how KT tracking compares with visual tracking, both a critical tracking and a stationary single-axis tracking task were conducted with and without velocity quickening. In the critical tracking tasks, the <b>visual</b> <b>displays</b> were superior; however, the quickened KT display was approximately equal to the unquickened <b>visual</b> <b>display.</b> In stationary tracking tasks, subjects adopted lag equalization with the quickened KT and <b>visual</b> <b>displays,</b> and mean-squared error scores were approximately equal. With the unquickened displays, subjects adopted lag-lead equalization, and the <b>visual</b> <b>displays</b> were superior...|$|R
40|$|Paying {{attention}} {{is a critical}} first step toward learning. For children in primary school classrooms there can be many things to attend to other than {{the focus of a}} lesson, such as <b>visual</b> <b>displays</b> on classroom walls. The aim {{of this study was to}} use eye-tracking techniques to explore the impact of <b>visual</b> <b>displays</b> on attention and learning for children. Critically, we explored these issues for children developing typically and for children with autism spectrum disorder (ASD). Both groups of children watched videos of a teacher delivering classroom activities— 2 of “story-time” and 2 mini lessons. Half of the videos each child saw contained high levels of classroom <b>visual</b> <b>displays</b> in the background (high <b>visual</b> <b>display</b> [HVD]) and half had none (no <b>visual</b> <b>display</b> [NVD]). Children completed worksheets after the mini lessons to measure learning. During viewing of all videos children’s eye movements were recorded. The presence of <b>visual</b> <b>displays</b> had a significant impact on attention for all children, but to a greater extent for children with ASD. <b>Visual</b> <b>displays</b> also had an impact on learning from the mini lessons, whereby children had poorer learning scores in the HVD compared with the NVD lesson. Individual differences in age, verbal, nonverbal, and attention abilities were important predictors of learning, but time spent attending the <b>visual</b> <b>displays</b> in HVD was the most important predictor. This novel and timely investigation has implications for the use of classroom <b>visual</b> <b>displays</b> for all children, but particularly for children with ASD...|$|R
40|$|Recent {{research}} on manual tracking with a kinesthetic-tactual (KT) display suggests that {{under certain conditions}} {{it can be an}} effective alternative or supplement to <b>visual</b> <b>displays.</b> In order to understand better how KT tracking compares with visual tracking, both a critical tracking and stationary single-axis tracking tasks were conducted with and without velocity quickening. In the critical tracking task, the <b>visual</b> <b>displays</b> were superior, however, the quickened KT display was approximately equal to the unquickened <b>visual</b> <b>display.</b> In stationary tracking tasks, subjects adopted lag equalization with the quickened KT and <b>visual</b> <b>displays,</b> and mean-squared error scores were approximately equal. With the unquickened displays, subjects adopted lag-lead equalization, and the <b>visual</b> <b>displays</b> were superior. This superiority was partly due to the servomotor lag in the implementation of the KT display and partly due to modality differences...|$|R
30|$|Researchers {{working within}} an SFL {{theoretical}} framework, ([O'Halloran, 2005, Lemke, 2003, Schleppegrell, 2007]), have shown how multiple meaning-making systems {{are used in}} mathematical texts. [O'Halloran's (2005)] analysis of mathematical texts demonstrates how three semiotic systems, mathematical symbolism, <b>visual</b> <b>display,</b> and natural language together construct mathematical meaning in ways that natural language alone does not. For instance, [O'Halloran (2005)] shows that mathematics argumentation in texts typically unfolds as problem-solution. In order to solve the problem, writers of mathematics will arrange mathematical symbols in patterned ways so that they encode a limited range of meanings. In addition, the writer must also include visual images, such as statistical graphs, geometric diagrams, {{and other kinds of}} drawn or computer-generated <b>visual</b> <b>displays.</b> Through these <b>visual</b> <b>displays,</b> the writer illustrates mathematical meanings in a space-time format. However, mathematical symbolism and <b>visual</b> <b>display</b> may be relatively limited in functionality, and the writer must accompany the symbols and <b>visual</b> <b>display</b> with natural language. Natural language functions serves to disambiguate and clarify meanings when the meaning potential of mathematical symbolism and <b>visual</b> <b>display</b> are exhausted.|$|R
30|$|As {{outlined}} in Table  1, operator adjustment of the PET <b>visual</b> <b>display</b> threshold {{was a key}} step to qualitatively distinguish between signal and noise for diagnostic reporting. The lower PET <b>visual</b> <b>display</b> threshold was set to zero for both target and non-target activity detection. For target activity detection, the upper PET <b>visual</b> <b>display</b> threshold setting was adjusted to reduce the background noise to a minimum. On our system, we used the following upper PET <b>visual</b> <b>display</b> threshold settings for target activity detection: median 53 % (7, 000 kBq/ml), mean 54 % (6, 900 kBq/ml), 95 % confidence interval (CI) 47 % to 61 % (5, 900 to 7, 900 kBq/ml).|$|R
40|$|Examining an ECG <b>visual</b> <b>display</b> {{with a view}} to {{understanding}} the meanings represented in the display is a precursor to correctly interpreting the display and providing an accurate diagnosis of the medical condition presented in the ECG <b>visual</b> <b>display.</b> Nurse education, both in university and field-placement contexts, places high priority on understanding and applying the meaning of diagnostic information conveyed in <b>visual</b> <b>display</b> formats. Understanding the condition of patients revealed by such displays is a critical factor in determining patient outcomes. A promising feature of eye-tracking technologies and methodologies investigating cognitive processes in diagnostic situations is to examine eye movements as nursing students analyse the information provided in real <b>visual</b> <b>displays...</b>|$|R
40|$|Abstract We have {{developed}} a gesture input system that provides a common interaction technique across mobile, wearable and ubiquitous computing devices of diverse form factors. In this paper, we combine our gestural input technique with speech output and test {{whether or not the}} absence of a <b>visual</b> <b>display</b> impairs usability in this kind of multimodal interaction. This is of particular relevance to mobile, wearable and ubiquitous systems where <b>visual</b> <b>displays</b> may be restricted or unavailable. We conducted the evaluation using a prototype for a system combining gesture input and speech output to provide information to patients in a hospital Accident and Emergency Department. A group of participants was instructed to access various services using gestural inputs. The services were delivered by automated speech output. Throughout their tasks, these participants could see a <b>visual</b> <b>display</b> on which a GUI presented the available services and their corresponding gestures. Another group of participants performed the same tasks but without this <b>visual</b> <b>display.</b> It was predicted that the participants without the <b>visual</b> <b>display</b> would make more incorrect gestures and take longer to perform correct gestures than the participants with the <b>visual</b> <b>display.</b> We found {{no significant difference in the}} number of incor-rect gestures made. We also found that participants with the <b>visual</b> <b>display</b> took longer than participants without it. It was suggested that for a small set of semantically distinct services with memorable and distinct gestures, the absence of a GUI <b>visual</b> <b>display</b> does not impair the usability of a system with gesture input and speech output...|$|R
40|$|Analogy is a {{powerful}} heuristic for problem solving. In design, where designers are constantly exposed to visual stimuli, visual analogies {{are considered to be}} particularly helpful. To date, however, few researchers have given enough attention to visual analogy and <b>visual</b> <b>displays</b> in design contexts. This study deals with the use of these tools by investigating the effect that expertise has on them, in ill-defined and well-defined problem solving. Novice and experienced designers solved both types of design problem with exposure to <b>visual</b> <b>displays</b> and with the requirement to use visual analogy. Results showed that when <b>visual</b> <b>displays</b> and instructions to use analogy were available no expertise differences existed in well-defined problem solving. However, with the availability of only <b>visual</b> <b>displays,</b> architects tend to perform better than students. Without guidance to use analogy, students found difficulty in spontaneous recall of <b>visual</b> <b>displays</b> as candidate analogues in well-defined problems. In ill-defined problem solving, architects performed better than novice students in the use of both analogy and <b>visual</b> <b>displays.</b> However, no {{differences were found between the}} experienced designers and the advanced students in any experimental condition in this problem context. ...|$|R
40|$|The {{quality of}} realism in virtual environments (VEs) is {{typically}} {{considered to be}} a function of visual and audio fidelity mutually exclusive of each other. However, the VE participant, being human, is multimodal by nature. Therefore, in order to validate more accurately the levels of auditory and visual fidelity that are required in a virtual environment, a better understanding is needed of the intersensory or crossmodal effects between the auditory and visual sense modalities. To identify whether any pertinent auditory-visual cross-modal perception phenomena exist, 108 subjects participated in three experiments which were completely automated using HTML, Java, and JavaScript programming languages. <b>Visual</b> and auditory <b>display</b> quality perceptions were measured intra- and intermodally by manipulating the pixel resolution of the <b>visual</b> <b>display</b> and Gaussian white noise level, and by manipulating the sampling frequency of the auditory display and Gaussian white noise level. Statistically significant results indicate that high-quality auditory displays coupled with highquality <b>visual</b> <b>displays</b> increase the quality perception of the <b>visual</b> <b>displays</b> relative to the evaluation of the <b>visual</b> <b>display</b> alone, and that low-quality auditory displays coupled with high-quality <b>visual</b> <b>displays</b> decrease the quality perception of the auditory displays relative to the evaluation of the auditory display alone. These findings strongly suggest that the quality of realism in VEs must be a function of both auditory and <b>visual</b> <b>display</b> fidelities inclusive of each other...|$|R
5000|$|Auditory output {{redundant}} {{with information}} on <b>visual</b> <b>displays</b> ...|$|R
5000|$|Part 300: Introduction to {{electronic}} <b>visual</b> <b>display</b> requirements ...|$|R
5000|$|... #Caption: The band's {{performance}} {{included a}} complex <b>visual</b> <b>display</b> ...|$|R
5000|$|Part 306: Field {{assessment}} {{methods for}} electronic <b>visual</b> <b>displays</b> ...|$|R
50|$|An {{electronic}} <b>visual</b> <b>display,</b> informally a screen, is {{a display}} device for presentation of images, text, or video transmitted electronically, without producing a permanent record. Electronic <b>visual</b> <b>displays</b> include television sets, computer monitors, and digital signage. They are also ubiquitous in mobile computing applications like tablet computers, smartphones, and information appliances.|$|R
40|$|An {{experimental}} {{evaluation is}} presented of the usability properties of a multimodal interaction style for music programming. The experiment investigated task performance and learning of procedures while performing music programming tasks {{in the presence}} or absence of a <b>visual</b> <b>display</b> combined with a tactual and auditory interface. The participant's task was to compile a music programme as quickly as possible. Task performance was measured by compilation time and number of actions executed. Procedural knowledge was assessed by a posttask questionnaire. Participants performed equally efficiently, i. e [...] not significantly differently, with and without a <b>visual</b> <b>display,</b> except for the first programming task. In the first task, performing a task without a <b>visual</b> <b>display</b> required significantly more time (approximately one additional minute) and more, but not significantly more, actions, probably due to explorative behaviour required to develop an internal representation of the interaction style. Earlier experience with a <b>visual</b> <b>display</b> did not improve task performance without a <b>visual</b> <b>display.</b> It also appeared that participants who had performed tasks non visually had learned more procedures. Nonvisual interaction requires the explicit discovery and memorization of procedures, which induces a higher degree of cognitive processing. It could therefore be demonstrated that tactual and auditory feedback can compensate for the visual modality in contextsof- use, in which <b>visual</b> <b>display</b> of information is impoverished or even absent. e. g [...] portable devices, remote controls, and car equipment...|$|R
40|$|Presented of the 6 th International Conference on Auditory Display (ICAD), Atlanta, GA, April 2 - 5, 2000 The {{quality of}} realism in virtual environments is {{typically}} {{considered to be}} a function of visual and audio fidelity mutually exclusive of each other. However, the virtual environment participant, being human, is multi-modal by nature. Therefore, in order to more accurately validate the levels of auditory and visual fidelity required in a virtual environment, a better understanding is needed of the intersensory or cross-modal effects between the auditory and visual sense modalities. To identify whether any pertinent auditory-visual cross-modal perception phenomena exist, 108 subjects participated in three experiments which are completely automated using HTML, Java, and JavaScript computer programming languages. <b>Visual</b> and auditory <b>display</b> quality perception are measured intramodally and intermodally by manipulating <b>visual</b> <b>display</b> pixel resolution and Gaussian white noise level and by manipulating auditory display sampling frequency and Gaussian white noise level. Statistically significant results indicate that 1) high-quality auditory displays coupled with high-quality <b>visual</b> <b>displays</b> increase the quality perception of the <b>visual</b> <b>displays</b> relative to the evaluation of the <b>visual</b> <b>display</b> alone, and 2) low-quality auditory displays coupled with high-quality <b>visual</b> <b>displays</b> decrease the quality perception of the auditory displays relative to the evaluation of the auditory display alone. These findings strongly suggest that the quality of realism in virtual environments must be a function of both auditory and <b>visual</b> <b>display</b> fidelities inclusive of each other...|$|R
5000|$|Part 304: User {{performance}} test methods for electronic <b>visual</b> <b>displays</b> ...|$|R
5000|$|Part 305: Optical {{laboratory}} test methods for electronic <b>visual</b> <b>displays</b> ...|$|R
40|$|In {{this study}} {{the focus of}} {{investigation}} was the reciprocal engagement between a professional scientist and the <b>visual</b> <b>displays</b> with which he interacted. <b>Visual</b> <b>displays</b> are considered inextricable from everyday scientific endeavors and their interpretation requires a "back-and-forthness" between the viewers and the objects being viewed. The query that drove this study was : How does a scientist engage with <b>visual</b> <b>displays</b> during the explanation of his understanding of extremely small biological objects? The conceptual framework was based in embodiment where the scientist's talk, gesture, and body position were observed and microanalyzed. The data consisted of open-ended interviews that positioned the scientist to interact with <b>visual</b> <b>displays</b> when he explained the structure and function of different sub- cellular features. Upon microanalyzing the scientist's talk, gesture, and body position during his interactions with two different <b>visual</b> <b>displays,</b> four themes were uncovered : Naming, Layering, Categorizing, and Scaling. Naming occurred when the scientist added markings to a pre -existing, hand-drawn <b>visual</b> <b>display.</b> The markings had meaning as stand-alone label and iconic symbols. Also, the markings transformed the pre-existing <b>visual</b> <b>display,</b> which resulted in its function as a new visual object. Layering occurred when the scientist gestured over images so that his gestures aligned {{with one or more}} of the image's features, but did not touch the actual <b>visual</b> <b>display.</b> Categorizing occurred when the scientist used contrasting categories, e. g. straight vs. not straight, to explain his understanding about different characteristics that the small biological objects held. Scaling occurred when the scientist used gesture to resize an image's features so that they fit his bodily scale. Three main points were drawn from this study. First, the scientist employed a variety of embodied strategies [...] coordinated talk, gesture, and body position [...] when he explained the structure and function of extremely small objects. Second, three descriptive areas appear to influence the scientist's interactions : the small biological objects' features, the interview context, and the interview space. Finally, the interaction of the scientist's body with the <b>visual</b> <b>displays</b> created a unique engagement that allowed the scientist to share his understanding about extremely small biological object...|$|R
40|$|Includes bibliographical {{references}} (p. 255 - 262). In {{this study}} {{the focus of}} investigation was the reciprocal engagement between a professional scientist and the <b>visual</b> <b>displays</b> with which he interacted. <b>Visual</b> <b>displays</b> are considered inextricable from everyday scientific endeavors and their interpretation requires a "back-and-forthness" between the viewers and the objects being viewed. The query that drove this study was : How does a scientist engage with <b>visual</b> <b>displays</b> during the explanation of his understanding of extremely small biological objects? The conceptual framework was based in embodiment where the scientist's talk, gesture, and body position were observed and microanalyzed. The data consisted of open-ended interviews that positioned the scientist to interact with <b>visual</b> <b>displays</b> when he explained the structure and function of different sub-cellular features. Upon microanalyzing the scientist's talk, gesture, and body position during his interactions with two different <b>visual</b> <b>displays,</b> four themes were uncovered : Naming, Layering, Categorizing, and Scaling. Naming occurred when the scientist added markings to a pre-existing, hand-drawn <b>visual</b> <b>display.</b> The markings had meaning as stand-alone label and iconic symbols. Also, the markings transformed the pre-existing <b>visual</b> <b>display,</b> which resulted in its function as a new visual object. Layering occurred when the scientist gestured over images so that his gestures aligned {{with one or more}} of the image's features, but did not touch the actual <b>visual</b> <b>display.</b> Categorizing occurred when the scientist used contrasting categories, e. g. straight vs. not straight, to explain his understanding about different characteristics that the small biological objects held. Scaling occurred when the scientist used gesture to resize an image's features so that they fit his bodily scale. Three main points were drawn from this study. First, the scientist employed a variety of embodied strategies [...] coordinated talk, gesture, and body position [...] when he explained the structure and function of extremely small objects. Second, three descriptive areas appear to influence the scientist's interactions : the small biological objects' features, the interview context, and the interview space. Finally, the interaction of the scientist's body with the <b>visual</b> <b>displays</b> created a unique engagement that allowed the scientist to share his understanding about extremely small biological object...|$|R
25|$|<b>Visual</b> display: <b>Visual</b> <b>displays</b> {{provide the}} <b>visual</b> {{stimulus}} to the user.|$|R
50|$|Common {{applications}} for electronic <b>visual</b> <b>displays</b> are televisions or computer monitors.|$|R
50|$|<b>Visual</b> display: <b>Visual</b> <b>displays</b> {{provide the}} <b>visual</b> {{stimulus}} to the user.|$|R
5000|$|Early high {{resolution}} colour <b>Visual</b> <b>display</b> units (mainly for military use) ...|$|R
5000|$|Part 307: Analysis and {{compliance}} test methods for electronic <b>visual</b> <b>displays</b> ...|$|R
40|$|International audienceAn {{experiment}} {{investigated the}} influence of eye movements on learning a simple motor sequence task when the <b>visual</b> <b>display</b> was magnified. The task was to reproduce a 1300 ms spatial-temporal pattern of elbow flexions and extensions. The spatial-temporal pattern was displayed {{in front of the}} participants. Participants were randomly assigned to four groups differing on eye movements (free to use their eyes/instructed to fixate) and the <b>visual</b> <b>display</b> (small/magnified). All participants had to perform a pre-test, an acquisition phase, a delayed retention test, and a transfer test. The results indicated that participants in each practice condition increased their performance during acquisition. The participants who were permitted to use their eyes in the magnified <b>visual</b> <b>display</b> outperformed those who were instructed to fixate on the magnified <b>visual</b> <b>display.</b> When a small <b>visual</b> <b>display</b> was used, the instruction to fixate induced no performance decrements compared to participants who were permitted to use their eyes during acquisition. The findings demonstrated that a spatial-temporal pattern can be learned without eye movements, but being permitting to use eye movements facilitates the response production when the visual angle is increased...|$|R
40|$|With {{advances}} in computing and <b>visual</b> <b>display</b> technology, {{the interface between}} man and machine has become increasingly complex. The usability of a modern interactive system depends {{on the design of}} the <b>visual</b> <b>display.</b> This dissertation aims to improve the design process by examining the relationship between human perception of depth and three-dimensional computer-generated imagery (3 D CGI). Depth i...|$|R
5000|$|The Street is My Gallery, a <b>visual</b> <b>display</b> {{of street}} art in Athens ...|$|R
40|$|The current ISO 9241 – 3 {{standard}} for <b>visual</b> <b>display</b> {{quality and the}} proposed user performance tests are reviewed. The standard {{is found to be}} more engineering than ergonomic and problems with system configuration, software applications, display settings, user behaviour, wear and physical environment are addressed. Different methods of <b>visual</b> <b>display</b> quality measurement are compared, direct subjective methods are considered to be the most valid and sensitive. Results with the Display Evaluation Scale, a tool for rating <b>visual</b> <b>display</b> quality, are evaluated. Three user groups of the standard are distinguished and a new configuration and content of standards is proposed to suit these user groups...|$|R
5000|$|... #Caption: Artist's {{impression}} of a striking and unusual <b>visual</b> <b>display</b> in a Lambeosaurus magnicristatus ...|$|R
2500|$|He {{uses the}} term [...] "data-ink ratio" [...] to argue against using {{excessive}} decoration in <b>visual</b> <b>displays</b> of quantitative information. In <b>Visual</b> <b>Display,</b> Tufte explains, [...] "Sometimes decoration can help editorialize about {{the substance of the}} graphic. But it is wrong to distort the data measures—the ink locating values of numbers—in order to make an editorial comment or fit a decorative scheme." ...|$|R
