2029|1161|Public
5|$|Along with {{quadratic}} probing {{and double}} hashing, linear probing {{is a form}} of open addressing. In these schemes, each cell of a <b>hash</b> <b>table</b> stores a single key–value pair. When the hash function causes a collision by mapping a new key to a cell of the <b>hash</b> <b>table</b> that is already occupied by another key, linear probing searches the table for the closest following free location and inserts the new key there. Lookups are performed in the same way, by searching the table sequentially starting at the position given by the hash function, until finding a cell with a matching key or an empty cell.|$|E
5|$|Linear probing is a {{component}} of open addressing schemes for using a <b>hash</b> <b>table</b> to solve the dictionary problem. In the dictionary problem, a data structure should maintain a collection of key–value pairs subject to operations that insert or delete pairs from the collection or that search for the value associated with a given key.|$|E
5|$|Using linear probing, {{dictionary}} operations can {{be implemented}} in constant expected time. In other words, insert, remove and search operations {{can be implemented}} in O(1), {{as long as the}} load factor of the <b>hash</b> <b>table</b> is a constant strictly less than one.|$|E
40|$|In this paper, {{the author}} proposes {{a series of}} {{multilevel}} double hashing schemes called cascade <b>hash</b> <b>tables.</b> They use several levels of <b>hash</b> <b>tables.</b> In each table, we use the common double hashing scheme. Higher level <b>hash</b> <b>tables</b> work as fail-safes of lower level <b>hash</b> <b>tables.</b> By this strategy, it could effectively reduce collisions in hash insertion. Thus it gains a constant worst case lookup time with a relatively high load factor(70 % − 85 %) in random experiments. Different parameters of cascade <b>hash</b> <b>tables</b> are tested. ...|$|R
40|$|The paper {{discusses}} {{different aspects}} of <b>hash</b> <b>tables</b> as a data structure in a Horn clause language such as Prolog, e. g., semantics, implementation and applications. <b>Hash</b> <b>tables</b> are a new concept in Prolog since {{they can not be}} efficiently implemented in the language itself. We give informal semantics for <b>hash</b> <b>tables</b> as partial functions on logical terms. We are careful to ensure that we can represent the <b>hash</b> <b>tables</b> as logical terms {{in order to make the}} ordinary syntactic unification apply. The implementation is a generalisation of the method suggested by Eriksson and Rayner. Their "mutable arrays" are a special case of our multiple version <b>hash</b> <b>tables.</b> The implementation has been used in Tricia, an implementation of Prolog developed at Uppsala University. <b>Hash</b> <b>tables</b> open application areas previously beyond the efficiency limits of Prolog. Application programs taking great benefit from the <b>hash</b> <b>tables</b> have been written in Tricia. An earlier version of this paper was presented at the 4 [...] ...|$|R
50|$|<b>Hash</b> <b>tables</b> store {{associations}} between data objects. Any object {{may be used}} as key or value. <b>Hash</b> <b>tables</b> are automatically resized as needed.|$|R
5|$|Alternatively, it is {{possible}} to use a lazy deletion strategy in which a key–value pair is removed by replacing the value by a special flag value indicating a deleted key. However, these flag values will contribute to the load factor of the <b>hash</b> <b>table.</b> With this strategy, it may become necessary to clean the flag values out of the array and rehash all the remaining key–value pairs once too large a fraction of the array becomes occupied by deleted keys.|$|E
5|$|Another {{method of}} {{constructing}} hash functions with both high quality and practical speed is tabulation hashing. In this method, the hash value for a key {{is computed by}} using each byte of the key as an index into a table of random numbers (with a different table for each byte position). The numbers from those table cells are then combined by a bitwise exclusive or operation. Hash functions constructed this way are only 3-independent. Nevertheless, linear probing using these hash functions takes constant expected time per operation. Both tabulation hashing and standard methods for generating 5-independent hash functions are limited to keys that have a fixed number of bits. To handle strings or other types of variable-length keys, {{it is possible to}} compose a simpler universal hashing technique that maps the keys to intermediate values and a higher quality (5-independent or tabulation) hash function that maps the intermediate values to <b>hash</b> <b>table</b> indices.|$|E
25|$|A <b>hash</b> <b>table</b> may use linked lists {{to store}} the chains of items that hash to the same {{position}} in the <b>hash</b> <b>table.</b>|$|E
50|$|Lists of pairs and {{functional}} maps both provide a purely functional interface. In contrast, <b>hash</b> <b>tables</b> provide an imperative interface. For many operations, <b>hash</b> <b>tables</b> are significantly faster than lists of pairs {{and functional}} maps.|$|R
50|$|Multiplicative hashing is {{a simple}} type of hash {{function}} often used by teachers introducing students to <b>hash</b> <b>tables.</b> Multiplicative <b>hash</b> functions are simple and fast, but have higher collision rates in <b>hash</b> <b>tables</b> than more sophisticated hash functions.|$|R
5000|$|The fixed-size header at the {{beginning}} of the database describes 256 <b>hash</b> <b>tables</b> by listing their position within the file and their length in slots. Data is stored as a series of records, each storing key length, data length, key, and data. There are no alignment or sorting rules. The records are followed by a set of 256 <b>hash</b> <b>tables</b> of varying lengths. Since zero is a valid length, there may be fewer than 256 <b>hash</b> <b>tables</b> physically stored in the database, but there are nonetheless considered to be 256 <b>tables.</b> <b>Hash</b> <b>tables</b> contain a series of slots, each of which contains a hash value and a record offset. [...] "Empty slots" [...] have an offset of zero.|$|R
25|$|Looking up {{data in a}} trie {{is faster}} in the worst case, O(m) time (where m is {{the length of a}} search string), {{compared}} to an imperfect <b>hash</b> <b>table.</b> An imperfect <b>hash</b> <b>table</b> can have key collisions. A key collision is the hash function mapping of different keys to the same position in a <b>hash</b> <b>table.</b> The worst-case lookup speed in an imperfect <b>hash</b> <b>table</b> is O(N) time, but far more typically is O(1), with O(m) time spent evaluating the hash.|$|E
25|$|The 2.4 branch also {{includes}} performance improvements to <b>hash</b> <b>table,</b> Array#max, Array#min, and instance variable access.|$|E
25|$|Buckets in a trie, {{which are}} {{analogous}} to <b>hash</b> <b>table</b> buckets that store key collisions, are necessary only if a single key {{is associated with}} more than one value.|$|E
50|$|<b>Hash</b> <b>tables</b> {{may also}} be used as disk-based data {{structures}} and database indices (such as in dbm) although B-trees are more popular in these applications. In multi-node database systems, <b>hash</b> <b>tables</b> are commonly used to distribute rows amongst nodes, reducing network traffic for hash joins.|$|R
50|$|<b>Hash</b> <b>tables</b> are {{commonly}} {{said to have}} expected O(1) insertion and deletion times, but this is only true when considering computation of the hash of the key to be a constant-time operation. When hashing the key is taken into account, <b>hash</b> <b>tables</b> have expected O(k) insertion and deletion times, but may take longer in the worst case depending on how collisions are handled. Radix trees have worst-case O(k) insertion and deletion. The successor/predecessor operations of radix trees are also not implemented by <b>hash</b> <b>tables.</b>|$|R
50|$|BEP-0005 extends BitTorrent {{to support}} {{distributed}} <b>hash</b> <b>tables.</b>|$|R
25|$|Most DHTs {{use some}} variant of {{consistent}} hashing or rendezvous hashing to map keys to nodes. The two algorithms {{appear to have}} been devised independently and simultaneously to solve the distributed <b>hash</b> <b>table</b> problem.|$|E
25|$|Some tries {{can require}} more space than a <b>hash</b> <b>table,</b> as memory may be {{allocated}} for each {{character in the}} search string, rather than a single chunk of memory for the whole entry, as in most hash tables.|$|E
25|$|Cuckoo hashing, another {{technique}} for implementing hash tables, guarantees constant time per lookup (regardless of the hash function). Insertions into a cuckoo <b>hash</b> <b>table</b> may fail, causing the entire table to be rebuilt, but such failures are sufficiently {{unlikely that the}} expected time per insertion (using either a truly random hash function or a hash function with logarithmic independence) is constant. With tabulation hashing, on the other hand, the best bound known on the failure probability is higher, high enough that insertions cannot be guaranteed to take constant expected time. Nevertheless, tabulation hashing is adequate to ensure the linear-expected-time construction of a cuckoo <b>hash</b> <b>table</b> for a static set of keys that does not change as the table is used.|$|E
50|$|Double hashing is a {{computer}} programming technique used in <b>hash</b> <b>tables</b> to resolve <b>hash</b> collisions, in cases when two different values to be searched for produce the same hash key. It is a popular collision-resolution technique in open-addressed <b>hash</b> <b>tables.</b> Double <b>hashing</b> is implemented in many popular libraries.|$|R
5000|$|For a pair {{of types}} , , the type [...] {{is the type of}} <b>hash</b> <b>tables</b> mapping type- keys to type- values. <b>Hash</b> <b>tables</b> are built into the language, with special syntax and {{built-in}} functions. [...] is a channel that allows sending values of type T between concurrent Go processes.|$|R
5000|$|Distributed <b>hash</b> <b>tables,</b> no {{metadata}} servers, true {{horizontal scaling}} ...|$|R
25|$|Exploiting {{distributed}} {{hash tables}} (DHT): This attack exploits {{the fact that}} distributed <b>hash</b> <b>table</b> (DHT) connections through Tor are impossible, so an attacker is able to reveal a target's IP address by looking {{it up in the}} DHT even if the target uses Tor to connect to other peers.|$|E
25|$|The {{birthday}} {{problem in}} this more generic sense applies to hash functions: the expected number of N-bit hashes that can be generated before getting a collision is not 2N, but rather only 2N/2. This is exploited by birthday attacks on cryptographic hash functions and {{is the reason why}} a small number of collisions in a <b>hash</b> <b>table</b> are, for all practical purposes, inevitable.|$|E
25|$|The {{best way}} {{to speed up the}} baby-step giant-step {{algorithm}} is to use an efficient table lookup scheme. The best in this case is a <b>hash</b> <b>table.</b> The hashing is done on the second component, and to perform the check in step 1 of the main loop, γ is hashed and the resulting memory address checked. Since hash tables can retrieve and add elements in O(1) time (constant time), this does not slow down the overall baby-step giant-step algorithm.|$|E
5000|$|Monomorphic <b>hash</b> <b>tables</b> {{are also}} {{supported}} using the [...] functor.|$|R
5000|$|... #Subtitle level 3: Weighted Distributed <b>Hash</b> <b>Tables</b> and Weighted Rendezvous Hash ...|$|R
40|$|I love <b>hash</b> <b>tables.</b> You will, too, {{after you}} see what they can do. This paper will start with the classic 2 table join. Nothing new, you say? But this version does it in one step without sorting. And, it's the fastest method around. Next, {{you will see the}} use of <b>hash</b> <b>tables</b> to join {{multiple}} datasets on different keys in one pass, again with no sorting. How about unlimited recursive chained lookups (like when you need to update an ID through as many incarnations as it may have had) ? <b>Hash</b> <b>tables</b> solve that one, too. And before you catch your breath, you'll see how to create n datasets from one (or more), where n is determined (at runtime) by the number of values encountered in a variable. No muss, no fuss. One step (well, OK, this one does require a prior sort) and you're done. We’ll wind up with an outer join, just to show that <b>hash</b> <b>tables</b> can do that, too. Take a look at what they can deliver, and you'll be counting the ways you love <b>hash</b> <b>tables</b> before the year is out...|$|R
25|$|Because {{tabulation}} hashing is {{a universal}} hashing scheme, {{it can be used}} in any hashing-based algorithm in which universality is sufficient. For instance, in hash chaining, the expected time per operation is proportional to the sum of collision probabilities, which is the same for any universal scheme as it would be for truly random hash functions, and is constant whenever the load factor of the <b>hash</b> <b>table</b> is constant. Therefore, tabulation hashing can be used to compute hash functions for hash chaining with a theoretical guarantee of constant expected time per operation.|$|E
25|$|Both {{consistent}} hashing and rendezvous hashing {{have the}} essential property that removal or addition of one node changes only {{the set of}} keys owned by the nodes with adjacent IDs, and leaves all other nodes unaffected. Contrast this with a traditional <b>hash</b> <b>table</b> in which addition or removal of one bucket causes nearly the entire keyspace to be remapped. Since any change in ownership typically corresponds to bandwidth-intensive movement of objects stored in the DHT from one node to another, minimizing such reorganization is required to efficiently support high rates of churn (node arrival and failure).|$|E
25|$|A {{distributed}} <b>hash</b> <b>table</b> (DHT) is a {{class of}} a decentralized distributed system that provides a lookup service similar to a hash table: (key, value) pairs are stored in a DHT, and any participating node can efficiently retrieve the value associated with a given key. Responsibility for maintaining the mapping from keys to values is distributed among the nodes, {{in such a way}} that a change in the set of participants causes a minimal amount of disruption. This allows a DHT to scale to extremely large numbers of nodes and to handle continual node arrivals, departures, and failures.|$|E
50|$|The {{language}} {{also supports}} immutable <b>hash</b> <b>tables,</b> implemented functionally, and immutable dictionaries.|$|R
50|$|Including <b>hash</b> <b>tables</b> (unordered {{associative}} containers) in the C++ standard {{library is}} one of the most recurring requests. It was not adopted in C++03 due to time constraints only. Although <b>hash</b> <b>tables</b> are less efficient than a balanced tree in the worst case (in the presence of many collisions), they perform better in many real applications.|$|R
50|$|In {{computer}} science, a hash list {{is typically}} {{a list of}} hashes of the data blocks in a file or set of files. Lists of hashes are used for many different purposes, such as fast <b>table</b> lookup (<b>hash</b> <b>tables)</b> and distributed databases (distributed <b>hash</b> <b>tables).</b> This article covers hash lists {{that are used to}} guarantee data integrity.|$|R
