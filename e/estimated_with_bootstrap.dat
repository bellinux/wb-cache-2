3|10000|Public
3000|$|This paper {{presents}} a probabilistic method of evaluating the {{final moisture content}} (MC) of lumber obtained {{at the end of}} the kiln-drying process. The final MC data of three different drying tests conducted in past studies were analyzed using the bootstrap method. Target MC was tentatively set below 20  % in the analysis. Two characteristic parameters representing the final MC were <b>estimated</b> <b>with</b> <b>bootstrap</b> confidence intervals. These parameters were the standard deviation (SD) and the percentage of the population that met the MC requirement of less than 20  % (P [...]...|$|E
40|$|Measurements by the hyperspectral spectrometers GOME, SCIAMACHY and GOME- 2 {{are used}} to {{determine}} the rate of linear change (and trends) in cloud top height (CTH) in the period between June 1996 and May 2012. The retrievals are obtained from Top-Of-Atmosphere (TOA) backscattered solar light in the oxygen A-band using the Semi-Analytical CloUd Retrieval Algorithm SACURA. The physical framework relies on the asymptotic equations of radiative transfer, valid for optically thick clouds. Using linear least-squares techniques, a global trend of − 1. 78 ± 2. 14 m yr − 1 in deseasonalized CTH has been found, in the latitude belt within ± 60 °, with diverging tendencies over land (+ 0. 27 ± 3. 2 m yr − 1) and ocean (− 2. 51 ± 2. 8 m yr − 1). The El Niño–Southern Oscillation (ENSO), strongly coupled to CTH, forces clouds to lower altitudes. The global ENSO-corrected trend in CTH amounts to − 0. 49 ± 2. 22 m yr − 1. At a global scale, no explicit regional pattern of statistically significant trends (at 95 % confidence level, <b>estimated</b> <b>with</b> <b>bootstrap</b> technique) have been found, which would be representative of typical natural synoptical features. One exception is North Africa, which exhibits the strongest upward trend in CTH sustained by an increasing trend in water vapour...|$|E
40|$|Tropospheric {{clouds are}} main {{players in the}} Earth climate system. Characterization of {{long-term}} global and regional cloud properties aims to support trace-gases retrieval, radiative budget assessment, and analysis of interactions with particles in the atmosphere. The information needed for the determination of cloud properties can be optimally obtained with satellite remote sensing systems. This is because the amount of reflected solar light depends both on macro- and micro-physical characteristics of clouds. At the time of writing, the spaceborne nadir-viewing Global Ozone Monitoring Experiment (GOME), together with the Scanning Imaging Absorption Spectrometer for Atmospheric Chartography (SCIAMACHY) and GOME- 2, make available a unique record of almost 17 years (June 1996 throughout May 2012) of global top-of-atmosphere (TOA) reflectances and form the observational basis of this work. They probe {{the atmosphere in the}} ultraviolet, visible and infrared regions of the electromagnetic spectrum. Specifically, in order to infer cloud properties such as optical thickness (COT), spherical albedo (CA), cloud base (CBH) and cloud top (CTH) height, TOA reflectances have been selected inside and around the strong absorption band of molecular oxygen in the wavelength range at 758 - 772 nm (the O 2 A-band). The retrieval is accomplished using the Semi-Analytical CloUd Retrieval Algorithm (SACURA). The physical framework relies on the asymptotic parameterizations of radiative transfer. The generated record has been throughly verified against synthetic datasets as function of cloud and surface parameters, sensing geometries, and instrumental specifications and validated against ground-based retrievals. The error budget analysis shows that SACURA retrieves CTH with an average accuracy of ± 400 m, COT within ± 20 % (given that COT > 5) and places CTH closer to ground-based radar-derived CTH, as compared to independent satellite-based retrievals. In the considered time period the global average CTH is 5. 2 ± 3. 0 km, for a corresponding average COT of 20. 5 ± 16. 1 and CA of 0. 62 ± 0. 11. Using linear least-squares techniques, global trend in deseasonalized CTH {{has been found to be}} - 1. 78 ± 2. 14 m * year- 1 in the latitude belt ± 60 °, with diverging tendency over land (0. 27 ± 3. 2 m * year- 1) and water (- 2. 51 ± 2. 8 m * year- 1) masses. The El Nino-Southern Oscillation (ENSO), observed through CTH and cloud fraction (CF) values over the Pacific Ocean, pulls clouds to lower altitudes. It is argued that ENSO must be removed for trend analysis. The global ENSO-cleaned trend in CTH amounts to - 0. 49 ± 2. 22 m * year- 1. At a global scale, no explicit patterns of statistically significant trends (at 95 % confidence level, <b>estimated</b> <b>with</b> <b>bootstrap</b> resampling technique) have been found, which are representative of peculiar natural climate variability. One exception is the Sahara region, which exhibits the strongest upward trend in CTH, sustained by an increasing trend in water vapor. Indeed, the representativeness of every trend is affected by the record length under study. 17 years of cloud data still might not be enough to provide any decisive answer to current open questions involving clouds. The algorithm used in this work can be applied to measurements provided by future planned Earth's observation missions. In this way, the existing cloud record will be extended and attribution of cloud property changes to natural or human causes and assessment of cloud feedback sign within the climate system can be investigated...|$|E
40|$|This paper {{combines}} the least squaress estimate, least absolute deviation estimate, least median <b>estimate</b> <b>with</b> <b>Bootstrap</b> method. When the overall error distribution is unknown {{or it is}} not the normal distribution, we estimate the regression coefficient and confidence interval of coefficient, and through data simulation, obtain Bootstrap method, which can improve stability of regression coefficient and reduce the length of confidence interval...|$|R
40|$|Funded by collage st. {{innovative}} projects This paper {{combines the}} least squaress estimate, least absolute deviation estimate, least median <b>estimate</b> <b>with</b> <b>Bootstrap</b> method. When the overall error distribution is unknown {{or it is}} not the normal distribution, we estimate the regression co-efficient and confidence interval of coefficient, and through data simulation, obtain Bootstrap method, which can improve stability of regression coefficient and reduce the length of confidence interval...|$|R
30|$|QVI were <b>estimated</b> by <b>bootstrapping</b> <b>with</b> {{replacement}} (Efron 1979, Efron and Tibshirani 1986), using 1000 iterations and extracting 50 % of {{the data}} from each iteration.|$|R
30|$|Phylogenetic {{analysis}} of the NPR 1 genes was conducted using MAGA 5.0 with the maximum likelihood (ML) method. The branch support was <b>estimated</b> using <b>bootstrapping</b> <b>with</b> 1, 000 replicates. The sequences {{used in this study}} were obtained from NCBI GenBank and are listed in the supplementary information.|$|R
40|$|Exponential inequalities, {{the law of}} the {{iterated}} logarithm and the bootstrap central limit theorem for U-processes indexed by Vapnik-Cervonenkis {{classes of}} functions are derived. These results are then applied to the asymptotics and the <b>bootstrap</b> of U-statistics <b>with</b> <b>estimated</b> parameters, in particular to the trimming of U-statistics. U-statistics <b>with</b> <b>estimated</b> parameters <b>Bootstrap</b> Trimming VC classes Empirical processes...|$|R
40|$|Background and {{objectives}} This paper analyses productivity {{growth in the}} Norwegian hospital sector {{over a period of}} 16 years, 1999 – 2014. This period was characterized by a large ownership reform with subsequent hospital reorganizations and mergers. We describe how technological change, technical productivity, scale efficiency and the estimated optimal size of hospitals have evolved during this period. Material and methods Hospital admissions were grouped into diagnosis-related groups using a fixed-grouper logic. Four composite outputs were defined and inputs were measured as operating costs. Productivity and efficiency were <b>estimated</b> <b>with</b> <b>bootstrapped</b> data envelopment analyses. Results Mean productivity increased by 24. 6 % points from 1999 to 2014, an average annual change of 1. 5 %. There was a substantial growth in productivity and hospital size following the ownership reform. After the reform (2003 – 2014), average annual growth was < 0. 5 %. There was no evidence of technical change. Estimated optimal size was smaller than the actual size of most hospitals, yet scale efficiency was high even after hospital mergers. However, the later hospital mergers have not been followed by similar productivity growth as around time of the reform. Conclusions This study addresses the issues of both cross-sectional and longitudinal comparability of case mix between hospitals, and thus provides a framework for future studies. The study adds to the discussion on optimal hospital size...|$|R
40|$|In this paper, I {{study the}} {{extension}} of the robust bootstrap [Salibian-Barrera, M., Zamar, R. H., 2002. Bootstrapping robust estimates of regression. Ann. Statist. 30, 556 - 582] to the case of fixed designs. The robust bootstrap is a computer-intensive inference method for robust regression estimators which is computationally simple (because we do not need to re-compute the robust <b>estimate</b> <b>with</b> each <b>bootstrap</b> sample) and robust to the presence of outliers in the bootstrap samples. In this paper, I prove the consistency of this method for the case of non-random explanatory variables and illustrate its use on a real data set. Simulation results indicate that confidence intervals based on the robust bootstrap have good finite-sample coverage levels. Bootstrap Fixed design MM-estimators Robustness Inference Linear Regression...|$|R
40|$|We {{construct}} √(n) -consistent and asymptotically normal {{estimates for}} the finite dimensional regression parameter in the current status linear regression model, which do not require any smoothing device and are based on maximum likelihood estimates (MLEs) of the infinite dimensional parameter. We also construct estimates, again only based on these MLEs, which are arbitrarily close to efficient estimates, if the generalized Fisher information is finite. This type of efficiency is also derived under minimal conditions for estimates based on smooth non-monotone plug-in estimates of the distribution function. Algorithms for computing the estimates and for selecting the bandwidth of the smooth <b>estimates</b> <b>with</b> a <b>bootstrap</b> method are provided. The connection with results in the econometric literature is also pointed out. Comment: 64 pages, 6 figure...|$|R
40|$|We {{review the}} use of {{artificial}} neural networks, particularly the feedforward multilayer perceptron with back-propagation for training (MLP), in ecological modelling. Overtraining on data or giving vague references to how it was avoided is the major problem. Various methods {{can be used to}} determine when to stop training in artificial neural networks: 1) early stopping based on cross-validation, 2) stopping after a analyst defined error is reached or after the error levels off, 3) use of a test data set. We do not recommend the third method as the test data set is then not independent of model development. Many studies used the testing data to optimize the model and training. Although this method may give the best model for that set of data it does not give generalizability or improve understanding of the study system. The importance of an independent data set cannot be overemphasized as we found dramatic differences in model accuracy assessed with prediction accuracy on the training data set, as <b>estimated</b> <b>with</b> <b>bootstrapping,</b> and from use of an independent data set. The comparison of the artificial neural network with a general linear model (GLM) as a standard procedure is recommended because a GLM may perform as well or better than the MLP. MLP models should not be treated as black box models but instead techniques such as sensitivity analyses, input variable relevances, neural interpretation diagrams, randomization tests, and partial derivatives should be used to make the model more transparent, and further our ecological understanding which is an important goal of the modelling process. Based on our experience we discuss how to build a MLP model and how to optimize the parameters and architecture. Comment: 22 pages, 2 figures. Presented in ISEI 3 (2002). Ecological Modelling in pres...|$|R
40|$|Reliable {{earthquake}} depth {{is fundamental}} to many seismological problems. In this paper, we present a method to jointly invert for centroid depths with local (distance < 5 °) seismic waveforms and regional (distance of 5 °– 15 °) Rayleigh wave amplitude spectra on sparse networks. We use earthquake focal mechanisms and magnitudes retrieved with the Cut-and-Paste (CAP) method to compute synthetic amplitude spectra of fundamental mode Rayleigh wave {{for a range of}} depths. Then we grid search to find the optimal depth that minimizes the joint misfit of amplitude spectra and local waveforms. As case studies, we apply this method to the 2008 Wells, Nevada Mw 6. 0 earthquake and a Mw 5. 6 outer-rise earthquake to the east of Japan Trench in 2013. Uncertainties <b>estimated</b> <b>with</b> a <b>bootstrap</b> re-sampling approach show that this joint inversion approach constrains centroid depths well, which are also verified by independent teleseismic depth-phase data...|$|R
40|$|The {{efficiency}} scores {{generated by}} DEA (Data Envelopment Analysis) models are clearly dependent {{on each other}} in the statistical sense. However, this dependency has been ignored in all published uses of these scores when used to make statistical inferences. For example, regression analysis has been widely applied to the analysis of the variation of the DEA efficiency scores. However, the conventional procedure, which has been generally followed in the literature, is invalid. Because of the presence of the inherent dependence among the DEA efficiency scores, one basic model assumption required by regression analysis, independence within the sample, is violated. This paper provides a Bootstrap method to overcome this dependency problem. The core idea is to substitute the incorrect conventional estimators for the standard errors of the regression coefficient <b>estimates</b> <b>with</b> the <b>Bootstrap</b> estimators for the standard errors of these estimates. The method is illustrated using an empirical exa [...] ...|$|R
40|$|International audienceWe {{describe}} a toolkit to fit hydraulic vulnerability curves, {{such as the}} percent loss of xylem hydraulic conductivity ('PLC curves') {{as a function of}} the water potential. The toolkit is implemented as an R package, and is thus free to use and open source. The package fits the Weibull or sigmoidal function to measurements of PLC, conductance or conductivity, at corresponding leaf or stem water potentials. From the fitted curve, estimates of P x (the water potential at which x% conductivity is lost, e. g. the P 50), and slope parameter (S x) are provided together with confidence intervals (CI) around the fitted line. The CIs are <b>estimated</b> <b>with</b> the <b>bootstrap.</b> We also demonstrate the advantages of using mixed-effects models in situations where multiple individuals are measured on a species, as compared to the more traditional approach of fitting curves separately and averaging the parameters. We demonstrate the use of the new package with example data on seven species measured with two different techniques...|$|R
40|$|Graduation date: 2002 Research on the {{distribution}} of juvenile salmonids in streams has been dominated by studies examining small areas over short periods. However, information relevant to freshwater influences on population persistence is likely to derive from longer-term, multi-scale studies. Relationships were examined among juvenile anadromous salmonids, their freshwater habitat, and landscape characteristics throughout the Elk River, Oregon over 7 years at multiple spatial scales. Ocean-type chinook salmon (Oncorhynchus tshawytscha), coho salmon (0. kisutch), coastal cutthroat trout (0. clarki) and winter-run steelhead (0. mykiss) comprised the salmonid assemblage. Habitat selection was quantified at stream system, valley segment, and channel unit scales by selection ratios <b>estimated</b> <b>with</b> <b>bootstrapping</b> methods. Unconstrained valleys in tributaries and pools in the mainstem were typically selected by each species except steelhead, which often avoided these. Valley segment types generally did not differ for characteristics routinely assessed in stream surveys. Thus, fish probably perceived other biotic or abiotic differences among valley segment types. Evidence suggested competition may have influenced selection by coho and chinook salmon. Discriminant analysis indicated that level of use by juvenile chinook salmon appeared related to valley segment type and spatial position. Unconstrained valleys, nearby valley segments, and valley segments with larger, deeper pools, containing more wood were most highly used by chinook salmon. Mean volume and maximum depth of pools were each directly related to catchment area, which explained more variation than landscape characteristics summarized at any of five spatial scales. At each scale except the most spatially extensive, wood density in valley segments was negatively related to the percent area in resistant rock types and positively related to the percent area in mature to old forests. The most variation was explained with these landscape variables summarized at an intermediate spatial scale (i. e., sub-catchment). Although spatial scales appeared similar in processes affecting wood density, finer scales omitted key source areas for wood delivery, and coarser scales included source areas less tightly coupled to wood dynamics in surveyed channels. If only 1 or 2 years of data or one spatial scale had been examined, as commonly occurs, conclusions may have differed substantially from those in this study...|$|R
40|$|Abstract In early April 2007, {{a series}} of {{moderate}} earthquakes (ML 4. 1 – 4. 8) oc-curred {{in the area of}} Trichonis Lake in western Greece. The earthquake activity was well recorded by the Hellenic Broadband Seismic Network (HL) operated by the Na-tional Observatory of Athens. Initial locations for 156 events of the swarm showed a diffuse image of seismicity. Subsequently, 101 events are precisely relocated, calcu-lating source-specific station terms and differential travel times from waveform cross correlation. Uncertainties in relocations are <b>estimated</b> <b>with</b> a <b>bootstrap</b> approach by randomly weighting the original picks and the differential times. Additionally, wave-forms of seven out of the eight largest earthquakes of the swarm were inverted in order to derive regional moment tensor solutions. The results showed a tight north-northwest–south-southeast cluster located on an offshore extension of a similarly oriented fault trace mapped onshore. Moment tensor solutions indicate normal fault-ing with a substantial component of left-lateral strike-slip motion. It is possible that this identified fault forms part of a link that connects the Gulf of Corinth rift system to the west-northwest–east-southeast fault zone south of Trichonis basin. Online Material: Event catalog and focal mechanism for the eight largest events...|$|R
40|$|AbstractIntensity Duration Frequency (IDF) curves {{form the}} basis for {{quantifying}} the magnitude of rainfall events those are used in the design of a variety of civil infrastructure, especially in an urban environment. It is important that the capacity of urban infrastructure (e. g. storm sewers, culverts, and storm water management ponds) be appropriately sized to avoid overdesigned or underdesigned, which could lead to economic losses, increased property damage and possible increased risk of loss of life. Thus, obtaining high quality estimates of IDF curves is important. Uncertainty in IDF curves is usually disregarded in the view of difficulties associated in assigning a value to it. Latin Hypercube Sampling (LHS) and regional frequency analysis based on L-moments approach were utilized in order to estimate the uncertainty in the IDF curves based on historical extreme precipitation quantiles from different stations in the Langat River Basin. Uncertainties of the rainfall intensity in IDF curves were <b>estimated</b> <b>with</b> the <b>bootstrap</b> sampling method, and were described by a GEV distribution. Shape parameter, scale parameter, and location parameter, were modeled as the functions of rainfall duration and rainfall intensity using 103 LHS set samples for all the durations and return periods considered for each rainfall station...|$|R
40|$|In this article, we {{test for}} the {{existence}} of a relationship between per capita Gross Domestic Product (GDP) and trade, for 15 Spanish Autonomous Communities between 1988 and 2004, using a panel cointegration methodology. In particular, we implement several panel unit root tests (Maddala and Wu, 1999; Levin et al., 2002; Im et al., 2003) and panel cointegration tests (Pedroni, 1999, 2004), with a special attention to their behaviour in a small sample. We also develop a Seemingly Unrelated Regression Estimation (SURE) residual based test, in order to explicitly take into account the cross regional correlation pattern. Appropriate confidence intervals are <b>estimated</b> <b>with</b> a sieve <b>bootstrap</b> designed for our small time sample, preserving the dependence structure among cross sectional units. Our cointegration tests reject the existence of a significant relationship between GDP per capita and exports. However, we do find some evidence of a significant relationship between GDP per capita and imports or with total trade. © 2011 Copyright Taylor and Francis Group, LLC...|$|R
40|$|AbstractObjetiveTo compare {{two methods}} in the {{estimation}} of the uncertainty in laboratory quality control. MethodsA computerized simulation is performed to compare the delta method (suggested by the International Organization for Standardization and the Entidad Nacional de Acreditación) and a bootstrap-based method. The simulation includes several situations with different environmental conditions and different relationships between the analyzed variables. ResultsThe mean in the coverage obtained by the estimated confidence intervals is higher {{and closer to the}} nominal using the bootstrap than using the delta method. The most important differences are observed in the coverage percent distribution: while using the bootstrap, a great number of simulations obtain coverages near the nominal of 95 %; using the delta method the coverages are more sparsed, including coverages of 100 % in some ocasions and lesser than 80 % in others. The bootstrap offers very similar results under different conditions, including in the presence of unknown and unmeasured variables or when the analyzed variables are correlated. The delta method shows poorer results in both situations. ConclusionThe uncertainty in the laboratory quality control can be <b>estimated</b> more accurately <b>with</b> <b>bootstrapping</b> than <b>with</b> the delta method...|$|R
30|$|The phylogenetic {{analyses}} were conducted based on maximum likelihood (ML) and Bayesian inference (BI) methods for the individual locus datasets (ITS/trnL-F) and combined dataset (ITS-trnL-F), using RAxML v 7.0. 4 (Stamatakis et al. 2008) and MrBayes v 3.3. 5 (Ronquist et al. 2012), respectively. The model GTR + Ґ {{was selected as the}} optimal model for both DNA regions based on the Akaike Information Criterion via jModeltest v 2.1. 4 (Posada 2008). For ML analyses, node support was <b>estimated</b> <b>with</b> nonparametric <b>bootstrap</b> (1000 replicates) following a thorough search for the best ML tree. For BI analyses, four runs of Metropolis-coupled Markov chain Monte Carlo (MCMCMC) {{analyses were}} conducted with one tree sampled for every 2000 generations over 20 million generations, starting with a random tree. Analyses were run until the average standard deviation of the split frequencies approached 0.01, indicating that two runs converged to a stationary distribution. The first 25 % of sampled trees corresponding to the burn-in period was discarded, and the remaining trees were used to construct a majority-rule consensus tree. We used bootstrap support (BS) ≥  70 % and posterior probability (PP) ≥  0.95 as the thresholds for strongly supported clades (Wang et al. 2014). To investigate congruence between the nuclear and chloroplast genomes, topologies of the ITS and trnL-F datasets of both ML and BI analyses were compared. Because a majority of clades with BS ≥  70 % and PP ≥  0.95 were congruent without significant conflicts, the concatenated dataset was presented for further discussion.|$|R
40|$|In {{northern}} Laos, {{intensification of}} cultivation on sloping land leads to accelerated erosion processes. Management of riparian land may counteract the negative impacts of higher sediment delivery rates on water quality. This study assessed {{water and sediment}} concentration trapping efficiencies of riparian vegetation in northern Laos {{and the effect of}} cultivation of riparian land on water quality. Runoff flowing in and out of selected riparian sites was monitored by means of open troughs. In 2005, two native grass, two bamboo, and two banana sites were monitored. In 2006, adjacent to steep banana, bamboo, and native grass sites, three upland rice sites were established and monitored. Water trapping efficiency (WTE) and sediment concentration trapping efficiency (SCTE) were calculated on an event basis; means and 95 % confidence intervals (CIs) were <b>estimated</b> <b>with</b> a <b>bootstrapping</b> approach. Confidence intervals were large and overlapping among sites. Seepage conditions severely limited trapping efficiency. Native grass resulted in the highest WTE (95 % CI, - 0. 10 to 0. 23), which was not significantly different from zero. Banana resulted in the highest SCTE (95 % CI, 0. 06 - 0. 40). Bamboo had negative WTE and SCTE. Median outflow runoff from rice sites was nine times the inflow. Median outflow sediment concentration from rice sites was two to five times that of their adjacent sites and two to five times the inflow sediment concentration. Although low-tillage banana plantation may reduce sediment concentration of runoff, cultivation of annual crops in riparian land leads to delivery of turbid runoff into the stream, thus severely affecting stream water quality...|$|R
5000|$|Phylogenetic tree {{construction}} UPGMA, Neighbour joining <b>with</b> <b>bootstrapping</b> {{and consensus}} trees ...|$|R
3000|$|... <b>with</b> <b>bootstrapped</b> {{confidence}} intervals by using cross-validation and bootstrap resampling as described above.|$|R
40|$|FIGURE 152. Consensus tree of Ichneumonosoma, Pelmatops, Pseudopelmatops, Soita and outgroup members {{obtained}} by TNT (resample the matrix <b>with</b> standard <b>bootstrap</b> method by using replicates 100 and traditional search) based on 27 adult morphological characters, <b>with</b> <b>bootstrap</b> shown {{next to the}} branches...|$|R
3000|$|... {{by using}} the CPF. The major steps of the CPF {{approach}} <b>with</b> <b>bootstrap</b> sampling are summarized by the following pseudocode [...]...|$|R
5000|$|Sensitivity analysis: A {{procedure}} {{to study the}} behavior of a system or model when global parameters are (systematically) varied. One {{way to do this is}} <b>with</b> <b>bootstrapping.</b>|$|R
30|$|The {{phylogenetic tree}} was {{constructed}} with MEGA version 4.0 using a neighbor-joining algorithm, and the Jukes-Cantor distance estimation method was performed <b>with</b> <b>bootstrap</b> analyses for 1000 replicates (Hesham et al. 2012).|$|R
30|$|For the {{heterogeneous}} treatment specifications, {{we could}} not <b>estimate</b> the parameters <b>with</b> the household weights as STATA does not enable the use of sampling weights <b>with</b> <b>bootstrapped</b> standard errors. However, {{to the extent that}} bootstrapped standard errors enable consistent estimation of parameters given misspecification (Goncalves and White 2005)—in this the omission of sampling weights—bootstrap estimates of the treatment effects without the sampling weights can mitigate/eliminate the bias caused by the sample that may not be fully representative of the population of interest.|$|R
40|$|Figure 1 - Species {{phylogeny}} of Malagasy Crematogaster (Orthocrema). A Results of Bayesian inference {{summarized as}} consensus tree in MrBayes. Support values on branches represent posterior probabilities; scalebar shows nucleotide changes per base pair. Newly defined species-groups, and the specimen representing former Crematogaster voeltzkowi are indicated. Outgroup species {{are marked by}} blue font B ML-consensus tree <b>with</b> <b>bootstrap</b> support values obtained from analysis <b>with</b> 100 <b>bootstrap</b> replicates in GARLI 2. 0...|$|R
50|$|The {{phylogeny}} {{shown below}} {{is based on}} the one published by Shuguang Jian and coauthors in 2008. All branches have 100% maximum likelihood bootstrap support except where labeled <b>with</b> <b>bootstrap</b> percentage. Monogeneric families are represented by genus names.|$|R
40|$|This paper {{describes}} {{applications of}} non-parametric and parametric methods for estimating forest growing stock volume using Landsat {{images on the}} basis of data measured in the field, integrated with ancillary information. Several k-Nearest Neighbors (k-NN) algorithm configurations were tested in two study areas in Italy belonging to Mediterranean and Alpine ecosystems. Field data were acquired by the regional forest inventory and forest management plans, and satellite images are from Landsat 5 TM and Landsat 7 ETM+. The paper describes the data used, the methodologies adopted and the results achieved in terms of pixel level accuracy of forest growing stock volume estimates. The results show that several factors affect estimation accuracy when using the k-NN method. For the two test areas a total of 3500 different configurations of the k-NN algorithm were systematically tested by changing the number and type of spectral and ancillary input variables, type of multidimensional distance measures, number of nearest neighbors and methods for spectral feature extraction using the leave-one-out (LOO) procedure. The best k-NN configurations were then used for pixel level estimation; the accuracy was <b>estimated</b> <b>with</b> a <b>bootstrapping</b> procedure; and the results were compared to estimates obtained using parametric regression methods implemented on the same data set. The best k-NN growing stock volume pixel level estimates in the Alpine area have a Root Mean Square Error (RMSE) ranging between 74 and 96 m 3 ha− 1 (respectively, 22 % and 28 % of the mean measured value) and between 106 and 135 m 3 ha− 1 (respectively, 44 % and 63 % of the mean measured value) in the Mediterranean area. On the whole, the results cast a promising light on the use of non-parametric techniques for forest attribute estimation and mapping with accuracy high enough to support forest planning activities in such complex landscapes. The results of the LOO analyses also highlight the importance of a local empirical optimization phase of the k-NN procedure before defining the best algorithm configuration. In the tests performed the pixel level accuracy increased, depending on the k-NN configuration, as much as 100 %. L'articolo è disponibile sul sito dell'editore www. sciencedirect. co...|$|R
50|$|The {{motivation}} for its designed {{was to provide}} {{the means by which}} some time-series econometric procedures that were difficult or unavailable in other packages could be undertaken. Such procedures include Impulse Response Analysis <b>with</b> <b>bootstrapped</b> confidence intervals for VAR/VEC modelling.|$|R
5000|$|Kickstart is an {{evolution}} of [...] "Kickstrap", {{which was designed}} to be a layer above Bootstrap. Kickstrap allows users to create their own customization and is loosely coupled <b>with</b> <b>Bootstrap</b> core. Thus it does not hinder any updates on Bootstrap.|$|R
40|$|FIGURES 41 – 42. 41 Phylogenetic {{relationships}} of Ixodes myrmecobii isolates obtained {{in this study}} (T 4, T 12, T 15 and T 16) with published reference material available at the CO 1 locus. Evolutionary history was inferred using the neighbour-joining method supported <b>with</b> <b>bootstrap</b> test of 1000 replicates (values> 50 % shown). Rhipicephalus sanguineus is included as the out group, 42 Phylogenetic {{relationships of}} Ixodes myrmecobii isolates obtained in this study (T 4, T 12, T 15 and T 16) with published reference material available at the ITS 2 locus. Evolutionary history was inferred using the neighbour-joining method supported <b>with</b> <b>bootstrap</b> test of 1000 replicates (values> 50 % shown). Rhipicephalus sanguineus is included as the out group...|$|R
40|$|Parametric {{proportional}} hazards {{fitting with}} left truncation and right censoring for common fami-lies of distributions,piecewise constant hazards, and discrete models. AFT regression for left truncated and right censored data. Binary and Poisson regression for clustered data, fixed and random effects <b>with</b> <b>bootstrapping...</b>|$|R
