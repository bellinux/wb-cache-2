10000|10000|Public
5|$|This section {{presents}} {{two examples}} of calculating the maximum spacing <b>estimator.</b>|$|E
5|$|The maximum spacing <b>estimator</b> is a {{consistent}} <b>estimator</b> {{in that it}} converges in probability to the true value of the parameter, θ0, as the sample size increases to infinity. The consistency of maximum spacing estimation holds under much more general conditions than for maximum likelihood estimators. In particular, {{in cases where the}} underlying distribution is J-shaped, maximum likelihood will fail where MSE succeeds. An example of a J-shaped density is the Weibull distribution, specifically a shifted Weibull, with a shape parameter less than 1. The density will tend to infinity as x approaches the location parameter rendering estimates of the other parameters inconsistent.|$|E
25|$|Mean squared error, {{a measure}} of how 'good' an <b>estimator</b> of a {{distributional}} parameter is (be it the maximum likelihood <b>estimator</b> or some other <b>estimator).</b>|$|E
40|$|In this paper, we {{consider}} several methods for estimating {{the location and}} scale parameters of the half-logistic distribution. The <b>estimators</b> considered are the maximum likelihood <b>estimators,</b> the approximate maximum likelihood <b>estimators,</b> method of moment <b>estimators,</b> <b>estimators</b> based on percentiles, least squares <b>estimators,</b> weighted least squares <b>estimators</b> and the <b>estimators</b> based on the linear combinations of order statistics. These <b>estimators</b> are compared via Monte Carlo simulations {{in terms of their}} biases and mean square errors...|$|R
50|$|Population {{parameters}} are estimated with many point <b>estimators.</b> Popular families of point-estimators include mean-unbiased minimum-variance <b>estimators,</b> median-unbiased <b>estimators,</b> Bayesian <b>estimators</b> (for example, the posterior distribution's mode, median, mean), and maximum-likelihood <b>estimators.</b>|$|R
40|$|We {{proposed}} several <b>estimators</b> for {{the negative}} binomial dispersion parameter. The proposed <b>estimators</b> are combinations of existing ones using appropriate weights. We then compare, by simulation, the biases and efficiencies {{of the proposed}} <b>estimators</b> {{with those of the}} method of moments <b>estimators</b> and the maximum quasi-likelihood <b>estimators.</b> The simulation results indicate that the proposed <b>estimators</b> perform well in terms of biases and efficiencies in many instances. We conclude from this study that the combined <b>estimators</b> significantly reduce the mean bias of the <b>estimators</b> and more efficient than existing <b>estimators.</b> The relative efficiencies of all the combined <b>estimators</b> increases as the sample size increases...|$|R
25|$|Quasi-maximum {{likelihood}} <b>estimator,</b> an MLE <b>estimator</b> that is misspecified, {{but still}} consistent.|$|E
25|$|The Rao–Blackwell theorem {{states that}} if g(X) is {{any kind of}} <b>estimator</b> of a {{parameter}} θ, then the conditional expectation of g(X) given T(X), where T is a sufficient statistic, is typically a better <b>estimator</b> of θ, and is never worse. Sometimes one can very easily construct a very crude <b>estimator</b> g(X), and then evaluate that conditional expected value to get an <b>estimator</b> that is in various senses optimal.|$|E
25|$|The mean squared {{error of}} the Rao–Blackwell <b>estimator</b> does not exceed that of the {{original}} <b>estimator.</b>|$|E
30|$|As we expect, {{the results}} {{reported}} in Table  4 show that ML <b>estimators</b> have both smaller one-step-ahead forecast bias and less MSE than OLS <b>estimators.</b> This reveals that ML <b>estimators</b> exhibit superior performance to OLS <b>estimators.</b> This confirms {{the fact that}} deviations from normality cause OLS <b>estimators</b> to be poor <b>estimators.</b>|$|R
40|$|We {{present a}} new class of <b>estimators</b> for {{approximating}} the entropy of multi-dimensional probability densities based on a sample of the density. These <b>estimators</b> extend the classic "m-spacing" <b>estimators</b> of Vasicek and others for estimating entropies of one-dimensional probability densities. Unlike plug-in <b>estimators</b> of entropy, which first estimate a probability density and then compute its entropy, our <b>estimators</b> avoid the difcult intermediate step of density estimation. For fixed dimension, the <b>estimators</b> are polynomial in the sample size. Similarities to consistent and asymptotically efficient one-dimensional <b>estimators</b> of entropy suggest that our <b>estimators</b> may share these properties...|$|R
40|$|Simultaneous {{estimation}} of variance components under quadratic risk is discussed. The <b>estimators</b> considered are scale preserving and location invariant and permit a simple closed expression for risk. For several one-way random normal models, nonnegative <b>estimators</b> are constructed which {{are comparable to}} the maximum likelihood <b>estimators</b> and to some of Portnoy's (1971) <b>estimators.</b> Variance components Quadratic risk function Improved <b>estimators</b> Bayesian <b>estimators...</b>|$|R
25|$|This <b>estimator</b> is {{unbiased}} up to {{the terms}} of order n−1, and is called the bias-corrected maximum likelihood <b>estimator.</b>|$|E
25|$|For {{univariate}} distributions {{that are}} symmetric about one median, the Hodges–Lehmann <b>estimator</b> is a robust and highly efficient <b>estimator</b> {{of the population}} median.|$|E
25|$|In its {{simplest}} form, the bound {{states that}} the variance of any unbiased <b>estimator</b> {{is at least as}} high as the inverse of the Fisher information. An unbiased <b>estimator</b> which achieves this lower bound is said to be (fully) efficient. Such a solution achieves the lowest possible mean squared error among all unbiased methods, and is therefore the minimum variance unbiased (MVU) <b>estimator.</b> However, in some cases, no unbiased technique exists which achieves the bound. This may occur even when an MVU <b>estimator</b> exists.|$|E
3000|$|... we may speak thus of non-randomized CoD <b>estimators,</b> {{including}} the resubstitution and leave-one-out CoD <b>estimators,</b> and randomized CoD <b>estimators,</b> including bootstrap and cross-validation CoD <b>estimators.</b> The CoD with the true values of [...]...|$|R
40|$|In {{this article}} we provide some nonnegative and {{positive}} <b>estimators</b> of the mean squared errors(MSEs) for shrinkage <b>estimators</b> of multivariate normal means. Proposed <b>estimators</b> are shown to improve on the uniformly minimum variance unbiased estimator(UMVUE) under a quadratic loss criterion. A similar improvement is also obtained for the <b>estimators</b> of the MSE matrices for shrinkage <b>estimators.</b> We also apply the proposed <b>estimators</b> of the MSE matrix to form confidence sets centered at shrinkage <b>estimators</b> and show their usefulness through numerical experiments. Comment: 29 page...|$|R
40|$|Separate ratio-type <b>estimators</b> for {{population}} mean with their properties are considered. Some separate ratio-type <b>estimators</b> {{for population}} mean using known parameters of auxiliary variate are proposed. The bias and {{mean squared error}} of the proposed <b>estimators</b> are obtained {{up to the first}} degree of approximation. It is shown that the proposed <b>estimators</b> are more efficient than unbiased <b>estimators</b> in stratified random sampling and usual separate ratio <b>estimators</b> under certain obtained conditions. To judge the merits of the proposed <b>estimators,</b> an empirical study was conducted...|$|R
25|$|A model-assisted <b>estimator</b> and {{synthetic}} <b>estimator</b> both gave accurate {{measures of the}} amount of biomass that had been removed, as confirmed by ground-based checks.|$|E
25|$|If both {{endpoints}} are unknown, {{then the}} sample range is a biased <b>estimator</b> {{for the population}} range, but correcting as for maximum above yields the UMVU <b>estimator.</b>|$|E
25|$|If <b>estimator</b> Tn {{is defined}} implicitly, for example as a value that maximizes certain {{objective}} function (see extremum <b>estimator),</b> then {{a more complicated}} argument involving stochastic equicontinuity has to be used.|$|E
40|$|We {{consider}} sampling with probability {{proportional to}} size or aggregate size and derive {{a number of}} <b>estimators</b> of the finite population distribution function. The <b>estimators</b> are compared theoretically and empirically. By inverting the distribution function <b>estimators,</b> we obtain <b>estimators</b> of the finite population median. The performance of these median <b>estimators</b> are investigated...|$|R
40|$|For a panel data {{regression}} equation with two-way unobserved heterogeneity, individual-specific and period-specific, 'within-individual' and 'within-period' <b>estimators,</b> {{which can be}} given Ordinary Least Squares (OLS) or Instrumental Variables (IV) interpretations, are considered. A class of <b>estimators</b> defined as linear aggregates of these <b>estimators,</b> is defined. Nine aggregate <b>estimators,</b> including between, within, and Generalized Least Squares (GLS), are special cases. Other <b>estimators</b> are shown to be more robust to simultaneity and measurement error bias than the standard aggregate <b>estimators</b> and more efficient than the 'disaggregate' <b>estimators.</b> Empirical illustrations relating to manufacturing productivity are given...|$|R
40|$|AbstractThis paper proposes some <b>estimators</b> for the {{population}} mean using the ratio <b>estimators</b> presented in [C.  Kadilar, H. Cingi, Ratio <b>estimators</b> in simple random sampling, Applied Mathematics and Computation 151 (2004) 893 – 902] and shows that all proposed <b>estimators</b> are always more efficient than the ratio <b>estimators.</b> This result is also supported by a numerical example...|$|R
25|$|The OLS <b>estimator</b> is {{identical}} to the maximum likelihood <b>estimator</b> (MLE) under the normality assumption for the error terms. This normality assumption has historical importance, as it provided the basis for the early work in linear regression analysis by Yule and Pearson. From the properties of MLE, we can infer that the OLS <b>estimator</b> is asymptotically efficient (in the sense of attaining the Cramér–Rao bound for variance) if the normality assumption is satisfied.|$|E
25|$|In statistics, the Rao–Blackwell theorem, {{sometimes}} referred to as the Rao–Blackwell–Kolmogorov theorem, is a result which characterizes the transformation of an arbitrarily crude <b>estimator</b> into an <b>estimator</b> that is optimal by the mean-squared-error criterion or any of a variety of similar criteria.|$|E
25|$|The maximum {{likelihood}} <b>estimator</b> is consistent.|$|E
30|$|In this section, HOS <b>estimators</b> are developed. Generally, HOS <b>estimators</b> can {{be divided}} into main families: the {{arithmetic}} and the exponential <b>estimators.</b>|$|R
40|$|This paper {{considers}} new semiparametric <b>estimators</b> (called pairwise-difference rank <b>estimators)</b> of {{the coefficient}} vector in a transformation model. The Monte Carlo simulations, including a simulation based on wage-equation estimation, {{indicate that the}} proposed <b>estimators</b> perform well in small samples and exhibit substantial efficiency gains compared to existing semiparametric <b>estimators.</b> The pairwise-difference rank <b>estimators</b> require no subjective bandwidth choice and have smooth objective functions (relative to existing rank <b>estimators).</b> Key words: Transformation model; rank estimation; semiparametric estimation...|$|R
40|$|We are {{concerned}} with a counter-matched nested case-control study. Assuming the proportional hazards model, the Mantel-Haenszel <b>estimators</b> of hazard rates are presented in two situations. The proposed <b>estimators</b> can be calculated without estimating the nuisance parameter. Consistent <b>estimators</b> of the variance of the proposed hazard rate <b>estimators</b> are also developed. We compare these <b>estimators</b> to the maximum partial likelihood <b>estimators</b> in the asymptotic variance. The methods are illustrated using the Colorado Plateau uranium miner cohort data...|$|R
25|$|LASSO <b>estimator</b> {{is another}} regularization method in statistics.|$|E
25|$|The Watterson <b>estimator</b> for the {{population}} mutation rate in population genetics.|$|E
25|$|Extremum <b>estimator,</b> a {{more general}} class of estimators to which MLE belongs.|$|E
40|$|AbstractMinimax-linear {{estimation}} {{with respect}} to the quadratic risk is considered among the class of linear <b>estimators</b> of β, under the linear regression model M = {y,Xβ,σ 2 I}. New classes of minimax-linear <b>estimators</b> of β are derived among certain subsets of linear <b>estimators,</b> which are simple {{from the point of view}} of minimax estimation. The admissible linear <b>estimators</b> of β under M are then characterized via these classes of minimax-linear <b>estimators,</b> and the relationship between the minimax-linear and admissible <b>estimators</b> of β is examined. Various properties of linear admissible <b>estimators</b> follow readily from this new characterization...|$|R
40|$|In this paper, {{we propose}} {{a new class}} of {{asymptotically}} efficient <b>estimators</b> for moment condition models. These <b>estimators</b> share the same higher order bias properties as the generalized empirical likelihood <b>estimators</b> and once bias corrected, have the same higher order efficiency properties as the bias corrected generalized empirical likelihood <b>estimators.</b> Unlike the generalized empirical likelihood <b>estimators,</b> our new <b>estimators</b> are much easier to compute. A simulation study finds that our <b>estimators</b> have better finite sample performance than the two-step GMM, and compare well to several potential alternatives in terms of both computational stability and overall performance. ...|$|R
40|$|Singh and Kumar (2011) {{suggested}} <b>estimators</b> for calculating {{population variance}} using auxiliary attributes. This paper proposes {{a family of}} <b>estimators</b> based on an adaptation of the <b>estimators</b> presented by Kadilar and Cingi (2004) and Singh et al. (2007), and introduces a new family of <b>estimators</b> using auxiliary attributes. The expressions of the mean square errors (MSEs) of the adapted and proposed families are derived. It is shown that adapted <b>estimators</b> and suggested <b>estimators</b> are more efficient than Singh and Kumar (2011) <b>estimators.</b> The theoretical findings are supported by a numerical example. Comment: 14 pages, 4 table...|$|R
