95|288|Public
5000|$|K {{is another}} secret key, {{derived from the}} {{original}} key K (by padding K to the right with extra zeroes to the <b>input</b> <b>block</b> size of the hash function, or by hashing K if it is longer than that block size), ...|$|E
5000|$|If the tweak key K2 is known, LRW {{does not}} offer indistinguishability under chosen {{plaintext}} attack (IND-CPA) anymore, and the same <b>input</b> <b>block</b> permutation attacks of ECB mode are possible. Leak of the tweak key {{does not affect the}} confidentiality of the plaintext.|$|E
5000|$|Break {{the input}} into 256 4-bit blocks, and map each {{through one of}} two 4-bit S-boxes, the choice being made by a 256-bit round-dependent key schedule. Equivalently, combine each <b>input</b> <b>block</b> with a key bit, and map the result through a 5→4 bit S-box.|$|E
50|$|HAS-160 {{is used in}} {{the same}} way as SHA-1. First it divides <b>input</b> in <b>blocks</b> of 512 bits each and pads the final block. A digest {{function}} updates the intermediate hash value by processing the <b>input</b> <b>blocks</b> in turn.|$|R
5000|$|For each key K, EK is a {{permutation}} (a bijective mapping) {{over the}} set of <b>input</b> <b>blocks.</b> Each key selects one permutation from the possible set of [...]|$|R
50|$|An {{example of}} this is the {{function}} block diagram, one of five programming languages defined in part 3 of the IEC 61131 (see IEC 61131-3) standard that is highly formalized (see formal system), with strict rules for how diagrams are to be built. Directed lines are used to connect <b>input</b> variables to <b>block</b> <b>inputs,</b> and <b>block</b> outputs to output variables and <b>inputs</b> of other <b>blocks.</b>|$|R
50|$|For comparison, classic turbo codes {{typically}} use two constituent codes configured in parallel, each {{of which}} encodes the entire <b>input</b> <b>block</b> (K) of data bits. These constituent encoders are recursive convolutional codes (RSC) of moderate depth (8 or 16 states) that are separated by a code interleaver which interleaves one copy of the frame.|$|E
5000|$|JH is a {{cryptographic}} hash function {{submitted to the}} NIST hash function competition by Hongjun Wu. Though chosen {{as one of the}} five finalists of the competition, JH ultimately lost to NIST hash candidate Keccak. JH has a 1024-bit state, and works on 512-bit input blocks. Processing an <b>input</b> <b>block</b> consists of three steps: ...|$|E
5000|$|The Lai-Massey scheme offers {{security}} properties {{similar to}} those of the Feistel structure. It also shares its advantage that the round function [...] does not have to be invertible. Another similarity is that is also splits the <b>input</b> <b>block</b> into two equal pieces. However, the round function is applied to the difference between the two, and the result is then added to both half blocks.|$|E
5000|$|When the {{duration}} of non-zero values of [...] is M < N, the output sequence includes N − M + 1 samples of [...] M-1 outputs are discarded from each block of N, and the <b>input</b> <b>blocks</b> are overlapped by that amount to prevent gaps.|$|R
40|$|This paper {{reports on}} {{implementation}} of gradi-ent computation for real-time dynamic optimiza-tion, where the dynamic models can be Model-ica models. Analytical methods for gradient com-putation based on sensitivity integration is com-pared to finite difference-based methods. A case study reveals that analytical methods outperforms finite difference-methods {{as the number}} of <b>inputs</b> and/or <b>input</b> <b>blocks</b> increases...|$|R
40|$|The {{design of}} a {{skylight}} intensity data logging system is presented. The proposed system is able to accept 32 inputs in two 16 <b>input</b> <b>blocks</b> with capabilities for further expansion. A typical application would be to simultaneously monitor real time change in light intensity in two skylight units, over a chosen periods of minutes, hours or days...|$|R
5000|$|The Hasty Pudding Cipher (HPC) is a variable-block-size {{block cipher}} {{designed}} by Richard Schroeppel, {{which was an}} unsuccessful candidate in the competition for selecting the U.S. Advanced Encryption Standard (AES). It {{has a number of}} unusual properties for a block cipher: its <b>input</b> <b>block</b> size and key length are variable, and it includes an additional input parameter called the [...] "spice" [...] for use as a secondary, non-secret key. The Hasty Pudding cipher was the only AES candidate designed exclusively by U.S. cryptographers.|$|E
5000|$|In an RA code, an {{information}} block of length [...] is repeated [...] times, scrambled by an interleaver of size , and then encoded by a rate 1 accumulator. The accumulator {{can be viewed}} as a truncated rate 1 recursive convolutional encoder with transfer function , but Divsalar et al. prefer to think of it as a block code whose <b>input</b> <b>block</b> [...] and output block [...] are related by the formula [...] and [...] for [...] The encoding time for RA codes is linear and their rate is [...] They are nonsystematic.|$|E
50|$|BLAKE and BLAKE2 are {{cryptographic}} hash functions based on Dan Bernstein's ChaCha stream cipher, but a permuted {{copy of the}} <b>input</b> <b>block,</b> XORed with some round constants, is added before each ChaCha round. Like SHA-2, there are two variants differing in the word size. ChaCha operates on a 4×4 array of words. BLAKE repeatedly combines an 8-word hash value with 16 message words, truncating the ChaCha result to obtain the next hash value. BLAKE-256 and BLAKE-224 use 32-bit words and produce digest sizes of 256 bits and 224 bits, respectively, while BLAKE-512 and BLAKE-384 use 64-bit words and produce digest sizes of 512 bits and 384 bits, respectively.|$|E
50|$|The {{number of}} {{recovery}} blocks {{is given by}} the user. Then the number of levels is determined along {{with the number of}} blocks in each level. The number in each level is determined by a factor B which is less than one. If there are N <b>input</b> <b>blocks,</b> the first recovery level has B*N blocks, the second has B*B*N, the third has B*B*B*N, and so on.|$|R
3000|$|... bits {{are needed}} for error {{detection}} in our proposed method. Hence, to solve this problem, we can compensate for this shortcoming by assuming greater lengths for the <b>input</b> <b>blocks</b> in the proposed encoder. However, we know that adding security and error detection capability to a compression encoder often leads to a compromise between the amounts of compression achieved {{and the amount of}} security and the robustness against channel errors incorporated.|$|R
50|$|So the {{recovery}} blocks in level one {{are just the}} xor of some set of <b>input</b> <b>blocks.</b> Similarly, {{the recovery}} blocks in level two are each the xor of some set of blocks in level one. The blocks used in the xor are chosen randomly, without repetition. However, the number of blocks xor'ed to make a recovery block is chosen from a very specific distribution for each level.|$|R
5000|$|This method uses a {{block size}} {{equal to the}} FFT size (1024). We {{describe}} it first in terms of normal or linear convolution. When a normal convolution is performed on each block, there are start-up and decay transients at the block edges, due to the filter latency (200-samples). Only 824 of the convolution outputs are unaffected by edge effects. The others are discarded, or simply not computed. That would cause gaps in the output if the input blocks are contiguous. The gaps are avoided by overlapping the input blocks by 200 samples. In a sense, 200 elements from each <b>input</b> <b>block</b> are [...] "saved" [...] and carried over to the next block. This method {{is referred to as}} overlap-save, although the method we describe next requires a similar [...] "save" [...] with the output samples.|$|E
5000|$|A {{block cipher}} {{consists}} of two paired algorithms, one for encryption, E, {{and the other for}} decryption, D. Both algorithms accept two inputs: an <b>input</b> <b>block</b> of size n bits and a key of size k bits; and both yield an n-bit output block. The decryption algorithm D is defined to be the inverse function of encryption, i.e., D = E−1. More formally, a block cipher is specified by an encryption functionwhich takes as input a key K of bit length k, called the key size, and a bit string P of length n, called the block size, and returns a string C of n bits. P is called the plaintext, and C is termed the ciphertext. For each K, the function EK(P) is required to be an invertible mapping on {0,1}n. The inverse for E is defined as a functiontaking a key K and a ciphertext C to return a plaintext value P, such that ...|$|E
5000|$|The {{reason for}} {{diffusion}} is the following: If one changes {{one bit of}} the plaintext, then it is fed into an S-box, whose output will change at several bits, then all these changes are distributed by the P-box among several S-boxes, hence the outputs {{of all of these}} S-boxes are again changed at several bits, and so on. Doing several rounds, each bit changes several times back and forth, therefore, by the end, the ciphertext has changed completely, in a pseudorandom manner. In particular, for a randomly chosen <b>input</b> <b>block,</b> if one flips the i-th bit, then the probability that the j-th output bit will change is approximately a half, for any i and j, which is the Strict Avalanche Criterion. Vice versa, if one changes one bit of the ciphertext, then attempts to decrypt it, the result is a message completely different from the original plaintext—SP ciphers are not easily malleable.|$|E
30|$|The {{technical}} {{details of}} our {{implementation of the}} hierarchical model are as follows. The lower network part consisted of the convolutional network described in earlier sections with two modifications. First, {{the size of the}} input layer was decreased from 17 frames to just 9 frames. This input size seems to be sufficient [27] because of the overlap of the local <b>input</b> <b>blocks</b> (see Fig. 7). Thus, the hierarchical model covers 29 frames of input vectors via its 5 local <b>input</b> <b>blocks</b> of 9 - 9 frames, which overlap by 5 frames. The second modification is that the size of the uppermost hidden layer was reduced to its fifth to form a bottleneck layer. This way, the input to the upper network part (consisting of five vectors from the lower part) was just of the same size as that for all the other layers. The network was the same CNN as that used earlier in all other respects, that is, it consisted of one convolutional layer and three hidden layers of maxout units, with the layer sizes given earlier.|$|R
5000|$|This {{method is}} known as overlap-add. [...] In our example, it uses {{contiguous}} <b>input</b> <b>blocks</b> of size 824 and pads each one with 200 zero-valued samples. Then it overlaps and adds the 1024-element output blocks. Nothing is discarded, but 200 values of each output block must be [...] "saved" [...] for the addition with the next block. Both methods advance only 824 samples per 1024-point IFFT, but overlap-save avoids the initial zero-padding and final addition.|$|R
40|$|We {{provide a}} general {{formulation}} for the code-based test compression problem with fixed-length <b>input</b> <b>blocks</b> and propose a solution approach based on Evolutionary Algorithms. In contrast to existing code-based methods, we allow unspecified values in matching vectors, which allows encoding of arbitrary test sets using {{a relatively small}} number of code-words. Experimental results for both stuck-at and path delay fault test sets for ISCAS circuits demonstrate an improvement compared to existing techniques. Comment: Submitted on behalf of EDAA ([URL]...|$|R
40|$|When it is {{necessary}} to securely transmit data in open networks, the encryption must be performed. Most of the cryptographic algorithms were mainly developed for text data. Unfortunately, algorithms those are good for textual data might not suitable for image. The OFB mode of encryption implemented to test five images of different resources, by using three combinations (schemes) : a combination of an 8 -bit <b>input</b> <b>block</b> with an 8 -bit feedback block, a combination of an 8 -bit <b>input</b> <b>block</b> with a 16 -bit, or a combination of a 16 -bit <b>input</b> <b>block</b> with a 16 -bit feedback block. Results showed that higher degree of encryption achieved when the <b>input</b> <b>block</b> and the feedback block are of the same size. The OFB mode achieved a mid range level of encryption with a mean entropy value of 7. 93...|$|E
40|$|Abstract: When it is {{necessary}} to securely transmit data in open networks, the encryption must be performed. Most of the cryptographic algorithms were mainly developed for text data. Unfortunately, algorithms those are good for textual data might not suitable for image. The OFB mode of encryption implemented to test five images of different resources, by using three combinations (schemes) : a combination of an 8 -bit <b>input</b> <b>block</b> with an 8 -bit feedback block, a combination of an 8 -bit <b>input</b> <b>block</b> with a 16 -bit, or a combination of a 16 -bit <b>input</b> <b>block</b> with a 16 -bit feedback block. Results showed that higher degree of encryption achieved when the <b>input</b> <b>block</b> and the feedback block are of the same size. The OFB mode achieved a mid range level of encryption with a mean entropy value of 7. 93. Key words: Image encryption, output feedback mode, encryption modes, image processin...|$|E
3000|$|... are integer, and {{in-between}} {{values are}} {{the sum of}} all the time input symbols in the <b>input</b> <b>block</b> with different complex weighting [19].|$|E
50|$|There {{are also}} methods {{for dealing with}} an x {{sequence}} that is longer than a practical value for N. The sequence is divided into segments (blocks) and processed piecewise. Then the filtered segments are carefully pieced back together. Edge effects are eliminated by overlapping either the <b>input</b> <b>blocks</b> or the output blocks. To help explain and compare the methods, we discuss them both {{in the context of}} an h sequence of length 201 and an FFT size of N = 1024.|$|R
40|$|MASH- 1 is modular {{arithmetic}} based hash function. It {{is presented}} in Part 4 of ISO/IEC 10118 standard for {{one and a half}} decade. Cryptographic strength of MASH- 1 hash function is based on factorization problem of an RSA modulus along with redundancy in the <b>input</b> <b>blocks</b> of compression functions. Despite of this, we are able to introduce two large classes of moduli which allow practical time collision finding algorithm for MASH- 1. In one case even multicollisions of arbitrary length can be constructed...|$|R
40|$|Unit {{electrical}} activity was recorded from single neurons in the isolated lumbo-sacral spinal cord of 14 - to 19 -day chick embryos, in situ. Spinal cord transection was combined with transection of all lumbo-sacral dorsal roots. The spontaneous discharge of cells is confined, {{for the most}} part, to the lower {{two thirds of the}} cord. A quasilinear {{reduction in the number of}} spontaneously active units was found during the developmental period studied. Comparable results were obtained in decentralized cord with sensory <b>inputs</b> <b>blocked</b> by xylocaine...|$|R
40|$|A {{recently}} developed method of constructing protograph-based low-density parity-check (LDPC) codes provides for low iterative decoding thresholds and minimum distances proportional to block sizes, {{and can be}} used for various code rates. A code constructed by this method can have either fixed <b>input</b> <b>block</b> size or fixed output block size and, in either case, provides rate compatibility. The method comprises two submethods: one for fixed <b>input</b> <b>block</b> size and one for fixed output block size. The first mentioned submethod is useful for applications in which there are requirements for rate-compatible codes that have fixed <b>input</b> <b>block</b> sizes. These are codes in which only the numbers of parity bits are allowed to vary. The fixed-output-blocksize submethod is useful for applications in which framing constraints are imposed on the physical layers of affected communication systems. An example of such a system is one that conforms to one of many new wireless-communication standards that involve the use of orthogonal frequency-division modulatio...|$|E
40|$|Abstract—The {{picture quality}} of {{conventional}} memory vector quantization techniques {{is limited by}} their supercodebooks. This paper presents a new dynamic finite-state vector quantization (DFSVQ) algorithm which provides better quality than the best quality that the supercodebook can offer. The new DFSVQ exploits the global interblock correlation of image blocks instead of local correlation in conventional DFSVQs. For an <b>input</b> <b>block,</b> we search the closest block from the previously encoded data using side-match technique. The closest block is then used as the prediction of the <b>input</b> <b>block,</b> or used to generate a dynamic codebook. The <b>input</b> <b>block</b> is encoded by the closest block, dynamic codebook or supercodebook. Searching for the closest block from the previously encoded data is equivalent to expand the codevector space; thus the picture quality achieved is not limited by the supercodebook. Experimental results reveal that the new DFSVQ reduces bit rate significantly and provides better visual quality, {{as compared to the}} basic VQ and other DFSVQs. Index Terms—Codebook, dynamic finite-state vector quantization, image compression, next-state function, vector quantization. I...|$|E
3000|$|The overall {{architecture}} of MDLSTM network for recognition of Urdu Nasta’liq text-lines (see Fig. 7) {{is composed of}} the <b>input</b> <b>block</b> size, hidden block size, sub sample size and LSTM layer size with {{the maximum number of}} nodes for CTC output layer. The <b>input</b> <b>block</b> is the size of small patches that scan the pixels of the image for further processing. The hidden block size is the size of small patches at each hidden layer in the MDLSTM network. The sub-sampling layers are between each pair of hidden layers {{and the size of the}} sub-sampling specifies the total number of feed forward [...] units in the layers of sub-sampling.|$|E
40|$|Abstract — Current speech {{processing}} in cochlear implants use a filterbank to analyse audio signals into several frequency bands, each associated with one electrode. Because the processing is performed on <b>input</b> signal <b>blocks</b> of fixed sizes, the filterbank provides a unique time-frequency resolution {{to represent the}} various signal features. However, different components of audio signals may require different time-frequency resolutions for an accurate representation and perception. In this paper we investigate the influence on speech intelligibility in cochlear implants users when filterbanks with different time-frequency resolutions are used. In order to represent all signal features accurately, an adaptive filterbank has been developed that accepts <b>input</b> <b>blocks</b> of different sizes. The different resolutions required are achieved by adequately switching between block sizes depending on the input signal characteristics. The filterbank was incorporated into the commercial Advanced Combinational Encoder (ACE) and acutely tested on six cochlear implant recipients. I...|$|R
3000|$|... {{is called}} a codeword. The {{generation}} of the codebook determines {{the performance of the}} VQ coding. VQ-based image quantization involves partitioning an image into numerous fixed-sized blocks and then comparing them with codewords in the codebook to find the closest pattern for each input vector. On the side of the VQ encoder, each of the <b>input</b> <b>blocks</b> is compressed into an index of the codebook. That index is associated with the codebook during the index decoding procedure to rapidly reconstruct the corresponding block through a table lookup operation.|$|R
30|$|Compared with FINAL, the FS-ECVQ {{was less}} memory {{exhausting}} in cdf model decision. This was mainly {{due to the}} two FSVQs (main FSVQ and sub-FSVQ), which adaptively reshaped the <b>input</b> <b>blocks</b> and merged the states with similar cdf models {{to be a new}} one, {{while at the same time}} no side information was needed to be transmitted. Thus, the number of states needed to be conserved contained in sub-FSVQ would be much fewer than those contained in the context-model of the FINAL. As a result, the FS-ECVQ further reduced the total memory requirements of the FINAL by up to 11.3 %.|$|R
