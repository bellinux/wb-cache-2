3|33|Public
5000|$|Noah Kagan's {{submission}} form {{had four}} fields: Name, <b>Email,</b> <b>URL,</b> Revenue. He decided {{to remove the}} [...] "revenue" [...] field altogether, leaving only three fields—Name, Email and URL. This small change meant an improvement in his conversion rate of 26%.|$|E
40|$|Defense {{in depth}} is vital as no single {{security}} product detects all of today’s attacks. To design {{defense in depth}} organizations rely on best practices and isolated product reviews with no way to determine the marginal benefit of additional security products. We propose empirically testing security products’ detection rates by linking multiple pieces of data such as network traffic, executable files, and an email to the attack that generated all the data. This allows us to directly compare diverse security products and to compute the increase in total detection rate gained by adding a security product to a defense in depth strategy not just its stand alone detection rate. This approach provides an automated means of evaluating risks and the security posture of alternative security architectures. We perform an experiment implementing this approach for real drive-by download attacks found in a real time email spam feed and compare over 40 security products and human click-through rates by linking <b>email,</b> <b>URL,</b> network content, and executable file attack data...|$|E
50|$|Also built in, is {{a support}} of long words {{division}} (with respect for language rules), roll-over images, clickable <b>emails</b> and <b>URL</b> (<b>emails</b> are obfuscated against spambots) and auto-correct tool for several typographic issues: national single and double quotation marks, ellipses, em dashes, dimension sign, nonbreakable spaces (e.g. in phone numbers), acronyms, arrows and many others.|$|R
50|$|PressDisplay.com is {{an online}} {{newspaper}} kiosk with over 1,900 publications, from 92 countries in 43 languages. PressDisplay.com {{allows users to}} view and interact with full replicas of publications using a generic Internet browser and includes interactive tables of content, foreign language translation, article blogging, <b>email</b> sharing, clickable <b>URL's,</b> <b>emails</b> and phone numbers, search and media monitoring, alerts, bookmarks, text-to-voice conversion. It runs on PCs, Macs, Ultra-mobile PCs, TabletPCs, iPhones, smartphones, Blackberry and iPod Touch devices and eReaders, like the iRex DR 1000.|$|R
40|$|Anonymization enables {{entities}} {{to share}} {{types of data}} that would otherwise not be shared (1) Private Data – User-identifiable information • user content (<b>Email</b> messages, <b>URLs)</b> • user behavior (access patterns, application usage) – Machine/Interface addresses • IP and MAC addresses (2) Secret Data – System configurations (services, topology, routing) – Traffic patterns (connections, mix, volume...|$|R
50|$|Moreover, in 2012, {{registered}} Philippines Nuffnangers {{are said}} to have received job offers from a certain platehead email domain where they believed that the spam emails came from Nuffnang database. Nuffnang Philippines has since clarified and confirmed that their database has been compromised and information such as the name, <b>emails</b> and <b>URLs</b> of the advertisers have been stolen.|$|R
5000|$|Fields: Each {{entry in}} a Table is a field. They {{are not just}} {{restricted}} to hold text. Airtable currently offers 16 basic field types. These are: single-line texts, long text articles, file attachments, check-boxes, single select from drop-down list, multiple-selects from drop-down lists, date and time, phone numbers, <b>email</b> ids, <b>URLs,</b> numbers, currency, percentage, auto-number, formulae and barcodes.|$|R
50|$|Snapfish {{members can}} share photo albums, {{individual}} photos, animated Snapshows, Group Rooms or Snapfish products. Members can share via <b>email,</b> link <b>URL,</b> and to various other web {{services such as}} Facebook, Blogger and MySpace. Like Facebook and unlike Flickr, a Snapfish account is required to view shared photos. An invitation sent from a member to view their photos on Snapfish would require the recipient to create a Snapfish account before viewing.|$|R
50|$|Images {{and videos}} were {{uploaded}} to yfrog via the website interface, or by <b>email.</b> The <b>URLs</b> of yfrog links were shorter than on ImageShack (e.g. ''''), {{in order to}} fit within the 140 characters limit of a tweet. The yfrog website was optimized for mobile viewing, and aimed to capture a market similar to TwitPic's. As of October 2010, 25 applications supported the yfrog upload API, including the official Twitter for iPhone app, TweetDeck, Seesmic, Twitterrific, and Twittelator.|$|R
40|$|The Internet {{has changed}} {{dramatically}} in recent years. In particular, the fundamental change has occurred {{in terms of who}} generates most of the content, the variety of applications used anl d the diverse ways normal users connect to the Internet. These factors have led to an explosion of the amount of user-specific meta-information that is required to access Internet content (e. g., <b>email</b> addresses, <b>URLs,</b> social graphs). In this paper we describe a foundational service for storing and sharing user-specific meta-information and describe how this new abstraction could be utilized in current and future applications...|$|R
5000|$|Many spam <b>emails</b> contain <b>URLs</b> to {{a website}} or websites. According to a Cyberoam report in 2014, {{there are an}} average of 54 billion spam {{messages}} sent every day. [...] "Pharmaceutical products (Viagra and the like) jumped up 45% from last quarter’s analysis, leading this quarter’s spam pack. Emails purporting to offer jobs with fast, easy cash come in at number two, accounting for approximately 15% of all spam email. And, rounding off at number three are spam emails about diet products (such as Garcinia gummi-gutta or Garcinia Cambogia), accounting for approximately 1%." ...|$|R
5000|$|... "Traditional" [...] {{internet}} cultural [...]sig practices {{assume the}} use of monospaced ASCII text because they pre-date MIME and {{the use of}} HTML in email. In this tradition, it is common practice for a signature block to consist of one or more lines containing some brief information on the author of the message such as phone number and <b>email</b> address, <b>URLs</b> for sites owned or favoured by the author - but also often a quotation (occasionally automatically generated by such tools as fortune), or an ASCII art picture. Among some groups of people it has been common to include [...]|$|R
40|$|One of the broadly used {{internet}} {{attacks to}} deceive customers financially in banks and agencies is unknown âzero-dayâ phishing Emails âzero-dayâ phishing Emails {{is a new}} phishing email {{that it has not}} been trained on old dataset, not included in black list. Accordingly, the current paper seeks to Detection and Prediction of unknown âzero-dayâ phishing Emails by provide a new framework called Phishing Evolving Neural Fuzzy Framework (PENFF) that is based on adoptive Evolving Fuzzy Neural Network (EFuNN). PENFF does the process of detection of phishing email depending on the level of features similarity between body <b>email</b> and <b>URL</b> <b>email</b> features. The totality of the common features vector is controlled by EFuNN to create rules that help predict the phishing email value in online mode. The proposed framework has proved its ability to detect phishing emails by decreasing the error rate in classification process. The current approach is considered a highly compacted framework. As a performance indicator; the Root Mean Square Error (RMSE) and Non-Dimensional Error Index (NDEI) has 0. 12 and 0. 21 respectively, which has low error rate compared with other approaches Furthermore, this approach has learning capability with footprint consuming memory. ...|$|R
40|$|Text preprocess (this preprocess service {{requires}} that the input text be in plain text format (file. txt) and UTF- 8). Basically, it carries out: (i) text segmentation into minor structural units (titles, paragraphs, sentences, etc.); (ii) detection of entities not found in dictionaries (numbers, abbreviations, <b>URLs,</b> <b>emails,</b> proper nouns, etc.); and (iii) the keeping of sequences {{of two or more}} words in a single block (dates, phrases, proper nouns, etc.) ...|$|R
50|$|All {{fields in}} ProcessWire are custom fields {{defined by the}} user. A given field may be {{assigned}} to multiple templates, which in turn hold individual chunks of content for each page using that template. Every field has a type, {{referred to as the}} “fieldtype” (1 word), a type of plugin module. ProcessWire includes several fieldtype modules including those that support text, numbers, files, images, page references, <b>URLs,</b> <b>email</b> addresses, dates and comments. Additional fieldtype modules are installed as 3rd party modules.|$|R
40|$|Abstract — Phishing emails usually {{contain a}} message from a {{credible}} looking source requesting a user to click a link to a website where user is asked to enter a password or other confidential information. Most phishing emails aim at withdrawing money from financial institutions or getting access to private information. Phishing has increased enormously {{over the last years}} and is a serious threat to global security and economy. Phishing attacks are becoming more frequent and sophisticated. There are a number of possible countermeasures to phishing. A number of anti-phishing solutions have been proposed to date. Some approaches attempt to solve the phishing problem at the e-mail level. A technique must be capable of determining whether an email is legitimate or a phishing, given only the <b>URL</b> and the <b>email</b> content. <b>URL</b> and textual content analysis of email will results in a highl...|$|R
40|$|Abstract — Authentication {{by means}} of {{username}} and password may not be more secure as the intrusion of the illegal person’s access may cause serious vulnerability. In this system, authentication {{by means of}} facial image to the email service has been proposed to provide maximal security. Outline the tags and automatic parsing of the HTML tags and read the data from each tuple. Comparison of the tags with the keywords and URL’s has been located in the centralized storage called repository {{using one of the}} parsing algorithm called top down parsing keyword search algorithm. Parsed unsolicited content are framed as tags and if the tags are matched with the content in the mail then that mail is identified as spam. We are notifying all the Private Domain related to the spam <b>emails</b> and <b>URL’s</b> received in the email will be repositised for future email rejections. A very goo...|$|R
5000|$|Shelia [...] {{is a high}} {{interaction}} client honeypot {{developed by}} Joan Robert Rocaspana at Vrije Universiteit Amsterdam. It integrates with an email reader and processes each <b>email</b> it receives (<b>URLs</b> & attachments). Depending {{on the type of}} URL or attachment received, it opens a different client application (e.g. browser, office application, etc.) It monitors whether executable instructions are executed in data area of memory (which would indicate a buffer overflow exploit has been triggered). With such an approach, SHELIA is not only able to detect exploits, but is able to actually ward off exploits from triggering.|$|R
40|$|The {{complementarity}} of polarized DIS {{experiments and}} polarized Proton-Proton experiments is illustrated for two examples. It is shown how the twist- 3 {{part of the}} second moment of $g_ 2 (x,Q^ 2) $ and the single-spin photon asymmetry are connected and it is discussed how the polarized gluon distribution {{can be obtained from}} the measurement of direct gamma asymmetries. Comment: Talk given at the Workshop on Deep Inelastic scattering and QCD, Paris, April 1995, LaTeX, 4 pp, 5 eps figures and style file included as compressed tar file. Available at ftp://ftp. th. physik. uni-frankfurt. de/pub/schaefer/dis. ps. Z [added figures - we had an <b>email</b> delay; corrected <b>URL...</b>|$|R
50|$|Zamzar is {{an online}} file converter, created by {{brothers}} Mike and Chris Whyley in England. It allows user to convert files without downloading a software tool, and supports over 1,000 different conversion types. Users can type in a URL or upload {{one or more}} files (if they are all of the same format) from their computer; Zamzar will then convert the file(s) to another user-specified format, such as an Adobe PDF file to a Microsoft Word document. Once conversion is complete, users receive an <b>email</b> with a <b>URL</b> from where they can download the converted file. It is also possible to send files for conversion by emailing them to Zamzar.|$|R
40|$|Phishing emails contain {{socially}} engineered {{messages to}} lure victims into performing certain actions, such as clicking on a URL where a phishing website is hosted, or executing a malware code. In a previous study, {{we proposed a}} lexical URL analysis approach for detecting phishing websites. In this study, we extend the approach to the phishing email classification domain. The primary motive behind {{this study is that}} most phishing <b>email</b> messages contain <b>URLs</b> that point to phishing websites, and lexically analyzing the URLs can enhance the classification accuracy of email messages. As evaluated in this study, the addition of URL lexical analysis in phishing email classification is effective and results in a highly accurate anti-phishing email classifier...|$|R
5000|$|Vincent Baker {{began using}} [...] "lumpley" [...] <b>email</b> {{addresses}} and <b>URLs</b> in kill puppies for satan (2002); {{he had used}} the name on various online systems, and it would quickly become the name of Baker's indie publishing company too. Baker produced 40 or 50 copies {{of the game and}} sold them all, which would give them the money for his next project; Baker says that he hasn't put a dime into Lumpley since that initial investment. The Cheap and Cheesy Fantasy Game (2001) was the first game by Baker that called itself [...] "a lumpley game." [...] Lumpley Games published Baker's Dogs in the Vineyard (2004). In 2010, Baker replaced the Forge GenCon booth with a small booth for just himself and Lumpley Press.|$|R
40|$|Language Model (LM) Adaptation {{has been}} shown to be very {{important}} to reduce the Word Error Rate (WER) in task specific speech recognition systems. Adaptation data collected in the real world, however, usually contain large amount of non-dictated text such as <b>email</b> headers, long <b>URL,</b> code fragments, included reply, signature, etc. that the user will never dictate. Adapting with these data may corrupt the LM. In this paper, we propose a Maximum Entropy (MaxEnt) based filter to remove a variety of non-dictated words from the adaptation data and improve the effectiveness of the LM adaptation. We argue that this generic filter is language independent and efficient. We describe the design of the filter, and show that the usage of the filter can give us 10 % relative WER reduction over LM adaptation without the filtering, and 22 % relative WER reduction over the un-adapted LM in English email dictation task. 1...|$|R
40|$|This paper {{describes}} {{the design and}} development of a service for 2 ̆ 7 visual media 2 ̆ 7 files. In {{the first phase of}} the ARIADNE project we reviewed the status of visual media resources (2 D, RTI, 3 D) in the archaeology domain, but there was possibility of publishing visual media resources on the web. To fill this gap we have designed a service aimed at providing easy and unsupervised publication on the web. The service provides a very easy interface (a simple web form) that allows the user to upload the visual media file; the data is then transformed in a web-compliant format, supporting multi-resolution encoding, compression, and progressive transmission. At the end of the data-processing phase the user receives an <b>email</b> containing the <b>URL</b> of the published asset (or, in case he/she wants to store the file locally, a. zip file). Specific browsers for the three types of media have also been developed, based on 3 DHOP technology...|$|R
40|$|Abstract—Traditional {{security}} models {{based on}} distinguishing trusted from untrusted pieces {{of data and}} program behavior continue to face difficulties keeping up with attackers levels of sophistication and ingenuity. In this position paper, we present a novel computing paradigm for trustworthy computing whose application, operating system (OS) and architecture can leverage social trust to enhance the robustness and diversity of security mechanisms of any Internet-based computing environment. Our model would allow online social network (OSN) users to assign trust values to her friends in a privacy-preserving fashion and maintain a trust repository with trust values for objects like <b>URLs,</b> <b>email</b> addresses, IP addresses and other pieces of data that can be consumed by a socially-aware OS, allowing for fine-grained trust decisions that take into account user context and add diversity to host behavior. Our model also automatically infer trust values for people a user is not directly connected. In this paper we sketch {{the design of a}} socially-aware operating system kernel and identify several research challenges for this new paradigm. I...|$|R
40|$|This paper {{attempts}} {{to develop an}} algorithm to recognize spam domains using data mining techniques with the focus on law enforcement forensic analysis. Spam filtering has been the major weapon against spam, but failed {{to reduce the number}} of spam emails sent to an indiscriminate set of recipients. The proposed algorithm accepts as input, spam mails of personal account and extracts features such as stylistic, semantic, related <b>email</b> subjects and <b>URLs</b> present in the emails. The individual features are then clustered and evaluated. Further, these clusters are mapped with their respective domains. These spam domains are the URL of the webpage that spammer is trying to promote. The WHOIS information of the domain helps to get information about the source of that domain. Parameters like overall purity and the number of emails present in the cluster with highest purity is used to measure result of the individual features. An Experimental result shows that clustering of spam mails by stylistic and semantic parameter 20 % less pure than other two features of spam mails...|$|R
40|$|Traditional {{security}} models {{based on}} distinguishing trusted from untrusted pieces {{of data and}} program behavior continue to face difficulties keeping up with attackers' levels of sophistication and ingenuity. In this position paper, we present a novel computing paradigm for trustworthy computing whose application, operating system (OS) and architecture can leverage social trust to enhance the robustness and diversity of security mechanisms of any Internet-based computing environment. Our model would allow online social network (OSN) users to assign trust values to her friends in a privacy-preserving fashion and maintain a trust repository with trust values for objects like <b>URLs,</b> <b>email</b> addresses, IP addresses and other pieces of data that can be consumed by a socially-aware OS, allowing for fine-grained trust decisions that take into account user context and add diversity to host behavior. Our model also automatically infer trust values for people a user is not directly connected. In this paper we sketch {{the design of a}} socially-aware operating system kernel and identify several research challenges for this new paradigm...|$|R
40|$|NOTE: This is {{intended}} to be read as an iBook (using iBook app on either an iPad or OS X). A PDF version has been made available however the PDF version does not allow the embedded slides to be viewed full screen, the viewing of the two included videos, or the live <b>email</b> links and <b>URLs</b> in the contributors section. This work represents the documentation that emerged from a panel at the Visible Evidence XXIV documentary conference, held in Buenos Aires in 2017. The panel consisted of a series of propositions and interrogations of new epistemologies and ontologies for understanding interactive documentary through a materialist lens. These have been collected here, and the panel members invited to further develop their thinking in light of the panel. The work, as curated here, is deliberately between the tone of the presentation and a finished article. They are more formal than the former, and shorter and less refined than the latter. In this manner they are part of an ongoing experiment in alternative academic practices and forms that seek to open, critique, and revision scholarship as a black box...|$|R
40|$|This {{observational}} study investigates the methods people {{use in their}} workplace to organize web information for re-use. In addition to the bookmarking and history list tools provided by web browsers, people observed in our study {{used a variety of}} other methods and associated tools. For example, several participants <b>emailed</b> web addresses (<b>URLs)</b> along with comments to themselves and to others. Other methods observed included printing out web pages, saving web pages to the hard drive, pasting the address for a web page into a document and pasting the address into a personal web site. Differences emerged between people according to their workplace role and their relationship to the information they were gathering. Managers, for example, depended heavily on email to gather and disseminate information and did relatively little direct exploration of the Web. A functional analysis helps to explain differences in “keeping” behavior between people and to explain the overall diversity of methods observed. People differ in the functions they require according to their workplace role and the tasks they must perform; methods vary widely in the functions they provide. The functional analysis can also help to assess the likely success of various tools, current and proposed...|$|R
5000|$|After Vincent Baker had {{recently}} left a job {{so that his}} wife Meguey Baker could keep working, he got out his anger on paper {{in the form of}} a role-playing game called kill puppies for satan (2001), the first game design he made available to the public. Baker described the game as a [...] "scream of rage" [...] against the state of roleplaying development at the time, and did not playtest it as he intended to make a political statement rather than create a playable game. Baker also wrote a supplement to kill puppies for satan called cockroach souffle (2002). As a result of encouragement from members of the website The Forge, Baker turned kill puppies for satan into a PDF and began selling it around December 2002, listing [...] "lumpley" [...] <b>email</b> addresses and <b>URLs</b> in the game; Baker had used the name Lumpley on various online systems and it soon became the name of his indie publishing company, making kill puppies for satan the first game for Lumpley Games. kill puppies for satan generated significant hate mail, much of which Baker reprinted and mocked on his website; this publicity helped the game, encouraging Baker to print copies of the game to sell at Gen Con Indy 2003.|$|R
40|$|This {{review paper}} argues that users of {{personal}} information management systems have three particularly pressing requirements, for which current systems {{do not fully}} cater: (i) To combat information overload, as the volume of information increases. (ii) To ease context switching, in particular, for users who face frequent interrupts in their work. (iii) To be supported in information integration, {{across a variety of}} applications. To meet these requirements, four broad technological approaches should be adopted in an incremental fashion: (i) The deployment of a unified file system to manage all information objects, including files, <b>emails</b> and webpage <b>URLs.</b> (ii) The use of tags to categorize information; implemented in a way which is backward-compatible with existing hierarchical file systems. (iii) The use of context to aid information retrieval; built upon existing file and tagging systems rather than creating a parallel context management system. (iv) The deployment of semantic technologies, coupled with the harvesting of all useful metadata. RESEARCH HIGHLIGHTS • Future file systems should be unified systems encompassing all information objects. • Tagging should be introduced in a way compatible with pre-existing folder systems. • Context should be used to aid information retrieval, using existing file structures. • Semantic technologies, with harvesting of metadata, can enhance the user experience. • Incremental development should draw on cognitive science and be rigorously tested...|$|R
40|$|This paper {{describes}} {{the results of}} an observational study into the methods people use to manage web information for re-use. People observed in our study used a diversity of methods and associated tools. For example, several participants <b>emailed</b> web addresses (<b>URLs)</b> along with comments to themselves and to others. Other methods observed included printing out web pages, saving web pages to the hard drive, pasting the address for a web page into a document and pasting the address into a personal web site. Ironically, two web browser tools that have been explicitly developed to help users track web information [...] the bookmarking tool and the history list [...] were not widely used by participants in this study. A functional analysis helps to explain the observed diversity of methods. Methods vary widely in the functions they provide. For example, a web address pasted into a self-addressed email can provide an important reminding function together with a context of relevance: The email arrives in an inbox which is checked at regular intervals and the email can include a few lines of text that explain the URL's relevance and the actions to be taken. On the other hand, for most users in the study, the bookmarking tool ("Favorites" or "Bookmarks" depending on the browser) provided neither a reminding function nor a context of relevance. The functional analysis can help to assess the likely success of various tools, current and proposed...|$|R
40|$|The World Wide Web {{provides}} a huge distributed web database. However, {{information in the}} web database is free formatted and unorganized. Traditional keyword-based retrieval approaches are no longer appropriate. In this paper, we consider a framework for constructing agents that can simulate the behavior of human browsing on the Internet. Given a specific target, such an agent will make use of existing search engines to navigate through the web to locate the sites containing the target information and extract them into a database. We refer to these types of agents as Personal Navigating Agents (PNA). Since the information service is domain specific, we shall first focus on those PNA that can retrieve people’s information on the web in this paper. In this particular experiment, given {{the name of a}} university, we shall extract the following information about its faculty: name, telephone number, fax number, <b>email</b> address and <b>URL.</b> We explore web page knowledge in two ways: First, we develop a tagging system for each web page to facilitate information extraction. Our tagging system employs an HTML parser together with a natural language semantic tagger. These semantic tags are more general than part-of-speech tags used in linguistics. Second, we equip our PNA with a navigation map. A navigation map will guide our PNA to traverse through related pages and to arrive at pages containing the target information. In our experiments, our prototype agents have successfully explored a university web site and extracted target information with a very high accuracy. 1...|$|R
40|$|Summary This release {{removes the}} {{questionnaire}} saving answers and also adds a warning when a user attempts to submit answers that their {{project will be}} locked. There were a few bug fixes including one that prevented the similar projects page from showing and one that would cause the application to crash if it couldn't email out. There were a few typos corrected and a small problem with dates being formatted incorrectly which prevented them from being displayed. Fixed Fix bug preventing people from {{not being able to}} view similar project details (Matthew McConnell) Fix a typo in the word quality (Matthew McConnell) gracefully handles smtp failures when no emails are successfully sent (Matthew McConnell) <b>email</b> had wrong <b>url</b> for the project (Patrick White) Update index. html wording to say Project instead of performance (PFWhite) fixed date formatting so that project start/stop dates work (Patrick White) Similar project bug causing the page to error if there were similar projects (Patrick White) Added Added mesh dedupe custom command (Patrick White) Add a popup to warn the user that the project is locked if submitted, add link back to project (Matthew McConnell) update departments to include all known UF departments (Patrick White) Changed Do not log calls to api/tags in the access log (Patrick White) Removed ajax endpoint so answers are only saved on project submittal (Patrick White) Change wording of project save button (Matthew McConnell) Resize icon images for ie alert page (Amber Allen) Create second image of caution sign for ie view (Amber Allen) modify classes on unsupported browser to display better in chrome (Amber Allen) Change Expertise to now pull from the mesh keyword list instead of free form text (Patrick White...|$|R

