5|20|Public
40|$|Objectives: To {{examine the}} effect on {{estimated}} levels of health conditions produced from large-scale surveys, when either list-wise respondent deletion or standard demographic item-level imputation is employed. To assess {{the degree to which}} further bias reduction results from the inclusion of correlated ancillary variables in the <b>item</b> <b>imputation</b> process. Design: Large cross-sectional (US level) household survey...|$|E
40|$|This paper {{addresses}} {{an evaluation}} of the methods for automatic <b>item</b> <b>imputation</b> to large datasets with missing data in the particular setting of financial data often used in economic and business settings. The paper aims {{to bridge the gap between}} purely methodological papers concerned with individual imputation techniques with their implementation algorithms and common practices of missing value treatment in social sciences and other research. Historical methods for handling the missing values are rendered obsolete with the rise of cheap computing power. Regardless of the condition of input data, various computer programs and software packages almost always return some results. In spite of this fact, <b>item</b> <b>imputation</b> in scientific research should be executed only to reproduce reality, not to create a new one. In the review papers comparing different methods we usually find data on performance of algorithms on artificial datasets. However, on a simulated dataset that replicates a real-life financial database, we show, that algorithms different from the ones that perform best on purely artificial datasets, may perform better...|$|E
40|$|Analysis {{of sample}} survey data often {{requires}} adjustments {{to account for}} missing data in the outcome variables of principal interest. Standard adjustment methods based on <b>item</b> <b>imputation</b> or on propensity weighting factors rely heavily {{on the availability of}} auxiliary variables for both responding and non-responding units. Application of these adjustment methods can be especially challenging in cases for which the auxiliary variables are numerous and are themselves subject to substantial incomplete-data problems. This paper shows how classification and regression trees and forests can overcome some of the computational difficulties. An in-depth simulation study based on incomplete-data patterns encountered in the U. S. Consumer Expenditure Survey is used to compare the methods with two standard methods for estimating a population mean in terms of bias, mean squared error, computational speed and number of variables that can be analyzed...|$|E
40|$|The paper shows results {{obtained}} when using a hierarchical log-linear model to produce <b>item</b> <b>imputations</b> {{based on the}} maximum likelihood estimator. We compare the results with those obtained using the sequential hot-deck imputation procedure. We apply the two procedures on the data collected in Sacramento for the 1998 dress rehearsal for Census 2000. To measure the relative {{differences between the two}} methodologies, we simulate the posterior and predictive distributions associated with the model. We run our simulation through data augmentation bayesian iterative proportional fitting (DABIPF). Gelman and Rubin (1991) first proposed a bayesian iterative proportional fitting (BIPF) to generate posterior conjugates for categorical log-linear models. Schafer (1997) proposes a variant of BIPF for direct application to hierarchical models. Schafer (1997) also extends the technique to DABIPF. In our situation Schafer's version of DABIPF yields: 1. An approximation for the posterior distribution [...] ...|$|R
40|$|OBJECTIVE: : To 1) {{compare the}} {{accuracy}} of the Montgomery-Asberg Depression Rating Scale (MADRS) and the Cornell Scale for Depression in Dementia (CSDD) in nursing home residents with dementia when professional caregivers are the only available source of information and 2) explore different methods to account for missing items. DESIGN: : Cross-sectional design. SETTING: : Nursing home (NH). PARTICIPANTS: : One hundred one residents with dementia. MEASUREMENTS: : NH residents with dementia were assessed on the presence of clinical depression using Provisional Diagnostic Criteria for Depression of Alzheimer's Disease. The MADRS and CSDD were administered in a structured interview with professional primary caregivers. RESULTS: : Receiver operating characteristic analyses revealed no significant differences between areas under the empirical curve for MADRS and CSDD. Imputation of a lowest possible item score for missing items revealed larger areas than three other methods (significant result only for CSDD). A MADRS cutoff score of "> 13 " yielded the highest sum of sensitivity (78 %) and specificity (66 %). A CSDD cutoff score of "> 6 " yielded the highest sum of sensitivity (94 %) and specificity (49 %). Both scales showed high negative predictive values up to 100 % and low positive predictive values not exceeding 50 %. CONCLUSION: : The proxy-based MADRS and CSDD did not differ in distinguishing depressed from nondepressed NH residents and may be used for screening purposes. For missing <b>items,</b> <b>imputation</b> of a lowest possib le item score may be applied. The MADRS and CSDD may be better used for ruling out rather than for ruling in depression...|$|R
40|$|Introduction: This study {{investigated}} the effects of imputing missing data in the WHO Quality of Life Abbreviated Questionnaire (WHOQOL-BREF). The imputation results from both the item and domain levels were compared {{and the impact of}} the missing data rate and the number of <b>items</b> included for <b>imputation</b> were examined. Methods: An empirical analysis and a simulation study were used {{to examine the effects of}} missing data rates and the number of <b>items</b> used for <b>imputation</b> on the accuracy for imputation. In the empirical analysis, both item-level and domain-level imputations were performed, and the missing values were imputed using different amounts of data. In the simulation study, sets of 2 %, 5 % and 10 % of the data were drawn randomly and replaced with missing values. Twenty datasets were generated for each situation. The data were imputed and the accuracy of the imputation was reported. Results: In the empirical study, the number of <b>items</b> used for <b>imputation</b> had only a small impact on the accuracy of imputation. Furthermore, in the simulation study, the accuracy rates of imputation did not significantly change as the proportions of missing data increased. However, the number of items used in the computation did contribute to some extent to the missing values imputed. Extreme responses had the worst computations and the lowest accuracy rates. Conclusion: It is recommended that as many items as possible be included for imputation within the same domain. However, it is not particularly helpful to use items from different domains for imputation. Researchers should exercise extra caution in interpreting the imputed values of extreme responses. Quality-of-life-rating-scales...|$|R
40|$|Key words: Census <b>Item</b> <b>Imputation,</b> Multiple Regression, Variance Estimation We {{present a}} model-based {{approach}} for imputing missing person age for the 2000 Census short form. We use {{a series of}} easy to implement multiple regression models, first proposed at the 1997 ASA Joint Meetings, to predict the missing age of a person. This paper extends our work by comparing our results {{with that of the}} hot-deck method used for the 1990 Census. By using a comprehensive set of information to determine the most important predictors of age, our modeling approach shows an improvement in finding key characteristics such as householder ages. In our paper, we also discuss a method of estimating the variance associated with replacing the missing ages with imputed ages for various demographic characteristics. These estimates of variance due to imputation are not available from past censuses. IMPUTING PERSON AGE FOR THE 2000 CENSUS SHORT FORM: A MODEL-BASED APPROACH Todd R. Williams, Bureau of the Cens [...] ...|$|E
40|$|As {{with all}} surveys, {{the quality of}} the American Community Survey (ACS) data {{reflects}} how well the data collection procedures address potential sources of nonsampling error, including coverage error, nonresponse and measurement errors, and errors that may arise during data capture and processing. Chapters 4 and 11 provide information regarding the steps the ACS takes to reduce sampling error while still managing costs. There are four primary sources of nonsampling error (Groves, 1989) : • Coverage Error. The failure to give some units in the target population any chance of selection into the sample, or giving units more than one chance of selection. • Nonresponse Error. The failure to collect data from all units in the sample. • Measurement Error. The inaccuracy in responses recorded on survey instruments, arising from: • The effects of interviewers on the respondents ’ answers to survey questions. • Respondents ’ inability to answer questions, lack of requisite effort to obtain the correct answer, or other psychological or cognitive factors. • Faulty wording of survey questions. • Data collection mode effects. • Processing Error. Errors introduced after the data are collected, including: • Data capture errors. • Errors arising during coding and classification of data. • Errors arising during editing and <b>item</b> <b>imputation</b> of data. This chapter identifies the operations and procedures designed to reduce these sources of nonsampling error and thus improve {{the quality of the}} data. It also includes information about ACS Quality Measures, which provide data users an indication of the potential for nonsampling error. The ACS releases the survey estimates, as well as the Quality Measures, at the same time each year, so that users can consider data quality in conjunction with the survey estimates. The ACS Quality Measures are available on the American FactFinder (AFF) Web sit...|$|E
40|$|Item {{non-response}} is {{a challenge}} faced by virtually all surveys. Item non-response occurs when a respondent skips over a question, refuses to answer a question, or indicates {{that they do not}} know the answer to a question. Hot deck imputation is one of the primary <b>item</b> non-response <b>imputation</b> tools used by survey statisticians. Recently, new competitor in the field of Weighted Sequential Hotdeck Imputation has arrived: PROC HOTDECK of SUDAAN®, version 10. We compared the results of imputation using the new procedure with the results of the Hotdeck SAS® Macro with respect to: a) how close the post-imputation weighted distributions and standard errors of the estimates are to those of the item respondent data; b) whether there is a difference in the number of times donors contribute to the imputation...|$|R
40|$|In {{order to}} {{overcome}} the problem of <b>item</b> nonresponse, random <b>imputations</b> are often used {{because they tend to}} preserve the distribution of the imputed variable. Among the methods of random imputation, the random hot-deck has the interesting property that the imputed values are observed values. We present a new random method of hot-deck imputation which enables us to select the imputed values such that some balancing equations are satisfied and such that the donors are selected in neighborhoods of the recipients...|$|R
40|$|In the U. S., {{analyses}} of poverty {{rates and the}} effects of anti-poverty programs rely almost exclusively on income data. In earlier work (Meyer and Sullivan, 2003) we emphasized that conceptual arguments generally favor using consumption data to measure the well-being of the poor, and, on balance, data quality issues favor consumption in the case of single mothers. Our earlier work did not show that income and consumption differ in practice. Here we further examine data quality issues and show that important conclusions about recent trends depend on whether one uses consumption or income. Changes in the distribution of resources for single mothers differ sharply in recent years depending on whether measured by income or consumption. Measures of overall and sub-group poverty also sharply differ. In addition to examining broader populations and a longer time period, we also consider new dimensions of data quality such as survey and <b>item</b> nonresponse, <b>imputation,</b> and precision. Finally, we demonstrate the flaws in a recent paper that compares income and consumption data. ...|$|R
40|$|This {{report has}} been {{prepared}} for limited distribution to the research community outside the United States Department of Agriculture (USDA). The views expressed herein are not necessarily those of NASS or USDA. i EXECUTIVE SUMMARY Since assuming responsibility for conducting the Census of Agriculture in 1997, the National Agricultural Statistics Service (NASS) has worked continuously to improve the processing capabilities of the census edit system. Following the 2002 Census of Agriculture, NASS realized that it needed to improve the speed and reliability of the edit system for the 2007 Census. The subroutine that performed <b>item</b> level <b>imputation</b> was one component of the edit that was targeted for improvement, and it underwent extensive redesign between 2002 and 2007. The imputation routine utilized several techniques that were new for 2007. These techniques included stratifying all potential donors into groups called profiles; running the donor program as a continually running program, known in computer terminology as a daemon, using SAS/Share to mediate interprocess communication between an edit job and a donor daemon; and storing donor data in temporary arrays in SAS to provide optimal access to the data. The new imputatio...|$|R
30|$|To {{reduce the}} {{influence}} of fatigue and learning effects resulting from completing a long test, PIRLS used a balanced incomplete block (BIB) design (Kennedy and Sainsbury [2007]). Because each student responded to {{only a portion of}} the test <b>items,</b> a multiple <b>imputation</b> technique was used to create five sets of plausible values of reading scores for the whole sample (Foy et al. [2007]). To maximize the evaluative precision of the test, an Item Response Theory (IRT) approach was also used to combine and scale students’ responses in the test (i.e., to estimate the reading attainment scores of students based on their responses to their respective subtests of questions from the overall test). Accordingly, reading scores were IRT scale scores with an international mean of 500 and a standard deviation of 100.|$|R
40|$|The {{scientific}} {{treatment of}} missing data {{has been the}} subject of research for nearly a century. Strangely, interest in missing data is quite new in the fields of educational science and psychology (Peugh & Enders, 2004; Schafer & Graham, 2002). It is now important to better understand how various common methods for dealing with missing data can affect well-used psychometric coefficients. The purpose of this study is to compare the impact of ten common fill-in methods on Cronbachs alpha (1951). We use simulation studies to investigate the behavior of alpha in various situations. Our results show that multiple imputation is the most effective method. Furthermore, simple imputation methods like Winer <b>imputation,</b> <b>item</b> mean, and total mean are interesting alternatives for specific situations. These methods can be easily used by non-statisticians such as teachers and school psychologists...|$|R
40|$|The National Household Education Survey (NHES) is a data {{collection}} {{system of the}} National Center for Education Statistics. It focuses on educational issues that are best addressed by contacting individual households rather than institutions. The {{primary purpose of the}} NHES is to conduct repeated measurements of the same phenomena at different points in time. In 1991, the NHES collected information about the school readiness of children up to grade 2 and about school safety and discipline. This paper presents information on unit response, weighting, <b>item</b> response, and <b>imputation</b> in the 1993 NHES. The section on "Unit Response in the NHES: 93 " describes responses and completion rates for the NHES: 93, including data on these rates for the Screener Interview, the extended School Readiness interviews, and the extended School Safety and Discipline interviews broken down for parents and students. "Weighting and Estimation " discusses the procedures used for producing the weights to estimate characteristics fro...|$|R
40|$|PURPOSE: To {{establish}} {{guidelines to}} reduce potential bias, ensure consistent estimates, and simplify analysis, by correcting inconsistent {{data in a}} data set (i. e., edits) or substituting values for missing (i. e., imputation) or inconsistent data in a data set (i. e., edits). KEY TERMS: cross-sectional, cross-sectional imputations, cross-wave imputations, edit, freshened sample, <b>imputation,</b> <b>item</b> nonresponse, key variables, longitudinal, nonresponse bias, overall unit nonresponse, response rate, stage of data collection, unit nonresponse, and universe. STANDARD 4 - 1 - 1 : All data must be edited. Data editing is an iterative and interactive process that includes procedures for detecting and correcting errors in the data. Data editing is first done prior to imputation. Data editing must be repeated after the data are imputed, and again after the data are altered during disclosure risk analysis (without jeopardizing the disclosure protections). At each stage, the data must be checked for the following and edited if errors are detected: 1. Credibility, based on range checks to determine if all responses fall within a prespecifie...|$|R
40|$|In {{order to}} {{overcome}} the problem of <b>item</b> nonresponse, random <b>imputation</b> methods are often used {{because they tend to}} preserve the distribution of the imputed variable. Among the random imputation methods, the random hot-deck has the interesting property of imputing observed values. A new random hot-deck imputation method is proposed. The key innovation of this method is that the selection of donors is viewed as a sampling problem and uses calibration and balanced sampling. This approach makes it possible to select donors such that if the auxiliary variables were imputed, their estimated totals would not change. As a consequence, very accurate and stable totals estimations can be obtained. Moreover, the method is based on a nonparametric procedure. Donors are selected in neighborhoods of recipients. In this way, the missing value of a recipient is replaced with an observed value of a similar unit. This new approach is very flexible and can greatly improve the quality of estimations. Also, this method is unbiased under very different models and is thus resistant to model misspecification. Finally, the new method makes it possible to introduce edit rules while imputing...|$|R
40|$|Longitudinal {{studies in}} survey {{research}} include panel surveys, rotating panel surveys, and cohort studies. This paper reviews {{a selection of}} issues arising {{in the design and}} analysis of such studies, supported by illustrations from existing surveys. Panel and rotating panel studies are widely analyzed both longitudinally and cross-sectionally, often as repeated cross-sections. For cross-sectional analysis, the longitudinal sample needs to be augmented by samples of new entrants to the population. It may also become necessary to augment the sample for panel losses. Many sets of weights may be needed for the various types of analyses. Measurement errors and imputed values can be particularly harmful for estimates of gross change. Panel conditioning may seriously affect the results of analyses that make comparisons over time. Changing modes of data collection and changes in respondents across waves of a panel can also affect comparability, as can <b>item</b> nonresponse and <b>imputation</b> for it. A commonly encountered design issue is whether to oversample certain domains; here it is important to distinguish between domains defined in terms of static characteristics and those defined in terms of transient characteristics. The geographical spread of the initial sample of primary sampling units needs to take account of subsequent population mobility...|$|R
40|$|Non-response in {{establishment}} surveys is a {{very important}} problem that can bias results of statistical analysis. The bias can be considerable when the survey data is used to do multivariate analysis that involve several variables with different response rates, which can reduce the effective sample size considerably. Fixing the non-response, however, could potentially cause other econometric problems. This paper uses an operational approach to analyze the sensitivity of results of multivariate analysis to multiple imputation procedures applied to the U. S. Census Bureau/NSF‘s Business Research and Development and Innovation Survey (BRDIS) to address <b>item</b> non-response. Multiple <b>imputation</b> is first applied using data from all survey units and periods for which there is data, presenting scenario 1. A scenario 2 involves separate imputation for units that have participated in the survey only once and those that repeat. Scenario 3 involves no imputation. Sensitivity analysis is done by comparing the model estimates and their standard errors, and measures of the additional uncertainty created by the imputation procedure. In all cases, unit non-response is addressed by using the adjusted weights that accompany BRDIS micro data. The results suggest that substantial benefit may be derived from multiple imputation, not only because it helps provide more accurate measures of the uncertainty due to item non-response but also because it provides alternative estimates of effect sizes and population totals...|$|R
40|$|Missing data is {{an eternal}} problem in data analysis. It is widely {{recognised}} that data is costly to collect, {{and the methods}} used to deal with missing data in the past relied on case deletion. There is no one overall best fix, but many different methodologies to use in different situations. This study was motivated by the writer's time spent analysing data in the nutrition study, and realising how much data was wasted by case deletion, and subsequently how this could bias inferences formed from the results. A better method (or methods), of dealing with missing data (than case deletion) is required, to ensure valuable information is not lost. What is being done: What is in the literature? The literature on this topic has exploded with new methods in recent times. Algorithms have been written and incorporated based on these methods {{into a number of}} statistical packages and add-on libraries. Statistical packages are also reviewed for their practicality and application in this area. The nutrition data is then applied to different methodologies, and software packages to assess different types of imputation. A set of questions are posed; based on type of data, type of missingness, extent of missingness, the required end use of the data, the size of the dataset, and how extensive that analysis needs to be. This can guide the investigator into using an appropriate form of imputation for the type of data at hand. A comparison of imputation methods and results is given with the principal result that imputing missing data is a very worthwhile exercise to reduce bias in survey results, which can be achieved by any researcher analysing their own data. Further to this, a conjecture is given for using Data Augmentation for ordinal data, particularly Likert scales. Previously this has been restricted to either person or <b>item</b> mean <b>imputation,</b> or hot deck methods. Using model based methods for imputation is far superior for other types of data. Model based methods for Likert data are achieved by means of inserting the linear by linear association model into standard missing data methodology...|$|R
40|$|Using data on annual {{individual}} {{labor income}} from three representative panel datasets (German SOEP, British BHPS, Australian HILDA) we investigate a) the selectivity of item non-response (INR) and b) {{the impact of}} imputation as a prominent post-survey means {{to cope with this}} type of measurement error on prototypical analyses (earnings inequality, mobility and wage regressions) in a cross-national setting. Given the considerable variation of INR across surveys as well as the varying degree of selectivity build into the missing process, there is substantive and methodological interest in an improved harmonization of (income) data production as well as of imputation strategies across surveys. All three panels make use of longitudinal information in their respective imputation procedures, however, there are marked differences in the implementation. Firstly, although the probability of INR is quantitatively similar across countries, our empirical investigation identifies cross-country differences with respect to the factors driving INR: survey-related aspects as well as indicators accounting for variability and complexity of labor income composition appear to be relevant. Secondly, longitudinal analyses yield a positive correlation of INR on labor income data over time and provide evidence of INR being a predictor of subsequent unit-non-response, thus supporting the "cooperation continuum" hypothesis in all three panels. Thirdly, applying various mobility indicators there is a robust picture about earnings mobility being significantly understated using information from completely observed cases only. Finally, regression results for wage equations based on observed ("complete case analysis") vs. all cases and controlling for imputation status, indicate that individuals with imputed incomes, ceteris paribus, earn significantly above average in SOEP and HILDA, while this relationship is negative using BHPS data. However, once applying the very same imputation procedure used for HILDA and SOEP, namely the "row-and-columnimputation" approach suggested by Little & Su (1989), also to BHPS-data, this result is reversed, i. e., individuals in the BHPS whose income has been imputed earn above average as well. In our view, the reduction in crossnational variation resulting from sensitivity to the choice of imputation approaches underscores the importance of investing more in the improved cross-national harmonization of <b>imputation</b> techniques. <b>Item</b> non-response, <b>imputation,</b> income inequality, income mobility, panel data, SOEP, BHPS, HILDA...|$|R
40|$|Missing {{data are}} {{ubiquitous}} in educational research settings, including item responses in multilevel data. Researchers in the Item Response Theory (IRT) context {{have shown that}} ignoring such missing data can create problems in the estimation of the IRT model parameters. Consequently, several imputation methods for dealing with missing item data have been proposed and shown to be effective when applied with traditional IRT models. Additionally, a non-imputation direct likelihood analysis {{has been shown to}} be an effective tool for handling missing observations in clustered data settings. This study investigates the performance of six simple imputation methods that have been found to be useful in other IRT contexts versus a direct likelihood analysis, in multilevel data from educational settings. Multilevel item response data were simulated based on two empirical datasets and part of the item scores were deleted such that they were either missing completely at random or missing at random. An explanatory IRT model was used for modeling the complete, incomplete and imputed datasets. It is shown that direct likelihood analysis of the incomplete datasets produces unbiased parameter estimates that are comparable to those of a complete data analysis. Multiple imputation approaches of the two-way means and corrected item means methods display varying degrees of effectiveness in imputing data that in turn can produce unbiased parameter estimates. The simple random imputation, adjusted random <b>imputation,</b> <b>item</b> means substitution and regression imputation methods seemed to be less effective in imputing missing item scores in multilevel data settings. Key words: Item Response Theory, multilevel data, missing data, imputation methodsstatus: publishe...|$|R
40|$|The {{definition}} and operationalization of wealth information in population surveys {{and the corresponding}} microdata requires {{a wide range of}} more or less normative assumptions. However, the decisions made in both the pre- and post-data-collection stage may interfere consid-erably with the substantive research question. Looking at wealth data from the German SOEP, this paper focuses on the impact of collecting information at the individual rather than house-hold level, and on "imputation and editing" as a means of dealing with measurement error. First, we assess how the choice of unit of aggregation or unit of analysis affects wealth distri-bution and inequality analysis. Obviously, when measured in "per capita household" terms, wealth is less unequally distributed than at the individual level. This is the result of significant redistribution within households, and also provides evidence of a significant persisting gender wealth gap. Secondly, we find multiple imputation to be an effective means of coping with selective non-response. There is a significant impact of imputation on the share of wealth holders (increas-ing on average by 15 %) and also on aggregate wealth (plus 30 %). However, with respect to inequality, the results are ambiguous. Looking at the major outcome variable for the whole population-net worth-the Gini coefficient decreases, whereas a top-sensitive measure dou-bles. The non-random selectivity built into the missing process and the consideration of this selectivity in the imputation process clearly contribute to this finding. Obviously, the treatment of measurement errors after data collection, especially with respect to the imputation of missing values, affects cross-national comparability and thus may require some cross-national harmonization of the imputation strategies applied to the various national datasets. Wealth, <b>Item</b> non-response, multiple <b>imputation,</b> SOEP...|$|R
40|$|PURPOSE: To {{establish}} {{guidelines to}} reduce potential bias, ensure consistent estimates, and simplify analysis, by correcting inconsistent {{data in a}} data set (i. e., edits) or substituting values for missing (i. e., imputation) or inconsistent data in a data set (i. e., edits). KEY TERMS: cross-sectional, cross-sectional imputations, cross-wave imputations, edit, freshened sample, <b>imputation,</b> <b>item</b> nonresponse, key variables, longitudinal, nonresponse bias, overall unit nonresponse, response rate, stage of data collection, unit nonresponse, and universe. STANDARD 4 - 1 - 1 : All NCES data must be edited. Data editing is an iterative and interactive process that includes procedures for detecting and correcting errors in the data. Data editing is first done prior to imputation. Data editing must be repeated after the data are imputed, and again after the data are altered during disclosure risk analysis (without jeopardizing the disclosure protections). At each stage, the data must be checked for the following and edited if errors are detected: 1. Credibility, based on range checks to determine if all responses fall within a pre-specified reasonable range; 2. Consistency based on checks across variables within individual records for noncontradictory responses (i. e., no logical inconsistencies); 3. Incorrect flow through prescribed skip patterns; 4. Missing data that can be directly filled from other portions of an individual’s record; 5. The omission and/or duplication of records; 6. Internal consistency across records, (e. g., the sum of categories matches the reported total); and 7. Inconsistency between estimates and outside sources. GUIDELINE 4 - 1 - 1 A: Editing should use available information and logical assumptions to derive substitute values for inconsistent values in a data file. GUIDELINE 4 - 1 - 1 B: When electronic data collection methods are used, data should be edited during, and if necessary after data collection. GUIDELINE 4 - 1 - 1 C: Possible actions when inconsistencies and other errors are found include the following...|$|R

