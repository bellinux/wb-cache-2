6|46|Public
40|$|Here we {{depart from}} the inhomogeneous {{solution}} of a lidar equation using the backward inversion algorithm that is nowadays generally {{referred to as the}} Klett method. In particular, we develop an error sensitivity study that relates errors in the user-input parameters boundary extinction and exponential term in the extinction-to-backscatter relationship to errors in the inverted extinction profile. The validity of the analysis presented is limited only by the validity of application of the inversion algorithm itself, its numerical performance having been tested for optical depths in the 0. 01 – 10 range. Toward this end, we focus on an introductory background about how uncertainties in these two parameters can apply to a family of inverted extinction profiles rather than a single profile and on its range-dependent behavior {{as a function of the}} optical thickness of the lidar inversion range. Next, we performed a mathematical study to derive the <b>error</b> <b>span</b> of the inverted extinction profile that is due to uncertainties in the above-mentioned user calibration parameters. This takes the form of upper and lower range-dependent error bounds. Finally, appropriate inversion plots are presented as application examples of this study to a parameterized set of atmospheric scenes inverted from both synthesized elastic-backscatter lidar signals and a live signal. Peer Reviewe...|$|E
40|$|Abstract: Error {{analysis}} involves detecting, diagnosing and correcting {{discrepancies between}} the text produced so far (TPSF) and the writers mental representation {{of what the}} text should be. While many factors determine the choice of strategy, cognitive effort is {{a major contributor to}} this choice. This research shows how cognitive effort during error analysis affects strategy choice and success as measured by a series of online text production measures. Text production is shown to be influenced most by <b>error</b> <b>span,</b> i. e. whether or not the error spans more or less than two characters. Next, it is influenced by input mode, that {{is whether or not the}} error has been generated by speech recognition or keyboard, and finally by lexicality, i. e. whether or not the error comprises an existing word. Correction of larger error spans are corrected more successful smaller errors. Writers impose a wise speed accuracy tradeoff during large error spans since correction is better, but preparation times and production times take longer, and interference reaction times are slower. During large error spans, there is a tendency to opt for error correction first, especially when errors occurred in the condition in which the TPSF is not preceded by speech. In general the addition of speech frees the cognitive demands of writing: shorter preparation and reaction times. Writers also opt more often to continue text production when the TPSF is presented auditory first...|$|E
40|$|Error {{analysis}} involves detecting, diagnosing and correcting {{discrepancies between}} the text produced so far (TPSF) and the writers mental representation {{of what the}} text should be. While many factors determine the choice of strategy, cognitive effort is {{a major contributor to}} this choice. This research shows how cognitive effort during error analysis affects strategy choice and success as measured by a series of online text production measures. Text production is shown to be influenced most by <b>error</b> <b>span,</b> i. e. whether or not the error spans more or less than two characters. Next, it is influenced by input mode, that {{is whether or not the}} error has been generated by speech recognition or keyboard, and finally by lexicality, i. e. whether or not the error comprises an existing word. Correction of larger error spans are corrected more successful smaller errors. Writers impose a wise speed accuracy tradeoff during large error spans since correction is better, but preparation times and production times take longer, and interference reaction times are slower. During large error spans, there is a tendency to opt for error correction first, especially when errors occurred in the condition in which the TPSF is not preceded by speech. In general the addition of speech frees the cognitive demands of writing: shorter preparation and reaction times. Writers also opt more often to continue text production when the TPSF is presented auditory first. Cognitive effort, Dictation, Dual task technique, Error analysis, Speech recognition, Text Produced So Far (TPSF), Text production, Technology of writing, Working memory...|$|E
40|$|This paper {{describes}} {{a number of}} experiments and analyses conducted to {{gain an understanding of}} the typical properties of <b>error</b> <b>spans</b> – the number of consecutive incorrect words generated when producing text with recognition technologies. The data were drawn from three studies of recognition applications: one study of an IBM dictation system, one of a non-IBM dictation system, and one of an IBM handwriting recognition application. Across the three recognition studies, the appropriate width for a text field designed to display text selected for correction appeared to be the width that would accommodate the presentation of three words (or 17 characters). An analysis of the position of correct text in the alternates list for the IBM dictation system indicated that a correction control should ideally present four alternates (covering 97 % of the cases for <b>error</b> <b>spans</b> of one word and 90 % of cases for <b>error</b> <b>spans</b> of two words). There appears to be little reason to provide a control that displays more than four alternates for any type of recognition user interface...|$|R
50|$|The {{star has}} about 1.46 times {{the mass of}} the Sun, along with 1.5 times the radius, and 14 times the luminosity. The {{abundance}} of elements other than hydrogen and helium, what astronomers term the star's metallicity, is only 17% of the abundance in the Sun. The star appears to be a very young, probably around 231 million years in age, but the margin of <b>error</b> <b>spans</b> 50-347 million years old.|$|R
40|$|Residual {{motion error}} {{is common in}} {{high-resolution}} circular Synthetic Aperture Radar (SAR) image defocusing. The Signal-to-Clutter Ratio (SCR) in echo data domain is relatively low; thus, the phase <b>error</b> <b>spans</b> several range bins. To solve this problem, we propose a focusing algorithm for circular SAR based on phase-error estimation in the image domain. The method estimates the point-target image window interception and then the phase error from echo regeneration in the defocused image. Subsequently, the range migration error is calculated, and finally, the phase error in the echo data is compensated for azimuth focusing and range cell migration correction. Simulation and real-data processing verified the proposed method...|$|R
40|$|Error {{analysis}} involves detecting and correcting {{discrepancies between}} the 'text produced so far' (TPSF) and the writer's mental representation {{of what the}} text should be. While many factors determine the choice of strategy, cognitive effort is {{a major contributor to}} this choice. This research shows how cognitive effort during error analysis affects strategy choice and success as measured by a series of online text production measures. We hypothesize that error correction with speech recognition software differs from error correction with keyboard for two reasons. Speech produces auditory commands and, consequently, different error types. The study reported on here measured the effects of (1) mode of presentation (auditory or visual-tactile), (2) <b>error</b> <b>span,</b> whether the error spans more or less than two characters, and (3) lexicality, whether the text error comprises an existing word. A multilevel analysis was conducted {{to take into account the}} hierarchical nature of these data. For each variable (interference reaction time, preparation time, production time, immediacy of error correction, and accuracy of error correction), multilevel regression models are presented. As such, we take into account possible disturbing person characteristics while testing the effect of the different conditions and error types at the sentence level. The results show that writers delay error correction more often when the TPSF is read out aloud first. The auditory property of speech seems to free resources for the primary task of writing, i. e. text production. Moreover, the results show that large errors in the TPSF require more cognitive effort, and are solved with a higher accuracy than small errors. The latter also holds for the correction of small errors that result in non-existing words...|$|E
40|$|The spatial QRS-T angle (SA) is a vectorcardiographic (VCG) {{parameter}} {{that has}} been identified as a marker for changes in the ventricular depolarization and repolarization sequence. The SA is defined as the angle subtended by the mean QRS-vector and the mean T- vector of the VCG. The SA is typically obtained from VCG data that is derived from the resting 12 -lead electrocardiogram (ECG). Resting 12 -lead ECG data is commonly recorded using a low-pass filter with a cutoff frequency of 150 Hz. The ability of the SA to quantify changes in the ventricular depolarization and repolarization sequence make the SA potentially attractive {{in a number of different}} 12 -lead ECG monitoring applications. However, the 12 -lead ECG data that is obtained in such monitoring applications is typically recorded using a low-pass filter cutoff frequency of 40 Hz. The aim of this research was to quantify the differences between the SA computed using 40 Hz low- pass filtered ECG data (SA 40) and the SA computed using 150 Hz low-pass filtered ECG data (SA 150). We assessed the difference between the SA 40 and the SA 150 using a study population of 726 subjects. The differences between the SA 40 and the SA 150 were quantified as systematic error (mean difference) and random <b>error</b> (<b>span</b> of Bland-Altman 95 % limits of agreement). The systematic error between the SA 40 and the SA 150 was found to be - 0. 126 ° [95 % confidence interval: - 0. 146 ° to - 0. 107 °]. The random error was quantified 1. 045 ° [95 % confidence interval: 0. 917 ° to 1. 189 °]. The findings of this research suggest that it is possible to accurately determine the value of the SA when using 40 Hz low-pass filtered ECG data. This finding indicates that it is possible to record the SA in applications that require the utilization of 40 Hz low-pass ECG monitoring filters...|$|E
40|$|The {{purpose of}} this thesis is to follow a {{methodology}} in which computational models with {{two degrees of freedom}} (2 DOF), based on the finite volume method, are validated and used to investigate how certain phenomena and parameters, especially the longitudinal position of the waterjet intake, affect resistance and thrust deduction (t) for the STREAMLINE hull. To arrive at a scientific answer to this question a methodology is carried out comprised of a grid dependence study followed by three studies of systematic variation. These include variation of Froude number, variation of initial trim (LCG) and variation of waterjet intake positions. Comparing the output from the Froude number variation and initial trim studies with towing tank test data, error trends are found and are used to obtain the reliability of the computational models. These error trends can then give an indication of the validity of the results from the variation of waterjet intake position study. The results from the grid dependence study show an effective mesh size for the bare hull model of around 2. 6 million elements. By applying this effective mesh size to the BH model, the error trends from the different parameter studies are obtained. The bare hull trends obtained from the Froude number and trim variation studies converge to an <b>error</b> <b>span</b> between 6 - 8 %. A similar validation process using error trends for self propulsion has not been possible {{due to a lack of}} model test data. However, with the four model test data points available, it seems that the self propulsion model is reliable within at least a Froude number span of 0. 44 - 0. 48. It is found that the effect of varying the Froude number for four degrees bow up initial trim and 2 DOF is similar to the effect of varying the Froude number for a zero degree initial trim with a fixed hull. It is also found that the simulated resistance peak for BH appears at a lower Froude number than the measured resistance peak for BH. Also, the SP resistance peak appears before the BH resistance peak. In all tested cases, the resistance peaks are found to be correlated with the transom clearance phenomena. The trim variation study shows that the optimum initial trim for the STREAMLINE hull at design speed (Fn = 0. 996) is the even keel trim for both BH and SP. The position variation study shows that the optimum longitudinal position for the center of the waterjet intake is 465 mm fore of the transom. This study also shows that the difference in resistance between the best and worst position for each Froude number differs with about 2 - 6 %. These results are a good start in gaining knowledge about waterjet-hull interaction and how waterjets can be delivered and installed to improve performance and sustainability...|$|E
40|$|End-to-end {{protocols}} {{lack the}} functionality to efficiently adjust their error control strategies to the distinct characteristics of network environments and to specific constraints of communicating devices. In {{the context of}} heterogeneous networks, error control needs to be responsive {{to the nature of}} the <b>errors</b> <b>spanning</b> a conservative-through-to-aggressive behavioral spectrum. In the context of mobile, batterypowered devices, the recovery strategy should yield good performance with a minimal transmission effort. We exploit the potential of a probing device to implement an adaptive error control strategy efficiently by shaping data transmission to be assorted with the distinctive characteristics of the underlying wired or wireless network components. We graft our mechanism onto standard TCP; we present encouraging experimental results with wired, wireless and mobile networks. q 2002 Elsevier Science B. V. All rights reserved...|$|R
40|$|The {{decomposition}} of networking functions into smaller subsets simplifies implementation {{and leads to}} open standards. Nonetheless, in applications such as mobile Wireless ATM, where resources are scarce, protocol stacks must not compromise on performance. The layers of a protocol stack progressively enhance and add reliability to the supported services. Along with additional new functions, the higher layers frequently implement error control algorithms to overcome residual errors that escape the lower layers. If relatively large numbers of residual errors escape, as is likely in the mobile Wireless ATM environment, then frequent retransmissions or reinitialization of the higher layer protocol entity could result in drastic loss of throughput. In this paper we examine the consequences of <b>errors</b> <b>spanning</b> two layers of a protocol stack using an explicit model {{to account for the}} interaction between two layers. I. INTRODUCTION The advantages that accrue from the use of a layered protocol [...] ...|$|R
40|$|We have {{investigated}} the possibility of inferring peculiar velocities for clusters of galaxies from the Doppler shift of scattered cosmic microwave background (CMB) photons. We find that if the core radius of the gas distribution or the beam size of the instrument is larger than 3 - 7 arcminutes, then the maximum attainable signal-to-noise ratio is determined by confusion with primary fluctuations. For smaller angular scales, ``cosmic confusion'' is less important and instrumental noise and/or foreground emission will be the limiting factor. For a cluster with the optical depth of the Coma cluster and for an optimal filtering technique, typical one-sigma <b>errors</b> <b>span</b> the wide range from 400 to 1600 km/s, depending on the cosmological model, {{the resolution of the}} instrument and the core radius of the cluster. The results have important implications for the design of future high-resolution surveys of the CMB. Individual peculiar velocities will be measurable only for a few fast moving clusters at intermediate redshift unless cosmic fluctuations are smaller than most standard cosmological scenarios predict. However, a reliable measurement of bulk velocities of ensembles of X-ray bright clusters will be possible on very large scales (100 - 500 Mpc/h) ...|$|R
50|$|Dascenzo {{began his}} Major League career by {{playing in a}} then-National League record 241 {{consecutive}} games without making an <b>error.</b> The streak <b>spanned</b> from his debut in 1988 to the 1991 season when he committed his first error in a game on August 25.|$|R
40|$|Abstract. In extreme reversed-shear plasma discharges, {{the ratio}} between the poloidal and the {{toroidal}} magnetic fields obtained from motional-stark-effect measurements displays significant relative errors inside the core region, with the <b>error</b> bar <b>spanning</b> from small positive values into small negative ones, and therefore magnetic equilibria with toroidal-current reversal cannot be excluded. Following a perturbative approach {{to solve the}} Grad-Shafranov equation, a numerical scheme allowing toroidal-current reversed equilibria constrained by experimental data to be computed is proposed and subsequently tested using available measurements from JT 60 -U plasmas in a typical current-hole scenario. 1...|$|R
40|$|In this paper, {{we present}} a new algorithm, such that, for the {{learning}} with errors (LWE) problems, if the errors are bounded – the <b>errors</b> do not <b>span</b> the whole prime finite field Fq but a fixed known subset of size D (D < q), which we call the learning with bounded errors (LWBE) problems, we can solve it with complexity O(n D) ...|$|R
30|$|The {{proposed}} {{method for}} uniform live load provides excellent {{results for the}} preliminary design phase with errors below 10 % in terms of stress prediction. The main <b>errors</b> belong to <b>spans</b> lengths that are not used in cable suspension bridges, this happens due to the exchange of the initial cable shape resulted from high deformation. Therefore, concluding that this method is precise for general geometries.|$|R
40|$|Let H be the {{state-space}} of {{a quantum}} computer {{coupled with the}} environment {{by a set of}} <b>error</b> operators <b>spanning</b> a Lie algebra L. Suppose L admits a noiseless quantum code i. e., a subspace C⊂ H annihilated by L. We show that a universal set of gates over C is obtained by any generic pair of L-invariant gates. Such gates - if not available from the outset - can be obtained by resorting to a symmetrization with respect to the group generated by L. Any computation can then be performed completely within the coding decoherence-free subspace. Comment: One result added, to appear in Phys. Rev. A (RC) 4 pages LaTeX, no figure...|$|R
40|$|We have developedand evaluateda new {{procedure}} for detectingtrends in quality-control measurements and applieditolaboratory data. The method requires {{the use of}} sequential or “moving ” slope estimates to identify trends. Formulaearederivedto estimate the regression error for the moving slope directly from the standard deviation of the analytical measurements obtained during characterization runs. Control limits for the moving slope depend only on this regression <b>error,</b> the <b>span</b> of the slope, and the desired statistical level of control. The moving slope can be plotted with control limits to deter-mine out-of-control points. The statistical power of the moving slope is found to be much greater than that of an often-used test for trends. An example of the use of the moving slope is shown for quality-control measurement...|$|R
40|$|This article presents: (i) a formal, generic {{model for}} active sensing tasks; (ii) the insight that active sensing actions can very often be searched {{on less than}} six-dimensional {{configuration}} spaces (bringing an exponential reduction in the computational costs involved in the search); (iii) an algorithm for selecting actions explicitly trading off information gain, execution time and computational cost; and (iv) experimental results of touch-based localization in an industrial setting. Generalizing from prior work, the formal model represents an active sensing task by six primitives: configuration space, information space, object model, action space, inference scheme and action-selection scheme; prior work applications conform to the model as illustrated by four concrete examples. On top of the mentioned primitives, the task graph is then introduced as the relationship to represent an active sensing task as a sequence of low-complexity actions defined over different configuration spaces of the object. The presented act-reason algorithm is an action selection scheme to maximize the expected information gain of each action, explicitly constraining the time allocated to compute and execute the actions. The experimental contributions include localization of objects with: (1) a force-controlled robot equipped with a spherical touch probe; (2) a geometric complexity of the to-be-localized objects up to industrial relevance; (3) an initial uncertainty of (0. 4 m, 0. 4 m, 2 Π); and (4) a configuration of act-reason to constrain the allocated time to compute and execute the next action {{as a function of}} the current uncertainty. Localization is accomplished when the probability mass within a 5 -mm tolerance reaches a specified threshold of 80 %. Four objects are localized with final {mean; standard-deviation} <b>error</b> <b>spanning</b> from { 0. 0043 m; 0. 0034 m} to { 0. 0073 m; 0. 0048 m}...|$|R
50|$|Bridgetap - Bridge taps {{within a}} span can be {{detected}} by employing a number of test patterns {{with a variety of}} ones and zeros densities. This test generates 21 test patterns and runs for 15 minutes. If a signal <b>error</b> occurs, the <b>span</b> may have one or more bridge taps. This pattern is only effective for T1 spans that transmit the signal raw. Modulation used in HDSL spans negates the bridgetap patterns' ability to uncover bridge taps.|$|R
40|$|Wavefront sensing is {{a process}} by which optical system errors are deduced from the aberrations {{in the image of}} an ideal source. The method has been used {{successfully}} in near-normal incidence, but not for grazing incidence systems. This innovation highlights the ability to examine out-of-focus images from grazing incidence telescopes (typically operating in the x-ray wavelengths, but integrated using optical wavelengths) and determine the lower-order deformations. This is important because as a metrology tool, this method would allow the integration of high angular resolution optics without the use of normal incidence interferometry, which requires direct access to the front surface of each mirror. Measuring the surface figure of mirror segments in a highly nested x-ray telescope mirror assembly is difficult due to the tight packing of elements and blockage of all but the innermost elements to normal incidence light. While this can be done on an individual basis in a metrology mount, once the element is installed and permanently bonded into the assembly, it is impossible to verify the figure of each element and ensure that the necessary imaging quality will be maintained. By examining on-axis images of an ideal point source, one can gauge the low-order figure errors of individual elements, even when integrated into an assembly. This technique is known as wavefront sensing (WFS). By shining collimated light down the optical axis of the telescope and looking at out-of-focus images, the blur due to low-order figure errors of individual elements can be seen, and the figure error necessary to produce that blur can be calculated. The method avoids the problem of requiring normal incidence access to the surface of each mirror segment. Mirror figure <b>errors</b> <b>span</b> a wide range of spatial frequencies, from the lowest-order bending to the highest order micro-roughness. While all of these can be measured in normal incidence, only the lowest-order contributors can be determined through this WFS technique...|$|R
40|$|Abstract Time series {{analysis}} {{is a tool}} that is now commonly used when analysing the states of natural populations. This is a particularly complicated task for ungulates, since the data involved usually contain large observation <b>errors</b> and <b>span</b> {{short periods of time}} relative to the species’ life expectancies. Here we develop a method that expands on previous analyses, combining statistical state space modelling with biological mechanistic modelling. This enables biological interpretability of the statistical parameters. We used this method to analyse African ungulate census data, and it revealed some clarifying patterns. The dynamics of one group of species were generally independent of density and strongly affected by rainfall, while the other species were governed by a delayed density dependence and were relatively unaffected by rainfall variability. Dry season rainfall was more influential than wet season rainfall, which can be interpreted as indicating that adult survival is more important than recruitment in governing ungulate dynamics...|$|R
40|$|Inferring gene {{regulatory}} networks from {{expression profiles}} is a challenging {{problem that has}} been tackled using many different approaches. When posed as an optimization problem, the typical goal is to minimize {{the value of an}} error measure, such as the relative squared error, between the real profiles and those generated with a model whose parameters are to be optimized. In this paper, we use dynamic recurrent neural networks to model regulatory interactions and study systematically the "fitness landscape" that results from measuring the relative squared error. Although {{the results of the study}} indicate that the generated landscapes have a positive fitness-distance correlation, the <b>error</b> values <b>span</b> several orders of magnitude over very short distance variations. This suggests that the fitness landscape has extremely deep valleys, which can make general-purpose state-of-the-art continuous optimization algorithms exhibit a very poor performance. Further results, obtained from an analysis based on perturbations of the optimal network topology, support approaches in which the spaces of network topologies and of network parameters are decoupled. © 2010 Springer-Verlag. SCOPUS: cp. kinfo:eu-repo/semantics/publishe...|$|R
40|$|Analog {{integrated}} circuit design {{has become increasingly}} difficult in modern fabrication processes. The motivation for digital speed has posed problems for mixed-signal projects that wish to implement digital and analog blocks on the same chip. With the introduction of multigate transistors (also known as FinFETs), the challenges for analog design increase. This {{is due to the}} fact that FinFET devices will no longer have a continuum of width and lengths sizes (as previous technologies have exhibited), but instead, these parameters are now quantized. This work proposes a potential solution to the fixed-length problem, in a topology termed the ``series-stack". Foundries plan to launch the FinFET technology with a number of fixed-sized transistors (typically with minimum length). To the digital designer, this poses little problem, but for analog circuits, not being able to control device length compromises the ability to meet gain specifications. This work explores a simple method for implementing longer devices: connecting transistors in series, herein called series-stack. To test the feasibility of this architecture, a two-stage CMOS operational amplifier is designed. In lieu of application-specific design constraints, a structure strategy is presented. A key motivation for the series-stack as well as the design strategy is to bring the analog design process up a level of abstraction. The amplifier was planned to be put through the entire design cycle, from conception to lab testing, giving insight into the accuracy of simulation models. Schematic and post-layout results were collected from the TSMC 65 nm kit. Analysis of the results yield obvious simulation discrepancies. Namely, the schematic simulation vastly overestimates the parasitic resistances and capacitances when using finger-gate techniques. This is an important problem for which possible solutions are discussed. Additionally, the results show significant differences between conventional bulk length and series-stack, with a relative <b>error</b> <b>spanning</b> from 2 % to 20 % depending on the performance metric. Yet, most discrepancies are expected, and the two implementations follow similar trends with respect to current density and length. A final verdict cannot be delivered until physical chip testing is conducted, which is left to future work (complications in timeline did not allow for the lab test results to be included). Although chip testing was not completed, a thorough testing plan is formulated. Despite physical testing, the series-stack is deemed a suitable alternative to long transistor designs, especially when considering the organizational advantages at the layout level...|$|R
5000|$|For greater {{convenience}} {{in placing}} large and awkward loads, a platform can be floated on a cantilever beam system which brings the proportional force to a noseiron bearing; this pulls on a stilyard rod to transmit the reduced force to a conveniently sized beam. One still sees this design in portable beam balances of 500 kg capacity which {{are commonly used}} in harsh environments without electricity, {{as well as in}} the lighter duty mechanical bathroom scale (which actually uses a spring scale, internally). The additional pivots and bearings all reduce the accuracy and complicate calibration; the float system must be corrected for corner <b>errors</b> before the <b>span</b> is corrected by adjusting the balance beam and poise.|$|R
40|$|We have {{developed}} a predictive model for the partitioning of magnesium {{and a range of}} trivalent trace elements (rare earth elements, Y, In and Sc) between garnet and anhydrous silicate melt as a function of pressure, temperature and bulk composition. The model for the magnesium partition coefficient, DMg, is based on a thermodynamic description of the pyrope (Mg 3 Al 2 -Si 3 O 12) melting reaction between garnet and melt. Simple activity-composition relations, which take explicit account of garnet non-ideality, link DMg to the free energy of fusion (ΔGf) of pure pyrope without the need to invoke non-ideality in the liquid phase. The resulting predictive equation, based on the compositions of a large set (n = 160) of published garnet-melt pairs, produces values of DMg that are within 20 % of measured values at temperatures between 1, 450 and 1, 930 °C, and pressures between 2. 5 and 7. 5 GPa. The model for trivalent (3 +) trace elements is based on the lattice strain approach to partitioning, which describes mineral-melt partition coefficients in terms of three parameters; the effective radius, ro(3 +), of the site on which partitioning takes place (in this case, the garnet X-site); the apparent site Young's modulus Ex(3 +); and the partition coefficient Do(3 +) for a fictive trivalent element J 3 +, with radius ro(3 +), that does not strain the crystal lattice when entering the garnet X-site. Analogous to the model for DMg, simple activity-composition relations link Do(3 +) to ΔGf of a hypothetical garnet component incorporating a hypothetical rare earth element J 3 + through a YAG-type charge-balancing mechanism (J 3 +Mg 2 Al 3 Si 2 O 12). Through analysis of existing garnet-melt rare earth element partitioning data (n = 18 garnet-melt pairs), an expression is derived relating Do(3 +) to pressure, temperature and DMg. Predicted DREE/Y/Sc values agree to within 5 - 50 % of experimental measurements for all elements except La and Ce, which are liable to large experimental <b>errors,</b> <b>spanning</b> pressures between 2. 5 and 5. 0 GPa and temperatures between 1, 430 and 1, 640 °C. In conjunction with our new parameterisation for DMg, and previously published equations linking ro(3 +) and Ex(3 +) to garnet major element composition, this model gives a description of trivalent REE, Y, In and Sc partitioning between garnets and anhydrous melts over a range of pressures, temperatures and compositions relevant to melting of garnet-bearing sources in the Earth's upper mantle...|$|R
40|$|This paper {{suggests}} {{solutions to}} {{two different types}} of simulation errors related to Quasi-Monte Carlo integration. Likelihood functions which depend on standard deviations of mixed parameters are symmetric in nature. This paper shows that antithetic draws preserve this symmetry and thereby improves precision substantially. Another source of error is that models testing away mixing dimensions must replicate the relevant dimensions of the quasi-random draws in the simulation of the restricted likelihood. These simulation errors are ignored in the standard estimation procedures used today and this paper shows that the result may be substantial estimation- and inference <b>errors</b> within the <b>span</b> of draws typically applied. Quasi-Monte Carlo integration; Antithetic draws; Likelihood Ratio tests; simulated likelihood; panel mixed multinomial logit; Halton draws...|$|R
40|$|Estimates of {{the source}} sky {{location}} for gravitational wave signals are likely span areas ranging up to hundreds of square degrees or more, making it very challenging for most telescopes to search for counterpart signals in the electromagnetic spectrum. To boost the chance of successfully observing such counterparts, we have developed an algorithm which optimizes the number of observing fields and their corresponding time allocations by maximizing the detection probability. As a proof-of-concept demonstration, we optimize follow-up observations targeting kilonovae using telescopes including CTIO-Dark Energy Camera, Subaru-HyperSuprimeCam, Pan-STARRS and Palomar Transient Factory. We consider three simulated gravitational wave events with 90 % credible <b>error</b> regions <b>spanning</b> areas from ~ 30 deg^ 2 to ~ 300 deg^ 2. Assuming a source at 200 Mpc, we demonstrate that to obtain a maximum detection probability, there is an optimized number of fields for any particular event that a telescope should observe. To inform future telescope design studies, we present the maximum detection probability and corresponding number of observing fields for a combination of limiting magnitudes and fields-of-view over a range of parameters. We show that for large gravitational wave error regions, telescope sensitivity rather than field-of-view, is the dominating factor in maximizing the detection probability...|$|R
40|$|Lawyers err every day, in {{hard and}} easy cases, in trials and transactions, and in {{large and small}} firms. By turns commonplace and noteworthy, the errors fall in both the private shadow and the public light of for-profit, nonprofit, and {{government}} practice. The literature of lawyer and, by extension, law firm <b>error</b> <b>spans</b> common law doctrines, state ethics rules and opinions, federal rules, practitioner treatises, restatements, and academic casebooks and commentaries. Despite the breadth of this literature, the intertwined problems of lawyer or law firm error and client malpractice disclosure remain unresolved and surprisingly underappreciated. Against the backdrop of widening debates over the ethical culture and infrastructure of law firms, this Article recasts the originating problem of lawyer and law firm error narrowly in terms of client malpractice disclosure. Framing the problem of professional error in the limited terms of client disclosure focuses the inquiry more closely on lawyer and law firm acts of delay in communicating information to clients and acts of declination in withholding information from clients. Thus tailored, two questions stand out. First, when may a lawyer or law firm permissibly delay disclosure of information to a client about an error-related incident of malpractice? And second, when may a lawyer or law firm permissibly withhold information from a client about such an incident and its consequences? Like the instant symposium, both questions implicate considerations of law firm culture and infrastructure, ethical regulation, professional liability, and risk management policy and procedure. To address these fundamental yet often overlooked questions, the Article proceeds in three parts. Part I situates the problems of law firm error and client malpractice disclosure in the broader framework of lawyer regulation and law firm ethical culture. Part II summarizes the general law and rule framework governing law firm malpractice disclosure and illustrates the interplay of common law and rule-based claims and defenses in malpractice disputes. Part III examines the content of contemporary best practice guidelines for malpractice disclosure {{in the field of}} lawyer and law firm regulation. Taken together, the three parts construct a conventional, generalizable account of law firm malpractice disclosure disputes familiar to practitioners, regulators, and academics alike. Chronicled through a series of interwoven case illustrations, the account depicts the standard progression of malpractice disclosure disputes from the formation of the client-lawyer relationship (Illustration A), to the lawyer’s commission and the law firm’s discovery of a harmful performance error (Illustration B), to the law firm’s delay and omission in communicating information of the error to the client (Illustration C), to the undisclosed sharing of client information with the law firm’s in-house general counsel, malpractice insurance carrier, and outside professional liability counsel (Illustration D), and finally to the law firm’s internal investigation of client-lawyer conflicts of interest and the ultimate decision to withdraw from the representation (Illustration E) ...|$|R
40|$|I {{propose to}} define the errors: of {{composition}} and study their frequency in the ninth, tenth, eleventh, and twelfth-grade college-preparatory English classes at New Castle Senior High School, New Castle, Indiana. The primary aim would be to note any decrease {{in the frequency of}} <b>errors</b> in compositions <b>spanning</b> the four grade levels of a high school English curriculum. The prevalence of composition errors at various grade levels in high school should ideally follow a pattern showing a decrease. A study of their frequency would support or refute this supposition. Regardless of the outcome, graphs or charts showing error-frequency would be valuable in illustrating the "trouble-spots"' in high school English composition. It would further aid in pointing out those areas that need special emphasis in teaching composition in the public high school. Honors CollegeThesis (B. ?. ...|$|R
40|$|Space-time block codes {{typically}} only span a {{small number}} of time slots. However, <b>error</b> control coding <b>spanning</b> hundreds or thousands of symbol periods is often used with space-time coding. In this case, slow fading channels significantly degrade performance. In this letter, we show that performance can be significantly improved in slow fading conditions by using reconfigurable antennas. We propose switching the transmit antenna states during transmission so that the error control code experiences several different channel conditions over the transmission period. Hence, it can achieve higher diversity and the superior performance expected of a faster fading channel due to reconfigurable antenna state switching. If we also allow the receiver to select the reconfigurable receive antenna states based on channel energy, then an antenna state selection gain is also achieved...|$|R
40|$|The use of LANDSAT {{landmark}} {{data for}} orbit/attitude and camera bias estimation was studied. The preliminary {{results of these}} investigations are presented. The Goddard Trajectory Determination System (GTDS) error analysis capability was used to perform error analysis studies. A number of questions were addressed including parameter observability and sensitivity, effects on the solve-for parameter <b>errors</b> of data <b>span,</b> density, and distribution an a priori covariance weighting. The use of the GTDS differential correction capability with acutal landmark data was examined. The rms line and element observation residuals were studied {{as a function of}} the solve-for parameter set, a priori covariance weighting, force model, attitude model and data characteristics. Sample results are presented. Finally, verfication and preliminary system evaluation of the LANDSAT NAVPAK system for sequential (extended Kalman Filter) estimation of orbit, and camera bias parameters is given...|$|R
40|$|Neuropsychological and {{developmental}} models of number, counting, and arithmetical skills, {{as well as}} the supporting working memory and speed of articulation systems, were used as the theoretical framework for comparing groups of low- and average-IQ children. The low-IQ children, in relation to their average-IQ peers, showed an array of deficits, including difficulties in retaining information in working memory while counting, more problem solving <b>errors,</b> shorter memory <b>spans,</b> and slower articulation speeds. At the same time, the low-IQ children’s conceptual understanding of counting did not differ from that of their higher-IQ peers. Implications for the relation between IQ and mathematics achievement are discussed. Intelligence (IQ) is the best single predictor of academic achievement, years of schooling completed, and many other important outcomes (Jensen, 1998). Basically, intelligence represents the ease with which individuals can acquire novel and complex material, whether the content of the material is social...|$|R
40|$|In this paper, {{we present}} a new {{supervised}} learning method called NOVEL (Nonlinear Optimization Via External Lead) for training feed-forward neural networks. In general, such learning {{can be considered as}} a nonlinear global optimization problem in which the goal is to minimize the nonlinear <b>error</b> function that <b>spans</b> the space of weights. NOVEL is a trajectorybased nonlinear optimization method that combines global and local searches to find good local minima. It relies on an external force to pull a search out of a local minimum in its global search and employs local descents to locate local minima in its local search. The result is an efficient search method that identifies good basins without {{spending a lot of time}} in them. We have shown improved training and testing results for five neural-network benchmark problems as compared to those of existing minimization and neural-network learning algorithms. For the two-spiral problem, NOVEL has found a design using only four hidden units a [...] ...|$|R
40|$|In this paper, {{we study}} various {{supervised}} learning methods for training feed-forward neural networks. In general, such learning {{can be considered}} as a nonlinear global optimization problem in which the goal is to minimize a nonlinear <b>error</b> function that <b>spans</b> the space of weights using heuristic strategies that look for global optima (in contrast to local optima). We survey various global optimization methods suitable for neural-network learning, and propose the NOVEL method, a novel global optimization method for nonlinear optimization and neural network learning. By combining global and local searches, we show how NOVEL can be used to find a good local minimum in the error space. Our key idea is to use a user-defined trace that pulls a search out of a local minimum without having to restart it from a new starting point. Using five benchmark problems, we compare NOVEL against some of the best global optimization algorithms and demonstrate its superior improvement in performance. 1 In [...] ...|$|R
40|$|This paper {{analyses}} {{the numerical}} difficulties commonly encountered in solving fully coupled numerical models and proposes a numerical strategy apt to overcome them. The proposed procedure {{is based on}} space refinement and time adaptivity. The latter, which in mainly studied here, {{is based on the}} use of a finite element approach in the space domain and a Discontinuous Galerkin approximation within each time <b>span.</b> <b>Error</b> measures are defined for the jump of the solution at each time station. These constitute the parameters allowing for the time adaptivity. Some care is however, needed for a useful definition of the jump measures. Numerical tests are presented firstly to demonstrate the advantages and shortcomings of the method over the more traditional use of finite differences in time, then to assess the efficiency of the proposed procedure for adapting the time step. The proposed method reveals its efficiency and simplicity to adapt the time step in the solution of coupled field problems...|$|R
