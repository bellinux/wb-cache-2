5|41|Public
40|$|This article {{presents}} {{a method for}} determining building height using shadows in remotely sensed data. The approach classifies building shadows based on their relative within-scene characteristics and spatial context, rules for which are determined empirically against geolocated photographs of the study area. The <b>formal</b> <b>heuristic</b> is described. This method complements existing approaches for extracting building heights that require precise specification of the geometric characteristics of the remotely sensed data and/or large amounts of ancillary data. The approach is suitable for analyses which may only require approximate measures of building height such as those found in many other domains. Peer-reviewedPost-prin...|$|E
40|$|This study {{reports the}} {{development}} of a normative profile typology for showing what types of score patterns are commonplace in the standardization sample of the McCarthy Scales of Children's Abilities (McCarthy, 1972). The benefit of such a core profile typology is that it provides necessary contrasts for testing hypotheses that children's pro-files are descriptively or clinically unique. Stan-dard scores from the battery's primary scales were grouped according to similar shape and level using sequential minimum-variance cluster analysis with independent replications. A final solution of five core profile types met all <b>formal</b> <b>heuristic</b> and statistical criteria, including (a) satisfactory inter-nal cohesion, (b) external isolation, (c) replicabil...|$|E
40|$|While {{the concept}} of regular {{equivalence}} is equally applicable to dichotomous as well as valued networks, the identification of regular blocks in regular blockmodels is somewhat problematic when dealing with valued networks. Applying the standard procedure for identifying ties in such blockmodels, a procedure perhaps most suited for dichotomous networks, does tend to generate block images and reduced graphs that differ from intuitive notions of such structures. This paper outlines a <b>formal</b> <b>heuristic</b> procedure for identifying regular ties in valued networks where the “significance” of ties is related to each actor's role sets. Combined with measures for block criteria fulfillment, the procedure yields reduced graphs, which seem more sensitive to patterns, rather than strengths, of ties. Two data sets are used as examples in this paper: the St. Marks carbon flow web dataset, and a new dataset containing international trade flows of {{cereals and cereal products}} based on Comtrade data...|$|E
50|$|Parametricism offers {{functional}} and <b>formal</b> <b>heuristics</b> based on set of general abstract rules distilled {{from a very}} complex ecosystem of sustained avant-garde design research that spans over twenty five years of continuous innovative communication. Parametricism achieves elegance in both senses of the word - it is unified (compact) and beautiful (vital).|$|R
50|$|Similar to the Functional heuristics, {{there are}} unified <b>Formal</b> <b>heuristics</b> {{distinguishing}} Parametricism from other styles of architecture. The Negative Principles include {{the avoidance of}} rigid forms that lack malleability; the avoidance of simple repetition that lacks variety; and the avoidance of collage of isolated and unrelated elements that result in a lack of order. The Positive Principles include the intelligent information-rich deformation of soft forms; differentiation of all systems through gradients, thresholds and singularities; and interdependent correlation of all systems.|$|R
25|$|The {{tractability}} of the Poisson {{process means}} that {{sometimes it is}} convenient to approximate a non-Poisson point process with a Poisson one. The overall aim is to approximate the both number of points of some point process {{and the location of}} each point by a Poisson point process. There a number of methods {{that can be used to}} justify, informally or rigorously, approximating the occurrence of random events or phenomena with suitable Poisson point processes. The more rigorous methods involve deriving upper bounds on the probability metrics between the Poisson and non-Poisson point processes, while other methods can be justified by less <b>formal</b> <b>heuristics.</b>|$|R
40|$|This {{dissertation}} {{presents a}} computational multiscale framework for predicting behavioral tendencies related to human addiction. The research encompasses three main contributions. The first contribution presents a <b>formal,</b> <b>heuristic,</b> and exploratory framework to conduct interdisciplinary investigations about the neuropsychological, cognitive, behavioral, and recovery constituents of addiction. The second contribution proposes a computational framework {{to account for}} real-life recoveries that are not dependent on pharmaceutical, clinical, and counseling support. This exploration relies upon a combination of current biological beliefs together with unorthodox rehabilitation practices, such as meditation, and proposes a conjecture regarding possible cognitive mechanisms involved in the recovery process. Further elaboration of this investigation leads on to the third contribution, which introduces a computational hypothesis for exploring the allostatic theory of addiction. A person engaging in drug consumption is likely to encounter mood deterioration and eventually to suffer {{the loss of a}} reasonable functional state (e. g., experience depression). The allostatic theory describes how the consumption of abusive substances modifies the brain 2 ̆ 7 s reward system by means of two mechanisms which aim to viably maintain the functional state of an addict. The first mechanism is initiated in the reward system itself, whereas the second might originate in the endocrine system or elsewhere. The proposed computational hypothesis indicates that the first mechanism can explain the functional stabilization of the addict, whereas the second mechanism is a candidate for a source of possible recovery. ^ The formal arguments presented in this dissertation are illustrated by simulations which delineate archetypal patterns of human behavior toward drug consumption: escalation of use and influence of conventional and alternative rehabilitation treatments. Results obtained from this computational framework encourage an integrative approach to drug rehabilitation therapies which combine conventional therapies with alternative practices to achieve higher rates of consumption cessation and lower rates of relapse. ...|$|E
40|$|In {{computer}} game development, {{an interesting point}} which has been little or no studied at all is the formalization of intuition such as game playing concepts, including playing style. This work is devoted {{to bridge the gap}} between human reasoning in game playing and heuristic game playing algorithms. The idea is motivated as follows. In most chess-like games there exist many intuition-oriented concepts such as capture, attack, defence, threaten, blocked position, sacrifice, zugzwang position and different playing styles such as aggressive, conservative, tactical and positional. Most human players use to manage these concepts, pergaps in an intuitive way, as they were not well formalized in a precise manner. A good formalization of these concepts would be an important step towards the automation of human reasoning in chess (and other strategy games) for better understanding of the game, thus leading to better playing. The goal of this research is to take a first step towards the unification of both "paradigms", namely human reasoning in game play and more <b>formal</b> <b>heuristic</b> concepts. We focus on computer chess as an example but the result could be also applied to most two-player zero-sum perfect information games. The applications of such a formulation are practical, such as better game understanding and opponent modeling, as well as educational: it would be nice to have these concepts somehow formalized. Then we suggest a way of transfering these intuitions into formal definitions. We propose an interpretation technique for describing chess positions and evaluation functions. The technique consists of interpreting and mapping part of the algorithmic scenario into quantities such as integer numbers. With such a mapping a given concept is likely to be described in a very precise way. As an application we look for candidate definitions of the following concepts: attack, defence, threat, sacrifice, zugzwang, aggressive play and defensive play. For each one of them we use the previous technique and propose a formal definition. Thus we give the first formulation of game playing styles -at least to the author 2 ̆ 7 s knowledge- and we show how this definition goes through for the game of chess. We describe different possibilities when moving from intuition to the formal setting, varying from a simple formulation through a connectionist approach. Then we show as an application how an evaluation function can be modified in order to include a given concept. This new evaluation function should take into account the degree of presence of the given concept (eg. how defensive is a given position) and thus it can be incorporated into a computer chess program. An advantage of allowing one to modify in such a manner an evaluation function is that one can combine different evaluation functions and -perhaps- get the better of each one of them. Although this is a first step in the given direction, some more difficult tasks will remain, such as the formalization of the so called positional, strategic and tactical play. References B. Abramson. Learning expected-outcome evaluators in chess. In H. Berliner, editor, Proceedings of the AAAI Spring Symposium on Computer Game Playing, pages 26 - 28, Stanford University, 1988. B. Abramson. On learning and testing evaluation functions. Journal of Experimental and Theoretical Artificial Intelligence, 2 (3) : 182 - 193, 1990. T. S. Anantharaman. Evaluation tuning for computer chess: Linear discriminant methods. International Computer Chess Association Journal, 20 (4) : 224 - 242, 1997. E. B. Baum, Warren D. Smith. Best Play for Imperfect Players and Game Tree Search. 1993 J. Fürnkranz. Machine Learning in Computer Chess: The Next Generation Austrian Research Institute for Artificial Intelligence, Vienna, TR- 96 - 11, 1996. A. Plaat, J. Schaeffer, W. Pijls and A. De Bruin. Best-First Fixed-Depth Game-Tree Search in Practice. IJCAI 2 ̆ 795, Montreal. J. Schaeffer, P. Lu, D. Szafron and R. Lake. A Re-examination of Brute-Force Search Games: Planning and Learning, Chapel Hill, N. C., pp. 51 - 58, 1993. AAAI Report FS 9302...|$|E
2500|$|Many {{other choices}} have been suggested, both <b>formal</b> and <b>heuristic,</b> based on theory or {{simulations}} relevant in context. The following subsections discuss some of these. A narrower question is choosing a maximum (estimation {{of a population}} maximum), known as the German tank problem, for which similar [...] "sample maximum, plus a gap" [...] solutions exist, most simply [...] A more formal application of this uniformization of spacing occurs in maximum spacing estimation of parameters.|$|R
40|$|The {{foundations}} of the inverse mean field method (Imefim) {{and its relation to}} traditional approaches are discussed. Imefim predicts the energy dependence of the real central (nuclear) part of the optical model potential to have the functional form (1 + eEp) ‒ 3. Treating for the time being the constant e as an adjustable parameter, this prediction is shown to compare nicely with well-established <b>formal</b> and <b>heuristic</b> results. © 1983, Walter de Gruyter. All rights reserved...|$|R
50|$|Section A has as {{its primary}} focus the theory and methodologies that are central to {{computer}} science. The section welcomes contributions from across this spectrum as well as papers involving the novel application of theoretical research or the adaptation of established methodologies to computational problems in other domains or within software tools. Thematic areas include: algorithms and complexity; computational logic; <b>formal</b> methods; <b>heuristic</b> search; mathematics of computing; models of computation and unconventional computing; programming languages and semantics; and software engineering.|$|R
40|$|AbstractBackgroundKnowledge Organization Systems (KOS) {{and their}} {{associated}} mappings {{play a central}} role in several decision support systems. However, by virtue of knowledge evolution, KOS entities are modified over time, impacting mappings and potentially turning them invalid. This requires semi-automatic methods to maintain such semantic correspondences up-to-date at KOS evolution time. MethodsWe define a complete and original framework based on <b>formal</b> <b>heuristics</b> that drives the adaptation of KOS mappings. Our approach takes into account the definition of established mappings, the evolution of KOS and the possible changes that can be applied to mappings. This study experimentally evaluates the proposed heuristics and the entire framework on realistic case studies borrowed from the biomedical domain, using official mappings between several biomedical KOSs. ResultsWe demonstrate the overall performance of the approach over biomedical datasets of different characteristics and sizes. Our findings reveal the effectiveness in terms of precision, recall and F-measure of the suggested heuristics and methods defining the framework to adapt mappings affected by KOS evolution. The obtained results contribute and improve the quality of mappings over time. ConclusionsThe proposed framework can adapt mappings largely automatically, facilitating thus the maintenance task. The implemented algorithms and tools support and minimize the work of users in charge of KOS mapping maintenance...|$|R
40|$|International audienceBackground. Knowledge Organization Systems (KOS) {{and their}} {{associated}} mappings {{play a central}} role in several decision support systems. However, by virtue of knowledge evolution, KOS entities are modified over time, impacting mappings and potentially turning them invalid. This requires semi-automatic methods to maintain such semantic correspondences up-to-date at KOS evolution time. Methods. We define a complete and original framework based on <b>formal</b> <b>heuristics</b> that drives the adaptation of KOS mappings. Our approach takes into account the definition of established mappings, the evolution of KOS and the possible changes that can be applied to mappings. This study experimentally evaluates the proposed heuristics and the entire framework on realistic case studies borrowed from the biomedical domain, using official mappings between several biomedical KOSs. Results. We demonstrate the overall performance of the approach over biomedical datasets of different characteristics and sizes. Our findings reveal the effectiveness in terms of precision, recall and f-measure of the suggested heuristics and methods defining the framework to adapt mappings affected by KOS evolution. The obtained results contribute and improve the quality of mappings over time. Conclusions. The proposed framework can adapt mappings largely automatically, facilitating thus the maintenance task. The implemented algorithms and tools support and minimize the work of users in charge of KOS mapping maintenance...|$|R
40|$|Knowledge Organization Systems (KOS) {{and their}} {{associated}} mappings {{play a central}} role in several decision support systems. However, by virtue of knowledge evolution, KOS entities are modified over time, impacting mappings and potentially turning them invalid. This requires semi-automatic methods to maintain such semantic correspondences up-to-date at KOS evolution time. We define a complete and original framework based on <b>formal</b> <b>heuristics</b> that drives the adaptation of KOS mappings. Our approach takes into account the definition of established mappings, the evolution of KOS and the possible changes that can be applied to mappings. This study experimentally evaluates the proposed heuristics and the entire framework on realistic case studies borrowed from the biomedical domain, using official mappings between several biomedical KOSs. We demonstrate the overall performance of the approach over biomedical datasets of different characteristics and sizes. Our findings reveal the effectiveness in terms of precision, recall and F-measure of the suggested heuristics and methods defining the framework to adapt mappings affected by KOS evolution. The obtained results contribute and improve the quality of mappings over time. The proposed framework can adapt mappings largely automatically, facilitating thus the maintenance task. The implemented algorithms and tools support and minimize the work of users in charge of KOS mapping maintenance...|$|R
40|$|Background: Knowledge Organization Systems (KOS) {{and their}} {{associated}} mappings {{play a central}} role in several decision support systems. However, by virtue of knowledge evolution, KOS entities are modified over time, impacting mappings and potentially turning them invalid. This requires semi-automatic methods to maintain such semantic correspondences up-to-date at KOS evolution time. Methods: We define a complete and original framework based on <b>formal</b> <b>heuristics</b> that drives the adaptation of KOS mappings. Our approach takes into account the definition of established mappings, the evolution of KOS and the possible changes that can be applied to mappings. This study experimentally evaluates the proposed heuristics and the entire framework on realistic case studies borrowed from the biomedical domain, using official mappings between several biomedical KOSs. Results: We demonstrate the overall performance of the approach over biomedical datasets of different characteristics and sizes. Our findings reveal the effectiveness in terms of precision, recall and F-measure of the suggested heuristics and methods defining the framework to adapt mappings affected by KOS evolution. The obtained results contribute and improve the quality of mappings over time. Conclusions: The proposed framework can adapt mappings largely automatically, facilitating thus the maintenance task. The implemented algorithms and tools support and minimize the work of users in charge of KOS mapping maintenance. (C) 2015 Elsevier Inc. All rights reserved...|$|R
40|$|International audienceThis paper {{presents}} {{simple and}} practical approaches for controlling {{the complexity of}} neural networks (NN) in order to optimize their generalization ability. Several <b>formal</b> and <b>heuristic</b> methods have been proposed in the literature for improving the performances of NNs. It is of major importance for the user to understand which cf these methods are of practical use and which are the more efficient. We will try here {{to fill the gap}} between specialists of these techniques and the user by presenting and analyzing some methods which we have selected both for their simplicity and efficiency. We will consider only supervised learning...|$|R
40|$|When {{comparing}} the component structures of {{a multitude of}} variables across different groups, the conclusion often is that the component structures are very similar in general and differ in a few variables only. Detecting such "outlying variables" is substantively interesting. Conversely, it can help {{to determine what is}} common across the groups. This article proposes and evaluates two <b>formal</b> detection <b>heuristics</b> to determine which variables are outlying, in a systematic and objective way. The heuristics are based on clusterwise simultaneous component analysis, which was recently presented as a useful tool for capturing the similarities and differences in component structures across groups. The heuristics are evaluated in a simulation study and illustrated using cross-cultural data on values...|$|R
40|$|This paper {{presents}} the first {{results of a}} research work, which purposes were to develop the knowledge base and the architecture of computer tool to assist the design of welded products. The most important contributions of this research work were the definition of knowledge structure and its components, <b>formal</b> and <b>heuristic</b> knowledge components construction, {{and the development of}} a design process model for manufacturing and assembly and the validations of contributions on projects of local metal-mechanical industry. The knowledge base includes principles and rules to design, information about costs and welding times, selection factors for welding processes, welding standards, definition of welding construction types, pre-welding and post-welding processes and welding geometry among others aspects. ...|$|R
40|$|Graphs are {{suitable}} modeling formalisms for {{software and hardware}} systems involving aspects such as communication, object orientation, concurrency, mobility and distribution. State spaces of such systems can be represented by graph transition systems, which are basically transition systems whose states and transitions represent graphs and graph morphisms. Heuristic search is a successful Artificial Intelligence technique for solving exploration problems implicitly present in games, planning, and <b>formal</b> verification. <b>Heuristic</b> search exploits information about the problem being solved to guide the exploration process. The main benefits are significant reductions in the search effort {{and the size of}} solutions. We propose the application of heuristic search for the analysis of graph transition systems. We define algorithms and heuristics and present experimental results...|$|R
40|$|The paper {{describes}} {{an approach to}} automatic generation of assembly sequences, which uses <b>formal</b> and <b>heuristic</b> knowledge {{in the area of}} assembly. A design process is envisioned where the engineer specifies individual part geometries and relates parts to each other to create assemblies. A computerized tool then analyzes the design and suggests sequences for assembling the product. The high-level modeling scheme is described first. Parts are represented in terms of their features, while products consist of parts related to each other through mating conditions. Automatic reasoning processes applied to the design representation plan selected assembly sequences in two stages: A graph transformation method generates an exploded view of the product and a rule-based procedure derives selected assembly sequences. An implementation for triaxial assemblies is described and demonstrated...|$|R
40|$|User Interface Inspection Methods succinctly covers five {{inspection}} methods: heuristic evaluation, perspective-based {{user interface}} inspection, cognitive walkthrough, pluralistic walkthrough, and <b>formal</b> usability inspections. <b>Heuristic</b> evaluation {{is perhaps the}} best-known inspection method, requiring a group of evaluators to review a product against a set of general principles. The perspective-based user interface inspection {{is based on the}} principle that different perspectives will find different problems in a user interface. In the related persona-based inspection, colleagues assume th...|$|R
40|$|Abstract:- Generalized {{algorithms}} {{for solving}} problems of discrete, integer, and Boolean programming are discussed. These algorithms {{are associated with}} the method of normalized functions, are based on a combination of <b>formal</b> and <b>heuristic</b> procedures, and allow one to obtain quasioptimal solutions after a small number of steps, that promotes overcoming the NP-completeness of discrete optimization problems. Questions of building so-called "duplicate " algorithms are considered {{to improve the quality of}} discrete optimization problem solutions. The subsequent development of the algorithms is related to using their modifications to solve optimization problems under conditions of uncertainty within the framework of a general approach to analyzing models with fuzzy coefficients in objective functions and constraints. In practical aspect, the algorithms are already being used to solve diverse problems of power engineering...|$|R
40|$|It is {{well known}} that mathematicians switch to {{different}} views of a problem when needed. They can represent a problem at a <b>formal,</b> conceptual, <b>heuristic,</b> algorithmic or constraint level whenever necessary. To represent Mathematics within several formalisms {{has been the subject of}} many research projects. However, only few knowledge-based systems manage translations of representations between theories. The goal of this paper is twofold. One the one hand, we report on a hybrid knowledge representation and reasoning system called Mantra. The system provides four different knowledge representation methods [...] first-order logic, terminological language, semantic networks, and production rules [...] distributed into a three levels architecture. Specifications of mathematical domains of computation and their inherently related type inference mechanisms can be transformed into knowledge bases. On the other hand, we argue that a main requirement when designing future environments is the capabili [...] ...|$|R
40|$|We {{discuss the}} long-time {{behaviour}} of solutions to Smoluchowski's coagulation equation with kernels of homogeneity one, combining <b>formal</b> asymptotics, <b>heuristic</b> arguments based on linearization, and numerical simulations. The case {{of what we}} call diagonally dominant kernels is particularly interesting. Here one expects that the long-time behaviour is, after a suitable change of variables, the same as for the Burgers equation. However, for kernels that are close to the diagonal one we obtain instability of both, constant solutions and traveling waves and in general no convergence to N-waves for integrable data. On the other hand, for kernels not close to the diagonal one these structures are stable, but the traveling waves have strong oscillations. This has implications on the approach towards an N-wave for integrable data, which is also characterized by strong oscillations near the shock front. Comment: 24 pages, several figure...|$|R
40|$|A novel {{framework}} {{based on}} Bayes-based confidence measure (BBCM) for Multiple Classifier System (MCS) fusion is proposed. As shown here, BBCM based MCS combination scheme {{corresponds to the}} ordinary Bayes fusion weighted by the reliability of each individual classifier. BBCM provides a <b>formal</b> model for <b>heuristic</b> weighting functions employed elsewhere. When compared with the ordinary Bayesian fusion, the proposed method leads to reductions as high as 20 % and 50 % in EER and the area below the ROC curve, respectively, in speaker verification. Index Terms: speaker verification, multi-classifier system, confidence measure...|$|R
40|$|Cache misses form a major {{bottleneck}} for real-time multimedia applications {{due to the}} offchip accesses to {{the main}} memory. This results in both a major access bandwidth overhead (and related power consumption) as well as performance penalties. In this paper, we propose a new technique for organizing data in the main memory for data dominated multimedia applications so as to reduce majority of the conflict cache misses. The focus {{of this paper is}} on the <b>formal</b> and <b>heuristic</b> algorithm we use to steer the data layout decisions and the experimental results obtained using a prototype tool. Experiments on real-life demonstrators illustrate that we are able to reduce up to % of the conflict misses for applications which are already aggressively transformed at source-level. At the same time, we also reduce the off-chip data accesses by up to 78 %. Thus our approach is complimentary to the more conventional way of reducing misses by reorganizing the execution order...|$|R
40|$|Proof {{automation}} is {{a common}} bottleneck for industrial adoption of <b>formal</b> meth-ods. <b>Heuristic</b> search techniques fail to discharge every proof obligation (PO), and significant effort is spent on proving the remaining ones interactively. Luckily, they usually fall into several proof families, where a single idea is required to dis-charge all similar POs. However, interactive formal proof requires expertise and is expensive: repeating the ideas over multiple proofs adds up to significant costs. The AI 4 FM research project aims to alleviate the repetitive effort by “learning” from an expert doing interactive proof. The expert’s proof attempts can give rise to reusable strategies, which capture the ideas necessary to discharge similar POs. Automatic replay of these strategies would complete the remaining proof tasks within the same family, enabling the expert to focus on novel proof ideas. This thesis presents an architecture to capture the expert’s proof ideas as a high-level proof process. Expert insight is not reflected in low-level proof scripts, therefore a generic ProofProcess framework is developed to capture high-level proof infor...|$|R
40|$|Elements of tracer {{theory in}} the steady and non-steady state are {{presented}} in unified form, in terms common to system analysis in electronic and chemical engineering. Use of tracers requires re-examination of concepts used so far. Previous treatment is shown {{to be based on}} tacit assumptions of steady flux of matter through the system, resulting in steady state treatment. Description of tracer response in steady state system requires distinction between mass response and composition response. A system mass is described in terms of bivariate distribution in transit time, t, and age, T. The non-steady state is shown to be of more common occurrence in description of tracer systems than for mass response systems. The non-steady state requires separation of the concepts of weighting function from system response, and of composition response, hc(t), from tracer response, hT(t). Differences in normalization and in linearity properties of these functions are indicated. Several <b>formal</b> and <b>heuristic</b> examples demonstrate the properties and applications of these functions. While the theory of non-steady state system is still in the developmen...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThis study was undertaken {{to demonstrate the}} feasibility of applying expert system technology to the Navy's H- 46 helicopter maintenance process. A microcomputer-based prototype known as a computer-aided diagnostic system (CADS) was developed for this purpose. Given a helicopter electrical or hydraulic system discrepancy, the troubleshooter interacts with CADS to find the cause. The prototype CADS was developed utilizing the M. 1 knowledge-based system development tool by Teknowledge, Inc. The complexity of helicopter systems diagnosis, and inadequacies of the maintenance manuals, often result in unnecessary removal of system components. The prototype CADS is intended to demonstrate that a fully developed system, containing all the <b>formal</b> and <b>heuristic</b> knowledge of H- 46 diagnostic information, could eliminate these problems. Also, such a diagnostic system could provide a comprehensive, stable diagnostic knowledge base, regardless or personnel turnover. This study includes a description of current helicopter maintenance procedures, and how the integration of CADS could improve this process. Also included are descriptions of expert systems and the M. I. knowledge-based system development tool: how they work, and their applicability to structured selection problem-solving. The development and testing strategies used for CADS are discussed in detail. Results, conclusions, and recommendations for further study are provided. [URL] Commander, United States Nav...|$|R
40|$|Motivated by AdS/CFT, the {{extension}} {{is made to}} spin-half of a scalar calculation of the conformal anomalies and functional determinants of GJMS operators. The <b>formal</b> aspects are <b>heuristic</b> but sufficient. A Barnes zeta function representation again proves effective. The determinants are calculated for the two factorisations of the gamma-function (intertwiner) form of the GJMS operator, and shown to be equal, even including the multiplicative anomaly. A comment is made on the general eigenvalue problem and a few numerical results are presented. An alternative approach is detailed for odd dimensions and it is shown that the scalar determinants are {{expressed in terms of}} the spinor ones, and vice versa. An explicit, general form is given. Comment: 21 pages 5 figures. Section on an alternative calculational technique for odd dimensions added. This version submitted for publicatio...|$|R
40|$|Abstract—The {{problem of}} {{malicious}} shellcode detection in high-speed network channels {{is a significant}} part of the more general problem of botnet propagation detection and filtering. Many of the modern botnets use remotely exploitable vulnerabilities in popular networking software for automatic propagation. We formulate the problem of shellcode detection in network flow in terms of <b>formal</b> theory of <b>heuristics</b> combination, where a set of detectors are used to recognize specific shellcode features and each of the detectors has its own characteristics of shellcode space coverage, false negative and false positive rates and computational complexity. Since the set of detectors and their quality is the key to the problem’s solution, we will provide a survey of existing shellcode detection methods, including static, dynamic, abstract execution and hybrid, giving an estimation to the quality of the characteristics for each of the methods. Keywords-shellcode; malware; polymorphism; metamor-phism; botnet detection; I...|$|R
40|$|This paper {{discusses}} {{the outcomes of}} the course on Theory of Inventive Problem Solving (TRIZ unit) delivered at the Royal Melbourne Institute of Technology (RMIT) {{over the last five}} years. It analyses the results of numerous surveys on students&# 039; problem skills and compares the impact of the TRIZ unit with the impact of a typical engineering unit on students&# 039; problem solving abilities. The results presented in this paper suggest that the enrichment approach is superior to the infusion approach for teaching engineering problem solving. The TRIZ unit was also found to enhance students&# 039; problem solving self-efficacy significantly more than the four years of an engineering degree. It is concluded, that the most likely reasons for a success of the TRIZ unit can be explained based on the information-processing theories of problem solving: it explicitly teaches tools for problem representation as well as <b>formal</b> problem solving <b>heuristic...</b>|$|R
40|$|Wireless sensor {{networks}} (WSNs) {{are increasingly}} being adopted in critical applications. In these networks undesired events may undermine the reliability level; thus their effects need to be properly assessed from {{the early stages of}} the development process onwards to minimize the chances of unexpected problems during use. In this paper we propose two heuristic strategies: what-if analysis and robustness checking. They allow to drive designers towards optimal WSN deployment solutions, from the point of view of the connection and data delivery resiliency, exploiting a formal approach based on the event calculus <b>formal</b> language. The <b>heuristics</b> are backed up by a support tool aimed to simplify their adoption by system designers. The tool allows to specify the target WSN in a user-friendly way and it is able to elaborate the two heuristic strategies by means of the event calculus specifications automatically generated. The WSN reliability is assessed computing a set of specific metrics. The effectiveness of the strategies is shown in the context of three case studies...|$|R
40|$|The {{objective}} {{of this study was}} to provide a software tool capable of monitoring and optimizing a UNIX massively parallel computer by effectively removing the human operator as much as possible. Due to the complexity of the latest Unix System V release and the number of parameters that can affect system performance, it was highly desirable to provide a software tool capable of monitoring and tuning large parallel systems at frequencies within one second. The target system was the Infinity series manufactured by Encore Computer. Some models can have up to 64 nodes or subsystems, each having at least four Motorola 88100 or 88110 processors. Users of large UNIX massively parallel computer systems lack the ability to monitor their system 2 ̆ 7 s health and performance accurately and efficiently. The consequences are most notably seen when optimizing their systems. The inability to determine which kernel parameters to tune and identify the correct troubled node or subsystem can lead to wasted efforts, time, money, and in some instances, lost contracts for the computer integrator. The goal was accomplished with the creation of software agents that interact amongst themselves and the local kernels, collecting, normalizing, enforcing <b>formal</b> and <b>heuristic</b> rules, and presenting normalized data graphically within a second. The software agents were designed for efficiency and minimization of their signature loads on the system. Additional functionality included trend and predictive analysis modules. The capability to display global views on the system console was also provided via RPC data agents...|$|R
40|$|The {{experiment}} that {{was conducted with}} the first year engineering students in Australia in 2014 has been replicated with the first year students at three universities: Brno University of Technology in Czech Republic, Lappeenranta University of Technology in Finland and Komsomolsk-on-Amur State University in Russian Federation. It was anticipated {{that the results of}} the experiments in Russian Federation, Finland and Czech Republic will closely match the results from Australia. Similarly to the Australian experiment, students from the experimental groups that were shown the names of the eight fields of MATCEMIB outperformed the students from other groups. Moreover, they generated statistically significantly more ideas when their control group counterparts. Unexpectedly, the numbers of idea generated by the students from Czech Republic, Finland and Russian Federation, who were exposed to eight random words, did not significantly exceed the number of solution proposals of the students from the control groups as it occurred in the original Australian experiment. Students from the control groups from Czech Republic and Finland generated nearly the same number of ideas than the students that were exposed to eight random words. Students from the control group in Russian Federation performed significantly better than the students from the random word group. The outcomes of the experiments conducted in Russian Federation, Finland and Czech Republic support the conclusion drawn by Belski et al. (2014) that introducing engineering students to simple ideation heuristics is likely to enhance their problem solving skills. At the same time, the discrepancy in idea generation results of groups that were shown eight random words suggest that the Random Word heuristic may not be as useful for idea generation as some more <b>formal</b> ideation <b>heuristics...</b>|$|R
40|$|A {{wide range}} of {{inconsistencies}} can arise during requirements engineering as goals and requirements are elicited from multiple stakeholders. Resolving such inconsistencies sooner or later in the process is {{a necessary condition for}} successful development of the software implementing those requirements. The paper first reviews the main types of inconsistency that can arise during requirements elaboration, defining them in an integrated framework and exploring their interrelationships. It then concentrates on the specific case of conflicting formulations of goals and requirements among different stakeholder viewpoints or within a single viewpoint. A frequent, weaker form of conflict called divergence is introduced and studied in depth. <b>Formal</b> techniques and <b>heuristics</b> are proposed for detecting conflicts and divergences from specifications of goals/ requirements and of domain properties. Various techniques are then discussed for resolving conflicts and divergences systematically by introduction of new goals or by transformation of specifications of goals/objects toward conflict-free versions. Numerous examples are given throughout the paper to illustrate the practical relevance of the concepts and techniques presented. The latter are discussed in the framework of the KAOS methodology for goal-driven requirements engineering...|$|R
