230|270|Public
25|$|An exergoeconomic {{assessment}} {{accounting for}} the total and non-renewable unit exergy costs and specific CO2 emissions of Brazilian electricity is performed by Flórez-Orrego et al. (2014), comprising thermal, nuclear, hydro, wind farms and biomass-fired power plants. The analysis starts from the fuel obtainment and continues through the different stages of construction, fuel transportation and processing, operation and decommissioning of the plant, with electricity generation as the desired output. This approach allows the calculation of direct CO2 emissions {{as well as the}} upstream and downstream emissions, which {{play an important role in}} some technologies. In this way, a better comparison between the utilization of different fuels in the electricity generation can be achieved. An <b>iterative</b> <b>calculation</b> procedure is used to determine the unit exergy costs of electricity and processed fuels, since both electricity and processed fuel are used in their own production routes.|$|E
2500|$|In 1970, Saul B. Needleman and Christian D. Wunsch {{proposed}} a heuristic homology algorithm for sequence alignment, {{also referred to}} as the Needleman–Wunsch algorithm. It is a global alignment algorithm that requires [...] calculation steps ( [...] and [...] are the lengths of the two sequences being aligned). It uses the <b>iterative</b> <b>calculation</b> of a matrix for the purpose of showing global alignment. In the following decade, Sankoff, Reichert, Beyer and others formulated alternative heuristic algorithms for analyzing gene sequences. Sellers introduced a system for measuring sequence distances. In 1976, Waterman et al. added the concept of gaps into the original measurement system. In 1981, Smith and Waterman published their Smith–Waterman algorithm for calculating local alignment.|$|E
5000|$|For <b>iterative</b> <b>calculation</b> of {{the above}} series, the {{following}} alternative formulation may be useful: ...|$|E
5000|$|... {{represents}} the thermal conductivity ("k-value") at the average insulation temperature (for accurate results <b>iterative</b> <b>calculations</b> are necessary), and ...|$|R
40|$|A new {{earthquake}} design method performing <b>iterative</b> <b>calculations</b> with {{secant stiffness}} was developed. Since basically the proposed design method uses linear analysis, it is convenient and stable in numerical analysis. At the same time, the proposed design method can accurately estimate the inelastic {{strength and ductility}} demands of the structural members through <b>iterative</b> <b>calculations.</b> In the present study, the procedure of the proposed design method was established, and a computer program incorporating the proposed method was developed. Design examples using the proposed method were presented to verify its advantages. The proposed method, as an integrated analysis and design method, can directly address the earthquake design strategy intended by the engineer, such as limited ductility of member {{and the concept of}} strong column- weak beam. Through <b>iterative</b> <b>calculations</b> on a structural model with member sizes preliminarily assumed, the strength and ductility demands of each member can be determined so as to satisfy the given design strategy. As the result, structural safety and economical design can be achieved...|$|R
40|$|NMR {{spectra of}} {{molecules}} oriented in thermotropic liquid crystalline media {{provide information on}} the molecular structure and order. The spins are generally strongly dipolar coupled and the spectral analyse require the tedious and time consuming numerical <b>iterative</b> <b>calculations.</b> The present study demonstrates the application of multiple quantum spin state selective detection of single quantum transitions for mimicking the homonuclear decoupling and the direct estimation of an element of ordering matrix. This information is utilized to estimate the nearly accurate starting dipolar couplings for <b>iterative</b> <b>calculations.</b> The studies on the spectra of strongly dipolar coupled five and six interacting spin systems are reported...|$|R
50|$|The inverse Sakuma-Hattori {{function}} {{can be used}} without <b>iterative</b> <b>calculation.</b> This is {{an addition}} advantage over integration of Planck's Law.|$|E
50|$|Extended Erlang B is an <b>iterative</b> <b>calculation,</b> {{rather than}} a formula, that adds an extra parameter, the recall factor, which defines the recall attempts.|$|E
5000|$|The <b>Iterative</b> <b>Calculation</b> of a Few of the Lowest Eigenvalues and Corresponding Eigenvectors of Large Real-Symmetric Matrices, Ernest R. Davidson; Journal of Computational Physics 17, 87-94 (1975) ...|$|E
40|$|Techniques for {{deforming}} polygonal meshes {{are demonstrated}} by using two-dimensional lattices of control points or functions for pasting features. The deformations use a shape-preserving parametrization that embeds the mesh's vertices in a normalized two-dimensional space while preserving shape consistency for non-flat surfaces. A discrete smoothing {{used for the}} parametrization has inefficient <b>iterative</b> <b>calculations,</b> which is unsuitable for manipulations of dense meshes, and an initial approximation for the smoothing is therefore proposed {{in order to reduce}} the number of iterations. The approximation uses a graph-searching algorithm and a discrete normalization whose computational costs are negligible in comparison with that of the <b>iterative</b> <b>calculations...</b>|$|R
3000|$|... (Setting {{the goal}} constraint) First, the goal {{time of the}} {{operation}} strategy for all stops is calculated; this serves as the maximum for the goal constraint. <b>Iterative</b> <b>calculations</b> num = num + 1 then begin.|$|R
40|$|Trigonometric {{functions}} are often needed in embedded real-time software. To fulfill concrete resource demands, different implementation strategies of trigonometric {{functions are}} possible. In this paper we analyze the resource demands of <b>iterative</b> <b>calculations</b> {{compared to other}} implementation strategies, using the trigonometric functions as a case study. By analyzing the worst-case execution time (WCET) of the different calculation techniques of trigonometric functions we got the surprising result that the WCET of <b>iterative</b> <b>calculations</b> is quite competitive to alternative calculation techniques, while their economics on memory demand is far superior. Finally, {{a discussion of the}} general applicability of the obtained results is given as a design guide for embedded software. ...|$|R
50|$|A {{distinction}} {{should be}} made with processes containing a circular reference between those that are incomputable {{and those that are}} an <b>iterative</b> <b>calculation</b> with a final output. The latter may fail in spreadsheets not equipped to handle them but are nevertheless still logically valid.|$|E
50|$|A corner {{transfer}} matrix A2m (defined for an m×m quadrant) may {{be expressed in}} terms of smaller corner transfer matrices A2m-1 and A2m-2 (defined for reduced (m-1)×(m-1) and (m-2)×(m-2) quadrants respectively). This recursion relation allows, in principle, the <b>iterative</b> <b>calculation</b> of the corner {{transfer matrix}} for any lattice quadrant of finite size.|$|E
50|$|This {{special case}} is how {{expected}} value of perfect information and expected value of sample information are calculated where risk neutrality is implicitly assumed. For cases where decision-maker is risk averse or risk seeking, this simple calculation does not necessary yield correct result, and <b>iterative</b> <b>calculation</b> {{is the only}} way to ensure correctness.|$|E
3000|$|... [...]) of the {{prestressing}} strands {{exceeds the}} limitation specified in design codes for serviceability {{check of the}} Class C PSC members, the proposed method do not require the cracked section analysis that involves very complex and time-consuming <b>iterative</b> <b>calculations.</b>|$|R
3000|$|... in Eq. (11), {{by which}} more {{economical}} designs can be achieved. As mentioned above, however, the cracked section analysis, which requires complex <b>iterative</b> <b>calculations</b> (Lee and Kim 2011; ACI Committee 318 2014), {{need to be}} conducted to estimate Δf [...]...|$|R
50|$|An {{improvement}} over VisiCalc (though using {{much the same}} command structure using the slash key), SuperCalc {{was one of the}} first spreadsheet programs capable of iteratively solving circular references (cells that depend on each other's results). It would be over 10 years after the introduction of SuperCalc before this feature was implemented in Microsoft Excel, although in Lotus 1-2-3, manual programming of iterative logic could also be used to solve this issue. According to the SuperCalc product manager, <b>iterative</b> <b>calculations</b> were added when Sorcim changed from binary-coded decimal to binary math. Since the precision of the two math packages was different, some IF statements resolved differently, and <b>iterative</b> <b>calculations</b> helped solve this problem.|$|R
50|$|In 1979, Johann Gasfeiger and Mario Marsili {{published}} {{a method for}} the <b>iterative</b> <b>calculation</b> of atomic partial charges in molecules. This work is his most-cited publication. Between 1987 and 1991 Johann Gasteiger was a project manager {{for the development of}} the ChemInform RX database.Since 1985, the 3D structure generator CORINA is developed in his group.|$|E
50|$|The {{starting}} point for the transient is some steady state point (e.g. Ground Idle, Sea Level Static, ISA). A ramp of fuel flow versus time is, for instance, fed into the model to simulate, say, a slam acceleration (or deceleration). The transient calculation is first undertaken for time zero, with the steady state fuel flow as the engine match, which should result in zero excess turbine power. By definition, the first transient calculation should reproduce the datum steady state point. The fuel flow for '''''' is calculated from the fuel flow ramp and is used as the revised engine match in the next transient <b>iterative</b> <b>calculation.</b> This process is repeated until the transient simulation is completed.|$|E
5000|$|In 1970, Saul B. Needleman and Christian D. Wunsch {{proposed}} a heuristic homology algorithm for sequence alignment, {{also referred to}} as the Needleman-Wunsch algorithm. It is a global alignment algorithm that requires [...] calculation steps ( [...] and [...] are the lengths of the two sequences being aligned). It uses the <b>iterative</b> <b>calculation</b> of a matrix for the purpose of showing global alignment. In the following decade, Sankoff, Reichert, Beyer and others formulated alternative heuristic algorithms for analyzing gene sequences. Sellers introduced a system for measuring sequence distances. In 1976, Waterman et al. added the concept of gaps into the original measurement system. In 1981, Smith and Waterman published their Smith-Waterman algorithm for calculating local alignment.|$|E
40|$|Graphic {{method is}} used to select {{allowable}} stresses in thermally loaded structures. Equations are used for determining the mode of failure for specific materials in order to plot a range of stress curves. Linear assumption and <b>iterative</b> <b>calculations</b> are eliminated resulting in comparatively high accuracy...|$|R
40|$|This {{document}} published {{results of}} <b>iterative</b> <b>calculations</b> for maximum tank farm transfer secondary pipe (encasement) pressure upon {{failure of the}} primary pipe. The maximum pressure was calculated from a primary pipe guillotine break. Results show encasement pipeline design or testing pressures can be significantly lower than primary pipe pressure criteria...|$|R
40|$|Includes bibliographical {{references}} (leaf 24). "This {{work was}} supported in part by a Summer Grant from the Investors in Business Education. "Three new <b>iterative</b> <b>calculations</b> which converge to the ratio matrix of the discrete generalized geometric distribution are presented for queuing systems having this limiting distribution. Two criteria for convergence are provided...|$|R
5000|$|The {{goal of the}} {{analysis}} algorithm is to extract the pure decay curve from the measured decay and to estimate the lifetime(s). The latter is usually accomplished by fitting single or multi exponential functions. A variety of methods {{have been developed to}} solve this problem. The most widely used technique is the least square iterative re-convolution which is based on the minimization of the weighted sum of the residuals. In this technique theoretical exponential decay curves are convoluted with the instrument response function, which is measured separately, and the best fit is found by <b>iterative</b> <b>calculation</b> of the residuals for different inputs until a minimum is found. For a set of observations [...] of the fluorescence signal in time bin i, the lifetime estimation is carried out by minimization of: ...|$|E
50|$|An exergoeconomic {{assessment}} {{accounting for}} the total and non-renewable unit exergy costs and specific CO2 emissions of Brazilian electricity is performed by Flórez-Orrego et al. (2014), comprising thermal, nuclear, hydro, wind farms and biomass-fired power plants. The analysis starts from the fuel obtainment and continues through the different stages of construction, fuel transportation and processing, operation and decommissioning of the plant, with electricity generation as the desired output. This approach allows the calculation of direct CO2 emissions {{as well as the}} upstream and downstream emissions, which {{play an important role in}} some technologies. In this way, a better comparison between the utilization of different fuels in the electricity generation can be achieved. An <b>iterative</b> <b>calculation</b> procedure is used to determine the unit exergy costs of electricity and processed fuels, since both electricity and processed fuel are used in their own production routes.|$|E
30|$|In the {{progress}} of <b>iterative</b> <b>calculation,</b> {{the value of the}} initial pressure and saturation is given in the equations.|$|E
30|$|As {{shown in}} the output result for Fig.  6, when using the {{multilayer}} perception kernel function, the system only needs 35 <b>iterative</b> <b>calculations</b> to obtain the file tr 1. model, composed of 62 support vectors; using the model file to classify 63 test samples, the classification accuracy was relatively low, only about 60.3175  %.|$|R
30|$|Figure  5 {{shows that}} using the RBF radial basis as kernel {{function}} {{to obtain the}} SVM model results in 48 <b>iterative</b> <b>calculations,</b> made up of 62 support vectors; the accuracy of using the file tr 1.model to classify the 63 test samples was higher than the polynomial kernel function, with the classification accuracy of about 87.3016  %.|$|R
40|$|Introduction Difficulty in {{determination}} of source brain activities for observed biomagnetic signals (magnetoencephalograms, MEGs) {{is known as}} the biomagnetic inverse problem. Moving Dipole Method [1] has been developed for solution of this problem. Two difficulties have been pointed out for this approach. The first problem is that the number of sources must be known before the analysis. However, it is difficult to fix the number of sources for the brain activities which change from time to time. The second problem arises for the case of a relatively large source. The estimated location of the source tended to deviate from that of the real source [2]. We have developed a new algorithm, called Moving Mesh Method (MMM) [3][4] in order to overcome these problems. One of the difficulties in MMM was that it required a large number of <b>iterative</b> <b>calculations.</b> It was also difficult to find good initial conditions for <b>iterative</b> <b>calculations.</b> The first problem was solved by introducing the m...|$|R
30|$|Two of {{the most}} {{troublesome}} and imperative issues identified with inclusions are the foundation of generalized inclusions and the improvement of an <b>iterative</b> <b>calculation.</b> In this article, two systems of variational inclusions were presented and contemplated, which is a broader aim than the numerous current systems of generalized ordered variational inclusions in the literature. An <b>iterative</b> <b>calculation</b> is performed with a weak ARD mapping to an inexact solution of our systems, and the convergence criterion is likewise addressed.|$|E
30|$|So the <b>iterative</b> <b>calculation</b> of (25), (28), and (29) {{can achieve}} {{the purpose of}} fast {{solution}} of the minimization of (14) and the segmentation of ROI.|$|E
40|$|Abstract. On {{the basis}} of the pipe network {{hydraulic}} calculation of basic theory, the pipe network utilization, using graph theory and peak array build relationships and storage node and pipe sections between the pipeline node associated with the hydraulic parameters of pipe sections linked together establish a common heating pipe network hydraulic calculation models. Matrix for the model were derived solving ideas based on the finite element method and linearization method for existing node equation method to improve to get a new heating network hydraulic calculation methods common to construct a new <b>iterative</b> <b>calculation</b> equation, to improve the convergence of <b>iterative</b> <b>calculation...</b>|$|E
30|$|Capacity {{optimization}} of a HESS is non-linear {{with a large}} computation requirement. Also, {{there exist}} only 2 decision variables that are the capacities of 2 ESS, which means, a multi-algorithm {{is not required to}} improve the astringency. Under these circumstances, the particle swarm optimization algorithm (PSO) is applied in this paper, because it can converge quickly, perform accurate <b>iterative</b> <b>calculations</b> and can be implemented easily.|$|R
40|$|A {{simplified}} {{heat transfer}} model of above and underground insulated piping systems {{was developed to}} perform <b>iterative</b> <b>calculations</b> for fluid temperatures along the entire pipe length. It is applicable to gas, liquid, fluid flow with no phase change. Spreadsheet computer programs of the model have been developed and used extensively to perform the above calculations for thermal resistance, heat loss and core fluid temperature...|$|R
40|$|If {{a pattern}} is {{projected}} onto a concave screen, the desired view cannot be correctly observed {{due to the}} influence of inter-reflections. This paper proposes a simple but effective technique for photometric compensation in consideration of inter-reflections. The compensation is accomplished by canceling inter-reflections estimated by the radiosity method. The significant advantage of our method is that <b>iterative</b> <b>calculations</b> are not necessary because it analytically solves the inverse problem of inter-reflections...|$|R
