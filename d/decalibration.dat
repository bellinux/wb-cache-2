34|2|Public
40|$|Voyager 1 and 2 were {{launched}} from Cape Kennedy to Jupiter, Saturn, and beyond on September 5, 1977 and August 20, 1977. The {{role of the}} Image Processing Laboratory is to provide the Voyager Imaging Team with the necessary support to identify atmospheric features (tiepoints) for Jupiter and Saturn data, and to analyze and display them in a suitable form. This support includes the software needed to acquire and store tiepoints, the hardware needed to interactively display images and tiepoints, and the general image processing environment necessary for <b>decalibration</b> and enhancement of the input images. The objective is an understanding of global circulation in the atmospheres of Jupiter and Saturn. Attention {{is given to the}} Voyager imaging subsystem, the Voyager imaging science objectives, hardware, software, display monitors, a dynamic feature study, <b>decalibration,</b> navigation, and data base...|$|E
40|$|Accidental {{damage and}} <b>decalibration</b> prevented. Linearity, sensitivity, and {{frequency}} response of microphone measured with commercial sound generator modified so placed directly over or under microphone. Artificial voice connected to oscillator and measuring amplifier. Requisite frequencies and sound pressures applied to microphone, and microphone output recorded. New calibration method used on aircraft and wind-tunnel microphones...|$|E
40|$|Techniques {{for using}} image {{processing}} in astronomy are identified and {{developed for the}} following: (1) geometric and radiometric <b>decalibration</b> of vidicon-acquired spectra, (2) automatic identification and segregation of stars from galaxies; and (3) display of multiband radio maps in compact and meaningful formats. Examples are presented of these techniques applied {{to a variety of}} objects...|$|E
40|$|In this paper, {{we present}} a {{procedure}} to predict bow pressing force in a violin from data acquired in real recordings. We focus on the calibration procedure that must be robust to the bow tension changes in long recordings and fast enough to not disturb the recording session. Because of this last limitation, the calibration method here proposed does not exhaustively cover all the possible bow conditions that potentially may appear in the recording. We propose the use of Support Vector Regression to predict all these missing scenarios and compute the predicted force. On the other hand, bow tension variations in long recordings produce <b>decalibrations</b> on the acquisition system. After analyzing their behavior, we propose a solution to compensate this effect based on post processing and a specific behavior of the performer {{at the beginning and}} the end of each phrase. 1...|$|R
40|$|Comparison of the Viking Orbiter 2 Approach mosaic taken 11 Mars {{months later}} {{provides}} qualitative {{information on the}} photometric properties of the martian albedo features, {{and the distribution of}} dust and sand deposits responsible for the atmosphere near the northern summer solstice. The approach mosaic was taken at L (sub s) 106 degrees (early N. summer), phase angle 106 degrees; and airmasses varying from 4. 6 at 30 degrees N to 3. 3 near 10 degrees S. The apoapsis mosaic was taken in four sequences between L (sub s) 72 degrees and 76 degrees (late N. spring), near phase angles of 47 degrees, and at airmasses near 2. 5. Systematic differences in the photometric <b>decalibrations</b> used to generate these mosaics may induce multiplicative errors of 5 - 10 percent of the observed albedos in comparisons of the mosaics, but they are probably nearer 3 percent of the albedos. In the study area (30 degrees N to 20 degrees S, 57 degrees E to 75 degrees W), scene-average approach Minnaert albedos were about 10 percent greater than apoapsis albedos and slightly less 'red'. The preferred explanation for the observed approach-apoapsis albedo difference is that both Arabia and Meridiani materials are smoother on millimeter and larger scales than other units in the study area. This is in good agreement with preliminary conclusions of Thorpe and (for dark intracrater Meridiani splotches) Regner et al. This is also consistent with reasonable models of these surfaces. 'Dark Blue' Meridiani surfaces are interpreted as consisting of sand dunes and sand sheets, which would be expected to have macroscopically smooth, nonshadowing surfaces. Viking Lander images of the surfaces at both landing sites show that smooth drift area's brightnesses are close to those of adjacent rough soil areas at low phase angles, but drifts become much brighter than rough soils when looking up-sun at high phase angles. Smooth patches of duricrust at both landing sites, interpreted by Strickland as eolian deposits (regardless of how they became salt enriched and cemented), also show this behavior...|$|R
40|$|The goal of {{the project}} is to enable plant {{operators}} to detect with high sensitivity and reliability the onset of <b>decalibration</b> drifts {{in all of the}} instrumentation used as input to the reactor heat balance calculations. To achieve this objective, the collaborators developed and implemented at DBNPS an extension of the Multivariate State Estimation Technique (MSET) pattern recognition methodology pioneered by ANAL. The extension was implemented during {{the second phase of the}} project and fully achieved the project goal...|$|E
40|$|Four {{applications}} of image processing to astronomy, automated location {{and analysis of}} star and galaxy images, geometric and radiometric <b>decalibration</b> of vidicon spectra, display of multiband radio images, and generation of high resolution polarization direction and magnitude maps from images are presented with illustrative examples. The technique by which a digital image can be analyzed automatically to locate and segregate between stars and galaxies and the steps performed by the classifier to determine the nature of each object are outlined. The classification program executed on a 48 inch Schmidt plate of the cluster of galaxies 655 is described. The calibration and <b>decalibration</b> steps to remove geometric and radiometric distortions from a silicon vidicon camera digital spectra are discussed. Three methods of displaying multispectral radio data, generating a mosaic of each image, producing a color coded image to depict radio velocity, and producing a stereo pair with radial velocity as depth are described. The generation of polarization information from images obtained through linear polarizing filters is illustrated, and it is concluded that in each case information was displayed using digital techniques which could not readily have been provided visually...|$|E
40|$|The {{applicability}} of wing deformation measurements techniques based on stereoscopic photogrammetry {{has already been}} demonstrated for in-flight applications. The position and alignment of a stereoscopic camera setup is described in the external parameters of the camera calibration by a rotation matrix and a translation vector. Especially at dynamic flight manoeuvres varying multidirectional loads and the vibration level can affect the camera installation and induce slight changes in the camera position and alignment. Those camera movements induce a <b>decalibration</b> of the measurement system and affect {{the accuracy of the}} results directly. The paper describes criteria to assess the <b>decalibration</b> level of a stereo camera system. An approach for a recalibration and its limits are demonstrated for a dynamic wing deformation measurement performed on an Evektor Cobra VUT 100 airplane by means of the Image Pattern Correlation Technique (IPCT). IPCT combines the principles of stereoscopic photogrammetry with image correlation methods to determine corresponding areas in a stereo image pair and delivers the 3 D wing shape as result. The measurement was part of the project ‘Advanced In-Flight Measurement Techniques 2 ’ (AIM²) funded by the 7 th Framework Programme for Research of the EU...|$|E
40|$|Abstract—We {{consider}} {{the problem of}} calibrating a compressed sensing measurement system {{under the assumption that}} the <b>decalibration</b> consists of unknown complex gains on each measure. We focus on blind calibration, using measures performed on a few unknown (but sparse) signals. In the considered context, we study several sub-problems and show that they can be formulated as convex optimization problems, which can be solved easily using off-the-shelf algorithms. Numerical simulations demonstrate the effectiveness of the approach even for highly uncalibrated measures, when a sufficient number of (unknown, but sparse) calibrating signals is provided. I...|$|E
40|$|This paper {{discusses}} {{new digital}} processing techniques {{as applied to}} the Voyager Imaging Subsystem and devised to explore atmospheric dynamics, spectral variations, and the morphology of Jupiter, Saturn and their satellites. Radiometric and geometric <b>decalibration</b> processes, the modulation transfer function, and processes to determine and remove photometric properties of the atmosphere and surface of Jupiter and its satellites are examined. It is exhibited that selected images can be processed into 'approach at constant longitude' time lapse movies which are useful in observing atmospheric changes of Jupiter. Photographs are included to illustrate various image processing techniques...|$|E
40|$|The {{digital image}} {{processing}} performed by the Image Processing Laboratory (IPL) at JPL {{in support of the}} Mariner 9 mission is summarized. The support is divided into the general categories of image <b>decalibration</b> (the removal of photometric and geometric distortions from returned imagery), computer cartographic projections in support of mapping activities, and adaptive experimenter support (flexible support to provide qualitative digital enhancements and quantitative data reduction of returned imagery). Among the tasks performed were the production of maximum discriminability versions of several hundred frames to support generation of a geodetic control net for Mars, and special enhancements supporting analysis of Phobos and Deimos images...|$|E
40|$|Improved {{temperature}} measurement in the melting and delivery {{systems of the}} glass making process will aid in energy conservation. The ``Needs Analysis`` survey found the greatest problem was the inability to identify in situ <b>decalibration</b> (drift). Phase I objectives are: a more rugged reliable sensor; high quality inner protective sheath; improved data transmission hardened to the melt tank environs; a system that reduces or eliminates drift; and an improved outer protection sheath. Results show that 4 of the 5 problem areas have been resolved; {{with the help of}} the Univ. of Missouri-Rolla`s materials group, the fifth may be solvable. The major identified problem, the inability to identify in-situ drift has been solved...|$|E
40|$|Planetary imaging from {{unmanned}} spacecraft, {{almost exclusively}} done by digital systems, is examined. The Mars Mariner 9 television camera, representative of such systems, is considered. Each image consists of 700 lines, each containing 832 picture elements, or pixels. Each pixel contains nine binary {{bits of information}} capable of displaying 512 discrete brightness levels. Several problems inherent in television systems are discussed. These include nonuniform target response, residual images, noise, and blemishes. These defects can be removed to some extent by <b>decalibration</b> of the image. The final product is geometrically corrected for camera distortion and photometrically corrected. Several versions of the decalibrated images are available. The most generally useful are the geometrically corrected images with enhanced contrast. The Mariner 10 imaging of Mercury is briefly discussed...|$|E
40|$|This paper {{describes}} the digital processing {{performed on the}} images of Mercury returned to earth from Mariner 10. Each image contains considerably more information than can be displayed in a single picture. Several specialized processing techniques and procedures are utilized to display the particular information desired for specific scientific analyses: radiometric <b>decalibration</b> for photometric investigations, high-pass filtering to characterize morphology, modulation transfer function restoration to provide the highest possible resolution, scene-dependent filtering of the terminator images to provide maximum feature discriminability in the regions of low illumination, and rectification to cartographic projections to provide known geometric relationships between features. A principal task was the construction of full disk mosaics {{as an aid to}} the understanding of surface structure on a global scale...|$|E
40|$|The {{purpose of}} this paper is to {{describe}} the system for the display, processing, and production of image data products created to support the Mariner 9 Television Experiment. Of necessity, the system was large in order to respond to the needs of a large team of scientists with a broad scope of experimental objectives. The desire to generate processed data products as rapidly as possible to take advantage of adaptive planning during the mission, coupled with the complexities introduced by the nature of the vidicon camera, greatly increased the scale of the ground image processing effort. This paper describes the systems that carried out the processes and delivered the products necessary for real-time and near-real-time analyses. References are made to the computer algorithms used for the different levels of <b>decalibration</b> and analysis...|$|E
40|$|International audienceWe {{consider}} {{the problem of}} calibrating a compressed sensing measurement system {{under the assumption that}} the <b>decalibration</b> consists in unknown gains on each measure. We focus on blind calibration, using measures performed on a few unknown (but sparse) signals. A naive formulation of this blind calibration problem, using l 1 minimization, is reminiscent of blind source separation and dictionary learning, which are known to be highly non-convex and riddled with local minima. In the considered context, we show that in fact this formulation can be exactly expressed as a convex optimization problem, and can be solved using off-the-shelf algorithms. Numerical simulations demonstrate the effectiveness of the approach even for highly uncalibrated measures, when a sufficient number of (unknown, but sparse) calibrating signals is provided. We observe that the success/failure of the approach seems to obey sharp phase transitions...|$|E
40|$|Experience with Type C {{thermocouples}} operating {{for long}} periods in the 1400 to 1600 °C temperature range indicate that significant <b>decalibration</b> occurs, often leading to expensive downtime and material waste. As {{part of an effort}} to understand the mechanisms causing drift in these thermocouples, the Idaho National Laboratory conducted a long duration test at 1500 °C containing eight Type C thermocouples. As report in this document, results from this long duration test were adversely affected due to oxygen ingress. Nevertheless, results provide key insights about the impact of precipitate formation on thermoelectric response. Post-test examinations indicate that thermocouple signal was not adversely impacted by the precipitates detected after 1, 000 hours of heating at 1, 500 °C and suggest that the signal would not have been adversely impacted by these precipitates for longer durations (if oxygen ingress had not occurred in this test) ...|$|E
40|$|We {{consider}} {{the problem of}} calibrating a compressed sensing mea-surement system {{under the assumption that}} the <b>decalibration</b> consists in unknown gains on each measure. We focus on blind calibration, us-ing measures performed on a few unknown (but sparse) signals. A naive formulation of this blind calibration problem, using ` 1 minimization, is reminiscent of blind source separation and dictionary learning, which are known to be highly non-convex and riddled with local minima. In the considered context, we show that in fact this formulation can be exactly expressed as a convex optimization problem, and can be solved using off-the-shelf algorithms. Numerical simulations demonstrate the effectiveness of the approach even for highly uncalibrated measures, when a sufficient number of (unknown, but sparse) calibrating signals is provided. We ob-serve that the success/failure of the approach seems to obey sharp phase transitions. ...|$|E
30|$|In {{spite of}} these limits, our {{findings}} are valuable for several reasons. First, a large sample (> 23, 000) of hospital stays of ICU has been analyzed, including more than 3200 stays involving patients aged ≥ 80  years. The administrative database {{used in the study}} is regularly controlled to avoid over-coding and missing data. Hospitals are financially encouraged to code for the procedures reflecting the treatment intensity. Data came from a high number of medical ICUs. Patients treated with different medical strategies had been included, offering the opportunity to analyze the relationships between age, treatment intensity and mortality. The large number of ICUs allowed us to collect data about a huge number of patients treated during a 3 -year period. The shortness of the period of data collection increased the homogeneity of the sample and limited the bias related to the <b>decalibration</b> of SAPS 2 [21].|$|E
40|$|Abstract — In {{vehicular}} applications {{based on}} motion-stereo using monocular side-looking cameras, pairs of images must usually be rectified very well, {{to allow the}} application of dense stereo methods. But also long-term installations of stereo rigs in vehicles require approaches that cope with the <b>decalibration</b> of the cameras. The need for such methods is further underlined {{by the fact that}} offline camera calibration is a costly and timeconsuming procedure at vehicle production sites. In this paper we propose an approach for dense stereo matching that overcomes issues arising from inaccurately rectified images. For this, we significantly increase the search range for correspondences, but still preserve a high efficiency of the method to allow operation on platforms with highly limited processing resources. We demonstrate the performance of our ideas quantitatively using well known stereo datasets and qualitatively using real video sequences of a motion-stereo application. I...|$|E
40|$|The Mariner 9 {{television}} experiment {{used two}} cameras to photograph Mars from an orbiting spacecraft. For quantitative {{analysis of the}} image data transmitted to earth, the pictures were processed by digital computer to remove camera-induced distortions. The removal process was performed by the JPL Image Processing Laboratory (IPL) using calibration data measured during prelaunch testing of the cameras. The Reduced Data Record (RDR) is the set of data which results from the distortion-removal, or <b>decalibration,</b> process. The principal elements of the RDR are numerical data on magnetic tape and photographic data. Numerical data {{are the result of}} correcting for geometric and photometric distortions and residual-image effects. Photographic data are reproduced on negative and positive transparency films, strip contact and enlargement prints, and microfiche positive transparency film. The photographic data consist of two versions of each TV frame created by applying two special enhancement processes to the numerical data...|$|E
40|$|The {{ubiquity of}} {{approximately}} sparse data has led {{a variety of}} com- munities to great interest in compressed sensing algorithms. Although these are very successful and well understood for linear measurements with additive noise, applying them on real data can be problematic if imperfect sensing devices introduce deviations from this ideal signal ac- quisition process, caused by sensor <b>decalibration</b> or failure. We propose a message passing algorithm called calibration approximate message passing (Cal-AMP) that can treat a variety of such sensor-induced imperfections. In addition to deriving the general form of the algorithm, we numerically investigate two particular settings. In the first, {{a fraction of the}} sensors is faulty, giving readings unrelated to the signal. In the second, sensors are decalibrated and each one introduces a different multiplicative gain to the measures. Cal-AMP shares the scalability of approximate message passing, allowing to treat big sized instances of these problems, and ex- perimentally exhibits a phase transition between domains of success and failure. Comment: 27 pages, 9 figure...|$|E
40|$|Stereo {{matching}} commonly requires rectified {{images that}} are computed from calibrated cameras. Since all under-lying parametric camera models are only approximations, calibration and rectification will never be perfect. Addi-tionally, {{it is very hard}} to keep the calibration perfectly sta-ble in application scenarios with large temperature changes and vibrations. We show that even small calibration er-rors of a quarter of a pixel are severely amplified on cer-tain structures. We discuss a robotics and a driver assis-tance example where sub-pixel calibration errors cause se-vere problems. We propose a filter solution based on signal theory that removes critical structures and makes stereo al-gorithms less sensitive to calibration errors. Our approach does not aim to correct <b>decalibration,</b> but rather to avoid amplifications and mismatches. Experiments on ten stereo pairs with ground truth and simulated decalibrations as well as images from robotics and driver assistance scenar-ios demonstrate the success and limitations of our solution that can be combined with any stereo method. 1...|$|E
40|$|Results of {{investigation}} of probable atmospheric effects appearing in Mariner ' 69 TV pictures that have undergone noise removal and preliminary <b>decalibration</b> are described. Two distinct types of haze are distinguished: north polar haze, seen prominently against {{the face of}} the planet in blue photographs, and thin haze, usually identified by its appearance on the limb and not strongly colored. Thin haze is surprisingly widespread, particularly in the southern hemisphere. Discrete bright features, which may be evidence for condensation on the ground or in the atmosphere, are described. These occur where bright features have often been seen from earth, in a region where very large multiple-ringed structures seem to dominate the surface morphology. The speculation that these may be evidence for local water-vapor exchange between ground and atmosphere is raised, and some constraints on local subsurface water-vapor sources in the Mars tropics are described. Finally, some implications of the Mariner ' 69 results for atmospheric exploration by Mariner ' 71 are briefly discussed...|$|E
40|$|The {{purpose of}} this paper is twofold. The rst purpose is to present {{important}} issues in designing fault tolerant systems for autonomous robots. The second, is to present the fault tolerance capabilities we implemented on our autonomous robot. Our approach ischaracterized by a distributed network of concurrently running processes. To tolerate hardware failures, a set of fault tolerance processes are written for each component. These processes are responsible for detecting faults in their respective component, and minimizing the impact of the failure on the robot's performance. By exploiting concurrency and distributedness, the system monitors, detects, and compen-sates for component failures silmultaneously. The capabilities of this system have been tested by physically disabling and enabling the robot's sensors and actuators. The sys-tem quickly recognizes and compensates for both minor and severe sensor and actuator failures. It tolerates a variety of sensor failures such as <b>decalibration,</b> erroneous read-ings, and permanent failures. It also tolerates various combinations of failures such as individual failures, concurrent failures, and accumulative failures. We hope this work will inspire further research in fault tolerant autonomy...|$|E
40|$|A <b>decalibration</b> of a {{stereoscopic}} camera system caused by slight {{movements of the}} cameras can influence {{the accuracy of the}} measured 3 D positions significantly. Especially for large scale in-flight applications this is difficult to avoid, e. g. due to the high loads and the vibration level occurring during dynamic flight manoeuvres. Thus a practicable approach for a correction of the results by a recalibration of the camera system is necessary. The Image pattern correlation technique (IPCT) delivers large area surface results which enables the assessment of its triangulation error in detail as a measure for the quality of the results. The objective of the presented recalibration is a minimisation of the overall triangulation error by a correction of the external camera parameters. The criteria to assess the reliability of the 3 D-surface results and the deformation results derived from are described as well as the limitations of the method. A wing deformation measurement on a VUT 100 Cobra Aeroplane by means of stereoscopic IPCT was used as a test case to demonstrate the applicability of the recalibration method on real flight test data...|$|E
40|$|Abstract—Wireless Sensor Networks {{are well}} suited for track-ing targets {{carrying}} RFID tags in indoor environments. Tracking based on the received signal strength indication (RSSI) {{is by far the}} cheapest and simplest option, but suffers from secular biases due to effects of multi-path, occlusions and <b>decalibration,</b> as well as large unbiased errors due to measurement noise. We propose a novel algorithm that solves these problems in a distributed, scalable and power-efficient manner. Firstly, our proposal includes a tandem incremental estimator that learns and tracks the radio environment of the network, and provides this knowledge {{for the use of the}} tracking algorithm, which eliminates the secular biases due to radio occlusions etc. Secondly, we reduce the unbiased tracking error by exploiting the co-dependencies in the motion of several targets (as in crowds or herds) via a fully distributed and tractable particle filter. We thereby extract a significant “diversity gain ” while still allowing the network to scale seamlessly to a large tracking area. In particular, we avoid the pitfalls of network congestion and severely shortened battery lifetimes that plague procedures based on the joint multi-target probability density...|$|E
40|$|In this paper, {{we present}} RegNet, the first deep {{convolutional}} neural network (CNN) to infer a 6 degrees of freedom (DOF) extrinsic calibration between multimodal sensors, exemplified using a scanning LiDAR and a monocular camera. Compared to existing approaches, RegNet casts all three conventional calibration steps (feature extraction, feature matching and global regression) into a single real-time capable CNN. Our method does not require any human interaction and bridges the gap between classical offline and target-less online calibration approaches as it provides both a stable initial estimation as well as a continuous online correction of the extrinsic parameters. During training we randomly decalibrate our system in order to train RegNet to infer the correspondence between projected depth measurements and RGB image and finally regress the extrinsic calibration. Additionally, with an iterative execution of multiple CNNs, that are trained on different magnitudes of <b>decalibration,</b> our approach compares favorably to state-of-the-art methods in terms of a mean calibration error of 0. 28 degrees for the rotational and 6 cm for the translation components even for large decalibrations up to 1. 5 m and 20 degrees. Comment: published in IEEE Intelligent Vehicles Symposium, 201...|$|E
40|$|A precise GNSS (Global Navigation Satellite System) {{localization}} {{is vital}} for autonomous road vehicles, especially in cluttered or urban environments where satellites are occluded, preventing accurate positioning. We propose to fuse GPS (Global Positioning System) data with fisheye stereovision to face this problem independently to additional data, possibly outdated, unavailable, and needing correlation with reality. Our stereoscope is sky-facing with 360 ° × 180 ° fisheye cameras to observe surrounding obstacles. We propose a 3 D modelling and plane extraction through following steps: stereoscope self-calibration for <b>decalibration</b> robustness, stereo matching considering neighbours epipolar curves to compute 3 D, and robust plane fitting based on generated cartography and Hough transform. We use these 3 D data with GPS raw data to estimate NLOS (Non Line Of Sight) reflected signals pseudorange delay. We exploit extracted planes to build a visibility mask for NLOS detection. A simplified 3 D canyon model allows to compute reflections pseudorange delays. In the end, GPS positioning is computed considering corrected pseudoranges. With experimentations on real fixed scenes, we show generated 3 D models reaching metric accuracy and improvement of horizontal GPS positioning accuracy by more than 50 %. The proposed procedure is effective, and the proposed NLOS detection outperforms CN 0 -based methods (Carrier-to-receiver Noise density) ...|$|E
40|$|The {{presented}} paper {{shows the}} results of the laboratory study on the relation between chosen malfunctions of a fuel injector and composition of exhaust gas from the marine engine. The object of research is a marine 3 -cylinder, four-stroke, direct injection diesel engine with an intercooler system. The engine was loaded with a generator and supercharged. The generator was electrically connected to the water resistance. The engine operated with a load between 50 kW and 250 kW at a constant speed. The engine load and speed, parameters of the turbocharger, systems of cooling, fuelling, lubricating and air exchange, were measured. Fuel injection and combustion pressures in all cylinders of the engine were also recorded. Exhaust gas composition was recorded by using a electrochemical gas analyzer. Air pressure, temperature and humidity were also recorded. Emission characteristics of the engine were calculated according to ISO 8178 standard regulations. During the study the engine operated at the technical condition recognized as „working properly” and with simulated fuel injector malfunctions. Simulation of malfunctions consisted in the increasing and decreasing of fuel injector static opening pressure, <b>decalibration</b> of fuel injector holes and clogging 2 neighboring of 9 fuel injector holes on one of 3 engine cylinders...|$|E
40|$|Today digital sources supply an {{unprecedented}} component of human sensorimotor data, {{the consumption of}} which is correlated with poorly understood maladies such as Internet Addiction Disorder and Internet Gaming Disorder. This paper offers a mathematical understanding of human sensorimotor processing as multiscale, continuous-time vibratory interaction. We quantify human informational needs using the signal processing metrics of entropy, noise, dimensionality, continuity, latency, and bandwidth. Using these metrics, we define the trust humans experience as a primitive statistical algorithm processing finely grained sensorimotor data from neuromechanical interaction. This definition of neuromechanical trust implies that artificial sensorimotor inputs and interactions that attract low-level attention through frequent discontinuities and enhanced coherence will decalibrate a brain's representation of its world {{over the long term}} by violating the implicit statistical contract for which self-calibration evolved. This approach allows us to model addiction in general as the result of homeostatic regulation gone awry in novel environments and digital dependency as a sub-case in which the <b>decalibration</b> caused by digital sensorimotor data spurs yet more consumption of them. We predict that institutions can use these sensorimotor metrics to quantify media richness to improve employee well-being; that dyads and family-size groups will bond and heal best through low-latency, high-resolution multisensory interaction such as shared meals and reciprocated touch; and that individuals can improve sensory and sociosensory resolution through deliberate sensory reintegration practices. We conclude that we humans are the victims of our own success, our hands so skilled they fill the world with captivating things, our eyes so innocent they follow eagerly. Comment: 59 pages, 14 figure...|$|E
30|$|In [40], Klappstein et al. {{studied the}} {{detection}} of moving objects {{as a part of}} a driver assistant system using either a monocular or a stereoscopic system that captured real-world vehicle image sequences. Their approach tries to detect and distinguish between the ‘static motion’ generated by the motion of the camera, usually known as ego-motion, and the ‘dynamic objects motion’. It is based on tracking feature points in sequential images and estimating depth information from 3 D reconstructed images. Their method consists of five processing stages, in which different techniques are performed according to the vision system in use. The optical flow computation is the starting point of both processing sequences, although an initial 3 D reconstruction is also performed in the stereoscopic system. In the second step, the ego-motion is estimated based on optical flow and 3 D information, which is necessary to estimate or enhance the 3 D reconstruction prior to motion detection and final object segmentation. The 3 D stereo reconstruction is enhanced by fusing the optical flow and stereo information using a Kalman filter. The detection of moving objects is based on individual tracked feature points. In the monocular system, this is done by looking for inconsistencies of feature points in the 3 D reconstructed image, according to several constraints that must be satisfied by a static 3 D point. In the stereoscopic system, this is done by analyzing the velocity of feature points. The segmentation stage is based on a globally optimal graph-cut algorithm. The stereoscopic system is reported to offer similar but more accurate results than the monocular system; however, it suffers from a problem of <b>decalibration</b> of the stereo cameras and has higher computational costs.|$|E
30|$|Much of {{the recent}} {{criticism}} of GFT seems to stem from two issues: {{the first is the}} effect of changing user behavior during anomalous events [19, 20] and the second is whether real-time, nowcasting of influenza using GFT adds value to the existing systems available to public health authorities. The first criticism, changing behavior during anomalous events, is an issue for both existing systems and proposed systems based on NDS. The key difference is that existing systems may be both better understood and easier to validate in real-time. While such criticisms may not undermine the case for use of NDS, they do emphasize that the validation of any NDS approach is an ongoing process, and even a perfectly validated system in one period or location may become uncalibrated as behaviors change. It is therefore not meaningful to say that a particular NDS system is or is not informative; that statement must be qualified in space and time. Moreover, the fact that <b>decalibration</b> to “gold standard” systems cannot be detected immediately but only in retrospect is another reason why NDS can only supplement and never fully replace such systems. The second criticism, the need for nowcasting, may depend on the user’s access to different data sources. For public health authorities with access to high-resolution data on reported cases of influenza, simple autoregressive models can be used to nowcast with high accuracy [19]. However, access to these high resolution data-sets varies by public health level (local, state, federal, and international) as well as by user group: researchers, public health authorities, and the private sector. As a result, the utility of GFT varies by user, but for those without access to high-resolution data, it remains an important source of information.|$|E
40|$|Image based {{metrology}} such as Particle Image Velocimetry (PIV) {{depends on}} the comparison of two images of an object taken in fast succession. Cameras for these applications provide the so-called 'double shutter' mode: One frame is captured with a short exposure time and in direct succession a second frame with a long exposure time can be recorded. The difference in the exposure times is typically no problem since illumination is provided by a pulsed light source such as a laser and the measurements are performed in a darkened environment to prevent ambient light from accumulating in the long second exposure time. However, measurements of self-luminous processes (e. g. plasma, combustion [...] .) as weil as experiments in ambient light are difficult to perform and require special equipment (external shutters, highspeed image sensors, multi-sensor systems [...] .). Unfortunately, all these methods incorporate different drawbacks such as reduced resolution, degraded image quality, decreased light sensitivity or increased susceptibility to <b>decalibration.</b> In the solution presented here, off-the-shelf CCD sensors are used with a special timing to combine neighbouring pixels in a binning-like way. As a result, two frames of short exposure time can be captured in fast succession. They are stored in the on-chip vertical register in a line-interleaved pattern, read out in the common way and separated again by software. The two resultant frames are completely congruent; they expose no insensitive lines or line shifts and thus enable sub-pixel accurate measurements. A third frame can be captured at the full resolution analogue to the double shutter technique. Image based measurement techniques such as PIV can benefit from this mode when applied in bright environments. The third frame is useful e. g. for acceleration measurements or for particle tracking applications. © (2017) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only...|$|E
40|$|As {{part of the}} Advanced Test Reactor National Scientific User Facility (ATR NSUF) program, the Idaho National Laboratory (INL) has {{developed}} in-house capabilities to fabricate, test, and qualify new and enhanced sensors for irradiation testing. To meet recent customer requests, an array of temperature monitoring options is now available to ATR users. The method selected is determined by test requirements and budget. Melt wires are the simplest and least expensive option for monitoring temperature. INL has recently verified the melting temperature {{of a collection of}} materials with melt temperatures ranging from 100 to 1000 C with a differential scanning calorimeter installed at INL’s High Temperature Test Laboratory (HTTL). INL encapsulates these melt wires in quartz or metal tubes. In the case of quartz tubes, multiple wires can be encapsulated in a single 1. 6 mm diameter tube. The second option available to ATR users is a silicon carbide temperature monitor. The benefit of this option is that a single small monitor (typically 1 mm x 1 mm x 10 mm or 1 mm diameter x 10 mm length) can be used to detect peak irradiation temperatures ranging from 200 to 800 C. Equipment has been installed at INL’s HTTL to complete post-irradiation resistivity measurements on SiC monitors, a technique that has been found to yield the most accurate temperatures from these monitors. For instrumented tests, thermocouples may be used. In addition to Type-K and Type-N thermocouples, a High Temperature Irradiation Resistant ThermoCouple (HTIR-TC) was developed at the HTTL that contains commercially-available doped molybdenum paired with a niobium alloy thermoelements. Long duration high temperature tests, in furnaces and in the ATR and other MTRs, demonstrate that the HTIR-TC is accurate up to 1800 C and insensitive to thermal neutron interactions. Thus, degradation observed at temperatures above 1100 C with Type K and N thermocouples and <b>decalibration</b> due to transmutation with tungsten-rhenium and platinum rhodium thermocouples can be avoided. INL is also developing an Ultrasonic Thermometry (UT) capability. In addition to small size, UT’s offer several potential advantages over other temperature sensors. Measurements may be made near the melting point of the sensor material, potentially allowing monitoring of temperatures up to 3000 C. In addition, because no electrical insulation is required, shunting effects are avoided. Most attractive, however, is the ability to introduce acoustic discontinuities to the sensor, as this enables temperature measurements at several points along the sensor length. As discussed in this paper, the suite of temperature monitors offered by INL is not only available to ATR users, but also to users at other MTRs...|$|E
40|$|A major {{objective}} of the grant was to complete the fabrication, test, and evaluation of the atmosphere structure experiment on the Galileo Probe, and to receive, analyze, and interpret data received from the spacecraft. The grantee was competitively selected to be Principal Investigator of Jupiter's atmosphere structure on the Galileo Probe. His primary motivation was {{to learn as much}} as possible about Jupiter's atmosphere by means of a successful atmosphere structure experiment, and to support the needs and schedule of the Galileo Project. After a number of launch delays, the Flight instrument was shipped to Kennedy Space Center 2 years after the start of this collaboration, on April 14, 1989, at which time it was determined from System level tests of the ASI on the Probe that the instrument was in good working order and ready for flight. The spacecraft was launched on October 18, 1989. Data analysis of test and calibration data taken over a period of years of instrument testing was continued in preparation for the encounter. The initial instrument checkout in space was performed on October 26, 1989. The data set received by telemetry was thoroughly analyzed, and a report of the findings was transmitted to the Probe Operations Office on Feb. 28, 1990. Key findings reported were that the accelerometer biases had shifted by less than 1 mg through launch and since calibration at Bell Aerospace in 1983; accelerometer scale factors, evaluated by means of calibration currents, fell on lines of variation with temperature established in laboratory calibrations; pressure sensor offsets, correlated as a function of temperature, fell generally within the limits of several years of ground test data; atmospheric and engineering temperature sensor data were internally consistent within a few tenths of a degree; and the instrument electronics performed all expected functions without any observable fault. Altogether, this checkout was highly encouraging of the prospects of instrument performance, although performed greater than 5 years prior to Jupiter encounter. Capability of decoding the science data from the Experiment Data Record to be provided at encounter was developed and exercised using the tape recording of the first Cruise Checkout data. A team effort was organized to program the selection and combination of data words defining pressure, temperature, acceleration, turbulence, and engineering quantities; to apply <b>decalibration</b> algorithms to convert readings from digital numbers to physical quantities; and to organize the data into a suitable printout. A paper on the Galileo Atmosphere Structure Instrument was written and submitted for publication in a special issue of Space Science Reviews. At the Journal editor's request, the grantee reviewed other Probe instrument papers submitted for this special issue. Calibration data were carefully taken for all experiment sensors and accumulated over a period of 10 years. The data were analyzed, fitted with algorithms, and summarized in a calibration report for use in analyzing and interpreting data returned from Jupiter's atmosphere. The sensors included were the primary science pressure, temperature, and acceleration sensors, and the supporting engineering temperature sensors. This report was distributed to experiment coinvestigators and the Probe Project Office...|$|E
