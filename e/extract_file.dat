4|459|Public
50|$|The latest {{available}} {{results are}} those of the 2013 survey. Two general types of data set are provided first, the full public data set is given in DAP/SAS, Stata and ASCII computer formats; second, an <b>extract</b> <b>file</b> of summary variables is provided in Microsoft Excel spreadsheet format. All of these files are provided in compressed form as ZIP files.|$|E
40|$|Abstract {{copyright}} UK Data Service {{and data}} collection copyright owner. The New Earnings Survey is almost certainly the most detailed and comprehensive earnings series anywhere in the world. It is a one in a hundred sample survey of employees in Britain, giving information on aspects of earnings and employment based on a week in April each year. The NES enquiry is conducted by the Department of Employment under {{the provisions of the}} Statistics of Trade Act (1947). Under the terms of this Act, data so obtained and relating solely to any individual may not be released into the public domain. All the data described here are in a form that ensures that there is no disclosure of individual information. They have been processed into a minimally aggregated form approved by the Department of Employment: any data record released relates to an aggregate of not less than three individuals. Main Topics : The dataset consists of fourteen separate extract data files from the original New Earnings Survey files held by the Department of Employment. Each <b>extract</b> <b>file</b> had been constructed to allow investigation of a particular aspect of the data contained in the Survey, as follows: AGG 01 National Collective Agreements AGG 02 Manual Skill Differentials AGG 03 Regional Implications AGG 04 Age implications AGG 05 Dispersion of Pay within Occupations AGG 06 Shiftwork AGG 07 Pay in relation to hours worked AGG 08 Public/Private Sector Pay Movements AGG 09 White Collar Pay Movements AGG 10 Sex Differentials AGG 11 Incentive Pay and Payment Schemes AGG 12 Incentive Payment Schemes and Age AGG 14 Pay in Relation to Size of Company and Plant AGG 15 Pay in Relation to Company Size and Region Eight of the aggregate files (numbers 2, 3, 4, 5, 7, 8, 9 and 10) relate to dimensions recorded in the Survey in each year and comprise 13 annual files, one for each year 1970 - 1982. A further two aggregate files (numbers 1 and 6) contain 10 annual files for the years 1973 - 1982 inclusive, omitting the years 1970 - 1972, AGG 01, due to the introduction of new occupations codes in 1973, and AGG 06 {{due to the lack of}} shift pay premium prior to 1973. The remaining four files (numbers 11, 12, 14 and 15) relate to a single year only and are based on the special question included in that year...|$|E
40|$|Figure 1 {{shows the}} Honeypot system {{deploying}} diagram. As mentioned above, A Honeypot consists of real service network (100) and virtual network (103). The ingress point, router(101) transport input traffic to both network {{and if the}} honey sensor(102) detects a sign of cracking, the input traffic is transported to only Honeypot network and Honeypot system stores every forensic evidences. 3. P 2 P Honeypot In this paper we adopt the idea of honeypot and design a method to trace users who spread illegal or harmful contents in P 2 P network and store forensic evidences. To do this, we operate several P 2 P service on bait P 2 P server farm. To gather illegal/harmful contents list from these bait P 2 P server, we need to analyze each P 2 P service protocol and hook and reassemble the P 2 P packets and <b>extract</b> <b>file</b> lists. And then, we store these file list in central DB. From this information we can figure out the trend of distribution of illegal/harmful contents. Each file on file list can be classified automatically to illegal/harmful using machine learning. In chapter 4, we show these machine learning algorithms. To gather files in file list, we use the P 2 P client function. So our bait P 2 P function can execute server function and client function. If our classification module judges the file illegal/harmful, we store the forensic evidences that are source/target IP, request time, target file and user etc. In the case {{that we need to}} prevent from spreading a certain file, we can make hash code of that file. With this hash code, we can trace the spreading trend of that file. Figure 2. P 2 P Honeypot deploy diagram Figure 2 shows the deploy diagram of P 2 P honeypot. In Figure 1 we install honeypot sensor(103) to protect a service server farm(100). P 2 P honeypot is not for protecting service server but for pretending that it is normal P 2 P server and alluring the users who spread illegal/harmful files. The illegal/harmful file list is stored in function DB(130). We have several bait P 2 P server for supporting different P 2 P service and arranging a number of servers for each P 2 P service. All illegal/harmful file lists are collected by central DB(150). The system consists of five modules. Figure 3 depicts honeypot system(120, 130) more precisely. � P 2 P server/client function module(123, 125) � Packet hooking and data collecting module(121...|$|E
5000|$|Ability {{to create}} self <b>extracting</b> <b>files</b> for the Win32 {{platform}} ...|$|R
50|$|Programs {{to decode}} and <b>extract</b> <b>files</b> from TNEF-encoded {{attachments}} {{are available on}} many platforms.|$|R
50|$|There {{is also a}} GPL {{command line}} program called unADF, {{which allows you to}} <b>extract</b> <b>files</b> from an ADF file.|$|R
40|$|Software Product Lines (SPLs) allow {{variants}} of a software {{system to be}} generated based on the configuration selected by the user. In this thesis, we focus on C based software systems with build-time variability using a build system and C preprocessor. Such systems usually consist of a configuration space, a code space, and a build space. The configuration space describes the features that the user can select and any configuration constraints between them. The features and the constraints between them are commonly documented in a variability model. The code and build spaces contain the actual implementation of the system where the former contains the C code files with conditional compilation directives (e. g., #ifdefs), and the latter contains the build scripts with conditionally compiled files. We study {{the relationship between the}} three spaces as follows: (1) we detect variability anomalies which arise due to inconsistencies among the three spaces, and (2) we use anomaly detection techniques to automatically extract configuration constraints from the implementation. For (1), we complement previous research which mainly focused on the relationship between the configuration space and code space. We additionally analyze the build space to ensure that the constraints in all three spaces are consistent. We detect inconsistencies, which we call variability anomalies, in particular dead and undead artifacts. Dead artifacts are conditional artifacts which are not included in any valid configuration while undead artifacts are those which are always included. We look for such anomalies at both the code block and source file levels using the Linux kernel as a case study. Our work shows that almost half the configurable features are only used to control source file compilation in Linux’s build system, KBUILD. We analyze KBUILD to <b>extract</b> <b>file</b> presence conditions which determine under which feature combinations is each file compiled. We show that by considering the build system, we can detect an additional 20 % variability anomalies on the code block level when compared to only using the configuration and code spaces. Our work also shows that file level anomalies occur less frequently than block level ones. We analyze the evolution of the detected anomalies and identify some of their causes and fixes. For (2), we develop novel analyses to automatically extract configuration constraints from implementation and compare them to those in existing variability models. We rely on two means of detecting variability anomalies: (a) conditional build-time errors and (b) detecting under which conditions a feature has an effect on the compiled code (to avoid duplicate variants). We apply this to four real-world systems: uClibc, BusyBox, eCos, and the Linux kernel. We show that our extraction is 93 % and 77 % accurate respectively for the two means we use and that we can recover 19 % of the existing variability-model constraints using our approach. We qualitatively investigate the non-recovered constraints and find that many of them stem from domain knowledge. For systems with existing variability models, understanding where each constraint comes from can aid in traceability between the code and the model which can help in debugging conflicts. More importantly, in systems which do not have a formal variability model, automatically extracting constraints from code provides the basis for reverse engineering a variability model. Overall, we provide tools and techniques to help maintain and create software product lines. Our work helps to ensure the consistency of variability constraints scattered across SPLs and provides tools to help reverse engineer variability models...|$|E
5000|$|SecureZIP 12.5 (released on May 12, 2010) added {{integration}} with Microsoft Office 2010, custom alternative extensions for mailed [...]ZIP archives, <b>extracting</b> WavPack <b>files</b> within ZIP archives, <b>extracting</b> <b>files</b> from archives created on IBM z/OS using hardware compression tools, changes in FIPS Mode to support NIST algorithm changes affective end of 2010.|$|R
5000|$|QuArKSAS: The QuArK Steam Access System, or QuArKSAS, is a {{command-line}} {{program that}} allows the user to <b>extract</b> <b>files</b> from the Steam filesystem.|$|R
5000|$|Can {{automatically}} <b>extract</b> compressed <b>files</b> {{and combine}} and <b>extracted</b> multipart zip <b>files</b> after download ...|$|R
5000|$|EXPAND.EXE, {{only since}} version 6 (which is {{included}} from Windows Vista to above) can <b>extract</b> <b>files</b> to their paths. The previous versions don't do it.|$|R
5000|$|Winmail File Viewer+ — Paid Universal app for the iPhone, iPod Touch and iPad to open winmail.dat {{attachments}} with {{option to}} save and share <b>extracted</b> <b>files</b> ...|$|R
40|$|This report {{presents}} shadowcopy, tool {{written in}} Python that <b>extracts</b> and deduplicates <b>files</b> from Microsoft NTFS Shadow copies using the Microsoft Volume Shadow Service (VSS), copies the files to an external volume, and prepares {{a report of}} each <b>extracted</b> <b>file's</b> name, timestamp, original path, and MD 5 hash value...|$|R
50|$|WinZip 3.0 added {{full support}} for LZH files, {{including}} self-extracting LZH files, configurable support for most virus scanners, simplified options to add and <b>extract</b> <b>files</b> in a subdirectory tree.|$|R
5000|$|WinMail.dat Viewer - Browse Outlook winmail.dat files — Paid Universal app for the iPhone, iPod Touch and iPad to open winmail.dat {{attachments}} with {{option to}} view, save and share <b>extracted</b> <b>files</b> ...|$|R
50|$|As well as {{offering}} standard FTP functions, net2ftp {{also offers}} a variety of features including archiving and <b>extracting</b> <b>files</b> and directories, downloading a selected group of files and/or directories as an archive.|$|R
50|$|Not all CD drives can {{properly}} extract such hidden tracks. Some drives {{will report}} errors when reading these tracks, and some will seem to extract them properly, but the <b>extracted</b> <b>file</b> will contain only silence.|$|R
50|$|Updates to the Code are {{published}} weekly to the nigp.com site for download by users. Codes requested directly from users are typically delivered via <b>extract</b> <b>files</b> to the user, with general {{release of the}} new codes in the weekly website update.|$|R
50|$|IExpress Wizard {{interface}} {{guides the}} user {{through the process}} of creating a self-extracting package. It asks what the package should do: <b>extract</b> <b>files</b> and then run a program, or just <b>extract</b> <b>files.</b> It then allows the user to specify a title for the package, add a confirmation prompt, add a license agreement that the end-user must accept in order to allow extraction, select files to be archived, set display options for the progress window, and finally, specify a message to display upon completion.If the option to create an archive and run a program is selected, then there will be an additional step, prompting the user to select the program that will be run upon extraction.|$|R
50|$|B1 Pack is an {{open-source}} software {{project that}} produces a cross-platform command-line tool and a Java library for creating and <b>extracting</b> <b>file</b> archives in the B1 archive format. Source code of the project is published at GitHub.B1 Pack Project is released under the Apache License. The B1 Pack Tool module builds a single executable JAR file which can create, list, and <b>extract</b> B1 archive <b>files</b> from a command-line interface.|$|R
50|$|Filzip {{supports}} {{seven different}} archive formats, allowing {{the user to}} add and <b>extract</b> <b>files</b> from the archives. These include ZIP, BH, CAB, JAR, LHA (LZH), TAR, and gzip. A handful of other formats are supported for extraction only, including ACE, ARC, ARJ, RAR, and ZOO.|$|R
5000|$|The Info-ZIP Windows tools {{also support}} NTFS {{filesystem}} permissions, {{and will make}} an attempt to translate from NTFS permissions to Unix permissions or vice versa when <b>extracting</b> <b>files.</b> This can result in potentially unintended combinations, e.g[...]exe files being created on NTFS volumes with executable permission denied.|$|R
40|$|We {{present a}} novel {{mechanism}} of phase-dependent electric transport in diffusive normal metal-superconductor structures. We provide a detailed theoretical and numerical analysis of recent unexplained experiments essentially explaining them. Comment: Self <b>extracting</b> <b>file,</b> 7 pages latex and 4 postscript figures. The paper {{is also available}} at [URL] In this revision we resolved some printing problems concerning figures 2 and...|$|R
5000|$|Apple has not {{released}} any documentation on the format, but attempts {{to reverse engineer}} parts of the format have been successful. The encrypted layer was reverse engineered in an implementation called VileFault (a spoonerism of FileVault). There are few options available to <b>extract</b> <b>files</b> or mount the proprietary Apple Disk Image format. Some cross-platform conversion utilities are: ...|$|R
5000|$|Compressed Folders: Windows ME {{includes}} {{support for}} ZIP files through a shell extension known as Compressed Folders. Originally {{introduced in the}} Plus! 98 pack for Windows 98, this feature allows users to create, access and <b>extract</b> <b>files</b> from ZIP archives similar to a regular folder in Windows. The user can also restrict access to files with a password.|$|R
50|$|Unrar is {{the name}} of two {{different}} command-line applications for <b>extracting</b> RAR <b>file</b> archives.|$|R
40|$|Data {{migration}} {{is a process}} of transferring data from existing system to another system and it is divided into two processes: (a) extracting data from an existing system into an <b>extracted</b> <b>file</b> and (b) loading data from <b>extracted</b> <b>file</b> into the new system. Before data migration procedure first you check the database structure and tables of the koha software. After checking the data format, collect data from the libsys software. It includes bibliographical details of the documents, items details, transaction records and patron details. Then using MarcEdit software collected data has been formatted as per the koha software data base structure. Using koha’s data import facility data are uploaded to koha database. Last and most important step is the checking data consistency. This step completes the data migration procedure and certified the koha to be run independently. In this paper we tried to explain the step by step method of bibliographic data migration from libsys to koha, checking data consistency and the problems faced during data migratio...|$|R
5000|$|DirectVobSub is able {{to extract}} {{subtitles}} from a DVD without first <b>extracting</b> the <b>files</b> from it.|$|R
5000|$|... resource: (which may <b>extract</b> a <b>file</b> {{from within}} the JAR of the MIDlet, but is implementation-dependent) ...|$|R
50|$|BetterZip is a trialware file archiver {{and data}} {{compression}} utility developed by Robert Rezabek, and first released in May 2006. It is developed {{solely for the}} OS X platform. Unlike the built-in Archive Utility from Apple it includes the ability to extract and compress in many archive formats, {{as well as the}} ability to view an archive and selectively <b>extract</b> <b>files</b> without automatically <b>extracting</b> the entire contents.|$|R
5000|$|A free (GPL) {{alternative}} is HFSExplorer written by Erik Larsson. HFSExplorer is {{an application for}} viewing and <b>extracting</b> <b>files</b> from an HFS+ volume (Mac OS Extended) or an HFSX volume (Mac OS Extended, Case-sensitive) located either on a physical disk, on a [...]dmg disk image, or in a raw file system dump. However, HFSExplorer can only read from, but not write to, HFS formatted volumes.|$|R
5000|$|SecureZIP 14 (released in October 2011) {{added support}} {{to create and}} <b>extract</b> OpenPGP <b>files,</b> support for digital time {{stamping}} of signed archives, support for processors with AES-NI instruction for faster AES encryption, ability to <b>extract</b> 7-Zip <b>files</b> and CD/DVD Data Image files, new Auto Select View, ability to preserve Zone Identifier information in downloaded files, support to add and extract NTFS alternate streams, added file search logic.|$|R
3000|$|Instrumentation starts when Instrumentor <b>extracts</b> {{essential}} <b>files</b> {{from the}} original APK file, including the manifest file (manifest [...]...|$|R
3000|$|The genes {{located in}} the four genomic regions for the {{identified}} QTLs were <b>extracted</b> (Additional <b>file</b> 2 [...]...|$|R
5000|$|The tar utility {{included}} in most Linux distributions can <b>extract</b> [...]tar.gz <b>files</b> by passing the [...] option, e.g., [...]|$|R
5000|$|... fUnZip <b>extracts</b> a <b>file</b> in a ZIP or gzip file {{directly}} to output from archives or other piped input.|$|R
