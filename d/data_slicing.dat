40|366|Public
25|$|The {{reconstruction}} {{is accomplished}} by a two-step process, first images are aligned to account for errors in the positioning of a sample; such errors can occur due to vibration or mechanical drift. Alignment methods use image registration algorithms, such as autocorrelation methods to correct these errors. Secondly, using a reconstruction algorithm, such as filtered back projection, the aligned image slices can be transformed from a set of two-dimensional images, I'j(x,y), to a single three-dimensional image, Ij(x,y,z). This three-dimensional image {{is of particular interest}} when morphological information is required, further study can be undertaken using computer algorithms, such as isosurfaces and <b>data</b> <b>slicing</b> to analyse the data.|$|E
40|$|Abstract. In this paper, {{we present}} a formal {{description}} of <b>data</b> <b>slicing,</b> which is a type-directed program transformation technique that separates a program’s heap into several independent regions. Pointers within each region mirror the structure of pointers in the original heap; however, each field whose type is a base type (e. g., the integer type) appears in {{only one of these}} regions. In addition, we discuss several applications of <b>data</b> <b>slicing.</b> First, <b>data</b> <b>slicing</b> can be used to add extra fields to existing data structures without compromising backward compatibility; the CCured project uses <b>data</b> <b>slicing</b> to preserve library compatibility in instrumented programs at a reasonable performance cost. <b>Data</b> <b>slicing</b> {{can also be used to}} improve locality by separating “hot ” and “cold ” fields in an array of data structures, and it can be used to protect sensitive data by separating “public ” and “private ” fields. Finally, <b>data</b> <b>slicing</b> can serve as a refactoring tool, allowing the programmer to split data structures while automatically updating the code that manipulates them. ...|$|E
40|$|Many {{techniques}} {{have been designed}} for privacy preserving and micro data publishing, such as generalization and bucketization. Several works showed that generalization loses some amount of information especially for high dimensional data. So it’s not efficient for high dimensional data. In case of Bucketization, it does not prevents membership disclosure and also does not applicable for data {{that do not have}} a clear separation between Quasi-identifying attributes and sensitive attributes. In this paper, we presenting an innovative technique called <b>data</b> <b>slicing</b> which partitions the data. An efficient algorithm is developed for computing sliced data that obeys l-diversity requirement. we also show how <b>data</b> <b>slicing</b> is better than generalization and bucketization. <b>Data</b> <b>slicing</b> preserves better utility than generalization and also does not requires clear separation between Quasi-identifying and sensitive attributes. <b>Data</b> <b>slicing</b> is also used to prevent attribute disclosure and develop an efficient algorithm for computing the sliced data that obeys l-diversity requirement. Experimental results confirm that <b>data</b> <b>slicing</b> preserves data utility than generalization and more effective than bucketization involving sensitive attributes. Experimental results demonstrate the effectiveness of this method...|$|E
40|$|We {{examine the}} {{functional}} cohesion of procedures using a <b>data</b> <b>slice</b> abstraction. Our analysis identifies the data tokens that lie {{on more than}} one slice as the "glue" that binds separate components together. Cohesion is measured in terms of the relative number of glue tokens, tokens that lie {{on more than one}} <b>data</b> <b>slice,</b> and super-glue tokens, tokens that lie on all <b>data</b> <b>slices</b> in a procedure, and the adhesiveness of the tokens. The intuition and measurement scale factors are demonstrated through a set of abstract transformations...|$|R
40|$|In visual {{exploration}} {{and analysis of}} data, determining how to select and transform the data for visualization is a challenge for data-unfamiliar or inexperienced users. Our main hypothesis is that for many data sets and common analysis tasks, there are relatively few "data slices" that result in effective visualizations. By focusing human users on appropriate and suitably transformed parts of the underlying data sets, these <b>data</b> <b>slices</b> can help the users carry their task to correct completion. To verify this hypothesis, we develop a framework that permits us to capture exemplary <b>data</b> <b>slices</b> for a user task, and to explore and parse visual-exploration sequences into a format that makes them distinct and easy to compare. We develop a recommendation system, DataSlicer, that matches a "currently viewed" <b>data</b> <b>slice</b> with the most promising "next effective" <b>data</b> <b>slices</b> for the given exploration task. We report the results of controlled experiments with an implementation of the DataSlicer system, using four common analytical task types. The experiments demonstrate statistically significant improvements in accuracy and exploration speed versus users without access to our system...|$|R
40|$|A method, apparatus, system, {{article of}} manufacture, and {{computer}} readable storage medium provide {{the ability to}} measure wind. Data at a first resolution (i. e., low resolution data) is collected by a satellite scatterometer. Thin <b>slices</b> of the <b>data</b> are determined. A collocation of the <b>data</b> <b>slices</b> are determined at each grid cell center to obtain ensembles of collocated <b>data</b> <b>slices.</b> Each ensemble of collocated <b>data</b> <b>slices</b> is decomposed into a mean part and a fluctuating part. The data is reconstructed at a second resolution from the mean part and a residue of the fluctuating part. A wind measurement is determined from the data at the second resolution using a wind model function. A description of the wind measurement is output...|$|R
40|$|Dynamic slicing {{algorithms}} {{are used}} to narrow {{the attention of the}} user or an algorithm to a relevant subset of executed program statements. Although dynamic slicing was first introduced to aid in user level debugging, increasingly applications aimed at improving software quality, reliability, security, and performance are finding opportunities to make automated use of dynamic slicing. In this paper we present the design and evaluation of three precise dynamic <b>data</b> <b>slicing</b> algorithms called the full preprocessing (FP), no preprocessing (NP) and limited preprocessing (LP) algorithms. The algorithms differ in the relative timing of constructing the dynamic data dependence graph and its traversal for computing requested dynamic data slices. Our experiments show that the LP algorithm is a fast and practical precise <b>data</b> <b>slicing</b> algorithm. In fact we show that while precise data slices can be orders of magnitude smaller than imprecise dynamic data slices, for small number of <b>data</b> <b>slicing</b> requests, the LP algorithm is faster than an imprecise dynamic <b>data</b> <b>slicing</b> algorithm proposed by Agrawal and Horgan...|$|E
40|$|This paper {{presents}} {{a new technique}} for <b>data</b> <b>slicing</b> of distributed programs running on a hierarchy of machines. <b>Data</b> <b>slicing</b> can be realized as a program transformation that partitions heaps of machines in a hierarchy into independent regions. Inside each region of each machine, pointers preserve the original pointer structures in the original heap hierarchy. Each heap component of the base type (e. g., the integer type) goes only to a region {{of one of the}} heaps. The proposed technique has the shape of a system of inference rules. In addition, this paper {{presents a}} simply structure type system to decide type soundness of distributed programs. Using this type system, a mathematical proof that the proposed slicing technique preserves typing properties is outlined in this paper as well. Comment: 9 pages, 8 figure...|$|E
30|$|The {{final stage}} of the {{exploration}} workflow—time-series extraction—is specific to time-resolved techniques. Thanks to HDF 5 ’s <b>data</b> <b>slicing</b> feature and careful performance optimizations within iris, dynamics in scattering patterns can be explored interactively, in real-time. Further data transformations are also possible, most importantly azimuthal averaging of scattering patterns from polycrystalline samples. Time-series extraction in the GUI is shown in Fig. 2 for two types of samples, single-crystal and polycrystal.|$|E
40|$|We {{examine the}} {{functional}} cohesion of procedures using a <b>data</b> <b>slice</b> abstraction. Our analysis identifies the data tokens that lie {{on more than}} one slice as the "glue" that binds separate components together. Cohesion is measured in terms of the relative number of glue tokens, tokens that lie {{on more than one}} <b>data</b> <b>slice,</b> and super-glue tokens, tokens that lie on all <b>data</b> <b>slices</b> in a procedure, and the adhesiveness of the tokens. The intuition and measurement scale factors are demonstrated through a set of abstract transformations and composition operators. Index terms [...] - software metrics, cohesion, program slices, measurement theory 1 Introduction Cohesion is an attribute of a software unit or module that refers to the "relatedness" of module components. A highly cohesive software module is a module that has one basic function and is indivisible [...] - it is difficult to split a cohesive module into separate components. Module cohesion can be classified using an ordinal scale that incl [...] ...|$|R
40|$|The use of 3 dviewnix, a {{three-dimensional}} medical imaging program, is explored for {{the display of}} <b>data</b> <b>slices</b> of a human male provided by the National Library of Medicine. The data underwent a series of conversions, including changing the data from RGB color to greyscale, {{in order to be}} compatible with 3 dviewnix. Once the data is converted, 3 dviewnix can be used to create three-dimensional reconstructions of the <b>data</b> <b>slices,</b> as well as extracting the skeletal structure from the reconstructions. The <b>data</b> <b>slices,</b> which are in one millimeter increments across the body, do not always convey much information. A program was designed to alleviate this by making vertical slices instead of horizontal. This program required a small amount of pre-processing because the slices were surrounded by a background of blue frozen gelatin that had to be removed. The goal of the project is to make the slices, reconstructions, and the extracted skeleton available via the World Wide Web for educational purpo [...] ...|$|R
40|$|Abstract- Cloud {{computing}} {{has become}} a new platform for personal computing. However, while designing the strategy of data placement, there still lacks the consideration of systematic diversity of distributed transaction costs. This paper proposes the use of genetic algorithms to address the data placement problem in cloud computing. This strategy has adequately considered the correlation between <b>data</b> <b>slices</b> to minimize {{the total cost of}} distributed transactions. Compared to other methods, genetic algorithms have proven to comprehensively consider the correlation between the <b>data</b> <b>slices</b> in cloud computing, therefore greatly reducing the amount and cost of distributed transactions...|$|R
30|$|Iris handles large reduced {{datasets}} by storing them {{using the}} Hierarchical Data Format version 5 (HDF 5), an archival format designed for large n-dimensional numerical datasets [3, 13]. HDF 5 supports transparent compression and data-corruption detection mechanisms. Most importantly, HDF 5 supports <b>data</b> <b>slicing,</b> which allows to read specific portions of an HDF 5 file {{without having to}} load the entire file—which can require tens of gigabytes of memory {{in the case of}} fully reduced UES datasets.|$|E
40|$|Nonsingular {{estimation}} of high dimensional covariance matrices {{is an important}} step in many statistical procedures like classification, clustering, variable selection an future extraction. After a review of the essential background material, this paper introduces a technique we call slicing for obtaining a nonsingular covariance matrix of high dimensional <b>data.</b> <b>Slicing</b> is essentially assuming that the data has Kronecker delta covariance structure. Finally, we discuss the implications of the results in this paper and provide an example of classification for high dimensional gene expression data...|$|E
40|$|Advanced {{techniques}} for Excel power users. Crunch and analyze Excel data {{the way the}} professionals do with this clean, uncluttered, visual guide to advanced Excel techniques. Using numerous screenshots and easy-to-follow numbered steps, this book clearly shows you how to perform professional-level modeling, charting, data access, <b>data</b> <b>slicing,</b> and other functions. You'll find super {{techniques for}} getting {{the most out of}} Excel's statistical and financial functions, Excel PivotTables and PivotCharts, Excel Solver and BackSolver, and more. : Provides a clear look at power-using Excel, the world'...|$|E
30|$|<b>Data</b> must be <b>sliced</b> into {{segments}} {{small enough}} such that each segment bears no meaningful information to malicious entities. With <b>data</b> <b>sliced</b> in this way, malicious entities {{may be able}} to access an individual data segment, but the access to the data segment should not compromise the confidentiality of the data as a whole.|$|R
40|$|In {{this paper}} we propose a new {{compression}} algorithm geared to reduce the time needed to test scan-based designs. Our scheme compresses the test vector set by encoding the bits that need to be flipped in the current test <b>data</b> <b>slice</b> in order to obtain the mutated subsequent test <b>data</b> <b>slice.</b> Exploitation of the overlap in the encoded data by effective traversal search algorithms results in drastic overall compression. The technique we propose can be utilized as not only a stand-alone technique but also can be utilized on test data already compressed, extracting even further compression. The performance of the algorithm is mathematically analyzed and its merits experimentally confirmed on the larger examples of the ISCAS' 89 benchmark circuits...|$|R
40|$|FIGURES 8 – 21. CT reconstructions of Balticoroma wheateri {{new species}} (male holotype, GPIH). (8) frontal view showing chelicerae and labral spur; (9) view of right {{pedipalp}} showing embolus; (10 – 14) various views of right metatarsus 1, showing y-shaped clasping structure; (15) anterior view of specimen showing the section taken through the chelicerae {{to produce the}} raw <b>data</b> <b>slice</b> in Figure 16; (16) raw <b>data</b> <b>slice</b> demonstrating that the chelicerae and clypeal extentions are clearly separated; (20 – 21) various views of the right pedipalp. C, chelicera; ce, clypeal extension; co, dorsal cymbial outgrowth; cy, cymbium; e, embolus; eb, embolic base; ec, embolic coil;? fc, functional conductor sensu Wunderlich (2004); ls, labral spur; t, tegulum...|$|R
40|$|This paper {{describes}} the analog front-end of a fully integrated CMOS TV decoder, {{suitable for the}} reception of terrestrial as well as satellite signals, based on the D 2 -MAC transmission system. While the video reconstruction is undertaken using DSP, the front-end subsystem incorporates many linear and non-linear analog functions, including amplitude measuring, AGC, clamping, <b>data</b> <b>slicing,</b> clock recovery and of course, A/D conversion for the MAC signal processing. The chip is fabricated in 1 -u CMOS, and operates from a single 5 -V supply...|$|E
40|$|It is {{difficult}} to debug a program when the data set that causes it to fail is large (or voluminous). The cues that may help in locating the fault are obscured by {{the large amount of}} information that is generated from processing the data set. Clearly, a smaller data set which exhibits the same failure should lead to the diagnosis of the fault more quickly than the initial, large data set. We term such a smaller data set a data slice and the process of creating it <b>data</b> <b>slicing.</b> The problem of creating a data slice is undecidable. In this paper, we investigate four generateand-test heuristics for deriving a smaller data set that reproduces the failure exhibited by a large data set. The four heuristics are: invariance analysis, origin tracking, random elimination, and programspecific heuristics. We also provide a classification of programs based upon a certain relationship between its input and output. This classification may be used to choose an appropriate heuristic in a given debugging scenario. As evidenced from a database of debugging anecdotes at the Open University, U. K., debugging failures exhibited by large data sets require inordinate amounts of time. Our <b>data</b> <b>slicing</b> techniques would significantly reduce the effort required in such scenarios. ...|$|E
30|$|The {{important}} {{goal of this}} work is to preserve {{the privacy of the}} multiple SA and to improve the utility of the health care <b>data.</b> <b>Slicing</b> algorithm helps in preserving correlation and utility and anatomization minimizes the information loss. The advanced clustering algorithms exhibited its efficiency by minimizing the time and complexity. In addition, this work follows the principle of k-anonymity, l-diversity. This yields the means for the prevention of privacy threats like membership, identity and attributes disclosure. Also, this method can used to operate for any number of SA in an efficient manner.|$|E
40|$|Digital {{television}} {{terrestrial broadcasting}} (DTTB) networks {{can help to}} alleviate the congestion problem in cellular networks by delivering rich contents to {{a large number of}} clients simultaneously. In particular, recently, there is a strong interest of extending current DTTB systems to support multimedia broadcasting services. The lack of return channel and long transmission time interval however impose great challenge to the resource allocation for this application in DTTB networks. The reliable resource allocation is studied for multi-services with data delivery delay constraints in the second generation digital video broadcasting terrestrial (DVB-T 2) system. To solve this challenging problem, the data cells of a T 2 -frame are divided into <b>data</b> <b>slices</b> which are indexed by binary numbers. These <b>data</b> <b>slices</b> are organised in a binary tree, and each node in the tree is associated with a certain number of non-adjacent <b>data</b> <b>slices.</b> Then a node can be allocated to a service by using the predefined policies. Based on this scheme, this study proposes a heuristic algorithm to allocate resources to multi-services. Simulation results validate the effectiveness of the proposed algorithm and demonstrate its advantage over the current resource allocation scheme in DVB-T 2 networks...|$|R
40|$|Backward slicing {{has been}} used {{extensively}} in program understanding, debugging and scaling up of program analysis. For large programs, {{the size of the}} conventional backward slice is about 25 % of the program size. This may be too large to be useful. Our investigations reveal that in general, the size of a slice is influenced more by computations governing the control flow reaching the slicing criterion than by the computations governing the values relevant to the slicing criterion. We distinguish between the two by defining <b>data</b> <b>slices</b> and control slices both of which are smaller than the conventional slices which can be obtained by combining the two. This is useful because for many applications, the individual <b>data</b> or control <b>slices</b> are sufficient. Our experiments show that for more than 50 % of cases, the <b>data</b> <b>slice</b> is smaller than 10 % of the program in size. Besides, the time to compute <b>data</b> or control <b>slice</b> is comparable to that for computing the conventional slice. Comment: 10 pages, 5 figures, two algorithm...|$|R
40|$|The {{explosion}} in the volume of data about urban environments has opened up opportunities to inform both policy and administration and thereby help governments {{improve the lives of}} their citizens, increase the efficiency of public services, and reduce the environmental harms of development. However, cities are complex systems and exploring the data they generate is challenging. The interaction between the various components in a city creates complex dynamics where interesting facts occur at multiple scales, requiring users to inspect a large number of <b>data</b> <b>slices</b> over time and space. Manual exploration of these slices is ineffective, time consuming, and in many cases impractical. In this paper, we propose a technique that supports event-guided exploration of large, spatio-temporal urban data. We model the data as time-varying scalar functions and use computational topology to automatically identify events in different <b>data</b> <b>slices.</b> To handle a potentially large number of events, we develop an algorithm to group and index them, thus allowing users to interactively explore and query event patterns on the fly. A visual exploration interface helps guide users towards <b>data</b> <b>slices</b> that display interesting events and trends. We demonstrate the effectiveness of our technique on two different data sets from New York City (NYC) : data about taxi trips and subway service. We also report on the feedback we received from analysts at different NYC agencies. ...|$|R
40|$|The {{standardization}} {{and performance}} testing of analysis tools {{is a prerequisite}} to widespread adoption of genome-wide sequencing, particularly in the clinic. However, performance testing is currently complicated by the paucity of standards and comparison metrics, {{as well as by}} the heterogeneity in sequencing platforms, applications and protocols. Here we present the genome comparison and analytic testing (GCAT) platform to facilitate development of performance metrics and comparisons of analysis tools across these metrics. Performance is reported through interactive visualizations of benchmark and performance testing data, with support for <b>data</b> <b>slicing</b> and filtering. The platform is freely accessible a...|$|E
40|$|Abstract Background One {{important}} {{concept in}} traditional Chinese medicine (TCM) is "treating different diseases {{with the same}} therapy". In TCM practice, some patients with Rheumatoid Arthritis (RA) and some other patients with Coronary Heart Disease (CHD) can be treated with similar therapies. This suggests {{that there might be}} something commonly existed between RA and CHD, for example, biological networks or biological basis. As the amount of biomedical data in leading databases (i. e., PubMed, SinoMed, etc.) is growing at an exponential rate, {{it might be possible to}} get something interesting and meaningful through the techniques developed in data mining. Results Based on the large data sets of Western medicine literature (PubMed) and traditional Chinese medicine literature (SinoMed), by applying <b>data</b> <b>slicing</b> algorithm in text mining, we retrieved some simple and meaningful networks. The Chinese herbs used in treatment of both RA and CHD, might affect the commonly existed networks between RA and CHD. This might support the TCM concept of treating different diseases with the same therapy. Conclusions First, the data mining results might show the positive answer that there are biological basis/networks commonly existed in both RA and CHD. Second, there are basic Chinese herbs used in the treatment of both RA and CHD. Third, these commonly existed networks might be affected by the basic Chinese herbs. Forth, discrete derivative, the <b>data</b> <b>slicing</b> algorithm is feasible in mining out useful data from literature of PubMed and SinoMed. </p...|$|E
40|$|A {{high-frequency}} {{ultrasound system}} for the range 10 MHz to 100 MHz is expanded to allow ultrasound volume measurements by using an 8 bit transient digitizer with 400 MHz sampling rate. The full A-signal response is digitized at each point of a meander xy-scan. Data are acquired in 200 x 200 points laterally and in 200 digitization points in the time axis. Measurement times are less than two times as required for a conventional C-scan. A graphic workstation is used for visualization of ultrasound <b>data.</b> <b>Slicing</b> techniques and ray-casting techniques are available in real-time operations. The method {{has been applied to}} visualize small inhomogeneities in Zirconia and SiSiC ceramic plates...|$|E
5000|$|The reverse {{remote control}} channel is usually fixed at 433.92 MHz, using {{whatever}} modulation {{is on the}} 34 kHz to 45 kHz IR remote [...] "carrier". ASK/OOK schemes such as RC5 and RC6 work best over the RF link as the receiver uses a <b>data</b> <b>slicer</b> and AGC designed for ASK/OOK with Manchester encoding.|$|R
50|$|The chipset {{consisted}} of three chip designs: the COMANCHE B-cache and memory controller, the DECADE <b>data</b> <b>slice,</b> and the EPIC PCI controller. The DECADE chips implemented the data paths in 32-bit slices, and therefore the 21071 has two such chips while the 21072 has four. The EPIC chip has a 32-bit path to the DECADE chips.|$|R
40|$|Dynamic slicing {{algorithms}} {{have been}} considered to aid in debugging for many years. However, {{as far as we}} know, no detailed studies on evaluating the benefits of using dynamic slicing for locating real faults present in programs have been carried out. In this paper we study the effectiveness of fault location using dynamic slicing for a set of real bugs reported in some widely used software programs. Our results show that of the 19 faults studied, 12 faults were captured by <b>data</b> <b>slices,</b> 7 required the use of full slices, and none of them required the use of relevant slices. Moreover, it was observed that dynamic slicing considerably reduced the subset of program statements that needed to be examined to locate faulty statements. Interestingly, we observed that all of the memory bugs in the faulty versions were captured by <b>data</b> <b>slices.</b> The dynamic slices that captured faulty code included 0. 45 % to 63. 18 % of statements that were executed at least once...|$|R
40|$|Abstract—People-centric urban sensing {{is a new}} {{paradigm}} gaining popularity. A main obstacle to its widespread deploy-ment and adoption are the privacy concerns of participating individuals. To tackle this open challenge, this paper presents the design and evaluation of PriSense, a novel solution to privacy-preserving data aggregation in people-centric urban sensing systems. PriSense {{is based on the}} concept of <b>data</b> <b>slicing</b> and mixing and can support a wide range of statistical additive and non-additive aggregation functions such as Sum, Average, Variance, Count, Max/Min, Median, Histogram, and Percentile with accurate aggregation results. PriSense can support strong user privacy against a tunable threshold number of colluding users and aggregation servers. The efficacy and efficiency of PriSense are confirmed by thorough analytical and simulation results. I...|$|E
40|$|International audienceGrid {{systems are}} complex {{heterogeneous}} systems, and their modeling constitutes a highly challenging goal. This paper {{is interested in}} modeling the jobs handled by the EGEE grid, by mining the Logging and Bookkeeping files. The goal is to discover meaningful job clusters, going beyond the coarse categories of ”successfully terminated jobs” and ”other jobs”. The presented approach is a threestep process: i) <b>Data</b> <b>slicing</b> is used to alleviate the job heterogeneity and afford discriminant learning; ii) Constructive induction proceeds by learning discriminant hypotheses from each data slice; iii) Finally, double clustering is used on the representation built by constructive induction; the clusters are fully validated after the stability criteria proposed by Meila (2006). Lastly, the job clusters are submitted to the experts and some meaningful interpretations are foun...|$|E
40|$|In this paper, {{we present}} a set of {{security}} requirements for critical systems, fundamental premises that those requirements would entail, and ideas for implementations that would instantiate those premises.  We discuss the overriding requirement guiding our paradigm: that "first principles" reflects the only real security strategy, where first principles are ideally provable, often measurable; and at minimum, possible to order and bound.  These principles allow us {{to take into account}} that many security policies may be even be in conflict, and as such, proofs, measures, and ordering gives an analyst (or even better, an automated system) the metrics that one needs in order to make informed decisions about how to resolve conflicts.  We demonstrate several metrics that enable this, including state replication, <b>data</b> <b>slicing,</b> collusion, and information theory...|$|E
30|$|The {{finite element}} method (FEM) model was {{generated}} from CT volumetric <b>data</b> (<b>slice</b> thickness of 0.300  mm) of a 42 -year-old male patient of the Department of Biomedical Sciences at Ohio University, where {{informed consent was obtained}} prior to data collection. DICOM raw data was extracted from the CT and imported into Mimics 13.1 software (Materialise, Leuven, Belgium) to reconstruct a 3 D model (Wu Laboratory, UCLA Bioengineering).|$|R
30|$|It is {{sometimes}} challenging for a 3 D printer to directly read a file made from 3 D modeling software. Therefore, {{it needs to}} convert STL data made from 3 D modeling software into the path <b>data</b> <b>sliced</b> into layers, where each layer is as thick as one-time movement of printer head. This path data is called G-Code and the software converting into G-Code is called ‘slicing program’.|$|R
40|$|This thesis {{deals with}} {{accelerated}} 3 D rendering of medical data, e. g. computed tomography, using a graphics processor and OpenGL library. Raw <b>data</b> <b>slices</b> are send to graphic memory and rendered by a ray-casting algorithm. The {{goal of this}} project is high quality visual output and full user interaction at the same time. Multiple rendering modes are avaiable to the user: MIP, X-Ray simulation and realistic shading...|$|R
