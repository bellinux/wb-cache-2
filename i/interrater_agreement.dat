458|40|Public
50|$|The Levine scaling system {{persists}} as {{the gold}} standard for grading heart murmur intensity. It provides accuracy, consistency, and <b>interrater</b> <b>agreement</b> which are essential for diagnostic purposes, particularly to distinguish innocent from pathological murmurs. Louder murmurs (grade ≥3) are more likely believed to represent cardiac defects that tend to have hemodynamic consequences.|$|E
40|$|<b>Interrater</b> <b>agreement</b> in {{qualitative}} research is rarely quantified. We present a new method for assessing <b>interrater</b> <b>agreement</b> in the coding of focus group transcripts, based on vector space methods. We also demonstrate similarities between this vector method and two previously published <b>interrater</b> <b>agreement</b> methods. Using these methods, {{we showed that}} <b>interrater</b> <b>agreement</b> for the qualitative data was quite low, attributable {{in part to the}} subjective nature of the codes and in part to the very large number of possible codes. These methods of assessing inter-rater agreement have the potential to be useful in determining and improving reliability of qualitative codings...|$|E
40|$|Much {{empirical}} {{research has been}} done recently on evaluating and modeling <b>interrater</b> <b>agreement</b> in software process assessments. <b>Interrater</b> <b>agreement</b> {{is the extent to which}} assessors agree in their ratings of software process capabilities when presented with the same evidence and performing their ratings independently. This line of research was based on the premise that lack of <b>interrater</b> <b>agreement</b> can lead to erroneous decisions from process assessment scores. However, thus far we do not know the impact of <b>interrater</b> <b>agreement</b> on the cost of assessments. In this paper we report on a study that evaluates the relationship between <b>interrater</b> <b>agreement</b> and the cost of the consolidation activity in assessments. The study was conducted in the context of two assessments using the emerging international standard ISO/IEC 15504. Our results indicate that for organizational processes, the relationship is strong and in the expected direction. For project level processes no relationship was found. [...] ...|$|E
40|$|OBJECTIVE: The Alcohol Use Disorders Identification Test (AUDIT) is an {{important}} screening tool but has never been administered to Korean Americans. This {{study was conducted to}} examine the psychometric properties of a Korean version of the AUDIT referred to as AUDIT-K and to determine which cutoff score of the scale would perform better in Korean Americans. METHOD: Translation and backtranslation of the AUDIT-K were conducted to obtain a measure consistent with the Korean cultural understanding of alcohol use. Following satisfactory <b>interrater</b> <b>agreements</b> on each item about its Korean translation, the AUDIT-K was administered to 118 Korean American men (Time 1) and to 93 of the men approximately 1 month later (Time 2). Data were analyzed for internal consistency reliability, test-retest reliability, and construct validity. RESULTS: Approximately 47. 5...|$|R
40|$|Object Interrater {{reliability}} {{as measured}} by the kappa (κ) statistic is a widely used and valuable tool to measure the robustness of a scoring system. Seizure frequency reduction is a central outcome measure following vagus nerve stimulation (VNS). A specific VNS scoring system has been proposed by McHugh, but its interrater reliability has not been tested. The authors assessed its interrater reliability and compared it with that of the Engel and International League Against Epilepsy (ILAE) systems. Methods Using the Engel, ILAE, and McHugh scoring systems, 3 observers independently rated the medical records of children who had undergone vagus nerve stimulator implantation between January 2001 and April 2011 at the Southampton University Hospital. The <b>interrater</b> <b>agreements</b> were then calculated using the κ statistic. Results Interrater reliability for the McHugh scale (κ 0. 693) was very good and was superior to those of the Engel (κ 0. 464) and ILAE (κ 0. 491) systems for assessing outcome in patients undergoing VNS. Conclusions The authors recommend considering the McHugh scoring system when assessing outcomes following VNS...|$|R
40|$|INTRODUCTION The aims of {{this study}} were to compare lateral cephalograms with other radiologic methods for {{diagnosing}} suspected fusions of the cervical spine and to validate the assessment of congenital fusions and osteoarthritic changes against the anatomic truth. METHODS Four cadaver heads were selected with fusion of vertebrae C 2 and C 3 seen on a lateral cephalogram. Multidetector computed tomography (MDCT) and cone-beam computed tomography (CBCT) were performed and assessed by 5 general radiologists and 5 oral radiologists, respectively. Vertebrae C 2 and C 3 were examined for osseous fusions, and the left and right facet joints were diagnosed for osteoarthritis. Subsequently, the C 2 and C 3 were macerated and appraised by a pathologist. Descriptive analysis was performed, and <b>interrater</b> <b>agreements</b> between and within the groups were computed. RESULTS All macerated specimens showed osteoarthritic findings of varying degrees, but no congenital bony fusion. All observers agreed that no fusion was found on MDCT or CBCT. They disagreed on the prevalence of osteoarthritic deformities (general radiologists/MDCT, 100...|$|R
30|$|The {{calculated}} Kappa {{was over}} 0.86, which shows excellent <b>interrater</b> <b>agreement.</b>|$|E
40|$|Currently, {{guidelines}} do {{not exist}} for applying <b>interrater</b> <b>agreement</b> indices to {{the vast majority of}} methodological and theoretical problems that organizational and applied psychology researchers encounter. For a variety of methodological problems, we present critical values for interpreting the practical significance of observed average deviation (AD) values relative to either single items or scales. For a variety of theoretical problems, we present null ranges for AD values, relative to either single items or scales, to be used for determining whether an observed distribution of responses within a group is consistent with a theoretically specified distribution of responses. Our discussion focuses on important ways to extend the usage of <b>interrater</b> <b>agreement</b> indices beyond problems relating to the aggregation of individual level data. Keywords average deviation (AD), <b>interrater</b> <b>agreement,</b> multilevel research, aggregation, null distribution Assessments of <b>interrater</b> <b>agreement,</b> or the degree to which raters are interchangeable (Kozlowski & Hattrup, 1992), 1 are integral to many types of organizational and applied psychology research. For instance, <b>interrater</b> <b>agreement</b> assessments have recently been central with respect to addressing substantive questions within domains such as organizational climate and leadership (e. g., Dawson...|$|E
40|$|A latent {{variable}} modeling method {{for evaluation of}} <b>interrater</b> <b>agreement</b> is outlined. The procedure is useful for point and interval estimation {{of the degree of}} agreement among a given set of judges evaluating a group of targets. In addition, the approach allows one to test for identity in underlying thresholds across raters as well as to identify possibly aberrantly evaluating judges. A measure of <b>interrater</b> <b>agreement</b> is proposed, which is related to popular indexes of interrater reliability for observed variables and composite reliability. The outlined method also permits the examina-tion of underlying common sources of ratings variability, provides a useful comple-ment to the literature on <b>interrater</b> <b>agreement</b> with manifest measures, and relaxes some of its assumptions. The procedure is illustrated with numerical data. Keywords <b>interrater</b> <b>agreement,</b> {{latent variable}} modeling, reliability, raters, targets In behavioral, educational, social, and biomedical research, a study design is often-times used where several ‘‘raters’ ’ (or ‘‘judges’’) —such as psychiatrists, counselors, teachers, clinicians, evaluators, or observers—are asked to evaluate a group o...|$|E
40|$|OBJECTIVE: To {{compare the}} dental arch {{relationship}} following one-stage repair of unilateral cleft lip and palate (UCLP) in Warsaw with a matched sample of patients {{treated by the}} Oslo Cleft Team. MATERIAL: Study models of 61 children (mean age, 11. 2; SD, 1. 7) with a nonsyndromic complete UCLP consecutively treated with one-stage closure of the cleft at 9. 2 months (range, 6. 0 to 15. 8 months; SD, 2. 0) by the Warsaw Cleft Team at the Institute of Mother and Child, Poland, were compared with a sample drawn from a consecutive series of patients with UCLP treated by the Oslo Cleft Team and matched for age, gender, and soft tissue band. METHODS: The study models were given random numbers to blind their origin. Four examiners rated the dental arch relationship using the GOSLON Yardstick. The strength of agreement of rating was assessed with weighted Kappa statistics. An independent t-test was carried out to compare the GOSLON scores between Warsaw and Oslo samples, and Fisher's exact tests were performed to evaluate the difference of distribution of the GOSLON scores. RESULTS: The intrarater and <b>interrater</b> <b>agreements</b> were high (K > or =. 800). No difference in dental arch relationship between Warsaw and Oslo groups was found (mean GOSLON score = 2. 68 and 2. 65 for Warsaw and Oslo samples, respectively). The distribution of the GOSLON grades was similar in both groups. CONCLUSIONS: The dental arch relationship following one-stage repair (Warsaw protocol) was comparable with {{the outcome of the}} Oslo Cleft Team's protocol...|$|R
40|$|BACKGROUND AND PURPOSE: CT {{remains the}} most {{commonly}} used imaging technique in acute stroke but is often delayed after minor stroke. Interobserver reliability in distinguishing hemorrhagic transformation of infarction from intracerebral hemorrhage may depend on delays to CT but has not been reported previously despite the clinical importance of this distinction. METHODS: Initial CT scans with intraparenchymal hematoma from the first 1000 patients with stroke in the Oxford Vascular Study were independently categorized as intracerebral hemorrhage or hemorrhagic transformation of infarction by 5 neuroradiologists, both blinded and unblinded to clinical history. Thirty scans were reviewed twice. Agreement was quantified by the kappa statistic. RESULTS: Seventy-eight scans showed intraparenchymal hematoma. Blinded pairwise <b>interrater</b> <b>agreements</b> for a diagnosis of intracerebral hemorrhage ranged from kappa= 0. 15 to 0. 48 with poor overall agreement (kappa= 0. 35; 95 % CI, 0. 15 to 0. 54) even after unblinding (kappa= 0. 41; 0. 21 to 0. 60). Blinded intrarater agreements ranged from kappa= 0. 21 to 0. 92. Lack of consensus after unblinding was greatest in patients scanned >or= 24 hours after stroke onset (67 % versus 25 %, P= 0. 001) and in minor stroke (National Institutes of Health Stroke Scale or= 24 hours after minor stroke and in 48 % of all 30 -day stroke survivors in whom reliable diagnosis would be expected to influence long-term management. CONCLUSIONS: Reliability of diagnosis of intraparenchymal hematoma on CT brain scan in minor stroke is poor, particularly if scanning is delayed. Immediate brain imaging is justified in patients with minor stroke...|$|R
40|$|The aim of {{this study}} was to {{evaluate}} <b>interrater</b> and intermodality <b>agreement</b> in assessing health status using the Health Utilities Index. A random sample from a Dutch cohort of 14 -year-old Very Low Birth Weight children and their parents were invited to participate in a face-to-face (n = 150) or telephone (n = 150) interview. All 300 participants were also sent a questionnaire by mail. Response rate was 68 %. <b>Interrater</b> and intermodality <b>agreement</b> were high for the physical HUI 3 attributes and poor for the psychological attributes. Children and parents reported more dysfunction in the psychological attributes when interviewed than when completing the mailed questionnaire. High agreement on the physical attributes may have resulted from the fact that hardly any dysfunction was reported in these attributes, and poor agreement in the psychological attributes may have been a result of the fact that in these attributes much more dysfunction was reported. In measuring children's health status using the HUI 3, the results and their interpretation vary with the source of information and the modality of administration. For maximum comparability between studies, written self-report questionnaires seem the preferred optio...|$|R
40|$|The {{present study}} {{attempted}} {{to replicate the}} results found in Roch and Paquin (2004). Raters in their study viewed a video tape performance and rated a target 2 ̆ 7 s performance during a leaderless group discussion. Their results indicated that as item specificity decreased (i. e., as items became more vague), {{there was an increase}} in <b>interrater</b> <b>agreement,</b> a positive correlation between item difficulty and interater agreement, and a positive correlation between performance and <b>interrater</b> <b>agreement.</b> The present study also examined the impact of item exposure as it related to these hypotheses and the relationship between item exposure and <b>interrater</b> <b>agreement.</b> Using a sample of 299 participants, the present study used the same video of a leaderless group discussion, rating form, and specificity ratings as used in Roch and Paquin (2004). The results of this study found a positive correlation between specificity and agreement and that item exposure significantly positively impacted <b>interrater</b> <b>agreement...</b>|$|E
40|$|The authors present {{guidelines}} for establishing a useful range for <b>interrater</b> <b>agreement</b> and a cutoff for acceptable <b>interrater</b> <b>agreement</b> when using Burke, Finkelstein, and Dusig’s average deviation (AD) index {{as well as}} critical values for tests of statistical significance with the AD index. Under the assumption that judges respond randomly to an item or set of items in a measure, the authors show that a criterion for acceptable <b>interrater</b> <b>agreement</b> or practical significance when using the AD index can be approximated as c/ 6, where c {{is the number of}} re-sponse options for a Likert-type item. The resulting values of 0. 8, 1. 2, 1. 5, and 1. 8 are discussed as standards for acceptable <b>interrater</b> <b>agreement</b> when using the AD index with 5 -, 7 -, 9 -, and 11 -point items, respectively. Using similar logic, the AD agreement index and interpretive standard are generalized to the case of a re-sponse scale that involves percentages or proportions, rather than discrete cate-gories, or at the other extreme, the assessment of <b>interrater</b> <b>agreement</b> with respect to the rating of a single target on a dichotomous item (e. g., yes-no, agree-disagree, true-false item formats). Finally, the usefulness of these {{guidelines for}} judging ac...|$|E
30|$|Statistical {{analysis}} was performed using SPSS (version 21.0, IBM Corporation, Armonk, New York, United States). The <b>interrater</b> <b>agreement</b> for the classified tibialis posterior tendon appearance described in the surgical report (OP 1, OP 2) and the appearance in the preoperative MRI scan (MR 1, MR 2) was analyzed for each classification system (M 1 – 4) using Cohen’s kappa coefficient. Furthermore, <b>interrater</b> <b>agreement</b> between the 2 orthopaedic and the 2 radiological investigators was calculated. The strength of the <b>interrater</b> <b>agreement</b> was considered as poor (kappa <  0), slight (kappa 0.01 – 0.20), fair (kappa 0.21 – 0.40), moderate (0.41 – 0.60), substantial (0.81 – 0.80), and almost perfect (kappa >  0.80) according to Landis and Koch (1977).|$|E
40|$|Instrument content {{validity}} is often established through qualitative expert reviews, yet quantita-tive analysis of reviewer agreements is also advocated in the literature. Two quantitative approaches to {{content validity}} estimations were compared and contrasted using a newly devel-oped instrument called the Osteoporosis Risk Assessment Tool (ORAT). Data {{obtained from a}} panel of eight expert judges were analyzed. A Content Validity Index (CVI) initially determined that only one item lacked <b>interrater</b> proportion <b>agreement</b> about its relevance to the instrument as a whole (CVI = 0. 57). Concern that higher proportion agreement ratings {{might be due to}} ran-dom chance stimulated further analysis using a multirater kappa coefficient of agreement. An additional seven items had low kappas, ranging from 0. 29 to 0. 48 and indicating poor agreement among the experts. The findings supported the elimination or revision of eight items. Pros and cons to using both proportion agreement and kappa coefficient analysis are examined...|$|R
40|$|Abstract- An {{automatic}} {{human brain}} segmentation system for magnetic resonance images is presented. It has two main parts: a fuzzy clustering algorithm {{and a set}} of cluster combination rules. Images are segmented into ten classes by the unsupervised fuzzy c-means clustering algorithm. Then a knowledge-based system labels the clusters into the tissues of interest: cerebrospinal fluid, gray matter and white matter. This approach can process MRI data that comes from different scanners with different sequences and head coils, using several different spin-echo images (with different echo times) and different slice thickness. The system adapts without manual intervention. Segmented synthetic image data from the brainweb Simulated Normal Brain Database resulted in a one voxel away accuracy of 90 %. The results from real data from various magnetic resonance imagers were compared with a radiologist’s segmentation and found to generally agree within 10 %, the typical range of <b>interrater</b> radiologist <b>agreement...</b>|$|R
40|$|Many estimators of {{the measure}} of {{agreement}} between two dichotomous ratings of a person have been proposed. The results of Fleiss (1975) are extended, and it is shown that four estimators- Scott’s (1955) π coefficient, Cohen’s (1960) kˆ, Maxwell & Pilliner’s (1968) r₁₁, and Mak’s (1988) p˜-are interpretable both as chance-corrected measures of agreement and as intraclass correlation coefficients for different ANOVA models. Relationships among these estimators are established for finite samples. Under Kraemer’s (1979) model, it is shown that these estimators are equivalent in large samples, and that the equations for their large sample variances are equivalent. Index terms: index of <b>agreement,</b> <b>interrater</b> reliability, intraclass correlation, kappa statistic...|$|R
40|$|OBJETIVE: The main {{objective}} {{of this study was}} to assess the <b>interrater</b> <b>agreement</b> for the Schedule for Affective Disorders and Schizophrenia Epidemiological version for School-Age Children (K-SADS-E). METHODS: Four interviewers being trained with the K-SADS-E scored independently 29 videotaped interviews performed with psychiatric outpatients in the ADHD Outpatient Clinic at Hospital de Clínicas de Porto Alegre. <b>Interrater</b> <b>agreement</b> analysis was performed using the kappa coefficient (k). RESULTS: Kappa coefficients were. 93 (p<. 001) for affective disorders,. 9 (p<. 001) for anxiety disorders,. 94 (p<. 001) for attention-deficit/hyperactivity disorders and disruptive behavior disorders. CONCLUSION: These findings suggest an excellent <b>interrater</b> <b>agreement</b> for the diagnosis of several mental disorders in childhood and adolescence by the Brazilian Portuguese version of the K-SADS-E...|$|E
40|$|This study {{compared}} the <b>interrater</b> <b>agreement</b> for pattern differentiation and acupoints prescription between {{two groups of}} human patients simulated with different diagnostic outcomes. Patients were simulated using a dataset about zangfu patterns and separated into groups (n= 30 each) according to the diagnostic outcome determined by a computational model. A questionnaire with 90 patients was delivered to 6 TCM experts (4 -year minimal of clinic experience) who {{were asked to indicate}} a single pattern (among 73) and 8 acupoints (among 378). <b>Interrater</b> <b>agreement</b> was higher for pattern differentiation than for acupuncture prescription. <b>Interrater</b> <b>agreement</b> on pattern differentiation was slight for both groups with correct (Light’s κ= 0. 167, 95 % CI = [0. 108; 0. 254]) and incorrect diagnosis (Light’s κ= 0. 190, 95 % CI = [0. 120; 0. 286]). <b>Interrater</b> <b>agreement</b> on acupuncture prescription was slight for both groups of correct (ι= 0. 029, 95 % CI = [0. 015; 0. 057]) and incorrect diagnosis (ι= 0. 040, 95 % CI = [0. 023; 0. 058], P= 0. 075). Diagnostic performance of raters yielded the following: accuracy = 60. 9 %, sensitivity = 21. 7 %, and specificity = 100 %. An overall improvement in the <b>interrater</b> <b>agreement</b> and diagnostic accuracy was observed when the data were analyzed using the internal systems instead of the pattern’s labels...|$|E
40|$|Ambiguous phrases are {{the bane}} of researchers' {{attempts}} to ensure acceptably high <b>interrater</b> <b>agreement</b> in the encoding of texts. When modal usages and their associated rationales are encoded {{as part of a}} text analysis, ambiguities arise in characteristic (and thus identifiable) ways. This article illustrates the typical sources of disagreement among coders involved in encoding data during a modality analysis and provides concrete strategies for improving <b>interrater</b> <b>agreement...</b>|$|E
40|$|Purpose: The {{purpose of}} this study was to examine the psychometric {{properties}} of the Children's Somatization Inventory (CSI) in Turkish schoolchildren and adolescents. Methods: The CSI was translated using translation and back-translation. The participants were 813 schoolchildren, adolescents and their parents (n= 453). Content and construct validity were assessed to test the validity of the CSI- 24. Internal consistency reliability, <b>interrater</b> reliability (child-parent <b>agreement)</b> and test-retest reliability were assessed to test the reliability of the CSI- 24. Results: Psychometric analyses of the Turkish version of the CSI- 24 indicate high reliability and good content and construct validity. Conclusion: The Turkish version of the CSI- 24 is a useful instrument for measuring self-reported somatic complaints in Turkish schoolchildren and adolescents between the ages of 9 and 15...|$|R
40|$|ABSTRACT Background: Speech {{difficulties}} {{are found in}} 34 % to 65 % of patients with oral cancer and perceptual assessment of these difficulties is common practice among speech and language therapists (SLTs). A longitudinal study using a validated assessment tool regarding speech quality in patients with oral cancer is missing. Aim: Firstly to obtain insight in the development over time regarding speech quality {{up to five years}} after oral oncological treatment. Secondly to identify the clinical factors during a 5 -year period that are of influence on speech quality in patients treated for oral cancer. Finally to determine the reliability of the parameter ‘grade’ of the London Speech Evaluation (LSE). Methods: Perceptual assessment of speech using the parameter ‘grade’ of the LSE was carried out 4 - 6 weeks before treatment, 4 - 6 weeks after surgery and/or 4 - 6 weeks after radiotherapy, and 6, 12, and 60 months after treatment. A generalized linear mixed backward stepwise model for ‘grade’ was constructed. To determine intra- and <b>interrater</b> level of <b>agreement</b> weighted Kappa’s were calculated. Results: Speech quality decreased significantly after intervention and increased significantly between the 6 and 12 months assessments. Assessment period, age shortly before oncological intervention, tumour location, tumour size, reconstruction method and dental status had a significant effect on the parameter ‘grade’. <b>Interrater</b> level of <b>agreement</b> ranged from moderate to substantial, intrarater agreement ranged from moderate to almost perfect. Conclusion and implications: The development over time in patients with oral cancer has been obtained. Quality of speech is influenced by the effect of assessment period, age shortly before the oncological intervention, location of the tumour, tumour size, reconstruction method, and dental status. The parameter ‘grade’ of the LSE {{has proven to be a}} valid manner to assess speech quality. This parameter can be used by SLTs to monitor effects of their intervention...|$|R
3000|$|A {{researcher}} {{trained to}} reliability at Frank Porter Graham (FPG) Child Development Institute at the University of North Carolina {{served as the}} “gold standard” for inter-observer reliability. All data collectors were either FPG-trained researchers or were trained in the field, attaining reliability with the “gold standard” rater. Inter-rater reliability was calculated using both percentages of exact agreement and Cohen’s kappa coefficient. The observation team completed repeated independent observations and training until a weighted kappa of at least [...]. 60 for exact agreement was attained. An agreement level of weighted kappa = [...]. 61 was attained for the ECERS-R between the “gold standard” rater and the author. Kappas for two additional data collectors were [...]. 82 and [...]. 80. <b>Interrater</b> percent <b>agreements</b> within one scale point for the ECERS-R were 88 %, 99 % and 98 %, respectively. A weighted kappa of [...]. 60 was attained for the ITERS-R between the “gold standard” rater and the author. The kappas between the “gold standard” rater and the other data collectors For ITERS-R were [...]. 78 and [...]. 80. Inter-rater percent agreements within one scale point for the ITERS-R were 87 %, 99 % and 93 % respectively. Mid-point reliability visits were completed with each data collector and the “gold standard” rater or the author to ensure inter-rater agreement continued to exceed 80 % agreement within one scale point.|$|R
30|$|No {{significant}} statistical {{association was}} found between the number of test bites recorded and respective quality. The <b>interrater</b> <b>agreement</b> produced almost perfect and substantial agreements.|$|E
40|$|Using {{functional}} analysis results to prescribe treatments {{is the preferred}} method for developing behavioral interventions. Little is known, however, about {{the reliability and validity}} of visual inspection for the interpretation of {{functional analysis}} data. The purpose of this investigation was to develop a set of structured criteria for visual inspection of multielement functional analyses that, when applied correctly, would increase <b>interrater</b> <b>agreement</b> and agreement with interpretations reached by expert consensus. In Study 1, 3 predoctoral interns interpreted functional analysis graphs, and <b>interrater</b> <b>agreement</b> was low (M =. 46). In Study 2, 64 functional analysis graphs were interpreted by a panel of experts, and then a set of structured criteria were developed that yielded interpretive results similar to those of the panel (exact agreement =. 94). In Study 3, the 3 predoctoral interns from Study 1 were trained to use the structured criteria, and the mean <b>interrater</b> <b>agreement</b> coefficient increased to. 81. The results suggest that (a) the interpretation of functional analysis data may be less reliable than is generally assumed, (b) decision-making rules used by experts in the interpretation of functional analysis data can be operationalized, and (c) individuals can be trained to apply these rules accurately to increase <b>interrater</b> <b>agreement.</b> Potential uses of the criteria are discussed...|$|E
40|$|A {{new method}} to assess {{causality}} of suspected adverse drug reactions, the Liverpool Adverse Drug Reaction Causality Assessment Tool (LCAT), showed high <b>interrater</b> <b>agreement</b> when used by its developers. Our {{aim was to}} compare the <b>interrater</b> <b>agreement</b> achieved by LCAT to that achieved by another causality assessment method, the World Health Organization-Uppsala Monitoring Centre system for standardised case causality assessment (WHO-UMC system), in our setting. Four raters independently assessed adverse drug reaction causality of 48 drug-event pairs, identified during a hospital-based survey. A randomised design ensured that no washout period was required between assessments with the two methods. We compared the methods' <b>interrater</b> <b>agreement</b> by calculating agreement proportions, kappa statistics, and the intraclass correlation coefficient. We identified potentially problematic questions in the LCAT by comparing raters' responses to individual questions. Overall unweighted kappa was 0. 61 (95 % CI 0. 43 to 0. 80) on the WHO-UMC system and 0. 27 (95 % CI 0. 074 to 0. 46) on the LCAT. Pairwise unweighted Cohen kappa ranged from 0. 33 to 1. 0 on the WHO-UMC system and from 0. 094 to 0. 71 on the LCAT. The intraclass correlation coefficient was 0. 86 (95 % CI 0. 74 to 0. 92) on the WHO-UMC system and 0. 61 (95 % CI 0. 39 to 0. 77) on the LCAT. Two LCAT questions were identified as significant points of disagreement. We were unable to replicate the high <b>interrater</b> <b>agreement</b> achieved by the LCAT developers and instead found its <b>interrater</b> <b>agreement</b> to be lower than that achieved when using the WHO-UMC system. We identified potential reasons for this and recommend priority areas for improving the LCAT...|$|E
40|$|Introduction: This study {{aimed to}} {{validate}} a new observational measure of socially disinhibited behavior {{for use in}} a population of individuals with traumatic brain injury (TBI). Method: Participants were twenty-two adults with severe TBI (mean age = 50. 45 years) and 21 healthy comparison participants (mean age = 45. 29 years). Ratings of observed social disinhibition were correlated with the disinhibition domain scores of the Neuropsychiatric Inventory–Disinhibition (NPI–D) and with Sydney Psychosocial Reintegration Scale (SPRS) scores. A regression analysis was undertaken to determine whether formal measures of disinhibition could predict observed disinhibition. Results: The <b>interrater</b> absolute <b>agreement</b> for the social disinhibition ratings was good, intraclass correlation coefficient (ICC) =. 69. Participants with TBI were rated as significantly more disinhibited than comparison participants, t(25. 05) = – 2. 07, p =. 049. The ratings were positively correlated with the NPI frequency score (r =. 45, p =. 038) and distress score (r =. 45, p =. 035). The ratings were not related to change in employment or in interpersonal relationships on the SPRS, and formal measures of disinhibition were unable to predict observed social disinhibition. Conclusions: This study demonstrates good interrater reliability and construct validity of the observational measure. The results evidence the usefulness of this measure and the NPI–D for detecting social disinhibition after TBI. 13 page(s...|$|R
40|$|Body {{condition}} scoring (BCS) is {{a subjective}} semiquantitative method of assessing body fat and muscle. Scoring systems use a scale {{in which the}} midrange represents optimal body condition, lower values represent lean to emaciated conditions, and higher values indicate excessive body fat. A valid BCS system is clearly described, relevant to the species, shows agreement within and between raters, and is consistent with objective measures. The goal {{of the current study}} was to assess intra- and interrater variability of a BCS system that uses a 1 -to- 5 scale and entails the palpation of key anatomic sites (hips, spine, pelvis, thorax, and abdomen) to assess prominence of bony structures, muscle mass, and subcutaneous fat. To assess interrater variability, 4 raters independently assessed BCS in 616 rhesus macaques (Macaca mulatta) in 4 age groups: infant, younger than 1 y; juvenile, 1 to 4 y; subadult, 4 to 7 y; and adult, 7 to 17 y. To assess intrarater variability, each rater independently reevaluated a subset of adult macaques (n = 15) within 2 wk of initial evaluation. A weighted κ score was used to analyze intra- and <b>interrater</b> variability. <b>Agreement</b> between raters was highest for subadult and adult macaques, intermediate for juveniles, and least for infants. Intrarater agreement was high for all raters except one, for which it was moderate. Our results suggest that raters applied the BCS system most consistently to adult and subadult macaques and less so to juvenile and infant animals. However, the percentage agreement between raters to within one half of a score unit increased markedly when raters scored infants in the context of ‘as is’ rather than ‘ideal for age. ...|$|R
40|$|OBJECTIVE: To assess <b>interrater</b> {{reliability}} and <b>agreement</b> of 5 Ergo-Kit functional capacity evaluation lifting tests {{in subjects with}} low back pain (LBP). DESIGN: Within-subjects design, with 2 repeated measurements. SETTING: Academic medical center in The Netherlands. PARTICIPANTS: Twenty-four subjects (10 men, 14 women) with LBP. INTERVENTIONS: Not applicable. MAIN OUTCOME MEASURE: Five Ergo-Kit lifting tests (2 isometric, 3 dynamic) were assessed on 2 occasions (t 1, t 2), by 2 different raters (R 1, R 2). The interval between the test sessions was 3 days. Interrater reliability level was expressed with the intraclass correlation coefficient (ICC), {{and the level of}} agreement between raters with the standard error (SE) of measurement. RESULTS: ICCs means (reliability) of isometric and dynamic Ergo-Kit lifting tests ranged from. 94 to. 97, and SE of measurement values (agreement) ranged from 1. 9 to 8. 6 kg. CONCLUSIONS: There was good {{reliability and}} agreement between raters of the isometric and dynamic Ergo-Kit lifting tests in subjects with LBP, which supports the use of these tests to assess functional lifting capacit...|$|R
30|$|Early {{childhood}} practitioners used investigator-developed recording {{forms to}} code {{the number of}} learning opportunities (trials) afforded each child per game and to compute the percent of trials per game that were associated with a child response-contingent behavior having an environmental consequence. A trial {{was defined as the}} availability of a child reinforcement where a child-specific targeted behavior did or did not result in a social or non-social environmental consequence (e.g., a parent talking to her son or daughter in response to the child smiling at the parent). The two child measures were used to assess parent effort and effectiveness, respectively, where both were used as a latent variable for measuring efficacious parenting practices. Research assistants made 95 joint visits with the practitioners to ascertain <b>interrater</b> <b>agreement</b> {{over the course of the}} 8  weeks of intervention. <b>Interrater</b> <b>agreement</b> for number of learning opportunities was 92 %, and <b>interrater</b> <b>agreement</b> for the number of child behavior that resulted in reinforcing consequences was 91 %.|$|E
30|$|The two scales each {{included}} {{four items}} {{scored on a}} 5 -point scale ranging from occurring Not-At-All to occurring A Great Deal. Principal components {{factor analysis of the}} two sets of ratings at each measurement occasion all produced univariate factor solutions with average internal reliability estimates of α[*]=[*]. 69 and α[*]=[*]. 85 for the proximal and distal measures, respectively. Research assistants assessed parent social-affective behavior on the same 95 occasions that child response-contingent behavior was coded. <b>Interrater</b> <b>agreement</b> for the four proximal parent affective behavior items was 90 %, and <b>interrater</b> <b>agreement</b> for the four distal parent affective behavior items was 95 %.|$|E
40|$|The {{study of}} <b>interrater</b> <b>agreement</b> and itnerrater {{reliability}} attract extensive attention, {{due to the}} fact that the judgments from multiple raters are subjective and may vary individually. To evaluate <b>interrater</b> <b>agreement</b> and interrater reliability, five different methods or indices are proposed: percentage of agreement, kappa coefficient, the correlation coefficient, intraclass correlation coefficient (ICC), and generalizability (G) theory. In this study, we introduce and discuss the advantages and disadvantages of these methods to evaluate <b>interrater</b> <b>agreement</b> and reliability. Then we review and explore the rank across these five indices by use of frequency in practice in the past five years. Finally, we illustrate how to use these five methods under different circumstances and provide SPSS and SAS code to analyze <b>interrater</b> <b>agreement</b> and reliability. We apply the methods above to analyze the data from Parent-Child Interaction System of global ratings (PARCHISY), and conclude as follows: (1) ICC is the most often used method to evaluate interrater reliability in recent five years, while generalizability theory is the least often used method. The G coefficients provide similar interrater reliability with weighted kappa and ICC on most items, based on the criteria. (2) When the reliability is high itself, different methods provide consistent indication on interrater reliability based on different criteria. If the reliability is not consistent among different methods, both ICC and G coefficient will provide better interrater reliability based on the criteria, and they also provide consistent results...|$|E
40|$|BACKGROUND AND PURPOSE: Standardized {{scales are}} a {{prerequisite}} for rehabilitation and research. This {{study was designed to}} determine {{the reliability and validity of}} scores on items of the trunk assessment of the Melsbroek Disability Scoring Test (MDST) and Trunk Impairment Scale (TIS) in people with multiple sclerosis (MS). SUBJECTS: Thirty people with MS participated in the study. METHODS: Interrater and test-retest reliability and construct validity were assessed. RESULTS: Kappa and weighted kappa values for the items of the trunk assessment of the MDST ranged from. 74 to. 95, and the kappa and weighted kappa values for the TIS items ranged from. 46 to 1. 00. Intraclass correlation coefficients for <b>interrater</b> and test-retest <b>agreement</b> were. 93 and. 92, respectively, for the trunk assessment of the MDST and. 97 and. 95, respectively, for the TIS. Bland-Altman analysis showed consistency of scores without observer bias. Construct validity was established. DISCUSSION AND CONCLUSION: The MDST and TIS provide reliable assessments of the trunk and are valid scales for measuring trunk performance in people with MS. status: publishe...|$|R
40|$|Objectives - To {{determine}} {{the frequency of}} incidental maxillary sinus findings using cone-beam computed tomography (CBCT) images made for orthodontic purposes. Setting and Sample Population - One hundred thirty-nine consecutive CBCTs from 134 patients treated at the Department of Orthodontics and Dentofacial Orthopedics, University of Bern, Bern, Switzerland. Indications for CBCT imaging included the localization of impacted teeth and root resorption related to impacted teeth. Material and Methods Population - Two experienced observers reviewed the CBCT scans (fields of view: 4 × 4 / 6 × 6 / 8 × 8 cm) and recorded all incidental maxillary sinus findings according to standardized categories. The patient's age and gender, {{the size of the}} field of view, the season of CBCT image taking, and the thickness of the Schneiderian membrane were evaluated to identify potential influencing factors. Results - In 65 CBCTs (46. 8 %), incidental maxillary sinus findings were found (<b>interrater</b> classification <b>agreement</b> of 95. 7 %/ 95 % CI: 90. 9 - 97. 9 %). Three types of incidental findings were diagnosed: flat mucosal thickening (23. 7 %), polypoid mucosal thickening (19. 4 %), and signs of acute sinusitis (3. 6 %). There was no correlation between the field of view of the CBCT and the number of incidental findings inside the field. There was no correlation between the season during which the CBCT was made and the number of incidental findings. The mean thickness of the mucosal lining in the maxillary sinus was 1. 58 mm (95 % CI: 1. 17 - 1. 98 mm). Conclusions - A high percentage of the CBCTs made for orthodontic diagnostic purposes exhibit incidental maxillary sinus findings not associated with the primary indication. © 2010 John Wiley & Sons A/S. Link_to_subscribed_fulltex...|$|R
40|$|Objective: Medical {{ultrasound}} examinations {{are performed}} by diverse professional cohorts: sonographers are one group. Little evidence exists regarding the teaching practices used in medical ultrasonography and their effectiveness. We report the continued development and validation of {{an instrument to}} measure sonographer skill-teaching practice perceptions (SonoSTePs). Methods: An online survey was administered to {{a convenience sample of}} sonographers who were employed in Queensland, Australia. This paper reports on the continued psychometric testing of the measurement tool. Findings: The 25 -item scale demonstrated good internal reliability. Exploratory factor analysis generated four factors with acceptable internal reliability: Factor 1 (Skill execution feedback, Cronbach’s α= 0. 89), Factor 2 (Cognitive overload, Cronbach’s α= 0. 68), Factor 3 (Teach new skill, Cronbach’s α= 0. 70), and Factor 4 (Assist learners scanning, Cronbach’s α= 0. 67). The combined instrument value was 0. 83. The weighted kappa of the test–retest items identified that the majority of items achieved an <b>interrater</b> level of <b>agreement</b> of ≥ 0. 5. Conclusion: Results indicate that the SonoSTePs instrument items and factors are underpinned by theories and principles related to teaching a complex psychomotor skill. The initial data suggest that the tool is both reliable and valid...|$|R
