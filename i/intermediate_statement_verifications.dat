0|65|Public
40|$|AbstractIn {{relational}} semantics, the input-output semantics of {{a program}} is a relation on its set of states. We generalise this in considering elements of Kleene algebras as semantical values. In a nondeterministic context, the demonic semantics is calculated by considering the worst behaviour of the program. In this paper, we concentrate on while loops. Calculating the semantics of a loop is difficult, but showing the correctness of any candidate abstraction is much easier. For deterministic programs, Mills has described a checking method known as the while <b>statement</b> <b>verification</b> rule. A corresponding programming theorem for nondeterministic iterative constructs is proposed, proved and applied to an example. This theorem {{can be considered as}} a generalisation of the while <b>statement</b> <b>verification</b> rule to nondeterministic loops. The paper generalises earlier relation-algebraic work to the setting of modal Kleene algebra, an extension of Kozen’s Kleene algebra with tests that allows the internalisation of weakest liberal precondition and strongest liberal postcondition operators...|$|R
40|$|This study {{adds to the}} {{existing}} literature {{on the ability to}} understand irony of typically developing versus gifted students (aged 12 – 15). In addition to the canonical condition of polarized statements applied to oppositely polarized situations, we also considered the case of <b>intermediate</b> <b>statements</b> and situations. The results showed {{a significant difference between the}} two groups of participants. Both groups recognized an ironic interpretation in the more usual condition of a polarized statement applied to a clearly oppositely polarized situation and they also grasped the idea that the bigger the contrast, the more ironic the message. However, gifted students demonstrated greater mastery, with regard to both polarized and <b>intermediate</b> <b>statements.</b> They also demonstrated greater ability compared with their non-gifted peers in the task which required them to explain the “rule” underlying the conditions which applied to the comments they had judged as ironic and to then produce ironic stories demonstrating the specificity of irony (not to be confused with generic humor) ...|$|R
40|$|Fitness {{functions}} derived {{for certain}} white-box test goals can cause problems for Evolutionary Testing (ET), {{due to a}} lack of sufficient guidance to the required test data. Often this is because the search does not take into account data dependencies within the program, and the fact that some special <b>intermediate</b> <b>statement</b> (or statements) needs to have been executed in order for the target structure to be feasible. This paper proposes a solution which combines ET with the Chaining Approach. The Chaining Approach is a simple [...] ...|$|R
40|$|Cut-elimination is {{the most}} {{prominent}} form of proof transformation in logic. The elimination of cuts in formal proofs corresponds to the removal of <b>intermediate</b> <b>statements</b> (lemmas) in mathe-matical proofs. Cut-elimination {{can be applied to}} mine real mathematical proofs, i. e. for extracting explicit and algorithmic information. The system CERES (cut-elimination by resolution) is based on automated deduction and was successfully applied to the analysis of nontrivial mathematical proofs. In this paper we focus on the input-output environment of CERES, and show how users can interact with the system and extract new mathematical knowledge. ...|$|R
5000|$|Abraham Saba {{is not to}} be confounded with R. Abraham Saba of Adrianople, who is {{mentioned}} in the responsa of R. Elijah Mizraḥi, No. 52. Note: This last <b>statement</b> requires <b>verification,</b> since the title page to Ẓeror ha-Mor clearly states that its author resided in Adrianpolou, see here: http://hebrewbooks.org/30801.|$|R
50|$|In 2014, EMEC {{expanded}} its scope attaining the International Standard ISO/IEC 17020 for verification {{of the performance}} of new environmental technologies. Open to energy technologies, water treatment and monitoring technologies, and materials, waste and resources, EMEC-ETV (Environmental Technology Verification) can help innovative technologies reach the market via the provision of a <b>Statement</b> of <b>Verification.</b>|$|R
50|$|The {{anthropologist}} John Alden Mason {{has described}} Wilkins's research as pseudohistory {{and noted that}} most of his <b>statements</b> capable of <b>verification</b> turned out to be incorrect.|$|R
40|$|We {{present a}} novel {{technique}} called comparison checking that helps optimizer writers debug optimizers by testing, for given inputs, that the semantics {{of a program}} are not changed by the application of optimizations. We have successfully applied comparison checking to a large class of program transformations that alter (1) the relative ordering in which values are computed by the <b>intermediate</b> code <b>statements,</b> (2) {{the form of the}} <b>intermediate</b> code <b>statements,</b> and (3) the control flow structure using code replication. We outline the key steps that lead to the automation of comparison checking. The application of comparison checking to test the implementations of high level loop transformations, low level code optimizations, and global register allocation for given program inputs is then described...|$|R
40|$|Abstract. Cut-elimination is {{the most}} {{prominent}} form of proof trans-formation in logic. The elimination of cuts in formal proofs corresponds to the removal of <b>intermediate</b> <b>statements</b> (lemmas) in mathematical proofs. The cut-elimination method CERES (cut-elimination by resolu-tion) works by extracting a set of clauses from a proof with cuts. Any resolution refutation of this set then serves as a skeleton of an ACNF, an LK-proof with only atomic cuts. The system CERES, an implementation of the CERES-method has been used successfully in analyzing nontrivial mathematical proofs (see [4]). In this paper we describe the main features of the CERES system with spe-cial emphasis on the extraction of Herbrand sequents and simplification methods on these sequents. We demonstrate the Herbrand sequent ex-traction and simplification by a mathematical example. ...|$|R
40|$|AbstractWe {{present a}} novel {{technique}} called comparison checking that helps optimizer writers debug optimizers by testing, for given inputs, that the semantics {{of a program}} are not changed by the application of optimizations. We have successfully applied comparison checking to a large class of program transformations that alter (1) the relative ordering in which values are computed by the <b>intermediate</b> code <b>statements,</b> (2) {{the form of the}} <b>intermediate</b> code <b>statements,</b> and (3) the control flow structure using code replication. We outline the key steps that lead to the automation of comparison checking. The application of comparison checking to test the implementations of high level loop transformations, low level code optimizations, and global register allocation for given program inputs is then described...|$|R
40|$|Cut-elimination is {{the most}} {{prominent}} form of proof transformation in logic. The elimination of cuts in formal proofs corresponds to the removal of <b>intermediate</b> <b>statements</b> (lemmas) in mathematical proofs. The cut-elimination method CERES (cut-elimination by resolution) works by constructing a set of clauses from a proof with cuts. Any resolution refutation of this set then serves as a skeleton of an LK-proof with only atomic cuts. In this paper we present an extension of CERES to a calculus LKDe which is stronger than the Gentzen calculus LK (it contains rules for introduction of definitions and equality rules). This extension makes {{it much easier to}} formalize mathematical proofs and increases the performance of the cut-elimination method. The system CERES already proved efficient in handling very large proofs...|$|R
40|$|In {{relational}} semantics, the input-output semantics of {{a program}} is a relation on its set of states. We generalize this in considering elements of Kleene algebras as semantical values. In a nondeterministic context, the demonic semantics is calculated by considering the worst behavior of the program. In this paper, we concentrate on while loops. Calculating the semantics of a loop is difficult, but showing the correctness of any candidate abstraction is much easier. For deterministic programs, Mills has described a checking method known as the while <b>statement</b> <b>verification</b> rule. A corresponding programming theorem for nondeterministic iterative constructs is proposed, proved and applied to an example. This theorem {{can be considered as}} a generalization of the while statement erification rule to nondeterministic loops. The paper generalizes earlier relation-algebraic work to the setting of modal Kleene algebra, an extension of Kozen's Kleene algebra with tests that allows the internalization of weakest liberal precondition and strongest liberal postcondition operators...|$|R
40|$|This {{study of}} {{language}} development {{was intended to}} chart the developmental course of the spontaneous use of negatives and affirmatives by 1 1 / 2 - 3 -year-olds in response to true or false statements concerning familiar objects, properties, and actions. Forty children, 18, 24, 30, and 36 months of age, were assessed fol. : knowledge of the words used in the experiment and were then given a series of 48 <b>statement</b> <b>verification</b> tasks. Stimuli were simple existential statements of either true affirmatives ("That is a ball" about a ball), false affirmatives ("That is a car " about a ball), false negatives ("That is not a ballu'about a ball), or true negatives ("That is not a car " about a ball). All sessions were videotaped and coded for affirmatives and negatives used by the children, for use of referent words (e. g. ball and car), and for nonverbal measures that distinguished comprehension strategies...|$|R
40|$|Abstract. Fitness {{functions}} derived {{for certain}} white-box test goals can cause problems for Evolutionary Testing (ET), {{due to a}} lack of sufficient guidance to the required test data. Often this is because the search does not take into account data dependencies within the program, and the fact that some special <b>intermediate</b> <b>statement</b> (or statements) needs to have been executed in order for the target structure to be feasible. This paper proposes a solution which combines ET with the Chaining Approach. The Chaining Approach is a simple method which probes the data dependencies inherent to the test goal. By incorporating this facility into ET, the search can be directed into potentially promising, unexplored areas of the test object’s input domain. Encouraging results were obtained with the hybrid approach for seven programs known to originally cause problems for ET. ...|$|R
40|$|The {{original}} publication {{is available}} at www. springerlink. comInternational audienceCut-elimination is the most prominent form of proof trans- formation in logic. The elimination of cuts in formal proofs corresponds to the removal of <b>intermediate</b> <b>statements</b> (lemmas) in mathematical proofs. The cut-elimination method CERES (cut-elimination by resolu- tion) works by extracting a set of clauses from a proof with cuts. Any resolution refutation of this set then serves as a skeleton of an ACNF, an LK-proof with only atomic cuts. The system CERES, an implementation of the CERES-method has been used successfully in analyzing nontrivial mathematical proofs (see [4]). In this paper we describe the main features of the CERES system with spe- cial emphasis on the extraction of Herbrand sequents and simplification methods on these sequents. We demonstrate the Herbrand sequent ex- traction and simplification by a mathematical example...|$|R
40|$|Abstract. Debugging OWL ontologies can be aided with {{automated}} reasoners {{that generate}} entailments, including undesirable ones. This information is, however, only useful if developers {{understand why the}} entailments hold. To support domain experts (with limited knowledge of OWL), we are developing a system that explains, in English, why an entailment follows from an ontology. In planning such explanations, our system starts from a justification of the entailment and constructs a proof tree including <b>intermediate</b> <b>statements</b> that link the justification to the entailment. Proof trees are constructed from a set of intuitively plausible deduction rules. We here report on a study in which we collected em-pirical frequency data on the understandability of the deduction rules, resulting in a facility index for each rule. This measure forms the basis for making a principled choice among alternative explanations, and identi-fying steps in the explanation {{that are likely to}} require extra elucidation...|$|R
40|$|Solving {{problems}} in many real-world domains requires integrating knowledge from several past experiences. This integration requires the dynamic retrieval of multiple experiences and {{the extraction of}} their relevant subparts. Our solution is the Multi-Plan Adaptor (MPA), a method for merging partial-order plans {{in the context of}} case-based least-commitment planning. MPA provides this ability by extracting an <b>intermediate</b> goal <b>statement</b> from a partial plan, clipping a stored plan to the <b>intermediate</b> goal <b>statement,</b> and then splicing the clipping into the original partial plan. MPA is implemented in the NICOLE multistrategy reasoning system, where it is paired with MOORE, an asynchronous, resource-bounded memory module. MOORE initially retrieves its current "best guess" but continues search, spontaneously returning a better retrieval as soon as it is found. 1. Introduction Taking advantage of past experiences is the foundation of case-based reasoning. When confronted with a problem, a c [...] ...|$|R
40|$|We {{study the}} (∞, 1) -category of autoequivalences of ∞-operads. Using {{techniques}} introduced by Toën, Lurie, and Barwick and Schommer-Pries, we prove that this (∞, 1) -category is a contractible ∞-groupoid. Our calculation {{is based on}} the model of complete dendroidal Segal spaces introduced by Cisinski and Moerdijk. Similarly, we prove that the (∞, 1) -category of autoequivalences of non-symmetric ∞-operads is the discrete monoidal category associated to Z/ 2 Z. We also include a computation of the (∞, 1) -category of autoequivalences of (∞, n) -categories based on Rezk's Θ_n-spaces. Comment: 43 pages, v 2 : updated according to the revised version of [BSP 13], minor correction in the proof of Proposition 3. 5. 3, v 3 : journal version, minor changes, numbering has changed, v 4 : minor corrections in Section 4 due to a mistake in the previous Remark 4. 3. 2, some <b>intermediate</b> <b>statements</b> slightly changed, none of the main results are affected, numbering has not change...|$|R
40|$|We {{present a}} {{formulation}} of quantum circuits where {{the focus is}} set on whether a given circuit (made of unitary operators and projective measurements with definite outcomes) does reflect an actually realizable physical experiment. In order to do this, we introduce <b>verifications</b> <b>statements</b> which are purely epistemic assertions indicating whether a outcome is possible at some point and develop our formalism which, in the end, consists {{in a set of}} logical rules about <b>verification</b> <b>statements.</b> Finally, we argue that our formalism provides a Lorentz-invariant realistic formulation of quantum circuits and illustrate this by considering a circuit corresponding to Hardy's paradox and showing how our formalism prevents making contradictory assertions regarding our knowledge about the circuit...|$|R
40|$|Modern energy {{efficient}} building is heated and cooled by five rows of flat plate solar collectors; its {{domestic hot water}} needs are also met. Final report includes detailed drawings and photographs, manufacturer's literature, performance specifications, acceptance test data, and performance <b>verification</b> <b>statements.</b> Operation and maintenance manual is also attached...|$|R
40|$|Cut-elimination is {{the most}} {{prominent}} form of proof transformation in logic. The elimination of cuts in formal proofs corresponds to the removal of <b>intermediate</b> <b>statements</b> (lemmas) in mathematical proofs. The cut-elimination method CERES (cut-elimination by resolution) works by constructing a set of clauses from a proof with cuts. Any resolution refutation of this set can then serve as a skeleton of a proof with only atomic cuts. In this paper we present a systematic experiment {{with the implementation of}} CERES on a proof of reasonable size and complexity. It turns out that the proof with cuts can be transformed into two mathematically different proofs of the theorem. In particular, the application of positive and negative hyperresolution yield different mathematical arguments. As an unexpected side-effect the derived clauses of the resolution refutation proved particularly interesting as they can be considered as meaningful universal lemmas. Though the proof under investigation is intuitively simple, the experiment demonstrates that new (and relevant) mathematical information on proofs can be obtained by computational methods. It can be considered as a first step in the development of an experimental culture of computer-aided proof analysis in mathematics...|$|R
40|$|One {{advantage}} of studying history is to explain present practice {{or at least}} to help place current phenomena in perspective. This paper seeks to explore two related themes which have proved problematic from the earliest times of company auditing in the UK: the nature of the auditor's responsibility; and the public's perception of his role. The conventional view is that auditors were initially concerned mainly with fraud detection, and that {{it was not until the}} 1930 s that greater emphasis was devoted to the <b>verification</b> of financial <b>statements.</b> This study suggests that <b>statement</b> <b>verification</b> was the primary audit concern in relation to public companies as early as the 1830 s, though we acknowledge that later in the century more emphasis was placed on fraud detection. We therefore see the current debate over the auditor's responsibility as merely the latest movement in a continuing and fluctuating theme. We also show that the profession has encountered great difficulty in reconciling public expectations with the practicalities of auditing. General confusion over the role of the auditor has existed to such an extent that it has been difficult even for the profession to reach agreement on the main purpose of company auditing, and the message to be sent to the investing public. In these endeavours, the accounting profession was at the same time both helped and hindered by legal developments...|$|R
40|$|The {{effect of}} {{stimulus}} exposure in semantic verification task {{within the framework}} of two levels of complexity of experimental context was examined in nine experiments. The complexity of experimental context was defined in terms of number of constitutive elements of a statement (subject, predicate and copula) which vary in the course of an experiment. Four experiments with different exposure duration have been performed {{within the framework of}} minimal complexity (where only one element of statement varies), while five experiments with different exposure duration have been conducted within the framework of medium complexity (where two elements of statement vary). Systematic manipulation of the two variables (i. e. complexity and exposure duration) provides variation of processing load in verification process, thus influencing the efficiency of working memory during the verification task. The obtained results indicate that the cognitive load variation (defined in terms of variation of exposure duration) affects processing of subject-predicate congruency, which is an obligatory step in <b>statement</b> <b>verification.</b> The shorter the exposure duration, the bigger the difference in processing latencies between “congruent” and “incongruent” statements. The effects are more pronounced for statements presented in the framework of medium complexity. Also, the reduction of exposure duration affects the error rate as well as processing of statement’s quality (i. e. affirmative vs. negative form of a statement) ...|$|R
30|$|We finally {{mention that}} the {{verification}} theorem also holds on our control problem, i.e., {{the solution of the}} HJB equation is the value function of our control problem. The proof is standard and it heavily depends on the bounded solutions to the HJB equations discussed above. Hence, we omit the <b>statement</b> of the <b>verification</b> theorem and the proof.|$|R
40|$|International audienceWhen {{we follow}} a route in an unknown city, we build a mental {{representation}} of the environment, which is stored in long-term memory and could be used later in different kind of tasks like recognizing the place, redoing the route, or drawing a map. In this experiment, {{we focus on the}} cognitive resources that are used to build and then manipulate that mental representation, and the effect of aging on the spatial representation. Previous research has demonstrated age-related decline in the use of spatial representation, but the spatial representation is complex and requires the integration of many components that may show differential susceptibility to aging. Different tasks were used to assess the effect of aging on different components of the spatial representation. The video of an unfamiliar route in a city near Paris was projected to the young and older participants twice. After the presentation, four tasks were used to assess the mental {{representation of the}} participants (visual recognition task, direction decision task, order decision task, and <b>statement</b> <b>verification).</b> Different tests were used to identify the factors that explain individual differences in the performance. The results show that older adults have poorer performance than younger adults in the 4 tasks. Analyses of mediation show that the effect of aging on visual recognition task would be explained by the impairment of episodic memory. Moreover, the effect of aging on the direction task would be explained by the impairment of working memory...|$|R
50|$|Dyna-Flites {{has been}} {{credited}} with being the first brand that had commercial airlines license them to produce die-cast models for them.(This <b>statement</b> needs <b>verification).</b> Among the airlines that had models released by Dyna-Flites included FedEx, Delta Air Lines, Eastern Air Lines, Pan Am, TWA and Braniff. Additional airlines in the Dyna-Flite range included Austrian, Hawaiian, ANA, Japan Airlines, BOAC, United Airlines CP Airlines, Western Airlines, KLM, British Airways and more.Most of the airline issues date from a time when licensing from the airline for use of their name was usually an overlooked matter! However, ZEE did issue an authorized gift set for United Air Lines in 1993.|$|R
40|$|Cut-elimination, {{introduced}} by Gentzen, {{plays an important}} role in automating the analysis of mathematical proofs. The removal of cuts corresponds to the elimination of <b>intermediate</b> <b>statements</b> (lemmas), resulting in an analytic proof. CERes is a method of cut-elimination by resolution that relies on global proof transformations, in contrast to reductive methods, which use local proof-rewriting transformations. By avoiding redundant operations, it obtains a speed-up over Gentzen 2 ̆ 7 s traditional method (and its variations). CERes has been successfully implemented and applied to mathematical proofs, and it is fully developed for classical logic (first and higher order), multi-valued logics and Gödel logic. But when it comes to mathematical proofs, intuitionistic logic also {{plays an important role}} due to its constructive characteristics and computational interpretation. This paper presents current developments on adapting the CERes method to intuitionistic sequent calculus LJ. First of all, we briefly describe the CERes method for classical logic and the problems that arise when extending the method to intuitionistic logic. Then, we present the solutions found for the mentioned problems for the subclass LJ- (the class of intuitionistic proofs of an end-sequent containing no strong quantifiers and no formula on the right). In addition, we explain, with an example, some ideas for improving the method and covering a bigger fragment of LJ proofs. Finally, we summarize the results and point the direction for future research...|$|R
40|$|The Rising Above the Gathering Storm report (National Academy of Sciences, 2007) {{emphasizes}} a {{need for}} improved science education in United States schools. Instruction, informed by assessment, has been repeatedly demonstrated to be effective for increasing students 2 ̆ 7 performance. In particular, the use of curriculum-based measurement (CBM) to assist with making screening decisions {{has been shown to}} increase the likeliness of students meeting meaningful outcomes. While CBM tools for assisting with making screening decisions in reading, mathematics, and written language have been well examined, tools for use in content areas (e. g., science and social studies) remain in the beginning stages of research. In this study, two alternate forms of a new CBM tool, <b>Statement</b> <b>Verification</b> for Science (SV-S), for assisting with making screening decisions regarding students 2 ̆ 7 science content knowledge is examined for technical adequacy. A total of 1, 545 students across Grades 7 (N = 799) and 8 (N = 746) completed Forms A and B of SV-S the week prior to, and within two weeks after, a statewide high-stakes test of accountability including Science, Reading, and Mathematics. Obtained data were used in order to examine internal consistency, test-retest with alternate forms reliability, and evidence of criterion- and construct-related validity. Promising results were found for reliability, in particular internal consistency, while results related to evidence of criterion- and construct-related validity were less than desired. Such results, along with additional exploratory analyses, provide support for future research of SV-S as a CBM tool to assist teachers and other educators with making screening decisions...|$|R
40|$|National Laboratory, the {{verification}} effort described herein. This {{report has}} been peer and administratively reviewed {{and has been}} approved for publication as an EPA document. Mention of trade names or commercial products does not constitute endorsement or recommendation for use of a specific product. EPA-VS-SCM- 13 The accompanying notice {{is an integral part}} of this <b>verification</b> <b>statement</b> August 1998 iii TECHNOLOGY TYPE: POLYCHLORINATED BIPHENYL (PCB) FIELD ANALYTICA...|$|R
40|$|Abstract Most {{verifications}} of out-of-order microprocessors compare state-machine-based implementations and specifications, {{where the}} specification {{is based on}} the instruction-set architecture. The different efforts use a variety of correctness <b>statements,</b> implementations, and <b>verification</b> approaches. We present a framework for classifying correctness statements about safety that is independent of implementation representation and verification approach. We characterize the relationships between the different statements and illustrate how existing and classical approaches fit within this framework. ...|$|R
50|$|Electronic Verification System (eVS) is the Postal Service's {{integrated}} mail management {{technology that}} centralizes payment processing and electronic postage reports. Part of an evolving suite of USPS electronic payment services called PostalOne!, eVS allows mailers shipping {{large volumes of}} parcels through the Postal Service a way to circumvent use of hard-copy manifests, postage <b>statements</b> and drop-shipment <b>verification</b> forms. Instead, mailers can pay postage automatically through a centralized account and track payments online.|$|R
40|$|Information {{processing}} {{is generally}} biased toward global cues, {{often at the}} expense of local information. Equivocal extant data suggests that arousal states may accentuate either a local or global processing bias, at least partially dependent {{on the nature of the}} manipulation, task and stimuli. To further differentiate the conditions responsible for such equivocal results we varied caffeine doses to alter physiological arousal states and measured their effect on tasks requiring the retrieval of local versus global spatial knowledge. In a double-blind, repeated-measures design, non-habitual (Exp. 1; N= 36, M= 42. 5 &# 177; 29 mg/day caffeine) and habitual (Exp. 2; N= 34, M= 579. 5 &# 177; 311. 5 mg/day caffeine) caffeine consumers completed four test sessions corresponding to each of four caffeine doses (0 mg, 100 mg, 200 mg, 400 mg). During each test session, participants consumed a capsule containing one of the three doses of caffeine or placebo, waited sixty minutes, and then completed two spatial tasks, one involving memorizing maps and one spatial descriptions. A spatial <b>statement</b> <b>verification</b> task tested local versus global spatial knowledge by differentially probing memory for proximal versus distal landmark relationships. On the map learning task, results indicated that caffeine enhanced memory for distal (i. e. global) compared to proximal (i. e. local) comparisons at 100 (marginal), 200, and 400 mg caffeine in non-habitual consumers, and marginally beginning at 200 mg caffeine in habitual consumers. On the spatial descriptions task, caffeine enhanced memory for distal compared to proximal comparisons beginning at 100 mg in non-habitual but not habitual consumers. We thus provide evidence that caffeine-induced physiological arousal amplifies global spatial processing biases, and these effects are at least partially driven by habitual caffeine consumption...|$|R
30|$|The current {{technology}} {{allows the}} integration {{on a single}} die of complex systems-on-chip (SoCs) that are composed of manufactured blocks (IPs), interconnected through specialized networks on chip (NoCs). IPs have usually been validated by diverse techniques (simulation, test, formal verification) and the key problem remains the validation of the communication infrastructure. This paper addresses the formal verification of NoCs {{by means of a}} mechanized proof tool, the ACL 2 theorem prover. A metamodel for NoCs has been developed and implemented in ACL 2. This metamodel satisfies a generic correctness <b>statement.</b> Its <b>verification</b> for a particular NoC instance is reduced to discharging a set of proof obligations for each one of the NoC constituents. The methodology is demonstrated on a realistic and state-of-the-art design, the Spidergon network from STMicroelectronics.|$|R
40|$|This {{thesis is}} {{dedicated}} to the automatic and formal verification of the heap propertiesofobjectorientedprograms. Programverificationisthecheckthat a given program satisfies given properties. Program verification is called formal if both the semantics of the specifications and the program execution are defined formally as mathematical entities. The verification is called automatic if it is performed automatically without interaction or with limited interaction with a user. Our approach is targeted towards the verification of the preservation of heap-topological properties. It is also aimed towards the verification of the effects and the frame properties of the program <b>statements.</b> Automatic <b>verification</b> of heap structures is crucial for the verification of multi-object invariants, the verification of concurrent programs (e. g., absence of race conditions and deadlocks), software engineering (e. g., enabling encapsulation and modular development, handlingdesign patterns), and the verification of security properties (e. g., isolation) ...|$|R
40|$|Teenage {{isolation}} and alienation carry negative consequences. Adolescents who work through {{struggles of the}} self, however, can achieve self and social identity. This grounded theory study seeks to understand the process of adolescent choral singers’ social identity development within three mid-sized, Midwestern high school choral ensembles. Forty-nine total {{interviews were conducted with}} thirty-six different Mixed Choir participants. Data were openly, axially and selectively coded. Analytic products included categories with dimensionalized examples, a temporal matrix as well as fourteen propositional <b>statements.</b> <b>Verification</b> procedures included using peer review, writing analytic and reflective memos, engaging participants in member checks, and triangulating data sources. Results indicate the presence of three contextual conditions, including the time spent in rehearsal, both week-to-week and year-to-year, the range of intensities from rehearsal to performance, {{and the size of the}} singing group. The core phenomenon of social identity development emerged as “team,” with three supportive categories: (1) “everyone is there for one reason,” (2) “we will all be together,” and (3) “musical family. ” Actions and interactions that contribute to the core phenomenon of “team” included the decision to audition, being chosen to be in Mixed Choir by the choral director and singing with others. Intervening conditions include the presence of cliques and student egos within Mixed Choir. Feelings of acknowledgement and being accomplished as well as strong feelings of pride developed as consequences. At the end of the developmental continuum, results indicate that choral membership has bolstered members’ sense of self. Increased self-concept encourages participants to give back in the forms of leadership and performance as well as engaging in the larger choral legacy at their schools. Future recommendations for research include investigating the relationships between music identity development and in-school music participation, examining the experiences of students who are not accepted into auditioned choirs, and studying the influence of choral programs on the social identity development of special needs’ students. ...|$|R
40|$|Developing {{effective}} debugging {{strategies to}} guarantee the reliability of software is important. By analyzing the debugging process used by experienced programmers, four distinct tasks {{are found to be}} consistently performed: (1) determining statements involved in program failures, (2) selecting suspicious statements that might contain faults, (3) making hypotheses about suspicious faults (variables and locations), and (4) restoring program state to a specific <b>statement</b> for <b>verification.</b> If all four tasks could be performed with direct assistance from a debugging tool, the debugging effort would become much easier. We have built a prototype debugging tool, Spyder, to assist users in conducting the first and last tasks. Spyder executes the first task by using dynamic program slicing and the fourth task by backward execution. This research focuses on the second task, reducing the search domain containing faults, referred to as fault localization. Several heuristics are presented here bas [...] ...|$|R
