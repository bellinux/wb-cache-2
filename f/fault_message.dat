9|104|Public
50|$|Out-Only: The {{reverse of}} In-Only. It {{primarily}} supports event notification. It cannot trigger a <b>fault</b> <b>message.</b>|$|E
5000|$|Robust Out-Only: Similar to the out-only pattern, {{except it}} can trigger a <b>fault</b> <b>message.</b> The {{outbound}} message initiates the transmission.|$|E
5000|$|The {{remainder}} of the messages occurred from 02:11 UTC to 02:14 UTC, containing a <b>fault</b> <b>message</b> for an Air Data Inertial Reference Unit (ADIRU) and the Integrated Standby Instrument System (ISIS). At 02:12 UTC, a warning message NAV ADR DISAGREE {{indicated that there was}} a disagreement between the three independent air data systems. At 02:13 UTC, a <b>fault</b> <b>message</b> for the flight management guidance and envelope computer was sent. One of the two final messages transmitted at 02:14 UTC was a warning referring to the air data reference system, the other ADVISORY was a [...] "cabin vertical speed warning", indicating that the aircraft was descending at a high rate.|$|E
5000|$|Fault {{endpoint}} -- the endpoint {{to which}} <b>fault</b> <b>messages</b> should be dispatched (EPR) ...|$|R
5000|$|RS (Reconciliation Sublayer) - This {{sublayer}} processes PHY Local/Remote <b>Fault</b> <b>messages</b> and handles DDR conversion ...|$|R
40|$|Locomotives, {{like many}} modern complex machines, are {{equipped}} with the capability to generate on-board <b>fault</b> <b>messages</b> indicating the presence of anomalous conditions. Such messages tend to generate in large quantities and difficult and time consuming to interpret manually. This paper presents the design and development of a case-based reasoning system for diagnosing locomotive faults using such <b>fault</b> <b>messages</b> as input. The process of using historical repair data and expert input for case generation and validation is described. An algorithm for case matching is presented along with some results on pilot data. 1 Introduction There is a recent move in industry towards supporting equipment servicing {{as a means of}} augmenting traditional revenue sources such as those generated by equipment sales with limited warrenties and subsequent parts supply. This is especially applicable in the case of heavy machinery which due to its design complexity is often best serviced by the manufactur [...] ...|$|R
50|$|Service Stubbing with Rational Service Tester for SOA Quality {{enables the}} testing of your SOA {{application}} prior {{to the creation of}} all services. Test stubs are automatically generated from WSDL files and will be automatically deployed before test execution. After the generation of service stubs, users can modify stub behavior to meet specific testing scenarios. For example, simulation of error conditions such as an external service being unavailable or answering with a <b>fault</b> <b>message</b> are typical uses.|$|E
40|$|In {{the domain}} of modular robotic systems, self-configuration, {{self-diagnosis}} and self-repair {{are known to be}} highly challenging tasks. This paper presents a novel fault self-diagnosis strategy which consists of two parts: fault detection and <b>fault</b> <b>message</b> transmission. In fault detection, a bionic synchronization ‘healthy heartbeat’ method is used to guarantee the high efficiency of the exogenous detection strategy. For <b>fault</b> <b>message</b> transmission, the Dijkstra method is modified to be capable of guiding the passage of fault messages along the optimal path. In a modular robotic system, <b>fault</b> <b>message</b> transmission depends mainly on local communications between adjacent modules, so {{there is no need for}} global broadcast information. Computational simulations of one system form, M-Lattice, have demonstrated the practical effectiveness of the proposed strategy. The strategy should be applicable in modular robotic systems in general...|$|E
40|$|The {{proposed}} {{model is}} to configure Byzantine Fault Tolerance mechanism for every SOAP <b>fault</b> <b>message</b> that is transmitted. The {{reliability and availability}} are of major requirements of Web services since they operate in the distributed environment. One of the reliability issues is handling faults. Fault occurs in all the phases of Service Oriented Architecture i. e. during publishing, discovery, composition, binding, and execution. These faults may lead to service downtime, behaves abnormally, and may send incorrect responses. These abnormalities are classified as Byzantine faults in Web services. Even though SOAP specification provides fault handling mechanisms, the correctness of the received SOAP fault messages are not known. In this paper, a model is proposed to check the correctness of the SOAP <b>fault</b> <b>message</b> received, by incorporating the Byzantine agreement for fault tolerance. The existing fault tolerant mechanism detects server failure and routes the request to the next available server without {{the knowledge of the}} client. The proposed model ensures a transparent environment by providing fault handling information to the client. This is achieved by incorporating an active replication technique...|$|E
50|$|The {{forward and}} reverse paths used for unicast and {{multicast}} traffic in an IEEE 802.1aq network are symmetric. This symmetry permits the normal Ethernet Continuity <b>Fault</b> <b>Messages</b> (CFM) IEEE 802.1ag to operate unchanged for SPBV and SPBM and has desirable properties {{with respect to}} time distribution protocols such as Precision Time Protocol (PTP Version 2). Also existing Ethernet loop prevention is augmented by loop mitigation to provide fast data plane convergence.|$|R
5000|$|The quasi-opportunistic {{paradigm}} aims {{to overcome}} this by achieving {{more control over the}} assignment of tasks to distributed resources and the use of pre-negotiated scenarios for the availability of systems within the network. Quasi-opportunistic distributed execution of demanding parallel computing software in grids focuses on the implementation of grid-wise allocation agreements, co-allocation subsystems, communication topology-aware allocation mechanisms, <b>fault</b> tolerant <b>message</b> passing libraries and data pre-conditioning. In this approach, <b>fault</b> tolerant <b>message</b> passing is essential to abstractly shield against the failures of the underlying resources.|$|R
40|$|A multi-layered {{architecture}} of self-organizing neural networks {{is being developed}} {{as part of an}} intelligent alarm processor to analyse a stream of power grid <b>fault</b> <b>messages</b> and provide a suggested diagnosis of the fault location. Feedback concerning the accuracy of the diagnosis is provided by an object-oriented grid simulator which acts as an external supervisor to the learning system. The utilization of artificial neural networks within this environment should result in a powerful generic alarm processor which will not require extensive training by a human expert to produce accurate results...|$|R
40|$|Abstract. This paper {{applies the}} {{empirical}} mode decomposition (EMD) methods to gearbox vibration signal analysis capture from vibrating acceleration sensor for gearbox fault diagnosis. The original modulation fault vibration signals are firstly decomposed {{into a number}} of intrinsic mode function (IMF) by the EMD method. Then the fault information diagnosis of the gearbox vibration signals can be extracted from the coefficient-energy value of intrinsic mode function. Experiment result has shown the feasibility and efficiency of the EMD algorithms and energy characteristic method in fault diagnosis and <b>fault</b> <b>message</b> abstraction. It is significant for the monitor operating state of gearbox and detects incipient faults as soon as possible...|$|E
40|$|Deployment of {{multicast}} routing {{services in}} corporate networks and Internet Service Providers is still tentative. Among other problems, {{there is a}} lack of monitoring and management tools and systems. Previous work in multicast management has failed to address the scalability problem present in multicast fault isolation and reporting. We propose a hierarchical, passive monitoring scheme, HPMM, that relies on a series of pre-deployed, self-organized monitoring daemons. With HPMM, <b>fault</b> <b>message</b> aggregation and local fault detection and isolation is more ecient than previous approaches. HPMM satises a number of design goals: scalable reporting; fault isolation; no dependencies on multicast routing for reporting; no modications to existing routing or diagnosis protocols; and ease of deployability. The tradeo of using HPMM is it leverages a large number of software daemons deployed along a local domain. We compare the performance of HPMM and previous work with a simulation. 1 In [...] ...|$|E
40|$|Abstract- Semiotic {{analysis}} {{is often used}} for describing the inter-relationship of structure, function and behavior of any artifacts as the means for designing various computerized tools for machine diagnosis and operation procedure. In this study, a graphical method called Multilevel Flow Models (MFM) is applied for supporting machine maintenance work of commercially available Micro Gas Turbine System (MGTS), to describe and handle the relationships between goals and functions that exist in various parameters of MGTS including signal, alarm and fault. A new three-step method including alarm validation, fault condition checkup and fault identification is proposed for fault diagnosis based on MFM. A trial software has been developed by using Visual C++ and Excel for monitoring and diagnosing the MGTS based on the proposed fault diagnosis method. And it was tested by several typical actual fault cases, {{to show that the}} proposed method is efficient to monitor the running state of MGTS and to diagnose the real reason of <b>fault</b> <b>message</b> from the operation software provided by the vendor of Micro Gas Turbine...|$|E
40|$|Problem {{determination}} in a Web services {{setting is}} simplified by standardizing a base set {{of information that}} may appear in <b>fault</b> <b>messages.</b> WS-BaseFaults defines an XML Schema type for base faults, along with rules for how this base fault type is used and extended by Web services. Status: This document is published by this TC as a "Committee Specification". Committee members should send comments on this specification to the wsrf@lists. oasisopen. org list. Others may submit comments to the TC via the web form found on the TC's web page a...|$|R
40|$|According to the {{requirements}} of quality inspecting for the transmission, this paper proposed an implementation method for the transmission fault diagnosis based on order tracking. The implementation steps are given in detail after analyzed the necessity of resample vibration signal. The analysis result from the transmission&# 39;s data shown that the method proposed by this paper can identify the <b>fault</b> <b>messages</b> of meshing gears and locate the fault part&# 39;s exactly location accurately. IEEE Beijing Section; Hunan University of Humanities, Science and Technology; Tongji University; Xiamen University; Central South Universit...|$|R
40|$|Consider a {{distributed}} system that delivers each message from {{a process to}} its destination if the message transmission does not experience any faults and only delivers those sent by a non-faulty system process. Such a system {{is referred to as}} a reliable message passing system. A reliable message passing system requires a reliable channel, a communication channel between a pair of processes that always detects a <b>fault</b> in <b>message</b> transmission and each detected fault is an actual fault, to be implemented. In this paper, we first identify the necessary conditions to detect some restricted form of Byzantine <b>Faults</b> in a <b>message</b> passing system where n disjoint paths exist between each pair of endpoints. We consider Byzantine Faults (BF) whose effect is limited to the modification of a <b>message</b> metadata, omission <b>faults,</b> and <b>message</b> replay. We then present a protocol implementing a reliable channel in message passing systems in the presence of n — 1 Byzantine Faults using n disjoint paths between each pair of communication endpoints where the paths with faults are not known apriori. The proposed protocol detects Byzantine Faults, where each detected fault, an actual <b>fault,</b> authenticates <b>message</b> origins, identifies faulty paths and classifies faults in the presence of multiple messages sent by various system processes...|$|R
50|$|ATA 100 {{contains}} {{the reference to}} the ATA numbering system which is a common referencing standard for all commercial aircraft documentation. This commonality permits greater ease of learning and understanding for pilots, aircraft maintenance technicians, and engineers alike. The standard numbering system was published by the Air Transport Association on June 1, 1956. While the ATA 100 numbering system has been superseded, it continued to be widely used until it went out of date back in 2015, especially in documentation for general aviation aircraft, on aircraft <b>Fault</b> <b>Messages</b> (for Post Flight Troubleshooting and Repair) and the electronic and printed manuals.|$|R
50|$|<b>Fault</b> Tolerant <b>Messaging</b> — {{the ability}} to transparently {{failover}} a call or request from one service transport protocol to another upon failure with no changes to the functional code or business logic implementation.|$|R
50|$|The quasi-opportunistic {{approach}} {{enables the}} execution of demanding applications within computer grids by establishing grid-wise resource allocation agreements; and <b>fault</b> tolerant <b>message</b> passing to abstractly shield against {{the failures of the}} underlying resources, thus maintaining some opportunism, while allowing a higher level of control.|$|R
40|$|Airlines are {{extremely}} {{sensitive to the}} amount of dollars spent on maintaining the external engine hardware in the field. Analysis reveals that many problems revolve around a central issue, reliability. Fuel and oil leakage due to seal failure and electrical <b>fault</b> <b>messages</b> due to wire harness failures {{play a major role in}} aircraft delays and cancellations (D&C's) and scheduled maintenance. Correcting these items on the line requires a large investment of engineering resources and manpower after the fact. The smartest and most cost effective philosophy is to build the best hardware the first time. The only way to do that is to completely understand and model the operating environment, study the field experience of similar designs and to perform extensive testing...|$|R
40|$|Abstract. Locomotives {{are complex}} {{electromechanical}} systems. Continuously monitoring the health state of locomotives {{is critical in}} modern cost-effective maintenance strategy. A typical locomotive is equipped with the capability to monitor their state and generate <b>fault</b> <b>messages</b> and a snapshot of sensed parametric readings in response to anomalous conditions. In our previous studies, we have developed and deployed a case-based reasoning system for locomotive diagnostics where fault codes were used as the inputs to the system. In order to increase the lead-time from detection to failure and allow for more proactive actions, one important effort in locomotive diagnostics is to perform anomaly detection on parametric operational data. In this paper, we present an anomaly detection strategy {{that is based on}} a combination of nonparametric statistical testing and machine learning methodology. We demonstrate the effectiveness of the anomaly detection strategy using real-worl...|$|R
25|$|Quasi-opportunistic {{supercomputing}} aims {{to provide}} a higher quality of service than opportunistic resource sharing. The quasi-opportunistic approach enables the execution of demanding applications within computer grids by establishing grid-wise resource allocation agreements; and <b>fault</b> tolerant <b>message</b> passing to abstractly shield against {{the failures of the}} underlying resources, thus maintaining some opportunism, while allowing a higher level of control.|$|R
40|$|Abstract. We {{present a}} method to enhance {{wormhole}} routing algorithms for deadlock-free fault-tolerant routing in tori. We consider arbitrarily-located faulty blocks and assume only local knowledge of <b>faults.</b> <b>Messages</b> are routed via shortest paths {{when there are no}} faults, and this constraint is only slightly relaxed to facilitate routing in the presence of faults. The key concept we use is that, for each fault region, a fault ring consisting of fault free nodes and physical channels can be formed around it. These fault rings can be used to route <b>messages</b> around <b>fault</b> regions. We prove that at most four additional virtual channels are su cient to make any fully-adaptive algorithm tolerant tomultiple faulty blocks in torus networks. As an example of this technique, we present simulation results for a fully-adaptive algorithm and show that good performance can be obtained with as many as 10 % links faulty...|$|R
40|$|We {{present a}} method to enhance {{wormhole}} routing algorithms for deadlock-free fault-tolerant routing in tori. We consider arbitrarily-located faulty blocks and assume only local knowledge of <b>faults.</b> <b>Messages</b> are routed via shortest paths {{when there are no}} faults, and this constraint is only slightly relaxed to facilitate routing in the presence of faults. The key concept we use is that, for each fault region, a fault ring consisting of fault free nodes and physical channels can be formed around it. These fault rings can be used to route <b>messages</b> around <b>fault</b> regions. We prove that at most four additional virtual channels are sufficient to make any fully-adaptive algorithm tolerant to multiple faulty blocks in torus networks. As an example of this technique, we present simulation results for a fully-adaptive algorithm and show that good performance can be obtained with as many as 10 % links faulty. Keywords: adaptive routing, deadlocks, fault-tolerant routing, multicomputer networks, m [...] ...|$|R
40|$|The Galileo Mission Telemetry System (MTS) has {{a cluster}} of {{computer}} subsystems configured as a star network. The MTS handles the real-time processing of spacecraft telemetry and ground monitor data. Large volumes of status and <b>fault</b> <b>messages</b> are generated {{as a result of}} changes in the system environment. These messages are triggered by the conditions that exist on any one particular subsystem or device. The order of message generation is in time sequence and does not always correlate to the function sequence of active processes. A significant number of messages provide context with varying degrees of uncertainty. As such, highly skilled telemetry controllers are required to regularly go through high volumes of messages generated by the MTS to identify, diagnose, and isolate faults. A knowledge-based system prototype is being developed to monitor the Galileo Mission Telemetry System performance. The system design approach features temporal reasoning, uncertainty management, and intelligent graphic user interfaces...|$|R
40|$|The state-of-the-practice Shuttle {{caution and}} warning system warns {{the crew of}} {{conditions}} that may create a hazard to orbiter operations and/or crew. Depending on {{the severity of the}} alarm, the crew is alerted with a combination of sirens, tones, annunciator lights, or <b>fault</b> <b>messages.</b> The combination of anomalies (and hence alarms) indicates the problem. Even with much training, determining what problem a particular combination represents is not trivial. In many situations, an automated diagnosis system can help the crew more easily determine an underlying root cause. Due to limitations of diagnosis systems,however, it is not always possible to explain a set of alarms with a single root cause. Rather, the system generates a set of hypotheses that the crew can select from. The ISHM Decision Analysis Tool (IDAT) assists with this task. It presents the crew relevant information that could help them resolve the ambiguity of multiple root causes and determine a method for mitigating the problem. IDAT follows graphical user interface design guidelines and incorporates a decision analysis system. I describe both of these aspects...|$|R
40|$|Abstract — Nowadays, {{clusters}} and grids {{are made of}} more and more computing nodes. The programming of multi-processes applications is the most often achieved through message passing. The increase {{of the number of}} processes implies that theses applications need to use a <b>fault</b> tolerant <b>message</b> passing library. In this paper, we present two implementations of fault tolerant protocols based on MPICH, a blocking one and a non blocking one. We then compare their efficiency and the overhead induced during a failure-free execution. I...|$|R
40|$|Abstract. Four {{kinds of}} {{abstraction}} {{for the design}} and analysis of fault– tolerant distributed systems are discussed. These abstractions concern system <b>messages,</b> <b>faults,</b> fault–masking voting, and communication. The abstractions are formalized in higher–order logic, and are intended to facilitate specifying and verifying such systems in higher–order theorem– provers. ...|$|R
5000|$|In {{a case of}} an {{exception}} in a user mode code, the operating system parses the thread's _EXCEPTION_REGISTRATION_RECORD list and calls each exception handler in sequence until a handler signals it has handled the exception (by return value) or the list is exhausted. The last one in the list is always the [...] which displays the General protection <b>fault</b> error <b>message.</b> Then the list is traversed once more giving handlers a chance to clean up any resources used. Finally, the execution returns to kernel mode where the process is either resumed or terminated.|$|R
40|$|Four {{kinds of}} {{abstraction}} {{for the design}} and analysis of fault tolerant distributed systems are discussed. These abstractions concern system <b>messages,</b> <b>faults,</b> fault masking voting, and communication. The abstractions are formalized in higher order logic, and are intended to facilitate specifying and verifying such systems in higher order theorem provers...|$|R
40|$|In recent years, {{rapid growth}} in wind energy as a {{substantial}} source of electricity generation has created greater demands on wind turbine system reliability and availability. To reduce service costs and maximize return on investment, wind farm operators have begun {{to take a more}} proactive approach to turbine problems by relying on intelligent condition monitoring and automated failure detection systems. The challenge is how to effectively convert large amounts of data into actionable decisions to detect and isolate failures at an early stage. This paper describes a unique data analysis and modeling technique for online turbine health monitoring and automatic root cause assessment. It provides a means to capture failure signatures for specific root causes based on historical events as well as engineering knowledge. Both continuous and discrete turbine condition monitoring data are processed to provide a failure probability assessment. First, statistical trend analysis, feature extraction and classification methods are developed to analyze a continuous sensor data set. Secondly, a pattern recognition method is applied to calculate failure indicators from various discrete control system events, or <b>fault</b> <b>messages.</b> Then failure likelihoods derived from both the continuous and the discrete models are combined in a fusion model to increase predictive accuracy. A demonstration of the method on bearing failure modeling using SCADA data will be provided with promising results...|$|R
5000|$|Specification 3.0 also {{describes}} how {{more than two}} devices can be connected by OpenTherm. Whilst OpenTherm is a point-to-point connection, an extra device (gateway) is added between the master and the slave. This gateway has 1 slave and 1 (or more) master interfaces. The gateway controls which data is passed to each slave.An application example is a room temperature controller connected to a heat recovery unit, which is connected to a boiler. The heat recovery unit is then functioning as gateway.In another possible configuration, a thermostat or room controller is connected to a sequencer with further Opentherm interfaces connected {{to more than one}} boiler. The room controller can be a standard unit, since it only 'sees' one heat-producer. The sequencer includes additional software to increase or decrease the number of running boilers to match the actual heat demand. The sequencer also needs a sensor to measure the temperature of the combined output from the boilers and usually would also control a main circulation pump. What happens after a fault occurs (resequencing remaining units, passing <b>fault</b> <b>messages</b> through for display on the room controller, etc.) is also part of the sequencer functionality. (The hydraulic design of such a system must also take account of different combinations of boilers running at the same time: a Low Loss Header / Hydraulic Separator is usually included to combine the flows from the boilers.) ...|$|R
40|$|Abstract—Deterministic {{replay of}} a {{parallel}} application {{is commonly used}} for discovering bugs or to recover from a hard fault with message-logging <b>fault</b> tolerance. For <b>message</b> passing programs, {{a major source of}} overhead during forward execution is recording the order in which messages are sent and received. During replay, this ordering must be used to deterministically reproduce the execution. Previous work in replay algorithms often makes minimal assumptions about the programming model and application to maintain generality. However, in many applications, only a partial order must be recorded due to determinism intrinsic in the program, ordering constraints imposed by the execution model, and events that are commutative (their relative execution order during replay {{does not need to be}} reproduced exactly). In this paper, we present a novel algebraic framework for reasoning about the minimum dependencies required to represent the partial order for different orderings and interleavings. By exploiting this framework, we improve on an existing scalable message-logging fault tolerance scheme that uses a total order. The improved scheme scales to 131, 072 cores on an IBM BlueGene/P with up to 2 × lower overhead. Keywords—replay, partial-order dependencies, <b>fault</b> tolerance, <b>message</b> logging, determinism, execution model I...|$|R
40|$|Today’s {{construction}} buildings {{contain more}} advanced technologies than before. The development of newer and stricter regulations {{to the construction}} industry makes the door environment more and more complex. In large construction projects the door environment is held up as a major challenge to get to in a good way. It turned out after a discussion with one project manager and a site manager at NCC, that the door environment is difficult to get right in major construction projects and it often causes many error reports in to the aftermarket after that the final inspection is completed. Interviews have been done with key figures in the building process at NCC and Akademiska hus. A study on a university building for around 3500 students and with 300 workplaces was made. There {{were a lot of}} data with fault reports to the go trough to find fault report from doors and be analyzed and statistic. The results from the data collection showed that many <b>fault</b> <b>messages</b> on the doors in was caused by the door operator. And from the interviews the content was clear that the coordination has to be good to have a functioning and a good door environment. One part that can contribute to a better door environment is to plan space for possible door operator over the door. Another part is that the construction documents which production workers are working with, is {{to find a way to}} always have the latest documents in hand...|$|R
