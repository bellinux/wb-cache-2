164|6093|Public
25|$|In 2009, {{a simple}} {{electronic}} circuit consisting of an LC network and a memristor {{was used to}} model experiments on adaptive behavior of unicellular organisms. It was shown that subjected to a train of periodic pulses, the circuit learns and anticipates the next pulse similar to the behavior of slime molds Physarum polycephalum where the viscosity of channels in the cytoplasm responds to periodic environment changes. Applications of such circuits may include, e.g., pattern recognition. The DARPA SyNAPSE project funded HP Labs, {{in collaboration with the}} Boston University Neuromorphics Lab, has been developing neuromorphic architectures which may be based on memristive systems. In 2010, Versace and Chandler described the MoNETA (Modular Neural Exploring Traveling Agent) model. MoNETA is the first large-scale neural network model to implement whole-brain circuits to power a virtual and robotic agent using memristive hardware. Application of the memristor crossbar structure in the construction of an analog soft computing system was demonstrated by Merrikh-Bayat and Shouraki. In 2011 they showed how memristor crossbars can be combined with fuzzy logic to create an analog memristive neuro-fuzzy computing system with fuzzy input and output terminals. Learning is based on the creation of fuzzy relations inspired from <b>Hebbian</b> <b>learning</b> <b>rule.</b>|$|E
5000|$|... #Subtitle level 3: <b>Hebbian</b> <b>learning</b> <b>rule</b> for Hopfield {{networks}} ...|$|E
5000|$|This {{model is}} merely a {{modified}} form of the <b>Hebbian</b> <b>learning</b> <b>rule,</b> , and requires a suitable choice of activation function, or rather, the output threshold, to avoid the Hebbian problems of instability. This threshold was derived rigorously in BCM noting that with [...] and the approximation of the average output , for one to have stable learning it is sufficient that ...|$|E
40|$|Zero-sum <b>Hebbian</b> <b>learning</b> <b>rules</b> that {{reinforce}} well correlated inputs {{have been}} used by others to model the competitive self-organization of afferents from the lateral geniculate nucleus to produce orientation selectivity and ocular dominance columns. However, the application of these simple Hebbian rules to the development of direction selectivity (DS) is problematic because the best correlated inputs are those that are well correlated in both the preferred and nonpreferred directions of motion. Such afferents would combine to produce non-DS cortical units. Afferents that are in spatiotemporal quadrature would combine to produce DS cortical units, but are poorly correlated in the nonpreferred direction. In this paper, the development of DS is reduced to the problem of associating a pair of units in spatiotemporal quadrature in the face of competition from a third, non-quadrature unit. As expected, simple <b>Hebbian</b> <b>learning</b> <b>rules</b> perform poorly at associating the quadrature pair. However [...] ...|$|R
40|$|Backpropagating action {{potentials}} (bAPs) are {{an important}} signal for associative synaptic plasticity in many neurons, but they often fail to fully invade distal dendrites. In this issue of Neuron, Sjöström and Häusser show that distal propagation failure leads to a spatial gradient of Hebbian plasticity in neocortical pyramidal cells. This gradient can be overcome by cooperative distal synaptic input, leading to fundamentally distinct <b>Hebbian</b> <b>learning</b> <b>rules</b> for distal versus proximal synapses...|$|R
40|$|Experimental {{data has}} shown that {{synaptic}} strength modification in some types of biological neurons depends upon precise spike timing differences between presynaptic and postsynaptic spikes. Several temporally-asymmetric <b>Hebbian</b> <b>learning</b> <b>rules</b> motivated by this data have been proposed. We argue that such <b>learning</b> <b>rules</b> are suitable to analog VLSI implementation. We describe an easily tunable circuit to modify {{the weight of a}} silicon spiking neuron according to those <b>learning</b> <b>rules.</b> Test results from the fabrication of the circuit using a 0. 6 m CMOS process are given...|$|R
50|$|The {{neural network}} model assumes that neurons in a neural network form a complex network with other neurons, forming a highly {{interconnected}} network; each neuron {{is characterized by the}} activation value, and the connection between two neurons is characterized by the weight value. Interaction between each neuron is characterized by the McCullough-Pitts dynamical rule, and change of weight and connections between neurons resulting from learning is represented by the <b>Hebbian</b> <b>learning</b> <b>rule.</b>|$|E
5000|$|Anderson [...] {{shows that}} {{combination}} of <b>Hebbian</b> <b>learning</b> <b>rule</b> and McCullough-Pitts dynamical rule allow network {{to generate a}} weight matrix that can store associations between different memory patterns - such matrix is the form of memory storage for the neural network model. Major differences between the matrix of multiple traces hypothesis and the neural network model is that while new memory indicates extension of the existing matrix for the multiple traces hypothesis, weight matrix of the neural network model does not extend; rather, the weight {{is said to be}} updated with introduction of new association between neurons.|$|E
50|$|In 2009, {{a simple}} {{electronic}} circuit consisting of an LC network and a memristor {{was used to}} model experiments on adaptive behavior of unicellular organisms. It was shown that subjected to a train of periodic pulses, the circuit learns and anticipates the next pulse similar to the behavior of slime molds Physarum polycephalum where the viscosity of channels in the cytoplasm responds to periodic environment changes. Applications of such circuits may include, e.g., pattern recognition. The DARPA SyNAPSE project funded HP Labs, {{in collaboration with the}} Boston University Neuromorphics Lab, has been developing neuromorphic architectures which may be based on memristive systems. In 2010, Versace and Chandler described the MoNETA (Modular Neural Exploring Traveling Agent) model. MoNETA is the first large-scale neural network model to implement whole-brain circuits to power a virtual and robotic agent using memristive hardware. Application of the memristor crossbar structure in the construction of an analog soft computing system was demonstrated by Merrikh-Bayat and Shouraki. In 2011 they showed how memristor crossbars can be combined with fuzzy logic to create an analog memristive neuro-fuzzy computing system with fuzzy input and output terminals. Learning is based on the creation of fuzzy relations inspired from <b>Hebbian</b> <b>learning</b> <b>rule.</b>|$|E
40|$|A {{zinc oxide}} (ZnO) -reduced {{graphene}} oxide (rGO) composite thin film memristive device is reported. Further, {{it has been}} shown that it is possible to implement <b>Hebbian</b> <b>learning</b> <b>rules</b> like, the spike-timing-dependent plasticity, using this device. Furthermore, a circuit on PCB is developed; this circuit can imitate the biological spike firing scheme and activate the memristor synapse. The fabricated device along with the custom made circuit can be extended for developing future neuromorphic circuit application...|$|R
30|$|From a futures {{research}} perspective, {{the analogy}} between <b>Hebbian</b> <b>learning</b> <b>rules</b> and unregulated capitalism, both inherently unstable as “the rich get richer”, {{need to be}} seriously considered. Economics may be called {{to act as a}} natural bridge able to connect social and technological aspects. This positioning may sound extremely risky, the recent financial meltdown and the inability of the economic models to forecast these extreme events, has done nothing but reinforced the old motto “economics is the dismal science”.|$|R
40|$|There is {{a growing}} {{interest}} in using Kalman-filter models for brain modelling. In turn, it is of considerable importance to represent Kalman-filter in connectionist forms with local <b>Hebbian</b> <b>learning</b> <b>rules.</b> To our best knowledge, Kalman-filter has not been given such local representation. It seems that the main obstacle is the dynamic adaptation of the Kalman-gain. Here, a connectionist representation is presented, which is derived {{by means of the}} recursive prediction error method. We show that this method gives rise to attractive local <b>learning</b> <b>rules</b> and can adapt the Kalman-gain...|$|R
50|$|Following {{the same}} basic idea contributed by Robins, Ans and Rousset (1997) have also {{proposed}} a two-network artificial neural architecture with memory self-refreshing that overcomes catastrophic interference when sequential learning tasks are carried out in distributed networks trained by backpropagation. The principle is to interleave, {{at the time when}} new external patterns are learned, those to-be-learned new external patterns with internally generated pseudopatterns, or 'pseudo-memories', that reflect the previously learned information. What mainly distinguishes this model from those that use classical pseudorehearsal in feedforward multilayer networks is a reverberating process that is used for generating pseudopatterns. This process which, after a number of activity re-injections from a single random seed, tends to go up to nonlinear network attractors, is more suitable for optimally capturing the deep structure of previously learned knowledge than a single feedforward pass of random activation. Ans and Rousset (2000) have shown that the learning mechanism they proposed avoiding catastrophic forgetting, provides a more appropriate way to deal with knowledge transfer as measured by learning speed, ability to generalize and vulnerability to network damages. Musca, Rousset and Ans (2009) have also shown that pseudopatterns originating from an artificial reverberating neural network could induce familiarity in humans with never seen items in the way predicted by simulations conducted with a two-network artificial neural architecture. Furthermore, Ans (2004) has implemented a version of the self-refreshing mechanism using only one network trained by the Contrastive <b>Hebbian</b> <b>Learning</b> <b>rule,</b> a training rule considered as more realistic than the largely used backpropagation algorithm, but fortunately equivalent to the latter.|$|E
40|$|In {{ensemble}} teacher learning, ensemble {{teachers have}} only uncertain {{information about the}} true teacher, and this information is given by an ensemble consisting of {{an infinite number of}} ensemble teachers whose variety is sufficiently rich. In this learning, a student learns from an ensemble teacher that is iteratively selected randomly from a pool of many ensemble teachers. An interesting point of ensemble teacher learning is the asymptotic behavior of the student to approach the true teacher by learning from ensemble teachers. The student performance is improved by using the <b>Hebbian</b> <b>learning</b> <b>rule</b> in the learning. However, the perceptron learning rule cannot improve the student performance. On the other hand, we proposed a perceptron learning rule with a margin. This learning rule is identical to the perceptron learning rule when the margin is zero and identical to the <b>Hebbian</b> <b>learning</b> <b>rule</b> when the margin is infinity. Thus, this rule connects the perceptron learning rule and the <b>Hebbian</b> <b>learning</b> <b>rule</b> continuously through the size of the margin. Using this rule, we study changes in the learning behavior from the perceptron learning rule to the <b>Hebbian</b> <b>learning</b> <b>rule</b> by considering several margin sizes. From the results, we show that by setting a margin of kappa > 0, the effect of an ensemble appears and becomes significant when a larger margin kappa is used. Comment: 12 pages, 5 figures. Journal of Physical Society of Japan, vol. 81, (2012) 06400...|$|E
40|$|Abstract—Transient {{synchronization}} {{has been}} used as a mechanism of recognizing auditory patterns using integrate-and-fire (IF) neural networks. We first extend the mechanism to vision tasks and investigate the role of spike dependent learning. We show that such a temporal <b>Hebbian</b> <b>learning</b> <b>rule</b> significantly improves accuracy of detection. Second, we demonstrate how multiple patterns can be identified by a single pattern selective neuron and how a temporal album can be constructed. This principle may lead to multidimensional memories, where the capacity per neuron is considerably increased with accurate detection of spike synchronization. Index Terms—Detection of spike synchronization, <b>Hebbian</b> <b>learning</b> <b>rule,</b> integrate-and-fire (IF) model, temporal vision, transient synchrony. I...|$|E
40|$|Nonlinear {{extensions}} of one-unit and multi-unit Principal Component Analysis (PCA) neural networks, introduced earlier by the author, are reviewed. The networks and their nonlinear <b>Hebbian</b> <b>learning</b> <b>rules</b> {{are related to}} other signal expansions like the Projection Pursuit (PP) and the Independent Component Analysis (ICA). 1 Introduction A Principal Component Analysis (PCA) network is a one-layer feedforward neural network which is able to extract the principal components of the stream of input vectors. Typically <b>Hebbian</b> type <b>learning</b> <b>rules</b> are used based on the one-unit learning algorithm originally proposed by the author in (Oja, 1982). Many different versions and {{extensions of}} this basic algorithm have been proposed during the recent years; for reviews and introductions, see e. g. (Oja, 1992; Haykin, 1994). PCA networks are useful in signal characterization, optimal feature extraction, and data compression. PCA is also the basis of subspace classifiers {{that have been used}} e. g. in [...] ...|$|R
40|$|We {{present a}} {{simplified}} binocular {{neural network model}} of the primary visual cortex with separate ON/OFF-pathways and modifiable afferent as well as intracortical synaptic couplings. Natural image stimuli drive the weight adaptation which follows <b>Hebbian</b> <b>learning</b> <b>rules</b> stabilized with constant norm and constant sum constraints. The simulations consider the development of orientation selective cortical cells and orientation maps under different conditions concerning stimulus patterns and lateral couplings. Strong short range excitatory lateral connections emerge between individual cortical neurons with inhibitory couplings being less specific and rather diffuse...|$|R
40|$|We {{propose a}} model of {{intrinsic}} plasticity for a continuous activation model neuron based on information theory. We then show how intrinsic and synaptic plasticity mechanisms interact and allow the neuron to discover heavy-tailed directions in the input. We also demonstrate that intrinsic plasticity may be an alternative explanation for the sliding threshold postulated in the BCM theory of synaptic plasticity. We present a theoretical analysis of the interaction of intrinsic plasticity with different <b>Hebbian</b> <b>learning</b> <b>rules</b> for the case of clustered inputs. Finally, we perform experiments on the “bars ” problem, a popular nonlinear independent component analysis problem...|$|R
40|$|-simple {{cells are}} likely to be the same {{population}} as complex cells, arising from different convergence of the <b>Hebbian</b> <b>learning</b> <b>rule.</b> The input to complex &#x 201 c;complex&#x 201 d; cells are dendritic branches with simple cell properties;|$|E
40|$|Presented is a {{model of}} an integrate-and-fire neuron with active {{dendrites}} and a spike-timing dependent <b>Hebbian</b> <b>learning</b> <b>rule.</b> The learning algorithm effectively trains the neuron when responding to several types of temporal encoding schemes: temporal code with single spikes, spike bursts and phase coding...|$|E
40|$|We discuss {{appropriate}} {{modifications of}} the <b>Hebbian</b> <b>learning</b> <b>rule</b> for Q-state Potts neural networks with biased patterns, the purpose being {{to prevent the}} storage capacity from decreasing drastically with increasing bias. Several prescriptions are compared. As an illustration their retrieval performance is studied numerically for Q = 3. status: publishe...|$|E
40|$|This paper {{focuses on}} {{developing}} self-adapting auto-matic object detection systems to achieve robust per-formance. Two general methodologies for performance improvement are first introduced. They {{are based on}} optimization of parameters of an algorithm and adap-tation of the input to an algorithm. Different modified <b>Hebbian</b> <b>learning</b> <b>rules</b> are used to build adaptive feature extractors which transform the input data into a desired form for a given object detection algorithm. To show its feasibility, input adaptors for object detection are de-signed and tested using multisensor data including SAR, FLIR, and color images. Test results are presented and discussed in the paper. ...|$|R
40|$|This paper proposes an {{unsupervised}} neural algorithm for trajectory {{production of}} a 6 -DOF robotic arm. The model encodes these trajectories in a single training iteration by using competitive and temporal <b>Hebbian</b> <b>learning</b> <b>rules</b> and operates by producing the current and the next position for the robotic arm. In this paper we will focus on trajectories with one common point. These types of trajectories introduce some ambiguities, but even so, the neural algorithm is able to reproduce them accurately and unambiguously due to context units used {{as part of the}} input. In addition, the proposed model is shown to be fault-tolerant...|$|R
40|$|This paper {{focuses on}} the {{developing}} of neural network and pattern recognization, using adaptive reasoning theory and <b>hebbian</b> <b>learning</b> <b>rules,</b> {{with the objective of}} classification accuracy of the network on unseen test data. In ART when a training vector is inserted, a Hebbian rule is chosen to be trained through the network in order to make algorithm real time predictions of network traffic. There is considerable physiological evidence that a Hebb-like <b>learning</b> <b>rule</b> applies to the strengthening of synaptic efficacy. <b>Hebbian</b> <b>learning</b> can strengthen the neural response that is elicited by an input; this can be useful if the response made is appropriate to the situation, {{but it can also be}} counterproductive if a different response would be more appropriate. From a computational point of view, <b>Hebbian</b> <b>learning</b> can certainly lead one in the wrong direction, and some form of control over this is necessary. Also, experimental findings clearly show that learning can be affected by accuracy or outcome feedback...|$|R
40|$|We {{report a}} {{neural network model}} {{that is capable of}} {{learning}} arbitrary input sequences quickly and online. It is also capable of completing a sequence upon cueing from any point in the sequence and in the presence of background noise. The architecture of the neural network utilizes sigmoid-pulse generating spiking neurons together with a <b>Hebbian</b> <b>learning</b> <b>rule</b> with synaptic noise. ...|$|E
40|$|Abstract: In this paper, {{we discuss}} a neural network based on <b>hebbian</b> <b>learning</b> <b>rule</b> for finding the inverse of a matrix. First we {{described}} finding the inverse of a matrix by mentioned neural network. Finally, experimental results for square and non-square matrices are presented {{to show the}} effectiveness of the approach. Proposed method is also scalable for finding the inversion of large-scale matrices...|$|E
40|$|This {{abstract}} describes simulations using {{a reasonably}} biological accurate point neuron model, a fatiguing leaky integrate and fire model. These model neurons use a novel compensatory <b>Hebbian</b> <b>learning</b> <b>rule</b> to categorise data items, a standard machine learning task. The resulting {{system is a}} kind of self organising map, which compares favourably with a Kohonen map on one machine learning task...|$|E
40|$|We {{show that}} it is {{possible}} to relate the Support Vector Machine formalism to <b>Hebbian</b> <b>Learning</b> in the context of olfactory learning in the insect brain. Since neurons cannot have negative firing rates, two neurons and synaptic inhibition are required to encode a binary classification problem in a biologically realistic way. We show that the two neuron system with plausible <b>Hebbian</b> <b>learning</b> <b>rules</b> can be mapped to a large margin classifier. Two formalisms are analyzed: regular SVMs and the so-called inhibitory SVMs. The regularization term in regular SVMs brings the synaptic vectors of the two neurons close to each other, while the inhibitory SVM can bring them to 0 resembling the memory loss process in <b>Hebbian</b> <b>learning.</b> Based on the analogy to large margin classifiers we also predict the existence of a negative Hebbian leaning rule for negative reinforcement signals...|$|R
40|$|Auto-associative network {{models have}} proven {{extremely}} useful in modelling the hypothesised {{function of the}} CA 3 region of the hippocampus in declarative memory. To date, {{the majority of these}} models have made use of Hebbian plasticity rules mediated by correlations between mean firing rates. However, recent neurobiological evidence suggests that synaptic plasticity in the hippocampus, and many other cortical regions, also depends explicitly on the temporal relationship between afferent action potentials and efferent spiking - a phenomena known as spike-timing dependent plasticity (STDP). Few {{attempts have been made to}} reconcile previous rate-coded <b>Hebbian</b> <b>learning</b> <b>rules</b> or auto-associative network function with this novel plasticity formulation. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|<b>Hebbian</b> <b>learning</b> <b>rules</b> are {{generally}} formulated as static rules. Under changing condition (e. g. neuromodulation, input statistics) most rules {{are sensitive to}} parameters. In particular, recent work has focused on two different formulations of spike-timing-dependent plasticity rules. Additive STDP [1] is remarkably versatile but also very fragile, whereas multiplicative STDP [2, 3] is more robust but lacks attractive features such as synaptic competition and rate stabilization. Here we {{address the problem of}} robustness in the additive STDP rule. We derive an adaptive control scheme, where the learning function is under fast dynamic control by postsynaptic activity to stabilize learning under a variety of conditions...|$|R
40|$|We {{present a}} new {{neural network model}} for {{processing}} of temporal patterns. This model, the gamma neural model, is as general as a convolution delay model with arbitrary weight kernels w(t). We show that the gamma model can be formulated as a (partially prewired) additive model. A temporal <b>hebbian</b> <b>learning</b> <b>rule</b> is derived and we establish links to related existing models for temporal processing. ...|$|E
40|$|An {{unsupervised}} learning algorithm for recurrent neural networks is proposed, that generalizes PCA to time series. A linear recurrent neural network using Oja's constrained <b>Hebbian</b> <b>learning</b> <b>rule</b> is presented. We demonstrate that this network extracts complex temporal {{information from a}} sequence of inputs. Temporal sequences stored in the network can be retrieved in the reverse order of presentation, providing a straightforward implementation of a logical stack...|$|E
40|$|Abstract. Restorative brain-computer interafces (BCIs) {{have been}} {{exploited}} {{by a number}} of BCI labs for the purpose of stroke rehabilitation. The results that are achieved with commonly used technology are rather promising, but inconsistent. In this abstract we propose a novel paradigm for restorative BCI designs, which is based on motor learning theory and the <b>Hebbian</b> <b>learning</b> <b>rule.</b> It is expected to enhance the degree of neuroplasticity in stroke patients...|$|E
40|$|Model neural {{networks}} can perform dimensional reductions of input data sets using correlation-based <b>learning</b> <b>rules</b> {{to adjust their}} weights. Simple <b>Hebbian</b> <b>learning</b> <b>rules</b> lead to an optimal reduction at the single unit level but result in highly redundant network representations. More complex rules designed to reduce or remove this redundancy can develop optimal principal component representations, {{but they are not}} very compelling from a biological perspective. Neurons in biological networks have restricted receptive fields limiting their access to the input data space. We find that, within this restricted receptive field architecture, simple correlation-based <b>learning</b> <b>rules</b> can produce surprisingly efficient reduced representations. When noise is present, the size of the receptive fields can be optimally tuned to maximize the accuracy of reconstructions of input data from a reduced representation. ...|$|R
40|$|This paper {{focuses on}} the third {{generation}} of neural networks- Spiking neural networks (SNNs), the novel Spiking neuron model- probabilistic Spiking neuron model (pSNM), and their applications. pSNM is used in mobile robots' behavior control, and a novel mobile robots' wall-following controller based on pSNM is proposed. In the pSNM controller, Spiking time-delayed coding {{is used for the}} sensory neurons of the input layer and pSNM is used for the motor neurons in the output layer. Thorpe and <b>Hebbian</b> <b>learning</b> <b>rules</b> are used in the controller. The experimental results show that the controller can control the mobile robots to follow the wall clockwise and counterclockwise successfully. The structure of the controller is simple, and the controller can study online. © (2012) Trans Tech Publications, Switzerland. This paper {{focuses on the}} third generation of neural networks- Spiking neural networks (SNNs), the novel Spiking neuron model- probabilistic Spiking neuron model (pSNM), and their applications. pSNM is used in mobile robots' behavior control, and a novel mobile robots' wall-following controller based on pSNM is proposed. In the pSNM controller, Spiking time-delayed coding is used for the sensory neurons of the input layer and pSNM is used for the motor neurons in the output layer. Thorpe and <b>Hebbian</b> <b>learning</b> <b>rules</b> are used in the controller. The experimental results show that the controller can control the mobile robots to follow the wall clockwise and counterclockwise successfully. The structure of the controller is simple, and the controller can study online. © (2012) Trans Tech Publications, Switzerland...|$|R
40|$|This paper proposes an {{unsupervised}} {{neural network}} for trajectory learning of a robotic arm. The neural network encodes trajectories by using competitive and temporal <b>Hebbian</b> <b>learning</b> <b>rules</b> and operates by producing the current and the next position for the robotic arm. Different types of trajectories can be learned independently of their complexity. Tests will focus on trajectories with one crossing point. The algorithm is able to reproduce the trajectories accurately and unambiguously due to context units used together with the input. Also, the proposed model is shown to be fault-tolerant and can respond well {{in the presence of}} noisy inputs. Copyright 1999 IFAC Keywords: Trajectory planning, robot control, neural networks, learning algorithms, faulttolerance...|$|R
