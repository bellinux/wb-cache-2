2000|10000|Public
5|$|FinMkt {{was founded}} in 2011 by Srikanth Goteti and Luan Cox, both of whom {{previously}} worked for Fintech. Cox is the firm's CEO, Goteti is its CTO, and Nathan Barber is its CFO/COO. Cox is a sales and marketing specialists {{who was involved in}} establishing such firms as Quote.com, Stockpoint, and ScreamingMedia. She helped build Interactive Data. Goteti, a former Senior Director of Technology at Interactive Data Managed Solutions, is an expert in <b>data</b> <b>modeling,</b> derivatives, transformation, and normalization of securities market data.|$|E
25|$|The Workbench is a {{graphical}} {{user interface}} (GUI) and integrated development environment (IDE) for all IRI software products, built on Eclipse™. The Workbench is a free, optional place to design, run, and manage data connections, metadata, and jobs, and to use third-party plug-ins for business intelligence, <b>data</b> <b>modeling,</b> version control, etc.|$|E
25|$|In applied statistics, total {{least squares}} {{is a type}} of errors-in-variables regression, a least squares <b>data</b> <b>modeling</b> {{technique}} in which observational errors on both dependent and independent variables are taken into account. It is a generalization of Deming regression and also of orthogonal regression, and can be applied to both linear and non-linear models.|$|E
5000|$|<b>Data</b> <b>modelling</b> is {{the process}} of {{creating}} a <b>data</b> <b>model</b> by applying formal <b>data</b> <b>model</b> descriptions using <b>data</b> <b>modelling</b> techniques. <b>Data</b> <b>modelling</b> is a technique for defining business requirements for a database. It is sometimes called database <b>modelling</b> because a <b>data</b> <b>model</b> is eventually implemented in a database.|$|R
5000|$|A <b>data</b> <b>model</b> instance, i.e. {{applying}} a <b>data</b> <b>model</b> theory {{to create a}} practical <b>data</b> <b>model</b> instance for some particular application.|$|R
40|$|This paper {{introduces}} a Conceptual <b>Data</b> <b>Model</b> for <b>Data</b> Warehouse including multidimensional aggregation. It {{is based on}} Entity-Relationships <b>data</b> <b>model.</b> The conceptual <b>data</b> <b>model</b> gracefully extends standard Entity-Relationship <b>data</b> <b>model</b> with multidimensional aggregated entities. The model has a clear mathematical theoretic semantics grounded on standard ER semantics and the GMD logic-based multidimensional <b>data</b> <b>model.</b> The aim of this work is not to propose yet another conceptual <b>data</b> <b>model,</b> but {{to find the most}} general and precise formalism considering all the proposals for a conceptual <b>data</b> <b>model</b> in the <b>data</b> warehouse field, making therefore a possible formal comparison of the differences of the models in the literature, and to study the formal properties or extensions of such <b>data</b> <b>models.</b> ...|$|R
25|$|IRI {{software}} {{is designed to}} transform, convert, report, and protect large data volumes rapidly in distributed, heterogeneous computing environments. These functions are built into the CoSort package or through spin-offs for data extraction, generation, security, and migration. Each tool uses the same graphical IDE built on Eclipse, and metadata format for defining and manipulating data. IRI's open data definition file format is also supported by AnalytiX DS and Meta Integration Technology (MITI) so that third-party ETL, BI, and <b>data</b> <b>modeling</b> tool users can convert or re-use their existing metadata in IRI product environments.|$|E
2500|$|In {{the least}} squares method of <b>data</b> <b>modeling,</b> the {{objective}} function, S, ...|$|E
2500|$|Object Model) is a data {{exchange}} and <b>data</b> <b>modeling</b> standard {{for use in}} encoding ...|$|E
50|$|Note, see Logical <b>data</b> <b>model</b> for {{discussion}} {{of the relationship of}} these three DIV <b>data</b> <b>models,</b> with comparison of the Conceptual, Logical & Physical <b>Data</b> <b>Models.</b>|$|R
40|$|This paper {{describes}} the conceptual integration and computer-based support of two important groups of conceptual <b>data</b> <b>models,</b> Entity Relationship Models and Object Role Models (e. g. NIAM). We perform conceptual integration using the conceptual <b>data</b> <b>modelling</b> language CoCoA to specify separate <b>data</b> <b>models</b> of individual notations. We then merge these into an integrated conceptual <b>data</b> <b>model</b> for both notations. These <b>data</b> <b>models</b> {{form the basis}} of the repository for an I-CASE tool supporting modelling with both notations, with full consistency management between the two notation <b>data</b> <b>models...</b>|$|R
50|$|A georelational <b>data</b> <b>model</b> is a {{geographic}} <b>data</b> <b>model</b> that represents geographic features as an interrelated set of spatial and attribute <b>data.</b> The georelational <b>model</b> {{is the fundamental}} <b>data</b> <b>model</b> used in coverages.|$|R
2500|$|Process {{modelling}} in EEML, {{according to}} Krogstie (2006) [...] "supports the modeling of process logic which is mainly expressed through nested structures of tasks and decision points. The sequencing {{of the tasks}} is expressed by the flow relation between decision points. Each task has minimum an input port and an output port being decision points for modeling process logic, Resource roles are used to connect resources of various kinds (persons, organisations, information, material objects, software tools and manual tools) to the tasks. In addition, <b>data</b> <b>modeling</b> (using UML class diagrams), goal modeling and competency modeling (skill requirements and skills possessed) can be integrated with the process models".|$|E
2500|$|Research on {{customer}} attrition <b>data</b> <b>modeling</b> {{may provide}} businesses with several tools for enhancing customer retention. Using data mining and software, one may apply statistical methods to develop nonlinear attrition causation models. One researcher notes that [...] "...retaining existing customers is more profitable than acquiring new customers due primarily to savings on acquisition costs, the higher volume of service consumption, and customer referrals." [...] The {{argument is that}} to build an [...] "...effective customer retention program," [...] managers {{have to come to}} an understanding of [...] "...why customers leave" [...] and [...] "...identify the customers with high risk of leaving" [...] by accurately predicting customer attrition.|$|E
50|$|This {{article is}} a {{comparison}} of <b>data</b> <b>modeling</b> tools which are notable, including standalone, conventional <b>data</b> <b>modeling</b> tools and modeling tools supporting <b>data</b> <b>modeling</b> {{as part of a}} larger modeling environment.|$|E
40|$|AbstractBusiness process {{management}} (BPM) is becoming popular in business, {{and the business}} process modelling {{is a way of}} representing an organisation to enable its analysis and improvement. A business-friendly modelling is very helpful for business people, and also can act as a communication tool between them and technical IT people. This paper focuses on a new <b>data</b> <b>model,</b> called Source-Transaction-Agent (STA) <b>data</b> <b>model,</b> as a modelling technique for business process <b>modelling.</b> STA <b>data</b> <b>model</b> uses business metadata to assist business and IT person to communicate and participate effectively and efficiently in business <b>data</b> <b>modelling</b> of a system development process. The STA <b>data</b> <b>model</b> uses relational database concept and semantic <b>data</b> <b>modelling,</b> developed by combining Resource-Event-Agent (REA) <b>data</b> <b>model</b> and form-based approach. Entity Relationship Diagram (ERD) is used as the benchmark for the STA effectiveness evaluation. The results show that the STA <b>data</b> <b>model</b> is an effective <b>data</b> <b>model</b> technique for business process modelling...|$|R
30|$|For {{modeling}} IS, we generalize {{the concept}} of <b>data</b> <b>models.</b> <b>Data</b> <b>models</b> consist of collections (of data) so that each collection has gotten a name. The collections are set of data or multi-set (bag) of data of data types with well-defined properties and structure; the most typical representation of <b>data</b> <b>model</b> is either relational <b>data</b> <b>model</b> or object-relational <b>data</b> <b>model.</b> The instances of data types make up finite subsets of potential dataset.|$|R
40|$|There {{are many}} <b>data</b> <b>modelling</b> {{languages}} used in today’s information systems engineering environment. Some of the <b>data</b> <b>modelling</b> languages used {{have a degree}} of hype surrounding their quality and applicability. We would like to understand exactly what makes some <b>data</b> <b>modelling</b> languages successful and in some way suggest how useful <b>data</b> <b>modelling</b> languages {{will be in the}} context of an organisation and why. We are also interested in a theory capable of unifying the disparate range of languages. To do these things we select a theory based on ontology using which <b>data</b> <b>modelling</b> languages can be investigated. In this context theory should allow us to understand, compare, evaluate, and strengthen <b>data</b> <b>modelling</b> languages. The theory may also be used to suggest how useful various <b>data</b> <b>modelling</b> languages may be in an organisational setting. In this paper we present Chisholm’s ontology which we use to investigate <b>data</b> <b>modelling</b> languages. We show how Chisholm’s ontology can be used as a unifying theory of <b>data</b> <b>models,</b> develop methods for comparing <b>data</b> <b>modelling</b> languages based on this theory and summarise our findings. In conclusion, we evaluate the methods and the theory and examine avenues for future research. In this paper we present a deeper understanding of method together with analysis of new <b>data</b> <b>modelling</b> languages...|$|R
5000|$|<b>Data</b> <b>modeling</b> in {{software}} engineering {{is the process}} of creating a data model by applying formal data model descriptions using <b>data</b> <b>modeling</b> techniques. <b>Data</b> <b>modeling</b> is a technique for defining business requirements for a database. It is sometimes called database modeling because a data model is eventually implemented in a database.|$|E
5000|$|<b>Data</b> <b>Modeling.</b> Some hubs require {{extensive}} <b>data</b> <b>modeling</b> {{to allow}} even basic processing. Others allow more flexible handling of many data sources. There are tradeoffs to both approaches.|$|E
50|$|<b>Data</b> <b>modeling</b> is the {{technique}} of identifying entities, associating attributes to the entities and deciding the data structure to represent the attributes. In the traditional database scenario, a logical data model is created at the beginning to represent the entities and their associated attributes. In evolutionary <b>data</b> <b>modeling</b> {{the technique}} of <b>data</b> <b>modeling</b> is performed in an iterative manner, that is multiple data models are developed, each model representing a different aspect of the database. This kind of <b>data</b> <b>modeling</b> technique is practiced in an agile environment {{and it is one}} of the main principles of agile development.|$|E
50|$|A {{semantic}} <b>data</b> <b>model</b> {{in software}} engineering {{is a technique}} to define the meaning of data {{within the context of}} its interrelationships with other data. A semantic <b>data</b> <b>model</b> is an abstraction which defines how the stored symbols relate to the real world. A semantic <b>data</b> <b>model</b> is sometimes called a conceptual <b>data</b> <b>model.</b>|$|R
40|$|GRE) {{program to}} {{document}} and evaluate geologic resources related to approximately 270 NPS units (national parks, monuments, recreation areas, historic sites, seashores, etc.). The GRE program is currently developing digital geologic-GIS maps and geologic resources summary reports {{for each of}} these 270 NPS units. Colorado State University (CSU) is a partner in the production of these products and is the primary developer of the NPS GRE geology-GIS <b>data</b> <b>model</b> adopted for the creation of GRE digital geologic-GIS data. Over the last ten years the NPS GRE geology-GIS <b>data</b> <b>model</b> has evolved from its initial ESRI coverage-based format (Coverage <b>Data</b> <b>Model),</b> to implementation within an ESRI personal geodatabase (v. 1. x <b>Data</b> <b>Model),</b> to a recent redesign to streamline the <b>data</b> <b>model</b> and its implementation (v. 2. x <b>Data</b> <b>Model).</b> Using GRE geologic-GIS data for Mount Rainer National Park, Washington (MORA) we present the evolution of the NPS GRE geology-GIS <b>data</b> <b>model</b> to convey Why the <b>data</b> <b>model</b> format was adopted, What the basic GIS <b>data</b> <b>model</b> components are (including data layer architecture, attribute tables, domains, subtypes, topology and table relationships), and How the <b>data</b> <b>model</b> is implemented...|$|R
40|$|A <b>data</b> <b>model</b> {{empowers}} us to store, retrieve {{and manipulate}} {{data in a}} unified way. We consider the biological data consists of DNA (De-Oxyribonucleic Acid), RNA (Ribonucleic Acid) and protein structures. In our Bioinformatics Lab (Bioinformatics Lab, Alkhawarizmi Institute of Computer Science, University of Engineering & Technology, Lahore, Pakistan), we have already proposed two <b>data</b> <b>models</b> for DNA and protein structures individually. In this paper, we propose a unified <b>data</b> <b>model</b> by using the <b>data</b> <b>models</b> of TOS (Temporal Object Oriented System) after making some necessary modifications to this <b>data</b> <b>model</b> and our already proposed the two <b>data</b> <b>models.</b> This proposed unified <b>data</b> <b>model</b> {{can be used for}} the modeling and maintaining the biological data (i. e. DNA, RNA and protein structures), in a single unified wa...|$|R
50|$|The SERM (Structured Entity Relationship Model) is an {{amplification}} of the ERM {{which is}} commonly used for <b>data</b> <b>modeling.</b> It was first proposed from Prof. Dr. Elmar J. Sinz in 1988. The SERM {{is commonly used}} in the SAP-world for the <b>data</b> <b>modeling.</b>|$|E
5000|$|Veryard {{made the}} point, that the modeler has some choice in {{whether to use}} an entity, {{relationship}} or attribute to represent a given universe of discourse (UoD) concept. This justifies a common position, that [...] "data models of the same UoD may differ, but the differences {{are the result of}} shortcomings in the <b>data</b> <b>modeling</b> language. The argument is that <b>data</b> <b>modeling</b> is essentially descriptive, but that current <b>data</b> <b>modeling</b> languages allow some choice in how the description is documented." ...|$|E
50|$|Architect - adds <b>data</b> <b>modeling</b> tools.|$|E
50|$|Logical <b>data</b> <b>models</b> {{should be}} based on the {{structures}} identified in a preceding conceptual <b>data</b> <b>model,</b> since this describes the semantics of the information context, which the logical model should also reflect. Even so, since the logical <b>data</b> <b>model</b> anticipates implementation on a specific computing system, the content of the logical <b>data</b> <b>model</b> is adjusted to achieve certain efficiencies.|$|R
40|$|<b>Data</b> <b>models</b> {{provide a}} map of the {{components}} of an information system. Prior research has indicated that more expressive conceptual <b>data</b> <b>models</b> (despite their increased size) result in better performance for problem solving tasks. An initial experiment using logical <b>data</b> <b>models</b> indicated that more expressive logical <b>data</b> <b>models</b> also enhanced end-user performance for information retrieval tasks. However, the principles of parsimony and bounded rationality imply that, past some point, increases in size lead to a level of complexity that results in impaired performance. The results of this study support these principles. For a logical <b>data</b> <b>model</b> of increased but still modest size, users composing queries for the more expressive logical <b>data</b> <b>model</b> did not perform as well as users composing queries for the corresponding less expressive but more parsimonious logical <b>data</b> <b>model.</b> These results indicate that, when constructing logical <b>data</b> <b>models,</b> <b>data</b> modelers should consider tradeoffs between parsimony and expressiveness...|$|R
40|$|Abstract- Currently {{database}} {{researchers are}} investigating new <b>data</b> <b>models</b> {{in order to}} remedy the deficiences of the flat relational model when applied to non-business applications. Herein we concentrate on a recent graph-based <b>data</b> <b>model</b> called the hypernode model. The single underlying data structure of this model is the hypernode which is a digraph with a unique defining label. We present in detail the three components of the <b>model,</b> namely its <b>data</b> structure, the hypernode, its query and update language, called HNQL, and its provision for enforcing integrity constraints. We first demonstrate that the said <b>data</b> <b>model</b> is a natural candidate for formalising hypertext. We then compare it with other graph-based <b>data</b> <b>models</b> and with set-based <b>data</b> <b>models.</b> We also investigate the expressive power of HNQL. Finally, using the hypernode model as a paradigm for graph-based <b>data</b> <b>modelling,</b> we show how {{to bridge the gap}} between graph-based and set-based <b>data</b> <b>models,</b> and at what computational cost this can be done. Index Terms- graph-based <b>data</b> <b>model,</b> set-based <b>data</b> <b>model,</b> hypernode database, hypernode functional dependency, hypertext, query and update language, computable update, nonwell-founded sets. I...|$|R
50|$|In 1998, PerfectO was {{translated}} into Java resulting in SILVERRUN-JD (Java Designer). With the addition of relational <b>data</b> <b>modeling,</b> the product was renamed to SILVERRUN ModelSphere and released in 2002. Later on, more features were added including support for business process modeling, conceptual <b>data</b> <b>modeling,</b> and UML diagramming.|$|E
5000|$|Supports large, complex <b>data</b> <b>modeling</b> {{and data}} {{warehousing}} projects ...|$|E
5000|$|Modeling: {{involves}} business modeling, <b>data</b> <b>modeling,</b> {{and process}} modeling.|$|E
40|$|The {{principle}} {{objective of}} this research was to design conceptual <b>data</b> <b>model</b> for Hyrcanian forest. The <b>data</b> <b>model</b> is based on E-R diagram as useful tool for designing the data used in <b>data</b> <b>model.</b> As a result a forest conceptual <b>data</b> <b>model</b> describes system elements and their relationships in Hyrcanian forest unit. It {{can be used as a}} primary core of a more comprehensive forest information system. Finally, this study indicates the advantages of designing conceptual <b>data</b> <b>model</b> for Hyrcanian forest management...|$|R
50|$|The NIEM <b>data</b> <b>model</b> uses {{concepts}} {{originating from}} object-oriented programming (OOP). OOP defines a {{class as a}} specific entity in the <b>data</b> <b>model,</b> which may represent a real-world object but may also represent any conceptual object, such as relationships and messages. An object's properties are said to describe the object. When the NIEM XML Schemas are generated from the NIEM <b>data</b> <b>model,</b> <b>data</b> <b>model</b> classes are represented as XML Schema types, and <b>data</b> <b>model</b> properties are represented as XML elements and attributes.|$|R
40|$|Abstract: In {{this paper}} we present the current {{situation}} at the field of hypermedia <b>data</b> <b>modelling.</b> We list {{advantages and disadvantages of}} the most primitive hypermedia <b>data</b> <b>model,</b> i. e. the nodelink <b>data</b> <b>model</b> and the models of the second generation hypermedia, such as the Hyperwave <b>data</b> <b>model</b> and HM-Data Model. We believe that an introduction of a semantic hypermedia <b>data</b> <b>model,</b> called the HC-Data Model can solve a number of problems in modern hypermedia <b>data</b> <b>modelling.</b> Thus, we give a formal definition of the HC-Data Model. At the end we in short present Structure Editor, an application that supports the HC-Data Model. 1...|$|R
