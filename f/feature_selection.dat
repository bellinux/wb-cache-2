10000|2540|Public
25|$|Rough set {{methods can}} be applied as a {{component}} of hybrid solutions in machine learning and data mining. They {{have been found to be}} particularly useful for rule induction and <b>feature</b> <b>selection</b> (semantics-preserving dimensionality reduction). Rough set-based data analysis methods have been successfully applied in bioinformatics, economics and finance, medicine, multimedia, web and text mining, signal and image processing, software engineering, robotics, and engineering (e.g. power systems and control engineering). Recently the three regions of rough sets are interpreted as regions of acceptance, rejection and deferment. This leads to three-way decision making approach with the model which can potentially lead to interesting future applications.|$|E
2500|$|Quafafou M. and Boussouf M. (2000). Generalized rough sets based <b>feature</b> <b>selection.</b> Journal Intelligent Data Analysis, 4:1 pp3 – 17 ...|$|E
2500|$|Dimensionality {{reduction}} [...] {{the process}} of {{reducing the number of}} random variables under consideration, and can be divided into <b>feature</b> <b>selection</b> and feature extraction.|$|E
5000|$|Iowa Author Award 2009 Boston Authors Club 2009 award: “highly recommended.” [...] Progressive Book Club <b>featured</b> <b>selection.</b> History Book Club <b>featured</b> <b>selection.</b> Boston Globe bestseller (nonfiction) Amazon.com bestseller Reviewed in over 100 {{newspapers}} and magazines Over 30,000 hardcover copies sold Korean, Mandarin, and Arabic translations ...|$|R
30|$|The {{large number}} of {{extracted}} features causes that some among them share the same information content. This {{will lead to a}} dimensionality problem. The obvious solution is the <b>features</b> <b>selection,</b> that is, reducing the dimension by selecting some features and discarding the rest. A features space with a smaller dimension will allow more accurate classification (regardless the classifier) due to data organization and projecting data to another space in which the discrimination is more obvious. The output of the <b>features</b> <b>selection</b> process is the input of the feed-forward neural network. Then, <b>features</b> <b>selection</b> also affects the neural network convergence and allows speeding its learning process and reducing its size. Among several possible <b>features</b> <b>selection</b> algorithms, we will investigate principal component analysis (PCA) and linear discriminate analysis (LDA).|$|R
50|$|The album <b>features</b> <b>selections</b> chosen {{exclusively}} from Mantronix's 1990 album, This Should Move Ya.|$|R
2500|$|El-Manzalawy, Y., Hsieh, T-Y., Shivakumar, M., Kim, D., and Honavar, V. (2017). Min-Redundancy and Max-Relevance Multi-view <b>Feature</b> <b>Selection</b> for Predicting Ovarian Cancer Survival using Multi-omics Data. In: Translational Bioinformatics Conference.|$|E
2500|$|... {{which is}} {{symmetric}} and nonnegative. This quantity {{has sometimes been}} used for <b>feature</b> <b>selection</b> in classification problems, where P and Q are the conditional pdfs of a feature under two different classes.|$|E
2500|$|Adjusted R2 {{does not}} have the same {{interpretation}} as R2—while R2 is a measure of fit, adjusted R2 is instead a comparative measure of suitability of alternative nested sets of explanators. [...] As such, care must be taken in interpreting and reporting this statistic. Adjusted R2 is particularly useful in the <b>feature</b> <b>selection</b> stage of model building.|$|E
50|$|The musical {{introduction}} to the concert <b>featured</b> <b>selections</b> from Brian Eno's Apollo: Atmospheres and Soundtracks.|$|R
5000|$|The Call Of The Tropics is a {{compilation}} <b>featuring</b> <b>selections</b> {{from the first}} three LPs ...|$|R
5000|$|Limited-edition CD <b>featuring</b> <b>selections</b> and edits {{from the}} An Angel Moves Too Fast To See box-set.|$|R
2500|$|One of {{the prime}} {{differences}} between Lasso and ridge regression is that in ridge regression, as the penalty is increased, all parameters are reduced while still remaining non-zero, while in Lasso, increasing the penalty will cause {{more and more of}} the parameters to be driven to zero. This is an advantage of Lasso over ridge regression, as driving parameters to zero deselects the features from the regression. Thus, Lasso automatically selects more relevant features and discards the others, whereas Ridge regression never fully discards any features. Some <b>feature</b> <b>selection</b> techniques are developed based on the LASSO including Bolasso which bootstraps samples, [...] and FeaLect which analyzes the regression coefficients corresponding to different values of [...] to score all the features.|$|E
2500|$|Weighted {{correlation}} network analysis, {{also known}} as weighted gene co-expression network analysis (WGCNA), is a widely used data mining method especially [...] for studying [...] biological networks based on pairwise [...] correlations between variables. While it {{can be applied to}} most [...] high-dimensional data sets, it has been most widely used in [...] genomic applications. [...] It allows one to define modules (clusters), intramodular hubs, and network nodes with regard to module membership, to study the relationships between co-expression modules, and to compare the network topology of different networks (differential network analysis). WGCNA can be used as [...] data reduction technique (related to oblique [...] factor analysis [...] ), as [...] clustering method (fuzzy clustering), as [...] <b>feature</b> <b>selection</b> method (e.g. as gene screening method), as framework for integrating complementary (genomic) data (based on weighted correlations between quantitative variables), and as [...] data exploratory technique. Although WGCNA incorporates traditional data exploratory techniques, its intuitive network language and analysis framework transcend any standard analysis technique. [...] Since it uses network methodology and is well suited for integrating complementary genomic data sets, it can be interpreted as systems biologic or systems genetic data analysis method. By selecting intramodular hubs in consensus modules, WGCNA also gives rise to network based [...] meta analysis techniques ...|$|E
50|$|<b>Feature</b> <b>selection</b> {{with social}} media data - Transforming <b>feature</b> <b>selection</b> {{to harness the}} power of social media.|$|E
5000|$|The July 1970 {{issue of}} Album <b>featured</b> <b>selections</b> from the 50,000 prints in the George Eastman House Collections ...|$|R
50|$|The Earth 2100 website, however, does <b>feature</b> <b>selections</b> of user-created videos {{representing}} the crisis points of 2015, 2050, and 2100.|$|R
5000|$|Ryan and Jimmy And the Well in Africa That Brought Them Together by Herb Shoveller, a <b>featured</b> <b>selection</b> on Oprah.com.|$|R
50|$|MPCA features: Supervised MPCA <b>feature</b> <b>selection</b> {{is used in}} object {{recognition}} while unsupervised MPCA <b>feature</b> <b>selection</b> is employed in visualization task.|$|E
50|$|<b>Feature</b> <b>selection</b> {{techniques}} {{should be}} distinguished from feature extraction. Feature extraction creates new features from functions of the original features, whereas <b>feature</b> <b>selection</b> returns {{a subset of the}} features. <b>Feature</b> <b>selection</b> techniques are often used in domains where there are many features and comparatively few samples (or data points). Archetypal cases for the application of <b>feature</b> <b>selection</b> include the analysis of written texts and DNA microarray data, where there are many thousands of features, and a few tens to hundreds of samples.|$|E
50|$|<b>Feature</b> <b>Selection</b> Toolbox (FST) is {{software}} {{primarily for}} <b>feature</b> <b>selection</b> {{in the machine}} learning domain, written in C++, developed at the Institute of Information Theory and Automation (UTIA), of the Czech Academy of Sciences.|$|E
40|$|In {{this paper}} we discuss the {{construction}} of software products from customer-specific <b>feature</b> <b>selections.</b> We address variability management with the Feature Description Language (FDL) to capture variation points of product line architectures. We describe feature packaging which covers selecting and packaging implementation components according to <b>feature</b> <b>selections</b> using the autobundle tool. Finally, we discuss a generic approach, based on the abstract factory design pattern, to make instantiated (customer-specific) variability accessible in applications...|$|R
50|$|A soundtrack {{album was}} {{released}} on December 9, 2016, by Interscope Records, <b>featuring</b> <b>selections</b> from Hurwitz's score and songs performed by the cast.|$|R
50|$|The film's {{musical score}} was {{composed}} by James Horner. Milan Records released an album <b>featuring</b> <b>selections</b> from the score on August 2, 1994.|$|R
5000|$|Mutual {{information}} {{has been used as}} a criterion for <b>feature</b> <b>selection</b> and feature transformations in machine learning. It can be used to characterize both the relevance and redundancy of variables, such as the minimum redundancy <b>feature</b> <b>selection.</b>|$|E
50|$|The first {{generation}} of <b>Feature</b> <b>Selection</b> Toolbox (FST1) was a Windows application with user interface allowing users to apply several sub-optimal, optimal and mixture-based <b>feature</b> <b>selection</b> methods on data stored in a trivial proprietary textual flat file format.|$|E
50|$|Minimum {{redundancy}} <b>feature</b> <b>selection</b> is an algorithm {{frequently used}} in a method to accurately identify characteristics of genes and phenotypes and narrow down their relevance and is usually described in its pairing with relevant <b>feature</b> <b>selection</b> as Minimum Redundancy Maximum Relevance (mRMR).|$|E
3000|$|Which top-selection (number of features) is the best: 50, 100, 200 or 300 {{and which}} <b>features</b> <b>selection</b> method: DF, FS and GR is superior? [...]...|$|R
3000|$|Despite that SFFS method {{proved the}} superiority over SFS [32]; however, same results were {{obtained}} when {{small number of}} <b>features</b> <b>selection</b> were needed (i.e., [...]...|$|R
5000|$|Samuel Beckett: The Complete Short Prose, 1928-1989, Grove Press, 1995. Readers' Subscription <b>featured</b> <b>selection,</b> 1996-7; and New York Times Book Review [...] "New and Noteworthy Paperbacks".|$|R
5000|$|In machine {{learning}} and statistics, <b>feature</b> <b>selection,</b> {{also known as}} variable selection, attribute selection or variable subset selection, {{is the process of}} selecting a subset of relevant features (variables, predictors) for use in model construction. <b>Feature</b> <b>selection</b> techniques are used for four reasons: ...|$|E
5000|$|... #Subtitle level 2: Minimum-redundancy-maximum-relevance (mRMR) <b>feature</b> <b>selection</b> ...|$|E
5000|$|<b>Feature</b> <b>Selection</b> Award: Documentary: Adventures in Plymptoons ...|$|E
50|$|Blues for Duke is {{an album}} by saxophonist Sonny Stitt <b>featuring</b> <b>selections</b> {{associated}} with Duke Ellington recorded in 1975 and released on the Muse label in 1978.|$|R
50|$|The book, <b>featuring</b> <b>selections</b> {{gathered}} from Harcourt's extensive personal musical knowledge, comprises numerous essays on various genres and movements, and Harcourt includes several esoteric and original categories, as well.|$|R
50|$|Recently, the National Art Museum of Sport has <b>featured</b> <b>selections</b> {{from their}} {{permanent}} collections for basketball and racing exhibits on Monument Circle in downtown Indianapolis at the IPL headquarters.|$|R
