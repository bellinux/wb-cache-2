6|19|Public
50|$|The {{idea of a}} {{congruence}} {{is probably}} better explained by giving an example than by a definition. Consider the smooth manifold R². Vector fields can be specified as first order linear partial differential operators, such as These correspond {{to a system of}} first order linear ordinary differential equations, in this casewhere dot denotes a derivative with respect to some (<b>dummy)</b> <b>parameter.</b> The solutions of such systems are families of parameterized curves, in this caseThis family is what is often called a congruence of curves, or just congruence for short.|$|E
40|$|This {{vignette}} {{of examples}} supplements the main flexsurv user guide. Keywords:˜survival. 1. Examples of custom distributions 1. 1. Proportional hazards generalized gamma model Crowther and Lambert (2013) discuss using the stgenreg Stata package {{to construct a}} pro-portional hazards parameterisation of the three-parameter generalised gamma distribution. A similar trick {{can be used in}} flexsurv. A four-parameter custom distribution is created by defining its hazard (and cumulative hazard) functions. These are obtained by multiplying the built-in functions hgengamma and Hgengamma by an extra <b>dummy</b> <b>parameter,</b> which is used as the location parameter of the new distribution. The intercept of this parameter is fixed at 1 when calling flexsurvreg, so that the new model is no more complex than the generalized gamma AFT model fs 3, but covariate effects on the <b>dummy</b> <b>parameter</b> are now interpreted as hazard ratios. > library(flexsurv) > hgengammaPH <- function(x, dummy, mu= 0, sigma= 1, Q) { + dummy * hgengamma(x=x, mu=mu, sigma=sigma, Q=Q...|$|E
40|$|The BioRID II {{has been}} {{recommended}} {{to be used}} in future legislative dynamic rear-end impact seat performance tests. Recommended injury criteria and assessment reference values to be used with the dummy is however still pending. This is mainly due to the incomplete understanding of the injury site and mechanisms responsible for the symptoms presented after such impacts. This lack of biomechanical data limits the possibility to evaluate any proposed injury criteria and associated reference values. The aim with this study is to address these limitations by comparing crash test <b>dummy</b> <b>parameter</b> values from performed sled tests with real-life accident data. The results are expected to indicate the injury predictability of the complete sled test method, which includes performance criteria, the use of generic sled acceleration pulse, the use of the BioRID II and its current positioning procedure, etc. Real-life injury risk was calculated for groups with similar seat designs from data provided by Folksam. By introducing grouped data, i. e. by dividing applicable data into groups with similar characteristics, the reliability of the insurance data increased while the dummy measurements remained constant. Two different injury risks were used in this study; those that had documented symptoms for more than 1 month and those that were classified as a permanent impairment as the consequence of a rear-end impact. The injury risks for the groups were compared to single crash test <b>dummy</b> <b>parameter</b> values from sled tests performed with a BioRID II in 16 kph medium Euro-NCAP pulse. In the comparison, 12 seat groups were compared with 6665 insurance cases (range from 94 to 1575 cases/group). Regression coefficients (R 2) were calculated. The analysis of groups with similar seat design provided the most reliable results. The analysis showed that NIC, upper neck shear force, vertical head acceleration and lower neck bending moment were the parameters that best predicted the risk of developing permanent impairment given that the occupant had initial symptoms following a rear-end impact. Similarly, NIC, vertical head acceleration and lower neck moment were parameters that best predicted the risk of short term (> 1 month) symptoms. These results are supported by recent studies...|$|E
40|$|Periodically {{integrated}} {{time series}} require a periodic differencing filter {{to remove the}} stochastic trend. A non-periodic integrated time series needs the first-difference filter for similar reasons. When the changing sea- sonal fluctuations for the non-periodic integrated series can be described by seasonal dummy variables for which the corresponding parameters are not constant within the sampie, such a series may not be easily & stinguished from a periodically integrated time series. In this paper, nested and non-nested testing procedures are proposed to distinguish between these two alternative stochastic and non-stochastic seasonal processes, When it is assumed {{there is a single}} unknown structural break in the seasonal <b>dummy</b> <b>parameters.</b> Several empirical examples using quarterly real macroeconomic time series for the United Kingdom illustrate the nested and non-nested approaches...|$|R
40|$|We {{propose a}} {{sequential}} learning policy for noisy discrete global optimization and ranking and selection (R&S) problems with high dimensional sparse belief functions, {{where there are}} hundreds or even thousands of features, but {{only a small portion}} of these features contain explanatory power. We aim to identify the sparsity pattern and select the best alternative before the finite budget is exhausted. We derive a knowledge gradient policy for sparse linear models (KGSpLin) with group Lasso penalty. This policy is a unique and novel hybrid of Bayesian R&S with frequentist learning. Particularly, our method naturally combines B-spline basis expansion and generalizes to the nonparametric additive model (KGSpAM) and functional ANOVA model. Theoretically, we provide the estimation error bounds of the posterior mean estimate and the functional estimate. Controlled experiments show that the algorithm efficiently learns the correct set of nonzero parameters even when the model is imbedded with hundreds of <b>dummy</b> <b>parameters.</b> Also it outperforms the knowledge gradient for a linear model...|$|R
30|$|To {{calculate}} interaction effects, {{consider the}} following equation: y=a*Italy+b*Single+c*(Single *Italy), with Italy and Single dummies, and Single*Italy the interaction term <b>dummy.</b> The <b>parameter</b> of French single (relative to reference category) is b, the parameter of Italian single is a+b+c. Similar behaviours between France and Italy are obtained by testing the following equality b=a+b+c, i.e. a+c= 0 for each item of family situation covariate (Table  6).|$|R
40|$|Machine [Leroy 90] {{although}} I have omitted any optimisations {{and added}} a few instructions of my own to handle objects. 3. 6. 1 Overview of the Byte Code Machine The internal state of the byte code machine consists of: a program which {{is a list of}} instructions to be executed, the head of the list is the next one to be executed, an accumulator, an environment, an argument stack and a return stack. Unlike a real computer the accumulator can hold more than just a 32 -bit number, it can hold the value top, an integer, a <b>dummy</b> <b>parameter</b> for types (explained later), a closure or an object (which is similar to a list of closures). This is the advantage of using a byte code machine designed yourself; you can customise it to work with high level data and not just numbers like a real CPU. This simplifies compilation a great deal. The environment is used to store the 'context' of a function. Whenever a function is defined it is stored as a closure which is a function body along with the current to [...] ...|$|E
40|$|Purpose – Mobile {{recommender}} systems aim {{to solve}} the information overload problem by recommending products or services to users of web services on mobile devices, such as smartphones or tablets, at any given point in time and in any possible location. They utilize recommendation methods, such as collaborative filtering or content-based filtering and use {{a considerable amount of}} contextual information in order to provide relevant recommendations. However due to privacy concerns users are not willing to provide the required personal information that would allow their views to be recorded and make these systems usable. Design/methodology/approach – This work is focused on user privacy by providing a method for context privacy-preservation and privacy protection at user interface level. Thus, a set of algorithms {{that are part of the}} method have been designed with privacy protection in mind, which is done by using realistic <b>dummy</b> <b>parameter</b> creation. To demonstrate the applicability of the method, a relevant context-aware dataset has been used to run performance and usability tests. Findings – The proposed method has been experimentally evaluated using performance and usability evaluation tests and is shown that with a small decrease in terms of performance user privacy can be protected. Originality/value – This is a novel research paper that proposes a method for protecting the privacy of mobile recommender systems users when context parameters are used...|$|E
40|$|In the past, EEVC WG 12 and 20 have {{evaluated}} rear-impact dummies {{and reviewed}} associated injury criteria and assessment reference values for seat performance evaluations (Hynd et al. 2007 and Hynd and Carroll 2008). The BioRID II was recommended {{to be used}} in future legislative dynamic rear-end impact seat performance tests. Recommended injury criteria and assessment reference values to be used with the dummy are however still pending. This is mainly due to the incomplete understanding of the injury site and mechanisms responsible for the symptoms presented after such impacts. This lack of biomechanical data limits the possibility to evaluate any proposed injury criteria and associated reference values. The aim {{of this study is to}} address these limitations by comparing crash test <b>dummy</b> <b>parameter</b> values from performed sled tests with real-life accident data. The results are expected to indicate the injury predictability of the complete sled test method, which includes performance criteria, the use of a generic sled acceleration pulse, the use of the BioRID II and its current positioning procedure. Real-life injury risk was calculated for 32 individual car models and for 17 groups of similar seat designs from data provided by Folksam. When grouped data was introduced, i. e. by dividing applicable data into groups with similar seat designs, the reliability of the insurance data was raised, while the dummy measurements remained constant. The number of insurance cases ranges from 32 to 1023 for individual car models and from 132 to 1023 for groups with similar seat designs. Regression coefficients (r 2) were calculated and the data presented graphically. Two types of injury risks were used in this study: those that had documented symptoms for more than one month and those that were classified as a permanent medical impairment as the consequence of a rear-end impact. These injury risks were compared to crash test <b>dummy</b> <b>parameter</b> values from sled tests performed with a BioRID II in 16 km/h medium Euro-NCAP pulse. It was found that the analysis of groups of similar seat designs provided the most reliable results. Analysing individual data clearly showed that the insurance cases were too low per seat model {{to be used in}} an evaluation of seat performance criteria. In conclusion, the results obtained in the analysis of individual data did not invalidate the results obtained using grouped datasets. This conclusion was based on the observation that the correlations found in the analysis of grouped datasets could exist also for individual car model data. When comparing groups of seats, the analysis showed that the Neck Injury Criterion (NIC), the maximum rearward Occipital Condyle x-displacements in a coordinate system that moves with the T 1 and the maximum L 1 x-acceleration were the parameters that best predicted the risk of developing permanent medical impairment, and symptoms for more than one month given that the occupant had initial symptoms following a rear-end impact. The maximum rearward head rel. T 1 angular displacement, T 1 x-acceleration and upper neck shear load (U. N. Fx, head r. w.) were parameters that also could predict the risk of permanent medical impairment and symptoms for more than one month. These results are supported by recent studies. In comparison with a previous report, this study includes additional seat tests data which allowed additional data points to be included in the regression analysis. An expanded insurance claim database, about three times more insurance claims, was included in the analysis, which made the results more reliable. The insurance data was compensated for differences in the definitions of short term symptoms and permanent medical impairment during the accident data sampling period. This reduced errors that could have been introduced by the market share change during the sampling period for the various vehicle models included in this study. In the future, a logistic regression including error estimation that covers all available insurance and test data should be carried out. The advantage of such an analysis would be that data could be included independent of the number of accidents. Another advantage of this is that a larger proportion of the data would be from tests and real life accidents with newer cars than those included in this study. Therefore the recommended parameters to use in seat evaluations would be more suitable for modern car seat systems...|$|E
3000|$|Another way {{to tackle}} {{discrete}} parameters {{is to apply}} the PSO algorithm modified for binary parameters proposed in [16]. The idea {{is to work with}} <b>dummy</b> continuous <b>parameters,</b> transform them to have the range from 0 to 1, and consider the results as probabilities of the optimization parameters taking value 0 or 1. Therefore, {{there is no need to}} round off the parameters as in the previous algorithm. The transformation function proposed in [16] is a sigmoid limiting function, [...]...|$|R
50|$|An added {{complication}} {{arises in}} languages such as OCaml that support both optional named parameters and partial application. It is impossible in general {{to distinguish between}} a function partly applied, and a function to which a subset of parameters have been provided. OCaml resolves this ambiguity by requiring a positional parameter after all optional named parameters: its presence or absence is used to decide if the function has been fully or partly applied. If all parameters are optional, the implementor may solve the issue by adding a <b>dummy</b> positional <b>parameter</b> of type unit.|$|R
40|$|Abstract—A new algorithm, varying {{dimensional}} {{particle swarm optimization}} (VD-PSO), {{is proposed}} for entities with varying-dimensional components and each component assumes continuous-valued parameters. Such problems are distinct from current benchmark problems where the dimension of the par-ticles is fixed. One well-studied application of VD-PSO is probability density estimation by Gaussian Mixture Models. A particle in VD-PSO includes a discrete number {{as the number of}} components and a set of real-valued component parameters. The number of components varies according to a random scheme, which dictates how many sets of components remain or expand. The component parameters are matched up supporting a linked update. Three other methods, the last two of which are also proposed by us as intuitive attempts to solve the varying dimen-sional problems, are compared with VD-PSO: 1. binary-headered PSO, 2. exhaustive PSO, 3. discrete-headered PSO. Simulations on known data with specified components show that VD-PSO provides a competitive density estimation to the exhaustive PSO, but spends the least wall-clock-time among all algorithms, almost 12 % of the exhaustive PSO, while the binary-headered PSO or discrete-headered PSO do not achieve similar performance, because the binary-headered PSO is adversely affected by the <b>dummy</b> <b>parameters,</b> and the discrete-headered PSO makes too many dimension variations before the parameters are well tuned. The new VD-PSO algorithm is a viable and efficient solution to varying dimensional optimization problems...|$|R
40|$|This paper {{examines}} volatility spillovers from mature {{to emerging}} stock markets and tests {{for changes in}} the transmission mechanism - contagion - during turbulences in mature markets. Tri-variate GARCH-BEKK models of returns in global (mature), regional, and local markets are estimated for 41 emerging market economies (EMEs), with a <b>dummy</b> capturing <b>parameter</b> shifts during turbulent episodes. LR tests suggest that mature markets influence conditional variances in many emerging markets. Moreover, spillover parameters change during turbulent episodes. Conditional variances in most EMEs rise during these episodes, but there is only limited evidence of shifts in conditional correlations between mature and emerging markets...|$|R
50|$|Notion in {{the theory}} of Universal Grammar (UG) and {{language}} acquisition. Parameters specify certain options that are not specified in UG. The values of parameters are not genetically fixed. Thus, language acquisition becomes a process of parameter setting. Linguistic diversity is characterized in terms of the values of parameters, for example the null subject parameter. Certain languages such as Italian and Spanish may have sentences with no overt subject, while other languages such as English must have an overt subject, even in cases in which this is nonreferential (<b>dummy</b> subject). <b>Parameter</b> theory, thus, provides an explanation for systematic syntactic variation between languages and imposes restrictions on the number of choices which the language learner has to make.|$|R
40|$|In 1996, the European Community {{released}} {{new regulations}} relating to frontal impact vehicle crash. One of the tests, the European offset crash, consists of crashing {{the car on}} a deformable barrier at 56 km/h with 40 % of the car impacting on the barrier. The regulations require the <b>dummy</b> injury <b>parameters</b> such as the HIC, chest deflection, chest acceleration, femur loads and the tibia index be lower than established values. MADYMO occupant simulations have been performed on a DaimlerChrysler vehicle in order to accurately predict the tibia index. Many parameters {{play a role in}} the tibia index evaluation such as the toe pan intrusion, the intrusion mode, the intrusion rate and the motion of the accelerator pedal. These simulations have been validated and then used to determine the best restraint system and structural package to reduce this injury paramete...|$|R
40|$|The trivial {{equilibrium}} of a van der Pol–Duffing oscillator with a nonlinear {{feedback control}} may lose its stability via Hopf bifurcations, {{when the time}} delay involved in the feedback control reaches certain values. Nonresonant Hopf–Hopf interactions may occur in the controlled van der Pol–Duffing oscillator when the corresponding characteristic equation has two pairs of purely imaginary roots. With the aid of normal form theory and centre manifold theorem {{as well as a}} perturbation method, the dynamic behaviour of the nonresonant co-dimension two bifurcation is investigated by studying the possible solutions and their stability of the four-dimensional ordinary differential equations on the centre manifold. In the vicinity of the nonresonant Hopf bifurcation, the oscillator may exhibit the initial equilibrium solution, two periodic solutions as well as a quasi-periodic solution on a two-dimensional torus, depending on the <b>dummy</b> unfolding <b>parameters</b> and nonlinear terms. The analytical predictions are found to be in good agreement with the results of numerical integration of the original delay differential equation...|$|R
40|$|This Working Paper {{should not}} be {{reported}} as representing {{the views of the}} IMF. The views expressed in this Working Paper are those of the author(s) and do not necessarily represent those of the IMF or IMF policy. Working Papers describe research in progress by the author(s) and are published to elicit comments and to further debate. This paper examines volatility spillovers from mature to emerging stock markets and tests for changes in the transmission mechanism—contagion—during turbulences in mature markets. Tri-variate GARCH-BEKK models of returns in global (mature), regional, and local markets are estimated for 41 emerging market economies (EMEs), with a <b>dummy</b> capturing <b>parameter</b> shifts during turbulent episodes. LR tests suggest that mature markets influence conditional variances in many emerging markets. Moreover, spillover parameters change during turbulent episodes. Conditional variances in most EMEs rise during these episodes, but there is only limited evidence of shifts in conditional correlations between mature an...|$|R
40|$|This paper {{examines}} volatility spillovers from mature {{to emerging}} stock markets and tests {{for changes in}} the transmission mechanism-contagion-during turbulences in mature markets. Tri-variate GARCH-BEKK models of returns in global (mature), regional, and local markets are estimated for 41 emerging market economies (EMEs), with a <b>dummy</b> capturing <b>parameter</b> shifts during turbulent episodes. LR tests suggest that mature markets influence conditional variances in many emerging markets. Moreover, spillover parameters change during turbulent episodes. Conditional variances in most EMEs rise during these episodes, but there is only limited evidence of shifts in conditional correlations between mature and emerging markets. Stock markets;Emerging markets;Developed countries;Economic models;correlations, stock market, emerging stock markets, financial contagion, covariance, stock market indices, equations, samples, correlation, financial markets, statistics, standard errors, stock returns, maximum likelihood method, equity markets, equation, autocorrelation, descriptive statistics, correlation analysis, surveys, probability, skewness, financial market, covariances, kurtosis, dummy variable, time series, asset markets, sample sizes, bootstrap, heteroscedasticity, financial economics, stock market crash, international finance, statistic, stock market crashes, maximum likelihood estimation, probability models, currency crisis, local stock markets, bond, equity market, stock indices, probabilities...|$|R
30|$|In {{addition}} to the notable differences in the sorting behavior {{of men and women}} across occupational groups revealed by the covariance parameters, the other parameters also reveal {{differences between men and women}} during the two eras. We use age as a proxy for the different regulation and structural changes that people born in different cohorts were facing. We use the continuous variable instead of age intervals to avoid multicollinearity with the educational <b>dummies.</b> The estimated <b>parameters</b> are significant for women during the communist period and in 1994 and 2000 but only for men in 1994, and they indicate that the probability of choosing a female-dominated occupation increased with age during these years.|$|R
40|$|The spatial {{distributions}} of cities {{fall into two}} groups: one is the simple distribution with characteristic scale (e. g. exponential distribution), {{and the other is}} the complex distribution without characteristic scale (e. g. power-law distribution). The latter belongs to scale-free distributions, which can be modeled with fractal geometry. However, fractal dimension is not suitable for the former distribution. In contrast, spatial entropy can be used to measure any types of urban distributions. This paper is devoted to generalizing multifractal parameters by means of dual relation between Euclidean and fractal geometries. The main method is mathematical derivation and empirical analysis, and the theoretical foundation is the discovery that the normalized fractal dimension is equal to the normalized entropy. Based on this finding, a set of useful spatial indexes termed <b>dummy</b> multifractal <b>parameters</b> are defined for geographical analysis. These indexes can be employed to describe both the simple distributions and complex distributions. The dummy multifractal indexes are applied to the population density distribution of Hangzhou city, China. The calculation results reveal the feature of spatio-temporal evolution of Hangzhou's urban morphology. This study indicates that fractal dimension and spatial entropy can be combined to produce a new methodology for spatial analysis of city development. Comment: 23 pages, 3 figures, 5 table...|$|R
40|$|In this study, {{the authors}} present a novel fuzzy-logic signal-processing and sensor-fusion {{algorithm}} with quaternion implementation to compute <b>dummy</b> kinematic <b>parameters</b> {{in a vehicle}} crash event using inertial sensing. This algorithm is called Quaternion Fuzzy Logic Adaptive Signal Processing for Biomechanics (QFLASP-B). This algorithm is efficient and uses 3 rates obtained using gyroscopes and 3 accelerations obtained using accelerometers (one gyro and accelerometer pair per axis) to compute kinematic parameters in a crash event. In this study, this QFLASP-B algorithm was validated using MSC-ADAMS and Life-Mod simulation software. In virtual simulations of crash testing, the problem of forward kinematics was solved using MSC-ADAMS and Life-Mod to obtain body accelerations and body angular velocities. The inverse kinematic problem of computing inertial solution using body rates and accelerations was solved using QFLASP-B. The results of these two analyses were then compared. The results revealed close similarities. In the experimental validation, the solution obtained from the Nine Accelerometer Package (NAP) was compared with the solution obtained from the three gyros and three accelerometers, or Inertial Measurement Unit (IMU), using the QFLASP-B algorithm for head orientation computation. These results were also closely aligned. The QFLASP-B algorithm is computationally efficient and versatile. It is capable of very high data rates enabling real-time solution computation and kinematic parameters determination. The adaptive filtering in QFLASP-B enables engineers to use low-cost MEMS gyroscopes and accelerometers, about $ 30 each, which are typically noisy and show significant temperature dependence and bias drift—both short-term and long-term—to obtain meaningful and accurate results with the superior signalprocessing and sensor-fusion algorithm. It is anticipated that this inertial tracking/sensing approach will provide an inexpensive alternative for engineers interested in measuring kinematic parameters in a crash event...|$|R
40|$|A {{series of}} {{software}} tools {{has been developed}} for numerical process optimization with respect to structure properties of directionally solidified turbine blades. This software {{is part of the}} 3 D-FE-code CASTS and includes the following moduls: - A quality analysis program which assesses structure properties of a component: This program calculates scalar values related to material qualities of a turbine blade. Different quality requirements for a component - determined by production process and operating conditions, including specific restrictions (e. g. low microporosity, planar interface) - can be quantified. - A new data structure for the thermal FE-analysis which allows variable sets of process parameters, e. g. transient heater temperatures or withdrawal speed. - An automatic optimization loop: Numerical variation of process parameters taking into account specific process restrictions improves quality of the casting. In order to tackle this complex problem the following algorithms have been implemented and applied: - gradient methods, coupled with a quadratic line search - conjugate gradient method - evolution strategies - quasi Newtonian algorithm. Using an evolution strategy, the solidification time of a directionally solidified dummy blade has been minimized. The withdrawal profile to be optimized was defined by 15 <b>parameters.</b> <b>Dummy</b> blades have been cast in order to demonstrate the performance of this optimization exercise. (orig.) SIGLEAvailable from TIB Hannover: F 96 B 1780 +a / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekBundesministerium fuer Bildung, Wissenschaft, Forschung und Technologie, Bonn (Germany) DEGerman...|$|R
40|$|Many {{seemingly}} different {{problems in}} machine learning, artificial intelligence, and symbolic processing {{can be viewed}} as requiring the discovery of a computer program that produces some desired output for particular inputs. When viewed in this way, the process of solving these problems becomes equivalent to searching a space of possible computer programs for a highly fit individual computer program. The recently developed genetic programming paradigm described herein provides a way to search the space of possible computer programs for a highly fit individual computer program to solve (or approximately solve) a surprising variety of different problems from different fields. In genetic programming, populations of computer programs are genetically bred using the Darwinian principle of survival of the fittest and using a genetic crossover (sexual recombination) operator appropriate for genetically mating computer programs. Genetic programming is illustrated via an example of machine learning of the Boolean 11 -multiplexer function and symbolic regression of the econometric exchange equation from noisy empirical data [...] Hierarchical automatic function definition enables genetic programming to define potentially useful functions automatically and dynamically during a run – much as a human programmer writing a complex computer program creates subroutines (procedures, functions) to perform groups of steps which must be performed with different instantiations of the <b>dummy</b> variables (formal <b>parameters)</b> in more than one place in the main program. Hierarchical automatic function definition is illustrated via the machine learning of the Boolean 11 -parity function...|$|R
40|$|The 21 th {{century is}} the age of information. Every year the amount of new data that is {{accessible}} to analysts get higher. More sensor networks get installed, higher resolutions in both, the data dimensions itself and the time, get possible, more communication over the global network is made and new methods for measuring health, environment, social and finance parameters are developed. If this data {{is used by the}} right, ethical way, it can have a huge impact on our society, pushing the global development and making new technologies possible. Not just since “Big Data” got the buzzword of the year and thousands of new companies try to sell their products for data analysis, it is clear that this amount of data can only be analyzed using fast computers and clever algorithms. In the last decades many researchers developed methods to group data, predict new data or find anomalies. But more data does not only mean more data points, it also means more dimensions. And so, many methods getting slow or does not work anymore. This fact is known as curse of dimensionality. When the number of dimensions get higher, dimensions can be grouped together because they are similar or describe disjunct features. This process is also called feature selection and the groups can be called subspaces. If the data is projected into this subspaces, standard analysis methods can be used. So researchers developed methods for feature selection. They offer good results when used with high number of objects and a high number of dimensions {{and some of them are}} also proven in a theoretical way, but have one common problem: they are really slow. Having a cubic or higher complexity in the number of dimensions, it is not possible to use them for todays or future data sets. For many of them, it is also not possible to use them in a parallel way, which is highly important today and will get essential in the next years. Another problem are the parameters of the algorithms. If there are too many parameters which interfere with each other and are not intuitive, analysts just choose default, random or <b>dummy</b> <b>parameters</b> and are not able to get a good result. It is also very common to have parameters which ranges that heavily depends on the structure of the input data and not only the data size. The perfect case would be a few parameters with intersected effects and fixed ranges. So why is this a problem so relevant, you may ask. Just use a faster computer and more memory or wait a day, a week, a month or even longer. It is important because we are wasting the most important resource we have. It is not water, not energy, not oil, not gold or lithium. It is not knowledge or intelligence. Our most important resource is time and we all are running out of them. I believe that there is a way to get the relevant information faster, just at the moment when you need them. Even when ad-hoc data analysis is not possible today, I believe it is possible in the future. And it will change everything — the way we consume information and media, the art of describing our environment and our society, the way people life and communicate, the methods of research, production, planning and design, even the way we think and decide. As a first, small step to this future I propose a new faster method for feature selection: Graph Based Subspace Search, or GraBaSS. It may not be as exact or mathematically proven as some of the competitors. But it works fast, parallel, the parameters are easy to choose and the results are intuitive. Depending on the chosen parameters, it can be used for automatic and manual data processing. It is also possible to choose how strong the requirement of similarity is, depending on the data set and the ways the subspaces should used later. GraBaSS is build on the insight that in the most subspaces all dimensions are similar to each other, which forms a binary relationship. In contrast to other approaches, which often use a bottom up approach to find subspaces and require an enormous amount of time and space, GraBaSS uses this binary relationship to form a graph. This graph gets optimized and the cliques in the resulting graph are forming the subspaces. This work also discusses how to decide if two dimensions are similar and builds a framework around it. This can be used for further research and as possibility to modify GraBaSS for special purposes, e. g. to find subspaces with only linear relationships or where the dimensions are a special transformation of each other. A special chapter of this thesis discusses the implementation of the algorithm. It explains decisions about programming languages and frameworks and gives useful tips that can be used to implement other methods in a high performance way. The implementation of GraBaSS is provided as Open Source, so that everyone can use it to process its own data and to learn from the implementation. Together with GraBaSS a data backend is provided that stores parsed data like a column storage but gives low level access for good performance. It enables to use the same parsed data set for other tasks like cluster search or outlier detection after doing the subspace analysis. Because dimensions are stored separately, no changes of the storage is required if other tasks are only done in a specific subspace...|$|R

