40|622|Public
50|$|The below table {{provides}} some common <b>filter</b> <b>values.</b>|$|E
5000|$|In 2004, Vox {{introduced}} {{a new series of}} amplifier called the AC30 Custom Classic. It claims to combine attributes of the original AC30 with what Vox sales literature refers to as a [...] "boutique" [...] of features. Specifications of the AC30CC series are two Inputs (Top Boost and Normal), an Input Link Switch for blending channels, a Normal Volume knob, a Brilliance Switch, a Top Boost Volume knob, a Treble knob, an EQ Standard/Custom Switch, Bass and Reverb Controls (Tone, Mix, and a Dwell Switch), Tremolo Speed & Depth knobs, a Tone Cut knob, a Master Volume knob, a Standby and a Power Switch, switchable cathode bias (Output Bias switch: [...] "50 Hot": 33W at full power, [...] "82 Warm": 22W at low power), switchable <b>filter</b> <b>values</b> (vintage/modern), and a true bypass effects loop. Newer AC30CC (or [...] "Custom Classic") reissues (CC1, CC2X CCH head) are produced in China.|$|E
30|$|The alpha and beta PLL <b>filter</b> <b>values</b> are {{dynamically}} adjusted {{according to}} the phase of tracking stage, which will be demonstrated in Section 5.|$|E
3000|$|... where S_x^i and S_x+ 1 ^i are the <b>filtered</b> <b>values</b> at the {{position}} i of features from scale x and x + 1 respectively.|$|R
5000|$|... where [...] is the <b>filtered</b> <b>value</b> of {{the image}} at point , [...] is the {{unfiltered}} value {{of the image}} at point , [...] is the weighting function, and the integral is evaluated over [...]|$|R
3000|$|The main {{difference}} between the Wiener–Kolmogorov <b>filtered</b> <b>values,</b> [...] x_t|∞^K, and the Kalman <b>filter</b> smoothed <b>values,</b> x_t|T^K, results from the dependence of the former on a double infinite sequence of observations (but see Levinson 1947). As shown by Fiorentini (1995) and Gómez (1999), though, they can be made numerically identical by replacing both pre- and post-sample observations by their least squares projections onto the linear span of the sample observations.|$|R
30|$|In the {{simulation}} process, the forgetting factor {{is set to}} 0.125. Four groups of PLL parameters are used for specified number of frames sequentially, and the alpha and beta <b>filter</b> <b>values</b> reduce as the group index increasing: the first group with α[*]=[*] 0.1, β[*]=[*] 0, the second group with α[*]=[*] 0.1, β[*]=[*] 0.0025, the third group with α[*]=[*] 0.05, β[*]=[*] 6.25 × 10 – 4 and the fourth group with α[*]=[*] 0.01, β[*]=[*] 2.5 × 10 – 5. As the alpha and beta <b>filter</b> <b>values</b> reduce the PLL loses its ability to make large adjustments to the timing, but tracks the training symbol to a great resolution.|$|E
30|$|From the {{mathematical}} expression of this circularly symmetric filter at different <b>filter</b> <b>values,</b> {{the number of}} pixels representing the width between the diametrically opposite zero-crossing points in this filter can be calculated. The width of the filter at different <b>filter</b> <b>values</b> is obtained by evaluating the Laplacian of the Gaussian spatial distribution along the x and y directions. The lower the filter value, the smaller is the filter width in the spatial domain and the larger is the pass-band region of the filter in the frequency domain, highlighting fine details or features in the filtered image in the spatial domain. Similarly in the spatial domain, a higher filter value allows coarse features to be highlighted in the filtered image.|$|E
40|$|We {{present a}} real-time, compact {{architecture}} for translation-invariant windowed nonlinear discrete filters represented in computational mathematical morphology (CMM). The architecture enables <b>filter</b> <b>values</b> to be computed in a deterministic {{number of operations}} and thus can be pipelined. Memory requirements are proportional {{to the size of}} the filter basis. A filter is implemented by three steps: 1) each component of a vector observation is used as an index into a table of bit vectors; 2) all retrieved bit vectors are "ANDed" together; and 3) the position of the unique nonzero bit is used as an index to a table of <b>filter</b> <b>values.</b> We motivate and describe CMM and illustrate the architecture through examples. We also formally analyze the representation upon which the architecture rests. A modification of the basic architecture provides for increasing filters...|$|E
3000|$|... where g(p, q, m) {{represents}} the intensity value of voxel at position (p, q, m) of volume, O(i, j, k) {{represents the}} modified neighborhood of voxel at position (i, j, k), f(i, j, k) represents the <b>filtered</b> <b>value,</b> w [...]...|$|R
40|$|In {{order to}} {{accelerate}} full scale fatigue testing, the original flight load spectrum of a combat aircraft was modified by eliminating small amplitude load excursions lower than a prefixed <b>filter</b> <b>value.</b> Different levels of <b>filter</b> <b>value</b> {{were used to}} derive several different test load spectrum having lesser number of fatigue cycles than the flight load spectrum. The fatigue crack growth behavior under all these spectrum load sequences was predicted in two different airframe materials viz., D 16 aluminum and Ti 6 A 14 V alloys using a fatigue crack growth model derived from constant amplitude fatigue crack growth tests, which incorporated crack closure effects. Results show that the basis of spectrum modification should not only be the elimination of small amplitude load cycles {{but also on the}} fatigue crack growth behavior of the structural materials used in airframe construction...|$|R
30|$|In the row or column filtering, the pixels {{located in}} the image block borders also need {{adjacent}} pixels from other thread blocks to compute the DWT. The apron region must also be loaded in the shared memory, but only for reading purposes, because the <b>filtered</b> <b>value</b> of the pixels located there is computed by other thread blocks.|$|R
40|$|Fix: Corrected MSK {{canonical}} isoforms {{for four}} genes as explained in PR # 127. Fix: Changed comma-delimited <b>FILTER</b> <b>values</b> to semicolon-delimited, per VCF specs. maf 2 vcf adds dummy FILTER descriptions to VCF header, to play nice with GATK and bcftools. vcf 2 vcf retains header lines for source, reference, assembly and phasing, if found...|$|E
30|$|As FBMC {{does not}} require any {{transmit}} filter, as its in-built filter matches adjacent channel requirement; its complexity does not change with the occupied bandwidth. For CP-OFDM, complexities of modulation and demodulation are heavily dependent {{on the length of}} the transmit <b>filter.</b> <b>Values</b> in Table 4 do not take complexity of the equalizer into account. When considered, complexity figures are mitigated, and CP-OFDM and FBMC are expected to be comparable.|$|E
40|$|A digital {{correlation}} {{mask that}} induces orthogonality among a prescribed set of reference imagery is described. In this particular implementation, the resulting correlation is undersampled and shift-variant, though {{if it is}} applied to optical correlators, those limitations are removed. A method of introducing orthogonality among the weights among the training set from which <b>filter</b> <b>values</b> are obtained is derived, so that the correlation value from a given filter {{is representative of the}} unique nature of the reference object as compared against the other objects in the training class...|$|E
3000|$|Here, FRDIF(t) and FRDIF(t - 1) are the <b>filtered</b> <b>values</b> of RDIF(t) and RDIF(t - 1), respectively. FHDIF(t) and FHDIF(t– 1) {{follow the}} same convention. The {{relative}} importance of the current and prior measurement period in deciding FRDIF(t) and FHDIF(t) {{is controlled by the}} averaging factor, β. Here, β is chosen depending on the measurement period (T [...]...|$|R
40|$|We study a hedging {{problem for}} an European {{contingent}} claim {{in a certain}} incomplete market model by using a homogeneous <b>filtered</b> <b>value</b> measure. We consider the minimal hedging risk in discrete time model and its continuous limit. As a result, we show that this limit is described by the unique viscosity solution {{of a kind of}} Hamilton-Jacobi-Bellman equation...|$|R
40|$|The author {{studies on}} a hedging problem for an European {{contingent}} claim {{in a certain}} incomplete market model by using a homogeneous <b>filtered</b> <b>value</b> measure. He considers the minimal hedging risk in discrete time model and its continuous limit. As a result, he shows that this limit is described by a viscosity solution of some Hamilton-Jacobi-Bellman equation. ...|$|R
40|$|A {{study was}} {{conducted}} to compare measuring particulate mercury (HgP) with the manual filter method and the automated Tekran system. Simultaneous measurements were conducted with the Tekran and Teflon filter methodologies in the marine and coastal continental atmospheres. Overall, the filter HgP values were on the average 21 % higher than the Tekran HgP, and > 85 % of the data were outside of ± 25 % region surrounding the 1 : 1 line. In some cases the <b>filter</b> <b>values</b> were as much as 3 -fold greater, wit...|$|E
40|$|Aim: This {{pilot study}} investigates the {{heterogeneity}} in focal breast lesions and surrounding tissue assessed on mammography is potentially related to cancer invasion and hormone receptor status. Materials and Methods: Texture analysis (TA) assessed {{the heterogeneity of}} focal lesions and their surrounding tissues in digitised mammograms from 11 patients randomly selected from an imaging archive (Ductal Carcinoma in Situ (DCIS) only, n= 4; Invasive Carcinoma (IC) with DCIS, n= 3; IC only, n= 4). TA utilised band-pass image filtration to highlight image features at different spatial frequencies (filter values: 1. 0 - 2. 5) from fine to coarse texture. The distribution of features in the derived images was quantified using uniformity. Results: Significant differences in uniformity were observed between patient groups for all <b>filter</b> <b>values.</b> With medium scale filtration (filter value = 1. 5) pure DCIS was more uniform (median = 0. 281) than either DCIS with IC (median = 0. 246, p= 0. 0102) or IC (median = 0. 249, p= 0. 0021). Lesions {{with high levels of}} oestrogen receptor expression were more uniform, most notably with coarse filtration (<b>filter</b> <b>values</b> 2. 0 and 2. 5, rs = 0. 812, p = 0. 002). Comparison of uniformity values in focal lesions and surrounding tissue showed significant differences between DCIS with or without IC versus IC (p= 0. 0009). Conclusion: This pilot study shows the potential for computer-based assessments of heterogeneity within focal mammographic lesions and surrounding tissue to identify adverse pathological features in mammographic lesions. The technique warrants further investigation as a possible adjunct to existing computer aided diagnosis systems...|$|E
40|$|Aim To {{undertake}} {{a pilot study}} assessing whether tumour heterogeneity evaluated using computed tomography texture analysis (CTTA) {{has the potential to}} provide a marker of tumour aggression and prognosis in oesophageal cancer. Materials and methods In 21 patients, unenhanced CT images of the primary oesophageal lesion obtained using positron-emission tomography (PET) -CT examinations underwent CTTA. CTTA was carried out using a software algorithm that selectively filters and extracts textures at different anatomical scales between <b>filter</b> <b>values</b> 1. 0 (fine detail) and 2. 5 (coarse features) with quantification as entropy and uniformity (measures image heterogeneity). Texture parameters were correlated with average tumour 2 -[18 F]-fluoro- 2 -deoxy-d-glucose (FDG) uptake [standardized uptake values (SUVmean and SUVmax) ] and clinical staging as determined by endoscopic ultrasound (nodal involvement) and PET-CT (distant metastases). The relationship between tumour stage, FDG uptake, and texture with survival was assessed using Kaplan–Meier analysis. Results Tumour heterogeneity correlated with SUVmax and SUVmean. The closest correlations were found for SUVmean measured as uniformity and entropy with coarse filtration (r = – 0. 754, p < 0. 0001; and r = 0. 748, p = 0. 0001 respectively). Heterogeneity was also significantly greater in patients with clinical stage III or IV for <b>filter</b> <b>values</b> between 1. 0 and 2. 0 (maximum difference at filter value 1. 5 : entropy: p = 0. 027; uniformity p = 0. 032). The median (range) survival was 21 (4 – 34) months. Tumour heterogeneity assessed by CTTA (coarse uniformity) was an independent predictor of survival [odds ratio (OR) = 4. 45 (95...|$|E
5000|$|... #Caption: Four {{examples}} of Rhodes Chroma parameter 1 [...] "Patch" [...] configurations, {{from left to}} right: value = 9 - Series <b>Filter</b> Mode, <b>value</b> = 6 - Parallel Filter Mode with oscillator synchronization, value = 4 - Independent Channel Mode with <b>filter</b> frequency modulation, <b>value</b> = 15 - Variable Mix Filter Mode with ring modulation.|$|R
3000|$|... where FHDIF(t) is the <b>filtered</b> HDIF(t) <b>value</b> {{calculated}} by Equation  8. RI(t), {{calculated by}} Equation  9, indicates {{the rate of}} increase in FHDIF.|$|R
50|$|A TSDB {{allows users}} to create, enumerate, update and destroy various time series and {{organize}} them in some fashion. These series may be organized hierarchically and optionally have companion metadata available with them. The server often supports a number of basic calculations that work on a series as a whole, such as multiplying, adding, or otherwise combining various time series into a new time series. They can also filter on arbitrary patterns defined by {{the day of the}} week, low <b>value</b> <b>filters,</b> high <b>value</b> <b>filters,</b> or even have the values of one series filter another. Some TSDBs also build in additional statistical functions that are targeted to time series data.|$|R
40|$|To {{evaluate}} {{the association of}} computed tomography (CT) texture features of locally advanced rectal cancer with neoadjuvant chemoradiotherapy (CRT) response and disease-free survival (DFS). The institutional review board approved this retrospective study. 95 patients who received neoadjuvant CRT, followed by surgery, for locally advanced rectal cancer were included. Texture features (entropy, uniformity, kurtosis, skewness, and standard deviation) were assessed in pretreatment CT images and obtained without filtration and with Laplacian of Gaussian spatial filter of various <b>filter</b> <b>values</b> (1. 0, 1. 5, 2. 0, and 2. 5). Dworak pathologic grading was used for treatment response assessment. Independent t-test {{was used to compare}} each texture feature between the treatment responder and non-responder groups. DFS was assessed with Kaplan-Meier method, and differences were compared with log-rank test. Cox proportional hazards models were constructed to predict prognosis based on stage, age, and each texture feature. Treatment responders (n = 32) showed significantly lower entropy, higher uniformity, and lower standard deviation in no filtration, fine (1. 0), and medium (1. 5) <b>filter</b> <b>values.</b> Entropy, uniformity, and standard deviation without filtration showed significant difference in DFS in Kaplan-Meier analysis (P = 0. 015, 0. 025, and 0. 038). Homogeneous texture features (≤ 6. 7 for entropy, > 0. 0118 for uniformity, and ≤ 28. 06 for standard deviation) were associated with higher DFS. Entropy, uniformity, and standard deviation were independent texture features in predicting DFS (P = 0. 017, 0. 03, and 0. 036). Homogeneous texture features are associated with better neoadjuvant CRT response and higher DFS in patients with locally advanced rectal cancer...|$|E
40|$|Fingerprint {{identification}} {{is the most}} important area in biometric identification. The quality of the fingerprint image is an important one for a reliable matching process. The contrast of the image can be enhanced at the preprocessing stage of fingerprint matching. Contrast is the difference between two neighboring pixels. In this paper we describe a fuzzy modeling approach which may be used for reducing the noise and increaseing the brightness of the ridges. Fuzzy <b>filter</b> <b>values</b> are analyzed and better results are produced in the image domain. The probabilities of gray values are determined from the position of the input image pixel. The result indicates the good performance of the proposed fuzzy histogram...|$|E
40|$|A {{simple and}} elegant method is {{presented}} to perform anti-aliasing in computer-generated images. The method uses stratified sampling to reduce the occurrence of artifacts in the image and features a B-spline filter, of some desired order, to compute the final luminous intensity at each pixel. The method is scalable through the specification of the filter order. A B-spline filter of order one amounts to a simple anti-aliasing scheme with box filtering. Increasing {{the order of the}} B-spline generates progressively smoother filters. Computation of the <b>filter</b> <b>values</b> is done in a recursive way, as part of a sequence of Newton-Raphson iterations, to obtain the optimal sample positions in screen space. The method is currently used to anti-alias images generated by ray casting of implicit procedural surfaces...|$|E
5000|$|... #Caption: The above filter {{realised}} as {{a ladder}} low-pass <b>filter.</b> Component <b>values</b> {{are given in}} terms of L and C, the component values of a constant k half-section.|$|R
30|$|A {{low-pass}} Butterworth filter is used {{to filter}} the time sequence data. A Butterworth filter is used, as opposed to another type of filter, because of its relatively uniform transfer function scaling over the frequencies of interest. The transfer function is {{the ratio of the}} <b>filtered</b> <b>value</b> to the unfiltered value as a function of frequency. Uniform scaling over the transfer function is important because it does not add extraneous frequency content to the data. This ensures that any peaks in the frequency domain data are from real events, not distortion from the filter.|$|R
40|$|A closure {{model for}} lagrangian {{tracking}} of particles, starting from LES flow data, is presented. The basic {{idea is to}} reconstruct the velocity field from the knowledge of its <b>filtered</b> <b>values</b> on a coarse grid, by means of fractal interpolation. Validation is carried out {{by means of a}} priori tests for turbulent channel flow. DNS data are filtered through a cut-off filter; different filter widths are considered. Three sets of particles having different relaxation times are tracked. Particle statistics are computed, with and without fractal interpolation, and compared to those obtained starting from the DNS flow fields...|$|R
40|$|International audienceA new {{approach}} for easy & fast modeling and optimization of EMI filter used in aircraft application is proposed. A modular description in a user friendly environment allows describing {{the model and}} accounting for all technological parameters of the system: control strategy, inverter model (semiconductors, layout, etc.), filter, cables and AC machine. The model in the frequency domain is automatically built from this description, {{as well as the}} model gradients. In a second step, using this model, an optimization of the <b>filter</b> <b>values</b> can be carried out, using various algorithms, to reach the smallest volume. This methodology {{can also be used to}} investigate the impact of some technological choices of the electrical drive (control strategy or cable characteristics for instance) on the filter volume...|$|E
40|$|The {{experimental}} {{validation of}} a real-time optimization (RTO) {{strategy for the}} optimal operation of a solid oxide fuel cell (SOFC) stack is reported in this paper. Unlike many existing studies, the RTO approach presented here utilizes the constraint-adaptation methodology, which assumes that the optimal operating point lies {{on a set of}} constraints and then seeks to satisfy those constraints in practice via bias update terms. These biases correspond to the difference between predicted and measured outputs and are updated at each steady-state iteration, allowing the RTO to successfully meet the optimal operating conditions of a 6 -cell SOFC stack, despite significant plant-model mismatch. The effects of the bias update <b>filter</b> <b>values</b> and of the RTO frequency on the power tracking and constraint handling are also investigated...|$|E
40|$|A {{number of}} {{behavioural}} phenomena distinguish {{the recognition of}} faces and objects, even when members {{of a set of}} objects are highly similar. Because faces have the same parts in approximately the same relations, individuation of faces typically requires specification of the metric variation in a holistic and integral representation of the facial surface. The direct mapping of a hypercolumn-like pattern of activation onto a representation layer that preserves relative spatial <b>filter</b> <b>values</b> in a two-dimensional (2 D) coordinate space, as proposed by C. von der Malsburg and his associates, may account for many of the phenomena associated with face recognition. An additional refinement, in which each column of filters (termed a 'jet') is centred on a particular facial feature (or fiducial point), allows selectivity of the input into the holistic representation to avoid incorporation of occluding or nearby surfaces. The initial hypercolumn representation also characterizes the first stage of object perception, but the image variation for objects at a given location in a 2 D coordinate space may be too great to yield sufficient predictability directly from the output of spatial kernels. Consequently, objects can be represented by a structural description specifying qualitative (typically, non-accidental) characterizations of an object's parts, the attributes of the parts, and the relations among the parts, largely based on orientation and depth discontinuities (as shown by Hummel & Biederman). A series of experiments on the name priming or physical matching of complementary images (in the Fourier domain) of objects and faces documents that whereas face recognition is strongly dependent on the original spatial <b>filter</b> <b>values,</b> evidence from object recognition indicates strong invariance to these values, even when distinguishing among objects that are as similar as faces...|$|E
30|$|In both cases, {{the model}} {{parameters}} are estimated by maximising the Gaussian log-likelihood {{function of the}} observed data, which can be readily obtained either as a by-product of the Kalman filter prediction equations or from Whittle’s (1962) frequency domain asymptotic approximation. Once the parameters have been estimated, <b>filtered</b> <b>values</b> of the unobserved components can be extracted {{by means of the}} Kalman smoother or its Wiener–Kolmogorov counterpart. These estimation and filtering issues are well understood (see Harvey 1989; Durbin and Koopman 2012 for textbook treatments), and {{the same can be said}} of their efficient numerical implementation (see Commandeur et al. 2011 and the references therein).|$|R
40|$|B-spline filter kernals {{have proved}} useful in many pixel {{sampling}} applications. A cubic B-spline filter kernal, having a width of four pixels, is particularly effective. In distribution ray tracing, pixel filters are evaluated implicitly {{by having the}} density of sampling proportional to the <b>filter</b> <b>value.</b> In this work we present a simple method to generate random samples having an underlying B-spline density function. To reduce error {{it is important to}} stratify the samples, akin to jittering for uniform sampling. We provide an algebraic and a numerical technique for doing this for B-spline kernals of degree 1, 2, and 3. ...|$|R
40|$|The {{dependent}} variables used for computer based meteorological predictions and in plans for oceanographic predictions are wave number and frequency <b>filtered</b> <b>values</b> that retain only scales resolvable by the model. Scales unresolvable by the grid in use become 'turbulence'. Whether or not properly processed data {{are used for}} initial values is important, especially for sparce data. Fickian diffusion with a constant eddy diffusion {{is used as a}} closure for many of the present models. A physically realistic closure based on more modern turbulence concepts, especially one with a reverse cascade at the right times and places, could help improve predictions...|$|R
