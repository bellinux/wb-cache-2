1|7|Public
40|$|Fault {{diagnosis}} (FD) using data-driven methods {{is essential}} for monitoring complex process systems, but its performance is severely affected {{by the quality of}} the used information. Additionally, processing huge amounts of data recorded by modern monitoring systems may be complex and time consuming if no data mining and/or preprocessing methods are employed. Thus, features selection for FD is advisable {{in order to determine the}} optimal subset of features/variables for conducting statistical analyses or building a machine-learning model. In this work, features selection are formulated as an optimization problem. Several relevancy indices, such as Maximum Relevance (MR), Value Difference Metric (VDM), and Fit Criterion (FC), and redundancy indices such as Minimum Redundancy (mR), Redundancy VDM (RVDM), and Redundancy Fit Criterion (RFC) are combined to determine the optimal subset of features. Another approach of features selection is based on the optimal performance of the classifier, which is achieved by a classifier wrapped with genetic algorithm. Efficiency of this strategy is explored considering different classifiers, namely Support Vector Machine (SVM), Decision Tree (DT), K-Nearest Neighbours (KNN) Classifier and Gaussian Naïve Bayes (GNB). A Genetic algorithm (GA), as a Derivative Free Optimization (DFO) technique, has been used due to the robustness to deal with different kinds of problems. The optimal subset of obtained features has been tested with SVM, <b>DT,</b> <b>KNN,</b> and GNB for the Tennessee-Eastman process benchmark with 19 classes. Results show that, when the performance of the classifier is used as the objective function the wrapper method obtains the best features set. Peer ReviewedPostprint (author's final draft...|$|E
30|$|Clearly, the {{proposed}} approach outperforms other existing algorithms. It {{is easy to}} see that {{the proposed}} ABBDT approach outperforms <b>DT,</b> SVM, <b>kNN</b> and BPN, individually.|$|R
40|$|Content based {{retrieval}} {{and recognition}} of objects represented in images is a challenging problem making it an active research topic. Shape analysis {{is one of the}} main approaches to the problem. In this paper we propose the use of a reduced set of features to describe 2 D shapes in images. The design of the proposed technique aims to result in a short and simple to extract shape description. We conducted several experiments for both retrieval and recognition tasks and the results obtained demonstrate usefulness and competiveness against existing descriptors. For the retrieval experiment the achieved bull¿s eye performance is about 60 %. Recognition was tested with three different classifiers: decision trees (<b>DT),</b> k-nearest neighbor (<b>kNN)</b> and support vector machines (SVM). Estimated mean accuracies range from 69 % to 86 % (using 10 -fold cross validation). The SVM classifier presents the best performance, followed by the simple kNN classifier...|$|R
40|$|Activity {{recognition}} (ARs) is {{a classification}} problem that cuts across many domains. The introduction of ARs accuracy {{which may be}} significantly low with decision tree algorithm {{and the use of}} smartphone sensing in previous studies has proven its relevance for effective disaster mitigation in our society. Smartphone sensing is an approach found to be useful for activity recognition to monitor people in large gatherings due to the power of embedded sensors on the handheld devices. In this paper, a multitask activity recognition architecture is proposed for proper monitoring of people in large gatherings to control disaster occurrences in crowd, flood, road and fire accidents using related activity scenario in time of danger. We implement the proposed architecture to determine the outcome of activity recognized with K-nearest neighbour (KNN) for k= 3 and 4 to compare performance to that of weka using accelerometer and digital compass (dc) sensors on the same dataset. The results of ARs accuracy of 100 % and 99 % in weka, 85 % and 89 % with KNN shows an improved performance in both tools. The performance of Multilayer Perceptron (MLP), Support Vector Machine (SVM), Naives baye (NB), Decision tree (<b>DT),</b> against <b>KNN</b> were investigated using precision, recall and f-measure in weka as well. The results show significant improvement with performance parameters on accelerometer and dc against the use of accelerometer sensor only with <b>KNN</b> and <b>DT</b> having low number of classified activity recognized on training and testing data...|$|R
40|$|Electronic nose (EN) systems play a {{significant}} role for gas monitoring and identification in gas plants. Using an EN system which consists of an array of sensors provides a high performance. Nevertheless, this performance is bottlenecked by the high system complexity incorporated with the high number of sensors. In this paper a new EN system is proposed using data sets collected from an in-house fabricated 4 × 4 tin-oxide gas array sensor. The system exploits the theory of compressive sensing (CS) and distributed compressive sensing (DCS) to reduce the storage capacity and power consumption. The obtained results have shown that compressing the transmitted data to 20 % of its original size will preserve the information by achieving a high reconstruction quality. Moreover, exploiting DCS will maintain the same reconstruction quality for just 15 % of the original size. This high quality of reconstruction is explored for classification using several classifiers such as decision tree (<b>DT),</b> K-nearest neighbour (<b>KNN)</b> and extended nearest neighbour (ENN) along with linear discrimination analysis (LDA) as feature reduction technique. CS-based reconstructed data has achieved a 95 % classification accuracy. Furthermore, DCS-based reconstructed data achieved a 98. 33 % classification accuracy which is the same as using original data without compression...|$|R
40|$|Ovarian {{cancer is}} the fifth highest cause of cancer in women and {{the leading cause of}} death from {{gynecological}} cancers. Accurate diagnosis of ovarian cancer from acquired images is dependent on the expertise and experience of ultrasonographers or physicians, and is therefore, associated with inter observer variabilities. Computer Aided Diagnostic (CAD) techniques use a number of different data mining techniques to automatically predict the presence or absence of cancer, and therefore, are more reliable and accurate. A review of published literature in the field of CAD based ovarian cancer detection indicates that many studies use ultrasound images as the base for analysis. The key objective of this work is to propose an effective adjunct CAD technique called GyneScan for ovarian tumor detection in ultrasound images. In our proposed data mining framework, we extract several texture features based on first order statistics, Gray Level Co-occurrence Matrix and run length matrix. The significant features selected using t-test are then used to train and test several supervised learning based classifiers such as Probabilistic Neural Networks (PNN), Support Vector Machine (SVM), Decision Tree (<b>DT),</b> k-Nearest Neighbor (<b>KNN),</b> and Naïve Bayes (NB). We evaluated the developed framework using 1300 benign and 1300 malignant images. Using 11 significant features in KNN/PNN classifiers, we were able to achieve 100 % classification accuracy, sensitivity, specificity, and positive predictive value in detecting ovarian tumor. Even though more validation using larger databases would better establish the robustness of our technique, the preliminary results are promising. This technique could be used as a reliable adjunct method to existing imaging modalities to provide a more confident second opinion on the presence/absence of ovarian tumo...|$|R
40|$|Diabetes Mellitus (DM), {{a chronic}} lifelong condition, is {{characterized}} by increased blood sugar levels. As {{there is no cure}} for DM, the major focus lies on controlling the disease. Therefore, DM diagnosis and treatment is of great importance. The most common complications of DM include retinopathy, neuropathy, nephropathy and cardiomyopathy. Diabetes causes cardiovascular autonomic neuropathy that affects the Heart Rate Variability (HRV). Hence, in the absence of other causes, the HRV analysis can be used to diagnose diabetes. The present work aims at developing an automated system for classification of normal and diabetes classes by using the heart rate (HR) information extracted from the Electrocardiogram (ECG) signals. The spectral analysis of HRV recognizes patients with autonomic diabetic neuropathy, and gives an earlier diagnosis of impairment of the Autonomic Nervous System (ANS). Significant correlations with the impaired ANS are observed of the HRV spectral indices obtained by using the Discrete Wavelet Transform (DWT) method. Herein, in order to diagnose and detect DM automatically, we have performed DWT decomposition up to 5 levels, and extracted the energy, sample entropy, approximation entropy, kurtosis and skewness features at various detailed coefficient levels of the DWT. We have extracted relative wavelet energy and entropy features up to the 5 th level of DWT coefficients extracted from HR signals. These features are ranked by using various ranking methods, namely, Bhattacharyya space algorithm, t-test, Wilcoxon test, Receiver Operating Curve (ROC) and entropy. The ranked features are then fed into different classifiers, that include Decision Tree (<b>DT),</b> K-Nearest Neighbor (<b>KNN),</b> Naïve Bayes (NBC) and Support Vector Machine (SVM). Our results have shown maximum diagnostic differentiation performance by using a minimum number of features. With our system, we have obtained an average accuracy of 92. 02 %, sensitivity of 92. 59 % and specificity of 91. 46 %, by using DT classifier with ten-fold cross validatio...|$|R
40|$|Machine Learning {{algorithms}} {{have been}} widely used to solve various kinds of data classification problems. Classification problem especially for high dimensional datasets have attracted many researchers in order to find efficient approaches to address them. However, the classification problem has become very complicated and computationally expensive, especially when the number of possible different combinations of variables is so high. In this research, we evaluate the performance of four basic classifiers (naïve Bayes, k-nearest neighbour, decision tree and rule induction), ensemble classifiers (bagging and boosting) and Support Vector Machine. We also investigate two widely-used feature selection algorithms which are Genetic Algorithm (GA) and Particle Swarm Optimization (PSO). Our experiments show that feature selection algorithms especially GA and PSO significantly reduce the number of features needed as well as greatly reduce the computational cost. Furthermore, these algorithms do not severely reduce the classification accuracy and in some cases they can improve the accuracy as well. PSO has successfully reduced the number of attributes of 9 datasets to 12. 78 % of original attributes on average while GA is only 30. 52 % on average. In terms of classification performance, GA is better than PSO. The datasets reduced by GA have better classification performance than their original ones on 5 of 9 datasets while the datasets reduced by PSO have their classification performance improved in only 3 of 9 datasets. The total running time of four basic classifiers (NB, <b>kNN,</b> <b>DT</b> and RI) on 9 original datasets is 68, 169 seconds while the total running time of the same classifiers on GA-reduced datasets is 3, 799 seconds and on PSO-reduced dataset is only 326 seconds (more than 209 times faster). We applied ensemble classifiers such as bagging and boosting as a comparison. Our experiment shows that bagging and boosting do not give a significant improvement. The average improvement of bagging when applied to nine datasets is only 0. 85 % while boosting average improvement is 1. 14 %. Ensemble classifiers (both bagging and boosting) outperforms single classifier in 6 of 9 datasets. SVM has been proven to perform much better when dealing with high dimensional datasets and numerical features. Although SVM work well with default value, the performance of SVM can be improved significantly using parameter optimization. Our experiment shows SVM parameter optimization using grid search always finds near optimal parameter combination within the given ranges. SVM parameter optimization using grid search is very powerful and it is able to improve the accuracy significantly. Unfortunately, grid search is very slow; therefore it is very reliable only in low dimensional dataset with few parameters. SVM parameter optimization using Evolutionary Algorithm (EA) can be used {{to solve the problem of}} grid search. EA has proven to be more stable than grid search. Based on average running time, EA is almost 16 times faster than grid search (294 seconds compare to 4680 seconds). Overall, SVM with parameter optimization outperforms other algorithms in 5 of 9 datasets. However, SVM does not perform well in datasets which have non-numerical attributes. <br/...|$|R

