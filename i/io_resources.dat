10|13|Public
50|$|The PowerNode 9080 was a dual {{processor}} 32-bit Superminicomputer produced by Fort Lauderdale, Florida based electronics company Gould Electronics in the 1980s. Its UTX/32 4.3BSD Berkeley Unix-based operating system {{was one of}} the very first multi-processor shared memory implementations of Unix, although the processors operated in a Master-Slave configuration with a Mutual Exclusion (MutEx) lock on all Kernel <b>IO</b> <b>resources.</b> Machines could be configured for either single or {{dual processor}} operation.|$|E
40|$|Achieving {{predictable}} {{performance in}} shared cloud storage services is hard. Tenants want reservations {{in terms of}} system-wide application-level throughput, but the provider must ultimately deal with low-level <b>IO</b> <b>resources</b> at each storage node where contention arises. Such a guarantee has thus proven elusive, due to the complexities inherent to modern storage stacks: non-uniform IO amplification, unpredictable IO interference, and non-linear IO performance. This paper presents Libra, a local IO scheduling framework designed for a shared SSD-backed key-value storage system. Libra guarantees per-tenant application-request throughput while achieving high utilization. To accomplish this, Libra leverages two techniques. First, Libra tracks the IO resource consumption of a tenant’s application-level requests across complex storage stack interactions, down to low-level IO operations. This allows Libra to allocate per-tenant <b>IO</b> <b>resources</b> for achieving app-request reservations based on their dynamic IO usage profile. Second, Libra uses a disk-IO cost model based on virtual IO operations (VOP) that captures the non-linear relationship between SSD IO bandwidth and IO operation (IOP) throughput. Using VOPs, Libra can both account for {{the true cost of}} an IOP and determine the amount of provisionable <b>IO</b> <b>resources</b> available under IO interference. An evaluation shows that Libra, when applied to a LevelDB-based prototype with SSD-backed storage, satisfies tenant app-request reservations and achieves accurate low-level VOP allocations over a range of workloads, while still supporting high utilization. 1...|$|E
30|$|Using an {{embedded}} IO solution {{also offers}} other significant {{benefits to the}} FPGA vendor or the data centre that is offering FPGA nodes as Infrastructure as a Service (IaaS). The integration of future IO controllers yet to be released (such as SATA controllers), will not require their own individual FIFO and Bridge IP logic for integration to the accelerator design. Furthermore, these <b>IO</b> <b>resources</b> can be virtualized over multiple users’ accelerators without comprising on security by allowing each user to access the IO controller directly.|$|E
40|$|Virtualized servers run {{a diverse}} set of virtual {{machines}} (VMs), ranging from interactive desktops to test and development environments and even batch workloads. Hypervisors {{are responsible for}} multiplexing the underlying hardware resources among VMs while providing desired isolation using various resource management controls. Existing methods [3, 43] provide many knobs for allocating CPU and memory to VMs, but support for I/O resource allocation has been quite limited. <b>IO</b> <b>resource</b> management in a hypervisor introduces significant new challenges and needs more extensive controls compared to the commodity operating systems. This paper introduces a novel algorithm for <b>IO</b> <b>resource</b> allocation in a hypervisor. Our algorithm, mClock, supports proportional-share fairness subject to a minimum reservation and a maximum limit on the IO allocation for VMs. We present the design and implementation of mClock as a prototype inside VMware ESX hypervisor. Our results indicate that these rich set of QoS controls are quite effective in isolating VM performance and providing lower latencies to applications. We also show adaptation of mClock (called dmClock) to a distributed storage environment, where storage is jointly provided by multiple nodes. ...|$|R
40|$|Virtualized servers run {{a diverse}} set of virtual {{machines}} (VMs),rangingfrominteractivedesktopstotestanddevelopmentenvironmentsandevenbatchworkloads. Hypervisorsareresponsibleformultiplexingtheunderlying hardware resources among VMs while providing them the desired {{degree of isolation}} using resource management controls. Existing methods provide many knobs forallocatingCPUandmemorytoVMs,butsupportfor controlof IO resourceallocationhasbeenquitelimited. IOresourcemanagementinahypervisorintroducessignificant new challenges and needs more extensive controlsthanincommodityoperatingsystems. This paper introduces a novel algorithm for <b>IO</b> <b>resource</b> allocation in a hypervisor. Our algorithm, mClock, supports proportional-share fairness subject to minimum reservations and maximum limits on the IO allocations for VMs. We present the design of mClock andaprototypeimplementationinsidetheVMwareESX server hypervisor. Our results indicate that these rich QoS controlsare quite effective in isolating VM performanceandprovidingbetterapplicationlatency. We also show an adaptation of mClock (called dmClock) for a distributedstorageenvironment,wherestorageisjointly providedbymultiplenodes...|$|R
40|$|A computable general {{equilibrium}} {{model is}} used to estimate the impact a resource supply constraint, that restricts federal timber harvest, has on a timber dependent region. Impacts are compared to impacts generated from an input-output mode and indicate an upward bias in estimated income and employment losses using <b>IO</b> methods. <b>Resource</b> /Energy Economics and Policy,...|$|R
30|$|With popular {{commodity}} {{hard disk}} and SSD nowadays, sequential disk writing {{has the best}} performance [7, 22] so the strategy for the new key-value store is to support sequential data writing, and minimize number of disk seeks in every operation. To use all capacity of limited <b>IO</b> <b>resources,</b> achieve high-performance and low-latency, key-value storage must minimize number of disk seeking in every operation and all writing operations should be sequential or append only on disk. This research presents algorithms that implement efficient storage of key-value data on drive. They will minimize the required number of disk seeking. This research is done to optimize disk reading/writing operation in data services of applications.|$|E
40|$|The year 2000 {{was a year}} of {{evolutionary}} change for the Fermilab computer farms. Additional compute capacity was acquired {{by the addition of}} PCs for the CDF, D 0 and CMS farms. This was done in preparation for Run 2 production and for CMS Monte Carlo production. Additional I/O capacity was added for all the farms. This continues the trend to standardize the I/O systems on the SGI O 2 x 00 architecture. Strong authentication was installed on the CDF and D 0 farms. The farms continue to provide large CPU resources for experiments and those users whose calculations benefit from large CPU/low <b>IO</b> <b>resources.</b> The user community will change in 2001 now that the 1999 fixed-target experiments have almost finished processing and Run 2, SDSS, miniBooNE, MINOS, BTeV, and other future experiments and projects will be the major users in the future...|$|E
40|$|Abstract This paper {{describes}} {{the design for}} testability (DFT) challenges and techniques of Godson- 3 microprocessor, which is a scalable multicore processor based on the scalable mesh of crossbar (SMOC) on-chip network and targets high-end applications. Advanced techniques are adopted to make the DFT design scalable and achieve low-power and low-cost test with limited <b>IO</b> <b>resources.</b> To achieve a scalable and flexible test access, a highly elaborate test access mechanism (TAM) is implemented to support multiple test instructions and test modes. Taking advantage of multiple identical cores embedding in the processor, scan partition and on-chip comparisons are employed to reduce test power and test time. Test compression technique is also utilized to decrease test time. To further reduce test power, clock controlling logics are designed with ability to turn off clocks of non-testing partitions. In addition, scan collars of CACHEs are designed to perform functional test with low-speed ATE for speed-binning purposes, which poses low complexity and has good correlation results...|$|E
40|$|One of {{the major}} costs in {{system-on-chip}} (SOC) development is test cost, especially the cost related to test integration. Although there have been plenty of research works on individual topics about SOC testing, few of them took into account the practical integration issues. In this paper, we stress the practical SOC test integration issues, including real problems found in test scheduling, test IO reduction, timing of functional test, scan IO sharing, etc. A test scheduling method is proposed based on our test architecture and test access mechanism (TAM), considering <b>IO</b> <b>resource</b> constraints. Detailed scheduling further reduces the overall test time of the system chip. We also present a test wrapper architecture that supports the coexistence of scan test and functional test. The test integration platform {{has been applied to}} an industrial SOC case. The chip has been designed and fabricated. The measurement results justify the approach— simple and efficient, i. e., short test integration cost, short test time, and small area overhead. ...|$|R
40|$|Guaranteed <b>IO</b> <b>resource</b> {{allocation}} is {{a challenging}} prob-lem in storage servers {{due to the}} variability and unpre-dictability in their dynamic capacity. Existing scheduling mechanisms for CPU and memory allocation are insuffi-cient in providing the guarantees that are needed by appli-cations in today’s shared system infrastructures. In this paper, we examine the problem of providing pro-portional bandwidth allocation, with a minimum reser-vation per application for shared IO access. We look at two practical use cases: (1) I/O resource management in virtualized servers, and (2) Distributed storage infras-tructure. We present a scheduling algorithm, Reservation Based Fair Queuing RBFQ,that provides proportionate al-location while meeting per-application reservation con-straints. RBFQ uses two novel ideas (i) real-time dual tagging and (ii) prioritized scheduling to accomplish its goals. We also propose idle credits as a mechanism to handle bursts in IO workloads and improve IO efficiency. Experiments with RBFQ, using both a prototype imple-mentation in VMware’s ESX Server and a distributed storage environment, show that it provides good isolation, and meets reservations {{in the presence of}} fluctuating ca-pacity, dynamic workloads, and hot-spotting by various clients. ...|$|R
40|$|In virtualized data centers, {{multiple}} VMs are consolidated {{to access}} a shared storage system. Effective storage resource management, however, {{turns out to}} be challenging, as VM workloads exhibit various IO patterns and diverse loads. To multiplex the underlying hardware resources among VMs, providing fairness and isolation while maintaining high re-source utilization becomes imperative for effective storage resource management. Existing schedulers such as Linux CFQ or SFQ can provide some fairness, but it has been ob-served that synchronous IO tends to lose fair shares signifi-cantly when competing with aggressive VMs. In this paper, we introduce vFair, a novel scheduling framework that achieves <b>IO</b> <b>resource</b> sharing fairness among VMs, regardless of their IO patterns and workloads. The de-sign of vFair takes per-IO cost into consideration and strikes a balance between fairness and storage resource utilization. We have developed a Xen-based prototype of vFair and eval-uated it {{with a wide range of}} storage workloads. Our re-sults from both micro-benchmarks and real-world applica-tions demonstrate the effectiveness of vFair, with signifi-cantly improved fairness and high resource utilization...|$|R
40|$|The {{high degree}} of storage {{consolidation}} in modern virtualized datacenters requires flexible and efficient ways to allocate <b>IO</b> <b>resources</b> among virtual machines (VMs). Existing IO resource management techniques have two main deficiencies: (1) they are restricted {{in their ability to}} allocate resources across multiple hosts sharing a storage device, and (2) they do not permit the administrator to set allocations for a group of VMs that are providing a single service or belong to the same application. In this paper we present the design and implementation of a novel software system called Storage Resource Pools (SRP). SRP supports the logical grouping of related VMs into hierarchical pools. SRP allows reservations, limits and proportional shares, at both the VM and pool levels. Spare resources are allocated to VMs in the same pool in preference to other VMs. The VMs may be distributed across multiple physical hosts without consideration of their logical groupings. We have implemented a prototype of storage resource pools in the VMware ESX hypervisor. Our results demonstrate that SRP provides hierarchical performance isolation and sharing among groups of VMs running across multiple hosts, while maintaining high utilization of the storage device. ...|$|E
40|$|This paper {{describes}} a hardware architectural {{design of a}} real-time counter based entropy coder at a register transfer level (RTL) computing model. The architecture {{is based on a}} lossless compression algorithm called Rice coding, which is optimal for an entropy range of bits per sample. The architecture incorporates a word-splitting scheme to extend the entropy coverage into a range of bits per sample. We have designed a data structure in a form of independent code blocks, allowing more robust compressed bitstream. The design focuses on an RTL computing model and architecture, utilizing 8 -bit buffers, adders, registers, loader-shifters, select-logics, down-counters, up-counters, and multiplexers. We have validated the architecture (both the encoder and the decoder) in a coprocessor for 8 bits/sample data on an FPGA Xilinx XC 4005, utilizing 61 % of F&G-CLBs, 34 % H-CLBs, 32 % FF-CLBs, and 68 % <b>IO</b> <b>resources.</b> On this FPGA implementation, the encoder and decoder can achieve 1. 74 Mbits/s and 2. 91 Mbits/s throughputs, respectively. The architecture allows pipelining, resulting in potentially maximum encoding throughput of 200 Mbit/s on typical real-time TTL implementations. In addition, it uses a minimum number of register elements. As a result, this architecture can result in low cost, low energy consumption and reduced silicon area realizations...|$|E
40|$|Abstract. This paper {{describes}} a hardware architectural {{design of a}} real-time counter based entropy coder at a register transfer level (RTL) computing model. The architecture {{is based on a}} lossless compression algorithm called Rice coding, which is optimal for an entropy range of 5. 25. 1 H bits per sample. The architecture incorporates a word-splitting scheme to extend the entropy coverage into a range of 5. 75. 1 H bits per sample. We have designed a data structure in a form of independent code blocks, allowing more robust compressed bitstream. The design focuses on an RTL computing model and architecture, utilizing 8 -bit buffers, adders, registers, loader-shifters, select-logics, down-counters, up-counters, and multiplexers. We have validated the architecture (both the encoder and the decoder) in a coprocessor for 8 bits/sample data on an FPGA Xilinx XC 4005, utilizing 61 % of F&G-CLBs, 34 % H-CLBs, 32 % FF-CLBs, and 68 % <b>IO</b> <b>resources.</b> On this FPGA implementation, the encoder and decoder can achieve 1. 74 Mbits/s and 2. 91 Mbits/s throughputs, respectively. The architecture allows pipelining, resulting in potentially maximum encoding throughput of 200 Mbit/s on typical real-time TTL implementations. In addition, it uses a minimum number of register elements. As a result, this architecture can result in low cost, low energy consumption and reduced silicon area realizations...|$|E
40|$|The growing {{popularity}} of multi-tenant, cloud-based computing platforms is increasing interest in resource allocation models that permit flexible sharing of the underlying infrastructure. This thesis introduces a novel <b>IO</b> <b>resource</b> allocation model that better captures the requirements of paying tenants sharing a physical infrastructure. The model addresses a major concern regarding application performance stability when clients migrate from a dedicated to a shared platform. Specifically, while clients would like their applications to behave similarly in both situations, traditional models of fairness, like proportional share allocation, do not exhibit this behavior {{in the context of}} modern multi-tiered storage architectures. We also present a scheduling algorithm, the Reward Scheduler, that implements the new allocation policy, by rewarding clients with better runtime characteristics, resulting in benefits to both the clients and the service provider. Moreover, the Reward scheduler also supports weight-based capacity allocation subject to a minimum reservation and maximum limitation on the IO allocation for each task. Experimental results indicate that the proposed algorithm proportionally allocates the system capacity in proportion to their entitlements...|$|R
50|$|KiVi {{also support}} {{columnar}} storage bringing the corresponding acceleration for analytical queries that only bring to memory the columns accessed by the query what saves an important amount of <b>resources</b> (<b>IO,</b> CPU, memory) when processing queries over tables with {{large number of}} columns.|$|R
40|$|Petascale science {{simulations}} generate 10 s of TBs {{of application}} data per day, {{much of it}} devoted to their checkpoint/restart fault tolerance mechanisms. Previous work demonstrated the importance of carefully managing such output to prevent application slowdown due to <b>IO</b> blocking, <b>resource</b> contention negatively impacting simulation performance and to fully exploit the IO bandwidth available to the petascale machine. This paper takes a further step in understanding and managing extreme-scale IO. Specifically, its evaluations seek {{to understand how to}} efficiently read data for subsequent data analysis, visualization, checkpoint restart after a failure, and other read-intensive operations. In their entirety, these actions support the ‘end-toend’ needs of scientists enabling the scientific processes bein...|$|R
40|$|Project (M. S., Computer Science) [...] California State University, Sacramento, 2011. Parallel {{execution}} is {{the ability}} to apply multiple CPU???s and <b>IO</b> <b>resources</b> to the execution of a single database operation. A major advantage of Oracle Database 10 g???s parallel execution architecture is its ability to leverage the underlying hardware infrastructure to its fullest extent using every single CPU, every byte of memory, and the fully available I/O bandwidth. Oracle Database 10 g is the only prominent vendor that implements dynamic parallelism at execution time by shared everything approach (every server can directly access every disk). Oracle Database 10 g???s Real Application Clusters (RAC) provide near linear scalability and availability for high volume, mission-critical applications, thus providing unlimited power and growth for your systems. Oracle Real Application Clusters uses modern cluster interconnect facilities to reduce disk I/O and can exploit the emerging high bandwidth low latency interconnects to provide cost-effective scalability. It fully exploits the expanded CPU, memory and disk resources of the clustered system to drive more transactions. Cache Fusion??? is a fundamental component of Oracle???s Real Application Cluster configuration, a shared-cache clustered-database architecture that transparently extends database applications from single systems to multi-node shared-disk clusters. Shared disk might be important bottleneck of whole system if it is not intelligent enough to satisfy different application i/o needs. Application aware storage architecture helps to solve this. In addition, this project presents a simulation of concurrent disk access to a RAID disk. This simulation thus provide insight into sophisticated mass storage resource demands presented by modern applications. Computer Scienc...|$|E
40|$|Virtual Machine (VM) {{live storage}} {{migration}} is widely {{performed in the}} data cen- ters of the Cloud, {{for the purposes of}} load balance, reliability, availability, hardware maintenance and system upgrade. It entails moving all the state information of the VM being migrated, including memory state, network state and storage state, from one physical server to another within the same data center or across different data centers. To minimize its performance impact, this migration process is required to be transparent to applications running within the migrating VM, meaning that ap- plications will keep running inside the VM as if there were no migration operations at all. In this dissertation, a thorough literature review is conducted to provide a big picture of the VM live storage migration process, its problems and existing solutions. After an in-depth examination, we observe that a severe IO interference between the VM IO threads and migration IO threads exists and causes both types of the IO threads to suffer from performance degradation. This interference {{stems from the fact that}} both types of IO threads share the same critical IO path by reading from and writing to the same shared storage system. Owing to <b>IO</b> <b>resource</b> contention and requests interference between the two different types of IO requests, not only will the IO request queue lengthens in the storage system, but the time-consuming disk seek operations will also become more frequent. Based on this fundamental observation, this dissertation research presents three related but orthogonal solutions that tackle the IO interference problem in order to improve the VM live storage migration performance. First, we introduce the Workload-Aware IO Outsourcing scheme, called WAIO, to improve the VM live storage migration efficiency. Second, we address this problem by proposing a novel scheme, called SnapMig, to improve the VM live storage migration efficiency and eliminate its performance impact on user applications at the source server by effectively leveraging the existing VM snapshots in the backup servers. Third, we propose the IOFollow scheme to improve both the VM performance and migration performance simultaneously. Finally, we outline the direction for the future research work. Advisor: Hong Jian...|$|R
40|$|Strategic {{marketing}} of educational institutions H su n (Tony) Huang, Victoria University, tony 7 huang@yahoo. com Wayne Binney, Victoria University, Wayne. Binney@vu. edu. au Anne - Marie Hede, Victoria University, Anne - M arie. Hede@vu. edu. au Abstract Strategy development {{in higher education}} (HE) institutions has not been investigated a great extent. To address this issue, this study reports on {{the first stage of}} a larger investigation of strategy development in HE. The the oretical background draws on two theories of strategy and competitive advantage, namely, industrial organisation (<b>IO)</b> and <b>resource</b> - based view (RBV). These are used to guide 32 in - depth interviews that explor e the elements of external industry structure, in ternal resources a nd capabilities, and institutional performance with senior HE decision - makers. F actors of competitive advantage and the indicators of institutional performance identified in the study verify and further develop the limited understanding r elating to strategic {{marketing of}} educational institutions. [...] ANZMAC 2010 conference name: 'Doing more with less' - Conference held: 29 November - 1 December 2010, College of Business and Economics, University of Canterbury, Christchurch, N. Z...|$|R
40|$|Xilinx and Mathworks jointly {{proposed}} System Generator (SysGen) and Simulink {{to accelerate}} development of DSP (digital signal processing) style applications on Field Programmable Gate Array (FPGA) chips. However, most of developments with Simulink and SysGen end at simulation stage without complete stand-alone implementation on FPGA since these tools {{do not come}} up with sufficient IO system libraries. In this paper, we present the process of implementing a full system to reconstruct CT (computed tomography) images on FPGA using Simulink and SysGen. In this process, we solve technical issues regarding algorithm mapping, design of additional modules, system <b>IO</b> library and <b>resource</b> allocation. Our experience will be taken as useful reference for researchers working on FPGA applications and high resolution medical image applications. © 2010 IEEE...|$|R
40|$|Although natural {{resources}} {{form the basis}} of our economy, they are not always used in a sustainable way. To achieve a more sustainable economic growth, resource consumption needs to be measured. Therefore, resource footprint frameworks (RFF) are being developed. To easily provide results, these RFF integrate inventory methodologies, at macrolevel mostly input-output (<b>IO)</b> models, with <b>resource</b> accounting methodologies, of which the Ecological Footprint is probably the best known one. The objective of this work is {{the development of a new}} RFF, in which a world IO-model (Exiobase), providing a global perspective, is integrated with the CEENE methodology (Cumulative Exergy Extraction from the Natural Environment), offering a more complete resource range: fossil fuels, metals, minerals, nuclear resources, water resources, land resources, abiotic renewable resources, and atmospheric resources. This RFF, called IO-CEENE, allows one to calculate resource footprints for products or services consumed in different countries as the exergy extracted from nature. The way the framework is constructed makes it possible to show which resources and countries contribute to the total footprint. This was illustrated by a case study, presenting the benefits of the framework’s worldwide perspective. Additionally, a softwarefile is provided to easily calculate results...|$|R

