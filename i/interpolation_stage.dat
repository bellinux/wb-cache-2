25|19|Public
50|$|Digital Circuit Multiplication Equipment (DCME) was a type {{of voice}} {{compression}} equipment that is installed {{at either end of}} a long-distance link (typically communications satellite or submarine communications cable). The main characteristics of DCME are defined in ITU-T recommendation G.763. DCME consists of a voice <b>interpolation</b> <b>stage,</b> which is a form of statistical multiplexor applied to voiceband signals, and a low rate encoding stage which exploits correlation between successive voiceband samples on an individual input channel to reduce the transmitted bitrate from that required by PCM of equivalent quality.|$|E
30|$|After filtering, the {{coefficients}} are interpolated to adapt their rate to {{the signal that}} comes from the IEEE 802.11 p transceivers at 10 MHz. To see an extensive description of the components and the functioning of the <b>interpolation</b> <b>stage</b> consult Section 3.5. 4 in [4].|$|E
40|$|AbstractCorners among linear {{segments}} {{are normally}} smoothed {{in order to}} improve the CNC machining efficiency. The interpolation of the corner smoothing trajectories is important to guarantee high geometric accuracy and good dynamics performance. This paper proposes a two-stage interpolation algorithm for corner smoothing trajectories, which includes the off-line pre-processing stage and the on-line <b>interpolation</b> <b>stage.</b> The off-line pre-processing stage calculates the path lengths, and approximates the relation of spline parameters and spline lengths with 7 th order polynomial splines within preset tolerance. The on-line <b>interpolation</b> <b>stage</b> generates interpolated drive commands with the trapezoidal acceleration profile, where chord errors and centripetal accelerations at transition corners are also constrained. Simulation and experiment results show that the proposed interpolation algorithm can improve the machining efficiency and satisfy the preset geometric error and dynamics constraints as well...|$|E
40|$|This paper {{describes}} {{a new approach}} for ScanSAR data image generation, which performs efficient data processing and multilook operation without the need of <b>interpolation</b> <b>stages.</b> This new technique {{is based on the}} spectral analysis method (SPECAN) combined with the well-known chirp Z-transform. In this paper, it is also shown that this new method is very efficient for obtaining near real-time images for browsing or on-board burst mode image generation overcoming classical problems of the standard SPECAN method. Peer ReviewedPostprint (published version...|$|R
40|$|The {{majority}} of multispectral (MS) pansharpening methods may {{be labeled as}} spectral or spatial, {{depending on whether the}} geometric details that shall be injected into the interpolated MS bands are extracted from the panchromatic (P) image by means of a spectral transformation of MS pixels or a spatial transformation of the P image, achieved by means of linear shift-invariant digital filters. Spectral methods are known as component substitution; spatial methods are based on multiresolution analysis (MRA). In this paper, the authors show that, under the most general conditions, MRA-based pansharpening is characterized by a unique separable low-pass filter, which can be parametrically optimized based on the modulation transfer function (MTF) of the MS instrument, possibly followed by decimation and <b>interpolation</b> <b>stages.</b> This happens for the discrete wavelet transform (DWT) and its undecimated version (UDWT), for the 'à-trous' wavelet (ATW) transform and its decimated version, i. e., the generalized Laplacian pyramid (GLP), and for nonseparable wavelet transforms, such as the nonsubsampled contourlet transform (NSCT). Hybrid methods, in which MRA fusion is performed on the intensity component derived from a spectral transformation, are equivalent to MRA fusion with a specific detail injection model. ATW and GLP are preferable to DWT, UDWT, and NSCT, because of computational benefits and of a looser choice of the low-pass filter, unconstrained from the requirement of generating a perfect reconstruction filter bank. Ultimately, GLP outperforms ATW, because its decimation and <b>interpolation</b> <b>stages</b> allow the aliasing impairments intrinsically present in the original MS bands to be removed from the pansharpened product...|$|R
40|$|AbstractThe Fourier {{interpolation}} of 3 D data-sets is {{a performance}} critical operation in many fields, including {{certain forms of}} image processing and density functional theory (DFT) quantum chemistry codes based on plane wave basis sets, to which this paper is targeted. In this paper we describe three different algorithms for performing this operation built from standard discrete Fourier transform operations, and derive theoretical operation counts. The algorithms compared consist of the most straightforward implementation and two that exploit techniques such as phase-shifts and knowledge of zero padding to reduce computational cost. Through a library implementation (tintl) we explore the performance characteristics of these algorithms and the performance impact of different implementation choices on actual hardware. We present comparisons within the linear-scaling DFT code ONETEP where we replace the existing interpolation implementation with our library implementation configured to choose the most efficient algorithm. Within the ONETEP Fourier <b>interpolation</b> <b>stages,</b> we demonstrate speed-ups of over 1. 55 ×...|$|R
30|$|In recent years, {{there have}} been {{investigations}} into more sophisticated demosaicing algorithms. Based on the assumption of smooth hue transition, demosaicing is performed using a ratio model which assumes that the ratio between luminance and chrominance at the same position is constant in the neighborhood [3]. Instead of using color ratios, many methods also make use of interchannel color differences which assume {{that the difference between}} luminance and chrominance is smooth in small region [4 – 7]. Since human visual systems are sensitive to the edges in images, edge-directed demosaicing method chooses the interpolation direction to avoid interpolating across edges, instead interpolating along any edges in the image [8 – 11]. Instead of choosing a certain interpolation direction, the edge indicator function is used in [12 – 17]. The edge indicator functions in several directions are defined as measures of edge information and a missing pixel is determined as a weighted sum of its neighbors. Color demosaicing is performed by reconstruction approaches [18 – 21]. Demosaiced image is obtained by deriving Minimum Mean Square Error (MMSE) estimator [18]. Regularization approaches are proposed in [19] and the color channels are reconstructed using the projections onto convex sets (POCSs) technique [20]. In [21], the demosaicing problem is formulated as a Bayesian estimation problem. Another recent demosaicing approach, which is referred as decision-based demosaicing algorithm, divided the demosaicing procedure into an <b>interpolation</b> <b>stage</b> and decision stage [22 – 27]. In the <b>interpolation</b> <b>stage,</b> horizontally and vertically interpolated images are produced, respectively. In the decision stage, soft-decision or hard-decision methods are employed for choosing the pixels interpolated in the direction with fewer artifacts. The Fisher discriminant is used as the decision criterion [22]. Homogeneity map is used to improve the reasonability of the criterion [23]. The second order Taylor series are used to produce directionally interpolated images in the <b>interpolation</b> <b>stage</b> [24]. Demosaicing error is minimized by the directional linear minimum mean square-error estimation technique [25]. In [26], Chung and Chan presented an adaptive demosaicing algorithm by using the variances of color differences along horizontal and vertical edge directions. However, in the method proposed by Tsai and Song [27], the decision stage is performed before the <b>interpolation</b> <b>stage.</b>|$|E
3000|$|Once the {{transmit}} signals {{have been}} generated, {{the base station}} is notified and the over-the-air transmission process starts. First, the signals are read from the corresponding source file and transferred to the USRP, where they are again interpolated in the FPGA before reaching the (DAC). Note that this <b>interpolation</b> <b>stage</b> is needed for adapting the signal sampling rate to the sampling frequency of the DAC, thus not affecting the signal bandwidth. Next, the signals are up-converted to the central frequency f [...]...|$|E
40|$|We {{consider}} {{the problem of}} estimating the intensity map of a spatially random phenomenon over a geographical area observed by a sensor network. The spatial phenomenon of interest is modeled using a Gaussian random field specified by its nonlinear mean and covariance functions. Our proposed algorithm includes two stages: a novel greedy sparse recovery algorithm to estimate {{the parameters of the}} mean function, and a spatial <b>interpolation</b> <b>stage</b> using an algorithm called simple kriging. Further, we study the application of the proposed algorithm to radio spectrum cartography, and show that it offers a significant advantage in terms of accuracy of map reconstruction compared to existing methods...|$|E
40|$|Abstract—In this paper, {{we address}} the {{problems}} of unreliable motion vectors that cause visual artifacts but cannot be detected by high residual energy or bidirectional prediction difference in motion-compensated frame interpolation. A correlation-based motion vector processing method is proposed to detect and correct those unreliable motion vectors by explicitly considering motion vector correlation in the motion vector reliability classification, motion vector correction, and frame <b>interpolation</b> <b>stages.</b> Since our method gradually corrects unreliable motion vectors based on their reliability, we can effectively discover the areas where no motion is reliable to be used, such as occlusions and deformed structures. We also propose an adaptive frame interpolation scheme for the occlusion areas based on the analysis of their surrounding motion distribution. As a result, the interpolated frames using the proposed scheme have clearer structure edges and ghost artifacts are also greatly reduced. Experimental results show that our interpolated results have better visual quality than other methods. In addition, the proposed scheme is robust even for those video sequences that contain multiple and fast motions. Index Terms—Frame rate up conversion, motion-compensated frame interpolation (MCFI), motion vector correlation, motion vector processing, video occlusions. I...|$|R
40|$|The {{advantages}} {{provided by}} the generalized Laplacian pyramid (GLP) over the widespread "a trous" wavelet (ATW) transform for multispectral (MS) pansharpening based on multiresolution analysis (MRA) are investigated. The most notable difference depends on the way GLP and ATW deal with aliasing possibly occurring in the MS data, which is originated by insufficient sampling step size, or equivalently by a too high amplitude value of the modulation transfer function (MTF) at Nyquist frequency and may generate annoying jagged patterns that survive in the sharpened image. In this paper, it is proven that GLP is capable of compensating the aliasing of MS, unlike ATW, and analogously to component substitution (CS) fusion methods, thanks to the decimation and <b>interpolation</b> <b>stages</b> present in its flowchart. Experimental results will be presented in terms of quality/distortion global score indexes (SAM, ERGAS and Q 4) for increasing amounts of aliasing, measured by the amplitude at Nyquist frequency of the Gaussian-like lowpass filter simulating the average MTF of the individual spectral channels of the instrument. GLP and ATW-based methods, both using the same MTF filters and the same global injection gain, will be compared to show the advantages of GLP over ATW {{in the presence of}} aliasing of the MS bands...|$|R
40|$|A {{fundamental}} theorem of Digital Signal Processing is Shannon's sampling theorem, whichdictates {{the minimum}} rate (called the Nyquist rate") {{at which a}} continuous-time signalmust be sampled in order to faithfully reproduce the signal from its samples. If a signalcan be reproduced from its samples, then clearly no information about the original signalhas {{been lost in the}} sampling process. However, when a signal is sampled at a rate lowerthan the Nyquist Rate, the true spectral content of the original signal is distorted due toaliasing," wherein frequencies in the original signal greater than the sampling frequencyappear as lower frequencies in the sampled signal. This distortion is generally held to beirrecoverable, i. e., whenever aliasing occurs, information is considered to be inevitably lost. This research challenges this notion and presents a technique for identifying aliasingand recovering an unaliased version of a signal from its aliased samples. The method isapplicable to frequency-modulated (FM) signals with a continuous instantaneous frequency(IF), and utilizes analysis of the IF of the aliased signal to 1) determine whether the signalhas potentially been aliased and, if so, 2) compensate for the aliasing by reconstructingan estimate of the true IF of the signal. Time-frequency methods are used to analyzethe potentially aliased signal and estimate the IF, together with modulation, re-samplingand <b>interpolation</b> <b>stages</b> to reconstruct an estimate of the unaliased signal. The proposedtechnique can yield excellent reconstruction of FM signals given ideal estimates of the IF...|$|R
40|$|International audienceThis paper {{presents}} a JPEG-like coder for image compression of single-sensor camera images using a Bayer Color Filter Array (CFA). The originality {{of the method}} is a joint scheme of compression. demosaicking in the DCT domain. In this method, the captured CFA raw data is first separated in four distinct components and then converted to YCbCr. A JPEG compression scheme is then applied. At the decoding level, the bitstream is decompressed until reaching the DCT coefficients. These latter are used for the <b>interpolation</b> <b>stage.</b> The obtained results are better than those obtained by the conventional JPEG in terms of CPSNR, DeltaE 2000 and SSIM. The obtained JPEG-like scheme is also less complex...|$|E
30|$|We adopt {{a method}} to mathematically {{determine}} the optimal parameters used for the RBF <b>interpolation</b> <b>stage.</b> Previously in [6], we made use of an RBF network consisting of Gaussian radial functions to model the non-Lambertian contribution. The parameters in this model, including the Gaussian dispersion σ and Tikhonov regularization coefficient τ, were taken heuristically and remained constant across all pixels. In this work, we start off from a theorem that minimizes error in a leave-one-out analysis by optimizing the Gaussian dispersion parameter. Such a theorem is not new, but here we extend its use to whole images and three-channel colour. More importantly, however, we also extend the theorem to optimize over the Tikhonov regularization. For the fairly large size matrices being inverted, these optimizations matter and make a substantial difference to results obtained.|$|E
40|$|Before remediating a {{site with}} {{contaminated}} soil or groundwater, the contaminant plume {{must first be}} characterized. This involves sampling the contaminant concentration at a set of locations {{in and around the}} contaminated area. To present the measured concentrations in a meaningful form, the concentrations are typically interpolated to the nodes of a three-dimensional grid and the plume is visualized by constructing isosurfaces from the gridded data. The critical step in this process is the <b>interpolation</b> <b>stage.</b> Improper application of an interpolation scheme can result in grossly misleading threedimensional plume maps. There are a number of problems that often occur when interpolating contaminant plume data including the generation of negative concentrations, oscillation of interpolated values, improper estimation of maximum concentrations, and skewing of the results due to data clustering. These and other difficulties associated with plume characterization are discussed along with a [...] ...|$|E
40|$|AbstractAmong the {{symplectic}} integrators for {{the numerical}} solution of general Hamiltonian systems, implicit Runge-Kutta methods of Gauss type (RKG) {{play an important}} role. To improve {{the efficiency of the}} algorithms {{to be used in the}} solution of the nonlinear equations of stages, accurate starting values for the iterative process are required. In this paper, a class of starting algorithms, which are based on numerical information computed in two previous steps, is studied. For two- and three-stages RKG methods, explicit starting algorithms for the stage equations with orders three and four are derived. Finally, some numerical experiments comparing the behaviour of the new starting algorithms with the standard first iterant based on Lagrange <b>interpolation</b> of <b>stages</b> in the previous step are presented...|$|R
40|$|The {{interpolation}} {{technology is}} critical to performance of CNC and industrial robots; this paper proposes a new precision interpolation algorithm based on analysis of root cause in speed and acceleration. To satisfy continuity of speed and acceleration in interpolation process, this paper describes, respectively, variable acceleration precision <b>interpolation</b> of two <b>stages</b> and three sections. Testing shows that CNC system can be enhanced significantly by using the new fine interpolation algorithm in this paper...|$|R
5000|$|Macbeth {{was first}} {{printed in the}} First Folio of 1623 and the Folio is the only source for the text. Some {{scholars}} contend that the Folio text was abridged and rearranged from an earlier manuscript or prompt book. [...] Often cited as <b>interpolation</b> are <b>stage</b> cues for two songs, whose lyrics {{are not included in}} the Folio but are included in Thomas Middleton's play The Witch, which was written between the accepted date for Macbeth (1606) and the printing of the Folio. Many scholars believe these songs were editorially inserted into the Folio, though whether they were Middleton's songs or preexisting songs is not certain. It is also widely believed that the character of Hecate, as well as some lines of the First Witch (4.1 124-31), were not part of Shakespeare's original play but were added by the Folio editors and possibly written by Middleton, though [...] "there is no completely objective proof" [...] of such interpolation.|$|R
40|$|In this article, {{we present}} a new NTSC system based on multidimensional crosstalk-free {{transmultiplexer}} theory. The system is truly crosstalk-free and is compatible with existing television sets. The new encoded NTSC composite signal can be demodulated with slight degradation by a conventional television receiver, with some improvements by a comb-filter-equipped television (bigger screen) and with best performance with the new decoder. We use new sampling structures for luminance and chrominance signals {{in order to obtain}} nearperfect -reconstruction. The detailed structure of the proposed crosstalk-free NTSC system is presented (encoder and decoder). The NTSC encoder is composed of a decimation stage followed by a near-perfect-reconstruction transmultiplexer encoder. The NTSC decoder is composed of the transmultiplexer's decoder followed by an <b>interpolation</b> <b>stage.</b> We show structures of multidimensional two-channel FIR filter banks which allow nearperfect -reconstruction. Such structure [...] ...|$|E
40|$|Some image {{processing}} applications (e. g. computer graphics and robot vision) require the rotation, scaling and translation of digitized images in realtime (25 - 30 images per second). Today's standard image processors {{can not meet}} this timing constraint so other solutions have to be considered. This paper describes a multi-ASIC solution which is capable of doing the {{image processing}} tasks in real-time. The first ASIC is a so-called affine transformer which calculates a one-dimensional coordinate every 25 ns. The second ASIC is a bilinear interpolator which calculates an interpolated value from four known surrounding values, again every 25 ns. This ASIC is designed in a modular setup which results in a flexible accuracy of the interpolation. If more accurate interpolation is required, another ASIC (containing an <b>interpolation</b> <b>stage)</b> is used. In this way for each application a proper accuracy is implemented, reaching optimal silicon area utilization and desired accuracy of interpolation. U [...] ...|$|E
40|$|Abstract—This paper {{addresses}} {{the analysis and}} mitigation of the signal distortion caused by oscillator phase noise (PN) in OFDM communications systems. Two new PN mitigation techniques are proposed, especially targeted for reducing the intercarrier interference (ICI) effects due to PN. The first proposed method is a fairly simple one, stemming {{from the idea of}} linearly interpolating between two consecutive common phase error (CPE) estimates to obtain a linearized estimate of the time-varying phase characteristics. The second technique, in turn, is an extension to the existing state-of-the-art ICI estimation methods. Here the idea is to use an additional <b>interpolation</b> <b>stage</b> to improve the phase estimation performance around the boundaries of two consecutive OFDM symbols. The paper also verifies the performance improvement of these new PN estimation techniques by comparing them to the existing state-of-the-art techniques using extensive computer simulations. To emphasize practicality, the simulations are carried out in 3 GPP-LTE downlink –like system context, covering both additive white Gaussian noise (AWGN) and extended ITU-R Vehicular A multipath channel types. Index Terms—phase noise; OFDM; common phase error; intercarrier interference; LT...|$|E
40|$|Due to the {{internal}} nature of mammalian development, {{much of the research}} performed is of a static nature and depends on <b>interpolation</b> between <b>stages</b> of development. This approach cannot explore the dynamic interactions that are essential for normal development. While roller culture overcomes the problem of inaccessibility of the embryo, the constant motion of the medium and embryos makes it impossible to observe and record development. We have developed a static mammalian culture system for imaging development of the mouse embryo. Using this technique, it is possible to sustain normal development for periods of 18 - 24 h. The success of the culture was evaluated based on the rate of embryo turning, heart rate, somite addition, and several gross morphological features. When this technique is combined with fluorescent markers, it is possible to follow the development of specific tissues or the movement of cells. To highlight some of the strengths of this approach, we present time-lapse movies of embryonic turning, somite addition, closure of the neural tube, and fluorescent imaging of blood circulation in the yolk sac and embryo. status: publishe...|$|R
40|$|This paper {{deals with}} {{starting}} algorithms for Newton-type schemes for solving the stage equations of implicit s–stages Runge-Kutta methods applied to stiff problems. We present {{a family of}} starting algorithms with orders from 0 to s + 1 and, with estimations of the error in these algorithms, we give a technique for selecting, at each step, the most convenient in the family. The proposed algorithms, that can be {{expressed in terms of}} divided differences, are based on the Lagrange <b>interpolation</b> of the <b>stages</b> of the last two integration steps. We also analyse the orders of the starting algorithms for the non-stiff case, for the Prothero and Robinson model and the stiff order. Finally, by means of some numerical experiments we show that this technique allows, in general, to improve greatly the performance and reliability of implicit Runge-Kutta methods on stiff problems...|$|R
40|$|The {{statistical}} {{techniques used}} by Raup and Sepkoski (1984 and 1986) {{to identify a}} 26 -Myr periodicity in the biological extinction record for the past 250 Myr are reexamined, responding in detail to the criticisms of Stigler and Wagner (1987). It is argued that evaluation {{of a much larger}} set of extinction data using a time scale with 51 sampling intervals supports the finding of periodicity. In a reply by Sigler and Wagner, the preference for a 26 -Myr period is attributed to a numerical quirk in the Harland et al. (1982) time scale, in which the subinterval boundaries are not linear <b>interpolations</b> between the <b>stage</b> boundaries but have 25 -Myr periodicity. It is stressed that the results of the stringent statistical tests imposed do not disprove periodicity but rather indicate that the evidence and analyses presented so far are inadequate...|$|R
40|$|International audienceThis {{document}} {{presents an}} interpolation operator on unstructured tetrahedral meshes that satisfies {{the properties of}} mass conservation, P 1 P 1 -exactness (order 2) and maximum principle. Interpolation operators are important for many applications in scientific computing. For instance, {{in the context of}} anisotropic mesh adaptation for time-dependent problems, the <b>interpolation</b> <b>stage</b> becomes crucial as the error due to solution transfer accumulates throughout the simulation. This error can eventually spoil the overall solution accuracy. When dealing with conservation laws in CFD, solution accuracy requires enforcement of mass preservation throughout the computation, in particular in long time scale computations. In the proposed approach, the conservation property is achieved by local mesh intersection and quadrature formulae. Derivatives reconstruction is used to obtain a second order method. Algorithmically, our goal is to design a method which is robust and efficient. The robustness is mandatory to obtain a reliable method on real-life applications and to apply the operator to highly anisotropic meshes. The efficiency is achieved by designing a matrix-free operator which is highly parallel. A multi-thread parallelization is given in this work. Several numerical examples are presented to illustrate the efficiency of the proposed approach...|$|E
40|$|Near-field antenna {{characterization}} {{consists of}} measuring, on proper scanning surfaces, the near-field radiated by an Antenna Under Test (AUT) and of determining, from those measurements, the far-field pattern (Near-Field/Far-Field – NFFF – transformation) [1]. Whenever possible, both the amplitude and {{the phase of}} the near-field are acquired on a sole scanning surface, so that the NFFF transformation involves only linear operations on the data [1]. When, on the contrary, measuring the phase becomes expensive or difficult, the only amplitude of the near-field is usually collected on two measurement surfaces and the NFFF transformation involves non-linear operations on the data, typically consisting of the iterative optimization of a cost functional [2]. In recent years, significant advancements have been achieved {{in the framework of}} the near-field antenna characterization which have concerned new scanning geometries (plane-polar [3], spiral and helicoidal [4]), advanced non-uniform sampling techniques [5] capable to significantly reducing the acquisition time and the ill-conditioning as compared to uniform samplings, and new processing schemes [2, 6]. Whenever an estimate or a measurement of the “complex” (i. e., amplitude and phase) near-field is available, transforming a near-field into the far-field conceptually requires a simple Fourier transform operation. However, when non-uniform grids are involved, such Fourier transformation cannot be performed by a standard Fast Fourier Transform (FFT) algorithm, having a convenient asymptotic computational complexity growing as NlogN. Then, in the case of complex measurements, following the acquisition of non-uniform samples, the most simple way to transform the data to the far-field is to perform an <b>interpolation</b> <b>stage</b> prior to the FFT. This establishes a trade-off between accuracy and computational complexity [7]. Such a trade-off becomes even more relevant in the case of algorithms relying on phaseless data which, at each iteration step, require evaluating the spectrum from estimates of the near-field on non-uniform lattices or calculating the near-field on non-uniform grids from estimates of the spectrum. In the last years, Non-Uniform FFT (NUFFT) algorithms [8] have been developed enabling to evaluate Fourier transforms from non-uniform grids to uniform ones (Non-Equispaced Data – NED - NUFFT), from uniform grids to non-uniform ones (Non-Equispaced Results – NER - NUFFT) and from non-uniform grids to non-uniform ones (“type- 3 ” NUFFT). Such algorithms enable to accurately performing the <b>interpolation</b> <b>stage</b> in a computationally convenient way, so that their computational burden is proportional to that of a standard FFT. Furthermore, NUFFTs can be made available as library routines to strongly simplify their usage and, accordingly, their exploitation in NFFF transformations. Purpose {{of this paper is to}} show how NUFFT algorithms can be conveniently employed in near-field characterization algorithms, operating both with complex or phaseless data...|$|E
40|$|We {{propose a}} new {{approach}} for image compression in digital cameras, where {{the goal is to}} achieve better quality at a given rate by using the characteristics of a Bayer color filter array. Most digital cameras produce color images by using a single CCD plate, so that each pixel in an image has only one color component and therefore an interpolation method is needed to produce a full color image. After the image processing stage, {{in order to reduce the}} memory requirements of the camera, a lossless or lossy compression stage often follows. But in this scheme, before decreasing redundancy through compression, redundancy is increased in an <b>interpolation</b> <b>stage.</b> In order to avoid increasing the redundancy before compression, we propose algorithms for image compression in which the order of the compression and interpolation stages is reversed. We introduce image transform algorithms, since non interpolated images cannot be directly compressed with general image coders. The simulation results show that our algorithm outperforms conventional methods with various color interpolation methods in a wide range of compression ratios. Our proposed algorithm provides not only better quality but also lower encoding complexity because the amount of luminance data used is only half of that in conventional methods...|$|E
40|$|Higher-order {{integrated}} wavetable synthesis (HOIWS) is {{an efficient}} technique to reduce aliasing in wavetable and sampling synthesis. A periodic audio signal is integrated repeatedly {{before it is}} stored in a wavetable. During playback, the pitch of the audio signal can be changed using interpolation techniques and the resulting signal is differentiated {{as many times as}} the wavetable has been integrated. Previous discrete-time integrators approximate ideal integration, which leads to magnitude and phase errors. This paper proposes an ideal integration method, which is applied in the frequency domain {{with the help of the}} FFT. Its remarkable advantage is that both the magnitude and the phase errors are completely avoided in the special case of periodic signals. The proposed ideal integrator shows a superior performance over previous digital integration methods. It improves the sound quality of the HOIWS algorithm and helps it to maintain the original waveform after <b>interpolation</b> and differentiation <b>stages...</b>|$|R
40|$|AbstractRadial basis {{function}} <b>interpolation</b> involves two <b>stages.</b> The {{first is}} fitting, solving a linear system {{corresponding to the}} interpolation conditions. The second is evaluation. The systems occurring in fitting problems are often very ill-conditioned. Changing the basis in which the radial basis function space is expressed can greatly improve the conditioning of these systems resulting in improved accuracy, {{and in the case}} of iterative methods, improved speed, of solution. The change of basis can also improve the accuracy of evaluation by reducing loss of significance errors. In this paper new bases for the relevant space of approximants, and associated preconditioning schemes are developed which are based on Floater’s mean value coordinates. Positivity results and scale independence results are shown for schemes of a general type. Numerical results show that the given preconditioning scheme usually improves conditioning of polyharmonic spline and multiquadric interpolation problems in R 2 and R 3 by several orders of magnitude. The theory indicates that using the new basis elements (evaluated indirectly) for both fitting and evaluation will reduce loss of significance errors on evaluation. Numerical experiments confirm this showing that such an approach can improve overall accuracy by several significant figures...|$|R
40|$|Continuity and {{interpolation}} {{have been}} crucial topics for computer graphics since its very beginnings. Every time {{we want to}} interpolate values across some area, {{we need to take}} a set of samples over that interpolating region. However, interpolating samples faithfully allowing the results to closely match the underlying functions can be a tricky task as the functions to sample could not be smooth and, in the worst case, it could be even impossible when they are not continuous. In those situations bringing the required continuity is not an easy task, and much work has been done to solve this problem. In this paper, we focus on the state of the art in continuity and <b>interpolation</b> in three <b>stages</b> of the real-time rendering pipeline. We study these problems and their current solutions in texture space (2 D), object space (3 D) and screen space. With this review of the literature in these areas, we hope to bring new light and foster research in these fundamental, yet not completely solved problems in computer graphicsWe would like to thank the people that helped this work to become true, in particular the ViRVIG-GGG members. This work was partially funded by the TIN 2013 - 47137 -C 2 - 2 -P project from Ministerio de Economia y Competitividad, Spai...|$|R
40|$|Most of the {{existing}} image resolution enhancement algorithms assume that the image is clean and noise free, but this assumption is not practically valid. One strategy for interpolation of noisy images is to denoise the image first and then interpolate the denoised image. However, this strategy {{does not lead to}} satisfying results because denoising may smooth image details and also other artifacts such as blurring and blocking introduced due to image denoising will be amplified in the following <b>interpolation</b> <b>stage.</b> Thus, in this paper we propose a joint salt and pepper noise removal and resolution enhancement algorithm using dual-tree complex wavelet transform and feedforward neural networks. In this algorithm, the wavelet subbands corresponding to noise free high resolution image are estimated from noisy low resolution image by multi-layer perceptron (MLP). Therefore the noise free high resolution image is obtained by complex wavelet reconstruction of the estimated subbands. Takeing advantages of complex wavelet transform such as nearly shift invariance and directional selectivity the subband estimation by neural networks is done with high accuracy. As it is verified in the experimental results, the proposed algorithm has better performance both subjectively and objectively and is able to maintain the image fine structures well...|$|E
40|$|Synthetic {{aperture}} radar processing is {{a complex}} task that involves advanced signal processing techniques and intense computational effort. While the first issue has now reached a mature stage, {{the question of how}} to produce accurately focused images in real-time, without mainframe facilities, is still under debate. The recent introduction of general-purpose graphic processing units seems to be quite promising in this view, especially for the decreased per-core cost barrier and for the affordable programming complexity. The authors explain, in this work, the main computational features of a range-Doppler Synthetic Aperture Radar (SAR) processor, trying to disclose the degree of parallelism in the operations at the light of the CUDA programming model. Given the extremely flexible structure of the Single Instruction Multiple Threads (SIMT) model, the authors show that the optimization of a SAR processing unit cannot reduce to an FFT optimization, although this is a quite extensively used kernel. Actually, it is noticeable that the most significant advantage is obtained in the range cell migration correction kernel where a complex <b>interpolation</b> <b>stage</b> is performed very efficiently exploiting the SIMT model. Performance show that, using a single Nvidia Tesla-C 1060 GPU board, the obtained processing time is more than fifteen time better than our test workstation...|$|E
40|$|The {{objective}} {{of this study is}} to show an alternative for the interpolation and integration of GPS data of different precisions, aiming a Digital Terrain Modeling (DTM). GPS surveying data were obtained using relative positioning with kinematic technique, using the phases of the L 1 and L 2 carriers, or only with L 1 carrier phase, and the use of the C/A-code observations. The precisions obtained with the post-processing of the GPS surveying data are in the centimetric (≈ 20 up to 50 cm) and metric (≈ 1 up to 3 m) orders, respectively. The proposed methodology was based on integration technique using Artificial Neural Networks (ANN) algorithm. The results were tested by means of analyses of the behavior of the gridding generated with the conventional interpolation algorithm Inverse Square Distance (ISD), which was adopted as the reference model for the comparisons among methods. Qualitative (isolines and DTM) and quantitative (residuals) analyses were accomplished and comparisons with the gridding generated by the ANN in the <b>interpolation</b> <b>stage</b> and integration of the GPS data were done. The tested method was shown viable in comparison with the ISD algorithm. In conclusion RNA was showed able for the interpolation and integration of GPS data of different precisions...|$|E
40|$|This report gives {{a summary}} of the {{research}} accomplished under this project which included: Collocation analysis of multistage separation systems; Heat and mass transport fundamentals; Fractionation tray modeling; and Computational and statistical methods. The large equation sets encountered in tray-by-tray modeling of distillation systems are a major obstacle in computer-aided process engineering. The authors addressed this difficulty by approximating the multistage equations with much smaller sets, obtained by orthogonal polynomial <b>interpolation</b> over the <b>stages.</b> A new approach to column design was initiated when they discovered how to extend their collocation formulas to modules containing non-integer numbers of stages. This extension proved equally useful for column simulations in the presence of more than one liquid phase. They also investigated strategies to handle locally steep concentration profiles and high-purity separations. As a basis for realistic modeling of fractionating trays, a comprehensive comparison of various computation methods for multicomponent mass transfer was undertaken. A theoretical study was carried out for asymptotic forms for heat and mass transfer rates in boundary layers. Newton`s method was used in several of the computational algorithms for equation solving, parametric sensitivity analysis and nonlinear parameter estimation. The authors investigated modifications of the Newton method designed to achieve convergence over a wider range of initial guesses. Other computer codes have been enhanced {{as a result of this}} project...|$|R
40|$|The {{needs of}} future High Energy Physics (HEP) {{experiments}} {{require the use}} of time measurements systems with remarkable benefits in terms of resolution, dynamic range and high operating frequency. This thesis is focused on the development and evaluation of high resolution and long range Time to Digital Converter (TDC) architectures suitable for the measurement of ~ 100 ns time intervals, ~ 100 ps resolution and MHz repetition rate {{in the context of the}} detectors of the KLOE experiment. The KLOE experiment has been designed in order to record e+e- collisions at DAΦNE, the ϕ-factory at the Laboratori Nazionali di Frascati. The thesis work is aimed to the specific requirements of the HEP experimental environment; nevertheless, it is also useful in many applications of science and industry where high resolution time measurements are required. Different configurations of tapped delay lines are widely used to measure nanosecond time intervals both in ASIC and FPGA devices. I designed and built a daughter board hosting the Virtex- 5 FPGA from Xilinx, to test two different TDC architectures. Both TDC approaches use the classic Nutt method based on the two <b>stage</b> <b>interpolation</b> within the system clock cycle. The fine measurement of the short intervals have been performed by means of two different methods. The delay elements exploit either general purpose FPGA’s resources, like logic element, or special purpose resources, like dedicated carry logic. In the Virtex- 5, these chain structures provide short predefined routes between identical logic elements. They are ideal for TDC delay chain implementation. The first architecture uses carry chain delays, while in the second one a differential tapped delay line is used. On the TDC Tester board two high stability oscillators, have been installed in order to compare their performances. The oscillators contain an internal voltage regulator for improved stability and noise performance. The Tester daughter board is hosted by a VME module which allows us to test and read-out the TDC via an embedded microprocessor. In this thesis I show the board and FPGA architectures together with experimental results. The performance of the TDCs was examined over the temperature range from 25 °C to 75 °C. The carry chain delay line TDC architecture seems to be the best one. The resolution values obtained are well fitting the needs of the time measurements of the KLOE experiment. In fact, in the KLOE experiment, the dynamic range is defined by the kinematics of kaon decays (the KL meson lifetime is about 50 ns); so the range of measurement needed is about 200 - 300 ns. In this range the carry chain delay line TDC shows a very good resolution of about 20 ps. Furthermore, the range of measurement of this kind of TDC is not limited to that value, but can easily extend until 20 μs with a time resolution below 35 ps. The time resolution increases of about 0. 18 ps/°C; in fact for a 100 ns time interval measurements, it is between 17 ps (at 25 °C) and 26 ps (at 75 °) for a 50 °C temperature shift. Moreover such a TDC is virtually dead time free; it implies its use in high trigger rate environment like Super B Factories where the estimated trigger rate is about 150 KHz at a luminosity of 1036 cm- 2 s- 1...|$|R
40|$|We {{develop an}} eective algorithm, {{based on the}} {{filtered}} backprojection (FBP) approach, for the imaging of vegetation. Under the FBP scheme, the reconstruction amounts at a non-trivial Fourier inversion, since the data are Fourier samples arranged on a non-Cartesian grid. The computational issue is eciently tackled by Non-Uniform Fast Fourier Transforms (NUFFTs), whose complexity grows asymptotically {{as that of a}} standard FFT. Furthermore, significant speed-ups, as compared to fast CPU implementations, are obtained by a parallel versions of the NUFFT algorithm, purposely designed to be run on Graphic Processing Units (GPUs) by using the CUDA language. The performance of the parallel algorithm has been assessed in comparison to a CPU-multicore accelerated, Matlab implementation of the same routine, to other CPU-multicore accelerated implementations based on standard FFT and employing linear, cubic, spline and sinc interpolations and to a dierent, parallel algorithm exploiting a parallel linear <b>interpolation</b> <b>stage.</b> The proposed approach has resulted the most computationally convenient. Furthermore, an indoor, polarimetric experimental setup is developed, capable to isolate and introduce, one at a time, dierent non-idealities of a real acquisition, as the sources (wind, rain) of temporal decorrelation. Experimental far-field polarimetric measurements on a thuja plicata (western redcedar) tree point out the performance of the set up algorithm, its robustness against data truncation and temporal decorrelation as well as the possibility of discriminating scatterers with dierent features within the investigated scene...|$|E
