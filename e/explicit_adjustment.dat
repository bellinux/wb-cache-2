16|26|Public
40|$|A common aim of {{epidemiological}} {{research is to}} estimate the causal effect of a particular exposure on a particular outcome. Towards this end, observed associations are often ‘adjusted’ for potential confounding variables. When the potential confounders are unmeasured, <b>explicit</b> <b>adjustment</b> becomes unfeasible. It has been demonstrated that causal effects can be estimated even {{in the presence of}} umeasured confounding, utilizing a method called ‘front-door blocking’. In this paper we generalize this method to longitudinal studies. We demonstrate that the method of front-door blocking poses a number of challenging statistical problems, analogous to the famous problems associ- ated with the method of ‘back-door blocking’...|$|E
40|$|Information {{sharing in}} {{distance}} collaboration: A software engineering perspective, QueenslandFactors in software engineering workgroups such as geographical dispersion and background discipline can be conceptually characterized as "distances", {{and they are}} obstructive to team collaboration and information sharing. This thesis focuses on information sharing across multidimensional distances and develops an information sharing distance model, with six core dimensions: geography, time zone, organization, multi-discipline, heterogeneous roles, and varying project tenure. The {{research suggests that the}} effectiveness of workgroups may be improved through mindful conducts of information sharing, especially proactive consideration of, and <b>explicit</b> <b>adjustment</b> for, the distances of the recipient when sharing information...|$|E
40|$|There {{are often}} {{intervals}} between price changes, a rigidity {{that is usually}} modeled by an <b>explicit</b> <b>adjustment</b> cost. This paper shows that such dynamics of prices can also represent the optimal actions of price-setters who are rationally inattentive. The model generates {{a wide spectrum of}} observed price series properties that sticky-price models cannot explain. The one information constraint implies that prices can change frequently, prices move back and forth between a few rigid values, hazard functions are downward sloping, and responses to persistent shocks are sluggish. The methodological contribution is that the model fully implements rational inattention with no simplifying assumptions on the functional forms of the processed signals...|$|E
40|$|Using {{a scanner}} data set that covers nearly all {{computer}} {{sales in the}} Netherlands {{for a period of}} three years, this paper investigates whether there is a significant difference between a matched model index and a hedonic imputed index, which also takes non-matched observations into account. The result of this study was that this {{does not appear to be}} the case. The lack of significance of the difference can be attributed to two reasons: the high share in sales values of the matched items, and the mediocre fit of the hedonic model. Given the fact that an earlier study based on a different data set (Van Mulligen, 2002) also pointed out that the difference between a matched model index and a hedonic imputed index is small, we draw the conclusion that making <b>explicit</b> <b>adjustments</b> for non-matched items is not necessary. Although the official CPI for computers also uses the matched model methodology, it appears to introduce a substantial downward bias in the actual quality adjusted price index, due to biased sampling and the lack of representative weighting of individual items. The main area for improvement of the CPI for computers (and possibly other durables as well) lies therefore in more frequent sampling, weighting and chaining of the indices rather than making <b>explicit</b> quality <b>adjustments.</b> ...|$|R
40|$|This paper studies {{conditions}} under which prices are sticky in a non-competitive market {{even though there is}} no menu cost associated with price changes. We posit that a typical seller encounters a series of repeat-buyers some of whom may revise their reservation prices (in an unknown fashion) if the seller changes the price offer. In this sense the reservation prices are pliable, or flexible. The seller fails to learn some of the changes in reservation prices from the market data instantaneously. As a result, a rational seller may find it profitable to adjust the price partially in order to collect more endogenous information about the unknown demand parameters. An incomplete price adjustment will thus {{turn out to be the}} optimal pricing strategy of a seller even if there is no <b>explicit</b> price <b>adjustment</b> cost, such as menu costs. Price rigidity, in the absence of <b>explicit</b> price <b>adjustment</b> costs (such as menu costs), can assume central importance in providing a theoretical salience to fix-price models and, thereby, explain persistence of unemployment. ...|$|R
40|$|Equations are {{developed}} {{with which to}} calculate lift and drag coefficients along the spans of torsionally-stiff rotating airfoils of the type used in wind turbine rotors and wind tunnel fans, at angles of attack in both the unstalled and stalled aerodynamic regimes. <b>Explicit</b> <b>adjustments</b> are made {{for the effects of}} aspect ratio (length to chord width) and airfoil thickness ratio. Calculated lift and drag parameters are compared to measured parameters for 55 airfoil data sets including 585 test points. Mean deviation was found to be - 0. 4 percent and standard deviation was 4. 8 percent. When the proposed equations were applied to the calculation of power from a stall-controlled wind turbine tested in a NASA wind tunnel, mean deviation from 54 data points was - 1. 3 percent and standard deviation was 4. 0 percent. Pressure-rise calculations for a large wind tunnel fan deviated by 2. 7 percent (mean) and 4. 4 percent (standard). The assumption that a single set of lift and drag coefficient equations can represent the stalled aerodynamic behavior {{of a wide variety of}} airfoils was found to be satisfactory...|$|R
30|$|In {{order to}} account for {{reallocation}} dynamics in their projection model Maier et al. (2014) let employer-set wages partially depend on labour supply scarcity. Labour supply, in turn, responds to differences in relative wages of occupations by changing their occupational mobility behaviour in that the workers propensity {{to stay in their}} training occupation correlates positively with a lower outside option. In this model set-up, wage is the only <b>explicit</b> <b>adjustment</b> channel of employers and worker behaviour in response to misallocations of labour. All other factors, which influence mobility decisions of workers, are assumed to follow a constant time trend. Other factors, which drive wage setting of the employer, are assumed to relate to the production process and outside wage pressures.|$|E
40|$|A discrete-time {{dynamical}} {{system is}} proposed to model {{a class of}} binary choice games with externalities as those described by Schelling (1973, 1978). In order to analyze some oscillatory time patterns and problems of equilibrium selection that were not considered in the qualitative analysis given by Schelling, we introduce an <b>explicit</b> <b>adjustment</b> mechanism. We perform a global dynamic analysis {{that allows us to}} explain the transition toward nonconnected basins of attraction when several coexisting attractors are present. This gives a formal explanation of some overshooting effects in social systems and of the consequent cyclic behaviors qualitatively described in Schelling (1978). Moreover, we show how the occurrence of a global bifurcation may lead to the explanation of situations of path dependence and the creation of thresholds observed in real life situations of collective choices, leading to extreme forms of irreversible departure from an equilibrium and uncertainty about the long run evolution of the some social systems...|$|E
40|$|This paper uses {{a dynamic}} computable general {{equilibrium}} model to simulate the effects of unilateral reductions by the U. S. in tariffs and "voluntary" export restraints (VER's). We consider 50 percent cuts in tariffs and in ad valorem VER equivalents, separately and in combination. The model features intertemporal optimization by households and firms, <b>explicit</b> <b>adjustment</b> dynamics, an integrated treatment of the current and capital accounts of the balance of payments, and industry disaggregation. Central findings include: (1) VER's are considerably more significant than tariffs {{in terms of the}} magnitude of the macroeconomic effects induced by their reduction; (2) while VER reductions enhance domestic welfare, unilateral tariff cuts reduce domestic welfare (as a consequence of U. S. monopsony power and associated adverse terms of trade effects); (3) international capital movements critically regulate the responses of the U. S. and foreign economies to these trade initiatives and produce significant differences between short and long-run effects; and (4) effects differ substantially across industries. Together, these findings indicate that simulation analyses that disregard international capital movements, adjustment dynamics, and industry differences may generate seriously misleading results. ...|$|E
40|$|To provide layered {{multicast}} with responsiveness, {{efficiency in}} network utilization, scalability and fairness (including inter-protocol fairness, intra-protocol fairness, intra-session fairness and TCP-friendliness) for layered multicast, we propose {{in this paper}} a new multicast congestion control, called <b>Explicit</b> Rate <b>Adjustment</b> (ERA). Our protocol uses an algorithm relying on TCP throughput equation and Packet-bunch Probe techniques to detect optimal bandwidth utilization; then adjusts the reception rate accordingly. We have built ERA into a network simulator (ns 2) and demonstrate via simulations that the goals are reached...|$|R
40|$|Abstract 1 : Using {{a scanner}} data set that covers nearly all {{computer}} {{sales in the}} Netherlands {{for a period of}} three years, this paper investigates whether there is a significant difference between a matched model index and a hedonic imputed index, which also takes non-matched observations into account. The result of this study was that this {{does not appear to be}} the case. The lack of significance of the difference can be attributed to two reasons: the high share in sales values of the matched items, and the mediocre fit of the hedonic model. Given the fact that an earlier study based on a different data set (Van Mulligen, 2002) also pointed out that the difference between a matched model index and a hedonic imputed index is small, we draw the conclusion that making <b>explicit</b> <b>adjustments</b> for non-matched items is not necessary. Although the official CPI for computers also uses the matched model methodology, it appears to introduce a substantial downward bias in the actual quality adjusted price index, due to biased sampling and the lack of representative weighting of individual items. The main area for improvement of the CPI for computers (and possibly other durables as well) lies therefore in more frequen...|$|R
40|$|MR-MCC) scheme {{has been}} {{considered}} as a suitable scheme for multicasting, for. a very large heterogeneous group of receivers. In. this work, we propose- a new design of MR-MCC using <b>explicit</b> rate. <b>adjustment</b> based on TCP throughput equation and Packet-pair Probe. The design. goals are: scalability, resporkiveness, fast convergence, fairness (including inter-protocol fairness, intra-protocol fairness, intra-session fairness and TCP-friendliness) and feasibility. We have implemented our design into a network simulator (ns 2) and undertake a perfoynce evaluation to investigate it. The results show that our protocol holds good prope&es of the design goal [...] . ~...|$|R
40|$|We {{document}} substantial practitioner {{interest in}} {{measures of the}} downside tail risk of hedge funds, such as maximum drawdown (MDD) and worst one-period loss, together with a general sentiment that “classical ” performance measures such as the Sharpe Ratio do not convey enough information about tail risk. We characterize the �nite-sample distribution of these measures and show that it depends linearly on variance. Sample variance, appropriately transformed, as well as Bayesian estimates of the entire distribution of returns, are shown to yield better forward-looking estimates of extreme measures than in-sample extremes themselves. We then show that worst oneperiod loss is “manipulation-proof ” {{in the sense of}} Ingersoll et al (2007). Moreover, MDD has a degree of robustness against return smoothing in the spirit of Getmansky et al. (2004). We prove that one cannot have it both ways: “manipulation-proof ” measures are not “smoothing-proof ” and vice-versa. Finally, we show that the expectation of manipulation-proof measures is an increasing function of sample size, and for worst one-period loss we provide an <b>explicit</b> <b>adjustment</b> for this problem...|$|E
40|$|A non-associated {{plasticity}} {{theory for}} granular materials {{has been developed}} in Part 1 based {{on the concept of}} a characteristic stress state of vanishing incremental dilation. The model is fully three-dimensional and is defined by six material parameters: two for elastic stiffness, one for plastic stiffness, two for the shapes of yield and plastic potential surfaces and one for the dilation at failure. In this paper a calibration procedure is developed using test data only from a standard triaxial test. It is found that the shape parameter for the yield surface can be estimated from the plastic how parameters, thus reducing the number of free parameters to five. Calibration examples are shown, as well as predictions made, for different confining stress levels and constant volume tests on sand. The model is found to represent stress-strain behaviour and development of volumetric strain in standard triaxial tests well. The model provides good predictions of constant volume behaviour of dense as well as loose sand on the basis of calibration by standard triaxial test data. A simple explicit formula is derived for the failure asymptote in constant volume testing, enabling <b>explicit</b> <b>adjustment</b> of the parameters, if incompressible-test data is available...|$|E
40|$|Anti-poverty {{programmes}} often seek {{to improve}} their impact by targeting households for assistance according to welfare measures in a single time period. However, a growing literature shows the importance to poor households of fluctuations in their welfare from month to month and year to year. This study uses a five-year panel of 686 households from rural Pakistan to investigate the magnitude of chronic or transitory poverty making an <b>explicit</b> <b>adjustment</b> for measurement error. The impact of two types of policies (those designed to 'smooth' incomes and those designed to promote income growth) on the severity of chronic and transitory poverty is examined. Since the largest part of the squared poverty gap in our sample is transitory, large reductions in poverty {{can be achieved by}} interventions designed to 'smooth' incomes, but reducing chronic poverty in the long-term requires large and sustained growth in household incomes. The level and variability of incomes is then modelled as a function of household characteristics, education and assets. The resulting model of the income generation process is used to simulate the impact that a range of transfer and investment policies would have upon chronic and transitory poverty. ...|$|E
40|$|This book {{provides}} an advanced treatment of option valuation. The general setting {{is that of}} 2 D continuous-time models with stochastic volatility. <b>Explicit</b> equilibrium risk <b>adjustments</b> and many other new results are provided. Mathematica code for the more important formulas is included. For a summary of results, see the Chapter 1 excerpt. option pricing, stochastic volatility, equilibrium, smile, term structure, implied volatility, eigenvalue, variational, Mathematica, GARCH diffusion, local martingale...|$|R
40|$|ISBN 07340 2953 5 A {{number of}} studies have been {{conducted}} which use the Bloom taxonomy to improve teaching and learning. However, to our knowledge, neither the Bloom taxonomy nor any other established learning taxonomy {{has been used as a}} basis to develop a quantifiable tool that will enable teachers to analyse the cognitive process embedded in the objectives and assessment of a subject, as well as provide a methodology to assess alignment of those objectives with the assessment tasks. This paper presents the development of such a quantifiable tool. We discuss the assumptions, method and potential benefits of the outlined approach and in particular its value in providing a mechanism for comparison between subjects, both over time for agiven subject and between subjects. The approach has been applied to a specific example in the education system of a profession, the Institute of Actuaries of Australia. The approach is not mathematically difficult to develop. The model requires a number of parameters to be specified. Once these parameters are specified then the methodology is robust. Adjustments to results are made by <b>explicit</b> <b>adjustments</b> to the parameters and not the methodology. A key consequence of this is that once the methodology is accepted, results and any changes can be explicitly tracked and the causes unambiguously identified. In environments where subjective opinion may be pronounced, such an approach raises the level and quality of discussion significantly - from ‘shooting the messenger’ to ‘addressing the message’. We also comment on the potential for extensions of this work. In an economic and educational environment where teachers are being held moreaccountable for the attainment of promised learning outcomes for their students the development of the tool proposed here is potentially powerful and widely applicable. Open Acces...|$|R
40|$|The {{incidence}} of obesity {{has increased dramatically}} in the U. S. Obese individuals tend to be sicker and spend more on health care, raising {{the question of who}} bears the {{incidence of}} obesity-related health care costs. This question is particularly interesting among those with group coverage through an employer given the lack of <b>explicit</b> risk <b>adjustment</b> of individual health insurance premiums in the group market. In this paper, we examine the incidence of the healthcare costs of obesity among full time workers. We find that the incremental healthcare costs associated with obesity are passed on to obese workers with employer-sponsored health insurance in the form of lower cash wages. Obese workers in firms without employer-sponsored insurance do not have a wage offset relative to their non-obese counterparts. Our estimate of the wage offset exceeds estimates of the expected incremental health care costs of these individuals for obese women, but not for men. We find that a substantial part of the lower wages among obese women attributed to labor market discrimination {{can be explained by the}} higher health insurance premiums required to cover them. ...|$|R
40|$|The {{gravitation}} {{process of}} market prices towards production prices {{is here presented}} {{by means of an}} analytical framework where the classical capital mobility principle is coupled with a determination of the deviation of market from normal (natural) prices which closely follows the description provided by Adam Smith: each period the level of the market price of a commodity will be higher (lower) than its production price if the quantity brought to the market falls short (exceeds) the level of effectual demand. This approach also simplifies the results with respect to those obtained in cross-dual literature. At the same time, anchoring market prices to effectual demands and quantities brought to the markets requires a careful study of the dynamics of the 'dimensions' along with that of the 'proportions' of the system. Three different versions of the model are thus proposed, to study the gravitation process: i) assuming a given level of aggregate employment; ii) assuming a sort of Say’s law; iii) and on the basis of an <b>explicit</b> <b>adjustment</b> of actual outputs to effectual demands. All these cases describe dynamics in which market prices can converge asymptotically towards production prices...|$|E
40|$|We {{conduct a}} {{laboratory}} experiment {{to shed light}} on the cognitive limitations that may affect the way decision makers respond to changes in their economic environment. The subjects solve a tracking problem: they estimate the probability of a binary event, which changes stochastically. The subjects observe draws and indicate their draw-by-draw estimate. Our subjects depart from the optimal Bayesian benchmark in systematic ways, but these deviations are not simply the result of some boundedly rational, but deterministic rule. Rather, there is a random element in the subjects' response to any given history of evidence. Moreover, subjects adjust their forecast in discrete jumps rather than after each new ring draw, even though there are no <b>explicit</b> <b>adjustment</b> costs. They adjust by both large and small amounts, contrary to the predictions of a simple Ss model of optimal adjustment subject to a fixed cost. Finally, subjects prefer to report 2 ̆ 01 cround number 2 ̆ 01 d probabilities, even though that requires exerting additional effort. Each of these regularities resembles the behavior of firms setting prices for their products. We develop a model of inattentive adjustment and compare its quantitative fit with alternative models of stochastic discrete adjustment used in the literature on price adjustment...|$|E
40|$|Anti-poverty {{programs}} often seek {{to improve}} their impact by targeting households for assistance according {{to one or more}} criteria. Since such targeting criteria are often based upon measurements of welfare in a single time period, they tend to be chosen to provide an indication of the long-run level of welfare. However a growing literature shows the importance to poor households of fluctuations in their welfare from month to month and year to year. This paper measures the extent to which poverty is caused by fluctuations in welfare as well as the long-run level of welfare, using the IFPRI household food security panel which tracked 686 households from rural Pakistan between 1986 / 76 to 1990 / 91. The article compares the poverty impact of policies designed to increase mean incomes ('growth' policies) and those designed to even out fluctuations of income over time ('smoothing policies') after making an <b>explicit</b> <b>adjustment</b> for measurement error. Since the majority of poverty in our sample is transitory, large reductions in poverty can be achieved by interventions designed to 'smooth' incomes, but reducing chronic poverty in the long-term will require large and sustained growth in household incomes. The income generation process is then modelled as a function of household characteristics and the resulting model is used to estimate the poverty impact of a range of interventions including transfer policies and measures designed to build human and physical capital. transitory poverty, policy targeting, measurement...|$|E
40|$|First paragraph: This paper {{attempts}} {{to show the}} importance of the work on cyclical manhour variation, pioneered by Oi (1962), Rosen (1968), Nadiri and Rosen (1969), (1974 a) and (1974 b)) and Ehrenberg (1971), towards understanding the observed short-run relationships between nominal wage changes and unemployment. The focal point for the inflation analysis is the Phillips curve derivation of Barro and Grossman (1976) and Grossman (1974) since their approach of denning excess demand in terms of manhours provides a crucial link between employment and inflation theories. While the general Barro and Grossman macroeconomic system (see also (1971)) represents an extreme case within the class of non-market clearing models, given its arbitrary fixed price assumption, its use here, apart from convenience, is defended for two reasons. First, the adoption of a more sophisticated non-market clearing model incorporating an <b>explicit</b> price <b>adjustment</b> equation would not alter the main arguments substantively. Secondly, although the applicability of the Barro and Grossman methodology was originally questioned, since it left unexplained why prices may fail to clear markets, more recent work on price adjustment has helped to give a firmer foundation to this approach (see Gordon (1981)) ...|$|R
40|$|Inconsistencies {{can arise}} in ocean {{circulation}} models when {{part of the}} physical processes responsible for vertical mixing is described in the usual differential form and part is formulated as adjustment processes. Examples for the latter class are <b>explicit</b> convective <b>adjustment</b> and Kraus–Turner type models of the surface mixed layer. Implicit convective adjustment as well as various representations of interior-ocean mixing are normally described in differential form. All these schemes mix density, with a mixing intensity that itself depends on stratification. This requires that information concerning static stability is passed through the individual mixing routines in a consistent sequence. It is shown that inconsistencies can arise when coupling a Kraus–Turner type model of wind-induced mixing with both a standard implicit convective adjustment {{as well as with}} an isopycnal mixing scheme. This leads to considerably overestimated mixed layer depths, for example, by hundreds of meters in the subpolar North Atlantic. The problem is eliminated first by ensuring that dissipation of potential energy during convection is included in the mixing scheme, even when considering wind-induced turbulence only, and second, by either calling the mixed layer routine before the differential vertical mixing scheme or tapering the vertical diffusivities to zero within the surface mixed layer...|$|R
40|$|Over {{the past}} 20  years, {{advances}} in satellite remote sensing of pollution-relevant species have made space-borne observations {{an increasingly important}} part of atmospheric chemistry research and air quality management. This progress has been facilitated by advanced UV–vis spectrometers, such as the Ozone Monitoring Instrument (OMI) on board the NASA Earth Observing System (EOS) Aura satellite, and continues with new instruments, such as the Ozone Mapping and Profiler Suite (OMPS) on board the NASA–NOAA Suomi National Polar-orbiting Partnership (SNPP) satellite. In this study, we demonstrate that it is possible, using our state-of-the-art principal component analysis (PCA) retrieval technique, to continue the long-term global SO 2 pollution monitoring started by OMI with the current and future OMPS instruments that will fly on the NOAA Joint Polar Satellite System (JPSS) 1, 2, 3, and 4 satellites in addition to SNPP, with a very good consistency of retrievals from these instruments. Since OMI SO 2 data have been primarily used for (1)  providing regional context on air pollution and long-range transport {{on a daily basis}} and (2)  providing information on point emission sources on an annual basis after data averaging, we focused on these two aspects in our OMI–OMPS comparisons. Four years of retrievals (2012 – 2015) have been compared for three regions: eastern China, Mexico, and South Africa. In general, the comparisons show relatively high correlations (r [*]=[*] 0. [*] 79 – 0. 96) of daily regional averaged SO 2 mass between the two instruments and near-unity regression slopes (0. 76 – 0. 97). The annual averaged SO 2 loading differences between OMI and OMPS are small (<  0. 03 Dobson unit (DU) over South Africa and up to 0. 1  DU over eastern China). We also found a very good correlation (r [*]=[*] 0. [*] 92 – 0. 97) in the spatial distribution of annual averaged SO 2 between OMI and OMPS over the three regions during 2012 – 2015. The emissions from ∼  400 SO 2 sources calculated with the two instruments also show a very good correlation (r [*]=[*]∼[*] 0. 9) in each year during 2012 – 2015. OMPS-detected SO 2 point source emissions are slightly lower than those from OMI, but OMI–OMPS differences decrease with increasing strength of source. The OMI–OMPS SO 2 mass differences on a pixel by pixel (daily) basis in each region can show substantial differences. The two instruments have a spatial correlation coefficient of 0. 7 or better on <[*]∼[*] 50  % of the days. It is worth noting that consistent SO 2 retrievals were achieved without any <b>explicit</b> <b>adjustments</b> to OMI or OMPS radiance data and that the retrieval agreement may be further improved by introducing a more comprehensive Jacobian lookup table than is currently used...|$|R
40|$|In {{this paper}} we analyse the {{observed}} systematic differences in costs for teaching hospitals (THhenceforth) in Spain. Concern has been voiced regarding {{the existence of}} a bias in the financing of TH’s has been raised once prospective budgets are in the arena for hospital finance, and claims for adjusting {{to take into account the}} ‘legitimate’ extra costs of teaching on hospital expenditure are well grounded. We focus on the estimation of the impact of teaching status on average cost. We used a version of a multiproduct hospital cost function taking into account some relevant factors from which to derive the observed differences. We assume that the relationship between the explanatory and the dependent variables follows a flexible form for each of the explanatory variables. We also model the underlying covariance structure of the data. We assumed two qualitatively different sources of variation: random effects and serial correlation. Random variation refers to both general level variation (through the random intercept) and the variation specifically related to teaching status. We postulate that the impact of the random effects is predominant over the impact of the serial correlation effects. The model is estimated by restricted maximum likelihood. Our results show that costs are 9 % higher (15 % in the case of median costs) in teaching than in non-teaching hospitals. That is, teaching status legitimately explains no more than half of the observed difference in actual costs. The impact on costs of the teaching factor depends on the number of residents, with an increase of 51. 11 % per resident for hospitals with fewer than 204 residents (third quartile of the number of residents) and 41. 84 % for hospitals with more than 204 residents. In addition, the estimated dispersion is higher among teaching hospitals. As a result, due to the considerable observed heterogeneity, results should be interpreted with caution. From a policy making point of view, we conclude that since a higher relative burden for medical training is under public hospital command, an <b>explicit</b> <b>adjustment</b> to the extra costs that the teaching factor imposes on hospital finance is needed, before hospital competition for inpatient services takes place. Cost functions, semi-parametric estimation, regression analysis, teaching hospitals, prospective payments...|$|E
40|$|As sessile and photoautotrophic organisms, plants {{require an}} <b>explicit</b> <b>adjustment</b> of the {{development}} processes with the prevailing environmental conditions. Light {{is the most important}} environmental factor, which acts not only as the energy source, but also as the regulation signal for numerous physiological processes in the plants. A set of photosensing molecules – photoreceptors – have been developed in plants to perceive light of different quality, intensity, direction and continuance. Phytochromes are photoreceptors that perceive the red and far-red regions of the light spectrum (650 - 750 nm). In Arabidopsis the phytochrome gene family consists of five members (PHYA-E). Phytochromes can be divided into “light-labile” (phyA) and light-stable (phyB-E) types. Phytochromes mediate responses that can be categorized as follows: very low fluence responses (VLFR), low fluence responses (LFR) and high irradiance responses (HIR). The light-labile phyA mediates responses that are characterized by a low Pfr/Pr ratio and R/FR irreversibility, namely VLFR and FR-HIR. This study describes the Arabidopsis thaliana mutant (psm, renamed phyA- 5) showing a distinct photomorphogenic phenotype. Molecular mapping revealed a new missense mutation in the PHYA amino terminal extension (NTE) domain. The phyA- 5 mutant exhibits a hyposensitive phenotype in continuous low-intensity far-red light, whereas in high-intensity conditions the mutant resembles the wild-type. Both VLFR and HIR are reduced in the mutant. The mutation does not affect the expression level of PHYA. The dark-accumulated level of the mutated phyA- 5 protein and R light-induced degradation were shown to be normal, whereas higher residual amounts of phyA- 5 were detected in low FR. It has been shown that the complex mutant phenotype and the abnormal stability of the mutated protein under low intensity of FR light are caused by the impaired nuclear import of the phyA- 5 under these conditions, whereas high-fluence light induces normal nuclear import, resulting in a phenotype resembling the wild-type. Furthermore, it has been demonstrated that the reduced nuclear import of phyA- 5 is caused by the decreased binding affinity of the mutant photoreceptor to the nuclear import facilitators FHY 1 and FHL. Studies on transgenic plants expressing phyA- 5 -YFP-NLS protein in phyA- 201 background provided evidence that phyA- 5 behaves identically to wild-type phyA, i. e. it is constitutively localized in the nucleus. To sum up, the data obtained show that the NTE domain influences the regulation of phyA nuclear import through participation in the assembling of the FHY 1 /FHL/PHYA Pfr complex and the resulting aberrant nucleo/cytoplasmic distribution impairs light-induced degradation of phyA. Results of this study underline the interconnection between phototransformation of phyA, its nuclear import, functioning and degradation. ...|$|E
40|$|Introduction: It {{has been}} shown that machine-learning methods applied to voxel-based morphometry (VBM) data allows the {{prediction}} of brain age [1]. Dimensionality reduction is a critical aspect of such brain-based prediction of phenotypical characteristics to counter the curse of dimensionality associated with voxel-wise analysis. While previous age-predictions have employed PCA based compression, non-negative matrix factorization (NNMF) has recently been suggested as a plausible factorization of high-dimensional VBM data [4]. Non-negativity and sparsity of the components obtained from NNMF facilitate relatively more optimal solution than the PCA based compression [4]. Here, we evaluate, i) whether NNMF compression allows predictions of biological age that reproduce those from previously reported analyses [2], ii) the impact of the NNMF’s granularity on the prediction accuracy, iii) the possible effect of the factorizations derived from different datasets on the prediction, and iv) whether <b>explicit</b> <b>adjustment</b> can address the model bias inherent to many brain-based predictions. Methods: VBM 8 preprocessing (using only non-linear modulation and 8 mm FWHM smoothing [3]) was used to compute voxel-wise GM volumes for two datasets, 1) 693 healthy older adults (age: 55 - 75 years) scanned at a single site (“ 1000 BRAINS) [1], 2) 1084 healthy adults (age: 18 - 81 years), scanned at multiple sites (“Mixed”) (Fig 1 A). NNMF solutions for both groups were derived at different levels of granularity. Age prediction was performed by fitting LASSO regression models either on the coefficient matrix from the respective NNMF or by those that were derived from projecting a group’s data on the respective other groups components. Model generalization was evaluated by 10 -fold cross-validation replicated 25 times. To address the known bias towards the mean, i. e., overestimation of young and underestimation of older subjects, we additionally tested models that explicitly fitted the regression-slope between the real and predicted training set and used this to adjust the expected slope of the test set to 45 degrees. Results: In both datasets, NNMF components resembled neurobiologically reasonable patterning of the brain (Fig 1 B). Prediction accuracy based on the projection of data on the components from either group was virtually identical (Fig 2 A). For both datasets, mean absolute errors (MAE) declined with higher granularity of the components and reached values well comparable to previous approaches even when using components derived from an independent sample (MAE: 3. 6 years for 1000 BRAINS; 6. 4 years for Mixed). Plotting the prediction error relative to the biological age of the subjects revealed the bias towards the mean across both datasets (Fig 2 B). Adjusting for the slope estimated in the training set allows removing this bias, though it needs to be noted that this comes at the cost of reduced precision, i. e., unbiased estimates yield a slightly higher MAE. Conclusion: NNMF allows the definition of co-variation patterns in VBM data. Due to the non- negativity and sparseness, NNMF enable substantially easier and higher biological interpretation than other methods for data compression such as PCA [4]. We showed that NNMF compression of VBM data over the lifespan allows predicting previously unseen subjects’ age with a precision that is comparable to earlier reports using PCA for data compression [2], while offering the potential for neurobiological interpretation. Importantly, accuracy seems to be independent of whether the components were derived from the same dataset or from a dataset that is not only independent but also different in age distribution. We note that accuracies tend to continuously decrease with higher granularity, although performance tends to plateau at about 300 components. Finally, adjusting the inherent bias of sparse regression models yields unbiased out-of-sample predictions but comes at the expense of slightly higher mean errors. Peer reviewe...|$|E
40|$|When a {{manufacturing}} process {{is subject to}} random shocks, detecting {{the changes in the}} process and adjusting an out-of-target process are two essential functions of process quality control. Traditional SPC techniques emphasize process change detection, but do not provide an <b>explicit</b> process <b>adjustment</b> method. This paper discusses a general sequential adjustment procedure based on Stochastic Approximation techniques and combines it with several commonly used control charts. The performance of these methods depends on the sensitivity of the control chart to detect shifts in the process mean, on the accuracy of the initial estimate of shift size, and on the number of sequential adjustments that are made. It is shown that sequential adjustments are superior to single adjustment strategies for almost all types of process shifts and magnitudes considered. A combined CUSUM chart used in conjunction with our sequential adjustment approach can improve the average squared deviations, the performance index considered herein, more than any other combined scheme unless the shift size is very large. The proposed integrated approach is compared to always applying a standard integral (EWMA) controller with no monitoring component. The number of adjustments in the proposed approach is justified by comparing the cost and the benefit of the adjustment. We show that this strategy – combining control charts and sequential adjustments – is recommended for monitoring and adjusting a process when random shocks occur infrequently in time. ...|$|R
40|$|Multi-rate Multicast Congestion Control (MR-MCC) is a {{promising}} opportunity {{to tackle the}} multicast congestion control problem in huge and heterogeneous networks like the global Internet. However, {{it is not easy}} to provide an MR-MCC design with responsiveness, efficiency of network utilisation, low packet loss, scalability and fairness (including inter-protocol fairness, intra-protocol fairness, intra-session fairness and TCP-friendliness) as well as feasible implementation. This thesis is concerned with the design and performance evaluation of multi-rate multicast congestion control. We aim to address the problems faced by the previous proposals. In doing so, we have established a rigorous performance evaluation methodology via netwrok simulation, and defined a set of key evaluation criteria to test MR-MCC protocols. Then, we have undertaken a performance evaluation of the previously proposed MR-MCC protocols (RLM, RLC, FLID-DL and PLM). Having learnt from our simulation analysis of previous proposals, we propose our innovative design of an experimental MR-MCC protocol, called <b>Explicit</b> Rate <b>Adjustment</b> (ERA). The design goals are scalability, responsiveness, fast convergence, fairness (including intra-session fairness, intra-protocol fairness, and inter-protocol fairness, in particular TCP friendliness), efficiency in network utilisation, and simplicity to implement. We have also implemented our experimental MR-MCC protocol in the ns- 2 network simulation package. Through simulation, we demonstrate the performance evaluation of our MR-MCC extensively and demonstrate that it provides the desirable properties mentioned previously. ...|$|R
40|$|Considers {{why this}} has {{occurred}} and the ONS response to reducing {{the potential for}} such bias in futureHistorically, Office for National Statistics (ONS) early estimates {{of the growth of}} the volume of gross domestic product have tended to be revised up on average. This article considers why this has occurred and ONS's response to reducing the potential for such bias in future. It concludes that there is insufficient evidence for ONS to make an <b>explicit</b> aggregate level <b>adjustment</b> to these early estimates to anticipate potential future bias, and that there are many more fundamental reasons why it should not make such an adjustment. Instead ONS should continue to research the reasons for any bias, and seek to reduce or remove it altogether. Economic & Labour Market Review (2008) 2, 48 – 52; doi: 10. 1057 /elmr. 2008. 105...|$|R
40|$|It {{has been}} shown that machine-learning methods applied to voxel-based morphometry (VBM) data allows the {{prediction}} of brain age [1]. Dimensionality reduction is a critical aspect of such brain-based prediction of phenotypical characteristics to counter the curse of dimensionality associated with voxel-wise analysis. While previous age-predictions have employed PCA based compression, non-negative matrix factorization (NNMF) has recently been suggested as a plausible factorization of high-dimensional VBM data [4]. Non-negativity and sparsity of the components obtained from NNMF facilitate relatively more optimal solution than the PCA based compression [4]. Here, we evaluate, i) whether NNMF compression allows predictions of biological age that reproduce those from previously reported analyses [2], ii) the impact of the NNMF’s granularity on the prediction accuracy, iii) the possible effect of the factorizations derived from different datasets on the prediction, and iv) whether <b>explicit</b> <b>adjustment</b> can address the model bias inherent to many brain-based predictions. Methods:VBM 8 preprocessing (using only non-linear modulation and 8 mm FWHM smoothing [3]) was used to compute voxel-wise GM volumes for two datasets, 1) 693 healthy older adults (age: 55 - 75 years) scanned at a single site (“ 1000 BRAINS) [1], 2) 1084 healthy adults (age: 18 - 81 years), scanned at multiple sites (“Mixed”) (Fig 1 A). NNMF solutions for both groups were derived at different levels of granularity. Age prediction was performed by fitting LASSO regression models either on the coefficient matrix from the respective NNMF or by those that were derived from projecting a group’s data on the respective other groups components. Model generalization was evaluated by 10 -fold cross-validation replicated 25 times. To address the known bias towards the mean, i. e., overestimation of young and underestimation of older subjects, we additionally tested models that explicitly fitted the regression-slope between the real and predicted training set and used this to adjust the expected slope of the test set to 45 degrees. Results:In both datasets, NNMF components resembled neurobiologically reasonable patterning of the brain (Fig 1 B). Prediction accuracy based on the projection of data on the components from either group was virtually identical (Fig 2 A). For both datasets, mean absolute errors (MAE) declined with higher granularity of the components and reached values well comparable to previous approaches even when using components derived from an independent sample (MAE: 3. 6 years for 1000 BRAINS; 6. 4 years for Mixed). Plotting the prediction error relative to the biological age of the subjects revealed the bias towards the mean across both datasets (Fig 2 B). Adjusting for the slope estimated in the training set allows removing this bias, though it needs to be noted that this comes at the cost of reduced precision, i. e., unbiased estimates yield a slightly higher MAE. Conclusion:NNMF allows the definition of co-variation patterns in VBM data. Due to the non- negativity and sparseness, NNMF enable substantially easier and higher biological interpretation than other methods for data compression such as PCA [4]. We showed that NNMF compression of VBM data over the lifespan allows predicting previously unseen subjects’ age with a precision that is comparable to earlier reports using PCA for data compression [2], while offering the potential for neurobiological interpretation. Importantly, accuracy seems to be independent of whether the components were derived from the same dataset or from a dataset that is not only independent but also different in age distribution. We note that accuracies tend to continuously decrease with higher granularity, although performance tends to plateau at about 300 components. Finally, adjusting the inherent bias of sparse regression models yields unbiased out-of-sample predictions but comes at the expense of slightly higher mean errors...|$|E
40|$|A {{significant}} change in the structure of employment in Brazil in the period 1988 - 95 took place. Industrial employment fell, especially in the most traditional sectors such as clothing, textiles and footwear. There has been a shift of workers from the industrial sector to the services sector and an increase in the share of self-employed workers and informal wage earners. The quality of jobs being created in the services sector are not of the same 'quality' as those in the industrial sector, leading to a deterioration of employment conditions and of the labour market performance. Hence the rate of unemployment is not a very good measure of labour market conditions during adjustment. The creation of jobs in general might therefore not be an <b>explicit</b> target under <b>adjustment.</b> Rather the objective should be trying to change labour market institutions in order {{to improve the quality of}} the jobs and the quality of the labour relations. ...|$|R
40|$|The Supplementary Green Book Guidance on Optimism Bias (HM Treasury 2003) with {{reference}} to the Review of Large Public Procurement in the UK (Mott MacDonald 2002) notes that there is a demonstrated, systematic, tendency for project appraisers to be overly optimistic and that to redress this tendency appraisers should make <b>explicit,</b> empirically based <b>adjustments</b> to the estimates of a project’s costs, benefits, and duration. HM Treasury recommends that these adjustments be based on data from past projects or similar projects elsewhere, and adjusted for the unique characteristics of the project in hand. In the absence of a more specific evidence base, HM Treasury has encouraged departments to collect data to inform future estimates of optimism, and in the meantime use the best available data. In response to this, the Department for Transport (henceforth DfT), has contracted Bent Flyvbjerg in association with COWI to undertake the consultancy assignment "Procedures for dealing with Optimism Bias in Transport Planning". The present Guidance Document is the result of this assignment...|$|R
40|$|This study {{examined}} adaptive changes of eye-hand coordination during a visuomotor rotation task. Young adults made aiming movements to targets on a horizontal plane, while {{looking at the}} rotated feedback (cursor) of hand movements on a monitor. To vary the task difficulty, three rotation angles (30 °, 75 °, and 150 °) were tested in three groups. All groups shortened hand movement time and trajectory length with practice. However, control strategies used were different among groups. The 30 ° group used proportionately more implicit adjustments of hand movements than other groups. The 75 ° group used more on-line feedback control, whereas the 150 ° group used <b>explicit</b> strategic <b>adjustments.</b> Regarding eye-hand coordination, timing of gaze shift to the target was gradually changed with practice from the late to early phase of hand movements in all groups, indicating an emerging gaze-anchoring behavior. Gaze locations prior to the gaze anchoring were also modified with practice from the cursor vicinity to an area between the starting position and the target. Reflecting various task difficulties, these changes occurred fastest in the 30 ° group, followed by the 75 ° group. The 150 ° group persisted in gazing at the cursor vicinity. These {{results suggest that the}} function of gaze control during visuomotor adaptation changes from a reactive control for exploring the relation between cursor and hand movements to a predictive control for guiding the hand to the task goal. That gaze-anchoring behavior emerged in all groups despite various control strategies indicates a generality of this adaptive pattern for eye-hand coordination in goal-directed actions...|$|R
40|$|We {{assessed}} electrophysiological activity {{over the}} medial frontal cortex (MFC) during outcome-based behavioral adjustment using a probabilistic reversal learning task. During recording, participants were presented two abstract visual patterns on each trial {{and had to}} select the stimulus rewarded on 80 % of trials and to avoid the stimulus rewarded on 20 % of trials. These contingencies were reversed frequently during the experiment. Previous EEG work has revealed feedback-locked electrophysiological responses over the MFC (feedback-related negativity; FRN), which correlate with the negative prediction error [Holroyd, C. B., & Coles, M. G. The neural basis of human error processing: Reinforcement learning, dopamine, and the error-related negativity. Psychological Review, 109, 679 - 709, 2002] and which predict outcome-based adjustment of decision values [Cohen, M. X., & Ranganath, C. Reinforcement learning signals predict future decisions. Journal of Neuroscience, 27, 371 - 378, 2007]. Unlike previous paradigms, our paradigm enabled us to disentangle, on the one hand, mechanisms related to the reward prediction error, derived from reinforcement learning (RL) modeling, {{and on the other}} hand, mechanisms related to <b>explicit</b> rule-based <b>adjustment</b> of actual behavior. Our results demonstrate greater FRN amplitudes with greater RL model-derived prediction errors. Conversely expected negative outcomes that preceded rule-based behavioral reversal were not accompanied by an FRN. This pattern contrasted remarkably with that of the P 3 amplitude, which was significantly greater for expected negative outcomes that preceded rule-based behavioral reversal than for unexpected negative outcomes that did not precede behavioral reversal. These data suggest that the FRN reflects prediction error and associated RL-based adjustment of decision values, whereas the P 3 reflects adjustment of behavior on the basis of explicit rules...|$|R
40|$|During {{system and}} network {{overload}} periods, excessive delay or even data loss may occur. To maintain {{the quality of}} control of an NCS, the implementation system (including both computer and network) overload must be correctly handled. In this chapter, {{as an alternative to}} the <b>explicit</b> sampling period <b>adjustment,</b> we present an indirect sampling period adjustment approach which is based on selective sampling data dropping according to the (m, k) -firm model. The interest of this alternative is its easy implementation despite having less adjustment quality, since only the multiples of the basic sampling period can be exploited. Upon overload detection, the basic idea is to selectively drop some samples according to the (m, k) -firm model to avoid long consecutive data drops. The consequence is that the shared network and processor will be less loaded. However, the control stability and performance must still be maintained to an acceptable level. This can be achieved by keeping either the total control tasks on a same processor or the messages sharing a same network bandwidth schedulable under the (m, k) -firm constraint...|$|R
