155|10000|Public
50|$|The Java UI and PHP <b>image</b> <b>processing</b> <b>pipeline</b> produce {{books for}} www.archive.org and other digital libraries.|$|E
50|$|Windows Imaging Component {{provides}} an extensible architecture for image codecs, pixel formats, and metadata, with automatic run-time discovery of new formats. It supports {{reading and writing}} of arbitrary metadata in image files, {{with the ability to}} preserve unrecognized metadata during editing. While working with images, it preserves high bit depth image data, up to 32 bits per channel, throughout the revamped high dynamic range <b>image</b> <b>processing</b> <b>pipeline</b> built into Windows Vista.|$|E
3000|$|A novel, human-guided <b>image</b> <b>processing</b> <b>pipeline</b> {{to extract}} and track insects in outdoor field environments {{with high levels}} of noise [...]...|$|E
30|$|Further, the test-retest {{difference}} {{has been}} assessed and produced {{an average of}} approximately of 2 % or less between test and retest Centiloid values on each <b>pipeline.</b> The different <b>image</b> <b>processing</b> <b>pipelines</b> did not produce significantly different results for the test-retest data, {{with an average of}} ≤[*] 2 % difference between these pipelines: SPM 8, PMOD and FSL.|$|R
40|$|International audienceIn this paper, {{we present}} a new approach, called SATIS (Semantically AnnotaTed Intentions for Services), relying on {{semantic}} web technologies and models, to assist collaboration {{among the members of}} a neurosciences community. The main expected result of this innovative work is to derive and share semantic web service specifications from neuro-scientists requirements in order to perform <b>image</b> <b>processing</b> <b>pipelines...</b>|$|R
3000|$|... [18 F]flutemetamol {{data can}} now be {{expressed}} in Centiloid units, enhancing its utility in clinical and research applications for β-amyloid imaging. The standard Centiloid method also demonstrates that [18 F]flutemetamol has favourable performance compared with PiB and other β-amyloid tracers. Test-retest difference averaged 2 %, with no difference between <b>image</b> <b>processing</b> <b>pipelines.</b> Centiloid scaling is robust and can be implemented {{on a number of}} platforms.|$|R
40|$|Many {{creative}} ideas are being proposed for image sensor designs, and {{these may be}} useful in applications ranging from consumer photography to computer vision. To understand and evaluate each new design, we must create a corresponding <b>image</b> <b>processing</b> <b>pipeline</b> that transforms the sensor data into a form that is appropriate for the application. The need to design and optimize these pipelines is time-consuming and costly. We explain a method that combines machine learning and image systems simulation that automates the pipeline design. The approach is based on a new way of thinking of the <b>image</b> <b>processing</b> <b>pipeline</b> as a large collection of local linear filters. We illustrate how the method has been used to design pipelines for novel sensor architectures in consumer photography applications...|$|E
40|$|We apply Uppaal Tiga to {{automatically}} compute adaptive scheduling strategies for an industrial case study {{dealing with a}} state-of-the-art <b>image</b> <b>processing</b> <b>pipeline</b> of a printer. As far as we know, {{this is the first}} application of timed automata technology to an industrial scheduling problem with uncertainty in job arrivals. ...|$|E
40|$|The {{implementation}} of a dual <b>image</b> <b>processing</b> <b>pipeline</b> camera on a state-of-art FPGA system is described. This camera can simultaneously acquire dual images of a scene and process, analyze and merge both images to enable a range of real-time image enhancement algorithms to be explored. Several examples of image enhancement algorithms implemented on the prototype camera are described...|$|E
40|$|Abstract. We {{summarise}} {{the properties}} of the Sloan Digital Sky Sur-vey (SDSS) project, discuss our software infrastructure, and outline the architecture of the SDSS <b>image</b> <b>processing</b> <b>pipelines.</b> We then discuss two of the algorithms used in the SDSS <b>image</b> <b>processing,</b> the KL-transform based modelling of the spatial variation of the PSF, and the use of galaxy models in star/galaxy separation. We conclude with the first author’s personal opinions on the challenges that the astronomical community faces with major software projects...|$|R
40|$|Although many <b>image</b> <b>processing</b> <b>pipelines</b> were {{proposed}} tointegrate colour <b>image</b> <b>processing</b> for {{the digital}} camera. Manyof them reduces {{the sharpness of}} image while filtering thatresult into the blurred image therefore {{it is necessary to}} takesome action that will not allows noise-level as a by product. Inthis paper we are reviewing colour edge detection, bilateralnoise filter edge and colour enhancement based on suitablecolour space. We then observe different scene of picture andresult shows that these approaches can effectively reducesnoises while preserving and enhancing edges...|$|R
30|$|Statistically {{inspired}} {{approaches have}} been developed {{in an attempt to}} overcome the limitations of traditional <b>image</b> <b>processing</b> <b>pipelines.</b> Khan et al. developed an effective particle tracking system using Markov chain Monte Carlo [12]. Their method is capable of tracking interacting agents demonstrating good results when used to track ants in the lab. Kimura et al. described a novel technique based on vector quantization to track large numbers of densely packed honeybees in hives [13].|$|R
40|$|A major hurdle facing data {{intensive}} grid applications is {{the appropriate}} handling of failures {{that occur in the}} grid-environment. Implementing the fault-tolerance transparently at the grid-middleware level would make different data intensive applications fault-tolerant without each having to pay a separate cost and reduce the time to grid-based solution for many scientific problems. We analyzed the failures encountered by four real-life production data intensive applications: NCSA <b>image</b> <b>processing</b> <b>pipeline,</b> WCER video processing pipeline, US-CMS pipeline and BMRB BLAST pipeline. Taking the result of the analysis into account, we have designed and implemented Phoenix, a transparent middleware-level fault-tolerance layer that detects failures early, classifies failures into transient and permanent and appropriately handles the transient failures. We applied our fault-tolerance layer to a prototype of the NCSA <b>image</b> <b>processing</b> <b>pipeline</b> and considerably improved the failure handling and report on the insights gained in the process. 1...|$|E
30|$|In {{order to}} render images {{captured}} {{with a single}} chip image sensor as a viewable image, an <b>image</b> <b>processing</b> <b>pipeline</b> is required. The most important parts of this <b>image</b> <b>processing</b> <b>pipeline</b> are demosaicing and the automatic white balance (AWB). Since only one color component is available at each pixel, the other two missing color components have to be estimated from the neighboring pixels. This process {{is referred to as}} CFA demosaicing or CFA interpolation. The color constancy property of the human visual system allows the perceived color to remain relatively constant at different color temperatures [2]. This capability is required for cameras to generate natural-looking images that match with human perception. The goal of the AWB method is to emulate human color constancy. This is normally achieved by adjusting the image so that it looks as if it were taken under a canonical light (usually daylight).|$|E
40|$|Abstract—We {{experiment}} with interactive machine learning for mouse behavior classification, following the pioneering work JAABA [1]. Here, we describe a simple <b>image</b> <b>processing</b> <b>pipeline</b> that allows extracting individual body parts from single mouse top view video. Our experiments show that behavior classification accuracy increases substantially when transitioning from whole-body descriptors to features computed from individual body parts, their position and motion. I...|$|E
40|$|An âedge-basedâ <b>image</b> <b>processing</b> {{architecture}} for {{the internet}} of things (IoT) is devised, drawing on the existing <b>image</b> <b>processing</b> <b>pipelines</b> that are implemented in todayâs imaging modules in digital cameras and smartphones. A key element of this IoT framework is that image or video data {{does not have to be}} streamed across the network, but can be largely processed on the sensing nodes with metadata transmitted to intermediate control nodes that can detect certain events/conditions and trigger corresponding actions. SFI Strategic PartnershipProgram by Science Foundation Ireland (SFI) and FotoNation Ltd. Project ID: 13 /SPP/I 286...|$|R
40|$|We {{summarise}} {{the properties}} of the Sloan Digital Sky Survey (SDSS) project, discuss our software infrastructure, and outline the architecture of the SDSS <b>image</b> <b>processing</b> <b>pipelines.</b> We then discuss two of the algorithms used in the SDSS image processing; the KL-transform based modelling of the spatial variation of the PSF, and the use of galaxy models in star/galaxy separation. We conclude with the first author’s personal opinions on the challenges that the astronomical community faces with major software projects...|$|R
40|$|The IJBlob {{library is}} a free ImageJ library for {{connected}} component analysis. Furthermore, it implements several contour based shape features to describe, filter or classify binary objects in images. Other features are extensible by the IJBlob extension framework. Because connected component labeling is a fundamental operation in many <b>image</b> <b>processing</b> <b>pipelines</b> (e. g. pattern recognition), the library could be useful for many ImageJ projects. The library is written in Java and the recent release is available at [URL]...|$|R
40|$|The {{efficient}} {{repair of}} cellular DNA {{is essential for}} the maintenance and inheritance of genomic information. In order to cope with the high frequency of spontaneous and induced DNA damage, a multitude of repair mechanisms have evolved. These are enabled by a wide range of protein factors specifically recognizing different types of lesions and finally restoring the normal DNA sequence. This work focuses on the repair factor XPC (xeroderma pigmentosum complementation group C), which identifies bulky DNA lesions and initiates their removal via the nucleotide excision repair pathway. The binding of XPC to damaged DNA can be visualized in living cells by following the accumulation of a fluorescent XPC fusion at lesions induced by laser microirradiation in a fluorescence microscope. In this work, an automated <b>image</b> <b>processing</b> <b>pipeline</b> is presented which allows to identify and quantify the accumulation reaction without any user interaction. The <b>image</b> <b>processing</b> <b>pipeline</b> comprises a preprocessing stage where the image stack data is filtered and the nucleus of interest is segmented. Afterwards, the images are registered to each other in order to account for movements of the cell, and then a bounding box enclosing the XPC-specific signal is automatically determined. Finally, the time-dependent relocation of XPC is evaluated by analyzing the intensity change within this box. Comparison of the automated processing results with the manual evaluation yields qualitatively similar results. However, the automated analysis provides more accurate, reproducible data with smaller standard errors. The <b>image</b> <b>processing</b> <b>pipeline</b> presented in this work allows for an efficient analysis of large amounts of experimental data with no user interaction required...|$|E
40|$|Electronic design {{tools and}} {{techniques}} {{for the implementation}} of a stereoscopic camera based on an FPGA (Field Programmable Gate Array) are presented. The stages of an IPP (<b>Image</b> <b>Processing</b> <b>Pipeline)</b> are presented together with the development tools and languages used to implement a stereoscopic camera in hardware. In a further development of the basic system, aspects of the implementation of a 3 D camera are presented...|$|E
40|$|Abstract—The {{efficient}} {{repair of}} cellular DNA {{is essential for}} the maintenance and inheritance of genomic information. In order to cope with the high frequency of spontaneous and induced DNA damage, a multitude of repair mechanisms have evolved. These are enabled by a wide range of protein factors specifically recognizing different types of lesions and finally restoring the normal DNA sequence. This work focuses on the repair factor XPC (xeroderma pigmentosum complementation group C), which identifies bulky DNA lesions and initiates their removal via the nucleotide excision repair pathway. The binding of XPC to damaged DNA can be visualized in living cells by following the accumulation of a fluorescent XPC fusion at lesions induced by laser microirradiation in a fluorescence microscope. In this work, an automated <b>image</b> <b>processing</b> <b>pipeline</b> is presented which allows to identify and quantify the accumulation reaction without any user interaction. The <b>image</b> <b>processing</b> <b>pipeline</b> comprises a preprocessing stage where the image stack data is filtered and the nucleus of interest is segmented. After-wards, the images are registered to each other in order to account for movements of the cell, and then a bounding box enclosing the XPC-specific signal is automatically determined. Finally, the time-dependent relocation of XPC is evaluated by analyzing the intensity change within this box. Comparison of the automated processing results with the manual evaluation yields qualitatively similar results. However, the automated analysis provides more accurate, reproducible data with smaller standard errors. The <b>image</b> <b>processing</b> <b>pipeline</b> presented in this work allows for an efficient analysis of large amounts of experimental data with no user interaction required. Index Terms—automated intensity measurement; DNA repair; fluorescence microscopy; I...|$|E
40|$|Abstract — We have {{developed}} an automated feature detection/classification system, called Genie (GENetic Imagery Exploitation), which {{has been designed to}} generate <b>image</b> <b>processing</b> <b>pipelines</b> for a variety of feature detection/classification tasks. Genie is a hybrid evolutionary algorithm that addresses the general problem of finding features of interest in multi-spectral remotely-sensed images. We describe our system in detail together with experiments involving comparisons of Genie with several conventional supervised classification techniques, for a number of classification tasks using multi-spectral remotely-sensed imagery...|$|R
40|$|Sipna {{college of}} engg. & tech. Amravati,India. Although many <b>image</b> <b>processing</b> <b>pipelines</b> were {{proposed}} to integrate colour <b>image</b> <b>processing</b> for the digital camera. Many of them reduces {{the sharpness of}} image while filtering that result into the blurred image therefore {{it is necessary to}} take some action that will not allows noise-level as a by product. In this paper we are reviewing colour edge detection, bilateral noise filter edge and colour enhancement based on suitable colour space. We then observe different scene of picture and result shows that these approaches can effectively reduces noises while preserving and enhancing edges...|$|R
40|$|For decades, {{researchers}} have been developing algorithms for <b>image</b> <b>processing</b> <b>pipelines.</b> <b>Image</b> <b>Processing</b> <b>Pipelines</b> (IPPs) are algorithmic constructions built to iteratively modify an input image {{into a series of}} abstractions for the purposes of decoding its contents into a higher level representation. There have been many proposed IPPs, varying in both physical construction, and in algorithmic paradigm, but by and large these propositions have been based in Boolean computation and arithmetic. Studies and trends have shown that Boolean computers are hitting a theoretical ceiling on their performance in terms of transistor size, energy consumption/heat dissipation, clock rates, and by extension computational time. Due to these issues, {{researchers have}} proposed using non-Boolean approaches, where possible, for various computations in common algorithms. One of the emerging technologies in the field of non-Boolean computation has been the use of coupled oscillators. A proposed use of coupled oscillators is for pattern matching, which can also be interpreted as a high-dimensional distance measurement. Using an approach based on the use of coupled oscillators as a basic computational primitive, this work aims to utilize the benefits gained from this new computational paradigm to gain performance in terms of both speed and power with respect to IPPs, without decreasing the accuracy of their algorithms...|$|R
40|$|We apply three {{different}} modeling frameworks — timed automata (Uppaal), colored Petri nets and synchronous data flow — to model a challenging industrial case study that involves an existing stateof- the-art <b>image</b> <b>processing</b> <b>pipeline.</b> Each {{of the resulting}} models is used to derive schedules for multiple concurrent jobs {{in the presence of}} limited resources (processing units, memory, USB bandwidth, [...] ). The three models and corresponding analysis results are compared...|$|E
40|$|Digital Still Color Cameras {{have gained}} {{significant}} popularity in recent years, with projected {{sales in the}} order of 44 million units by the year 2005. Such an explosive demand calls for an understanding of the processing involved and the implementation issues, bearing in mind the otherwise difficult problems these cameras solve. This article presents an overview of the <b>image</b> <b>processing</b> <b>pipeline,</b> first from a signal processing perspective and later from an implementation perspective, along with the trade-offs involved...|$|E
40|$|This paper {{describes}} the <b>image</b> <b>processing</b> <b>pipeline</b> {{used to enhance}} images of text captured by a hand-held low-resolution camera, and a fast text extraction method. The main advantages of the approach are its inherent lightweight structure, speed and relative robustness under poor lighting and focus conditions. The computational efficiency (and a careful implementation) of the approach has allowed its deployment in an interactive-time text capture and foreign-text translation demo on a PDA with a VGA camera attachmen...|$|E
3000|$|... [18 F]flutemetamol {{data can}} now be {{expressed}} in Centiloid units, enhancing its utility in both clinical and research applications for β-amyloid imaging. Standardised quantification can provide supplementary information to compliment visual assessment, especially in equivocal cases, and also provide a means to assess patients longitudinally. The standard Centiloid method also demonstrates that [18 F]flutemetamol has favourable performance compared with PiB and other β-amyloid tracers. Test-retest difference averaged 2 %, with no difference between <b>image</b> <b>processing</b> <b>pipelines.</b> Centiloid scaling is robust and can be implemented {{on a number of}} platforms.|$|R
30|$|The {{second class}} of systems that is aiming at the correct display of HDR scenes in {{standard}} dynamic-range <b>image</b> <b>processing</b> <b>pipelines</b> is content-based metering. In this approach, {{the objective is}} to distinguish relevant and/or meaningful metering parts in the image. The basic problem of the conventional metering systems is that large background areas of high luminance are spoiling the average luminance measurement, resulting in an underexposed foreground. The dynamic-weighting metering schemes can partially improve this drawback. However, a possible and more powerful approach would be to apply intelligent processing in the camera to better distinguish the important image parts.|$|R
40|$|Although the {{processing}} of data streams {{has been the focus}} of many research efforts in several areas, the case of remotely sensed streams in scientific contexts has received less attention. We present an extensible architecture to compose streaming <b>image</b> <b>processing</b> <b>pipelines</b> spanning multiple nodes on a network using a scientific workflow approach. This architecture includes (i) a mechanism for stream query dispatching so new streams can be dynamically generated from within individual processing nodes as a result of local or remote requests, and (ii) a mechanism for making the resulting streams externally available. As complete <b>processing</b> <b>image</b> <b>pipelines</b> can be cascaded across multiple interconnected nodes in a dynamic, scientist-driven way, the approach facilitates the reuse of data and the scalability of computations. We demonstrate the advantages of our infrastructure with a toolset of stream operators acting on remotely sensed data streams for realtime change detection...|$|R
40|$|An {{implementation}} of a real-time 3 D video acquisition system on a single FPGA is presented. Our approach {{is based on the}} use of stereo image sensors and can display real-time 3 D video when combined with shutter glasses. The 3 D effect is generated based on the parallax of the stereo sensors. An overview of the system architecture is given, including some details of the FPGA {{implementation of}} actual <b>image</b> <b>processing</b> <b>pipeline.</b> This architecture provides the basis for a low-cost, personal 3 D imaging appliance...|$|E
40|$|Biological {{research}} is increasingly dependent on analyzing {{vast amounts of}} microscopy datasets. Technologies such as Fiji/ImageJ 2 and KNIME support knowledge extraction from biological data by providing a large set of configurable algorithms and an intuitive pipeline creation and execution interface. The increasing complexity of required analysis pipelines and the growing amounts of data to be processed nurture the desire to run existing pipelines on HPC (High Performance Computing) systems. Here, we propose {{a solution to this}} challenge by presenting a new HPC integration method for KNIME (Konstanz Information Miner) using the UNICORE middleware (Uniform Interface to Computing Resources) and its automated data processing feature. We designed the integration to be efficient in processing large data workloads on the server side. On the client side it is seamless and lightweight to only minimally increase the complexity for the users. We describe our novel approach and evaluate it using an <b>image</b> <b>processing</b> <b>pipeline</b> that could previously not be executed on an HPC system. The evaluation includes a performance study of the induced overhead of the submission process and of the integrated <b>image</b> <b>processing</b> <b>pipeline</b> based on a large amount of data. This demonstrates how our solution enables scientists to transparently benefit from vast HPC resources without the need to migrate existing algorithms and pipelines...|$|E
40|$|In this report, {{we present}} an {{original}} <b>image</b> <b>processing</b> <b>pipeline</b> {{that we have}} developed under the ImageJ software for a specific application {{in the domain of}} plant science. In this domain, numerical X-ray imaging is applied more and more often for its capabilities for noninvasive inspection of internal structures of interest, in order to assess, for instance, the germinative quality of dry seeds [1]. Here, we address the task of detecting {{the presence or absence of}} an embryo in dry seed of sugarbeet. Each X-ray image can contain from ten to one hundred seeds. Our <b>image</b> <b>processing</b> <b>pipeline</b> first performs a segmentation of each seed, and constructs the histogram of the gray levels of each segmented seed. It then calculates a relative-entropy based distance, or Kullback-Leibler distance [2] between the normalized histogram of each seed and the histogram of a seed of reference with embryo and a seed of reference without embryo. The smallest Kullback-Leibler distance is chosen to decide the content of the seed under test. We will present in the extended version of the report, the confrontation of the ImageJ pipeline for automated detection, with the detection performed from visual inspection by a human expert. The pipeline, fully automated, can contribute to high-throughput screening and phenotyping of large quantities of seeds...|$|E
40|$|Automated <b>image</b> <b>processing</b> methods enable objective, {{reproducible}} {{and high}} quality analysis of fluorescent cell images {{in a reasonable}} amount of time. Therefore, we propose the application of <b>image</b> <b>processing</b> <b>pipelines</b> based on established segmentation algorithms which can handle massive amounts of whole slide imaging data of multiple fluorescent labeled cells. After automated parameter adaption the segmentation pipelines provide high quality cell delineations revealing significant differences in the spreading of B cells: LPS-activated B cells spread significantly less on anti CD 19 mAb than on anti BCR mAb and both processes could be inhibited by the F-actin destabilizing drug Cytochalasin D. Moreover, anti CD 19 mAb induce a more symmetrical spreading than anti BCR mAb as reflected by the higher cell circularity...|$|R
40|$|Los Alamos National Laboratory has {{developed}} and demonstrated a highly capable system, GENIE, for the twoclass problem of detecting a single feature {{against a background}} of non-feature. In addition to the two-class case, however, a commonly encountered remote sensing task is the segmentation of multispectral image data into a larger number of distinct feature classes or land cover types. To this end we have extended our existing system to allow the simultaneous classification of multiple features/classes from multispectral data. The technique builds on previous work and its core continues to utilize a hybrid evolutionary-algorithm-based system capable of searching for <b>image</b> <b>processing</b> <b>pipelines</b> optimized for specific image feature extraction tasks...|$|R
40|$|<b>Image</b> <b>processing</b> {{applications}} require {{high performance}} software implementations {{in order to}} satisfy large input data and run on smaller mobile devices that require high efficiency. Halide is a language and compiler for optimizing <b>image</b> <b>processing</b> <b>pipelines.</b> Halide introduces a separation between algorithm, the logics behind the program, and a schedule, the order of execution. This thesis focuses on providing interactive GUI for visual analysis of Halide schedules. It creates a visualization of the order of execution and provides tools for analyzing three important aspects of <b>image</b> <b>processing</b> schedules: redundancy, locality and parallelism. Tool is designed for Halide programers who want to gain better understanding of scheduling in Halide and receive guidance for schedule optimizations. by Jovana Knezevic. Thesis: M. Eng., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2013. Cataloged from PDF version of thesis. Includes bibliographical references (page 43) ...|$|R
