16|25|Public
5000|$|... hypernym: {{sometimes}} spelled <b>hyperonym,</b> {{a generic}} word that {{stands for a}} class or group of equally ranked items, such as [...] "tree" [...] for [...] "beech" [...] or [...] "elm," [...] or [...] "house" [...] for [...] "chalet" [...] or [...] "bungalow." [...] A hypernym {{is said to be}} [...] "superordinate" [...] to a hyponym.|$|E
5000|$|... hyponym: an {{item that}} belongs to and is equally ranked in a generic class or group, for example [...] "lily" [...] or [...] "violet" [...] {{in the class of}} [...] "flowers"; or [...] "limousine" [...] or [...] "hatchback" [...] in the class of [...] "automobiles." [...] A hyponym is said to be [...] "subordinate" [...] to a <b>hyperonym.</b>|$|E
5000|$|In some words, the -onym form {{has been}} {{modified}} by replacing (or dropping) the [...] "o". In the examples ananym and metanym, the correct forms (anonym and metonym) were pre-occupied by other meanings. Other, late 20th century examples, such as hypernym and characternym, are typically incorrectly formed neologisms {{for which there}} are more traditional words formed in -onym (<b>hyperonym</b> and charactonym).|$|E
40|$|International audienceWe present {{new methods}} for pruning and {{enhancing}} item- sets for text classification via association rule mining. Pruning methods {{are based on}} dependency syntax and enhancing methods are based on replacing words by their <b>hyperonyms</b> of various orders. We discuss {{the impact of these}} methods, compared to pruning based on tfidf rank of words...|$|R
5000|$|According to Cloem, dictionaries, ontologies and {{proprietary}} claim-drafting algorithms {{are used}} to draft alternative claims based on a client's original set of claims. In particular, the original set of claims is subject to various permutations and linguistic manipulations [...] "by considering alternative definitions for terms as well as “synonyms, hyponyms, <b>hyperonyms,</b> meronyms, holonyms, and antonyms.”" ...|$|R
40|$|We present {{new methods}} for pruning and {{enhancing}} item- sets for text classification via association rule mining. Pruning methods {{are based on}} dependency syntax and enhancing methods are based on replacing words by their <b>hyperonyms</b> of various orders. We discuss {{the impact of these}} methods, compared to pruning based on tfidf rank of words. Comment: 16 pages, 2 figures, presented at DMNLP 201...|$|R
5000|$|In linguistics, a hyponym (from Greek hupó, [...] "under" [...] and ónoma, [...] "name") {{is a word}} {{or phrase}} whose {{semantic}} field is included within that of another word, its <b>hyperonym</b> or hypernym (from Greek hupér, [...] "over" [...] and ónoma, [...] "name"). In simpler terms, a hyponym shares a type-of relationship with its hypernym. For example, pigeon, crow, eagle and seagull are all hyponyms of bird (their hyperonym); which, in turn, is a hyponym of animal.|$|E
40|$|A {{special case}} of lexical {{contrast}} involves contrasting a <b>hyperonym</b> and a hyponym (as in clothes and socks), {{leading to the}} narrowing of the hyperonym’s sense. However, not all hyperonym/hyponym pairs are amenable to contrast (e. g. ?animals and cats). While category prototype structure forms a strong motivating and constraining factor for hyperonym/hyponym contrast (e. g. Lehrer 1990), what is lacking in previous work is a systematic consideration of the phenomenon in real language use. To that end, data from the GloWbE corpus (Davies 2013) was used to investigate which terms for items of clothing (e. g. coat, bra, jeans) can be contrasted with their <b>hyperonym</b> (either clothes or clothing). While marginal members of the ITEM OF CLOTHING category (e. g. belt, hat) have a stronger potential for contrasting with the <b>hyperonym,</b> even prototypical hyponyms (e. g. shirt, jeans) contrasted with clothes/clothing {{in at least some}} contexts. Language users can therefore manipulate category boundaries to meet their discourse needs, exploiting a range of dimensions of difference to create contrast. Many clothing terms were also found to contrast more readily with clothes than with clothing, suggesting that the meaning of clothes is generally narrower than that of its near-synonym clothing...|$|E
40|$|At {{the present}} time the {{distinction}} between compatible versus incompatible co-hyponyms and polysemy versus homonymy is not expressible in WordNet. We aim to enhance WordNet's expressiveness, through GL inheritance patterns. Hyponymy will then {{be reflected in the}} qualia structures shared by the hyponyms and the <b>hyperonym...</b>|$|E
40|$|In {{this paper}} we {{will focus on}} corpora as a {{resource}} for researching language processing for terminological purposes. Based on the TEI guide, we present the templates used to tag our TxtCeram corpus and its features when working with WordSmith, a text analysis tool. We present an experiment for studying the frequency of <b>hyperonyms</b> in the introduction section of texts, while testing WordSmith’s suitability to work with our tagged corpus. 1...|$|R
40|$|Abstract. The {{disambiguation}} of verbs {{is usually}} {{considered to be}} more difficult with respect to other part-of-speech categories. This is due both to the high polysemy of verbs compared with the other categories, and to the lack of lexical resources providing relations between verbs and nouns. One of such resources is WordNet, which provides plenty of information and relationships for nouns, whereas it is less comprehensive with respect to verbs. In this paper we focus on the disambiguation of verbs by means of Support Vector Machines and the use of WordNet-extracted features, based on the <b>hyperonyms</b> of context nouns. ...|$|R
40|$|The {{topic of}} this diploma thesis is an {{analysis}} of history of special language of architecture in Paul Decker´s ?Ausführliche Anleitung zur Civilbaukunst (Bd. 1) ?. This thesis is divided into nine basic parts. The first, second and third parts characterize the special language, baroque and Paul Decker´s life and work. The following parts concentrate on {{the analysis of the}} special language presenting in this Decker´s work, that means on morphology, word-formative processes, the role of metaphors and foreign words, polysemy and synonyms, hyponyms, <b>hyperonyms</b> and co-hyponyms as well as the relation between verbal and nonverbal language part...|$|R
40|$|A test of {{directional}} entailment {{properties of}} classical quantifiers {{defined by the}} theory of generalised quantifiers (Barwise and Cooper, 1981) is described. Participants had to solve a task which consisted of four kinds of inference. In the first one, the premise was of the type Q - hyponym - verb - blank predicate, where Q is a classical quantifier, (e. g., some cats are []), and the question was to indicate what, if anything, can be concluded by filling up the slots in [...] - <b>hyperonym</b> - verb - blank predicate (e. g., [...] animals are []). The second kind of inference was the same, except that the <b>hyperonym</b> was in the premise and the hyponym in the conclusion. The third and fourth kinds of inference differed from the first two by {{the position of the}} <b>hyperonym</b> (resp. hyponym) which occupied the place of the predicate (e. g., some [] are animals). It was observed that when the directional entailment holds people respond accorddingly in most cases and that when the entailment does not hold they correctly fail to produce it. These results provide elementary, but essential empirical support to this semantic approach to quantification, as well as a prerequisite for its application to the study of reasoning with quantifiers. The implications for the psychology of reasoning are discussed...|$|E
40|$|FinnWordNet is a WordNet for Finnish that {{conforms}} to the framework given in Fellbaum (1998) and Vossen (ed.) (1998). FinnWordNet is open source and currently contains 117, 000 synsets. A classic WordNet consists of synsets, or sets of partial synonyms whose shared meaning is described and exemplified by a gloss, a common part of speech and a <b>hyperonym.</b> Synsets in a WordNet are arranged in hierarchical partial orderings according to semantic relations like hyponymy/hyperonymy. Together the gloss, part of speech and <b>hyperonym</b> fix {{the meaning of a}} word and constrain the possible translations of a word in a given synset. The Finnish group has opted for translating Princeton WordNet 3. 0 synsets wholesale into Finnish by professional translators, because the translation process can be controlled with regard to quality, coverage, cost and speed of translation. The project was financed by FIN-CLARIN at the University of Helsinki. According to our preliminary evaluation, the translation process was diligent and the quality is {{on a par with the}} original Princeton WordNet...|$|E
40|$|U radu se na temelju prikupljenoga korpusa pleonazama daju različiti formalni, sintaktički i semantički kriteriji podjele pleonazama te se pleonazmi dijele na nepotrebne i potrebne. Pleonazmi se analiziraju s deskriptivnoga i normativnoga stajališta. The {{authors have}} {{gathered}} a large corpus of pleonasms from language manuals, journals, dictionaries, students’ papers and texts {{they have received}} for language editing. They were divided into different categories {{on the basis of}} syntactic (word, syntagm, sentence, text), semantic (<b>hyperonym</b> – hyponym, synonymy, anthonymy, inclusion) and formal criteria (number of words). They have been analyzed from a descriptive and normative point of view and the cases of necessary pleonasms (stylistic, phraseological, onomastic, required by grammar, required by situation or context etc.) have been discussed...|$|E
40|$|This {{paper is}} an introdnction to KASSYS, {{a system that}} has been {{designed}} to extract information from detining statements in natural language. Only peronymous detinitions are dealt with here, for which systematic processing has been devised and implemented in the initial version of the system. The paper describes how KASSYS builds a taxinomic hierarchy by extracting the <b>hyperonyms</b> from these definitions. It also explains {{the way in which the}} system can answer closed questions (yes/no), thus enabling the user to check very quickly that a definition has been assimilated correctly. The under- lying formalism is that of conceptual gral) hs, with which the reader is assumed to be lhmiliar...|$|R
40|$|Sense based query {{expansion}} never proved {{its effectiveness}} {{except for the}} so-called “open domain question answering ” task. The present work is still inconclusive at this regard, due to some experimental limitations, but we provide interesting evidence suggesting new guidelines for future research. Word sense disambiguation is in fact {{only one of the}} problems involved with sense based query expansion. The second is how to use sense information (and ontologies in general) to expand the query. We show that expanding with synonyms or <b>hyperonyms</b> has a limited effect on web information retrieval performance, while other types of semantic information derivable from an ontology are much more effective at improving search results. ...|$|R
40|$|This paper {{describes}} the WSD system developed for our participation to the SemEval- 1. It combines various methods {{by means of}} a fuzzy Borda voting. The fuzzy Borda votecounting scheme {{is one of the best}} known methods in the field of collective decision making. In our system the different disambiguation methods are considered as experts that give a preference ranking for the senses a word can be assigned. Then the preferences are evaluated using the fuzzy Borda scheme in order to select the best sense. The methods we considered are the sense frequency probability calculated over SemCor, the Conceptual Density calculated over both <b>hyperonyms</b> and meronyms hyerarchies in WordNet, the extended Lesk by Banerjee and Pedersen, and finally a method based on WordNet domains...|$|R
40|$|The {{method of}} {{organization}} of word mean-ings {{is a crucial}} issue with lexical databases. Our purpose in this research is to extract word hierarchies from corpora automatically. Our initial task to this end is to determine adjec-tive hyperonyms. In order to find adjective hyperonyms, we utilize abstract nouns. We constructed linguistic data by extracting se-mantic relations between abstract nouns and adjectives from corpus data and classifying abstract nouns based on adjective similarity using a self-organizing semantic map, which is a neural network model (Kohonen 1995). In this paper we describe how to hierarchi-cally organize abstract nouns (adjective hy-peronyms) in a semantic map mainly using CSM. We compare three hierarchical organi-zations of abstract nouns, according to CSM, frequency (Tf. CSM) and an alternative simi-larity measure based on coefficient overlap, to estimate <b>hyperonym</b> relations between words. 1...|$|E
40|$|The {{article focuses}} on cognitive-onomasiological {{analysis}} of Ukrainian denominative verbs that modifies isolation depending on the word, which is a concept component, chosen by a motivator {{in the process of}} selection in the projection on semantic and onomasiological structure of a derivative, such a type of motivation as proposition-dictum. The semantics of derivative verbs depends on dictum position of a motivator. Each argument in the concept structure is associated with its own equonyms, notions of the same level f generalization with joint <b>hyperonym</b> as a sign of a kind. The formation of Ukrainian denominative verbs produces mostly equonimic pairs of meditative, transgressive, finite, partonym, object, etc. Equonyms are used as motivators {{because of the lack of}} sign means of notation considering the creative factor of replenishing language resources. The goal of the research is to substantiate proposition-dictum motivation, to find and describe cognitive-onomasiological features of equonymic type on the material of the Ukrainian denominative verbs. Conclusions and research prospects. Obviously, the most common type of motivation among Ukrainian denominative verbs is proposition-dictum based on the choice of motivation basis of a verb from the fragment of true objective knowledge about the denoted. Each of the arguments in the structure of the concept is associated with its own equonyms being the notions of one level of generalization with the common <b>hyperonym</b> as a type feature. Equonymic (hyponymic) motivation is peculiar to the derivatives, which contain the components “mostly”, “particularly”, “etc. ” in dictionary explanations since, along with main features having great distinctive force, minor secondary feature can be selected that are associated with the primary on the basis of classification type-type relations. The formation of Ukrainian denominative verbs considers mostly equonymic pairs of meditative, transgressive, finite, partonym, object, etc. Equonyms are used as motivators, in our opinion, due to the lack of sign means of denotation considering the creative factor of language resources replenishment. We consider the prospect of further investigation concerning proposition-dictum motivation in the description of modus and pseudomotivation...|$|E
40|$|FinnWordNet is a Finnish wordnet which {{complies}} with {{the structure of the}} Princeton WordNet. It was created by translating all the words in Princeton WordNet. It is open source and contains over 117 000 synsets. We are now testing different methods in order to improve and expand the content of FinnWordNet. Since wordnets are structured ontologies, a location for a word in FinnWordNet can bepinpointed by its relations to other words. To us, finding a location for a word therefore means finding a <b>hyperonym,</b> a hyponym or a synonym for the word. This article describes some methods for finding a location for a new word in FinnWordNet. Our methods include searching for multiword terms, compounds and lexico-syntactic patterns. Testing shows that with a few simple methods, we were able to find an indicator of the location for 83. 2 % of new words. Out of the new synonym pairs we tested, we were able to find an indication for 86. 7 %...|$|E
40|$|This paper {{addresses}} {{the problem of}} classifying web documents using domain ontology. Our goal {{is to provide a}} method for improving the classification of medical documents by exploiting the MeSH thesaurus (Medical Subject Headings) which will allow us to generate a new representation based on concepts. This approach was tested with two well-known data mining algorithms C 4. 5 and KNN, and a comparison was made with the usual representation using stems. The enrichment of vectors using the concepts and the <b>hyperonyms</b> drawn from the domain ontology has significantly boosted their representation, something essential for good classification. The results of our experiments on the benchmark biomedical collection Ohsumed confirm the importance of the approach by a very significant improvement in the performance of the ontology-based classification compared to the classical representation (Stems) by 30 %...|$|R
40|$|The aim of {{this paper}} is twofold. Firstly, it aims at {{defining}} the notion of lexical layer as applied to the derivational morphology of Old English. Secondly, this paper addresses the question of how lexical layers are accounted for by Old English dictionaries, including An Anglo-Saxon Dictionary and Supplement (Bosworth and Toller 1973), A Concise Anglo-Saxon Dictionary (Clark Hall 1996), and The Dictionary of Old English (Healey 2003). The data for the analysis have been retrieved from the lexical database of Old English Nerthus ([URL] The conclusion is reached that empty morphs draw a dividing line between lexical layers, while dictionaries associate the lexical layer of affixless derivation with <b>hyperonyms</b> and the one of affixal derivation with more specific meanings, throughout a process of progressive meaning specialization. This conference paper is not available in ORA...|$|R
40|$|Numbers, Translation and Semiosis": This short {{paper is}} devoted to the problem of the {{translation}} of the numbers, that is a field in which it is expexted that an exact translation is always straightforward. As soon as they are enriched by different systems of conventions and of <b>hyperonyms</b> translation may become uncertain. A real case was given by 30 miles, that can be translated approximatively (but realistically) into 50 chilometri, 48 chilometri (more exact than the original) or 30 miglia, where a foreign unit is left in use. Even in the interior of the very decimal system there are differences between hecto[gram] (Italian), 100 grams (English), 10 deka[grams] or better the almost equivalent Half-quarter [of Kilo](German). Other examples taken from the technical vocabulary show some interesting divergences that arise in different language systems as soon as we consider the perceptual clusters...|$|R
40|$|In {{specific}} culture, {{there are}} words referring to certain culture terminologies {{which do not}} easily {{come up with the}} equivalent meanings in other languages outside the local culture called realia. This research was conducted for the purpose of analyzing types of realia found in Truly, Madly novel and the strategies used in translating realia into Bahasa Indonesia. In addition, the researcher aimed at gathering information about the most dominant strategy in translating realia based on related theory. Exploratory sequential design was employed within this research. The data were taken from novel entitled Truly, Madly written by Heather Webber and its translation version in Bahasa Indonesia. The data were in the forms of dialogues, paragraphs or quotations of the story. In the analysis, the document analysis was used because this research obtained the data from written materials. From the analysis, some findings were gained. The result of the analysis showed that there were six types of realia found in Truly, Madly novel. They covered (1) historical realia, (2) geographical realia, (3) private-institutional realia, (4) public-institutional realia, (5) units, and (6) socio-cultural realia. The number of realia obtained by the researcher was 356 realia. Moreover, the research findings showed that eight strategies were used to translate realia. They were maintenance strategy, calque/loan translation, approximation, description, generalization (<b>hyperonym),</b> adaptation, omission, and combination strategy. From 356 realia, there were 144 realia (40. 45...|$|E
40|$|The present thesis derives {{information}} from three main sources which helped us analyse how native speakers of four European languages perceive, understand and evaluate youth. We used dictionaries, corpuses and a questionnaire research {{to study the}} meanings, metaphors, colours, images and emotions speakers connect with youth. According to the dictionaries, the prototypical bearers of youth are creatures and plants. The dictionaries of synonyms prove that {{the meaning of the}} word youth partially overlaps with {{the meaning of the word}} childhood, and thus, depending on the context, we can understand youth either as a <b>hyperonym</b> of childhood, or as its synonym. In all the four languages a prototypical young person has not come of age and there is a secondary meaning of the word youth as a state of the body and mind creating an illusion of a younger age. Czech phraseology shows that we speak about human youth using metaphors of birds' youth and unripe plants which gives our image of youth green colour. Czech proverbs characterise youth as a period of joy, growth, health, energy, mobility and learning on the one hand, and inexperience and unwiseness on the other hand. Some proverbs (a young idler, an old beggar or who so learnt young forgets not when he is old) exist in many languages, express general human wisdom [...] ...|$|E
40|$|The present paper {{deals with}} culture-specific items as a {{translational}} problem. It {{is based on}} two Polish translations of four Swedish books from the famous detective series by Maj Sjöwall and Per Wahlöö. The novels chosen for the study provide a set of cultural references that are familiar to Swedish, but not to Polish readers, due to which the translators are required to adopt certain translation strategies and methods to overcome language and cultural barriers. The aim {{of the study is}} to analyze some of the practices employed by two Polish translators against the translation methods proposed by Newmark (1988), Svane (2002) and Ingo (2009). For this purpose a number of highly specific cultural items have been collected and the methods applied by translators have been identified. The analysis has shown a big difference between translators in dealing with culture-specific items. The first translator, Maria Olszańska, adopts such translation methods as calque translation, <b>hyperonym,</b> functional equivalent, paraphrase and omission. The other translator, Halina Thylwe, prefers transference and calque translation combined with additional explanations, either in {{the main part of the}} text or in footnotes. The methods employed by both translators are a result of choosing between global translation strategies domestication and foreignization (Venuti 1995). Domestication, adopted in the older translations, minimizes the strangeness of the Swedish text to the Polish readers, whereas foreignization in the newer translations retains the foreignness of the original...|$|E
40|$|Jonas Brekle and Sebastian Hellmann contributed {{equally to}} this work. Abstract. We present a {{declarative}} approach implemented {{in a comprehensive}} open-source framework based on DBpedia to extract lexicalsemantic resources – an ontology about language use – from Wiktionary. The data currently includes language, part of speech, senses, definitions, synonyms, translations and taxonomies (hyponyms, <b>hyperonyms,</b> synonyms, antonyms) for each lexical word. Main focus is on flexibility to the loose schema and configurability towards differing language-editions of Wiktionary. This is achieved by a declarative mediator/wrapper approach. The goal is to allow the addition of languages just by configuration without the need of programming, thus enabling the swift and resource-conserving adaption of wrappers by domain experts. The extracted data is as fine granular as the source data in Wiktionary and additionally follows the lemon model. It enables use cases like disambiguation or machine translation. By offering a linked data service, we hope to extend DBpedia’s {{central role in the}} LOD infrastructure to the world of Open Linguistics. ...|$|R
40|$|Dipl. Inf. Sebastian HellmannWe {{present a}} {{declarative}} approach implemented {{in a comprehensive}} opensource framework (based on DBpedia) to extract lexical-semantic resources (an ontology about language use) from Wiktionary. The data currently includes language, part of speech, senses, definitions, synonyms, taxonomies (hyponyms, <b>hyperonyms,</b> synonyms, antonyms) and translations for each lexical word. Main focus is on flexibility to the loose schema and configurability towards differing language-editions of Wiktionary. This is achieved by a declarative mediator/wrapper approach. The goal is, to allow the addition of languages just by configuration without the need of programming, thus enabling the swift and resource-conserving adaptation of wrappers by domain experts. The extracted data is as fine granular as the source data in Wiktionary and additionally follows the lemon model. It enables use cases like disambiguation or machine translation. By offering a linked data service, we hope to extend DBpedia’s {{central role in the}} LOD infrastructure to the world of Open Linguistics...|$|R
40|$|Results of {{cognitive}} psychology research are analysed {{to explain why}} {{it is difficult for}} retrieval system users to bring to mind alternative search terms. A human memory model is modified {{in such a way that}} it produces additional search terms instead of human associations. A small experiment shows that such a spreading activation network can find alternative terms - with a performance similar to the normally used similarity measures. 1. Introduction A thesaurus in the field of Information and Documentation is an ordered compilation of concepts which serves for indexing and retrieval in one documentation domain. A central point is not only to define terms but also relations between terms. Such relations are synonymy (container - receptacle), broader terms or <b>hyperonyms</b> (container - tank), narrower terms or hyponyms (tank - container), the part-of relation (car - tank), antonymy (acceleration - deceleration) and compatibility (car - drive). We introduce the concept term association fo [...] ...|$|R
30|$|The {{third class}} of {{educational}} applications of question generation aims at assessing {{the knowledge of}} students. Heilman and Smith (2009) developed an approach to generating questions for assessing students’ acquisition of factual knowledge from reading materials. The authors developed general-purpose rules to transform declarative sentences into questions. The approach includes an algorithm to extract simplified statements from appositives, subordinate clauses, and other constructions in complex sentences of reading texts. Evaluation {{studies have been conducted}} to assess the quality and precision of automatically generated questions using Wikipedia and news articles. The authors reported that the acceptability of top-ranked WH questions is around 40 – 50  %. Furthermore, K- 12 teachers created factual questions by selecting and revising suggestions from the system with less effort than by writing questions on their own (Heilman 2011). One common form for assessing student’s factual knowledge is the use of multiple-choice tests. Mitkov and colleagues (Mitkov et al. 2006) developed a computer-aided environment for generating multiple-choice test items. The authors deployed various natural language processing techniques (shallow parsing, automatic term extraction, sentence transformation, and computing of semantic distance). In addition, the authors exploited WordNet, which provides language resources for generating distractors for multiple-choice questions. In addition to generating test items automatically, the system provides the user the option to post-process the test items. The authors reported that the time required for generating questions including manual correction was less than for manually creating questions alone (Mitkov et al. 2006). Also with the purpose of assessing students’ knowledge, Brown and colleagues (Brown et al. 2005) developed the system REAP which is intended to provide students with texts to read according to their individual reading levels. The system chooses text documents which include 95  % of words that are known to the student while the remaining 5  % of words are new to the student and need to be learned. After reading the text, the student’s understanding is assessed. The system generates different types of questions including word bank and multiple-choice questions. In contrast to Mitkov and colleagues who used WordNet to generate distractors, Brown et al. (2005) used WordNet to generate different types of questions (definition, synonym, antonym, <b>hyperonym,</b> hyponym, and cloze questions). Experimental results have been reported that with automatically generated questions, students achieved a measure of vocabulary skill that is comparable to performance on independently developed human-generated questions. Another form of assessing student’s knowledge is to rely on fill-in-the-blank questions. Hoshino and Nakagawa (2005) proposed to deploy standard classification methods to decide the position of the gap in a fill-in-the-blank item. Sumita et al. (2005) developed fill-in-the-blank questions by replacing verbs with gaps in an input sentence. Possible distractors are retrieved from a thesaurus by choosing the same Part of Speech (e.g., noun, verb, adjective) and similar word frequency in a tagged corpus. A new sentence is created by placing a distractor in the gap position in the original sentence and is then used as the input for a search on the Internet. If the sentence is found on the Internet, the distractor is considered invalid. Here, participants who took a test consisting of automatically generated items achieved scores that highly correlated with their scores in the Test of English for International Communication (TOEIC).|$|E
40|$|Abstract. This article {{describes}} the participation of a group from the University of Évora in the CLEF 2013 QA 4 MRE main task. Our system has a superficial text analysis based approach. The methodology starts with the preprocessing of background collection documents, whose texts are lemmatized and then indexed. Named entities and numerical ex-pressions are sought in questions and their candidate answers. Then the lemmatizer is applied and stop words are removed. Answer patterns are formed for each question+answer pair, with a search query for docu-ment retrieval. Original search terms are expanded with synonyms and <b>hyperonyms.</b> Finally, the texts retrieved for each candidate response are segmented and scored for answer selection. Considering only the main questions, the system best result was obtained in the third run, having answered to 206 questions, with 0. 24 c@ 1 and 51 correct answers. When evaluating main and auxiliary questions, the final run continued to have our better results, being answered 245 questions, with 64 right answers and 0. 26 for c@ 1. The use of hypernyms {{proved to be an}} improvement factor in the third run, which results had a 12 % increase of correct an-swers and a 0. 02 gain i...|$|R
40|$|Controlled-vocabulary based {{approach}} to automated subject classification of textual Web pages The {{purpose of this}} project is to explore the role of controlled vocabularies such as thesauri and classification schemes in automated subject classification of text. Apart from for improving classification performance, controlled vocabularies {{have been used in}} information retrieval systems to improve information retrieval, which is the application context of this project. The classification algorithm comprises string-to-string matching between words in the documents to be classified and words in term lists derived from the controlled vocabulary. The advantage of using this type of algorithm is that no training documents are required and, unlike in document clustering, an appropriate, good-quality controlled vocabulary can be chosen. The chosen test controlled vocabulary is Engineering Information (Ei) classification scheme, which has mappings to the corresponding Ei thesaurus. Intended end-users are engineering students and other subject experts. Evaluation would be performed at two main levels: comparison of automatically-against manually-assigned classes, and information retrieval relevance assessments. The project includes the following research questions: to what degree different types of terms in Ei thesaurus and classification scheme influence automated classification performance, enriched with their inflected, derivated, permuted forms as well as synonyms and <b>hyperonyms,</b> and comparison of this algorithm with an SVM and a clustering algorithm. ...|$|R
40|$|This article {{describes}} the participation of a group from the University of Évora in the CLEF 2013 QA 4 MRE main task. Our system has a superficial text analysis based approach. The methodology starts with the preprocessing of background collection documents, whose texts are lemmatized and then indexed. Named entities and numerical expressions are sought in questions and their candidate answers. Then the lemmatizer is applied and stop words are removed. Answer patterns are formed for each question+answer pair, with a search query for document retrieval. Original search terms are expanded with synonyms and <b>hyperonyms.</b> Finally, the texts retrieved for each candidate response are segmented and scored for answer selection. Considering only the main questions, the system best result was obtained in the third run, having answered to 206 questions, with 0. 24 c@ 1 and 51 correct answers. When evaluating main and auxiliary questions, the final run continued to have our better results, being answered 245 questions, with 64 right answers and 0. 26 for c@ 1. The use of hypernyms {{proved to be an}} improvement factor in the third run, which results had a 12 % increase of correct answers and a 0. 02 gain in c@ 1...|$|R
40|$|The paper {{deals with}} the {{structure}} of written scientific-technical text {{in the field of}} civil engineering in the context of English language teaching, in particular development of reading skills at tertiary level. Special emphasis is put on three main variables of textual structure: 1) cohesion; 2) texture; 3) coherence. These features of textual structure are closely interconnected. The paper explores the notion of cohesion. The focus is on lexical cohesion and means of its expression(e. g. repetition of lexical items, word and contextual synonyms, pronoun substitutions, <b>hyperonyms</b> and hyponyms). Cohesive devices are classified and exemplified. Various approaches to textual analysis are highlighted, in particular a transformational method of analysis. Transformations are analyzed on the level of type constructions and illustrated by a selection of examples based on the scientific-technical discourse corpus. Transformational analysis is aimed at revealing implicit predicates in order to extend a verbal-predicate base of the text. Benefits of using a linguistic framework and its practical application in the analysis of scientific-technical texts are described. Textual analysis can be a valuable tool for English language teaching/learning, in particular acquisition of reading skills at tertiary level. In addition, the study results can be used in delivering a course of lectures on text linguistics in higher education institutions...|$|R
40|$|The {{practice}} of computer-mediated communication {{is determined by}} its linguistic content and information format that preconditions syncretic study of computermediated discourse with due regard to its dual nature – linguistic and information. This approach allows consideration of phenomenological foundations of information-dependent speech activity and {{can be described as}} linguistic and informational. Modern communication environment is characterized by particular dynamics: system-forming elements and relationships are constantly developing, well-known phenomena and technical capabilities pass on, new ones – emerge. In this regard it is relevant to form a model of sphere, a kind of infrastructure that reflects the underlying system relationships and allows to complete and modify them according to their dynamics. Media and communicational infrastructure characterizes computer-mediated communication on a number of linguistic and informational criteria, acts as a model and forms the basis of categorical metalinguistic apparatus of the sphere with due regard to its functional specifics. Differentiation of communicational relationships using the oppositional potential of the dichotomy 'objectivity – subjectivity' allows to consruct the conceptual framework of computer-mediated communication into a single discursive system of coordinates, avoiding the interference of <b>hyperonyms</b> "environment" and "situation". The linguistic and informational format of computer-mediated communication suggests the importance of linguistic support of relevant practice, in particular, the identification and systematization of discursive regularities...|$|R
