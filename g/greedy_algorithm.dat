4191|2445|Public
5|$|The {{bottleneck}} of this <b>greedy</b> <b>algorithm</b> is the subproblem {{of finding}} which two clusters to merge in each step.|$|E
5|$|Known {{methods for}} {{repeatedly}} finding the closest pair of clusters in a dynamic set of clusters either require superlinear space {{to maintain a}} data structure that can find closest pairs quickly, or they take greater than linear time to find each closest pair. The nearest-neighbor chain algorithm uses a smaller {{amount of time and}} space than the <b>greedy</b> <b>algorithm</b> by merging pairs of clusters in a different order. In this way, it avoids the problem of repeatedly finding closest pairs. Nevertheless, for many types of clustering problem, it can be guaranteed {{to come up with the}} same hierarchical clustering as the <b>greedy</b> <b>algorithm</b> despite the different merge order.|$|E
5|$|It is {{possible}} to interpret the Sylvester sequence {{as the result of}} a <b>greedy</b> <b>algorithm</b> for Egyptian fractions, that at each step chooses the smallest possible denominator that makes the partial sum of the series be less than one. Alternatively, the terms of the sequence after the first can be viewed as the denominators of the odd greedy expansion of 1/2.|$|E
40|$|We study greedy-type {{algorithms}} {{such that}} at a greedy step we pick several dictionary elements contrary to a single dictionary element in standard greedy-type algorithms. We call such <b>greedy</b> <b>algorithms</b> super <b>greedy</b> <b>algorithms.</b> The idea of picking several elements at a greedy step of the algorithm is not new. Recently, we observed the following new phenomenon. For incoherent dictionaries these new type of <b>algorithms</b> (super <b>greedy</b> <b>algorithms)</b> provide the same (in the sense of order) upper bound for the error as their analogues from the standard <b>greedy</b> <b>algorithms.</b> The super <b>greedy</b> <b>algorithms</b> are computationally simpler than their analogues from the standard <b>greedy</b> <b>algorithms.</b> We continue to study this phenomenon...|$|R
40|$|International audienceIn {{this paper}} we present our study of <b>greedy</b> <b>algorithms</b> for solving the minimum sum {{coloring}} problem (MSCP). We propose two families of <b>greedy</b> <b>algorithms</b> for solving MSCP, and suggest {{improvements to the}} two <b>greedy</b> <b>algorithms</b> most often {{referred to in the}} literature for solving the graph coloring problem (GCP) : DSATUR and RLF...|$|R
40|$|Abstract: There {{has been}} little work on how to {{construct}} <b>greedy</b> <b>algorithms</b> to solve new optimization problems efficiently. Instead, <b>greedy</b> <b>algorithms</b> have generally been designed on an ad hoc basis. On the other hand, dynamic programming {{has a long history}} of being a useful tool for solving optimization problems, but is often inefficient. We show how dynamic programming can be used to derive efficient <b>greedy</b> <b>algorithms</b> that are optimal for a wide variety of problems. This approach also provides a way to obtain less efficient but optimal solutions to problems where derived <b>greedy</b> <b>algorithms</b> are nonoptimal...|$|R
5|$|Next, the dissimilarity {{function}} is extended from pairs of points to pairs of clusters. Different clustering methods perform this extension in different ways. For instance, in the single-linkage clustering method, {{the distance between}} two clusters is defined to be the minimum distance between any two points from each cluster. Given this distance between clusters, a hierarchical clustering may be defined by a <b>greedy</b> <b>algorithm</b> that initially places each point in its own single-point cluster and then repeatedly forms a new cluster by merging the closest pair of clusters.|$|E
25|$|The {{notion of}} matroid has been {{generalized}} {{to allow for}} other types of sets on which a <b>greedy</b> <b>algorithm</b> gives optimal solutions; see greedoid and matroid embedding for more information.|$|E
25|$|The maximum (worst) {{number of}} colors {{that can be}} {{obtained}} by the <b>greedy</b> <b>algorithm,</b> by using a vertex ordering chosen to maximize this number, is called the Grundy number of a graph.|$|E
40|$|Combinatorial {{approaches}} to testing {{are used in}} several fields, and have recently gained momentum {{in the field of}} software testing through software interaction testing. One-test-at-atime <b>greedy</b> <b>algorithms</b> are used to automatically construct such test suites. This paper discusses basic criteria of why <b>greedy</b> <b>algorithms</b> have been appropriate for this test generation problem in the past and then expands upon how <b>greedy</b> <b>algorithms</b> can be utilized to address test suite prioritization...|$|R
40|$|We empirically {{compare the}} local ratio {{algorithm}} for the profit maximization {{version of the}} dynamic storage allocation problem against various <b>greedy</b> <b>algorithms.</b> Our main conclusion is that, at least on our input distributions, the local ratio algorithms performed worse on average than the more naive <b>greedy</b> <b>algorithms...</b>|$|R
5000|$|It was {{not until}} 1971 that Jack Edmonds {{connected}} weighted matroids to <b>greedy</b> <b>algorithms</b> in his paper [...] "Matroids and the greedy algorithm". Korte and Lovász would generalize these ideas to objects called greedoids, which allow even larger classes of problems to be solved by <b>greedy</b> <b>algorithms.</b>|$|R
25|$|A {{straightforward}} distributed {{version of}} the <b>greedy</b> <b>algorithm</b> for (Δ+1)-coloring requires Θ(n) communication rounds in the worst case − information {{may need to be}} propagated {{from one side of the}} network to another side.|$|E
25|$|Dynamic {{programming}} algorithms {{are often}} used for optimization. A dynamic programming algorithm will examine the previously solved subproblems and will combine their solutions to give the best solution for the given problem. In comparison, a <b>greedy</b> <b>algorithm</b> treats the solution as some sequence of steps and picks the locally optimal choice at each step. Using a <b>greedy</b> <b>algorithm</b> does not guarantee an optimal solution, because picking locally optimal choices {{may result in a}} bad global solution, but it is often faster to calculate. Some greedy algorithms (such as Kruskal's or Prim's for minimum spanning trees) are however proven to lead to the optimal solution.|$|E
25|$|Every finite or countably {{infinite}} graph is an {{induced subgraph}} of the Rado graph, {{and can be}} found as an induced subgraph by a <b>greedy</b> <b>algorithm</b> that builds up the subgraph one vertex at a time. The Rado graph is uniquely defined, among countable graphs, by an extension property that guarantees the correctness of this algorithm: no matter which vertices have already been chosen to form part of the induced subgraph, and no matter what pattern of adjacencies is needed to extend the subgraph by one more vertex, there will always exist another vertex with that pattern of adjacencies that the <b>greedy</b> <b>algorithm</b> can choose.|$|E
5000|$|... #Subtitle level 3: Grammatical {{inference}} by <b>greedy</b> <b>algorithms</b> ...|$|R
30|$|Remark 5. Recently, <b>greedy</b> <b>algorithms</b> {{such as the}} {{matching}} pursuit and the orthogonal matching pursuit {{have been shown to}} approach the performance of (11) and (12) when the regression matrix is sufficiently block incoherent, or, when the block restricted isometry property holds [27]. In the problem at hand, wherein the regression matrix exhibits correlation among blocks, simulated tests have shown that <b>greedy</b> <b>algorithms</b> suffer from severe error propagation. In fact, a cloud of change points is typically declared around a true change point. For these reasons, <b>greedy</b> <b>algorithms</b> will not be considered hereafter.|$|R
3000|$|However, Donoho [11] {{indicated}} {{that the problem of}} the l 0 -norm minimization is NP-hard; an exhaustive search on the C_M^K [...] combinations of Y is necessary to the acquisition of a global optimal solution. Therefore, the algorithms for obtaining suboptimal solution are provided in succession, and these algorithms are divided into three kinds, namely, convex optimization algorithms, combination <b>algorithms,</b> and <b>greedy</b> <b>algorithms</b> [12]. Convex optimization algorithms have fairly high reconstruction accuracy and less measurement, but the complicated reconstruction process affects its practicability. Combination algorithms have shorter reconstruction time than convex optimization algorithms but need more measurements, which are hardly satisfied in practice. <b>Greedy</b> <b>algorithms</b> have low complexity and high reconstruction efficiency, although their reconstruction accuracy is inferior to the convex optimization <b>algorithm.</b> Nevertheless, <b>greedy</b> <b>algorithms</b> have better application prospect, and MP <b>algorithms</b> mostly represent <b>greedy</b> <b>algorithms.</b> Therefore, we investigated MP algorithms to increase reconstruction efficiency and reconstruction accuracy.|$|R
25|$|This {{optimization}} algorithm {{may be used}} to characterize matroids: if a family F of sets, closed under taking subsets, has the property that, no matter how the sets are weighted, the <b>greedy</b> <b>algorithm</b> finds a maximum-weight set in the family, then F must be the family of independent sets of a matroid.|$|E
25|$|A maximal {{matching}} can {{be found}} with a simple <b>greedy</b> <b>algorithm.</b> A maximum matching is also a maximal matching, and hence {{it is possible to}} find a largest maximal matching in polynomial time. However, no polynomial-time algorithm is known for finding a minimum maximal matching, that is, a maximal matching that contains the smallest possible number of edges.|$|E
25|$|The {{vertices}} of any graph may {{always be}} ordered {{in such a}} way that the <b>greedy</b> <b>algorithm</b> produces an optimal coloring. For, given any optimal coloring in which the smallest color set is maximal, the second color set is maximal with respect to the first color set, etc., one may order the vertices by their colors. Then when one uses a <b>greedy</b> <b>algorithm</b> with this order, the resulting coloring is automatically optimal. More strongly, perfectly orderable graphs (which include chordal graphs, comparability graphs, and distance-hereditary graphs) have an ordering that is optimal both for the graph itself and for all of its induced subgraphs. However, finding an optimal ordering for an arbitrary graph is NP-hard (because it could be used to solve the NP-complete graph coloring problem), and recognizing perfectly orderable graphs is also NP-complete. For this reason, heuristics have been used that attempt to reduce the number of colors while not guaranteeing an optimal number of colors.|$|E
40|$|International audienceThe event B method {{provides}} a general framework for modelling both data structures and algorithms. B models are validated by discharging proof obligations ensuring safety properties. We {{address the problem}} of development of <b>greedy</b> <b>algorithms</b> using the seminal work of S. Curtis; she has formalised <b>greedy</b> <b>algorithms</b> in a relational calculus and has provided a list of results ensuring optimality results. Our first contribution is a re-modelling of Curtis's results in the event B framework and a mechanical checking of theorems on <b>greedy</b> <b>algorithms</b> The second contribution is the reuse of the mathematical framework for developing <b>greedy</b> <b>algorithms</b> from event B models; since the resulting event B models are generic, we show how to instantiate generic event B models to derive specific greedy algorithms; generic event B developments help in managing proofs complexity. Consequently, we contribute to the design of a library of proof-based developed algorithms...|$|R
40|$|Abstract. In this paper, {{basis in}} SC-FDMA uplink systems is investiga <b>greedy</b> <b>algorithms</b> are {{proposed}} allocation at each transmission time interval. Selection criteria based on spectral efficiency and fairness are also proposed {{to choose the}} final allocatio transmission time interval. Simulation results show that {{when the number of}} users and the velocity of the users were varied, the improved algorithms that use selection criteria based on spectral efficiency and fairness could outperform the existing mean <b>greedy</b> <b>algorithms</b> that employ user spectral efficiency and fairness. Moreover, the improved algorithms not only showed better performance but also had the same time complexity as the existing mean <b>greedy</b> <b>algorithms...</b>|$|R
40|$|Recent {{developments}} {{in the use of}} <b>greedy</b> <b>algorithms</b> in linear programming are reviewed and extended. We find a common generalization of some theorems of Queyranne–Spieksma– Tardella, Faigle–Kern, and Fujishige about <b>greedy</b> <b>algorithms</b> for linear programs in diverse contexts. Additionally, we extend a well-known theorem of Topkis about submodular functions on the product of chains to submodular functions on the product of lattices...|$|R
25|$|The {{seemingly}} rational decisions {{during the}} game are in fact clearly irrational once one realizes that they {{are nothing more than}} a <b>greedy</b> <b>algorithm,</b> which is of course not guaranteed to give a globally optimal solution. In this case there is actually a globally optimal solution, which is to not play the game at all unless one is certain that there are no other players.|$|E
25|$|One {{can find}} a factor-2 {{approximation}} by repeatedly taking both endpoints of an edge into the vertex cover, then removing them from the graph. Put otherwise, we find a maximal matching M with a <b>greedy</b> <b>algorithm</b> and construct a vertex cover C that consists of all endpoints of the edges in M. In the following figure, a maximal matching M is marked with red, and the vertex cover C is marked with blue.|$|E
25|$|In {{the study}} of graph {{coloring}} problems in mathematics and computer science, a greedy coloring is a coloring of the vertices of a graph formed by a <b>greedy</b> <b>algorithm</b> that considers the vertices of the graph in sequence and assigns each vertex its first available color. Greedy colorings do not in general use the minimum number of colors possible. However, they {{have been used in}} mathematics as a technique for proving other results about colorings and in computer science as a heuristic to find colorings with few colors.|$|E
40|$|Quantum {{control is}} {{valuable}} for various quantum {{technologies such as}} high-fidelity gates for universal quantum computing, adaptive quantum-enhanced metrology, and ultra-cold atom manipulation. Although supervised machine learning and reinforcement learning are widely used for optimizing control parameters in classical systems, quantum control for parameter optimization is mainly pursued via gradient-based <b>greedy</b> <b>algorithms.</b> Although the quantum fitness landscape is often compatible for <b>greedy</b> <b>algorithms,</b> sometimes <b>greedy</b> <b>algorithms</b> yield poor results, especially for large-dimensional quantum systems. We employ differential evolution algorithms to circumvent the stagnation problem of non-convex optimization, and we average over the objective function to improve quantum control fidelity for noisy systems. To reduce computational cost, we introduce heuristics for early termination of runs and for adaptive selection of search subspaces. Our implementation is massively parallel and vectorized to reduce run time even further. We demonstrate our methods with two examples, namely quantum phase estimation and quantum gate design, for which we achieve superior fidelity and scalability than obtained using <b>greedy</b> <b>algorithms...</b>|$|R
30|$|In this section, we {{analyze the}} {{complexity}} of the proposed brute-force search and low-complexity <b>greedy</b> <b>algorithms.</b>|$|R
30|$|It {{has been}} shown that sparse methods such as IRL 1 and SBL can be used to {{estimate}} the lighting directions in the context of uncalibrated PST [19]. It is highly possible that <b>greedy</b> <b>algorithms</b> such as OMP can also be extended to be applied for such a purpose. Future studies may reveal more applications of <b>greedy</b> <b>algorithms</b> in different aspects of the PST framework.|$|R
25|$|The {{problem of}} maximizing the total area of three circles in a {{triangle}} is never solved by the Malfatti circles. Instead, the optimal solution {{can always be}} found by a <b>greedy</b> <b>algorithm</b> that finds the largest circle within the given triangle, the largest circle within the three connected subsets of the triangle outside of the first circle, and the largest circle within the five connected subsets of the triangle outside {{of the first two}} circles. Although it was first formulated in 1930, the correctness of this procedure was not proven until 1994.|$|E
25|$|The {{conjecture}} is wrong; , {{who went}} back to the original Italian text, observed that for some triangles a larger area can be achieved by a <b>greedy</b> <b>algorithm</b> that inscribes a single circle of maximal radius within the triangle, inscribes a second circle within one of the three remaining corners of the triangle, the one with the smallest angle, and inscribes a third circle within the largest of the five remaining pieces. The difference in area for an equilateral triangle is small, just over 1%, but as Howard Eves pointed out in 1946, for an isosceles triangle with a very sharp apex, the optimal circles (stacked one atop each other above the base of the triangle) have nearly twice the area of the Malfatti circles.|$|E
25|$|A <b>greedy</b> <b>algorithm</b> {{is similar}} to a dynamic {{programming}} algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.|$|E
30|$|The {{reconstruction}} by L 1 -minimization as in BP {{is stable}} but slow. <b>Greedy</b> <b>algorithms</b> increase the reconstruction speed by applying heuristic rules. In OMP [31], the heuristic rule is created {{based on the}} assumption that y has the large correlation to the bases corresponding to the non-zero elements (or the elements with large magnitude) of s. OMP selects the bases of the non-zero elements according to the correlation and estimates the values of the non-zero elements by the least squared method. The selection is iterated until the certain condition is reached. The reconstruction by <b>greedy</b> <b>algorithms</b> has a fast runtime, but lacks stability and uniform guarantee. RIP is not seriously considered in the <b>greedy</b> <b>algorithms</b> [12].|$|R
40|$|Solving an under-determined {{system of}} {{equations}} for the sparsest solution has attracted considerable attention in recent years. Among the two well known approaches, the <b>greedy</b> <b>algorithms</b> like matching pursuits (MP) are simpler to implement and can produce satisfactory results under certain conditions. In this paper, we compare several <b>greedy</b> <b>algorithms</b> {{in terms of the}} sparsity of the solution vector and the approximation accuracy. We present two new <b>greedy</b> <b>algorithms</b> based on the recently proposed complementary matching pursuit (CMP) and the sensing dictionary framework, and compare them with the classical MP, CMP, and the sensing dictionary approach. It is shown that in the noise-free case, the complementary matching pursuit algorithm performs the best among these algorithms. 1...|$|R
30|$|The {{augmentation}} phase: Creation of a K-dimensional initial subspace, {{using one}} of the locally optimal <b>greedy</b> <b>algorithms.</b>|$|R
