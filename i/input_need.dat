19|1833|Public
50|$|Computer CRT {{monitors}} {{usually have}} a black border (unless they are fine-tuned by a user to minimize it)—these {{can be seen in}} the video card timings, which have more lines than are used by the desktop. When a computer CRT is advertised as 17-inch (16-inch viewable), it will have a diagonal inch of the tube covered by the plastic cabinet; this black border will occupy this missing inch (or more) when its geometry calibrations are set to default (LCDs with analog <b>input</b> <b>need</b> to deliberately identify and ignore this part of the signal, from all four sides).|$|E
30|$|We {{surveyed}} {{few more}} works that are related, though {{may not be}} directly relevant, to our work. In these works, emotion detection {{plays an important role}} and various methods were proposed for the same. However, the <b>input</b> <b>need</b> not have been taken from the users directly, unlike the works discussed in the previous sections.|$|E
40|$|Assuming that {{votes are}} independent, the epistemically optimal {{procedure}} in a binary collective choice problem {{is known to}} be a weighted supermajority rule with weights given by personal log likelihood ratios. It is shown here that an analogous result holds in a much more general model. Firstly, the result follows from a more basic principle than expected-utility maximisation, namely from an axiom (“Epistemic Monotonicity”) which requires neither utilities nor prior probabilities of the ‘correctness’ of alternatives. Secondly, a person’s <b>input</b> <b>need</b> not be a vote for an alternative; it may be any type of input, for instance a subjective degree of belief or probability of the correctness of one of the alternatives. The case of a profile of subjective degrees of belief is particularly appealing, since no parameters such as competence parameters need to be known here...|$|E
3000|$|The <b>inputs</b> <b>needed</b> {{to perform}} the {{analysis}} of stakeholders and construct their profiles are the following: [...]...|$|R
5000|$|... all <b>inputs</b> <b>need</b> to be {{considered}} and a change {{to any one of}} them changes the answer.|$|R
5000|$|... #Caption: [...] Sustainability, {{external}} <b>inputs</b> <b>needed,</b> and {{labour requirements}} of selected plant disease management practices of traditional farmers.|$|R
40|$|Here is a {{framework}} for judgment {{in terms of a}} continuum of “subjective” probabilities, {{a framework}} in which probabilistic judgments need not stand on a foundation of certainties. In place of propositional data bases, this “radical ” probabilism (“probabilities all the waydown to the roots”) envisages full or partial probabilityassignments to probabilityspaces, together with protocols for revising those assignments and their interconnections in the light of fresh empirical or logico-mathematical input. This <b>input</b> <b>need</b> not be of the limiting 0 -or- 1 sort. Updating byordinaryconditioning is generalized (sec. 2. 2) to probability kinematics, where an observation on a random variable X need not single out one value, but mayprompt a new probability distribution over all values of X. The effect of an observation itself, apart from the influence of prior new odds probabilities (sec. 3), is given bythe (“Bayes”) factor...|$|E
40|$|Native starch {{containing}} 12 % {{water was}} melt processed in presence of 23 % of various plasticizers at 120 °C, either by simple compression molding or by extrusion using a laboratory scale microcompounder. Glycerol, a typical starch plasticizer, {{was used as}} a reference and compared to three choline salts: raw choline chloride (which is a solid in dry state with a melting point above 300 °C), and two ionic liquids synthesized from this precursor (choline acetate and choline lactate, liquids below 100 °C). These ionic plasticizers were shown to allow a more efficient melting of native starch in both processes. The investigation of macromolecular structure changes during processing shows that this efficiency can be ascribed to a starch chain scission mechanism, resulting in lower specific mechanical energy <b>input</b> <b>need</b> for starch thermoplasticization compared to glycerol plasticized starch. Compared to the synthesized ionic liquids, raw commercial choline chloride leads to a good compromise between limited chain scission, and final water uptake and thermomechanical properties...|$|E
40|$|We {{studied the}} effect of {{increased}} water content on {{the dynamics of the}} lithosphere-asthenosphere boundary in a postsubduction setting. Results from numerical mantle convection models show that the resultant decrease in mantle viscosity and the peridotite solidus produce small-scale convection at the lithosphere-asthenosphere boundary and magmatism that follows the spatially and temporally scattered style and volumes typical for collision magmatism, such as the late Cenozoic volcanism of the Turkish-Iranian Plateau. An inherent feature in small-scale convection is its chaotic nature that can lead to temporally isolated volcanic centers tens of millions of years after initial continental collision, without evident tectonic cause. We also conclude that water input into the upper mantle during and after subduction under the circum-Mediterranean area and the Tibetan Plateau can account for the observed magmatism in these areas. Only fractions (200 – 600 ppm) of the water <b>input</b> <b>need</b> to be retained after subduction to induce small-scale convection and magmatism on the scale of those observed from the Turkish-Iranian Plateau...|$|E
50|$|Guided input {{generation}} process aims {{to minimize}} the number of <b>inputs</b> <b>needed</b> to find each bug by taking program behavior information for past inputs into account.|$|R
50|$|This {{formulation}} is {{of particular}} use in developing Artificial Intelligence programs, because it gives {{an indication of the}} <b>input</b> <b>needed</b> for a system that attempts to emulate human activities.|$|R
50|$|Tuxedo {{middleware}} {{also uses}} OBBs to mention C and C++ arrays, or typed data buffers. This probably (<b>input</b> <b>needed</b> from experts) {{is the oldest}} reference to OBBs used in a computer system.|$|R
40|$|Sponsored Report (for Acquisition Research Program) System-of-systems (SoS) {{acquisition}} {{research has}} identified lack of alignment {{and lack of}} collaboration as two important issues leading to problems in SoS acquisition. This report captures the exploratory work toward improving alignment between and collaboration among the individual system programs {{in the development of}} an SoS. An SoS inter-program collaboration approach is proposed. It is inspired by some existing web-based collaborative systems, such as eBay, Facebook, and Eureka, and suggests an attraction mechanism to effect SoS inter-program collaboration. In addition, a web-based collaborative system is also suggested. Based on an architecture for distributed and interoperable management of multi-site production projects, it allows personnel of all programs associated with an SoS to <b>input</b> <b>need</b> points for component system inputs and retrieve information required to align the individual programs. Furthermore, contracting structures for facilitating collaboration among the system programs are also considered. Finally, this work forms a basis for implementing a web-based SoS collaborative system to support Department of Defense (DoD) SoS acquisition programs. Acquisition Research Progra...|$|E
40|$|Instructional {{design is}} not a linear process: {{designers}} have to weigh {{the advantages and disadvantages}} of alternative solutions, taking into account different kinds of conflicting and changing constraints. To make sure that they eventually choose the most optimal one, they have to keep on collecting information, reconsidering continuously whether their own decisions are still justified in the light of the latest insights. We have studied the role of iteration during instructional design. For our research, we have used an ISD-based method for the specification of training simulators. During our empirical evaluation study, we introduced five events that are likely to cause iteration. The results show that the quality of the designs is not directly related to the amount of iteration. We conclude that there are different kinds of iteration, triggered by different kinds of actions and events. We propose a list of triggers for iteration some of which originate from outside (new information, new opinions/arguments and acquisition procedures); others are caused by, or evolve from interaction with the design process itself (discovery of missing <b>input,</b> <b>need</b> to repair errors, new insights based on work later on in the design process, and new ideas of the designers) ...|$|E
40|$|Chang et al. [1], [2] {{consists}} of two stages. First, a load-balancing stage converts arriving packets into uniform traffic. Then, a forwarding stage transfers packets from the linecards to their final output destination. Load-balanced switches do not need a centralized scheduler and can achieve 100 % throughput for a broad class of traffic distributions. However, load-balanced switches may cause packets at the output port {{to be out of}} sequence. Several schemes have been proposed to tackle the out-of-sequence problem of the load-balanced switch. They are either too complex to implement, or introduce a large additional delay. In this paper, we present a practical load-balanced switch, called the Byte-Focal switch, which uses packet-by-packet scheduling to significantly improve the delay performance over switches of comparable complexity. We prove that the queues at the <b>input</b> <b>need</b> only finite buffering, and that the overall switch is stable under any traffic matrix. Our analysis shows that the average queuing delay is roughly linear with the switch size N, and although the worst case resequencing delay is N 2, the average resequencing delay is much smaller. This means that we can reduce the required resequencing buffer size significantly. Index Terms—Load-balanced switches, throughput. I...|$|E
30|$|Based {{on the two}} data sets, we now {{calculate}} the two empirical <b>inputs</b> <b>needed</b> for our agent-based model, namely the size distribution of the collaboration events and the activity distribution of the agents.|$|R
5000|$|The LTTD {{process may}} also take {{advantage}} of the temperature gradient available at power plants, where large quantities of warm cooling water are discharged from the plant, reducing the energy <b>input</b> <b>needed</b> to create a temperature gradient.|$|R
5000|$|Batch {{jobs are}} not {{interactive}} {{and so all}} <b>input</b> <b>needs</b> to be prepared in advance {{with little or no}} opportunity for the user to alter the input (at least not without programming) once the batch job starts to execute.|$|R
40|$|For {{dealing with}} the {{adjacent}} input fuzzy sets having overlapping information, non-additive fuzzy rules are formulated by defining their consequent {{as the product of}} weighted input and a fuzzy measure. With the weighted <b>input,</b> <b>need</b> arises for the corresponding fuzzy measure. This is a new concept that facilitates the evolution of new fuzzy modeling. The fuzzy measures aggregate the information from the weighted inputs using the λ-measure. The output of these rules {{is in the form of}} the Choquet fuzzy integral. The underlying non-additive fuzzy model is investigated for identification of non-linear systems. The weighted input which is the additive S-norm of the inputs and their membership functions provides the strength of the rules and fuzzy densities required to compute fuzzy measures subject to q-measure are the unknown functions to be estimated. The use of q-measure is a powerful way of simplifying the computation of @l-measure that takes account of the interaction between the weighted inputs. Two applications; one real life application on signature verification and forgery detection, and another benchmark problem of a chemical plant illustrate the utility of the proposed approach. The results are compared with those existing in the literature...|$|E
40|$|Rainfall {{prediction}} can be {{used for}} various purposes and the accuracy in predicting is important in many ways.  In this research, data of rainfall prediction use daily rainfall data from 2013 - 2014 years at rainfall station in Putussibau, West Kalimantan. Rainfall prediction using four parameters: mean temperature, average humidity, wind speed and mean sea level pressure. This research to determine how performance Neural Fuzzy Inference System with Levenberg-Marquardt training algorithm for rainfall prediction. Fuzzy logic {{can be used}} to resolve the linguistic variables used in rule of rainfall. While neural networks have ability to adapt and learning process, due to recognize patterns of data from <b>input</b> <b>need</b> training to prediction. And Levenberg-Marquardt algorithm is used for training because of effectiveness and convergence acceleration. The results showed five models NFIS-LM developed using a variety of membership functions as input obtained that model NFIS-LM with twelve of membership functions and use four inputs, such as mean temperature, average humidity, wind speed and mean sea level pressure gives best results to predict rainfall with values Mean Square Error (MSE) of 0. 0262050. When compared with model NN-Backpropagation, NFIS-LM models showed lower accuracy. It is shown from MSE generated where model NN-Backpropagation generate MSE of 0. 0167990...|$|E
40|$|Computational {{problems}} that involve dynamic data {{have been an}} important subject of study in programming languages. Recent advances in self-adjusting computation have developed techniques that enable programs to respond automatically and efficiently to dynamic changes in their inputs. But these techniques have re-quired an explicit programming style, where the programmer must use specific monadic types and primitives to identify, create and operate on data that can change over time. Our paper, “Implicit Self-Adjusting Com-putation for Purely Functional Programs ” (ICFP ’ 11), describes the theory underlying implicit self-adjusting computation, where the programmer need only annotate the (top-level) input types of the programs to be translated. A type-directed translation rewrites the (purely functional) source program into an explicitly self-adjusting target program. The subject of this talk is a prototype implementation (an extension of MLton) and experimental results that are competitive with explicitly self-adjusting handwritten code. 2 Implicit Self-Adjusting Computation In self-adjusting computation, programs respond efficiently to changes in their input. The parts of the computation that could be affected by such changes are changeable; the parts that cannot be affected are stable. (Not all of a program’s <b>input</b> <b>need</b> be changeable, but if we actually change the “stable ” part of the input, we must run the program from scratch. ...|$|E
5000|$|The {{fighting}} {{system is}} different from other fighting games in {{that there are not}} a lot of directional <b>inputs</b> <b>needed</b> for moves. Instead, special moves are set by collecting [...] "Bullets" [...] and then activating them with the DS's touch screen.|$|R
50|$|There is no {{switching}} signal to indicate S-Video. Some TVs can auto-detect {{the presence of}} the S-Video signal but more commonly the S-Video <b>input</b> <b>needs</b> to be manually selected. The same for the rare component YPbPr, which is in many cases implemented over a composite or rgb scart.|$|R
50|$|Especially as a {{small-scale}} crop, Arthrospira {{still has}} considerable potential for development, for example for nutritional improvement. New countries where this could happen, should dispose of alkaline-rich ponds on high altitudes or saline-alkaline-rich groundwater or coastal areas with high temperature. Otherwise, technical <b>inputs</b> <b>needed</b> for new spirulina farms are quite basic.|$|R
40|$|Thesis (Ph. D.) [...] University of Washington, 2005. Nineteen {{undergraduate}} mathematic {{students from}} three Pre-Calculus classes were observed during class meetings, in lab sessions, and in one-on-one interviews. The study used a qualitative methodology {{to consider the}} validity of their attitudes toward their graphing calculators. In the interview sessions, students talked about classroom problem solving situations and also solved mathematics problems posed by the researcher {{with the use of}} think-aloud commentaries. Findings suggest that students' beliefs about the effectiveness of their graphing calculators in helping them learn and retain mathematical ideas and concepts are often suspect, and that researchers who rely on this type of student <b>input</b> <b>need</b> to be wary of its validity. Thus, the study calls into question the effectiveness of surveys as an instrument for determining whether or not students benefit from the use of graphing calculators, especially if such instruments are the only tool used to gauge student calculator perceptions. Finally, the researcher postulates that assessment practices of the instructors participating in the study were naive in that these assessments allowed many students to use the graphing calculator in ways that caused the instructors to believe that the students knew more about class topics than they actually did...|$|E
40|$|We {{describe}} {{a method for}} automatically learning a parser from labeled, bracketed corpora that results in a fast, robust, lightweight parser that is suitable for realtime natural language systems and similar applications. Unlike ordinary parsers, all grammatical knowledge is captured in the learned decision trees, so no explicit phrase-structure grammar is needed. Another characteristic of the architecture is robustness, since the <b>input</b> <b>need</b> not fit pre-specified productions. The runtime architecture is very slim and references two learned decision trees that allow the parser to operate in a "strictly deterministic" manner in Marcus' (1977) sense. The basis is a shift-reduce parser (Aho, Sethi, & Ullman 1986) consisting of a stack, an input stream and a decision control mechanism. The core part of our work is to learn the decision control mechanism, for which we employ a novel Shift/Reduce decision algorithm and a novel Constituent Labeling decision algorithm. The features used for both the Shift/Reduce and Constituent Labeling decision tasks are restricted to the constituent labels in the stack and the part-of-speech tags of {{the words in the}} input. Even without using specific lexical features, we have achieved respectable labeled bracket accuracies of about 81 % precision and 82 % recall on the Penn Treebank corpus. Processing speed on a Sparc Ultra I machine is more than 500 words per CPU second. The high processing speed makes our parser suitable for applications like online language understanding and machine translation applications. Without any optimization, the decision trees consume only 6 M of memory, making it possible to run on platforms with limited memory. Since the only resource needed to train our parser is a labeled and bracketed corpus, we believe the learning method is readily applicable to other languages. Preliminary experiments on a Chinese corpus (which contains about 3000 sentences from Chinese primary school text) have yielded results comparable to that for English...|$|E
40|$|AbstractIn {{the frame}} of the Land Use/Land Cover Area Frame Survey {{sampling}} of topsoil was carried out on around 22, 000 points in 25 EU Member States in 2009 and in additional 2 Member States in 2012. Besides other basic soil properties soil phosphorus (P) content of the samples were also measured in a single laboratory in both years. Based on the results of the LUCAS topsoil survey we performed an assessment of plant available P status of European croplands. Higher P levels can be observed in regions where higher crop yields can be expected and where high fertilizer P inputs are reported. Plant available phosphorus levels were determined using two selected fertilizer recommendation systems: one from Hungary and one from the United Kingdom. The fertilizer recommendation system of the UK does not recommend additional fertilizer use on croplands with highest P supply, which covers regions mostly in Belgium and the Netherlands. According to a Hungarian advisory system {{there is a need for}} fertilizer P input in all regions of the EU. We established a P fertilizer need map based on integrating results from the two systems. Based on data from 2009 and 2012, P input demand of croplands in the European Union was estimated to 3, 849, 873 [*]tons(P 2 O 5) /year. Meanwhile we found disparities of calculated <b>input</b> <b>need</b> and reported fertilizer statistics both on local (country) scale and EU level. The first ever uniform topsoil P survey of the EU highlights the contradictions between soil P management of different countries of the Union and the inconsistencies between reported P fertilizer consumption and advised P doses. Our analysis shows a status of a baseline period of the years 2009 and 2012, while a repeated LUCAS topsoil survey can be a useful tool to monitor future changes of nutrient levels, including P in soils of the EU...|$|E
5000|$|Each {{player can}} equip their own skills, which are moves {{that can be}} {{activated}} by pressing the unique <b>inputs</b> <b>needed</b> and [...] "FreeStyles", or modifiers of normal moves that all characters have that will either appear at random in the normal move's place, or takes its place in a set order.|$|R
25|$|Life cycle energy {{analysis}} (LCEA) is an approach {{in which all}} energy inputs to a product are accounted for, not only direct energy inputs during manufacture, but also all energy <b>inputs</b> <b>needed</b> to produce components, materials and services needed for the manufacturing process. An earlier term for the approach was {{energy analysis}}.|$|R
40|$|We {{study the}} value of the light quark masses {{combination}} mu+md in QCD using both Finite Energy Sum Rules and Laplace Sum Rules. We have performed a detailed analysis of both the perturbative QCD and the hadronic (parametrization <b>inputs</b> <b>needed</b> in these 2 Sum Rules. As main result, we obtain mu 1 GeV) ...|$|R
40|$|Scaling-up {{antiretroviral}} treatment (ART) to socially meaningful {{levels in}} low-income countries {{with a high}} AIDS burden is constrained by (1) the continuously growing caseload of people to be maintained on long-term ART; (2) evident problems of shortage and skewed distribution in the health workforce; and (3) the heavy workload inherent to presently used ART delivery models. If we want to imagine how health systems can react to such challenges, {{we need to understand}} better {{what needs to be done}} regarding the different types of functions ART requires, and how these can be distributed through the care supply system, knowing that different functions rely on different rationales (professional, bureaucratic, social) for which the human <b>input</b> <b>need</b> not necessarily be found in formal healthcare supply systems. Given the present realities of an increasingly pluralistic healthcare supply and highly eclectic demand, we advance three main generic requirements for ART interventions to be successful: trustworthiness, affordability and exclusiveness - and their constituting elements. We then apply this analytic model to the baseline situation (no fundamental changes) and different scenarios. In Scenario A there are no fundamental changes, but ART gets priority status and increased resources. In Scenario B the ART scale-up strengthens the overall health system: we detail a B 1 technocratic variant scenario, with profoundly re-engineered ART service production, including significant task shifting, away from classical delivery models and aimed at maximum standardisation and control of all operations; while in the B 2 community-based variant scenario the typology of ART functions is maximally exploited to distribute the tasks over a human potential pool that is as wide as possible, including patients and possible communities. The latter two scenarios would entail a high degree of de-medicalisation of ART. HIV Antiretroviral treatment (ART) Health systems Health care Sub-Saharan Africa Human resources...|$|E
40|$|In {{the frame}} of the Land Use/Land Cover Area Frame Survey {{sampling}} of topsoil was carried out on around 22. 000 points in 25 EU Member States in 2009 and in additional 2 member States in 2012. Beside other basic soil properties soil phosphorus (P) content of the samples were also measured in a single laboratory in both years. Based on the results of the LUCAS Topsoil survey we performed an assessment of plant available P status of European croplands. Higher P levels can be observed in regions where higher crop yields can be expected and where high fertiliser P inputs are reported. Plant available phosphorus levels were determined using two selected fertilizer recommendation systems; one from Hungary and one from the United Kingdom. The fertiliser recommendation system of the UK does not recommend additional fertiliser use on croplands with highest P supply, which covers regions mostly in Belgium and the Netherlands. According to a Hungarian advisory system {{there is a need for}} fertilizer P input in all regions of the EU. We established a P fertiliser need map based on integrating results from the two systems. Based on data from 2009 and 2012, P input demand of croplands in the European Union was estimated to 3, 849, 873 tons(P 2 O 5) /year. In the meanwhile we found disparities of calculated <b>input</b> <b>need</b> and reported fertiliser statistics both on local (country) scale and on EU level. The first ever uniform topsoil P survey of the EU highlights the contradictions between soil P management of different countries of the Union while also highlights the inconsistencies between reported P fertiliser consumption and advised P doses. Our analysis shows a status of a baseline period of the years 2009 and 2012, while a repeated LUCAS topsoil survey can be a useful tool to monitor future changes of nutrient levels, including P in soils of the EU. JRC. H. 5 -Land Resources Managemen...|$|E
40|$|Farida Nugrahani. T 1203002. The Teaching and Learning of Appreciative Literature in SMA Surakarta {{from the}} Perspective of Competency-Based Curriculum: An Evaluation Study. Disertation. Graduate Program, Sebelas Maret University, 2008. This {{research}} uses {{the framework of}} Context, Input, Process, and Product (CIPP). In terms of the Context, the research examines the characteristics of learners, teachers, and schools supporting {{the process of the}} teaching of appreciative literature. In relation to the Input, the research examines the development of teaching materials and supporting learning facilities, concerning with the Process, the research examines the implementation of teaching and learning activities, while Product examines the achievement of learning outcomes. The aim of this research is to examine how effective the learning objectives are achieved and the effect resulted from the program and policy implementation. This research is also designed to identify the strength and weaknesses of the process of literature teaching in SMA Surakarta, to hope this process could be usefull as foundation to develop suggestion for quality of literature learning process at school where this research have been taken. The Indonesian government launched the 2004 Curriculum. This curriculum has been implemented since academic years 2004 / 2005. The essence of the curriculum is adopted to develop the 2006 Curriculum which is known as Unit Level Curriculum). Hopefully, the curriculum is {{supposed to be able to}} improve the quality of the teaching of literature at schools, which is still not successful. This is a formative evaluation research and it is qualitative in nature. It is classified as a single and embedded case study. The research was conducted at SMA Negeri 1, SMA Negeri 8, SMA Al-Islam 1, and SMA Murni Surakarta from January 2005 to July 2006. The techniques applied to collect the data were document analysis, in-depth interviewing, and participant observation. The credibility of the data was examined using method triangulation, data triangulation and review of key informants. The data, then, was analyzed by an interactive model of analysis suggested by Miles and Huberman. The research finding shows that the process of the teaching of appreciative literature varies. The product also varies. In general, however, the teaching of appreciative literature has achieved the designed objectives of curriculum, that is the students’ competence in appreciating, expressing, and creating literary works. In this case, the condition of the context is more influential for Process than Input that developed. The context here refers to students’ good academic achievement, their positive attitudes, and their good interest in literary works. Besides, the context also refers to teachers who are experienced and those who are serious in implementing the process of teaching and learning in order to achieve objectives. It is admitted, however, that the teachers’ competence in teaching appreciative literature and their creation to develop <b>input</b> <b>need</b> improving. Based on the research finding, it is suggested that (1) the Process should be accompanied by the Context and relevant Input so that the process of teaching and learning in the classroom can run effectively and achieved the designed Product, because naturally there’s no effective strategy that can applied for every Context; (2) The teachers should participate in teacher professional development so that they are able to show a better performance in the teaching of appreciative literature; (3) The teachers should develop student’s interest in literary works so that they can join the class successfully; and (4) the schools need to develop teaching and learning facilities and make condusive environment in order to realize a successful teaching and learning literature...|$|E
40|$|This paper {{attempts}} {{to evaluate the}} energy <b>inputs</b> <b>needed</b> to produce rural buildings. Based on a survey, a comparison is carried out of traditional and innovative technologies with reference to their energy consumption. Some basic data regarding energies in transportation are also presented. The implications of this analysis for development objectives is discussed...|$|R
40|$|This paper {{addresses}} {{the problem of}} semi-global nonlinear output regulation for a class of nonlinear systems possessing a nonlinear internal model. It is shown that, under appropriate (and verifiable) hypotheses, the standard assumption that the feed-forward <b>inputs</b> <b>needed</b> to keep the zero error manifold invariant satisfy a linear differential equation can be weakened...|$|R
50|$|Life cycle energy {{analysis}} (LCEA) is an approach {{in which all}} energy inputs to a product are accounted for, not only direct energy inputs during manufacture, but also all energy <b>inputs</b> <b>needed</b> to produce components, materials and services needed for the manufacturing process. An earlier term for the approach was {{energy analysis}}.|$|R
