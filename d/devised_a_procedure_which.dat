3|10000|Public
2500|$|Chronic {{cerebrospinal}} {{venous insufficiency}} (CCSVI or CCVI) {{is a term}} developed by Italian researcher Paolo Zamboni in 2008 to describe compromised flow {{of blood in the}} veins draining the central nervous system. [...] Zamboni hypothesized that it {{played a role in the}} cause or development of multiple sclerosis (MS). Zamboni also <b>devised</b> <b>a</b> <b>procedure</b> <b>which</b> was termed by the media as [...] "liberation procedure" [...] or [...] "liberation therapy", involving venoplasty (or stenting) of certain veins in an attempt to improve blood flow.|$|E
40|$|With the {{increasing}} availability of diachronic corpora, the automatic extraction of phraseological phenomena {{is becoming an}} important concern of diachronic phraseological research. To date, {{there have been few}} suggestions for corpus-driven extraction procedures specifically tailored to linguistic research on German corpora and the very prospect of a useful corpus-driven extraction is challenged by a number of remaining problems. In this paper we sought to assess the feasibility of an entirely corpus-driven approach to multiword sequence extraction and the influence of factors such as the part-lemmatisation of source data, various filters and the incorporation of sequence-internal variable slots. Using a subcorpus of the Swiss Text Corpus as test data, we first developed an operationalization of multiword sequences and then <b>devised</b> <b>a</b> <b>procedure</b> <b>which</b> is able to extract them with a precision of upward of 70 % while also providing adequate recall and transparency of results. Best results where obtained with a frequency-based filter combined with a lexico-structural filter, part-lemmatisation and the incorporation of optional variable slots...|$|E
40|$|Abstract — In {{this paper}} we <b>devise</b> <b>a</b> <b>procedure,</b> <b>which</b> we call GTM classifier, for data {{classification}} {{based on the}} Generative Topographic Mapping (GTM) {{and apply it to}} the classification of motor unit action potentials (MUAPs). The results of the classification of experimental MUAPs show that classification success rate of up to 93 % may be obtained. They also indicate that the GTM classifier may be successfully employed as a tool for outlier rejection, with rejection rate of up to 100 %. I...|$|R
40|$|We are {{presenting}} new empirical fitting {{functions for}} the Lick/IDS line-strength indices as measured in MILES (Medium-resolution INT Library of Empirical Spectra). Following previous {{work in the}} field, these functions describe the empirical behaviour of the line-strength indices with the atmospheric stellar parameters (T eff, logg, [Fe/H]). In order to derive the fitting functions we have <b>devised</b> <b>a</b> new <b>procedure</b> <b>which,</b> being fully automatic, provides a better description of the line-strength index variations in the stellar parameter space...|$|R
40|$|Given a finitely {{presented}} group G = 〈X; R〉, {{the word}} problem asks to decide effectively whether a given {{word in the}} free group w ∈ F (X) represents the identity element in G or not. Another important computational problem is to check effectively whether G is finite or not. For the ”No ” parts of these problems, we can use quotient tests. For instance, for the ”No ” part of the word problem, we can <b>devise</b> <b>a</b> <b>procedure</b> <b>which</b> proves w ̸=G 1 or returns "Dont’t know " if we have an effectively computable group homomorphism ϕ: G − → H and if the ”No ” part of the word problem is decidable in H. To find a suitable group homomorphism ϕ: G − → H, we use universal linear representations of G. Special cases of such representations {{have been used in}} [12] and [2] for other purposes. Here the universal linear representation ϱ: G − → SL(n, QR) is constructed as follows. We map each generator xk of G to a matrix of indeterminates (a (k) i,j). In the polynomial ring P having these indeterminates, we form the ideal IR generate...|$|R
40|$|We {{consider}} {{the problem of}} realizing lossless behaviours {{with respect to the}} supply rate equal to the scalar product of the input and of the derivative of the output variables. Using polynomial algebraic method we <b>devise</b> <b>a</b> realization <b>procedure</b> <b>which,</b> starting from <b>an</b> image representation, yields the same state representation used by van der Schaft and Oeloff in the context of model reduction. We also apply the insights derived from this realization procedure to the synthesis of lossless mechanical systems with given transfer functions using springs and masses...|$|R
5000|$|Ashwell’s goal as a {{researcher}} was to <b>devise</b> <b>a</b> labeling serum glycoproteins in order to study the role of ceruloplasmin in Wilson disease. [...] With another researcher named Anatol G. Morell, he worked to propose that membrane lectins remove senescent circulating glycoproteins, and discovered {{one of the earliest}} known carbohydrate receptors. They were able to <b>devise</b> <b>a</b> labeling <b>procedure</b> <b>which</b> allowed them to remove enzymes of the glycoproteins' sialic acid residue. By completing this process, they were able to incorporate other substances into the protein. In 1974, Ashwell and Morell happened to discover that a certain receptor in a human’s liver is able to recognize a specific glycoprotein called asialoglycoprotein. [...] Ashwell admitted that he was not specifically looking for the asialoglycoprotein when he found it.|$|R
40|$|AbstractThe {{problem of}} {{determining}} the optimal values of extrapolated iterative schemes, as they apply to the solution of large-scale least-squares problems, is addressed here. Based on algebraic and geometric eigenvalue properties of the Accelerated Gauss—Seidel (AGS), we <b>devise</b> <b>a</b> simple algorithmic <b>procedure,</b> <b>which</b> successfully yields the optimal values of the Extrapolated AGS (EAGS). Comparisons with the optimal SOR scheme reveal that the two optimal schemes strongly compete. Numerical examples are used to demonstrate our results...|$|R
40|$|Abstract—The {{problem of}} field {{focusing}} onto a target location in an unknown scenario is considered. In particular, we <b>devise</b> <b>an</b> adaptive <b>procedure</b> in <b>which</b> first <b>an</b> {{image of the}} unknown region where the target point is located is formed via the linear sampling method (LSM). Then, the LSM result is used also to define the excitations coefficients for the array elements needed to focus the field. This novel approach to focusing is described and tested with numerical examples. 1...|$|R
40|$|ABSTRACT: Cellular {{decision}} making is accomplished by complex networks, the structure of which has traditionally been inferred from mean gene expression data. In addition to mean data, quantitative measures of distributions across a population can be obtained using techniques such as flow cytometry that measure expression in single cells. The resulting distributions, which reflect a population’s variability or noise, constitute a potentially rich source of information for network reconstruc-tion. A significant portion of molecular noise in a biological process is propagated from the upstream regulators. This propagated component provides additional information about causal network connections. Here, we <b>devise</b> <b>a</b> <b>procedure</b> in <b>which</b> we exploit equations for dynamic noise propagation in a network under nonsteady state conditions to distinguish between alternate gene regulatory relationships. We test our approach in silico using data obtained from stochastic simulations {{as well as in}} vivo using experimental data collected from synthetic circuits constructed in yeast...|$|R
40|$|It {{is known}} that speech {{recognition}} performance degrades if systems are not trained and tested under similar speaking conditions. This is particularly true if a speaker is exposed to high demanding workload stress or noise. For current recognition systems {{to be successful in}} applications susceptible to stress, speech recognizers should address the adverse conditions experienced by the user. In this paper, we consider the problem of improved recognition training for speech recognition for various stressed speaking conditions (e. g., slow, loud, and Lombard effect speaking styles). The main objective is to <b>devise</b> <b>a</b> training <b>procedure</b> <b>which</b> produces <b>a</b> hidden Markov model recognizer which better characterizes a given stressed speaking style, without the need for directly collecting such stressed data. The novel approach is to construct a word production model using a previously suggested source generator framework [6], by employing knowledge of the statistical nature of duration and spectra [...] ...|$|R
40|$|Consistency {{limits are}} {{extensively}} used in geotechnical engineering practice. Conventional test <b>procedures</b> <b>which</b> {{have been developed}} to determine the liquid limit are based on the strength, with the liquid limit corresponding to a shearing resistance of 1. 7 - 2. 0 kPa, However, the mechanisms controlling the test procedures do not simulate the mechanisms controlling the water-holding capacity of soils which the liquid limit is supposed to represent. Further, the test procedures have their limitation in that they are relatively arbitrary. In this paper an attempt has been made to <b>devise</b> <b>a</b> test <b>procedure</b> <b>which</b> will give the water-holding capacity equivalent to the liquid limit as the equilibrium water content under a specified effective vertical stress of 0. 9 kPa in normal compression. It has been found that the proposed test procedure is simple, accurate, relatively experimental and personal error-free, and could also be carried out easily for soils of low plasticit...|$|R
40|$|Abstract: We {{provide a}} new {{algorithm}} {{for the treatment}} of inverse problems which combines the traditional SVD inversion with an appropriate thresholding technique in a well chosen new basis. Our goal is to <b>devise</b> <b>an</b> inversion <b>procedure</b> <b>which</b> has the advantages of localization and multiscale analysis of wavelet representations without losing the stability and computability of the SVD decompositions. To this end we utilize the construction of localized frames (termed ") built upon the SVD bases. We consider two dierent situations: the " scenario, where the needlets are as-sumed to behave similarly to true wavelets, and the -type " scenario, where we assume that the properties of the frame truly depend on the SVD basis at hand (hence on the op-erator). To illustrate each situation, we apply the estimation algorithm respectively to the deconvolution problem and to the Wicksell problem. In the latter case, where the SVD ba-sis is a Jacobi polynomial basis, we show that our scheme is capable of achieving rates of convergence which are optimal in the L 2 case, we obtain interesting rates of convergence for other Lp norms which are new (to the best of our knowledge) in the literature, and we also give a simulation study showing that the NEED-VD estimator outperforms other standard algorithms in almost all situations...|$|R
40|$|Unrecognized {{cross-contamination}} {{has been}} known to occur in laboratories frequently, especially with sensitive recovery system like BACTEC 460 TB. In March 2001, we investigated a pseudo-outbreak of Mycobacterium tuberculosis isolates in three smear negative clinical specimens and would like to present our experience in this communication. Methods : All suspected cases were confirmed by checking the drug susceptibility and DNA fingerprints using spoligotyping as well as restriction fragment length polymorphism. Results: On investigation, the most likely cause was found to be the use of common decontamination reagents and phosphate buffer. Conclusions: To avoid erroneous diagnosis, we have <b>devised</b> <b>a</b> dedicated decontamination <b>procedure,</b> <b>which</b> includes separate aliquoting of phosphate buffer and decontamination reagents per patient. Timely molecular analysis and appropriate changes to specimen processing have been identified as useful measures for limiting laboratory cross contamination...|$|R
40|$|We {{provide a}} new {{algorithm}} {{for the treatment}} of inverse problems which combines the traditional SVD inversion with an appropriate thresholding technique in a well chosen new basis. Our goal is to <b>devise</b> <b>an</b> inversion <b>procedure</b> <b>which</b> has the advantages of localization and multiscale analysis of wavelet representations without losing the stability and computability of the SVD decompositions. To this end we utilize the construction of localized frames (termed "needlets") built upon the SVD bases. We consider two different situations: the "wavelet" scenario, where the needlets are assumed to behave similarly to true wavelets, and the "Jacobi-type" scenario, where we assume that the properties of the frame truly depend on the SVD basis at hand (hence on the operator). To illustrate each situation, we apply the estimation algorithm respectively to the deconvolution problem and to the Wicksell problem. In the latter case, where the SVD basis is a Jacobi polynomial basis, we show that our scheme is capable of achieving rates of convergence which are optimal in the $L_ 2 $ case, we obtain interesting rates of convergence for other $L_p$ norms which are new (to the best of our knowledge) in the literature, and we also give a simulation study showing that the NEED-D estimator outperforms other standard algorithms in almost all situations. Comment: Published at [URL] in the Electronic Journal of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|Abstract Purpose. Skew ray {{ambiguity}} {{is present}} in most videokeratoscopic measurements when azimuthal components of the corneal curvature are not taken into account. There have been some reported studies based on theoretical predictions and measured test surfaces suggesting that skew ray ambiguity is significant for highly deformed corneas or decentered corneal measurements. However, the effect of skew ray ambiguity in ray tracing through videokeratoscopic data has not been studied in depth. Methods. We have evaluated {{the significance of the}} skew ray ambiguity and its effect on the analyzed corneal optics. This has been achieved by <b>devising</b> <b>a</b> <b>procedure</b> in <b>which</b> we compared the corneal wavefront aberrations estimated from 3 D ray tracing with those determined from 2 D (meridional based) estimates of the refractive power. The latter was possible due to recently developed concept of refractive Zernike power polynomials which links the refractive power domain with that of the wavefront. Simulated corneal surfaces as well as data from a range of corneas (from two different Placido disk-based videokeratoscopes) were used to find the limit at which the difference in estimated corneal wavefronts (or the corresponding refractive powers) would have clinical significance (e. g., equivalent to 0. 125 D or more). Results. The inclusion/exclusion of the skew ray in the analyses showed some differences in the results. However, the proposed procedure showed clinically significant differences only for highly deformed corneas and only for large corneal diameters. Conclusions. For the overwhelming majority of surfaces, the skew ray ambiguity is not a clinically significant issue in the analysis of the videokeratoscopic data indicating that the meridional processing such as that encountered in calculation of the refractive power maps is adequate...|$|R
40|$|The {{traditional}} approach to dissection, with students following {{a set of}} specific instructions to reveal the structures for studying, does not make maximal use of the small group setting in the gross anatomy laboratory. In problem-oriented dissection (POD), a clinical case is introduced before the students start dissecting the cadavers, which have been prepared to mimic the clinical condition in the case. The students need {{to reflect on their}} basic anatomical knowledge in order to <b>devise</b> <b>a</b> clinical <b>procedure</b> <b>which</b> needs to be done on their patients (i. e., the cadavers). They then perform their self-devised procedure on the cadaver, followed by the dissection of the region to look at the results of their procedure. The dissection results prompt the students to reflect on their self-devised procedure and the anatomy. Students then go to the literature to search for the recommended way(s) of performing the clinical <b>procedure,</b> <b>which</b> will stimulate students to further reflect by comparing their self-devised procedure to the recommended one(s). Different groups of students then gather together again in the laboratory to share their self-devised procedures, the results of their procedures on the cadavers, and their reflections. The teacher in the POD should be a facilitator, whose role is to guide the students to reflect and to apply their anatomical knowledge. One method to structure the teacher-student interactions is the one-minute preceptor model, a learner-centered and time-efficient framework which provides rich feedback and promotes reflection. It consists of five microskills: get a commitment from the students; probe for supporting evidence; reinforce what was done right; correct errors and fill in omissions; and teach a general rule...|$|R
40|$|International audienceDespite {{the high}} {{success of the}} NICA-Donnan (N-D) model to {{describe}} the interaction of protons and metal ions with natural organic matter, {{the large number of}} fit parameters is a major hindrance to its capacity to provide unique numerical solutions. This well-known difficulty is reflected in the unusually low value of the generic proton binding constant for carboxylic-type groups of fulvic acid (pKH 1 = 2. 34), and to some extent of humic acid (2. 93), and by the considerable covariance of the other generic N-D parameters. In some studies, the number of parameters obtained by regression is reduced by estimating some values independently with other techniques. Alternatively, the applicability of the model can be improved by <b>devising</b> <b>a</b> rigorous simulation <b>procedure,</b> <b>which</b> constrains the model-fit to converge toward chemically and physically realistic values. <b>A</b> <b>procedure</b> based on three successive iterations is proposed, and the solution is shown to be stable and invariant with the initial set of parameter values. The new generic parameters, in particular pKH 1 (FA) = 3. 54 and pKH 1 (HA) = 3. 87, derived from the same data set as the previous generic parameters, are in better agreement with literature data...|$|R
40|$|We have <b>devised</b> <b>a</b> two-step <b>procedure</b> by <b>which</b> {{multiple}} {{copies of}} a heterologous gene can be consecutively integrated into the Bacillus subtilis 168 chromosome without the simultaneous integration of markers (antibiotic resistance). The procedure employs {{the high level of}} transformability of B. subtilis 168 strains and makes use of the observation that thymine-auxotrophic mutants of B. subtilis are resistant to the folic acid antagonist trimethoprim (Tmp(r)), whereas thymine prototrophs are sensitive. First, a thymine-auxotrophic B. subtilis mutant is transformed to prototrophy by integration of a thymidylate synthetase encoding gene at the desired chromosomal locus. In a second step, the mutant strain is transformed with a DNA fragment carrying the heterologous gene and Tmp(r) colonies are selected. Approximately 5 % of these appear to be thymine auxotrophic and contain a single copy of the heterologous gene at the chromosomal locus previously carrying the thymidylate synthetase-encoding gene. Repetition of the procedure at different locations on the bacterial chromosome allows the isolation of strains carrying multiple copies of the heterologous gene. The method was used to construct B. subtilis strains carrying one, two, and three copies of the Bacillus stearothermophilus branching enzyme gene (glgB) in their genomes...|$|R
40|$|Candida albicans {{is one of}} {{the most}} common {{opportunistic}} fungal pathogens, causing life-threatening disease in immunocompromised patients. As it is not primarily a pathogen, but can exist in a commensal state, we aimed at the identification of new anti-infective compounds which do not eradicate the fungus, but primarily disable a virulence determinant. The yeast–hyphae-dimorphism of C. albicans is considered a major contributor to fungal disease, as mutants locked into either yeast or hyphal state have been shown to be less virulent in the mouse-model. We <b>devised</b> <b>a</b> high-throughput screening <b>procedure</b> <b>which</b> allows us to find inhibitors of the induction of hyphae. Hyphae-formation was induced by nitrogen starvation at 37 °C and neutral pH in a reporter strain, which couples promoter activity of the hyphae-specific HWP 1 to β-galactosidase expression. In a pilot screening of 720 novel synthetic compounds, we identified substances which inhibited the outgrowth of germ tubes. They belonged to chemical classes not yet known for antimycotic properties, namely methyl aryl-oxazoline carboxylates, dihydrobenzo[d]isoxazolones and thiazolo[4, 5 -e]benzoisoxazoles. In conclusion we developed a novel screening assay, which addresses the morphological switch from the yeast form of C. albicans to its hyphal form and identified novel chemical structures with activity against C. albicans...|$|R
40|$|Molecular cloning is a crucial, {{and often}} speed-limiting, step in many {{standard}} procedures of molecular biology. Ligation of cohesive DNA ends is normally {{carried out at}} 12 – 16 �C to ensure a good balance between enzyme activity and stability of annealed DNA overhangs. Low temperatures generally reduce ligase activity, whereas too high temperatures may reduce cloning efficiencies by melting annealed DNA overhangs and increase overall molecular motion in the ligation reaction. Several procedures have been described {{to increase the efficiency}} ligation reactions, including the addition of condensing agents as polyethylene glycol (1) or hexamminedicobalt chloride (2). These approaches induce macromolecular crowding, and thus serve to mimic higher DNA concentrations in the ligation reaction. Other approaches seek to increase the efficiency of molecular cloning procedures by omitting the ligation step by generating long single-stranded DNA overhangs that can be annealed and transformed directly into an appropriate Escherichia coli host (3). Ligation of blunt-ended DNA fragments is normally carried out at room temperature using higher concentrations of T 4 DNA ligase. We have <b>devised</b> <b>a</b> simple <b>procedure</b> in <b>which</b> high enzyme activity and DNA annealing is balanced by constant temperature cycling. We find temperature-cycle ligations (TCL) may increase the efficiency of staggered cut cloning ∼ 4 – 8 -fold, while the efficiency of blunt-end clonings are increased ∼ 4 – 6 -fold (see Table 1). The bacterial cloning vector pBluescript II KS + was digested with AflIII and HindIII producing two cohesive end-fragments of 434 and 2526 bp respectively, or PvuII generating two blunt-end fragments of 448 and 2512 bp respectively. All enzymes were purchased from Amersham. Digested plasmid DNA was separated on 1 % low-melting agarose (NuSieve) and purified as described by Sambrook et al. (1). For each digestion a ligation master mix was prepared containing 50 mM Tris–HCl (pH 7. 6) ...|$|R
40|$|Robert Batterman’s ontological {{insights}} (2002, 2004, 2005) are apt: Nature abhors singularities. “So should we,” responds the physicist. However, the epistemic {{assessments of}} Batterman concerning the matter {{prove to be}} less clear, for {{in the same vein}} he write that singularities play an essential role in certain classes of physical theories referring to certain types of critical phenomena. I <b>devise</b> <b>a</b> <b>procedure</b> (“methodological fundamentalism”) <b>which</b> exhibits how singularities, at least in principle, may be avoided within the same classes of formalisms discussed by Batterman. I show that we need not accept some divergence between explanation and reduction (Batterman 2002), or between epistemological and ontological fundamentalism (Batterman 2004, 2005). Though I remain sympathetic to the ‘principle of charity’ (Frisch (2005)), which appears to favor a pluralist outlook, I nevertheless call into question some of the forms such pluralist implications take in Robert Batterman’s conclusions. It is difficult to reconcile some of the pluralist assessments that he and some of his contemporaries advocate with {{what appears to be a}} countervailing trend in a burgeoning research tradition known as Clifford (or geometric) algebra. In my critical chapters (2 and 3) I use some of the demonstrated formal unity of Clifford algebra to argue that Batterman (2002) equivocates a physical theory’s ontology with its purely mathematical content. Carefully distinguishing the two, and employing Clifford algebraic methods reveals a symmetry between reduction and explanation that Batterman overlooks. I refine this point by indicating that geometric algebraic methods are an active area of research in computational fluid dynamics, and applied in modeling the behavior of droplet-formation appear to instantiate a “methodologically fundamental” approach. I argue in my introductory and concluding chapters that the model of inter-theoretic reduction and explanation offered by Fritz Rohrlich (1988, 1994) provides the best framework for accommodating the burgeoning pluralism in philosophical studies of physics, with the presumed claims of formal unification demonstrated by physicists choices of mathematical formalisms such as Clifford algebra. I show how Batterman’s insights can be reconstructed in Rohrlich’s framework, preserving Batterman’s important philosophical work, minus what I consider are his incorrect conclusions...|$|R
40|$|The {{selection}} of a best-subset regression model from a candidate family is a common problem that arises in many analyses. In best-subset model selection, we consider all possible subsets of regressor variables; thus, numerous candidate models {{may need to be}} fit and compared. One of the main challenges of best-subset selection arises from the size of the candidate model family: specifically, the probability of selecting an inappropriate model generally increases as the size of the family increases. For this reason, it is usually difficult to select an optimal model when best-subset selection is attempted based on a moderate to large number of regressor variables. Model selection criteria are often constructed to estimate discrepancy measures used to assess the disparity between each fitted candidate model and the generating model. The Akaike information criterion (AIC) and the corrected AIC (AICc) are designed to estimate the expected Kullback-Leibler (K-L) discrepancy. For best-subset selection, both AIC and AICc are negatively biased, and the use of either criterion will lead to overfitted models. To correct for this bias, we introduce a criterion AICi, which has a penalty term evaluated from Monte Carlo simulation. A multistage model selection <b>procedure</b> AICaps, <b>which</b> utilizes AICi, is proposed for best-subset selection. In the framework of linear regression models, the Gauss discrepancy is another frequently applied measure of proximity between a fitted candidate model and the generating model. Mallows 2 ̆ 7 conceptual predictive statistic (Cp) and the modified Cp (MCp) are designed to estimate the expected Gauss discrepancy. For best-subset selection, Cp and MCp exhibit negative estimation bias. To correct for this bias, we propose a criterion CPSi that again employs a penalty term evaluated from Monte Carlo simulation. We further <b>devise</b> <b>a</b> multistage <b>procedure,</b> CPSaps, <b>which</b> selectively utilizes CPSi. In this thesis, we consider best-subset selection in two different modeling frameworks: linear models and generalized linear models. Extensive simulation studies are compiled to compare the selection behavior of our methods and other traditional model selection criteria. We also apply our methods to a model selection problem in a study of bipolar disorder...|$|R
50|$|In 1960 Berners-Lee {{had evolved}} a {{technique}} for editing text—including hyphenation—for metal typesetting of printed material. As space in memory and backing store was a scarce and valuable resource in those days, {{he had also}} <b>devised</b> <b>a</b> <b>procedure</b> for compressing text which in 1963 he sent to Bob Bemer at Univac.|$|R
5000|$|Pearson in 1894 was {{the first}} to <b>devise</b> <b>a</b> <b>procedure</b> to test whether a {{distribution}} could be resolved into two normal distributions. This method required the solution of a ninth order polynomial. In a subsequent paper Pearson reported that for any distribution skewness2 + 1 < kurtosis. Later Pearson showed that ...|$|R
40|$|Abstract. In {{this paper}} we present an {{algorithm}} for estimating the rotation index of a closed loop {{from the number}} of singular points on it. On the basis of this index we also <b>devise</b> <b>a</b> <b>procedure</b> that guarantees to have an arbitrary loop being traced only once. Combining this <b>procedure</b> with <b>a</b> higher-order stepping algorithm, a robust intersection technique is resulted. ...|$|R
40|$|Includes bibliographical {{references}} (pages 62 - 64) This project examines various {{educational institution}} management models. From these models, one is {{selected as the}} best suited to Los Angeles Valley College. The project continues by <b>devising</b> <b>a</b> <b>procedure</b> for developing <b>a</b> philosophy, goals, and objectives for Los Angeles Valley College. The final section of the project illustrates sample philosophies, goals and objectives...|$|R
40|$|WEAT (1, 2) <b>devised</b> <b>a</b> <b>procedure</b> for the fluorometric {{determination}} of corticosterone and hydrocortisone in plasma. It {{is sensitive to}} {{a fraction of a}} microgram and employs chromatographic separation. In 1957, Peterson (3) published a modified version of this procedure, incorporating isotope dilution, for the determination of corticosterone in human plasma. Although both technics appear to be sensitive, specific, and accurate, for some purposes a less specific, more practical analytical method would be useful. In rat plasma, the predominant adrenal cortical steroid is known to be corticosterone (4), and only small volumes are available for analysis. In man, the predominant steroid is hydrocortisone (3, 5) and the procedures generally used require about 5 or 10 ml. of plasma for a single determination. Since both of these steroids fluoresce in sulfuric acid (1), a pro-cedure which does not separate them might be successfully applied to the plasma of man, the rat, or other species by merely using the appropriate steroid as the standard. A description of <b>a</b> <b>procedure</b> <b>which</b> can be applied with minimal effort to small volumes of plasma or to 1 or 2 rat adrenals to reflect changes in cortical steroid concentrations follows...|$|R
40|$|We {{examine the}} {{recently}} introduced NSB estimator of entropies of severely undersampled discrete variables and <b>devise</b> <b>a</b> <b>procedure</b> for calculating the involved integrals. We {{discover that the}} output of the estimator has a well defined limit for large cardinalities of the variables being studied. Thus one can estimate entropies with no a priori assumptions about these cardinalities, and a closed form solution for such estimates is given. ...|$|R
40|$|The {{subject of}} my thesis is a {{comparison}} of selected methods of measurement suitable for land in the Znojmo area, assess the appropriateness of valuation methods {{in comparison with the}} cost of actually realized by sale, <b>devise</b> <b>a</b> <b>procedure</b> for pricing and evaluating the most common method of valuation of land in the selected location. For this work I chose the land, the sale was conducted last year by one of Znojmo Realtors...|$|R
50|$|Sappey was {{a highly}} {{regarded}} anatomist remembered for his research of the lymphatic system. In 1874 he published an anatomical atlas that included a detailed study of cutaneous lymphatic drainage. He <b>devised</b> <b>a</b> <b>procedure</b> to define and delineate the lymphatic system by injecting mercury into {{the skin of a}} cadaver in order to properly view the individual lymphatic vessels. Anatomist Henri Rouvière (1876-1952) continued Sappey's anatomical work of the human lymphatic system.|$|R
5000|$|The Texas Education Agency {{specified}} {{that the parents}} and/or guardians of students zoned to a school with uniforms may apply for a waiver {{to opt out of}} the uniform policy so their children do not have to wear the uniform https://web.archive.org/web/20060908183929/http://www.tea.state.tx.us:80/field/uniforms.html; parents must specify [...] "bona fide" [...] reasons, such as religious reasons or philosophical objections. Forney ISD <b>devised</b> <b>a</b> <b>procedure</b> to verify the parents' reasons for requesting exemptions to the uniform policy http://forney.ednet10.net//prod_site/student_uniform.html ...|$|R
40|$|The Bureau of Mines <b>devised</b> <b>a</b> <b>procedure</b> for {{selectively}} extracting platinum-group metals (PGM) {{and gold}} from Stillwater Complex flotation concentrate. The Stillwater Complex {{is the only}} major U. S. PGM resource. Development of a suitable extraction technique will contribute to its exploitation. The concentrate was roasted at 1, 05020 C to convert host base-metal sulfides to oxides and the PGM from sulfide minerals to their elemental states. The roasted concentrate was preleached with dilute sulfuric acid to remove easily soluble gangue minerals. After pre-leaching, the concentrate was slurried with 6 M HCI and leached at ambient temperature and pressure with a strong-oxidizing agent. Hydrogen peroxide, chlorine, sodium hypochlorite, nitric acid, and a persulfate salt were the oxidants investigated. The two-stage leaching scheme ex-tracted up to 97 pct of the platinum, 92 pct of the palladium, and 99 pct of the gold from the roasted concentrate. The base metals were not solubilized and reported to the residue. No {{attempt was made to}} <b>devise</b> <b>a</b> <b>procedure</b> to recover the copper and nickel because they comprise less than 5 pct {{of the value of the}} concentrate. Viable techniques for recovering the precious metals from the pregnant solution were sulfide precipitation, cementation with nickel, or adsorption on activated carbon...|$|R
40|$|Detecting {{changes in}} data-streams is an impor-tant part of {{enhancing}} learning quality in dy-namic environments. We <b>devise</b> <b>a</b> <b>procedure</b> for detecting concept drifts in data-streams that re-lies on analyzing the empirical loss of learning algorithms. Our method {{is based on}} obtaining statistics from the loss distribution by reusing the data multiple times via resampling. We present theoretical guarantees for the proposed proce-dure based on {{the stability of the}} underlying learning algorithms. Experimental results show that the method has high recall and precision, and performs well in the presence of noise. 1...|$|R
40|$|We {{discuss some}} {{arguments}} {{in favour of}} the proposal that the quantum correlations contained in the pure state-vector evolving according to Schoedinger equation can be eliminated {{by the action of}} multiply connected wormholes during measurement. We <b>devise</b> <b>a</b> <b>procedure</b> to obtain <b>a</b> proper master equation which governes the changes of the reduced density matrix of matter fields interacting with doubly connected wormholes. It is shown that this master equation predicts an appropriate damping of the off-diagonal correlations contained in the state vector. Comment: 9 pages, LaTex, to appear in Int. J. Theor. Phy...|$|R
40|$|We {{analyze the}} global {{stability}} of the coexisting equilibria for several models of commensalism, first by <b>devising</b> <b>a</b> <b>procedure</b> to modify several Lyapunov functionals which were introduced earlier for corresponding models of mutualism, further confirming their usefulness. It is seen that commensalism promotes global stability, in connection with higher-order self-limiting terms which prevent unboundedness. We then use the theory of asymptotically autonomous systems to prove global stability results for models of commensalism which are subject to Allee effects, finding that commensalisms of appropriate strength can overcome the influence of strong Allee effects...|$|R
