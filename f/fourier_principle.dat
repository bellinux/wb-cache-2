3|36|Public
40|$|A new {{approach}} to analysis output current ripple from inverters is proposed. Current ripple has been analyzed by many different approach, such as time domain analysis, frequency domain analysis, and numerical method using digital simulation. Each of those method provides advantages and disadvantages itself. In this paper, frequency domain analysis is used. This method {{is based on the}} <b>Fourier</b> <b>principle</b> applied for inverter output waveform. The analysis is done by deriving the phase-to-phase mean square value of inverter’s load. Type of inverters will be analyzed in this research are three-phase inverter and six-phase inverter with dual wound stator induction motor. A technique to control the switching of these inverter is Pulse Width Modulation (PWM). Simulation also provided to verify the analytical result...|$|E
40|$|Two {{aspects of}} {{conductive}} heat are focused here (i) {{the nature of}} conductive heat, defined as that form of energy that is transferred {{as a result of}} a temperature di�erence and (ii) the nature of the intermolecular potentials that induces both thermal energy flow and the temperature profile at the steady state for a 1 -D lattice chain. It is found that the standard presuppositions of people like Benofy and Quay (BQ) following Joseph Fourier do not obtain for at least a certain specified regime of intermolecular potential parameters related to harmonic (quadratic) potentials for nearest neighbor interactions. For these harmonic potentials, it appears from the simulation results that steady state solutions exist utilizing non-synthetic thermostats that couple not just the two particles at the extreme ends of the lattice chain, but to a control volume of N particles located at either ends of the chain that does not accord with the unique analytical solutions that obtains for single particle thermostatting at the ends of the lattice with a di�erent thermostatting algorithm that utilizes coupling coe�cients. If the method used here is considered a more ”realistic” or feasible model of the physical reality, then a re-evaluation of some aspects of the standard theoretical methodology is warranted since the standard model solution profile does not accord with the simulation temperature profile determined here for this related model. We also note that the sinusoidal temperature profile generated suggests that thermal integrated circuits with several thermal P-N junctions may be constructed, opening a way to create more complex thermal transistor circuits. A stationary principle is proposed for regions that violate the <b>Fourier</b> <b>principle</b> Jq:rT � 0, where Jq is the heat current vector and T the temperature...|$|E
40|$|The Carnot {{cycle and}} its {{deduction}} of maximum conversion efficiency of heat inputted and outputted isothermally at different temperatures necessitated {{the construction of}} isothermal and adiabatic pathways within the cycle that were mechanically "reversible", leading eventually to the Kelvin-Clausius development of the entropy function S with differential dS = dq/T such that [symbol: see text]C dS = 0 where the heat absorption occurs at the isothermal paths of the elementary Carnot cycle. Another required condition is that the heat transfer processes take place infinitely slowly and "reversibly", implying that rates of transfer are not explicitly featured in the theory. The definition of 'heat' as that form of energy that is transferred {{as a result of}} a temperature difference suggests that the local mode of transfer of "heat" in the isothermal segments of the pathway implies a Fourier-like heat conduction mechanism which is apparently irreversible, leading to an increase in entropy of the combined reservoirs at either end of the conducting material, and which is deemed reversible mechanically. These paradoxes are circumvented here by first clarifying the terms used before modeling heat transfer as a thermodynamically reversible but mechanically irreversible process and applied to a one dimensional atomic lattice chain of interacting particles subjected to a temperature difference exemplifying Fourier heat conduction. The basis of a "recoverable trajectory" i. e. that which follows a zero entropy trajectory is identified. The Second Law is strictly maintained in this development. A corollary to this zero entropy trajectory is the generalization of the Zeroth law for steady state non-equilibrium systems with varying temperature, and thus to a statement about "equilibrium" in steady state non-thermostatic conditions. An energy transfer rate term is explicitly identified for each particle and agrees quantitatively (and independently) with the rate of heat absorbed at the reservoirs held at different temperatures and located at the two ends of the lattice chain in MD simulations, where all energy terms in the simulation refer to a single particle interacting with its neighbors. These results validate the theoretical model and provides the necessary boundary conditions (for instance with regard to temperature differentials and force fields) that thermodynamical variables must comply with to satisfy the conditions for a recoverable trajectory, and thus determines the solution of the differential and integral equations that are used to model these processes. These developments and results, if fully pursued would imply that not only can the Carnot cycle be viewed as describing a local process of energy-work conversion by a single interacting particle which feature rates of energy transfer and conversion not possible in the classical Carnot development, but that even irreversible local processes might be brought within the scope of this cycle, implying a unified treatment of thermodynamically (i) irreversible (ii) reversible (iii) isothermal and (iv) adiabatic processes by conflating the classically distinct concept of work and heat energy into a single particle interactional process. A resolution to the fundamental and long-standing conjecture of Benofy and Quay concerning the <b>Fourier</b> <b>principle</b> is one consequence of the analysis...|$|E
50|$|Gaussian {{functions}} centered at zero {{minimize the}} <b>Fourier</b> uncertainty <b>principle.</b>|$|R
50|$|In {{spectroscopy}} {{the term}} is used {{to indicate that the}} experiment is performed with a mixture of frequencies at once and their respective response unravelled afterwards using the <b>Fourier</b> transform <b>principle.</b>|$|R
40|$|In {{the initial}} article [Phys. Rev. Lett. 110, 044301 (2013), arXiv: 1208. 4611] it was claimed that human hearing {{can beat the}} <b>Fourier</b> {{uncertainty}} <b>principle.</b> In this Comment, we demonstrate that the experiment designed and implemented in the original article was ill-chosen to test Fourier uncertainty in human hearing. Comment: 2 pages, 1 figure, accepted to Phys. Rev. Let...|$|R
50|$|Gasera has {{developed}} {{new technologies to}} overcome drawbacks in common measurement solutions. Technology solutions are based on cantilever enhanced photoacoustic spectroscopy, <b>Fourier</b> transform infrared <b>principle</b> and widely tunable mid-infrared lasers.|$|R
50|$|This {{property}} {{is related to}} the Heisenberg uncertainty principle, but not directly - see Gabor limit for discussion. The product of the standard deviation in time and frequency is limited. The boundary of the uncertainty principle (best simultaneous resolution of both) is reached with a Gaussian window function, as the Gaussian minimizes the <b>Fourier</b> uncertainty <b>principle.</b> This is called the Gabor transform (and with modifications for multiresolution becomes the Morlet wavelet transform).|$|R
40|$|<b>Fourier</b> Transforms: <b>Principles</b> and Applications {{explains}} transform {{methods and}} their applications to electrical systems from circuits, antennas, and signal processors-ably guiding readers from vector space concepts through the Discrete Fourier Transform (DFT), Fourier series, and Fourier transform to other related transform methods.   Featuring chapter end summaries of key results, {{over two hundred}} examples and four hundred homework problems, and a Solutions Manual this book is perfect for graduate students in signal processing and communications as well as practicing engineers...|$|R
40|$|Abstract. Previously, Bennet and Feynman {{asked if}} Heisenberg’s {{uncertainty}} principle puts a limitation on a quantum computer (Quantum Mechanical Computers, Richard P. Feynman, Foundations of Physics, Vol. 16, No. 6, p 597 - 531, 1986). Feynman’s answer was negative. In this paper, we will revisit {{the same question}} for the discrete time <b>Fourier</b> transform uncertainty <b>principle.</b> We will show that the discrete time <b>Fourier</b> transform uncertainty <b>principle</b> plays {{a fundamental role in}} showing that Shor’s type of quantum algorithms has efficient running time and conclude that the discrete time uncertainty principle is an aid in our current formulation and understanding of Shor’s type of quantum algorithms. It turns out that for these algorithms, the probability of measuring an element in some set T (at the end of the algorithm) can be written in terms of the time-limiting and band-limiting operators from finite Fourier analysis. Associated with these operators is the finite <b>Fourier</b> transform uncertainty <b>principle.</b> The uncertainty principle provides a lower bound for the above probability. We will derive lower bounds for these types of probabilities in general. We will call these lower bounds quantum algorithm uncertainty principles or QAUP. QAUP are important because they give us some sense of the probability of measuring something desirable. We will use these lower bounds t...|$|R
40|$|We study {{relationships}} between different formulations {{of the local}} principle. Also we establish a connection among the local principle and the non-commutative Fourier transform approach to the investigation of convolution operator algebras. Keywords: Sectional Representations, Local <b>Principle,</b> <b>Fourier</b> Transform, Convolution Algebras. Comment: 7 pages, LaTeX 2 e, no figure...|$|R
40|$|Previously, Bennet and Feynman {{asked if}} Heisenberg's {{uncertainty}} principle puts a limitation on a quantum computer (Quantum Mechanical Computers, Richard P. Feynman, Foundations of Physics, Vol. 16, No. 6, p 597 - 531, 1986). Feynman's answer was negative. In this paper, we will revisit {{the same question}} for the discrete time <b>Fourier</b> transform uncertainty <b>principle.</b> We will show that the discrete time <b>Fourier</b> transform uncertainty <b>principle</b> plays {{a fundamental role in}} showing that Shor's type of quantum algorithms has efficient running time and conclude that the discrete time uncertainty principle is an aid in our current formulation and understanding of Shor's type of quantum algorithms. It turns out that for these algorithms, the probability of measuring an element in some set $T$ (at the end of the algorithm) can be written in terms of the time-limiting and band-limiting operators from finite Fourier analysis. Associated with these operators is the finite <b>Fourier</b> transform uncertainty <b>principle.</b> The uncertainty principle provides a lower bound for the above probability. We will derive lower bounds for these types of probabilities in general. We will call these lower bounds quantum algorithm uncertainty principles or QAUP. QAUP are important because they give us some sense of the probability of measuring something desirable. We will use these lower bounds to derive Shor's factoring and discrete log algorithms...|$|R
40|$|As {{the radio}} {{frequency}} spectrum becomes increasingly overcrowded, interference with mission-critical DSN operations is rising {{at an alarming}} rate. To alleviate this problem the DSN is developing a wideband surveillance system for on-site detection and identification of potential sources of radio frequency interference (RFI), which will complement the existing frequency coordination activities. The RFI monitoring system {{is based on a}} wideband, multi-look discrete spectrum analyzer operating on fast <b>Fourier</b> transform <b>principles.</b> An extensive general statistical analysis is presented of such spectrum analyzers and derives threshold detection performance formulas for signals of interest. These results are then applied to the design of the RFI spectrum analyzer under development...|$|R
50|$|Another {{class of}} {{reconstruction}} filters include the Gaussian for various widths, or cardinal B-splines of higher order - the box filter and tent filter being the 0th and 1st order cardinal B-splines. These filters fail to be interpolating filters, since their impulse response do not vanish at all non-zero original sample points - for 1:1 resampling, {{they are not}} the identity, but rather blur. On the other hand, being nonnegative, they do not introduce any overshoot or ringing artifacts, and by being wider in the time domain they can be narrower in the frequency domain (by the <b>Fourier</b> uncertainty <b>principle),</b> though at the cost of blurring, which is reflected in passband roll-off ("scalloping").|$|R
40|$|The {{properties}} of nonlinear devices, semiconductor diodes, were determined at high frequencies {{using the method}} of spectral characterisation. Such characterisation was carried out employing a specially developed technique where {{the components of the}} harmonic spectrum generated within these diodes at radio and microwave frequencies were measured. The theory of spectral analysis, based on <b>Fourier</b> <b>principles,</b> was reviewed. It was applied to the periodic gate function, which plays a fundamental role in signal analysis, in order to lay the foundation for the theoretical investigation carried out between pulses of known shapes and their corresponding spectra. Some useful relationships were established and applied in the evaluation of devices. Based on the fundamental {{properties of}} the periodic gate function, two new sampling procedures were introduced. The harmonic generating properties of practical diodes, where the nonlinearity in the element is an inherent condition, were examined. It was established that the spectrum generated within the device, at a particular drive level, gives the "fingerprint" of the diode, i. e. represents fully its nonlinearity. Measurement methods, both at low and high frequencies, were also discussed. The new technique, called the Multiple Reflections Resonant Line (MRRL) method was developed and described in the thesis to measure a complete spectrum. The method employed a coaxial slotted line system terminated by the device under test. The basic transmission line theory was extended to include the phenomena of multiple reflections along and resonance of, the line. The properties of the standing waves were then related to the device parameters. The twelve microwave diodes were successfully modelled which included parasitics using the new spectral technique. An attempt was made to evaluate these devices for particular applications...|$|R
40|$|An {{uncertainty}} principle due to Hardy for Fourier transform pairs on R {{says that if}} the function f is “very rapidly decreasing ” then the Fourier transform cannot also be “very rapidly decreasing unless f is indentically zero. ” In this paper we study the relevant data for G 4 and state and prove an analogue of Hardy theorem for low-dimensional nilpotent Lie groups G 4. Key words: <b>Fourier</b> transform, uncertainity <b>principle,</b> Nilpotent Lie group...|$|R
40|$|Potentials of {{infrared}} thermography in analyzing a thermal regime of the 7. 5 MeV betatron power supply are discussed. Both the heating rate and thermal inertia of particular electronic components {{have been evaluated}} by processing pixel-based temperature histories. The data treatment has been performed by using the original ThermoFit Pro software to illustrate that some advanced processing algorithms, such as the <b>Fourier</b> transform and <b>principle</b> component analysis, are valuable in identifying thermal dynamics of particular power supply parts...|$|R
40|$|Due to its versatility, Fourier-transform {{spectroscopy}} (FTS) {{has found}} wide-spread application {{in research and}} development, monitoring of industrial processes, forensics, etc 1; its uses continue to expand 2. Despite decades of development and improvement of Fouriertransform (FT) spectrometers many of their attributes still show the potential for significant improvement. Two attributes that are of major significance {{for the performance of}} FT spectrometers are the increase of spectral radiance of the light sources employed in conventional spectrometers and the increase of measurement speed. Modern superluminescent light sources offer superior levels of spectral radiance when compared to traditionally employed incandescent light sources. Due to this higher brightness, measurements with a higher signal-to-noise ration can be made. Also, the higher light throughput enables measurements at a higher repetition rate, so that the rate is ultimately limited by the dynamics of the (usually mechanical) scanning process employed in FT spectrometers. Repetition rates of up to 2 kHz can be achieved with rotating interferometers 3, ii and non-mechanical interferometers exhibiting even higher repetition rates are under development 4. An entirely different approach to high-throughput high-speed FTS was pursued by van der Weide et al. 5 - 7. It is based on heterodyne frequency-comb spectroscopy and employs two frequency combs with slightly different mode spacing. This approach, termed frequencycomb Fourier-transform spectroscopy (c-FTS), relies on expensive mode-locked lasers but enables high-throughput FTS with demonstrated repetition rates of ~ 1 kHz. However, even in this case one has to rely on moving cavity mirrors 5. This thesis presents a new approach to c-FTS that employs continuous-wave light sources (CW c-FTS). It combines the benefits of conventional FTS (broad spectral coverage, high throughput) with the following advantages: no moving parts, high repetition rate, and good signal-to-noise ratio (SNR) due to high spectral radiance sources. By engineering stable, broadband combs, CW c-FTS could result in a universal and simple approach for spectroscopy at almost arbitrary measurement speeds and spectral resolutions limited only by <b>Fourier</b> <b>principles...</b>|$|R
40|$|The thesis {{deals with}} wavelet signal {{transform}} {{with the intention}} of image signal. First part of this work contains the basic methods for signal processing. <b>Fourier</b> transformation and <b>principles</b> of wavelet signal transform is discussed, the fundamentals of segmented discrete-time wavelet transform (SegDTWT) and the segmented discrete-time wavelet 2 D signal transform (SegDTWT 2 D) derivation are described as well. Main purpose of this thesis is in the segmented discrete-time wavelet 2 D signal transform (SegDTWT 2 D) concept and its implementation which can work with two dimension signal...|$|R
40|$|Significant {{efforts to}} assess the {{bidirectional}} transmittance distribution functions (BTDFs) of complex fenestration systems (CFS) have been undertaken in recent years. This paper presents a methodology for including these photometric raw data sets into the daylighting simulation and design process. The method is based on computing the luminous intensity distribution on the inward facing side of façade elements from BTDF data (measured or calculated) and the outside luminance distribution {{as seen from the}} façade element. <b>Fourier</b> optical <b>principles</b> are applied to derive filter conditions for BTDF interpolation. Since the BTDF raw data sets are big in volume, data compression techniques are introduced and applied. Control algorithms for dynamic façade systems (i. e. time variant systems like automated blinds) are accounted for. The procedure is validated against analytical and numerical test cases. Limitations of the approach are being discussed. As an application example, the method is incorporated into lighting simulation engines and into a complex fenestration system database...|$|R
40|$|Abstract: A {{new design}} of {{laser warning system}} is proposed. This novel system is {{composed}} of an optical antenna based on fiber bundle and a spatially modulated stationary Michelson interferometer as Fourier transform device. The interferometer has no moving parts, which can distinguish laser from background noise and from other non-coherent radiation such as flares, sunlight, and lighting. Based on <b>Fourier</b> transform spectrometer <b>principle,</b> we can timely get laser source spectrum information from interference pattern. We describe the configuration and operating principle of the system in detail. The experiments show that the performances of the system are practical...|$|R
40|$|The {{method of}} Fourier {{spectroscopy}} {{requires that the}} interferogram produced by a Fourier spectrometer be inverted by a Fourier transformation. In practice, this may be done numerically by approximating to the transform by a finite <b>Fourier</b> series. The <b>principle</b> effect of this approximation is to introduce an intensity error into the resultant spectrum over and above that due to a finite spectral resolution. An investigation is made of {{the manner in which}} the Fourier series converges to a transform both analytically and by the use of illustrative numerical examples. The excess error is shown to be small in the majority of cases, but might be significant in certain circumstances...|$|R
40|$|Abstract — In this paper, {{we present}} CLAIRVOYANCE: a fully {{automatic}} photomosaicing system for building ultra highresolution composited images from image sequences captured by tailored in-house motorized active pan-tilt digital camera units. Our stand-alone mobile systems {{built over the}} past five years are computationally fast, robust to various datasets and deliver unprecedented consumer-level image quality. We describe our simple yet novel lens calibration and radiometric correction procedures based on a fast block matching algorithm. All of our core image stitching components are based on the 2 D <b>Fourier</b> phase correlation <b>principle,</b> and are thus easily amenable to hardware LSI implementation. We validate our approach by presenting sharp photomosaics obtained from a few hundreds up to a few thousands data sets of images. I...|$|R
30|$|Boundary value {{problems}} of partial differential equations concerned with temperature as the unknown may {{be solved by}} a finite Fourier transform method. The temperature at points other than the boundary, if they should be needed, {{can be obtained by}} summing the Fourier coefficients. For potential problems, the temperature at the boundary should be as accurate as possible. The finite Fourier transform method which gives the exact boundary temperature within the computer accuracy is shown to be an extremely powerful mathematical tool for the analysis of boundary value {{problems of}} partial differential equations with applications in physics. Also the finite Fourier transform method differs from the usual Fourier transformation method in that the solutions are obtained without performing the inverse <b>Fourier</b> transforms. In <b>principle,</b> the finite <b>Fourier</b> transform method may be extended to analog simulations of heat equations in three space variables, and it may also be a very efficient technique for the solution of multidimensional heat equations.|$|R
40|$|Abstract. In this paper, {{we will be}} {{concerned}} with a class of quantum algorithms that contains Shor’s factoring and discrete log algorithms. It turns out that for these algorithms, the probability of measuring an element in some set T (at {{the end of the}} algorithm) can be written in terms of the time-limiting and band-limiting operators from finite Fourier analysis. Associated with these operators is the finite <b>Fourier</b> transform uncertainty <b>principle.</b> The uncertainty principle provides a lower bound for the above probability. The main goal {{of this paper is to}} derive lower bounds for these types of probabilities. We will call these lower bounds quantum algorithm uncertainty principles or QAUP. QAUP are important because they give us some sense of the probability of measuring something desirable. We will use these lower bounds to derive Shor’s factoring and discrete log algorithms. It turns out that these two applications are quite natural and elegant. We will also show tha...|$|R
50|$|The {{quadratic}} lens intercepts {{a portion}} of this spherical wave, and refocuses it onto a blurred point in the image plane. For a single lens, an on-axis point source in the object plane produces an Airy disc PSF in the image plane. This comes about in the following way. It can be shown (see <b>Fourier</b> optics, Huygens-Fresnel <b>principle,</b> Fraunhofer diffraction) that the field radiated by a planar object (or, by reciprocity, the field converging onto a planar image) is related to its corresponding source (or image) plane distribution via a Fourier transform (FT) relation. In addition, a uniform function over a circular area (in one FT domain) corresponds to the Airy function, J1(x)/x in the other FT domain, where J1(x) is the first-order Bessel function of the first kind. That is, a uniformly-illuminated circular aperture that passes a converging uniform spherical wave yields an Airy function image at the focal plane. A graph of a sample 2D Airy function {{is shown in the}} adjoining figure.|$|R
40|$|We {{present a}} new {{approach}} to the problems of Fourier synthesis in the experimental context of aperture synthesis. Depending on what is emphasized, this method is called FIRST or WIPE:FIRST for the <b>principles</b> (<b>Fourier</b> Interpolation and Reconstruction via Shannon-type Techniques), and WIPE for the corresponding deconvolution method (WIPE reminds of CLEAN, a well-known deconvolution technique in astronomy). The regularization principle of FIRST refers to the Shannon sampling formula and to theoretical considerations related to multiresolution analysis. To describe the imaging kernel of FIRST (WIPE), the authors adopt a terminology derived from that of CLEAN. At each iteration of the selected constructive process (conjugate gradients for example), WIPE compares the dusty map with the dusty map of the model. In the corresponding truncated discrete convolution, the discrete point-spread function, the dusty beam, has two components: the traditional dirty beam and the regularization beam. Besides the clarity of the principles, the authors show, in a concrete manner, the advantages of WIPE over CLEAN. In a more general way, they also indicate how FIRST can be used for analysing the results provided by other methods...|$|R
40|$|This {{study is}} to {{construct}} a non-contact pulse automatic positioning measurement system for Traditional Chinese Medicine (TCM) using optical triangulation measurements. The system consists of a linear laser, a CMOS image sensor and image analysis software. The linear laser is projected on the pulse beat location on the wrists; the CMOS image sensor records {{the process and the}} software analyzes the images. The program mainly uses the optical centroid and fast <b>Fourier</b> transform (FFT) <b>principles</b> to calculate centroid changes (pulse amplitude changes) from the images taken by the CMOS image sensor. It returns the positions of cun, guan and chi pulses automatically in terms of the amplitudes and the signals are then transformed from the time domain (time-amplitude) into the frequency domain (frequency-amplitude) via FFT to obtain the waveforms and frequencies of the cun, guan and chi pulses. It successfully extracts the data from the TCM pulse reading and can be a medical aid system for TCM. Combining the advantages of optical measurement and computer automation, this system provides a non-contact, easy to operate, fast in detection and low-cost equipment design...|$|R
40|$|Geographical {{information}} systems (GIS) assist us in mapping and analyzing outbreaks of diseases in plants, animals and humans. This paper describes how GIS {{are being used}} to model the intensity of the outbreak of a plant virus, bean golden mosaic virus (BGMV) in Guatemala, Honduras and El Salvador. BGMV is a geminivirus affecting beans (Phaseolus vulgaris) and is transmitted by a vector, the sweet potato whitefly (Bemisia tabaci). Once a plant is infected by the virus yield losses, at varying locations, can range from 40 % to 100 %. Plant pathologists can improve upon integrated pest management strategies to monitor virus movement and outbreaks by estimating the likelihood of risk in a cropping systems. For the purpose of this analysis three techniques were selected (multivariate logistic regression, <b>Fourier</b> transform with <b>principle</b> components analysis and a multi-process boolean analysis) to predict the spatial occurrence of BGMV in beans. The methods selected are based on the location of the virus (presence/absence) and the environmental factors determining the distribution of the vector. The process involves predicting the distribution of the vector by modeling and mapping the probability of occurrence using environmental variables, such as minimum and maximum temperature ranges, elevation, rainfall and number of dry months. The results of the methods are compared, evaluated and discussed...|$|R
40|$|Course Objective : This {{course is}} {{intended}} to teach physical and mathematical principles of major medical imaging modalities {{as well as their}} usage areas. The course also aims to introduce the students about the description of various image processing algorithms and applications such as reconstruction, enhancement, segmentation, registration and compression applied to medical images. Dersin Amacı : Tıbbi görüntüleme sistemlerinin fiziksel ve matematiksel prensiplerine yönelik bilgilerin kullanim alanları ile birlikte öğretilmesi. Öğrencilerin yeniden inşa, iyileştirme, bölütleme, çakıştırma, ve sıkıştırma gibi tıbbi görüntülere uygulanan değişik görüntü işleme algoritmaları ve uygulamaları ile tanıştırılması. Ders İçeriği (İngilizce) : Medical imaging technology, systems, and modalities. Projection radiography: X-Ray systems, digital radiography. Computed tomography (CT) : principles, reconstruction methods, hardware. Magnetic resonance imaging (MRI) : mathematics, spin physics, NMR spectroscopy, <b>fourier</b> transforms, imaging <b>principles.</b> Ultrasound (US) : mathematical principles, echo equation, impulse response, diffraction, lateral and depth resolution, phased array systems, noise removal. Nuclear Medicine: positron emission tomography (PET), single photon emission computed tomography (SPECT), imaging methods, resolution, 3 -D imaging. Medical image storage, archiving and communication systems and formats: PACS; DICOM, TIFF. Image processing applications on medical images: enhancement, segmentation, registration, compression, etc. Ders İçeriği (Türkçe) : Tıbbi görüntüleme teknolojisi, sistemleri ve şekilleri. İzdüşüm radyografi: X ışınlı sistemler...|$|R
40|$|<b>Fourier</b> optics, the <b>principle</b> {{of using}} <b>Fourier</b> Transformation to {{understand}} the functionalities of optical elements, {{lies at the heart}} of modern optics, and has been widely applied to optical information processing, imaging, holography etc. While a simple thin lens is capable of resolving Fourier components of an arbitrary optical wavefront, its operation is limited to near normal light incidence, i. e. the paraxial approximation, which put a severe constraint on the resolvable Fourier domain. As a result, high-order Fourier components are lost, resulting in extinction of high-resolution information of an image. Here, we experimentally demonstrate a dielectric metasurface consisting of high-aspect-ratio silicon waveguide array, which is capable of performing Fourier transform for a large incident angle range and a broad operating bandwidth. Thus our device significantly expands the operational Fourier space, benefitting from the large numerical aperture (NA), and negligible angular dispersion at large incident angles. Our Fourier metasurface will not only facilitate efficient manipulation of spatial spectrum of free-space optical wavefront, but also be readily integrated into micro-optical platforms due to its compact size. Comment: There are some errors in the proofs. I have been trying to move the errors for months, and it seems to take some more tim...|$|R
40|$|Visual Saliency is the {{capability}} of vision system to select distinctive parts of scene and {{reduce the amount of}} visual data that need to be processed. The presentpaper introduces (1) a novel approach to detect salient regions by considering color and luminance based saliency scores using Dynamic Mode Decomposition (DMD), (2) a new interpretation to use DMD approach in static image processing. This approach integrates two data analysis methods: (1) <b>Fourier</b> Transform, (2) <b>Principle</b> Component Analysis. The key idea of our work is to create a color based saliency map. This is based on the observation thatsalient part of an image usually have distinct colors compared to the remaining portion of the image. We have exploited the power of different color spaces to model the complex and nonlinear behavior of human visual system to generate a color based saliency map. To further improve the effect of final saliency map, weutilized luminance information exploiting the fact that human eye is more sensitive towards brightness than color. The experimental results shows that our method based on DMD theory is effective in comparison with previous state-of-art saliency estimation approaches. The approach presented in this paperis evaluated using ROC curve, F-measure rate, Precision-Recall rate, AUC score etc. Comment: 8 page...|$|R
40|$|The {{research}} communities, technologies, {{and tools}} for image formation are diverse. On the one hand, computer vision and graphics researchers analyze incoherent light using coarse geometric approximations from optics. On the other hand, array signal processing and acoustics researchers analyze coherent sound waves using stochastic estimation theory and diffraction formulas from physics. The ability to inexpensively fabricate analog circuitry and digital logic for millimeter-wave radar and ultrasound creates opportunities in comparing diverse perspectives on image formation, and presents challenges in implementing imaging systems that scale in size. We present algorithms, architectures, and abstractions for image formation that relate the different communities, technologies, and tools. We address practical technical challenges in operating millimeter-wave radar and ultrasound {{systems in the}} presence of phase noise and scattering. We model a broad class of physical phenomena with isotropic point sources. We show that the optimal source location estimator for coherent waves reduces to processing an image produced by a conventional camera, provided the sources are well separated relative to the system resolution, and in the limit of small wavelength and globally incoherent light. We introduce quasi light fields to generalize the incoherent image formation process to coherent waves, offering resolution tradeoffs that surpass the traditional <b>Fourier</b> uncertainty <b>principle</b> by leveraging time-frequency distributions. We show that the number of sensors in a coherent imaging array defines a stable operating point relative to the phase noise. We introduce a digital phase tightening algorithm to reduce phase noise. We present a system identification framework for multiple-input multiple-output (MIMO) ultrasound imaging that generalizes existing approaches with time-varying filters. Our theoretical results enable the application of traditional techniques in incoherent imaging to coherent imaging, and vice versa. Our practical results suggest a methodology for designing millimeter-wave imaging systems. Our conclusions reinforce architectural principles governing transmitter and receiver design, the role of analog and digital circuity, and the tradeoff between data rate and data precision. by Anthony Accardi. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2010. This electronic version was submitted by the student author. The certified thesis is available in the Institute Archives and Special Collections. Cataloged from student submitted PDF version of thesis. Includes bibliographical references (p. 171 - 177) ...|$|R
40|$|AbstractThis {{paper is}} {{concerned}} with recent developments of wavelet schemes for the numerical treatment of operator equations with special emphasis on two issues: adaptive solution concepts and nontrivial domain geometries. After describing a general multiresolution framework the key features of wavelet bases are highlighted, namely locality, norm equivalences and cancellation properties. Assuming first that wavelet bases with these properties {{are available on the}} relevant problem domains, the relevance of these features for a wide class of stationary problems is explained in subsequent sections. The main issues are preconditioning and the efficient (adaptive) application of wavelet representations of the involved operators. We indicate then how these ingredients combined with concepts from nonlinear or best N-term approximation culminate in an adaptive wavelet scheme for elliptic selfadjoint problems covering boundary value problems as well as boundary integral equations. These schemes can be shown to exhibit convergence rates that are in a certain sense asymptotically optimal. We conclude this section with some brief remarks on data structures and implementation, interrelations with regularity in a certain scale of Besov spaces and strategies of extending such schemes to unsymmetric or indefinite problems. We address then the adaptive evaluation of nonlinear functionals of wavelet expansions as a central task arising in connection with nonlinear problems. Wavelet bases on nontrivial domains are discussed next. The main issues are the development of <b>Fourier</b> free construction <b>principles</b> and criteria for the validity of norm equivalences. Finally, we indicate possible combinations of wavelet concepts with conventional discretizations such as finite element or finite volume schemes in connection with convection dominated and hyperbolic problems...|$|R
40|$|Certain {{mathematical}} {{conditions of}} differentiation are necessarily {{included in a}} Fourier solution {{which is based on}} the hypothesis that a stress function and its partial derivatives can be represented by Fourier series. To illustrate these conditions, a summary of essential theory is followed by examples, using full-range Fourier series and also half-range Fourier series. In these examples of stress solutions it is found that the assigned boundary trac-tions provide equations sufficient to determine the Fourier coefficients. Additional mathematical conditions, however, are imposed by the hypothesis of termwise differentiation. Unless it can be proved that these additional conditions are fulfilled, the conclusion is that the hypothetical representation by Fourier series may not have sufficient generality to satisfy all required conditions and furnish a solution. Notations THE following notations are frequently used: <xn = rnr/b. 2 a = length of plate in ^-direction, in full-range example. 26 = length of plate in ^-direction, in full-range example. a, b = lengths of plate in (x, y) -directions, respectively, in half-range example. Other notations are conveniently defined where introduced. Differentiation of full-range <b>Fourier</b> series Essential <b>principles</b> of differentiation, expounded by Hobson (1), are as follows. We consider an even function F(x) and its derivative F'(x), which comply with sufficient conditions to permit representation by Fourier series in the interval (—a, a), FW = $ + 2 °»«» 0 m*. (!) F'(x) = | b'msinpmx. (2) If F(x) is a bounded function and is piecewise continuous in (—a, a), nfimxd, (3) the summation 2 referring to the points xd of ordinary discontinuity o...|$|R
40|$|Lightweight floor {{structure}} {{is widely used}} in building industries and to have better sound insulation builders come up with different ways of construction. Depending on the construction the {{floor structure}} could either be coupled (floor and ceiling coupled by beams) or decoupled (no mechanical connection between floor and ceiling). Although there are many models on coupled structure but for decoupled structure the number is not too many. Keeping that in mind the present thesis talks about lightweight floors: the construction, properties, behaviour etc {{with a focus on}} developing a model for decoupled floor structure where the core contribution being the decoupling and adding the moment effect at plate beam joints. The advantage of decoupled structure is that it disconnects the sound bridge through the beams. One consequence on the other hand is that cavity resonance dominates the low frequency region. A comparative analysis is also done with the coupled model. While developing the model this talks about different mathematical tools such as <b>Fourier</b> transform, Floquet <b>principle,</b> Poisson's sum formula etc This also gives an overview of different types of modelling technique available such as analytical, Numerical, energy based approach, empirical method etc. A parametric study is also done here to find out the relative influence of different elements on sound pressure level. Godkänd; 2010; 20100809 (sazmos); LICENTIATSEMINARIUM Ämnesområde: Teknisk akustik/Engineering Acoustics Examinator: Professor Anders Ågren, Luleå tekniska universitet Diskutant: Docent Jonas Brunskog, DTU, Department of Electrical Engineering, Denmark Tid: Tisdag den 7 september 2010 kl 14. 00 Plats: F 719 Taylor, Luleå tekniska universite...|$|R
