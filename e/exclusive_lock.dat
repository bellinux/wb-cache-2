18|54|Public
25|$|SQL Server allows {{multiple}} {{clients to}} use the same database concurrently. As such, it needs to control concurrent access to shared data, to ensure data integrity—when multiple clients update the same data, or clients attempt to read data that {{is in the process of}} being changed by another client. SQL Server provides two modes of concurrency control: pessimistic concurrency and optimistic concurrency. When pessimistic concurrency control is being used, SQL Server controls concurrent access by using locks. Locks can be either shared or exclusive. <b>Exclusive</b> <b>lock</b> grants the user exclusive access to the data—no other user can access the data as long as the lock is held. Shared locks are used when some data is being read—multiple users can read from data locked with a shared lock, but not acquire an <b>exclusive</b> <b>lock.</b> The latter would have to wait for all shared locks to be released.|$|E
50|$|Lock {{upgrades}} and downgrades {{release the}} old lock before applying the new lock. If an application downgrades an <b>exclusive</b> <b>lock</b> to a shared lock while another application is blocked {{waiting for an}} <b>exclusive</b> <b>lock,</b> the latter application may get the <b>exclusive</b> <b>lock</b> and lock the first application out. This means that lock downgrades can block, which may be counterintuitive.|$|E
5000|$|Two {{kinds of}} locks are offered: shared locks and {{exclusive}} locks. In {{the case of}} , different kinds of locks may be applied to different sections (byte ranges) of a file, or else to the whole file. Shared locks can be held by multiple processes at the same time, but an <b>exclusive</b> <b>lock</b> can only be held by one process, and cannot coexist with a shared lock. To acquire a shared lock, a process must wait until no processes hold any exclusive locks. To acquire an <b>exclusive</b> <b>lock,</b> a process must wait until no processes hold either kind of lock. Unlike locks created by , those created by [...] are preserved across s, making them useful in forking servers. It is therefore possible {{for more than one}} process to hold an <b>exclusive</b> <b>lock</b> on the same file, provided these processes share a filial relationship and the <b>exclusive</b> <b>lock</b> was initially created in a single process before being duplicated across a [...]|$|E
50|$|In {{addition}} to shared (S) <b>locks</b> and <b>exclusive</b> (X) <b>locks</b> from other locking schemes, like strict two-phase locking, MGL also uses intention shared and intention <b>exclusive</b> <b>locks.</b> IS locks conflict with X locks, while IX locks conflict with S and X locks. The null lock (NL) {{is compatible with}} everything.|$|R
5000|$|... put {{a lock on}} a resource. WebDAV {{supports}} both {{shared and}} <b>exclusive</b> <b>locks.</b>|$|R
50|$|Shared <b>locks</b> {{differ from}} <b>exclusive</b> <b>locks</b> {{in that the}} holder list can contain {{multiple}} entries. Shared locks allow all holders to read {{the contents of the}} record knowing that the record cannot be changed until after the lock has been released by all holders. <b>Exclusive</b> <b>locks</b> cannot be obtained when a record is already locked (exclusively or shared) by another entity.|$|R
50|$|A simple {{example is}} when Process 1 has {{obtained}} an <b>exclusive</b> <b>lock</b> on Resource A, and Process 2 has obtained an <b>exclusive</b> <b>lock</b> on Resource B. If Process 1 then tries to lock Resource B, {{it will have}} to wait for Process 2 to release it. But if Process 2 then tries to lock Resource A, both processes will wait forever for each other.|$|E
5000|$|Exclusive (EX). This is the {{traditional}} <b>exclusive</b> <b>lock</b> which allows read and update access to the resource, and prevents others from having any access to it.|$|E
5000|$|Write-lock (<b>exclusive</b> <b>lock)</b> is {{associated}} with a database object by a transaction (Terminology: [...] "the transaction locks the object," [...] or [...] "acquires lock for it") before writing (inserting/modifying/deleting) this object.|$|E
50|$|Java and Ada {{only have}} <b>exclusive</b> <b>locks</b> {{because they are}} thread based and rely on the compare-and-swap {{processor}} instruction.|$|R
5000|$|A {{transaction}} Ti that inserts, updates or deletes a tuple ti in {{a relation}} r must update all indices to r {{and it must}} obtain <b>exclusive</b> <b>locks</b> on all index leaf nodes affected by the insert/update/delete ...|$|R
50|$|Two {{types of}} locks are {{utilized}} by the basic protocol: Shared and <b>Exclusive</b> <b>locks.</b> Refinements {{of the basic}} protocol may utilize more lock types. Using locks that block processes, 2PL {{may be subject to}} deadlocks that result from the mutual blocking of two or more transactions.|$|R
50|$|A {{nickname}} or server jupe {{takes advantage}} of the fact that certain identifiers are unique; by using an identifier, one acquires an <b>exclusive</b> <b>lock</b> that prevents other users from making use of it.|$|E
50|$|Byte-range locking type is {{determined}} by the dwFlags parameter in the LockFileEx function used to lock a region of a file. The Windows API function LockFile can also be used and acquires an <b>exclusive</b> <b>lock</b> on the region of the file.|$|E
5000|$|More {{than one}} process can hold an {{exclusive}} [...] {{on a given}} file if the <b>exclusive</b> <b>lock</b> was duplicated across a later [...] This simplifies coding for network servers and helps prevent race conditions, but can be confusing to the unaware.|$|E
5000|$|<b>Exclusive</b> <b>locks</b> are, as {{the name}} implies, {{exclusively}} held by a single entity, usually {{for the purpose of}} writing to the record. If the locking schema was represented by a list, the holder list would contain only one entry. Since this type of lock effectively blocks any other entity that requires the lock from processing, care must be used to: ...|$|R
5000|$|To {{comply with}} the S2PL protocol, a {{transaction}} needs to comply with 2PL, and release its write (<b>exclusive)</b> <b>locks</b> only after it has ended, i.e., being either committed or aborted. On the other hand, read (shared) locks are released regularly during phase 2. This protocol is not appropriate in B-trees because it causes Bottleneck (while B-trees always starts searching from the parent root).|$|R
40|$|Data {{replications}} and transaction deadlocks can severely {{affect the}} performance of distributed database systems. Many current evaluation techniques ignore these aspects, because {{it is difficult to}} evaluate through analysis and time consuming to evaluate through simulation. Here, a technique is discussed that combines simulation and analysis to closely illustrate the impact of deadlock and evaluate performance of replicated distributed databases with both shared and <b>exclusive</b> <b>locks...</b>|$|R
50|$|A lock value {{block is}} {{associated}} with each resource. This can be read by any process that has obtained a lock on the resource (other than a null lock) and can be updated by a process that has obtained a protected update or <b>exclusive</b> <b>lock</b> on it.|$|E
50|$|If lock {{requests}} {{for the same}} entity are queued, then once a shared lock is granted, any queued shared locks may also be granted. If an <b>exclusive</b> <b>lock</b> is found next on the queue, it must wait until all shared locks have been released. As with exclusive locks, these shared locks should be held for the least time possible.|$|E
50|$|SQL Server allows {{multiple}} {{clients to}} use the same database concurrently. As such, it needs to control concurrent access to shared data, to ensure data integrity—when multiple clients update the same data, or clients attempt to read data that {{is in the process of}} being changed by another client. SQL Server provides two modes of concurrency control: pessimistic concurrency and optimistic concurrency. When pessimistic concurrency control is being used, SQL Server controls concurrent access by using locks. Locks can be either shared or exclusive. <b>Exclusive</b> <b>lock</b> grants the user exclusive access to the data—no other user can access the data as long as the lock is held. Shared locks are used when some data is being read—multiple users can read from data locked with a shared lock, but not acquire an <b>exclusive</b> <b>lock.</b> The latter would have to wait for all shared locks to be released.|$|E
5000|$|Shared locks are {{sometimes}} called [...] "read locks" [...] and <b>exclusive</b> <b>locks</b> {{are sometimes}} called [...] "write locks". However, because locks on Unix are advisory, this isn't enforced. Thus {{it is possible}} for a database to have a concept of [...] "shared writes" [...] vs. [...] "exclusive writes"; for example, changing a field in place may be permitted under shared access, whereas garbage-collecting and rewriting the database may require exclusive access.|$|R
5000|$|Microsoft Windows XP and Server 2003 editions have {{introduced}} volume snapshot (VSS) capability to NTFS, allowing open files to be accessed by backup software despite any <b>exclusive</b> <b>locks.</b> However, unless software is rewritten to specifically support this feature, the snapshot will be crash consistent only, while properly supported applications can assist {{the operating system}} in creating [...] "transactionally consistent" [...] snapshots. Other commercial software for accessing locked files under Windows include File Access Manager and Open File Manager. These work by installing their own drivers to access the files in kernel mode.|$|R
40|$|A {{number of}} {{algorithms}} {{have been proposed}} for accessing B-trees concurrently, but the performance of these algorithms is not yet well understood. In this paper, we study the performance of various concurrency control algorithms using a detailed simulation model of B-tree operations in a centralized DBMS. Our study considers {{a wide range of}} data contention situations and resource conditions. Results from our experiments indicate that algorithms in which updaters lock-couple using <b>exclusive</b> <b>locks</b> perform poorly as compared to those that permit more optimistic index descents. In particular, the B-link algorithms provide the most concurrency and the best overall performance...|$|R
5000|$|Pessimistic locking: a user {{who reads}} a record with the {{intention}} of updating it places an <b>exclusive</b> <b>lock</b> on the record to prevent other users from manipulating it. This means no one else can manipulate that record until the user releases the lock. The downside is that users can be locked out for a very long time, thereby slowing the overall system response and causing frustration.|$|E
5000|$|In MGL, locks are set on {{objects that}} contain other objects. MGL {{exploits}} the hierarchical {{nature of the}} contains relationship. For example, a database may have files, which contain pages, which further contain records. This {{can be thought of}} as a tree of objects, where each node contains its children. A lock on such as a shared or <b>exclusive</b> <b>lock</b> locks the targeted node as well as all of its descendants.|$|E
50|$|Concurrency model {{describes}} how {{changes to the}} working copy are managed to prevent simultaneous edits from causing nonsensical data in the repository. In a lock model, changes are disallowed until the user requests and receives an <b>exclusive</b> <b>lock</b> on the file from the master repository. In a merge model, users may freely edit files, but are informed of possible conflicts upon checking their changes into the repository, whereupon the version control system may merge changes on both sides, or let the user decide when conflicts arise. Note that distributed version control almost always implies a merge concurrency model.|$|E
40|$|Real-time {{collaborative}} editing {{systems are}} distributed groupware systems that allow multiple users to edit the same document {{at the same}} t ime from multiple sites. A specific type of collaborative editing system is the object-based collaborative graphics editing system. f i a-ditionally, locking has been used as the major concur-rency control techniques in this type of system. This paper examines locking i n a supporting role to the con-currency control technique of multi-versioning. Two types of locks are examined: object and region. Two optional and responsive locking schemes, instant lock-ing and instant <b>exclusive</b> <b>locking,</b> are presented. Their advantages and disadvantages are discussed. ...|$|R
40|$|Abstract: Since the {{convergence}} of transactional Web Services and workflow management, human interaction can be a determining factor {{for the length of}} a distributed business-to-business transaction. Such transactions of unknown duration (e. g. due to human interaction) can be modeled properly neither as a short-running WS-AtomicTransaction nor as a long-running WS-BusinessActivity. Our proposal is to add the concept of ready-to-commit timeouts to the <b>exclusive</b> <b>locking</b> model of the WS-AtomicTransaction protocol by making a few minor extensions. The result is a simple and straightforward implementation strategy for transactions with unknown duration based on the Web Services Transaction Framework. ...|$|R
40|$|For twenty years, the {{transaction}} has been acknowledged {{as the central}} abstraction in preventing concurrent applications from corrupting {{the contents of a}} database, through errors such as lost update, dirty read or unrepeatable read. The original concurrency control algorithm, strict two-phase locking with shared and <b>exclusive</b> <b>locks,</b> is still widely used in practice, since it is simple to implement and guarantees serializability. Many alternative algorithms have been proposed and, in commercial systems these include variants of keyrange locking to avoid phantoms, and escrow reads to improve throughput on hotspot data. New algorithms continue to appear. These algorithms are usually evaluated by simulation rather than being implemented...|$|R
50|$|In {{computer}} science, a readers-writer (RW) or shared-exclusive lock (also {{known as}} a multiple readers/single-writer lock or multi-reader lock, or push lock) is a synchronization primitive that solves one of the readers-writers problems. An RW lock allows concurrent access for read-only operations, while write operations require exclusive access. This means that multiple threads can read the data in parallel but an <b>exclusive</b> <b>lock</b> is needed for writing or modifying data. When a writer is writing the data, all other writers or readers will be blocked until the writer is finished writing. A common use might be to control access to a data structure in memory that cannot be updated atomically and is invalid (and should not be read by another thread) until the update is complete.|$|E
5000|$|Level 1 OpLocks / Exclusive Locks: When an {{application}} opens in [...] "shared mode" [...] a file hosted on an SMB server {{which is not}} opened by any other process (or other clients) the client receives an exclusive OpLock from the server. This means that the client may now assume {{that it is the}} only process with access to this particular file, and the client may now cache all changes to the file before committing it to the server. This is a performance improvement, since fewer round-trips are required in order to read and write to the file. If another client/process tries to open the same file, the server sends a message to the client (called a break or revocation) which invalidates the <b>exclusive</b> <b>lock</b> previously given to the client. The client then flushes all changes to the file.|$|E
40|$|The thesis {{consists}} of three parts. In the first part, we introduce the notion of device-cache-aware schedulers. Modern disk subsystems have many megabytes of memory for various purposes such as prefetching and caching. Current disk scheduling algorithms make decisions oblivious of the underlying device cache algorithms. In this thesis, we propose a scheduler architecture that is aware of underlying device cache. We also describe how the underlying device cache parameters can be automatically deduced and incorporated into the scheduling algorithm. In this thesis, we have only considered adaptive caching algorithms as modern high end disk subsystems are by default configured to use such algorithms. We implemented a prototype for Linux anticipatory scheduler, where we observed, compared with the anticipatory scheduler, upto 3 times improvement in query execution times with Benchw benchmark and upto 10 percent improvement with Postmark benchmark. The second part deals with implementing cooperative caching for the Redhat Global File System. The Redhat Global File System (GFS) is a clustered shared disk file system. The coordination between multiple accesses is through a lock manager. On a read, a lock on the inode is acquired in shared mode and the data is read from the disk. For a write, an <b>exclusive</b> <b>lock</b> on the inode is acquired and data is written to the disk; this requires all nodes holding the lock to write their dirty buffers/pages to disk and invalidate all the related buffers/pages. A DLM (Distributed Lock Manager) is a module that implements the functions of a lock manager. GFS’s DLM has some support for range locks, {{although it is not}} being used by GFS. While it is clear that a data sourced from a memory copy is likely to have lower latency, GFS currently reads from the shared disk after acquiring a lock (just as in other designs such as IBM’s GPFS) rather than from remote memory that just recently had the correct contents. The difficulties are mainly due to the circular relationships that can result between GFS and the generic DLM architecture while integrating DLM locking framework with cooperative caching. For example, the page/buffer cache should be accessible from DLM and yet DLM’s generality has to be preserved. The symmetric nature of DLM (including the SMP concurrency model) makes it even more difficult to understand and integrate cooperative caching into it (note that GPFS has an asymmetrical design). In this thesis, we describe the design of a cooperative caching scheme in GFS. To make it more effective, we also have introduced changes to the locking protocol and DLM to handle range locks more efficiently. Experiments with micro benchmarks on our prototype implementation reveal that, reading from a remote node over gigabit Ethernet can be upto 8 times faster than reading from a enterprise class SCSI disk for random disk reads. Our contributions are an integrated design for cooperative caching and lock manager for GFS, devising a novel method to do interval searches and determining when sequential reads from a remote memory perform better than sequential reads from a disk. The third part deals with selecting a primary network partition in a clustered shared disk system, when node/network failures occur. Clustered shared disk file systems like GFS, GPFS use methods that can fail in case of multiple network partitions and also in case of a 2 node cluster. In this thesis, we give an algorithm for fault-tolerant proactive leader election in asynchronous shared memory systems, and later its formal verification. Roughly speaking, a leader election algorithm is proactive if it can tolerate failure of nodes even after a leader is elected, and (stable) leader election happens periodically. This is needed in systems where a leader is required after every failure to ensure the availability of the system and there might be no explicit events such as messages in the (shared memory) system. Previous algorithms like DiskPaxos are not proactive. In our model, individual nodes can fail and reincarnate at any point in time. Each node has a counter which is incremented every period, which is same across all the nodes (modulo a maximum drift). Different nodes can be in different epochs at the same time. Our algorithm ensures that per epoch there can be at most one leader. So if the counter values of some set of nodes match, then there can be at most one leader among them. If the nodes satisfy certain timeliness constraints, then the leader for the epoch with highest counter also becomes the leader for the next epoch (stable property). Our algorithm uses shared memory proportional to the number of processes, the best possible. We also show how our protocol can be used in clustered shared disk systems to select a primary network partition. We have used the state machine approach to represent our protocol in Isabelle HOL logic system and have proved the safety property of the protocol...|$|E
40|$|We give a clear yet {{rigorous}} correctness proof for Moss's algorithm {{for managing}} {{data in a}} nested transaction system. The algorithm, which {{is the basis of}} concurrency control and recovery in the Argus system, uses read- and write-locks and a stack of versions of each object to ensure the serializability and recoverability of transactions accessing the data. Our proof extends earlier work on <b>exclusive</b> <b>locking</b> to prove that Moss's algorithm generates serially correct executions in the presence of concurrency and transaction aborts. The key contribution is the identification of a simple property of cead operations, called transparency, that permits shared locks to be used for read operations...|$|R
40|$|This paper {{presents}} the Reliable Multicast Protocol (RMP). RMP provides a totally ordered, reliable, atomic multicast service {{on top of}} an unreliable multicast datagram service such as IP Multicasting. RMP is fully and symmetrically distributed so that no site bears an undue portion of the communication load. RMP provides a wide range of guarantees, from unreliable delivery to totally ordered delivery, to K-resilient, majority resilient, and totally resilient atomic delivery. These QoS guarantees are selectable on a per packet basis. RMP provides many communication options, including virtual synchrony, a publisher/subscriber model of message delivery, a client/server model of delivery, an implicit naming service, mutually exclusive handlers for messages, and mutually <b>exclusive</b> <b>locks...</b>|$|R
40|$|Synchronizing {{access to}} shared data {{structures}} {{is a difficult}} problem for simulation programs. Frequently, synchronizing operations within and between simulation steps substantially curtails parallelism. This paper presents a general technique for performing this synchronization while sustaining parallelism. The technique combines fine-grained, <b>exclusive</b> <b>locks</b> with futures, a write-once data structure supporting producer-consumer parallelism. The combination allows multiple operations within a simulation step to run in parallel; further, successive simulation steps can overlap without compromising serializability or requiring roll-backs. The cumulative effect of these two sources of parallelism is dramatic: the example {{presented in this paper}} shows almost 20 -fold increase in parallelism over traditional synchronization mechanisms. 1 Introduction Simulation programs present a difficult challenge for parallel computing. Although many simulations have a large parallel component, controll [...] ...|$|R
