182|61|Public
500|$|Issues {{with the}} game's engine and [...] "a {{confused}} <b>high-level</b> <b>vision</b> for the game" [...] led the game's production {{to be transferred}} from the team in the United States to another studio in the United Kingdom, to ensure the final product would be [...] "a tighter, more focused game that will provide fans with the experience they want... a Silent Hill experience"; the release date was also pushed forward from winter 2006 to Q3/Q4 2007. The version of Origins that the United Kingdom development team received {{was intended to be}} a dark comedy inspired by the American television series Scrubs. Konami allowed the team to change the game, provided that the changes were done within the same budget and time frame; Originss script, level design, and monsters were redone within a week by Sam Barlow. For Origins, the developers intentionally replicated aspects of gameplay and atmosphere from the first installment; for example, the monsters behave more aggressively than those in previous installments, as a throwback to the first game. Later previews showed that the game had changed significantly and contained gameplay more in line with that found in the previous titles in the series. The changes were well received by video game journalists. On 19 August 2007, a demo of the game was leaked to internet download sites; Climax promptly denied they were the source of the leaked content.|$|E
5000|$|... #Subtitle level 2: Recognizing {{an object}} through <b>high-level</b> <b>vision</b> ...|$|E
50|$|Middle {{vision is}} {{the stage in}} visual {{processing}} that combines all the basic features in the scene into distinct, recognizable object groups. This stage of vision comes before <b>high-level</b> <b>vision</b> (understanding the scene) and after early vision (determining the basic features of an image). When perceiving and recognizing images, mid-level vision comes into use {{when we need to}} classify the object we are seeing. Higher-level vision is used when the object classified must now be recognized as a specific member of its group. For example, through mid-level vision we perceive a face, then through <b>high-level</b> <b>vision</b> we recognize a face of a familiar person. Mid-level vision and <b>high-level</b> <b>vision</b> are crucial for understanding a reality that is filled with ambiguous perceptual inputs.|$|E
40|$|Abstract. Classing {{object is}} an {{important}} step for the <b>high-level</b> <b>visions</b> processing tasks, such as security managing, and abnormality event analysis. In this paper, we address these challenges of abnormal water surface monitoring in real-world unconstrained environments where the background is complex and dynamic. In the algorithm proposed, we extract the moment features of water surface in HSI space, and a technique is developed to monitor the abnormal surface of water based on moment features. Experimental results show that our algorithm works efficiently and robustly...|$|R
30|$|This {{paper was}} {{motivated}} by the high-level recommendations, plans, and roadmaps introduced {{at the beginning of}} this contribution and that we formulated more clearly as one potential vision named as ‘ScienceTube’. This is not a product but rather a vision of federating the various existing approaches to the most possible degree enabling data sharing across scientific communities. We aim to implement some of these <b>high-level</b> <b>visions</b> with approaches underpinned with bottom-up activities that are scientific user-driven in order to ensure that these solutions are really used in production data infrastructures.|$|R
40|$|Real-world sensors {{suffer from}} noise, blur, and other {{imperfections}} that make <b>high-level</b> computer <b>vision</b> tasks like scene segmentation, tracking, and scene understanding difficult. Making <b>high-level</b> computer <b>vision</b> networks robust is imperative for real-world applications like autonomous driving, robotics, and surveillance. We propose a novel end-to-end differentiable architecture for joint denoising, deblurring, and classification that makes classification robust to realistic noise and blur. The proposed architecture dramatically improves {{the accuracy of}} a classification network in low light and other challenging conditions, outperforming alternative approaches such as retraining the network on noisy and blurry images and preprocessing raw sensor inputs with conventional denoising and deblurring algorithms. The architecture learns denoising and deblurring pipelines optimized for classification whose outputs differ markedly from those of state-of-the-art denoising and deblurring methods, preserving fine detail {{at the cost of}} more noise and artifacts. Our results suggest that the best low-level image processing for computer vision is different from existing algorithms designed to produce visually pleasing images. The principles used to design the proposed architecture easily extend to other <b>high-level</b> computer <b>vision</b> tasks and image formation models, providing a general framework for integrating low-level and high-level image processing...|$|R
50|$|In {{the field}} of <b>high-level</b> <b>vision</b> and attention, Tse’s group has focused on two main {{directions:}} the influence of top-down volitional operations on visual experience, {{and the nature of}} volitional mental operations that go into the construction of internal virtual experience or imagination.|$|E
5000|$|Haralick {{has made}} a series of {{contributions}} in the field of computer vision. In the <b>high-level</b> <b>vision</b> area, he has worked on inferring 3D geometry from one or more perspective projection views. He has also identified a variety of vision problems which are special cases of the consistent labeling problem. His papers on consistent labeling, arrangements, relation homomorphism, matching, and tree search translate some specific computer vision problems to the more general combinatorial consistent labeling problem and then discuss the theory of the look-ahead operators that speed up the tree search. The most basic of these is called Forward Checking. [...] This gives a framework for the control structure required in <b>high-level</b> <b>vision</b> problems. He has also extended the forward-checking tree search technique to propositional logic.|$|E
50|$|Prosopagnosia is a {{disorder}} that causes {{a person to}} be unable to identify faces. The visual system undergoes mid-level vision and identifies a face, but <b>high-level</b> <b>vision</b> fails to identify who the face belongs to. In this case, the visual system identifies an ambiguous object, a face, but is unable to resolve the ambiguity using memory, leaving the affected unable to determine who they are seeing.|$|E
40|$|The {{movements}} {{by national}} governments, funding agencies, universities, and research communities toward “open data” face many difficult challenges. In <b>high-level</b> <b>visions</b> of open data, researchers’ data and metadata practices {{are expected to}} be robust and structured. The integration of the internet into scientific institutions amplifies these expectations. When examined critically, however, the data and metadata practices of scholarly researchers often appear incomplete or deficient. The concepts of “accountability” and “transparency” provide insight in understanding these perceived gaps. Researchers’ primary accountabilities are related to meeting the expectations of research competency, not to external standards of data deposition or metadata creation. Likewise, making data open in a transparent way can involve a significant investment of time and resources with no obvious benefits. This paper uses differing notions of accountability and transparency to conceptualize “open data” as the result of ongoing achievements, not one-time acts...|$|R
40|$|Fuzzy {{set theory}} {{has been used to}} handle {{uncertainty}} in various aspects of image processing, pattern recognition and computer <b>vision.</b> <b>High-level</b> computer <b>vision</b> applications hold a great potential for fuzzy set theory because of its natural language capabilities. Scene description, a language-based interpretation of regions and their relationships, is one such application that has used fuzzy sets with some success. This paper extends our earlier and ongoing work in scene description in the following sense. If we have a linguistic description (from the system or from a human), and we revisit the scene, perhaps from a different orientation, can we match the scene objects and their relationships to be confident that we are indeed in the same place. We develop a scene matching methodology to accomplish this using histograms of forces between objects. 1...|$|R
40|$|The {{low-level}} task of foreground-background segregation is {{an important}} foundation for many <b>high-level</b> computer <b>vision</b> tasks and has been intensively researched in the past. Nonetheless, unregulated environments usually impose challenging problems, especially the difficult and often neglected underwater environment. There, among others, the edges are blurred, the contrast is impaired and the colors attenuated. Our approach to this problem uses an efficient Background Subtraction algorithm and evaluates it in combination with different spatial models...|$|R
50|$|His primary {{research}} {{interest was}} limits in information processing within <b>high-level</b> <b>vision.</b> He argued {{along with his}} collaborator Wei Ji Ma that the capacity limits commonly seen in visual short term memory and change blindness are caused, not by a high-level bottleneck in the number items that can be attended and/or stored in memory, but {{by an increase in}} neuronal noise in stimulus representations as complexity of visual information increases.|$|E
5000|$|The International Game Developers Association awarded Cerny {{with the}} Lifetime Achievement Award at the Game Developers Choice Awards (IGDA) in 2004. IGDA stated, [...] "It's rare {{to find a}} 'jack-of-all-trades' who {{not only has the}} <b>high-level</b> <b>vision</b> for great game design but can act as the glue to adhere all the pieces together. His unusual but highly {{effective}} methodology has brought us some of the most entertaining games in history." [...] He was described as [...] "a master collaborator". His Crash Bandicoot and Spyro the Dragon games have collectively sold more than 30 million units.|$|E
50|$|A Website Management Team (WMT) can {{be defined}} as an authorizing body of a website {{responsible}} for setting and achieving high-level goals for a site. This body includes content owner stakeholders and site production staff. In some organizations, a Chief Web Officer leads the WMT.Responsibilities and authorities of website staff may be grouped by strategic, tactical and operational roles, and may be organized as a cross-functional web team. A strategic site sponsor articulates the <b>high-level</b> <b>vision</b> of the site, and determines if the vision is adequately fulfilled; a tactical-level staff translates the vision into detail by prioritizing projects, specifying site design and negotiating placement of content. The tactical staff may be a group serving on a website governance board or steering team representing the main constituencies as defined by the organization's overall business plan.|$|E
50|$|The {{focus of}} Tse’s work in Cognitive Neuroscience is mid- and <b>high-level</b> human <b>vision.</b> In {{the domain of}} mid-level vision his group has worked on {{deciphering}} the rapid form-motion computations that go into the construction of subsequent conscious visual experience. His group focuses on visual illusions because they are mistakes made by the visual system that can inform us {{about the nature of}} processing that goes into the construction of conscious experience.|$|R
40|$|Article dans revue scientifique avec comité de lecture. Fuzzy set {{methods have}} been used to model and manage {{uncertainty}} in various aspects of image processing, patternr recognition and computer <b>vision.</b> <b>High-level</b> computer <b>vision</b> applications hold a great potential for fuzzy set theory because of its links to natural language. Linguistic scene description, a language-based interpretation of regions and their relationships, in one such application that is starting to bear the fruits of fuzzy set theoric involvment. In this paper, we are expanding on two earlier endeavors. We introduce new families of histograms of forces. These families preserve important relative position properties. They provide inputs to a fuzzy rule base that produces logical linguistic descriptions along with assessments as to the validity of the descriptions. Each linguistic output uses hedges from a dictionary of about 30 adverbs and other terms that can be tailored to individual users. Excellent results from several synthetic and real image examples show the applicability of this approach...|$|R
50|$|A used {{business}} operation or activity and how IS/IT supports that operation. A Solution Architecture typically applies {{to a single}} project or project release, assisting in the translation of requirements into a solution <b>vision,</b> <b>high-level</b> business and/or IT system specifications, and a portfolio of implementation tasks.|$|R
50|$|Anatomically, the {{fusiform}} gyrus {{is the largest}} macro-anatomical structure within the ventral temporal cortex, which mainly includes structures involved in <b>high-level</b> <b>vision.</b> The term {{fusiform gyrus}} (lit. „spindle-shaped convolution“) refers {{to the fact that}} the shape of the gyrus is wider at its centre than at its ends. This term is based on the description of the gyrus by Emil Huschke in 1854. (see also section on history). The fusiform gyrus is situated at the basal surface of the temporal and occipital lobes and isdelineated by the collateral sulcus (CoS) and occipitotemporal sulcus (OTS), respectively. The OTS separates the fusiform gyrus from the inferior temporal gyrus (located laterally in respect to the fusiform gyrus) and the CoS separates the fusiform gyrus from the parahippocampal gyrus (located medially in respect to the fusiform gyrus).|$|E
50|$|Goodale was {{a pioneer}} {{in the study of the}} neural {{substrates}} of visuomotor control, first in animals and later in humans. Goodale’s early work in the 1980s, in which he demonstrated that visual perception is functionally independent of the visual control of action, laid the foundation for the ‘duplex’ account of <b>high-level</b> <b>vision</b> which he developed later, together with his long-time colleague, David Milner (originally based at the University of St. Andrews but now at Durham University). In a short paper, Goodale and Milner proposed that the distinction between vision-for-perception and vision-for-action could be mapped onto the two streams of visual projections arising from early visual areas in the primate cerebral cortex: the ventral stream which projects to inferotemporal cortex and the dorsal stream which projects to the posterior parietal cortex. This account provides a convincing resolution to conflicting accounts of visual function that has characterized much of the work in the field for the last one hundred years. Over the last decade, Goodale has led much of neuroimaging and psychophysical research that has refined and extended the two-visual-systems proposal. These ideas have had an enormous influence in the life sciences and medicine. The two-visual-systems proposal is now part of almost every textbook in vision, cognitive neuroscience, and psychology.|$|E
5000|$|Issues {{with the}} game's engine and [...] "a {{confused}} <b>high-level</b> <b>vision</b> for the game" [...] led the game's production {{to be transferred}} from the team in the United States to another studio in the United Kingdom, to ensure the final product would be [...] "a tighter, more focused game that will provide fans with the experience they want... a Silent Hill experience"; the release date was also pushed forward from winter 2006 to Q3/Q4 2007. The version of Origins that the United Kingdom development team received {{was intended to be}} a dark comedy inspired by the American television series Scrubs. Konami allowed the team to change the game, provided that the changes were done within the same budget and time frame; Originss script, level design, and monsters were redone within a week by Sam Barlow. For Origins, the developers intentionally replicated aspects of gameplay and atmosphere from the first installment; for example, the monsters behave more aggressively than those in previous installments, as a throwback to the first game. Later previews showed that the game had changed significantly and contained gameplay more in line with that found in the previous titles in the series. The changes were well received by video game journalists. On 19 August 2007, a demo of the game was leaked to internet download sites; Climax promptly denied they were the source of the leaked content.|$|E
30|$|In this paper, we {{investigate}} {{the utilization of}} contextual information for pitch accent and boundary detection by using the auto-context algorithm, which was first proposed in[3] for <b>high-level</b> computer <b>vision</b> tasks like image segmentation. In this algorithm, the classification probabilities obtained from the preceding iteration are used to provide possible contextual clues, together with acoustic features to improve the next iteration. Each detection object is supported by combinations of contextual probabilities from any contextual range. Our experimental results show that this algorithm enhances detection performance for both pitch accent and boundary detection tasks.|$|R
40|$|Abstract—Fuzzy set {{methods have}} been used to model and manage {{uncertainty}} in various aspects of image processing, pat-tern recognition, and computer <b>vision.</b> <b>High-level</b> computer <b>vision</b> applications hold a great potential for fuzzy set theory because of its links to natural language. Linguistic scene description, a language-based interpretation of regions and their relationships, is one such application that is starting to bear the fruits of fuzzy set theoretic involvement. In this paper, we are expanding on two earlier endeavors. We introduce new families of fuzzy directional relations that rely on the computation of histograms of forces. These families preserve important relative position properties. They provide inputs to a fuzzy rule base that produces logical linguistic descriptions along with assessments as to the validity of the descriptions. Each linguistic output uses hedges from a dictio-nary of about 30 adverbs and other terms that can be tailored to individual users. Excellent results from several synthetic and real image examples show the applicability of this approach. Index Terms—Force histograms, fuzzy logic, linguistic descrip-tions, relative positions, scene understanding, spatial relations. I...|$|R
40|$|We propose hinge-loss Markov random fields (HL-MRFs), a {{powerful}} class of continuous-valued graphical models, for <b>high-level</b> computer <b>vision</b> tasks. HL-MRFs {{are characterized by}} log-concave density functions, {{and are able to}} perform efficient, exact inference. Their templated hinge-loss potential functions naturally encode soft-valued logical rules. Using the declarative modeling language probabilistic soft logic, one can easily define HL-MRFs via familiar constructs from first-order logic. We apply HL-MRFs to the task of activity detection, using principles of collective classification. Our model is simple, intuitive and interpretable. We evaluate our model on two datasets and show that it achieves significant lift over the low-level detectors. 1...|$|R
40|$|Conventionally, image {{denoising}} and <b>high-level</b> <b>vision</b> {{tasks are}} handled separately in computer vision, and their connection is fragile. In this paper, we {{cope with the}} two jointly and explore the mutual influence between them with the focus on two questions, namely (1) how image denoising can help solving <b>high-level</b> <b>vision</b> problems, and (2) how the semantic information from <b>high-level</b> <b>vision</b> tasks {{can be used to}} guide image denoising. First we propose a deep convolutional neural network for image denoising which is able to outperform the state-of-the-art. Second we propose a deep neural network solution that cascades two modules for image denoising and various high-level tasks, respectively, and propose the use of joint loss for updating only the denoising network to allow the semantic information flowing into the optimization of the denoising network via back-propagation. Our experimental results demonstrate that on one hand, the proposed architecture is able to overcome the performance degradation of different <b>high-level</b> <b>vision</b> tasks, e. g., image classification and semantic segmentation, due to image noise or artifacts caused by conventional denoising approaches such as over-smoothing. On the other hand, with the guidance of <b>high-level</b> <b>vision</b> information, the denoising network can further preserve more fine details and generate more visually appealing results. To the best of our knowledge, this is the first work to systematically investigate the benefit of using <b>high-level</b> <b>vision</b> semantics for image denoising via deep learning...|$|E
40|$|Covers {{the basics}} of fMRI, the {{strengths}} and limitations of fMRI compared to other techniques, and the design and analysis of fMRI experiments, focusing primarily on experiments on <b>high-level</b> <b>vision.</b> Upon completion, students {{should be able to}} understand and critique published fMRI papers, have a good grasp on what is known about <b>high-level</b> <b>vision</b> from fMRI, and design their own fMRI experiments. From the course home page: Course Description Fundamental questions about the human brain can now be answered using straightforward applications of fMRI. This is particularly true in the area of <b>high-level</b> <b>vision,</b> the study of how we interpret and use visual information (including object recognition, visual attention, perceptual awareness, visually guided action, visual memory, and other topics). Students will read, present to the class, and critique current neuroimaging articles, as well as write detailed proposals for experiments of their own. This course covers {{the basics of}} fMRI, the strengths and limitations of fMRI compared to other techniques, and the design and analysis of fMRI experiments, focusing primarily on experiments on <b>high-level</b> <b>vision.</b> Upon completion, students should be able to understand and critique published fMRI papers, have a good grasp on what is known about <b>high-level</b> <b>vision</b> from fMRI, and design their own fMRI experiments...|$|E
3000|$|Hardware {{acceleration}} {{techniques for}} machine vision systems belonging to low-, mid- and <b>high-level</b> <b>vision</b> (according to the aforementioned taxonomy) [...]...|$|E
40|$|<b>High-level</b> {{computer}} <b>vision</b> {{and natural}} language processing are thoroughly intertwined, {{with the potential to}} jointly improve performance. We propose a well-defined subset of this underexplored overlap of problems, centered around improving grounded parsing of text and object recognition in images for related pairs of images and text descriptions. We gather a new dataset and present a parsing algorithm to extract object attributes and relations from natural descriptions of images. Using ground truth data, we evaluate our performance and visualize object co-occurences and prepositions using an annotated set of images. Our results are highly encouraging, and inform our suggestions for further work. ...|$|R
40|$|Harnessing Our Ocean Wealth is an Integrated Marine Plan (IMP), {{setting out}} a roadmap for the Government’s <b>vision,</b> <b>high-level</b> goals and {{integrated}} actions across policy, governance and business to enable our marine {{potential to be}} realised. Implementation of this Plan will see Ireland evolve an integrated system of policy and programme planning for our marine affairs...|$|R
40|$|Abstract-Relational {{models are}} {{frequently}} used in <b>high-level</b> com-puter <b>vision.</b> Finding a correspondence between a relational model and an image description {{is an important}} operation {{in the analysis of}} scenes. In this paper the process of finding the correspondence is formalized by defining a general relational distance measure that computes a numeric distance between any two relational descriptions-a model and an image description, two models, or two image descriptions. The dis-tance measure is proved to be a metric, and is illustrated with examples of distance between object models. A variant measure used in our past studies is shown not to be a metric. Index Terms-Matching, metric, relational distance, structural description. I...|$|R
3000|$|The Sub-Algorithm  2 {{provides}} a <b>high-level</b> <b>vision</b> {{of how the}} price competition is simulated. This sub-algorithm calls to procedure calls ”DemandPrediction“ [...]...|$|E
40|$|Most of {{the current}} {{boundary}} detection systems rely exclusively on low-level features, such as color and texture. However, perception studies suggest that humans employ object-level reasoning when judging if a particular pixel is a boundary. Inspired by this observation, in this work we show how to predict boundaries by exploiting object-level features from a pretrained object-classification network. Our method {{can be viewed as}} a "High-for-Low" approach where high-level object features inform the low-level boundary detection process. Our model achieves state-of-the-art performance on an established boundary detection benchmark and it is efficient to run. Additionally, we show that due to the semantic nature of our boundaries we can use them to aid a number of <b>high-level</b> <b>vision</b> tasks. We demonstrate that using our boundaries we improve the performance of state-of-the-art methods on the problems of semantic boundary labeling, semantic segmentation and object proposal generation. We can view this process as a "Low-for-High" scheme, where low-level boundaries aid <b>high-level</b> <b>vision</b> tasks. Thus, our contributions include a boundary detection system that is accurate, efficient, generalizes well to multiple datasets, and is also shown to improve existing state-of-the-art <b>high-level</b> <b>vision</b> methods on three distinct tasks...|$|E
40|$|In {{this section}} we will {{introduce}} some terminology related to computer vision. Terms such as computational vision, image processing, computer vision, image understanding, low-level vision, intermediate-level vision, <b>high-level</b> <b>vision,</b> top-down analysis, bottom-up analysis, and other terms {{are often used}} to describe some aspect o...|$|E
40|$|Part {{detectors}} are {{a common}} way to handle the variability in appearance in <b>high-level</b> computer <b>vision</b> problems, such as detection and semantic segmentation. Identifying good parts, however, remains an open question. Anatomical parts, such as arms and legs, are difficult to detect reliably because parallel lines are common in natural images. In contrast, a visual conjunction such as "half of a frontal face and a left shoulder" may be a perfectly good discriminative visual pattern. We propose a new computer vision part, called a poselet, which is trained {{to respond to a}} given part of the object at a given viewpoint and pose. There is a wide variety of poselets [...] a frontal face, a profile face, a head-and-shoulder configuration, etc. A requirement for training poselets is that the visual correspondence of object parts in the training images be provided. We create a new dataset, H 3 D, in which we annotate the locations of keypoints of people, infer their 3 D pose and label their parts (the face, hair, upper clothes, etc.). Our richly annotated dataset allows for creation of poselets as well as other queries not possible with traditional datasets. To train a poselet associated with a given image patch, we find other patches that have the same local configuration of keypoints and use them as positive training examples. We use HOG features and linear SVM classifiers. The resulting poselet is trained to recognize the visual patterns associated with the given local configuration of keypoints, which, in turn, makes it respond to a specific pose under a specific viewpoint regardless of the variation in appearance. <b>High-level</b> computer <b>vision</b> is challenging because the image is a function of multiple somewhat independent factors, such as the appearance model of the object, its pose, and the camera viewpoint. Poselets allow us to "untie the knot", i. e. decouple the pose from the appearance and model them separately. We show that this property helps in a variety of <b>high-level</b> computer <b>vision</b> tasks. Our person detector based on poselets is the leading method on the PASCAL VOC 2009 and 2010 person detection competitions and naturally extends to other visual classes. We currently have the best semantic segmentation engine for person and several other categories on the PASCAL 2010 segmentation datasets. We report competitive performance for pose and action recognition and we are the first method to do attribute classification for people under any viewpoint and pose...|$|R
40|$|Discriminative tasks, {{including}} object categorization and detection, {{are central}} components of <b>high-level</b> computer <b>vision.</b> However, sometimes {{we are interested}} in a finer-grained characterization of the object’s properties, such as its pose or articulation. In this paper we develop a probabilistic method (LOOPS) that can learn a shape and appearance model for a particular object class, and be used to consistently localize constituent elements (landmarks) of the object’s outline in test images. This localization effectively projects the test image into an alternative representational space that makes it particularly easy to perform various descriptive tasks. We apply our method to a range of object classes in cluttered images and demonstrate its effectiveness in localizing objects and performing descriptive classification, descriptive ranking, and descriptive clustering...|$|R
40|$|A split-and-merge {{framework}} {{based on}} a maximum variance criterion is proposed for disparity clustering. The proposed algorithm transforms low-level stereo disparity information to mid-level planar surface information {{which can be used}} further to carry out <b>high-level</b> computer <b>vision</b> tasks such as shape classification. Unlike conventional clustering, the proposed algorithm assumes that the number of clusters is unknown. Instead, a maximum variance criterion is applied to extract planar surfaces from the disparity image. The split phase of the algorithm creates clusters based on spatial continuity and the merge phase combines these clusters such that variance per cluster does not exceeded an allowable value. For efficient maximum variance clustering, a greedy branch-and-bound procedure is introduced. Efficiency of the approach is verified through experiments. 1...|$|R
