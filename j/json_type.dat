5|19|Public
5000|$|... ezboard used a {{flat file}} system {{consisting}} of binary objects. It had no relational database. Attempts {{were made to}} migrate the database to a traditional RDBMS but attempts failed {{because they were too}} slow. ezboard was an early form of a NoSQL style database as the objects were essentially <b>JSON</b> <b>type</b> data (key-value pairs). The flat files were stored in deep nested directory structures ...|$|E
40|$|International audienceRecent {{years have}} seen the {{widespread}} use of JSON as a data format to represent massive data collections. JSON data collections are usually schemaless. While this ensures several advantages, the absence of schema information has important negative consequences: the correctness of complex queries and programs cannot be statically checked, users cannot rely on schema information to quickly figure out structural properties that could speed up the formulation of correct queries, and many schema-based optimizations are not possible. In this paper we deal with the problem of inferring a schema from massive JSON data sets. We first identify a <b>JSON</b> <b>type</b> language which is simple and, at the same time, expressive enough to capture irregularities and to give complete structural information about input data. We then present our main contribution, which is the design of a schema inference algorithm, its theoretical study and its implementation based on Spark, enabling reasonable schema inference time for massive collections. Finally, we report about an experimental analysis showing the effectiveness of our approach in terms of execution time, precision and conciseness of inferred schemas, and scalability...|$|E
40|$|Do {{you ever}} wonder if and how two {{well-known}} people are related? Do {{you ever wonder}} {{how to find the}} relationships among people using a simple and effective way? We proposed to tackle and solve this problem by providing a system with a user-friendly and Google-like interface which can easily generate the answer to this type of questions. With the help of SemanticProxy. com, {{which is part of the}} OpenCalais Initiative 1, we are able to obtain <b>JSON</b> <b>type</b> data which contains a list of relations between two objects which is similar to rdf data model on a given webpage link. From the list, Java parser is used to extract the relations among people and then recursively generate the relations for each person, by querying individual Wikipedia page, until the two given persons ’ names from the input boxes on the GUI (Graphical User Interface) are found. The system is capable of, if each person’s Wikipedia page exists, dynamically extracting relationships for individuals located on multiple Wikipedia pages using the concept of semantic web and information retrieval...|$|E
50|$|Clutter {{can build}} user {{interfaces}} using a specialized JSON dialect. The entire scene graph is defined using <b>JSON</b> <b>types</b> and built at run time through the ClutterScript class.|$|R
5000|$|Data {{structure}} agnostic and type-rich database, handles {{variable data}} structure XML or JSON documents {{in a single}} database. Supports unstructured textual data, dates, numbers, meta-data (all XML and <b>JSON</b> <b>types)</b> ...|$|R
5000|$|BSON {{types are}} {{nominally}} a superset of <b>JSON</b> <b>types</b> (<b>JSON</b> {{does not have}} a date or a byte array type, for example), with one exception of not having a universal [...] "number" [...] <b>type</b> as <b>JSON</b> does.|$|R
40|$|Recent {{years have}} seen the {{widespread}} adoption of JSON as a data format to represent massive data collections managed and analysed by crucial applications. JSON data collections are usually schemaless, allowing thus for a flexible management of data. However, the absence of schema information has several disadvantages: the correctness of complex queries and programs cannot be statically checked, users {{have no way to}} figure out structural properties of the underlying data, and, more generally, schema-based optimisations cannot be applied. In this paper we deal with the problem of inferring a schema from massive JSON datasets. Our first contribution is the identification and definition of a <b>JSON</b> <b>type</b> language, which is a good compromise between simplicity and the ability of capturing complex structural properties of the input data. Our second contribution is the design of a schema inference algorithm and its implementation on Spark, in order to ensure a reasonable schema inference time for massive collections. Finally, we report about a preliminary experimental analysis showing the effectiveness of our approach in terms of precision and conciseness of inferred schemas...|$|E
40|$|In {{the recent}} years JSON affirmed {{as a very}} popular data format for {{representing}} massive data collections. JSON data collections are usually schemaless. While this ensures sev- eral advantages, the absence of schema information has im- portant negative consequences: the correctness of complex queries and programs cannot be statically checked, users cannot rely on schema information to quickly figure out the structural properties that could speed up the formulation of correct queries, and many schema-based optimizations are not possible. In this paper {{we deal with the}} problem of inferring a schema from massive JSON datasets. We first identify a <b>JSON</b> <b>type</b> language which is simple and, at the same time, expressive enough to capture irregularities and to give com- plete structural information about input data. We then present our main contribution, which is the design of a schema inference algorithm, its theoretical study, and its implemen- tation based on Spark, enabling reasonable schema infer- ence time for massive collections. Finally, we report about an experimental analysis showing the effectiveness of our ap- proach in terms of execution time, precision, and conciseness of inferred schemas, and scalability...|$|E
5000|$|SQL-like query {{language}} with adjustments to match <b>JSON</b> data <b>types.</b>|$|R
5000|$|The nebulous <b>JSON</b> 'number' <b>type</b> is {{strictly}} defined in Ion {{to be one}} of ...|$|R
50|$|Clusterpoint {{database}} has multi-master shared-nothing, distributed, document-oriented database architecture storing XML and <b>JSON</b> data <b>types.</b>|$|R
50|$|In {{addition}} to supporting <b>JSON</b> for <b>type</b> and protocol definitions, Avro includes experimental {{support for an}} alternative interface description language (IDL) syntax known as Avro IDL. Previously known as GenAvro, this format is designed to ease adoption by users familiar with more traditional IDLs and programming languages, with a syntax similar to C/C++, Protocol Buffers and others.|$|R
50|$|The type is a 1-byte ASCII {{character}} used {{to indicate}} the type of the data following it. The ASCII characters were chosen to make manually walking and debugging data stored in the UBJSON format as easy as possible (e.g. making the data relatively readable in a hex editor). Types are available for the five <b>JSON</b> value <b>types.</b> There is also a no-op type used for stream keep-alive.|$|R
5000|$|JSON Patch is a web {{standard}} format for describing {{changes in a}} JSON document. It {{is meant to be}} used together with HTTP Patch which allows for the modification of existing HTTP resources. The <b>JSON</b> Patch media <b>type</b> is [...]|$|R
5000|$|Part-6: Support for JavaScript Object Notation (JSON). In 2017 ISO/IEC {{published}} a first technical {{report about the}} effort to integrate the data <b>type</b> <b>JSON</b> into the SQL standard. Please consider that technical reports reflects {{the current state of}} the discussion and are not part of the standard.|$|R
50|$|Clusterpoint {{database}} enables {{to perform}} transactions in a distributed document database {{model in the}} same way as in a SQL database. Users can perform secure real-time updates, free text search, analytical SQL querying and reporting at high velocity in very large distributed databases containing XML or <b>JSON</b> document <b>type</b> data. Transactions are implemented without database consistency issues plaguing most of NoSQL databases and can safely run at high-performance speed previously available only with relational databases. Real time Big data analytics, replication, loadsharing and high-availability are standard features of Clusterpoint database software platform.|$|R
50|$|There exist {{several other}} {{approaches}} for {{the representation of}} tree-structured data, be it XML, JSON or other formats, such as the nested set model, in a relational database. On the other hand, database vendors have begun to include JSON and XML support into their data structures and query features, like in IBM DB2, where XML data is stored as XML separate from the tables, using Xpath queries as part of SQL statements, or in PostgreSQL, with a <b>JSON</b> data <b>type</b> that can be indexed and queried. These developments accomplish, improve or substitute the EAV model approach.|$|R
5000|$|Support {{only for}} pure <b>JSON</b> data <b>types.</b> Most notably, DocumentDB lacks support for date-time data {{requiring}} that you store this data using {{the available data}} types. For instance, it can be stored as an ISO-8601 string or epoch integer. MongoDB, the database to which DocumentDB is most often compared, extended JSON in their BSON binary serialization specification to cover date-time data as well as traditional number types, regular expressions, and Undefined. However, many argue that DocumentDB's choice of pure JSON is actually an advantage as it's a better fit for JSON-based REST APIs and the JavaScript engine built into the database.|$|R
40|$|With {{the release}} of a new {{generation}} intelligent phone Android and global 3 G network into operation, the communications between intelligent phone platform and database server become more and more important. But due to the constraints of the mobile phone platform memory capacity, friendly interface and transmission cost, the communication mechanism will be more stringent requirements. The framework of communication mechanism, which this paper studied, used the light-weight <b>type</b> <b>JSON</b> (JavaScript Object Notation) data format as the transmission medium and used the design patterns of MVC (model-view-controller). Then the mobile phone client will be a user interface to interact with the users and all the data processing will be done in the server, so that the communication mechanism will be optimized to achieve...|$|R
30|$|Messages {{carrying}} application {{specific data}} are still uninterpreted and sent as raw bytes by the system, but their type indicates {{that they are}} application messages. By convention, commands encode them using <b>JSON,</b> sending a <b>type</b> name as a prefix. They permit programs in a pipeline to send extra information along with the data being processed. As an example, early versions of the framework used such messages to forward file addresses (file names and line numbers) from programs locating lines. Such addresses were consumed by other programs running later in the same pipeline. Since file addresses became often used, they were promoted to a well-known message type, as a convenience. Application specific messages are also used by some text processing filters to select part of the data flowing through the stream by sending all other data as strings. Such ignored data may be promoted to actual data at later stages of the pipeline. There are examples of this in Section  7.|$|R
40|$|Spark SQL {{is a new}} module in Apache Spark that {{integrates}} rela-tional processing with Spark’s {{functional programming}} API. Built on our experience with Shark, Spark SQL lets Spark program-mers leverage the benefits of relational processing (e. g., declarative queries and optimized storage), and lets SQL users call complex analytics libraries in Spark (e. g., machine learning). Compared to previous systems, Spark SQL makes two main additions. First, it offers much tighter integration between relational and procedural processing, through a declarative DataFrame API that integrates with procedural Spark code. Second, it includes a highly extensible optimizer, Catalyst, built using features of the Scala programming language, that {{makes it easy to}} add composable rules, control code generation, and define extension points. Using Catalyst, we have built a variety of features (e. g., schema inference for <b>JSON,</b> machine learning <b>types,</b> and query federation to external databases) tailored for the complex needs of modern data analysis. We see Spark SQL as an evolution of both SQL-on-Spark and of Spark itself, offering richer APIs and optimizations while keeping the benefits of the Spark programming model...|$|R
40|$|International audienceActorScript(TM) is {{a general}} purpose {{programming}} language for efficiently implementing robust applications (with no single point of failure) using discretionary, adaptive concurrency that manages resources and demand. It is differentiated from previous languages by the following:- Universality* Ability to specify what Actors can do* Specify interface between hardware and software* Everything in the language is accomplished using message passing including {{the very definition of}} ActorScript itself* Functional, Imperative, Logic, and Concurrent programming are integrated. * Concurrency dynamically adapts to resources available and current load. * Programs do not expose low-level implementation mechanisms such as threads, tasks, locks, cores, etc. * Messages can be directly communicated without requiring indirection through brokers, channels, class hierarchies, mailboxes, pipes, ports, queues etc. * Variable races are eliminated. * Binary XML and <b>JSON</b> are data <b>types.</b> * Application binary interfaces are afforded so that no identifier symbol need be looked up at runtime. - Safety and Security* Programs are extension invariant, i. e., extending a program does not change its meaning. * Applications cannot directly harm each other. - Performance* Impose no overhead on implementation of Actor systems* Message passing has essentially same overhead as procedure calling and looping. * Allow execution to be dynamically adjusted for system load and capacity (e. g. cores) * Locality because execution is not bound by a sequential global memory model* Inherent concurrency because execution is not bound by communicating sequential processes* Minimize latency along critical path...|$|R
40|$|This paper investigates {{language}} constructs for high-level and type-safe {{manipulation of}} JSON objects in a typed functional language. A major obstacle in representing JSON in a static type system is their heterogeneous nature: in most practical JSON APIs, a JSON array is a heterogeneous list consisting of, for example, objects having common fields and possibly some optional fields. This paper presents a typed calculus that reconciles static typing constraints and heterogeneous JSON arrays {{based on the}} idea of partially dynamic records originally proposed and sketched by Buneman and Ohori for complex database object manipulation. Partially dynamic records are dynamically typed records, but some parts of their structures are statically known. This feature enables us to represent <b>JSON</b> objects as <b>typed</b> data structures. The proposed calculus smoothly extends with ML-style pattern matching and record polymorphism. These results yield a typed functional language where the programmer can directly import JSON data as terms having static types, and can manipulate them with the full benefits of static polymorphic type-checking. The proposed calculus has been embodied in SML#, an extension of Standard ML with record polymorphism and other practically useful features. This paper also reports on the details of the implementation and demonstrates its feasibility through examples using actual Web APIs. The SML# version 3. 1. 0 compiler includes JSON support presented in this paper, and is available from Tohoku University as open-source software under a BSD-style license...|$|R
40|$|ActorScript(TM) is {{a general}} purpose {{programming}} language for implementing discretionary, adaptive concurrency that manages resources and demand. It is differentiated from previous languages by the following: - Universality *** Ability to specify what Actors can do *** Specify interface between hardware and software *** Everything in the language is accomplished using message passing including {{the very definition of}} ActorScript itself *** Functional, Imperative, Logic, and Concurrent programming are integrated. *** Concurrency dynamically adapts to resources available and current load. *** Programs do not expose low-level implementation mechanisms such as threads, tasks, locks, cores, etc. *** Messages can be directly communicated without requiring indirection through brokers, channels, class hierarchies, mailboxes, pipes, ports, queues etc. *** Variable races are eliminated. *** Binary XML and <b>JSON</b> are data <b>types.</b> *** Application binary interfaces are afforded so that no identifier symbol need be looked up at runtime. - Safety and Security *** Programs are extension invariant, i. e., extending a program does not change its meaning. *** Applications cannot directly harm each other. - Performance *** Impose no overhead on implementation of Actor systems *** Message passing has essentially same overhead as procedure calling and looping. *** Allow execution to be dynamically adjusted for system load and capacity (e. g. cores) *** Locality because execution is not bound by a sequential global memory model *** Inherent concurrency because execution is not bound by communicating sequential processes *** Minimize latency along critical pathsComment: Added explanation of facets of an Actor. Admin note: text overlap with arXiv: 1008. 145...|$|R

