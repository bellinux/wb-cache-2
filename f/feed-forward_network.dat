288|1952|Public
50|$|The NEAT {{approach}} {{begins with}} a perceptron-like <b>feed-forward</b> <b>network</b> of only input neurons and output neurons. As evolution progresses through discrete steps, {{the complexity of the}} network's topology may grow, either by inserting a new neuron into a connection path, or by creating a new connection between (formerly unconnected) neurons.|$|E
50|$|A synfire chain (synchronous firing chain) is a <b>feed-forward</b> <b>network</b> {{of neurons}} with {{multiple}} layers or pools. In a synfire chain, neural impulses propagate synchronously {{back and forth}} from layer to layer. Each neuron in one layer feeds excitatory connections to neurons in the next, while each neuron in the receiving layer is excited by neurons in the previous layer.|$|E
50|$|In the {{mathematical}} theory of artificial neural networks, the universal approximation theorem states that a <b>feed-forward</b> <b>network</b> {{with a single}} hidden layer containing {{a finite number of}} neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of Rn, under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters.|$|E
40|$|Connectionist <b>feed-forward</b> <b>networks,</b> {{trained with}} back-propagation, {{can be used}} both for {{nonlinear}} regression and for (discrete one-of-C) classification, depending on the form of training. This report contains two papers on <b>feed-forward</b> <b>networks.</b> The papers can be read independently. They are intended for the theoretically-aware practitioner or algorithm-designer; however, they also contain a review and comparison of several learning theories so they provide a perspective for the theoretician. The first paper works through Bayesian methods to complement back-propagation in the training of <b>feed-forward</b> <b>networks.</b> The second paper addresses a problem raised by the first: how to efficiently calculate second derivatives on <b>feed-forward</b> <b>networks...</b>|$|R
3000|$|ANNs can {{be viewed}} as {{weighted}} directed graphs in which directed edges with weights are used to connect artificial neuron nodes. ANNs can be categorized as <b>feed-forward</b> <b>networks</b> and recurrent <b>networks.</b> Properties of <b>feed-forward</b> <b>networks</b> are: [...]...|$|R
40|$|Recent {{research}} has focused on <b>feed-forward</b> <b>networks</b> with complex weights and activation values such as [GK 92, Hir 92 b, Hir 92 a, Hir 93]. This paper extends this formalism to <b>feed-forward</b> <b>networks</b> with weight and activation values taken from a Clifford algebra (see also [PB 92, PB 94 b]). A Clifford algebra is a multi-dimensional generalization of the complex numbers and the Quaternions. Essentially a Clifford algebra is obtained by extending vector spaces to allow an associative multiplication compatible with the natural metric on the vector space. This paper presents an extension of the well known back-error propagation algorithm to Clifford valued <b>feed-forward</b> <b>networks,</b> and presents some experimental results with simple encoder-decoder problems. A discussion of the difference between real and Clifford valued networks is also included. Finally a Universal Approximation similar to the results found in [HSW 89] is proved. 1 Introduction Most current research into neural network [...] ...|$|R
50|$|The {{simplest}} kind {{of neural}} network is a single-layer perceptron network, {{which consists of}} a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. In this way it can be considered the simplest kind of <b>feed-forward</b> <b>network.</b> The sum of {{the products of the}} weights and the inputs is calculated in each node, and if the value is above some threshold (typically 0) the neuron fires and takes the activated value (typically 1); otherwise it takes the deactivated value (typically -1). Neurons with this kind of activation function are also called artificial neurons or linear threshold units. In the literature the term perceptron often refers to networks consisting of just one of these units. A similar neuron was described by Warren McCulloch and Walter Pitts in the 1940s.|$|E
40|$|A <b>feed-forward</b> <b>network</b> can {{be viewed}} as a {{graphical}} representation of parametric function which takes a set of input values and maps them to a corresponding set of output values (Bishop, 1995). Figure 1 shows an example of a <b>feed-forward</b> <b>network</b> of a kind that is widely used in practical applications. Nodes in the bias z 0 bias y 1 z 1 outputs y c x 0 x 1 xd inputs hidden units Figure 1 : A <b>feed-forward</b> <b>network</b> having two layers of adaptive parameters. graph represent either inputs, outputs or ‘hidden ’ variables, while the edges of the graph correspond to the adaptive parameters. We can write down the analytic function corresponding to this network follows...|$|E
40|$|In {{this paper}} we {{investigate}} the feed-forward learning problem. The well-known ill-conditioning which {{is present in}} most feed-forward learning problems is shown {{to be the result}} of the structure of the network. Also, the well-known problem that weights between `higher' layers in the network have to settle before `lower' weights can converge is addressed. We present a solution to these problems by modifying the structure of the network through the addition of linear connections which carry shared weights. We call the new network structure the linearly augmented <b>feed-forward</b> <b>network,</b> and it is shown that the universal approximation theorems are still valid. Simulation experiments show the validity of the new method, and demonstrate that the new network is less sensitive to local minima and learns faster than the original network. Keywords: conditioning, local minima, <b>feed-forward</b> <b>network,</b> learning problem, linearly augmented <b>feed-forward</b> <b>network,</b> conjugate gradient learning. 1 Introduc [...] ...|$|E
40|$|In {{this paper}} a novel scheme is {{proposed}} for training <b>feed-forward</b> <b>networks</b> in an ensemble. In this approach {{we do not}} only minimize the error of each network on the training set, but we also reduce the correlation of the errors among the networks. Experimental results on various datasets show that the performance of bagging (i. e., taking the unweighted mean of the networks) improves {{when compared to the}} traditional training scheme. 1 Introduction Recently ensembles of <b>feed-forward</b> <b>networks</b> receive a lot of attention. In this approach multiple <b>feed-forward</b> <b>networks</b> which are trained for the same problem using dierent initial weight congurations, optimization procedures, training sets, or even architectures are combined into a joined forecast. This forecast, a linear combination of the network outputs, has a lower error variance in comparison to single networks (for the statistical background, see [2]) and experimental results show that this technique gives a signicant improve [...] ...|$|R
40|$|Abstract:- The paper {{presents}} (on {{the basis}} of passive investment strategies analysis) {{the design of the}} Takagi-Sugeno fuzzy inference system and the <b>feed-forward</b> neural <b>network</b> (with pre-processing of inputs time series) for prediction of the index fund. By means of the Takagi-Sugeno fuzzy inference system and the <b>feed-forward</b> neural <b>network</b> the investor is able to predict the closing price of the index fund. Key-Words:- Takagi-Sugeno fuzzy inference systems, <b>feed-forward</b> neural <b>network,</b> prediction, index fund, indicator...|$|R
40|$|<b>Feed-forward</b> neural <b>networks</b> are {{commonly}} used for pattern classification. The classification accuracy of <b>feed-forward</b> neural <b>networks</b> depends on: a) the configuration selected and b) the training process. Once {{the architecture of the}} network is decided, training algorithms, usually gradient descent techniques, are used to determine the connection weights of the <b>feed-forward</b> neural <b>network.</b> However, gradient descent techniques often get trapped in local optima of the search landscape. To address this issue, an ant colony optimization (ACO) algorithm is applied to train <b>feed-forward</b> neural <b>networks</b> for pattern classification in this paper. In addition, the ACO training algorithm is hybridized with gradient descent training. Both standalone and hybrid ACO training algorithms are evaluated on several benchmark pattern classification problems, and compared with other swarm intelligence, evolutionary and traditional training algorithms. The experimental results show the efficiency of the proposed ACO training algorithms for <b>feed-forward</b> neural <b>networks</b> for pattern classification...|$|R
40|$|In this thesis, {{the storage}} {{capacities}} of the Bidirectional Associative Memories (BAM) and the Hopfield {{network and the}} applications of the multi-layer <b>feed-forward</b> <b>network</b> are studied and presented. There are four sub-topics in this thesis, {{the first one is}} the use of ring and cascade architectures in storing temporal or ordered patterns. A comparison of these architectures is presented. The second sub-topic is to use some new methods to increase the storage capacities of the BAM and the Hopfield network. The uses of a modified Hebb rule and multi-threshold values are studied. Comparisons among the Hebb rule, the delta rule and these new methods are also presented. Results show that these new methods can store ten 35 -pixel images while the Hebb rule cannot. The third sub-topic is the use of a multi-layer <b>feed-forward</b> <b>network</b> with the back-propagation training to do pulse compression. Results of this approach are presented. Lastly, the use of a multi-layer <b>feed-forward</b> <b>network</b> to do filtering is also studied and the results are presented. Source: Masters Abstracts International, Volume: 30 - 03, page: 0840. Supervisor: H. K. Kwan. Thesis (M. A. Sc.) [...] University of Windsor (Canada), 1990...|$|E
40|$|Data {{available}} for training a neural network may be deficient {{not only in}} quantity of data but entire independent variables with their data may be missing such {{as is often the}} situation for software engineering data. This may cause the relation based on the available data to exhibit the property of one-to-many (o-m) valuedness or almost o-m valuedness. Multiplayer perceptrons or <b>feed-forward</b> <b>network</b> however are generally trained to represent functions or m-o mappings. The solution consists of adding another input to the standard <b>feed-forward</b> <b>network</b> and of modifying the training algorithm to allow for determination of this input for which no training data is available. If the values for the additional input are restricted to a discrete set then they may be perceived as cluster identifiers and the training method may be perceived as another form of clustering or segmentation of the input. If the missing input variable is assumed to have a finite number of values the method proposed here may be compared to mixture of experts and mixture density networks except that the proposed method is more direct solution. The modified <b>feed-forward</b> <b>network</b> and training method has been successfully applied to several examples...|$|E
40|$|It has {{previously}} been known that a <b>feed-forward</b> <b>network</b> with time-delay can be unfolded into a conventional <b>feed-forward</b> <b>network</b> with a time history as input. In this paper, We show explicitly how this unfolding operation can occur, with a newly defined Network Unfolding Algorithm (NUA) that involves creation of virtual units and moving all time delays to a preprocessing stage consisting of the time histories. The NUA provides a tool for analyzing {{the complexity of the}} ATNN. From this tool, we concluded that the ATNN reduces the cost of network complexity by at least a factor of O(n) compared to an unfolded Backpropagation net. We then applied the theorem of Funahashi, Hornik et al and Stone-Weierstrass to state the general function approximation ability of the ATNN. We furthermore show a lemma (Lemma 1) that the adaptation of time-delays is mathematically equivalent to the adjustment of interconnections on a unfolded <b>feed-forward</b> <b>network</b> provided there are a large enough number (h 2 nd) of hidden units. Since this number of hidden units is often impractically large, we can conclude that the TDNN and ATNN are thus more powerful than BP with a time history...|$|E
40|$|This paper {{looks at}} some of the issues {{involved}} in estimating errors and confidence limits in <b>feed-forward</b> <b>networks,</b> and results are presented on an example of muscle tremor classification. Errors and Confidences We define firstly a training data set, D, which consists of N examples of input output pairs,...|$|R
40|$|Previously, we have {{demonstrated}} that <b>feed-forward</b> <b>networks</b> {{may be used to}} estimate local output probabilities in hidden Markov model (HMM) speech recognition systems. Here these connectionist techniques are integrated into the DECIPHER system, with experiments being performed using the speaker independent DARPA RM database. Our results indicate that:. connectionist probability estimation can improve performance of a context independent maximum likelihood trained HMM system,. performance of the connectionist system is close to what can be achieved using (context dependent) HMM systems of much higher complexity, and. mixing connectionist and maximum likelihood estimates can improve the performance of a state-of-theart context dependent HMM system. 1 INTRODUCTION Previous investigations, both theoretical and experimental, have indicated that <b>feed-forward</b> <b>networks</b> (typically, multilayer perceptrons, MLPs) may be used to estimate local HMM output probabilities [1, 6]. Our previous p [...] ...|$|R
30|$|ANN usually {{consists}} of several layers. Each layer {{is composed of}} several neurons or nodes. The connections among each node and the other nodes are characterized by weights. The output of each node is the output of a transfer function which its input is the summed weighted activity of all node connections. Each ANN {{has at least one}} hidden layer besides the input and the output layers. There are two known architectures of ANNs: the <b>feed-forward</b> neural <b>networks</b> and the feedback ones. There are several popular <b>feed-forward</b> neural <b>network</b> architectures such as multi-layer perceptrons (MLPs), radial basis function (RBF) networks, and self-organizing maps (SOMs). We had chosen MLP <b>feed-forward</b> <b>networks</b> in our work because of their simplicity and effective implementations; also they are extensively used in pattern recognition and data classification problems.|$|R
40|$|As Moore’s law forces modern {{computer}} microarchitectures to increasingly rely on speculation, accurately predicting branches {{becomes more}} important in keeping the pipeline full. Previous work {{has shown that the}} perceptron, a simple linear discriminator, {{can be used as a}} powerful branch predictor. This paper expands on that research by experimenting with three other branch predictors, a neural network with one hidden layer (a <b>feed-forward</b> <b>network),</b> a neural network with one hidden layer and recurrent (feedback) connections (aka an Elman network), and a combined predictor, using a 2 -bit saturating counter to vote between a perceptron and a <b>feed-forward</b> <b>network.</b> We show how the usually realvalued networks can be approximated with integer math using a look-up table to approximate the activation function and its derivative. ...|$|E
40|$|A novel, two stage, neural {{architecture}} for the segmentation {{of range}} data and their modeling with undeformed superquadrics is presented. The system is composed by two distinct neural networks: a SOM {{is used to}} perform data segmentation, and, for each segment, a multi-layer <b>feed-forward</b> <b>network</b> performs model estimation...|$|E
40|$|In this master's thesis {{we compare}} and combine {{learning}} algorithms for two different architectures. The {{first is the}} standard Multi-layered <b>Feed-forward</b> <b>Network,</b> {{the second is the}} BP-SOM architecture. The BP-SOM architecture combines a Multi-layered <b>Feed-forward</b> <b>Network</b> with one or more Self-Organizing Maps. The second part of this Master's Thesis looks at rule-extraction in the BP-SOM architecture. We show how the Self-Organizing Map can be used to extract rules, and how these rules show us the information contained in the Self-Organizing map. This Master's Thesis was written at the Computer Science Department of Leiden University. My supervisors were Dr. I. G. Sprinkhuizen-Kuyper and Dr. W. A. Kosters. Contents 1 Introduction 4 I Multi-layered Feed-forward Networks 6 2 Multi-Layered Feed-Forward Networks 7 2. 1 McCulloch-Pitts Neurons...................... 8 3 Back-Propagation 9 3. 1 Errors................................. 11 3. 2 [...] ...|$|E
40|$|The {{full text}} file {{attached}} to this record is the authors final peer reviewed version. The publishers version of record can be found by following the DOI link below. <b>Feed-forward</b> neural <b>networks</b> are commonly used for pattern classification. The classification accuracy of <b>feed-forward</b> neural <b>networks</b> depends on the configuration selected and the training process. Once {{the architecture of the}} network is decided, training algorithms, usually gradient descent techniques, are used to determine the connection weights of the <b>feed-forward</b> neural <b>network.</b> However, gradient descent techniques often get trapped in local optima of the search landscape. To address this issue, an ant colony optimization (ACO) algorithm is applied to train <b>feed-forward</b> neural <b>networks</b> for pattern classification in this paper. In addition, the ACO training algorithm is hybridized with gradient descent training. Both standalone and hybrid ACO training algorithms are evaluated on several benchmark pattern classification problems, and compared with other swarm intelligence, evolutionary and traditional training algorithms. The experimental results show the efficiency of the proposed ACO training algorithms for <b>feed-forward</b> neural <b>networks</b> for pattern classification...|$|R
40|$|We present {{experiments}} {{demonstrating that}} some other form of capacity control, different from network size, plays {{a central role in}} learning multilayer <b>feed-forward</b> <b>networks.</b> We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning. Comment: 9 pages, 2 figure...|$|R
40|$|Partially-recurrent {{networks}} have advantages over strictly <b>feed-forward</b> <b>networks</b> for certain spatiotemporal pattern classification or prediction tasks. However networks involving recurrent links {{are generally more}} difficult to train than their nonrecurrent counterparts. In this paper we demonstrate {{that the costs of}} training a recurrent network can be greatly reduced by initialising the network prior to training with weights 'transplanted' from a non-recurrent architecture. Introduction The approaches taken to adapting <b>feed-forward</b> <b>networks</b> to temporal processing can be divided into two main categories: non-recurrent and recurrent. Both of these approaches make use of fixed timedelays on the connections within the network to provide a means of storing temporal information. However in non-recurrent networks all such connections feed into higher layers within the network, whereas nodes in a recurrent network can also have time-delayed links to their own or lower levels. Non-recurrent ar [...] ...|$|R
40|$|Contents 1. Introduction [...] . 5 1. 1 Analog neural {{networks}} [...] . 5 1. 2 The motivation of this thesis [...] 6 2. Artificial Neural Network [...] . 9 2. 1 <b>Feed-forward</b> <b>network</b> [...] 9 2. 2 Back-propagation [...] . 11 2. 3 Learning with backprop [...] . 12 3. Basic ANN computations i...|$|E
40|$|This paper {{describes}} NTUNE (Nottingham Trent University Neural Environment), a multi-platform {{neural network}} simulation tool for <b>feed-forward</b> <b>network</b> architectures, which was especially designed {{as an educational}} tool for teaching purposes in higher education. NTUNE can easily be extended and trained networks can be exported {{to be used in}} other C++ applications...|$|E
40|$|The {{backpropagation}} algorithm for feed-forward networks (Figure 1 a) {{has been}} successfully applied {{to a wide range}} of problems from neuroscience to consumer electronics (see BACK-PROPAGATION and APPLICATIONS OF NEURAL NETWORKS). However, what can be implemented by a <b>feed-forward</b> <b>network</b> is just a static mapping of the input vectors. It i...|$|E
40|$|The {{hidden layer}} neurons in a multi-layered <b>feed-forward</b> neural <b>network</b> serve a {{critical}} role. From one perspective, the hidden layer neurons establish (linear) decision boundaries in the feature space. These linear decision boundaries are then combined by succeeding layers leading to convex-open and thereafter arbitrarily shaped decision boundaries. In this paper {{we show that}} the use of unidirectional Gaussian lateral connections from a hidden layer neuron to an adjacent hidden layer leads to a much richer class of decision boundaries. In particular the proposed class of networks has the advantage of sigmoidal <b>feed-forward</b> <b>networks</b> (global characteristics) but with the added flexibility of being able to represent local structure. An algorithm to train the proposed network is presented and its training and validation performance shown using a simple classification problem. Keywords: Lateral Connections, <b>Feed-Forward</b> Neural <b>Networks,</b> Multi-Layered Perceptrons, Radial Basis Function Net [...] ...|$|R
40|$|Learning {{algorithms}} {{have been}} used both on <b>feed-forward</b> deterministic <b>networks</b> and on feed-back statistical networks to capture input-output relations and do pattern classification. These learning algorithms are examined for a class of problems characterized by noisy or statistical data, in which the networks learn the relation between input data and probability distributions of answers. In simple but nontrivial networks the two learning rules are closely related. Under some circumstances the learning problem for the statistical networks can be solved without Monte Carlo procedures. The usual arbitrary learning goals of <b>feed-forward</b> <b>networks</b> can be given useful probabilistic meaning...|$|R
50|$|Representing fuzzification, fuzzy {{inference}} and defuzzification through multi-layers <b>feed-forward</b> connectionist <b>networks.</b>|$|R
40|$|A {{method of}} K-set {{canonical}} correlation analysis capable of joint multivariate nonlinear transformations of data was proposed. The method consists of K nonlinear data transformation modules, {{each of which}} is a multilayered <b>feed-forward</b> <b>network,</b> and one integrator module which combines information from the K transformation modules. The proposed method is useful for integrating information from K concurrent sources...|$|E
30|$|As it is considered, the {{performances}} of two hidden layer neural networks are {{much better than the}} one-layer networks, even when a two-layer network has fewer neurons. It shows less computational requirements and of course better operation. Therefore, a <b>feed-forward</b> <b>network</b> with two hidden layers is used, in which temperature and pressure are input variables and density is the output variable.|$|E
30|$|The {{most common}} neural network {{architecture}} is the feed-forward neural network. <b>Feed-forward</b> <b>network</b> is the network structure {{in which the}} information or signals will propagate only in one direction, from input to output. A three layered feed-forward neural network with back propagation algorithm can approximate any nonlinear continuous function to an arbitrary accuracy (Brown and Harris 1994; Hornick et al. 1989).|$|E
40|$|In {{this paper}} we {{consider}} four alternative approaches to complexity control in <b>feed-forward</b> <b>networks</b> based respectively on architecture selection, regularization, early stopping, and training with noise. We {{show that there}} are close similarities between these approaches and we argue that, for most practical applications, the technique of regularization should be the method of choice...|$|R
30|$|Based on the topology, ANNs {{are mainly}} {{divided into two}} groups of <b>feed-forward</b> and {{recurrent}} <b>networks.</b> For univariate forecasting analysis, the use of recurrent topology is more common than the <b>feed-forward</b> <b>networks.</b> Hussain et al. (2007), Lin et al. (2006), and Saad et al. (1998) are the ones who employed recurrent topology. However, in the field of financial forecasting, especially in multivariate forecasting analysis, the feed-forward topology has gained much more attention. As a result, the latter topology will be used in this research as well.|$|R
40|$|Abstract. Several {{architectures}} and algorithms of <b>feed-forward</b> <b>networks</b> and neural associative memories {{as well as}} GMDH-based polynomial NNs are {{tried for}} proteomic data analysis. The problem of chemotherapy responsiveness prediction by data of mass-spectroscopy is considered to explore potential applications of different neural paradigms for this domain. Keywords Artificial neural networks, polynomial neural networks, proteomics array...|$|R
