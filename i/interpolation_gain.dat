1|19|Public
40|$|Discrete spectra {{can be used}} {{to measure}} {{frequencies}} of sinusoidal signal components. Such a measurement consists in digitizing a compound signal, performing windowing of the signal samples and computing their discrete magnitude spectrum, usually by means of the Fast Fourier Transform algorithm. Frequencies of individual components can be evaluated from their locations in the discrete spectrum with a resolution depending on the number of samples. However, the frequency of a sinusoidal component can be determined with improved resolution by fitting an interpolating parabola through the three largest consecutive spectrum bins corresponding to the component. The abscissa of its maximum constitutes a better frequency approximation. Such a method has been used for tune measurement systems in circular accelerators. This paper describes the efficiency of the method, depending on the windowing function applied to the signal samples. A typical <b>interpolation</b> <b>gain</b> is one order of magnitude. Better results are obtained with Gaussian interpolation, offering frequency resolution improvement by more than two orders of magnitude when used with windows having fast sidelobe decay. An improvement beyond three orders of magnitude is possible with steep Gaussian windows. These results are confirmed by laboratory measurements. Both methods assume the measured frequency to be constant during acquisition and the spectral peak corresponding to the measured component to constitute a local maximum in a given band of the input signal discrete spectrum...|$|E
40|$|We show that, through careful {{control of}} noise sources, it is {{possible}} to determine the microbunching gain curve for the FERMI@ELETTRA linac using the particle tracking code elegant. In addition to using a sufficiently large number of particles (60 × 10 ^{ 6 }), use of a low-pass filter is very helpful in controlling noise and providing convenient intrabin <b>interpolation.</b> <b>Gains</b> of up to 1500 are seen for modulation wavelengths down to 25 [*][*]μm. Because of the high gain, very small initial modulations are needed to avoid saturation, which further motivates the use {{of a large number of}} particles. We also show, for the first time, how the density modulation evolves in detail inside the dipoles of a multichicane system...|$|R
40|$|This {{paper is}} {{concerned}} with the design of gain-scheduled controllers with guaranteed H 1 performance for a class of Linear Parameter-Varying (LPV) plants. Here the plant state-space matrices are assumed to depend affinely on a vector ` of time-varying real parameters. Assuming real-time measurement of these parameters, they can be fed to the controller to optimize the performance and robustness of the closed-loop system. The resulting controller is therefore time-varying and automatically "gainscheduled " along parameter trajectories. Based on the notion of quadratic H 1 performance, solvability conditions are obtained for continuousand discrete-time systems. In both cases, the synthesis problem reduces to solving a system of Linear Matrix Inequalities (LMIs). The main benefit of this approach is to bypass most difficulties associated with more classical schemes such as <b>gain</b> <b>interpolation</b> or <b>gain</b> scheduling techniques. The methodology presented in this paper is applied to the gain-sched [...] ...|$|R
40|$|ABSTRACT. Due {{to modern}} day aircrafts evolution, {{requirements}} for flight control laws {{are more and}} more numerous and varied. For that reason, control laws designers need now to use multiobjectives synthesis techniques. We then choose to develop to solve today aircraft issue. The whole designed procedure use: first H 1 to compute an initial stabilizing controller, then Q-	 parametrization to introduce the design shapping, convex synthesis to optimize the controller with respect to the requirements set, robust modal control to produce a performing and reduced 	order controller, and last observer state feedback controller <b>interpolation</b> for <b>gain</b> scheduling. ...|$|R
40|$|We {{discuss the}} {{solution}} of the fault detection problem for parameter uncertain nonlinear systems using a gain scheduling based approach. By using a set of linearized plant models, a set of least order linear fault detection filters is determined to serve for <b>interpolation</b> based <b>gain</b> scheduling. The basic synthesis approach is the nullspace method for constant linear systems, whose extension to the case of linear parameter varying (LPV) models provides the methodological framework for the synthesis of robust fault detection filters. The resulting gain scheduled fault detection filter provides robustness with respect to both model nonlinearities as well as parametric uncertainties. In a general setting, we consider the case when part of the uncertain parameters are non-measurable and part of them are measurable...|$|R
40|$|A new {{approach}} to gain scheduling of linear controllers is proposed and applied to a longitudinal flight control pro-blem. Traditionally, gain scheduling is done a posteriori by the <b>interpolation</b> of controller <b>gains</b> designed for several operating points or conditions. The method proposed here is based on guardian maps and does not require as many linear controller syntheses as there are design points. Rather, it extends the performance of an initial single controller carried out on an arbitrary operating point to the entire domain while ensuring generalized stability all along the process. The method, which uses a given fixed architecture controller, is successfully applied on the longitudinal flight control of a business jet aircraft...|$|R
40|$|Receive path {{includes}} dual 10 -bit analog-to-digital converters {{with internal}} or external reference, 50 MSPS and 80 MSPS versions Transmit path includes dual 10 -bit, 200 MSPS digital-toanalog converters with 1 ×, 2 ×, or 4 × <b>interpolation</b> and programmable <b>gain</b> control Internal clock distribution block includes a programmable phase-locked loop and timing generation circuitry, allowing single-reference clock operation 20 -pin flexible I/O data interface allows various interleaved or noninterleaved data transfers in half-duplex mode and interleaved data transfers in full-duplex mode Configurable through register programmability or optionally limited programmability through mode pins Independent Rx and Tx power-down control pins 64 -lead LFCSP package (9 mm × 9 mm footprint) 3 configurable auxiliary converter pin...|$|R
40|$|A new 1200 bps speech coder {{designed}} with a tree searched multistage matrix quantization scheme is proposed. To improve speech quality {{and reduce the}} average bit rate, we have developed a new residual multistage matrix quantization method with the joint design technique. The new joint design algorithm reduces the codebook training complexity. Other new techniques for improving the performance include joint quantization of pitch and voiced/unvoiced/mixed decisions and <b>gain</b> <b>interpolation.</b> For the new matrix quantization based speech coder (MQBC), the listening tests have proven that an efficient and high quality coding has been achieved at bit rate 1200 bps. Test results are compared with the 2400 bps LPC 10 e coder and the new 2400 bps MELP coder which has been chosen as the new 2400 bps Federal Standard...|$|R
40|$|In this work, a novel sub-pixel {{interpolation}} {{algorithm is}} proposed for video coders targeted towards high resolution and high fidelity use cases. Proposed scheme is based on adapting the interpolation filter's symmetry assumptions in a Rate-Distortion-Optimized fashion, {{taking into account the}} coding rate and the statistical properties of each image of the video sequence. Experimental results show that, using the proposed algorithm a gain of up-to 1. 1 dB is achieved compared to non-adaptive sub-pixel interpolation of H. 264 /AVC. Compared to other state-of-the-art adaptive sub-pixel <b>interpolation</b> methods, a <b>gain</b> of up-to 0. 5 dB is achieved. Proposed scheme outperforms H. 264 /AVC for all test cases; however, improvement is more significant at high bitrates and at high resolutions. This is especially important for future video coding solutions targeting high fidelity video applications. Index Terms- Video Coding, Interpolation 1...|$|R
40|$|In recent years, {{comprehensive}} learning {{particle swarm optimization}} (CLPSO) {{has attracted}} the attention of many scholars for using in solving multimodal problems, as it is excellent in preserving the particles' diversity and thus preventing premature convergence. However, CLPSO exhibits low solution accuracy. Aiming to address this issue, we proposed a novel algorithm called LILPSO. First, this algorithm introduced a Lagrange interpolation method to perform a local search for the global best point (gbest). Second, to gain a better exemplar, one gbest, another two particle's historical best points (pbest) are chosen to perform Lagrange <b>interpolation,</b> then to <b>gain</b> a new exemplar, which replaces the CLPSO's comparison method. The numerical experiments conducted on various functions demonstrate the superiority of this algorithm, and the two methods are proven to be efficient for accelerating the convergence without leading the particle to premature convergence...|$|R
40|$|Deep {{learning}} based {{methods have}} recently pushed the state-of-the-art {{on the problem}} of Single Image Super-Resolution (SISR). In this work, we revisit the more traditional interpolation-based methods, that were popular before, now with the help of deep learning. In particular, we propose to use a Convolutional Neural Network (CNN) to estimate spatially variant interpolation kernels and apply the estimated kernels adaptively to each position in the image. The whole model is trained in an end-to-end manner. We explore two ways to improve the results for the case of large upscaling factors, and propose a recursive extension of our basic model. This achieves results that are on par with state-of-the-art methods. We visualize the estimated adaptive <b>interpolation</b> kernels to <b>gain</b> more insight on the effectiveness of the proposed method. We also extend the method to the task of joint image filtering and again achieve state-of-the-art performance...|$|R
40|$|In {{this paper}} {{we present a}} new method for the {{interpolation}} of functions using Feynman path integrals. It is argued that least-biased interpolations are obtained. The interpolation method is essentially numerical and an algorithm is given for determining the interpolation values. The core of the method is {{the identification of the}} interpolation values as expectation values of a random function. The expectation values are obtained by a Metropolis algorithm. The method is illustrated with a number of examples and an application in TCAD. 1 Introduction Interpolation techniques deal with the problem of finding a function that exactly reproduces the outcomes (responses) of a given data set obtained by experiments. Recently, <b>interpolation</b> techniques have <b>gained</b> interest in the field of Response Surface Methodology (RSM) applied to Computer Experiments and interpolation techniques are used to predict the outcomes (responses) of expensive computer experiments as was done by Sacks, Welch, Mitc [...] ...|$|R
40|$|Fuzzy rule {{interpolation}} enables fuzzy {{systems to}} perform inference with a sparse rule base. However, common approaches to fuzzy rule interpolation assume that rule antecedents are of equal significance while searching for rules to implement interpolation. As such, inaccurate or incorrect interpolated {{results may be}} produced. To help minimise the adverse impact of the equal significance assumption, this paper presents a novel approach for rule <b>interpolation</b> where information <b>gain</b> is utilised to evaluate the relative significance of rule antecedents in a given rule base. The approach is enabled {{by the introduction of}} an innovative reverse engineering technique that artificially creates training data from a given sparse rule base. The resulting method facilitates informed choice of most appropriate rules to compute interpolation. The work is implemented for scale and move transformation-based fuzzy rule interpolation, but the underlying idea can be extended to other rule interpolation methods. Comparative experimental evaluation demonstrates the efficacy of the proposed approac...|$|R
40|$|The {{diagnosis}} of defective castings {{has always been}} a centre of attention in the manufacturing industry. This is mainly because the cause and effect relationship in a casting process is complex and non-linear. Furthermore, a large number of parameters are needed to be coordinated with each other in an optimal way to minimise the occurrence of defective castings. An intelligent diagnosis system is needed to diagnose effectively the causal representation and also justify its diagnosis. A previous method, known as the Knowledge Hyper-surface method which used Lagrange <b>Interpolation</b> polynomials has <b>gained</b> more popularity in learning cause and effect analysis in casting processes. The current method show that the belief value of the occurrence of cause with respect to the change in the belief value in the occurrence of effect can be modeled by linear, quadratic or cubic relationships and the method retained the advantages of neural networks and overcomes their limitations in learning the input-output mapping function in the presence of noisy, limited and sparse data. However, the methodolog...|$|R
40|$|Deep optical {{images are}} often crowded with {{overlapping}} objects. This {{is especially true}} in the cores of galaxy clusters, where images of dozens of galaxies may lie atop one another. Accurate measurements of cluster properties require deblending algorithms designed to automatically extract a list of individual objects and decide what fraction of the light in each pixel comes from each object. We present new software called the Gradient And <b>INterpolation</b> based deblender (<b>GAIN)</b> as a secondary deblender to improve deblending the images of cluster cores. This software relies on using image intensity gradient and using an image interpolation technique usually used to correct flawed terrestrial digital images. We test this software on Dark Energy Survey coadd images. GAIN helps extracting unbiased photometry measurement for blended sources. It also helps improving detection completeness while introducing only a modest amount of spurious detections. For example, when applied to deep images simulated with high level of deblending difficulties, this software improves detection completeness from 91 % to 97 % for sources above the 10 ? limiting magnitude at 25. 3 mag. We expect this software to be a useful tool for cluster population measurements. Comment: Published on PAS...|$|R
40|$|MIRO (Microwave Instrument for the Rosetta Orbiter) is a lightweight, uncooled, dual-frequency {{heterodyne}} radiometer. The MIRO encountered asteroid Steins in 2008, {{and during}} the flyby, MIRO used the Asteroid Mode to measure the emission spectrum of Steins. The Asteroid Mode {{is one of the}} seven modes of the MIRO operation, and is designed to increase the length of time that a spectral line is in the MIRO pass-band during a flyby of an object. This software is used to calibrate the continuum measurement of Steins emission power during the asteroid flyby. The MIRO raw measurement data need to be calibrated in order to obtain physically meaningful data. This software calibrates the MIRO raw measurements in digital units to the brightness temperature in Kelvin. The software uses two calibration sequences that are included in the Asteroid Mode. One sequence is {{at the beginning of the}} mode, and the other at the end. The first six frames contain the measurement of a cold calibration target, while the last six frames measure a warm calibration target. The targets have known temperatures and are used to provide reference power and gain, which can be used to convert MIRO measurements into brightness temperature. The software was developed to calibrate MIRO continuum measurements from Asteroid Mode. The software determines the relationship between the raw digital unit measured by MIRO and the equivalent brightness temperature by analyzing data from calibration frames. The found relationship is applied to non-calibration frames, which are the measurements of an object of interest such as asteroids and other planetary objects that MIRO encounters during its operation. This software characterizes the gain fluctuations statistically and determines which method to estimate gain between calibration frames. For example, if the fluctuation is lower than a statistically significant level, the averaging method is used to estimate the gain between the calibration frames. If the fluctuation is found to be statistically significant, a linear <b>interpolation</b> of <b>gain</b> and reference power is used to estimate the gain between the calibration frames...|$|R
40|$|The {{flight control}} of X- 33 poses a {{challenge}} to conventional gain-scheduled flight controllers due to its large attitude maneuvers from liftoff to orbit and reentry. In addition, {{a wide range of}} uncertainties in vehicle handling qualities and disturbances must be accommodated by the attitude control system. Nonlinear tracking and decoupling control by trajectory linearization can be viewed as the ideal gain-scheduling controller designed at every point on the flight trajectory. Therefore it provides robust stability and performance at all stages of flight without <b>interpolation</b> of controller <b>gains</b> and eliminates costly controller redesigns due to minor airframe alteration or mission reconfiguration. In this paper, a prototype trajectory linearization design for an X- 33 ascent flight controller is presented along with 3 -DOF and 6 -DOF simulation results. It is noted that the 6 -DOF results were obtained from the 3 -DOF design with only a few hours of tuning, which demonstrates the inherent robustness of the design technique. It is this "plug-and-play" feature that is much needed by NASA for the development, test and routine operations of the RLV'S. Plans for further research are also presented, and refined 6 -DOF simulation results will be presented in {{the final version of the}} paper...|$|R
40|$|The {{research}} on the analysis {{of cause and effect}} relationships in castings has always been a center of attention in the manufacturing industry. An intelligent diagnosis system should be able to diagnose effectively the causal representation and also justify its diagnosis. Recently, a method, known as the Knowledge Hyper-surface method which used Lagrange <b>Interpolation</b> polynomials has <b>gained</b> more popularity in learning cause and effect analysis in casting processes. The current method show that the belief value of the occurrence of cause with respect to the change in the belief value in the occurrence of effect can be modeled by linear, quadratic or cubic relationships and the method retained the advantages of neural networks and overcomes their limitations in learning the input-output mapping function in the presence of noisy, limited and sparse data. However, the methodology was unable to model exponential increase/decrease in belief values in cause and effect relationships. This paper proposed an enhancement to the current Knowledge Hyper-surface method by introducing midpoints in the existing shape formulation which further constrains the shape of the Knowledge hyper-surfaces to model an exponential rise in belief values but without exposing the data set to the limitations of 'over fitting'. The ability of the proposed method to capture the exponential change in the belief variation of the cause when the belief in the effect is at its minimum is compared to the current method on real casting data...|$|R
40|$|P-Cable seismic {{acquisition}} {{system is a}} rather new technology; developed through the past 20 years. Consequently, the use of P-Cable technology has been limited, although researchers still believe that further improvements can achieve better results {{with this type of}} data. Characteristics associated with P-Cable seismic data include broad frequency bandwidth, shot offset, and low fold. The objectives of this thesis will be to provide a processing workflow with the main goal of enhancing imaging of ultrahigh resolution P-Cable 3 D seismic, acquired at Vestnesa Ridge and Snøhvit Field. This has been done by testing various methods, corresponding parameters, and through constructing a synthetic model to confirm whether improvements have been reached or not. This project differs from previous studies dealing with P-Cable 3 D seismic data as it will also focus on analyzing the behavior of the synthetic model to compare the different migration techniques. The processing steps will also be considered with emphasis to aspects such as time-efficiency, resolution and geological reasoning. However, the result should ideally present images of higher resolution than previously possible to obtain with published workflows. This work has revealed how different approaches can lead to completely different results regarding signal-to-noise ratio with similar resolution, precision in presenting accurate imaging of the subsurface, and cost-efficiency. The results have shown that noise filtered Stolt migrated data including pre-migration operation such as brute stack, F-XY deconvolution, missing data <b>interpolation,</b> and automatic <b>gain</b> control obtains good imaging with high spatial resolution and improved signal-to-noise ratio. Stolt migration also presented images with more continuous reflectors, and resolution sufficient to confidently separate features. Compared, prestack time migration constructed images of less diffraction imprint, but contained more extensive acoustic blanking and noise bursts. The use of synthetic seismic data has shown to be useful in diagnosing properties of the different migration techniques, and for eliminating uncertainties related to velocity variation. However, results also point to independence between imaging quality of P-Cable seismic data and the input velocity model, as prestack- and reverse- time migration did not show any significant changes after applying it with constant versus actual velocity model...|$|R
40|$|Allseas {{engineering}} BV is {{an offshore}} company that uses underwater vehicles for conducting subsea operations. The two primary classification of such underwater vehicles are: Remote operated vehicle (ROV) and Autonomous underwater vehicle (AUV). As the names suggests, the former requires a human operator {{to control and}} {{the latter is a}} fully autonomous vehicle. As of now, the company (and most of the offshore industries) customarily use ROVs. In the recent past, Allseas was inclined to use AUVs as a measure of reducing the operational costs and conducted tests runs with an industrial grade AUV. But the test runs were unsuccessful and the plan of using such an AUV was dropped. It was concluded that the major problem was in launch and recovery operations of the AUV, which were conducted from the deck of a ship. To be specific, the disturbances from the ship’s thrusters, ocean currents, and waves, were proved to be impossible to compensate for by the AUV during the launch and recovery operations. Therefore, this thesis aims to investigate on underwater docking capabilities of AUVs, which not only eliminates the launch and recovery issues but improves AUV’s overall operational capabilities. In this thesis, a docking problem is formulated and the solution to the problem covers the following aspects: modeling of AUV, motion control of AUV, and guidance strategies. For the motion control, an appropriate model of the vehicle is necessary. Various models used in the literature were studied which include: 6 degree of freedom (DOF) non-linear model, 3 -DOF horizontal plane model, and 3 -DOF vertical plane model. As an example, a 6 -DOF non-linear model of ARIES AUV from literature was decoupled into the respective 3 -DOF models. These models are used for controller design and docking strategies. A linear parameter varying (LPV) frame work based, gain scheduled feedback and feed forward controllers were developed for the control of vehicle heading and depth. The controller design involves the following steps: Firstly, a third order Quasi- LPV control plant for heading and depth were derived from horizontal and vertical plane models. Then, linear controllers (gains) were designed for fixed values of scheduling variables. Based on the dimension of the scheduling variables, a stability preserving <b>interpolation</b> of these <b>gains</b> were performed to obtain the final controller. A PI controller was designed for controlling the longitudinal velocity of the vehicle. The performance of the controllers were checked on 6 DOF and 3 DOF models of the ARIES AUV. Finally, two docking strategies: Three point and N-point, were developed using lookahead based path following and investigated their performance for two scenarios: stationary dock and non-stationary dock. The docking strategies addressed the docking problem in two aspects: geometric path generation and path following. Finally, the controller performance in closed loop, as in, the guidance, tracking controllers, and vehicle dynamics was compared for the two developed strategies. It was shown that the N-point docking strategy yielded better convergence to the paths than the three point docking. systems and controlDelft Center for Systems and ControlMechanical, Maritime and Materials Engineerin...|$|R

