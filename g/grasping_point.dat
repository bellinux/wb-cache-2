39|308|Public
50|$|The end {{effector}} of {{an assembly line}} robot would typically be a welding head, or a paint spray gun. A surgical robot's {{end effector}} could be a scalpel or others tools used in surgery. Other possible end effectors are machine tools, like a drill or milling cutters. The end effector on the space shuttle’s robotic arm uses a pattern of wires which close like the aperture of a camera around a handle or other <b>grasping</b> <b>point.</b>|$|E
40|$|Abstract—Objects {{often have}} {{multiple}} purposes, {{and the way}} humans grasp a certain object may vary based on the different intended purposes. To enable robots to acquire similar ability, instead of inferring <b>grasping</b> <b>point</b> for objects only based on their appearance, a new method, object purpose based <b>grasping</b> <b>point</b> inference, is proposed. First, shape context descriptor and SVMs are applied for detecting purposes of objects. Then generalized linear model and multi-scale vision features are used to predict objects ’ <b>grasping</b> <b>point</b> based on their appearance. Experimental results for both purpose detection and <b>grasping</b> <b>point</b> inference are provided, which show that our method gives reasonably good performance in terms of accuracy in prediction. T I...|$|E
40|$|A vision-guided {{grasping}} {{system for}} Phalaenopsis tissue culture plantlets (PTCPs) {{was developed and}} tested. in manually transplanting Phalaenopsis plantlets, the plantlet is usually grasped at the root or the stem since the leaf is fragile and is easily damaged by the gripper. An image-processing algorithm for locating a <b>grasping</b> <b>point</b> was employed to determine a suitable <b>grasping</b> <b>point</b> on the roots. A binocular stereovision algorithm was applied to compute the 3 D coordinates of the <b>grasping</b> <b>point.</b> Furthermore, a grasping device with a gripper suitable for grasping the PTCP plantlet was developed and tested. Finally, the binocular vision locating algorithm was then integrated with the robotic gripper in order to construct an automatic PTCP grasping system. The experimental {{results indicated that the}} automatic grasping system had a success rate of 78. 2 % in grasping the plantlets in an appropriate position. (C) 2009 Elsevier B. V. All rights reserved...|$|E
40|$|AbstractThe {{purpose of}} this study is to {{investigate}} whether <b>grasp</b> <b>point</b> of each target object is switched between central and peripheral visual field conditions. We measured <b>grasp</b> <b>points</b> of a lift-up task (LT) and a pinch task (PT), that did not lift from a table, under the three visual field conditions. As a result, in the normal condition, the <b>grasp</b> <b>points</b> of LT and PT were different and the <b>grasp</b> <b>points</b> of LT were almost the centers of gravity of objects. While, the <b>grasp</b> <b>points</b> of LT were switched to those of PT under the central visual field condition and the <b>grasp</b> <b>points</b> of PT were switched to those of LT under the peripheral visual field condition. These results indicate that in the human brain there are at least two types of <b>grasp</b> <b>point</b> computation and the central and peripheral vision systems respectively contribute to these computations of PT and LT...|$|R
40|$|We {{examined}} {{whether the}} movement path is considered when selecting the positions {{at which the}} digits will contact the object’s surface (<b>grasping</b> <b>points).</b> Subjects <b>grasped</b> objects of different heights but with the same radius at various locations on a table. At some locations, one digit crossed {{to the side of}} the object opposite of where it started. In doing so, it moved over a short object whereas it curved around a tall object. This resulted in very different paths for different objects. Importantly, the selection of <b>grasping</b> <b>points</b> was unaffected. That subjects do not appear to consider the path when selecting <b>grasping</b> <b>points</b> suggests that the <b>grasping</b> <b>points</b> are selected before planning the movements towards those points...|$|R
40|$|International audienceThis paper {{describes}} {{the development of}} a novel vision-based grasping system for unknown objects based on range images. We realize a synthesis of the calculated <b>grasp</b> <b>points</b> with a 3 D model of a hand prosthesis, which we are using as gripper. We locally find <b>grasp</b> <b>point</b> candidates based on the shape of the object and validate the globally by checking collisions between the gripper and surrounding objects and the table top. Our approach integrates a robust object segmentation and <b>grasp</b> <b>point</b> detection for every object on a table in front of a 7 -DOF robot arm. The algorithm analyzes the top surface of every object and outputs the generated <b>grasp</b> <b>points</b> and the required gripper pose to grasp the desired object. Additionally we can calculate the optimal opening angle of the gripper. The first experimental results show that the presented automated grasping system is able to generate successful <b>grasp</b> <b>points</b> {{for a wide range of}} different objects...|$|R
40|$|Abstract. Recently, the {{research}} on robots performing every-day tasks at home, like {{to take care of}} elderly or disabled people, has pursued the problem of the ma-nipulation of everyday objects. Among them, grasping a cloth {{is one of the most}} challenging tasks, since the textiles are highly-deformable and it is not straight-forward to define a generic <b>grasping</b> <b>point.</b> In this paper, we address this problem by introducing a new robot interaction method that enables unexperienced users to control the robot in a natural way. Given a textile lying on the table the robot pro-poses a <b>grasping</b> <b>point</b> and the user is able to teach the robot a new <b>grasping</b> <b>point.</b> The data collected using this method is then used for training the system using a Vector Autoregression method, which produces better grasping points allowing po-tentially better manipulation actions. The experiments demonstrates the validity of the new interaction method and its potential to improve the point-grasping selection algorithm in different configurations of a polo-shirt...|$|E
40|$|We {{develop an}} {{assembly}} robot system for assembling the flexible belt-shaped subject. An {{image processing method}} is developed to recognize the belt-shaped subject. This method is able to determine the <b>grasping</b> <b>point</b> and grasping angle for piking up a subject by a multiple hands unit. CAD information is {{used to determine the}} <b>grasping</b> <b>point.</b> The multiple hands unit is developed, which is able to grasp all grasping points of a subject at a time. In addition, the image processing method is used to judge whether a subject is fastened accurately at right position or not during the assembly...|$|E
40|$|To grasp {{an object}} the digits {{need to be}} placed at {{suitable}} positions on its surface. The selection of such grasping points depends on several factors. Here the authors examined whether {{being able to see}} 1 of the selected grasping points is such a factor. Subjects grasped large cylinders or oriented blocks that would normally be grasped with the thumb continuously visible and the final part of the index finger's trajectory occluded by the object in question. An opaque screen that hid the thumb's usual <b>grasping</b> <b>point</b> was used to examine whether individuals would choose a grip that was oriented differently to maintain vision of the thumb's <b>grasping</b> <b>point.</b> A transparent screen was used as a control. Occluding the thumb's <b>grasping</b> <b>point</b> made subjects move more carefully (adopting a larger grip aperture) and choose a slightly different grip orientation. However, the change in grip orientation was much too small to keep the thumb visible. The authors conclude that humans do not particularly aim for visible grasping points. © 2012 Taylor and Francis Group, LLC...|$|E
40|$|Abstract — This paper {{describes}} {{a process for}} identifying possible grasping locations on a randomly positioned pile of clothes. For each pile, a robotic gripper is remote controlled to determine the possible grasp locations. These are highlighted on images captured from the robot’s onboard camera. <b>Grasping</b> <b>points</b> are generally edges of clothes and rarely folds. A catalogue of visual features categorising <b>grasping</b> <b>points</b> is produced. The orientation of the grasp varies over the workspace reflecting the kinematics abilities of the robot. The relation between gripper orientation and visual feature orientation is analysed. A method is proposed for visually identifying feasible <b>grasping</b> <b>points</b> which reflect the robots ability. T I...|$|R
40|$|A {{scheme for}} {{learning}} to grasp objects using visual information is presented. A system is considered that coordinates a parallel-jaw gripper (hand) and a camera (eye). Given an object, and considering its geometry, the system chooses <b>grasping</b> <b>points,</b> and performs the grasp. The system learns while performing grasping trials. For each grasp we store location parameters that code {{the locations of}} the <b>grasping</b> <b>points,</b> quality parameters that are relevant features for the assessment of grasp quality, and the grade. We learn two separate subproblems: (1) to choose <b>grasping</b> <b>points,</b> and (2) to predict the quality of a given grasp. The location parameters are used to locate <b>grasping</b> <b>points</b> on new target objects. We consider a function from the quality parameters to the grade, learn the function from examples, and later use it to estimate grasp quality. In this way grasp quality for novel situations can be generalized and estimated. An experimental setup using an AdeptOne manipulator to test th [...] ...|$|R
40|$|AbstractIn this paper, a new Multi-Aspect Grasp (MAG) {{performance}} index is presented for evaluating grasp quality during an object manipulation task. The position of <b>grasp</b> <b>points,</b> {{the configuration of}} cooperative manipulators, and the kinetic aspects of manipulating arms and the grasped object are considered in the MAG index. The MAG index is {{used to evaluate the}} candidate points to choose the best <b>grasp</b> <b>point</b> and to select the most effective branch of the inverse kinematics solution, with respect to the given task. Simulation results, which are validated with analytical solutions, show the merits of the proposed index. According to these results, the MAG index indicates that in planar object manipulation tasks without rotation, the best <b>grasp</b> <b>point</b> is the object center of the mass, which is physically meaningful...|$|R
40|$|Master of ScienceDepartment of Computing and Information SciencesDavid A. GustafsonOctant analysis, when {{combined}} with properties of the multivariate central limit theorem and multivariate normal distribution, allows finding a reasonable <b>grasping</b> <b>point</b> on an unknown novel object possible. This thesis’s original contribution {{is the ability to}} find progressively improving grasp points in a poor and/or sparse point cloud. It is shown how octant analysis was implemented using common consumer grade electronics to demonstrate the applicability to home and office robotics. Tests were carried out on three novel objects in multiple poses to determine the algorithm’s consistency and effectiveness at finding a grasp point on those objects. Results from the experiments bolster the idea that the application of octant analysis to the <b>grasping</b> <b>point</b> problem seems promising and deserving of further investigation. Other applications of the technique are also briefly considered...|$|E
40|$|The {{research}} on robots performing every-day tasks at home, has pursued {{the problem of}} the manipulation of everyday objects. Among them, grasping a cloth is a challenging task, as the textile is highly-deformable and it is not straightforward to define a generic <b>grasping</b> <b>point.</b> In this paper, we address this problem by introducing a new robot interaction method that enables unexperienced users to control the robot in a natural way. When the robot proposes a <b>grasping</b> <b>point,</b> the user is able to teach the robot a new one. The data collected using this method is then used for training a system using linear regression method, which produces better grasping points and allowing better manipulation actions. The experiments demonstrates the validity of the new interaction method and its potential to improve the point-grasping selection algorithm. Peer ReviewedPostprint (author's final draft...|$|E
40|$|Trabajo presentado a la 18 th Catalan Conference on Artificial Intelligence (CCIA) celebrada en Valencia (España) del 21 al 23 de octubre de 2015. The {{research}} on robots performing every-day tasks at home has pursued {{the problem of}} the manipulation of everyday objects. Among them, grasping a cloth is a challenging task, as the textile is highly-deformable and it is not straightforward to define a generic <b>grasping</b> <b>point.</b> In this paper, we address this problem by introducing a new robot interaction method that enables unexperienced users to control the robot in a natural way. When the robot proposes a <b>grasping</b> <b>point,</b> the user is able to teach the robot a new one. The data collected using this method is then used for training a system using linear regression method, which produces better grasping points and allowing better manipulation actions. The experiments demonstrates the validity of the new interaction method and its potential to improve the point-grasping selection algorithm. Peer Reviewe...|$|E
40|$|When we grasp an object, our visuomotor {{system has}} to solve an {{intricate}} problem: {{how to find the}} best out of an infinity of possible contact points of the fingers with the object? The contact point selection model (CoPS) we present here solves this problem and predicts human <b>grasp</b> <b>point</b> selection in precision grip grasping by combining a few basic rules that have been identified in human and robotic grasping. Usually, not all of the rules can be perfectly satisfied. Therefore, we assessed their relative importance by creating simple stimuli that put them into conflict with each other in pairs. Based on these conflict experiments we made model-based <b>grasp</b> <b>point</b> predictions for another experiment with a novel set of complexly shaped objects. The results show that our model predicts the human choice of <b>grasp</b> <b>points</b> very well, and that observers´ preferences for their natural grasp angles is as important as physical stability constraints. Incorporating a human <b>grasp</b> <b>point</b> selection model like the one presented here could markedly improve current approaches to cortically guided arm and hand prostheses by making movements more natural while also allowing for a more efficient use of the available information...|$|R
40|$|When {{picking up}} objects, {{we tend to}} <b>grasp</b> at contact <b>points</b> that {{minimize}} slippage and torsion, which depend on the particular shape. Normally, <b>grasp</b> <b>points</b> could be planned before initiating movement. We tested whether <b>grasp</b> <b>points</b> can be determined during online control. Subjects reached to grasp virtual planar objects with varied shapes. On some trials, the object was changed during movement, either rotated by 45 ° or replaced with a different object. In all conditions, grasp axes were well adapted to the target shape, passing {{near the center of}} mass with low force closure angles. On perturbed trials, corrective adjustments were detectable within 320 ms, and were toward the same grasp axes observed on unperturbed trials. Perturbations had little effect on either kinematics or the optimality of final <b>grasp</b> <b>points.</b> Our results demonstrate that the visuomotor system is capable of online processing of shape information to determine appropriate contact <b>points</b> for <b>grasping...</b>|$|R
40|$|Poster Session - Perception and action: Reaching and grasping: no. 33. 304 When {{picking up}} objects with a {{precision}} grip, {{we tend to}} <b>grasp</b> at contact <b>points</b> that minimize slippage and torsion. For smooth objects with complex shape, the set of ideal <b>grasp</b> <b>points</b> is limited and depends on the particular shape. In normal conditions, <b>grasp</b> <b>points</b> could potentially be determined during a planning stage prior to movement. This study tested whether the visual motor system can identify appropriate <b>grasp</b> <b>points</b> during an ongoing movement. We used a perturbation method to isolate online control of grasping. Observers reached to grasp virtual planar objects with varied shapes. On a subset of trials, the object was changed after the onset of movement by either rotating the object by 45 ° (Experiment 1) or by replacing with an entirely different object (Experiment 2). Optimality of the final <b>grasp</b> <b>points</b> was evaluated by two measurements: torque control and force closure control (Blake, 1992). With or without perturbations, grasp axes passed close {{to the center of}} mass (1 mm deviation on average), and had small angular deviations from the surface normals (average force closure angle of 25 °). Corrective adjustments in response to perturbations were detectable within 100 ms of perturbation onset. There was no slowing of the hand or increase in movement duration on perturbed trials, and final <b>grasp</b> <b>points</b> showed no reduction in optimality relative to unperturbed trials with the same object. These results demonstrate that the visual-motor system is capable of rapid, online processing of shape information for guiding the hand to optimal contact <b>points</b> during <b>grasping.</b> link_to_OA_fulltex...|$|R
40|$|PURPOSES: To {{assess the}} tensile {{strength}} of the modified 4 -strand cruciate technique for obliquely lacerated tendons, and to compare the findings with the strength of transversely lacerated tendons repaired at various grasping depths. METHODS: 60 porcine front foot tendons were evenly divided into 4 groups. In groups 1 to 3, tendons were transversely lacerated and repaired with grasping points at both ends away from the laceration by 5 mm, 10 mm, and 15 mm respectively. In group 4, tendons were obliquely lacerated and repaired with a <b>grasping</b> <b>point</b> 5 mm away from the laceration on one end and 15 mm on the other. All tendons were repaired with a modified 4 -strand core suture and continuous epitendinous suture, and then tested to failure in a tensile machine. RESULTS: The tensile strength in group 1 was significantly lower than that in the other 3 groups (p< 0. 005). The tensile strength in group 4 was {{not significantly different from}} groups 2 and 3. CONCLUSION: The tensile strength of modified 4 -strand cruciate repair configuration is not weakened in obliquely lacerated tendons; the <b>grasping</b> <b>point</b> {{at one end of the}} tendon being 15 mm away from laceration provides sufficient strength to compensate for the relatively weak 5 -mm end. So long as one <b>grasping</b> <b>point</b> is away from the laceration site by 10 mm, the ultimate tensile {{strength of the}} transversely lacerated tendons appears acceptable. The modified 4 -strand cruciate repair is safe to use for repairing obliquely lacerated tendons. link_to_OA_fulltex...|$|E
40|$|In Norway, {{the final}} stage of front half chicken {{harvesting}} is still a manual operation {{due to a lack of}} automated systems that are suitably flexible with regard to production efficiency and raw material utilisation. This paper presents the ‘GRIBBOT’ – a novel 3 D vision-guided robotic concept for front half chicken harvesting. It functions using a compliant multifunctional gripper tool that grasps and holds the fillet, scrapes the carcass, and releases the fillet using a downward pulling motion. The gripper has two main components; a beak and a supporting plate. The beak scrapes the fillet down the rib cage of the carcass following a path determined by the anatomical boundary between the meat and the bone of the rib cage. The supporting plate is actuated pneumatically in order to hold the fillet. A computer vision algorithm was developed to process images from an RGB-D camera (Kinect v 2) and locate the <b>grasping</b> <b>point</b> in 3 D as the initial contact point of the gripper with the chicken carcass for harvesting operation. Calibration of camera and robot was performed so that the <b>grasping</b> <b>point</b> was defined using 3 D coordinates within the robot’s base coordinate frame and tool centre point. A feed-forward Look-and-Move control algorithm was used to control the robot arm and generate the motion trajectories, based on the 3 D coordinates of the <b>grasping</b> <b>point</b> as calculated from the computer vision algorithm. The results of an experimental proof-of-concept demonstration showed that GRIBBOT was successful both in scraping the carcass, grasping chicken fillets automatically and in completing the front half fillet harvesting process. It demonstrated a potential for the flexible robotic automation of the chicken fillet harvesting operation. Its commercial application, with further development, can result in automated fillet harvesting, while future research may also lead to optimal raw material utilisation. GRIBBOT shows that there is potential to automate even the most challenging processing operations currently carried out manually by human operators...|$|E
40|$|Abstract. In {{this paper}} we {{address the problem of}} finding an initial good <b>grasping</b> <b>point</b> for the robotic {{manipulation}} of textile objects lying on a flat surface. Given as input a point cloud of the cloth acquired with a 3 D camera, we propose choosing as grasping points those that maximize a new measure of wrinkledness, computed from the distribution of normal directions over local neighborhoods. Real grasping experiments using a robotic arm are performed, showing that the proposed measure leads to promising results...|$|E
40|$|This paper {{presents}} {{an analysis of}} the mechanics for multifingered grasps of planar and solid objects. Squeezing and frictional effects between the fingers and the grasped objects is fully visualized through our approach. An algorithm for qualitively choosing the <b>grasp</b> <b>points</b> is developed based on the mechanics of grasping. It is shown further that our method can be easily extended for the soft-fingered grasp model where the torsional moments along the contact normals can be transmitted through the <b>grasp</b> <b>points...</b>|$|R
40|$|Abstract—This {{research}} {{describes the}} implementation of a vi-sion-based algorithm that is capable of rapidly determining robotic <b>grasp</b> <b>points</b> for planar objects. A representation of the target and a quadtree expansion generate candidate grasps that are compared using a cost function. The approach returns the first acceptable <b>grasp</b> <b>point</b> at a given tree resolution. The system has an execution time on the order of seconds and it is suitable for a large number of planar or near planar objects. Index Terms—Grasp planning, image-based cost function, quad tree. I...|$|R
40|$|In this paper, a novel {{approach}} of grasp planning is applied {{to find out the}} appropriate <b>grasp</b> <b>points</b> for a reconfigurable parallel robot called PARAGRIP (Parallel Gripping). This new handling system is able to manipulate objects in the six-dimensional Cartesian space by several robotic arms using only six actuated joints. After grasping, the contact elements {{at the end of the}} underactuated arm mechanisms are connected to the object which forms a closed loop mechanism similar to the architecture of parallel manipulators. As the mounting and <b>grasp</b> <b>points</b> of the arms can easily be changed, the manipulator can be reconfigured to match the user's preferences and needs. This paper raises the question, how and where these <b>grasp</b> <b>points</b> are to be placed on the object to perform well for a certain manipulation task. This paper was presented at the IFToMM/ASME International Workshop on Underactuated Grasping (UG 2010), 19 August 2010, Montréal, Canada. ...|$|R
40|$|In {{this work}} we {{address the problem}} of object {{detection}} for the purpose of object manipulation in a service robotics scenario. Several implementations of state-of-the-art object detection methods were tested, and the one with the best performance was selected. During the evaluation, three main practical limitations of current methods were identified in relation with long-range object detection, <b>grasping</b> <b>point</b> detection and automatic learning of new objects; and practical solutions are proposed for the last two. Finally, the complete pipeline is evaluated in a real grasping experiment. Preprin...|$|E
40|$|In {{this paper}} we {{address the problem of}} finding an initial good <b>grasping</b> <b>point</b> for the robotic {{manipulation}} of textile objects lying on a flat surface. Given as input a point cloud of the cloth acquired with a 3 D camera, we propose choosing as grasping points those that maximize a new measure of wrinkledness, computed from the distribution of normal directions over local neighborhoods. Real grasping experiments using a robotic arm are performed, showing that the proposed measure leads to promising results. Postprint (author’s final draft...|$|E
40|$|The e. Deorbit {{mission is}} devoted to safely remove Envisat from its orbit by robotic capture means. The major {{challenges}} in the close range are the motion synchronisation between the Chaser and the Target satellite Envisat and the coupled control during capture employing the robot arm. This paper {{is devoted to}} the coupled control phase, during which the Chaser performs station keeping at the Capture Point, which is a point relative to the Target in the Target body frame, while the robot is grasping the Target. The robot arm has to place the end-effector at the <b>Grasping</b> <b>Point,</b> a well-defined position at the Target's launch adapter ring, while compensating the station keeping errors of the Chaser platform. The impedance controlled robot operates in operational space coordinates defining the pose of the robot end-effector with respect to the <b>Grasping</b> <b>Point</b> and also directly controls the robot joint configuration. The bandwidths of the two controllers considered in this study differ by more than two orders of magnitude, allowing independent control design of the two. The overall performance of the coupled control in terms of station keeping performance for the Chaser and positioning performance of the end-effector is demonstrated in Monte Carlo simulations...|$|E
40|$|In {{this work}} {{a system for}} {{recognizing}} <b>grasp</b> <b>points</b> in RGB-D images is proposed. This system {{is intended to be}} used by a domestic robot when deploying clothes lying at a random position on a table. By taking into consideration that the <b>grasp</b> <b>points</b> are usually near key parts of clothing, such as the waist of pants or the neck of a shirt. The proposed system attempts to detect these key parts first, using a local multivariate contour that adapts its shape accordingly. Then, the proposed system applies the Vessel Enhancement filter to identify wrinkles in the clothes, allowing to compute a roughness index for the clothes. Finally, by mixing (i) the key part contours and (ii) the roughness information obtained by the vessel filter, the system is able to recognize <b>grasp</b> <b>points</b> for unfolding a piece of clothing. The recognition system is validated using realistic RGB-D images of different cloth types. Comment: Accepted in the RoboCup Symposium 2017. Final version will be published at Springe...|$|R
40|$|This paper {{presents}} work on vision based robotic grasping. The proposed method {{adopts a}} learning framework where prototypical <b>grasping</b> <b>points</b> are learnt from several examples and then used on novel objects. For representation purposes, we apply {{the concept of}} shape context and for learning we use a supervised learning approach in which the classifier is trained with labelled synthetic images. We evaluate and compare the performance of linear and nonlinear classifiers. Our results show {{that a combination of}} a descriptor based on shape context with a non-linear classification algorithm leads to a stable detection of <b>grasping</b> <b>points</b> for a variety of objects...|$|R
40|$|The {{selection}} of <b>grasping</b> <b>points,</b> the positions {{at which the}} digits make contact with an object's surface in order to pick it up, depends on several factors. In this study, we examined the influence of obstacles on the {{selection of}} <b>grasping</b> <b>points.</b> Subjects reached to grasp a sphere placed on a table. Obstacles were placed either near the anticipated <b>grasping</b> <b>points</b> or near the anticipated elbow position {{at the time of}} contact with the object. In all cases, subjects adjusted the way they moved when there was an obstacle nearby, but only obstacles near the thumb had a consistent influence across subjects. In general, the influence of the obstacle increased as it was placed closer to the digit or elbow, rather than the subject grasping in a manner that would be appropriate for all conditions. This suggests that under these circumstances the configuration of the arm and hand at the moment of contact was a critical factor when selecting at which <b>points</b> to <b>grasp</b> the objects. © 2012 Elsevier B. V...|$|R
30|$|In {{order to}} {{determine}} a grasping position, {{it is necessary to}} recognize how a cloth product is placed and then to find the position to grasp. Recognition of the placed situation is accomplished by performing shape-based registration processing between a task experience data and the current sensor data. On the other hand, <b>grasping</b> <b>point</b> determination is accomplished by detecting visually recognizable borders and counting overlapping of cloth that can be observed there. However, since ambiguity remains, by observing the relationship of the number of the overlapping on a neighboring border, the determination accuracy is improved.|$|E
40|$|The {{grasping}} and {{stabilization of}} a tumbling, non-cooperative target satellite {{by means of}} a free-flying robot is a challenging control problem, which has been addressed in increasing degree of complexity since 20 years. A novel method for computing robot trajectories for grasping a tumbling target is presented. The problem is solved as a motion planning problem with nonlinear optimization. The resulting solution includes a first maneuver of the Servicer satellite which carries the robot arm, taking account of typical satellite control inputs. An analysis of the characteristics of the motion of a <b>grasping</b> <b>point</b> on a tumbling body is used to motivate this grasping method, which is argued to be useful for grasping targets of larger size...|$|E
40|$|Fast {{transportation}} of objects {{is important for}} productivity and efficient improvement in factory operations. Therefore, fast grasping of unknown objects is important. To improve the grasping efficiency, we propose a method for a mobile robot with a gripper to grasp an unknown object quickly based on partial shape information from two 2 D range sensors. The objects can be grasped and lifted by a gripper if the following three conditions are satisfied: There are flat parallel surfaces or parallel tangent planes on the objects. The distance between parallel flat surfaces or parallel tangent planes is not larger than the maximum opening width of the gripper. There is no obstacle near the grasping part when a robot is grasping objects. The conditions given above {{can be defined as}} three features, depth differences, flat surfaces or parallel tangent planes, and gripper insertion space, all of which are used to identify the <b>grasping</b> <b>point.</b> The 2 D range sensor scans the object N times while the robot is moving forward to acquire the partial shape information of an object. The features are then extracted from the scanned data of partial information of an object. Whether a pair of grasping points is included in the scanned data is determined {{on the basis of these}} features. If no grasping points are detected, the robot moves to the next scan position to detect a possible <b>grasping</b> <b>point.</b> Otherwise, if a pair of grasping points exists in the scanned data, the grasping position is calculated. Finally, the robot moves to the grasping position and grasps the object. The proposed approach is tested with experiments. A mobile robot with a parallel-jaw gripper can successfully grasp a wide variety of objects. The grasp success rate is about 90 %. The grasping time of the proposed approach is 49 % shorter than that with the 3 D model construction method...|$|E
40|$|Choosing {{appropriate}} <b>grasp</b> <b>points</b> {{is necessary}} for successfully interacting with objects in our environment. We brought two possible determinants of <b>grasp</b> <b>point</b> selection into conflict: the attempt to grasp an object near its center of mass to minimize torque and ensure stability and the attempt to minimize movement distance. We let our participants grasp two elongated objects of different mass and surface friction that were approached from different distances to {{both sides of the}} object. Maximizing stability predicts <b>grasp</b> <b>points</b> close to the object's center, while minimizing movement costs predicts a bias of the grasp axis toward the side at which the movement started. We found smaller deviations from the center of mass for the smooth and heavy object, presumably because the larger torques and more slippery surface for the heavy object increase the chance of unwanted object rotation. However, our right-handed participants tended to grasp the objects {{to the right of the}} center of mass, irrespective of where the movement started. The rightward bias persisted when vision was removed once the hand was half way to the object. It was reduced when the required precision was increased. Starting the movement above the object eliminated the bias. Grasping with the left hand, participants tended to grasp the object to the left of its center. Thus, the selected <b>grasp</b> <b>points</b> seem to reflect a compromise between maximizing stability by grasping near the center of mass and grasping on the side of the acting hand, perhaps to increase visibility of the object. © 2014 Springer-Verlag...|$|R
40|$|Perception and action: Reaching and {{grasping}} - Poster Session: no. 33. 303 When {{picking up}} smooth objects with complex shape, {{we tend to}} grasp at particular contact points that minimize slip and torsion. We have recently found that observers can quickly adjust movement and adopt ideal <b>grasp</b> <b>points</b> when an object is unexpectedly changed during an ongoing movement. This study tested whether shape processing for online control of grasping can utilize second-order contour information. The dorsal processing stream {{is thought to be}} less sensitive to second-order stimuli, so it might be difficult to make fast, online corrections to <b>grasp</b> <b>points</b> when shapes are specified by second-order contours. We recorded finger positions as observers reached to grasp virtual planar objects with smooth, random shapes. On perturbed trials, the initial object was replaced by a new object with different shape after the movement had begun, and observers adjusted their movement to grasp the new object. Three stimulus conditions were compared. In the baseline condition, stimuli were solid colored 2 D shapes on a darker background. In the second-order condition, a visible contour was created by inverting a region of a low-pass noise pattern corresponding to the interior of a shape. A coherent shape can be perceived from the integrated boundary edges, which change luminance polarity along the contour. In a third condition, the base images were high-pass filtered, which removes low frequencies but preserves polarity of edges along the contour. In all conditions, we observed smooth corrective responses to perturbations, resulting in final <b>grasp</b> <b>points</b> that were near optimal and equivalent to the <b>grasp</b> <b>points</b> of unperturbed trials with the same object. We found no differences between the three stimulus conditions; performance was as accurate for the second-order contours as for shapes specified by luminance difference. The visual-motor system appears to be capable of robust shape processing for online control of grasping. link_to_OA_fulltex...|$|R
40|$|Abstract — In {{this paper}} we study the {{learning}} of affordance knowledge through self-experimentation. We study {{the learning of}} local visual descriptors that anticipate {{the success of a}} given action executed upon an object. Consider, for instance, the case of grasping. Although graspable is a property of the whole object, the grasp action will only succeed if applied in the right part of the object. We propose an algorithm to learn local visual descriptors of good <b>grasping</b> <b>points</b> based on a set of trials performed by the robot. The method estimates the probability of a successful action (grasp) based on simple local features. Experimental results on a humanoid robot illustrate how our method is able to learn descriptors of good <b>grasping</b> <b>points</b> and to generalize to novel objects based on prior experience. I...|$|R
