176|124|Public
25|$|The Google Earth's {{cache size}} {{is limited to}} 2000MB whereas World Wind has no limit on cache size. In Norkart Virtual Globe the <b>disk</b> <b>cache</b> can be set by the user.|$|E
25|$|In practice, if {{the main}} {{database}} is being frequently searched, the aux-aux index {{and much of}} the aux index may reside in a <b>disk</b> <b>cache,</b> so they would not incur a disk read.|$|E
25|$|Firefox 1.5 (and other Gecko-based browsers) {{and later}} {{versions}} use fast Document Object Model (DOM) caching. JavaScript is only executed on pageload from net or <b>disk</b> <b>cache,</b> {{but not if}} it is loaded from DOM cache. This can affect JavaScript-based tracking of browser statistics.|$|E
5000|$|... 2008: IEEE Reynold B. Johnson Information Storage Systems Award for [...] "contributions to the {{performance}} analysis of computer storage systems, including improvements to <b>disk</b> <b>caches,</b> prefetching and data placement" ...|$|R
50|$|It {{includes}} Norton Disk Doctor, Disk Editor, Disk Tools, Speed <b>Disk,</b> Norton <b>Cache,</b> <b>Disk</b> Monitor, Diskreet, NDisk, System Information, NDOS.|$|R
40|$|Client disks ure a {{valuable}} resource that ure not adequately ex-ploited by current client-server ciatabase systems. In this papeG we propose the use of client disks for caching database pages in an extended cache architecture. We describe four algorithms. fi) r managing <b>disk</b> <b>caches</b> and investigate the tradeoffs inherent in keeping a large volume of disk-cached data consistent using u detailed simulation model. The study shows that significant performance gains can be obtained through client disk caching; particulurly if the client <b>disk</b> <b>caches</b> ure kept consistent. We also address two extensions to the algorithms that arise due to the pc,tiormance characteristics of large disk caches: I) methods to reduce the work performed by the server to ensure transaction durability, and 2) techniques for bringing a large disk-residen...|$|R
2500|$|Enable {{advanced}} performance {{option for}} hard disks: When enabled, {{the operating system}} may cache disk writes as well as disk reads. [...] In previous Windows operating systems, only the disk's internal disk caching, if any, was utilised for disk write operations when the <b>disk</b> <b>cache</b> was enabled by the user. [...] Enabling this option causes Windows {{to make use of}} its own local cache in addition to this, which speeds up performance, at the expense of a little more risk of data loss during a sudden loss of power.|$|E
2500|$|ZFS uses {{different}} layers of <b>disk</b> <b>cache</b> {{to speed up}} read and write operations. Ideally, all data should be stored in RAM, but that is usually too expensive. Therefore, data is automatically cached in a hierarchy to optimize performance versus cost; these are often called [...] "hybrid storage pools". Frequently accessed data will be stored in RAM, and less frequently accessed data can be stored on slower media, such as solid state drives (SSDs). Data that is not often accessed is not cached and left on the slow hard drives. If old data is suddenly read a lot, ZFS will automatically move it to SSDs or to RAM.|$|E
5000|$|No <b>Disk</b> <b>Cache</b> - The browser <b>disk</b> <b>cache</b> {{has been}} {{disabled}} to decrease disk size {{and the number of}} writes to the disk, possibly increasing disk life.|$|E
40|$|Low disk {{throughput}} {{is one of}} {{the main}} impediments to improving the performance of data-intensive servers. In this paper, we propose two management techniques for the <b>disk</b> controller <b>cache</b> that can significantly increase disk throughput. The first technique, called File-Oriented Read-ahead (FOR), adjusts the number of read-ahead blocks brought into the <b>disk</b> controller <b>cache</b> according to file system information. The second technique, called Host-guided Device Caching (HDC), gives the host control over part of the <b>disk</b> controller <b>cache.</b> As an example use of this mechanism, we keep the blocks that cause the most misses in the host buffer cache permanently cached in the disk controller. Our detailed simulations of real server workloads show that FOR and HDC can increase disk throughput by up to 34 % and 24 %, respectively, in comparison to conventional <b>disk</b> controller <b>cache</b> management techniques. When combined, the techniques can increase throughput by up to 47 %...|$|R
40|$|The I/O {{subsystem}} in {{a computer}} system is becoming the bottleneck {{as a result of}} recent dramatic improvements in processor speeds. <b>Disk</b> <b>caches</b> have been effective in closing this gap but the benefit is restricted to the read operations as the write I/Os are usually committed to disk to maintain consistency and to allow for crash recovery. As a result, write I/O traffic is becoming dominant and solutions to alleviate this problem are becoming increasingly important. A simple solution which can easily work with existing file systems is to use non-volatile <b>disk</b> <b>caches</b> together with a write-behind strategy. In this study, we look at the issues around managing such a cache using a detailed trace driven simulation. Traces from three different commercial sites are used in the analysis of various policies for managing the write cache. We observe that even a simple write-behind policy for the write cache is effective in reducing the total number of writes by over 50 %. We further obser [...] ...|$|R
40|$|The {{algorithm}} is much simpler than previous linear time algorithms {{that are all}} based on the more complicated suffix tree data structure. Since sorting is a well studied problem, we obtain optimal algorithms for several other models of computation, e. g. external memory with parallel <b>disks,</b> <b>cache</b> oblivious, and parallel. The adaptations for BSP and EREW-PRAM are asymptotically faster than the best previously known algorithms...|$|R
5000|$|Page cache, a <b>disk</b> <b>cache</b> that {{utilizes}} {{virtual memory}} mechanism ...|$|E
5000|$|... 1982 - Microcache (the world's first <b>disk</b> <b>cache</b> for microcomputers) {{released}} ...|$|E
5000|$|Has an {{adjustable}} {{read and write}} <b>disk</b> <b>cache</b> for improved disk throughput.|$|E
50|$|The {{requirement}} that all disks spin synchronously (in a lockstep) added design considerations {{to a level}} that provided no significant advantages over other RAID levels, so it quickly became useless and is now obsolete. Both RAID 3 and RAID 4 were quickly replaced by RAID 5. RAID 3 was usually implemented in hardware, and the performance issues were addressed by using large <b>disk</b> <b>caches.</b>|$|R
5000|$|Storage <b>caches</b> (including <b>disk</b> <b>caches</b> for files, or {{processor}} caches {{for either}} code or data) work also like a lookup table. The table is built with very fast memory {{instead of being}} stored on slower external memory, and maintains two pieces of data for a subrange of bits composing an external memory (or disk) address (notably the lowest bits of any possible external address): ...|$|R
50|$|The Independent Reference Model (I.R.M) is a {{conceptual}} model {{used in the}} analysis of storage system: <b>disk</b> drives, <b>caches,</b> etc.|$|R
5000|$|Flashcache a <b>disk</b> <b>cache</b> {{component}} for the Linux kernel, initially {{developed by}} Facebook ...|$|E
5000|$|PC-Cache — a {{licensed}} <b>disk</b> <b>cache</b> of HyperCache from the HyperDisk Speed Kit ...|$|E
50|$|On low-memory systems, the <b>disk</b> <b>cache</b> can be {{disabled}} altogether or set {{to smaller}} limit, to save memory.|$|E
40|$|The {{full text}} {{of this article is}} not {{available}} on SOAR. WSU users can access the article via IEEE Xplore database licensed by University Libraries: [URL] replacement algorithms are an important component of any cache controller, specially for <b>disk</b> <b>caches.</b> In recent years, complex and sophisticated algorithms have been developed, that reduce the miss-rate of <b>disk</b> <b>caches.</b> All of these algorithms require some sort of tuning in order to achieve their full performance. Up until now, the “optimal” parameters have been determined by running simulations with access traces. Parameters found have then been implemented {{in the hope that the}} actual access patterns would not deviate too much from the ones used in the simulations. When they did, miss-rates would increase, often exceeding miss-rates of more conventional replacement strategies like the least recently used block replacement algorithm (LRU). In this paper, we present a method of adjusting an algorithm's tuning parameter at run-time, by constantly monitoring its performance. We will show that dynamic tuning is able to approach the results of an optimal value that was determined by using simulations. Additionally, we will demonstrate the ability of this new technique to adapt to completely different workloads. Peer reviewed articl...|$|R
40|$|Abstract The rapid {{progress}} in mass storage technology is enabling designers to implement large databases {{for a variety}} of applications. The possible configurations of the hierarchical storage structures (HSS) are numerous and result in various tradeoffs, e. g., storage cost, throughput, initial latency, etc. A naive design might waste system resources and result in high cost. For example, one naive design decision is to add <b>disk</b> <b>caches</b> for those applications whose bandwidth is already satisfied by the tertiary storage device and can tolerate a high latency...|$|R
5000|$|Cache (page 126): {{there is}} a {{confusion}} between the high-speed variant on RAM, known as CPU cache, and the hard <b>disk</b> web <b>cache</b> used by a web browser ...|$|R
50|$|Java Quick Starter reduces {{application}} start-up time by preloading part of JRE data at OS startup on <b>disk</b> <b>cache.</b>|$|E
5000|$|Akamai's {{web servers}} use Bloom filters to prevent [...] "one-hit-wonders" [...] from being stored in its disk caches. One-hit-wonders are web objects {{requested}} by users just once, something that Akamai found applied to nearly three-quarters of their caching infrastructure. Using a Bloom filter {{to detect the}} second request for a web object and caching that object only on its second request prevents one-hit wonders from entering the <b>disk</b> <b>cache,</b> significantly reducing disk workload and increasing <b>disk</b> <b>cache</b> hit rates.|$|E
50|$|As part {{of their}} Multimedia Cloaking product, Helix {{provided}} cloaked versions of Logitech's MOUSE 6.33 driver, Microsoft's MSCDEX, and a home-grown <b>disk</b> <b>cache</b> to replace Microsoft's SmartDrive drivers.|$|E
50|$|Filesystems {{going through}} the device mapper {{interface}} (including software RAID and LVM implementations) may not support barriers, and will issue a warning if that mount option is used. There are also some disks that do not properly implement the write cache flushing extension necessary for barriers to work, which causes a similar warning. In these situations, where barriers are not supported or practical, reliable write ordering is possible by turning off the <b>disk's</b> write <b>cache</b> and using the data=journal mount option. Turning off the <b>disk's</b> write <b>cache</b> may be required even when barriers are available.|$|R
40|$|DOLIB (Distributed Object Library) emulates global {{shared memory}} in {{distributed}} memory environments intended for scientific applications. Access to global arrays is through explicit calls {{to gather and}} scatter. Use of DOLIB does not rely on language extension, compiler or operating system supports. Shared memory provided by DOLIB was also used by DONIO (Distributed Network I/O Library) as large <b>disk</b> <b>caches</b> that gave improvements of 15 to 30 fold on the Intel Paragon. DOLIB shared memory simplifies the parallelization of the CHAMMP Semi-Lagrangian Transport (SLT) code that has particle tracking as the kernel computation...|$|R
50|$|One lesser-known {{feature of}} NCQ is that, unlike its ATA TCQ predecessor, {{it allows the}} host to specify whether {{it wants to be}} {{notified}} when the data reaches the disk's platters, or when it reaches the <b>disk's</b> buffer (on-board <b>cache).</b> Assuming a correct hardware implementation, this feature allows data consistency to be guaranteed when the <b>disk's</b> on-board <b>cache</b> is used in conjunction with system calls like fsync. The associated write flag, which is also borrowed from SCSI, is called Force Unit Access (FUA).|$|R
50|$|EnhanceIO is a <b>disk</b> <b>cache</b> module for the Linux kernel. Its {{goal is to}} use fast but {{relatively}} small SSD drives to improve the performance of large but slow hard drives.|$|E
5000|$|Microcosm Ltd is a UK company {{established}} in 1979. Its early claims to fame included Silicon Disk System in 1981 and Microcache (the world's first <b>disk</b> <b>cache</b> for microcomputers) in 1982.|$|E
50|$|The Google Earth's {{cache size}} {{is limited to}} 2000 MB whereas World Wind has no limit on cache size. In Norkart Virtual Globe the <b>disk</b> <b>cache</b> can be set by the user.|$|E
50|$|Sprite (operating system) used large <b>disk</b> block <b>caches.</b> These {{were located}} in main-memory to achieve high {{performance}} in its file system. The term CacheFS has found little or no use to describe caches in main memory.|$|R
40|$|The {{functionality}} that {{is provided}} by Mass Storage Systems can be implemented using data grid technology. Data grids already provide many of the required features, including a logical name space and a storage repository abstraction. We demonstrate how management of tape resources can be integrated into data grids. The resulting infrastructure {{has the ability to}} manage archival storage of digital entities on tape or other media, while maintaining copies on distributed, remote <b>disk</b> <b>caches</b> that can be accessed through advanced discovery mechanisms. Data grids provide additional levels of data management including the ability to aggregate data into containers before storage on tape, and the ability to migrate collections across a hierarchy of storage devices...|$|R
40|$|Data grids {{federate}} storage resources. They {{provide a}} logical name space {{that can be}} used to register digital entities, a storage repository abstraction for manipulating data, and a high level abstraction for supporting user-selected interfaces. Data grids can be used to build persistent collections. Data can be stored across multiple types of storage systems with persistent copies kept in archives. Persistent identifiers can be kept in the logical name space. By integrating Grid Bricks (commodity based <b>disk</b> <b>caches)</b> with archival storage systems, one can assemble a data management environment that supports both interactive access (data picking) and long-term persistent storage. Examples of the creation of interactive data picking environments will be given that integrate Grid Brick technology with large-scale archives...|$|R
