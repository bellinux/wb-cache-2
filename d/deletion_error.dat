22|141|Public
30|$|An {{incorrectly}} classified audio segment (a substitution) is computed {{both as a}} <b>deletion</b> <b>error</b> for one AC and an insertion error for another. A forgiveness {{collar of}} 1 s (both + and -) is not scored around each reference boundary. This accounts for both the inconsistencies of human annotation and the uncertainty about when an AC begins/ends.|$|E
3000|$|... [...]) is {{the total}} {{duration}} of all the i th AC instances according to the reference file. The incorrectly classified audio segment (a substitution) is computed both as a <b>deletion</b> <b>error</b> for one AC and an insertion error for another. A collar of 1 s is not scored around each reference boundary to avoid the uncertainty about when an AC begins or ends.|$|E
40|$|The {{problem of}} {{selecting}} a code to transmit four messages over the binary symmetric channel is studied {{in relation to}} two types of channel noise (“substitution≓ error and “deletion≓ error) and to two types of decoding strategy (maximum hit and minimum error). It is shown that mean Hamming distance is a good general guide to coding efficiency, except {{in the case of}} a minimum error strategy with a <b>deletion</b> <b>error</b> channel, where coding efficiency is critically dependent on noise level. An experiment in which subjects selected codes in an artificial language suggests that the process of recall from memory is similar to the process of transmitting over a <b>deletion</b> <b>error</b> channel with a minimum error strategy. A similar interpretation can be placed on the analysis of consonant systems in English, French, German and Welsh, where the sets of consonants of a given class in a given environment are considered as codes whose alphabet is the phonological distinctive feature system of Halle (1958...|$|E
40|$|<b>Deletion</b> <b>errors</b> {{are most}} usually {{occurred}} in connected Mandarin digit string speech recognition when speaking rate is fast, {{and are the}} main reasons leading to the increasing of the recognition error rate {{and the decline of}} the recognition accuracy.   In this paper, a new training method named neighboring digits pattern is given based on sufficient statistics of recognition errors of  the traditional system in order to eliminate most of <b>deletion</b> <b>errors</b> which seriously affect the system recognition rate. The training process is presented and the performance evaluation is given. The result analysis demonstrates that the new method can reduce the <b>deletion</b> <b>errors</b> effectively and improve the system recognition rate from 96. 4 % to 98. 3 %...|$|R
3000|$|... [...]. These {{parameters}} are usually set experimentally to values where {{the number of}} insertion <b>errors</b> and <b>deletion</b> <b>errors</b> in recognition is nearly equal. We fixed the values to [...]...|$|R
30|$|It {{is worth}} {{mentioning}} that insertion errors (IE) never {{occurred in the}} proposed PIC detector in the tests done and the <b>deletion</b> <b>errors</b> only occur when the polyphony number is estimated.|$|R
40|$|Background: 454 {{pyrosequencing}} is {{a commonly}} used massively parallel DNA sequencing technology {{with a wide}} variety of application fields such as epigenetics, metagenomics and transcriptomics. A well-known problem of this platform is its sensitivity to base-calling insertion and deletion errors, particularly in the presence of long homopolymers. In addition, the base-call quality scores are not informative with respect to whether an insertion or a <b>deletion</b> <b>error</b> is more likely. Surprisingly, not much effort has been devoted to the development of improved base-calling methods and more intuitive quality scores for this platform...|$|E
40|$|Abstract: A new insertion/deletion {{correction}} {{scheme is}} presented for standard convolutional codes that {{makes use of}} multiple parallel-interconnected Viterbi decoders. Whenever an insertion or <b>deletion</b> <b>error</b> occurs, the connections between different Viterbi decoders ensure that decoding will proceed from the decoder that is in synchronization. In this way, a larger Viterbi decoder is created that can correct insertion and/or deletion errors by extending the Viterbi algorithm to encompass all parallel decoders. Further, it is shown how the performance can be improved by inverting certain bits during the encoding of the convolutional codes. This lowers the frequency of occurrence of repeating sequences, which is detrimental to synchronization when dealing with insertions/deletions...|$|E
40|$|When a multi-lingual question-answering (QA) system {{provides}} {{an answer that}} has been incorrectly translated, {{it is very likely}} to be regarded as irrelevant. In this paper, we propose a novel method for correcting a <b>deletion</b> <b>error</b> that affects overall understanding of the sentence. Our post-editing technique uses information available at query time: examples drawn from related documents determined to be relevant to the query. Our results show that 4 %- 7 % of MT sentences are missing a main verb and on average, 79 % of the sentences modified by our system are judged to be more comprehensible. QA performance also benefits from the improved MT: 7 % of irrelevant response sentences become relevant...|$|E
40|$|D. Ing. In Information Theory, {{synchronization}} errors can be modelled as {{the insertion}} and <b>deletion</b> of symbols. <b>Error</b> correcting codes are proposed {{in this research}} {{as a method of}} recovering from a single insertion or deletion error; adjacent multiple deletion errors; or multiple insertion, <b>deletion</b> and substitution <b>errors.</b> A moment balancing template is a single insertion or deletion correcting construction based on number theoretic codes. The implementation of this previously published technique is extended to spectral shaping codes, (d, k) constrained codes and run-length limited sequences. Three new templates are developed. The rst one is an adaptation to DC-free codes, and the second one is an adaptation to spectral null codes. The third one is a generalized moment balancing template for both (d, k) constrained codes and run-length limited sequences. Following this, two new coding methods are investigated to protect a binary sequence against adjacent <b>deletion</b> <b>errors.</b> The rst class of codes is a binary code derived from the Tenengolts non-binary single insertion or deletion correcting code, with additional selection rules. The second class of codes is designed by using interleaving techniques. The asymptotic cardinality bounds of these new codes are also derived. Compared to the previously published codes, the new codes are more exible, since they can protect against any given xed known length of adjacent <b>deletion</b> <b>errors.</b> Based on these two methods, a nested construction is further proposed to guarantee correction of adjacent <b>deletion</b> <b>errors,</b> up to a certain xed number...|$|R
30|$|The beat {{detection}} {{errors are}} divided into three classes: substitution, insertion and <b>deletion</b> <b>errors.</b> Substitution error means that a beat is poorly estimated {{in terms of the}} tempo or bar-position. Insertion <b>errors</b> and <b>deletion</b> <b>errors</b> are false-positive and false-negative estimations. We assume that a player does not know the other's score, thus one estimates score position by number of beats {{from the beginning of the}} performance. Beat insertions or deletions undermine the musical ensemble because the cumulative number of beats should be correct or the performers will lose synchronization. Algorithm 1 shows how to detect inserted and deleted beats. Suppose that a beat-tracker correctly detects two beats with a certain false estimation between them. When the method just incorrectly estimates a beat there, we regard it as a substitution error. In the case of no beat or two beats there, they are counted as a deleted or inserted beats, respectively.|$|R
40|$|This paper {{discusses}} {{the usage of}} short term energy contour of a speech smoothed by a fuzzy-based method to automatically segment the speech into syllabic units. Two additional procedures, local normalization and postprocessing, are proposed to improve the method. Testing to Indonesian speech dataset shows that local normalization significantly improves the accuracy of fuzzy smoothing. In postprocessing step, the procedure of splitting missed short syllables reduces the <b>deletion</b> <b>errors,</b> but unfortunately it increases the insertion ones. On the other hand, an assimilation of a single consonant segment into its previous or next segment reduces the insertion errors, but increases the deletion ones. The sequential combination of splitting and then assimilation gives quite significant improvement of accuracy as well as reduction of <b>deletion</b> <b>errors,</b> but it slightly increases the insertion ones...|$|R
40|$|For spoken term detection, it {{is crucial}} to {{consider}} out-of-vocabulary (OOV) and the mis-recognition of spoken words. Therefore, various sub-word unit based recognition and re-trieval methods have been proposed. We also proposed a dis-tant n-gram indexing/retrieval method for spoken queries, which is based on a syllable n-gram and incorporates a dis-tance metric in a syllable lattice. The distance represents confidence score of the syllable n-gram assumed the recog-nition error such as substitution error, insertion error and <b>deletion</b> <b>error.</b> To address spoken queries, we propose a com-bination of candidates obtained through some ASR systems which are based on syllable or word units. We run some ex-periments on the NTCIR- 11 SpokenQuery&Doc Task and report the evaluation results...|$|E
40|$|Abstract—We {{investigate}} {{the problem of}} designing pairs () of words with the property that, if each word of a coded message is prefixed by and suffixed by, the resulting set of coded messages is error detecting with finite delay. We consider (combinatorial) channels permitting any combination of the substitution, insertion, and deletion (SID) error types, and address the cases of both scattered and burst errors. A pair () with the above property is evaluated in terms of three parameters: redundancy, delay of decoding, and frequency of the detectable errors. In the case of SID channels with burst errors, we provide a complete and explicit characterization of their error-detecting pairs (), which involves {{the period of the}} word. IEEE Proof Index Terms—Burst errors, decoding delay, <b>deletion,</b> <b>error</b> detection, insertion, period of word...|$|E
40|$|Abstract: A {{combined}} code is a {{code that}} combines {{two or more}} characteristics of other codes. A construction is {{presented in this paper}} of permutation codes that are self-synchronizing and able to correct a number of deletion errors per codeword, thus a combined permutation code. Synchronization errors, modelled as deletion(s) and/or insertion(s) of bits or symbols, can be catastrophic if not detected and corrected. Some classes of codes have been proposed that are synchronizable, i. e. they can be used to regain synchronization although the error leading to the loss of synchronization is not corrected. Typically, different classes of codes are needed to correct deletion and/or insertion errors after codeword boundaries have been detected. The codebooks presented in this paper consist of codewords divided into segments. By imposing restrictions on the segments, the codewords are synchronizable. One <b>deletion</b> <b>error</b> can be detected and corrected per segment...|$|E
40|$|Abstract—We {{propose a}} new channel {{model for the}} write channel in Magnetic Recording with Bit-Patterned Media. We study {{information}} theoretic propoerties of this channel and suggest a simplistic rate- 1 / 2 coding scheme that achieves zero error. Based on this channel model, we propose a channel with insertion and <b>deletion</b> <b>errors.</b> I...|$|R
40|$|Part 2 : Work in ProgressInternational audienceAn {{invisible}} flow watermarking QIM scheme {{based on}} linear error-correcting subcodes for channels with substitution and <b>deletion</b> <b>errors</b> is proposed in this paper. The evaluation of scheme demonstrates similar to known scheme performance but with lower complexity as soon as its implementation is mainly based on linear decoding operations...|$|R
40|$|Transcription and {{editorial}} phenomena Which {{features of a}} primary source might one want to include in a transcription? variant letter forms page layout orthography capitalisation word division punctuation abbreviations additions and <b>deletions</b> <b>errors</b> and omissions But wait! We also transcribe spoken texts and much of this applies to them as well! Transcription {{and editorial}} phenomen...|$|R
40|$|Molecular sequences, {{like all}} {{experimental}} data, have finite error rates. The impact of errors {{on the information}} content of molecular sequence data {{is dependent on the}} analytic paradigm used to interpret the data. We studied the impact of nucleic acid sequence errors on the ability to align predicted amino acid sequences with the sequences of related proteins. We found that with a simultaneous translation and alignment algorithm, identification of sequence homologies is resilient to the introduction of random errors. Proteins with greater than 30 % sequence identity can be reliably recognized even in the presence of 1 % frameshifting (insertion or <b>deletion)</b> <b>error</b> rates and 5 % base substitution rates. Incorporation of prior knowledge about the location and characteristics of errors improves tolerance to error of amino acid sequence alignments. Similarly, inclusion of prior knowledge of biased codon utilization by yeast (Saccharomyces cerevisiae) allows reliable detection of correct reading frames in yeast sequences even in the presence of 5 % substitution and 1 % frameshift errors...|$|E
40|$|Abstract: This paper {{presents}} {{our work}} on vowel detection and classification {{as part of}} a project of Persian continuous speech recognition based on demi-syllable units. Recent work in machine learning has focused on models, such as the support vector machine (SVM), that automatically control the generalization and discrimination as part of overall optimization process. In this research we track all vowel phonemes in speech signal and in order to achieve minimum vowel losses and accurate detection, we focus on taking special care of vowel detector and classifier using support vector machines and acoustic data of speech signal. At first this technique has been tested with Persian isolated words database and good results have been achieved from this algorithm Experiments with this database result in 1. 62 % of total detection and classification errors. The insertion error is 1. 06 %, the <b>deletion</b> <b>error</b> is 0. 56 % and the classification error is 0...|$|E
40|$|In this work, {{we propose}} constructions that correct duplications of {{multiple}} consecutive symbols. These errors {{are known as}} tandem duplications, where a sequence of symbols is repeated; respectively as palindromic duplications, where a sequence is repeated in reversed order. We compare the redundancies of these constructions with code size upper bounds that are obtained from sphere packing arguments. Proving that an upper bound on the code cardinality for tandem deletions is also an upper bound for inserting tandem duplications, we derive the bounds based on this special tandem <b>deletion</b> <b>error</b> as this results in tighter bounds. Our upper bounds on the cardinality directly imply lower bounds on the redundancy which we compare with the redundancy {{of the best known}} construction correcting arbitrary burst insertions. Our results indicate that the correction of palindromic duplications requires more redundancy than the correction of tandem duplications and both significantly less than arbitrary burst insertions. Comment: 24 pages, 1 figure. arXiv admin note: text overlap with arXiv: 1707. 0005...|$|E
40|$|Error {{estimating}} codes (EEC) {{have recently}} been proposed for mea-suring the bit error rate (BER) in packets transmitted over wireless links. They however can provide such measurements only {{when there are no}} insertion and <b>deletion</b> <b>errors,</b> which could occur in various wireless network environments. In this work, we propose “idEEC”, the first technique that can do so even in the presence of insertion and <b>deletion</b> <b>errors.</b> We show that idEEC is provable robust under most bit insertion and deletion scenarios, provided in-sertion/deletion errors occur with much lower probability than bit flipping errors. Our idEEC design can build upon any existing EEC scheme. The basic idea of the idEEC encoding is to divide the packet into a number of segments, each of which is encoded using the underlying EEC scheme. The basic idea of the idEEC decoding is to divide the packet into a few slices in a randomized manne...|$|R
40|$|This paper {{investigates the}} issue of {{automatic}} segmentation of speech recordings for broadcast news (BN) and broadcast conversation (BC) speech recognition. Our previous segmentation algorithm often exhibited high <b>deletion</b> <b>errors,</b> where some speech segments were misclassified as non-speech and thus were never {{passed on to the}} recognizer. In contrast with our previous segmentation models, which only differentiated between speech and non-speech segments, phonetic knowledge is applied to represent speech by using multiple models for different types of speech segments. Moreover, the “pronunciation ” of the speech segment has been modified to loosen the minimum duration constraint. This method makes use of language specific knowledge, while keeping the number of models low to achieve fast segmentation. Experimental results show that the new segmenter outperforms our previous segmenter significantly, particularly in reducing <b>deletion</b> <b>errors.</b> Index Terms: speech recognition, automatic acoustic segmentation, machine translation, broadcast news, broadcast conversation 1...|$|R
40|$|This paper {{presents}} {{a new kind}} of acoustic features for HMM speech recognition. These features try to capture phone-specific segmentation information using multiple temporal resolutions. Experiments show that word accuracy can be improved by 7 % when combining these features with traditional mel-cepstral coefficients in a speaker-independent word recogniser. This improvement is mostly due to a reduced number of insertion and <b>deletion</b> <b>errors...</b>|$|R
40|$|Abstract 				 				 					 						Background 					 					Recent {{methods have}} been {{developed}} to perform high-throughput sequencing of DNA by Single Molecule Sequencing (SMS). While Next-Generation sequencing methods may produce reads up to several hundred bases long, SMS sequencing produces reads up to tens of kilobases long. Existing alignment methods are either too inefficient for high-throughput datasets, or not sensitive enough to align SMS reads, which have a higher error rate than Next-Generation sequencing. 				 				 					 						Results 					 					We describe the method BLASR (Basic Local Alignment with Successive Refinement) for mapping Single Molecule Sequencing (SMS) reads that are thousands of bases long, with divergence between the read and genome dominated by insertion and <b>deletion</b> <b>error.</b> The method is benchmarked using both simulated reads and reads from a bacterial sequencing project. We also present a combinatorial model of sequencing error that motivates why our approach is effective. 				 				 					 						Conclusions 					 					The results indicate {{that it is possible to}} map SMS reads with high accuracy and speed. Furthermore, the inferences made on the mapability of SMS reads using our combinatorial model of sequencing error are in agreement with the mapping accuracy demonstrated on simulated reads...|$|E
40|$|Abstract Background Recent {{methods have}} been {{developed}} to perform high-throughput sequencing of DNA by Single Molecule Sequencing (SMS). While Next-Generation sequencing methods may produce reads up to several hundred bases long, SMS sequencing produces reads up to tens of kilobases long. Existing alignment methods are either too inefficient for high-throughput datasets, or not sensitive enough to align SMS reads, which have a higher error rate than Next-Generation sequencing. Results We describe the method BLASR (Basic Local Alignment with Successive Refinement) for mapping Single Molecule Sequencing (SMS) reads that are thousands of bases long, with divergence between the read and genome dominated by insertion and <b>deletion</b> <b>error.</b> The method is benchmarked using both simulated reads and reads from a bacterial sequencing project. We also present a combinatorial model of sequencing error that motivates why our approach is effective. Conclusions The results indicate {{that it is possible to}} map SMS reads with high accuracy and speed. Furthermore, the inferences made on the mapability of SMS reads using our combinatorial model of sequencing error are in agreement with the mapping accuracy demonstrated on simulated reads. </p...|$|E
40|$|Background: 454 {{pyrosequencing}} is {{a commonly}} used massively parallel DNA sequencing technology {{with a wide}} variety of application fields such as epigenetics, metagenomics and transcriptomics. A well-known problem of this platform is its sensitivity to base-calling insertion and deletion errors, particularly in the presence of long homopolymers. In addition, the base-call quality scores are not informative with respect to whether an insertion or a <b>deletion</b> <b>error</b> is more likely. Surprisingly, not much effort has been devoted to the development of improved base-calling methods and more intuitive quality scores for this platform. Results: We present HPCall, a 454 base-calling method based on a weighted Hurdle Poisson model. HPCall uses a probabilistic framework to call the homopolymer lengths in the sequence by modeling well-known 454 noise predictors. Base-calling quality is assessed based on estimated probabilities for each homopolymer length, which are easily transformed to useful quality scores. Conclusions: Using a reference data set of the Escherichia coli K- 12 strain, we show that HPCall produces superior quality scores that are very informative towards possible insertion and deletion errors, while maintaining a base-calling accuracy that is better than the current one. Given the generality of the framework, HPCall has the potential to also adapt to other homopolymer-sensitive sequencing technologies...|$|E
40|$|This paper {{deals with}} the problem of {{estimating}} a transmitted string X, from the corresponding received string Y, which is a noisy version of X,. We assume that Y contains*any number of substitution, insertion, and <b>deletion</b> <b>errors,</b> and that no two consecutive symbols of X, were deleted in transmission. We have shown that for channels which cause independent errors, and whose error probabilities exceed those of noisy strings studied in the literature [121, at least 99. 5 % of the erroneous strings will not contain two consecutive <b>deletion</b> <b>errors.</b> The best estimate X * of X, is defined as that element of H which minimizes the generalized Levenshtein distance D (X/Y) between X and Y. Using dynamic programming principles, an algorithm is presented which yields X+ without computing individually the distances between every word of H and Y. Though this algorithm requires more memory, it can be shown that it is, in general, computationally less complex than all other existing algorithms which perform the same task...|$|R
40|$|RNA {{can be used}} as a {{high-density}} {{medium for}} data storage and transmission; however, an important RNA process [...] replication [...] is noisy. This paper presents an error analysis for RNA as a data transmission medium, analyzing how <b>deletion</b> <b>errors</b> increase in a collection of replicated DNA strands over time. Comment: Accepted for publication in the 2016 IEEE International Conference on Nanotechnology (IEEE NANO), Sendai, Japa...|$|R
3000|$|... where Substitution errors (SE): {{happen when}} a note, {{that does not}} exist in the chord, is {{detected}} as played note, <b>Deletion</b> <b>errors</b> (DE): appear when the number of detected notes is smaller than the number of notes in a chord, Insertion errors (IE): appear when the number of detected notes is larger than the number of notes in a chord, NN: represents the number of notes in the chords.|$|R
40|$|An {{algorithm}} is described that can detect certain errors within coding regions of DNA sequences. The {{algorithm is}} {{based on the idea that}} an insertion or <b>deletion</b> <b>error</b> within a coding sequence would interrupt the reading frame and cause the correct translation of a DNA sequence to require one or more frameshifts. If the coding sequence shows similarity to a known protein sequence then such errors can be detected by comparing the conceptual translations of DNA sequences in all six reading frames with every sequence in a protein sequence data base. We have incorporated these ideas into a computer program, called DETECT, that can serve as an aid to the experimentalist who is determining new DNA sequences so that obvious errors may be located and corrected. The program has been tested using raw experimental data and against sequences from the European Molecular Biology Laboratory data base, annotated as containing frameshifts. We have also tested it using unidentified open reading frames that flank known, annotated genes in the GenBank data base. Many potential errors are apparent and in some cases functions can be suggested for the "corrected" versions of these reading frames leading to the identification of new genes. As more sequences are determined the power of this method will increase substantially...|$|E
40|$|Abstract Background 454 {{pyrosequencing}} is {{a commonly}} used massively parallel DNA sequencing technology {{with a wide}} variety of application fields such as epigenetics, metagenomics and transcriptomics. A well-known problem of this platform is its sensitivity to base-calling insertion and deletion errors, particularly in the presence of long homopolymers. In addition, the base-call quality scores are not informative with respect to whether an insertion or a <b>deletion</b> <b>error</b> is more likely. Surprisingly, not much effort has been devoted to the development of improved base-calling methods and more intuitive quality scores for this platform. Results We present HPCall, a 454 base-calling method based on a weighted Hurdle Poisson model. HPCall uses a probabilistic framework to call the homopolymer lengths in the sequence by modeling well-known 454 noise predictors. Base-calling quality is assessed based on estimated probabilities for each homopolymer length, which are easily transformed to useful quality scores. Conclusions Using a reference data set of the Escherichia coli K- 12 strain, we show that HPCall produces superior quality scores that are very informative towards possible insertion and deletion errors, while maintaining a base-calling accuracy that is better than the current one. Given the generality of the framework, HPCall has the potential to also adapt to other homopolymer-sensitive sequencing technologies. </p...|$|E
40|$|DNA polymerases of the A and B families, {{and reverse}} transcriptases, {{share a common}} {{mechanism}} for preventing incorporation of ribonucleotides: a highly conserved active site residue obstructing the position that would be occupied by a 2 ′ hydroxyl group on the incoming nucleotide. In the family Y (lesion bypass) polymerases, the enzyme active site is more open, with fewer contacts to the DNA and nucleotide substrates. Nevertheless, ribonucleotide discrimination by the DinB homolog (Dbh) DNA polymerase of Sulfolobus solfataricus is as stringent as in other polymerases. A highly conserved aromatic residue (Phe 12 in Dbh) occupies a position analogous to the residues responsible for excluding ribonucleotides in other DNA polymerases. The F 12 A mutant of Dbh incorporates ribonucleoside triphosphates almost as efficiently as deoxyribonucleoside triphosphates, and, unlike analogous mutants in other polymerase families, shows no barrier to adding multiple ribonucleotides, suggesting that Dbh can readily accommodate a DNA–RNA duplex product. Like {{other members of the}} DinB group of bypass polymerases, Dbh makes single-base deletion errors at high frequency in particular sequence contexts. When making a <b>deletion</b> <b>error,</b> ribonucleotide discrimination by wild-type and F 12 A Dbh is the same as in normal DNA synthesis, indicating that the geometry of nucleotide binding is similar in both circumstances...|$|E
40|$|Motivation: Next-generation {{sequencing}} generates {{large amounts}} of data affected by errors {{in the form of}} substitutions, insertions or <b>deletions</b> of bases. <b>Error</b> correction based on the high-coverage information, typically improves de novo assembly. Most existing tools can correct substitution errors only; some support insertions and deletions, but accuracy in many cases is low. Results: We present Karect, a novel error correction technique based on multiple alignment. Our approach supports substitution, insertion and <b>deletion</b> <b>errors.</b> It can handle non-uniform coverage as well as moderately covered areas of the sequenced genome. Experiments with data from Illumina, 454 FLX and Ion Torrent sequencing machines demonstrate that Karect is more accurate than previous methods, both in terms of correcting individual-bases errors (up to 10...|$|R
40|$|One {{issue in}} a Reading Tutor that listens is to {{determine}} which words the student read correctly. We describe a confidence measure that uses a variety of features to estimate {{the probability that a}} word was read correctly. We trained two decision tree classifiers. The first classifier tries to fix insertion and substitution errors made by the speech decoder, while the second classifier tries to fix <b>deletion</b> <b>errors.</b> By applying the two classifiers together, we achieved a relative reduction in false alarm rate by 25. 89 % while holding the miscue detection rate constant. 1...|$|R
40|$|Abstract: We {{present the}} {{construction}} of interleaving arrays for correcting clusters as well as diffuse bursts of insertion or <b>deletion</b> <b>errors</b> in constrained data. In this construction, a constrained information sequence is systematically encoded by computing {{a small number of}} parity checks and inserting markers such that the resulting code word is also constrained. Insertions and deletions lead to a shift between successive markers which can thus be detected and recovered using the parity checks. In this paper, as an example, the scheme is developed for Manchester-encoded information sequences...|$|R
