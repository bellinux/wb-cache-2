33|7|Public
50|$|ILNumerics is {{distributed}} {{as an extension}} to Visual Studio. It adds a tool window to the IDE for the <b>graphical</b> <b>inspection</b> of mathematical objects while stepping through user code.|$|E
5000|$|The ICC of the Rasch {{model for}} dichotomous data {{is shown in}} Figure 4. The grey line maps {{a person with a}} {{location}} of approximately 0.2 on the latent continuum, to the probability of the discrete outcome [...] for items with different locations on the latent continuum. The location of an item is, by definition, that location at which the probability that [...] is equal to 0.5. In figure 4, the black circles represent the actual or observed proportions of persons within Class Intervals for which the outcome was observed. For example, {{in the case of an}} assessment item used in the context of educational psychology, these could represent the proportions of persons who answered the item correctly. Persons are ordered by the estimates of their locations on the latent continuum and classified into Class Intervals on this basis in order to graphically inspect the accordance of observations with the model. There is a close conformity of the data with the model. In addition to <b>graphical</b> <b>inspection</b> of data, a range of statistical tests of fit are used to evaluate whether departures of observations from the model can be attributed to random effects alone, as required, or whether there are systematic departures from the model.|$|E
30|$|This {{analysis}} tested {{only for}} differences in {{the mean of the}} measures between boys and girls. We also evaluated the equality of the entire distribution of the measures across gender by <b>graphical</b> <b>inspection</b> and by performing the Kolmogorov-Smirnov test of equality of distributions.|$|E
40|$|Statistical models (e. g., ARIMA models) {{have been}} {{commonly}} used in time series data analysis and forecasting. Typically one model is selected based on a selection criterion (e. g., AIC), hypothesis testing, and/or <b>graphical</b> <b>inspections.</b> The selected model is then used to forecast future values. However, model selection is often unstable and may cause an unnecessarily high variability in the final estimation/prediction. In this work, we propose {{the use of an}} algorithm AFTER to convexly combine the models for a better performance of prediction. The weights are sequentially updated after each additional observation. Simulations and real data examples are used to compare performance of our approach with model selection methods. The results show advantage of combining by AFTER over selection in term of forecasting accuracy at several settings...|$|R
40|$|As item {{response}} theory (IRT) {{has developed}} and is widely applied, investigating {{the fit of}} a parametric model becomes {{an important part of}} the measurement process when implementing IRT. The usefulness and successes of IRT applications rely heavily on the extent to which the model reflects the data, so it is necessary to evaluate model-data fit by gathering sufficient evidence before any model application. There is a lack of promising solutions on the detection of model misfit in IRT. In addition, commonly used fit statistics are not satisfactory in that they often do not possess desirable statistical properties and lack a means of examining the magnitude of misfit (e. g., via <b>graphical</b> <b>inspections).</b> ^ In this dissertation, a newly-proposed nonparametric approach, RISE was thoroughly and comprehensively studied. Specifically, the purposes of this study are to (a) examine the promising fit procedure, RISE, (b) compare the statistical properties of RISE with that of the commonly used goodness-of-fit procedures, and (c) investigate how RISE may be used to examine the consequences of model misfit. ^ To reach the above-mentioned goals, both a simulation study and empirical study were conducted. In the simulation study, four factors including ability distribution, sample size, test length and model were varied as the factors which may influence the performance of a fit statistic. The results demonstrated that RISE outperformed G 2 and S-X 2 in that it controlled Type I error rates and provided adequate power under all conditions. In the empirical study, the three fit statistics were applied to one empirical data and the misfitting items were flagged. RISE and S-X 2 detected reasonable numbers of misfitting items while G 2 detected almost all items when sample size is large. To further demonstrate an advantage of RISE, the residual plot on each misfitting item was shown. Compared to G 2 and S-X 2, RISE gave a much clearer picture of the location and magnitude of misfit for each misfitting item. ^ Other than statistical properties and graphical displays, the score distribution and test characteristic curve (TCC) were investigated as model misfit consequence. The results indicated that for the given data, there was no practical consequence on classification before and after replacement of misfitting items detected by three fit statistics. ...|$|R
40|$|Abstract: Graphical user {{interfaces}} (GUIs) are critical components of todays soft-ware. Given their increased relevance, correctness and usability of GUIs are becom-ing essential. This paper describes the latest {{results in the}} development of our tool to reverse engineer the GUI layer of interactive computing systems. We use static analysis techniques to generate models of the user interface behaviour from source code. Models help in <b>graphical</b> user interface <b>inspection</b> by allowing designers to concentrate on its more important aspects. One particular type of model that the tool is able to generate is state machines. The paper shows how graph theory can be useful when applied to these models. A number of metrics and algorithms are used in the analysis of aspects of the user interface’s quality. The ultimate goal of the tool is to enable analysis of interactive system through GUIs source code inspection...|$|R
40|$|International audienceGeneralized Procrustes {{analysis}} {{is a popular}} method for matching several configurations by translations, rotations/reflections and scaling constants. It aims at producing a group average from these Euclidean similarity transformations followed by bi-linear approximation of this group average for <b>graphical</b> <b>inspection.</b> An extension that allows for anisotropic scaling (i. e. different scaling of different dimensions) is proposed. This method is illustrated using two real data sets from sensory analysis...|$|E
40|$|Generalized Procrustes {{analysis}} {{is a popular}} method for matching several configurations by translations, rotations/reflections and scaling constants. It aims at producing a group average from these Euclidean similarity transformations followed by bi-linear approximation of this group average for <b>graphical</b> <b>inspection.</b> An extension that allows for anisotropic scaling (i. e. different scaling of different dimensions) is proposed. This method is illustrated using two real data sets from sensory analysis. (C) 2010 Elsevier B. V. All rights reserved...|$|E
40|$|The given data {{is a set}} of {{observations}} on functionals of a trajectory of a system of differential equations. The a priori information is that the system is a member of a parametric family of systems of increasing complexity. The problem is to use the data to identify the particular member of this family which generated the observed data. The method associates each candidate model with the analogue of a generalised smoothing spline fitted to the given data. The resulting values of the smoothing parameter as well as <b>graphical</b> <b>inspection</b> of fit provide a basis for model selection...|$|E
40|$|Graphical user {{interfaces}} (GUIs) are critical components of today's open source software. Given their increased relevance, the correctness and usability of GUIs are becoming essential. This paper describes the latest {{results in the}} development of our tool to reverse engineer the GUI layer of interactive computing open source systems. We use static analysis techniques to generate models of the user interface behavior from source code. Models help in <b>graphical</b> user interface <b>inspection</b> by allowing designers to concentrate on its more important aspects. One particular type of model that the tool is able to generate is state machines. The paper shows how graph theory can be useful when applied to these models. A number of metrics and algorithms are used in the analysis of aspects of the user interface's quality. The ultimate goal of the tool is to enable analysis of interactive system through GUIs source code inspection...|$|R
40|$|The {{aim of this}} {{research}} was to investigate the incorporation of a 'structured approach' to teaching children with autism, with precision teaching assessment methods. The rationale for the research focused on the limited evidence base regarding educational approaches for children with autism (Jones, 2002), and the growing need to provide appropriate educational provision for this group (Ali & Frederickson, 2006). One of the more widely used approaches in the UK is 'Treatment and Education of Autistic and related Communication handicapped Children' (TEACCH) (Tutt, Powell, & Thornton, 2006), with a derivative of this, called a 'structured approach', in place in the local context of the research. The use of fluency building approaches in education, such as precision teaching, has been proposed as potentially beneficial for children with autism due to their dysfluencies and difficulties generalising skills (Weiss, 2001), however there is limited research for this population. The 'structured approach' within the local context did not incorporate fluency building procedures, and therefore the research sought to investigate whether a precision teaching framework could augment a 'structured approach' for children with autism. A pragmatic, mixed methods approach was utilised in {{this research}}. It employed a series of three case studies, each incorporating multiple A-B single case experimental designs (SCED), in order to explore the impact of the precision teaching intervention on the pupils' learning, affect, and behaviour. A focus group provided additional information regarding the implementation of the precision teaching intervention. The SCED measures were analysed through <b>graphical</b> visual <b>inspection</b> and the focus group data was thematically analysed. The research found that precision teaching positively augmented a structured approach for the focus children, which was particularly apparent when it was implemented consistently. Improvements were identified in the pupils' learning, affect, and behaviour. The implications of this research are discussed and opportunities for further research highlighted...|$|R
40|$|M. A. 1. Introduction There is a {{need for}} more {{effective}} ways in which marital preparation programmes can be presented, as the demand for psychological services exceeds the supply. Psychological education and the application of psychotechnological aids, consisting of printed and video material, computer technology and interactive video-systems may be a major source through which a solution for the above-mentioned problem may be found. 2. Aim of the present study The object {{of this study was to}} compare two ways of presenting marital preparation. The effectiveness of the traditional way in which marital preparation is presented, consisting of the help of a program presenter, was compared with the effectiveness of marital preparation presented by psychotechnological aids. 3. Method of research 3. 1 The research design The research design selected for the purpose of this investigation was the N = 1 sample design. This design enables one to study a single subject or a couple intensively. The effect of certain interventions can thus be evaluated. Experimental evaluation in research where the N = 1 sample design is used, is found through <b>graphical</b> representation, visual <b>inspection</b> as well as a description of the graphics and visual interpretation. 3. 2 The subjects Two engaged couples participated in the research project. Neither had any former experience of a marriage preparation programme. 3. 3 The measuring instruments The measuring instruments used in this investigation were: (1) The Dyadic Adjustment Scale; (2) The Empathy Scale; (3) The Communication Skills Test; (4) The' Questionnaire on Positive Reinforcement; (5) The Scoring Procedures for Problem Solving Skills; (6) The Intimacy Questionnaire; (7) Qualitative evaluation. 3. 4 The experimental procedure The subjects were exposed to the marriage preparation programme in separate ways. Both couples received a workbook for engaged couples. Supplementary to the workbooks the researcher served as the presenter to the control couple. The experimental couple received interactive video modules, to be completed after each session in the workbook. In spite of the different ways of presentation the subjects had to complete the above-mentioned measuring instruments after specific sessions. It was of utmost importance that the measuring procedure was precisely the same for both couples. On the completion of the program, an-interview situation was used to obtain the qualitative evaluation from each independent couple, in regard to content and presentation of the marital preparation programme. 5. Results of the investigation In regard to the results obtained in the investigation, it appeared that both forms of presentation was duly effective. Both the control and the experimental couples showed a definite improvement with regard to relationship adjustment, from being exposed to the marital preparation programme. It was irrespective of the form of presentation. The conclusion gained was that both forms of presentation should be used supplementary to each other to achieve optimal efficiency. It was shown that both forms of presentation had valuable learning processes...|$|R
40|$|ACDP is a Java {{application}} for processing protein circular dichroism (CD) spectra either individually or within a series. Data processing includes spectrum subtraction (`baseline correction'), {{conversion of the}} raw CD into units of mean residue ellipticity, wavelength monitoring and <b>graphical</b> <b>inspection.</b> Three different algorithms for secondary structure deconvolution have been implemented, and spectra can be analysed {{without the need for}} data reformatting, using a neural network approach, variable selection or linear combination of prototype spectra. The application is written entirely in Java and is thus portable {{to a wide variety of}} platforms, requiring only the Java Runtime Environment. Griffith Sciences, School of Natural SciencesFull Tex...|$|E
40|$|This paper {{proposes a}} novel {{interactive}} tool designed {{to support the}} <b>graphical</b> <b>inspection</b> of large semantic models. The idea is to view the iterative inspection process {{as a series of}} recurring and cross-referencing model interrogations. Our tool employs movable lens filters [SFB 94] as primitive interrogation units. Each lens is associated with a single operation, such as querying, result visualisation or filtering. A variety of complex queries and visualisation mappings can be constructed by superimposing the lenses. This paper describes the design and the anticipated benefits of the tool, with examples taken from the GRAIL medical terminology model, an extensive system currently containing around 8000 concepts. ...|$|E
40|$|OBJECTIVE: Suicide {{rates in}} Quebec over {{the second half}} of the 20 th century show a wide range of {{variation}} depending on age and time period. However, few studies have verified the presence of a cohort effect affecting trends in Quebec suicide rates. This study is designed to evaluate the potential effects of age, period and cohort (APC) on trends in suicide between 1950 and 2009 in Quebec. METHOD: For these APC analyses, we used a multiphase approach combining a <b>graphical</b> <b>inspection</b> followed by an analysis that isolates the cohort effect from age and period effects (linear regression of the residuals from a median polish of the rates). RESULTS: The <b>graphical</b> <b>inspection</b> of trends in rates points to combined effects of age, period and cohort among both men and women. However, the median polish analysis attributes primary importance to period effects, followed by age effects, but also shows weak cohort effects that are significant only among men born between 1950 and 1979. CONCLUSION: The variation in Quebec suicide rates appears to be primarily a reflection of period, age and, to a lesser degree, birth cohort. Thus, in addition to sex, selection of risk groups should be based more on age and time period than on birth cohort. KEY WORDS: Quebec; suicide; age-period-cohort; cohort effect; median polish La traduction du résumé se trouve à la fin de l’article. Can J Public Health 2013; 104 (2) :e 118 -e 123 Suicide ranks tenth among all causes of death in Canada and isthe principal cause of death by trauma. 1 In Quebec there weremore than 1, 100 deaths by suicide in 2009, nearly double the number associated with motor vehicles. 2 For a number of years...|$|E
40|$|We {{present a}} new {{indicator}} of house prices in Italy, with more extensive geographical and time coverage. The new indicator now {{makes it possible}} to analyze medium- and long-term trends with satisfactory representation of the Italian housing market. It also allows for timely updating, for prompt assessment of housing input both to the business cycle and to inflationary pressures. We offer a preliminary identification, based solely on <b>graphical</b> <b>inspection,</b> of four different property price cycles since the late 1960 s; the latest began {{at the end of the}} 1990 s and signaled a slowdown since 2006. Finally, we tentatively assess the effect of including transactions in dwellings in the Italian HICP basket according to the net acquisition approach, which apparently results in about a quarter point of additional inflation each year since 2000. business cycle, housing market, property prices, inflation measures...|$|E
3000|$|The {{information}} on the negotiated and residual part of the wage also allows us to investigate whether {{an increase in the}} negotiated wage induces firms to adjust the other component of the wage. The red line in Figs.  11 and 12 represents the negotiated part of the wages. Note that due to the institutional setting, the evolution of the negotiated part of the wages is sticky as it is usually predetermined for 3  years. As a result the negotiated part of the wages continued to rise even during years of economic downturn (grey area). 16 However, a simple <b>graphical</b> <b>inspection</b> of the evolution of the negotiated (red line) and residual (black line) part of the wages shows that in 2009 and in 2014, firms compensated for the increase of the negotiated part of the wages by reducing the residual component. 17 [...]...|$|E
40|$|<b>Graphical</b> <b>inspection</b> of {{multimodality}} {{is demonstrated}} using unsupervised lateralinhibition neural networks. Three projection pursuit indices are compared on low dimensional simulated and real-world data: principal components [22], Legendre polynomial [6] and projection pursuit network [16]. 1 Introduction Modes exploration {{of an unknown}} multivariate distribution from observations, arises in exploratory multivariate data analysis. The task is sensitive to the dimensionality, as robust estimation requires an exponential growth of the observations number with the dimensionality [1]. Investigation of the modality in a density function has been considered by several authors. A few [4, 10] seem to depend on implicit or explicit choice of {{the scale of the}} density being studied. A simple approach based on kernel density estimate was proposed [24], by making this choice automatically. The classical parametric and nonparametric methods for estimating density functions have a number of drawbac [...] ...|$|E
40|$|We {{describe}} {{in this paper}} a method allowing to order submodels in linear regression. A real function is attached to each submodel, allowing to graphically compare and order them. Our procedure defines an objective function depending on two factors (lack of fit and multicolinearity) with the property that the minimum of this function over subsets of the regressors determines the "best" such subset of variables. The latter will be found by systematically examining all possible subsets having at least two regressors. The method is built {{in such a way}} that it allows a <b>graphical</b> <b>inspection</b> of the submodels. Although it is presented here only in the framework of linear regression with a single, continuous, response variable, the procedure may be extended in a natural way to more general regression models. Simulation and implementation on various data sets gave very satisfying result...|$|E
40|$|SummaryAligned α helix peptide dipoles sum to a “macroscopic” dipole {{parallel}} to the helix axis that has been implicated in protein folding and function. However, in aqueous solution the dipole is counteracted by an electrostatic reaction field generated by the solvent, {{and the strength of}} the helix dipole may reduce drastically from its value in vacuum. Here, using atomic-detail helix models and Poisson-Boltzmann continuum electrostatics calculations, the net effective dipole moment, μeff, is calculated. Some initially surprising results are found. Whereas in vacuum μeff increases with helix length, the opposite is found to be the case for transmembrane helices. In soluble proteins, μeff is found to vary strongly with the orientation and position of the helix relative to the aqueous medium. A set of rules is established to estimate of the strength of μeff from <b>graphical</b> <b>inspection</b> of protein structures...|$|E
40|$|Summary: We {{introduce}} the Skill Plot, {{a method that}} it is directly relevant to a decision maker who must use a diagnostic test. In contrast to ROC curves, the skill curve allows easy <b>graphical</b> <b>inspection</b> of the optimal cutoff or decision rule for a diagnostic test. The skill curve and test also determine whether diagnoses based on this cutoff improve upon a naive forecast (of always present or of always absent). The skill measure {{makes it easy to}} directly compare the predictive utility of two different classifiers in analogy to the area under the curve statistic related to ROC analysis. Finally, this paper shows that the skill based cutoff inferred from the plot is equivalent to the cutoff indicated by optimizing the posterior odds in accordance with Bayesian decision theory. A method for constructing a confidence interval for this optimal point is presented and briefly discussed...|$|E
40|$|A new {{approach}} to the solicitation and measurement of relevance judgments is presented, which attempts to resolve some of the difficulties inherent {{in the nature of}} relevance and human judgment, and which further seeks to examine how users' judgments of document representations change as more information about documents is revealed to them. Subjects (university faculty and doctoral students) viewed three incremental versions of documents, and recorded ratio-level relevance judgments for each version. These judgments were analyzed by a variety of methods, including <b>graphical</b> <b>inspection</b> and examination of the number and degree of changes of judgments as new information is seen. A post questionnaire was also administered to obtain subjects' perceptions of the process and the individual fields of information presented. A consistent pattern of perception and importance of these fields is seen: Abstracts are by far the most important field and have the greatest impact, followed by titles, bibliographic information, and indexing...|$|E
30|$|Despite the {{importance}} of this topic, the results of research that assess the existence and degree of job polarisation in Spain are mixed. For example, Anghel et al. (2014) conclude that the employment structure became more polarised between 1997 and 2012, while Oesch and Rodríguez-Menés (2011) and Eurofound (2015) show a pattern of progressive upgrading for the same period. Moreover, two recent studies covering Spain, based on the European Labour Force Survey, diverge in their results. Goos et al. (2009, 2014) conclude that, on average, the employment structure in Spain became more polarised between 1993 and 2006. Using the same period of analysis, Fernández-Macías (2012) conversely shows an upgrading process (high-wage occupations expanding at the expenses of low-wage jobs) and does not provide evidence of a pervasive polarisation. These five papers have relied on <b>graphical</b> <b>inspection</b> to identify the phenomenon: terciles (Goos et al. 2009, 2014; Fernández-Macías 2012), or quintiles (Eurofound 2015; and Oesch and Rodríguez-Menés 2011).|$|E
40|$|Part 1 : Long and Short Papers (Continued) International audienceEmploying post-WIMP {{interfaces}}, i. e. {{user interfaces}} {{going beyond the}} traditional WIMP (Windows, Icons, Menu, Pointer) paradigm, often implies a more complex authoring process for applications. We present a novel authoring method and a corresponding tool that aims to enable developers {{to cope with the}} added level of complexity. Regarding the development as a process conducted on different layers, we introduce a specific layer for post-WIMP in addition to layers addressing implementation or traditional GUI elements. We discuss the concept of cross layer authoring that supports different author groups in the collaborative creation of post-WIMP applications permitting them working independently on their respective layer and contributing their specific skills. The concept comprises interactive visualization techniques that highlight connections between code, GUI and post-WIMP functionality. It allows for <b>graphical</b> <b>inspection</b> while transitioning smoothly between layers. A cross layer authoring tool has been implemented and was well received by UI developers during evaluation...|$|E
40|$|Abstract. Employing post-WIMP {{interfaces}}, i. e. {{user interfaces}} {{going beyond the}} traditional WIMP (Windows, Icons, Menu, Pointer) paradigm, often implies a more complex authoring process for applications. We present a novel authoring method and a corresponding tool that aims to enable developers {{to cope with the}} added level of complexity. Regarding the development as a process conducted on different layers, we introduce a specific layer for post-WIMP in addition to layers addressing implementation or traditional GUI elements. We discuss the concept of cross layer authoring that supports different author groups in the collaborative creation of post-WIMP applications permitting them working independently on their respective layer and contributing their specific skills. The concept comprises interactive visualization techniques that highlight connections between code, GUI and post-WIMP functionality. It allows for <b>graphical</b> <b>inspection</b> while transitioning smoothly between layers. A cross layer authoring tool has been implemented and was well received by UI developers during evaluation...|$|E
40|$|Abstract-A new {{approach}} to the solicitation and measurement of relevance judgments is presented, which attempts to resolve some of the difficulties inherent {{in the nature of}} relevance and human judgment, and which further seeks to examine how users ’ judgments of document representations change as more information about documents is revealed to them. Subjects (university faculty and doctoral students) viewed three incremental versions of documents, and recorded ratio-level relevance judgments for each version. These judgments were analyzed by a variety of methods, including <b>graphical</b> <b>inspection</b> and examination of the number and degree of changes of judgments as new information is seen. A post questionnaire was also administered to obtain subjects ’ perceptions of the process and the individual fields of information presented. A consistent pattern of perception and importance of these fields is seen: Abstracts are by far the most important field and have the greatest impact, followed by titles, bibliographic information, and indexing. I. INTRODUCTION/STATEMENT OF THE PROBLEM When approaching the study of relevance from an experimental perspective, a parado...|$|E
40|$|The task of {{predicting}} accurately the cost {{required for the}} completion of a new software project is a challenging issue in the Software Cost Estimation area, since it is closely related with the activities of project management and the wise decision-making of organizations in order to bid, plan and budget a forthcoming system. However, the accurate prediction of the cost is often obtained with great uncertainty and for this reason there has been noted a lack of convergence in experimental studies. The main reason for the discrepancy can be derived from the inherent characteristic of prediction methodologies, since they produce point estimates without taking into account the risk covering the whole process. In this study, we propose a statistical framework, so as to focus on the construction of Prediction Intervals which provide an “optimistic ” and a “pessimistic ” guess for the true magnitude of the cost. The proposed framework that incorporates different accuracy indicators, formal hypothesis testing and <b>graphical</b> <b>inspection</b> of the predictive performance is applied on a dataset with real software projects...|$|E
40|$|The {{isolation}} of DNA markers that {{are linked to}} interesting genes helps plant breeders to select parent plants that transmit useful traits to future generations. Such 'marker-assisted breeding and selection' heavily leans on statistical testing of associations between markers and a well-chosen trait. Statistical association analysis is guided by classical "p"-values or the false discovery rate and thus relies predominantly on the null hypothesis. The main concern of plant breeders, however, is to avoid missing an important alternative. To judge evidence from this perspective, we complement the traditional "p"-value with a one-sided 'alternative "p"-value' which summarizes evidence against a target alternative {{in the direction of}} the null hypothesis. This "p"-value measures 'impotence' as opposed to significance: how likely is it to observe an outcome as extreme as or more extreme than the one that was observed when data stem from the alternative? We show how a <b>graphical</b> <b>inspection</b> of both "p"-values can guide marker selection when the null and the alternative hypotheses have a comparable importance. We derive formal decision tools with balanced properties yielding different rejection regions for different markers. We apply our approach to study rye-grass plants. Copyright 2005 Royal Statistical Society. ...|$|E
40|$|A {{computer}} code {{has been written}} at the Pacific Northwest Laboratory (PNL) to synthesize the results of typical gamma ray spectroscopy experiments. The code, dubbed SYNTH, allows a user to specify physical characteristics of a gamma ray source, the quantity of the nuclides producing the radiation, the source-to-detector distance {{and the presence of}} absorbers, the type and size of the detector, and the electronic set up used to gather the data. In the process of specifying the parameters needed to synthesize a spectrum, several interesting intermediate results are produced, including a photopeak transmission function versus energy, a detector efficiency curve, and a weighted list of gamma and x rays produced from a set of nuclides. All of these intermediate results are available for <b>graphical</b> <b>inspection</b> and for printing. SYNTH runs on personal computers. It is menu driven and can be customized to user specifications. SYNTH contains robust support for coaxial germanium detectors and some support for sodium iodide detectors. SYNTH is not a finished product. A number of additional developments are planned. However, the existing code has been compared carefully to spectra obtained from National Institute for Standards and Technology (NIST) certified standards with very favorable results. Examples of the use of SYNTH and several spectral results are presented...|$|E
40|$|Abstract: The {{interpretation}} of the short-range static and/or dynamic disorder in a crystal structure from X-ray data is often a rather complex problem. Recently, we synthesized and characterized three co-crystalline complexes between fluorene, showing C 2 v symmetry, and three electron withdrawing D 2 h compounds, employing FTIR and Raman spectroscopy and single crystal X-ray diffraction techniques. The crystal structures of the complexes of fluorene with the three different electron withdrawing molecules are disordered in the solid state and only approximate structures were obtained by refinement of the single crystal data. Indeed, the fluorene moiety presents a very irregular geometry, showing for example C-C bonds ranging from 1. 25 Å to 1. 75 Å. <b>Graphical</b> <b>inspection</b> of the solved crystal structures indicates that the fluorene molecules in these co-crystalline complexes can assume two possible positions, both with 50 % population. A two-step procedure to improve the disordered models is described. At first, the two possible ordered structures, with the fluorene molecule in {{only one of the}} two populated positions, are "separated " by molecular graphic techniques and then their geometry is fully optimized employing the periodic ab initio “CRYSTAL ” code to obtain a chemically sensible model with reasonable distance...|$|E
40|$|ABSTRACT This study {{aimed to}} test the {{application}} of the technique of support vector machines (SVM) to estimate the volume of eucalyptus trees. The data used in this study were from of 2307 trees of clonal hybrids (Eucalyptus grandis x Eucalyptus urophylla) located in southern Bahia. In the definition of stratification traditionally used 53 stratums were defined (defined by the stratification project and clone). He set the model of Schumacher and Hall for each stratum. The SVM were constructed to correlate the volume of trees on the basis of other independent variables which may be numeric as dbh and height and categorical as genetic material and design. The estimates were analyzed using statistical and graphical analysis of residues. The analysis consisted of the <b>graphical</b> <b>inspection</b> statistical dispersion of errors (residuals) in relation to the percentage of the values observed, and the analysis of the histogram of residues. The statistics used were the correlation between the observed and estimated volumes. The model of Schumacher and Hall showed the correlation between observed and predicted values of 0, 993, and the SVM set of correlated 0, 994. The SVM technology showed good adaptation to the problem, and this can use to predict the volumetric production of planted forests...|$|E
40|$|Generalised Procrustes {{analysis}} (GPA) is {{a method}} for producing a group average from rotated versions {{of a set of}} individual data matrices followed by bi-linear approximation of this group average for <b>graphical</b> <b>inspection.</b> Partial Least Squares Regression (PLSR) {{is a method}} for relating one data matrix to another data matrix, via bi-linear low-rank regression modelling. The merger of these methods proposed aims to produce an average (e. g. a sensory group panel average), which balances an "intersubjective", internal consensus between the individual assessors' data against an "objective" external correspondence between the sensory data and other types of data on the same samples (e. g. design information, chemical or physical measurements or consumer data). Several ways of merging GPA with PLSR are possible, of which one is selected and applied. The proposed "GP-PLSW" method is compared to a conventional GPA followed by an independent PLSR, using a data set about milk samples assessed by a group of sensory judges with respect to a set of sensory descriptor terms, and also characterised by experimental design information about the samples. The GP-PLSR gave a more design-relevant group average than traditional GPA. The proposed algorithm was tested under artificially increased noise levels. (C) 2003 Elsevier B. V. All rights reserved...|$|E
40|$|In the National Natural Reserve of the Qilian Mountains, {{northwest}} China, automated dendrometers {{were installed}} on Qilian juniper (Sabina przewalskii Kom.) in 2010. Here, {{we present a}} complete 1 -year data series of dendrometer measurements and synchronous meteorological records. The mean daily radius change index curve obtained by first-difference standardization was analyzed. The results indicate strong similarities of stem radius variations among the studied trees. By <b>graphical</b> <b>inspection</b> of sub-diurnal shrinkage and expansion patterns, seasonal stem radius variations could {{be divided into two}} general phases. During early winter to spring, daily amplitudes show strong fluctuations, and meteorological data indicate that both air and soil temperatures are positively related to stem radius swelling. From late March to early November 2011, stem radius expansion is negatively related to both air and soil temperatures, as well as to vapor pressure deficit, and positively related to precipitation and soil moisture. We found that the dividing lines between the two phases were strongly linked to the occurrence of a 0 °C daily mean air temperature, indicating that air temperature played an important effect on stem radius variations in this cold and arid environment. During the growing season in 2011, May and June are the most productive months...|$|E
40|$|The {{purpose of}} this study was to analyze the informational {{constraints}} that influence the dynamics of 1 v 1 situations in football. Specifically, we analyzed the contribution of interpersonal distance and relative velocity to explain phase transitions that occurred between stable and unstable coordinative states in 1 v 1 in football. Participants were six, U 13 level football players (Mean age: 11. 8 ± 0. 4 yrs), who performed 5 trials of a representative task at four different start distances (1, 1. 5, 2 and 2. 5 meters) between the defender and the ball (5 repetitions x 6 players x 4 distances = 120 1 v 1 trials). Players’ motion was captured by digital video camera. For image treatment and to extract data on player movement coordinates, we used the TACTO 8. 0 software at 25 Hz. Following, we constructed plots with the time-series of these two control parameters candidates associated with the time-series of players’ distance to the end line. <b>Graphical</b> <b>inspection</b> showed in all trials a clear tendency for greater relative velocity and lower interpersonal distance values during phase transitions between coordinative states. To confirm this interpretation, we computed a time-series analysis of Z-scores of relative velocity and interpersonal distance data, and a time-series analysis of Z-score differences between these variables. Mean data showed that maximum peaks in Z-score differences were related to all phase transitions...|$|E
40|$|Purpose – The {{purpose of}} this paper is to {{identify}} the main “models of growth” characterising the EU countries in the last two decades, with particular reference to the employment-productivity relationship, and to reveal the key determinants of productivity. Design/methodology/approach – After a survey of the relevant literature, the empirical section analyses the “models of growth” by <b>graphical</b> <b>inspection,</b> identifying four models (for EU- 27 in the 1990 - 2008 period) : extensive, intensive, virtuous, and stagnant. Then different econometric investigations (beta convergence, dynamic panel with GMM estimation, fixed effects panel, cross-section) are used to test the “diminishing returns of employment rate” hypothesis (for the 2000 - 2006 period), to assess the convergence processes and to determine the key variables affecting productivity. Findings – The main finding is the confirmation of the hypothesis mentioned: high employment growth is likely to lead to slower productivity growth. Moreover, besides verifying the beta convergence of productivity per worker, the most significant determinants of productivity are the following: education, a transition index, some structural indicators, and a “shadow economy” proxy. Finally, the descriptive analysis shows that “old” EU countries, coming from two decades of “jobless growth”, shifted to an “extensive” growth model; in contrast, transition countries (NMS) followed the opposite path: reducing employment and raising productivity. Research limitations/implications – It would be advisable to extend the period of the analysis, as soon as new data become available. Practical implications – The main policy implication is to get the EU Lisbon strategy – i. e. to create “more and better” jobs – working effectively. Originality/value – The most original finding is the clear assessment of an employment-productivity trade-off. Also, the different models of growth are categorised simply and effectively. ...|$|E
40|$|Calibration of car-following models against {{trajectory}} {{data has}} been widely applied {{as the basis for}} several type of studies ranging from the investigation and benchmarking of models, to the study of parameters correlation, or other theoretical issues like the inter/intra driver heterogeneity or the multi-anticipative driving behaviour. However, very few of these studies attempted also to analyze and quantify the uncertainty entailed in the calibration process and its impacts on the accuracy and reliability of results. A thorough understanding of the whole calibration problem (against trajectory data), {{as well as of the}} mutual effect of the specific problems raised in the field literature, indeed, does not yet exist. In this view, a general methodology to assess a calibration procedure was proposed and applied to the calibration of the Gipps’ car-following model. Compact indicators were proposed to evaluate the capability of a calibration setting to find the “known” global solution, in terms of both the accuracy and the robustness as to the variation of the starting conditions of the optimisation algorithm. Then, a <b>graphical</b> <b>inspection</b> method, based on the so called cobweb plots, was proposed to explore the existence and the nature of the local minima found by the algorithms, as well as to give insights into the measure of performances and the goodness of fit functions used in the calibration experiments. The methodology has been applied to all the calibration settings (i. e. combinations of algorithms, measure of performances and goodness of fit functions) utilized in the field literature so far. The study allowed us to highlight and motivate, for the model under investigation, the limits of some of these calibration settings. Research lines towards the definition of robust settings for the problem of car-following models calibration based on real trajectory data, are finally outlined...|$|E
