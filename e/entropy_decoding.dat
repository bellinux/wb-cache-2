48|18|Public
50|$|The Unified Video Decoder (UVD) SIP core is on-die in the HD 2400 and the HD 2600. The HD 2900 GPU dice do {{not have}} a UVD core, as its stream {{processors}} were powerful enough to handle most of the steps of video acceleration in its stead except for <b>entropy</b> <b>decoding</b> and bitstream processing which are left for the CPU to perform.|$|E
50|$|An {{outdated}} {{version of}} an anti-virus application may create a new thread for a scan process, while its GUI thread waits for commands from the user (e.g. cancel the scan). In such cases, a multi-core architecture is of little benefit for the application itself due to the single thread doing all the heavy lifting and the inability to balance the work evenly across multiple cores. Programming truly multithreaded code often requires complex co-ordination of threads and can easily introduce subtle and difficult-to-find bugs due to the interweaving of processing on data shared between threads (see thread-safety). Consequently, such code {{is much more difficult}} to debug than single-threaded code when it breaks. There has been a perceived lack of motivation for writing consumer-level threaded applications because of the relative rarity of consumer-level demand for maximum use of computer hardware. Although threaded applications incur little additional performance penalty on single-processor machines, the extra overhead of development has been difficult to justify due to the preponderance of single-processor machines. Also, serial tasks like decoding the entropy encoding algorithms used in video codecs are impossible to parallelize because each result generated is used to help create the next result of the <b>entropy</b> <b>decoding</b> algorithm.|$|E
40|$|<b>Entropy</b> <b>Decoding</b> is an {{essentially}} sequential task. Executing this task on a processor that benefits from Instruction Level Parallelism (ILP), Data Level Parallelism (DLP) or both requires an efficient implementation of <b>Entropy</b> <b>Decoding.</b> <b>Entropy</b> <b>Decoding</b> forms {{the part of}} MPEG- 2 Decoding that exploits the least parallelism. Creating more parallelism in <b>Entropy</b> <b>Decoding</b> is expected to optimize MPEG- 2 Decoding significantly. When writing an efficient MPEG- 2 Decoder the result needs to be conform to the Berkeley MPEG- 2 Decoder. In this thesis the <b>Entropy</b> <b>Decoding</b> process is optimized by compressing the data-stream from the Lookup Table to the Variable Length Decoder. In addition, the number of branches in the <b>Entropy</b> <b>Decoding</b> process is reduced. This stimulates Instruction Level Parallelism (ILP), that is exploited even more by using preloading data. The result of these three optimization steps is evaluate...|$|E
50|$|UVD 3 adds {{support for}} {{additional}} hardware MPEG2 <b>decoding</b> (<b>entropy</b> <b>decode),</b> DivX and Xvid via MPEG-4 Part 2 <b>decoding</b> (<b>entropy</b> <b>decode,</b> inverse transform, motion compensation) and Blu-ray 3D via MVC (<b>entropy</b> <b>decode,</b> inverse transform, motion compensation, in-loop deblocking). along with 120 Hz stereo 3D support, and is optimized to utilize less CPU processing power.UVD 3 also adds support for Blu-ray 3D stereoscopic displays.|$|R
30|$|Decoding of {{received}} packets involves decoding the FEC {{code and}} <b>decoding</b> the <b>entropy</b> code. <b>Decoding</b> the FEC code {{can be done}} by, e.g., Gaussian elimination, which has complexity O(N^ 3) per layer, and therefore at most O(N^ 4) for decoding the entire control vector. <b>Decoding</b> of the <b>entropy</b> code is done by a look-up table and has, thus, complexity O(N), since the control vector contains N elements.|$|R
40|$|The {{concentric}} mosaics offer a quick {{solution to}} the construction and navigation of a virtual environment. To reduce the vast data amount of the concentric mosaics, a compression scheme based on 3 D wavelet transform has been proposed in a previous paper. In this work, we investigate the efficient implementation of the renderer. It is preferable not to expand the compressed bitstream as a whole, so that the memory consumption of the renderer can be reduced. Instead, only the data necessary to render the current view are accessed and decoded. The progressive inverse wavelet synthesis (PIWS) algorithm is proposed to provide the random data access and to reduce the calculation for the data access requests to a minimum. A mixed cache is used in PIWS, where the <b>entropy</b> <b>decoded</b> wavelet coefficient, intermediate result of lifting and fully synthesized pixel are all stored at the same memory unit because of the in-place calculation property of the lifting implementation. PIWS operates with a finite state machine, where each memory unit is attached with a state to indicate what type of content is currently stored. The computation saving achieved by PIWS is demonstrated with extensive experiment results...|$|R
30|$|<b>Entropy</b> <b>decoding</b> is {{a process}} whereby a {{codeword}} is converted into a number or symbol. The codeword is loaded from external memory into internal memory in the <b>entropy</b> <b>decoding</b> process. In addition, the operations in the <b>entropy</b> <b>decoding</b> are mostly bit-level operations. However, frequent accesses to the external memory can significantly degrade decoding performance. To resolve this problem and achieve real-time decoding with a flexible design, it is desirable to use a designated bitstream decoding processor.|$|E
3000|$|... [...]. For example, given a coded frame, after <b>entropy</b> <b>decoding,</b> the {{macroblock}} {{information is}} extracted as follows, the type is intra, the partition belongs to [...]...|$|E
40|$|An {{efficient}} {{scheme for}} JPEG 2000 SNR progressive decoding is proposed, which {{is capable of}} handling JPEG 2000 compressed image data with SNR progressiveness. In order to avoid <b>entropy</b> <b>decoding</b> of the same compressed data more than once when decoding SNR progressive images, two techniques are introduced in our decoding scheme; reuse of intermediate decoding result and differential inverse discrete wavelet transform (differential IDWT). Comprehensive evaluation of our scheme demonstrating that with 26. 6 % increase of required memory size, up to 50 % of computational cost of <b>entropy</b> <b>decoding</b> can be reduced in comparison with conventional non-progressive decoding scheme when 9 / 7 irreversible DWT filter is used. Key words: JPEG 2000, Progressive decoding, Discrete wavelet transfor...|$|E
30|$|The H. 264 video decoder {{application}} {{is taken as}} main use case application. It is a high quality video compression algorithm relying on several efficient strategies extracting spatial (within a frame) and temporal dependencies (between frames). This {{application is}} characterized by a flexible coding, high compression and high quality resolution. Moreover, it is a promising standard for embedded devices. The main steps of the H. 264 decoding process consist in the following: First, a compressed bit stream coming from the Network application layer (NAL), which formats the representation of the video and provides header information in a manner appropriate for conveyance by particular transport layers, is received at the input of the decoder. Then, the <b>entropy</b> <b>decoded</b> bloc begins with decoding the slice header where each slice consists of one or more 1616 macroblocks, and then it decodes the other parameters. The <b>decoded</b> data are <b>entropy</b> and sorted to produce a set of quantized coefficients. These coefficients are then inverse quantized and inverse transformed. Thereafter, the data obtained are added to the predicted data from the previous frames depending upon the header information. Finally the original block is obtained after the de-blocking filter to compensate the block artifacts effect.The H. 264 video decoder application can be broken down into various tasks sets corresponding to different types of parallelization. In our experiments, we use the slices version, one of the task models of H. 264 proposed by Thales Group, France[21] in the context of French national project Pherma[22].|$|R
40|$|To cater {{the needs}} of diverse {{application}} domains, three basic feature sets called profiles are established in H. 264 standard: the Baseline, Main, and Extended profiles. The Baseline profile is designed to minimize complexity and provide high robustness and flexibility for use over {{a broad range of}} network environments and conditions; the Main profile is designed with an emphasis on compression coding efficiency capability; and the Extended profile is designed to combine the robustness of the Baseline profile with a higher degree of coding efficiency and greater network robustness. The H. 264 bitstream is organized as a series of NAL units, which are first, processed by the NAL unit decoder module. NAL unit decoder does the job of NAL unit separation and parsing of the header information. The output data elements of NAL unit decoder are <b>entropy</b> <b>decoded</b> and reordered to produce a set of quantized coefficients by the variable length decoder module. The quantized coefficients are then rescaled and inverse transformed to give difference macroblock using the transformation and quantization module. Using the header information decoded from the bitstream, the motion compensation and picture construction module produces distorted macroblocks which are then filtered using the deblocking filter to create decoded macroblocks. The output is stored as a yuv file, which can be played using any YUV player/viewer. Thus the visual input from the sensors of the robots can be compressed by this technique before delivered to the actuators on the scene...|$|R
40|$|In {{this paper}} we propose a novel Real-Valued Free Distance Metric (RV-FDM) for {{comparing}} the error correction capabilities of Variable Length Error Correction (VLEC) codebooks that have the same integer-valued free distance lower bounds. We demonstrate that VLEC codebooks having higher RV-FDMs tend to have EXtrinsic Information Transfer (EXIT) functions with more pronounced 'S'-shapes. Furthermore, we show that higher-accuracy EXIT chart matching can be achieved if the component EXIT functions of an irregular code exhibit more variety. This motivates the employment of our novel genetic algorithm for designing the component VLEC codes of irregular variable length coding, that have particular EXIT functions, in addition to exhibiting desirable bit <b>entropies</b> and <b>decoding</b> complexities...|$|R
40|$|The paper {{presents}} a Design Space Exploration (DSE) experiment {{which has been}} carried out {{in order to determine the}} optimum FPGA [...] based Variable-Length Decoder (VLD) computing resource and its associated instructions, with respect to an <b>entropy</b> <b>decoding</b> task which is to be executed on the FPGA-augmentedTriMedia/CPU 64 processor. We first outline the extension of the TriMedia/CPU 64 architecture, which consists of an FPGA [...] based Reconfigurable Functional Unit (RFU) and the associated generic instructions. Then we address <b>entropy</b> <b>decoding</b> and propose a strategy to partially break the data dependency related to variable-length decoding. Three VLDs (VLD- 1, VLD- 2, VLD- 3) instructions which can return 1, 2, or 3 symbols, respectively, are subsequently analyzed. After completing the DSE, we determined that VLD- 2 instruction leads to the most efficient <b>entropy</b> <b>decoding</b> in terms of instruction cycles and FPGA area. The FPGA [...] based implementation of the computing resource associated to VLD- 2 instruction is subsequently presented. When mapped on an ACEX EP 1 K 100 FPGA from Altera, VLD- 2 exhibits a latency of 8 TriMedia cycles, and uses all the Electronic Array Blocks and 51 % of the logic cells of the device. The simulation results indicate that the VLD- 2 [...] based entropy decoder is 43 % faster than its pure software counterpart...|$|E
30|$|CAVLD is {{considered}} to be a bottleneck in both parallel and sequential decoders of H. 264 /AVC because of its bit-by-bit dependency. Furthermore, as video resolution is increasing based on market demands, the common bitrate for high-resolution videos such as full high definition (full HD) now ranges from 10 to 20 megabits per second (Mbps) for high-quality applications. For such high-bitrate applications, a portion of the complexity for the <b>entropy</b> <b>decoding</b> would significantly increase. For fast CAVLD of H. 264 /AVC, there are several algorithms such as table mapping, which can improve the overall latency of <b>entropy</b> <b>decoding</b> without parallelism. In order to develop an efficient table mapping approach for multi-stage pipelined processors, it is necessary to consider not only the memory requirements and the number of memory accesses, but also the number of conditional branches.|$|E
30|$|The rest of {{the paper}} is {{organized}} as follows. In section  2, the conventional <b>entropy</b> <b>decoding</b> algorithms and the BsPU are introduced. In section  3, the proposed fast CAVLD based on a new table mapping algorithm is presented. In section  4, experimental results are shown and discussed. Finally, concluding remarks are given in section  5.|$|E
40|$|Abstract—In {{this paper}} we propose a novel Real-Valued Free Distance Metric (RV-FDM) for {{comparing}} the error correction capabilities of Variable Length Error Correction (VLEC) codebooks that have the same integer-valued free distance lower bounds. We demonstrate that VLEC codebooks having higher RV-FDMs tend to have EXtrinsic Information Transfer (EXIT) functions with more pronounced ‘S’-shapes. Furthermore, we show that higher-accuracy EXIT chart matching can be achieved if the component EXIT functions of an irregular code exhibit more variety. This motivates the employment of our novel genetic algorithm for designing the component VLEC codes of irregular variable length coding, that have particular EXIT functions, in addition to exhibiting desirable bit <b>entropies</b> and <b>decoding</b> complexities. Index Terms—Variable length codes, joint source and channel coding, trellis codes, information rates. I...|$|R
40|$|Beam {{search is}} a {{desirable}} choice of test-time decoding algorithm for neural sequence models because it potentially avoids search errors made by simpler greedy methods. However, typical cross entropy training procedures for these models do not directly consider the behaviour {{of the final}} decoding method. As a result, for cross-entropy trained models, beam decoding can sometimes yield reduced test performance when compared with greedy decoding. In order to train models that can more effectively make use of beam search, we propose a new training procedure {{that focuses on the}} final loss metric (e. g. Hamming loss) evaluated on the output of beam search. While well-defined, this "direct loss" objective is itself discontinuous and thus difficult to optimize. Hence, in our approach, we form a sub-differentiable surrogate objective by introducing a novel continuous approximation of the beam search decoding procedure. In experiments, we show that optimizing this new training objective yields substantially better results on two sequence tasks (Named Entity Recognition and CCG Supertagging) when compared with both cross <b>entropy</b> trained greedy <b>decoding</b> and cross <b>entropy</b> trained beam <b>decoding</b> baselines. Comment: Updated for clarity and notational consistenc...|$|R
40|$|Abstract. Low-density {{parity check}} codes (LDPCs) based on {{irregular}} graphs {{have been shown}} to outperform the most advanced error-correcting codes to date. In this paper we apply methods of statistical physics to study the typical properties of simple irregular LDPCs. We employ the replica method to compute the <b>entropy</b> of the <b>decoding</b> solutions to find a phase transition which coincide with Shannon’s coding bound. The decoding dynamics is studied by mean-field techniques; the solutions obtained are in good agreement with simulations based on practical decoding methods. We compare the performance of irregular with that of regular codes and discuss the factors that lead to the improvement in performance. PACS numbers: 89. 70 +c 89. 90 +n 05. 50 +q 1...|$|R
30|$|On {{the other}} hand, {{software-based}} multi-format video decoders {{have been developed}} on multi-core platforms. Various parallel implementations are utilized to alleviate the implementation costs of multi-format decoders with hardwired circuits. On the multi-core platform, {{it is possible to}} develop multi-format decoders that allow easy performance evaluation with minimum cost and effort compared to other hardwire designs. In addition, the multi-core platform requires less power for fast decoding of multi-format multimedia content due to the low clock speed. Parallel video decoders with multi-core platform can be implemented based on both data-level and functional-level parallelism. However, considering resolution, scalability, and performance in parallelism, the macroblock-level parallelism approach, which is a form of data-level parallelization, is widely used for video decoders [2]–[4]. For macroblock-level parallelism, the 2 D wave-front approach is widely known and used. This method can perform parallel decoding of multiple macroblocks without any decoding dependencies. However, this 2 D wave-front approach cannot be used for <b>entropy</b> <b>decoding</b> because of bit-by-bit dependency in a slice, even though back-end decoding can be parallelized with the multi-core platform [5]. Because the performance of parallel video decoders is highly influenced by sequential parts such as <b>entropy</b> <b>decoding,</b> the high-performance entropy decoder is an essential prerequisite for parallel video decoders. In addition, <b>entropy</b> <b>decoding</b> is one form of bottleneck that can decrease decoding throughput not only in parallel decoders but also in sequential decoders, especially for high-bitrate streams.|$|E
30|$|Many {{previous}} studies on fast implementation of CAVLD have proposed {{the use of}} hardwire logic [9]–[11]. However, hardware implementations have many drawbacks such as the long development period, the large silicon area, and the low reusability of multi-format video decoders, as previously mentioned. This paper focuses on fast <b>entropy</b> <b>decoding</b> algorithms for processor-based implementations. The simplest approach is the table lookup by sequential search (TLSS) as implemented in the joint model (JM) reference software [12]. TLSS finds symbols by comparing all possible codewords with part of the input bitstream. This approach requires {{a large number of}} comparisons and therefore cannot be used for real-time applications. Table lookup by binary search (TLBS) has also been proposed to improve the <b>entropy</b> <b>decoding</b> speed. The codewords are rearranged into a binary search tree structure, and the symbols can be extracted from the tree by using binary search. TLBS offers much better performance than TLSS; however, a more efficient implementation is needed for real-time applications.|$|E
40|$|One way to {{save the}} power {{consumption}} in the H. 264 decoder is for the H. 264 encoder to generate decoderfriendly bit streams. By following this idea, a decoding complexity model of context-based adaptive binary arithmetic coding (CABAC) for H. 264 /AVC is investigated in this research. Since different coding modes {{will have an impact}} on the number of quantized transformed coefficients (QTCs) and motion vectors (MVs) and, consequently, the complexity of <b>entropy</b> <b>decoding,</b> the encoder with a complexity model can estimate the complexity of <b>entropy</b> <b>decoding</b> and choose the best coding mode to yield the best tradeoff between the rate, distortion and decoding complexity performance. The complexity model consists of two parts: one for source data (i. e. QTCs) and the other for header data (i. e. the macro-block (MB) type and MVs). Thus, the proposed CABAC decoding complexity model of a MB is a function of QTCs and associated MVs, which is verified experimentally. The proposed CABAC decoding complexity model can provide good estimation results for variant bit streams. Practical applications of this complexity model will also be discussed. 1...|$|E
40|$|The {{upper bound}} on the {{capacity}} of a 3 -node discrete memoryless relay channel is considered, where a source X wants to send information to destination Y {{with the help of}} a relay Z. Y and Z are independent given X, and the link from Z to Y is lossless with rate $R_ 0 $. A new inequality is introduced to upper-bound the capacity when the encoding rate is beyond the capacities of both individual links XY and XZ. It is based on generalization of the blowing-up lemma, linking conditional <b>entropy</b> to <b>decoding</b> error, and channel simulation, to the case with side information. The achieved upper-bound is strictly better than the well-known cut-set bound in several cases when the latter is $C_{XY}+R_ 0 $, with $C_{XY}$ being the channel capacity between X and Y. One particular case is when the channel is statistically degraded, i. e., either Y is a statistically degraded version of Z with respect to X, or Z is a statistically degraded version of Y with respect to X. Moreover in this case, the bound is shown to be explicitly computable. The binary erasure channel is analyzed in detail and evaluated numerically. Comment: Submitted to IEEE Transactions on Information Theory, 21 pages, 6 figure...|$|R
40|$|Wavelet video coding using motion vectors {{estimated}} simultaneously at {{the transmitter}} and receiver side from the transmitted image data {{have been reported to}} have good compression capabilities, comparable to the non-scalable version of H. 263. When scalability is required, the comparison turns even more in favour of the wavelet coding scheme. This paper shows {{that it is possible to}} reduce the bit-rate further in backward motion estimation schemes by using the certainty of each estimated motion vector. In this paper we report a lowering in bit rate of about 20 % by using the motion vector certainty as background information in the <b>entropy</b> coding / <b>decoding</b> process. We also propose a low-complexity algorithm which does not require motion estimation / compensation, but uses the motion vector certainty. Keywords: Certainty, confidence, motion estimation, backward motion estimation, scalable video, scalability, wavelet video, multiresolution video coding, motion compensated wavelet coding, [...] ...|$|R
40|$|<b>Entropy</b> {{encoding}} and <b>decoding</b> is {{a crucial}} part of any multimedia system that can be highly demanding in terms of computing power. Hardware implementation of typ-ical compression and decompression algorithms is cum-bersome, while conventional software implementations are slow due to bit-level operations, data dependencies and conditional branching. Several solutions have been pro-posed along the years, ranging from hardware accelerators for high-end systems to careful implementations in VLIW processors and instruction-set extensions, both hardwired and reconfigurable. Multimedia systems must often imple-ment several encoders and decoders for different formats. Hence, a programmable solution is mandatory. However, programmable processors may be challenged by highly-complex algorithms. In this work, a highly efficient and low cost alternative is presented based on an array processor. The dataflow of several entropy coding algorithms has been studied, leading to the choice of an efficient programming model, processor layout and interconnection system. Re-sults are presented for JPEG and H. 264 image and video coding standards. ...|$|R
40|$|Abstract — A codebook-level duality between Slepian-Wolf coding {{and channel}} coding is established. Specifically, it is shown that using linear codes over ZM (the ring of {{integers}} mod M), each Slepian-Wolf coding problem {{is equivalent to}} a channel coding problem for a semi-symmetric additive channel under optimal decoding, belief propagation decoding, and minimum <b>entropy</b> <b>decoding.</b> Various notions of symmetric channels are discussed and their connections with semi-symmetric additive channels are clarified. I...|$|E
30|$|For {{the level}} and run_before syntax elements, the {{multi-level}} table mapping algorithms [22] are also used for fast decoding. The level syntax element is represented with the Exp-Golomb code, the codewords of which have leading zeros which are less than 15. The first 8 -bit codeword is decoded using table mapping, {{and the rest of}} the bits are decoded using arithmetic operations by employing characteristics of Exp-Golomb. Codewords with more than 15 leading zeros before the separator require exceptional handling. This algorithm also requires several conditional branches. For the run_before syntax, one of multiple tables is selected based on the number of zeros among the rest of coefficients to be decoded. When the number of zeros is greater than six, a larger mapping table is required. In this case, two-stage table mapping should be employed to reduce the amount of memory. For the remaining cases, one table mapping algorithm can perform the entire <b>entropy</b> <b>decoding.</b> This algorithm can significantly reduce the table memory requirement; however, many conditional branches should be involved, and these can reduce overall performance of <b>entropy</b> <b>decoding</b> with a multi-stage pipelined processor.|$|E
30|$|Conventional video coding {{architectures}} {{are primarily}} based on hybrid {{discrete cosine transformation}} (DCT) and interframe predictive video coding (PVC) frameworks. These frameworks allocate codec functionalities such {{that most of the}} high complexity operations that involve exploiting spatial and temporal correlation, e.g. motion estimation and compensation, are executed at the encoder, while the decoder performs lower complexity operations such as <b>entropy</b> <b>decoding,</b> frame prediction, inverse quantization, and DCT on the bitstream received from encoder (Hsia et al. 2013; Kim et al. 2014).|$|E
40|$|This paper {{presents}} a novel method driven by tree mapping template (TMT) which improve {{the accuracy of}} prosodic phrase boundary prediction. The TMT is capable of capturing the isomorphic relation between non-terminal nodes in hi-erarchical prosodic tree and nodes in binary tree approximation, performing pruning at the decoding phase and revising the baseline maximum entropy model with boosting method (AdaBoost). The model is statistical driven because TMTs are extracted automatically from hierarchical prosodic tree and binary tree ap-proximation generated by the Maximum <b>Entropy</b> model. In <b>decoding,</b> TMT is em-ployed to perform pruning, readjusting and local combination. To alleviate data sparse in limited labeled corpus, language model interpolation is made by intro-ducing a large scale unlabeled data. The experiments show that the TMT driven method reduces the decoding complexity for the prosodic phrasing prediction which is crucial for a real-time TTS system, achieving an improvement of 11. 5 % in terms of F-Measure compared to a conventional maximum entropy model without TMT...|$|R
40|$|Developing {{parallel}} {{applications that}} can harness and efficiently use future many-core architectures {{is the key}} challenge for scalable computing systems. We contribute to this challenge by presenting a parallel implementation of H. 264 that scales to {{a large number of}} cores. The algorithm exploits the fact that independent macroblocks (MBs) can be processed in parallel, but whereas a previous approach exploits only intra-frame MB-level parallelism, our algorithm exploits intra-frame as well as inter-frame MB-level parallelism. It is based on the observation that inter-frame dependencies have a limited spatial range. The algorithm has been implemented on a many-core architecture consisting of NXP TriMedia TM 3270 embedded processors. This required to develop a subscription mechanism, where MBs are subscribed to the kick-off lists associated with the reference MBs. Extensive simulation results show that the implementation scales very well, achieving a speedup of more than 54 on a 64 -core processor, in which case the previous approach achieves a speedup of only 23. Potential drawbacks of the 3 D-Wave strategy are that the memory requirements increase since there can be many frames in flight, and that the frame latency might increase. Scheduling policies to address these drawbacks are also presented. The results show that these policies combat memory and latency issues with a negligible effect on the performance scalability. Results analyzing the impact of the memory latency, L 1 cache size, and the synchronization and thread management overhead are also presented. Finally, we present performance requirements for <b>entropy</b> (CABAC) <b>decoding.</b> This work was performed while the fourth author was with NXP Semiconductors. Peer ReviewedPostprint (author's final draft...|$|R
40|$|A chip is {{described}} that will perform lossless compression and decompression using the Rice Algorithm. The chip set {{is designed to}} compress and decompress source data in real time for many applications. The encoder is designed to code at 20 M samples/second at MIL specifications. That corresponds to 280 Mbits/second at maximum quantization or approximately 500 Mbits/second under nominal conditions. The decoder is designed to decode at 10 M samples/second at industrial specifications. A wide range of quantization levels is allowed (4 [...] . 14 bits) and both nearest neighbor prediction and external prediction are supported. When the pre and post processors are bypassed, the chip set performs high speed <b>entropy</b> coding and <b>decoding.</b> This frees the chip set from being tied to one modeling technique or specific application. Both the encoder and decoder are being fabricated in a 1. 0 micron CMOS process that has been tested to survive 1 megarad of total radiation dosage. The CMOS chips are small, only 5 mm on a side, and both are estimated to consume less than 1 / 4 of a Watt of power while operating at maximum frequency...|$|R
40|$|The paper {{describes}} a software implementation of an MPEG [...] compliant Entropy Decoder on a TriMedia/CPU 64 processor. We first outline <b>entropy</b> <b>decoding</b> basics and TriMedia/CPU 64 architecture. Then, {{we describe the}} reference implementation of the entropy decoder, which consists mainly of a software pipelined loop. On each iteration, a set of look-up tables partitioning the VariableLength Codes (VLC) table defined by the MPEG standard are accessed in order to retrieve the run-level pair, or detect an end-of-block or error condition. An average of 21. 0 cycles are needed to decode a DCT coefficient according to this reference implementation. Then, we focus on software techniques to optimize the <b>entropy</b> <b>decoding</b> software pipelined loop. In particular, we propose {{a new way to}} partition the VLC table such that by exposing the loop prologue to the compiler, testing each of the end-of-block and error conditions within the prologue becomes superfluous. This is based on the observation that either an end-of-block or error condition will never occur within the first table look-up. For the proposed implementation, the simulation results indicate that an average of 16. 9 cycles are needed to decode a DCT coefficient. That is, our entropy decoder is more than 20 % faster than its reference counterpart...|$|E
3000|$|... [...]. For video decoder, a bound {{of energy}} {{constraints}} also exists. It {{implies that the}} optimal energy control method can be obtained when the total energy consumption is deduced by the method tends to the energy bound as closely as possible. Of course, the video decoding function contains many subfunctions such as interpolation (INTP), deblocking filter (DF), <b>entropy</b> <b>decoding</b> (END), and inverse transform (IDCT) [32]. According to bound constraint definition, designing an optimal energy/power consumption video decoding system can be transferred {{to find the best}} control among these subfunctions to achieve lower power/energy consumption, so that we can prolong the available battery duration.|$|E
40|$|QFHD) offers {{significantly}} enhanced visual experience. However, {{the corresponding}} huge data throughput {{of up to}} 530 Mpixels/s greatly challenges the design of real-time video decoder VLSI with the extensive requirement on both DRAM bandwidth and compu-tational power. In this work, a lossless frame recompression tech-nique and a partial MB reordering scheme are proposed to save the DRAM access of a QFHD video decoder chip. Besides, pipelining and parallelization techniques such as NAL/slice-parallel <b>entropy</b> <b>decoding</b> are implemented to efficiently enhance its computational power. The chip supporting H. 264 /AVC high profile is fabricated in 90 nm CMOS and verified. It delivers a maximum throughput o...|$|E
40|$|Abstract—We {{analyze the}} {{dependencies}} between the variables {{involved in the}} source and channel coding chain. This analysis is {{carried out in the}} framework of Bayesian networks, which provide both an intuitive representation for the global model of the coding chain and a way of deriving joint (soft) decoding algorithms. Three sources of dependencies are involved in the chain: 1) the source model, a Markov chain of symbols; 2) the source coder model, based on a variable length code (VLC), for example a Huffman code; and 3) the channel coder, based on a convolutional error correcting code. Joint decoding relying on the hidden Markov model (HMM) of the global coding chain is intractable, except in trivial cases. We advocate instead an iterative procedure inspired from serial turbo codes, in which the three models of the coding chain are used alternately. This idea of using separately each factor of a big product model inside an iterative procedure usually requires the presence of an interleaver between successive components. We show that only one interleaver is necessary here, placed between the source coder and the channel coder. The decoding scheme we propose {{can be viewed as a}} turbo algorithm using alternately the intersymbol correlation due to the Markov source and the redundancy introduced by the channel code. The intermediary element, the source coder model, is used as a translator of soft information from the bit clock to the symbol clock. Index Terms—Bayesian network, data compression, <b>entropy</b> coding, iterative <b>decoding,</b> joint source-channel decoding, probabilistic inference, soft decoding, turbo code, variable length code. I...|$|R
30|$|Among the {{different}} quality metrics {{used to assess}} the video quality, an objective full reference quality metric is proposed in Abharana et al. (2009) using natural decrease in <b>entropy</b> of <b>decoded</b> frame due to compression and vertical and horizontal artifacts due the blockiness effect and apart from that the spatial and temporal masking properties of human visual system are compared against other standard full reference metrics. But no reference quality metric has more advantage in terms of the computational complexity and the reference availability. Even though there are many works (Brandao et al. 2009; Arum et al. 2012) experimented for quality assessment on the compressed video, there are full reference metric as in Eden (2007), proposed a measure of picture quality as peak signal to noise ratio (PSNR) which is a full reference metric and estimated statistically using transform coefficients as no reference metric. A revised PSNR no-reference model is presented in Brandao and Queluz (2010) that estimate video quality using estimated DCT coefficients which are derived using Maximum Likelihood techniques. Content spatial-temporal activity calculation based on average SAD and display format based perceptual MOS calculation model is proposed in Joskowicz and Ardao (2010) and the relationship between the bit-rate and the MOS is derived. But only using the bit-rate is limiting the estimation quality of certain video service. In Valenzise et al. (2012), proposed an estimation of the pattern of lost macroblocks which produces an accurate estimate of the mean-square-error (MSE) distortion introduced by channel errors. The results of the proposed method are well correlated with the MSE distortion computed in full-reference mode, with a linear correlation coefficient of 0.9 at frame level. A two part no reference quality metric calculation consists of training and test is proposed in Kawano et al. (2010). In the training phase, they calculate the sensitivity from features like blockiness, blur and edge business etc. and rank these features using the Principal Component Analysis (PCA) method. In Rossholm and Lovstroem (2008), the author try to find a linear relationship between quality measurement method and media-layer metrics such as quantization parameter, bits per frame, frame rate, and mean motion vector length. The proposed methods in Ries et al. (2007) uses the video quality calculation using parameters such as bit rate, zero length motion vectors, mean motion vector lengths and motion vector direction. Even though bit rate is a key parameter (ITU-T G. 10702012) for estimating the coding distortion, the subjective quality of different video sequences cannot be correlated well with only the bitrate. So this proposed method, uses impairments such as blockiness, blur and jerkiness introduced by the spatial and temporal activities to improve the estimation accuracy in the encoder for the head end quality assurance.|$|R
40|$|This {{dissertation}} provides strong resilience techniques {{applying to}} general networks. Examples of important networks are private wired networks connecting large datacenters, overlay topology networks delivering live television broadcasts, and wireless mesh networks rapidly set {{up after a}} disaster to replace existing damaged infrastructure. Given the trends of increased reliance on networks and capabilities of attackers, network security is vital to national security. Network attacks can be characterized along the two dimensions of access and motivation. Attacker access can be either as an insider or outsider. An insider has more capabilities of having full control of some routing nodes. Attacker motivation can be targeting confidentiality, data integrity, or availability where availability {{is the only one}} that cannot typically be dealt by the known cryptographic techniques of encryption, digital signatures, or message authentication codes. In this dissertation, we focus on insider attackers that attack the availability of the network. ^ Our first step towards resilience is to ensure that an attacker cannot compromise nodes that partition the network since such an attack trivially succeeds in preventing availability. Such large compromises are likely in today 2 ̆ 7 s typical network deployment where all routers have identical components and a single successful exploit can be repeatedly used against all routers in the network. In our work, we demonstrate how diversity alleviates such problems when assigning diversity optimally to routers in the network. Routers that are diverse enough to not permit common exploits must have different components such as hardware, operating systems, routing code, and even administrators. These types of diversity are limited, so our assignment of diversity to routers typically has very few variants which must be assigned to a large number of routing nodes. We provide a comprehensive study of diversity assignment in networks by proposing problems for various network goals, techniques to solve these problems optimally or at scale, and demonstrated benefits of applying such analysis to real topologies. ^ Diversity ensures that a network remains well-connected by honest nodes even after sophisticated compromise attempts. However, an attacker can still succeed in attacking availability by attacking the routing protocol. We provide techniques resistant to insider attacks when using network coding. Network coding offers higher performance in a network by performing encoding techniques on packets. Insiders can attack the encoding technique in two ways by either forcing incorrect decoding or delaying decoding. For pollution attacks, forcing incorrect decoding, our work proposes a new defense against pollution attacks overcoming limitations of prior work which includes expensive security computation at routers, communication overhead that scales with the number of insiders, and delayed verification. For <b>entropy</b> attacks, delaying <b>decoding,</b> to the best of our knowledge our work is the first to demonstrate the effectiveness of such attacks along with considering defenses for sophisticated entropy attackers which collude. ...|$|R
