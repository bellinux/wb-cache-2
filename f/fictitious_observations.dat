13|3|Public
40|$|Logistic {{regression}} {{is one of}} {{the most}} used statistical techniques for explaining the probabilistic behavior of a given phenomenon. Data separation is a frequent problem in this model, as successes appear separated from failures and make it impossible to find the maximum likelihood estimators. Objective: to present a revision and a solution to the problem, and to compare it with other solutions. Methodology: a simulation of the logistic model and an estimation of the parameters’ bias using the proposed classical and Bayesian solution with <b>fictitious</b> <b>observations,</b> as well as the Firth method. Results: the bias found is lower when the pair of <b>fictitious</b> <b>observations</b> are generated using the Bayesian method. An example about the age at which menarche occurs is presented. Discussion: an appropriate solution to the problem of separation is provided using a simulation in a simple logistic model. Conclusions: the generation of <b>fictitious</b> <b>observations</b> within the separation region is recommended, and the best solution method is based on Bayesian theory, which achieves convergence of the parameters of the logistic model...|$|E
40|$|A {{new data}} abimilation {{technique}} is presented, {{based on the}} ensemble Kalman filter (EnKF), and makes particularly sense whenever few observations in time are available, and a stiff evolutionary equation such as the Richards' equation is integrated forward in time. Because of the Monte Carlo nature of EnKF, a cheap numerical method would be conve-nient to integrate the Richards'equation, thus a lot of observations are desiderable in order to frequently correct the numerical solution. Nevertheleb, abuming to have few observations in time, a grid of <b>fictitious</b> <b>observations</b> is settled just by interpolating the available observations. The measurement error covariance matrix abociated to these <b>fictitious</b> <b>observations</b> is estimated abuming that these errors evolve in time as Brownian bridges, with diffusion coefficients depending on the goodneb-To-fit of the polinomial curves {{with respect to the}} observed data...|$|E
40|$|The frequentist forward search {{yields a}} {{flexible}} and informative form of robust regression. The device of <b>fictitious</b> <b>observations</b> provides a natural way to include prior {{information in the}} search. However, this extension is not straightforward, requiring weighted regression. Bayesian versions of forward plots are used to exhibit the presence of multiple outliers in a data set from banking with 1903 observations and nine explanatory variables which shows, in this case, the clear advantages from including prior information in the forward search. Use of observation weights from frequentist robust regression is shown to provide a simple general method for robust Bayesian regression...|$|E
40|$|Based on the {{assumption}} that the slope bodies are rigid, the dynamic model of the landsiding (forward model) was put forward. According to the dynamic model, the system equations of Kalman filter were constituted. The mechanical status of a slope was hence combined with the monitoring data by Kalman filter. The model uncertainties or model errors could also be considered through some <b>fictitious</b> <b>observation</b> equations. Different from existed methods, the presented method can make use for not only the statistic information contained in the data but also the information provided by the mechanical and geological aspect of slopes. At last a numerical example was given out to show the feasibility of the method. Department of Land Surveying and Geo-InformaticsAuthor name used in this publication: 朱建军Author name used in this publication: 丁晓利, DING Xiao-liAuthor name used in this publication: 陈永奇Journal title in Traditional Chinese: 中國有色金屬學會會刊 (英文版...|$|R
40|$|We select among {{rules for}} {{learning}} which of two actions in a stationary decision problem achieves a higher expected payo¤when payoffs realized by both actions are known in previous instances. Only a bounded set containing all possible payoffs is known. Rules are evaluated using maximum risk with maximin utility, minimax regret, competitive ratio and selection procedures being special cases. A randomized variant of fictitious play attains minimax risk for all risk functions with ex-ante expected payoffs increasing {{in the number}} of <b>observations.</b> <b>Fictitious</b> play itself has neither of these two properties. Tight bounds on maximal regret and probability of selecting the best action are included. fictitious play, nonparametric, finite sample, matched pairs, foregone payoffs, minimax risk, ex-ante improving, selection procedure...|$|R
40|$|Abstract. Two {{approaches}} to determining the Earth’s external potential field by using GPS technique are proposed. The {{first one is}} that the relation between the geopotential difference and the light signal’s frequency shift, between two separated points, is applied. The second {{one is that the}} spherical harmonic expansion series and a new technique dealing with the “downward continuation ” problem are applied. Given the boundary value provided by GPS “geopotential frequency shift ” on the Earth’s surface, the Earth’s external field could be determined based on the “fictitious compress recovery” method. Given the boundary value derived by on-board GPS technique on the satellite surface, the Earth’s external field could be determined by using a new technique for solving the “downward continuation” problem, which is also based on the “fictitious compress recovery ” method. The main idea of the “fictitious compress recovery ” is that an iterative procedure of “compress ” and “recovery ” between the given boundary (the Earth’s surface or the satellite surface) and the surface of Bjerhammar sphere is executed and a fictitious field is created, which coincides with the real field in the domain outside the Earth. Simulation tests support the new approach. Key words: GPS <b>observation,</b> <b>fictitious</b> compress recovery, geopotential frequency shift, potential field determination...|$|R
40|$|The Very Long Base Interferometry. The VLBI {{modeling}} {{software packages}} CALC 6. 0 (GSFC) and MASTERFIT (JPL) are compared in some detail. Theoretical model delays are calculated {{for a set}} of 120 <b>fictitious</b> <b>observations</b> which involve a variety of sources, baselines, and antennas. Discrepancies between the total delays given by the two programs are of the order of 2 cm (RMS). The modeling of antenna offsets appears to account for approximately half of this difference. Relativistic bending contributions to the delay differ by 3 cm (RMS), and the there appears to be some mutual cancellation of errors involving antenna offsets, bending, and the effects of the two different Solar System ephemerides employed by CALC and MASTERFIT. This cancellation has not been completely characterized here...|$|E
40|$|A hybrid bundle {{adjustment}} is presented {{that allows for}} the integration of a generalised building model into the pose estimation of image sequences. These images are captured by an Unmanned Aerial System (UAS) equipped with a camera flying in between the buildings. The relation between the building model and the images is described by distances between the object coordinates of the tie points and building model planes. Relations are found by a simple 3 D distance criterion and are modelled as <b>fictitious</b> <b>observations</b> in a Gauss-Markov adjustment. The coordinates of model vertices {{are part of the}} adjustment as directly observed unknowns which allows for changes in the model. Results of first experiments using a synthetic and a real image sequence demonstrate improvements of the image orientation in comparison to an adjustment without the building model, but also reveal limitations of {{the current state of the}} method...|$|E
40|$|GNSS and VLBI antennas were {{connected}} to the identical hydrogen maser clocks at seven sites during CONT 11, which means that clock parameters {{can be regarded as}} common parameters at those sites as well as troposphere parameters. We construct GNSS single differences between the ranges from two stations to a satellite, using corrected phase measurements with the c 5 ++ software. Combining GNSS single difference and VLBI data during CONT 11, we estimate station coordinates and site common parameters, i. e. zenith wet delays, troposphere gradients and clock parameters, with the Vienna VLBI Software (VieVS). Local tie vectors, which contribute to the combination of terrestrial frames between GNSS and VLBI, are introduced as <b>fictitious</b> <b>observations.</b> We compare combination solutions with single technique solutions, assess the impact of the combination at the observation level with respect to geodetic results and discuss the current limitation and potentials to be developed...|$|E
40|$|Abstract — This work {{presents}} an efficient algorithm for an observation targeting {{problem that is}} complicated by the combinatorial number of targeting choices. The approach explicitly incorporates an ensemble forecast to ensure that the measurements are chosen based on their expected improvement in the forecast at a separate verification time and location. The primary improvements in the efficiency are obtained by computing the impact of each possible measurement on the uncertainty reduction over this verification site backwards. In particular, the approach determines the impact of a series of <b>fictitious</b> <b>observations</b> taken at the verification site back on the search space (and time), which provides all of the information needed to optimize the set of measurements to take and significantly reduces the number of times that the computationally expensive ensemble updates must be performed. A computation time analysis and numerical performance simulations using the two-dimensional Lorenz- 95 chaos model are presented to validate the computational advantage of the proposed algorithm over conventional search strategies. I...|$|E
40|$|Abstract—Distributed {{detection}} in {{the presence}} of cooperative (Byzantine) attack is considered. It is assumed that a fraction of the monitoring sensors are compromised by an adversary, and these compromised (Byzantine) sensors are reprogrammed to transmit <b>fictitious</b> <b>observations</b> aimed at confusing the decision maker at the fusion center. For detection under binary hypotheses with quantized sensor observations, the optimal attacking distributions for Byzantine sensors that minimize the detection error exponent are obtained using a “water-filling ” procedure. The smallest error exponent, {{as a function of the}} Byzantine sensor population, characterizes the power of attack. Also obtained is the minimum fraction of Byzantine sensors that destroys the consistency of detection at the fusion center. The case when multiple measurements are made at the remote nodes is also considered, and it is shown that the detection performance scales with the number of sensors differently from the number of observations at each sensor. Index Terms—Byzantine attack, distributed detection, network defense. I...|$|E
40|$|It is {{the goal}} of this paper to {{describe}} a contribution to CIPA’s Zurich test. The test project has been worked out using the PC-based digital photogrammetric program ORPHEUS. In order to show the flexibility and applicability of the program under quite general circumstances, several variants of both blocks were computed. After that, 3 D photo models of the Zurich city hall were created. The results are reported, and the variants are compared to each other. The test blocks were additionally plotted using the widely used software package PhotoModeler in order to obtain a reference for comparison. The paper describes the main features of ORPHEUS, with emphasis on robust hybrid adjustment, the observation types which can be used and the work flow for the production of 3 D photo models. <b>Fictitious</b> <b>observations</b> were applied for orientation/self-calibration (e. g., as horizontal and/or plumb lines) and for the 3 D determination of points only being visible in one photograph. ORPHEUS is very flexible with respect to the control information which can be used. Another focus of the paper will be the presentation {{of the results of the}} Zurich blocks and a comparison of different variants of block computation...|$|E
40|$|Unlike CONT 11, CONT 14 {{does not}} have {{official}} information on common frequency standards for co-located sites. Nevertheless, according to Kwak et al. (2015) [1], we have a possibility to find the co-located sites, which used the same clocks, through comparing clock rates from single technique solutions. Moreover, CONT 14 includes co-located VLBI radio telescopes, i. e. HOBART 26 and HOBART 12. Therefore, {{it is also a}} good test bed to develop the analysis strategy for future twin/sibling telescopes. In this study, we compute VLBI-like GNSS delays (GNSS single differences) between the ranges from two stations to a satellite, using phase measurements with most of the errors corrected by the c 5 ++ software. We estimate station coordinates and site common parameters, i. e. zenith wet delays, troposphere gradients and clock parameters, with the Vienna VLBI Software. Common clock parameters are limited to the sites sharing the same frequency standard and having good performance of it during CONT 14. Local tie vectors are introduced as <b>fictitious</b> <b>observations</b> for co-located instruments, GNSS-VLBI and even VLBI-VLBI, i. e. at Hobart. In this paper, we show the comparison results between the combination solutions and the single technique solutions in terms of station position repeatability during 15 days...|$|E
40|$|A {{new method}} for {{semi-automatic}} building extraction {{together with a}} concept for storing building models alongside with terrain data in a topographical information system (TIS) is presented. A user interface based on Constructive Solid Geometry is combined with an internal data structure completely based on boundary representation. Each building can be de-composed into a set of simple primitives which are reconstructed individually. After selecting a primitive from a data base of common building shapes, the primitive parameters can be modified by interactive measurement in digital images {{in order to provide}} approximate values for automatic fine measurement. In all phases, the properties of the boundary models are directly connected to parameter estimation: the parameters of the building primitives are determined in a hybrid adjustment of camera co-ordinates and <b>fictitious</b> <b>observations</b> of points being situated on building faces. Automatic fine measurement is an application of a general framework for object surface reconstruction using hierarchical feature based object space matching. The integration of object space into the matching process is achieved by the new modelling technique. The management of both building and terrain data in a TIS is based on a unique principle. Meta data are managed in a relational data base, whereas the actual data are treated as binary large objects. The new method is evaluated in a test project (image scale: 1 : 4500, 70 % overlap, 50 % side lap). The automatic tool gives results with an accuracy of +- 2 - 5 cm in planimetric position and +- 5 - 10 cm in height. ...|$|E
40|$|A {{method for}} the {{simultaneous}} co-registration and georeferencing of multiple 3 D pointclouds and associated intensity information is proposed. It is a generalization of the 3 D surface matching problem. The simultaneous co-registration {{provides for a}} strict solution to the problem, as opposed to sequential pairwise registration. The problem is formulated as the Least Squares matching of overlapping 3 D surfaces. The parameters of 3 D transformations of multiple surfaces are simultaneously estimated, using the Generalized Gauss-Markoff model, minimizing the sum of squares of the Euclidean distances among the surfaces. An observation equation is written for each surface-to-surface correspondence. Each overlapping surface pair contributes a group of observation equations to the design matrix. The parameters are introduced into the system as stochastic variables, as a second type of (<b>fictitious)</b> <b>observations.</b> This extension allows to control the estimated parameters. Intensity information is introduced into the system {{in the form of}} quasisurfaces as the third type of observations. Reference points, defining an external (object) coordinate system, which are imaged in additional intensity images, or can be located in the pointcloud, serve as the fourth type of observations. They transform the whole block of “models ” to a unique reference system. Furthermore, the given coordinate values of the control points are treated as observations. This gives the fifth type of observations. The total system is solved by applying the Least Squares technique, provided that sufficiently good initial values for the transformation parameters are given. This method can be applied to data sets generated from aerial as well as terrestrial laser scanning or other pointcloud generating methods. 1...|$|E
40|$|The 3 -OC {{is one of}} {{the newest}} digital three line {{scanners}} on the market. Unlike other three line scanners using a single optical system, the 3 -OC uses three different optical systems moving together. Therefore, this thesis aimed to develop a photogrammetric model for the 3 -OC. To precisely relate ground space and the corresponding image space, all the exterior orientation (E. O.) parameters of image lines need to be estimated using a bundle block adjustment. The biggest hurdle in this problem is the large number of exterior orientation parameters because one image strip of the 3 -OC usually contains tens of thousands of lines. To reduce the number of unknown E. O. parameters, the E. O. parameters of all the three cameras at an instant imaging time were represented by transformed parameters with respect to the gimbal rotation center. As a result, the unknown E. O. parameters were reduced to one third of original number of parameters. However, the number of E. O. parameters is still too big and estimating these E. O. parameters requires enough observations which are practically very difficult to obtain. To resolve this problem, there have been two kinds of approaches. One is reducing the number of unknown parameters and the other is providing <b>fictitious</b> <b>observations</b> using a stochastic model. As the title of this thesis implies, a stochastic trajectory model was implemented in this thesis. The stochastic relationships between two adjacent lines, as described in previous work, were expanded to the stochastic relationships between two adjacent image observations, so that the E. O. parameters of the lines between two adjacent observations can be recovered by interpolation. By providing enough pass points, it was possible to recover all the E. O. parameters accurately. In addition, the number of unknown E. O. parameters was drastically reduced as well. In this thesis, aerial triangulations of the suggested photogrammetric model were performed with self-calibrating some of the system parameters. As a result, the exterior orientation parameters were successfully estimated and the system parameters were calibrated as well. The RMSE of image misclosures on check points was less than 1. 2 pixel and the RMSE of ground misclosures at check points was less than 0. 6 ft (nominal GSD is 0. 5 ft). ...|$|E

