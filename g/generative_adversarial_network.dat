172|510|Public
5000|$|This method uses <b>generative</b> <b>adversarial</b> <b>network</b> (GAN), a deep machine {{learning}} technique where computers work {{against each other}} {{to create a more}} believable image or soon, pieces of audio. [...] "...algorithms were able to produce speech that occasionally sounded perceptually similar to the target speaker but work remains to be done." ...|$|E
40|$|We {{demonstrate}} that a <b>generative</b> <b>adversarial</b> <b>network</b> {{can be trained}} to produce Ising model configurations in distinct regions of phase space. In training a <b>generative</b> <b>adversarial</b> <b>network,</b> the discriminator neural network becomes very good a discerning examples from the training set and examples from the testing set. We {{demonstrate that}} this ability {{can be used as}} an anomaly detector, producing estimations of operator values along with a confidence in the prediction. Peer reviewed: NoNRC publication: Ye...|$|E
40|$|Image {{generation}} {{remains a}} fundamental problem in artificial intelligence {{in general and}} deep learning in specific. The <b>generative</b> <b>adversarial</b> <b>network</b> (GAN) was successful in generating high quality samples of natural images. We propose a model called composite <b>generative</b> <b>adversarial</b> <b>network,</b> that reveals the complex structure of images with multiple generators in which each generator generates {{some part of the}} image. Those parts are combined by alpha blending process to create a new single image. It can generate, for example, background and face sequentially with two generators, after training on face dataset. Training was done in an unsupervised way without any labels about what each generator should generate. We found possibilities of learning the structure by using this generative model empirically...|$|E
40|$|In this paper, a novel {{strategy}} of Secure Steganograpy based on <b>Generative</b> <b>Adversarial</b> <b>Networks</b> is proposed to generate suitable and secure covers for steganography. The proposed architecture has one generative network, and two discriminative networks. The generative network mainly evaluates the visual {{quality of the}} generated images for steganography, and the discriminative networks are utilized to assess their suitableness for information hiding. Different from the existing work which adopts Deep Convolutional <b>Generative</b> <b>Adversarial</b> <b>Networks,</b> we utilize another form of <b>generative</b> <b>adversarial</b> <b>networks.</b> By using this new form of <b>generative</b> <b>adversarial</b> <b>networks,</b> significant improvements are made on the convergence speed, the training stability and the image quality. Furthermore, a sophisticated steganalysis network is reconstructed for the discriminative network, and the network can better evaluate {{the performance of the}} generated images. Numerous experiments are conducted on the publicly available datasets to demonstrate the effectiveness and robustness of the proposed method...|$|R
30|$|The {{recognition}} of surgical workflow based on deep learning requires {{a large number}} of labeled data. However, surgical video is more massive unlabeled data and a small number of labeled data. By <b>generative</b> <b>adversarial</b> <b>networks,</b> we can not only effectively utilize {{a large number of}} unlabeled surgical video to pre-train the models, but also generate surgical video samples [31]. This paper draws on the idea of unsupervised <b>generative</b> <b>adversarial</b> <b>networks,</b> trains a <b>generative</b> network based on surgical data sets, and then uses the discriminant network as the spatial feature extraction model in this paper. The spatial feature of the surgical workflow is mainly the image feature of the surgical video. By training the <b>generative</b> <b>adversarial</b> <b>networks,</b> the discriminant network can effectively understand the image features of the surgical workflow, so the network can be used as spatial feature extraction for surgical workflow. Our implementation is mainly learned from the [31], and details are shown in the experimental part of this paper.|$|R
40|$|Traditional image {{steganography}} modifies {{the content}} of the image more or less, it is hard to resist the detection of image steganalysis tools. To address this problem, a novel method named generative coverless information hiding method based on <b>generative</b> <b>adversarial</b> <b>networks</b> is proposed in this paper. The main idea of the method is that the class label of <b>generative</b> <b>adversarial</b> <b>networks</b> is replaced with the secret information as a driver to generate hidden image directly, and then extract the secret information from the hidden image through the discriminator. It's {{the first time that the}} coverless information hiding is achieved by <b>generative</b> <b>adversarial</b> <b>networks.</b> Compared with the traditional image steganography, this method does not modify {{the content of}} the original image. therefore, this method can resist image steganalysis tools effectively. In terms of steganographic capacity, anti-steganalysis, safety and reliability, the experimen shows that this hidden algorithm performs well. Comment: arXiv admin note: text overlap with arXiv: 1703. 05502 by other author...|$|R
30|$|We {{propose to}} {{mitigate}} the problem stated {{in the previous section}} by using a generative neural model for appliance load sequence generation. We pre-train this model using a <b>Generative</b> <b>Adversarial</b> <b>Network</b> (GAN) (Goodfellow et al., 2014) architecture and integrate it into the Neural NILM disaggregation process.|$|E
40|$|URL] We {{provide a}} bridge between {{generative}} modeling in the Machine Learning community and simulated physical processes in High Energy Particle Physics by applying a novel <b>Generative</b> <b>Adversarial</b> <b>Network</b> (GAN) architecture {{to the production of}} jet images [...] 2 D representations of energy depositions from particles interacting with a calorimeter. We propose a simple architecture, the Location-Aware <b>Generative</b> <b>Adversarial</b> <b>Network,</b> that learns to produce realistic radiation patterns from simulated high energy particle collisions. The pixel intensities of GAN-generated images faithfully span over many orders of magnitude and exhibit the desired low-dimensional physical properties (i. e., jet mass, n-subjettiness, etc.). We shed light on limitations, and provide a novel empirical validation of image quality and validity of GAN-produced simulations of the natural world. This work provides a base for further explorations of GANs for use in faster simulation in High Energy Particle Physics...|$|E
30|$|The speech {{enhancement}} {{based on the}} <b>generative</b> <b>adversarial</b> <b>network</b> has achieved excellent results with large quantities of data, but performance in the low-data regime and tasks like unseen data learning still lag behind. In this work, we model Wasserstein Conditional Generative Adversarial Network-Gradient Penalty {{speech enhancement}} system and introduce the elastic network into the objective function to simplify and improve {{the performance of the}} model in low-resource data environment. We argue that the regularization is significant in learning with small amounts of data and the available information of the input data is key in speech enhancement performance and generalization ability of the model, which means that network parameters and network structure can be set up and designed according to the characteristics of actual input data. Experiments on the noisy speech corpus show that the improved algorithm outperforms previous <b>generative</b> <b>adversarial</b> <b>network</b> speech enhancement approach.|$|E
5000|$|In 2017 {{online service}} Algorithmia {{released}} free-of-charge, although sample limited service that uses neural nets trained on colorized images fed to deep <b>generative</b> <b>adversarial</b> <b>networks</b> along with automatically generated grayscale.http://demos.algorithmia.com/colorize-photos/ ...|$|R
40|$|Auto-encoding <b>generative</b> <b>adversarial</b> <b>networks</b> (GANs) {{combine the}} {{standard}} GAN algorithm, which discriminates between real and model-generated data, with a reconstruction loss given by an auto-encoder. Such models aim to prevent mode {{collapse in the}} learned generative model by ensuring that it is grounded in all the available training data. In this paper, we develop a principle upon which auto-encoders can be combined with <b>generative</b> <b>adversarial</b> <b>networks</b> by exploiting the hierarchical structure of the generative model. The underlying principle shows that variational inference can be used a basic tool for learning, but with the in- tractable likelihood replaced by a synthetic likelihood, and the unknown posterior distribution replaced by an implicit distribution; both synthetic likelihoods and implicit posterior distributions can be learned using discriminators. This allows us to develop a natural fusion of variational auto-encoders and <b>generative</b> <b>adversarial</b> <b>networks,</b> combining {{the best of both}} these methods. We describe a unified objective for optimization, discuss the constraints needed to guide learning, connect to the wide range of existing work, and use a battery of tests to systematically and quantitatively assess the performance of our method...|$|R
40|$|<b>Generative</b> <b>Adversarial</b> <b>Networks</b> (GANs) {{convergence}} in a high-resolution {{setting with}} a computational constrain of GPU memory capacity (from 12 GB to 24 GB) has been beset with difficulty {{due to the}} known lack of convergence rate stability. In order to boost network convergence of DCGAN (Deep Convolutional <b>Generative</b> <b>Adversarial</b> <b>Networks)</b> and achieve good-looking high-resolution results we propose a new layered network structure, HR-DCGAN, that incorporates current state-of-the-art techniques for this effect. A novel dataset, CZ Faces (CZF), containing human faces from different ethnical groups {{in a wide variety}} of illumination conditions and image resolutions is introduced. We conduct extensive experiments on CelebA and CZF...|$|R
40|$|This paper {{describes}} InfoGAN, an information-theoretic {{extension to}} the <b>Generative</b> <b>Adversarial</b> <b>Network</b> that {{is able to}} learn disentangled representations in a completely unsupervised manner. InfoGAN is a <b>generative</b> <b>adversarial</b> <b>network</b> that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure {{can be interpreted as}} a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3 D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods...|$|E
40|$|We {{provide a}} bridge between {{generative}} modeling in the Machine Learning community and simulated physical processes in High Energy Particle Physics by applying a novel <b>Generative</b> <b>Adversarial</b> <b>Network</b> (GAN) architecture {{to the production of}} jet images [...] 2 D representations of energy depositions from particles interacting with a calorimeter. We propose a simple architecture, the Location-Aware <b>Generative</b> <b>Adversarial</b> <b>Network,</b> that learns to produce realistic radiation patterns from simulated high energy particle collisions. The pixel intensities of GAN-generated images faithfully span over many orders of magnitude and exhibit the desired low-dimensional physical properties (i. e., jet mass, n-subjettiness, etc.). We shed light on limitations, and provide a novel empirical validation of image quality and validity of GAN-produced simulations of the natural world. This work provides a base for further explorations of GANs for use in faster simulation in High Energy Particle Physics. Comment: 23 pages, 23 figures, 1 table, and appendix; Added new validation metric, acknowledgements, minor correction...|$|E
40|$|In this work, {{we present}} the Text Conditioned Auxiliary Classifier <b>Generative</b> <b>Adversarial</b> <b>Network,</b> (TAC-GAN) a text to image <b>Generative</b> <b>Adversarial</b> <b>Network</b> (GAN) for synthesizing images from their text descriptions. Former {{approaches}} {{have tried to}} condition the generative process on the textual data; but allying it to the usage of class information, known to diversify the generated samples and improve their structural coherence, has not been explored. We trained the presented TAC-GAN model on the Oxford- 102 dataset of flowers, and evaluated the discriminability of the generated images with Inception-Score, {{as well as their}} diversity using the Multi-Scale Structural Similarity Index (MS-SSIM). Our approach outperforms the state-of-the-art models, i. e., its inception score is 3. 45, corresponding to a relative increase of 7. 8 % compared to the recently introduced StackGan. A comparison of the mean MS-SSIM scores of the training and generated samples per class shows that our approach is able to generate highly diverse images with an average MS-SSIM of 0. 14 over all generated classes...|$|E
40|$|The goal of {{this paper}} is not to {{introduce}} a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of <b>generative</b> <b>adversarial</b> <b>networks.</b> In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training <b>generative</b> <b>adversarial</b> <b>networks.</b> The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them...|$|R
40|$|Despite {{the successes}} in {{capturing}} continuous distributions, {{the application of}} <b>generative</b> <b>adversarial</b> <b>networks</b> (GANs) to discrete settings, like natural language tasks, is rather restricted. The fundamental reason is the difficulty of back-propagation through discrete random variables combined with the inherent instability of the GAN training objective. To address these problems, we propose Maximum-Likelihood Augmented Discrete <b>Generative</b> <b>Adversarial</b> <b>Networks.</b> Instead of directly optimizing the GAN objective, we derive a novel and low-variance objective using the discriminator's output that follows corresponds to the log-likelihood. Compared with the original, the new objective is proved to be consistent in theory and beneficial in practice. The experimental results on various discrete datasets demonstrate {{the effectiveness of the}} proposed approach. Comment: 11 pages, 3 figure...|$|R
40|$|Since its appearance, <b>Generative</b> <b>Adversarial</b> <b>Networks</b> (GANs) have {{received}} {{a lot of interest in}} the AI community. In image generation several projects showed how GANs are able to generate photorealistic images but the results so far did not look adequate for the quality standard of visual media production industry. We present an optimized image generation process based on a Deep Convolutional <b>Generative</b> <b>Adversarial</b> <b>Networks</b> (DCGANs), in order to create photorealistic high-resolution images (up to 1024 x 1024 pixels). Furthermore, the system was fed with a limited dataset of images, less than two thousand images. All these results give more clue about future exploitation of GANs in Computer Graphics and Visual Effects. Comment: 3 pages, 4 figure...|$|R
40|$|A {{conditional}} <b>Generative</b> <b>Adversarial</b> <b>Network</b> {{allows for}} generating samples conditioned on certain external information. Being {{able to recover}} latent and conditional vectors from a condi- tional GAN can be potentially valuable in various applications, ranging from image manipulation for entertaining purposes to diagnosis of the neural networks for security purposes. In this work, we show {{that it is possible}} to recover both latent and conditional vectors from generated images given the generator of a conditional <b>generative</b> <b>adversarial</b> <b>network.</b> Such a recovery is not trivial due to the often multi-layered non-linearity of deep neural networks. Furthermore, the effect of such recovery applied on real natural images are investigated. We discovered that there exists a gap between the recovery performance on generated and real images, which we believe comes from the difference between generated data distribution and real data distribution. Experiments are conducted to evaluate the recovered conditional vectors and the reconstructed images from these recovered vectors quantitatively and qualitatively, showing promising results. Comment: Under consideration for Pattern Recognition Letters, 11 page...|$|E
30|$|To summarize, in this work, {{we first}} propose an {{end-to-end}} {{convolutional neural network}} model to learn effective features from the blurred face images, and then estimate a latent one. To constrain the network, we introduce to utilize a transfer learning framework to learn the multiple features. In addition, we adopt well-established deep networks to obtain extremely expressive features and achieve high quality results. Specifically, we also utilize the <b>generative</b> <b>adversarial</b> <b>network</b> (GAN) to optimize image realistic.|$|E
40|$|Although Generative Adversarial Networks (GANs) {{have shown}} {{remarkable}} success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) aiming at generating high-resolution photo-realistic images. First, we propose a two-stage <b>generative</b> <b>adversarial</b> <b>network</b> architecture, StackGAN-v 1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and {{colors of the}} object based on given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage <b>generative</b> <b>adversarial</b> <b>network</b> architecture, StackGAN-v 2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v 2 consists of multiple generators and discriminators in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v 2 shows more stable training behavior than StackGAN-v 1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images. Comment: 14 pages, 14 figures. arXiv admin note: text overlap with arXiv: 1612. 0324...|$|E
50|$|<b>Generative</b> <b>adversarial</b> <b>networks</b> (GANs) are a {{class of}} {{artificial}} intelligence algorithms used in unsupervised machine learning, implemented by a system of two neural networks contesting {{with each other in}} a zero-sum game framework. They were introduced by Ian Goodfellow et al. in 2014.|$|R
40|$|<b>Generative</b> <b>adversarial</b> <b>networks</b> (GANs) are a {{powerful}} framework for generative tasks. However, they {{are difficult to}} train and tend to miss modes of the true data generation process. Although GANs can learn a rich representation of the covered modes of the data in their latent space, the framework misses an inverse mapping from data to this latent space. We propose Invariant Encoding <b>Generative</b> <b>Adversarial</b> <b>Networks</b> (IVE-GANs), a novel GAN framework that introduces such a mapping for individual samples from the data by utilizing features in the data which are invariant to certain transformations. Since the model maps individual samples to the latent space, it naturally encourages the generator to cover all modes. We demonstrate the effectiveness of our approach in terms of generative performance and learning rich representations on several datasets including common benchmark image generation tasks. Comment: under review at ICLR 201...|$|R
30|$|The {{paper is}} {{organised}} as follows. In Sect.  2 {{we describe the}} <b>Generative</b> <b>Adversarial</b> <b>Networks.</b> Section  3 contains the information on N-body simulations used. Our implementation of the algorithm is described in Sect.  4 and diagnostics used to evaluate its performance are detailed in Sect.  5. We present the results in Sect.  6 and conclude in Sect.  7.|$|R
40|$|A major {{bottleneck}} {{for developing}} general reinforcement learning agents is determining rewards that will yield desirable behaviors under various circumstances. We introduce a general mechanism for automatically specifying meaningful behaviors from raw pixels. In particular, we train a <b>generative</b> <b>adversarial</b> <b>network</b> to produce short sub-goals represented through motion templates. We demonstrate {{that this approach}} generates visually meaningful behaviors in unknown environments with novel agents and describe how these motions {{can be used to}} train reinforcement learning agents. Comment: Deep Reinforcement Learning Symposium, NIPS 201...|$|E
40|$|Face {{transfer}} animates {{the facial}} {{performances of the}} character in the target video by a source actor. Traditional methods are typically based on face modeling. We propose an end-to-end face transfer method based on <b>Generative</b> <b>Adversarial</b> <b>Network.</b> Specifically, we leverage CycleGAN to generate the face image of the target character with the corresponding head pose and facial expression of the source. In order {{to improve the quality}} of generated videos, we adopt PatchGAN and explore the effect of different receptive field sizes on generated images...|$|E
40|$|While the {{visualization}} of statistical data tends to a mature technology, the {{visualization of}} textual data {{is still in}} its infancy, especially for the artistic text. Due to the fact that visualization of artistic text is valuable and attractive in both art and information science, we attempt to realize this tentative idea in this article. We propose the <b>Generative</b> <b>Adversarial</b> <b>Network</b> based Artistic Textual Visualization (GAN-ATV) which can create paintings after analyzing the semantic content of existing poems. Our GAN-ATV consists of two main sections: natural language analysis section and visual information synthesis section. In natural language analysis section, we use Bag-of-Word (BoW) feature descriptors and a two-layer network to mine and analyze the high-level semantic information from poems. In visual information synthesis section, we design a cross-modal semantic understanding module and integrate it with <b>Generative</b> <b>Adversarial</b> <b>Network</b> (GAN) to create paintings, whose content are corresponding to the original poems. Moreover, in order to train our GAN-ATV and verify its performance, we establish a cross-modal artistic dataset named "Cross-Art". In the Cross-Art dataset, there are six topics and each topic has their corresponding paintings and poems. The experimental results on Cross-Art dataset are shown in this article. Comment: 6 pages, 3 figure...|$|E
50|$|Goodfellow is {{best known}} for inventing <b>generative</b> <b>adversarial</b> <b>networks,</b> an {{approach}} to machine learning used heavily at Facebook. He is also the lead author of the textbook Deep Learning. At Google, he developed a system enabling Google Maps to automatically transcribe addresses from photos taken by Street View cars and demonstrated security vulnerabilities of machine learning systems.|$|R
40|$|We propose Information Theoretic-Learning (ITL) {{divergence}} {{measures for}} variational regularization of neural networks. We also explore ITL-regularized autoencoders {{as an alternative}} to variational autoencoding bayes, <b>adversarial</b> autoencoders and <b>generative</b> <b>adversarial</b> <b>networks</b> for randomly generating sample data without explicitly defining a partition function. This paper also formalizes, generative moment matching networks under the ITL framework. Comment: 8 pages, 4 figure...|$|R
40|$|Anomaly {{detection}} is used {{to identify}} abnormal observations that don t follow a normal pattern. Inthis work, we {{use the power of}} <b>Generative</b> <b>Adversarial</b> <b>Networks</b> in sampling from image distributionsto perform anomaly detection with images and to identify local anomalous segments within thisimages. Also, we explore potential application of this method to support pathological analysis ofbiological tissue...|$|R
30|$|As {{this paper}} {{presents}} first results of implementing the concept of ANN-encoded abstracted flexibility for energy management and smart grids, {{we are working on}} applying the concept in additional realistic scenarios, with more and other types of DERs, considering additional constraints, improving the used ANNs, as well as refining the patterns. Regarding the poor performance of our ANNs on some load profile generation tasks in contrast to the good classification performance, future research could take advantage of the classification ANN for a <b>Generative</b> <b>Adversarial</b> <b>Network</b> (Goodfellow et al. 2014) to generate more accurate load profiles.|$|E
40|$|We {{present the}} Neural Photo Editor, an {{interface}} {{for exploring the}} latent space of generative image models and making large, semantically coherent changes to existing images. Our interface is powered by the Introspective Adversarial Network, a hybridization of the <b>Generative</b> <b>Adversarial</b> <b>Network</b> and the Variational Autoencoder designed {{for use in the}} editor. Our model makes use of a novel computational block based on dilated convolutions, and Orthogonal Regularization, a novel weight regularization method. We validate our model on CelebA, SVHN, and ImageNet, and produce samples and reconstructions with high visual fidelity...|$|E
40|$|This paper {{describes}} a general, scalable, end-to-end framework {{that uses the}} <b>generative</b> <b>adversarial</b> <b>network</b> (GAN) objective to enable robust speech recognition. Encoders trained with the proposed approach enjoy improved invariance by learning to map noisy audio to the same embedding space as that of clean audio. Unlike previous methods, the new framework does not rely on domain expertise or simplifying assumptions as are often needed in signal processing, and directly encourages robustness in a data-driven way. We show the new approach improves simulated far-field speech recognition of vanilla sequence-to-sequence models without specialized front-ends or preprocessing...|$|E
40|$|In {{this study}} we propose a new method to {{simulate}} hyper-realistic urban patterns using <b>Generative</b> <b>Adversarial</b> <b>Networks</b> trained with a global urban land-use inventory. We generated a synthetic urban "universe" that qualitatively reproduces the complex spatial organization observed in global urban patterns, while being able to quantitatively recover certain key high-level urban spatial metrics. Comment: 4 pages, 4 figure...|$|R
40|$|This paper {{describes}} {{a method for}} using <b>Generative</b> <b>Adversarial</b> <b>Networks</b> to learn distributed representations of natural language documents. We propose a model {{that is based on}} the recently proposed Energy-Based GAN, but instead uses a Denoising Autoencoder as the discriminator network. Document representations are extracted from the hidden layer of the discriminator and evaluated both quantitatively and qualitatively...|$|R
30|$|There {{has also}} been {{increasing}} interest in taking advantage of unlabeled data, i.e., semi-supervised learning, to overcome a small-data problem. Examples of this attempt include pseudo-label [36] and incorporating generative models, such as <b>generative</b> <b>adversarial</b> <b>networks</b> (GANs) [37]. However, whether these techniques can really help improve the performance of deep learning in radiology is not clear and remains an area of active investigation.|$|R
