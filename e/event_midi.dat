0|56|Public
50|$|<b>MIDI</b> <b>event</b> list editor.|$|R
50|$|Up to 256 <b>MIDI</b> <b>events</b> {{can trigger}} {{dedicated}} functions; processing {{of the event}} list requires about 300 µS. <b>MIDI</b> <b>events</b> can also be processed by a user routine for sysex parsing or similar jobs. A user timer is available for time triggered code.|$|R
40|$|Abstract. The present work {{proposes a}} model for a Multi-Agent System capable to deal with music. We believe that many Computer Music {{problems}} could be approached through such a Musical Multi-Agent System. Our proposal is based on a community of agents that interact through musical <b>events</b> (<b>MIDI),</b> simulating the behavior of a musical group. As a case study we have implemented a rhythmic accompaniment system. “Listening ” to each other, the agents were able to play their instruments in synchronism. 1...|$|R
50|$|China Dolls {{was partly}} {{inspired}} by the real life <b>events</b> of <b>Midi</b> Taka of the vaudeville act Taka Sisters. Midi was tragically murdered in a love triangle.|$|R
5000|$|Provides basic {{functionality}} to receive and transmit <b>MIDI</b> <b>events</b> ...|$|R
5000|$|Send tempo changes, patch changes, {{and other}} <b>MIDI</b> <b>events</b> programmatically.|$|R
50|$|The RM1x {{can save}} up to 20 songs {{at a time}} in memory, from sequenced or realtime {{recorded}} <b>MIDI</b> <b>events.</b>|$|R
50|$|In 2016, Gevolt had a mini tour in China {{where it}} {{performed}} in two major <b>events,</b> Taihu <b>Midi</b> Festival and Dream Sonic Festival. For this tour the band formed {{a new line}} up {{with the return of}} Michael Gimmervert and including Alex Zvulun (Bass), Marianne Tur (Violin), and Dror Goldstein (Drums).|$|R
40|$|Abstract: This paper {{describes}} SWARMUSIC, {{an interactive}} music improviser. A particle swarm algorithm {{is used to}} generate musical material by a mapping of particle positions onto <b>events</b> in <b>MIDI</b> space. Interaction with an external musical source arises through the attraction of the particle swarm to a target. SWARMUSIC is the first application of swarm intelligence to music. ...|$|R
50|$|ZIPI used {{completely}} new message {{system and a}} complex note addressing scheme based on Music Parameter Description Language (MPDL) protocol, which was a direct replacement to <b>MIDI</b> <b>events.</b>|$|R
40|$|We {{present a}} new {{protocol}} to transmit time ordered events in real-time over Internet and {{to operate a}} correct time rendering on the receiver side. This protocol provides solutions {{to compensate for the}} network latency, to optimize the bandwidth use and to take account of the clock drift of the different stations involved in a transmission. It is particularly suitable to transmit musical <b>events</b> such as <b>MIDI</b> <b>events.</b> The implementation is based on the User Datagram Protocol (UDP) however, the proposed solution is independant of the underlying network layers. 1...|$|R
50|$|The {{company is}} known for its Animusic {{compilations}} of computer-generated animations, based on <b>MIDI</b> <b>events</b> processed to simultaneously drive the music and on-screen action, leading to and corresponding to every sound.|$|R
5000|$|Each <b>MIDI</b> <b>event</b> in the RTP-MIDI payload {{can then}} be {{strictly}} synchronized with the global clock. The synchronization accuracy directly depends on the clock source defined when opening the RTP-MIDI session. RFC 6295 gives some examples based on an audio sampling clock, {{in order to get}} a sample accurate timestamping of <b>MIDI</b> <b>events.</b> Apple's RTP-MIDI implementation (as all other related implementations like rtpMIDI driver for Windows or KissBox embedded systems) use a fixed clock rate of 10 kHz rather than a sampling audio rate. The timing accuracy of all <b>MIDI</b> <b>events</b> is then 100 microseconds for these implementations.Sender and receiver clocks are synchronized when the session is initiated, and they are kept synchronized during the whole session period by the regular synchronization cycles, controlled by the session initiators. This mechanism has the capability to compensate any latency, from a few hundreds of microseconds (as seen on LAN applications) to seconds (being able then to compensate the latency introduced by the Web for example, allowing real-time execution of music piece over the Internet) ...|$|R
40|$|When {{travelling}} on a train, {{many people}} enjoy {{looking out of}} the window at the landscape passing by. We present an application that translates the perceived movement of the landscape and other occurring events such as passing trains into music. The continuously changing view outside the window is captured with a camera and translated into <b>midi</b> <b>events</b> that are replayed instantaneously. This allows for a reflection of the visual impression, adding a sound dimension to the visual experience and deepening the state of contemplation. The application can both be run on mobile phones (with built-in camera) and on laptops (with a connected Web-cam). We present and discuss different approaches to translate the video signal into <b>midi</b> <b>events.</b> 1...|$|R
5000|$|Intermorphic's Noatikl (2007-present). Noatikl is {{described}} by Intermorphic as [...] "The Evolution of Koan", and was launched in 2007 {{as a replacement for}} the no-longer-available Koan. Noatikl is a generative music engine that generates <b>MIDI</b> <b>events</b> in accordance with a rule set that can be manipulated in real-time through a graphical user interface. Noatikl can operate as a Hyperinstrument by responding to incoming <b>MIDI</b> <b>event</b> data, with optional extension through user-supplied Lua scripts. Noatikl is available as a standalone tool for iOS, macOS and Windows, and there are VST and AU plug-ins for desktop music sequencers. Noatikl 2 was released in May 2012. Noatikl 3 for iOS, macOS and Windows was released in November 2015.|$|R
50|$|In 2007 MagicScore’s {{developers}} {{began to}} reach out to other music experts and consultants from mostly European countries including Switzerland, Germany, Finland, Ukraine, Russia, and Moldova. Through these efforts they added new functions including cross-staff beaming, <b>MIDI</b> <b>events</b> editors, a new piano roll (playback) editor, and others.|$|R
50|$|In 1906 Delesalle was {{involved}} in developing the Charter of Amiens.In this manifesto the CGT proclaimed that it was independent of all political movements.Delesalle took {{the position that the}} union was a basic part of the worker's life, unlike a political party which people could join or leave at any time as their opinions changed. The workers did not need capitalists or politicians, who served no useful function.Due to a CGT poster after the <b>events</b> in <b>Midi</b> in 1907 he was charged with insulting the army and provoking soldiers to disobedience, but was eventually acquitted.|$|R
40|$|Conventional {{sequence}} software systems, {{which are}} com-monly used to edit MIDI-encoded music, possess {{two kinds of}} problems due to interactions with MIDI data through multiple independent windows. We address the problems by develop-ing a system, called comp-i (Comprehensible MIDI Player-Interactive), which provides music composers and arrangers with a novel type of 3 D interactive virtual space, where the users are allowed to explore global music structures and to edit local features, both are embedded in a time-series of mul-tichannel asynchronous <b>events</b> of <b>MIDI</b> datasets while keep-ing their cognitive maps...|$|R
40|$|Almost {{all work}} on music {{information}} retrieval {{to date has}} concentrated on music in the audio and <b>event</b> (normally <b>MIDI)</b> domains. However, music {{in the form of}} notation, especially Conventional Music Notation (CMN), is of much interest to musically-trained persons, both amateurs and professionals, and searching CMN has great value for digital music libraries. One obvious reason little has been done on music retrieval in CMN form is the overwhelming complexity of CMN, which requires a very substantial investment in programming before one can even begin studying music IR. This paper reports on work adding music-retrieval capabilities to Nightingale®, an existing professional-level music-notation editor. 1...|$|R
40|$|This paper {{presents}} an algorithm for converting <b>midi</b> <b>events</b> into logical voices. The algorithm is fundamentally {{based on the}} pitch proximity principle. New heuristics are introduced and evaluated in order to handle unsolved situations. The algorithm is tested on ground truth data: inventions and fugues by J. S. Bach. Due to its left to right processing it also runs on real time input...|$|R
5000|$|The first Buchla Lightning was {{developed}} in 1991, the second 1996, and the third in 2008. With each succession the Buchla Lightning has improved {{on its ability to}} control these <b>MIDI</b> <b>events.</b> The Buchla Lightning is said to lend itself mostly to percussive musical styles. The Buchla Lightning II was a vast improvement on the first in technical terms (how the wands were made) and diversity (how they can be used).|$|R
50|$|<b>MIDI</b> <b>events</b> can be sequenced with {{computer}} software, or in specialized hardware music workstations. Many digital audio workstations (DAWs) {{are specifically designed}} to work with MIDI as an integral component. MIDI piano rolls have been developed in many DAWs so that the recorded MIDI messages can be extensively modified. These tools allow composers to audition and edit their work much more quickly and efficiently than did older solutions, such as multitrack recording.|$|R
5000|$|This {{mechanism}} is however mainly designed for pre-recorded MIDI streams, {{like the one}} coming from a sequencer track. When RTP-MIDI is used for real-time applications (e.g. controlling devices from a RTP-MIDI compatible keyboard [...] ), deltatime is mostly set to the specific value of 0, {{which means that the}} related <b>MIDI</b> <b>event</b> shall be interpreted as soon as it is received). With such usecase, the latency compensation mechanism described previously can not be used.|$|R
5000|$|Microscope Sequence editing with Event and Global Modes. Step through <b>events</b> {{and change}} <b>MIDI</b> note value, {{velocity}} and duration via the up/down buttons or simply play the desired note on a MIDI keyboard. Spot erase any note. This {{process can be}} done on one or all tracks simultaneously. In Global Mode, you can transpose, erase or scale velocities and duration of notes. You can also specify a range of notes on which these Global edits occur.|$|R
40|$|Abstract. In this paper, we {{describe}} {{how we can}} generate music by simulating moves of artificial ants on a graph where vertices represent notes and edges represent possible transitions between notes. As ants can deposit pheromones on edges, they collectively build a melody which is a sequence of <b>Midi</b> <b>events.</b> Different parameter settings are tested to produce different styles of generated music with several instruments. We also introduce a mechanism {{that takes into account}} music files to initialize the pheromone matrix. ...|$|R
50|$|Parts {{contain a}} {{sequence}} of <b>MIDI</b> <b>events</b> such as notes and control changes. There are three ways to record data to a part: step recording, realtime recording, and TR-Rec. Step recording allows the entry of notes step by step. Realtime recording allows both notes and control changes to be added. TR-Rec allows each of the 16-pads to represent a beat, and {{makes it easy to}} enter percussion tracks. There is also a microscope editing mode that allows detailed event editing.|$|R
40|$|This article {{presents}} FTM, a shared library {{and a set}} of modules extending the Max/MSP environment. It also gives a brief description of additional sets of modules based on FTM. The article particularly addresses the community of researchers and musicians familiar with Max or Max-like programming environments such as Pure Data. FTM extends the signal and message data flow paradigm of Max permitting the representation and processing of complex data structures such as matrices, sequences or dictionaries as well as tuples, <b>MIDI</b> <b>events</b> or score elements (notes, silences, trills etc.). 1...|$|R
40|$|Much of what {{we might}} call "high-art music" {{occupies}} the difficult end of listening for contemporary audiences. Concepts such as pitch, meter and even musical instruments often {{have little to do with}} such music, where all sound is typically considered as possessing musical potential. As a result, such music can be challenging to educationalists, for students have few familiar pointers in discovering and understanding the gestures, relationships and structures in these works. This paper describes on-going projects at the University of Hertfordshire that adopt an approach of mapping interactions within visual spaces onto musical sound. These provide a causal explanation for the patterns and sequences heard, whilst incorporating web interoperability thus enabling potential for distance learning applications. While so far these have mainly driven pitch-based <b>events</b> using <b>MIDI</b> or audio files, it is hoped to extend the ideas using appropriate technology into fully developed composition tools, aiding the teaching of both appreciation/analysis and composition of contemporary music...|$|R
40|$|This paper {{proposes a}} method for the multi-pitch {{estimation}} of polyphonic music signals. Instead of on the frame level, the estimation {{is based on the}} Partial Event, which is defined like the note <b>event</b> in <b>MIDI.</b> All partial <b>events</b> in a piece of music are extracted dynamically in the process of the frame by frame Short Time Fourier Transform (STFT). For each event, Net Support degree received from other events is calculated and the events with the highest support degrees are selected to be the fundamental frequency (F 0) events. From another point of view, the support is transferred from higher frequency par-tial events to lower ones and finally concentrated on the F 0 events. This method can estimate the number of concurrent sounds, the onset and offset times of the notes. Experiments on both randomly mixed chord signals and synthesized en-semble music signals in “wav ” format are conducted and the results are promising. 1...|$|R
40|$|Almost {{all work}} on music {{information}} retrieval {{to date has}} concentrated on music in the audio and <b>event</b> (normally <b>MIDI)</b> domains. However, music {{in the form of}} notation, especially Conventional Music Notation (CMN), is of much interest to musically trained persons, both amateurs and professionals, and searching CMN has great value for digital music libraries. One obvious reason little has been done on music retrieval in CMN form is the overwhelming complexity of CMN, which requires a very substantial investment in programming before one can even begin studying music information retrieval. This paper reports on work adding music retrieval capabilities to Nightingale(R]. Nightingale[R] is a professional-level music notation editor for the Macintosh computer, written in the C language; it has been marketed commercially for a number of years. Nightingale(R], was used as a platform for studying CMN-based music information retrieval by adding several music searching features and commands. The resulting program is called "NightingaleSearch. " (Contains 23 references.) (Author/AEF) Reproductions supplied by EDRS are the best that can be made from the original document...|$|R
5000|$|The General MIDI (GM) {{software}} {{standard was}} devised in 1991 {{to serve as}} a consistent way of describing a set of over 200 sounds (including percussion) available to a PC for playback of musical scores. [...] For the first time, a given MIDI preset consistently produced a specific instrumental sound on any GM-compatible device. The Standard MIDI File (SMF) format (extension [...] ) combined <b>MIDI</b> <b>events</b> with delta times - a form of time-stamping - and became a popular standard for exchanging music scores between computers. In the case of SMF playback using integrated synthesizers (as in computers and cell phones), the hardware component of the MIDI interface design is often unneeded.|$|R
5000|$|The {{cathode ray}} tube made {{possible}} the oscilloscope, an early electronic device that can produce images that are easily associated with sounds from microphones. The modern Laser lighting display displays wave patterns produced by similar circuitry. The imagery used to represent audio in digital audio workstations is largely based on familiar oscilloscope patterns. The Animusic company (originally called 'Visual Music') has repeatedly demonstrated {{the use of computers}} to convert music [...] - [...] principally pop-rock based and composed as <b>MIDI</b> <b>events</b> [...] - [...] to animations. Graphic artist-designed virtual instruments which either play themselves or are played by virtual objects are all, along with the sounds, controlled by MIDI instructions.|$|R
5000|$|Moreover, RTP-MIDI as {{described}} in RFC 6295 contains a latency compensation mechanism (a similar mechanism is found in most plugins, which can inform {{the host of the}} latency they add on the processing path. The host can then send samples to the plugin in advance, so the samples are ready and sent synchronously with other audio streams). The compensation mechanism described in RF6295 uses a relative timestamp system, based on the MIDI deltatime ({{as described}} in [...] ) Each <b>MIDI</b> <b>event</b> transported in the RTP payload has a leading deltatime value, related to the current payload time origin (defined by the Timestamp field in RTP header).|$|R
2500|$|The General MIDI (GM) {{software}} {{standard was}} devised in 1991 {{to serve as}} a consistent way of describing a set of over 200 sounds (including percussion) available to a PC for playback of musical scores. [...] For the first time, a given MIDI preset consistently produced a specific instrumental sound on any GM-compatible device. The Standard MIDI File (SMF) format (extension [...]mid) combined <b>MIDI</b> <b>events</b> with delta times – a form of time-stamping – and became a popular standard for exchanging music scores between computers. [...] In the case of SMF playback using integrated synthesizers (as in computers and cell phones), the hardware component of the MIDI interface design is often unneeded.|$|R
40|$|This {{paper is}} about Q-Midi, an {{interface}} for developing MIDI {{applications in the}} Q programming language. Q is a modern functional language based on term rewriting; this means that a Q program is just a collection of equations which are used as rewriting rules to simplify expressions. Q-Midi represents <b>MIDI</b> <b>events</b> as symbolic data which {{makes it easy to}} formulate functional programs to manipulate and process MIDI sequences on a high level of abstraction. Realtime programming and the manipulation of MIDI files are also supported. Q-Midi is based on Grame’s MidiShare, a C library for portable MIDI programming. The interface works on Linux, Mac OS X and Windows systems. Therefore Q-Midi provides an interesting tool for developing portable computer music applications in a high-level functional programming language. ...|$|R
40|$|The {{study of}} music is highly interdisciplinary, and thus {{requires}} the combination of datasets from multiple musical domains, such as catalog metadata (authors, song titles, dates), industrial records (labels, producers, sales), and music notation (scores). While today an abundance of music metadata exists on the Linked Open Data cloud, datasets containing interoperable symbolic descriptions of music itself, i. e. music notation with note and instrument level information, are scarce. This is the MIDI Linked Data Cloud, a dataset that represents multiple collections of digital music in the MIDI standard format as Linked Data. At the time of writing, the dataset comprises 10, 215, 557, 355 triples of 308, 443 interconnected MIDI scores, and provides Web-compatible descriptions of their <b>MIDI</b> <b>events...</b>|$|R
