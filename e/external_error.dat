54|170|Public
40|$|It was {{observed}} that the vacuum magnetic island produced by an <b>external</b> <b>error</b> magnetic field in the large helical device shrank {{in the presence of}} plasma. This was evidenced by the disappearance of flat regions in the electron temperature profile obtained by Thomson scattering. This island behavior depended on the magnetic configuration in which the plasmas were produced...|$|E
40|$|FIGURE 4. A. Box plots {{from the}} {{distribution}} and variance of surface cortex (n = 10) sclerites of the examined specimens. The median line is inside the 25 th and 75 th percentiles with <b>external</b> <b>error</b> bars at the 10 th and 90 th percentiles. B, Scatter plot of the surface cortex sclerites length vs width (different symbols indicate the different specimens) ...|$|E
30|$|To further {{understand}} of {{how well}} these methods behave {{in the presence of}} non-Lambertian effects, we tested their performance on Sphere with varying degrees of specularity and on Venus, where {{a large portion of the}} scene is concave, and as such, is heavily polluted by cast shadow. To find out the robustness of these methods against <b>external</b> <b>error</b> introduced by the experimental setup, we also tested the methods with additive image noise and light calibration error.|$|E
40|$|When {{designing}} a software module or system, a software engineer needs to consider and differentiate between {{how the system}} handles <b>external</b> and internal <b>errors.</b> <b>External</b> <b>errors</b> must be tolerated by the system, while internal errors should be discovered and eliminated. This paper presents a development strategy based on design contracts to minimize the amount of internal errors in a software system while accommodating <b>external</b> <b>errors.</b> A distinction is made between weak and strong contracts that corresponds to the distinction between <b>external</b> and internal <b>errors.</b> According to the strategy, strong contracts should be applied initially to promote the correctness of the system. Before release, the contracts governing external interfaces should be weakened and <b>error</b> management of <b>external</b> <b>errors</b> enabled. This transformation of a strong contract to a weak one is harmless to client modules. In addition to presenting the strategy, the paper also presents {{a case study of}} an industrial project where this strategy was successfully applied. ...|$|R
40|$|Some {{selected}} catalogs of {{the effective}} temperatures for F, G and K stars are analyzed. By an improved technique we estimate the <b>external</b> <b>errors</b> of these catalogs from data intercomparisons. The {{values of the}} effective temperatures are then averaged with the appropriate weights to produce a mean homogeneous catalog based on the selected data. This catalog, containing 800 stars, is compared with some other independent catalogs for estimating their <b>external</b> <b>errors.</b> The data {{may be used as}} a source of reliable homogeneous values of effective temperatures, together with their errors. Comment: 16 pages, 10 figure...|$|R
40|$|In {{the present}} research, human errors existed in tasks of {{operators}} working in two control rooms (Northern and Southern control rooms) in the Tehran oil refinery are identified and evaluated. Then, corrective strategies and actions {{are advised to}} decrease errors. At first, using hierarchical task analysis (HTA) method, four positions including shift controller, head operators, control room’s operators, and outside operators are analyzed. Afterwards, human errors in the considered positions are identified and assessed using technique for retrospective and predictive analysis of cognitive errors (TRACEr). Results present 670 internal <b>errors</b> and 738 <b>external</b> <b>errors</b> in the Northern sector while 661 internal <b>errors</b> and 744 <b>external</b> <b>errors</b> in the Southern sector. In two sectors, this number of errors was identified for 27 major tasks and 108 minor tasks. Action errors are the most repeated errors among the internal errors while violation errors are the least. In addition, most of <b>external</b> <b>errors</b> are related to communication errors while the least errors are related to time and sequence errors. According to results of applying psychological error mechanism (PEM) in th...|$|R
40|$|Radiocarbon (C- 14) dating {{remains the}} {{predominant}} method to build robust chronologies from peat deposits for paleoclimate reconstruction. Although {{it is widely}} known that the C- 14 content of different chemical fractions in peat varies, this heterogeneity is not fully accounted for while constructing age depth models. Since peat is a complex and heterogeneous matrix, we tried to characterize this uncertainty on our C- 14 dates by experimenting with pre-treatment procedures on peat sampled from a high elevation site in southern India, where reliable and continuous records of paleoenvironments are scarce. Dated to similar to 40 kyr, the Sandynallah peat accumulation in valleys > 2000 m asl in the Nilgiris, Western Ghats, remains {{an important source of}} paleoenvironmental information. We subsampled 2 peat cores (labelled Cores 1 and 2) from this site at 1 cm and 2 cm resolution, respectively, and obtained C- 14 dates using Accelerator Mass Spectrometry (AMS) for 73 Core 1 and 40 Core 2 samples. The results indicate that the uncertainty (possibly due to sample heterogeneity; henceforth called <b>external</b> <b>error)</b> for each date is at least tenfold the internal error (reported from the AMS). When this <b>external</b> <b>error</b> estimate was included as an added variance to the internal error on the radiocarbon dates, the numerous minor date reversals on deposits up to about 30 kyr were better explained for by the age-depth model than when only the internal error was used. The remaining large date reversals on deposits older than about 30 kyr are consistent with previous studies from the Sandynallah basin and, hence, could correspond to large deposit level changes/fluctuations. Based on these results we argue that using internal error as the total uncertainty associated with a date given by AMS is insufficient, resulting in models of high precision over accuracy. The internal error should be used in conjunction with a reliable estimate of <b>external</b> <b>error</b> in an age-depth model for more realistic dating of paleoclimatic events. (C) 2016 Elsevier B. V. All rights reserved...|$|E
40|$|Photoelectric radial-velocity measurements, {{obtained}} with <b>external</b> <b>error</b> {{as small as}} 0. 1 km/s using the 200 -inch Hale telescope at Palomar Observatory during the period 1971 - 1986, are reported for over 400 candidate members (with V magnitude between 6 and 14) of the Hyades cluster. The history of Hyades observations is recalled; the Palomar instrumentation and observing program are described; the data-reduction and standardization procedures are discussed in detail; and the data are presented in extensive tables and graphs. About 200 of the stars are classified as cluster members, including 60 spectroscopic binaries...|$|E
40|$|With a {{multi-point}} (200) repetitive (50 - 200 Hz) Thomson scattering system, {{we studied}} {{the shape and}} evolution of the electron temperature (T_e) Profiles of the plasma confined in LHD. We first survey various shapes of the observed T_e Profiles and then describe two notable findings in some detail: (1) A pedestal often appeared on T_e Profiles around the iota/ 2 pi = 1 surface, but its correlation with confinement was weak; (2) A magnetic island generated by an <b>external</b> <b>error</b> field changed its size in plasma. Normally the island shrank in plasma, but grew upon a hydrogen pellet injection...|$|E
40|$|UBVRI {{photoelectric}} photometry {{is presented}} for 269 late spectral type, high proper motion stars {{belonging to the}} 'Lowell Proper Motion Survey' and included in the present version of the Hipparcos Input Catalogue. The observations and data reduction are described. The <b>external</b> <b>errors</b> obtained by comparison of the results with those obtained in other studies are presented...|$|R
40|$|Abstract. A good {{knowledge}} of both {{accuracy and precision}} of the Hipparcos parallaxes {{is one of the}} keys for their future scientific use. For this purpose, the Hipparcos preliminary parallaxes, as obtained after the processing of the first 30 months of Hipparcos data, are compared to various ground-based parallax estimates, using astrometric, photometric and spectroscopic data. In order to find unbiased values of the global zero-point and of <b>external</b> <b>errors,</b> a new maximum-likelihood algorithm has been built, taking into account the censorships of the observed data. Applying the method to a sample of distant stars, it is shown that the global zero-point error of the Hipparcos preliminary parallaxes should be smaller than 0. 1 mas and that the <b>external</b> <b>errors</b> are unlikely to be underestimated by more than about 5 %. Key words: Hipparcos – stars: distances – astrometry – methods: data analysis 1...|$|R
40|$|Errors pose {{a serious}} threat to the output {{validity}} of modern data processing, which is often performed by computer programs. In scientific computation, data are collected through instruments or sensors that may be exposed to rough environmental conditions, leading to errors. Furthermore, during the computation process data may not be precisely represented due to the limited precision of the underlying machine, leading to representation errors. Computational processing of these data may hence produce unreliable output results or even faulty conclusions. We call them reliability problems. ^ We consider the reliability problems that are caused by two kinds of errors. The first kind of errors includes input and parameter errors, which originate from the external physical environment. We call these <b>external</b> <b>errors.</b> The other kind of errors is due to the limited representation of floating-point values. They occur when values cannot be precisely represented by machines. We call them internal representation errors, or internal errors. They are usually at a much smaller scale compared to <b>external</b> <b>errors.</b> Nonetheless, such tiny errors may still lead to unreliable results and serious problems. ^ In this dissertation, we develop program analysis techniques to enable reliable data processing. For <b>external</b> <b>errors,</b> we propose techniques to improve the sampling efficiency of Monte Carlo methods, namely execution coalescing and white-box sampling. For internal errors, we develop efficient monitoring techniques to detect instability problems at runtime in floating point program executions. ...|$|R
40|$|Estimates of {{effective}} parameters for unsaturated flow models are typically based on observations taken on length scales {{smaller than the}} modeling scale. This complicates parameter estimation for heterogeneous soil structures. In this paper we attempt to account for soil structure not present in the flow model by using so-called <b>external</b> <b>error</b> models, which correct for bias in the likelihood function of a parameter estimation algorithm. The performance of <b>external</b> <b>error</b> models are investigated using data from three virtual reality experiments and one real world experiment. All experiments are multistep outflow and inflow experiments in columns packed with two sand types with different structures. First, effective parameters for equivalent homogeneous models for the different columns were estimated using soil moisture measurements taken at a few locations. This resulted in parameters that had a low predictive power for the averaged states of the soil moisture if the measurements did not adequately capture a representative elementary volume of the heterogeneous soil column. Second, parameter estimation was performed using error models that attempted to correct for bias introduced by soil structure not {{taken into account in}} the first estimation. Three different error models that required different amounts of prior knowledge about the heterogeneous structure were considered. The results showed that the introduction of an error model can help to obtain effective parameters with more predictive power with respect to the average soil water content in the system. This was especially true when the dynamic behavior of the flow process was analyzed...|$|E
40|$|Abstract. A {{linearity}} test shows H 0 {{to decrease}} by 7 % out to 18000 km s − 1. The value at 10000 km s − 1 {{is a good}} approximation to the mean value of H 0 over very large scales. The construction of the extragalactic distance scale is discussed. Field galaxies, cluster distances relative to Virgo, and blue supernovae of type Ia yield H 0 (cosmic) with increasing weight; they give consistently H 0 = 57 ± 7 (<b>external</b> <b>error).</b> This value is supported by purely physical distance determinations (SZ effect, gravitational lenses, MWB fluctuations). Arguments for H 0 > 70 are discussed and shown to be flawed. 1...|$|E
40|$|Well-established {{procedures}} are consolidated {{to determine the}} associated measurement uncertainty for a given antenna and measurements scenario [1 - 2]. Similar criteria for establishing uncertainties in numerical modelling of the same antenna are still to be established. In this paper, we investigate the achievable agreement between antenna measurement and simulation when <b>external</b> <b>error</b> sources are minimized. The test object, is a reflector fed by a wideband dual ridge horn (SR 40 -A and SH 4000). The highly stable reference antenna has been selected to minimize uncertainty related to finite manufacturing and material parameter accuracy. Two frequencies, 10. 7 GHz and 18 GHz have been selected for detailed investigation...|$|E
40|$|<b>External</b> <b>errors</b> of {{effective}} temperatures of stars for selected libraries are estimated from data intercomparisons. It is {{found that the}} obtained errors are mainly in a good correspondence with the published data. The results {{may be used to}} homogenize the effective temperatures by averaging the data (with the weights inversely proportional to the squared errors) from independent sources. Comment: 4 pages, 4 figure...|$|R
30|$|A {{real-time}} system requires its {{hardware and}} software to perform the assigned work within the specified time while the system has the ability of detecting and reacting to internal and <b>external</b> <b>errors</b> in a controlled fashion [1]. However, in many applications of the hyperspectral image real-time processing, the processing time is often too long. There are three {{ways to improve the}} hyperspectral image processing speed.|$|R
40|$|FOCUS, a {{simulation}} environment for conducting fault-sensitivity analysis of chip-level designs, is described. The environment {{can be used}} to evaluate alternative design tactics at an early design stage. A range of user specified faults is automatically injected at runtime, and their propagation to the chip I/O pins is measured through the gate and higher levels. A number of techniques for fault-sensitivity analysis are proposed and implemented in the FOCUS environment. These include transient impact assessment on latch, pin and functional <b>errors,</b> <b>external</b> pin <b>error</b> distribution due to in-chip transients, charge-level sensitivity analysis, and error propagation models to depict the dynamic behavior of latch errors. A case study of the impact of transient faults on a microprocessor-based jet-engine controller is used to identify the critical fault propagation paths, the module most sensitive to fault propagation, and the module with the highest potential for causing <b>external</b> <b>errors...</b>|$|R
40|$|International audienceSeveral {{studies of}} {{procedural}} learning in Parkinson's disease (PD) {{have demonstrated that}} these patients are impaired with respect to age-matched control subjects. In order to examine more closely the specific impairment, we considered three dimensions along which a procedural learning task could vary. These are: (1) implicit vs explicit learning, (2) instance vs rule learning, and (3) learning with internal vs <b>external</b> <b>error</b> correction. We consider two hypotheses that could explain the impairments observed in PD for different types of explicit motor learning: (H 1) an impairment related to the acquisition of rules vs specific instances, and (H 2) an impairment in learning when no explicit error feedback is provided. In order to examine the condition of rule learning with <b>external</b> <b>error</b> feedback, we developed {{a modified version of}} the serial reaction time (SRT) protocol that tests analogical transfer in sequence learning (ATSL). Reaction times are measured for responses to visual stimuli that appear in several different repeating sequences. While these isomorphic sequences are different, they share a common rule. Verbatim learning of a sequence would result in negative transfer from one sequence to a different one, while rule learning would result in positive transfer. Parkinson's patients and age-matched controls demonstrate significant acquisition and positive transfer of the rule between sequences. Our results demonstrate that PD patients are capable of learning and transferring rule or schema-based representations in an explicit learning format, and that this form of learning may be functionally distinct from learning mechanisms that rely on representations of the verbatim or statistical structure of sequences...|$|E
30|$|The entire {{statistical}} analyses {{were carried out}} by means of SPSS 22 (IBM) software. The conducted tests were duplicate (n =  2). In the enumeration of the bacteria in each replication, plates with the colonies were counted and their internal concentration mean was used to prevent any internal error. Finally, the mean of two replications was calculated to remove <b>external</b> <b>error.</b> The number of the bacteria was reported {{in terms of the}} number of the colonies per 1  ml. After confirmation of normality of data by Kolmogrov-Smirnov test further analysis were carried out using Repeated Measures ANOVA test and P <  0.05 regarded to be significant. The graph (Fig.  5) has been constructed using GraphPad Prism version 6 software.|$|E
40|$|Abstract—In {{this paper}} we {{investigate}} {{the possibility of}} using measurement-based propagation models in the framework of Dynamic Spectrum Access (DSA), where secondary users can op-portunistically access the licensed spectrum of a primary system. In principle this will provide a precise representation of the radio environment since it takes into account all environmental details at a given period of time. However the accuracy of the estimated models will depend on sensor density, propagation factors, and any <b>external</b> <b>error.</b> We study the impact of these factors on the accuracy of the estimated model and the satisfaction of primary and secondary networks. We also propose a guideline to overcome these problems in DSA scenarios by using a power margin. I...|$|E
40|$|From data {{recorded}} by DELPHI between 1991 and 1994, which correspond to 3. 2 million hadronic Z^ 0 decays, a {{measurement of the}} meson lifetime has been performed based on the inclusive reconstruction of 3520 ± 150 semileptonic decays of the type → X ℓ^- ν_ℓ, The result is: τ() = 1. 537 ± 0. 041 (stat.) ± 0. 039 (syst.) ps. The contribution to the systematic uncertainty which depends on <b>external</b> <b>errors</b> is ± 0. 014 ps...|$|R
40|$|Galaxy {{velocity}} data {{taken with}} the Steward Observatory multiple aperture fiber optic spectrograph are presented for four Abell clusters. The root-mean-square <b>external</b> <b>errors</b> in these velocities are about 100 km/s; accuracy which compares favorably with that obtained from single-object observations. It is expected that the recent adoption of a CCD detector should decrease <b>external</b> <b>errors</b> to about 50 km/s. All four of the clusters observed are known X-ray sources and the present data agree well with empirically derived velocity dispersion-X-ray luminosity relations for clusters of galaxies. Abell 400 is interesting in this regard, since both its X-ray luminosity and its velocity dispersion are quite small. Such objects are particularly important in determining {{the slope of the}} velocity dispersion-X-ray luminosity relation. The large microwave decrement observed in A 576 was initially interpreted as due to Compton scattering of the microwave background by the X-ray-emitting intracluster gas. White and Silk have presented Einstein X-ray data which indicate that A 576 contains too little gas to produce the observed microwave decrement by Compton scattering. The velocity dispersion obtained here for 47 members of this cluster strengthens their conclusion...|$|R
40|$|It {{has been}} shown that frontal {{cortical}} areas increase their activity during error perception and error processing. However, it is not yet clear whether perception of motor errors is processed in the same frontal areas as perception of errors in cognitive tasks. It is also unclear whether brain activity level is influenced by the magnitude of error. For this purpose, we conducted a study in which subjects were confronted with motor and non-motor errors, and had them perform a sensorimotor transformation task in which they were likely to commit motor errors of different magnitudes (internal errors). In addition to the internally committed motor <b>errors,</b> non-motor <b>errors</b> (<b>external</b> <b>errors)</b> were added to the feedback in some trials. We found that activity in the anterior insula, inferior frontal gyrus (IFG), cerebellum, precuneus, and posterior medial frontal cortex (pMFC) correlated positively with the magnitude of <b>external</b> <b>errors.</b> The middle frontal gyrus (MFG) and the pMFC cortex correlated positively with the magnitude of the total error fed back to subjects (internal plus external). No significant positive correlation between internal error and brain activity could be detected. These results indicate that motor errors have a differential effect on brain activity compared with non-motor errors...|$|R
40|$|This paper {{reports on}} {{systematic}} height offsets {{that have been}} observed in TanDEM-X by evaluating {{a large number of}} digital elevation model (DEM) acquisitions. Besides the expected instrument and baseline offsets, which are compensated in the calibration chain, two unexpected <b>external</b> <b>error</b> sources have been identified that apply to any formation flying bistatic SAR interferometer. The first contribution is due to relativistic effects and can be well explained within Einstein’s special theory of relativity. The second effect is due to differential delays in the troposphere. It is shown that the theoretic predictions are in good agreement with the observed offsets. All necessary corrections have in the meantime been integrated in the operational TanDEM-X processing chain...|$|E
40|$|Background: The {{adoption}} of laparoscopic pyloromyotomy (LPM) by pediatric surgeons {{has been limited}} due to concerns about long execution times and higher-than-expected morbidity. The aim {{of the present study}} was to examine the performance of LPM by pediatric surgeons during the initial stages of their experience. Methods: Complete videotapes of 50 early LPM performed in one hospital were subjected to Observational Clinical Human Reliability Analysis (OCHRA) by an independent team. Results: This series had a total morbidity of 6 % (one intraoperative bleed, one gastric perforation, one incomplete pyloromyotomy). Using OCHRA, we identified 77 consequential and 233 inconsequential errors (mean of 6 +/- 5. 4 per operation, 16. 7 % total error probability) during an average operative time of 29. 8 min. Eighty percent of the errors were of the execution type. A high probability of error was observed with the use of the following key instruments: holding graspers (68 %), retractable blade (79 %), and splitting forceps (77 %). The OCHRA system confirmed that task III was the hazard zone for LPM. Excessive force (task III) resulted in gastric perforation and bleeding from the pyloric mass. Movement in the wrong direction and misorientation in tissue planes were the <b>external</b> <b>error</b> modes underlying misaligned cuts of the pyloric mass and poor tissue splitting (task zones II and III). Conclusions: This early series of LPM was associated with an appreciable execution error rate, largely due to the poor functionality of the specific instruments used for the procedure. Human factors identified by the <b>external</b> <b>error</b> modes played a subsidiary but important role, underscoring the importance of skills training and experience (proficiency-gain curve) ...|$|E
40|$|Abstract. The firing {{calibration}} of Artillery {{fire control}} system with closed-loop calibration manner {{can improve the}} firing accuracy significantly. The miss distance prediction is the theory core of closed-loop calibration. Whether the prediction algorithm of closed-loop calibration corrects or not {{is directly related to}} the calibration’s success or failure. Mainly for the <b>external</b> <b>error</b> sources not caused by fire control computer system, this paper established a closed-loop calibration model based on Kalman filtering algorithm, researched the specific implementation process of algorithm model, and applied the MATLAB software simulating the model. Simulation results show that the closed-loop calibration model based on Kalman filtering algorithm can significantly inhibit firing error and is a feasible algorithm and provides a good theoretical basis for forecasting error...|$|E
40|$|From data {{recorded}} by DELPHI between 1991 and 1994, which corresponds to 3. 2 million hadronic Z 0 decays, a {{measurement of the}} B 0 d meson lifetime, based on the inclusive reconstruction of 3520 150 semileptonic decays of the type B 0 d ! D+ X ` `; has been performed. The result is: (B 0 d) = 1 : 532 0 : 041 (stat:) 0 : 040 (syst:) ps. The contribution to the systematic uncertainty which depends on <b>external</b> <b>errors</b> is 0 : 015...|$|R
40|$|ABSTRACT. In this paper, a {{new method}} of {{investigation}} of the external radio source position catalogs RSPCs stochastic errors is presented. Using this method the stochastic errors of nine recently published RSPCs were evaluated. It {{has been shown that}} the result can be affected by the systematic differences between catalogs if the latter are not accounted for. It was also found that the formal uncertainties of the source position in the RSPCs correlate with the <b>external</b> <b>errors.</b> We also investigated several topics related to the formal uncertainties and systematic errors of RSPC. 1...|$|R
40|$|Intercalibration {{between the}} astronomical and radio-isotopic dating methods {{provides}} a means to improving accuracy and reducing uncertainty of an integrated, multi-chronometer geologic timescale. Here we report a high-precision 40 Ar/ 39 Ar age for the FishCanyon sanidine (FCs) neutron fluence monitor, by multi-collector noble gas mass spectrometry, through cross-calibration with A 1 tephra sanidines (A 1 Ts) of the direct astronomically tuned Faneromeni section (Crete). The astronomically intercalibrated 40 Ar/ 39 Ar age of FCs of 28. 172 ± 0. 028 Ma (2 σ, <b>external</b> <b>errors)</b> is within the uncertainty of, but more precise (± 0. 10 %) than, the previous 40 Ar/ 39 Ar age determined by intercalibration with astronomically tuned tephras from the Melilla Basin (Morocco). Using this proposed age for FCs, combined with measurements using the A 1 Ts as the neutron fluence monitor, a weighted mean Bishop Tuff 40 Ar/ 39 Ar sanidine age of 0. 7674 ± 0. 0022 Ma (2 σ, <b>external</b> <b>errors)</b> is indistinguishable from the ID-TIMS U/Pb zircon age (0. 7671 ± 0. 0019 Ma). The consistency between the astronomicallycalibrated 40 Ar/ 39 Ar sanidine age and U/Pb zircon age for this Quaternary unit suggests that agreement between these two radio-isotopic dating techniques is now achievable at better than ± 0. 3 % (2 σ) in the youngest part of geologic time (< 1 Ma) ...|$|R
40|$|The {{impact of}} {{potential}} error sources on geocoded products {{has been investigated}} {{with respect to the}} high resolution capabilities of the TerraSAR-X sensor. Datum shift parameters, maps, digital terrain and surface models have been identified as <b>external</b> <b>error</b> sources. The accuracy of the geocoded products depends heavily on the quality and availability of this information, which underlies regional variations. Error sources closely related to the sensor are its position, sampling window start time and Doppler centroid frequency. Another error source is given by atmospheric refraction. Ionospheric and atmospheric path delays have a considerable impact. Appropriate modeling can mitigate this effect. Further, high requirements on radiometric accuracy ask for an improved antenna gain pattern correction, which depends on the actual elevation angle and the terrain height. ...|$|E
40|$|A {{linearity}} test shows H_ 0 {{to decrease}} by 7 % out to 18000 km/s. The value at 10000 km/s {{is a good}} approximation to the mean value of Ho over very large scales. The construction of the extragalactic distance scale is discussed. Field galaxies, cluster distances relative to Virgo, and blue supernovae of type Ia yield Ho(cosmic) with increasing weight; they give consistently H_ 0 = 57 +/- 7 (<b>external</b> <b>error).</b> This value is supported by purely physical distance determinations (SZ effect, gravitational lenses, MWB fluctuations). Arguments for H_ 0 > 70 are discussed and shown to be flawed. Comment: 17 pages, LaTeX (crckapb. sty + psfig. sty), 4 figures. To appear in the Proceedings of the IAU Symposium 183 "COSMOLOGICAL PARAMETERS AND EVOLUTION OF THE UNIVERSE", August 18 - 22 1997, Kyoto Japa...|$|E
40|$|A basic {{challenge}} in decision-making {{is to know}} how long to search for information, and how to adapt search processes as performance, goals, {{and the nature of}} the task environment vary. We consider human performance on two experiments involving a sequence of simple multiple-cue decision-making trials, which allow search to be measured, and provide feedback on decision accuracy. In both experiments, the nature of the trials changes, unannounced, several times. Initially minimal search is required, then more extensive search is required, and finally only minimal search is again required to achieve decision accuracy. We find that people, considered both on aggregate, and as individuals, are sensitive to all of these changes. We discuss the theoretical implications of these findings for modeling search and decision-making, and emphasize that they show adaptation to an <b>external</b> <b>error</b> signal must be accompanied by some sort of internal self-regulation in any satisfactory account of people’s behavior...|$|E
40|$|In this paper, a {{new method}} of {{investigation}} of the external radio source position catalogs RSPCs stochastic errors is presented. Using this method the stochastic errors of nine recently published RSPCs were evaluated. It {{has been shown that}} the result can be affected by the systematic differences between catalogs if the latter are not accounted for. It was also found that the formal uncertainties of the source position in the RSPCs correlate with the <b>external</b> <b>errors.</b> We also investigated several topics related to the formal uncertainties and systematic errors of RSPC. Comment: Presented at the Journees 2013 "Systemes de reference spatio-temporels", Paris, France, 16 - 18 September 201...|$|R
40|$|Recent {{advances}} in associative memory design through structured pattern sets and graph-based inference algorithms allow reliable learning and recall of exponential numbers of patterns. Though these designs correct <b>external</b> <b>errors</b> in recall, they assume neurons compute noiselessly, {{in contrast to}} highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as internal noise {{is less than a}} specified threshold, error probability in the recall phase can be made exceedingly small. More surprisingly, we show internal noise actually improves performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional benefit to noisy neurons in biological neuronal networks. ...|$|R
5000|$|Threats are <b>external</b> factors or <b>errors</b> {{that are}} outside the {{influence}} of flight crews. These increase the complexity of normal operations because they can occur unexpectedly; pilots and flight crews {{may not be able}} to plan and fully investigate the cause of a threat if it occurs suddenly during the flight. Examples of threats are weather, terrain, aircraft malfunctions, and other <b>external</b> <b>errors</b> related to a personnel in charge of flight operations other than cockpit members. Non-standard care, decision strategy errors, procedural errors and protocol deviations are also examples of external factors. A LOSA expert, who is seated on the jump seat, must record such threats. A study on crew performance using the TEM approach, discovered that a captain who had less than 6 hours of sleep the day before a regular flight schedule carried out poorer threat management. First Officers experienced frustration and crews experienced Heightened Emotional Activity (HEA) as a result of restricted sleep. An understanding of HEA as a major part of the TEM approach is important during normal flight operations.|$|R
