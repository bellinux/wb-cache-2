32|2|Public
40|$|International audienceIn this paper, {{we explore}} various sparse regularization {{techniques}} for analyzing fMRI data, such as LASSO, elastic net and the recently introduced <b>k-support</b> norm. Employing sparsity regularization {{allow us to}} handle the curse of dimensionality, a problem commonly found in fMRI analysis. We test these methods on real data of both healthy subjects as well as cocaine addicted ones and we show that although LASSO has good prediction, it lacks interpretability since the resulting model is too sparse, and results are highly sensitive to the regularization parameter. We find that we can improve prediction performance over the LASSO using elastic net or the <b>k-support</b> norm, which is a convex relaxation to sparsity with an L 2 penalty that is tighter than the elastic net. Elastic net and <b>k-support</b> norm overcome the problem of overly sparse solutions, resulting in both good prediction and interpretable solutions, while the <b>k-support</b> norm gave better prediction performance. Our experimental results support the general applicability of the <b>k-support</b> norm in fMRI analysis, both for prediction performance and interpretability...|$|E
40|$|In this paper, {{we explore}} various sparse regularization {{techniques}} for analyzing fMRI data, such as LASSO, elastic net and the recently introduced <b>k-support</b> norm. Employing sparsity regularization {{allow us to}} handle the curse of dimensionality, a problem commonly found in fMRI analysis. We test these methods on real data of both healthy subjects as well as cocaine addicted ones and we show that although LASSO has good prediction, it lacks interpretability since the resulting model is too sparse, and results are highly sensitive to the regularization parameter. We find that we can improve prediction performance over the LASSO using elastic net or the <b>k-support</b> norm, which is a convex relaxation to sparsity with an ℓ 2 penalty that is tighter than the elastic net. Elastic net and <b>k-support</b> norm overcome the problem of overly sparse solutions, resulting in both good prediction and interpretable solutions, while the <b>k-support</b> norm gave better prediction performance. Our experimental results support the general applicability of the <b>k-support</b> norm in fMRI analysis, both for prediction performance and interpretability. Index Terms — Functional magnetic resonance imaging (fMRI), variable selection, sparsity regularization 1...|$|E
40|$|International audienceSparsity regularization allows {{handling}} {{the curse of}} dimensionality, a problem commonly found in fMRI data. In this paper, we compare LASSO (L 1 regularization) and the recently introduced <b>k-support</b> norm {{on their ability to}} predict real valued variables from brain fMRI data for cocaine addiction, in a principled model selection setting. Furthermore, in the context of those two regularization methods, we compare two loss functions: squared loss and absolute loss. With the squared loss function, <b>k-support</b> norm outperforms LASSO in predicting real valued behavioral variables measured in an inhibitory control task given fMRI data from a different task, designed to capture emotionally-salient reward. The absolute loss function leads to significantly better predictive performance for both methods in almost all cases and the <b>k-support</b> norm leads to more interpretable and more stable solutions often by an order of magnitude. Our results support the use of the <b>k-support</b> norm for fMRI analysis and the generalizability of the I-RISA model of cocaine addiction...|$|E
40|$|The {{search for}} {{interesting}} association rules {{is an important}} topic in knowledge discovery in spatial gene expression databases. The set of admissible rules for the selected support and confidence thresholds can easily be extracted by algorithms based on support and confidence, such as Apriori. However, they may produce {{a large number of}} rules, many of them are uninteresting. The challenge in association rule mining (ARM) essentially becomes one of determining which rules are the most interesting. Association rule interestingness measures are used to help select and rank association rule patterns. Besides support and confidence, there are other interestingness measures, which include generality reliability, peculiarity, novelty, surprisingness, utility, and applicability. In this paper, the application of the interesting measures entropy and variance for association pattern discovery from spatial gene expression data has been studied. In this study the fast mining algorithm has been used which produce candidate itemsets and it spends less time for calculating <b>k-supports</b> of the itemsets with the Boolean matrix pruned, and it scans the database only once and needs less memory space. Experimental results show that using entropy as the measure of interest for the spatial gene expression data has more diverse and interesting rules...|$|R
40|$|The {{catalytic}} conversion of glycerol was performed with iron oxide-based catalysts for production of allyl alcohol using a fixed-bed flow reactor at 623 K under atmospheric pressure. The glycerol dehydration proceeds on acid sites of catalysts while the allyl alcohol production {{is assumed to}} be catalyzed by non-acidic sites of catalysts through a hydrogen transfer mechanism. Different alkali metals, including Na, K, Rb, and Cs were supported on ZrO 2 -FeOx and all of them gave impressively higher allyl alcohol yield and suppressed glycerol dehydration due to the reduced catalyst acidic property. <b>K-supported</b> ZrO 2 -FeOx (K/ZrO 2 -FeOx) was chosen for further studies, and allyl alcohol yield remarkably increased up to 27 mol% C at the K content of 3 - 5 mol%. Since no external hydrogen gas is supplied to the system, the hydrogen transfer mechanism should take place between the reaction of glycerol and either hydrogen atoms derived from formic acid forming during the reaction, or active hydrogen species produced from the decomposition of H 2 O by ZrO 2. Addition of Al 2 O 3 to K/ZrO 2 -FeOx (K/Al 2 O 3 -ZrO 2 -FeOx) was examined in order to improve structure stability during the glycerol conversion. Al 2 O 3 addition to the catalyst was effective to achieve higher structure stability, leading to high glycerol conversion with stable allyl alcohol yield of above 25 mol% C. Moreover, K/Al 2 O 3 -ZrO 2 -FeOx can be applicable to the conversion of crude glycerol which is the waste solution obtained from biodiesel production. (C) 2013 Elsevier B. V. All rights reserved...|$|R
40|$|The <b>k-support</b> norm is a regularizer {{which has}} been {{successfully}} applied to sparse vector prediction problems. We show that it belongs to a general class of norms which can be formulated as a parameterized infimum over quadratics. We further extend the <b>k-support</b> norm to matrices, and we observe {{that it is a}} special case of the matrix cluster norm. Using this formulation we derive an efficient algorithm to compute the proximity operator of both norms. This improves upon the standard algorithm for the <b>k-support</b> norm and allows us to apply proximal gradient methods to the cluster norm. We also describe how to solve regularization problems which employ centered versions of these norms. Finally, we apply the matrix regularizers to different matrix completion and multitask learning datasets. Our results indicate that the spectral <b>k-support</b> norm and the cluster norm give state of the art performance on these problems, significantly outperforming trace norm and elastic net penalties...|$|E
40|$|We study a regularizer {{which is}} defined as a {{parameterized}} infimum of quadratics, and which we call the box-norm. We show that the <b>k-support</b> norm, a regularizer proposed by [Argyriou et al, 2012] for sparse vector prediction problems, belongs to this family, and the box-norm can be generated as a perturbation of the former. We derive an improved algorithm to compute the proximity operator of the squared box-norm, and we provide a method to compute the norm. We extend the norms to matrices, introducing the spectral <b>k-support</b> norm and spectral box-norm. We note that the spectral box-norm is essentially equivalent to the cluster norm, a multitask learning regularizer introduced by [Jacob et al. 2009 a], and which in turn can be interpreted as a perturbation of the spectral <b>k-support</b> norm. Centering the norm is important for multitask learning and we also provide a method to use centered versions of the norms as regularizers. Numerical experiments indicate that the spectral <b>k-support</b> and box-norms and their centered variants provide state of the art performance in matrix completion and multitask learning problems respectively...|$|E
40|$|International audienceWe explore various sparse regularization {{techniques}} for analyzing fMRI data, {{such as the}} 1 norm (often called LASSO {{in the context of}} a squared loss function), elastic net, and the recently introduced <b>k-support</b> norm. Employing sparsity regularization allows us to handle the curse of dimensionality, a problem commonly found in fMRI analysis. In this work we consider sparse regularization in both the regression and classification settings. We perform experiments on fMRI scans from cocaine-addicted as well as healthy control subjects. We show that in many cases, use of the <b>k-support</b> norm leads to better predictive performance, solution stability, and interpretability as compared to other standard approaches. We additionally analyze the advantages of using the absolute loss function versus the standard squared loss which leads to significantly better predictive performance for the regulariza-tion methods tested in almost all cases. Our results support the use of the <b>k-support</b> norm for fMRI analysis and on the clinical side, the generalizability of the I-RISA model of cocaine addiction...|$|E
40|$|Abstract—Coverage is a {{fundamental}} problem in wireless sensor networks (WSNs). From both economic and applicable concerns, designers always would like to provide guaranteed QoS of coverage of WSNs. In this paper, we address two path-coverage problems in WSNs, maximum <b>k-support</b> path coverage (a. k. a. best case coverage) and minimum k-breach path coverage (a. k. a. worst case coverage), in which every point on the desired resultant path is covered by at least k sensors simultaneously while optimizing certain objectives. We present two polynomial-time approaches to find optimal solutions for both maximum <b>k-support</b> coverage problem and minimum k-breach coverage problem. The time complexity of both algorithms are Oðk 2 n lognÞ, where n {{is the number of}} deployed sensor nodes and k is the coverage degree. In addition, a number of properties of kth-nearest point Voronoi diagram are presented, which is new to the literature. Index Terms—Optimum k-coverage, <b>k-support</b> path, k-breach path, wireless sensor networks Ç...|$|E
40|$|We explore various sparse regularization {{techniques}} for analyzing fMRI data, {{such as the}} ℓ 1 norm (often called LASSO {{in the context of}} a squared loss function), elastic net, and the recently introduced <b>k-support</b> norm. Employing sparsity regularization allows us to handle the curse of dimensionality, a problem commonly found in fMRI analysis. In this work we consider sparse regularization in both the regression and classification settings. We perform experiments on fMRI scans from cocaine-addicted as well as healthy control subjects. We show that in many cases, use of the <b>k-support</b> norm leads to better predictive performance, solution stability, and interpretability as compared to other standard approaches. We additionally analyze the advantages of using the absolute loss function versus the standard squared loss which leads to significantly better predictive performance for the regularization methods tested in almost all cases. Our results support the use of the <b>k-support</b> norm for fMRI analysis and on the clinical side, the generalizability of the I-RISA model of cocaine addiction. status: publishe...|$|E
40|$|Abstract—Data Aggregation is a {{fundamental}} problem in wireless sensor networks (WSNs). From both economic and applicable concerns, designers always would like to provide guaranteed QoS of coverage of WSNs. In this paper, we address two path-coverage problems in WSNs, maximum <b>k-support</b> path coverage (a. k. a. best case coverage) and minimum k-breach path coverage (a. k. a. worst case coverage), in which every point on the desired resultant path is covered by at least k sensors simultaneously while optimizing certain objectives. We present an approaches to find optimal solutions for both maximum <b>k-support</b> coverage problem and minimum k-breach coverage problem. I...|$|E
40|$|The {{removal of}} mixed Gaussian-impulse noise plays an {{important}} role in many areas, such as remote sensing. However, traditional methods may be unaware of promoting the degree of the sparsity adaptively after decomposing into low rank component and sparse component. In this paper, a new problem formulation with regular spectral <b>k-support</b> norm and regular <b>k-support</b> l 1 norm is proposed. A unified framework is developed to capture the intrinsic sparsity structure of all two components. To address the resulting problem, an efficient minimization scheme within the framework of accelerated proximal gradient is proposed. This scheme is achieved by alternating regular k-shrinkage thresholding operator. Experimental comparison with the other state-of-the-art methods demonstrates the efficacy of the proposed method...|$|E
40|$|Sparsity, or cardinality, as a {{tool for}} feature {{selection}} is extremely common in a vast number of current computer vision applications. The <b>k-support</b> norm is a recently proposed norm with the proven property of providing the tightest convex bound on cardinality over the Euclidean norm unit ball. In this paper we present a re-derivation of this norm, with the hope of shedding further light on this particular surrogate function. In addition, we also present a connection between the rank operator, the nuclear norm and the <b>k-support</b> norm. Finally, based on the results established in this re-derivation, we propose a novel algorithm with significantly improved computational efficiency, empirically validated on a number of different problems, using both synthetic and real world data. Anders Eriksson, Trung Thanh Pham, Tat-Jun Chin, Ian Rei...|$|E
40|$|The <b>k-support</b> norm {{has been}} {{recently}} introduced to perform correlated sparsity regularization. Although Argyriou et al. only reported experiments using squared loss, here we {{apply it to}} several other commonly used settings resulting in novel machine learning algorithms with interesting and familiar limit cases. Source code for the algorithms described here is available...|$|E
40|$|The {{spectral}} <b>k-support</b> norm enjoys good estimation {{properties in}} low rank matrix learning problems, empirically outperforming the trace norm. Its unit ball is the convex hull of rank k matrices with unit Frobenius norm. In this paper we generalize the norm to the spectral (k,p) -support norm, whose additional parameter p {{can be used}} to tailor the norm to the decay of the spectrum of the underlying model. We characterize the unit ball and we explicitly compute the norm. We further provide a conditional gradient method to solve regularization problems with the norm, and we derive an efficient algorithm to compute the Euclidean projection on the unit ball in the case p=∞. In numerical experiments, we show that allowing p to vary significantly improves performance over the spectral <b>k-support</b> norm on various matrix completion benchmarks, and better captures the spectral decay of the underlying model...|$|E
40|$|We derive a novel norm that {{corresponds}} to the tightest convex relaxation of spar-sity combined with an ` 2 penalty. We show that this new <b>k-support</b> norm provides a tighter relaxation than the elastic net and can thus be advantageous in in sparse prediction problems. We also bound the looseness of the elastic net, thus shedding new light on it and providing justification for its use. ...|$|E
40|$|International audienceIn this paper, {{we present}} a novel method for disease {{classification}} between two patient populations based on features extracted from Magnetic Resonance Imaging (MRI) data. Anatomically meaningful features are extracted from structural data (T 1 - and T 2 -weighted MR images) and Diffusion Tensor Imaging (DTI) data, and used to train a new machine learning algorithm, the <b>k-support</b> SVM (ksup-SVM). The <b>k-support</b> regularized SVM has an inherent feature selection property, and thus it eliminates the requirement for a separate feature selection step. Our dataset consists of patients that suffer from facioscapulohumeral muscular dystrophy (FSH) and Myotonic muscular dystrophy type 1 (DM 1) and our proposed method achieves a high performance. More specifically, it achieves a mean Area Under the Curve (AUC) of 0. 7141 and mean accuracy 77 % ± 0. 013. Moreover, we provide a sparsity visualization of the features in order to indentify their discriminative value. The results suggest {{the potential of the}} combined use of MR markers to diagnose myopathies, and the general utility of the ksup-SVM. Source code is also available at [URL]...|$|E
3000|$|... {{other items}} in the {{outsourced}} dataset. The major issue left open by [18] is a formal protection result: their privacy analysis is entirely conducted empirically on various synthetic datasets. Tai et al. [34] show that their outsourced data set satisfies <b>k-support</b> anonymity, but only explores set based attack empirically. Unfortunately, both works have potential privacy flaws: Molloy et al. [35] show how privacy can be breached {{in the framework of}} [18]. We have discuss the details of the flaws in the framework of [34] in [21].|$|E
40|$|International audienceMany {{epidemiological}} studies are undertaken with a use of large epidemiological databases, which involves the simultaneous {{evaluation of a}} large number of variables. Epidemiologists face a number of problems when dealing with large data sets: multicolinearity (when variables are correlated to each other), confounding factors (when risk factor is correlated with both exposure and outcome variable), and interactions (when the direction or magnitude of an association between two variables differs due to the effect of a third variable). Correct variable selection helps to address these issues and helps to obtain unbiased results. Selection of relevant variables is a complicated and a time consuming task. Flawed variable selection methods still prevail in the scientific literature; there is a need to demonstrate the usability of new algorithms using real data. In this paper we propose to use a novel machine learning method, <b>k-support</b> regularized logistic regression, for discovering predictors of mental health service utilization in the National Epidemiologic Survey for Alcohol and Related Conditions (NESARC). We show that <b>k-support</b> regularized logistic regression yields better prediction accuracy than 1 or 2 regularized logistic regression as well as several baseline methods on this task, and we qualitatively evaluate the top weighted variates. The selected variables are supported by related epidemiological research, and give important cues for public policy...|$|E
40|$|Abstract Universum-support vector machine (U-SVM) is {{an elegant}} method for 2 -class {{classification}} problem. It is systematically studied in this paper, including {{the existence and}} uniqueness of the primal problem {{as well as the}} relation between the solutions of primal problem and dual problem. We find that U-SVM uses 3 -class classification approach to solve the 2 -class classification problem. So we have compared <b>K-support</b> vector classification regression (K-SVCR) and support vector ordinal regression machine (SVORM). Our conclusion is that, selecting their parameters properly, these three models get the same decision function essentially...|$|E
40|$|Abstract. The restriction, from {{a compact}} Lie group K to a closed subgroup, of a polynomially bounded {{representation}} remains polynomially bounded provided a geometric assumption on the asymptotic <b>K-support</b> of the representation is satisfied. This is a theorem of T. Kobayashi. We give a {{proof of this}} theorem using microlocal analysis {{in the setting of}} distribution rather than hyperfunction theory. The proof is based on a characterization, up to the natural K × K action, of the wavefront set of a distribution on K in terms of the asymptotic behavior of its Fourier coefficients. 1...|$|E
30|$|The {{problem of}} secured {{outsourcing}} of frequent itemset mining on the multi-cloud environments is studied by Tai et al. (2013). Concerning {{the challenges in}} big data analysis, they suggested to partition the data into several parts and outsourced each part independently to different cloud based on pseudo-taxonomy, anonymization technique, known as KAT. They proposed DKNT to ensure the privacy security for each partial data outsourced to different clouds. Experimental results demonstrated excellent achievement in terms of protection and better computation efficiency as compared to those on a single machine. Tai et al. (2010) presented <b>K-support</b> anonymity, which provided protection against a knowledgeable attacker with the exact support information. To achieve the <b>K-support</b> anonymity, a pseudo taxonomy tree is introduced with the third party mine for the generalized frequent item-sets. The construction of the pseudo taxonomy tree facilitated the hiding of the original items and limited the fake items introduced in the encrypted database. The results showed very good privacy protection with moderate storage overhead. K-anonymity is further enhanced and improved by Pan et al. (2012). They analyzed and compared the developed K-anonymity models and discussed their applications. The modified K-anonymity models such as the L-diversity, (α, K)-anonymity and (α, L)-diversification K-anonymity overcome the existing limitations related to privacy. Few K-anonymous methods are employed in obtaining the main technology.|$|E
40|$|International audienceWe {{study the}} problem of {{statistical}} estimation with a signal known to be sparse, spatially contiguous, and containing many highly correlated variables. We take inspiration from the recently introduced <b>k-support</b> norm, which has been successfully applied to sparse prediction problems with correlated features, but lacks any explicit structural constraints commonly found in machine learning and image processing. We address this problem by incorporating a total variation penalty in the <b>k-support</b> framework. We introduce the (k, s) support total variation norm as the tightest convex relaxation of the intersection {{of a set of}} sparsity and total variation constraints. We show that this norm leads to an intractable combinatorial graph optimization problem, which we prove to be NP-hard. We then introduce a tractable relaxation with approximation guarantees that scale well for grid structured graphs. We devise several first-order optimization strategies for statistical parameter estimation with the described penalty. We demonstrate the effectiveness of this penalty on classification in the low-sample regime, classification with M/EEG neuroimaging data, and image recovery with synthetic and real data background subtracted image recovery tasks. We extensively analyse the application of our penalty on the complex task of identifying predictive regions from low-sample high-dimensional fMRI brain data, we show that our method is particularly useful compared to existing methods in terms of accuracy, interpretability, and stability...|$|E
40|$|We {{characterize}} the streaming space complexity of every symmetric norm l (a norm on R^n invariant under sign-flips and coordinate-permutations), by relating this space complexity to the measure-concentration characteristics of l. Specifically, we provide nearly matching {{upper and lower}} bounds on the space complexity of calculating a (1 ±ϵ) -approximation to the norm of the stream, for every 0 2. In addition, we apply our general results to easily derive bounds for several norms that were not studied before in the streaming model, including the top-k norm and the <b>k-support</b> norm, which was recently employed for machine learning tasks. Overall, these results make progress on two outstanding problems in the area of sublinear algorithms (Problems 5 and 30 in ...|$|E
40|$|Abstract—Coverage is a {{fundamental}} problem in wireless sensor networks since sensors may be spread in arbitrary manner. In this paper, we addressed the <b>k-support</b> coverage problem. Most of the existing works assume that the coverage degree is 1, i. e. every point in a certain path falls within the sensing range {{of at least one}} sensor node. In this work, we focus on the case when we require that every point on the path is covered by at least k sensors, while optimizing certain objectives. We give an optimal polynomial-time algorithm and prove that the time complexity of our algorithm is O(k 2 n 2). To the best of our knowledge, this is the first polynomial time algorithm finding an optimum k-coverage path in sensor networks with general sensing radius and general k. I...|$|E
40|$|Discovery of {{association}} rules {{has been found}} useful in many applications. In previous work, all items in a basket database are treated uniformly. We generalize this to the case where items are given weights to reflect their importance to the user. The weights may correspond to special promotions on some products, or the profitability of different items. We can mine the weighted association rules with weights. The downward closure property of the support measure in the unweighted case no longer exist and previous algorithms cannot be applied. In this paper, two new algorithms will be introduced to handle this problem. In these algorithms we make use of a metric called the <b>k-support</b> bound in the mining process. Experimental results show {{the efficiency of the}} algorithms for large databases. Keywords: data mining, association rules, basket data, support, confidence, weighted items. 1 Introduction Computers store large amounts of retailing transactions in a retailing business. Marketin [...] ...|$|E
40|$|In this paper, {{we present}} a unified {{analysis}} of matrix completion under general low-dimensional structural constraints induced by any norm regularization. We consider two estimators for the general problem of structured matrix completion, and provide unified upper bounds on the sample complexity and the estimation error. Our analysis relies on results from generic chaining, and we establish two intermediate results of independent interest: (a) in characterizing the size or complexity of low dimensional subsets in high dimensional ambient space, a certain partial complexity measure encountered {{in the analysis of}} matrix completion problems is characterized in terms of a well understood complexity measure of Gaussian widths, and (b) it is shown that a form of restricted strong convexity holds for matrix completion problems under general norm regularization. Further, we provide several non-trivial examples of structures included in our framework, notably the recently proposed spectral <b>k-support</b> norm. Comment: published in NIPS 2015. Advances in Neural Information Processing Systems 28, 201...|$|E
40|$|Abstract — As {{the age of}} {{big data}} evolves, {{outsourcing}} of data mining tasks to multi-cloud environments has become a popular trend. To ensure the data privacy in outsourcing of mining tasks, the concept of support anonymity was proposed to hide sensitive information about patterns. Existing methods that tackle the privacy issues, however, do not address the related parallel mining techniques. To fill this gap, we refer to a pseudo-taxonomy based technique, called as <b>k-support</b> anonymity, and improve it into multi-cloud environments with secrete sharing scheme. This has several advantages. First, outsourcing to multi-cloud environments can meet the requirement of great computational resources in big data mining, and also parallelize the mining tasks for better efficiency. Second, the data that we send out to a cloud can be partial. An assaulter who gets the data in one cloud can never re-construct the original data. That means {{it is more difficult}} for an assailant to violate the privacy in outsourced data. Experimental results also demonstrated that our approaches can achieve good protection and better computation efficiency. I...|$|E
40|$|It is of {{significant}} importance for any classification and recognition system, which claims near {{or better than}} human performance to be immune to small perturbations in the dataset. Researchers found out that neural networks are not very robust to small perturbations and can easily be fooled to persistently misclassify by adding a particular class of noise in the test data. This, so-called adversarial noise severely deteriorates the performance of neural networks, which otherwise perform really well on unperturbed dataset. It has been recently proposed that neural networks can be made robust against adversarial noise by training them using the data corrupted with adversarial noise itself. Following this approach, in this paper, we propose a new mechanism to generate a powerful adversarial noise model based on <b>K-support</b> norm to train neural networks. We tested our approach on two benchmark datasets, namely the MNIST and STL- 10, using muti-layer perceptron and convolutional neural networks. Experimental results demonstrate that neural networks trained with the proposed technique show significant improvement in robustness as compared to state-of-the-art techniques...|$|E
40|$|In this paper, {{we study}} k-road-coverage {{problems}} in {{wireless sensor networks}} (WSNs). Assume there is a 2 -dimensional area! with a given road mapR = (V,E) where E contains all road segments and V consists of all intersection points on!. The first question we study is about ‘sensor deployment’, i. e., how to deploy a minimum number of sensor nodes on! such that each path (each road segment) on R is k-covered when all sensor nodes have the same sensing range. When sensors can only be deployed {{in a set of}} discrete locations, we propose an efficient method with the approximation ratio 6 + ϵ for the special case where k = 1 and O(k) generally. If sensors can be deployed in arbitrary locations, we propose an efficient method with the approximation ratio 24 + ϵ when k = 1 and O(k) generally. The second question we study is about ‘path query’, i. e., how to find the k-covered path or <b>k-support</b> path connecting any given source/destination pair of points on the road mapR. Basically, given any source/destination pair of points S and D, we present two algorithms which can efficiently find a k-covered path connecting S and D and a k-supported path connecting S and D, respectively. Copyright © 2010 Joh...|$|E
40|$|Recently, the {{paradigm}} of data mining-as-a-service in cloud computing environment has been attracting interests. In this paradigm, a company (data owner), lacking data storage, computational resources and expertise, stores its data in the cloud and outsources its mining tasks to the cloud service provider (server). In {{order to protect the}} privacy of the outsourced database and the association rules mined, k-anonymity, <b>k-support,</b> and k-privacy techniques have been proposed to perturb the data before it is uploaded to the server. These techniques are computationally expensive. If the data owner has resources to use these techniques, then it is often able to execute association rule mining locally. In this paper, we consider a scenario where a user (data owner) encrypts its data and stores it in the cloud. To mine association rules from its data, the user outsources the task to n (≥ 2) "semi-honest" servers, which cooperate to perform association rule mining on the encrypted data in the cloud and return encrypted association rules to the user. In this setting, we provide three solutions to protecting data privacy during association rule mining. Our solutions are built on the distributed ElGamal cryptosystem and achieve item privacy, transaction privacy and database privacy, respectively, as long as at least one out of the n servers is honest. To reduce the possibility that all servers are compromised, the user can use servers from different cloud providers. Our implementation and experiments demonstrate that our solutions are practical...|$|E
40|$|In this paper, {{we propose}} a unified theory for convex {{structured}} sparsity-inducing norms on vectors associated with combinatorial penalty functions. Specifically, {{we consider the}} situation of a model simultaneously (a) penalized by a set-function defined on {{the support of the}} unknown parameter vector which represents prior knowledge on supports, and (b) regularized in p-norm. We show that each of the obtained combinatorial optimization problems admits a natural relaxation as an optimization problem regularized by a matching sparsity-inducing norm. To characterize the tightness of the relaxation, we introduce a notion of lower combinatorial envelope of a set-function. Symmetrically, a notion of upper combinatorial envelope produces the most concise norm expression. We show that these relaxations take the form of combinatorial latent group Lassos associated with min-cover penalties also known as block-coding schemes. For submodular penalty functions, the associated norm, dual norm and the corresponding proximal operator can be computed efficiently using a generic divide-and-conquer algorithm. Our framework obtains constructive derivations for the Lasso, group Lasso, exclusive Lasso, the OWL, OSCAR and SLOPE penalties, the <b>k-support</b> norm, several hierarchical penalties considered in the literature for chains and tree structures, and produces also new norms. It leads to general efficient algorithms for all these norms, recovering as special cases several algorithms proposed in the literature and yielding improved procedures for some cases. For norms associated with submodular penalties, including a large number of non-decomposable norms, we generalize classical support recovery and fast rates convergence results based respectively on generalization of the irrepresentability condition and the restricted eigenvalue condition...|$|E

