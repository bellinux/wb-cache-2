16|27|Public
40|$|Real-time {{applications}} such as computer and video games, virtual reality and scientific simulation require rendering of complex models for realism. Graphics rendering engines include multiresolution modelling techniques to accelerate the visualization process. The Discrete Level of Detail framework (DLoD) is usually the most popular while the Continuous Level of Detail framework (CLoD) is still not as widely used by software developers. In this paper, we first discuss the benefits and drawbacks of both frameworks. Then, we present a model based on coding a discrete number of levels of detail (LoDs), with more LoDs coded than is usual in DLoD, and with an <b>incremental</b> <b>representation,</b> which is often used in CLoD. This model obtains a performance similar to DLoD by providing optimized LoDs for efficient visualization, while the popping effect is imperceptible. We present specific proposals {{for each of the}} three main stages involved in multiresolution processing: geometry simplification, construction of the <b>incremental</b> <b>representation</b> and retrieval of either uniform or view-dependent LoDs...|$|E
40|$|The paper {{reports on}} the {{methodology}} and preliminary results of {{a case study in}} automatically extracting ontological knowledge from Italian legislative texts. We use a fully-implemented ontology learning system (T 2 K) that includes a battery of tools for Natural Language Processing (NLP), statistical text analysis and machine language learning. Tools are dynamically integrated to provide an <b>incremental</b> <b>representation</b> of the content of vast repositories of unstructured documents. Evaluated results, however preliminary, show the great potential of NLP-powered incremental systems like T 2 K for accurate large-scale semi-automatic extraction of legal ontologies...|$|E
40|$|In {{this paper}} we {{provide the basis}} for new methods of {{inference}} for max-stable processes xi on general spaces that admit a certain <b>incremental</b> <b>representation,</b> which, in important cases, has a much simpler structure than the max-stable process itself. A corresponding peaks-over-threshold approach will incorporate all single events that are extreme in some sense and will therefore rely on a substantially larger amount of data in comparison to estimation procedures based on block maxima. Conditioning a process eta in the max-domain of attraction of xi on being extremal, several convergence results for the increments of eta are proved. In a similar way, the shape functions of mixed moving maxima (M 3) processes can be extracted from suitably conditioned single events eta. Connecting the two approaches, transformation formulae for processes that admit both an incremental and an M 3 representation are identified...|$|E
40|$|Using the {{cognitive}} architecture ACT-R/E, we designed {{a framework for}} implementing cognitively plausible spoken language understanding on an embodied agent using <b>incremental</b> frame <b>representations</b> for multiple levels of linguistic knowledge. Emphasis is placed on semantics, pragmatics, and speaker intent...|$|R
40|$|In several {{practical}} applications, it {{is required}} to resort to a distributed <b>incremental</b> field <b>representation</b> to overcome those difficulties that may occur in applying ray methods close to and at caustics. This field representation may also improve upon the field estimate whenever the stationary phase condition {{that leads to the}} ray field regime is not yet wel...|$|R
40|$|A category-theoretic {{account of}} neural network {{semantics}} {{has been used}} to characterize <b>incremental</b> concept <b>representation</b> in neural memory. It involves a category of concepts and concept morphisms together with categories of objects and morphisms representing the activity in connectionist structures at different stages of weight adaptation. Colimits express the more specialized concepts as combinations of abstract concepts along shared subconcept relationships specified in diagrams. This provides a mathematical model of concept blending, in which designated relationships among concepts are preserved in a combination. Structure-preserving mappings called functors from the concept to neural categories provide a mathematical model of <b>incremental</b> concept <b>representation</b> through stages of adaptation. The work reported here extends these ideas to express temporal sequences of events, such as episodic memories. This requires an extended notion of neural morphism and a design principle for diagrams involving concepts in a temporal sequence. This is tested in a new architecture that involves a notion of supertemplates, which are ART network templates extending over a multi-level ART hierarchy with an interposed temporal integrator network. Defense Threat Reduction Agency (DTRA), United States Department of Defens...|$|R
40|$|This paper {{addresses}} model checking {{based on}} SAT solvers and Craig interpolants. We tackle major scalability problems of state-of-the-art interpolation-based approaches, and we achieve two main results: (1) a novel model checking algorithm; (2) {{a new and}} flexible way to handle an <b>incremental</b> <b>representation</b> of (over-approximated) forward reachable states. The new model checking algorithm (IGR: Interpolation with Guided Refinement), partially takes inspiration from IC 3 and interpolation sequences. It bases its robustness and scalability on incremental refinement of state sets, and guided unwinding/simplification of transition relation unrollings. State sets, the central data structure of our algorithm, are incrementally refined, and they represent a valuable information to be shared among related problems, either in concurrent or sequential (multiple-engine or multiple property) execution schemes. We provide experimental data, showing that IGR extends the capability of a state-of-the-art model checker, with a specific focus on hard-to-prove propertie...|$|E
40|$|This article {{explores the}} use of Simple Synchrony Networks (SSNs) for {{learning}} to parse English sentences drawn from a corpus of naturally occurring text. Parsing natural language sentences requires taking a sequence of words and outputting a hierarchical structure representing how those words fit together to form constituents. Feed-forward and Simple Recurrent Networks have had great difficulty with this task, {{in part because the}} number of relationships required to specify a structure is too large for the number of unit outputs they have available. SSNs have the representational power to output the necessary O(n 2) possible structural relationships, because SSNs extend the O(n) incremental outputs of Simple Recurrent Networks with the O(n) entity outputs provided by Temporal Synchrony Variable Binding. This article presents an <b>incremental</b> <b>representation</b> of constituent structures which allows SSNs to make effective use of both these dimensions. Experiments on learning to [...] ...|$|E
30|$|This paper {{presents}} a methodology that aims at the <b>incremental</b> <b>representation</b> of areas inside environments {{in terms of}} attractive forces. It is proposed a parametric representation of velocity fields ruling the dynamics of moving agents. It is assumed that attractive spots in the environment are responsible for modifying the motion of agents. A switching model is used to describe near and far velocity fields, which in turn are used to learn attractive characteristics of environments. The effect of such areas is considered radial over all the scene. Based on the estimation of attractive areas, a map that describes their effects {{in terms of their}} localizations, ranges of action, and intensities is derived in an online way. Information of static attractive areas is added dynamically into a set of filters that describes possible interactions between moving agents and an environment. The proposed approach is first evaluated on synthetic data; posteriorly, the method is applied on real trajectories coming from moving pedestrians in an indoor environment.|$|E
40|$|A {{number of}} {{techniques}} {{have been developed}} for reconstructing surfaces by integrating groups of aligned range images. A desirable set of properties for such algorithms includes: <b>incremental</b> updating, <b>representation</b> of directional uncertainty, the ability to fill gaps in the reconstruction, and robustness in the presence of outliers. Prior algorithms possess subsets of these properties. In this paper, we present a volumetric method for integrating range images that possesses all of these properties...|$|R
40|$|A variational {{principle}} for limit analysis of beams and pi&es is developed from a yield function {{based on the}} Frobenius matrix norm. The formulation produced a pair of maximization and minimiza-tion probIems with a duality relation between them. Exact solutions of two simple problems are presented as verification to {{the validity of the}} new variational principle. An iterative algorithm is constructed to solve the minimization problem. The algorithm, tested successfully on the two example problems, is intended for beams and plates with general ioading and boundary condi~ons and shapes. Plasticity as a model for mechanics of a class of materials presents some unusual mathema-tical difficulties that require special treatment, To correctly represent the slip-yield phenomenon of the crystalline structure, the field functions like the displacement and strain should be allowed discuntinuous behavior. The lack of a one to one constitutive relation requires inequality or <b>incremental</b> <b>representations.</b> The differential geometry of large defor-mation further introduces unpleasant nonlinearities. Great efforts have been made to over-come these difficulties, leading to some large scale incremental computer programs [l, 21. Limit analysis provides an alternative approach. It avoids the tedium of the incremental analysis and attains the limit solution directly. Earlier development of limit analysis was base...|$|R
40|$|Abstract. In real life, visual {{learning}} {{is supposed to}} be a continuous process. Humans have an innate facility to recognize objects even under less-than-ideal conditions and to build robust representations of them. These representations can be altered with the arrival of new information and thus the model of the world is continuously updated. Inspired by the biological paradigm, we propose in this paper an <b>incremental</b> subspace <b>representation</b> for cognitive vision processes. The proposed approach has been applied to the problem of face recognition. The experiments performed on a custom database show that at the end of incremental learning process the recognition performance achieved converges towards the result obtained using an off-line learning strategy. ...|$|R
40|$|Personal {{conceptions of}} {{intelligence}} {{seem to make}} {{a significant contribution to}} overcoming a reading deficit, as indicated in our earlier research. The present aim was to assess improvements in reading-decoding following training of children with reading-decoding problems and different conceptions of intelligence (incremental or entity). It was expected that treatment of children with an <b>incremental</b> <b>representation</b> would improve more. Participants were 20 children (10 girls, 10 boys) whose average age was 8. 6 yr., who attended Grade 3 of elementary school, and who were selected from 675 pupils. Children were given a multimedia test to measure motivational factors such as conceptions of intelligence, achievement goals, perception of controllability, and causal attributions. The participants took part in a multimedia training. Posttest evaluations showed more improvement in reading-decoding by children holding an incremental theory of intelligence. The importance of treatment programmes in which account is taken of both specificity of deficits and motivational factors should be explored further as the present sample was very small...|$|E
40|$|This paper {{presents}} a methodology that aims at the <b>incremental</b> <b>representation</b> of areas inside environments {{in terms of}} attractive forces. It is proposed a parametric representation of velocity fields ruling the dynamics of moving agents. It is assumed that attractive spots in the environment are responsible for modifying the motion of agents. A switching model is used to describe near and far velocity fields, which in turn are used to learn attractive characteristics of environments. The effect of such areas is considered radial over all the scene. Based on the estimation of attractive areas, a map that describes their effects {{in terms of their}} localizations, ranges of action, and intensities is derived in an online way. Information of static attractive areas is added dynamically into a set of filters that describes possible interactions between moving agents and an environment. The proposed approach is first evaluated on synthetic data; posteriorly, the method is applied on real trajectories coming from moving pedestrians in an indoor environment...|$|E
40|$|This work {{deals with}} the sensor-based path {{planning}} problem. Particularly, we present the case for manipulator arms. We assume that the knowledge of workspace is partially known and {{that it can be}} increased by using a laser-like sensor. We propose a representation of the workspace that is well adapted to exploit the information obtained from the sensors and then compute collision detection. A sensor-based version of the Ariadne's Clew Algorithm (ACA) is used to incrementally search for the free space and compute a path to a goal conguration. 1 Introduction This work {{deals with the}} problem of sensor-based path planning in unknown environments. Particularly, we present the case for manipulators arms. The problem consists of nding a free trajectory for a robot arm from an initial conguration to a nal conguration in a partially known environment. In order to execute this task, the robot must construct an <b>incremental</b> <b>representation</b> of the workspace by means of some sensor, {a laser b [...] ...|$|E
40|$|Abstract. In this paper, {{we argue}} that the {{resolution}} of anaphoric expressions in an utterance is essentially an abductive task following [HSAM 93] who use a weighted abduction scheme on horn clauses to deal with reference. We give a semantic representation for utterances containing anaphora that enables us to compute possible antecedents by abductive inference. We extend the disjunctive model construction procedure of hyper tableaux [BFN 96, Küh 97] with a clause transformation turning the abductive task into a model generation problem and show the completeness of this transformation {{with respect to the}} computation of abuctive explanations. This abductive inference is applied to the resolution of anaphoric expressions in our general model constructing framework for <b>incremental</b> discourse <b>representation</b> [Küh 99] which we argue to be useful for computing information updates from natural language utterances [Vel 96]. ...|$|R
40|$|A {{number of}} {{techniques}} {{have been developed}} for reconstructing surfaces by integrating groups of aligned range images. A desirable set of properties for such algorithms includes: <b>incremental</b> updating, <b>representation</b> of directional uncertainty, the ability to fill gaps in the reconstruction, and robustness in the presence of outliers. Prior algorithms possess subsets of these properties. In this paper, we present a volumetric method for integrating range images that possesses all of these properties. Our volumetric representation consists of a cumulative weighted signed distance function. Working with one range image at a time, we first scan-convert it to a distance function, then combine this with the data already acquired using a simple additive scheme. To achieve space efficiency, we employ a run-length encoding of the volume. To achieve time efficiency, we resample the range image to align with the voxel grid and traverse the range and voxel scanlines synchronously. We generate the [...] ...|$|R
40|$|Abstract — Several recent {{algorithms}} have formulated the SLAM {{problem in}} terms of non-linear pose graph optimization. These algorithms are attractive because they offer lower computational and memory costs than the traditional Extended Kalman Filter (EKF), while simultaneously avoiding the linearization error problems that affect EKFs. In this paper, we present a new non-linear SLAM algorithm that allows incremental optimization of pose graphs, i. e., allows new poses and constraints to be added without requiring the solution to be recomputed from scratch. Our approach builds upon an existing batch algorithm that combines stochastic gradient descent and an <b>incremental</b> state <b>representation.</b> We develop an incremental algorithm by adding a spatially-adaptive learning rate, and a technique for reducing computational requirements by restricting optimization to only the most volatile portions of the graph. We demonstrate our algorithms on real datasets, and compare against other online algorithms. I...|$|R
40|$|AbstractHomogenization {{methods were}} {{developed}} to relate microstructure and local deformation mechanisms to overall behavior. The well-known self-consistent scheme was successfully developed to describe mechanical interactions for elastic, elastoplastic and viscoplastic behaviors. When complex space–time couplings and non-linearity are involved, new estimations have to be established. This paper addresses to local behavior exhibiting elastic, anelastic and inelastic strains. A new estimation of mechanical interactions is proposed. It uses an <b>incremental</b> <b>representation</b> of the behavior {{and is based on}} translated fields techniques and self-consistent approximation. First, the case of a purely anelastic (Kelvin–Voigt) heterogeneous medium is treated. The solution of the anelastic heterogeneous problem is then used to solve the complete problem where the local behavior is described through a Burger element. Results obtained with the present modeling are compared with results obtained with other models found in literature in the case of linear behavior. They show that a good description of the time-dependent spatial interactions is obtained. Thanks to this incremental approach, the present modeling can be easily used for non linear behavior...|$|E
40|$|International audienceThe {{continuum}} damage mechanics-based elasto-plastic damage theory, {{that extends}} the total form of Hayakawa and Murakami equations, is developed. Weak elastic-plastic dissipation coupling is assumed {{by the use}} of two dissipation potentials, plastic and damage, where only isotropic plasticity and damage hardening is included, whereas kinematic hardening is not accounted for. Unilateral damage condition, based on the concept of generalized projection operators, accounts for a partial damage deactivation, which allows for an influence of negative principal components of the stress tensor on damage evolution. The <b>incremental</b> <b>representation</b> of the elastic-damage constitutive equations is derived. Both elastic-damage and plastic-damage compliance matrices are developed for plane stress condition, and implemented to ABAQUS finite element code by the user-supplied procedure for non-standard material properties. Effective computation algorithm for plastic and damage loading/unloading conditions based on the doubly passive predictor and plastic-damage corrector approach is proposed. Numerical examples are presented by applying the model calibration by Hayakawa and Murakami for the spheroidized graphite cast iron FCD 400. The examples illustrate the capability of the model to describe elastic-plastic damage evolution under monotonic loading. Under reverse loading conditions a partial elastic stiffness recovery was demonstrated on the consecutive increasing strain-controlled loading cycles and some limitation of the model was shown...|$|E
40|$|Functional prototypes and {{simulations}} are a well recognised {{and valued}} tool {{for building a}} shared understanding of requirements between users and developers. However, the develop-ment of such artefacts does {{not sit well with}} traditional modelling processes which tend to focus on structure rather than behaviour. Consequently building prototypes and simulations is a diversion form the mainstream development process, sometimes even competing with it. We propose that the resolution in this conflict lies in promoting the role of executable behavioural models, so that artefacts supporting behavioural simulation are a by-product of the mainstream modelling process. We discuss why conventional modelling techniques are not suited to this, and we describe an innovative behavioural modelling technique, Protocol Modelling, that is well adapted to direct execution. Using Protocol Modelling, a behavioural entity (business object or process) is modelled in terms of its event protocol: the conditions under which it accepts or refuses events. Such models capture the behavioural integrity rules at the level of business events; and can be composed using the semantics of Hoare’s CSP, allowing concise and <b>incremental</b> <b>representation.</b> Direct execution of the model is achieved using a tool that simulates a normal user interface, so that non-technical stakeholders can review and explore behaviour while requirements are being solidified. ...|$|E
40|$|Agent-based {{modeling}} {{has proven}} to be a natural way to express various types of problems or situations. Some research has focused on the analysis and design of agent models, but little has truly addressed the need for automated assistance when creating agent-based models from the initial problem comprehension. This paper proposes an approach addressing this gap and supporting the iterative process of generating executable agent models. In particular, this approach enables the <b>incremental</b> conceptual <b>representation</b> of a problem and the development of agent models. This paper presents how to develop an agent-based model using a predefined generic Scenarization Vocabulary. It then describes the technical approach that was chosen in order to exploit the initial conceptual design and facilitate the development of models by eliminating software development technicalities. This approach is part of a broader research effort known as IMAGE, which proposes a toolset supporting collaborative understanding of complex situations. ...|$|R
40|$|In this paper, {{we argue}} that the {{resolution}} of anaphoric expressions in an utterance is essentially an abductive task following [12] who use a weighted abduction scheme on horn clauses to deal with reference. We give a semantic representation for utterances containing anaphora that enables us to compute possible antecedents by abductive inference. We extend the disjunctive model construction procedure of hyper tableaux [3, 14] with a clause transformation turning the abductive task into a model generation problem and show the completeness of this transformation {{with respect to the}} computation of abductive explanations. This abductive inference is applied to the resolution of anaphoric expressions in our general model constructing framework for <b>incremental</b> discourse <b>representation</b> which we argue to be useful for computing information updates from natural language utterances. Keywords: Model construction, abduction, coreference, anaphora resolution 1 Introduction There is a lot of wor [...] ...|$|R
40|$|Abstract. Prototype {{classifiers}} {{have been}} studied for many years. But most methods adopt single vectors as prototypes to represent the original data. It is difficult to learn the local information [1] with such prototypes. In this paper, we propose an incremental classifier named Subspace Based Prototype Classifier (SBPC) to handle the issue. SBPC designs an augmented strategy to enhance traditional prototype classifiers. Instead of using single vector, each prototype of SBPC represents a subset of input data using a subspace. And we employ an <b>incremental</b> subspace <b>representation</b> method to learn a subspace for each prototype. By designing a self-adaptive threshold policy, SBPC automatically learns the number and value of prototypes without any prior knowledge. Through adopting both condensing scheme and editing scheme [3], the prototypes are incremental learned, automatically adjusted (condensing scheme) and removed (editing scheme). Results of experiments described herein show that the proposed SBPC accommodates the non-stationary data environment and provides good recognition performance and storage efficiency...|$|R
40|$|International audiencePrevious {{approaches}} to deformable model shape estimation and tracking have assumed a fixed class of shapes representation (e. g., deformable superquadrics), initialized prior to tracking. Since the shape {{coverage of the}} model is fixed, such approaches do not directly accommodate <b>incremental</b> <b>representation</b> discovery during tracking. As a result, model shape coverage is decoupled from tracking, thereby limiting both processes in terms of scope and robustness. We present a novel deformable model framework that accommodates the incremental incorporation during tracking of new geometric primitives (lines, in addition to points) that are not explicitly captured in the initial deformable model but that are moving consistently with its image motion. As these new features are detected via consistency checks, they {{are added to the}} model, providing incremental soft constraints on the estimation of its rigid parameters. The consistency checks are based on trilinear relationships between geometric primitives. Consequently, we not only increase both model scope and, ultimately, its higher-level shape coverage, but improve tracking robustness and accuracy, by directly employing the new features in both forward prediction and reconstruction. Our new formulation is a step towards automating model shape estimation and tracking, since it requires significantly reduced initial model hand-crafting. We demonstrate our approach on two separate image-based tracking domains, each involving complex 3 D object shape and motion...|$|E
40|$|AbstractÐThis article {{explores the}} use of Simple Synchrony Networks (SSNs) for {{learning}} to parse English sentences drawn from a corpus of naturally occurring text. Parsing natural language sentences requires taking a sequence of words and outputting a hierarchical structure representing how those words fit together to form constituents. Feed-forward and Simple Recurrent Networks have had great difficulty with this task, {{in part because the}} number of relationships required to specify a structure is too large for the number of unit outputs they have available. SSNs have the representational power to output the necessary O 8 ̆ 5 n 2 possible structural relationships because SSNs extend the O 8 ̆ 5 n incremental outputs of Simple Recurrent Networks with the O 8 ̆ 5 n entity outputs provided by Temporal Synchrony Variable Binding. This article presents an <b>incremental</b> <b>representation</b> of constituent structures which allows SSNs to make effective use of both these dimensions. Experiments on learning to parse naturally occurring text show that this output format supports both effective representation and effective generalization in SSNs. To emphasize the importance of this generalization ability, this article also proposes a short-term memory mechanism for retaining a bounded number of constituents during parsing. This mechanism improves the O 8 ̆ 5 n 2 speed of the basic SSN architecture to linear time, but experiments confirm that the generalization ability of SSN networks is maintained. Index TermsÐConnectionist networks, natural language processing, simple synchrony networks, syntactic parsing, temporal synchrony variable binding. æ...|$|E
40|$|Time cues are {{ubiquitous}} {{in language and}} the ability to interpret them is essential for understanding events during discourse comprehension. Temporal markers that signal ongoing versus completed events, like the progressive and simple past tense, prompt distinct mental event representations. However, the detailed properties of ongoing event representations remain unexplored. Drawing from both the simulation and semantic association approaches to knowledge representation, this study examines the novel prediction that ongoing events engender <b>incremental</b> discourse <b>representation</b> updating processes. Experimental sentences cued either early or late phases of an ongoing event (e. g. Alice had recently started/almost finished baking a cake). Targets in a post-sentential lexical decision task were strongly associated with either early or late event phases (e. g. EGGS/AROMA). Facilitation priming was predicted for congruent sentence-target pairs, however, priming was found for the early event phase exclusively. The results have implications for models of knowledge representation, theories of semantic priming, and discourse model updating...|$|R
40|$|Statically {{exploring}} the inter-procedural control flow of objectoriented applications {{is often difficult}} because {{of the use of}} abstraction, polymorphism, and dynamic binding. To ease this problem, in this tool demonstration, we present a new profiler that dynamically explores the inter-procedural control flow of Java applications while they are executing. Our profiler visualizes the complete Calling Context Tree (CCT) with various dynamic metrics, such as method invocations, executed bytecodes, or allocated objects, and enables efficient navigation in large CCTs comprising up to several millions of nodes. We show that our tool can render data quite fast with response times in the range of 14 – 204 ms upon user interactions. Thanks to a carefully tuned <b>incremental</b> data <b>representation,</b> the profiling data produced by a running application can be updated several times per second on a standard laptop. The visualization can also show recently active parts of the application. The collection of profiling data uses an aspect-based dynamic program analysis technique that simplifies extension and customization of the tool...|$|R
40|$|This paper {{concerns}} making {{large scale}} Kernel Principal Component Analysis (KPCA) feasible on regular hardware. The KPCA {{has been proven}} a useful non-linear feature extractor in several computer vision applications. The standard computation method for KPCA, however, scales badly with the problem size, thus limiting {{the potential of the}} technique for large scale data. We propose a novel method to alleviate this problem. The essence of our solution lies in partitioning the data and greedily filtering each partition for a sparse <b>representation.</b> <b>Incremental</b> KPCA is then utilized to merge each partition to arrive at the overall KPCA. We also provide experimental results which demonstrate the effectiveness of the approach. 1...|$|R
40|$|One goal of explanation-based {{learning}} is to transform knowledge into an operational form for efficient use. Typically, this involves rewriting concept descriptions {{in terms of}} the predicates used to describe examples. In this paper we present RINCON, a system that extends domain theories from examples with the goal of maximizing classification efficiency. RINCON'S basic learning operator involves the introduction of new intermediate concepts into a domain theory, which can be viewed as the inverse of the operationalization process. We discuss the system's learning algorithm and its relation to work on explanation-based learning, <b>incremental</b> concept formation, <b>representation</b> change, and pattern matching. We also present experimental evidence from two natural domains that indicates the addition of intermediate concepts can improve classification efficiency. ...|$|R
40|$|Robots {{must be able}} to {{function}} in the real world. The real world involves processes and agents that move independently of the actions of the robot, sometimes in an unpredictable manner. A real-time integrated route planning and spatial representation system for planning routes through dynamic domains is presented. The system will find the safest most efficient route through space-time as described by a set of user defined evaluation functions. Because the route planning algorthims is highly parallel and can run on an SIMD machine in O(p) time (p is the length of a path), the system will find real-time paths through unpredictable domains when used in an <b>incremental</b> mode. Spatial <b>representation,</b> an SIMD algorithm for route planning in a dynamic domain, and results from an implementation on a traditional computer architecture are discussed...|$|R
40|$|Wet {{laboratory}} mutagenesis {{to determine}} enzyme activity changes is expensive and time consuming. This paper expands on standard one-shot learning by proposing an incremental transductive method (T 2 bRF) for {{the prediction of}} enzyme mutant activity during mutagenesis using Delaunay tessellation and 4 -body statistical potentials for <b>representation.</b> <b>Incremental</b> learning is in tune with both eScience and actual experimentation, as it accounts for cumulative annotation effects of enzyme mutant activity over time. The experimental results reported, using cross-validation, show that overall the incremental transductive method proposed, using random forest as base classifier, yields better results compared to one-shot learning methods. T 2 bRF is shown to yield 90 % on T 4 and LAC (and 86 % on HIV- 1). This is significantly better than state-of-the-art competing methods, whose performance yield is at 80 % or less using the same datasets...|$|R
40|$|In this paper, {{we propose}} {{a method for}} model {{predictive}} control of linear parameter-varying (LPV) systems described in an input-output (IO) representation and subject to input- and output constraints. By assuming exact knowledge of the future trajectory of the scheduling variable, the on-line computations reduce to the solution of a nominal predictive control problem. An <b>incremental</b> non-minimal state-space <b>representation</b> {{is used as a}} prediction model, giving a controller with integral action suitable for tracking piecewise-constant reference signals. Closed-loop asymptotic stability is guaranteed by a terminal cost and terminal set constraint, and the computation of an ellipsoidal terminal set is discussed. Numerical examples demonstrate the properties of the proposed approach. When exact future knowledge of the scheduling variable is not available, we argue and show that good practical performance can be obtained by a scheduling prediction strategy...|$|R
40|$|In {{this work}} {{we present a}} novel method to {{automate}} the computation of global constraints cost for local search. The method {{is based on the}} representation of a global constraints as graph properties on a binary constraint network. This formulation simplifies the implementation of global constraints in local search, and provides a cost that can be readily compared to one obtained for subproblems using binary constraints exclusively. The cost obtained can be efficiently updated during the search using <b>incremental</b> methods. The <b>representation</b> of a global constraint as outlined above can also be used for generation of suitable neighborhoods for the constraint. This is done using simple repair functions applied on the elementary constraints in the global constraint graph. We show the usability of our approach by presenting formulations of global constraints in non-overlapping and cumulative scheduling. ...|$|R
40|$|Agent-based {{modeling}} {{has been}} of interest to researchers for some time now. Some {{research has focused on}} the analysis and design of such software, but none has truly addressed the need for automated assistance in creating agent-based simulators from initial problem comprehension. This paper proposes an approach addressing the gap and supporting the spiral process of generating an agent-based simulator. In particular, this approach enables the <b>incremental</b> and iterative <b>representation</b> of a problem and its translation into an executable model. Initially using an unconstrained ontology, the designer draws conceptual graphs representing the problem. Progressively, graph elements are linked hierarchically under concepts that are part of a predefined generic Scenarization Vocabulary (i. e., agent, patient, behaviour, attribute, parameter, variable �). This Scenarization semantic defines roles in the simulation. This approach is part of a broader research effort known as IMAGE that develops a toolset concept supporting collaborative understanding of complex situations. ...|$|R
40|$|Computing {{the set of}} reachablestates of {{a finite}} state machine, is an {{important}} component of many problems in the synthesis, and formal verification of digital systems. The process of design is usually iterative, and the designer may modify and recompute information many times, and reachability is called each time the designer modifies the system, because current methods for reachability analysis are not <b>incremental.</b> Unfortunately, the <b>representation</b> of the reachable states that is currently used [1] in synthesis and verification, is inherently non-updatable. We solve this problem by presenting alternate ways to represent the reachable set, and incremental algorithms that can update the new representation each time the designer changes the system. The incremental algorithms use the reachable set computed at a previous iteration, and information about the changes to the system to update it, rather than compute the reachable set from the beginning. This results in computational savings, [...] ...|$|R
