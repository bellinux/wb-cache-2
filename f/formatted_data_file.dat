4|10000|Public
40|$|We {{developed}} an automated measurement system using a PC running a LabView application, a Velmex BiSlide X-Y positioner, and a HP 85 l 0 C network analyzer. The system provides high positioning accuracy and requires no user supervision. After the user inputs the necessary parameters into the LabView application, LabView controls the motor positioning and performs the data acquisition. Current parameters and measured data are {{shown on the}} PC display in two 3 -D graphs and updated after every data point is collected. The final output is a <b>formatted</b> <b>data</b> <b>file</b> for later processing...|$|E
40|$|We {{conducted}} 2 validation {{studies for}} {{a modified version}} of the local lymph node assay (LLNA), which was designated as the LLNA-DA. A total of 17 laboratories tested the validity of the assay by using 14 chemicals. Here, in addition to the experimental protocol, we prepared the study protocols describing the study purpose, the role of the participants, etc. Technology transfer was conducted by the developer of the assay. Prior to the studies, preliminary tests using only a positive control chemical were conducted to determine whether the experimental protocol prescribed for the assay was appropriate. A <b>formatted</b> <b>data</b> <b>file</b> was developed for data management. Fortunately, the results of these studies revealed small interlaboratory variations, and we believe that one of the factors that contributed to the successful results was the development of strategie...|$|E
40|$|This release adds missing {{functionality}} to the SHGrids, SHCoeffs, and SHWindow classes, {{and adds}} support for PyPI. Add pyshtools to PyPI repository. Can now be installed using pip install pyshtools. Add new function SHBiasKMask {{which is the}} arbitrary window counterpart to the spherical cap window SHBiasK. Add get_biasedpowerspectrum() method to SHWindows for arbitrary windows. Add copy() method to all classes, which returns a deep copy of the instance. Add __sub__, __add__, __rsub__, __radd__, __mul__, __div__, __truediv__, and __pow__ operators for two sets of coefficient or grid classes, or one coefficient or grid class and a scalar. Add nwinrot option when rotating spherical cap windows in SHWindow that will rotate only the first nwinrot windows. Add degrees option to get_lats() and get_lons() methods. Add the constructor from_file() to initialize an SHGrid with a numpy <b>formatted</b> <b>data</b> <b>file.</b> Add option to read coeffs and grids from a numpy formatted binary file. Addtofile() methods to output raw grid and coefficient data as either text or binary formatted files. Update Intro 1 notebook and add example to Intro notebook 2 showing how to use arbitrary localization windows. Convert notebooks to html and add links to web documentation. Add option fixed_power to SHCoeffs. from_random() method to generate random coefficients that fit exactly the expected power spectrum. Add Earth topography coefficients referenced to mean sea level to the example files, expanded to degree 300 : srtmp 300. msl...|$|E
5000|$|... pbdNCDF4 --- {{interface}} to Parallel Unidata NetCDF4 <b>format</b> <b>data</b> <b>files</b> ...|$|R
40|$|Software {{has been}} {{developed}} for the Radio Shack TRS 80 (TM) microcomputer which creates <b>data</b> <b>files</b> for a large computer data base management system (TAXIR). The programs perform five functions: (1) format CRT screens to emulate data input forms, (2) input data, (3) verify data, (4) edit <b>data,</b> and (5) <b>format</b> <b>data</b> into <b>files</b> acceptable to TAXIR. The <b>formatted</b> <b>data</b> <b>files</b> are stored on a minidisk. Commercially available software is used to turn the microcomputer into a remote data terminal for transmission of the files to the large computer (an Amdahl/V 8 using the MTS operating system. ...|$|R
40|$|This paper {{proposes a}} tool that {{converts}} non-FCA <b>format</b> <b>data</b> <b>files</b> into an FCA format, thereby making {{a wide range of}} public data sets and data produced by non-FCA tools interoperable with FCA tools. This will also offer the power of FCA to a wider community of data analysts. A repository of converted data is also proposed, as a consistent resource of public data for analysis and for the testing, evaluation and comparison of FCA tools and algorithms. </p...|$|R
40|$|EQ 3 / 6 is a {{software}} package for geochemical modeling of aqueous systems. This report describes version 7. 0. The major {{components of the}} package include: EQ 3 NR, a speciation-solubility code; EQ 6, a reaction path code which models water/rock interaction or fluid mixing in either a pure reaction progress mode or a time mode; EQPT, a data file preprocessor, EQLIB, a supporting software library; and five supporting thermodynamic data files. The software deals with the concepts of thermodynamic equilibrium, thermodynamic disequilibrium, and reaction kinetics. The five supporting data files contain both standard state and activity coefficient-related data. Three {{support the use of}} the Davies or B-dot equations for the activity coefficients; the other two support the use of Pitzer`s equations. The temperature range of the thermodynamic data on the data files varies from 25 {degree}C only to 0 [...] 300 {degree}C. EQPT takes a <b>formatted</b> <b>data</b> <b>file</b> (a data 0 file) and writes an unformatted near-equivalent called a datal file, which is actually the form read by EQ 3 NR and EQ 6. EQ 3 NR is useful for analyzing groundwater chemistry data, calculating solubility limits, and determining whether certain reactions are in states of partial equilibrium or disequilibrium. It is also required to initialize an EQ 6 calculation. EQ 6 models the consequences of reacting an aqueous solution with a set of reactants which react irreversibly. It can also model fluid mixing and the consequences of changes in temperature. This code operates both in a pure reaction progress frame and in a time frame...|$|E
40|$|Ptplot {{is a set}} of two {{dimensional}} signal plotters components written in Java with multiple properties, such as being embeddable in applets or applications, utilizing automatic or manual tick marks, logarithmic axes, infinite zooming, and much more. The World Data Centre of IPS applies Ptplot as a multiple function online data plot tool by converting various text <b>format</b> <b>data</b> <b>files</b> into Ptplot recognizable XML files with the AWK language. At present, Ptplot has allowed eight archived solar-terrestrial science data sets to be easily plotted, viewed, and downloaded from the IPS web site...|$|R
40|$|Hera is {{the data}} {{processing}} facility {{provided by the}} High Energy Astrophysics Science Archive Research Center (HEASARC) at the NASA Goddard Space Flight Center for analyzing astronomical data. Hera provides all the pre-installed software packages, local disk space, and computing resources need to do general processing of FITS <b>format</b> <b>data</b> <b>files</b> residing on the users local computer, and to do research using the publicly available data from the High ENergy Astrophysics Division. Qualified students, educators and researchers may freely use the Hera services over the internet of research and educational purposes...|$|R
40|$|SYSGEN, Power Generation System Production costing and Reliability Analysis program, simulates {{production}} costs {{and reliability of}} electric utility with and without time-dependent generating units. FEPS transforms inputs into proper <b>formats,</b> builds <b>data</b> <b>files</b> and adds Job Control Language necessary to running SYSGEN program...|$|R
40|$|TES Product File Reader {{software}} extracts {{data from}} publicly available Tropospheric Emission Spectrometer (TES) HDF (Hierarchical <b>Data</b> <b>Format)</b> product <b>data</b> <b>files</b> using publicly available format specifications for scientific analysis in IDL (interactive data language). In this innovation, the software returns data fields as simple arrays {{for a given}} file. A file name is provided, and the contents are returned as simple IDL variables...|$|R
40|$|The Income and Expenditure Survey of 2000 is {{the most}} recent {{comprehensive}} source of information on income and expenditure patterns of South African households. This data is used to compile various household-related sub-matrices {{for a series of}} Social Accounting Matrices for South Africa. By linking the Labour Force Survey of September 2000, which contains detailed employment data on occupation codes, activity codes and wages of workers, with the Income and Expenditure Survey of 2000 various factor-related sub-matrices can also be extracted. This paper discusses the steps followed to extract data, correct problems and errors where appropriate or necessary, merge various files, and create Stata <b>format</b> <b>data</b> <b>files</b> {{that can be used to}} compile the relevant submatrices. The focus remains highly technical throughout. Consumer/Household Economics,...|$|R
40|$|AbstractAmino acid {{racemization}} (AAR) dating {{methods have}} been used since the mid- 1960 s. Since that time, information technologies have evolved as AAR laboratories have worked to appropriately catalog sample collections and analyses. The University of Delaware AAR Database (UDAARDB) is a database of AAR and other geochronological data from coastal Quaternary sites in North and South America {{that has been in}} development for over 25 years. In that time, database and software platforms have changed and a concerted effort has been made to digitize legacy data for preservation and to make these data available for future use. To ensure data preservation, all or part of UDAARDB is redundantly hosted at three institutions as <b>data</b> <b>files</b> and maps. Furthermore, the flexible nature of accessing the data (i. e., as online maps and common <b>format</b> <b>data</b> <b>files)</b> helps to maintain a public presence and, therefore, assists in their preservation...|$|R
25|$|AcroForms {{can keep}} form field values in {{external}} stand-alone files containing key:value pairs. The external files may use Forms <b>Data</b> <b>Format</b> (FDF) and XML Forms <b>Data</b> <b>Format</b> (XFDF) <b>files.</b> The usage rights (UR) signatures define rights for import form <b>data</b> <b>files</b> in FDF, XFDF and text (CSV/TSV) formats, and export form <b>data</b> <b>files</b> in FDF and XFDF formats.|$|R
50|$|Intermediate <b>Data</b> <b>Format</b> (IDF) <b>files</b> {{are used}} interoperate between {{electronic}} design automation (EDA) software and solid modeling mechanical computer-aided design (CAD) software.|$|R
40|$|China is {{developing}} the nuclear data processing code Ruler, {{which can be}} used for producing multi-group cross sections and related quantities from evaluated nuclear data in the ENDF format [1]. The Ruler includes modules for reconstructing cross sections in all energy range, generating Doppler-broadened cross sections for given temperature, producing effective self-shielded cross sections in unresolved energy range, calculating scattering cross sections in thermal energy range, generating group cross sections and matrices, preparing WIMS-D <b>format</b> <b>data</b> <b>files</b> for the reactor physics code WIMS-D [2]. Programming language of the Ruler is Fortran- 90. The Ruler is tested for 32 -bit computers with Windows-XP and Linux operating systems. The verification of Ruler has been performed by comparison with calculation results obtained by the NJOY 99  [3] processing code. The validation of Ruler has been performed by using WIMSD 5 B code...|$|R
40|$|This package {{contains}} {{data sets}} and scripts (in an Org-mode file) related to our submission to IPDPS 2017, {{under the title}} "Using Simulation to Evaluate and Tune the Performance of Dynamic Load Balancing of an Over-decomposed Geophysics Application". The following contents are included: 	IPDPS 2017. org : Org mode (Emacs) file containing the shell (Bash) and R scripts used to: 	 		run the load balancing simulation; 		process the traces of both real executions (Tau traces) and simulation (Pajé traces); 		generate the graphics. 	 	 	lb_traces/: this directory contains the raw traces from real executions and SMPI emulations of the Ondes 3 D application. 	processed_data/: this directory contains {{the results of the}} processing of the traces in the form of CSV <b>format</b> <b>data</b> <b>files</b> which are be used to generate the graphics. 	img/: this directory contains the generate graphics, in PNG format...|$|R
40|$|The BOREAS RSS- 14 team {{collected}} and processed GOES- 7 and - 8 {{images of the}} BOREAS region {{as part of its}} effort to characterize the incoming, reflected, and emitted radiation at regional scales. This data set contains surface radiation parameters, such as net radiation and net solar radiation, that have been interpolated from GOES- 7 images and AMS data onto the standard BOREAS mapping grid at a resolution of 5 km N-S and E-W. While some parameters are taken directly from the AMS data set, others have been corrected according to calibrations carried out during IFC- 2 in 1994. The corrected values as well as the uncorrected values are included. For example, two values of net radiation are provided: an uncorrected value (Rn), and a value that has been corrected according to the calibrations (Rn-COR). The data are provided in binary image <b>format</b> <b>data</b> <b>files.</b> Some of the <b>data</b> <b>files</b> on the BOREAS CD-ROMs have been compressed using the Gzip program. See section 8. 2 for details. The <b>data</b> <b>files</b> are available on a CD-ROM (see document number 20010000884), or from the Oak Ridge National Laboratory (ORNL) Distributed Active Archive Center (DAAC) ...|$|R
40|$|ErmineJ is {{software}} {{for the analysis of}} functionally interesting patterns in large gene lists drawn from gene expression profiling data or other high-throughput genomics studies. It can be used by biologists with no bioinformatics background to conduct sophisticated analyses of gene sets with multiple methods. It allows users to assess whether microarray data or other gene lists are enriched for a particular pathway or gene class. This protocol provides steps on how to <b>format</b> <b>data</b> <b>files,</b> determine analysis type, create custom gene sets and perform specific analyses-including overrepresentation analysis, genes score resampling and correlation resampling. ErmineJ differs from other methods in providing a rapid, simple and customizable analysis, including high-level visualization through its graphical user interface and scripting tools through its command-line interface, as well as custom gene sets and a variety of statistical methods. The protocol should take approximately 1 h, including (one-time) installation and setup...|$|R
50|$|Like many game <b>data</b> <b>formats,</b> the <b>file</b> {{is able to}} archive several files used by the game. These {{may include}} graphics, 3d models, animations, sound files, and scripts.|$|R
40|$|This {{guide is}} {{targeted}} towards individuals that use {{geographic information system}} (GIS) technology. It describes several techniques for translating data from Geographic Resources Analysis Support System (GRASS) <b>format</b> <b>data</b> <b>files</b> to the ARC/INFO <b>data</b> <b>format</b> used with Environmental Systems Research Institute, INC (ESRI) products. The primary conversion routines discussed here {{are a result of}} a collaborative effort between the U. S. Army Construction Engineering Research Laboratories (USACERL) and ESRI. GRASS is a public domain geographic information system originally developed by USACERL. This report is intended {{to be used as a}} reference during the data conversion process; it describes the two <b>data</b> <b>formats</b> and contains tips that may facilitate the conversion process. Approved for public release; distribution is unlimited. The contents of this report are not to be used for advertising, publication, or promotional purposes. Citation of trade names does not constitute an official endorsement or approval of the use of such commercial products. The findings of this report are not to be construed as an official Department of the Army position, unless so designated by other authorized documents...|$|R
40|$|A {{program was}} written to {{calculate}} the probability of patient survival, tabulate the results {{in the form of}} a life table and compare two patient groups by the Mantel-Haenszel (logrank) test on a microcomputer. The program reads data in either a text form or the <b>format</b> of <b>data</b> <b>files</b> of the dBASE III/III plus (Ashton-Tate). The program is entirely menu-driven and extremely easy to use. The structure of the <b>data</b> <b>files</b> and the function of the program are described. This program is useful to those without easy access to mainframe computers. link_to_subscribed_fulltex...|$|R
50|$|BodyParts3D polygon {{data are}} {{distributed}} in the OBJ <b>file</b> <b>format.</b> The entire <b>data</b> <b>file's</b> size is 127 MB (polygon reduced) and 521 MB (high quality) as of version 3.0. The number of body parts (organs) registered in BodyParts3D is 1,523 as of version 3.0.|$|R
40|$|The {{program is}} {{designed}} to make it easy to extract selected games from a PGN <b>format</b> <b>data</b> <b>file</b> based {{on a wide variety of}} criteria. The criteria include: textual move sequences; the position reached after a sequence of moves; information in the tag fields; fuzzy board position; and material balance in the ending. The program includes a semantic analyser which will report errors in game scores and it is also able to detect duplicate games found in its input files. The range of input move formats accepted is fairly wide and includes recognition of lower-case piece letters for English and upper-case piece letters for Dutch and German. The output is normally in English Standard Algebraic Notation (SAN), but this can be varied. Extracted games may be written out either including or excluding comments, NAGs, variations, move numbers, tags and/or results. Games may be given ECO classifications derived from the accompanying file eco. pgn, or a customised version provided by the user...|$|R
5000|$|... 1. Article 1(2) of the Computer Programs Directive (Council Directive 91/250/EEC of 14 May 1991) must be {{interpreted}} as meaning that neither the functionality of a computer program nor the programming language and the <b>format</b> of <b>data</b> <b>files</b> used in a computer program in order to exploit certain of its functions constitute a form of expression of that program and, as such, are not protected by copyright in computer programs {{for the purposes of}} that directive.|$|R
40|$|The Boreal Ecosystem-Atmosphere Study (BOREAS) Remote Sensing Science (RSS) - 10 team {{investigated}} {{the magnitude of}} daily, seasonal, and yearly variations of Photosynthetically Active Radiation (PAR) from ground and satellite observations. This data set contains satellite estimates of surface-incident PAR (400 - 700 nm, MJ/sq m) at one-degree spatial resolution. The spatial coverage is circumpolar from latitudes of 41 to 66 degrees north. The temporal coverage is from May through September for years 1979 through 1989. Eleven-year statistics are also provided: (1) mean, (2) standard deviation, and (3) coefficient of variation for 1979 - 89. The PAR estimates were derived from the global gridded ultraviolet reflectivity data product (average of 360, 380 nm) from the Nimbus- 7 Total Ozone Mapping Spectrometer (TOMS). Image mask data are provided for identifying the boreal forest zone, and ocean/land and snow/ice-covered areas. The data are available as binary image <b>format</b> <b>data</b> <b>files.</b> The PAR <b>data</b> {{are available from the}} Earth Observing System Data and Information System (EOSDIS) Oak Ridge National Laboratory (ORNL) Distributed Active Archive Center (DAAC). The <b>data</b> <b>files</b> are available on a CD-ROM (see document number 20010000884) ...|$|R
40|$|This paper {{describes}} the <b>format</b> of material <b>data</b> <b>files</b> that hold parameters needed by thermal and hygrothermal simulation {{tools such as}} Delphin, Hajawee (Dynamic Room Model) and Nandrad. The Material <b>Data</b> <b>Files</b> are containers for storing parameters and functions for heat and moisture transport and storage models. The article also discusses the application programming interface of the Material library {{that can be used}} to read/write material <b>data</b> <b>files</b> conveniently and efficiently...|$|R
50|$|Along {{with the}} DIGDAT program, there are viewers for the AC3D, XML, and LFI <b>format</b> output <b>files.</b> <b>Data</b> tables {{can easily be}} output to the screen or to PNG files for {{inclusion}} into reports.|$|R
50|$|Open <b>data</b> <b>format</b> <b>files</b> {{have their}} {{internal}} structures available to {{users of the}} file {{through a process of}} metadata publishing. Metadata publishing implies that the structure and semantics of all the possible data elements within a file are available to users.|$|R
40|$|Motivation: The amplified {{interest}} in metabolic profiling has generated {{the need for}} additional tools to assist in the rapid analysis of complex data sets. Results: A new program; metabolomics spectral formatting, alignment and conversion tools, (MSFACTs) is described here for the automated import, reformatting, alignment, and export of large chromatographic data sets to allow more rapid visualization and interrogation of metabolomic data. MSFACTs incorporates two tools: one for the alignment of integrated chromatographic peak lists and another for extracting information from raw chromatographic ASCII <b>formatted</b> <b>data</b> <b>files.</b> MSFACTs is illustrated in the processing of GC/MS metabolomic data from different tissues of the model legume plant, Medicago truncatula. The results document that various tissues such as roots, stems, and leaves from the same plant can be easily differentiated based on metabolite profiles. Further, similar types of tissues within the same plant, such as the first to eleventh internodes of stems, could also be differentiated based on metabolite profiles. Availability: Freely available upon request for academic and non-commercial use. Commercial use is available through licensing agreemen...|$|R
40|$|Abstract. The SDPA-M (Semidefinite Programming Algorithm in MATLAB) Version 6. 2. 0 is a MATLAB {{interface}} of the SDPA Version 6. 2. 0 [1], {{which is}} known as a fast and numerically stable solver for SDPs (semidefinite programs) [3, 4]. The SDPA-M inherits various features from the SDPA. Particularly, the SDPA-M can read SDPA dense and sparse <b>format</b> input <b>data</b> <b>files</b> of SDPs. In addition, users can easily manipulate and transform their own problems in the MATLAB language, and then solve them by the SDPA-M. This manual and the SDPA-M can be found a...|$|R
40|$|The {{purpose of}} this report {{is to provide a}} brief {{overview}} of the far-field seismic data collected by the array of instruments (Figures 1 and 2) deployed by the Source Physics experiment for shots 1 (roughly 100 kg TNT equivalent at a depth of 60 m) and shot 2, (roughly 2000 kg TNT equivalent at a depth of 45 m). 'Far-field' is taken to refer to instruments in the zone of purely elastic response at distances of 100 m or greater. The primary focus is data from the main instrument array and hence data from other groups is not considered. Infrasound data is not addressed nor any remote sensing data. Data processing was done at LLNL in parallel with the effort at UNR. Raw reftek data was sent via hard disk from NsTec. Reftek data was converted to SEGY and then to SAC <b>format.</b> <b>Data</b> <b>files</b> were renamed according to station and channel information. Reftek logs were reviewed. These data have been reviewed for consistency with the UNR data on the server. The primary goal was quality check and a summary is provided in Tables 1 and 2...|$|R
5000|$|The Python {{programming}} language can access netCDF files with the PyNIO module (which also facilitates {{access to a}} variety of other <b>data</b> <b>formats).</b> netCDF <b>files</b> can also be read with the Python module , and into a pandas-like [...] with the [...] module.|$|R
50|$|The High Court {{referred}} {{several questions}} of {{the interpretation of the}} Software Directive and the Copyright Directive to the Court of Justice of the European Union, under the preliminary ruling procedure. Advocate-General Yves Bot gave his Opinion on 29 November 2011. The full judgement was handed down by the European Court of Justice on 2 May 2012. It largely adopted the Advocate-General's Opinion, holding that neither the functionality of a computer program nor the programming language and the <b>format</b> of <b>data</b> <b>files</b> used in a computer program in order to exploit certain of its functions are covered by copyright.|$|R
40|$|The {{minutes and}} {{associated}} documents prepared from presentations and {{meetings at the}} Fifth Calibration/Data Product Validation Panel meeting in Boulder, Colorado, April 8 - 10, 1992, are presented. Key issues include (1) statistical characterization of data sets: finding statistics that characterize key attributes of the data sets, and defining ways to characterize the comparisons among data sets; (2) selection of specific intercomparison exercises: selecting characteristic spatial and temporal regions for intercomparisons, and impact of validation exercises on the logistics of current and planned field campaigns and model runs; and (3) preparation of data sets for intercomparisons: characterization of assumptions, transportable <b>data</b> <b>formats,</b> labeling <b>data</b> <b>files,</b> content of <b>data</b> sets, and data storage and distribution (EOSDIS interface) ...|$|R
40|$|Abstract {{copyright}} UK Data Service {{and data}} collection copyright owner. The aim {{of the study was}} to provide an estimation of population sizes at fixed times during the period when parish registration is the main source of demographic information. Main Topics : The text file is a dictionary listing all the evidence given in the other <b>data</b> <b>file</b> and describing the <b>format.</b> The <b>data</b> <b>file</b> consists of all the standard data available from a variety of sources such as Hearth Taxes, Visitations and Censuses, arranged in parish and chronological order, and in a format capable of analysis by the University of Pittsburgh SPSS program. Please note: this study does not include information on named individuals and would therefore not be useful for personal family history research. <br...|$|R
