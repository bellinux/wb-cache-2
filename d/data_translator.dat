16|50|Public
50|$|A CAD <b>data</b> <b>translator</b> {{for reading}} and writing all {{commonly}} used CAD format files.|$|E
5000|$|PowerMILL Pro {{includes}} PS-Exchange a CAD <b>data</b> <b>translator</b> for all neutral import {{and export}} (IGES and VDA and STEP) ...|$|E
5000|$|Ingres 10S and 10.2 include native {{comprehensive}} spatial support. Ingres {{includes the}} Geospatial Data Abstraction Library cross-platform spatial <b>data</b> <b>translator.</b>|$|E
5000|$|... <b>data</b> <b>translators</b> - NURBS-based {{geometry}} translator libraries,with interfaces for the SMLib, TSNLib, GSNLib,NLib, and SDLib {{family of}} products, includingIGES, STEP, VDAFS, SAT, and OpenNURBScapabilities.|$|R
40|$|Databases and {{software}} tools for electric power distribution systems {{have not been}} integrated, and this leads to extra costs and restrictions imposed on utilities and other stakeholders. For example, distributed resource integration studies and modern grid technology assessments are more difficult and costly. New vendors face high market entry barriers, because it’s necessary to interface with large and customized data systems at each potential utility customer. This project promotes data {{and software}} tool integration, {{through a set of}} <b>data</b> <b>translators</b> based on a common object model. The <b>data</b> <b>translators</b> are delivered as open-source software, using appropriate Web software technologies. The parties who benefit include electric utilities (and their ratepayers), researchers at government laboratories and universities, small software companies wishing to enter the electric utility market, and parties wishing to interconnect distributed generation to a utility system...|$|R
5000|$|<b>Data</b> converters: {{language}} <b>translators,</b> speech processing, URL shorteners...|$|R
50|$|In the seven-layer OSI {{model of}} {{computer}} networking, the presentation layer is layer 6 {{and serves as}} the <b>data</b> <b>translator</b> for the network. It is sometimes called the syntax layer.|$|E
50|$|Direct data translators {{provide a}} direct {{solution}} which entails translating the data {{stored in a}} product database directly from one CAD system format to another, usually in one step. There usually exists a neutral database in a direct <b>data</b> <b>translator.</b> The structure of the neutral database must be general, governed by the minimum required definitions {{of any of the}} modelling data types, and be independent of any vendor format. Major CAD systems, such as SolidWorks, PTC Creo, Siemens NX and CATIA can directly read and/or write other CAD formats, simply by using File Open and File Save As options. This option is limited by the fact that most CAD formats are proprietary therefore direct translators are typically unidirectional, partially functional and not standardized.|$|E
40|$|High-speed, single-shot, {{transient}} voltage is recorded {{on a video}} tape recorder, which, when played back, converts the single signal to a repetitive signal. This drives a sample <b>data</b> <b>translator</b> which lengthens the original transient production time, suiting it to an x-y plotter or computer tape recorder use...|$|E
40|$|A NEUTRAL INFORMATION MODEL FOR SIMULATING MACHINE SHOP OPERATIONS Small machine shops {{typically}} {{do not have}} the resources to develop custom simulations of their operations or <b>data</b> <b>translators</b> to import their data from other manufacturing software applications. This paper presents an overview of an information model currently under development at the National Institute of Standards and Technology (NIST) to address this problem. The model provides neutral data interfaces for integrating machine shop software applications with simulation. The information model provides mechanisms for describing data about organizations, calendars, work, resources, schedules, parts, process plans, and layouts within a machine shop environment. The model has been developed using th...|$|R
40|$|How {{could the}} {{manufacturing}} {{modeling and simulation}} process be improved? Today simulation analysts typically code their models from scratch and build custom <b>data</b> <b>translators</b> to import required data. Manufacturing simulations often are built as single monolithic software systems. The development of neutral, vendor-independent data formats for storing simulation models and transferring data could greatly improve the accessibility of simulation technology to industry. Simulation standards for these models and data could help to accelerate the modeling process and reduce modeling costs. How can we determine what simulation standards need to be developed? This panel will discuss simulation standards needs {{from the perspective of}} users, vendors, academia, and government. ...|$|R
40|$|Software Engineering Environments (SEE) {{are complex}} systems, for which {{configurability}} {{is an important}} requirement. Constructing SEEs out of existing tools is evidently desirable. During such a composition mismatches in the data models of different tools need to be dealt with. The brute force technique by hacking <b>data</b> <b>translators</b> into the implementation of individual tools has severe drawbacks regarding modularity, maintainability and extensibility of the composed system. We propose a novel technique for designing SEEs that uses explicit language constructs for bridging the mismatches in the data models, called dynamic view connectors (DVCs for short). We show how the separation of tool functionality from the concerns of bridging data model mismatches imposed by DVCs improved the configurability and maintainability of an existing SEE...|$|R
40|$|Artículos en revistasThe CIM (IEC 61968 /IEC 61970) and the IEC 61850 are the {{two main}} {{families}} of standards that promote the interoperability in the electric system domain. The IEC {{is working on the}} modifications to be performed in such standards to solve the existing mismatches between them. However, until these modifications have been standardized, interactions between systems using the current versions of the standards will still be required. Interactions between heterogeneous models are usually expensive and time-consuming processes. For that reason, a new methodology based on Semantic Web open-source resources is proposed in this paper to facilitate such interactions. The methodology is implemented through a <b>data</b> <b>translator</b> that imports its knowledge from files described in well-known languages, facilitating its adoption to other heterogeneous models. The tests performed have proved the ability of the <b>data</b> <b>translator</b> to make bi-directional translations between the standards and to recover the lost information during the translations. info:eu-repo/semantics/publishedVersio...|$|E
40|$|As part {{of efforts}} to adopt {{manufacturing}} automation in a scattered organizational structure the Finnish precast concrete industry has initiated {{the development of a}} number of solutions for data exchange. Guidelines concerning various aspects of using computers in the design/manufacturing process were defined in a manual which was widely distributed to involved parties. Standardized neutral file formats for data exchange between dissimilar computer systems were developed for three kinds of data: 1) drawings, 2) tables (e. g. bills of materials) and 3) product model-based <b>data.</b> <b>Translator</b> programs were developed for a number of common CAD-systems as well as a set of software tools to the users of standardized exchange files and software developers. The result of these developments have been widely adopted by fabricators, designers and software developer...|$|E
40|$|Transfer of {{information}} from source database into data warehouse i. e. data updating {{is one of the}} most important issue for discussion and research. Organizations that are going through it are solving by deploying dynamic and intelligent data warehouse in real-time manner. Overall strategy in dynamic and intelligent data warehouse relies on updates that are made to source database. These updates in source database are then stored in some temporary storage device through which they are transferred to data warehouse. This paper proposes a Dynamic and Intelligent Data Warehouse (DIDW) architecture with its implementation, using multi real-time data cache. Data cache stores updated data retrieved by update <b>data</b> <b>translator</b> from expandable horizontally partition database. Thus it is analyzed and suggested that using this DIDW architecture in real-time manner helps to the organizations...|$|E
40|$|Abstract: Modelling and {{simulation}} {{technology is}} recognized for facilitating training, reducing production cost, improving product quality, and shortening development time. However, this technology remains largely underutilized by industry today. This is because custom simulator development {{is complex and}} costly, and custom translators are needed to run commercial simulation software. Information models and standard interfaces could help address these problems. A machine shop information model, developed at the National Institute of Standards and Technology (NIST), provides data interfaces for integrating machine shop software applications with simulation. The interfaces include organizations, calendars, work, resources, schedules, parts, process plans, and layout within a machine shop environment. The model is represented by Extensible Markup Language and Unified Modelling Language. This paper briefly presents the machine shop information model, introduces a data transfer mechanism between the machine shop database and XML document, and discusses <b>data</b> <b>translators</b> and <b>data</b> import/export...|$|R
40|$|The {{problem of}} exchanging data between {{two or more}} {{organizations}} in a format that is accessible and understandable by each is a universal problem. Furthermore, the problem of translating or accessing data in the correct format for applications using proprietary data formats is challenging. Legacy software applications may endure for some time given regulatory expenditure pressures on electricity system operators and these require <b>data</b> <b>translators</b> (importer/exporter) and access facilities. The basis {{of this paper is}} that the EPRI common information model (CIM) in extensible markup language (XML) represents the first stage in a revolution of data exchange and manipulation for power systems. This paper explores the problem of translating data in the CIM XML format to the required format for such legacy power system analysis applications. This paper discusses solutions to some of the challenges in data translation, and illustrates how these solutions can be implemented...|$|R
40|$|Object {{oriented}} application libraries {{targeted to}} a specific application domain are an attractive means of reducing the software development time for sophisticated high performance applications. However, libraries can have the drawback of high abstraction penalties. We describe a domain specific, source-to-source translator that eliminates abstraction penalties in an array class library used to analyze turbulent flow simulation <b>data.</b> Our <b>translator</b> effectively flattens the abstractions, yielding performance within 75 % of C code that uses primitive C arrays and no user-defined abstractions. 1...|$|R
40|$|International Telemetering Conference Proceedings / October 02 - 04, 1967 / Marriott Motor Hotel, Washington, D. C. A Single Side Band <b>Data</b> <b>Translator</b> {{has been}} {{developed}} and used successfully with any combination of reference and data frequencies from 100 Hz to 10 MHz. This translator, consisting of a computer calculated 90 ° phase difference network and field effect transistor multiplier circuits yielded undesired sideband signals which were at least 50 db below the desired sideband level over an input frequency range of ten to one. The translator was used to implement a system which recorded 75 time-correlated broadband data channels on a single track of a 5 MHz rotating head tape recorder. After de-translation, a {{signal to noise ratio}} of 60 db was measured with the recorder by-passed and a signal to noise ratio of 40 db was measured with all channels recorded. This paper presents the results of research carried out under Contract No. N 123 (60530) 51701 A sponsored by U. S. Navy...|$|E
40|$|Currently, most CAD systems, {{even the}} feature-based design systems which were {{developed}} for CAPP, cannot provide exact information about an object (e. g. dimensions and tolerances). Some feature-based design systems can provide product data directly or indirectly; however, most CAPP systems {{still do not}} have an interface with CAD systems. The product data required by CAPP systems usually has a specific format which is unique to the system. In a CAPP system, it is essential for set-up planning to ensure the precision of the machining processes. Therefore, it is necessary to develop an interface with CAD models, so that the part data file can be obtained directly from the CAD representation. This paper proposes an approach for integrating the set-up planning system with a feature-based CAD system. By using an object-oriented approach-Product <b>Data</b> <b>Translator</b> (PDT), the computer-automated extraction of geometry and complete tolerance infor-mation was achieved; and the automated generation of the tool approach direction was developed...|$|E
40|$|ABSTRACT: The NATO PATHFINDER Integration Environment task groupMSG- 027 {{conducted}} {{several experiments}} {{in support of}} collecting knowledge for a web portal, which is described in earlier papers. Among these supporting experiments, two focused on C 2 /M&S Interoperability utilizing SISO activities. The first one set up a transatlantic federation of the German M&S system PABST, the Spanish M&S system SIMBAD, the Swedish Google Earth Adaptor for Visualization, the WebCOP C 2 Visualizer, and the US/Danish C 2 system SITAWARE based on Coalition Battle Management Language (C-BML) web services. The second connected DIS based LINK 16 elements produced by the US system ASCOT and the HLA based LINK 16 elements produced by the UK system AIME Fog-of-war and displayed the results via the US Joint Live Virtual Constructive <b>Data</b> <b>Translator</b> (JLVCDT) on the Global Command and Control System – Joint (GCCS-J). This paper describes the systems, summarizes the chosen approaches, and shows the benefit of open standards for distributed operations in NATO {{from the perspective of}} the experiment lead. ...|$|E
5000|$|In the 2000s, online {{translation}} services began incorporating TM. Machine translation services like Google Translate, {{as well as}} professional and [...] "hybrid" [...] translation services provided by sites like Gengo and Ackuna, incorporate databases of TM <b>data</b> supplied by <b>translators</b> and volunteers to make more efficient connections between languages provide faster translation services to end-users.|$|R
40|$|ITC/USA 2005 Conference Proceedings / The Forty-First Annual International Telemetering Conference and Technical Exhibition / October 24 - 27, 2005 / Riviera Hotel & Convention Center, Las Vegas, NevadaThe {{focus of}} this paper is to {{describe}} a unified methodology for developing both internal and external <b>data</b> display <b>translators</b> between an Instrumentation Support System (ISS) format and Data Display Markup Language (DDML), a neutral language for describing data displays. The methodology includes aspects common to both ISSs that have a well documented text-based save format and those that do not, as well as aspects that are unique to each type. We will also describe the means by which an external translator can be integrated into a translator framework. Finally, we will describe how an internal translator can be integrated directly into the ISS...|$|R
5000|$|Only sworn translators {{can do a}} sworn {{translation}} in Spain. To become a sworn translator in Spain for a combination of Spanish and another language, the candidate has to be certified by the Spanish Ministry of Foreign Affairs and Cooperation as a [...] "sworn translator and interpreter" [...] (traductor-intérprete jurado). Then, the translator is required to register their stamp and signature with the Ministry, who includes the <b>translator's</b> <b>data</b> in a public list of sworn interpreters.|$|R
40|$|In {{order to}} achieve true 3 D user {{interaction}} with geographic information, an interface between a virtual environment system and a geographic information system has been designed and implemented. This VE/GIS interface {{is based on a}} loose coupling of the underlying geographic database and the virtual environment system via a dynamic data-translator. This process monitors events initiated by the user in the virtual environment. Based on these events, appropriate queries are generated and sent to the geographic database. On the other hand, the <b>data</b> <b>translator</b> receives GIS data as a result of queries, and converts these data into appropriate representations for the virtual environment. Moreover, the VE/GIS interface performs data management tasks in order to efficiently utilize the limited amount of data that can be kept on-line in the virtual environment. To this aim, an object caching mechanism has been devised. The dynamic data-translator supports both explicit and implicit access to the geographic database. These concepts are illustrated in a virtual environment based user interface that provides basic interaction facilities for the intuitive exploration of geographic information. The approach chosen leads to a layered data management scheme where issues related to collaborative VE experiences, such as guaranteed performance, synchronization, concurrent access, and network traffic limitations, can be handled at an appropriate system level...|$|E
40|$|The term Connected Vehicle {{refers to}} the federal {{initiative}} to develop and deploy a fully connected transportation system where data is shared between different vehicles and between vehicles and the transportation infrastructure which shows promise for addressing mobility and safety issues (RITA, 2012). With respect to impacts to the transportation system due to weather events there {{is an area of}} Connected Vehicle applications specific to road weather where weather data can be collected from existing on-board vehicle systems to support the operations of the roadway segments during weather events (FHWA, 2011 a). Vehicle data could then be shared among other travelers or to the roadway agency operating the road. Weather data that can be collected from on-board vehicle systems include the barometric pressure, windshield wiper settings, air temperature, ABS Brake Status, traction and stability control, and differential wheel speeds (Drobot, 2009). The hope is that eventually vehicle manufacturers will begin integrating technology into their cars at the factory that will allow transmission to roadside data receivers but until this happens the vehicle system data must be read through a Can-bus reader. This raw data can then be processed into weather data useful for roadway operations {{through the use of a}} Vehicle <b>Data</b> <b>Translator</b> or VDT (Drobot et al...|$|E
40|$|US Joint Forces Command’s (USJFCOM) Joint National Training Capability (JNTC) {{program has}} been tasked by the Office of the Under Secretary of Defense (OUSD) for Personnel and Readiness (P&R) to develop and {{integrate}} a distributed, seamless Joint Training Environment (JTE) consisting of Live, Virtual, and Constructive (LVC) training technologies. Currently, LVC assets within the JNTC architecture are integrated through loosely defined protocols, multiple types and instances of protocol translators, {{and a collection of}} distributed messaging tools, most of which are either ill-defined, require technical expertise to use, or do not provide the level of interoperability necessary to meet OUSD P&R requirements. The Joint Training and Education Capability Group (JTECG) is studying future LVC integration concepts and tools that will address these limitations and result in more interoperable LVC systems. One such tool, the Joint LVC <b>Data</b> <b>Translator</b> (JLVCDT) Framework, will reduce the number and variety of protocol translators used to support Joint training, will support rapid development and integration of LVC protocols {{through the use of a}} scalable software architecture, and will act as a system and software platform for further research and development of a Common LVC Architecture (CLA). Developed in conjunction with the United States Services, the JLVCDT will reuse software and interface...|$|E
40|$|Data {{parallel}} programming stands for single threaded, global name space, and loosely synchronous parallel computation. This kind of {{parallel programming}} {{has been proven}} to be very user-friendly, easy to debug and easy to use. But this programming model is not available for most message passing multiprocessor architectures. Adaptor (Automatic <b>Data</b> Parallelism <b>Translator)</b> is a compilation system that transforms data parallel programs written in Fortran with array extensions, parallel loops, and layout directives to parallel programs with explicit message passing. The current version supports especially the translation of Connection Machine Fortran and High Performance Fortran programs to message passing programs. After a short {{description of the system}} it will be shown how efficient the compilation is. Therefore many results about speed-ups and efficiencies of real codes are presented. Furthermore, the automatically generated message passing programs are compared with hand written mess [...] ...|$|R
40|$|The VRFurnace is {{a unique}} VR {{application}} designed to analyze a complete coal-combustion CFD model of a power plant furnace. Although other applications have been created that analyze furnace performance, no other has included the added complications of particle tracking and the reactions associated with coal combustion. Currently the VRFurnace is a versatile analysis tool. <b>Data</b> <b>translators</b> have been written to allow data {{from most of the}} major commercial CFD software packages as well as standard data formats of hand-written code to be uploaded into the VR application. Because of this almost any type of CFD model of any power plant component can be analyzed immediately. The ease of use of the VRFurnace is another of its qualities. The menu system created for the application not only guides first time users through the various button combinations but it also helps the experienced user keep track of which tool is being used. Because the VRFurnace was designed for use in the C 6 device at Iowa State University's Virtual Reality Applications Center it is naturally a collaborative project. The projection-based system allows many people {{to be involved in the}} analysis process. This type of environment opens the design process to not only include CFD analysts but management teams and plant operators as well by making it easier for engineers to explain design changes. The 3 D visualization allows power plant components to be studied in the context of their natural physical environments giving engineers a chance to use their innate pattern recognition and intuitive skills to bring to light key relationships that may have previously gone unrecognized. More specifically, the tools that have been developed make better use of the third dimension that the synthetic environment provides. Whereas the plane tools make it easier to track down interesting features of a given flow field, the box tools allow the user to focus on these features and reduce the data load on the computer...|$|R
30|$|Individual {{interviews}} lasting from 90 to 120  min {{were conducted}} in Mandarin by the first author. These interviews were audio-recorded after obtaining participants’ permission. The audio-tapes of the interviews were then transcribed and translated from Mandarin into English by the first author. To enhance the descriptive validity of these interview <b>data,</b> two professional <b>translators</b> were asked to review approximately 60  % of the English translations of the interview transcripts. Both authors discussed and analyzed the translated and de-identified interview transcripts during the interpretive process.|$|R
40|$|As I was {{thinking}} about casting my vote for the Open Science Prize, I realized that I would in fact need a rubric for choosing. I was concerned that the public vote would tend towards popularity, familiarity, or bling, rather than {{the quality of the}} open science. But {{what does it mean to}} be “quality open science?” What should be the most important criteria? The different semi-finalist projects are all very different - on different topics, of varying degrees of maturity (some pre-date the competition and some do not), and targeting different audiences. If successful, each will have different societal impacts. I applaud them all. Recently, we evaluated over eighty manuscripts from PLOS to determine which ones were most significant, impactful, or otherwise representative to form the core of the new PLOS Open Data Collection. In this context, we created a rubric for evaluation and then scored each manuscript objectively. For each manuscript, what was the impact on policy change? Were ethical issues considered? Did the science advance our abilities to share data or use shared data? Did the project utilize shared data (the noble discipline of “data parasitism”) ? Was the community involved? How sexy were the figures? How much did the work foster cross-pollination of ideas and approaches across disciplines? And of course, what did people think about the work? I needed a similar rubric here, but for knowledgebases and not manuscripts. Knowledge is our collective insights, captured by experts and able to provide an explanatory framework for evaluating new observations. A knowledgebase makes that knowledge findable and computable. A recent NIH RFI: “Metrics to Assess Value of Biomedical Digital Repositories” highlighted the ineffectiveness of current knowledgebase evaluation. Traditional citation and impact factors as a measure of success or value are inadequate. For example, almost everyone in biomedicine relies on PubMed, but almost no one ever cites or mentions it in their publications. In response to this RFI, our group (consisting of the NCATS <b>Data</b> <b>Translator</b> and the Monarch Initiative) developed a rubric arranged according to the commonly cited FAIR principles [...] Findable, Accessible, Interoperable, and Reusable, but with three additional principles: Traceable, Licensed, and Connected. These latter three extensions are, in my opinion, fundamental to “quality open science”, as without them, you do not have computability, legal ability to reuse the data/knowledge, and no ability to navigate the fabric of the data landscape. Therefore, for evaluation of the open science projects, I applied the rubric we described in our response to the RFI, but with additional considerations throughout relating to the PLOS data science collection curation, and trying to take into account advances since the open science prize project began (since some projects were preexisting and backed by other funds/projects, where others were brand new). I note that this is as much an evaluation of the rubric as it is of the projects themselves. I purposefully did not watch any of the videos explaining the projects on the Open Science Prize website before performing the evaluation. I wanted to determine how well the projects themselves related their goals, content, and functionality. As a potential user of the data, I aimed to evaluate the ease of navigating the data and its access and reuse directly. Most importantly, I wanted to avoid bias where the real distinctions between projects might be obscured by video production quality, rather than highlighting each project’s genuine values and their differences. It would be all too easy to create a great video about a great idea, and then not implement a quality platform based on strong open science principles, such as open code and data access, or the FAIR+ principles: Findable, Accessible, Interoperable, and Reusable,Traceable, Licensed, and Connected. One might ask, why bother? The first reason was I wanted to determine how well the preliminary rubric we laid out in our response to the RFI might work in the real world, as we plan to write a more thorough proposal for knowledgebase/data repository evaluation in the future. The second reason is that I simply wanted the evaluation of these projects to inform the future development of the open science projects I work most on, such as the Monarch Initiative (genotype-phenotype data aggregation across species for diagnostics and mechanism discovery), Phenopackets (a new standard for exchanging computable phenotype data for any species in any context), and OpenRIF (computable representation of scholarly outputs and contribution roles to better credit non-traditional scientists). How can we all do better and learn from the Open Science competition? In other words, such a competition shouldn’t just be about the six finalists, but rather it should inform how we all go about practicing open science in general. So now you are probably wondering, which project(s) did I vote for? Well, that is for you to infer. As you review the musings below, consider your own values for what constitutes robust open science. The full text of my review is available at [URL] Comments and corrections entirely welcome on the force 11 page or tweet to @ontowonka...|$|E
40|$|This study characterizes and assesses {{alternative}} approaches to software component interoperability in distributed environments typical of C 4 ISR systems. Interoperability {{is the ability}} of systems to provide services to and accept services from other systems, and to use the services so exchanged to enable them to operate effectively together. This study characterizes and assesses {{alternative approaches}} to software component interoperability in distributed environments. Candidate approaches include wrappers, <b>translators,</b> <b>data</b> mediators, replicators, messaging, Object Request Broker (ORBs), and JINI for legacy systems and new systems...|$|R
40|$|AbstractCulturally {{specified}} {{numbers or}} numeratives have great significance in our life. People from ancient times believed in their magic power and created concepts connected with them. If we can reveal the factors, which influenced on formation of definite number concept, we would reveal national, historical essence of nation. This {{would help us}} to understand cultural specific features and peculiarities in the world perception of each culture. Therefore, we {{tried to find out}} factors which impacted on formation of numerative seven and analyzed semantics of numerative seven in different contexts. Results of analysis can used as background <b>data</b> for <b>translator</b> and it will help to overcome extra-linguistic barriers during process of translation. The data are restricted to cultural cutoms, traditions, religious ceremonies, phrasal verbs and proverbs. At the end of work there are given suggestions to interpret word combinations from Kazakh into English and from English into Kazakh...|$|R
40|$|The {{purpose of}} the web {{application}} development is the modernization of the current data acquisition and management model for new and existing translators in the company Iolar d. o. o. Previously <b>data</b> on <b>translators</b> who signed up {{to work in the}} company were entered multiple times as they were entered through several entry points. The acquired data were then manually entered into an MS Excel sheet and the Projetex program. We analyzed the current data acquisition and management model as well the new system for data acquisition and management. Based on the results obtained, we determined the architecture of the system and created a database. We created a web application through which the translators register and fill in their data. These data can later be reviewed and updated. Through an application add-in, the company personnel can now review and edit the data on all registered translators...|$|R
40|$|Abstract: Several {{goals of}} Intelligent Transportation Systems are to improve productivity, ease traffic {{congestion}} and minimize road risks {{through the use}} of advanced communications technologies. The RITIS software system developed at University of Maryland inside the Center for Advanced Transportation Technology Laboratory aims to improve communication among transportation agencies through data integration and sharing. We present the RITIS Development Framework, a software framework that aims to improve the design and maintainability of agency-to-RITIS <b>data</b> loaders and <b>translators,</b> while also minimizing the training and learning required to develop loaders for future data sources...|$|R
40|$|XML for B 2 B {{applications}} 2 This paper {{presents a}} Web based data integration methodology and tool framework, called X-TIME, {{for the development}} of Business-to-Business (B 2 B) design environments and applications. X-TIME provides a <b>data</b> model <b>translator</b> toolkit based on an extensible metamodel and XML. It allows the creation of adaptable semantics oriented metamodels to facilitate the design of wrappers or reconciliators (mediators) by taking into account several characteristics of interoperable information systems such as extensibility and composability. X-TIME defines a set of meta-types for representing meta-level semantic descriptors of data models found in the Web. The meta-types are organized in a generalization hierarchy to capture semantic similarities among modeling concepts of interoperable systems. We show how to use the X-TIME methodology to build cooperative environments for B 2 B platforms involving the integration of Web data and services. XML for B 2 B applications 3 XML integration and toolkit for B 2 B application...|$|R
