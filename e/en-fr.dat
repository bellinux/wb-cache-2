10|0|Public
5000|$|Nicole Brenez, French Experimental Cinema (<b>en-fr),</b> Mubi.com, 2010-2011 ...|$|E
5000|$|Rodolphe Olcèse, The filmic {{experience}} of the world (offline, <b>en-fr),</b> Art Press 2, n°21, 2011, Paris ...|$|E
40|$|This paper {{describes}} the monomodal and multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT 17 Shared Task on Multimodal Translation. We mainly explored two multimodal architectures where either global visual features or convolutional feature maps are integrated {{in order to}} benefit from visual context. Our final systems ranked first for both En-De and <b>En-Fr</b> language pairs according to the automatic evaluation metrics METEOR and BLEU. Comment: MMT System Description Paper for WMT 1...|$|E
40|$|This paper {{describes}} the Dublin City University terminology translation system used for our {{participation in the}} query translation subtask in the medical trans-lation task in the Workshop on Statisti-cal Machine Translation (WMT 14). We deployed six different kinds of terminol-ogy extraction methods, and participated in three different tasks: FR–EN and EN– FR query tasks, and the CLIR task. We obtained 36. 2 BLEU points absolute for FR–EN and 28. 8 BLEU points absolute for <b>EN–FR</b> tasks where we obtained the first place in both tasks. We obtained 51. 8 BLEU points absolute for the CLIR task. ...|$|E
30|$|We {{establish}} that language dyads do not edit {{articles about the}} same concept (co-occur) by chance. Large editions share concepts more frequently than expected: although in the data EN-DE and <b>EN-FR</b> overlap in 45 % of cases, only 15 % is expected by the null model. To little surprise, the amount of overlap between editions in the data decreases {{with the size of}} the editions. One notable exception is the Japanese edition which, despite being among the ten largest Wikipedias, co-occurs with other top editions noticeably less frequently. Similarly, the Uzbek edition, being among the ten smallest in the dataset, shows high concept overlap with large editions. By simply plotting frequencies of co-occurrences, we do not observe any local blocks or clusters, neither among large nor small editions (see Figure A 1 in Additional file 1).|$|E
40|$|In {{this paper}} {{we present a}} hybrid {{statistical}} machine translation (SMT) -example-based MT (EBMT) system that shows significant improvement over both SMT and EBMT baseline systems. First we present a runtime EBMT system using a subsentential translation memory (TM). The EBMT system is further combined with an SMT system for effective hybridization of the pair of systems. The hybrid system shows significant improvement in translation quality (0. 82 and 2. 75 absolute BLEU points) for two different language pairs (English–Turkish (En–Tr) and English– French (<b>En–Fr))</b> over the baseline SMT system. However, the EBMT approach suffers from significant time complexity issues for a runtime approach. We explore two methods {{to make the system}} scalable at runtime. First, we use an heuristic-based approach. Secondly, we use an IR-based indexing technique to speed up the time-consuming matching procedure of the EBMT system. The index-based matching procedure substantially improves run-time speed without affecting translation quality. ...|$|E
40|$|Recent work on {{end-to-end}} neural network-based architectures for {{machine translation}} has shown promising results for <b>En-Fr</b> and En-De translation. Arguably, {{one of the}} major factors behind this success has been the availability of high quality parallel corpora. In this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. Compared to a phrase-based and hierarchical baseline, we obtain up to $ 1. 96 $ BLEU improvement on the low-resource language pair Turkish-English, and $ 1. 59 $ BLEU on the focused domain task of Chinese-English chat messages. While our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as Cs-En and De-En where we obtain an improvement of $ 0. 39 $ and $ 0. 47 $ BLEU scores over the neural machine translation baselines, respectively. Comment: 9 pages, 2 figure...|$|E
40|$|In this paper, we extend an attention-based neural machine {{translation}} (NMT) model by allowing it to access an entire training set of parallel sentence pairs even after training. The proposed approach consists of two stages. In the first stage [...] retrieval stage [...] , an off-the-shelf, black-box search engine is used to retrieve a small subset of sentence pairs from a training set given a source sentence. These pairs are further filtered based on a fuzzy matching score based on edit distance. In the second stage [...] translation stage [...] , a novel translation model, called translation memory enhanced NMT (TM-NMT), seamlessly uses both the source sentence {{and a set of}} retrieved sentence pairs to perform the translation. Empirical evaluation on three language pairs (<b>En-Fr,</b> En-De, and En-Es) shows that the proposed approach significantly outperforms the baseline approach and the improvement is more significant when more relevant sentence pairs were retrieved. Comment: 8 pages, 4 figures, 2 table...|$|E
40|$|Translation {{capability}} of a Phrase-Based Statistical Machine Translation (PBSMT) system mostly depends on parallel data and phrases {{that are not}} present in the training data are not correctly translated. This paper describes a method that efficiently expands the existing knowledge of a PBSMT system without adding more parallel data but using external morphological resources. A set of new phrase associations is added to translation and reordering models; each of them corresponds to a morphological variation of the source/target/both phrases of an existing association. New associations are generated using a string similarity score based on morphosyntactic information. We tested our approach on <b>En-Fr</b> and Fr-En translations and results showed improvements of the performance in terms of automatic scores (BLEU and Meteor) and reduction of out-of-vocabulary (OOV) words. We believe that our knowledge expansion framework is generic and {{could be used to}} add different types of information to the model. JRC. G. 2 -Global security and crisis managemen...|$|E
40|$|We first {{observe a}} {{potential}} weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding {{more than one}} meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, {{in this paper we}} propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on <b>En-Fr</b> and En-De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly. Comment: 13 pages, 2 figure...|$|E

