5|9|Public
40|$|Cloud {{applications}} are growing {{more and more}} complex be-cause of user mobility, hardware heterogeneity, and multi-component nature. Today’s cloud infrastructure paradigm, based on distant data centers {{are not able to}} provide con-sistent performance and low enough communication latency for future applications. These discrepancies can be accom-modated using existing large-scale distributed cloud infras-tructure, also known as <b>Infinite</b> <b>Cloud,</b> which is amalgam of several Data Centres hosted by a Telecom Network. The <b>Infinite</b> <b>Cloud</b> provides opportunity for applications with high capacity, high availability, and low latency. The <b>Infinite</b> <b>Cloud</b> infrastructure and federated cloud paradigms intro-duce several challenges due to the heterogeneous nature of the resources of di↵erent scale, latencies due to geographi-cal locations and dynamic workload, to better accommodate distributed applications with increased diversity. Managing a vast heterogeneous infrastructure of this nature can no longer be done manually. Autonomous, distributed, collab-orative, and self-configuring systems need to be developed to manage the resources of the <b>Infinite</b> <b>Cloud</b> in order to meet application Service Level Agreements (SLAs), and the operators ’ internal management objectives, [8]. In this pa-per, we discuss some of the associated research challenges for such a system by formulating an optimization problem based on its constituent cost models. The decision maker takes into account the computational complexity as well as stability of the optimal solution...|$|E
40|$|The {{effects of}} finite and semi-infinite cloud shape on diurnal {{satellite}} observations were investigated by simulating diurnal satellite observations of a finite cubic and semi-infinite cloud and comparing these results with actual diurnal satellite observations of cloud fields. Theoretical satellite observations {{were created by}} computing relative radiances from a theoretical model which used the Monte Carlo method to simulate the scattering of solar radiation by a finite and <b>infinite</b> <b>cloud</b> for optical depths of 73. 5 and 20. 0. The diurnal variations in radiance for an infinite and finite cubic cloud viewed at a zenith angle (e) of 0 ° were both symmetric about local noon when the maximum radiance occurred. When viewing the same clouds from an average zenith angle {e) of 18 ° to 31 ° (measured westward from zenith), the diurnal variation in radiance for the <b>infinite</b> <b>cloud</b> was again nearly symmetric about local noon when the maximum radiance occurred. The finite cubic cloud...|$|E
40|$|Recent {{high quality}} {{spectral}} observations {{have allowed the}} derivation of constraints on the atmospheric structure of Uranus. The present analysis, {{which is based on}} the detailed modeling of a broadband geometric albedo spectrum and high resolution observations of the H 2 4 - 0 quadrupole and 6818. 9 -A CH 4 features, yields (1) a family of models which parameterize an upper tropospheric haze layer, (2) a lower, optically <b>infinite</b> <b>cloud</b> at a given pressure level, (3) the cloud-level methane molar fraction, and (4) the mean ortho/para ratio in the visible atmosphere. The single scattering albedo of atmospheric aerosols exhibits a steep darkening between 5890 and 6040 A...|$|E
3000|$|In 2008 the {{editor-in-chief}} of Wired, a popular magazine {{with an eye on}} the latest trends in technology, stated that [...] "faced with massive data, this approach to science - hypothesize, model, test - is becoming obsolete." [...] What he proposes instead is the [...] "petabyte age: Sensors everywhere. <b>Infinite</b> storage. <b>Clouds</b> of processors. Our ability to capture, warehouse, and understand massive amounts of data is changing science, medicine, business, and technology. As our collection of facts and figures grows, so will the opportunity to find answers to fundamental questions. Because in the era of big data, more isn't just more. More is different." [...] [...]...|$|R
40|$|We {{construct}} a steady analytic accretion flow {{model for a}} finite rotating gas cloud that accretes material to a central gravitational object. The pressure gradients of the flow {{are considered to be}} negligible and so, the flow is ballistic. We also assume a steady flow and consider the particles at the boundary of the spherical cloud to be rotating as a rigid body, with a fixed amount of inwards radial velocity. This represents a generalisation to the traditional <b>infinite</b> gas <b>cloud</b> model described by Ulrich (1976). We show that the streamlines and density profiles obtained deviate largely from the ones calculated by Ulrich. The extra freedom in the choice of the parameters on the model can naturally account for the study of protostars formed in dense clusters by triggered mechanisms, where a wide variety of external physical mechanisms determine the boundary conditions. Also, as expected, the model predicts the formation of an equatorial accretion disc about the central object with a radius different from the one calculated by Ulrich (1976). Comment: 9 pages, 9 figures, smaller corrections, a better mathematical approach to the problem and astrophysical applications added. Accepted for publication in MNRA...|$|R
40|$|Abstract Cloud {{computing}} {{has recently}} become an attrac-tive solution for massively multiplayer online games, {{also known as}} MMOGs, as it lifts operators from the burden of buying and maintaining large amount of computational, storage and communication resources, while offering the il-lusion of <b>infinite</b> scalability. Yet, <b>cloud</b> resources do not come for free: a careful orchestration is needed to minimize the economical cost. This paper proposes a novel architecture for MMOGs that combines an elastic cloud infrastructure with user-provided resources, to boost both the scalability and the economical sustainability provided by cloud com-puting. Our system dynamically reconfigures the platform while managing the trade-off between economical cost and quality of service, exploiting user-provided resources when-ever possible. Simulation results show that a negligible re-duction {{in the quality of}} service can reduce the cost of the platform up to 60 % percent...|$|R
40|$|Recently, {{biological}} applications {{start to}} be reimplemented into the applications which exploit many cores of GPUs for better computation performance. Therefore, by providing virtualized GPUs to VMs in cloud computing environment, many biological applications will willingly move into cloud environment {{to enhance their}} computation performance and utilize <b>infinite</b> <b>cloud</b> computing resource while reducing expenses for computations. In this paper, we propose a BioCloud system architecture that enables VMs to use GPUs in cloud environment. Because much of the previous {{research has focused on}} the sharing mechanism of GPUs among VMs, they cannot achieve enough performance for biological applications of which computation throughput is more crucial rather than sharing. The proposed system exploits the pass-through mode of PCI express (PCI-E) channel. By making each VM be able to access underlying GPUs directly, applications can show almost the same performance as when those are in native environment. In addition, our scheme multiplexes GPUs by using hot plug-in/out device features of PCI-E channel. By adding or removing GPUs in each VM in on-demand manner, VMs in the same physical host can time-share their GPUs. We implemented the proposed system using the Xen VMM and NVIDIA GPUs and showed that our prototype is highly effective for biological GPU applications in cloud environment...|$|E
40|$|An {{error model}} for Bayesian Monte Carlo {{retrieval}} algorithms which explicitly accounts for uncertainty {{introduced by the}} use of a finite database of realizations, as well as uncertainties associated with the modelling and measurement components of the retrieval is described. The model provides a rigorous estimate of the uncertainty in all retrieved parameters as well as a breakdown of this uncertainty into two components attributable to an imperfect database, and modelling and measurement uncertainties, respectively. This error information is critical for algorithm development, model validation and, in particular, in variational data assimilation where the relative accuracy of the observations and the background forecast determines how much the latter is modified in the assimilation process. Using the error model, uncertainties in the Tropical Rainfall Measuring Mission (TRMM) Microwave Imager (TMI) instantaneous surface rainfall product (2 A 12) are found to range from 40 to 60 percent in rainfall up to 20 mm h− 1. In heavier rain, uncertainties rapidly increase due to heavy attenuation in all TMI channels which requires surface rainfall to be inferred solely from the non-unique relationship between surface rain rate and the ice-scattering signature in the strongly coupled 37 and 85 GHz brightness temperatures. In light rain, the fact that the database attempts to approximate nature’s <b>infinite</b> <b>cloud</b> probability density function by a finite set of realizations, and the inherent inability of the TMI brightness temperatures to completely distinguish between al...|$|E
40|$|Spectral {{profiles}} of the H 2 S 4 (0) and S 4 (1) lines are presented for Neptune on three consecutive nights; no variation is detected in the equivalent widths of the H 2 4 - 0 features to within an observational uncertainty of about 20 percent. Comparisons with previous H 2 quadrupole observations indicate that no secular trends have been detected over about 15 yr. The equivalent-width error limits are interpreted {{in terms of the}} maximum variability of Neptunian tropospheric aerosols. Specifically, the error bars for the globally averaged equivalent widths of the two H 2 quadrupole absorption features constrain the bottom of the visible atmosphere, as defined by a bright optically <b>infinite</b> isotropically scattering <b>cloud,</b> to be 2. 9 + or - 0. 6 bars, while the methane haze opacity is constrained to be 0. 30 + or - 0. 25...|$|R
40|$|International audienceMatching and {{scheduling}} problem {{proved to be}} a critical problematic in different domains including Cloud computing. Therefore, to ensure the commercial success of the Cloud computing paradigm, it is necessary to develop methods that allow users to optimize the use of resources. Even though there are several algorithms for scheduling applications in heterogeneous environment such as grid computing, they cannot benefit from the recent advent of Cloud computing. Indeed, these algorithms assume that the number of resources available to users is bounded, this is against the illusion of <b>infinite</b> resources of <b>Cloud</b> computing. Also, only the execution time (makespan) is taken into account. However, Cloud computing business model is based on pay as you go. Accordingly, execution cost begot using a set of resources should be considered. To overcome the limitations of existing works, this paper propose new strategies for matching {{and scheduling}} business process instances in the Cloud context. The proposed strategies aim at scheduling business process instances while minimizing two conflicting criteria on the one hand, and ensuring fairness between the considered instances on the other hand. A serie of experiments demonstrate that they present good performance...|$|R
40|$|The {{observed}} astrophysical {{phenomenon of}} dark matter has generated new interest in the problem of whether the principles underlying QFT are consistent with invisibility/inertness of energy-momentum carrying "stuff" as e. g. "unparticles". We show that the 2 -dim. model which {{has been used to}} illustrate the meaning of unparticles belongs to the class of former infraparticles models and the latter are known to describe electrically charged particles in d= 1 + 3 which despite their nonlocality are our best particle physics "candles". The "invisibility" in this case refers to the <b>infinite</b> infrared photon <b>cloud</b> with energies below the resolution and can be made arbitrarily small by increasing the photon registering sensitivity. This is not quite the kind of invisibility which the unparticle community attributes to their invisible "stuff" and which would probably contradict the asymptotic completeness property. The main aim of the present work is to show that knowledge about this part of QFT {{is still in its infancy}} and express the hope that the work on unparticles may rekindle a new interest in conceptually subtle old unsolved important problems. Comment: 23 pages, Addition of many references and improvements of formulation...|$|R
60|$|So {{the little}} cubs took their {{first glimpse of}} the big world, of {{mountains}} and sea and sunshine, and children playing on the shore, and the world was altogether too wonderful for little heads to comprehend. Nevertheless one plain impression remained, the same that you see in the ears and nose and stumbling feet and wagging tail of every puppy-dog you meet on the streets, that this bright world is a famous place, just made a-purpose for little ones to play in. Sitting on their tails in a solemn row the wolf cubs bent their heads and pointed their noses gravely at the sea. There it was, all silver and blue and boundless, with tiny white sails dancing over it, winking and flashing like entangled bits of sunshine; and since the eyes of a cub, like those of a little child, cannot judge distances, one stretched a paw at the nearest sail, miles away, to turn it over and make it go the other way. They turned up their heads sidewise and blinked at the sky, all blue and calm and <b>infinite,</b> with white <b>clouds</b> sailing over it like swans on a limpid lake; and one stood up on his hind legs and reached up both paws, like a kitten, to pull down a cloud to play with. Then the wind stirred a feather near them, the white feather of a ptarmigan which they had eaten yesterday, and forgetting the big world and the sail and the cloud, the cubs took to playing with the feather, chasing and worrying and tumbling over each other, while the gaunt old mother wolf looked down from her rock and watched and was satisfied.|$|R
40|$|Cloud {{computing}} {{has given}} users access to virtually <b>infinite</b> computing power. <b>Cloud</b> service providers operate data centers consisting of hundreds, sometimes thousands of interconnected machines. The machines dissipate heat that is undesirable {{because it may}} lead components to overheat. Air conditioners are used to cool the air and regulate {{the temperature in the}} data center. Nerdalize is a company that aims to create a cloud by replacing servers in a data center with CloudBoxes in people's homes. A CloudBox contains servers and a boiler. The heat dissipated by the servers is used to warm up the water in the boiler and complement the central heating system. The challenge set out by Nerdalize is to develop a workflow management system to run on their infrastructure while making optimal use of generated heat. Many workflow management systems exist but none supporting a containers based environment. Flower is a workflow management system in a containerized environment with heat-aware scheduling. Flower consists of several components. The first component is the workflow management system responsible for executing a workflow on a cluster of machines. The second part contains various temperature-based scheduling policies. To {{the best of our knowledge}} we are the first ones to introduce the notion of heat when making scheduling decisions. The final Flower component is a workflow visualizer to monitor workflow execution. To manage product development, we made use of the agile development methodology Scrum. Further assistance was provided by multiple tools, such as Travis CI for continuous integration and GitHub for team communication. During the project we have had extensive conversations with employees from Nerdalize and professionals from companies like Google, RedHat, CoreOS and Amadeus. Electrical Engineering, Mathematics and Computer ScienceDistributed SystemsTI 380...|$|R
40|$|We are {{witnessing}} {{the transition of}} telecom services to cloud, but real-time and high QoS requirements make the transition a slow and difficult process. Through its successful application in IT world, {{we have seen that}} cloud offers numerous benefits, including cost reduction, scalability and automation. Furthermore, constantly decreasing hardware costs and auto-scaling capability give an illusion of having <b>infinite</b> resources in <b>cloud.</b> Thus, planning the capacity of a cloud seems unnecessary. In fact, {{it is one of the}} most underestimated problems in cloud computing. In order to guarantee service quality to their users, telecom service providers will need to manage network capacity in cloud as well. While in traditional network setting, where resources are static and dedicated, capacity planning involves estimating the optimal amount of hardware equipment to be purchased; in dynamic virtualized environment, it means estimating the amount of needed virtual resources. The main objectives of this thesis work are to: explore traditional network capacity planning and the current cloud capacity planning approaches, compare them in order to define the main requirements for a tool that will manage NW capacity in cloud; and to create and test a prototype based on these requirements. The used research methodology includes literature review, survey and a case study. Based on the research results it was concluded that there are many challenges in network capacity management, such as excessive manual work and the absence of an end-to-end tool. In cloud, solutions need to be auto-scalable, to react in real-time and have integrated performance monitor. For telecom, the biggest challenges in cloud are retaining high QoS and avoiding service interruption. These are the most important factors to be taken into account in capacity planning process. Thus, a network capacity planning tool primarily needs to addresses performance vs. scalability trade-off...|$|R

