196|31|Public
50|$|<b>Knowledge</b> <b>compilation</b> is {{a family}} of {{approaches}} for addressing the intractability ofa number of artificial intelligence problems.|$|E
50|$|His {{research}} interests include tractable inference, knowledge representation, stochastic search methods, theory approximation, <b>knowledge</b> <b>compilation,</b> planning, default reasoning, satisfiability solvers like WalkSAT, and connections between computer science and statistical physics (phase transition phenomena).|$|E
50|$|Making {{sustainable}} consumption choices are {{significantly related to}} the role of habit and routine behaviours. Habits {{can be thought of as}} procedural strategies to reduce the cognitive effort associated with making choices, particularly in situations that are relatively stable. They allow us to perform routine actions with a minimum of deliberation and often only limited awareness. Moreover, the evidence suggests that habit is a crucial component in a wide variety of environmentally-significant activities: travel behaviour, shopping patterns, household chores, waste disposal, leisure activities, and even personal hygiene. Habits are formed through repetition and reinforcement. Andersen (1982) identifies three stages in the formation of a new habit. The first stage, or declarative stage, involves information processing relating to a particular choice or action. At this stage the attitudinal and affective responses to this information are both important. The information challenges the existing choice, but at this stage does not actually change coffee-buying behaviour. In the second <b>knowledge</b> <b>compilation</b> stage, however, this information is converted into a new routine by exercising a different choice in practice. When the action itself is associated with a clear positive reinforcement, and repeated over time, a ‘cognitive script’ is developed which enables to repeat the same action in similar circumstances with very little cognitive effort. This final procedural stage locks into a new coffee-buying habit and virtually without thinking now the ethically traded coffee is tossed into the supermarket trolley week after week. At this stage, the behaviour is more or less automatized and bypasses rational deliberation almost completely.|$|E
5000|$|The Sword of <b>Knowledge</b> (1995) - <b>compilation</b> of {{the three}} titles in a single volume ...|$|R
50|$|Screen Sinatra is {{an album}} {{featuring}} songs by Frank Sinatra from various movies {{to which he}} has contributed. The tracks were mostly recorded between 1953 and 1960, though the final track—“Dream”—comes from the 1971 film Carnal <b>Knowledge.</b> The <b>compilation</b> was released in 1989 in a box set by EMI and was released in the United States by Capitol Records in 1996.|$|R
40|$|We {{describe}} the software environment {{that has been}} developed for the management and support of planning missions in the SAR domain. We have chosen to develop a configurable environment that simplifies the decision making process to the RCC staff. We also discuss the relation of this project with the Knowledge Management activities, especially {{with respect to the}} functions of <b>knowledge</b> creation, <b>compilation,</b> dissemination and application. We conside...|$|R
40|$|Researchers in {{the area}} of <b>knowledge</b> <b>compilation</b> are {{developing}} general purpose techniques for improving the efficiency of knowledge-based systems. In this article, an attempt is made to define <b>knowledge</b> <b>compilation,</b> to characterize several classes of <b>knowledge</b> <b>compilation</b> techniques, and to illustrate how some of these techniques can be applied to improve the performance of model-based reasoning systems...|$|E
40|$|<b>Knowledge</b> <b>compilation</b> {{by using}} the {{extension}} rule (KCER) has been recognized as a novel compilation approach, although this method can only deal with static knowledge bases. This paper proposes a novel incremental <b>knowledge</b> <b>compilation</b> method {{by using the}} extension rule {{so that it can}} tackle <b>knowledge</b> <b>compilation</b> problems in the dynamic environment. The method does not recompile the whole knowledge base, while it is to use the information of compiling original knowledge base to speed up the compiling process. The experimental results show that this method can deal with dynamic knowledge bases efficientl...|$|E
40|$|<b>Knowledge</b> <b>compilation</b> {{converts}} Boolean formulae {{for which}} some inference tasks are computationally expensive into a representation where the same tasks are tractable. ProbLog is a state-of-the-art Probabilistic Logic Programming system that uses <b>knowledge</b> <b>compilation</b> {{to reduce the}} expensive probabilistic inference to an efficient weighted model counting. Motivated to improve ProbLog's performance we present an approach that optimizes Boolean formulae in order to speed-up <b>knowledge</b> <b>compilation.</b> We identify 7 Boolean subformulae patterns {{that can be used}} to re-write Boolean formulae. We implemented an algorithm with polynomial complexity which detects and compacts 6 of these patterns. We employ our method in the inference pipeline of ProbLog and conduct extensive experiments. We show that our compaction method improves <b>knowledge</b> <b>compilation</b> and consecutively the overall inference performance. Furthermore, using compaction reduces the number of time-outs, allowing us to solve previously unsolvable problems. status: publishe...|$|E
40|$|Dynamic linking lets {{programs}} use {{the most}} recent versions of classes without re-compilation. In Java and. NET, bytecode specifies which classes should be dynamically linked. This information represents the compiler's <b>knowledge</b> of the <b>compilation</b> environment, but the execution environment might be di#erent...|$|R
40|$|Abstract. We {{describe}} the software environment {{that has been}} developed for the management and support of planning missions in the SAR domain. We have chosen to develop a configurable environment that simplifies the decision making process to the RCC staff. We also discuss the relation of this project with the Knowledge Management activities, especially {{with respect to the}} functions of <b>knowledge</b> creation, <b>compilation,</b> dissemination and application. We consider SARPA as a valuable tool that facilitates the organization to pursue the goals of success and efficiency. ...|$|R
40|$|Consider an {{intelligence}} analyst {{who has a}} large body of documents of various kinds. He would like answers to some of his questions based on the information in these documents, general <b>knowledge</b> available in <b>compilations</b> such as fact books, and commonsense. A search engine or a typical information retrieval (IR) system like Googl...|$|R
40|$|Abstract. In {{this paper}} we address the {{application}} of <b>knowledge</b> <b>compilation</b> techniques to product configuration problems. We argument that both the process of generating valid configurations, as well as validation of product configuration knowledge bases, can potentially be accelerated by compiling the instance independent part of the knowledge base. Besides giving transformations of both tasks into logical entailment problems, we give a short summary on <b>knowledge</b> <b>compilation</b> techniques, and present a new algorithm for computing unit-resolution complete knowledge bases. ...|$|E
40|$|In this paper, {{we study}} the <b>knowledge</b> <b>compilation</b> task for propositional epistemic logic S 5. We first extend {{many of the}} queries and transformations {{considered}} in the classical <b>knowledge</b> <b>compilation</b> map to S 5. We then show {{that the notion of}} disjunctive normal form (DNF) can be profitably extended to the epistemic case; we prove that the DNF fragment of S 5, when appropri-ately defined, satisfies essentially the same queries and transformations as its classical counterpart. 1...|$|E
40|$|This work {{describes}} a problem, knowledge contention, that arises when agent knowledge learned via <b>knowledge</b> <b>compilation</b> contends with the agent's original task decomposition {{knowledge in the}} execution of agent actions. We show {{the circumstances under which}} the problem occurs, present a solution that avoids knowledge contention, and provide empirical results that show that <b>knowledge</b> <b>compilation</b> under the proposed solution leads to agents that can integrate planning, plan execution, and learning without explicit knowledge design for learning...|$|E
5000|$|In 1835 he {{published}} A treatise on friendly societies {{in which the}} doctrine of the interest of money and the doctrine of probability are applied through Society for the Diffusion of Useful <b>Knowledge,</b> a <b>compilation</b> of information related to the illnesses and mortality rates of the working class of England. According to the Oxford Dictionary of National Biography, this treatise [...] "was probably the first rigorous examination of the subject", contributing to the professional success that ultimately led to his being known as [...] "the father of his profession" [...] in the final years of his life.|$|R
40|$|The eX treme Processing Platform (XPP) is {{a unique}} recon gurable {{computing}} (RC) architecture supported by {{a complete set of}} design tools. This paper presents the XPP V ectorizing C C ompiler XPP-VC, the rst high-level compiler for this architecture. It uses new mapping techniques, combined with ecient vectorization. A temporal partitioning phase guarantees the compilation of programs with unlimited complexity, provided that only the supported C subset is used. A new loop partitioning scheme permits to map large loops of any kind. It is not constrained by loop dependences or nesting levels. To our <b>knowledge,</b> the <b>compilation</b> performance is unmatched by any other compiler for RC...|$|R
40|$|This paper {{describes}} the testing methodology used in ANSI C efficiency testing, along with observations regarding the resulting measurements. The {{results of the}} measurements are included followed by conclusions regarding which algorithms have the most consistent performance across different platforms. Some <b>knowledge</b> regarding <b>compilation</b> and processor architectures is useful in understanding how the data was derived. However, the raw data in the document may be useful without necessarily understanding the derivation. The testing described in this paper {{is similar to that}} done in Round 1. The testing has obviously been restricted to the five Round 2 candidates. Additionally, Timing Tests for the Pentium based platforms has been omitted in favor of Cycle Count testing (see Section 3). 2. Scop...|$|R
40|$|<b>Knowledge</b> <b>compilation</b> {{is one of}} {{the more}} {{traditional}} approaches to model-based diagnosis, where a compiled system model is obtained in an offline phase, and then used to efficiently answer diagnostic queries in an on-line phase. We revisit this approach in this paper in light of recent advances in <b>knowledge</b> <b>compilation,</b> where we contrast two specific approaches based on compiling system descriptions into decomposable negation normal form (DNNF) and ordered binary decision diagrams (OBDDs). The comparison is along two dimensions: the succinctness of compiled system descriptions and the complexity of on-line diagnostic queries. Specifically, we show that system models can generally be more succinct when compiled into DNNF, and that DNNF operations required for on-line diagnosis incur lower complexities than their OBDD counterparts. Our discussion is supported by theoretical results from <b>knowledge</b> <b>compilation</b> and empirical evaluations using stateof-the-art DNNF and OBDD compilers. ...|$|E
40|$|Several {{artificial}} intelligence architectures and systems based on "deep" models of a domain have been proposed, in particular for the diagnostic task. These systems have several advantages over traditional knowledge based systems, {{but they have}} a main limitation in their computational complexity. One of the ways to face this problem is to rely on a <b>knowledge</b> <b>compilation</b> phase, which produces knowledge that can be used more effectively with respect to the original one. In this paper we show how a specific <b>knowledge</b> <b>compilation</b> approach can focus reasoning in abductive diagnosis, and, in particular, can improve the performances of AID, an abductive diagnosis system. The approach aims at focusing the overall diagnostic cycle in two interdependent ways: avoiding the generation of candidate solutions to be discarded a-posteriori and integrating the generation of candidate solutions with discrimination among different candidates. <b>Knowledge</b> <b>compilation</b> is used off-line to produce operational [...] ...|$|E
40|$|Introduced by Darwiche (2011), sentential {{decision}} diagrams (SDDs) {{are essentially}} as tractable as ordered binary decision diagrams (OBDDs), but {{tend to be}} more succinct in practice. This makes SDDs a prominent representation language, with many applications in artificial intelligence and <b>knowledge</b> <b>compilation.</b> We prove that SDDs are more succinct than OBDDs also in theory, by constructing a family of boolean functions where each member has polynomial SDD size but exponential OBDD size. This exponential separation improves a quasipolynomial separation recently established by Razgon (2013), and settles an open problem in <b>knowledge</b> <b>compilation...</b>|$|E
40|$|Isidore of Seville (560 — 636) was {{a crucial}} figure in the {{preservation}} and sharing of classical and early Christian <b>knowledge.</b> His <b>compilations</b> {{of the works of}} earlier authorities formed an essential part of monastic education for centuries. Due to the vast amount of information he gathered and its wide dissemination in the Middle Ages, Pope John Paul II even named Isidore the patron saint of the Internet in 1997. This volume represents a cross section of the various approaches scholars have taken toward Isidore’s writings. The essays explore his sources, how he selected and arranged them for posterity, and how his legacy was reflected in later generations’ work across the early medieval West. Rich in archival detail, this collection provides a wealth of interdisciplinary expertise on one of history’s greatest intellectuals...|$|R
40|$|Abstract. This paper {{presents}} a tutoring system aimed at teaching how to compile Java into {{the language of}} the Java Virtual Machine, and, at the same time, promotes {{a better understanding of the}} underlying mechanisms of object-oriented programming. The interaction with the systems takes the form of a 3 D videogame where the student must compete to provide the right machine instructions, collect resources needed by the instructions and use her <b>knowledge</b> about Java <b>compilation</b> to find the best strategy. ...|$|R
40|$|The use of {{bibliometrics}} indicators {{to study}} research activity {{is based on}} which the scientific publications are essential product of this activity, and provide information about the research process, its volume, evolution, visibility and structure. So, they allow to value the scientific activity, and influences (or impact) {{of the work and}} the sources. The bibliometrics studies, altogether with other indicators, allows an objective quantification of the knowledge, and are harnessed by the present explosion of the <b>knowledge</b> and its <b>compilation</b> in bibliographical data bases...|$|R
40|$|Abstract. The goal of <b>knowledge</b> <b>compilation</b> is {{to enable}} fast queries. Prior {{approaches}} had {{the goal of}} small (i. e., polynomial {{in the size of}} the initial knowledge bases) compiled knowledge bases. Typically, query-response time is linear, so that the efficiency of querying the compiled knowledge base depends on its size. In this paper, a target for <b>knowledge</b> <b>compilation</b> called the ri-trie is introduced; it has the property that even if they are large they nevertheless admit fast queries. Specifically, a query can be processed in time linear {{in the size of the}} query regardless of the size of the compiled knowledge base. ...|$|E
40|$|<b>Knowledge</b> <b>compilation</b> is {{concerned}} with compiling problems encoded in some input language into some tractable, output language, {{with the goal of}} allowing one to solve such problems efficiently if the compilation is successful. This paradigm was originally motivated by the need to push much of the computational overhead into an offline compilation phase, which can then be amortized over a large number of queries in an online computation phase. In this dissertation, we study various new approaches to enhance the offline compilation phase, both theoretically and practically. We also study <b>knowledge</b> <b>compilation</b> from a perspective where it is employed as a general methodology for computation instead of just a paradigm that {{is concerned}} with the offline/online divide. In particular, we introduce a hierarchy of complexity parameters to bound the sizes of compiled representations. These new parameters are based on incorporating the logical content of the input representations, as opposed to existing parameters (e. g., treewidth) that are only based on the structure of the input. Our results improve some of the best known upper bounds on the compilation of influential representations, such as DNNFs, SDDs, and OBDDs. Moreover, we develop two new practical compilation algorithms for the DNNF and SDD languages, leading to orders of magnitude faster compilations. Finally, we study solving Beyond-NP problems using <b>knowledge</b> <b>compilation,</b> while particularly extending the reach of <b>knowledge</b> <b>compilation</b> to tackling problems in the highly intractable complexity class PP^PP. Our results show the applicability of knowledge compilers as black-box tools for solving Beyond-NP problems, similar to the use of SAT solvers for solving NP-complete problems...|$|E
40|$|Deliverable D 2. 1. 2 (WP 2. 1) This {{deliverable}} shows examples about approximating symbolic inference {{engines in}} a Semantic Web environment. Approaches of language weakening, <b>knowledge</b> <b>compilation,</b> and approximated deduction are presented. The last one is evaluated in practical applications with mixed results...|$|E
40|$|Abstract—In this {{position}} paper, we present ideas {{about creating a}} next generation framework towards an adaptive interface for data communication and visualisation systems. Our objective {{is to develop a}} system that accepts large data sets as inputs and provides user-centric, meaningful visual information to assist owners to make sense of their data collection. The proposed framework comprises four stages: (i) the <b>knowledge</b> base <b>compilation,</b> where we search and collect existing state-of-the-art visualisation techniques per domain and user preferences; (ii) the development of the learning and inference system, where we apply artificial intelligence techniques to learn, predict and recommend new graphic interpretations (iii) results evaluation; and (iv) reinforcement and adaptation, where valid outputs are stored in our knowledge base and the system is iteratively tuned to address new demands. These stages, as well as our overall vision, limitations and possible challenges are introduced in this article. We also discuss further extensions of this framework for other knowledge discovery tasks. I...|$|R
40|$|This paper {{explores the}} {{usefulness}} of a technique from software engineering, namely code instrumentation, {{for the development of}} large-scale natural language grammars. Information about the usage of grammar rules in test sentences is used to detect untested rules, redundant test sentences, and likely causes of overgeneration. Results show that less than half of a large-coverage grammar for German is actually tested by two large testsuites, and that 10 - 30 % of testing time is redundant. The methodology applied {{can be seen as a}} re-use of grammar writing <b>knowledge</b> for testsuite <b>compilation.</b> Comment: 6 pages, LaTeX 2...|$|R
40|$|In this {{position}} paper, we present ideas {{about creating a}} next generation framework towards an adaptive interface for data communication and visualisation systems. Our objective {{is to develop a}} system that accepts large data sets as inputs and provides user-centric, meaningful visual information to assist owners to make sense of their data collection. The proposed framework comprises four stages: (i) the <b>knowledge</b> base <b>compilation,</b> where we search and collect existing state-ofthe-art visualisation techniques per domain and user preferences; (ii) the development of the learning and inference system, where we apply artificial intelligence techniques to learn, predict and recommend new graphic interpretations (iii) results evaluation; and (iv) reinforcement and adaptation, where valid outputs are stored in our knowledge base and the system is iteratively tuned to address new demands. These stages, as well as our overall vision, limitations and possible challenges are introduced in this article. We also discuss further extensions of this framework for other knowledge discovery tasks. Comment: The 9 th IEEE International Conference on Big Data Science and Engineering (IEEE BigDataSE- 15), pp. 128 - 135, 201...|$|R
40|$|We {{propose a}} {{perspective}} on <b>knowledge</b> <b>compilation</b> which calls for analyzing different compilation approaches according to two key dimensions: the succinctness of the target compilation language, and the class of queries and transformations that the language supports in polytime. We then provide a <b>knowledge</b> <b>compilation</b> map, which analyzes {{a large number of}} existing target compilation languages according to their succinctness and their polytime transformations and queries. We argue that such analysis is necessary for placing new compilation approaches within the context of existing ones. We also go beyond classical, flat target compilation languages based on CNF and DNF, and consider a richer, nested class based on directed acyclic graphs (such as OBDDs), which we show to include a relatively large number of target compilation languages. ...|$|E
40|$|Abstract. The reduced {{implicate}} trie, {{introduced in}} [11], is a data structure {{that may be}} used as a target language for <b>knowledge</b> <b>compilation.</b> It has the property that, even when large, it guarantees fast response to queries. Specifically, a query can be processed in time linear {{in the size of the}} query regardless of the size of the compiled knowledge base. The <b>knowledge</b> <b>compilation</b> paradigm typically assumes that the “intractable part” of the processing be done once, during compilation. This assumption could render updating the knowledge base infeasible if recompilation is required. The ability to install updates without recompilation may therefore considerably widen applicability. In this paper, several update operations not requiring recompilation are developed. These include disjunction, substitution of truth constants, conjunction with unit clauses, reordering of variables, and conjunction with clauses. ...|$|E
40|$|Hierarchical {{execution}} of domain knowledge {{is a useful}} approach for intelligent, real-time systems in complex domains. In addition, well-known techniques for <b>knowledge</b> <b>compilation</b> allow the reorganization of knowledge hierarchies into more ecient forms. However, these techniques have been developed {{in the context of}} systems that work in static domains. Our investigations indicate that it is not straightforward to apply <b>knowledge</b> <b>compilation</b> methods for hierarchical knowledge to systems that generate behavior in dynamic environments. One particular problem involves the compilation of non-contemporaneous constraints. This problem arises when a training instance dynamically changes during execution. After dening the problem, we analyze several theoretical approaches that address non-contemporaneous constraints. We have implemented the most promising of these alternatives within Soar, a software architecture for performance and learning. Our results demonstrate that the [...] ...|$|E
40|$|In {{this work}} we report {{our efforts to}} {{facilitate}} the creation of mixed-initiative conversational interfaces for novice and experienced developers of human language technology. Our {{focus has been on}} a framework that allows developers to easily specify the basic concepts of their applications, and rapidly prototype conversational interfaces for a variety of configurations. In this paper we describe the current <b>knowledge</b> representation, the <b>compilation</b> processes for speech understanding, generation, and dialogue turn management, as well as the user interfaces created for novice users and more experienced developers. Finally, we report our experiences with several user groups in which developers used this framework to prototype a variety of conversational interfaces...|$|R
40|$|This paper {{explores the}} uscfitlness of a {{technique}} from software engineering, (:ode instrumentation, fbr {{the development of}} large-scale natural language grammars. Infi) rmation about the usage of grammar rules in test and corpus sentences is used to improve grammar and testsuite, as well as adapting a grammar to a specific genre. Results show that {{less than half of}} a large-coverage grammar for German is actually tested by two large testsuites, and that 10 30 % of testing time is redundant. This mefiodology applied {{can be seen as a}} re-use of grammar writing <b>knowledge</b> tbr testsniCe <b>compilation.</b> The construction of genre-specific grammars results in perfbrmance gains of a factor of four...|$|R
40|$|Draws {{attention}} {{rich and}} varied occasional output of Chlebowski, {{who was a}} professional writer creating {{in the first half}} of the 17 th century. However, former researchers accused him of plagiarism at the same time regarding him as a talented compiler. Researches on the Chlebowski’s output are going to lead to recognise his writing technique and expand <b>knowledge</b> about <b>compilation</b> and imitation. Furthermore, the article presents the state of researches on the Chlebowski’s creativity starting from the oldest works of a bibliographic nature (end of the 18 th and the 19 th century) to the detailed studies, which concern the following issues: Zebrzydowski’s rebellion, political relations with the Grand Duchy of Moscow, hagiographic writings, political relations with Persia and the Ottoman Empire, images of natural disasters, the Tatar invasions and funeral writings. Moreover, the following directions for further studies have been indicated: recognising the biography of the poet; organising and enhancing the knowledge of his works; identification of the community, from the patronage of which Chlebowski benefited as a professional writer; redefinition of the relations between compilation, imitation and Chlebowski’s plagiarism; proving whose library resources the poet benefited from...|$|R
