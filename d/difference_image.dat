491|2270|Public
25|$|The {{background}} {{is assumed to}} be the frame at time t. This <b>difference</b> <b>image</b> would only show some intensity for the pixel locations which have changed in the two frames. Though we have seemingly removed the background, this approach will only work for cases where all foreground pixels are moving and all background pixels are static.|$|E
2500|$|A {{threshold}} [...] "Threshold" [...] {{is put on}} this <b>difference</b> <b>image</b> {{to improve}} the subtraction (see Image thresholding).|$|E
2500|$|The imaged {{object is}} placed near the central grating. Absolute phase images are {{obtained}} if the object intersects {{one of a}} pair of coherent paths. If the two paths both pass through the object at two locations which are separated by a lateral distance d, then a phase <b>difference</b> <b>image</b> of Φ(r) - Φ(r-d) is detected. [...] Phase stepping one of the gratings is performed to retrieve the phase images. The phase <b>difference</b> <b>image</b> Φ(r) - Φ(r-d) can be integrated to obtain a phase shift image of the object.|$|E
40|$|Abstract. Several moving objects {{detection}} methods were compared and an improved method {{on the basis}} of consecutive frames difference was proposed. Color images captured with camera were transformed into gray ones and the gray ones were processed with median filter. The adjacent frames of any three consecutive ones were handled with differential operation to obtain two <b>difference</b> <b>images</b> and threshold value was set to transform <b>difference</b> <b>images</b> into binary ones. The binary images were dealt with by using opening operation. The location of moving aircraft was determined by performing two <b>difference</b> <b>images</b> with AND operation. The simulation results show that the improved method can effectively detect and locate the moving aircraft in intelligent surveillance system...|$|R
40|$|Most {{thresholding}} algorithms {{have difficulties}} processing images with unimodal distributions. In this paper an algorithm, based on finding a {{corner in the}} histogram plot, is proposed {{that is capable of}} performing bilevel thresholding of such images. Its effectiveness is demonstrated on synthetic data as well as a variety of real data, showing its application to edges, <b>difference</b> <b>images,</b> optic flow, texture <b>difference</b> <b>images,</b> polygonal approximation of curves, and image segmentation. ...|$|R
25|$|This {{means that}} the <b>difference</b> <b>image's</b> pixels' intensities are 'thresholded' or {{filtered}} {{on the basis of}} value of Threshold.|$|R
50|$|A Laplacian pyramid is {{very similar}} to a Gaussian pyramid but saves the <b>difference</b> <b>image</b> of the blurred {{versions}} between each levels. Only the smallest level is not a <b>difference</b> <b>image</b> to enable reconstruction of the high resolution image using the difference images on higher levels. This technique can be used in image compression.|$|E
5000|$|A {{threshold}} [...] "Threshold" [...] {{is put on}} this <b>difference</b> <b>image</b> {{to improve}} the subtraction (see Image thresholding).|$|E
50|$|The imaged {{object is}} placed near the central grating. Absolute phase images are {{obtained}} if the object intersects {{one of a}} pair of coherent paths. If the two paths both pass through the object at two locations which are separated by a lateral distance d, then a phase <b>difference</b> <b>image</b> of Φ(r) - Φ(r-d) is detected. Phase stepping one of the gratings is performed to retrieve the phase images. The phase <b>difference</b> <b>image</b> Φ(r) - Φ(r-d) can be integrated to obtain a phase shift image of the object.|$|E
40|$|<b>Difference</b> imaging or <b>image</b> {{subtraction}} is {{a method}} that measures differential photometry by matching the pointing and point-spread function (PSF) between image frames. It {{is used for the}} detection of time-variable phenomena. Here we present a new category of method [...] -CPM <b>Difference</b> <b>Imaging,</b> in which <b>differences</b> are not measured between matched images but instead between image frames and a data-driven predictive model that has been designed only to predict the pointing, PSF, and detector effects but not astronomical variability. In CPM <b>Difference</b> <b>Imaging</b> each pixel is modelled by the Causal Pixel Model (CPM) originally built for modeling Kepler data, in which pixel values are predicted by a linear combination of other pixels at the same epoch but far enough away such that these pixels are causally disconnected, astrophysically. It does not require that the user have any explicit model or description of the pointing or point-spread function of any of the images. Its principal drawback is that [...] -in its current form [...] -it requires an imaging campaign with many epochs and fairly stable telescope pointing. The method is applied to simulated data and also the K 2 Campaign 9 microlensing data. We show that CPM <b>Difference</b> <b>Imaging</b> can detect variable objects and produce precise differentiate photometry in a crowded field. CPM <b>Difference</b> <b>Imaging</b> is capable of producing <b>image</b> <b>differences</b> at nearly photon-noise precision...|$|R
40|$|We {{propose a}} novel video {{signature}} extraction method based on projections of <b>difference</b> <b>images</b> between consecutive video frames. The <b>difference</b> <b>images</b> are projected onto random basis vectors {{to create a}} low dimensional bitstream representation of the active content (moving regions) between two video frames. A sequence of these signatures serves to identify the underlying video content in a robust manner. Our experimental {{results show that the}} proposed video signature is robust to most common signal processing operations on video content such as compression, resolution scaling, brightness scaling...|$|R
40|$|We {{have used}} <b>image</b> <b>difference</b> metrics {{to measure the}} quality of a set of images to know how well they predict {{perceived}} <b>image</b> <b>difference.</b> We carried out a psychophysical experiment with 25 observers along with a recording of the observers gaze position. The <b>image</b> <b>difference</b> metrics used were CIELAB _DEab, S-CIELAB, the hue angle algorithm, iCAM and. A frequency map from the eye tracker data was applied as a weighting to the <b>image</b> <b>difference</b> metrics. The results indicate an improvement in correlation between the predicted <b>image</b> <b>difference</b> and the perceived <b>image</b> <b>difference...</b>|$|R
50|$|DNA array is {{fabricated}} to {{test the}} G-G mismatch stabilizing properties of the naphthyridine dimer. Each of the four immobilized sequences in the array differed by one base. The position of this base is indicated by an X in sequence 1 as shown in Figure 16. The SPR <b>difference</b> <b>image</b> is only detected for the sequence having cytosine (C) base at the X position in sequence 1, the complementary sequence to sequence 2. However, the SPR <b>difference</b> <b>image</b> corresponding to the addition of sequence 2 {{in the presence of}} the naphthyridine dimer shows that, in addition to its complement, sequence 2 also hybridizes to the sequence that forms a G-G mismatch. These results demonstrate that SPR imaging is a promising tool for monitoring single base mismatches and screen out the hybridized molecules.|$|E
5000|$|A {{secondary}} {{issue was}} {{that although the}} image was privacy-enhanced, it was not secure. The secret image appeared as a [...] "ghost" [...] if one moved one's head rapidly - or struck the viewer's head with a soft object, thereby offsetting the two image fields and revealing {{the edges of the}} <b>difference</b> <b>image.</b>|$|E
50|$|The {{background}} {{is assumed to}} be the frame at time t. This <b>difference</b> <b>image</b> would only show some intensity for the pixel locations which have changed in the two frames. Though we have seemingly removed the background, this approach will only work for cases where all foreground pixels are moving and all background pixels are static.|$|E
40|$|International audienceThreshold {{selection}} {{is a critical}} step in using binary change detection methods. The threshold determines the accuracy of change detection results but is highly subjective and scene-dependent, depending on the familiarity with the study area and the analyst’s skill. Nearest neighbor classification is a non-parametric classifier, which was applied to remove the threshold. In order {{to find the most}} suitable feature to detect construction and farmland changes, a variety of single and multiple variables were explored. They were regional similarity (RSIM), brightness <b>difference</b> <b>images</b> (BDIs), multi-band <b>difference</b> <b>images</b> (MDIs), multi-band ratio <b>difference</b> <b>images</b> (MRDIs), a combination of RSIM and BDIs (RSIMBD), a combination of RSIM and a optimum band difference and a optimum band ratio difference (RSIMDR), MDIs and MRDIs multiple variable groups. All were tested for two study sites of the bi-temporal SPOT 5 imagery, the results indicated that RSIM, RSIMDR, RSIMBD were significantly better than other single and multiple variables...|$|R
30|$|We {{accumulate}} <b>difference</b> <b>images.</b> In this accumulated image, all {{movement of}} human, object, and noise are represented. Next, noise reduction, motion clustering, and hand detection procedures {{are applied to}} this motion image.|$|R
40|$|Abstract. <b>Image</b> <b>difference</b> {{operation}} is frequently used in automated {{printed circuit board}} (PCB) inspection system {{as well as in}} many other image processing applications. The inspection system performance depends critically on the speed of this operation, which is a common problem related to the <b>image</b> <b>difference.</b> The goal of our technique is to achieve real time inspection using wavelet transform. This paper presents a new wavelet-based algorithm for <b>image</b> <b>difference,</b> which computes <b>image</b> <b>difference</b> to the output of the wavelet transform. The results of applying the technique to PCB images showed significant improvement on the traditional <b>image</b> <b>differencing...</b>|$|R
50|$|In {{digital image}} processing, {{the sum of}} {{absolute}} differences (SAD) {{is a measure of}} the similarity between image blocks. It is calculated by taking the absolute difference between each pixel in the original block and the corresponding pixel in the block being used for comparison. These differences are summed to create a simple metric of block similarity, the L1 norm of the <b>difference</b> <b>image</b> or Manhattan distance between two image blocks.|$|E
50|$|Transmission. In this mode, either {{attenuation}} or {{phase shift}} of the X-ray beam by the sample can be measured. Absorption contrast {{can be used to}} map the sample’s density. Particular elemental constituents can be located using measurements on each side of an absorption edge to give an element-specific <b>difference</b> <b>image</b> with moderate sensitivity. Phase-contrast imaging can be sensitive to internal structure even when absorption is low and can be enhanced by tuning the X-ray energy.|$|E
5000|$|In {{order to}} make the {{typically}} faint compression artifacts more readily visible, the data to be analyzed is subjected to an additional round of lossy compression, this time at a known, uniform level, and the result is subtracted from the original data under investigation. The resulting <b>difference</b> <b>image</b> is then inspected manually for any variation in the level of compression artifacts. In 2007, N. Krawetz denoted this method [...] "error level analysis".|$|E
5000|$|... 5. Zubal IG, Spencer SS, Imam K, Seibyl J, Smith EO, Wisniewski G, Hoffer PB (1995). <b>Difference</b> <b>images</b> {{calculated}} from ictal and interictal technetium-99m-HMPAO SPECT scans of epilepsy. Journal of Nuclear Medicine, 36:684-689.|$|R
30|$|For {{the second}} stream for {{temporal}} features, we build convolutional features specialized in motion feature extraction. While ImageNet-pretrained features generalized well to detection tasks [16, 34], {{it does not}} work for motion handling because they are derived from the still-image classification task. Therefore, we train the ConvNets over <b>difference</b> <b>images</b> collected from video datasets. The training is necessary, if we want to work on temporal differences, because statistical distribution of <b>difference</b> <b>images</b> has different nature from that of still <b>images.</b> We collect <b>difference</b> <b>images</b> of UCF- 101 [21], a large activity recognition dataset with 13, 320 video clips categorized into 101 classes, and we train AlexNet on them. AlexNet is chosen because of its trainability. It has less layers than VGGNet- 16 or GoogleNet and therefore is easy to train on datasets smaller than ImageNet such as UCF- 101. For feature extraction, we used the second convolutional layer in AlexNet, which makes 256 -dimensional feature maps, the same dimension as that of the spatial stream. We refer to this trained AlexNet as UCFAlex.|$|R
3000|$|Gradient infimum {{value is}} {{computed}} from the gradient values of <b>difference</b> <b>images</b> of consecutive frames. It is utilized for making decision on watershed segments for classification as foreground or background. Left gradient image ([...] [...]...|$|R
5000|$|The {{following}} is a simplistic illustrated explanation of how motion compensation works. Two successive frames were captured from the movie Elephants Dream. As {{can be seen from}} the images, the bottom (motion compensated) difference between two frames contains significantly less detail than the prior images, and thus compresses much better than the rest. Thus the information that is required to encode compensated frame will be much smaller than with the difference frame. This also means that it is also possible to encode the information using <b>difference</b> <b>image</b> at a cost of less compression efficiency but by saving coding complexity without motion compensated coding; as a matter of fact that motion compensated coding (together with motion estimation, motion compensation) occupies more than 90% of encoding complexity.|$|E
50|$|When {{utilized}} for image enhancement, {{the difference of}} Gaussians algorithm is typically applied when the size ratio of kernel (2) to kernel (1) is 4:1 or 5:1. In the example images to the right, the sizes of the Gaussian kernels employed to smooth the sample image were 10 pixels and 5 pixels. The algorithm {{can also be used}} to obtain an approximation of the Laplacian of Gaussian when the ratio of size 2 to size 1 is roughly equal to 1.6. The Laplacian of Gaussian is useful for detecting edges that appear at various image scales or degrees of image focus. The exact values of sizes of the two kernels that are used to approximate the Laplacian of Gaussian will determine the scale of the <b>difference</b> <b>image,</b> which may appear blurry as a result.|$|E
5000|$|DDM {{is based}} on an {{algorithm}} proposed in and, which is conveniently named Differential Dynamic Algorithm (DDA). DDA works by subtracting images acquired at different times and taking advantage that, as the delay [...] between two subtracted images gets large, the energy content of the <b>difference</b> <b>image</b> increases correspondingly. A two-dimensional Fast Fourier Transform (FFT) analysis of the difference images allows to quantify the growth of the signal contains for each wave vector [...] and one can calculate the Fourier power spectrum of the difference images for different delays [...] to obtain the so-called image structure function [...] Calculation shows that for both scattering- and fluorescence-based DDMwhere [...] is the normalized intermediate scattering function that would be measured in a Dynamic Light Scattering (DLS) experiment, [...] the sample scattering intensity that would be measured in a Static Light Scattering (SLS) experiment, [...] a background term due to the noise along the detection chain [...] a transfer function that depends on the microscope details. Equation (...) shows that DDM can be used for DLS experiments, provided that a model for the normalized intermediate scattering function is available. For instance, in the case of Brownian motion one has [...] where [...] is the diffusion coefficient of the Brownian particles. If the transfer function [...] is determined by calibrating the microscope with a suitable sample, DDM can be employed also for SLS experiments. Alternative algorithms for data analysis are suggested in.|$|E
40|$|The Land use / Land cover {{change in}} urban areas and the {{difference}} of the earth surface after the flood can be detected from remote sensing images by performing <b>image</b> <b>differencing</b> algorithms. Although many algorithms were proposed to generate <b>difference</b> <b>images,</b> the results are inconsistent. In order to integrate the merits of difference algorithms, fusion techniques are used to merge multiple <b>difference</b> <b>images.</b> The image fusion algorithms applied here are based on Principal Component Analysis and Discrete Wavelet Transform. Principal Component Analysis is the unsupervised technique, the change is guaranteed to be preserved in the major component images. In Wavelet based method, image fusion is performed at the pixel level and the details from source images can be reserved at various scales. The algorithms are implemented on the satellite images and results are presented...|$|R
50|$|Laplacian image pyramids {{based on}} the {{bilateral}} filter provide a good framework for image detail enhancement and manipulation. The <b>difference</b> <b>images</b> between each layer are modified to exaggerate or reduce details at different scales in an image.|$|R
5000|$|... 2. Chang DJ, Zubal IG, Gottschalk C, Necochea A, Stokking R, Studholme C, Corsi M, Slawski J, Spencer SS, Blumenfeld H (2002). Comparison of Statistical Parametric Mapping and SPECT <b>Difference</b> <b>Imaging</b> in Patients with Temporal Lobe Epilepsy. Epilepsia, 43:68-74.|$|R
40|$|How {{to obtain}} {{accurate}} difference map {{remains an open}} challenge in change detection. To tackle this problem, we propose a change detection method based on saliency detection and wavelet transformation. We do frequency-tuned saliency detection in initial <b>difference</b> <b>image</b> (IDI) obtained by logarithm ratio to get a salient <b>difference</b> <b>image</b> (SDI). Then, we calculate local entropy of SDI to obtain an entropic salient <b>difference</b> <b>image</b> (ESDI). The final <b>difference</b> <b>image</b> (FDI) is the wavelet fusion of IDI and ESDI, and Otsu thresholding is used to extract difference map from FDI. Experimental results validate the effectiveness and feasibility...|$|E
40|$|Abstract— Reversible {{watermarking}} schemes {{are widely}} used to maintain {{the authenticity of the}} digital image. This research will discuss a method on histogram modification of <b>difference</b> <b>image</b> in which the <b>difference</b> <b>image</b> is created from the difference value of adjacent pixels of the image grayscale.   Embedding process begins by dividing the host image and watermark into b blocks, followed by making a <b>difference</b> <b>image</b> of the host image block. From the <b>difference</b> <b>image</b> histogram, determine the peak value, and modify the histogram based on the peak value. Then, insert each block of the watermark to the <b>difference</b> <b>image</b> that has been modified and transform back into the grayscale image. Extraction and recovery process is the inverted version of the embedding stage. This process begins by dividing the watermarked image into b blocks, followed by making a <b>difference</b> <b>image</b> of each block. Then, extract the data and shift <b>difference</b> <b>image</b> histogram using a peak value. That <b>difference</b> <b>image</b> is transformed back into a grayscale image. Experimental results demonstrate that the average insertion capacity is 14 % greater than Xue's with PSNR value over 48 dB for 4 x 4 pixels and 23 % greater with PSNR over 46 dB for 8 x 8 pixels. From the comparison of robustness to line and salt n’ pepper on the density of 0. 05 noises is obtained that the watermark with ECC is more robust than a watermark without ECC. JPEG compression in lossless mode may be applied to the watermarked image. Multiple insertion of watermark can be done with the consequence that the more insertions will result in lower PSNR values. Keywords—difference image, histogram modification, reversible watermarkin...|$|E
30|$|To {{ask what}} distinguishes masks from real faces, we next {{computed}} a <b>difference</b> <b>image</b> (average mask minus average face) separately for the veridical categories, the high-performance group, and the low-performance group. These three difference images {{are shown in}} Fig. 9 (lighter regions indicate greater difference). The veridical <b>difference</b> <b>image</b> (Fig. 9, center) indicates that the surrounding of the eye is especially informative, presumably because the eye holes in the mask can produce local anomalies in appearance (e.g. surface discontinuities if the mask is not flush with the wearer’s face; complexion discontinuities if the skin around the wearer’s eyes is exposed). The question is whether observers pick up on these subtle cues. Visual comparison confirms that the <b>difference</b> <b>image</b> for the high-performer group (Fig. 9, left) closely resembles the veridical <b>difference</b> <b>image</b> (Fig. 9, center). The <b>difference</b> <b>image</b> for the low-performer group (Fig. 9, right) resembles the veridical <b>difference</b> <b>image</b> less closely. This global pattern is perhaps to be expected, given {{the formation of the}} subgroups: if high performers did not track the veridical categories, they would not be high performers. However, local variations in this pattern may reveal specific cues that high performers exploit, and that low performers overlook. We investigated this possibility by comparing correlations between different image slices.|$|E
40|$|International audienceWhen deoxygenated, blood behaves as an {{effective}} susceptibility contrast agent. Changes in brain oxygenation can be monitored using gradient-echo echo-planar imaging. With this technique, <b>difference</b> <b>images</b> also demonstrate that blood oxygenation is increased during periods of recovery from respiratory challenge...|$|R
30|$|A visual {{inspection}} of the emission images shows no obvious artefacts {{in any of the}} images, also the acquired MR images showed no artefacts—either with or without phones. In the <b>difference</b> <b>images</b> between images with and without phones, an underestimation of up to 15 % {{can be seen in the}} case of headphones, whereas no remarkable deviation can be found in the case of earphones.|$|R
40|$|Accepted by MNRAS, 8 pages, 4 figuresInternational audienceThe {{numerical}} kernel {{approach to}} difference imaging has been implemented {{and applied to}} gravitational microlensing events observed by the PLANET collaboration. The effect of an error in the source-star coordinates is explored and a new algorithm is presented for determining the precise coordinates of the microlens in blended events, essential for accurate photometry of <b>difference</b> <b>images.</b> It is shown how the photometric reference flux need not be measured directly from the reference image but {{can be obtained from}} measurements of the <b>difference</b> <b>images</b> combined with the knowledge of the statistical flux uncertainties. The improved performance of the new algorithm, relative to ISIS 2, is demonstrated...|$|R
