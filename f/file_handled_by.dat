1|10000|Public
30|$|An {{important}} subfield of {{file format}} fuzzing is fuzzing on web browsers. With {{the development of}} web browsers, browsers are extended to support more function than ever. And the type of <b>file</b> <b>handled</b> <b>by</b> browsers has expanded from traditional HTML, CSS, JS files to other types of files, like pdf, svg and other file formats handled by browser extensions. Specifically, browsers parse the web pages into a DOM tree, which interprets the web page into a document object tree involved with events and responses. Particularly, the DOM parsing and page rendering of browsers are currently popular fuzzing targets. Well known fuzzing tools towards web browsers include the Grinder framework (Stephenfewer 2016), COMRaider (Zimmer 2013), BF 3 (Aldeid 2013) and so on.|$|E
60|$|In {{years past}} too much laxity prevailed in its management, and the <b>files</b> were <b>handled</b> <b>by</b> all comers, simply on their request, and {{returned}} at their will, {{or not at}} all.|$|R
5000|$|JSP Tag library pooling - Each tag markup in JSP <b>file</b> is <b>handled</b> <b>by</b> a tag handler class. Tag handler class objects can be pooled and reused in {{the whole}} JSP servlet.|$|R
25|$|Resource leaks, where {{a finite}} system {{resource}} (such as memory or <b>file</b> <b>handles)</b> become exhausted <b>by</b> repeated allocation without release.|$|R
5000|$|Non-HTML/text content: textual content {{encoded in}} {{multimedia}} (image or video) files or specific <b>file</b> formats not <b>handled</b> <b>by</b> search engines.|$|R
40|$|This article {{deals with}} the {{organization}} {{and management of the}} computer <b>files</b> <b>handled</b> <b>by</b> mechanical engineers on their personal computers. In engineering organizations, a wide variety of electronic files (documents) are necessary to support both business processes and the activities of design and manufacture. Whilst a large number of files and hence information is formally archived, a significant amount of additional information and knowledge resides in electronic files on personal computers. The widespread use of these personal information stores means that all information is retained. However, its reuse is problematic for all but the individual {{as a result of the}} naming and organization of the files. To begin to address this issue, a study of the use and current practices for managing personal electronic files is described. The study considers the fundamental classes of <b>files</b> <b>handled</b> <b>by</b> engineers and analyses the organization of these files across the personal computers of 40 participants. The study involves a questionnaire and an electronic audit. The results of these qualitative and quantitative elements are used to elicit an understanding of the practices and requirements of engineers for managing personal electronic files. A potential scheme for naming and organizing personal electronic files is discussed as one possible way to satisfy these requirements. The aim of the scheme is to balance the personal nature of data storage with the need for personal records to be shared with others to support knowledge reuse in engineering organizations. Although this article is concerned with mechanical engineers, the issues dealt with are relevant to knowledge-based industries and, in particular, teams of knowledge workers...|$|R
50|$|When {{creating}} audio CDs Brasero writes CD-TEXT information automatically found, using GStreamer. It {{also supports}} the editing of CD-TEXT information and can burn audio CDs on the fly. It can use all audio <b>file</b> formats <b>handled</b> <b>by</b> GStreamer local installation, including Ogg, FLAC and MP3. Brasero can also search for audio files that are inside dropped folders.|$|R
40|$|The goal of {{the project}} was to {{understand}} the method behind generating and recognizing the various forms of morphological variants of a given word and then to design files by ourselves to implement the method. The project assigned to us involved the following sequence of tasks-• Understand how a morphological analyzer like PC kimmo works. • Identify and analyze the different types of files that PC kimmo works with. • Develop rule, lexicon, and grammar files for Bengali words. • Run the files in PC Kimmo and Jkimmo and show generation and recognition of morphological variants. List of <b>files</b> <b>handled</b> <b>by</b> PC Kimmo • The rule file. • The lexicon file. • The grammar file. • The generation comparison file. • The recognition comparison file. • The pairs comparison file. • The synthesis comparison file. • The synthesis file. Structure of the rule fil...|$|R
500|$|<b>File</b> I/O is <b>handled</b> <b>by</b> the self-describing , , , and [...] {{statements}} {{along with}} a further three: , which updates a record; , which selects subsequent records to access by finding a record with a certain key; and , which releases a lock on the last record accessed.|$|R
40|$|CAMIR is a {{collection}} of programs for running information retrieval experiments. The <b>files</b> <b>handled</b> <b>by</b> CAMIR consist of binary records in VBS format. Each record consists of N words (i. e. 4 N bytes) where N is at least 2 and has no effective upper bound. The first word contains N, the length of the record. The second word contains a numeric identifier for the record in the file. This will be called the id-word. Words 3 to N contain data for the particular record. All, or almost all, data <b>handled</b> <b>by</b> CAMIR goes into this form, even when it not quite convenient. In fact this form of data is quite suitable for almost all of the experimental work that we have been doing. Representation of documents A document is represented by a record in the form / d t-j w-j t 2 v* 2 [...] . tn wn / d is the record id-word, and is simply the document number (1, 2, 3 [...] .). t-j, t 2 » [...] . tn are the term numbers for the document, sorted into increasing order, and wi is a weight for the term ti # For man...|$|R
5000|$|Includes {{an object}} in the page of the type {{specified}} by the [...] attribute. This may be in any MIME-type the user agent understands, such as an embedded HTML page, a <b>file</b> to be <b>handled</b> <b>by</b> a plug-in such as Flash, a Java applet, a sound file, etc.|$|R
40|$|International audienceIn this paper, {{we present}} a virtual machine, VMAD (Virtual Machine for Advanced Dynamic analysis), {{enabling}} an efficient implementation of advanced profiling and analysis of programs. VMAD is organized as a sequence of basic operations where external modules associated to specific profiling strategies are dynamically loaded when required. The program binary <b>files</b> <b>handled</b> <b>by</b> VMAD are previously instrumented at compile time to include necessary data, instrumentation instructions and callbacks to the VM. Dynamic information, such as memory locations of launched modules, are patched at startup in the binary file. The LLVM compiler has been extended to automatically instrument programs according to both VMAD and the handled profiling strategies. VMAD's potential is illustrated by presenting a profiling strategy dedicated to loop nests. It collects all memory addresses that are accessed during a selected number of successive iterations of each loop. The collected addresses are consumed by an analysis process trying to interpolate addresses successively accessed through each memory reference as a linear function of some virtual loop indices. This profiling strategy using VMAD has been run {{on some of the}} SPEC 2006 and Pointer Intensive benchmark suites, showing a very low time overhead, in most cases...|$|R
40|$|International audienceVMAD (Virtual Machine for Advanced Dynamic analysis) is a {{platform}} for advanced profiling and analysis of programs, consisting in a static component and a runtime system. The runtime system is organized {{as a set of}} decoupled modules, dedicated to specific instrumenting or optimizing operations, dynamically loaded when required. The program binary <b>files</b> <b>handled</b> <b>by</b> VMAD are previously processed at compile time to include all necessary data, instrumentation instructions and callbacks to the runtime system. For this purpose, the LLVM compiler has been extended to automatically generate multiple versions of the code, each of them tailored for the targeted instrumentation or optimization strategies. The compiler chooses the most suitable intermediate representation for each version, depending on the information to be acquired and on the optimizations to be applied. The control flow graph is adapted to include the new versions and to transfer the control to and from the runtime system, which {{is in charge of the}} execution flow orchestration. The strength of our system resides in its extensibility, as one can add support for various new profiling or optimization strategies, independently of the existing modules. VMAD's potential is illustrated by presenting several analysis and optimization applications dedicated to loop nests: instrumentation by sampling, dynamic dependence analysis, adaptive version selection...|$|R
40|$|Digital content blurs {{the lines}} of {{traditional}} library acquisition workflows and organization. For example: link resolvers and the loading of order confirmation record <b>files</b> may be <b>handled</b> <b>by</b> systems staff in one organization and by technical services staff in another. Lines are being crossed between acquisitions and interlibrary loan functions, notably with electronic versions of theses and dissertations. Regardless of this blurring of lines, library staff use vendors still in acquiring content for the library collection. The reasons for using vendors have stood for decades, but are changes in the information industry having an impact? In the current environment, what interactions with vendors are most useful to a library...|$|R
40|$|Each file on an NFS server {{is uniquely}} {{identified}} <b>by</b> a persistent <b>file</b> <b>handle</b> {{that is used}} whenever a client performs any NFS operation. NFS <b>file</b> <b>handles</b> reveal significant amounts {{of information about the}} server. If attackers can sniff the <b>file</b> <b>handle,</b> then {{they may be able to}} obtain useful information. For example, the encoding used <b>by</b> a <b>file</b> <b>handle</b> indicates which operating system the server is running. The fields of the <b>file</b> <b>handle</b> contain information such as the date that the file system was created—often the same time that the OS was installed. Since an NFS <b>file</b> <b>handle</b> contains relatively little random data, it is not difficult to guess. If attackers can guess a <b>file</b> <b>handle,</b> then they can bypass the normal mounting procedures. This allows an attacker to access data without appropriate accounting and logging. We have analyzed <b>file</b> <b>handles</b> on three common server operating systems: Linux, FreeBSD, and Solaris. Each one of them suffers from deficiencies when constructing <b>file</b> <b>handles.</b> We have modified the NFS server on Linux to use only randomly-generated <b>file</b> <b>handles</b> over the network. This makes it more difficult for an attacker to guess a <b>file</b> <b>handle,</b> or from utilizing information contained within a <b>file</b> <b>handle.</b> To persistently store <b>file</b> <b>handles</b> we use an in-kernel port of Berkeley DB. Our performance evaluation shows an acceptable overhead. ...|$|R
50|$|Memory can {{be treated}} as a resource, but memory {{management}} is usually considered separately, primarily because memory allocation and deallocation is significantly more frequent than acquisition and release of other resources, such as <b>file</b> <b>handles.</b> Memory managed <b>by</b> an external system has similarities to both (internal) memory management (since it is memory) and resource management (since it is managed by an external system). Examples include memory managed via native code and used from Java (via Java Native Interface); and objects in the Document Object Model (DOM), used from JavaScript. In both these cases, the memory manager (garbage collector) of the runtime environment (virtual machine) is unable to manage the external memory (there is no shared memory management), and thus the external memory is treated as a resource, and managed analogously. However, cycles between systems (JavaScript referring to the DOM, referring back to JavaScript) can make management difficult or impossible.|$|R
40|$|Cloud {{computing}} economically {{paradigm of}} data service outsourcing. Data owner share the data under the cloud servers. The cloudserver may leak data information due to unauthorised entities or even be hacked. Encrypted storage and retrieval model is used. Searchableencryption technique supports only Boolean search process. Large amount of users and <b>files</b> are not <b>handled</b> <b>by</b> this search process. The privacyenabled data searching scheme provides solution for secure ranked key word search over encrypted cloud data. Ranked search enhances systemusability by enabling search result relevance ranking. Relevance score is statistical measure approach {{is used in}} information retrieval. Relevance score is used in secure searchable index preparation process. One to many order preserving mapping technique is used to properly protect sensitive score information. Ranked searchable symmetric encryption is used to perform secured data retrieval process...|$|R
50|$|OSX.Keydnap is {{initially}} downloaded as a Zip archive. This archive contains a single Mach-O file and a Resource fork containing an icon for the executable file, which is typically a JPEG or text file image. Additionally, the dropper {{takes advantage of}} how OS X <b>handles</b> <b>file</b> extensions <b>by</b> putting a space behind {{the extension of the}} file name for example - as “keydnap.jpg ” instead of “keydnap.jpg”. Having a commonly seen icon image and name is used to exploit users' willingness to click on benign looking files. When the file is opened, the Mach-O executable runs by default in the Terminal instead of an image viewer like the user would expect.|$|R
5000|$|Other {{resources}} include <b>file</b> <b>handles</b> or network sockets - for example, copying one file {{to another}} is most simply done with two file handles: one for input, one for output, and thus has a [...] "file handle working set" [...] size of two. If only one <b>file</b> <b>handle</b> is available, copying {{can still be}} done, but requires acquiring a <b>file</b> <b>handle</b> for the input, reading from it (say into a buffer), releasing it, then acquiring a <b>file</b> <b>handle</b> for the output, writing to it, releasing it, then acquiring the input <b>file</b> <b>handle</b> again and repeating. Similarly a server may require many sockets, {{and if it is}} limited would need to repeatedly release and re-acquire sockets. Rather than thrashing, these resources are typically required for the program, and if it cannot acquire enough resources, it simply fails.|$|R
40|$|High-throughput {{technologies}} {{have had a}} profound impact in transcriptomics. Prior to microarrays, measuring gene expression was not possible in a massively parallel way. As of late, deep RNA sequencing has been constantly gaining ground to microarrays in transcriptomics analysis. RNA-Seq promises several advantages over microarray technologies, but it also comes with its own set of challenges. Different approaches exist to tackle each of the required processing steps of the RNA-Seq data. The proposed solutions need to be carefully evaluated to find the best methods depending on the particularities of the datasets and the specific research questions that are being addressed. In this thesis I propose a computational framework that allows the efficient analysis of RNA-Seq datasets. The parallelization of tasks and organization of the data <b>files</b> was <b>handled</b> <b>by</b> the Anduril framework on which the workflow was implemented. Particular emphasis was bestowed on the quality control of the RNA-Seq files. Several measures were taken to prune the data of low quality bases and reads that hamper the alignment step. Furthermore, various existing processing algorithms for transcript assembly and abundance estimation were tested. The best methods have been coupled together into an automated pipeline that takes the raw reads and delivers expression matrices at isoform and gene level. Additionally, a module for obtaining sets of differentially expressed genes under different conditions or when measuring an experiment across a time course is included...|$|R
50|$|User-friendlier <b>file</b> <b>handling</b> with {{multiple}} <b>file</b> uploads.|$|R
50|$|It is {{a variant}} with {{additional}} support for IDE editing. It also enhances <b>file</b> <b>handling,</b> <b>file</b> editing, HTML editing over UltraEdit.|$|R
50|$|<b>File</b> <b>handling</b> {{features}} include: Project Manager, Git/SVN/CVS version control.|$|R
5000|$|CIFF Specification on File/Directory {{organization}} and <b>File</b> <b>Handling</b> Protocol ...|$|R
5000|$|... #Subtitle level 2: Redirecting to {{and from}} the {{standard}} <b>file</b> <b>handles</b> ...|$|R
40|$|The recent {{growth of}} {{computational}} power of desktop computers calls for their efficient use in larger organizations, especially those, which {{need to run}} computationally intensive tasks, such as universities and research centers. We describe QADPZ ['kwod "pi: 'si:], a modular, open source implementation in C of a multiuser and multi-platform system for idle distributed computing in a TCP/IP network. The computing power of large number of idle desktop computers is utilized by automatically scheduled tasks that are submitted, monitored, and controlled by users. Flexibility {{of the system is}} implied by several user application modes. Task software and hardware requirements and input and output <b>files</b> are <b>handled</b> automatically <b>by</b> the system. The tasks can be mobile. Internal communication protocol is based on optionally encrypted XML messages using public/private keys, user names and passwords. We are currently using the system for research tasks in the areas of large-scale scientific visualization, evolutionary computation, and simulation of complex neural network models...|$|R
50|$|Files are {{accessed}} by applications in Windows <b>by</b> using <b>file</b> <b>handles.</b> These <b>file</b> <b>handles</b> can be explored with the Process Explorer utility. This utility {{can also be}} used to force-close handles without needing to terminate the application holding them. This can cause an undefined behavior, since the program will receive an unexpected error when using the force-closed handle and may even operate on an unexpected <b>file</b> since the <b>handle</b> number may be recycled.|$|R
5000|$|Automatic <b>file</b> <b>handling</b> (listeners {{interact}} with multiple MP3 files {{as a single}} book).|$|R
5000|$|Ldr are loader {{functions}} for PE <b>file</b> <b>handling</b> and starting of new processes.|$|R
40|$|We {{present the}} submodels OFFLEM, ONLEM, and TNUDGE for the Modular Earth Submodel System (MESSy). Prescribed {{emissions}} from input <b>files</b> are <b>handled</b> <b>by</b> OFFLEM. ONLEM deals with online-calculated emissions, i. e., emissions that are calculated during the simulation. The submodel TNUDGE uses the "tracer nudging" technique for pseudo-sources and -sinks. For species with highly uncertain emission fluxes and/or with sufficiently long lifetimes, e. g., CH 4, {{it is common}} to create such pseudo-fluxes by prescribing the observed mixing ratio of the species at a given boundary (e. g., the mixing ratio of methane at the surface, or the ozone mixing ratio at the tropopause). All three submodels substantially simplify the inclusion of emissions into a model. Specific emissions can easily be switched on or off. New prescribed emissions can be included without rewriting any code. New online emissions only require one additional subroutine containing the new parameterization. A major advantage is that input fields at arbitrary resolution can be used. The problem of incompatible grids between emission data and model is overcome by utilizing the MESSy data import interface. To further simplify the creation of new offline emission data, the preprocessing program EDGAR 2 NC is provided. EDGAR 2 NC transforms files from the EDGAR format into the netCDF format which is required by OFFLEM. The presented routines {{are a part of the}} community modeling project MESSy and can be made available for use to the atmospheric modeling community...|$|R
5000|$|Switch, a {{workflow}} tool which integrates and automates <b>file</b> <b>handling</b> {{and third}} party software.|$|R
5000|$|... a READDIRPLUS operation, to get <b>file</b> <b>handles</b> and {{attributes}} {{along with}} file names when scanning a directory; ...|$|R
40|$|We {{present a}} new {{overflow}} technique for B-trees. The technique is {{a hybrid of}} partial expansions and unbalanced splits. This technique is asymmetric and adaptive. Considering a growing file (only insertions), the storage utilization is 77 % for random keys, 70 % for sorted keys, and over 75 % for non-uniform distributed keys. Similar results are achieved when we have deletions mixed with insertions. One of the main properties of this technique is that the storage utilization is very stable with respect to changes of the data distribution. This technique {{may be used for}} other bucket-based file structures, like extendible hashing or bounded disorder files. 1 Introduction The B + -tree {{is one of the most}} widely used file organizations. In a B + -tree all the information is stored at the lowest level (buckets), and the upper levels are a B-tree index. <b>File</b> growth is <b>handled</b> <b>by</b> bucket splitting, that is, when a bucket overflows, a new bucket is allocated and half of the records from the o [...] ...|$|R
50|$|Checks that a {{file with}} a {{specified}} name exists. This check {{is essential for}} programs that use <b>file</b> <b>handling.</b>|$|R
50|$|It relocates DOS kernel, COMMAND.COM interpreter, DOS {{resources}} (e.g.: buffers, <b>file</b> <b>handles,</b> stacks, lastdrive). It supports DOS 3.2 or higher.|$|R
5000|$|Common {{functions}} such as: <b>file</b> <b>handling,</b> find in <b>files</b> code locator, go to line, tabs, automatic indentation, editor zoom, etc.|$|R
