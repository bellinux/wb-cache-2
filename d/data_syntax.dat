19|125|Public
50|$|The Robot Framework is {{a generic}} test {{automation}} framework for acceptance testing and acceptance test-driven development (ATDD). It is a keyword-driven testing framework that uses tabular test <b>data</b> <b>syntax.</b>|$|E
5000|$|The {{scope of}} ISO/IEC JTC 1/SC 31 is “Standardization of data formats, <b>data</b> <b>syntax,</b> data structures, data encoding, and {{technologies}} {{for the process}} of automatic identification and data capture and of associated devices utilized in inter-industry applications and international business interchanges and for mobile applications.” ...|$|E
50|$|CEPT Recommendation T/CD 06-01 was a {{standard}} set in 1981 by the European Conference of Postal and Telecommunications Administrations (CEPT) for {{the display of}} Videotex; specifically, for the Videotex Presentation Layer <b>Data</b> <b>Syntax.</b> It was revised {{a number of times}} in the 1980s, and also later redesignated as recommendation T/TE 06-01.|$|E
50|$|The {{creations of}} these {{functions}} can be automated by Haskell's <b>data</b> record <b>syntax.</b>|$|R
40|$|For years, {{users have}} asked for the ability to write {{functions}} in <b>DATA</b> step <b>syntax.</b> The day has come when this is possible. In SAS ® 9, the FCMP procedure provides the ability to write functions using <b>DATA</b> step <b>syntax.</b> In SAS ® 9. 2, these functions can be called from a DATA step. This paper explains how to write functions with PROC FCMP and how these functions are invoked from a DATA step. It also contains caveats when calling PROC FCMP functions from a DATA step...|$|R
5000|$|XPath {{access to}} {{variable}} <b>data</b> (XPath variable <b>syntax</b> $variable.part/location) ...|$|R
5000|$|Semantic {{interoperability}} {{is therefore}} concerned {{not just with}} the packaging of <b>data</b> (<b>syntax),</b> but the simultaneous transmission of the meaning with the data (semantics). This is accomplished by adding data about the data (metadata), linking each data element to a controlled, shared vocabulary. The meaning of the data is transmitted with the data itself, in one self-describing [...] "information package" [...] that is independent of any information system. It is this shared vocabulary, and its associated links to an ontology, which provides the foundation and capability of machine interpretation, inferencing, and logic.|$|E
40|$|* NeXML for robust <b>data</b> <b>syntax</b> and {{flexible}} metadata annotation. |$|E
3000|$|To support {{disclosure}} and replication in scientific research (Peters, Abraham, & Crutzen, 2015) and facilitate future meta-analyses, the <b>data,</b> <b>syntax</b> and statistical outputs {{used in the}} present study are available at [URL] [...]...|$|E
5000|$|... 1) Newer EAI or B2B systems often cannot handle EDI (Electronic <b>Data</b> Interchange) <b>syntax</b> directly. Simple syntax {{converters}} do a 1:1 conversion before. Their input is an EDIFACT transaction file, their output an XML/EDIFACT instance file.|$|R
5000|$|New <b>data</b> type {{declaration}} <b>syntax,</b> {{to specify}} the data type and other attributes of variables ...|$|R
5|$|An {{optional}} query, {{separated from}} the preceding part by a question mark (?), containing a query string of non-hierarchical <b>data.</b> Its <b>syntax</b> is not well defined, but by convention is most often a sequence of attribute–value pairs separated by a delimiter.|$|R
40|$|An {{adaptive}} fixed-length differential chain coding (FL-DCC) {{scheme is}} introduced for transmission of line graphics. Compared to differential chain coding(DCC), which uses vectors with different lengths, FL-DCC {{has a lower}} coding rate and a more uniform <b>data</b> <b>syntax</b> while maintaining a comparable subjective graphical quality...|$|E
40|$|The task of {{this work}} is {{implementation}} of application for relational algebra query evaluation and for visualization of particular operations on particular <b>data.</b> <b>Syntax</b> of queries {{is the same as}} is used in the course of Database systems at the Faculty of Mathematics and Physics of Charles University. Program also allows editing tables and queries within whole project...|$|E
40|$|A {{case history}} is {{presented}} in which the NASTRAN system provided both guidelines and working software {{for use in the}} development of a discrete element program, PATCHES- 111. To avoid duplication and to take advantage of the wide spread user familiarity with NASTRAN, the PATCHES- 111 system uses NASTRAN bulk <b>data</b> <b>syntax,</b> NASTRAN matrix utilities, and the NASTRAN linkage editor. Problems in developing the program are discussed along with details on the architecture of the PATCHES- 111 parametric cubic modeling system. The system includes model construction procedures, checkpoint/restart strategies, and other features...|$|E
30|$|The Research Data Centre (FDZ) of the German Federal Employment Agency (BA) at the Institute for Employment Research (IAB) {{provides}} a scientific use file of the PASS <b>data.</b> Stata <b>syntax</b> files for the analyses {{are available on}} request from the author.|$|R
5000|$|An {{optional}} query, {{separated from}} the preceding part by a question mark (...) , containing a query string of non-hierarchical <b>data.</b> Its <b>syntax</b> is not well defined, but by convention is most often a sequence of attribute-value pairs separated by a delimiter.|$|R
40|$|Mathematical {{computing}} {{can become}} easily accessible on the Internet. The distributed Internet Accessible Mathematical Computation (IAMC) system can supply mathematical computing power widely through TCP/IP, the Web, or email. A {{key enabling technology}} for distributed and parallel mathematical computation is a standard protocol for exchanging mathematical objects. The Multi Protocol (MP) allows cooperating applications to efficiently exchange mathematical data with a shared view of the <b>data's</b> <b>syntax</b> and semantics. While the MP design separates the encoding and transmission of data, the current C library for MP tied these closely. An object-oriented design of MP is described that decouples data representation from data transport to produce a pair of more reusable modules. The data representation module is also useful for the IAMC...|$|R
30|$|Trends {{in control}} and {{automation}} show an increase in data processing and communication in embedded automation controllers. The eXtensible Markup Language (XML) is emerging as a dominant <b>data</b> <b>syntax,</b> fostering interoperability, yet little is still known about how to provide predictable real-time performance in XML processing, as required {{in the domain of}} industrial automation. This paper presents an XML processor that is designed with such real-time performance in mind. The publication attempts to disclose insight gained in applying techniques such as object pooling and reuse, and other methods targeted at avoiding dynamic memory allocation and its consequent memory fragmentation. Benchmarking tests are reported in order to illustrate the benefits of the approach.|$|E
40|$|Abstract. An {{effective}} collaborative B 2 B relationship {{requires the}} right modeling of collaborative processes and business {{information needed to}} support these processes. Business information modeling implies modeling the <b>data</b> <b>syntax</b> and semantics. In order to model information semantics there are some ontology specification languages. However, from a B 2 B perspective, the main disadvantage of these languages {{is that they are}} mostly based on logic formalisms to support machine reasoning. This makes the language syntax unfamiliar to business analysts who model the business information to be exchanged within collaborative processes. The objective {{of this paper is to}} present a metamodel for modeling explicit and formal contextual ontologies, for human processing, associated to business documents...|$|E
40|$|Abstract Trends {{in control}} and {{automation}} show an increase in data processing and communication in embedded automation controllers. The eXtensible Markup Language (XML) is emerging as a dominant <b>data</b> <b>syntax,</b> fostering interoperability, yet little is still known about how to provide predictable real-time performance in XML processing, as required {{in the domain of}} industrial automation. This paper presents an XML processor that is designed with such real-time performance in mind. The publication attempts to disclose insight gained in applying techniques such as object pooling and reuse, and other methods targeted at avoiding dynamic memory allocation and its consequent memory fragmentation. Benchmarking tests are reported in order to illustrate the benefits of the approach. </p...|$|E
40|$|SAS {{provides}} {{a set of}} macros for designing experiments and analyzing choice <b>data.</b> The <b>syntax</b> and usage of these macros is discussed in this chapter. Two additional macros, one for scatter plots of labeled points and one for color interpolation, are documented here as well. ∗ The following SAS autocall macros are available...|$|R
5000|$|The {{identifier}} of {{a regular}} [...] "function" [...] in Ruby (which is really a method) cannot {{be used as a}} value or passed. It must first be retrieved into a [...] or [...] object to be used as first-class <b>data.</b> The <b>syntax</b> for calling such a function object differs from calling regular methods.|$|R
5000|$|... head is {{a program}} on Unix and Unix-like systems used to display the {{beginning}} of a text file or piped <b>data.</b> The command <b>syntax</b> is: ...|$|R
40|$|Trends {{in control}} and {{automation}} show an increase in data processing and communication in embedded automation controllers. The eXtensible Markup Language (XML) is emerging as a dominant <b>data</b> <b>syntax,</b> fostering interoperability, yet little is still known about how to provide predictable real-time performance in XML processing, as required {{in the domain of}} industrial automation. This paper presents an XML processor that is designed with such real-time performance in mind. The publication attempts to disclose insight gained in applying techniques such as object pooling and reuse, and other methods targeted at avoiding dynamic memory allocation and its consequent memory fragmentation. Benchmarking tests are reported in order to illustrate the benefits of the approach. Copyright © 2008 Esther Mínguez Collado et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. 1...|$|E
40|$|Abstract. We {{consider}} {{the problem of}} modeling syntax and semantics of probabilistic processes with continuous states (e. g. with continuous <b>data).</b> <b>Syntax</b> and semantics of these systems {{can be defined as}} alge-bras and coalgebras of suitable endofunctors over Meas, the category of measurable spaces. In order to give a more concrete representation for these coalgebras, we present an SOS-like rule format which induces an universal semantics, for which behavioural equivalence is a congruence. To this end, we solve several problems. In particular, the format has to specify how to compose the semantics of processes (which basically are continuous state Markov processes). This is achieved by defining a language of measure terms, i. e., expressions specifically designed for describing probabilistic measures. Thus, the transition relation associates processes with measure terms. As an example application, we model a CCS-like calculus of processes placed in an Euclidean space. The approach we follow in this case can be readily adapted to other quantitative aspects, e. g. Quality of Service, physical and chemical parameters in biological systems, etc. ...|$|E
40|$|First of all I {{would like}} to thank Prof. Dr. Marcel Waldvogel for {{providing}} the opportunity to write this thesis. Next, I {{would like to}} thank Jun. -Prof. Dr. Tobias Schreck and Dr. Florian Mansmann for many helpful discussions and ideas. Furthermore, I {{would like to thank}} Sebastian Graf for his advice and guidence throughout my master thesis. Thanks also for many helpful discussions. Special thanks to Marcus Wenz for reviewing my master thesis manuscript and some ideas which came up during several informal presentations of the prototype. Discussions with DiSy group members Lukas Lewandowski, Patrick Lang, Sebastian Belle and Thomas Zink also provided very helpful suggestions. I want to thank my parents for their great support and financial help during my studies. iAbstract. Today’s storage capabilities facilitate the accessibility and long term archival of increasingly large data sets usually refered to as ”Big Data”. Tree-structured hierar-chical data is very common, for instance phylogenetic trees, filesystem <b>data,</b> <b>syntax</b> trees and often times organizational structures. Analysts often face the problem of gathering information through comparison of multiple trees. Visual analytic tools aid analysts b...|$|E
40|$|Coding with SAS {{is easier}} than ever with SAS 9. 2. This paper {{highlights}} the top new features and performance improvements in DATA step, PROC SQL, and PROC SORT. Included are writing functions with <b>DATA</b> step <b>syntax,</b> improved performance when accessing an external database from PROC SQL, more intuitive and culturally acceptable sorting with PROC SORT, and several "Top 10 " SASware Ballot items...|$|R
5000|$|SDL-RT {{is based}} on the ITU Specification and Description Language {{replacing}} the data language with C. Latest version of SDL standard (SDL'2010) now includes the support of C <b>data</b> types and <b>syntax</b> making SDL-RT basic principle part of the official standard.|$|R
5000|$|Tuples: Tuples are {{containers}} for a fixed number of Erlang <b>data</b> types. The <b>syntax</b> [...] denotes a tuple whose arguments are [...] The arguments can be primitive data types or compound data types. Any {{element of a}} tuple can be accessed in constant time.|$|R
40|$|International audienceWe {{consider}} {{the problem of}} modeling syntax and semantics of probabilistic processes with continuous states (e. g. with continuous <b>data).</b> <b>Syntax</b> and semantics of these systems {{can be defined as}} algebras and coalgebras of suitable endofunctors over Meas, the category of measurable spaces. In order to give a more concrete representation for these coalgebras, we present an SOS-like rule format which induces an abstract GSOS over Meas; this format is proved to yield a fully abstract universal semantics, for which behavioural equivalence is a congruence. To this end, we solve several problems. In particular, the format has to specify how to compose the semantics of processes (which basically are continuous state Markov processes). This is achieved by defining a language of measure terms, i. e., expressions specifically designed for describing probabilistic measures. Thus, the transition relation associates processes with measure terms. As an example application, we model a CCS-like calculus of processes placed in an Euclidean space. The approach we follow in this case can be readily adapted to other quantitative aspects, e. g.  Quality of Service, physical and chemical parameters in biological systems, etc...|$|E
40|$|With the {{increasing}} need of spatial data in various decision support systems, the access and sharing of geospatial data over the Internet {{has become an}} important issue. But the underlying heterogeneity in geospatial <b>data</b> <b>syntax</b> and semantics are the major bottleneck towards this direction. The present standardization approach suggests {{that there should be}} a core schema for each of the data repositories providing the metadata information of the data sources. The development of the core schema at the organizational level, at the national level has also been thought of. Open Geospatial Consortium (OGC) has addressed the heterogeneity problem and defined several standards for data sharing and accessing. GML has been proposed by OGC to be the standard interoperable data format. This underlying data structure for GML data has been termed as application schema. In this paper we propose an approach for semantic based matching of the application schemas across several data repositories both at the element level and structure level. Through this mapping methodology, data conforming to one schema can be exported to the other schema. The application of ontology has been utilized to generate the mapping rules from one schema to the other. This will subsequently help in converting data in one schema format to the other...|$|E
40|$|This book {{provides}} a comprehensive introduction to latent variable growth curve modeling (LGM) for analyzing repeated measures. It presents the statistical basis for LGM and its various methodological extensions, including {{a number of}} practical examples of its use. It is designed {{to take advantage of}} the reader's familiarity with analysis of variance and structural equation modeling (SEM) in introducing LGM techniques. Sample <b>data,</b> <b>syntax,</b> input and output, are provided for EQS, Amos, LISREL, and Mplus on the book's CD. Throughout the book, the authors present a variety of LGM techniques that are useful for many different research designs, and numerous figures provide helpful diagrams of the examples. Updated throughout, the second edition features three new chapters-growth modeling with ordered categorical variables, growth mixture modeling, and pooled interrupted time series LGM approaches. Following a new organization, the book now covers the development of the LGM, followed by chapters on multiple-group issues (analyzing growth in multiple populations, accelerated designs, and multi-level longitudinal approaches), and then special topics such as missing data models, LGM power and Monte Carlo estimation, and latent growth interaction models. The model specifications previously included in the appendices are now available on the CD so the reader can more easily adapt the models to their own research. This practical guide is ideal for a wide range of social and behavioral researchers interested in the measurement of change over time, including social, developmental, organizational, educational, consumer, personality and clinical psychologists, sociologists, and quantitative methodologists, as well as for a text on latent variable growth curve modeling or as a supplement for a course on multivariate statistics. A prerequisite of graduate level statistics is recommended...|$|E
40|$|Bi-X Core is a {{general-purpose}} bidirectional transformation language, {{aiming to}} implement various systems that need synchronization between their input data and output <b>data.</b> In <b>syntax,</b> Bi-X Core is a first-order Î»-calculus extended with two structured data types, tuple and variant. For ease of use, some functional languages with more syntactic sugar {{can be defined}} based on Bi-X Core. The technical problem we solve in this paper is how to define bidirectional semantics for a general-pupose functional language. Bi-X Core is an ongoing work, and some examples are presented to demonstrate its usefulness...|$|R
40|$|The goal of {{the first}} part of this work is to {{summarize}} the basics of the Python programming language. Show how to create programs in Python, what are the modules, the basic <b>data</b> types, <b>syntax</b> and the possibility of approach to language. The second part will address the description of the initial programming courses, their objectives and what benefits it brings to teaching Python. Finally, then look at the (in) dependence on the Python platform for running and how to get from Python to another frequently used programming languages...|$|R
50|$|NeXML is an {{exchange}} standard for representing phyloinformatic data. It {{was inspired by}} the widely used Nexus file format but uses XML to produce a more robust format for rich phylogenetic <b>data.</b> Advantages include <b>syntax</b> validation, semantic annotation, and web services.The format is broadly supported and has libraries in many popular programming languages for bioinformatics.|$|R
