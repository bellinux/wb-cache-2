5|21|Public
5000|$|... 1600XL - codenamed Shakti, {{this was}} <b>dual-processor</b> <b>system</b> with 6502 and 80186 {{processors}} and two built-in 5¼″ floppy disk drives.|$|E
5000|$|Reference board codenamed [...] "Wahoo" [...] for <b>dual-processor</b> <b>system</b> {{reference}} design {{board with}} three physical PCI-E x16 slots, and [...] "HammerHead" [...] for single-socket system reference design board with four physical PCI-E x16 slots, also notable was the reference boards includes two ATA ports and only four SATA 3.0 Gbit/s ports (as being paired with SB600 southbridge).|$|E
40|$|Abstract—In this paper, {{we present}} {{the design and}} {{implementation}} of an open-source reconfigurable very long instruction word (VLIW) multiprocessor system. This processor is implemented as a softcore on a field-programmable gate arrays (FPGA) and its instruction set architecture (ISA) {{is based on the}} Lx/ST 200 ISA. This multiprocessor design is based on our earlier ρ-VEX processor design. Since the ρ-VEX processor is a parameterized processor, our multiprocessor design is also parameterized. By utilizing a freely available compiler and simulator in our development framework, we are able to optimize our design and map any application written in C to our multiprocessor system. This VLIW multiprocessor can exploit data level as well as instruction level parallelism inherent in an application and make its execution faster. More importantly, we achieve our results by saving expensive FPGA area through the sharing of resources. The results show that we can achieve two times better performance for our <b>dual-processor</b> <b>system</b> (with shared resources) compared to a uni-processor system or a 2 -cluster processor system for applications having data level and instruction level parallelism. I...|$|E
5000|$|<b>Dual-processor</b> SMP <b>system</b> {{with two}} 150 MHz MIPS R4400 {{microprocessors}} ...|$|R
50|$|In October 2006, DAZ 3D {{released}} Bryce 6.0 and {{has released}} an update (6.1), this includes a Mac Intel compatible update. New features include animation import, support for <b>dual-processor</b> <b>systems</b> {{as well as}} hyper-threading, random replicate tool, advanced terrain editing, HDRI support and other tweaks. The interface remained largely the same, but with a green tint to it, and different buttons in the create palette.|$|R
50|$|Phenom II is {{a family}} of AMD's {{multi-core}} 45 nm processors using the AMD K10 microarchitecture, succeeding the original Phenom. Advanced Micro Devices released the Socket AM2+ version of Phenom II in December 2008, while Socket AM3 versions with DDR3 support, along with an initial batch of triple- and quad-core processors were released on February 9, 2009. <b>Dual-processor</b> <b>systems</b> require Socket F+ for the Quad FX platform. The next-generation Phenom II X6 was released on April 27, 2010.|$|R
40|$|This paper {{addresses}} {{the problem of}} orchestrating and scheduling parallelism at multiple levels of granularity on heterogeneous multicore processors. We present mechanisms and policies for adaptive exploitation and scheduling of layered parallelism on the Cell Broadband Engine. Our policies combine event-driven task scheduling with malleable loop-level parallelism, which is exploited from the runtime system whenever task-level parallelism leaves idle cores. We present a scheduler for applications with layered parallelism on Cell and investigate its performance with RAxML, an application which infers large phylogenetic trees, using the Maximum Likelihood (ML) method. Our experiments show that the Cell benefits significantly from dynamic methods that selectively exploit the layers of parallelism in the system, in response to workload fluctuation. Our scheduler outperforms the MPI version of RAxML, scheduled by the Linux kernel, by up to a factor of 2. 6. We are able to execute RAxML on one Cell four times faster than on a <b>dual-processor</b> <b>system</b> with Hyperthreaded Xeon processors, and 5 – 10 % faster than on a single-processor system with a dual-core, quad-thread IBM Power 5 processor...|$|E
40|$|Packet-based on-chip {{interconnection}} networks, or Network-on-Chips (NoCs) are progressively replacing global {{on-chip interconnection}}s in Multi-processor System-on-Chips (MP-SoCs) thanks to better performances and lower power consumption. However, modern generations of MP-SoCs have an increasing sensitivity to faults {{due to the}} progressive shrinking technology. Consequently, in order to evaluate the fault sensitivity in NoC architectures, there is the need of accurate test solution which allows to evaluate the fault tolerance capability of NoCs. This paper presents an innovative test architecture based on a <b>dual-processor</b> <b>system</b> which is able to extensively test mesh based NoCs. The proposed solution improves previously developed methods since {{it is based on}} a NoC physical implementation which allows to investigate the effects induced by several kind of faults thanks to the execution of on-line fault injection within all the network interface and router resources during NoC run-time operations. The solution has been physically implemented on an FPGA platform using a NoC emulation model adopting standard communication protocols. The obtained results demonstrated the effectiveness of the developed solution in term of testability and diagnostic capabilities and make our solutions suitable for testing large scale NoC desig...|$|E
40|$|Abstract. <b>Dual-processor</b> (2 P) <b>systems</b> are the {{preferred}} building blocks in commodity Linux clusters {{because of their}} greater peak performance-to-price ratio relative to single-processor (1 P) systems. However, running memory-intensive applications on 2 P systems can lead to performance degradation if the memory subsystem cannot fulfill the increased demand from the second processor. Efficient utilization of the second processor depends on the scalability of the memory subsystem, especially for memory bandwidth-bound codes. This paper examines the performance of kernels and applications on two <b>dual-processor</b> <b>systems</b> with different memory subsystems: a shared bus architecture (Intel Xeon); and a NUMA-like architecture where each processor has an independent path to “local ” memory, and a channel for coherence and inter-processor memory access (AMD Opteron) ...|$|R
40|$|This {{document}} {{describes a}} proposed CMU multiprocessor {{system to be}} constructed around a set of PDP- 11 computers connected through a crosspoint switch to a large sharable primary memory. The present design constitutes a solution to {{a specific set of}} needs existing in our environment. The system has research consequences that reach well beyond the particular demands it was designed to satisfy. For although multiprocessors have been much talked about and advocated, there are remarkably few operational systems more complex than <b>dual-processor</b> <b>systems,</b> and even fewer documented scientific investigations into their performance and operating structure. This document is limited to a presentation and analysis of the (hardware) system. It gives enough description of the usage requirements, software, and the research potentials and problems to make clear why we believe the effort to be a sound one. It does not attempt a systematic discussion of the field of multiprocessor research, nor of alternative systems that might be of interest, either to meet our computing demands or as researc...|$|R
40|$|Abstract. Dual-processor nodes are the {{preferred}} building blocks in HPC clusters {{because of the}} greater performance-to-price ratio of such configurations relative to clusters comprising single-processor nodes. The arrival of 64 -bit commodity clusters for HPC is advantageous for applications that require large amounts of memory and I/O because of the larger memory addressability of these processors. Some of these 64 -bit processors also use more advanced memory subsystems, which provide increased performance for some applications. This paper examines the overall performance characteristics of three <b>dual-processor</b> <b>systems</b> based on commodity 64 -bit processors: Intel Itanium 2, AMD Opteron and IBM PowerPC 970, {{also known as the}} Apple PowerPC G 5. First, a low-level characterization of each system is obtained using a variety of computational kernels and micro-benchmarks to measure the speeds of the functional units and memory subsystems. Performance measurements and analysis of several scientific applications that span a wide range of computational requirements are presented next. Finally, we offer some general observations and insights on performance for applications developers and discuss 32 - to 64 -bit migration and interoperability issues. 1...|$|R
5000|$|On 23 May 2006, Intel {{released}} the dual-core CPU (Xeon branded 5000 series) codenamed Dempsey (product code 80555). Released as the Dual-Core Xeon 5000-series, Dempsey is a NetBurst microarchitecture processor produced using a 65 nm process, and is virtually identical to Intel's [...] "Presler" [...] Pentium Extreme Edition, {{except for the}} addition of SMP support, which lets Dempsey operate in <b>dual-processor</b> <b>systems.</b> Dempsey ranges between 2.50 GHz and 3.73 GHz (model numbers 5020-5080). Some models have a 667 MT/s FSB, and others have a 1066 MT/s FSB. Dempsey has 4 MB of L2 Cache (2 MB per core). A Medium Voltage model, at 3.2 GHz and 1066 MT/s FSB (model number 5063), has also been released. Dempsey also introduces a new interface for Xeon processors: LGA 771, also known as Socket J. Dempsey was the first Xeon core {{in a long time}} to be somewhat competitive with its Opteron-based counterparts, although it could not claim a decisive lead in any performance metric - that would have to wait for its successor, the Woodcrest.|$|R
40|$|In <b>dual-processor</b> <b>systems</b> {{the optimal}} {{assignment}} of the modules of a distributed program {{over the two}} processors may be found using a net-work flow algorithm. on the two processors and is not usually feasible to recompute each time the loads change. The optimal assignment is sensitive to the loads We {{address the problem of}} computing all optimal assignments for all possible load values in advance of actual execution of the distributed program. A mathematical model is developed wherein the situation is represented by a convex polyhedron in 3 -space, each of whose faces corresponds to a specific optimal assignment. It is proved that, even though there are 2 &quot; distinct assignments of n modules over the two processors and an infinite number of load values, the number of optimal assignments is O(n 2). A fast look-up technique that, given the polyhedron, finds the optimal assignment at a specific pair of loads in O(n log n) time is 7 described. An algorithm that finds the polyhedron in O(n) time is presented. The results presented here provide a means for adapting rapidly to changes in the load on both machines...|$|R
40|$|Abstract—To {{address the}} {{increasingly}} important energy efficiency problem, a Standby-Sparing scheme {{has been studied}} previously to reduce the energy consumption in <b>dual-processor</b> fault-tolerant <b>systems.</b> However, the dedicated (spare) processor in the Standby-Sparing scheme needs to execute backup tasks at full speed to preserve system reliability and thus cannot effectively exploit its available slack time. In this paper, based on our prior work of the Preference-Oriented Earliest Deadline (POED) scheduling algorithm, we study an Energy-Efficient Fault-Tolerant (EEFT) scheme for <b>dual-processor</b> real-time <b>systems,</b> which can effectively exploit the slack time on both processors for better energy savings. Specifically, the primary and backup tasks are first mapped in a mixed manner to both processors. Then, the POED algorithm is used to schedule the mixed tasks on each processor, where primary tasks are executed {{as soon as possible}} while backup tasks as late as possible to reduce the overlapped execution of primary and backup copies of the same task and thus reduce energy consumption. The online scheme that considers the additional slack from the cancellation of backup tasks and early completion of primary tasks is further explored. The proposed EEFT schemes are evaluated through extensive simulations. The results show that, compared to the state-of-theart Standby-Sparing scheme, significant (more than 40 %) energy savings can be obtained under the POED-based EEFT schemes. I...|$|R
40|$|<b>Dual-processor</b> (2 P) <b>systems</b> are the {{preferred}} building blocks in commodity Linux clusters {{because of their}} greater peak performance-to-price ratio relative to single-processor (1 P) systems. However, running memory-intensive applications on 2 P systems can lead to performance degradation if the memory subsystem cannot fulfill the increased demand from the second processor...|$|R
5000|$|IBM offered <b>dual-processor</b> {{computer}} <b>systems</b> {{based on}} its System/360 model 65 and the closely related model 67 and 67-2. [...] The operating systems that ran on these machines were OS/360 M65MP and TSS/360. Other software developed at universities, notably the Michigan Terminal System (MTS), used both CPUs. Both processors could access data channels and initiate I/O. In OS/360 M65MP, peripherals could generally be attached to either processor since the operating system kernel ran on both processors (though with a [...] "big lock" [...] around the I/O handler). [...] The MTS supervisor (UMMPS) {{has the ability to}} run on both CPUs of the IBM System/360 model 67-2. Supervisor locks were small and used to protect individual common data structures that might be accessed simultaneously from either CPU.|$|R
40|$|Ray tracing is {{a popular}} method of {{rendering}} 3 D scenes {{with a high level}} of detail. It automatically handles perspective, hidden surface removal, lighting, shadowing, reflect and refraction. Ray tracing is also generally thought of as being a very slow method of scene rendering. [...] This paper presents a method of discrete ray tracing that has been shown empirically to run (on certain modem uniprocessor and <b>dual-processor</b> <b>systems)</b> at average frame rates greater than 10 FPS (Frames Per Second), with a screen resolution of 128 xl 28 active pixels per frame. To achieve this, we use an octree representation of a discrete (or "rasterized") 3 D scene. The octree is augmented with neighbor pointers (creating what we will call a "direct access" octree) to facilitate high-speed traversal through the leaf nodes of the octree. A discrete representation of a scene allows highly efficient ray tracing, since intersection testing is uniform and simple. Representing this discrete scene in an octree allows us to skip large regions of empty space in a relatively short amount of time. Implementing the octree with neighbor pointers allows direct neighbor lookup, and greatly decreases the time required to find the next node that should be processed. In order to approach the level of detail found in most ray-traced scenes, a very high spatial resolution must be used in the rasterization process. Even with the compression inherent to an octree representation, this still requires very large amounts of memory. As with most algorithms, there is a trade-off between speed and memory. The ray tracer presented in this paper sacrifices memory (by rasterizing the scene and adding additional data that must be stored in each octree node) in favor of speed (fast scene intersection calculations and node-to-node traversal). With this trade-off: however, we are able to achieve fairly high-speed renderings with dynamic lighting, shadow, and reflection effects...|$|R
40|$|Abstract—The Standby-Sparing (SS) {{technique}} has been pre-viously explored to improve energy efficiency while providing fault tolerance in <b>dual-processor</b> real-time <b>systems.</b> In this paper, by considering both transient and permanent faults, we develop energy-efficient {{fault tolerance techniques}} for real-time systems deploying an arbitrary number of identical processors. First, we study the Paired-SS technique, where processors are organized as groups of two (i. e., pairs) and SS is applied within each pair of processors directly after partitioning tasks to the pairs. Then, we propose a Generalized-SS technique that partitions processors into two groups containing primary and secondary processors, respectively. The main and backup copies of tasks are executed on the primary and secondary processors under the partitioned-EDF and partitioned-EDL scheduling policies, respectively. The objective {{is to reduce the}} overlapped executions of the main and backup copies in order to improve energy savings. Our experimental evaluations show that, for a given system with fixed number of processors, typically there exists a configuration of primary and secondary processors under the Generalized-SS technique that can lead to better energy savings when compared to the Paired-SS technique. I...|$|R
40|$|In {{this paper}} {{the design and}} first {{measurements}} of a <b>dual-processor</b> embedded <b>system</b> for hand-held cartographic plotter are presented. The system study and relevant hardware/software partitioning are briefly addressed. As a result, two ARM 710 T cores together with several IPs connected by AHB bus have been integrated into a System-on-Chip. The main drivers for this architecture have been the computational power to reduce the cartographic map redraw time, {{the reduction of the}} overall power consumption, which is fundamental for any battery-powered device, and the compatibility with several kinds of LCDs, memories and standard interfaces. Finally, the ASIC design for a 0. 35 m CMOS technology is presented. It results in a overall peak computational power of 66 MIPS @ 33 MHz clock frequency, a standby power consumption of 20 mW for an overall area of 54. 37 mm 2. First measurements indicate an operating power consumption of 330 mW @ 25 MHz. This figure can be reduced, for typical applications, by 50 % by means of the built-in power controller...|$|R
40|$|Abstract—In {{fault-tolerant}} systems, {{the primary}} and backup copies of different tasks can be scheduled together on one processor, where primary tasks should be executed {{as soon as possible}} (ASAP) and backup tasks as late as possible (ALAP) for better performance (e. g., energy efficiency). To address such mixed requirements, in this paper, we propose the concept of preference-oriented execution and study the corresponding scheduling algorithms. Specifically, we formally define the optimality of preference-oriented schedules and show that such schedules may not always exist for general periodic task sets. Then, we propose an ASAP-Ensured Earliest Deadline (SEED) scheduling algorithm, which guarantees to generate an ASAPoptimal schedule for any schedulable task set. Moreover, to incorporate the preference for ALAP tasks, we extend SEED and develop a Preference-Oriented Earliest Deadline (POED) scheduling heuristic. For a <b>dual-processor</b> fault-tolerant <b>system,</b> we illustrate how such algorithms can be exploited to improve the energy savings. We evaluate the proposed schedulers through extensive simulations. The results confirm the optimality of SEED for ASAP tasks. When compared to the well-known EDF scheduler, both SEED and POED can perform better in preferenceoriented settings with reasonable overheads. Moreover, for a dualprocessor fault-tolerant system, significant energy savings (up to 20 %) can be obtained under POED when compared to the stateof-the-art standby-sparing scheme...|$|R
40|$|Power {{has become}} a primary concern for HPC systems. Dynamic voltage and {{frequency}} scaling (DVFS) and dynamic concurrency throttling (DCT) are two software tools (or knobs) for reducing the dynamic power consumption of HPC systems. To date, few works have considered the synergistic integration of DVFS and DCT in performance-constrained systems, and, {{to the best of}} our knowledge, no prior research has developed application-aware simultaneous DVFS and DCT controllers in real systems and parallel programming frameworks. We present a multi-dimensional, online performance prediction framework, which we deploy to address the problem of simultaneous runtime optimization of DVFS and DCT on multi-core systems. We present results from an implementation of the prediction framework in a runtime system linked to the Intel OpenMP runtime environment and running on a real <b>dual-processor</b> quad-core <b>system.</b> We show that the prediction framework derives near-optimal settings of the power-aware program adaptation knobs that we consider. Our overall runtime optimization framework achieves significant reductions in energy (19 % mean) and ED 2 (40 % mean), through simultaneous power savings (6 % mean) and performance improvements (14 % mean). Our prediction and adaptation framework outperforms earlier solutions that adapt only DVFS or DCT, as well as one that sequentially applies DCT then DVFS. Further, our results indicate that prediction-based schemes for runtime adaptation compare favorably and typically improve upon heuristic search-based approaches in both performance and energy savings...|$|R
40|$|Current {{and future}} SoC designs will contain an {{increasing}} number of heterogeneous programmable units combined with a complex communication architecture to meet flexibility, performance and cost constraints. Designing such a heterogeneous MP-SoC architecture bears enormous potential for optimization, but requires a system-level design environment and methodology to evaluate architectural alternatives. This paper proposes a methodology to jointly design and optimize the processor architecture together with the on-chip communi-cation based on the LISA Processor Design Platform in combination with SystemC Transaction Level Models. The proposed methodology advocates a successive refinement flow of the system-level models of both the pro-cessor cores and the communication architecture. This allows design decisions based on the best modeling efficiency, accuracy and simulation performance possible on the respective abstraction level. The effectiveness of our approach is demonstrated by the exemplary design of a <b>dual-processor</b> JPEG decoding <b>system.</b> 1...|$|R
40|$|Elliptic Curve Cryptosystems (ECC) were {{introduced}} in 1985 by Neal Koblitz and Victor Miller. Small key size made elliptic curve attractive for public key cryptosystem implementation. This thesis introduces solutions of efficient implementation of ECC in algorithmic level and in computation level. In algorithmic level, a fast parallel elliptic curve scalar multiplication algorithm based on a <b>dual-processor</b> hardware <b>system</b> is developed. The method has an average computation time of n 3 Elliptic Curve Point Addition on an n-bit scalar. The improvement is n Elliptic Curve Point Doubling compared to conventional methods. When a proper coordinate system and binary representation for the scalar k is used the average execution time will be as low as n Elliptic Curve Point Doubling, which makes this method about two times faster than conventional single processor multipliers using the same coordinate system. In computation level, a high performance elliptic curve processor (ECP) architecture is presented. The processor uses parallelism in finite field calculation to achieve high speed execution of scalar multiplication algorithm. The architecture relies on compile-time detection rather than of run-time detection of parallelism which results in less hardware. Implemented on FPGA, the proposed processor operates at 66 MHz in GF(2 167) and performs scalar multiplication in 100 muSec, which is considerably faster than recent implementations. Dept. of Electrical and Computer Engineering. Paper copy at Leddy Library: Theses 2 ̆ 6 Major Papers - Basement, West Bldg. / Call Number: Thesis 2004. A 57. Source: Masters Abstracts International, Volume: 44 - 03, page: 1446. Thesis (M. A. Sc.) [...] University of Windsor (Canada), 2005...|$|R
40|$|This paper {{reports on}} the results of a {{research}} project, on comparing one virtual collaborative environment with a first-person visual immersion (first-perspective interaction) and a second one where the user interacts through a sound-kinetic virtual representation of himself (avatar), as a stress-coping environment in real-life situations. Recent developments in coping research are proposing a shift from a trait-oriented approach of coping to a more situation-specific treatment. We defined as real-life situation a target-oriented situation that demands a complex coping skills inventory of high self-efficacy and internal or external "locus of control" strategies. The participants were 90 normal adults with healthy or impaired coping skills, 25 - 40 years of age, randomly spread across two groups. There was the same number of participants across groups and gender balance within groups. All two groups went through two phases. In Phase I, Solo, one participant was assessed using a three-stage assessment inspired by the transactional stress theory of Lazarus and the stress inoculation theory of Meichenbaum. In Phase I, each participant was given a coping skills measurement within the time course of various hypothetical stressful encounters performed in two different conditions and a control group. In Condition A, the participant was given a virtual stress assessment scenario relative to a first-person perspective (VRFP). In Condition B, the participant was given a virtual stress assessment scenario relative to a behaviorally realistic motion controlled avatar with sonic feedback (VRSA). In Condition C, the No Treatment Condition (NTC), the participant received just an interview. In Phase II, all three groups were mixed and exercised the same tasks but with two participants in pairs. The results showed that the VRSA group performed notably better in terms of cognitive appraisals, emotions and attributions than the other two groups in Phase I (VRSA, 92 %; VRFP, 85 %; NTC, 34 %). In Phase II, the difference again favored the VRSA group against the other two. These results indicate that a virtual collaborative environment seems to be a consistent coping environment, tapping two classes of stress: (a) aversive or ambiguous situations, and (b) loss or failure situations in relation to the stress inoculation theory. In terms of coping behaviors, a distinction is made between self-directed and environment-directed strategies. A great advantage of the virtual collaborative environment with the behaviorally enhanced sound-kinetic avatar is the consideration of team coping intentions in different stages. Even if the aim is to tap transactional processes in real-life situations, it might be better to conduct research using a sound-kinetic avatar based collaborative environment than a virtual first-person perspective scenario alone. The VE consisted of two <b>dual-processor</b> PC <b>systems,</b> a video splitter, a digital camera and two stereoscopic CRT displays. The system was programmed in C++ and VRScape Immersive Cluster from VRCO, which created an artificial environment that encodes the user's motion from a video camera, targeted at the face of the users and physiological sensors attached to the body...|$|R
40|$|Molecular {{simulations}} {{can provide}} a detailed picture of a desired chemical, physical, or biological process. It has been developed over last 50 years and is being used now to solve a large variety of problems in many different fields. In particular, quantum calculations are very helpful to study small systems at a high resolution where electronic structure of compounds is accounted for. Molecular dynamics simulations, in turn, are employed to study development of a certain molecular ensemble via its development in time and space. Chapter 1 gives a short overview of techniques used today in molecular simulations field, their limitations, and their development. Chapter 2 concentrates on the description of methods used in this work to perform molecular dynamics simulations of cucurbit[6]uril in aqueous and salt solutions as well as metal-isopropanol interface. This is followed by Chapter 3 that outlines main areas in our life where these systems can be used. The development of instruments {{is as important as}} the scientific part of molecular simulations like methods and algorithms. Parallelization procedure of the atomistic molecular dynamics program YASP for shared-memory computer architectures is described in Chapter 4. Parallelization was restricted to the most CPU-time consuming parts: neighbour-list construction, calculation of non-bonded, angle and dihedral forces, and constraints. Most of the sequential FORTRAN code was kept; parallel constructs were inserted as compiler directives using the OpenMP standard. Only in the case of the neighbour list the data structure had to be changed. The parallel code achieves a useful speed-up over the sequential version for systems of several thousand atoms and above. On an IBM Regatta p 690 +, the throughput increases with the number of processors up to a maximum of 12 - 16 processors depending on characteristics of the simulated <b>systems.</b> On <b>dual-processor</b> Xeon <b>systems,</b> the speed-up is about 1. 7. Certainly, these results will be of interest to other scientific groups in academia and industry that would like to improve their own simulation codes. In order to develop a molecular receptor or choose from already existing ones that fits certain needs one must have quite good knowledge of non-covalent host-guest interactions. One also wants to have control over the capture/release process via environment of the receptor (pH, salt concentration, etc.). Chapter 5 is devoted to molecular dynamics simulations preformed to study the microscopic structure and dynamics of cations bound to cucurbit[6]uril (CB[6]) in water and in aqueous solutions of sodium, potassium, and calcium chloride. The molarities are 0. 183 M for the salts, and 0. 0184 M for CB[6]. The cations bind only to CB[6] carbonyl oxygens. They are never found inside the CB[6] cavity. Complexes with Na+ and K+ mostly involve one cation, whereas with Ca 2 + single- and double-cation complexes are formed in similar proportions. The binding dynamics strongly depends on the type of cation. A smaller size or higher charge increases the residence time of a cation at a given carbonyl oxygen. The diffusion dynamics also corresponds to the binding strength of cations: the stronger binding the slower diffusion and reorientation dynamics. When bound to CB[6], sodium and potassium cations jump mainly between nearest or second-nearest neighbours. Calcium shows no hopping dynamics. It is coordinated predominantly by one CB[6] oxygen. A few water molecules (zero to four) can occupy the CB[6] cavity, which is delimited by the CB[6] oxygen faces. Their residence time is hardly influenced by sodium and potassium ions. In the case of calcium the residence time of the inner water increases notably. A simple structural model for the cations acting as “lids” over the CB[6] portal cannot, however, be confirmed. The slowing of the water exchange by the ions is a consequence of the generally slower dynamics in their presence and of their stable solvation shells. The study of binding behaviour of simple hydrophobic (Lennard-Jones) particles by CB[6] showed that these particles do not bind. A simple test showed that the size of hydrophobic particles in this case is important for a stable encapsulation. Another challenging field of research is the metal-organic interfaces. Particularly, transition metals are more difficult as they form chemical bonds, though sometimes very weak, with a large number of organic compounds. In Chapter 6 a molecular dynamics model and its parameterization procedure are devised and used to study adsorption of isopropanol on platinum(111) (Pt(111)) surface in unsaturated and oversaturated coverages regimes. Static and dynamic properties of the interface between Pt(111) and liquid isopropanol are also investigated. The magnitude of the adsorption energy at unsaturated level increases at higher coverages. At the oversaturated coverage (multilayer adsorption) the adsorption energy reduces, which coincides with findings by Panja et al. in their temperature-programmed desorption experiment (ref. 25). The density analysis showed a strong packing of molecules at the interface followed by a depletion layer and then by an oscillating density profile up to 3 nm. The distribution of individual atom types showed that the first adsorbed layer forms a hydrophobic methyl “brush”. This “brush” then determines the distributions further from the surface. In the second layer methyl and methine groups are closer to the surface and are followed by the hydroxyl groups; the third layer has exactly the inverted distribution. The alternating pattern extends up to about 2 nm from the surface. The orientational structure of molecules as a function of distance of molecules is determined by the atoms distribution and surprisingly does not depend on the electrostatic or chemical interactions of isopropanol with the metal surface. However, possible formation of hydrogen bonds in the first layer is notably influenced by these interactions. The surface-adsorbate interactions influence mobility of isopropanol molecules only in the first layer. Mobility in the higher layers is independent of these interactions. Finally, Chapter 7 summarizes main conclusions of the studies presented in this thesis and outlines perspectives of the future research...|$|R

