3942|3403|Public
25|$|Histogram {{equalization}} {{is useful}} in enhancing contrast within an image. This technique is used to increase local contrast. At {{the end of the}} processing, areas that were dark in the <b>input</b> <b>image</b> would be brightened, greatly enhancing the contrast among the features present in the area. On the other hand, brighter areas in the <b>input</b> <b>image</b> would remain bright or be reduced in brightness to equalize with the other areas in the image. Besides vessel segmentation, other features related to diabetic retinopathy can be further separated by using this pre-processing technique. Microaneurysm and hemorrhages are red lesions, whereas exudates are yellow spots. Increasing contrast between these two groups allow better visualization of lesions on images. With this technique, 2014 review found that 10 out of the 14 recently (since 2011) published primary research.|$|E
25|$|Modern {{image intensifiers}} no longer use a {{separate}} fluorescent screen. Instead, a caesium iodide phosphor is deposited {{directly on the}} photocathode of the intensifier tube. On a typical general purpose system, the output image is approximately 105 times brighter than the <b>input</b> <b>image.</b> This brightness gain comprises a flux gain (amplification of photon number) and minification gain (concentration of photons from a large input screen onto a small output screen) each of approximately 100. This level of gain is sufficient that quantum noise, due to {{the limited number of}} X-ray photons, is a significant factor limiting image quality.|$|E
2500|$|... {{and given}} any <b>input</b> <b>image</b> [...] the affine Gaussian scale-space is the three-parameter scale-space defined as ...|$|E
40|$|An image {{processing}} apparatus (100) comprises: receiving means for receiving a video signal representing input images; a film-detector unit (108) for detecting whether the <b>input</b> <b>images</b> have been captured by a film camera or by a video camera; and a sharpness enhancement unit (106) for calculating sharpness enhanced images on basis of the <b>input</b> <b>images.</b> The sharpness enhancement unit (106) {{is controlled by the}} film-detector unit (108), whereby the sharpness enhancement is larger if the film-detector unit (108) has detected that the <b>input</b> <b>images</b> have been captured by the film camera than if the film-detector unit (108) has detected that the <b>input</b> <b>images</b> have been captured by the video camera...|$|R
30|$|The symbol I(v) {{represents}} the intensity values taken {{directly from the}} <b>input</b> <b>images.</b> The G(v) symbol stands for the GVF [7] values, which are calculated using the GVF algorithm over the <b>input</b> <b>images.</b> Similarly, the E d g(v) symbol {{represents the}} values from the edge detector, which are defined using the Canny Edge Detector algorithm [27] over the <b>input</b> <b>images.</b> Similar to Equation (1), the symbols γ,δ, and ε represent the weights of particular components.|$|R
40|$|The {{widespread}} use of super-resolution methods, {{in a variety of}} applications such as surveillance has led to an increasing need for or quality assessment measures. The current quality measures aim to compare different fusion methods by assessing the quality of the fused images. They consider the information transferred between the super-resolved <b>image</b> and <b>input</b> <b>images</b> only. In this paper, we propose an objective quality evaluation algorithm for super-resolved images, which focuses on evaluating the quality of super-resolved images that are constructed from different conditions of <b>input</b> <b>images.</b> The proposed quality evaluation method combines both the relationship between the super-resolved <b>image</b> and the <b>input</b> <b>images,</b> and the relationship between the <b>input</b> <b>images.</b> Using the proposed measure, the quality of the super-resolved face images constructed from videos are evaluated under different conditions, including the variation of pose, lighting, facial expressions and the number of <b>input</b> <b>images.</b> ...|$|R
2500|$|... pngout and zopflipng {{provide an}} option to preserve/reuse the line-by-line filter set present in the <b>input</b> <b>image.</b>|$|E
50|$|An {{image is}} {{considered}} high resolution when it measures 128x96 pixels. Therefore, {{the goal of}} face hallucination {{is to make the}} <b>input</b> <b>image</b> reach that number of pixels. The most common values of the <b>input</b> <b>image</b> is usually 32x24 pixels or 16x12 pixels.|$|E
50|$|I = imread('I_RGB.png'); %input color image%Output_image = color_channel(I),%where color channel {{could be}} red, green or green. The three output images%are {{extracted}} from <b>input</b> <b>image</b> as followsred_channel = I(:,:,1);green_channel = I(:,:,2);blue_channel = I(:,:,3);%Output image = grayscale_image(I).%Note if <b>input</b> <b>image</b> I {{was already a}} grayscale image, grayscale channel%would have simply been equal to <b>input</b> <b>image,</b> ie, gray channel = Igray_channel = rgb2gray(I);It {{is clear from the}} above examples that a channel can be generated by either simply extracting specific information from the original image or by manipulating the <b>input</b> <b>image</b> in some form to obtain the desired channel. Dollár et al. defined a channel generation function as Ω, which can be used to relate a channel (that is, an output image) to the original image as follows.|$|E
30|$|It detects faces from <b>input</b> <b>images.</b>|$|R
40|$|Abstract. We {{propose a}} method for synthesizing face views with {{arbitrary}} poses and expressions combining multiple face images. In this method, arbitrary views of a 3 -D face can be generated without explicit reconstruction of its 3 -D shape. The 2 -D coordinate values {{of a set of}} feature points in an arbitrary facial pose and expression can be represented as a linear combination of those in the <b>input</b> <b>images.</b> Face images are synthesized by mapping the blended texture from multiple <b>input</b> <b>images.</b> By using the <b>input</b> <b>images</b> which have the actual facial expressions, realistic face views can be synthesized. ...|$|R
40|$|The goal {{of image}} fusion is {{to obtain a}} fused image that {{contains}} most significant information in all <b>input</b> <b>images</b> which were captured by different sensors from the same scene. In particular, the fusion process should improve the contrast and keep the integrity of significant features from <b>input</b> <b>images.</b> In this paper, we propose a region-based image fusion method to fuse spatially registered visible and infrared images while improving the contrast and preserving the significant features of <b>input</b> <b>images.</b> At first, the proposed method decomposes <b>input</b> <b>images</b> into base layers and detail layers using a bilateral filter. Then the base layers of the <b>input</b> <b>images</b> are segmented into regions. Third, a region-based decision map is proposed to represent the importance of every region. The decision map is obtained by calculating the weights of regions according to the gray-level difference between each region and its neighboring regions in the base layers. At last, the detail layers and the base layers are separately fused by different fusion rules {{based on the same}} decision map to generate a final fused image. Experimental results qualitatively and quantitatively demonstrate that the proposed method can improve the contrast of fused images and preserve more features of <b>input</b> <b>images</b> than several previous image fusion methods...|$|R
5000|$|... {{corresponding}} <b>input</b> <b>image</b> pixels {{are found}} {{relative to the}} kernel's origin.|$|E
5000|$|... {{checks to}} see if the painted strokes {{accurately}} represent the <b>input</b> <b>image</b> ...|$|E
5000|$|... {{generates the}} next set of strokes to more {{accurately}} match the <b>input</b> <b>image</b> ...|$|E
30|$|Repeatability {{of local}} feature regions in <b>input</b> <b>images.</b>|$|R
30|$|Counting {{individual}} local feature correspondences in <b>input</b> <b>images.</b>|$|R
30|$|Step 3 : Decompose the <b>input</b> <b>images</b> using Laplacian pyramid decomposition.|$|R
5000|$|Both {{techniques}} {{involve the}} real time {{execution of a}} spatial transformation from the <b>input</b> <b>image</b> to the output image, and both techniques require powerful hardware. The spatial transformation must be pre-defined for a particular desired geometric, and may be calculated by several different methods (more to follow). [...] In Graphics Processing, the spatial transformation consists of a polygon mesh (usually triangles). The transformation is executed by texture mapping from the rectilinear mesh of the <b>input</b> <b>image</b> to the transformed shape of the destination image. Each polygon on the <b>input</b> <b>image</b> is thus applied to an equivalent (but transformed in shape and location) polygon in the output image.|$|E
5000|$|First, SIFT {{features}} are {{obtained from the}} <b>input</b> <b>image</b> using the algorithm described above.|$|E
5000|$|Compute {{multiple}} registered image channels from an <b>input</b> <b>image,</b> using linear and non-linear transformations ...|$|E
40|$|In a motion-compensated {{processing}} of <b>images,</b> <b>input</b> <b>images</b> are down-scaled (scl) to obtain down-scaled images, the down-scaled images {{are subjected to}} motion- compensated processing (ME UPC) to obtain motion-compensated images, the motion- compensated images are up-scaled (sc 2) to obtain up-scaled motion-compensated images; and the up-scaled motion-compensated images are combined (M) with the <b>input</b> <b>images</b> to obtain output images...|$|R
40|$|The {{centroid}} method for the correction of turbulence consists in computing the Karcher-Fréchet {{mean of the}} sequence of <b>input</b> <b>images.</b> The direction of deformation between a pair of images {{is determined by the}} optical flow. A distinguishing feature of the {{centroid method}} is that it can produce useful results from an arbitrarily small set of <b>input</b> <b>images...</b>|$|R
40|$|Image fusion is {{a process}} of {{combining}} relevant information from <b>input</b> <b>images.</b> Several image fusion techniques are available and are used according to the application. Now-a-days advanced sensors are used for image acquisition. However these sensors usually cannot capture whole information. Hence images from different sensors are combined together to produce more informative image. When image fusion algorithm is applied, different solutions are available. Thus it is necessary to select an optimal solution for image fusion. This optimal solution fuses the <b>input</b> <b>images</b> giving a fused image which contains more information than either <b>input</b> <b>images.</b> Genetic algorithm is an optimization method used for searching solution of large number of problems. This paper gives a brief overview of image fusion techniques using genetic algorithms...|$|R
50|$|Here is {{an example}} of an <b>input</b> <b>image</b> and how edge {{detection}} would modify it.|$|E
5000|$|... pngout and zopflipng {{provide an}} option to preserve/reuse the line-by-line filter set present in the <b>input</b> <b>image.</b>|$|E
5000|$|Given <b>input</b> <b>image</b> vector , {{the mean}} image vector from the {{database}} , calculate {{the weight of}} the kth eigenface as: ...|$|E
40|$|Filtering module for {{transforming}} {{one or more}} <b>input</b> <b>images</b> into {{at least}} one synthesized output image is described wherein one or more <b>input</b> <b>images</b> are generated by an optical- digital imaging system associated with a non-filtered optical transfer function and wherein said filter module comprises: one or more filtering functions that are obtained from minimizing {{at least part of}} an objective function, wherein said objective function is a sum of terms, preferably a weighted sum of terms, wherein a term comprises one or more harmonic components of said one or more <b>input</b> <b>images</b> and one or more associated harmonic components of said output image, and, wherein at least part of said terms is minimum if the ratio between said one or more harmonic components of said one or more <b>input</b> <b>images</b> and output image matches the ratio between a (spatial); frequency bound of said non-filtered optical transfer function and said non-filtered optical transfer function. ImPhys/Imaging PhysicsApplied Science...|$|R
30|$|According to the <b>input</b> <b>images,</b> the {{multiple}} networks could be roughly divided into multi-view, multi-patch, and multi-task.|$|R
40|$|Regularizing images under a {{guidance}} signal {{has been}} used in various tasks in computer vision and computational photography, particularly for noise reduction and joint up-sampling. The aim is to transfer fine structures of guidance signals to <b>input</b> <b>images,</b> restoring noisy or altered struc-tures. One of main drawbacks in such a data-dependent framework {{is that it does not}} handle differences in structure between guidance and <b>input</b> <b>images.</b> We address this prob-lem by jointly leveraging structural information of guid-ance and <b>input</b> <b>images.</b> Image filtering is formulated as a nonconvex optimization problem, which is solved by the majorization-minimization algorithm. The proposed algo-rithm converges quickly while guaranteeing a local mini-mum. It effectively controls image structures at different scales and can handle a variety of types of data from differ-ent sensors. We demonstrate the flexibility and effectiveness of our model in several applications including depth super-resolution, scale-space filtering, texture removal, flash/non-flash denoising, and RGB/NIR denoising...|$|R
5000|$|The white top-hat {{transform}} returns an image, containing those [...] "objects" [...] or [...] "elements" [...] of an <b>input</b> <b>image</b> that: ...|$|E
5000|$|The input {{plane is}} defined as the locus of all points such that z = 0. The <b>input</b> <b>image</b> f is {{therefore}} ...|$|E
5000|$|Typically, a [...] "channel" [...] {{refers to}} a certain {{component}} that defines pixel values in a digital image. A color image, for example is an aggregate of three channels (red, green and blue). The color data of an image is stored in three arrays of values, known as channels. While this definition of a [...] "channel" [...] is widely accepted across various domains, there exists a broader definition in computer vision, which allows one to exploit other features of an image besides the color information. One such definition {{refers to a}} channel as a registered map of the original image where the output pixels are mapped to input pixels by some linear or non-transformation. According to this notion of a channel, color channels of an image can be redefined as output images that are obtained by extracting one specific color information point from the <b>input</b> <b>image</b> at a time. Similarly, a channel for a grayscale <b>input</b> <b>image</b> is simply equal to a grayscale <b>input</b> <b>image.</b> The simple MATLAB implementation below shows how color channels and grayscale channel can be extracted from an <b>input</b> <b>image.</b>|$|E
30|$|As {{with the}} {{previous}} approach (Section 3.4) <b>input</b> <b>images</b> to the weight calculation step are pre-filtered using a median filter.|$|R
30|$|Step 1 : Calculate {{the image}} quality {{measures}} defined (Equations 3 and 4) above {{for each of}} the <b>input</b> <b>images.</b>|$|R
50|$|PTtiff2psd†: Takes {{a series}} of <b>input</b> <b>images</b> and creates a Photoshop PSD file where each input file is a layer.|$|R
