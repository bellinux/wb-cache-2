341|1050|Public
40|$|Abstract:- This {{paper is}} a {{principal}} idea of case-based reasoning to <b>feature</b> <b>weighting.</b> The <b>feature</b> <b>weighting</b> method called CaDFeW (CAse-based Dynamic <b>FEature</b> <b>Weighting)</b> stores classification performance of randomly generated feature weight vectors. Also it retrieve similar <b>feature</b> <b>weighting</b> success story from the <b>feature</b> <b>weighting</b> case base and then designs a better feature weight vector dynamically for the a new input problem while solving the problem. The CaDFeW is wrapper model-based <b>feature</b> <b>weighting</b> method that uses classifier error rate as evaluation procedure. To explain the results of applications, this paper is introduced a new definition of input dependency of feature relevance and measured the new concept in the application domains. The empirically measured results showed that relative performance of a local <b>feature</b> <b>weighting</b> method to a global <b>feature</b> <b>weighting</b> method...|$|E
40|$|<b>Feature</b> <b>weighting</b> is a {{technique}} used to approximate the optimal degree of influence of individual features. This paper presents a <b>feature</b> <b>weighting</b> method for Document Image Retrieval System (DIRS) based on keyword spotting. In this method, we weight the feature using coefficient of multiple correlations. Coefficient of multiple correlations {{can be used to}} describe the synthesized effects and correlation of each feature. The aim {{of this paper is to}} show that <b>feature</b> <b>weighting</b> increases the performance of DIRS. After applying the <b>feature</b> <b>weighting</b> method to DIRS the average precision is 93. 23 % and average recall become 98. 66 % respectivel...|$|E
40|$|In text classification, <b>feature</b> <b>weighting</b> is a main step of preprocessing. Commonly used <b>feature</b> <b>weighting</b> methods only {{consider}} {{the distribution of}} a feature in the documents and do not {{consider the}} class information for <b>feature</b> <b>weighting.</b> Mutual Information (MI) method which represents the dependency of a feature in the regarding class, has been previously used for feature selection. The aim {{of this paper is}} to show that the use of MI method for <b>feature</b> <b>weighting</b> increases the performance of text classification, in terms of average recall and average precision. While K-nearest neighbor classifier is employed for classification, the average recall is increased about 18 % and average precision is increased about 10 %. It is shown that the results for average precision and average recall become 91. 7 % and 89. 29 % respectively...|$|E
40|$|Trained by maximizing {{likelihood}} {{of data and}} class CRF Model Trained by maximizing conditional {{likelihood of}} classes Features are assumed independent Dependency on features taken account by <b>feature</b> <b>weights</b> <b>Feature</b> <b>weights</b> set independently <b>Feature</b> <b>weights</b> are set mutually Good for sequence prediction 4 Max Ent Model vs. CRF Model Both are types of log-linear model...|$|R
40|$|Text {{categorization}} is {{an important}} application of machine learning {{to the field of}} document information retrieval. Most machine learning methods treat text documents as a feature vectors. We report text categorization accuracy for different types of features and different types of <b>feature</b> <b>weights.</b> The comparison of these classifiers shows that stemmed or un-stemmed single words as features give better classifier performance compared with other types of features, and LOG(tf) IDF <b>weight</b> as <b>feature</b> <b>weight</b> gives better classifier performance than other types of <b>feature</b> <b>weights...</b>|$|R
3000|$|One more step {{to rebuild}} the {{unsupervised}} lexica/chunks with insights from the model’s <b>feature</b> <b>weights</b> [...]...|$|R
40|$|<b>Feature</b> <b>weighting</b> {{is known}} empirically to improve {{classification}} accuracy for k-nearest neighbor classifiers in tasks with irrelevant features. Many <b>feature</b> <b>weighting</b> algorithms {{are designed to}} work with symbolic features, or numeric features, or both, but {{cannot be applied to}} problems with features that do not fit these categories. This paper presents a new k-nearest neighbor <b>feature</b> <b>weighting</b> algorithm that works with any kind of feature for which a distance function can be defined. Applied to an image classification task with unusual set-like features, the technique improves classification accuracy significantly. In tests on standard data sets from the UCI repository, the technique yields improvements comparable to weighting features by information gain. Keywords: <b>feature</b> <b>weighting,</b> k-nearest neighbors, unusual feature types. 1 Introduction Automatic classification {{is one of the most}} well-studied areas in machine learning. Researchers regularly introduce new supervised learning [...] ...|$|E
40|$|In this paper, we {{investigate}} {{the use of}} a hybrid genetic <b>feature</b> <b>weighting</b> and selection (GEFeWS) algorithm for multi-biometric recognition. Our results show that GEFeWS is able to achieve higher recognition accuracies than using genetic-based feature selection (GEFeS) alone, while using significantly fewer features to achieve approximately the same accuracies as using genetic-based <b>feature</b> <b>weighting</b> (GEFeW) ...|$|E
40|$|Abstract — Feature {{selection}} is of considerable importance in data mining and machine learning, especially for high dimensional data. In this paper, we propose a novel nearest neighbor-based <b>feature</b> <b>weighting</b> algorithm, which learns a <b>feature</b> <b>weighting</b> vector by maximizing the expected leave-one-out classification accuracy with a regularization term. The algorithm makes no parametric {{assumptions about the}} distribution of the data and scales naturally to multiclass problems. Experiments conducted on artificial and real data sets demonstrate that the proposed algorithm is largely insensitive to {{the increase in the number}} of irrelevant features and performs better than the state-of-the-art methods in most cases. Index Terms — feature selection, <b>feature</b> <b>weighting,</b> nearest neighbor I...|$|E
40|$|Minor bugfixes, including: Fixed a bug where DDTBOX crashed during group-level {{analysis}} of <b>feature</b> <b>weights</b> when using spatiotemporal decoding. Fixed a bug where <b>feature</b> <b>weight</b> analysis p-values output from the permutation test multiple comparisons correction were not copied into the correct MATLAB variable. Fixed a bug where the adjusted critical {{alpha for the}} Benjamini-Hochberg procedure for decoding performance analyses was erroneously copied into the wrong output variable...|$|R
40|$|Abstract — In this paper, {{we present}} a novel {{relevance}} feedback method for Content-Based Image Retrieval systems based on dynamic <b>feature</b> <b>weights.</b> The proposed method utilizes intracluster and inter-cluster information for representing the descriptive and discriminative properties of the features according to the labeled images by the user. Afterwards, <b>feature</b> <b>weights</b> are updated dynamically according to the user’s preferences for improving retrieval results. The proposed method has been thoroughly evaluated and selected results are illustrated in the paper. It is shown that, satisfactory improvements can be achieved with small number of iterations and labeled samples. Furthermore, it is a low-complex and flexible method {{that can be used}} on various databases and Content-Based Image Retrieval applications. Relevance feedback, content-based image retrieval, low-level <b>features,</b> <b>feature</b> <b>weights.</b> I...|$|R
3000|$|... [*]∈[*]Rd[*]×[*]u {{denotes the}} <b>feature</b> <b>weight</b> matrix, and u denotes {{the count of}} the {{selected}} features. The constraint P [...]...|$|R
40|$|Aiming at {{improving}} the well-known fuzzy compactness and separation algorithm (FCS), this paper proposes a new clustering algorithm based on <b>feature</b> <b>weighting</b> fuzzy compactness and separation (WFCS). In {{view of the}} contribution of features to clustering, the proposed algorithm introduces the <b>feature</b> <b>weighting</b> into the objective function. We first formulate the membership and <b>feature</b> <b>weighting,</b> and analyze the membership of data points falling on the crisp boundary, then give the adjustment strategy. The proposed WFCS is validated both on simulated dataset and real dataset. The experimental results demonstrate that the proposed WFCS has the characteristics of hard clustering and fuzzy clustering, and outperforms many existing clustering algorithms with respect to three metrics: Rand Index, Xie-Beni Index and Within-Between(WB) Index...|$|E
40|$|Abstract. It is {{acknowledged}} that overfitting {{can occur in}} feature selection using the wrapper method {{when there is a}} limited amount of training data available. It has also been shown that the severity of overfitting is related to the intensity of the search algorithm used during this process. We demonstrate that the problem of overfitting in <b>feature</b> <b>weighting</b> can be exacerbated if the <b>feature</b> <b>weighting</b> is fine grained. With greater representational power we risk learning not only the signal, but also the idiosyncrasies of the training data. In this paper we show that both of these effects can be ameliorated by the early-stopping strategy we present. Using this strategy <b>feature</b> <b>weighting</b> will outperform feature selection in most cases. ...|$|E
40|$|This paper explores feature {{scoring and}} {{selection}} based on weights from linear classification models. It investigates how these methods combine with various learning models. Our comparative analysis includes three learning algorithms: Naïve Bayes, Perceptron, and Support Vector Machines (SVM) {{in combination with}} three <b>feature</b> <b>weighting</b> methods: Odds Ratio, Information Gain, and weights from linear models, the linear SVM and Perceptron. Experiments show that feature selection using weights from linear SVMs yields better classification performance than other <b>feature</b> <b>weighting</b> methods when combined with the three explored learning algorithms. The results support the conjecture {{that it is the}} sophistication of the <b>feature</b> <b>weighting</b> method rather than its apparent compatibility with the learning algorithm that improves classification performance...|$|E
30|$|Psychological {{assessment}} procedures {{and data collection}} of anthropometric clinical <b>features</b> (<b>weight,</b> height, and BMI) were performed by the researchers.|$|R
5000|$|Minkowski {{weighted}} k-means automatically calculates cluster specific <b>feature</b> <b>weights,</b> {{supporting the}} intuitive {{idea that a}} feature may have different degrees of relevance at different <b>features.</b> These <b>weights</b> {{can also be used}} to re-scale a given data set, increasing the likelihood of a cluster validity index to be optimized at the expected number of clusters.|$|R
40|$|When {{employing}} a similarity function {{to measure the}} similarity between two cases, one large problem is how to determine the <b>feature</b> <b>weights.</b> This paper presents a new method for learning <b>feature</b> <b>weights</b> in a similarity function from the given similarity information. The similarity information {{can be divided into}} two kinds: One is called qualitative similarity information which represents the similarities between cases. The other is called relative similarity information which represents the relation between similarities of two case pairs both including a same case. We apply genetic algorithms to learn <b>feature</b> <b>weights</b> from these two kinds of information respectively. The proposed genetic algorithms are applicable to both linear and nonlinear similarity functions. Our experiments show the learning results are better even if the given similarity information include errors. 1 Introduction Similarity measures {{play a central role in}} case-based reasoning[Kolodner, 1992]. Most case-based sys [...] ...|$|R
40|$|This paper {{introduces}} an algorithm {{that combines}} nave Bayes classification with <b>feature</b> <b>weighting.</b> Most of the related approaches to feature transformation for nave Bayes suggest various heuristics and non-exhaustive search strategies for selecting {{a subset of}} features with which nave Bayes performs better than with the complete set of features. In contrast, the algorithm introduced in this paper employs <b>feature</b> <b>weighting</b> performed by a support vector machine. The weights are optimised such that the danger of overfitting is reduced. To {{the best of our}} knowledge, {{this is the first time}} that nave Bayes classification has been combined with <b>feature</b> <b>weighting.</b> Experimental results on 15 UCI domains demonstrate that WBC SVM compares favourably to state-of-the-art machine learning approaches...|$|E
40|$|Abstract. We propose {{and analyze}} new fast <b>feature</b> <b>weighting</b> {{algorithms}} based on {{different types of}} feature ranking. <b>Feature</b> <b>weighting</b> may be much faster than feature selection {{because there is no}} need to find cut-threshold in the raking. Presented weighting schemes may be combined with several distance based clas-sifiers like SVM, kNN or RBF network (and not only). Results shows that such method can be successfully used with classifiers...|$|E
40|$|In this paper, {{we propose}} a <b>feature</b> <b>weighting</b> method that works {{in both the}} input space and the kernel-induced feature space. It assumes only the {{availability}} of similarity (dissimilarity) information, {{and the number of}} parameters in the transformation does not depend on the number of features. Besides <b>feature</b> <b>weighting,</b> it can also be regarded as performing nonparametric kernel adaptation. Experimental results on both toy and real-world datasets show promising results...|$|E
40|$|In text classification, {{the purity}} of the Gini index can be used. When purity value is greater, the {{characteristic}} of the information contained in the attribute is higher, and the feature distinguishing capability is stronger. But using the Gini purity formula on <b>feature</b> <b>weight,</b> the classification result is not very good, {{one of the main reasons}} is those rare words only appearing in one category and not appearing in other categories can not be strictly differentiated. In order to solve this problem, On the basis of Gini index, an improved <b>feature</b> <b>weight</b> method based on Gini index has proposed. By introducing the approximation quality of features term in the categories, according to the category distinguishing ability adjust term weight, using the purity formula <b>feature</b> <b>weight</b> comparison, the above problem is well solved, which can effectively improve the performance of text classification. The experiments have verified the feasibility of the proposed method...|$|R
40|$|We {{present an}} {{approach}} to <b>feature</b> <b>weight</b> optimization for document-level decoding. This is an essential task for enabling future development of discourse-level statistical machine translation, as it allows easy integration of discourse features in the decoding process. We extend the framework of sentence-level <b>feature</b> <b>weight</b> optimization to the document-level. We show experimentally {{that we can get}} competitive and relatively stable results when using a standard set of features, and that this framework also allows us to optimize document- level features, which can be used to model discourse phenomena...|$|R
40|$|We {{can learn}} a lot about what {{features}} are important for retrieval by comparing similar cases in a case-base. We can determine which features are important in predicting outcomes and we can assign <b>weights</b> to <b>features</b> accordingly. In the same manner we can discover which features are important in specific contexts and determine localised <b>feature</b> <b>weights</b> that are specific to individual cases. In this paper we describe a comprehensive set of techniques for learning local <b>feature</b> <b>weights</b> and we evaluate these techniques on a case-base for conflict resolution in air traffic control. We show how introspective learning of <b>feature</b> <b>weights</b> improves retrieval and how {{it can be used to}} determine context sensitive local weights. We also show that introspective learning does not work well in case-bases containing only pivotal cases because there is no redundancy to be exploited. 2 Using introspective learning to improve retrieval in CBR: A case study in air traffic control 2 2 This [...] ...|$|R
40|$|Nearest {{neighbour}} (NN) is a {{very common}} classifier used to develop important remote sensing products like land use and land cover (LULC) maps. Evolutive computation has often been used to obtain <b>feature</b> <b>weighting</b> {{in order to improve}} the results of the NN. In this paper, a new algorithm based on evolutionary computation which has been called Label Dependent <b>Feature</b> <b>Weighting</b> (LDFW) is proposed. The LDFW method transforms the feature space assigning different weights to every feature depending on each class. This multilevel <b>feature</b> <b>weighting</b> algorithm is tested on remote sensing data from fusion of sensors (LIDAR and orthophotography). The results show an improvement on the NN and resemble the results obtained with a neural network which is the best classifier for the study area...|$|E
40|$|Content based labels, {{associated}} with image sequences in contemporary video indexing methods, can be textual, numerical {{as well as}} abstract, including colour-histograms and motion co-occurrence matrices. Abstract features or indices are not explicitly numeric entities but rather are composed of numeric entities. When multiple abstract features are involved, distance metrics between image sequences need to be weighted. Most <b>feature</b> <b>weighting</b> methods in the literature assume that the space is numeric (either discrete or continuous) and so not applicable to abstract <b>feature</b> <b>weighting.</b> This paper elaborates some <b>feature</b> <b>weighting</b> methods applicable to abstract features and both binary (feature selection) and real-valued weighting methods are discussed. The performance of different feature selection and weighting methods are provided and a comparative study based on motion classification-experiments is presented...|$|E
40|$|<b>Feature</b> <b>weighting</b> can be {{considered}} an extension of feature selection. Traditional methods of <b>feature</b> <b>weighting</b> assume that feature relevance is invariant over the task's domain. As a result, they learn a single set of weight for the entire data set. In this paper, a proposed algorithm has been used, which is called Simultaneous Clustering and Attribute Discrimination (SCAD) and performs clustering and <b>feature</b> <b>weighting</b> simultaneously. First, the algorithm is analyzed in detail, on this basis, {{through a series of}} compare experiments, confirms this algorithm to have the high clustering precision; Finally, the algorithm is applied in the analyzing of the bank loan repaid information, that can efficiently discover the weight association of the main factors in loan information and realize potential customer...|$|E
30|$|We {{proposed}} three improved weight {{calculation methods}} based on TF-IDF. The experimental {{results show that}} using the maximum value of the synonym group as the <b>feature</b> <b>weight</b> is the best way.|$|R
40|$|Abstract. Local {{support vector machine}} {{gives the}} <b>feature</b> same <b>weight</b> in classification. In fact, many {{datasets}} have some weak or irrelevant features related to the classification. Thus giving <b>features</b> same <b>weight</b> may reduce the classification accuracy of local support vector machine. This paper puts forward a new local support vector machine that the <b>feature</b> <b>weight</b> is optimized by PSO (Particle Swarm Optimization), it is tested on the international standard UCI data sets and the images of tree taxonomy data sets, {{the results show that}} the accuracy of the algorithm we proposed is better than the general local support vector machine...|$|R
40|$|This is {{the first}} paper on textual case-based {{reasoning}} to employ collective classification, a methodology for simultaneously classifying related cases that has consistently attained higher accuracies than standard classification approaches when cases are related. Thus far, case-based classifiers have not been examined for their use in collective classification. We introduce Case-Based Collective Classification (CBCC) and report that it outperforms a traditional case-based classifier on three tasks. We also address issues of case representation and <b>feature</b> <b>weight</b> learning for CBCC. In particular, we describe a cross-validation approach for tuning <b>feature</b> <b>weights</b> and show that it increases CBCC accuracy on these tasks...|$|R
40|$|TCD-CS- 2005 - 41 It is {{acknowledged}} that overfitting {{can occur in}} feature selection using the wrapper method {{when there is a}} limited amount of training data available. It has also been shown that the severity of overfitting is related to the intensity of the search algorithm used during this process. We demonstrate that the problem of overfitting in <b>feature</b> <b>weighting</b> can be exacerbated if the <b>feature</b> <b>weighting</b> is fine grained. With greater representational power we risk learning not only the signal, but also the idiosyncrasies of the training data. In this paper we show that both of these effects can be ameliorated by the early-stopping strategy we present. Using this strategy <b>feature</b> <b>weighting</b> will outperform feature selection in most cases...|$|E
40|$|Abstract — In this paper, {{we propose}} a <b>feature</b> <b>weighting</b> method that works {{in both the}} input space and the kernel-induced feature space. It assumes only the {{availability}} of similarity (dissimilarity) information, {{and the number of}} parameters in the transformation does not depend on the number of features. Besides <b>feature</b> <b>weighting,</b> it can also be regarded as performing nonparametric kernel adaptation. Experimental results on both toy and real-world datasets show promising results. I...|$|E
40|$|<b>Feature</b> <b>weighting</b> or {{selection}} {{is a crucial}} process to identify an important subset of features from a data set. Removing irrelevant or redundant features can improve the generalization performance of ranking functions in information retrieval. Due to fundamental differences between classification and ranking, <b>feature</b> <b>weighting</b> methods developed for classification cannot be readily applied to <b>feature</b> <b>weighting</b> for ranking. A {{state of the art}} feature selection method for ranking, called GAS, has been recently proposed, which exploits importance of each feature and similarity between every pair of features. However, GAS must compute the similarity scores of all pairs of features, thus it is not scalable for high-dimensional data and its performance degrades on nonlinear ranking functions. This paper proposes novel algorithms, RankWrapper and RankFil-ter, which is scalable for high-dimensional data and also perform...|$|E
30|$|We {{consider}} the MSN as another example (see Fig.  5), {{where some of}} the actors along with their common characteristic <b>features,</b> <b>weights,</b> and relations used in the MSN are shown in Table  5.|$|R
40|$|In this paper, a new {{clustering}} {{method is}} applied to segmentation and volume change assessment in lateral ventricles affected by hydrocephalus pathology in the infants. The hydrocephalus is characterized as such, which results from excessive accumulation of cerebrospinal fluid in the ventricles, leading to enlargement of cerebral ventricles. The complex shape o the ventricular system is evaluated by a visual assessment of MRI scans, but still there is necessity to note the amount of change in volume. We present a algorithm with adjustable <b>feature</b> <b>weight</b> with Optimal <b>feature</b> selection using validity function. The algorithm is called Optomal <b>feature</b> <b>Weight</b> adjustable FCM with Gaussian smoothing (OfwaFCM). There was need to develop new algorithm for segmenting non-symmetric images. The objective of this algorithm is to produce fine clustering results and to reduce effect of noise. In this method, clustering centroids and functions are {{used to evaluate the}} clustering results and validity fuction based on fuzzy partitioning is used to select Optimal <b>Feature</b> <b>Weights.</b> The results produced states that the proposed scheme provides Volume calculation and clustering performances for Detecting and segmenting the hydrocephalus. Keywords...|$|R
40|$|Objectives: Little {{is known}} about men with eating {{disorders}} (EDs) in primary care. The present study objectives were to (i) assess ED prevalence, ED <b>features,</b> <b>weight,</b> co-morbidities, exercise patterns, sexual orientation, and help-seeking in men attending a general practice setting and (ii) compare the results with a similar study of women. Method: Five hundred men chosen randomly from two general practices were screened with the Eating Disorders Examination-Questionnaire (EDE-Q), with an extra question concerning exercise for weight or shape control. Subsequently 50 subjects were interviewed to confirm psychiatric diagnoses and assess clinical <b>features,</b> <b>weight,</b> help-seeking for EDs and exercise. Results: The prevalence of eating disorders was 1. 2...|$|R
