0|10000|Public
40|$|Abstract [...] The {{expansive}} soil will be swelling upon wetting and shrinnking to drying. This behavior can damage of construction structures, particularly, light buildings and roads. The phenomenom {{is very interesting}} to be researched, how far the effect of water contents is towards the behavior of free swelling of {{expansive soil}}. The material of expansive soil are selected from Karang Jati Ngawi region East Java Province. The expansive soils result in a variation of plasticity index and activity in the soil from Karang Jati (Ngawi) mixed with Na-bentonite. The research on free swelling is done by being remolded with diameter 6. 35 cm and high 1. 70 cm, with initial water content of 32 % and dry density of 1. 26 g/cm 3. An electronic digital caliper is used to measure free swelling. Results of the research indicate that the water contents have a linear relation with <b>free</b> swelling. <b>Index</b> <b>Term</b> [...] The expansive soil, Na-bentonite, water content, free swelling. I...|$|R
40|$|Abstract — WSN is a {{group of}} {{wireless}} nodes in which each node communication with each other. Each sensor node comprises sense, giving out transmission, location finding structure, and control units. Sensor nodes are typically spread in sensor field, which is an area where the sensor nodes are deployed. Security is an important feature for the operation of wireless sensor network. In sensor system the attacker vampire attacks watch the whole system each node activity. In this paper we expected a protection scheme against attacker and false information of unique node. It means attackers are not drop the data packet of the node. The security scheme has showing the better performance of that is prove by simulation result. The security scheme has recovered the network presentation in company of attacker and offer attacks <b>free</b> network environment. <b>Index</b> <b>Terms</b> — WSN I...|$|R
40|$|With current technology, {{high quality}} recaptured {{images can be}} created from soft displays, such as an LCD monitor, using a digital still camera and {{professional}} image editing software. The task of verifying the ownership and past history of an image is, consequently, more difficult. One approach to detecting an image that has been recaptured from an LCD monitor is {{to search for the}} presence of aliasing due to the sampling of the monitor pixel grid. To validate this approach, an investigation into the aliasing introduced in a digitally recaptured image is conducted. An anti-forensic method for recapturing images that are free from aliasing is developed using a model of the image acquisition process. This is supported by a simulation of the acquisition process and illustrated with examples of recaptured images that are <b>free</b> from aliasing. <b>Index</b> <b>Terms</b> — forensic imaging, recapture, Bayer CFA, digital camer...|$|R
40|$|Phrase {{browsing}} applications {{provide information}} seekers {{with access to}} text content via structured lists of <b>index</b> <b>terms.</b> The <b>index</b> <b>terms,</b> which may be identified {{by a variety of}} techniques, are phrases that have been automatically extracted from full text documents. Browsing applications support interactive navigation of <b>index</b> <b>terms</b> and provide direct access to the original documents via the <b>index</b> <b>terms.</b> Term...|$|R
50|$|Many {{journals}} and databases provides access (also) to <b>index</b> <b>terms</b> made by authors to the articles being published or represented. The relative quality of indexer-provided <b>index</b> <b>terms</b> and author provided <b>index</b> <b>terms</b> {{is of interest}} to research in information retrieval. The quality of both kinds of <b>indexing</b> <b>terms</b> depends, of course, on the qualifications of provider. In general authors have difficulties providing <b>indexing</b> <b>terms</b> that characterizes his document relative to the other documents in the database. Author keywords {{are an integral part}} of literature.|$|R
40|$|AbstractThe {{purpose of}} this study is to reveal the {{characteristics}} of the terminological structure formed by the <b>index</b> <b>terms</b> of junior-high school, high school and university-level textbooks. We first identify the types of concept of <b>index</b> <b>terms,</b> and then uncover the conceptual structure that underlies <b>index</b> <b>terms.</b> We found that, as the school level progresses, (1) the balance of concept types of <b>index</b> <b>terms</b> shift from concrete objects to their behaviours or features, (2) <b>index</b> <b>terms</b> as a whole shifts from a set of fragmented terms to a single indirectly-related group, (3) the core part of the <b>index</b> <b>terms</b> comes to be occupied by terms which represent concepts other than concrete objects...|$|R
50|$|Indices not in contractions {{are called}} <b>free</b> <b>indices.</b>|$|R
40|$|In {{this study}} we propose {{statistical}} models to model the indexing of textual documents by human indexers in a hypertext environment. Previous research revealed that hypertext links connecting text indexed with the same subject <b>index</b> <b>term</b> assigned by a human indexer offer users {{an effective way to}} access information due to the structure, focus, and imbedded intelligence of a manually assigned subject index. We propose that for a document collection with an existing well developed subject index, the contingent relationship between subject <b>index</b> <b>terms</b> and words in the text can be used by a Bayesian inference rule in predicting which subject <b>index</b> <b>term</b> is relevant given the words occurring in a document. It was hypothesized that the indexing models (1) will be able to predict what <b>index</b> <b>terms</b> a human indexer will use, (2) serve as decision aids, and (3) will help users in information retrieval tasks. This problem area is approached by (1) direct comparison between the <b>index</b> <b>terms</b> assigned by the indexing models and those assigned by a human indexer The overlap between the two sets of <b>index</b> <b>terms</b> was measured by the proportion of <b>index</b> <b>terms</b> in the intersection for each set. (2) expert ratings on the relevance of <b>index</b> <b>terms</b> assigned by different means. The ratings as well as the quality of an <b>index</b> <b>term</b> (determined by the magnitude of the rating) were statistically analyzed. Potential errors made by the human indexer and the indexing models were estimated. (3) a controlled laboratory experiment in which users 2 ̆ 7 performance in information retrieval tasks using different indices was evaluated. Results obtained revealed that (1) a significant overlap exists between the <b>index</b> <b>terms</b> assigned by the indexing models and those by a human indexer, (2) <b>index</b> <b>terms</b> suggested by the indexing models received significantly higher ratings than <b>index</b> <b>terms</b> randomly selected and the estimated number of errors made by the indexing models and the human indexer were similar, and (3) users performed better or equally well in information retrieval tasks using the subject index assigned by the indexing models than when using the index prepared by the human indexer. ...|$|R
5000|$|Let: [...] be the {{occurrence}} matrix [...] be {{the occurrence}} matrix without the <b>index</b> <b>term</b> [...] and [...] be density of [...] Then: The discrimination {{value of the}} <b>index</b> <b>term</b> [...] is: ...|$|R
40|$|This thesis {{discusses}} {{the problems and}} the methods of finding relevant information in large collections of documents. The contribution of this thesis to this problem is to develop better content analysis methods {{which can be used}} to describe document content with <b>index</b> <b>terms.</b> <b>Index</b> <b>terms</b> can be used as meta-information that describes documents, and that is used for seeking information. The main point of this thesis is to illustrate the process of developing an automatic indexer which analyses the content of documents by combining evidence from word frequencies and evidence from linguistic analysis provided by a syntactic parser. The indexer weights the expressions of a text according to their estimated importance for describing the content of a given document {{on the basis of the}} content analysis. The typical linguistic features of <b>index</b> <b>terms</b> were explored using a linguistically analysed text collection where the <b>index</b> <b>terms</b> are manually marked up. This text collection is referred to as an <b>index</b> <b>term</b> corpus. Specific features of the <b>index</b> <b>terms</b> provided the basis for a linguistic term-weighting scheme, which was then combined with a frequency-based term-weighting scheme. The use of an <b>index</b> <b>term</b> corpus like this as training material is a new method of developing an automatic indexer. The results of the experiments were promising...|$|R
40|$|The {{invention}} {{relates to}} a feature weighing computation method {{based on a}} structural constraint in a Chinese information retrieval. The method comprises the following steps: a. carrying out structuring processing to inquiry and obtaining a structuring inquiry result, wherein the structuring processing comprises one or a plurality of following steps: splitting words, carrying out part-of-speech tagging to splitted works, carrying out shallow parsing to inquiry or carrying out parsing to inquiry; b. determining an <b>index</b> <b>term</b> according to the structuring inquiry result and then determining an inquiry-context property set of the <b>index</b> <b>term</b> according to the structuring inquiry result which is adjacent to the <b>index</b> <b>term</b> and positioned in a word list; c. computing the weighing value of each property in the inquiry-context property set; d. combining the weighing values of all properties into property values of the <b>index</b> <b>terms</b> through a first composite function; and f. combining the property values of the <b>index</b> <b>terms</b> by a second composite function and obtaining the <b>index</b> <b>term</b> weighing. The method can compute the weighing accurately no matter whether the <b>index</b> <b>terms</b> exist in the word list or not. 本发明是有关于一种中文信息检索中基于结构约束的特征权重计算方法，包括以下步骤：a. 对查询进行结构化处理，得到结构化查询结果；结构化处理包括：分词、对切分出的词进行词性标注、对查询进行浅层句法分析或对查询进行句法分析中一个或几个；b. 根据述结构化查询结果确定索引词，然后根据与所述索引词相邻并位于词列表中的结构化查询的结果，确定所述索引词的查询—上下文属性集；c. 计算查询—上下文属性集中每个属性的权重值；d. 通过第一组合函数将各个属性的权重值组合成所述索引词的属性值；e. 使用第二组合函数对所述索引词的属性值组合，得到所述索引词权重。无论索引词是否在词列表中，本发明的方法都能准确计算出其权重。Department of ComputingInventor name used in this publication: 陆永邦, Lu YongbangTitle in Traditional Chinese: 中文信息檢索中基於結構約束的索引詞權重計算方法Chin...|$|R
5000|$|The {{index of}} a book may report any number of {{references}} for a given <b>index</b> <b>term,</b> and thus may be coded as a multimap from <b>index</b> <b>terms</b> to any number of reference locations or pages.|$|R
50|$|An optimal <b>index</b> <b>term</b> is {{one that}} can {{distinguish}} two different documents {{from each other and}} relate two similar documents. On the other hand, a sub-optimal <b>index</b> <b>term</b> can not distinguish two different document from two similar documents.|$|R
5000|$|... #Subtitle level 3: Crucible {{swelling}} <b>index</b> (<b>free</b> swelling <b>index)</b> ...|$|R
40|$|In {{this paper}} the GK# {{model for the}} {{construction}} of thesaurus classes based on fuzzy semantic association measure between <b>index</b> <b>terms</b> and concepts (thesaurus classes) is presented. The association measure is obtained on the basis of fuzzy semantic relations between <b>index</b> <b>terms,</b> and it is used to cluster <b>index</b> <b>terms</b> into concepts. A hierarchical algorithm is introduced which runs on a simple numerical example. # 2001 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved...|$|R
5000|$|<b>Free</b> <b>indexing</b> {{language}} - any term (not {{only from}} the document) {{can be used to}} describe the document ...|$|R
40|$|Dengue haemorhagic fever (DHF) {{was one of}} most {{dangerous}} disease in Indo-nesia. The number of case showed increased year by year since first time observed. Vector control that used by government always using Insecticides. A continuity of it can caused resistances of mosquito, {{and it will be}} more dangerous. In District Cimahi, vector control doing by mosquitoes habitat eradication (PSN) for last 3 years. It done by invite people to participate to make environment clean. Every week, people on duty going house to house to check water container condition. They also give support and suggest how to prevent breeding mosquitoes to people. The aim of this research is to evaluate knowledge, practice and attitude people about DBD and to count Entomology Index in district Cimahi. This research observe that globally people attitude is good, knowledge is middle and prac-tice still bad. Larvae <b>Free</b> <b>Index</b> (Angka Bebas Jentik) when observe show that in Cibabat first week still below national Larvae <b>Free</b> <b>Index.</b> In Pasirkaliki, first week and second week Larvae <b>Free</b> <b>Index</b> below national Larvae <b>Free</b> <b>Index...</b>|$|R
5000|$|Then, do a cyclic {{permutation}} of the <b>free</b> <b>indices</b> [...] and , and add and subtract the three resulting equations: ...|$|R
40|$|Multiview video coding (MVC) {{exploits}} {{the temporal}} and spatial redundancy between neighboring frames of the same view or that of adjacent views to achieve compression. Free viewpoint switching, however, poses challenges to MVC, as when a user is able to choose different playback paths it would become unclear to encoder which previously reconstructed frame would be available for decoding the current frame. Therefore, to support free viewpoint switching in MVC, encoder would need to operate under uncertainty on the decoder predictor status. Extending our previous work on video compression with decoder predictor uncertainty, this paper proposes a MVC algorithm based on distributed source coding (DSC) to tackle the free viewpoint switching problem, where the encoder has access to several predictor candidates but there is uncertainty as to which one {{will be available at}} decoder to serve as predictor for the current frame. Since cross-view correlation could be much less significant than temporal correlation, a main challenge of the present DSC application is to achieve competitive compression efficiency. Some of the novelties of the proposed MVC algorithm are that it incorporates different macroblock modes and significance coding within the DSC framework, so that competitive coding performance can be achieved. Experiments demonstrate the proposed algorithm can outperform solutions based on intra or closed-loop predictive (CLP) coding in terms of compression efficiency. In addition, the proposed method incurs a negligible amount of drifting, making it an attractive solution to facilitate low-delay, <b>free</b> viewpoint switching. <b>Index</b> <b>Terms</b> — Distributed source coding, Slepian-Wolf, multiview video, free viewpoint switchin...|$|R
40|$|This paper {{describes}} a technique for automatically creating an index for handwritten notes captured as digital ink. No text recognition is performed. Rather, a dictionary of possible <b>index</b> <b>terms</b> is built by clustering groups of ink strokes corresponding roughly to words. Terms whose distribution varies significantly across note pages are {{selected for the}} index. An index page containing the <b>index</b> <b>terms</b> is created, and terms are hyper-linked back to their original location in the notes. Further, <b>index</b> <b>terms</b> occurring in a note page are highlighted to aid browsing. 1...|$|R
50|$|Each record {{contains}} a bibliographic citation, abstract, <b>index</b> <b>terms</b> from the Thesaurus of Psychological <b>Index</b> <b>Terms,</b> keywords, classification categories, population information, the geographical {{location of the}} research population, and cited references for journal articles, book chapters, and books, mainly from 2001 to present. Records of books include the book's table to contents.|$|R
40|$|Most {{information}} retrieval systems use stopword lists and stemming algorithms. However, {{we have found}} that recognizing singular and plural nouns, verb forms, negation, and prepositions can produce dramatically different text classification results. We present results from text classification experiments that compare relevancy signatures, which use local linguistic context, with corresponding <b>indexing</b> <b>terms</b> that do not. In two different domains, relevancy signatures produced better results than the simple <b>indexing</b> <b>terms.</b> These experiments suggest that stopword lists and stemming algorithms may remove or conflate many words {{that could be used to}} create more effective <b>indexing</b> <b>terms.</b> Introduction Most {{information retrieval}} systems use a stopword list to prevent common words from being used as <b>indexing</b> <b>terms.</b> Highly frequent words, such as determiners and prepositions, are not considered to be content words because they appear in virtually every document. Stopword lists are almost univer [...] ...|$|R
40|$|When {{professional}} indexers independently assign {{terms to}} a given document, the term sets generally differ between indexers. Studies of inter-indexer consistency measure the percentage of matching <b>index</b> <b>terms,</b> {{but none of them}} consider the semantic relationships that exist amongst these terms. We propose to represent multiple-indexers data in a vector space and use the cosine metric as a new consistency measure that can be extended by semantic relations between <b>index</b> <b>terms.</b> We believe that this new measure is more accurate and realistic than existing ones and therefore more suitable for evaluation of automatically extracted <b>index</b> <b>terms...</b>|$|R
40|$|Model {{the query}} as a {{sequence}} of input observations (<b>index</b> <b>terms),</b> • Model the doc as a discrete HMM composed of distribution of N-gram parameters • The relevance measure,, can be estimated by the N-gram probabilities of the <b>index</b> <b>term</b> sequence for the query,, predicted by the doc – A generative model for IR Q Nn qqqqQ [...] 21...|$|R
40|$|International audienceMost Information {{retrieval}} systems {{represent a}} query, also a document, as {{a bag of}} <b>indexing</b> <b>terms</b> without any relation between each other. This bag-based representation causes a problem for specialists when they deal with a specific domain like medical one. We present {{an alternative to the}} bag of <b>indexing</b> <b>terms</b> representation depending on semantic query structuring, in order to fulfill this need of precision in a specific domain. This structure of a query is obtained by grouping <b>indexing</b> <b>terms</b> using pre-defined categories called dimensions. These dimensions represent the different aspects that could appear in a query or a document. By using this notion, the relevant document to a given query should not only has a maximum number of shared <b>indexing</b> <b>terms</b> but also have a similar structure. Experimental results show precision improvement related to the granularity of dimensions and its distribution over the whole corpus...|$|R
50|$|An <b>index</b> <b>term,</b> subject term, subject heading, or descriptor, in {{information}} retrieval, {{is a term}} that captures {{the essence of the}} topic of a document. <b>Index</b> <b>terms</b> make up a controlled vocabulary for use in bibliographic records. They {{are an integral part of}} bibliographic control, which is the function by which libraries collect, organize and disseminate documents. They are used as keywords to retrieve documents in an information system, for instance, a catalog or a search engine. A popular form of keywords on the web are tags which are directly visible and can be assigned by non-experts. <b>Index</b> <b>terms</b> can consist of a word, phrase, or alphanumerical term. They are created by analyzing the document either manually with subject indexing or automatically with automatic indexing or more sophisticated methods of keyword extraction. <b>Index</b> <b>terms</b> can either come from a controlled vocabulary or be freely assigned.|$|R
5000|$|... {{one or more}} {{preferred}} <b>index</b> <b>terms</b> (at most one in each natural language) ...|$|R
5000|$|In 2001, Cairo and Alexandria Stock Exchange was {{included}} on the Morgan Stanley Capital International (MSCI) Emerging Market <b>Free</b> <b>Index</b> (EMF) and EMEA and All Country World Index.|$|R
40|$|A {{method of}} drawing <b>index</b> <b>terms</b> from text is presented. The {{approach}} uses no stop list, stemmer, or other language-and domain-specific component, allowing operation {{in any language}} or domain with only trivial modification. The method uses n-gram counts, achieving a function similar to, but more general than, a stemmer. The generated <b>index</b> <b>terms,</b> which the author calls “highlights, ” are suitable for identifying the topic for perusal and selection. An extension is also described and demonstrated which selects <b>index</b> <b>terms</b> to represent a subset of documents, distinguishing them from the corpus. Some experimental results are presented, showing operation in English, Spanish, German, Georgian, Russian, and Japanese...|$|R
40|$|This paper {{explores the}} {{effectiveness}} of <b>index</b> <b>terms</b> more complex than single words in conventional information retrieval systems. Retrieval is performed in two phases. In the first phase, a conventional retrieval method (the Okapi system) is used {{and in the second}} phase, complex <b>index</b> <b>terms</b> such as syntactic relations and single words with part of speech information are introduced to rerank the results of the first phase. The effectiveness of the different types of <b>index</b> <b>terms</b> are evaluated through experiments, in which the TREC- 7 test collection and 50 queries are used. The experiments show that retrieval effectiveness was improved for 32 out of 50 queries...|$|R
40|$|As an {{information}} retrieval system, PubMed® aims at providing efficient {{access to documents}} cited in MEDLINE®. For this purpose, it relies on matching representations of documents, as provided by authors and indexers to user queries. In this paper, we describe the growth of author keywords in biomedical journal articles and present a comparative study of author keywords and MeSH® <b>indexing</b> <b>terms</b> assigned by MEDLINE indexers to PubMed Central Open Access articles. A similarity metric is used to assess automatically the relatedness between pairs of author keywords and <b>indexing</b> <b>terms.</b> A set of 300 pairs is manually reviewed to evaluate the metric and characterize the relationships between author keywords and <b>indexing</b> <b>terms.</b> Results show that author keywords are increasingly available in biomedical articles and that over 60 % of author keywords {{can be linked to}} a closely related <b>indexing</b> <b>term.</b> Finally, we discuss the potential impact of this work on indexing and terminology development...|$|R
5000|$|... 2000 - Additional IV <b>Index</b> <b>terms</b> were added: 60, 90, 120, 150, 180, 360, 720 ...|$|R
5000|$|SSA (Salient Semantic Analysis) which <b>indexes</b> <b>terms</b> using salient {{concepts}} {{found in}} their immediate context.|$|R
5000|$|Extraction {{indexing}} involves taking words {{directly from}} the document. It uses natural language and lends itself well to automated techniques where word frequencies are calculated and those with a frequency over a pre-determined threshold are used as <b>index</b> <b>terms.</b> A stop-list containing common words (such as [...] "the", [...] "and") would be referred to and such stop words would be excluded as <b>index</b> <b>terms.</b>|$|R
40|$|Experimenting with {{different}} mathematical objects for text representation {{is an important}} step of building text classification models. In order to be efficient, such objects of a formal model, like vectors, have to reasonably reproduce language-related phenomena such as word meaning inherent in <b>index</b> <b>terms.</b> We introduce an algorithm for sense-based semantic ordering of <b>index</b> <b>terms</b> which approximates Cruse’s description of a sense spectrum. Following semantic ordering, text classification by support vector machines can benefit from semantic smoothing kernels that regard semantic relations among <b>index</b> <b>terms</b> while computing document similarity. Adding expansion terms to the vector representation can also improve effectiveness. This paper proposes a new kernel which discounts less important expansion terms based on lexical relatedness. ...|$|R
40|$|Abstract: Variation of <b>free</b> swell <b>index</b> {{of powder}} and {{granular}} bentonite in distilled water were reviewed. For this, {{four types of}} additives is used such as starch (SC), sodium carboxy-methyl cellulose (SCMC), polyvinyl alcohol (PVA-Iand PVA-II), super absorbent polymer (SAP). For PVA-I and PVA-II, <b>free</b> swell <b>index</b> increased with 0. 2 g addition. However, SC showed a little increase and SCMC showed higher increase. Especially for SAP addition, powder bentonite has larger <b>free</b> swell <b>index</b> to compare with granular bentonite. The sequence of contribution to <b>free</b> swell <b>index</b> of bentonite in descending order is SAP, SCMC, PVA-II, PVA-I, and SC...|$|R
