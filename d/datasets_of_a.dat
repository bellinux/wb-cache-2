96|10000|Public
50|$|Stallion is a 328 {{megapixel}} {{tiled display}} system, with over 150 times {{the resolution of}} a standard HD display, it is among the highest pixel count displays in the world. The cluster provides users {{with the ability to}} display high-resolution visualizations on a large 16x5 tiled display of 30-inch Dell monitors. This configuration allows for an exploration of visualizations at an extremely high level of detail and quality compared to a typical moderate pixel count projector. The cluster allows users access to over 82GB of graphics memory, and 240 processing cores. This configuration enables the processing of <b>datasets</b> <b>of</b> <b>a</b> massive scale, and the interactive visualization of substantial geometries. A 36TB shared file system is available to enable the storage of tera-scale size datasets.|$|E
40|$|One of {{the most}} {{complete}} aircraft reconnaissance and ground-based radar <b>datasets</b> <b>of</b> <b>a</b> single tropical cyclone was recorded in Hurricane Elena (1985) as it made a slow, 3 -day anticyclonic loop in the Gulf of Mexico. Eighty-eight radial legs and 47 vertical incidence scans were collected aboard NOAA WP- 3 D aircraft, and 1142 ground-based radar scans were made of Elena’s eyewall and inner rainbands as the stor...|$|E
40|$|A {{methodology}} {{is proposed}} {{to support the}} periodic review of manufacturing data in the pharmaceutical industry. Pattern recognition techniques are employed to isolate and analyze operation-relevant data segments {{to the purpose of}} automatically extracting the information embedded in large databases of secondary manufacturing systems. The results achieved by testing the proposed methodology on two six-month <b>datasets</b> <b>of</b> <b>a</b> commercial-scale drying unit demonstrate the potential of this approach, which can be easily extended to other manufacturing operations...|$|E
5000|$|There {{are four}} basic {{techniques}} for acquiring the three-dimensional (x,y,λ) <b>dataset</b> <b>of</b> <b>a</b> hyperspectral cube. The choice of technique {{depends on the}} specific application, seeing that each technique has context-dependent advantages and disadvantages.|$|R
40|$|Powerful {{technique}} for generating a digital 3 D volumetric <b>dataset</b> <b>of</b> <b>a</b> specimen from 2 D projections Wide range of materials Full {{characterization of the}} specimen’s exterior and interior structures without destroying or disassembling it Used for non-destructive testing and quality control Artem Amirkhanov et al. Fuzzy C...|$|R
40|$|This work proposes V-SMART-Join, a {{scalable}} MapReducebased {{framework for}} discovering all pairs of similar entities. The V-SMART-Join framework is applicable to sets, multisets, and vectors. V-SMART-Join {{is motivated by}} the observed skew in the underlying distributions of Internet traffic, and is <b>a</b> family <b>of</b> 2 -stage algorithms, where the first stage computes and joins the partial results, and the second stage computes the similarity exactly for all candidate pairs. The V-SMART-Join algorithms are very efficient and scalable {{in the number of}} entities, as well as their cardinalities. They were up to 30 times faster than {{the state of the art}} algorithm, VCL, when compared on <b>a</b> real <b>dataset</b> <b>of</b> <b>a</b> small size. We also established the scalability of the proposed algorithms by running them on <b>a</b> <b>dataset</b> <b>of</b> <b>a</b> realistic size, on which VCL never succeeded to finish. Experiments were run using real <b>datasets</b> <b>of</b> IPs and cookies, where each IP is represented as <b>a</b> multiset <b>of</b> cookies, and the goal is to discover similar IPs to identify Internet proxies. 1...|$|R
40|$|None of the {{available}} Viking Orbiter Infrared Thermal Mapper's thermal datasets can be uniquely interpreted in isolation from other data in terms of variations of a single, simple physical parameter of the martian surface. Other thermal <b>datasets</b> <b>of</b> <b>a</b> target area, thermal observations of adjacent materials, and visible and near-infrared observations of the same region can often sufficiently constrain interpretations to obtain one that is nearly unique. The major datasets, their derived information, and their ambiguities are given...|$|E
40|$|Automated 3 D {{modeling}} of building interiors {{is useful in}} applications such as virtual reality and entertainment. Using a human-operated backpack system equipped with 2 D laser scanners and inertial measurement units, we develop four scan-matching-based algorithms to localize the backpack and compare their performance and tradeoffs. We present results for two <b>datasets</b> <b>of</b> <b>a</b> 30 -meter-long indoor hallway and compare {{one of the best}} performing localization algorithms with a visual-odometry-based method. We find that our scan-matching-based approach results in comparable or higher accuracy. 1...|$|E
40|$|Monitoring of {{tropical}} forests and peat swamp forests becomes increasingly important {{in order to}} reduce carbon emissions from deforestation and forest degradation. SAR systems are suitable for that purpose due to their weather independence and sensitivity for changes in time series. The TanDEM-X mission delivers global <b>datasets</b> <b>of</b> <b>a</b> single-pass SAR interferometer, which provides more information in comparison to normal X-band SAR acquisitions. In particular the interferometric coherence can be regarded as useful information because it is dependent on tree height and canopy cover representing stand structure, which are important parameters for monitoring of forests...|$|E
25|$|MR {{images have}} also been {{obtained}} from the brain <b>of</b> <b>a</b> 3200-year-old Egyptian mummy. The perspectives are slim, however, that any three-dimensional imaging <b>dataset</b> <b>of</b> <b>a</b> fossil, semi-fossil or mummified brain will ever be of much use to morphometric analyses of the kind described here, since the processes of mummification and fossilization heavily alter the structure of soft tissues in a way specific to the individual specimen and subregions therein.|$|R
40|$|For the {{automated}} detection of the causal structures in datasets {{there are already}} approaches available that all have their benefits and drawbacks. In this article several data-driven methods {{for the detection of}} cause-effect relationships in time series were applied on benchmark datasets and their results visualized accordingly. Furthermore the methods were tested on <b>a</b> <b>dataset</b> <b>of</b> <b>an</b> industrial glass forming process and on the analysis <b>of</b> biosignals from <b>a</b> Functional Electrostimulation...|$|R
40|$|National audienceTemplate-based {{analysis}} {{techniques are}} good candidates to robustly detect transient temporal graphic elements (e. g. event-related potential, k-complex, sleep spindles, vertex waves, spikes) in noisy and multi-sources electro-encephalographic signals. More specifically, we present {{the impact on}} <b>a</b> large <b>dataset</b> <b>of</b> <b>a</b> wavelet denoising to detect evoked potentials in a single-trial P 300 speller. Using coiflets as a denoising process allows to obtain more stable accurracies for all subjects...|$|R
40|$|An {{ensemble}} of random decision trees {{is a popular}} classification technique, especially known {{for its ability to}} scale to large domains. In this paper, we provide an efficient strategy to compute bounds on the moments of the generalization error computed over all <b>datasets</b> <b>of</b> <b>a</b> particular size drawn from an underlying distribution, for this classification technique. Being able to estimate these moments can help us gain insights into the performance of this model. As we will see in the experimental section these bounds tend to be significantly tighter than the state-of-the-ar...|$|E
40|$|An {{explicit}} {{finite element}} modelling method is formulated using a layered shell element {{to examine the}} behaviour of masonry walls subject to out-of-plane loading. Masonry is modelled as a homogenised material with distinct directional properties that are calibrated from <b>datasets</b> <b>of</b> <b>a</b> “C” shaped wall tested under pressure loading applied to its web. The predictions of the layered shell model have been validated using several out-of-plane experimental datasets reported in the literature. Profound influence of support conditions, aspect ratio, pre-compression and opening to the strength and ductility of masonry walls is exhibited from the sensitivity analyses performed using the model...|$|E
40|$|Understanding and {{adapting}} to changes of customer behavior {{is an important}} aspect of surviving in a continuously changing market environment for a modem company. The concept of customer change model mining is introduced and its process is analyzed in this paper. A customer change model mining method based on swarm intelligence is presented, and the strategies of pheromone updating and items searching are given. Finally, an examination on two customer <b>datasets</b> <b>of</b> <b>a</b> telecom company illuminates that this method can achieve customer change model efficiently. IEEE Computat Intelligence Soc, Int Neural Network Soc, Natl Sci Fdn Chin...|$|E
40|$|Spatial {{data mining}} helps to {{identify}} interesting patterns from the spatial data sets. However, geo spatial data requires substantial data pre-processing before {{data can be}} interrogated further using data mining techniques. Multi-dimensional spatial data {{has been used to}} explain the spatial analysis and SOLAP for pre-processing data. This paper examines some of the methods for pre-processing of the data using Arc GIS 10. 2 and Spatial Analyst with <b>a</b> case study <b>dataset</b> <b>of</b> <b>a</b> watershed...|$|R
40|$|International audienceAn {{experimental}} <b>dataset</b> <b>of</b> <b>a</b> commercial Magneto-Rheological (MR) damper is {{exploited for}} identification <b>of</b> <b>a</b> Hysteresis-based Control-Oriented model. The model wellness for hysteresis, saturation and transient responses is shown through validation with experimental data. A study case that includes <b>a</b> Quarter <b>of</b> Vehicle (QoV) {{shows that the}} hysteresis phenomena could affect the primary ride and vehicle handling. Several analysis based on open and closed loop simulation demonstrated that hysteresis must be considered for controller design...|$|R
30|$|The {{pancreas}} and kidney organ models and their relative positions {{were based on}} <b>an</b> MRI <b>dataset</b> <b>of</b> <b>a</b> male patient <b>of</b> 87  kg. Forty sequential T 2 weighted HASTE images were acquired with a Siemens Magnetom TrioTim 3.0 Tesla MRI scanner as part of another clinical study. The volumes and distances between the organs measured in the MRI images were compared to those in CT images of humans imaged with 111 In-exendin, {{to make sure the}} anatomy was representative for our study population.|$|R
40|$|It is {{important}} to have long period datasets for scientific researches which includes econometric and statistic applications since to have significant results. But especially differences that occur in the system of International Standard Classification of Economic Activities (ISIC) prevent constituting historical <b>datasets</b> <b>of</b> <b>a</b> lot of sectoral variables. In other words because of not having entire adaptation between revise 2 and revise 3 classification systems, therefore it prevents using datasets efficiently. In this work the transformation ratio of Revise 3 /Revise 2 which is formed from the datas of the manufacturing industry is proposed to solve the problem arises from the differences in the classification system...|$|E
3000|$|... 3 D {{modeling}} first {{needs to}} establish pairwise epipolar geometry relationships within photo collections, thus {{provides a good}} example for mining inter-element connections within unordered visual datasets. Large-scale structure-from-motion systems started with <b>datasets</b> <b>of</b> <b>a</b> few thousand images [3, 4]. Using image retrieval techniques for overlap prediction, Agarwal et al. [5] processed 150 thousand images {{in a single day}} on a computer cluster. Frahm et al. [6] reconstructed 3 million images in one day on a single computer utilizing a compact binary image representation for clustering. Recently, Heinly et al. [7] pushed the envelope to tackle a world-scale dataset (100 million images) by using a streaming paradigm to identify connected images by looking at each image only once.|$|E
40|$|Feedback utterances {{are among}} the most {{frequent}} in dialogue. Feedback is also a crucial aspect of linguistic theories that take social interaction, involving language, into account. This paper introduces the corpora and <b>datasets</b> <b>of</b> <b>a</b> project scrutinizing this kind of feedback utterances in French. We present the genesis of the corpora (for a total of about 16 hours of transcribed and phone force-aligned speech) involved in the project. We introduce the resulting datasets and discuss how they are being used in on-going work with focus on the form-function relationship of conversational feedback. All the corpora created and the datasets produced in the framework of this project will be made available for research purposes...|$|E
40|$|Abstract. This paper {{describes}} {{big data}} technology layers, analyses the CDR (Call Data Records) real-time query scenario of telecommunications and brings forward a fast indexing and query solution {{based on the}} open source Hadoop platform. A CDR real-time query system was built according to the solution. A performance test was conducted with the real <b>dataset</b> <b>of</b> <b>a</b> city with 3 million subscribers. Compared with the existing system, the big data solution can greatly improve data processing performance and support real-time query with lower hardware and software investment...|$|R
40|$|This paper {{presents}} {{a novel approach}} for concatenative speech synthesis. This approach enables reduction <b>of</b> the <b>dataset</b> size <b>of</b> <b>a</b> concatenative text-to-speech system, namely the IBM trainable speech synthesis system, by more than <b>an</b> order <b>of</b> magnitude. <b>A</b> spectral acoustic feature based speech representation is used for computing a cost function during segment selection {{as well as for}} speech generation. Initial results indicate that even with <b>a</b> <b>dataset</b> size <b>of</b> <b>a</b> few megabytes it is possible to achieve quality which is significantly higher than existing small footprint formant based synthesizers. 1...|$|R
40|$|International audienceIn this paper, {{we address}} some sociological and topological issues {{associated}} with mobile phone communication. Based on <b>a</b> <b>dataset</b> <b>of</b> <b>a</b> few million users, we use customers' age and gender information to study relation between these parameters and the average behavior of users in terms of number of calls, number of SMS and calls duration. We also study the dataset from <b>a</b> networking point <b>of</b> view: we define different profiles based on the topological properties of the personal network of each individual and study the relations between these profiles {{and the age of}} customers...|$|R
40|$|Prior to any {{processing}} of multibaseline (MB) {{synthetic aperture radar}} (SAR) data stacks, a MB phase calibration is necessary to compensate for phase contributions due to non-compensated platform motions and/or atmospheric propagation delays. Classical calibration methods rely on the detection of point-like scatterers. However, especially in natural scenarios, their ﬁnal calibration performance could be impaired {{by the nature of}} the scattering and by the typical low number of baselines. In this paper, we propose a calibration method based on the minimization of the entropy of the vertical proﬁle of the backscattered power. This allows to potentially exploit the MB SAR signal independently of the nature of the scattering. The proposed method has been tested by processing simulated and real airborne <b>datasets</b> <b>of</b> <b>a</b> forest stand...|$|E
40|$|In this paper, a {{proposal}} {{for the treatment of}} driving dynamic <b>datasets</b> <b>of</b> <b>a</b> railway vehicle is outlined which is aimed at the development of a track condition monitoring system that can be implemented on board of trains being operated in standard revenue service. Compared to monitoring systems installed in special diagnostic trains this concept enables a {{reduction in the number of}} sensors used, to meet stringent constraint in terms of space available for the transducers, wirings and power supply. To this aim, use is made of a Kalman-filter state estimator replacing the direct measure of vehicle dynamics at some meaningful locations in the vehicle. A performance index is defined to summarize the performance of the diagnostic unit. The proposed approach is validated using multi-body simulation...|$|E
40|$|In data {{sonification}} research, {{there is a}} well-known perceptual problem that arises when abstract multivariate <b>datasets</b> <b>of</b> <b>a</b> certain size and complexity are parametrically mapped into sound. In listening to such sonifications, when a feature appears, it is sometimes difficult to ascertain whether that feature is actually {{a feature of the}} dataset or just a resultant of the psychoacoustic interaction between co-dependent parametric dimensions. A similar effect occurs in visualisation, such as when parallel lines can appear more or less curved on different backgrounds. Couched in psycho-philosophical terms, we can ask whether this failure is related to classical phenomenology's inability to produce an eidetic science of essential invariant forms that involve no assertion of actual material existence, or to there not yet having been found som...|$|E
40|$|Radiometric image {{normalization}} {{is one of}} {{the basic}} pre-processing methods used in satellite time series analysis. This paper presents a new multi-image approach able to estimate the parameters of relative radiometric normalization through a multiple and simultaneous regression with <b>a</b> <b>dataset</b> <b>of</b> <b>a</b> generic number <b>of</b> images. The method was developed to overcome the typical drawbacks of standard one-to-one techniques, where image pairs are independently processed. The proposed solution is based on multi-image pseudo-invariant features incorporated into a unique regression solved via Least Squares. Results for both simulated and real data are presented and discussed...|$|R
40|$|Abstract Churn rate {{refers to}} the {{proportion}} of contractual customers who leave a sup-plier during a given time period. This phenomenon is very common in highly competitive markets such as telecommunications industry. In a statistical setting, churn can be con-sidered as <b>an</b> outcome <b>of</b> some characteristics and past behavior of customers. In this paper, churn prediction is performed considering <b>a</b> real <b>dataset</b> <b>of</b> <b>an</b> European telecom-munications company. An appealing parallelized version of bagging classifiers is used leading to <b>a</b> substantial reduction <b>of</b> the classification error rates. The results are de-tailed discussed...|$|R
40|$|We {{examine how}} the {{numeracy}} level of employees influences {{the quality of their}} on-the-job decisions. Based on <b>an</b> administrative <b>dataset</b> <b>of</b> <b>a</b> retail bank we relate the performance of loan officers in a standardized math test to the accuracy of their credit assessments of small business borrowers. We find that loan officers with <b>a</b> high level <b>of</b> numeracy are more accurate in assessing the credit risk of borrowers. The effect is most pronounced during the pre-crisis credit boom period when it is arguably more difficult to pick out risky borrowers...|$|R
30|$|This paper {{presents}} an original and workable (i.e. assuming {{a fixed amount}} of resources and time) methodological approach that analyses the data quality {{of a group of}} <b>datasets</b> <b>of</b> <b>a</b> given database, by cross-comparing them with datasets from other databases, and identifies opportunities for improvement of the datasets/database under scrutiny. The six quality criteria indicators defined by the ILCD Handbook have been included in the method. These indicators have, however, been redefined in order to facilitate their implementation, and to ensure the quality of the assessment whenever expert judgement was required. The quality criteria indicators can be applied to any type of LCA dataset. However, in order to ensure the appropriateness and robustness of the methodology applied, in-depth knowledge of the analysed topic is required, since expert judgement values have been applied in many cases.|$|E
40|$|Publishing Linked Data is {{a process}} that {{involves}} several design decisions and technologies. Although some initial guidelines have been already provided by Linked Data publishers, these are still far from covering all the steps that are necessary (from data source selection to publication) or giving enough details about all these steps, technologies, intermediate products, etc. Furthermore, given the variety of data sources from which Linked Data can be generated, we believe {{that it is possible to}} have a single and uni�ed method for publishing Linked Data, but we should rely on di�erent techniques, technologies and tools for particular <b>datasets</b> <b>of</b> <b>a</b> given domain. In this paper we present a general method for publishing Linked Data and the application of the method to cover di�erent sources from di�erent domains...|$|E
40|$|Data anonymization {{has become}} a major {{technique}} in privacy preserving data publishing. Many methods have been proposed to anonymize one dataset and a series of <b>datasets</b> <b>of</b> <b>a</b> data holder. However, no method has been proposed for the anonymization scenario of multiple independent data publishing. A data holder publishes a dataset, which contains overlapping population with other datasets published by other independent data holders. No existing methods are able to protect privacy in such multiple independent data publishing. In this paper we propose a new generalization principle (ρ,α) -anonymization that effectively overcomes the privacy concerns for multiple independent data publishing. We also develop an effective algorithm to achieve the (ρ,α) -anonymization. We experimentally show that the proposed algorithm anonymizes data to satisfy the privacy requirement and preserves high quality data utility. ...|$|E
40|$|This paper {{contributes}} to the heated debate on the link between climate and civil war. We exploit <b>a</b> large <b>dataset</b> <b>of</b> <b>a</b> drought index commonly used in hydrology, the Palmer Drought Severity Index (PDSI). The PDSI {{is based on a}} hydrological model and is a cumulative measure that takes account of past climatic variables. Our analysis takes account of country fixed effects, removal of the most influential observations, use of alternative sample periods and changes to the battle-death threshold. Overall, results show a robust link between drought and civil war in Sub-Saharan African states after independence...|$|R
40|$|Using the {{proprietary}} <b>dataset</b> <b>of</b> <b>a</b> {{real estate}} agency, I analyse {{tens of thousands}} of housing sale and rental transactions in Central London during the 2005 - 2011 period. I run hedonic regressions on both prices and rents and show that price-rent ratios are higher for bigger and more central units. Since this result could be driven by differences in unobserved characteristics between properties for sale and properties for rent, I replicate my analysis using only units that were both sold and rented out within 6 months, and get similar results. I discuss several possible explanations for my findings...|$|R
40|$|After having {{described}} the mathematical background of copula functions we propose a scheme useful to apply <b>a</b> particular family <b>of</b> copulas - the Archimedean copulas - to indemnity payments and loss expenses <b>of</b> <b>an</b> insurance {{company with the}} aim of obtaining their joint probability distribution. The joint distribution is used to calculate - via Monte Carlo simulation - the premia <b>of</b> <b>a</b> reinsurance strategy in presence of policy limits and insurer’s retentions. Results coming from this strategy are compared with those obtained in independence hypothesis. Calculations and estimates are based on <b>a</b> large <b>dataset</b> <b>of</b> <b>an</b> italian non life insurance company KEYWORDS: copula functions, indemnity claims, reinsurance, stochastic simulation...|$|R
