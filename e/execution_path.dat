375|665|Public
25|$|The Burroughs/Unisys APLB {{interpreter}} (1982) was {{the first}} to use dynamic incremental compilation to produce code for an APL-specific virtual machine. It recompiled on-the-fly as identifiers changed their functional meanings. In addition to removing parsing and some error checking from the main <b>execution</b> <b>path,</b> such compilation also streamlines the repeated entry and exit of user-defined functional operands. This avoids the stack setup and take-down for function calls made by APL's built-in operators such as Reduce and Each.|$|E
2500|$|More recently, {{following}} the 2014 {{release of the}} APT 1 report from Mandiant, the industry has seen a shift towards signature-less approaches to the problem capable of detecting and mitigating zero-day attacks. [...] Numerous approaches to address these new forms of threats have appeared, including behavioral detection, artificial intelligence, machine learning, and cloud-based file detonation. According to Gartner, it is expected the rise of new entrants, such Carbon Black, Cylance and Crowdstrike will force EPP incumbents into a new phase of innovation and acquisition. One method from Bromium involves micro-virtualization to protect desktops from malicious code execution initiated by the end user. Another approach from SentinelOne and Carbon Black focuses on behavioral detection by building a full context around every process <b>execution</b> <b>path</b> in real time, while Cylance leverages an artificial intelligence model based on [...] machine learning. Increasingly, these signature-less approaches have been defined {{by the media and}} analyst firms as [...] "next-generation" [...] antivirus and are seeing rapid market adoption as certified antivirus replacement technologies by firms such as Coalfire and DirectDefense. In response, traditional antivirus vendors such as Trend Micro, Symantec and Sophos have responded by incorporating [...] "next-gen" [...] offerings into their portfolios as analyst firms such as Forrester and Gartner have called traditional signature-based antivirus [...] "ineffective" [...] and [...] "outdated".|$|E
50|$|A {{compiler}} often can, by rearranging its generated machine {{instructions for}} faster execution, improve program performance. It increases ILP (Instruction Level Parallelism) along the important <b>execution</b> <b>path</b> by statically predicting frequent <b>execution</b> <b>path.</b> Trace scheduling {{is one of}} many known techniques for doing so.|$|E
40|$|This paper {{presents}} {{a method of}} intra-task dynamic voltage scaling (DVS) for SoC design with hierarchical FSM and synchronous dataflow model (in short, HFSM-SDF model). To have an optimal intra-task DVS, exact <b>execution</b> <b>paths</b> need to be determined in compile time or runtime. In general programs, since determining exact <b>execution</b> <b>paths</b> in compile time or runtime is not possible, existing methods assume worst/average-case <b>execution</b> <b>paths</b> and take static voltage scaling approaches. In our work, we exploit a property of HFSM-SDF model to calculate exact <b>execution</b> <b>paths</b> in runtime. With the information of exact <b>execution</b> <b>paths,</b> our DVS method can calculate exact remaining workload. The exact workload enables to calculate optimal voltage level which gives optimal energy consumption while satisfying the given timing constraint. Experiments show {{the effectiveness of the}} presented method in lowpower design of an MPEG 4 decoder system...|$|R
40|$|Linear Temporal Logic (LTL) {{is widely}} used for {{defining}} conditions on the <b>execution</b> <b>paths</b> of dynamic systems. In the case of dynamic systems that allow for nondeterministic evolutions, one has to specify, along with an LTL formula #, which are the paths that are required to satisfy the formula. Two extreme cases are the universal interpretation A. #, which requires to satisfy the formula for all the possible <b>execution</b> <b>paths,</b> and the existential interpretation E. #, which requires to satisfy the formula for some <b>execution</b> <b>paths...</b>|$|R
5000|$|Multiple Choice - choose several <b>execution</b> <b>paths</b> {{from many}} {{alternatives}} ...|$|R
5000|$|Furthermore, {{because the}} first return {{statement}} is executed unconditionally, no feasible <b>execution</b> <b>path</b> reaches the second assignment to [...] Thus, the assignment is unreachable and can be removed.If the procedure had a more complex control flow, such as a label after the return statement and a [...] elsewhere in the procedure, then a feasible <b>execution</b> <b>path</b> might exist to the assignment to [...]|$|E
5000|$|Conditional Choice - {{choose one}} <b>execution</b> <b>path</b> from many {{alternatives}} according to discriminated status conditions ...|$|E
5000|$|Simulator: {{following}} {{one possible}} <b>execution</b> <b>path</b> {{through the system}} and presenting the resulting execution trace to the user.|$|E
40|$|Path-oriented {{scheduling}} methods, such as trace {{scheduling and}} hyperblock scheduling, use speculation to extract instruction-level parallelism from control-intensive programs. These methods predict important <b>execution</b> <b>paths</b> {{in the current}} scheduling scope using execution profiling or frequency estimation. Aggressive speculation is then applied to the important <b>execution</b> <b>paths,</b> possibly {{at the cost of}} degraded performance along other paths. Therefore, the speed of the output code can be sensitive to the compiler's ability to accurately predict the important <b>execution</b> <b>paths.</b> Prior work in this area has utilized the speculative yield function by Fisher, coupled with dependence height, to distribute instruction priority among <b>execution</b> <b>paths</b> in the scheduling scope. While this technique provides more stability of performance by paying attention to the needs of all paths, it does not directly address the problem of mismatch between compile-time prediction and run-time behavior. The work pre [...] ...|$|R
40|$|A {{program can}} be {{decomposed}} into a set of possible <b>execution</b> <b>paths.</b> These {{can be described in}} terms of primitives such as assignments, assumptions and coercions, and composition operators such as sequential composition and nondeterministic choice as well as finitely or infinitely iterated sequential composition. Some of these paths cannot possibly be followed (they are dead or infeasible), and {{they may or may not}} terminate. Decomposing programs into paths provides a foundation for analyzing properties of programs. Our motivation is timing constraint analysis of real-time programs, but the same techniques can be applied in other areas such as program testing. In general the set of <b>execution</b> <b>paths</b> for a program is infinite. For timing analysis we would like to decompose a program into a finite set of subpaths that covers all possible <b>execution</b> <b>paths,</b> in the sense that we only have to analyze the subpaths in order to determine suitable timing constraints that cover all <b>execution</b> <b>paths...</b>|$|R
40|$|Abstract. This paper {{presents}} an abstract semantics that uses information about <b>execution</b> <b>paths</b> to improve precision of data flow analyses of logic programs. The abstract semantics {{is illustrated by}} abstracting <b>execution</b> <b>paths</b> using call strings of fixed length and the last transfer of control. Abstract domains {{that have been developed}} for logic program analyses can be used with the new abstract semantics without modification...|$|R
5000|$|XQPreparedExpression - the {{expression}} is cached and the <b>execution</b> <b>path</b> is pre-determined {{allowing it to}} be executed multiple times in an efficient manner.|$|E
5000|$|Exclusive Choice - {{choose one}} <b>execution</b> <b>path</b> from many {{alternatives}} {{based on data}} that is available when {{the execution of the}} process reaches the exclusive choice ...|$|E
5000|$|... {{will not}} {{necessarily}} be followed by , i.e. there is an <b>execution</b> <b>path</b> from [...] {{to the end of}} the program that does not go through [...]|$|E
40|$|The strict {{globular}} omega-categories formalize the <b>execution</b> <b>paths</b> of {{a parallel}} automaton and the homotopies between them. One associates to such (and any) omega-category C three homology theories. The {{first one is}} called the globular homology. It contains the oriented loops of C. The two other ones are called the negative (resp. positive) corner homology. They contain in a certain manner the branching areas of <b>execution</b> <b>paths</b> or negative corners (resp. the merging areas of <b>execution</b> <b>paths</b> or positive corners) of C. Two natural linear maps called the negative (resp. the positive) Hurewicz morphism from the globular homology to the negative (resp. positive) corner homology are constructed. We explain the reason why these constructions allow the reinterpretation of some geometric problems coming from computer science...|$|R
30|$|We {{introduce}} a novel fuzzing method based on Lévy flights in the input space {{in order to}} maximize coverage of <b>execution</b> <b>paths.</b>|$|R
40|$|Business {{processes}} exhibit concurrent execution, where {{different and}} sometimes inter-dependent parts, are isolated and run in parallel. In current workflow tools, this requirement is supported through a synchronisation mechanism {{allowing for the}} convergence of parallel <b>paths</b> of <b>execution.</b> In this paper, we argue that more sophisticated forms of inter-process communication are necessary for increased workflow automation. In particular, three extensions which preserve the parallelisation of <b>execution</b> <b>paths</b> are proposed: the adaption of synchronous and asynchronous messaging for data/document flow across <b>execution</b> <b>paths,</b> aborts of related <b>execution</b> <b>paths,</b> and exclusive, i. e. atomic, execution of nested processes. Illustrations from an industrial case study show how these constructs combine with classical workflow constructs to capture complex but common themes of business processing. To precisely impart the ramifications for workflow execution, we show how a formal semantics can be assign [...] ...|$|R
5000|$|Negate {{the last}} path {{condition}} not already negated {{in order to}} visit a new <b>execution</b> <b>path.</b> If {{there is no such}} path condition, the algorithm terminates.|$|E
5000|$|... [...] the {{statement}} is cached and then the <b>execution</b> <b>path</b> is pre-determined on the database server allowing it to be executed multiple times in an efficient manner.|$|E
5000|$|... #Caption: <b>Execution</b> <b>path</b> {{tree for}} this example. Three tests are {{generated}} {{corresponding to the}} three leaf nodes in the tree, and three execution paths in the program.|$|E
40|$|Part 7 : Trust and Zero-Day VulnerabilitiesInternational audienceIt {{is argued}} that runtime {{verification}} techniques can be used to identify unknown application security vulnerabilities that are a consequence of unexpected <b>execution</b> <b>paths</b> in software. A methodology is proposed {{that can be used}} to build a model of expected application <b>execution</b> <b>paths</b> during the software development cycle. This model is used at runtime to detect exploitation of unknown security vulnerabilities using anomaly detection style techniques. The approach is evaluated by considering its effectiveness in identifying 19 vulnerabilities across 26 versions of Apache Struts over a 5 year period...|$|R
40|$|Abstract. This paper studies how {{to verify}} the {{conformity}} of a program with its specification and proposes a novel constraint-programming framework for bounded program verification (CPBPV). The CPBPV framework uses constraint stores to represent the specification and the program and explores <b>execution</b> <b>paths</b> nondeterministically. The input program is partially correct if each constraint store so produced implies the post-condition. CPBPV does not explore spurious <b>execution</b> <b>paths</b> as it incrementally prunes <b>execution</b> <b>paths</b> early by detecting that the constraint store is not consistent. CPBPV uses the rich language of constraint programming to express the constraint store. Finally, CPBPV is parametrized {{with a list of}} solvers which are tried in sequence, starting with the least expensive and less general. Experimental results often produce orders of magnitude improvements over earlier approaches, running times being often independent of the variable domains. Moreover, CPBPV was able to detect subtle errors in some programs for which other frameworks based on (bounded) model checking have failed. ...|$|R
40|$|There are two {{complementary}} {{aspects to}} testing a software system: functional testing determines {{how well the}} system performs its required op-erations; structural testing determines that {{the components of the}} system are sufficiently exer-cised during testing. Functional testing is based upon a specification of the system requirements; structural testing is based upon a model {{of the structure of the}} system. For rule-based systems, a structural model explicates the rule <b>execution</b> <b>paths</b> (possible causal sequences of rule firings). In this paper, a formal structural model for OPS 5 -like rule bases is developed; this model is de-signed to overcome weaknesses of previous struc-tural models. Two software tools are introduced: Path Hunter uses the structural model to deter-mine the <b>execution</b> <b>paths</b> in a rule base; Path Tracer analyzes dynamic rule firings that occur during functional testing, to determine the extent to which <b>execution</b> <b>paths</b> identified by the struc-tural model are exercised at run-time. We present results obtained from using Path Hunter and Path Tracer on a complex expert system rule base which had previously been subjected to functional test-ing...|$|R
5000|$|Synchronizing Merge - merge many {{execution}} paths; synchronize if many paths are taken; do {{the same}} as for a simple merge if only one <b>execution</b> <b>path</b> is taken ...|$|E
5000|$|Invoke an {{automated}} theorem prover {{to generate a}} new input. If there is no input satisfying the constraints, return to step 6 to try the next <b>execution</b> <b>path.</b>|$|E
50|$|Due to SHA1 collisions, an {{attacker}} {{can alter}} the <b>execution</b> <b>path</b> of the executable by serving altered chunks when the victim is downloading the executable using the BitTorrent protocol.|$|E
5000|$|Identify a {{database}} test data load {{tool for the}} development/component unit test environment; this is required {{to ensure that the}} database optimizer chooses correct <b>execution</b> <b>paths</b> and to enable reinitializing and reloading the database as needed ...|$|R
40|$|Abstract—In the {{verification}} of multitask software in embedded systems, general purpose model checkers do not inherently consider {{characteristics of the}} real time operating system, such as priority-based scheduling, priority inversion, and protocols for protecting shared memory resources. Since explicit-state model checkers generally explore all possible <b>execution</b> <b>paths</b> and task interleaving, this could potentially lead to exploring <b>execution</b> <b>paths</b> that are redundant, unnecessarily increasing verification complexity and hampering tractability. Based on this premise, in this work we investigate how one can improve the performance of explicit-state model checkers, such as SPIN, for the {{verification of}} multitask applications that target OSEK compliant real time operating systems. I...|$|R
40|$|The {{quality of}} service (QoS) {{is a major concern}} in the design and {{management}} of Web service composition. Existing methods for QoS calculation either do not take the probability of <b>path</b> <b>execution</b> into consideration when QoSs are provided for different <b>execution</b> <b>paths,</b> or do not take different <b>execution</b> <b>paths</b> into consideration when a single integrated QoS is provided for the whole composition. In this paper, a comprehensive QoS analysis approach is proposed that calculates the QoS probability distribution by considering both the execution probability and execution conditions of each path in the service composition. Four types of basic composition patterns in the service composition are discussed: sequential, parallel, loop and conditional. In particular, a QoS solution is provided for all types of loop structured service composition. 8 page(s...|$|R
50|$|It {{depends on}} the {{particular}} programming language specification whether an expression with no abstract side effects can legally be eliminated from the <b>execution</b> <b>path</b> by the processing {{environment in which the}} expression is evaluated.|$|E
5000|$|Iroh.js is a runtime code {{analysis}} {{library for}} JavaScript. It {{keeps track of}} the code <b>execution</b> <b>path,</b> provides runtime listeners to listen for specific executed code patterns and allows to intercept and manipulate the program's execution behavior.|$|E
50|$|This {{level is}} aware of the {{differences}} between the databases and able to construct an <b>execution</b> <b>path</b> of operations in all cases. However the conceptual layer defers to the physical layer for the actual implementation of each individual operation.|$|E
50|$|The Symbolic Execution Debugger visualizes {{the control}} flow {{of a program}} as a {{symbolic}} execution tree that contains all feasible <b>execution</b> <b>paths</b> through the program {{up to a certain}} point. It is provided as a plugin to the Eclipse development platform.|$|R
40|$|International audienceThis paper studies how {{to verify}} the {{conformity}} of a program with its specification and proposes a novel constraint-programming framework for bounded program verification (CPBPV). The CPBPV framework uses constraint stores to represent both the specification and the program and explores <b>execution</b> <b>paths</b> of bounded length nondeterministically. The CPBPV framework detects nonconformities and provides counter examples when a path of bounded length that refutes some properties exists. The input program is partially correct under the boundness restrictions, if each constraint store so produced implies the postcondition. CPBPV does not explore spurious <b>execution</b> <b>paths,</b> as it incrementally prunes <b>execution</b> <b>paths</b> early by detecting that the constraint store is not consistent. CPBPV uses the rich language of constraint programming to express the constraint store. Finally, CPBPV is parameterized {{with a list of}} solvers which are tried in sequence, starting with the least expensive and less general. Experimental results often produce orders of magnitude improvements over earlier approaches, running times being often independent {{of the size of the}} variable domains. Moreover, CPBPV was able to detect subtle errors in some programs for which other frameworks based on bounded model checking have failed...|$|R
40|$|Path-oriented {{scheduling}} methods, such as trace {{scheduling and}} hyperblock scheduling, use speculation to extract instruction-level parallelism from control-intensive programs. These methods predict important <b>execution</b> <b>paths</b> {{in the current}} scheduling scope using execution pro ling or frequency estimation. Aggressive speculation is then applied to the important <b>execution</b> <b>paths,</b> possibly {{at the cost of}} degraded performance along other paths. Therefore, the speed of the output code can be sensitive to the compiler's ability to accurately predict the important <b>execution</b> <b>paths.</b> Prior work in this area has utilized the speculative yield function by Fisher, coupled with dependence height, to distribute instruction priority among <b>execution</b> <b>paths</b> in the scheduling scope. While this technique provides more stability of performance by paying attention to the needs of all paths, it does not directly address the problem of mismatch between compile-time prediction and run-time behavior. The work presented in this paper extends the speculative yield and dependence height heuristic to explicitly minimize the penalty su ered by other paths when instructions are speculated along a <b>path.</b> Since the <b>execution</b> time of a path is determined by the number of cycles spent between a path's entrance and exit in the scheduling scope, the heuristic attempts to eliminate unnecessary speculation that delays any path's exit. Such control of speculation makes the performance much less sensitive to the actual path taken at run time. The proposed method has a strong emphasis on achieving minimal delay to all exits. Thus the name, speculative hedge, is used. This paper presents the speculative hedge heuristic, and shows how it controls over-speculation in a superblock/hyperblock scheduler. The stability of out...|$|R
