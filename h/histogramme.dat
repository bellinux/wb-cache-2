8|8|Public
5000|$|<b>Histogramme</b> Colors= id:lightgrey value:gray(0.9) id:darkgrey value:gray(0.7) id:sfondo value:rgb(1,1,1) id:barra value:rgb(0.99,0.60,0.40)ImageSize = width:700 height:350PlotArea = left:50 bottom:30 top:30 right:50DateFormat = x.yPeriod =from:0 till:21000TimeAxis = orientation:verticalAlignBars = justifyScaleMajor = gridcolor:darkgrey increment:2000 start:0ScaleMinor = gridcolor:lightgrey increment:500 start:0BackgroundColors = canvas:sfondoBarData= bar:1968 text:1968 bar:1975 text:1975 bar:1982 text:1982 bar:1990 text:1990 bar:1999 text:1999 bar:2006 text:2006 bar:2009 text:2009PlotData= color:barra width:15 align:left bar:1968 from:0 till: 20456 text:20 456 bar:1975 from:0 till: 19258 text:19 258 bar:1982 from:0 till: 18124 text:18 124 bar:1990 from:0 till: 17200 text:17 200 bar:1999 from:0 till: 16240 text:16 240 bar:2006 from:0 till: 15704 text:15 704 bar:2009 from:0 till: 15567 text:15 567 ...|$|E
40|$|This thesis {{introduces}} visibility histograms as {{a method}} for analyzing volumetric datasets. These histograms show how much the data points within a 3 D dataset that have the same scalar value influence the image which is created by rendering the dataset with a particular transfer function and from a particular viewing direction. These histograms {{can be used to}} gain insights into the internal structure of volumetric datasets, in particular information about occlusions. Furthermore, the possibility of automatically calculating transfer functions which generate a particular visibility histogram when applied to a dataset from a particular viewing direction is explored. Two methods which can be used to calculate a matching transfer function for a visibility histogram are explained, one of which is based on a genetic algorithm approach, while the other is an heuristic. Zusammenfassung In dieser Diplomarbeit werden Visibility <b>Histogramme</b> als Methode um volumetrische Datensätze zu analysieren eingeführt. Diese <b>Histogramme</b> zeigen an, welchen Einfluss die Datenpunkte mit demselben Skalarwert in einem solchen Datensatz auf das Gesamtbild, welches entsteht wenn der Datensatzes mit einer bestimmten Transferfunktion und von einer bestimmten Blickrichtun...|$|E
40|$|International audienceThe European Large Area ISO Survey (ELAIS) is {{a survey}} of 4 main regions of the sky at 15 and 90 μm. The {{follow-up}} programmes from the ground are summarized. We present preliminary results from near-infrared (K-band) observations in the southern ELAIS fields. We show the <b>histogramme</b> distribution of the K-band magnitude for all objects in the observed regions. The ``stellarity'' index derived from the source extraction is also shown. We roughly find that 2 / 3 of the objects detected in the fields are elongated (i. e. galaxies). For {{a small sample of}} ISOPHOT detections, the corresponding NIR sources are classified as follows: a) 20 % of stars: b) 70 % of galaxies and c) 10 % of interacting systems...|$|E
40|$|ABSTRACT. The {{spatial and}} {{temporal}} relationships between belugas (Delphinapterus leucas) and two characteristics of their habitat—bathymetry and ice concentration—were examined. Observed location-habitat correspondence histograms were compared to random location-habitat histograms, using a Kolmogorov-Smirnoff (K-S) statistical test. Results show that beluga distribution is bimodal with respect to bathymetry, with a larger mode in shallow water and a smaller mode in water approximately 500 m deep. They occur more often than expected by chance in the 0 / 10 ice class and less often than expected in the 10 / 10 ice class. Males and females associate differently with both depth and ice concentration. Females associate with bathymetry very differently in the fall than in the summer. There is a general tendency for males in the eastern North American Arctic {{to be associated with}} shallow water during the summer and deeper water (modes at 100 and 500 m) in the fall. Female locations are associated more often with the 0 / 10 ice class and less often with the 10 / 10 class than expected by chance. These trends were stronger in the western than in the eastern portions of the Canadian Arctic. Key words: bathymetry, beluga, climate change, Delphinapterus leucas, Geographic Information System, habitat, sea ice RÉSUMÉ. On a étudié les rapports spatiaux et temporels entre les bélougas (Delphinapterus leucas) et deux caractéristiques de leur habitat (bathymétrie et concentration de glace). À l’aide d’un test statistique de Kolmogorov-Smirnov (K-S), on a comparé les <b>histogrammes</b> de la correspondance observée emplacement-habitat à des <b>histogrammes</b> emplacement-habitat pris au hasard. Les résultats montrent que la distribution du bélouga est bimodale en ce qui a trait à la bathymétrie, un mode plus grand étan...|$|R
40|$|We {{describe}} an online database for extra-solar planetary-mass candidates, updated regularly as new data are available. We first discuss {{criteria for the}} inclusion of objects in the catalog: "definition" of a planet and several aspects of the confidence level of planet candidates. We are led to point out the conflict between sharpness of belonging or not to a catalogue and fuzziness of the confidence level. We then describe the different tables of extra-solar planetary systems, including unconfirmed candidates (which will ultimately be confirmed, or not, by direct imaging). It also provides online tools: <b>histogrammes</b> of planet and host star data, cross-correlations between these parameters and some VO services. Future evolutions of the database are presented. Comment: Accepted in Astronomy and Astrophysics (revised version...|$|R
40|$|The most {{complete}} catalogue of nearby stars (Gliese and Jarheiss, 1991) {{has been used}} to search for the stars whose photospheric thermal emission can be detected by the future millimeter array ALMA. We found that 446 nearby stars with spectral types ranging from A to M are detectable. A long-term astrometric observation programme (> 10 yr) of these stars with ALMA (theoretical astrometric precision = 0. 1 milliarcsecond) would be sensitive to wobbles caused by the gravitational pull of possible unseen planets. Such a programme would probe minimum planetary masses as low as 0. 1 Jupiter for an orbital period of 10 years. We provide <b>histogrammes</b> of these minimum masses and of ALMA integration times for these 446 ALMA stars. Comment: 5 pages; 1 table; 2 figures Late...|$|R
40|$|Abstract. The Nth order {{probability}} density function for pixels in a restricted neighborhood may {{be characterized by}} a set of N histograms (or some corresponding moments) computed along appropriately chosen axes. The projections on those axes are obtained from a local linear transform of the local neighborhood vector. This approach is closely related to filter bank analysis methods and gives a statistical justification for the extraction of texture properties by means of convolution operators or local matches. Optimal and sub-optimal linear operators are derived for texture analysis and classification. Experimental results indicate that the method is robust, flexible, and that it performs as well as standard co-occurrence based methods for texture classification. The proposed approach enables texture characterization with a lower number of features and it is also computationally more appealing. Zusammeafassung. Das Verfahren der Bildanalyse mit Hilfe lokaler linearer Transformationen rlaubt es, die N-dimensionale Verteilungsdichtefunktion der Punkte eines begrenzten Bildausschnitts durch N <b>Histogramme</b> anzun~ihern, die entlang eeigne...|$|E
40|$|This paper {{describes}} new {{results on}} {{the identification of the}} complex gravitational lens responsible for the double quasar Q 2345 + 007. A gravitational shear field was detected recently about 45 00 away from the QSO, centered on an excess of faint blue galaxies. The redshift distribution is still unknown, so the mass and the photometric properties of the deflector are still a matter of debate. We present deep photometric data obtained in the near-IR (J and K'), which are used together with the preexisting optical BRI photometry to build SEDs for all the objects of the field, and to derive a photometric redshift estimate by comparison with synthetic spectrophotometric data. We propose a statistical method to analyse the redshift distribution, based on the cumulative <b>histogramme</b> of the redshift ranges allowed for the different objects. An excess of galaxies at a redshift of z ' 0 : 75 is clearly detected in the field of Q 2345 + 007, with a 2 D distribution showing a maximum located on the c [...] ...|$|E
40|$|The European Large Area ISO Survey (ELAIS) is {{a survey}} of 4 main regions of the sky at 15 and 90 m. The {{follow-up}} programmes from the ground are summarized. We present preliminary results from near-infrared (K-band) observations in the southern ELAIS fields. We show the <b>histogramme</b> distribution of the K-band magnitude for all objects in the observed regions. The "stellarity" index derived from the source extraction is also shown. We roughly find that 2 / 3 of the objects detected in the fields are elongated (i. e. galaxies). For {{a small sample of}} ISOPHOT detections, the corresponding NIR sources are classified as follows: a) 20 % of stars: b) 70 % of galaxies and c) 10 % of interacting systems. Key words: ISO; infrared astronomy; deep survey; follow-up; near-infrared. 1. INTRODUCTION The European Large Area ISO Survey (ELAIS, P I. : M. Rowan-Robinson) is a consortium of several European institutes whose goal is to study the infrared objects and the infrared background in 4 areas in the sky, [...] ...|$|E
40|$|This paper marked our intial {{attempt to}} make use of {{artificial}} neural networks as the key component in a learning machine vision system. Many other publications followed over the next 10 years and some of the basic ideas, tentatively suggested here, became better worked out. In particular, after several years of searching we finally managed to show that the process of square-rooting the values in a pattern template is the appropriate way to construct a dot product similariy score for Poisson sampled data (Tina memos 1997 - 001 and 2001 - 010). We succeded in producing an invertable (complete) representation of line shape, which could be therefore guaranteed to solve the ambiguity issues presented in this paper (Tina memos 1995 - 004 and 2002 - 002). We also managed to generate an architectue which combined the possibility of associative layering with supervised training (Tina memo 1997 - 002). The combination of these ideas produces a statistically principled approach which has an optimal representation for recognition of line based shape in a system which is guaranteed to learn. As this paper illustrates, all of this was based (from the start) on the physiologically motivated models being developed at the time by Grossberg. Though other workers have applied statistical principles to network design we believe that we are still the only ones to have identified the link between frequency coding of signals and this way of performing a statistical comparison on a physiologically plausible neural architecture. In addition, though other authors in the area of image analysis (and particularly database retrieval) have since re-discovered the use of <b>histogrammes</b> for data indexing, we believ...|$|R
40|$|Summary: Over 100 {{laboratories}} {{participated in}} an external quality-control survey (EQCS) for T 3 and T 4, using the Munich Model as conceived in this laboratory ((1 - 4) : Marschner,!, et al. (1974), Horm. Met. Res. 6, 293 - 296; Horn,K. et al. (1976), this J. 14, 353 - 360; Marschner J. et al. (1976), this J. 14, 345 - 351; Wood, W. G. et al. (1980), this J. 18, 183 - 192) and carried out over the past 6 years. Twenty lyophilised serum samples, including independant hidden standard curves for T 3 and T 4, were dispatched by post together with a detailed questionnaire and full instructions on reconstitution of the samples. The returned data were processed as previously described ((4) : Wood, W. G. et al. (1980), this J. 18, 183 - 192) and each participant received a full analysis of his«own data {{and a set of}} <b>histogrammes</b> with which he could visually check his performance against other laboratories. An explanatory letter was sent, which explained the computer print-out, the coding of the kits and contained a con-structive report of the participant's performance and helpful advice as to how to improve the assay if this was necessary. From 110 laboratories returning data for T 3, 86 were fully useable, 22 partly and 2 unuseable because T 3 -uptake had been performed instead of T 3 -radioimmunoassay (RIA). From 124 laboratories returning T 4 EQCS data, 102 were fully useable and 22 partly useable. In both cases the partly useable sets of data did not contain important items such as the count rates (or absorption values) for each serum, needed for construction and read-off of value...|$|R
40|$|Nous avons étudié la {{dispersion}} (vitesse, durée, distance) d'alevins de saumon atlantique à l'émergence dans un ruisseau expérimental. Le rythme journalier de dévalaison suit étroitement le rythme d'émergence des alevins, ce qui montre une bonne corrélation {{entre les deux}} activités, tout au moins en début de période. Les <b>histogrammes</b> de capture des alevins échantillonnés tous les 10 m présentent des caractéristiques de forme similaires (durée, asymétrie et aplatissement). Jusqu'au pic des captures, près de 50 % des alevins dévalent en 5 jours. Ils se dispersent à partir de la frayère sur 50 m en 3 ou 4 nuits et forment une cohorte homogène d'après leur rythme de dévalaison et leur taille. Après le mode, les captures sont plus étalées dans {{le temps}} (environ 10 jours) et montrent plus de variabilité en fonction de la distance. Les alevins résidents ne sont pas distribués uniformément dans les cinq biefs : les densités, de même que les poids moyens, tendent à augmenter vers l'aval. Plus de 50 % de la population d'alevins survivants s'est établi dans les 50 m en aval de la frayère. Les résultats révèlent deux vagues de dévalants. La première vague de dispersion, aussitôt après l'émergence, est rapide et importante. Elle ne dépendrait pas directement de la compétition territoriale et de la densité, mais permettrait d'éviter des densités localement trop élevées et d'utiliser plus efficacement les zones productives situées en aval de la frayère. La deuxième vague d'alevins dévalants correspondrait aux émergents tardifs et aux poissons soumis aux effets de la compétition territoriale. We have analysed the dispersal patterns (rate, duration, extent) of Atlantic salmon fry at emergence. The rate and duration of movement, and the distance travelled were measured in an experimental stream, located near S-Pée-sur-Nivelle, in SW France. A batch of 8 850 eyed eggs, from the grilse wild stock of the Nivelle River, {{was buried in the}} gravel substrate at the upstream end of a series of 5 sections, each 10 m long by 3 m wide. Shortly before emergence, drift nets equipped with fry traps were installed at the downstream end of each section. The nets sampled about 1 / 10 of the flow, except for those nets at the downstream end of the last section which collected all downstream moving fry. The traps were visited every morning and the fry enumerated. At the time of peak movements, samples were collected for length-weight measurements. At the end of the dispersal period, fry which had settled in the different sections (residents) were captured with electro-fishing gear and measured. The pattern of downstream movement of fry in a set of drift nets was closely related to the pattern of emergence from an artificial redd upstream of the nets. Hence, emergence and downstream dispersion were well synchronized, at least during {{the first part of the}} dispersal from the redd. The time-frequency histograms of fry sampled every 10 m showed the same pattern and general shape (duration, skewness and kurtosis). Until the peak of captures, nearly 50 % of all downstream moving fry were caught within 5 days in each section. Dispersion from the redd over 50 m occurred within 3 - 4 nights. During this first period, the fry exhibited similar characteristics with respect to activity patterns and sizes. After the modal day of capture, catches were more evenly spread over time (about 10 days) and showed greater variability in relation to the distance travelled from the redd. Resident fry were not uniformly distributed in the 5 sections : densities, as well as average weights, increased from upstream to downstream. Over 50 % of the surviving fry (75. 3 % of planted eggs) settled within 50 m downstream from the redd. Our results showed two waves of downstream dispersion. The first dispersal wave, occurring soon after emergence, was swift and implied large numbers of fry. It was not the result of territorial competition or density, since it occurred before the onset of aggressive behaviour. Rather, this first wave appeared as process to avoid the formation of clumps and allow for a more efficient use of the more productive zones, generally located downstream from the redd. The second wave of downstream moving fry corresponded to late emerging fry and to those fry which, 10 - 12 days after emergence, were displaced by territorial competition...|$|R
40|$|International audienceObjectives: the {{presentation}} of the advantages of the new bibliometrics configuration option of HistCiteTM (2004) for the identification of papers;to show the additivity of the reference levels according to the reading domains of an author; to determine where the number of articles generated per author is higher than per coauthor; to show how when pointing out a citation year for an author the selection of a previous (or posterior) year (s) can be an advantage for him by means of the examination of two graphs yielded by the software; to model the feasibility of a paper as a reference, starting from the citations that it receives; considering that the obsolescence gives rise to an effect of cumulative advantage, it is approached the extraction of a graph that explicitly shows this effect in terms of citations to old frequently cited articles; to show which way inside the graph maker option the change in the initial step results in that the papers can not only be grouped under subject but also with a time-based criteria; to approach the identification of the nuclear, continuant and transient scientific authors community by using the HistCiteTM Graphmaker option; to discuss the software difficulties at the time of giving graphical sense. Methodology:Exploitation of the new indicators for the global and the local citation score as a function of time (GCS/t, LCS/t), facilitated by the 2004 version of HistCiteTM, starting from the same initial bibliographic set. Examination of the central limit theorem appliance conditions based on the graphic modelization of the Kendall question. Results:To study the history of the journal „Management Science‟ Editorial Department an histogram was elaborated. It expressed the local citation score in the end of the period (LCSe) 1970 - 2005 at the abcisas axis and the local citation score {{in the beginning of the}} same period (LCSb) at the axis of the ordenates. Some of the relevant key papers were observed, what featured some 50 % of the selected candidacies in the case study paper published by the scientific press. Both couple of indicators selected the same material, by assigning main relevance to the same initial author. This identified person is the same than the original leader as reported by the published article. Perhaps the second pair of indicators (GCS/t, LCS/t) is more informative because it scales all the other authors by assigning a discrimination criterion based on the subject. The theoretical or methodologic articles were only selected, and no empirical material nor derived from behaviour was obtained. The comparative study of the graphs produced by the software was induced by the employ of HistCiteTM to map the 1988 Dr Josef Steiner Cancer Research Foundation Award, investigating the period 1981 - 1986. The logical content of the bibliography was extracted through two ISI research fronts: code 83 - 1740, Oncogenes and the genetics of human; viral transforming genes and their DNA structure; and code 84 - 4046, Characterization of human and murine cellular oncogenes. Although the case history has been published, was expressed in terms of professional anticipacionism and used the Weibull distribution, HistCiteTM generates a remarkably similar structure based on co-citations. The crucial events, identified in terms of acyclic graphs by HistCiteTM also are the same than those underlined by a second publication that determines the connectivity into citation networks based in a study of similar sources. The automatic algorithmic model correctly analyzes the implied scientific authors demography. And is useful when identifying the research core population of authors, the research continuants along the whole process, and the population of those transients which assist in the elaboration of the results. When inquiring on the graphic sensibility of the HistCiteTM software graphic performance a visual answer to the Kendall question was tried (if we have a bibliographical data set for a year, the number of additional journals probably contributing to this set for the next two years can be estimated?). An annual bibliography was chosen – 2002 – and the number of journals for the combined bibliography 2002 + 2003 was pronosticated. Like in the original paper that poses this question for finite discrete distributions, the selected subject was the “muscle fiber”. After the „Ranked Source List‟ table from HistCiteTM, the 2002 journals were ranked by frequencies, and a total of 152 was signalled. By application of the formulation for the amount of additional journals, M, under the Central Limit Theorem assumptions, it was obtained that M(2002) = 98. In fact,the combined bibliography (2002 + 2003) – easily produced with the HistCiteTM „Add Set‟ option from the WOS records – listed a total of 231 journals. The value of the prediction, M(2002), is only partially correct because 152 + 98 = 252. The graphs that were produced for the bibliographies 2002 vs. 2002 + 2003 shows that : several selected records do not satisfy the annual employed criterion. Thus the graphs were employed to refine the bibliographies. Once the journals not belonging to the year were identified, the references were manually deleted. The „Graph Maker‟ option seems not prepared to manage with a selection of nodes (marked) from the main table. What makes it difficult to reformulate the question once posed. When answering the Kendall question, for instance, although the month of publication is facilitated in the articles of the bibliography, a graph is not immediately available for the papers of the first semester as something opposite to the graph that could be made with the published articles along the a second semester; what forbids the Kendall question formulation in its original terms. Objetivos: Presentar las ventajas de la nueva opción de configuración bibliométrica de HistCite TM (2004) para identificar artículos. Analizar los histogramas que produce HistCite TM en términos de ventaja acumulada y envejecimiento de las citas: mostrar la aditividad de los niveles de referencia que se producen a partir de los dominios de lectura de un autor; determinar dónde el número de artículos generado por autor es superior al que dá lugar el coautor; mostrar cómo a partir de un autor, para fijar un año de cita la elección de un año(s) anterior (o posterior) puede ser ventajosa para él a partir del examen del gráfico producido; modelar la factibilidad de un artículo como referencia, en base a las citas que recibe; en la medida en que tener en cuenta la obsolescencia da lugar a un efecto de ventaja acumulada, extraer un gráfico que la explicite mostrándola con claridad en términos de cita a artículos antiguos frecuentemente citados; mostrar cómo en el interior de la opción de elaboración de gráficos el cambio en el paso inicial produce que los artículos no sólo puedan ser agrupados por temas sino también temporalmente; aproximar la identificación de las comunidades de autores científicos nucleares, continuantes y transitorios usando la opción de construcción de grafos de HistCite TM; comentar dificultades del software a la hora de dar sentido gráfico. Analizar de forma comparada los resultados que produce HistCite TM : en sus indicadores de amplitud y reconocimiento; en el caso de un historiógrafo año por año para el período 1981 - 1986;en su tratamiento de los problemas de muestreo, por medio de la formalización que ofrece de la cuestión de Kendall. Metodología:Explotación de los nuevos indicadores de puntuación global y local de cita en función del tiempo (GCS/t, LCS/t), que facilita la versión 2004 de HistCiteTM. Análisis de las coincidencias interesantes entre las cartografías de estudios de casos publicados (en investigación sobre el cancer y ciencias de la gestión) y de los grafos que HistCiteTM produce, partiendo del mismo conjunto bibliográfico inicial. Examen de las condiciones deaplicación del teorema central del límite en base a la modelación gráfica de la cuestión de Kendall. Resultados:La investigación del período 1981 - 1986, utilizando HistCiteTM, en el caso de la cartografía de la concesión del premio 1988 del Dr. Josef Steiner Cancer Research Foundation Award indujo al estudio comparado de los gráficos producidos por el software. El contenido lógico de la bibliografía se pudo aislar en base a dos frentes de investigación ISI: código 83 - 1740, Oncogenes and the genetics of human; viral transforming genes and their DNA structure; y código 84 - 4046, Characterization of human and murine cellular oncogenes. Aunque la historia publicada del caso se expresaba en términos de anticipacionismo profesional y utilizaba la distribución de Weibull, HistCiteTM genera una estructura remarcablemente similar en base a cocitas. Los acontecimientos cruciales, identificados en términos de grafos acíclicos por HistCiteTM, son también los mismos que localiza una segunda publicación que determina la conectividad en redes de citas en base a un estudio de fuentes similares. El modelo algorítmico automático analiza correctamente la demografía de los autores científicos implicados. Y sirve para identificar la población de autores que está en el núcleo de la investigación, la que continúa en ella a lo largo de todo el proceso, y la de aquellos que a lo largo del proceso asisten en la elaboración de los resultados. Al indagar en la sensibilidad gráfica de las prestaciones del software HistCiteTM se optó por responder visualmente a la pregunta de Kendall (dado un conjunto de datos bibliográficos para un año, ¿se puede estimar el número de revistas adicionales que es probable contribuyan al mismo para el período de dos años?). Se escogió una bibliografía anual - 2002 - intentando pronosticar el número de revistas que cabía esperar en la bibliografía combinada 2002 + 2003. Como en el artículo original, que plantea esta pregunta en distribuciones discretas finitas, la materia escogida fue la "fibra del músculo". En base a la tabla del HistCiteTM 'Ranked Source list' se dispusieron las revistas del 2002 por frecuencias, anotándose un total de 152. Aplicando la fórmula para el número de revistas adicionales, M, según el teorema central del límite, se obtuvo que M(2002) = 98. De hecho, la bibliografía combinada (2002 + 2003) - producida fácilmente con la opción 'Add Set' de HistCiteTM a partir de los registros de WOS - listó un total de 231 revistas. El valor de la predicción, esto es M(2002), es sólo parcialmente correcto pues 152 + 98 = 252. Los gráficos producidos para las bibliografías 2002 vs. 2002 + 2003 muestran que: varios registros seleccionados no satisfacen el criterio anual empleado. Así pues los grafos se emplearon para refinar las bibliografías. Al identificar aquellas revistas no pertenecientes al año, las referencias se eliminaron manualmente. La opción de construcción de gráficos 'Graph Maker' no parece preparada para cargar una selección de nodos (marcados) a partir de la tabla principal. Lo que hace difícil reformular la cuestión una vez planteada. En respuesta a la cuestión de Kendall, por ejemplo, aunque el mes de publicación se facilita en los artículos de la bibliografía, no está disponible inmediatamente un gráfico para los trabajos del primer semestre como algo opuesto al gráfico que surgiría de los artículos publicados a lo largo del segundo semestre; lo que impide la reproducción de la formulación de la pregunta de Kendall en sus términos originales. Objectifs: présenter les avantages de la nouvelle option de la configuration bibliométrique de HistCiteTM (2004) pour mettre en évidence des articles; afficher l'additivité des points de repère fabriqués à partir des domaines de la lecture d'un auteur; déterminer le lieu où le nombre d'articles générés par les auteurs est supérieur au celui qui s'ensuit du coauteur; montrer comment à partir d'un auteur, en vue de fixer une année de citation le choix d'une année antérieure (ou ultérieure) peuut être avantageux pour lui (elle) sur la base de l'examen du visuel produit; modeler la faisabilité d'un article entendu comme un renvoi (référence), sur la base des citations qu'il reçoit; dans la mesure où tenir compte des dépréciations liées à l'obsolescence donne lieu à un effet d'avantage cumulatif, tirer un graphique qui rende explicite cela et les affiche avec netteté aux termes des références vers des articles anciens fréquemment cités; montrer comment pour réaliser les graphiques le changement dans le pas initial fait que les articles puissent non seulement être regroupés par thème mais aussi dans le temps; harmoniser l'identification des communautés des auteurs scientifiques nucléaires, tant ceux qui continuent (à publier et être cités) comme les transitoires, en utilisant l'option de construction de graphiques du logiciel HistCite TM; commenter les difficultés du logiciel pour trouver des réponses dans un sens graphique. Méthodologie:Exploitation des nouveaux indicateurs de ponctuation de citation globale et locale sur une base temporelle (GCS/t, LCS/t), qui facilite la version 2004 du HistCiteTM. Analyse des correspondances optimales entre les cartographies d'études de cas publiées (de recherche sur le cancer et sciences de la gestion) et des graphes produits par HistCiteTM, avec le même ensemble bibliographique de départ. Examen des conditions d'application du théorême de la limite centrale sur la base de la modélisation graphique de la question de Kendall. Résultats:L'histoire du service éditorial du journal Management Science a donné lieu à l'élaboration d'un <b>histogramme</b> pour marquer le score de citationslocales à la fin de la période couverte (LCSe) 1970 - 2005 sur l'axe des abscisses et le score de citations au début de la même période (LCSb) en ordonnée. Les résultats ont été comparés avec un deuxième <b>histogramme</b> dont l'abscisse a présenté le score annuel global des citations (GCS/t) et le score annuel local des citations (LCS/t) en ordonnées. Ainsi ont été obtenus certains des articles clés rélévants, qui ont representés un 50 % des candidats choisis en matière de l'étude de la presse scientifique. Les deux paires d'indicateurs ont trié le même matériel d'information,accordant priorité au même premier auteur. La personne identifiée coïncide avec celle positionnée en premier lieu dans l'article publié par celuiavec le rôle de leader à l'origine. Le second paire d'indicateurs (GCS/t, LCS/t) a sans doute été le plus indicatif, parce qu'elle dispose à échellele reste des auteurs en fixant un critère de discrimination sur la base du sujet visé. De sorte que seuls les items à caractère théorique ouméthodologique ont été l'objet de répérage, aucun matérial à fondement empirique ou résultant de l'analyse béhaviorale a été enregistré. Une étude additionnelle sur l'usage de HistCiteTM, est l'enquête de la période 1981 - 1986 dans le cas du prix 1988 de la Dr. Josef Steiner Cancer Research Foundation Award, conduisant à une étude comparative des graphiques produites par le logiciel. Le cadre logique de la bibliographie a pu être isolé sur deux fronts de recherche ISI: code 83 - 1740, Oncogenes and the genetics of human; viral transforming genes and their DNA structure; et code 84 - 4046, Characterization of human and murine cellular oncogenes. Bien que ce récit a été publié, rédigé en termes d'anticipationnisme professionnel et en faisant usage de la distribution de Weibull, le logiciel génère une structure remarquablement similairesur la base des co-citations. Les événements clés de l'histoire, clairement identifiés par les graphes acycliques produits par le logiciel HistCiteTM, sont exactement les mêmes que ceux exprimés en matière de connectivité par les réseaux des citations sur la base de sources d'informations similaires (résultats déjà publiés). Cette démographie (des auteurs scientifique) est convenablement analysée par le modèle algorithmique automatique. Et autorise l'identification des scientfiques au coeur de la recherche, de ceux qui y interviennent d'une façon continue, et de tous ceux qui aident à élaborer les résultats. La sensibilité graphique du logiciel HistCiteTM a été testée au vu de la question de Kendall (qui pose le problème: avec un jeu de données bibliographiques pour une année spécifique, peut-on estimer le nombre de revues scientifiques supplémentaires susceptibles de contribuer pour une période de deux années?). Une bibliographie annuelle - 2002 - tout en essayant de prévoir le nombre de journaux que l'on pouvait espérer obtenir décrits dans la bibliographie qui combine les données 2002 + 2003. Comme sur les pages de l'article original (Kendall (1960)), où la question se pose pour les distributions discrètes finies, le thème retenu pour chercher a été "le fibre du muscle". Les journaux de l'année 2002 ont été rangés par ordre de fréquence, sur la base de la table 'Ranked Source list' (produite par HistCiteTM), totalisant 152. Selon la formule pour le nombre de journaux supplémentaires, M, d'après le théorême de la limite centrale (convergence vers de la loi normale), il a été établi que M(2002) = 98. En réalité, la bibliographie qui combine les deux années (2002 + 2003) - facilement produite avec l'option 'Add Set' du logiciel HistCiteTM à partir des registres de la base de données Web of Science (WOS) - a énuméré un nombre total de 231 journaux. La valeur prédite, M(2002), est seulement en partie correcte, étant donné que 152 + 98 = 252. Pour comparer les bibliographies des années 2002 envers celle de 2002 + 2003, les graphes produits montrent que: plusieurs registres sélectionnés ne satisfont pas au critère annuel utilisé. Ainsi donc les graphes ont été usés pour affiner les données des bibliographies. En identifiant les journaux hors année prise en considération, les références ont été éliminées manuellement. L'option de graphiques 'Graph Maker' ne semble pas être prête pour charger une sélection des noeuds (marqués) à partir de la table principale. Ce qui rend difficile reformuler la question soulevée une fois qu'elle est posée. En réponse à la question de Kendall, par example, bien que le mois de publication est inclus dans les articles publiés au cours du second trimestre; ce qui empèche la formulation de la question de Kendall dans ses termes originaux...|$|E
40|$|Objective The {{first part}} of the study {{included}} method validation(intra-assay-repeatability, impact of hemolysis and lipaemia, effect of anticoagulant) and evaluation of reference intervals for dogs for kaolin-activated thromboelastography (TEG® 5000 Thrombelastograph, Haemonetics Corporation; formerly Haemoscope Corporation; Braintree, MA, USA) and the automated coagulation analyzer STA Compact (STA Compact®, Roche Diagnostics GmbH, Mannheim, Germany). In a second part of the study, the response of the coagulation system in a canine model of endotoxemia was evaluated. Material and Methods Before the establishment of reference intervals, repeatability and interferences were assessed. Investigated variables included the TEG values R, K, alpha, MA and G as well and the coagulation variables one stage prothrombin time (OSPT), activated partial thromboplastin time (aPTT), thrombin time (TT), fibrinogen, Factor VIII (FVIII), antithrombin (AT), protein C (PC), protein S (PS), resistance against activated protein C (APC-ratio), plasminogen, D-dimer, anti-FXa run on the coagulation analyzer STA Compact. Intra-assay repeatability for TEG variables was performed in 6 healthy dogs with duplicate measurements. The standard deviation required for calculation of the coefficient of variation (CV) was consistent with the root of the pooled variance estimate. The comparison between TEG results obtained with native whole blood or citrated whole blood was performed in 16 healthy dogs. For assessment of hemolysis on TEG results, 17 healthy dogs were included. Hemolytic samples were prepared with two techniques: mechanical stress (HM) and as well as freeze and thawing (HF). The intra-assay repeatability for variables assayed with the STA Compact was assessed with 15 replicate measurements in a sample taken from one healthy dog. The intra-assay variation was estimated for variables mentioned above. To assess the influence of lipemia on measurements run on the STA Compact analyzer, samples with three grades of lipemia were prepared by adding Liquigen® (Liquigen®, Pfrimmer Nutricia GmbH, Erlangen, Germany) to the samples of three healthy dogs. The effect of hemolysis on these coagulation parameters was performed by treating whole blood in afreeze-thaw cycle and three grades of hemolysis were obtained. Establishment of reference intervals for TEG parameters and coagulation parameters measured on the STA Compact was performed with 56 healthy dogs. After evaluating the default settings of human assays applied on the STA Compact for canine specimens, some modifications of the test methods were needed. A standard curve has to be prepared with canine pooled plasma for PC, PS and FVIII. Also for both analyzer measurements the impact of sex on the results was established. In the second part of this investigation, the impact of endotoxemia on primary, secondary and tertiary hemostasis as well as TEG variables was evaluated in 10 healthy Mongrel dogs. A control group (n= 5 dogs) and a treatment group (n= 5 dogs) was created. Dogs in the treatment group received endotoxin (LPS) dissolved in sterile saline 0. 9 % which was administered intravenously at a dosage of 0. 02 mg/kg. The control group received 0. 2 ml/kg sterile saline 0. 9 %. Venous blood samples were collected before application of LPS or placebo (0 hour), 1, 4 and 24 hours after treatment. Statistics Results were analyzed with the Graph Pad Prism (Graph Pad Software, San Diego, USA), Analyse-it Method evaluation (Analyse-it Method validation Edition version 2. 12 - © 1997 - 2008, Analyse-it Software Ltd.) and BMDP statistical software (BMDP Statitical software Inc., 1440 sepulveda Blvd, Los Angeles, CA 90025 USA). Intra-assay variation for TEG variables was assessed by calculating the arithmetic mean and the pooled variance estimate, based on the differences in the duplicate determinations. For variables measured on the STA compact fifteen-run intra-assay repeatability for normal values was calculated from a sample of a healthy dog. The effect of anticoagulant and interferences was assessed by a paired t-test (or Wilcoxin signed rank sum test). For the establishment of the reference intervals, an Anderson Darling test was performed to verify the assumption of normality. In case of normal and log normal distribution of data, double sided reference intervals were obtained by calculating the mean± 1. 96 SD (standard deviation) so that 95 % of the reference population wasincluded. If non-normal distribution of data was present, the non-parametric percentile method was applied. The 2. 5 and 97. 5 percentiles were calculated to obtain the 95 % double sided reference interval. Data were depicted as histograms with the reference interval as well as the 90 % confidence interval of the upper and lower reference limits. The influence of sex on the reference intervals was assessed by using an unpaired t-test or a comparable non-parametric test (Mann Whitney U test). The differences between control group and endotoxin treatment group were assessed with a two way analysis of varience and covariance with repeated measures. Results Intra-assay CVs for R; K; alpha; MA and G were 7. 6 %; 17. 7 %; 7. 4 %; 2. 9 % and 6. 6 % respectivley. Samples with hemolysis resulted in a significantlydecreased R value and decreased MA, G and alpha; value (P< 0. 001 to < 0. 0001). Furthermore, a significantly high K value was seen compared to the control group (P< 0. 01). There was no significant impact of anticoagulant on TEG variables. Inter-individual variation was higher in native samples than in citrated whole blood. Intra-assay CVs for the coagulation parameters OSPT, aPTT, TT, fibrinogen, FVIII, AT, PC, PS, APC-ratio, plasminogen, D-dimer and anti-FXa were 1. 22 %, 1. 02 %, 1. 64 %, 5. 6 %, 3. 89 %, 4. 68 %, 2. 36 %, 1. 4 %, 1. 45 %, 20. 16 %, 45. 92 % and 12. 83 %. FVIII-activity, antithrombin, proteinC, protein S, and APC-ratio were overestimated in haemolytic plasma, whereas fibrinogen, TT, and aPTT were underestimated. Lipemia resulted only in false high D-dimers. Reference intervals for the kaolin activated TEG were as follows: R= 1. 8 - 8, 6 min.; angle alpha= 36. 9 - 74. 6 degrees; K= 1. 3 - 5. 7 min.; MA= 42. 9 - 67. 9 mm and G= 3. 2 - 9. 6 Kdyn/cm 2. Reference intervals for the STA Compact automated analyzer were as follows: OSPT= 5. 7 - 8. 0 sec.; aPTT= 10. 0 - 14. 3 sec.; TT= 11. 9 - 18. 3 sec.; fibrinogen= 1. 3 - 3. 1 g/l; AT= 107. 9 - 128. 0 %; D-dimer= 0. 023 - 0. 65 µg/ml; anti-FXa= 0. 04 - 0. 26 IU/L; APC-ratio= 2. 0 - 3. 0; PC= 74. 4 - 160. 5 %; PS= 75. 5 - 118. 9 % and FVIII= 70. 9 - 136. 4 %. The results for PC, PS and FVIII have to be compared to a canine standard curve. There was no significant impact of sex on results of both analyzers. Second part of the study: The endotoxin-induced clinical signs included lethargy (n= 5 / 5), diarrhea (n= 4 / 5), vomitus (n= 4 / 5) and abdominal pain (2 / 5). Regarding the evaluation of the impact of endotoxin on the coagulation process severe leukopenia (mean 2. 5 ± 0. 7 × 109 /l; P<. 0001) and a significiant 2. 2 -fold increase in D-dimers (P= 0. 001) were observed at time point 1 hour. At time point 4 hours, a significant raise in body temperature (P= 0. 0006) and in OSPT (P= 0. 0042) was seen as well as a significant decrease in fibrinogen (P= 0. 0007), protein C (P= 0. 0021) and protein S (P= 0. 008). PLTs (P=. 0284) and antithrombin (P=. 0170) were lowest at time point 24 hours. TEG variables did not significantly differ between the groups. APC-ratio was higher in the sepsis group (P= 0. 0348), however, remained within the reference interval in 4 / 5 dogs. Conclusion The data provided useful reference ranges for kaolin-activated TEG and for the STA Compact automated analyser but some human tests (e. g., Protein C, protein S and factor VIII) have to be modified. Regarding the impact of endotoxemia on the hemostatic system, the earliest indicator of sepsis-associated coagulation abnormalities were D-dimers which were followed by increased OSPT and decreased protein C. APC-ratio and TEG variables were not considered to be good screening variables in early sepsis/endotoxemia. Ziel der Studie Das Ziel des ersten Teils der Studie war die Methodenevaluation (inklusive intra-assay Wiederholbarkeit, Einfluss von Hämolyse, Lipämie und Antikoagulant) sowie die Referenzwertbestimmung für Hunde für kaolin-aktivierte Thrombelastographie (TEG® 5000 Thrombelastograph, Haemonetics Corporation; formerly Haemoscope Corporation; Braintree, MA, USA) und das automatisierte Gerinnungsanalysegerät STA Compact (STA Compact®, Roche Diagnostics GmbH, Mannheim, Germany). Im zweiten Teil der Studie soll der Einfluss von Endotoxämie auf den Gerinnungsprozess bei einem Hunde-Endotoxinmodell untersucht werden. Material und Methoden Vor der Etablierung der Referenzwerte wurden die Wiederholbarkeit und Interferenzen auf die Untersuchungen durchgeführt. Die evaluierten Gerinnungsparameter umfassten die TEG Parameter R, K, alpha, MA und G sowie die Koagulationsparameter Prothrombinzeit (PT), aktivierte partielle Thromboplastinzeit (aPTT), Thrombinzeit (TT), Fibrinogen, Faktor VIII (FVIII), antithrombin (AT), Protein C (PC), Protein S (PS), Resistenz gegen aktiviertes Protein C (APC-ratio), Plasminogen, D-dimer und anti-FXa, die am STA Compact gemessen wurden. Die Wiederholbarkeit für die TEG-Parameter wurde mit Doppelmessungen bei 6 Hunden durchgeführt. Die Standardabweichung, die für die Berechnung des CV erforderlich war, war mit der Wurzel der gepoolten Varianz übereinstimmend. Der Vergleich von TEG Ergebnissen mit nativem und Zitrat antikoaguliertem Vollblut wurde bei 16 gesunden Hunden durchgeführt. Für die Beurteilung der Hämolyse auf TEG Ergebnisse wurden 17 gesunde Hunde eingeschlossen. Hämolytische Proben wurden mit zwei Verfahren hergestellt: mechanischer Stress (HM) und sowie Einfrieren und Auftauen (HF). Die intra-assay Wiederholbarkeit für die am STA Compact gemessenen Parameter wurde bei einer von einem gesunden Hund entnommenen Probe mit 15 Wiederholungsmessungen durchgeführt. Um den Einfluss von Lipämie auf die Resultate der Messungen beim STA Compact zu bestimmen, wurden drei verschiedene Grade von Lipämie vorbereitet, indem Liquigen® (Liquigen®, Pfrimmer Nutricia GmbH, Erlangen, Germany) in die Proben gemischt wurde. Die Auswirkungen von 3 verschiedenen Stufen von Hämolyse auf die Gerinnungsparameter wurde durch Gefrieren/Auftauen von Vollblut untersucht. Die Referenzwertbestimmung für die TEG Parameter und die am STA Compact gemessenen Gerinnungsparameter wurde mit 56 gesunden Hunden durchgeführt. Die ersten Untersuchungen zeigten, dass einige humane Gerinnungstests für die Anwendung für Hundeblutproben modifiziert werden mussten. So war für PC, PS und FVIII die Etablierung einer Standardkurve mit Hundepoolplasma notwendig. Weiterhin wurde das Vorhandensein möglicher geschlechtsabhängiger Unterschiede der Referenzwerte untersucht [...] Die Auswirkung der Endotoxämie auf pimäre, sekundäre und tertiäre Hämostase sowie TEG Werte wurden bei 10 gesunden Mischlingshunden ausgewertet. Eine Kontrollgruppe (n= 5 Hunde) und eine Behandlungsgruppe (n= 5 Hunde) wurde untersucht. Die Hunde in der Behandlungsgruppe erhielten in 0, 9 % sterile Kochsalzlösung gelöstes Endotoxin (LPS) intravenös in einer Dosis von 0, 02 mg/kg injiziert. Die Kontrollgruppe erhielt eine intravenöse Injektion von 0, 2 ml/kg 0, 9 % steriler Kochsalzlösung. Venöse Blutproben wurden vor der LPS- oder Placebo-Applikation (0 Stunde), sowie 1., 4. und 24. Stunden nach der Behandlung entnommen. Statistik Die Ergebnisse wurden mit dem Graphpad Prism (Graph Pad Software, San Diego, USA), Analyse-it Methoden Bewertung (Analyse-it Method validation Edition version 2. 12 - © 1997 - 2008, Analyse-it Software Ltd.) und BMDP Statistik Software (BMDP Statitical software Inc., 1440 sepulveda Blvd, Los Angeles, CA 90025 USA) analysiert. Die Intra-Assay-Varianz (Variationskoeffizient, CV) wurde für die am STA Compact gemessenen Parameter mittels Berechnung des arithmetrischen Mittelwertes und der Standardabweichung bestimmt. Für die TEG Parameter erfolgte die Berechnung durch eine Schätzung der gepoolten Varianz basierend auf den Unterschieden von Doppelbestimmungen. Die Wirkung von Antikoagulanz und Interferenzen auf die Messergebnisse wurde mit Hilfe eines gepaarten T-tests (oder Wilcoxin signed rank sum Tests) beurteilt. Für die Bestimmung der Referenzwerte und der Überprüfung der Normalverteilung wurde ein Anderson Darling Test durchgeführt. Beim Vorliegen einer Normalverteilung oder logarithmischer Normalverteilung der Daten, wurden doppelseitig Referenzbereiche durch Berechnung der mittleren ± 0, 96 SD (Standardabweichung) berechnet, so dass 95 % der Referenzpopulation einbezogen wurde. Lagen nicht-normal verteilte Daten vor, wurde die nicht-parametrische Perzentil-Methode angewendet; hier erfolgte eine Berechnung der 2, 5 und 97, 5 Perzentile, um das 95 % doppelseitige Referenz-Intervall zu erhalten. Die Daten wurden als <b>Histogramme</b> mit dem Referenz-Intervall und den 90 %-Konfidenzintervalle der oberen und unteren Referenzgrenzen dargestellt. Der Einfluss des Geschlechts auf die Referenzintervalle wurde mittels eines ungepaarten t-Test oder einem vergleichbaren nicht-parametrischen Test (Mann-Whitney-U-Test) beurteilt. Im zweiten Teil der Studie wurden die Unterschiede zwischen der Kontrollgruppe und der mit Endotoxin behandelten Gruppe mittels einer zweifaktoriellen Analyse von Varianz und Kovarianz und Messwiederholung untersucht. Ergebnisse Für die TEG Parameter R; K; alpha; MA und G konnten die folgenden Variationskoeffizienten von 7. 6 %; 17. 7 %; 7. 4 %; 2. 9 % und 6. 6 % festgestellt werden. Proben mit Hämolyse führten zu einem signifikant erniedrigten R, MA, G und alpha Wert (P 0. 0001). Darüber hinaus konnte ein signifikant höherer K Wert im Vergleich zur Kontrollgruppe festgestellt werden (P< 0. 01). Es war kein signifikanter Einfluss des Antikoagulanzes auf die TEG-Ergebnisse nachweisbar, jedoch war die interindividuelle Variationin nativen Proben höher als im Zitratblut. Die Intra-Assay CVs für die Gerinnungsparameter PT, aPTT, TT, Fibrinogen, FVIII, AT, PC, PS, APC-ratio, Plasminogen, D-dimer und Anti-FXa waren 1. 22 %, 1. 02 %, 1. 64 %, 5. 6 %, 3. 89 %, 4. 68 %, 2. 36 %, 1. 4 %, 1. 45 %, 20. 16 %, 45. 92 % und 12. 83 %. Die FVIII-Aktivität, Antithrombin, PC, PS und APC-ratio wurdenin hämolytischem Plasma falsch hoch gemessen, während Fibrinogen, TT und aPTT falsch niedrig waren. Lipämie führte lediglich zu falsch hohen D-dimer Werten. Folgende Referenzbereiche konnten für das Kaolin aktivierte TEG etabliert werden: R= 1. 8 - 8, 6 min.; alpha= 36. 9 - 74. 6 Grad; K= 1. 3 - 5. 7 min.; MA= 42. 9 - 67. 9 mm und G= 3. 2 - 9. 6 Kdyn/cm 2. Für die am STA Compact gemessenen Parameter wurden folgende Referenzbereicheerstellt: OSPT= 5. 7 - 8. 0 sek.; aPTT= 10. 0 - 14. 3 sek.; TT= 11. 9 - 18. 3 sek.; Fibrinogen= 1. 3 - 3. 1 g/l; AT= 107. 9 - 128. 0 %; D-dimer= 0. 023 - 0. 65 µg/ml; anti-FXa= 0. 04 - 0. 26 IU/l; APC-ratio= 2. 0 - 3. 0; PC= 74. 4 - 160. 5 %; PS= 75. 5 - 118. 9 % und FVIII= 70. 9 - 136. 4 %. Die Ergebnisse für PC, PS und FVIII mussten dabei jeweils mit einer Hunde-Standardkurve verglichen werden. Es gab keinen signifikanten Einfluss auf die Ergebnisse der beiden Analysatoren. Zweiter Teil der Studie: Die endotoxin-induzierten klinische Symptome umfassten Lethargie (n= 5 / 5), Durchfall (n= 5 / 4), Erbrechen (n= 5 / 4) und Bauchschmerzen (n= 2 / 5). Eine Stunde nach der Injektion von Endotoxin konnte eine hochgradige Leukopenie (Mittelwert 2. 5 ± 0. 7 × 109 /L; P< 0. 0001) und ein signifikanter 2. 2 -fach Anstieg der D-dimere (P= 0. 001) beobachtet werden. Zum Zeitpunkt 4. Stunden war eine deutliche Erhöhung der Körpertemperatur (P= 0. 0006) und der PT (P= 0. 0042) sowie ein signifikanter Abfall der Fibrinogen Plasmakonzentration (P= 0. 0007), des PC (P= 0. 0021) und des PS (P= 0. 008) nachweisbar. PLTs (P=o. 0284) und AT (P= 0. 0170) waren am niedrigsten zum Zeitpunkt 24 Stunden. Die TEG Parameter unterschieden sich nicht signifikant zwischen den Gruppen. Die APC-ratio war höher in der Endotoxin-Gruppe (P=o. 0348), blieb jedoch innerhalb des Referenz-Intervalls bei 4 / 5 Hunden. Schlussfolgerung Insgesamt konnten nützliche Referenzwerte für das kaolin-aktivierte TEG und das automatische Gerinnungsanalysegerät STA Compactetabliert werden, jedoch mussten einigehumane Testmethoden (Protein C, Protein S und Factor VIII) zuvor für den Hund modifiziert werden. Die Untersuchung des Einflusses von Endotoxämie auf die Gerinnung zeigte, dass erhöhte D-Dimere der früheste Indikator für Endotoxämie sind, denen später eine Verlängerung der PT und ein Absinken von Protein C folgt. Die APC-ratio und TEG Parameter waren in der aktuellen Studie keine gute Indikatoren für das Erkennen einer frühen Sepsis/Endotoxämie...|$|E
40|$|Thesis (M. Sc. (Botany)) [...] North-West University, Potchefstroom Campus, 2004. Land {{degradation}} is {{a process}} that causes the reduction in resource potential of natural rangelands and occurs widespread throughout southern Africa. This process is mainly characterized by the loss in vegetation cover, which leads to the occurrence of bare and denuded patches, increased soil erosion, changes in species composition as well as bush encroachment by indigenous and alien invasive plant species in savannah areas. Degradation of rangelands has drastically extended at an alarming rate during the last few decades with the main causes being overstocking, extended periods of drought, global climate change, overgrazing and general mismanagement of the land. Many researchers, however, feel that rangeland degradation is mainly caused by a combination of changes in land use practices and climate variability. Land users have, however, been applying a variety of technologies over the years in order to restore affected rangelands and mitigate the effect of degradation. These technologies include passive and active intervention methods, aimed at restoring bare and denuded areas and controlling indigenous bush and alien plant species encroachment. Bush control can be carried out by applying different technologies, involving chemical, mechanical, manual or biological control. The focal point of this study is on bush encroachment, the factors causing the problem, the possible ways of controlling this phenomenon and lastly the incorporation of such information into a user-friendly Decision Support System (DSS). The Decision Support System comprises of two databases as well as a related expert system. Bush encroachment is a matter of great concern in most southern African countries. This study therefore mainly included data from Namibia and to a lesser extent, South Africa, as the main study areas, seeing that this form of degradation greatly influences the biodiversity of rangelands in both these countries. The Namibia Agricultural Union (NLU) identified the need {{for the development of a}} user-friendly Decision Support System, in which case studies concerning the different bush control technologies could be stored in a database. Restoration technologies, regarding the occurrence of bare and denuded areas, that have been applied by the land users over a period of time and in a specific environment in the past, have been captured in a computerized database and expert system, serving as a Decision Support System (DSS) and user-friendly consulting tool in a similar study, carried out by Mr. Van der Merwe (1997). This DSS was based on CBR (Case Based Reasoning) methodologies by which a number of case studies, that have previously been stored in the database, can be searched by means of an expert system approach to advise the land user concerning the most appropriate solution (action) to similar degradation problems. The DSS developed by Mr. Van der Merwe was never published or made accessible to the land user in a format that could be consulted by either CD-ROM or the internet. Seeing that the NLU identified the need for a similar DSS containing bush control technologies, it was decided to incorporate both these databases into a single DSS, concerning bush control as well as the restoration of bare and denuded patches. The newly converted DSS is currently known as EcoRestore and consists on two databases: Grass Expert, which focuses on technologies to reclaim degraded rangelands, and Bush Expert, which is more focused on the control of bush encroachment and combating of alien invasives. As mentioned, this study focussed on the development of the Bush Expert database and will therefore only include results, discussions and conclusions of these case studies. The case studies in the Bush Expert database consist of results obtained by means of a questionnaire completed by the land user, in collaboration with the agricultural extension officer, as well as a quantitative vegetation assessment, to determine the success rate of the applied technology. The Bush Expert questionnaire, comprises of questions concerning personal information of the land user (e. g. location of the farm), the situation on the farm before bush control was applied (e. g. information on the environmental factors, such as density of problem trees), as well as the type of control technology applied and the situation of the rangeland after control (e. g. establishment of the herbaceous species). The quantitative vegetation assessments involved the sampling of the woody and herbaceous components in the area where a specific control technology was applied. The density and height classes of the woody component were determined by means of the belt-transect method. By using the descending-point method, the herbaceous component was surveyed to determine the abundance/frequency of the annual and perennial grass species. In order to increase the success of any restoration project, it is important to take the existing indigenous knowledge of local land users, concerning the problem of degradation and mitigation thereof, into consideration. By doing so, the local people and communities have greater control and responsibility over their resources and are able to command a greater range and level of resourcefulness. Taking indigenous knowledge into consideration finally enables the local land users to actively participate in and influence higher-level decision-making processes by which they are affected. A total of 175 case studies in Namibia and nine case studies in South Africa were surveyed. The Namibian case studies were surveyed in the central and northern arid and semi-arid regions, and South African case studies in a limited location within the Limpopo Province. Only 100 of the Namibian case studies have thus far been incorporated into the Bush Expert database. Multivariate data analyses techniques, analysis of variance and correlation analyses were used to analyse the data obtained from the questionnaires and quantitative vegetation surveys. Results were represented in the form of <b>histogrammes,</b> tables and multivariate analysis ordinations. From the results obtained for the Bush Expert database, it was clear that chemical control technologies were most often applied in Namibian and South African case studies (61 %). The herbicides most commonly applied as chemical control technology in Namibia included Grazer (20 %) and Savana (15 %), whilst in South Africa these included Access (33. 3 %) and Tordon Super (33. 3 %). Herbicides were mostly applied by means of aerial application (46 %) methods in Namibia and as cut-stump treatment (55. 5 %) by means of knapsack spraying or with a brush in South Africa. The dominant woody species causing bush encroachment problems in Namibia were found to be Acacia mellifera, Acacia reficiens and Dichrostachys cinerea, whereas in South Africa these species included Dichrostachys cinerea, Acacia erubescens and Acacia karroo. The wood of the controlled problem species (dead woody material) is mostly not utilized after control, but rather left on the land to disintegrate and thus contribute to the organic material content in the soil. Dead branches are also used for brush packing, which forms and ideal micro-climate for the germination and establishment of grass seeds, which serves as an erosion control medium and protects grass seedlings against grazing impacts. Some land users do however produce charcoal from certain controlled woody species, in order to recover some of the input costs of bush control. The majority of the case study sites (68 %) in Namibia occurred within the 300 - 450 mm short- and long-term rainfall zones and in South Africa the majority of case study sites occurred within the short-term rainfall zone of 550 - 600 mm (66. 6 %) and 400 - 500 mm long-term rainfall zone (55. 5 %). Case studies where chemical and manual bush control technologies were applied indicated the highest success rates after control (81. 7 % and 75. 2 % respectively). Success rate as an entity was greatly influenced by the type of control technology applied, the density of the problem woody species after bush control as well as environmental variables such as rainfall and soil clay percentage. No definite trend could be determined concerning the application of a specific bush control technology and a certain problem species. Land users tend to apply a chosen control technology, according to the resources available, such as labour, mechanical implements and finances. The only positive correlation between control technologies and the type of problem species could be found regarding Dichrostachys cinerea. This species was mainly chemically controlled by means of the application of certain herbicides. The most important lesson to be learnt from the surveys completed in the two countries is that it is an absolute necessity to apply a proper after-care programme as a management practice following the initial control of problem woody species. The implementation of after-care determines the final success rate of any applied bush control technology as a restoration practice within a rangeland. Only 11 % of the case studies surveyed for Namibia and South Africa indicated the implementation of an after-care programme, which usually involved biological control (e. g. browsing by boer goats or the use of controlled or accidental natural veld fires). The EcoRestore Decision Support System is currently available as an online webversion (www. puk. ac. za/EcoRestore), as well as a CD-ROM version. The CD-ROM version is available in a package containing the CD and user's manual. An example of the package is included in this dissertation. In consulting the databases through question-and-answer procedures, the best action will be proposed to the land user for future rangeland restoration, either the reclamation of denuded areas or the control of bush encroachment. Since the case studies are based on past and existing experiences and research, the land user will have an indication of the expected outcome, should the same advised technology be applied. The EcoRestore DSS does not only offer a consulting tool for extension workers and technicians, but also creates networking and participation between land users and researchers, both locally and between neighbouring countries. The DSS is linked to other national and international websites and databases, to offer users a wider range of information and technologies with regard to agricultural and conservation practices. Better awareness is created amongst land users concerning the problem of rangeland degradation, which might encourage closer monitoring of the degradation and mitigation processes. The EcoRestore DSS was developed in such a way for it to be as user-friendly as possible, in order to reach as many parties involved in current or future restoration programmes. This study involved the development of the first version of the DSS (Version 1. 0) and is thus only the prototype system. It is proposed that the Bush Expert database of the EcoRestore DSS, will be expanded in future and additional bush control case studies from other southern African countries will be included. The addition of such case studies will ultimately increase the effectivity of this DSS. Master...|$|R
40|$|SUMMARYThis methodological {{study of}} {{quantitative}} electroencephalography {{starts with the}} history of EEG methods of analysis and of their applications. This thesis is basically focused on a comparative study of the most important methods of analysis. In the presentation of methods I first present the analysis of the instantaneous amplitude histograms of EEGs which is dependent upon the sampling frequency. Considering now the Fourier spectral analysis this method implies to take quite a number of precautions before being properly applied to EEG. For instance, it is necessary to compute enough measures to allow later on the statistical validation of a power spectrum analysis G(f). Then, I propose the example of spectral multiple EEG channels analysis, which is based on the method of spectral regression. This method of analysis gives more precisely the relations of causality at specific frequencies by finding their sources across EEG channels and determining if those sources are based on real signals source or random noise. I have later specified the mathematical relations between the integrative method of Drohocki and spectral analysis. The mean value l of n measures of successive epochs of an EEG signal, which is rectified and integrated: l is proportional to the root­mean­square (rms) value of the analyzed signal and also to its standard error. The coefficient of variation CV(l) of the integrated measures is proportional to the spectral coefficient of variation CV(k), which for a first approximation is equal to k/√T, with T being the time epoch of analysis and k a “coefficient of spectral regression” that I have defined by the formula (k 2 = ∑G 2 /(∑G) 2) in reference to Blackman and Tuckey. This presentation of methods is achieved with the period analysis and its relations to spectral analysis, followed by a brief survey of new heuristic methods, which are mimicking the electroencephalographist practitioner in his way and are applying methods of linear prediction. My results are divided into three chapters. In the first chapter I present first Applications of quantitative EEG recorded in rat. Then I give three examples of applying the integrative method of Drohocki. First by computing the ratio of integrated values of ECoG/EMG for quantifying the phases of wakefulness and sleep. When this ratio is above or below an experimentally first computed predetermined threshold, this ratio can well determine the state of wakefulness or sleep. I have applied this technique {{to the study of the}} hypovariability of the ECoG and of the neck muscles EMG recorded in rats before and after administration of neuroleptics. The ECoG/EMG ratio provides the time­course of the electro­pharmacokinetic effect through hours of the neuroleptic treatment. Secondly I have studied the statistical decomposition of the observed polymodal composite distributions of values of integrated ECoG signals computed over successive periods of one­hour time span. Such an analysis provides a decomposition of these polymodal distributions into a sum of elementary Gaussian distributions. Each elementary Gaussian distribution being specific of a homogeneous state of vigilance. Thirdly, this chapter is mainly concerned with the comparative study between the three different tracings of occipital ECoG in the rat for quantifying homogeneous phases of wakefulness, slow wave sleep and REM sleep (paradoxical sleep). The four principal methods of EEG analysis previously compared theoretically, have then been now compared experimentally, based on the three different states of vigilance. I have first verified the precedent mathematical relationships established between the integrative method and spectral analysis. By correlation analysis and multi­linear regression, I have been able to obtain pertinent information which has been reduced to 5 independent parameters. Step-by­step discriminant analysis has shown that the mean frequency of the spectral peak and the mean integrated amplitude are sufficient for a good discrimination between the three analyzed states of vigilance. The second chapter of results is based on EEG recordings in man. in order to give Applications of quantitative EEG recorded in man. I describe a program of statistical spectral analysis, which works in real time based on four EEG channels simultaneously recorded with a double rejection of artifacts and a pre­treatment of the sampled EEGs. After longitudinal studies of different quantified recordings I have computed a four factor variance analysis on a transversal study sample of EEG recordings for a group of 7 subjects receiving two different treatments (placebo at the beginning of the night before and nitrazépam 5 mg, p. o. the day after), 2 sequences (eyes open followed by eyes closed EEG recordings), 4 posterior EEG channels, and computed characteristic spectral parameters. Results of variance analysis reveal that only sequences and parameters appear to be statistically different. Later further 3 factor variance analyses over the 32 computed spectral parameters have found, which parameters are the best discriminant parameters between EEG sequences : the spectral peak, the coefficient of resonance and of complexity, the fast mean frequencies. etc. These factorial analyses have allowed me to compare spectral differences between two mean power spectra by applying Student t tests in different conditions : between treatments, sequences, EEG channels and between subjects. Finally, in the third chapter of results, I have presented a first modulation analysis of EEG by applying Hilbert transform to EEG. Starting from an EEG signal x(t) we can evaluate a signal y(t), which is characterized by a Gaussian random narrow band process, from an analysis of modulation y(t) = m(t) cos(ω 0 t + φ(t)), with m(t) being the amplitude modulation and φ(t) the frequency modulation around a broadcasting frequency ω 0 = 2 πfo. This modulation analysis is based on the Hilbert transform ˆx(t) obtained from the Fourier transform X(f) of x(t), by a multiplication by (­j. sign(f)) followed by inverse transformation. This gives directly the computation of the “envelope” m(t) of x(t), in the radioelectric sense of the word envelope. The frequency modulation is obtained directly by derivation of the phase modulation. I have applied this analysis to the precedent three tracings of states of vigilance in rat. I have found that the hippocampal theta rhythm is characteristic of a specific amplitude modulation during the REM state of sleep in rat together with a frequency modulation, which is not present in the two other states of wakefulness and of slow wave sleep in rat. This last method can be applied in case of non­stationary EEG tracings and it keeps all the signal information. The amplitude and frequency modulations are specific respectively of the instantaneous amplitude and frequency and we know the difficulty to obtain directly this last instantaneous frequency. This is why I have attempted to apply the techniques of statistical radioelectricity in quantitative electroencephalography. In this thesis, which is based on 15 articles, I have wanted to illustrate the theory of analysis of electrobiological signal by some various examples of applications in animal and in man. I have wished to show also how new methods of analyses may lead and drive to new applications. Cette étude méthodologique de l'électroencéphalographie quantitative fait d'abord l'historique des méthodes d'analyse de l'EEG et de leurs applications. Cette thèse est centrée principalement autour de la comparaison des principales méthodes d'analyse. Dans l'exposé des méthodes, je présente tout d'abord l'analyse des <b>histogrammes</b> d'amplitudes instantanées de l'EEG, qui dépend de la fréquence d'échantillonnage. L'analyse spectrale exige un certain nombre de précautions pour être correctement utilisée. C'est ainsi qu'il convient de moyenner suffisamment les mesures effectuées si l'on veut procéder à une validation statistique d'un spectre de puissance G(f). Je propose ensuite l'exemple d'une analyse multivoies appliquée à quatre dérivations enregistrées simultanément et qui utilise la méthode de "régression spectrale". Cette analyse permet de préciser les relations de causalité de fréquences particulières, en déterminant leur origine parmi les dérivations et s'il s'agit d'une source ou d'un bruit. J'énonce ensuite les relations mathématiques qui relient particulièrement la méthode intégrative de DROHOCKI et l'analyse spectrale. La moyenne I de n mesures successives d'EEG redressé et intégré, est proportionnelle à la valeur efficace du signal analysé, ou bien encore à son écart type. Le coefficient de variation des mesures intégrées CV(I) est proportionnel à un coefficient de variation spectral CV(k) qui est égal en première approximation à k/√T. où T est la période d'analyse et k est un "coefficient de résonance spectral" que j'ai défini (k 2 = ∑G 2 /(∑G) 2) en référence aux travaux de BLACKMAN et TUKEY. L'exposé des méthodes s'achève par l'analyse de période et ses relations avec l'analyse spectrale, puis par un bref aperçu des nouvelles méthodes d'analyse, heuristiques, imitant la démarche de l'électro encéphalographiste ou utilisant des méthodes de prédiction linéaire. Mes résultats sont divisés en trois chapitres. Dans le premier chapitre, je présente des applications de l’électroencéphalographie quantitative chez le rat. Je donne ainsi trois exemples d'utilisation du rapport des valeurs intégrées ECoG/EMG; pour la quantification des phases d’évei 1 et de sommeil, par rapport à un dépassement de seuil prédéterminé et pour l'étude de l'hypovariabilité des tracés observée après administration de substance neuroleptique. Puis, j'étudie la décomposition statistique des distributions composites polymodales des valeurs intégrées d'ECoG, calculées pour des périodes successives d'une heure. Cette analyse permet de décomposer simplement en une somme de distributions gaussiennes élémentaires les distributions polymodales. Chaque distribution élémentaire correspond alors à un état de vigilance homogène. Enfin, ce chapitre est surtout consacré à l'étude comparée de trois tracés d'ECoG occipital pour des phases homogènes d'éveil, de sommeil à ondes lentes et de sommeil paradoxal. Les quatre principales méthodes d'analyse de l'EEG ont été ainsi comparées à partir de ces trois tracés. J'ai tout d'abord vérifié les relations mathématiques établies au préalable entre la méthode intégrative et l'analyse spectrale. Par analyse de corrélation et de régression multilinéaire, j'ai pu réduire l'information pertinente à 5 paramètres indépendants entre eux. Une analyse discriminante pas à pas a alors montré que la fréquence dominante du pic spectral et l'amplitude moyenne I suffisaient à bien discriminer entre eux les trois états de vigilance analysés. Le deuxième chapitre de résultats fait état d'applications de l’électroencéphalographie quantitative chez l’homme. J'expose le programme d'analyse spectrale statistique qui fonctionne en temps réel, à partir de quatre dérivations enregistrées simultanément et qui utilise un double rejet d'artéfacts ainsi qu'un prétraitement des EEG échantillonnés. Après des études longitudinales de divers enregistrements quantifiés, j'ai effectué une analyse de variance à quatre facteurs pour l'étude transversale d'un ensemble de tracés de 7 sujets: 2 traitements (placebo la veille au soir et nitrazépam 5 mg, p. o. le lendemain), 2 séquences (yeux ouverts ou fermés), 4 dérivations postérieures, et les paramètres spectraux caractéristiques. Seuls, les séquences et les paramètres apparaissent significativement différents. Puis des analyses de variance à trois facteurs pour chacun des 32 paramètres spectraux caractéristiques calculés révèlent quels sont ceux qui discriminent le mieux entre les traitements: pic spectral, coefficient de résonance et de complexité, fréquences rapides, etc. Ces analyses factorielles m'ont permis de valider l'utilisation de l'épreuve du t de Student appliquée aux différences spectrales que je préconise afin de comparer entre eux deux spectres moyens de puissance dans différentes conditions: intertraitements, interséquences, interdérivations, intra et intersujets. Enfin, dans le troisième chapitre de résultats, j'ai présenté l’analyse de modulation de l’EEG, qui partant d'un signal x(t), permet d'évaluer un signal y(t), caractérisant un processus aléatoire gaussien à bande étroite à partir d'une analyse de modulation: y(t) = m(t) cos(ω 0 t + φ(t)), où m(t) est alors la modulation d'amplitude et φ(t) la modulation de phase autour d'une fréquence porteuse ω 0 = 2 πfo. Cette analyse de modulation utilise la transformée de Hilbert ˆx(t) obtenue à partir de la transformée de Fourier X(f) de x(t), par multiplication par (-j. signef) et transformation de Fourier inverse. Cela conduit directement au calcul de "l'enveloppe" m(t) de x(t), au sens radioélectrique du terme. La modulation de fréquence est obtenue directement par dérivation de la modulation de phase. J'ai appliqué cette analyse aux tracés des trois états de vigilance chez le rat. J'ai trouvé pour le rythme thêta hippocampique caractéristique du tracé de sommeil paradoxal, une modulation d'amplitude particulière ainsi qu'une modulation de fréquence qui n'apparaît pas pour les deux autres tracés analysés. Cette dernière méthode est susceptible d'être appliquée dans le cas de tracés non-stationnaires, elle conserve toute l'information du signal. Les modulations d'amplitude et de fréquence caractérisent respectivement l'amplitude et la fréquence instantanée, on connaît la difficulté de l'obtention directe de cette dernière. J'ai ainsi tenté d'élaborer une première utilisation des techniques de radioélectricité statistique en électroencéphalographie quantitative. Dans cette thèse, qui s'appuie sur 15 publications, j'ai voulu illustrer la théorie de l'analyse du signal électrobiologique par des exemples d'applications variées pris chez l’homme et l'animal. J'ai souhaité montrer en retour que de nouvelles méthodes d'analyse peuvent conduire à de nouvelles applications...|$|R

