1136|254|Public
25|$|Before {{data mining}} {{algorithms}} can be used, a target data set must be assembled. As data mining can only uncover patterns actually {{present in the}} data, the target data set must {{be large enough to}} contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. <b>Data</b> <b>cleaning</b> removes the observations containing noise and those with missing data.|$|E
50|$|Tidy data provide {{standards}} and concepts for <b>data</b> <b>cleaning,</b> and with tidy data there’s {{no need to}} start from scratch and reinvent new methods for <b>data</b> <b>cleaning.</b>|$|E
5000|$|May 2010: U Potter’s Wheel: An Interactive <b>Data</b> <b>Cleaning</b> System ...|$|E
3000|$|Proof According to the {{definition}} of timeliness measurement, currency decreases because the jitter is eliminated after the <b>data</b> related <b>cleaning</b> process, while ƒ [...]...|$|R
5000|$|Data pre-processing, {{a step of}} <b>cleaning</b> <b>data</b> in {{data mining}} for {{analysis}} purposes ...|$|R
50|$|JTS {{defines a}} {{standards-compliant}} geometry system for building spatial applications; examples include viewers, spatial query processors, and tools for performing <b>data</b> validation, <b>cleaning</b> and integration. In {{addition to the}} Java library, the foundations of JTS and selected functions are maintained in a C++ port, for use in C-style linking on all major operating systems, {{in the form of}} the GEOS software library.|$|R
50|$|Automated and manual <b>data</b> <b>cleaning</b> is {{commonly}} performed in migration to improve data quality, eliminate redundant or obsolete information, and match {{the requirements of}} the new system.|$|E
50|$|The {{aim of the}} NBT toolbox is to make {{biomarker}} research easier at all levels. From having raw <b>data,</b> <b>cleaning</b> it, calculating biomarkers, to performing advanced statistics.|$|E
50|$|Once {{processed}} and organised, the data may be incomplete, contain duplicates, or contain errors. The need for <b>data</b> <b>cleaning</b> will arise from {{problems in the}} way that data is entered and stored. <b>Data</b> <b>cleaning</b> is the process of preventing and correcting these errors. Common tasks include record matching, identifying inaccuracy of data, overall quality of existing data, deduplication, and column segmentation. Such data problems can also be identified through a variety of analytical techniques. For example, with financial information, the totals for particular variables may be compared against separately published numbers believed to be reliable. Unusual amounts above or below pre-determined thresholds may also be reviewed. There are several types of <b>data</b> <b>cleaning</b> that depend on the type of data such as phone numbers, email addresses, employers etc. Quantitative data methods for outlier detection can be used to get rid of likely incorrectly entered data. Textual data spell checkers can be used to lessen the amount of mistyped words, but it is harder to tell if the words themselves are correct.|$|E
30|$|<b>Data</b> {{preparation}} and <b>cleaning</b> constant and identical columns are removed. For example, the engine torque value {{is identical to}} the correction of engine torque value. After deleting redundant features, we replace the missing values or wrong ones using the KNN method.|$|R
30|$|Six {{laboratory}} technicians, four nurses, and two supervisors {{were trained}} by the principal investigator to assist during data collection. Biochemical tests (HbA 1 c) were carried out using 902 Automatic Analyzer with Roche/Hitachi kit following a minimum of 10  hours fasting period. To ensure {{the quality of the}} interview and the acquisition of quality data, random checks were carried out by field supervisors and the principal investigator. <b>Data</b> entry, <b>cleaning,</b> and coding were done using Epi-info version 3.5. 3.|$|R
40|$|This {{research}} explores how {{items are}} grouped into product families {{in a manner}} that reduces travel time (or distance) for operators picking orders in a warehouse. By identifying items commonly ordered together and locating those items together in the warehouse, operators can pick multiple items using a single trip rather than separate trips to different warehouse locations. Using secondary data source provided by our business sponsor, this research uses K-means clustering analysis and Market basket analysis as the primary analytic methodologies to generate SKU families. Other technical tools are MS Excel, MS Access and SAS University Edition. This final report gathers the various deliverables pertaining to the four analytic steps: 1) Getting & Exploring <b>Data,</b> 2) <b>Cleaning</b> <b>Data,</b> 3) Analyzing Data, and 4) Presenting Results. Final results reveal fifty items grouped into 6 families can reduce warehouse picking expenses of $ 420, 000 by $ 40, 134. 85. This reflects a 9. 6 % reduction in total warehouse labor expenses. B. S. (Bachelor of Science...|$|R
50|$|Additionally, {{data may}} be {{conducted}} in the data warehouse development process after data has been loaded into staging, the data marts, etc. Conducting data at these stages helps ensure that <b>data</b> <b>cleaning</b> and transformations have been done correctly and in compliance of requirements.|$|E
50|$|Transform is a <b>data</b> <b>cleaning</b> engine {{built to}} deal with data {{problems}} from the developing world. Transform uses machine learning, entity recognition, fuzzy logic, geocoding and reverse geocoding, and more to identify errors, merge incompatible data, and work at scale on large databases or data systems.|$|E
50|$|DW {{describes}} the basic data pre-processing functionality with DW : S → S and W ∈ {T, C, SL, I} including data transformation functions DT, <b>data</b> <b>cleaning</b> functions DC, data selection functions DSL and data integration functions DI {{that are needed}} to make analysis functions applicable to the data set.|$|E
3000|$|There were 319 {{participants}} who finished the experimental tasks. The survey used filtering items {{to determine if}} participants were actually reading the items (e.g., there are 8  days in a week). Three respondents were eliminated based on the filtering items, two identified their location as outside the US and were deleted, and 19 were also eliminated due to large blocks of missing <b>data.</b> After <b>cleaning</b> the <b>data,</b> there were 295 participants with usable data. The sample consisted of {{more men than women}} (F =  133, M =  162), who were primarily Caucasian American (84.8 [...]...|$|R
40|$|Abstract — ETL is {{responsible}} for the extraction of <b>data,</b> their <b>cleaning,</b> conforming and loading into the target. ETL is a Critical layer in DW setting. It is widely recognized that building ETL processes is expensive regarding time, money and effort. In this, firstly we review commercial ETL tools and prototypes coming from academic world. After that we review designing works in ETL field and modelling ETL maintenance issues. We review works in connection with optimization and incremental ETL, then finally challenges and research opportunities around ETL processes...|$|R
5000|$|... {{three major}} {{versions}} of MacKeeper had been released. The first beta-version of MacKeeper 0.8 {{was released on}} 13 May 2010. [...] MacKeeper 1.0 was released on October 26, 2010. MacKeeper 2.0 was released on 30 January 2012 at Macworld - iWorld with an expanded number of utilities related to security, <b>data</b> control, <b>cleaning</b> and optimization. Kromtech Alliance acquired Mackeeper from ZeoBit in April 2013. MacKeeper 3.0 was released in June 2014 as {{software as a service}} with a new [...] "human expert" [...] feature and optimization with OS X Yosemite.|$|R
50|$|Data {{cleansing}} or <b>data</b> <b>cleaning</b> is {{the process}} of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting.|$|E
5000|$|The typical data {{pre-processing}} applying <b>data</b> <b>cleaning,</b> {{data integration}} and data transformation functions {{is defined as}} DP = DT(DI(DC(S1, ..., Sn))). After the pre-processing step either automated analysis methods HS = {fs1, ..., fsq} (i.e., statistics, data mining, etc.) or visualization methods VS : S → V, VS = {fv1, ..., fvs} are applied to the data, in order to reveal patterns {{as shown in the}} figure above.|$|E
50|$|Many silent {{calls are}} a result of process known as pinging. This is very similar to an Internet Protocol (IP) ping, where the {{intention}} is {{to see if there is}} life at the destination. This is often used by <b>data</b> <b>cleaning</b> companies as a method of removing dead telephone numbers from old lists. Each number is dialed and immediately dropped if the call is connected. If the call is not dropped quickly enough, it may result in a short ringing signal.|$|E
40|$|Recent {{data mining}} methods {{represent}} modern approaches capable of analyzing {{large amounts of}} data and extracting meaningful and potentially useful information from it. In this work, we discuss all the essential steps of the data mining process - including <b>data</b> preparation, storage, <b>cleaning,</b> <b>data</b> analysis as well as visualization of the obtained results. In particular, this work {{is focused on the}} data available publicly from the Insolvency Register of the Czech Republic, that comprises all insolvency proceedings commenced after 1. January 2008 in the Czech Republic. With regard to the considered type of data, several data mining methods have been discussed, implemented, tested and evaluated. Among others, the studied techniques include Market Basket Analysis, Bayesian networks and social network analysis. The obtained results reveal several social patterns common in the current Czech society...|$|R
40|$|This short paper {{addresses}} {{issues in}} connection with <b>cleaning</b> <b>data</b> integrated from multiple sources. It advocates {{the need for a}} new form of constraints to enforce bindings of semantically related data values, and then proposes a class of such constraints, referred to as conditional constraints. Finally, technical challenges for <b>cleaning</b> <b>data</b> based on these conditional constraints are briefly described. 1. <b>Cleaning</b> Integrated <b>Data</b> One of the central problems {{in connection with}} data integration is data cleaning: detect and remove errors, conflicts and inconsistencies from the target data integrated from multiple sources. Recent statistics reports that dirty date costs US businesses billions of dollars annually (cf. [1]). In our direct experience with Lucent Technologies for providing telecommunication services, errors in data routinely lead to problems like failure to bill for provisione...|$|R
40|$|We {{have been}} working on two {{different}} KDD systems for scientific data. One system involves comparative genomics, where the database contains more than 60, 000 plant gene and protein sequences plus results extracted from similarity searches against public sequence databases. The second system supports a several-decades long longitudinal field study of chimpanzee behavior. Both systems have components for the storing of raw <b>data</b> and for <b>cleaning</b> <b>data</b> before querying begins and for displaying data extractions. Both systems use a relational DBMS. In this paper we report on a) the extensions we made to the DBMS to support our analysis of the data, and b) the way that we used those extensions as, with users, we developed a thought from an initial idea to a richer analysis. We have found tha...|$|R
5000|$|Year 1 : Hiring and {{training}} of personnel, development of questionnaire (including translation to and back-translation from the relevant languages), establishment of IT infrastructure, preparation of the biobank with blood samples, and definition of source populationYears 2 & 3 : Enrollment of 20,000 cohort participants per site, {{for a total of}} 100,000Year 4 & 5 : [...] Follow-up, ascertainment and validation of outcomes, optimization of procedures for retention and compliance, and analyses of cross-sectional <b>data.</b> <b>Cleaning</b> databases, analyzing data, and preparing manuscripts based on the follow-up data ...|$|E
50|$|Before {{data mining}} {{algorithms}} can be used, a target data set must be assembled. As data mining can only uncover patterns actually {{present in the}} data, the target data set must {{be large enough to}} contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. <b>Data</b> <b>cleaning</b> removes the observations containing noise and those with missing data.|$|E
50|$|Another new {{direction}} is addressing the big data sets that are increasingly affecting or being contributed to {{in our daily}} lives. Statistician Rob Gould, creator of Data Cycle, The Musical dinner and theatre spectacular, outlines many {{of these types of}} data and encourages teachers to find ways to use the data and address issues around big data. According to Gould, new curriculum focused on big data will address issues of sampling, prediction, visualization, <b>data</b> <b>cleaning,</b> and the underlying processes that generate data, rather than traditionally emphasized methods of making statistical inferences such as hypothesis testing.|$|E
3000|$|... n= 1.96 ^ 2 (0.5 × 0.5 × 476)/ 0.05 ^ 2 (476 - 1)+ 1.96 ^ 2 × 0.5 × 0.5 = 457.15 / 2.15 = 213.63 [...] After <b>data</b> {{collection}} and <b>cleaning,</b> 13 questionnaires were detected as spoilt therefore discarded and 200 questionnaires {{were used for}} further synthesis and analyses.|$|R
50|$|Some {{industries}} where {{piece rate}} pay jobs are common are agricultural work, cable installation, call centers, writing, editing, translation, truck driving, <b>data</b> entry, carpet <b>cleaning,</b> craftwork and manufacturing. Working {{for a piece}} rate {{does not mean that}} employers are exempt from paying minimum wage or overtime requirements, which vary among nations and states.|$|R
40|$|This paper studies {{regional}} differences in price and competition in municipal competitive procurement in Sweden. The bidding {{process is a}} first-price, sealed bid auction. The piecewise pseudo-maximum likelihood estimator presented in Donald and Paarsch (1993) is applied to <b>data</b> on <b>cleaning</b> service contracts. The results show higher estimated costs for completing the contract, but lower estimated bids in the major city area of Stockholm compared {{with the rest of}} the country. This is explained by lower profit margins and higher operational costs. An analysis is also carried out of why the lowest bidder is not always the contracted bidder in all auctions. digitalisering@um...|$|R
50|$|Once {{the data}} is cleaned, it can be analyzed. Analysts may apply a variety of {{techniques}} referred to as exploratory data analysis to begin understanding the messages contained in the data. The process of exploration may result in additional <b>data</b> <b>cleaning</b> or additional requests for data, so these activities may be iterative in nature. Descriptive statistics such as the average or median may be generated to help understand the data. Data visualization may also be used to examine the data in graphical format, to obtain additional insight regarding the messages within the data.|$|E
50|$|Geographic {{knowledge}} discovery (GKD) is the human-centered {{process of}} applying efficient computational tools for exploring massive spatial databases. GKD includes geographic data mining, but also encompasses related {{activities such as}} data selection, <b>data</b> <b>cleaning</b> and pre-processing, and interpretation of results. GVis can also serve {{a central role in}} the GKD process. GKD is based on the premise that massive databases contain interesting (valid, novel, useful and understandable) patterns that standard analytical techniques cannot find. GKD can serve as a hypothesis-generating process for spatial analysis, producing tentative patterns and relationships that should be confirmed using spatial analytical techniques.|$|E
50|$|Following the sampling, {{each country}} {{is left with}} a {{representative}} national sample of its public. These persons are then interviewed during a limited time frame decided by the Executive Committee of the World Values Survey using the uniformly structured questionnaires. The survey is carried out by professional organizations using face-to-face interviews or phone interviews for remote areas. Each country has a Principal Investigator (social scientists working in academic institutions) {{who is responsible for}} conducting the survey in accordance with the fixed rules and procedures. During the field work, the agency has to report in writing according to a specific check-list. Internal consistency checks are made between the sampling design and the outcome and rigorous <b>data</b> <b>cleaning</b> procedures are followed at the WVS data archive. No country is included in a wave before full documentation has been delivered. This means a data set with the completed methodological questionnaire. and a report of country-specific information (for example important political events during the fieldwork, problems particular to the country). Once all the surveys are completed, the Principal Investigator has access to all surveys and data.|$|E
40|$|<b>Data</b> {{validation}} and <b>cleaning</b> {{are integral}} {{processes of the}} data qualitymanagement cycle. Domain specific knowledge is needed to detect and correct semantic errors. Ontologies {{can be used to}} represent valid and invalid attribute value combinations to detect and correct invalid data. We introduce reorganization operations formaintaining such an ontology in the data quality management cycle. ...|$|R
50|$|The Category C men's prison has an {{education}} department, and work opportunities for inmates in <b>Data</b> Input, Contract <b>Cleaning,</b> Gardens/Horticulture, and Catering. Other facilities include a gym and prison chaplaincy. The prison complex consists of six buildings. Three {{of these are}} residential treatment units which combine to house 160 men. The remaining three house 67 men each.|$|R
30|$|Concerning the {{structure}} of this article, in Section 2, we review classic sociological concepts for power analysis and summarize Manuel Castells’ theory. Still in the same section, we discuss some works that measure the influence or find the top influential users in social network research. In Section 3, we present the set of metrics and rules for network construction. In Section 4, we describe the process of <b>data</b> gathering, <b>cleaning</b> and analyzing <b>data</b> based on the metrics. In Section 5, we show {{the results of the}} analysis of the Brazilian political scenario. Finally, in Section 6, we highlight the limitations of our work and point out potential future work.|$|R
