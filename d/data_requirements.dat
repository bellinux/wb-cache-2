2292|3519|Public
25|$|For IT purposes, this is {{commonly}} expressed as the minimum application and <b>data</b> <b>requirements</b> {{and the time}} in which the minimum application and application data must be available.|$|E
25|$|An initial LVSR order {{issued on}} 31 May 2006 was valued at $28 million and ordered 22 cargo, two wrecker (recovery), and tractor truck LVSR variants, plus vehicle kits, {{training}} (operator/maintainer-cargo), test support-production verification testing cargo, meetings, and contract <b>data</b> <b>requirements.</b>|$|E
25|$|The results {{indicated}} that users still favoured a traditional field-based census. There are newly emerging and increasing <b>data</b> <b>requirements</b> for local-decision-making. There {{is a need for}} more timely and regular statistics on a wider-range of themes. Accuracy of data and statistics based on the local geography is of higher importance than the frequency of data production. Genealogists are concerned about the potential loss of historical records.|$|E
30|$|Availability of data {{is one of}} {{the most}} {{important}} factors in constructing intelligent systems. There are three elements in <b>data</b> considerations: <b>data</b> <b>requirement</b> and planning, <b>data</b> collection and data applications.|$|R
40|$|The time, space, {{and data}} {{complexity}} of an optimally data efficient isomorphism identification algorithm are presented. The data complexity, {{the amount of}} data required for an inference algorithm to terminate, is analyzed and shown to be the minimum possible for all possible isomorphism inference algorithms. The minimum <b>data</b> <b>requirement</b> is shown to be ⌈log 2 (n) ⌉, and a method for constructing this minimal sequence of data is presented. The average <b>data</b> <b>requirement</b> is shown to be approximately 2 log 2 (n). The time complexity is O(n 2 log 2 (n)) and the space requirement is O(n 2...|$|R
40|$|This {{appendix}} presents {{additional details}} on available registrant-submitted and open literature studies available on chlorothalonil and its major degradate of toxicological concern, SDS- 3701. Studies {{submitted to the}} Agency in support of pesticide registration or re-registration are categorized as either; acceptable, supplemental, or invalid. Acceptable means that all essential information was reported, the data are scientifically valid, and the study was performed according to recommended protocols. Studies in the “acceptable ” category fulfill the corresponding <b>data</b> <b>requirement</b> in 40 CFR Part 158 and are appropriate for use in risk assessment. Supplemental studies are also scientifically valid; however, they were either performed under conditions that deviate from recommended guideline protocols or certain data necessary for complete verification are missing. Supplemental studies may be used quantitatively in the risk assessment and can, at the Agency’s discretion, fulfill the corresponding <b>data</b> <b>requirement</b> in 40 CFR Part 158. Invalid studies are not scientifically valid, or deviate substantially from recommended protocols such {{that they are not}} useful for risk assessment. Invalid studies do not fulfill the corresponding <b>data</b> <b>requirement</b> in 40 CFR Part 158...|$|R
25|$|In 2000, Alex Biryukov, Adi Shamir and David Wagner {{showed that}} A5/1 can be cryptanalysed {{in real time}} using a time-memory {{tradeoff}} attack, based on earlier work by Jovan Golic. One tradeoff allows an attacker to reconstruct the key in one second from two minutes of known plaintext or in several minutes from two seconds of known plain text, but he must first complete an expensive preprocessing stage which requires 248 steps to compute around 300 GB of data. Several tradeoffs between preprocessing, <b>data</b> <b>requirements,</b> attack time and memory complexity are possible.|$|E
25|$|The FQPA {{requires}} that pesticide registration be reviewed periodically, {{with a goal}} of once every 15 years. The FQPA mandated changes {{in the collection of}} tolerance fee’s, increasing the amount of fee money from $14 million to $16 million to help with the reassessment of tolerances. The FQPA requires the EPA to specifically screen pesticides for disruption to the endocrine system. The FQPA requires the EPA to establish an integrated pest management education program and implement integrated pest management research and demonstration. The FQPA encourages the syncing of U.S. pesticide tolerances with those of international standards. Under the FQPA individual states are not allowed to set different pesticide tolerances than the EPA and the EPA is required to coordinate <b>data</b> <b>requirements</b> between the state and federal levels. The EPA is required to develop and distribute a food safety brochure and create an annual report on the progress of its registration program.|$|E
2500|$|Since such a bus {{architecture}} cannot {{keep up with}} the <b>data</b> <b>requirements</b> of the LHC experiments, ...|$|E
30|$|<b>Data</b> <b>Requirement</b> shape - Used for {{requirement}} description {{related to}} data. The system proposes patterns design on {{methods used to}} design databases or writing data to static files. According to the given description, a list of Data related Design Patterns is recommended.|$|R
50|$|Companies use the eOTD {{to create}} <b>data</b> <b>requirement</b> {{specifications}} as Identification Guides (IGs) or cataloging templates. These Identification Guides contain the class-property relationships {{and are used}} for cataloging, to measure data quality {{as well as to}} create requests for data or requests for data validation.|$|R
30|$|We {{therefore}} seek {{a network}} design tool {{with the following}} attributes: i) small <b>data</b> <b>requirement,</b> ii) considers both geographical and geometrical structures of the data, iii) considers local variation, and iv) good predictive performance. We introduce the variance quadtree algorithm {{as a tool for}} solar irradiance monitoring network design.|$|R
2500|$|Initially, {{when the}} primary purpose of a systems {{engineer}} is to comprehend a complex problem, graphic representations of a system are used to communicate a system's functional and <b>data</b> <b>requirements.</b> Common graphical representations include: ...|$|E
5000|$|In United States {{military}} contracts, {{the contract}} <b>data</b> <b>requirements</b> list (CDRL, pronounced SEE-drill) {{is a list}} of authorized <b>data</b> <b>requirements</b> for a specific procurement that forms a part of the contract.|$|E
5000|$|Standardisation of {{documents}} and electronic <b>data</b> <b>requirements</b> ...|$|E
40|$|Non-model-based agroecological {{indicators}} are nowadays proposed as tools with low <b>data</b> <b>requirement</b> for interpreting and integrating information about cropping and farming systems, and {{to draw conclusions}} about their sustainability (Bockstaller et al., 1997). In particular, nutrient budgets identify risks of nutrient accumulation/depletion in soil or nutrient losses to th...|$|R
40|$|In FY 03 QS 10 began {{building}} an S&MA web based data management tool,"Safety & Mission Assurance Requirements Tool" (SMART) that identifies S&MA requirements, tailors requirements to IAW project/program categories, tracks implementation, {{and provides a}} template for developing requirements and tracking waivers. This report provides a SMART process flow, typical application, typical <b>data</b> <b>requirement</b> deliverables, progress in 03, and FY 04 activities...|$|R
40|$|The steady {{increase}} of regulations and its acceleration {{due to the}} financial crisis heavily affect the management of regulatory compliance. Regulations, such as Basel III and Solvency II particularly impact data warehouses and lead to many organizational and technical changes. From an IS perspective modeling techniques for <b>data</b> warehouse <b>requirement</b> elicitation help to manage conceptual requirements. From a legal perspective attempts to visualize regulatory requirements – so called legal visualization approaches – have been developed. This paper investigates whether a conceptual modeling technique for regulatory-driven <b>data</b> warehouse <b>requirements</b> is applicable for representing <b>data</b> warehouse <b>requirements</b> in a legal environment. Applying the modeling technique H 2 for Reporting in three extensive modeling projects provides three contributions. First, evidence for the applicability of a modeling technique for regulatory-driven <b>data</b> warehouse <b>requirements</b> is given. Second, lessons learned for further modeling projects are provided. Third, a discussion towards a combined perspective of information modeling and legal visualization is presented. <br /...|$|R
50|$|The CDRL {{identifies}} what data {{products are}} to be formally delivered to the government by a contractor, as well as when and possibly how (e.g. format and quantity) they {{are to be}} delivered. The list typically consists of a series of individual data items, each of which is recorded on a Data Item form (DD Form 1423) containing the tailored <b>data</b> <b>requirements</b> and delivery information. The CDRL is the standard format for identifying potential <b>data</b> <b>requirements</b> in a solicitation, and deliverable <b>data</b> <b>requirements</b> in a contract. The purpose of the CDRL is to provide a standardized method of clearly and unambiguously delineating the government's minimum essential data needs. The CDRL groups all of the <b>data</b> <b>requirements</b> in a single place rather than having them scattered throughout the solicitation or contract.|$|E
5000|$|<b>Data</b> <b>requirements</b> {{are small}} and no {{calibration}} is necessary 3 ...|$|E
5000|$|The {{conversion}} strategy needs to carefully examine the <b>data</b> <b>requirements</b> ...|$|E
40|$|This {{plan was}} {{generated}} by the SeaSat-A satellite scatterometer experiment team to define the pre-and post-launch activities necessary to conduct sensor validation and geophysical evaluation. Details included are an instrument and experiment description/performance requirements, success criteria, constraints, mission <b>requirements,</b> <b>data</b> processing <b>requirement</b> and <b>data</b> analysis responsibilities...|$|R
30|$|<b>Data</b> {{protection}} <b>requirements</b> {{to be met}} in {{the confidentiality}} and data handling section of the ICF.|$|R
40|$|Due to {{the rapid}} modern advancements in new era, {{development}} of new products became necessary. Whether it is the PLC or the microprocessor that manipulates <b>data,</b> <b>requirement</b> of well managed, highly accurate and effective software€™s are needed to provide the desired results. The first step for the successful software development process is the effective requirement management. As the competition in the software development industry grows day by day {{it is necessary that}} one provides the cost effective and timely solutions. The paper is organized in a way to design a <b>Data</b> Model of <b>requirement</b> management version control mechanism...|$|R
50|$|In December 2010 the ECB the Governing Council of the ECB {{announced}} {{its decision to}} establish loan level <b>data</b> <b>requirements</b> (referred to as the ABS initiative). As of the summer of 2012 the published <b>data</b> <b>requirements</b> {{will have to be}} complied with before a financial institution can pledge ABS as collateral with the ECB.|$|E
5000|$|Each CDRL {{data item}} should be linked {{directly}} to {{statement of work}} (SOW) tasks and managed by the program office data manager. <b>Data</b> <b>requirements</b> can also be identified in the contract via special contract clauses (e.g., DFARS), which define special data provisions such as rights in data, warranty, etc. SOW guidance of MIL-HDBK-245D describes the desired relationship: [...] "Work requirements should be specified in the SOW, and all <b>data</b> <b>requirements</b> for delivery, format, and content {{should be in the}} contract <b>data</b> <b>requirements</b> list in conjunction with the appropriate Data Item Description (DID) respectively, with none of the requirements restated {{in other parts of the}} contract." ...|$|E
5000|$|Data models, i.e. <b>data</b> <b>requirements</b> {{expressed}} as a documented data model of some sort ...|$|E
50|$|CIRT also {{maintains}} a computer centre, {{catering to the}} communication and <b>data</b> storage <b>requirements</b> of the Institute.|$|R
5000|$|Governance: Defining policy driven {{access control}} and {{protection}} for data. Governance includes compliance with <b>data</b> protection <b>requirements.</b>|$|R
5000|$|The ANSI/TIA-942-A {{specification}} references {{private and}} public domain <b>data</b> center <b>requirements</b> for applications and procedures such as: ...|$|R
5000|$|Experience with IDEF1 {{revealed}} that the translation of information requirements into database designs was more difficult than had originally been anticipated. The most beneficial value of the IDEF1 information modeling technique was its ability to represent data independent of how those data were to be stored and used. It provided data modelers and data analysts {{with a way to}} represent <b>data</b> <b>requirements</b> during the requirements-gathering process. This allowed designers to decide which DBMS to use after the nature of the <b>data</b> <b>requirements</b> was understood and thus reduced the [...] "misfit" [...] between <b>data</b> <b>requirements</b> and the capabilities and limitations of the DBMS. The translation of IDEF1 models to database designs, however, proved to be difficult.|$|E
5000|$|The EDRC Project of FIATECH Capturing Equipment <b>Data</b> <b>Requirements</b> Using ISO 15926 and Assessing Conformance. Example {{data and}} videos.|$|E
5000|$|The {{documentation}} of the <b>data</b> <b>requirements</b> and structural business process (activity) rules. In DoDAF V1.5, this was the OV-7.|$|E
40|$|Embedded and {{automated}} tests reduce {{maintenance costs}} for embedded systems installed in remote locations. Testing multiple components of an embedded system, connected on a scan chain, using deterministic test patterns {{stored in a}} system provide high fault coverage but require large system memory. This thesis presents an approach to reduce test <b>data</b> memory <b>requirements</b> {{by the use of}} a test controller program, utilizing the observation of that there are multiple components of the same type in a system. The program use deterministic test patterns specific to every component type, which is stored in system memory, to create fully defined test patterns when needed. By storing deterministic test patterns specific to every component type, the program can use the test patterns for multiple tests and several times within the same test. The program also has the ability to test parts of a system without affecting the normal functional operation {{of the rest of the}} components in the system and without an increase of test <b>data</b> memory <b>requirements.</b> Two experiments were conducted to determine how much test <b>data</b> memory <b>requirements</b> are reduced using the approach presented in this thesis. The results for the experiments show up to 26. 4 % reduction of test <b>data</b> memory <b>requirements</b> for ITC´ 02 SOC test benchmarks and in average 60 % reduction of test <b>data</b> memory <b>requirements</b> for designs generated to gain statistical data...|$|R
3000|$|Output: OVSF {{codes are}} {{assigned}} to all flows based on their delay and average <b>data</b> rate <b>requirements,</b> [...]...|$|R
40|$|The planned {{activities}} {{involved in}} the inflight sensor calibration and performance evaluation are discussed and the supporting software requirements are specified. The possible sensor error sources and their effects on sensor measurements are summarized. The methods by which the inflight sensor performance will be analyzed and the sensor modeling parameters will be calibrated are presented. In addition, a brief discussion on the <b>data</b> <b>requirement</b> for the study is provided...|$|R
