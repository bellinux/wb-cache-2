28|885|Public
40|$|Four pigeons on {{concurrent}} variable <b>interval,</b> <b>variable</b> ratio approximated {{the matching}} relationship with biases toward the variable interval when time spent responding was {{the measure of}} behavior and toward the variable ratio when frequency of pecking was the measure of behavior. The local rates of responding were consistently higher on the variable ratio, even when there was overall preference for the variable interval. Matching on concurrent variable <b>interval,</b> <b>variable</b> ratio was shown to be incompatible with maximization of total reinforcement, given the observed local rates of responding and rates of alternation between the schedules. Furthermore, it was shown that the subjects were losing reinforcements {{at a rate of}} about 60 per hour by matching rather than maximizing...|$|E
30|$|By {{means of}} weight {{functions}} and Hermite–Hadamard’s inequality, and introducing a discrete <b>interval</b> <b>variable,</b> {{a more accurate}} half-discrete Hardy–Hilbert-type inequality related to the kernel of arc tangent function and a best possible constant factor is given, which {{is an extension of}} a published result. The equivalent forms and the operator expressions are also considered.|$|E
30|$|Regarding {{the case}} of half-discrete Hilbert-type inequalities with non-homogeneous kernels, Hardy, Littlewood and Polya {{provided}} some results in Theorem 351 of [1]. However, they had not proved that the constant factors in the new inequalities were best possible. Yang [15] proved some results by introducing an <b>interval</b> <b>variable</b> and that the constant factors are best possible.|$|E
40|$|In {{this paper}} we {{introduce}} {{the use of}} <b>interval</b> <b>variables</b> in classification problems of time series signals. By introducing the concept of interval kernel as a similarity measure among intervals, modifications for some well-known feature selection methods are developed in order to apply these methods to select the most relevant <b>interval</b> <b>variables.</b> A comparison against standard point attributes feature selection (Relief and FSDD) is made for purposes of validation. Peer ReviewedPreprin...|$|R
40|$|Abstract. Many {{scheduling}} problems involve reasoning about tasks {{which may}} or may not actually occur, so called optional tasks. The state-ofthe-art approach to modelling and solving such problems makes use of <b>interval</b> <b>variables</b> which allow a start time of ⊥ indicating the task does not run. In this paper we show we can model <b>interval</b> <b>variables</b> in a lazy clause generation solver, and create explaining propagators for scheduling constraints using these <b>interval</b> <b>variables.</b> Given the success of lazy clause generation on many scheduling problems, this combination appears to give a powerful new solving approach to scheduling problems with optional tasks. We demonstrate the new solving technology on wellstudied flexible job-shop scheduling problems where we are able to close 36 open problems. ...|$|R
40|$|<b>Interval</b> <b>variables</b> can be {{measured}} on very different scales. We first remind a general methodology used for measuring the dispersion of a variable from an optimal center and we define two measures of dispersions associated to two optimal "centers" for <b>interval</b> <b>variables.</b> Then we study {{the relations between the}} standardization of a data table and the use in clustering of a normalized distance. Finally we define two normalized distances between hyper-rectangles and their use in two normalized k-means clustering algorithms...|$|R
30|$|In this paper, {{by means}} of weight {{functions}} and Hermite–Hadamard’s inequality, and introducing a discrete <b>interval</b> <b>variable,</b> a more accurate half-discrete Hardy–Hilbert-type inequality related to the kernel of arc tangent function and a best possible constant factor is given, which {{is an extension of}} a published result mention in Yang and Debnath (2014). The equivalent forms and the operator expressions are considered.|$|E
30|$|This {{paper is}} {{organized}} as follows: in Section  2, we introduce the Euler-Maclaurin expansions for hyper-singular integrals of (2.1) {{at the end}} points of the integrand interval; in Section  3, we present high accuracy quadrature formulas for hyper-singular integrals (1.1) with an <b>interval</b> <b>variable,</b> and also we get their Euler-Maclaurin expansions; in Section  4, some numerical examples are tested. A few conclusions are drawn in Section  5.|$|E
30|$|In 1934, Hardy et al. [1] {{established}} a few {{results on the}} half-discrete Hilbert-type inequalities with the non-homogeneous kernel (see Theorem  351). But they did not prove that the constant factors are the best possible. However, Yang [4] gave a result by introducing an <b>interval</b> <b>variable</b> and proved that the constant factor is the best possible. Recently, Yang et al. [5 – 9] gave some half-discrete Hilbert-type inequalities and their reverses with the monotone kernels and best constant factors.|$|E
30|$|The best {{extension}} of (5) with two <b>interval</b> <b>variables,</b> some equivalent forms, operator expressions, some reverses {{as well as}} a few particular cases are also considered.|$|R
3000|$|... {{which is}} a {{generalization}} of (5). A best extension of (6) with two <b>interval</b> <b>variables,</b> some equivalent forms, the operator expressions, the reverses and some particular cases are considered.|$|R
40|$|An {{iterative}} hybrid structural {{dynamic reliability}} prediction {{model has been}} developed under multiple-time interval loads with and without consideration of stochastic structural strength degradation. Firstly, multiple-time interval loads have been substituted by the equivalent interval load. The equivalent interval load and structural strength are assumed as random variables. For structural reliability problem with random and <b>interval</b> <b>variables,</b> the <b>interval</b> <b>variables</b> {{can be converted to}} uniformly distributed random variables. Secondly, structural reliability with <b>interval</b> and stochastic <b>variables</b> is computed iteratively using the first order second moment method according to the stress-strength interference theory. Finally, the proposed method is verified by three examples which show that the method is practicable, rational and gives accurate prediction...|$|R
30|$|By {{means of}} the {{technique}} of real analysis, weight functions and Hermite–Hadamard’s inequality, and introducing a discrete <b>interval</b> <b>variable</b> and parameters, a more accurate half-discrete Hardy–Hilbert-type inequality related to the kernel of arc tangent function and a best possible constant factor is given. The equivalent forms and the operator expressions are also considered. The method of weight functions is very important, which {{is the key to}} help us proving the main results with the best possible constant factor. The lemmas and theorems provide an extensive account of this type of inequalities.|$|E
40|$|Introduction: The Epworth {{sleepiness}} scale (ESS) {{is widely}} used {{as a way of}} measuring subjective sleep propensity in research and clinical practice. Psychometric studies do not rule out the presence of more than one latent dimension underlying the items. Objective: Aims of the present study were to: (a) evaluate psychometric proprieties of the ESS by means of classic psychometric techniques; (b) compare them with those from a newly developed resistance to sleepiness scale (RSS); (c) evaluate, following the latent trait theory, whether the items of both ESS and RSS could be conceptualized as different levels of an <b>interval</b> <b>variable</b> representative of a single latent trait related to sleep propensity. Methods: One hundred and forty-six inpatients suffering from different sleep disorders filled in both the RSS and ESS in a sleep disorder centre. Results: Indexes of fit derived by the application of the extended logistic model are consistent with the idea that each ESS item can be conceptualized as different levels of an <b>interval</b> <b>variable</b> representative of a single latent trait. However, most of the ESS items are found to be located at the opposite extremes of this continuum. Conclusions: The under representation of situations characterized by an intermediate soporific nature in the ESS could limit ESS sensitivity to detect intermediate variations of sleep propensity. (C) 2003 International Federation of Clinical Neurophysiology. Published by Elsevier Science Ireland Ltd. All rights reserved...|$|E
40|$|Three pigeons {{were exposed}} {{to a series of}} {{multiple}} schedules of reinforcement-both multiple variable interval extinction, and multiple variable <b>interval</b> <b>variable</b> interval. The stimuli which signaled the two components of the multiple schedule were sometimes located on the response key, and sometimes removed from the key. Positive behavioral contrast (increases in variable-interval responding which accompany decreases in extinction responding) was only observed when the signal stimuli were on the response key. These findings are consistent with the view that positive contrast results from the introduction of a differential stimulus-reinforcer dependency which directs behavior toward the stimulus, as in the phenomenon of autoshaping...|$|E
30|$|The assumed {{continuous}} {{dependence of}} the probability density function (joint density function) on the random variable (<b>variables)</b> in an <b>interval</b> random <b>variable</b> (<b>interval</b> random <b>variables)</b> implies that the conditional probability density function is also continuous. This guarantees that the generalization of the conditional density function to the interval setting is always an interval.|$|R
40|$|AbstractIn this paper, {{we develop}} the <b>interval</b> dummy <b>variables.</b> In {{associated}} with the autoregressive conditional interval models with exogenous explanatory <b>interval</b> <b>variables</b> (ACIX), we explore the influences of the sub-prime financial crisis and the European debt crisis on crude oil prices with the proposed <b>interval</b> dummy <b>variables.</b> Hypothesis tests on the <b>interval</b> dummy <b>variables</b> suggest that the European debt crisis has little impact {{on the level of}} crude oil prices, while it reduces the range of crude oil prices. On the other hand, we find that the sub-prime financial crisis has signifi impact on the lower bound of crude oil prices and it increases the volatility of crude oil prices as well. Moreover, the effect of speculation on crude oil prices is not statistically significant in the short run, but is significant in the long run. In addition, our estimation is robust for the choices of Kernel K...|$|R
30|$|By {{using the}} way of weight {{functions}} and the technique of real analysis, a half-discrete Hardy-Hilbert’s inequality with two <b>interval</b> <b>variables</b> is derived. The equivalent forms, operator expressions, some reverses {{as well as a}} few particular cases are obtained.|$|R
40|$|Key words: {{structural}} design; interval analysis; confidence interval; random theory; {{finite element}} method Abstract: The uncertain parameters of the structures are expressed by interval variables, and the governed equations of the structural system are obtained with the {{finite element method}}, then interval analysis is used to model the uncertainty in the static structural analysis and design. After the interval arithmetic integrated with random theory is studied, the confidence interval of structural response is achieved, and the approximate calculation is used with the method of Monte Carlo. Furthermore, by combining the <b>interval</b> <b>variable</b> of the structural response with confidence interval, an analysis method of uncertain structural systems is proposed based on the random model. The presented method can restrain expanding interval, and easy to engineering due to its convenient calculation...|$|E
40|$|Maximization and {{matching}} predictions were examined for a time-based analogue of the concurrent variable-interval variable-ratio schedule. One alternative was a variable interval whose time base operated relatively {{independent of the}} schedule chosen, {{and the other was}} a discontinuous variable interval for which timing progressed only when selected. Pigeons switched between schedules by pecking a changeover key. The maximization hypothesis predicts that subjects will show a bias toward the discontinuous variable interval and undermatching; however the obtained results conformed closely to the predictions of the matching law. Finally, a quantitative comparison was made of the bias and sensitivity estimates obtained in published concurrent variable-interval variable-ratio analogue studies. Results indicated that only the ratio-based analogue of the concurrent variable <b>interval</b> <b>variable</b> ratio studied by Green, Rachlin, and Hanson (1983) produced significant bias toward the variable-ratio alternative and undermatching, as predicted by reinforcement maximization...|$|E
40|$|A new {{computation}} scheme {{proposed to}} tackle commensurate problems is developed by modifying the semi-analytic approach for minimizing computational complexity. Using the proposed scheme, the limit state equations, usually {{referred to as}} the failure surface, are obtained from transformation of an <b>interval</b> <b>variable</b> to a normalized one. In order to minimize the computational cost, two algorithms for optimizing the calculation steps have been proposed. The monotonicity of the objective function can be determined from narrowing the scope of interval variables in normalized infinite space by incorporating the algorithms into the computational scheme. Two examples are used to illustrate the operation and computational efficiency of the approach. The results of these examples show that the proposed algorithms can greatly reduce the computation complexity without sacrificing the computational accuracy. The advantage of the proposed scheme can be even more efficient for analyzing sophistic structures. Department of Industrial and Systems Engineerin...|$|E
30|$|Using the way {{of weight}} {{functions}} and the technique of real analysis, a half-discrete Hilbert-type inequality with a general homogeneous kernel is obtained, and a best extension with two <b>interval</b> <b>variables</b> is given. The equivalent forms, the operator expressions, the reverses and some particular cases are considered.|$|R
40|$|Symbolic Data Analysis can {{be defined}} as the {{extension}} of standard data analysis to more complex data tables. We illustrate the application of the Ascendant Hierarchical Cluster Analysis (AHCA) to a symbolic data set (with a known structure) in the field of the automobile industry (car data set), in which objects are described by variables whose values are intervals of the real data set (<b>interval</b> <b>variables).</b> The AHCA of thirty-three car models, described by eight <b>interval</b> <b>variables</b> (with different scales of measure), was based on the standardized weighted generalized affinity coefficient, by the method of Wald and Wolfowitz. We applied three probabilistic aggregation criteria in the scope of the VL methodology (V for Validity, L for Linkage). Moreover, we compare the achieved results with those obtained by other authors, and with a priori partition int...|$|R
40|$|The {{effect of}} several {{reinforcement}} schedules on {{the variability in}} topography of a pigeon's key-peck response was determined. The measure of topography was {{the location of a}} key peck within a 10 -in. wide by 0. 75 -in. high response key. Food reinforcement was presented from a magazine located below the center of the response key. Variability in response locus decreased to a low value during training in which each response produced reinforcement. Variability increased when fixed <b>intervals,</b> <b>variable</b> <b>intervals,</b> random intervals, or extinction were scheduled...|$|R
40|$|This paper {{presents}} what {{is believed}} to be the very first design and implementation of a deterministic data-parallel constraint language based on interval variables (integer or floating point intervals). The approach used consists of having one constraint and one <b>interval</b> <b>variable</b> per processor. Each processor narrows and intersects the constraint it stores within its memory using interval variables which are stored in different processors. Whenever a solution is found, it is printed and the processor is re-used in implementing the splitting operation. Similarly, a failed narrowing or intersection frees the corresponding processor to be re-used in a subsequent split operation. The algorithm starts with an initial set of processors corresponding to the constraints to be solved and their variables. Narrowing and intersections are applied until convergence. Splitting is done by selecting the variable with the smallest interval and introducing a new set of processors whose data is copied [...] ...|$|E
3000|$|The present {{paper has}} {{formulated}} a piecewise linearized {{version of the}} Generalized MDP Procedure and analyzed its properties. On doing so, I have extended the Fujigaki’s private goods economy to involve a public good, and have localized the intertemporal game à la Champsaur and Laroque ([1981, 1982]), by dividing the time interval and by applying the Generalized MDP Procedure for each interval. In the piecewise intertemporal game associated with any interval generated by our procedure, each player’s payoff is the utility increment at the initial point of each next <b>interval.</b> <b>Variable</b> step-sizes are used to formalize the piecewise linearized procedure that shares similar desirable properties with continuous procedures. This process involves the partitioning of the planning horizon into a specific sequence of time intervals. I have called this process the λ MDP Procedure and shown that it can simultaneously achieve efficiency and piecewise intertemporal strategy proofness. That is, it converges to a Pareto optimum; and the best replay strategy of each player at each date [...]...|$|E
40|$|Rates and {{patterns}} of responding of pigeons under response-independent and response-dependent schedules of brief-stimulus presentation were compared by superimposing 3 -min brief-stimulus schedules on a 15 -min fixed-interval schedule of food presentation. The brief-stimulus schedules were fixed time, fixed <b>interval,</b> <b>variable</b> time, and variable interval. When the brief stimulus was paired with food presentation, its effects depended upon the schedule and ongoing rates. Fixed- and variable-interval brief-stimulus schedules enhanced the low rates normally occurring early in the 15 -min interval, whereas fixed- and variable-time schedules suppressed these rates. Although the overall rates later in the interval were not affected to any great extent, the fixed brief-stimulus schedules generated patterns of positively accelerated responding between stimulus presentations. These patterns appeared less frequently under the variable brief-stimulus schedules. Initially, when not paired with food delivery, presentations of the brief stimulus produced relatively little effect on either response rate or patterning. However, once the stimulus had accompanied food presentation, the original performance under the nonpaired condition was not recovered. The effects were more like those occurring when the stimulus was paired with food...|$|E
30|$|An {{interval}} {{stochastic process}} is an indexed set of <b>interval</b> random <b>variables.</b>|$|R
40|$|This papaer {{considers}} {{the problem of}} interval scale data in {{the most widely used}} models of Data Envelopment Analysis (DEA), the CCR, and the BCC models. Radial models require inputs and outputs measured on the ratio scale. Our focus is {{on how to deal with}} <b>interval</b> scale <b>variables</b> especially when the <b>interval</b> scale <b>variable</b> is a difference of two ratio scale variables like profit or the decrease/increase in bank accounts. Using these ratio scale variables as variables in the DEA model we suggest radial models. An approach to how to deal with <b>interval</b> scale <b>variables</b> when we relax the radiality assumption is also discussed. ...|$|R
30|$|Although decision-tree {{techniques}} are effective for analyzing datasets such as this, the reader {{should be aware}} of certain limitations. For example, since trees use ranks to analyze both ordinal and <b>interval</b> <b>variables,</b> information can be lost. However, the most serious weakness of decision tree analysis is that the results can be unstable because small initial variations can lead to substantially different solutions.|$|R
40|$|This {{study was}} {{developed}} {{referring to the}} negative connotation in using the word “fundamentalism ” and its using as an individual or group label. The method {{used in this study}} were literature review using many literatures about Islam and fundamentalism in any perspectives. Based on psychology perspective, it said that fundamentalism is an individual psychological construct associated with beliefs and individual interpretation of something, such as ideology, nationality, and religion. According to this perspective, this study states religious fundamentalism as an individual psychological construct that chacactherized as an <b>interval</b> <b>variable.</b> Nevertheless, the concept of religious fundamentalism among Muslims in the psychology literatures was conceptualized and constructed without consideration to normative doctrine of Islam. In fact, the concept of fundamentalism was born from the tradition of Christian and still polemical in its definition. By using the simple map of the doctrine of Islam, this study concludes that fundamentalism as a psychological constructs can be divided into two dimensions, namely pathological fundamentalism and non-pathological fundamentalism. This is related to the dimensionality of Islam that contains core dimensions (ushuul) and branch dimentions (furuu) ...|$|E
40|$|New slope {{methods for}} sharper {{interval}} functions {{and a note}} on Fischer's acceleration method JoXo B. OLr~a This paper presents algorithms evaluating sharper txatnds fi~r interval fimcti, ms F(X) : IR ' ~ ~ IR. We revisit two methods that use partial derivatives of the fimctitm, and develop four other induskm methods using the set of slopes S t(::l:, z) of f at z ~ g with respect to some z E IR n, All methtmls can he implemented using urals that automatically ewaluate gradient and slope vectors by using a forward strategy, so the complex management nf reverse accumulation methocLs is awfided. The sharpest methtk-'ls compute each comptment of gradients and slopes separately, by substittltlng each <b>interval</b> <b>variable</b> at a time. Backward methtKis bring no great advantage in the sharpest algorithms, since ob~ct-oriented forward implementatitms are easy and imn|ediate. Fibber's acceleratkm scherne [2] was alu ~ tested with interval variables. This Ineth ~ allows the direct evaluation of the pnmluct f~(:r.) * (x- z) as a single real mnnber (instead of working witl'L. two vectors) and we used i ~ to cmr~pute F~(X) * (X- z) fi~r an interval vector X. We are led to decid...|$|E
40|$|AbstractThe paper {{discusses}} the pilgrimage tourism {{as one of}} the oldest forms of tourism. Although it is often considered as marginal, the pilgrimage tourism is indeed one of the most widespread forms of tourism. Due to the historical context all the necessary prerequisites for the use of its potential are developed in Slovakia. There is a large amount of destinations and sites that are by their nature or historical value of interest for domestic and foreign visitors. Visitors {{from all over the world}} participating in significant pilgrimages bounded to these destinations. However deficiencies in marketing communication, infrastructure and other areas hinder the development of pilgrimage and religious tourism in Slovakia. To determine the current state of marketing communications in pilgrimage tourism questionnaire survey was conducted. In achieving the purpose of the research, two hypotheses were formulated. Because, the relationship between a dependent variable was examined, and the <b>interval</b> <b>variable</b> is one, Single Factor Analysis of Variance ANOVA was used. Homogeneity of variance was verified using Bartlett's and Levene's test. Recommendations for the development of communications strategy are formulated based on the results of the analysis...|$|E
30|$|Since the {{dependent}} variable is a multicategorical variable, considering the common practice in domestic and foreign research, we used the method of multiple logistic regression and used STATA to analyze the data. The ISEI coefficient (divided by ten), years of parents’ education, and numbers of siblings were used as <b>interval</b> <b>variables.</b> The remaining five nominal variables were included the models as dummy variables.|$|R
40|$|A {{parametric}} modelling for interval data is proposed, {{assuming a}} multivariate Normal or Skew-Normal {{distribution for the}} midpoints and log-ranges of the <b>interval</b> <b>variables.</b> The intrinsic nature of the <b>interval</b> <b>variables</b> leads to special structures of the variance–covariance matrix, which is represented by five different possible configurations. Maximum likelihood estimation for both models under all considered configurations is studied. The proposed modelling is then considered {{in the context of}} analysis of variance and multivariate analysis of variance testing. To access the behaviour of the proposed methodology, a simulation study is performed. The results show that, for medium or large sample sizes, tests have good power and their true significance level approaches nominal levels when the constraints assumed for the model are respected; however, for small samples, sizes close to nominal levels cannot be guaranteed. Applications to Chinese meteorological data in three different regions and to credit card usage variables for different card designations, illustrate the proposed methodology...|$|R
40|$|This paper {{considers}} {{the problem of}} interval scale data in {{the most widely used}} models of Data Envelopment Analysis (DEA), the CCR and BCC models. Radial models require inputs and outputs measured on the ratio scale. Our focus is {{on how to deal with}} <b>interval</b> scale <b>variables</b> especially when the <b>interval</b> scale <b>variable</b> is a difference of two ratio scale variables like profit or the decrease/increase in bank accounts. Using these ratio scale variables as variables in a DEA model we suggest radial models. An approach to how to deal with <b>interval</b> scale <b>variables</b> when we relax the radiality assumption is also discussed Keywords: Efficiency Analysis, Data Envelopment Analysis, <b>Interval</b> Scale <b>Variables,</b> Negative Variables Acknowledgments The research was supported, in part, by grants from the Foundation of the Helsinki School of Economics and Business Administration and Academy of Finland. The authors wish to thank Professor Pekka Korhonen, IIASA, for valuable comments. About the Authors [...] ...|$|R
