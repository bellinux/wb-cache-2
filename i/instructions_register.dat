7|458|Public
5000|$|Through {{liveness}} analysis, compilers {{can determine}} which sets of variables are {{live at the}} same time, as well as variables which are involved in [...] instructions. Using this information, the compiler can construct a graph such that every vertex represents a unique variable in the program. Interference edges connect pairs of vertices which are live at the same time, and preference edges connect pairs of vertices which are involved in move <b>instructions.</b> <b>Register</b> allocation can then be reduced {{to the problem of}} K-coloring the resulting graph, where K is the number of registers available on the target architecture. No two vertices sharing an interference edge may be assigned the same color, and vertices sharing a preference edge should be assigned the same color if possible. Some of the vertices may be pre-colored to begin with, representing variables which must be kept in certain registers due to some instructions returning results in specific registers or calling conventions between modules. As graph coloring in general is NP-complete, so is register allocation. However, good algorithms exist which balance performance with quality of compiled code.|$|E
40|$|The rotary {{pipeline}} processor {{is a new}} architecture for superscalar computing. It {{is based on a}} simple and regular pipeline structure which can support several ALUs for efficient dispatching of multiple <b>instructions.</b> <b>Register</b> values flow around a rotary pipeline, constrained by local data dependencies. During normal operation the control circuits are not on the critical path and performance is only limited by data rates. The architecture is particularly well suited to implementation using self-timed logic. ...|$|E
30|$|In the {{analyzing}} stage, analysts try {{to determine}} the location and root cause of captured violations. The analysis is often processed {{with the help of}} debuggers, like GDB, windbg, or other binary analysis tools, like IDA Pro, OllyDbg, etc. Binary instrumentation tools, like Pin (Luk et al. 2005), could also be used to monitor the exact execution state of collected testcases, such as the thread information, <b>instructions,</b> <b>register</b> information and so on. Automatically crash analysis is another important field of research.|$|E
5000|$|SIR: (Scan <b>Instruction</b> <b>Register)</b> Performs an IEEE 1149.1 <b>Instruction</b> <b>Register</b> scan.|$|R
50|$|Clocking {{changes on}} TMS steps through a {{standardized}} JTAG state machine. The JTAG state machine can reset, access an <b>instruction</b> <b>register,</b> or access data {{selected by the}} <b>instruction</b> <b>register.</b>|$|R
30|$|While {{transferring}} {{data from}} any port {{to the internal}} memory banks, no execution of any other instruction is carried out. This control mechanism is achieved when the controller holds the instruction inside the <b>instruction</b> <b>register</b> by re-entering the same instruction to the <b>instruction</b> <b>register</b> {{till the end of}} the transfer process. The same mechanism works for vector instructions, where the controller re-enter the vector instruction to the <b>instruction</b> <b>register</b> till the end of the execution phase.|$|R
40|$|Code {{compression}} is a {{field where}} compression ratios between compiler-generated code and subsequent compressed code are highly dependent on decisions made at compile time. Most optimizations employed by compilers {{tend to focus on}} parameters such as program performance, minimizing resource dependencies and sometimes the option of reducing code size. This paper describes a post-compilation technique for the greedy reassignment of general purpose scratch registers to improve Hamming distance based code compression. The code translation renumbers registers based on the frequency of registers used by isomorphic instructions and employs a Gray coding scheme to reduce Hamming distances between similar <b>instructions.</b> <b>Register</b> reassignment has been successfully implemented in areas where the compiler optimizations do not include a particula...|$|E
40|$|Thanks for {{purchasing}} Roku SoundBridge! We {{hope you}} enjoy {{using it as}} much as we’ve enjoyed creating it. SoundBridge is a network music player that connects to your stereo or powered speakers to play your Mac or PC digital music library in any room of the house. You can even play thousands of free Internet radio stations on SoundBridge without the computer on. SoundBridge works with your Wi-Fi or Ethernet home network and is compatible with popular music servers such as iTunes, Windows Media Connect, Rhapsody and more. Enjoy! Run the Latest Software! We are always working on improving the software for our products. Your SoundBridge will let you know when a software update is available. We recommend you accept new software updates when they are offered. The update just takes two minutes, and enhances the features of your SoundBridge. All you need is an active Internet connection. See “Options for System Configuration ” on page 46 for easy <b>instructions.</b> <b>Register</b> Your SoundBridge! It’s quick, easy and ensures that you are notified of the latest software updates fo...|$|E
40|$|Code {{compression}} is a {{field where}} compression ratios between compiler-generated code and subsequent compressed code are highly dependent on decisions made at compile time. Most optimizations employed by compilers {{tend to focus on}} parameters such as program performance, minimizing resource dependencies and sometimes the option of reducing code size. This paper describes a post-compilation technique for the greedy reassignment of general purpose scratch registers to improve Hamming distance based code compression. The code translation renumbers registers based on the frequency of registers used by isomorphic instructions and employs a Gray coding scheme to reduce Hamming distances between similar <b>instructions.</b> <b>Register</b> reassignment has been successfully implemented in areas where the compiler optimizations do not include a particular metric, for example, power savings. Pro-gram values can be reassigned register numbers that reduce overall power consumption of the address bus and register file decoder, at no cost to code size or performance. The application of the register reassignment technique in this paper reduced the number of dictionary vectors required by a program on average by 9. 74 %. Code compression ratios of register-reassigned binaries were consistently around 3 - 4 % (of original program size) lower than code compression applied to original binaries, with the highest such reduction at nearly 7 %. General purpose register reassignment is a technique that allows for gains in compression efficiency with no penalty in hardware. Other techniques that could be trialed include commutative register switching, dead register detection and assignment and complete register re-allocation...|$|E
50|$|The machine {{instructions}} can {{be grouped}} into six categories: accumulator instructions, branch instructions, memory reference <b>instructions,</b> address <b>register</b> <b>instructions,</b> scratchpad <b>register</b> <b>instruction,</b> miscellaneous instructions (interrupt, input, output, indirect scratchpad register, load, and store).|$|R
5000|$|<b>Instruction</b> <b>register,</b> {{holding the}} <b>instruction</b> {{currently}} being executed.|$|R
50|$|<b>Instruction</b> <b>Register</b> - The <b>instruction</b> <b>register</b> {{holds the}} <b>instruction</b> to be {{executed}} by the computer. This instruction defines {{the type of operation}} to be performed such as add, subtract, etc.; specifies the location address of the operand when necessary and indicates the sector address of the next instruction.|$|R
40|$|As {{computer}} programs become more complex for both embedded and large-scale applications, bloated code size continues {{to become an}} ever increasing problem. It is of particular concern in an embedded environment, where {{the size of the}} final object code greatly effects the space required for memory, which in turn contributes to overall cost. Code compression techniques have been devised as a way of battling large compiler outputs without the need to re-write hand-optimised code. The algorithms in this field require code-specific techniques (as compared with simple file or data compression) to maintain the integrity of the program and ensure its functionality. Code compression is a field where compression ratios (defined as the ratio of compressed code to uncompressed code) between compiler-generated code and subsequent compressed code are highly dependent on decisions made at compile time. Most optimisations employed by compilers tend to focus on parameters such as program performance, efficiency and minimising resource dependencies, though some effort has been made in recent years, to optimise for space as well as power savings. The effects these compiler output optimisations have on the code compression applied subsequently had not been investigated. This thesis presents the results of research into the effects of compiler optimisations on subsequent code compression for VLIW processors. The initial work looks at applying known RISC code compression algorithms to VLIW processors and investigating the effects of various levels of compiler optimisation. These compression schemes include Operand Factorisation applied to instructions, opcode/operand pairs and instruction words; simple dictionary compression with and without compression of single-use instructions; arithmetic coding {{as an example of a}} statistical compression scheme; and dictionary compression applied to single and multipleinstruction codewords. Various decompression methods are also considered based on serial decompression of individual instructions or parallel decompression of instruction fetch packets. It is shown that compression ratios are not a useful indicator of the best code size as the best results for smallest overall code size (after compiler optimisations and code compression) are obtained when the compression schemes are applied to compiler size-optimised code. Furthermore, it is shown that dictionary compression schemes are affected by compiler outputs much more so than statistical compression schemes. Program object code built with no optimisation compressed markedly better under dictionary compression rather than optimised code (a difference of 30 %). However, compression ratios for statistical compression were largely independent of code optimisation. The technique of reordering parallel instructions within a VLIW compiler-issued fetch packet is also investigated, though it was found to only slightly improve compression in unoptimised code and did not affect the code compression when the benchmarks were already optimised for size. Various Vector Hamming Distance code compression techniques are investigated in this thesis, both for their own code size reduction potential, as well as how they are effected by the post-compilation technique of register re-assignment. The Vector Hamming Distance code compression technique is trialed with various dictionary selection methods including frequency, spanning and hybrid selection methods. It is shown that a dictionary selection method which considered both vector frequency as well as maximum spanning achieved better results than just considering either independently. The post-compilation technique for the greedy re-assignment of general purpose scratch registers is the final piece of research work in this thesis. The purpose of the technique is to improve Hamming distance based code compression by renumbering registers based on the register-pair frequency of the registers used by isomorphic instructions and employs a Gray coding scheme to reduce Hamming distances between similar <b>instructions.</b> <b>Register</b> reassignment had previously been successfully implemented in areas where the compiler optimisations do not include a particular metric, for example, power savings. Program values can be reassigned register numbers that reduce overall power consumption of the address bus and register file decoder, at no cost to code size or performance. This technique was shown to reduce the number dictionary entries required by over 9 %...|$|E
5000|$|In computing, an <b>{{instruction}}</b> <b>register</b> (IR) is {{the part}} of a CPU's control unit that holds the instruction currently being executed or decoded. In simple processors each instruction to be executed is loaded into the <b>instruction</b> <b>register</b> which holds it while it is decoded, prepared and ultimately executed, which can take several steps.|$|R
40|$|Base {{register}} with immediate offset [Rn, #+/-] memory_address = Rn +/- offset 12 Rn is unchanged after <b>instruction</b> Base <b>register</b> with register offset [Rn, +/-] memory_address = Rn +/- Rm Rn is unchanged after <b>instruction</b> Base <b>register</b> with shifted register offset [Rn, +/-, #] memory_address = Rn +/- shifted_Rm Rn is unchanged after <b>instruction</b> Base <b>register</b> with immediate offset, pre-indexed [Rn, #+/-]! memory_address = Rn +/- offset 12 Rn = memory_address after <b>instruction</b> Base <b>register</b> with register offset, pre-indexed [Rn, +/-]! memory_address = Rn +/- Rm Rn = memory_address after <b>instruction</b> Base <b>register</b> with shifted register offset, pre-indexed [Rn, +/-, #]! memory_address = Rn +/- shifted_Rm Rn = memory_address after <b>instruction</b> Base <b>register</b> with immediate offset, post-indexed [Rn], #+/- memory_address = Rn Rn = Rn +/- offset 12 after <b>instruction</b> Base <b>register</b> with register offset, post-indexed [Rn], +/- memory_address = Rn Rn = Rn +/- Rm after <b>instruction</b> Base <b>register</b> with shifted register offset, post-indexe...|$|R
5000|$|Example: [...] "IR [...] + 1 → IR [...] " [...] is read in prose: [...] "The {{contents}} of the finite-state machine's <b>Instruction</b> <b>Register</b> plus 1 is 'replaces the (previous) {{contents of}}' the <b>Instruction</b> Counter <b>Register</b> (ICR) [...] ".|$|R
5000|$|A {{fraction}} of a second later, the CPU copies the data from the MDR to the <b>instruction</b> <b>register</b> (IR) ...|$|R
5000|$|HIR: (Header <b>Instruction</b> <b>Register)</b> Specifies a header {{pattern that}} is {{prepended}} {{to the beginning}} of subsequent IR scan operations.|$|R
5000|$|TIR: (Trailer <b>Instruction</b> <b>Register)</b> Specifies {{a trailer}} pattern that is {{appended}} {{to the end}} of subsequent IR scan operations.|$|R
5000|$|Decode the {{instruction}}: During {{this cycle}} the encoded instruction {{present in the}} IR (<b>instruction</b> <b>register)</b> is interpreted by the decoder.|$|R
50|$|The {{control unit}} interprets and {{processes}} all machine functions {{and consists of}} a location counter, the <b>instruction</b> <b>register,</b> and the phase register.|$|R
50|$|In the {{instruction}} cycle, {{the instruction}} is {{loaded into the}} <b>Instruction</b> <b>register</b> after the processor fetches it from the memory location pointed by the program counter.|$|R
40|$|Abstract — We {{present an}} {{efficient}} programmable architecture for compute-intensive embedded applications. The processor architecture uses <b>instruction</b> <b>registers</b> {{to reduce the}} cost of delivering instructions, and a hierarchical and distributed data register organization to deliver data. <b>Instruction</b> <b>registers</b> capture <b>instruction</b> reuse and locality in inexpensive storage structures that are located near to the functional units. The data register organization captures reuse and locality in different levels of the hierarchy {{to reduce the cost}} of delivering data. Exposed communication resources eliminate pipeline registers and control logic, and allow the compiler to schedule efficient instruction and data movement. The architecture keeps a significant fraction of instruction and data bandwidth local to the functional units, which reduces the cost of supplying instructions and data to large numbers of functional units. This architecture achieves an energy efficiency that is 23 × greater than an embedded RISC processor. Index Terms — energy-efficient embedded processor architecture, <b>instruction</b> <b>registers,</b> hierarchical and distributed register organization I...|$|R
40|$|This paper {{describes}} a novel processor architecture, called hyperscalar processor architecture, which encompasses {{the advantages of}} superscalar, VLIW, and vector processor architectures and excludes their disadvantages. In brief hyperscalar is a processor, i) whose instruction size and instruction-fetch bandwidth {{are the same as}} those of superscalar, ii) whose datapath is as large as that of VLIW, iii) which provides every independent functional unit with one or more compiler-visible <b>registers,</b> called <b>instruction</b> <b>registers,</b> and iv) which allows the program itself to load the <b>instruction</b> <b>registers</b> with <b>instructions</b> fetched from the memory and to execute them as a subroutine. As compiler techniques for creating an object code placed in the <b>instruction</b> <b>registers,</b> this paper proposes pseudo vector processing and software pipelining, and further discusses several issues on applying software pipeling to hyperscalar processors. This paper evaluates the performance attainable in hyperscalar processors, and then concludes that hyperscalar processors can outperform conventional superscalar, VLIW, and vector processors in terms of cost/performance...|$|R
50|$|DLX {{instructions}} can {{be broken}} down into three types, R-type, I-type and J-type. R-type <b>instructions</b> are pure <b>register</b> <b>instructions,</b> with three <b>register</b> references contained in the 32-bit word. I-type <b>instructions</b> specify two <b>registers,</b> and use 16 bits to hold an immediate value. Finally J-type instructions are jumps, containing a 26-bit address.|$|R
40|$|Abstract — This paper {{analyzes}} a {{range of}} architectures for efficient delivery of VLIW instructions for embedded media kernels. The analysis takes an efficient Filter Cache as a baseline and examines the benefits from 1) removing the tag overhead, 2) distributing the storage, 3) adding indirection, 4) adding efficient NOP generation, and 5) sharing instruction memory. The result is a hierarchical <b>instruction</b> <b>register</b> organization that provides a 56 % energy and 40 % area savings over an already efficient Filter Cache. Index Terms — energy-efficient embedded processor architecture, hierarchical and distributed <b>instruction</b> <b>register</b> organization, VLIW <b>instruction</b> delivery I...|$|R
50|$|Control UnitControl unit {{contains}} a program counter and <b>instruction</b> <b>registers.</b> It fetches <b>instructions</b> and facilitates program flow. It supports single-operand instruction set {{and works with}} all 16 index registers of the arithmetic unit.|$|R
50|$|Decoding the op-code in the <b>instruction</b> <b>register</b> {{includes}} {{determining the}} instruction, determining where its operands are in memory, retrieving the operands from memory, allocating processor resources {{to execute the}} command (in superscalar processors), etc.|$|R
50|$|Each {{digital signal}} (pin or ball) {{on the package}} is defined, as are the {{registers}} and opcodes used in an IEEE 1149.1, IEEE 1149.6, IEEE 1149.8.1, IEEE 1532 and IEEE 1149.4 compliant IC. There is one <b>instruction</b> <b>register,</b> a minimum of a 1-bit bypass register, one boundary scan register and optionally a 32 bit device_id register. The registers other than the <b>instruction</b> <b>register</b> are called TDRs or Test Data Registers. The boundary scan register (BSR) is unique {{as it is the}} register which is also mapped to the I/O of the device. Many of the BSDL definitions are sets of single long string constants.|$|R
50|$|The {{front panel}} had three rows of red LEDs, {{displaying}} {{the contents of}} the accumulator, <b>instruction</b> <b>register,</b> and program counter (PC). A group of twenty switches and buttons were used to read or modify any selected register.|$|R
2500|$|State register: A special <b>Instruction</b> <b>Register</b> [...] "IR", finite and {{separate}} from the registers above, stores the current instruction to be executed and its address in the TABLE of instructions; this register and its TABLE {{is located in the}} finite state machine.|$|R
2500|$|But {{this does}} {{not solve the problem}} (unless one resorts to Gödel numbers). What is {{necessary}} is a method to fetch the address of a program instruction that lies (far) [...] "beyond/above" [...] the upper bound of the finite state machine's <b>instruction</b> <b>register</b> and TABLE.|$|R
5000|$|<b>Instruction</b> <b>register</b> sizes {{tend to be}} small, perhaps four {{or seven}} bits wide. Except for BYPASS and EXTEST, all {{instruction}} opcodes are defined by the TAP implementor, as are their associated data registers; undefined instruction codes should not be used. Two key instructions are: ...|$|R
5000|$|Addressing modes include Immediate (operand in instruction), Direct or [...] "Symbolic" [...] (operand {{address in}} <b>instruction),</b> <b>Register</b> (operand in {{workspace}} register), Register Indirect (operand address in workspace register) {{with or without}} auto-increment, Indexed (operand address in instruction indexed with workspace register content), and Program Counter Relative.|$|R
5000|$|Fetch the {{instruction}}: The next {{instruction is}} fetched from the memory address {{that is currently}} stored in the program counter (PC), and stored in the <b>instruction</b> <b>register</b> (IR). At {{the end of the}} fetch operation, the PC points to the next instruction that will be read at the next cycle.|$|R
5000|$|The BYPASS instruction, an opcode of all ones {{regardless}} of the TAP's <b>instruction</b> <b>register</b> size, must be supported by all TAPs. The instruction selects a single bit data register (also called BYPASS). The instruction allows this device to be bypassed (do nothing) while other devices in the scan path are exercised.|$|R
