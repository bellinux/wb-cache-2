83|618|Public
50|$|A sensor with an {{adjustable}} emissivity setting {{can also be}} used to calibrate the sensor for a given surface, or to measure the emissivity of a surface. When the temperature of a surface is accurately known (e.g. by measuring with a contact thermometer), then the sensor's emissivity setting can be adjusted until the temperature measurement by the <b>IR</b> <b>method</b> matches the measured temperature by the contact method; the emissivity setting will indicate the emissivity of the surface, which can be taken into account for later measurements.|$|E
50|$|Neumann {{has been}} institutionally active within the {{discipline}} of International Relations. He was {{the editor of the}} Norwegian IR journal Internasjonal politikk from 1993 to 1995, Cooperation and Conflict; Nordic Journal of International Relations from 1999 to 2003 and co-editor of the Routledge book series The New International Relations from 2005 to the present. He has published a string of books on <b>IR,</b> <b>method</b> and Norwegian foreign policy in his native Norwegian, and, from 2010 on, partook in launching the first Master’s programme in IR in his native country, at the Norwegian University of Life Sciences. He spent stints as visiting fellow at the London School of Economics in 1998 and at the University of Queensland in 2011, and was Visiting Professor at the University of Belgrade 2009-2011. Since 2003, Neumann has served as President of the Norwegian Oxford Committee, which awards the Norway Oxford Scholarship.|$|E
40|$|Motion-induced {{misalignment}} between {{transmission and}} emission scans {{can result in}} erroneous estimation of regional tissue activity concentrations. If this motion is of a random nature, mismatch between transmission and emission scans is {{likely to result in}} diminished signal-to-noise ratios. In the case of task-related motion, however, corresponding systematic reconstruction artefacts may lead to false-positive or false-negative results. The {{purpose of the present study}} was to investigate whether implementation of an image registration (<b>IR)</b> <b>method,</b> which allows for motion-corrected attenuation correction, would improve accuracy of H(2) (15) O PET activation studies. To evaluate the performance of this method, phantom studies as well as studies in human subjects were performed. Results were compared with three alternative methods: standard attenuation correction without motion correction, calculated attenuation correction, and no attenuation correction. The phantom measurements showed that, for quantitative assessment of regional activity concentrations, the <b>IR</b> <b>method</b> was superior to the other attenuation correction methods. In a single-subject study with intentional task-related motion during a visual stimulation paradigm, false-positive results, obtained with the standard attenuation correction method, disappeared after attenuation correction using the <b>IR</b> <b>method.</b> Finally, a group analysis of 11 patients indicated that an increase in signal-to-noise ratio was obtained with the <b>IR</b> <b>method.</b> Therefore, in our view, the <b>IR</b> <b>method</b> should be considered as a first choice for attenuation correction in PET activation studie...|$|E
40|$|Abstract. Job {{information}} retrieval (IR) exhibits unique characteristics com-pared to common IR task. First, searching precision on job posting full text is low because job descriptions cannot be properly used in common <b>IR</b> <b>methods.</b> Second, job names semantically {{similar to the}} one mentioned in the searching query cannot be detected by common <b>IR</b> <b>methods.</b> In this paper, job descrip-tions are handled under a two-step job IR framework to find job postings se-mantically similar to seeds job posting retrieved by the common <b>IR</b> <b>methods.</b> Preliminary experiments prove that this method is effective. ...|$|R
40|$|Seven {{to eight}} years ago, the number of {{applications}} of Information Retrieval (<b>IR)</b> <b>methods</b> in Software Engineering was close to zero. These days, IR and text mining methods are accepted approaches to analysis of textual artifacts generated during the software lifcycle. The incentive to try <b>IR</b> <b>methods</b> in such analysis is strong: the field comes {{with a reputation for}} proven industrial and academic success, and some important Software Engineering problems related to textual artifacts, can be translated into an instance of a standard IR problem in a reasonably straightforward manner. In this position paper, we observe that part of the success of IR as a field came from the use of established, well-maintained, and almost universally accepted benchmarks for testing the work of <b>IR</b> <b>methods.</b> We elaborate on the question “Is the field mature enough to talk about benchmarking? ” asked by the working session organizers. Our position is that without robust, well-designed time-tested, and, eventually well-established and accepted benchmarks, research on application of <b>IR</b> <b>methods</b> to problems in Software Engineering will not reach its full potential...|$|R
40|$|Information Retrieval (IR) {{has been}} widely {{accepted}} as a method for automated traceability recovery based on the textual similarity among the software artifacts. However, a notorious difficulty for IR-based methods is that artifacts may be related {{even if they are}} not textually similar. A growing body of work addresses this challenge by combining IR-based methods with structural information from source code. Unfortunately, the accuracy of such methods is highly dependent on the <b>IR</b> <b>methods.</b> If the <b>IR</b> <b>methods</b> perform poorly, the combined approaches may perform even worse. In this paper, we propose to use the feedback provided by the software engineer when classifying candidate links to regulate the effect of using structural information. Specifically, our approach only considers structural information when the traceability links from the <b>IR</b> <b>methods</b> are verified by the software engineer and classified as correct links. An empirical evaluation conducted on three systems suggests that our approach outperforms both a pure IR-based method and a simple approach for combining textual and structural information...|$|R
40|$|A {{computational}} {{technique for}} unconstrained optimal control problems is presented. First an Euler discretization {{is carried out}} to obtain a finite-dimensional approximation of the continous-time (infinite-dimensional) problem. Then an inexact restoration (<b>IR)</b> <b>method</b> due to Birgin and Martínez {{is applied to the}} discretized problem to find an approximate solution. Convergence of the technique to a solution of the continuous-time problem is facilitated by the convergence of the <b>IR</b> <b>method</b> and the convergence of the discrete (approximate) solution as finer subdivisions are taken. It is shown that a special case of the <b>IR</b> <b>method</b> is equivalent to the projected Newton method for equality constrained quadratic optimization problems. The technique is numerically demonstrated by means of a scalar system and the van der Pol system, and comprehensive comparisons are made with the Newton and projected Newton methods...|$|E
40|$|Aeroservoelastic (ASE) {{analytical}} {{models of}} a SensorCraft wind-tunnel model are generated using measured data. The data was acquired during the ASE wind-tunnel {{test of the}} HiLDA (High Lift-to-Drag Active) Wing model, tested in the NASA Langley Transonic Dynamics Tunnel (TDT) in late 2004. Two time-domain system identification techniques are applied {{to the development of}} the ASE analytical models: impulse response (<b>IR)</b> <b>method</b> and the Generalized Predictive Control (GPC) method. Using measured control surface inputs (frequency sweeps) and associated sensor responses, the <b>IR</b> <b>method</b> is used to extract corresponding input/output impulse response pairs. These impulse responses are then transformed into state-space models for use in ASE analyses. Similarly, the GPC method transforms measured random control surface inputs and associated sensor responses into an AutoRegressive with eXogenous input (ARX) model. The ARX model is then used to develop the gust load alleviation (GLA) control law. For the <b>IR</b> <b>method,</b> comparison of measured with simulated responses are presented to investigate the accuracy of the ASE analytical models developed. For the GPC method, comparison of simulated open-loop and closed-loop (GLA) time histories are presented...|$|E
40|$|A new {{adaptive}} mesh refinement {{algorithm is}} proposed for solving Euler discretization of state- and control-constrained optimal control problems. Our approach is designed to reduce the computational effort by applying the inexact restoration (<b>IR)</b> <b>method,</b> a numerical method for nonlinear programming problems, in an innovative way. The initial iterations of our algorithm start with a coarse mesh, which typically involves far fewer discretization points than the fine mesh over which we aim to obtain a solution. The coarse mesh is then refined adaptively, by using the sufficient conditions of convergence of the <b>IR</b> <b>method.</b> The resulting adaptive mesh refinement algorithm is convergent to a fine mesh solution, by virtue of convergence of the <b>IR</b> <b>method.</b> We illustrate the algorithm on a computationally challenging constrained optimal control problem involving a container crane. Numerical experiments demonstrate that significant computational savings {{can be achieved by}} the new adaptive mesh refinement algorithm over the fixed-mesh algorithm. Conceivably owing to the small number of variables at start, the adaptive mesh refinement algorithm appears to be more robust as well, i. e., it can find solutions with a much wider range of initial guesses, compared to the fixed-mesh algorithm. ...|$|E
40|$|Our aim {{here is to}} {{advocate}} for the integration of database-systems (DB) <b>methods</b> and information-retrieval (<b>IR)</b> <b>methods</b> to address applications that are emerging from the ongoing explosion and diversification of digital information. One grand goal of such an endeavor is the automatic building and maintenance of a comprehensive knowledge base of facts from encyclopedic sources and the scientific literature. Facts should be represented in terms of typed entities and relationships and allow expressive queries that return ranked results with precision in an efficient and scalable manner. We thus explore how DB and <b>IR</b> <b>methods</b> might contribute toward this ambitious goal...|$|R
40|$|This article advocates that database-systems (DB) and information-retrieval (<b>IR)</b> <b>methods</b> be {{integrated}} {{to address the}} needs of important applications that emerge with the ongoing explosion and diversification of digital information. A grand challenge that a joint DB&IR endeavor could aim for is to automatically build and maintain a comprehensive knowledge base that encompasses all important facts from encyclopedic sources and the scientific literature. It should represent facts in terms of typed entities and relations, and allow expressive queries that return ranked results with high precision in an efficient and scalable manner. The article presents various approaches and the roles of DB and <b>IR</b> <b>methods</b> towards this ambitious objective...|$|R
30|$|<b>IR</b> <b>methods</b> improve CT {{protocol}} quality, {{providing a}} potential dose reduction {{while maintaining a}} good image detectability. Model observer can in principle be useful to assist human performance in CT low-contrast detection tasks and in dose optimisation.|$|R
40|$|An {{approach}} called Inductive Rotation (IR), {{developed by}} artist Hofstetter Kurt, {{can be used}} to create intricate patterns that fill the 2 D plane from a single prototile by repeated translation and rotation. These patterns are seemingly nonperiodic and have interesting features, both from a mathematical and artistic viewpoint. The <b>IR</b> <b>method</b> has not yet been described in scientific literature. It is related to and has been inspired by aperiodic tilings like the well-known Penrose tilings. During the course of this thesis some research on the patterns generated by Inductive Rotation has been done and algorithms that allow for automatic generation of these patterns have been developed. The implementation is then called the Irrational Image Generator, a tool that on the one hand is a reference implementation of the <b>IR</b> <b>method...</b>|$|E
40|$|This paper {{describes}} {{our system}} developed for Expert Finding task of Enterprise Track for TREC 2005. This system employs 3 methods, traditional <b>IR</b> <b>method,</b> email clustering method and entry page finding method, to find experts {{related to a}} specific topic in W 3 C corpus. Experiment indicates that traditional <b>IR</b> <b>method</b> is useful to expert finding if the query is well generated, email clustering method is helpful when the mail list is relevant to a unique work group or committee, and entry page finding method is valuable while the topic {{is the theme of}} a special group. We use result aggregation methods of linear synthesis to combine the results generated by the three methods,. Of our 5 runs submitted for Expert Finding task, the best run is the one generated by linear synthesis, providing a MAP score of 0. 2174 (Bpref of 0. 4299 and p@ 10 of 0. 3460) Keywords Expert Finding, IR, Email Clustering, Entry Page Finding, Result Aggregation 1...|$|E
40|$|Finding a named {{person in}} {{broadcast}} news video {{is important in}} video retrieval. Relying on the text information such as video transcript and OCR text, this task suffers from the temporal mismatch between a person's visual appearance and the occurrence of his/her name in the text. By exploring video grammar regarding the concurrence pattern between faces and names, we propose an extended text-based <b>IR</b> <b>method</b> to overcome this problem, which yield superior performance. 1...|$|E
40|$|Abstract—Traceability {{recovery}} {{is a key}} software maintenance activity in which software engineers extract the relationships among software artifacts. Information Retrieval (IR) has been widely accepted as a method for automated traceability recovery based on the textual similarity among the software artifacts. However, a notorious difficulty for IR-based methods is that artifacts may be related {{even if they are}} not textually similar. A growing body of work addresses this challenge by combining IR-based methods with structural information from source code. Unfortunately, the accuracy of such methods is highly dependent on the <b>IR</b> <b>methods.</b> If <b>IR</b> <b>methods</b> perform poorly, the combined approaches may perform even worse. In this paper, we propose to use the feedback provided by software engineers when classifying candidate links to regulate the effect of using structural information. Specifically, our approach only considers structural information when the traceability links from the <b>IR</b> <b>methods</b> are verified by developers and classified as correct links. An empirical evaluation conducted on three systems suggests that our approach outperforms both a pure IR-based method and a simple approach for combining textual and structural information. Keywords-Traceability Link Recovery, Empirical studies. I...|$|R
40|$|International audienceExisting similar {{software}} variants, {{developed by}} ad-hoc reuse technique such as left clone-and-own right, represent {{a starting point}} to build a software product line (SPL) core assets. To re-engineer such legacy software variants into an SPL for systematic reuse, {{it is important to}} be able to identify a mapping between features and their implementing source code elements in different variants. Information Retrieval (<b>IR)</b> <b>methods</b> have been used widely to support this mapping in a single software product. This paper proposes a new approach to improve the performance of <b>IR</b> <b>methods</b> when they are applied to a collection of software variants. The novelty of our approach is twofold. On the one hand, it exploits what software variants have in common and how they differ to improve the accuracy of IR results. On the other hand, it reduces the abstraction gap between features and source code by introducing an intermediate level called left code-topic right, for increasing the number of retrieved links that are relevant. We have applied our approach to a collection of seven variants of a large-scale system by using the ArgoUML-SPL modeling tool. The experimental results showed that our approach outperforms conventional application of <b>IR</b> <b>methods</b> as well as the most recent and relevant work on the subject...|$|R
40|$|We {{reproduce}} {{recent research}} results combining semantic and information retrieval methods. Additionally, we expand the existing {{state of the}} art by combining the semantic representations with <b>IR</b> <b>methods</b> from the probabilistic relevance framework. We demonstrate a significant increase in performance, as measured by standard evaluation metrics...|$|R
40|$|The {{conformational}} dynamic {{capabilities of}} the in situ bacteriorhodopsin (bR) can be studied by determination of the changes of the bR net helical segmental tilt angle (the angle between the polypeptide segments and the membrane normal) induced by various perturbations of the purple membrane (PM). The analysis of the far-UV oriented circular dichroism (CD) of the PM provides one means of achieving this. Previous CD {{studies have indicated that}} the tilt angle can change from approximately 10 degrees to 39 degrees depending on the perturbants used with no changes in the secondary structure of the bR. A recent study has indicated that the bleaching-induced tilt angle can be enhanced from approximately 24 degrees to 39 degrees by cross-linkage and papain-digestion perturbations which by themselves do not alter the tilt angle. To add further credence, this study has been repeated using midinfrared (IR) linear dichroic spectral analysis. In contrast to the CD method, analysis by the <b>IR</b> <b>method</b> depends on the orientation of the amide plane of the helix assumed. Excellent consistency is achieved between the two methods only when it is assumed that the structural characteristics of the alpha-helices of the bR are equally alpha I and alpha II in nature. Furthermore, the analysis of the IR data becomes essentially independent of the three amide transitions utilized. The net tilt angle of segments completely randomized relative to the incident light must be 54. 736 in view of helix symmetry. A value of 54. 735 degrees +/- 0. 001 degree was achieved by the <b>IR</b> <b>method</b> for the ethanol-treated PM film, establishing this kind of film as an ideal random state standard and demonstrating the accuracy potential of the <b>IR</b> <b>method...</b>|$|E
40|$|Essential to (cardiac) 3 D {{ultrasound}} are 2 D matrix array transducer {{technology and}} the associated (two-stage) beam forming. Given {{the large number of}} degrees of freedom and the complexity of this problem, simulation tools play an important role. Hereto, the impulse response (<b>IR)</b> <b>method</b> is commonly used. Unfortunately, given the large element count of 2 D array transducers, simulation times become significant jeopardizing the efficacy of the design process. The aim of this study was therefore to derive a new analytical expression to more efficiently calculate the IR in order to speed up the calculation process. To compare accuracy and computation time, the reference and the proposed method were implemented in MATLAB and contrasted. For all points of observation tested, the IR with both methods was identical. The mean calculation time however reduced in average by a factor of 3. 93 ± 0. 03 times. The proposed <b>IR</b> <b>method</b> therefore speeds up the calculation time of the IR of an individual transducer element while remaining perfectly accurate. This new expression will be particularly relevant for 2 D matrix transducer design where computation times remain currently a bottle neck in the design process. status: publishe...|$|E
40|$|This work {{focuses on}} the study of the thermolu-minescence (TL) {{emission}} of a charoite silicate [K 4 NaCa 7 Ba 0. 75 Mn 0. 2 Fe 0. 05 (Si 6 O 15) 2 (Si 2 O 7) Si 4 O 9 (OH). 3 (H 2 O) ] with a very complex lattice structure The activation energy, E, has been calculated for samples of charoite preheated at different tempera-tures (TSTOP) in the range 240 - 340 ºC, by means of (i) initial rise (<b>IR)</b> <b>method</b> and (ii) glow curve fitting (GCF). TL maximum (peaked at 370 ºC) shifts to-wards higher temperatures as TSTOP is increased. The <b>IR</b> <b>method</b> is {{based on the premise that}} occupancies of the relevant states remain almost constant for the lowest temperature side of the TL peak and, conse-quently, this side of the peak will follow an exponen-tial dependence regardless of the kinetic order and the applicability of the quasi-equilibrium approxima-tion, i. e., in the Arrhenius plot (ln ITL vs 1 /T), the activation energy E can be obtained from the slope E/k, irrespectively of any other kinetic parameter. When the <b>IR</b> <b>method</b> is applied to a continuous distribution the obtained activation energy corresponds to an `effective¿ activation energy, Eeff, de-pending on the actual energy distribution of active traps. The experimental TL glow curves can be well fitted assuming an exponential distribution, as it could be expected for the TL glow curves obtained after thermal cleaning of the shallowest trapping levels, i. e. In such case, the effective activation energy is related to parameters in equation by Arrhenius plot of the low-T side of the 300 ºC preheated TL curve of charoite. E values calculated by IR are in good agreement with the E corresponding to the fitted parameters for the exponential distribution. Moreover, dependence of on kT is not significative in the IR range. Changes in the E values when TSTOP increases are consistent with the gradual emptying of trapping levels during preheatings. Peer Reviewe...|$|E
40|$|In {{the recent}} decade that {{research}} in <b>IR</b> <b>methods</b> for Intellectual Property domain has increased. The rst e orts in observing how information retrieval {{is done in}} patent domain were done with the series of Nist workshops (see for example [2]). Lately, more workshops and conferences are dedicated to bringin...|$|R
40|$|Abstract. The Clef-Ip test {{collection}} was rst made available in 2009 to support research in <b>IR</b> <b>methods</b> {{in the intellectual}} property domain. Since then several kinds of tasks, re ecting various speci c parts of patent expert's work ows, have been organized. We give here {{an overview of the}} tasks, topics, assessments and evaluations of the Clef-Ip 2012 lab. ...|$|R
40|$|Effective Document Image Retrieval (DIR) {{requires}} {{the use of}} appropriate Information Retrieval (<b>IR)</b> <b>methods.</b> Our research indicates that some IR techniques found to be effective for electronic text retrieval do not transfer to DIR in a simple predictable manner, and that modifications must be made to these methods to enable them to work effectively for DIR...|$|R
40|$|Abstract. Concept location, {{the problem}} of associating human {{oriented}} concepts with their counterpart solution domain concepts, is a fundamental problem {{that lies at the}} heart of software comprehension. Recent research has attempted to alleviate the impact of the concept location problem through the application of methods drawn from the Information Retrieval (IR) community. Here we present a new approach based on a complimentary <b>IR</b> <b>method</b> which also has a sound basis in cognitive theory. We compare our approach to related work through an experiment and present our conclusions [...] ...|$|E
40|$|A comparitive {{study has}} been carried out on the {{analysis}} of mineral oil in sediment and water samples by gas chromatography (GC) and infrared spectrometry (IR) according to the current standard methods NEN 5733 and 6675. In an application to sediment samples, results obtained with GC analysis were found to be about 20 % higher compared to the <b>IR</b> <b>method</b> (range + 6 to + 32 %). Causes for these differences were not found. Additionally, modifications to improve the performance of the NEN-procedures were investigated. Changes comprised (1) repeated removal of co-extracted dissolved organic carbon (DOC); (2) the use of pentaine instead of Freon- 113 as extraction solvent and (3) alkaline aqueous washing (back-extraction) of extracts to remove small to medium size acidic co-extractants. However, results obtained were not essentially different from those obtained with the standard method. Based on that, the following conclusions were drawn: (1) the present standard procedures can be considered as reliable methods for the analysis of mineral oil in water en sediment samples, (2) the performance of the GC and <b>IR</b> <b>method</b> is comparable in applications above the determination limit of the latter (0. 2 -. 5 mg/kg); (3) improvement of the current standards cannot be achieved with relative simple means; (4) pentane is an acceptable alternative for Freon- 113 and other halogenated hydrocarbons in case these solvents are banned for analytical use...|$|E
40|$|International audienceMicroaggregation by {{individual}} ranking (IR) {{is an important}} technique for masking confidential econometric data. While being a successful method for controlling the disclosure risk of observations, IR also affects the results of statistical analyses. We conduct a theoretical analysis on the estimation of arbitrary moments from a data set that has been anonymized {{by means of the}} <b>IR</b> <b>method.</b> We show that classical moment estimators remain both consistent and asymptotically normal under weak assumptions. This theory provides the justification for applying standard statistical estimation techniques to the anonymized data without having to correct for a possible bias caused by anonymization...|$|E
40|$|Two new ceramides, N-(4 -hydroxyphenethyl) octacosamide (1) and rel-(2 S, 3 S, 4 R, 16 E) - 2 -[(2 'R) - 2 '-hydroxynonadecanoylamino]-heneicosadec- 16 -ene- 1, 3, 4 -triol (2) were {{isolated}} from the EtOH extract of Acnistus arborescens. The structures were elucidated by spectroscopic (1 D and 2 D NMR experiments, HR-ESI-MS, LR-MS and <b>IR)</b> <b>methods...</b>|$|R
50|$|ESSIR aims {{to give a}} {{deep and}} {{authoritative}} insight of the core <b>IR</b> <b>methods</b> and subjects along these three dimensions and also for this reason it is intended for researchers starting out in IR, for industrialists who wish {{to know more about}} this increasingly important topic and for people working on topics related to management of information on the Internet.|$|R
40|$|Our {{study is}} {{dedicated}} {{to the development of the}} methodology for an accurate characterization of the support and the inks of the Dead Sea Scrolls. To that aim we use optical and electron microscopy, micro-XRF, 3 D-SY-XRF, different <b>IR</b> <b>methods</b> including synchrotron radiation based reflectance spectroscopy. Simulation experiments to identify different water sources and binding agents in the carbon inks are presented...|$|R
40|$|Analysis of {{moisture}} availability, determined from GOES IR surface temperature measurements and a boundary layer model, and antecedent precipitation are presented for data taken over Texas and Argentina. Correlations between antecedent precipitation and moisture availability are high where the spatial variation in moisture availability {{is much larger}} than an inherent uncertainty in the method, which is judged to be about + or - 0. 2 for the moisture availability parameter. In regions where there are large gradients of precipitation, the <b>IR</b> <b>method</b> can provide useful information on variations in soil moisture availability and, by implication, in precipitation...|$|E
40|$|Background: The {{purpose of}} this study was to assess the {{feasibility}} of infrared (IR) spectroscopy for the simultaneous quantification of serum LDL-cholesterol (LDL-C) and HDL-cholesterol (HDL-C) concentrations. Methods: Serum samples (n 90) were obtained. Du-plicate aliquots (5 L) of the serum specimens were dried onto IR-transparent barium fluoride substrates, and transmission IR spectra were measured for the dry films. In parallel, the HDL-C and LDL-C concentrations were determined separately for each specimen by stan-dard methods (the Friedewald formula for LDL-C and an automated homogeneous HDL-C assay). The pro-posed <b>IR</b> <b>method</b> was then developed with a partial least-squares (PLS) regression analysis to quantitatively correlate IR spectral features with the clinical analytica...|$|E
40|$|Abstract. In {{contrast}} to conventional documents, a Web document con-sists {{of a number}} of tags which provide hints on the structure of the docu-ments. In this paper, we propose a Web-document retrieval method using the characteristics of HTML tags. This method learns the importance of tags from a training text set. We use a genetic algorithm for learning the importance weights. We also present a modied similarity measure which uses the tag information. Experiments have been performed on the TREC document collection consisting of 247, 491 documents. Compared to the traditional <b>IR</b> <b>method,</b> the proposed method has achieved 15 % improvement in average precision. ...|$|E
40|$|Abstract. TF*IDF-based methods, as {{they are}} easily to implement, are widely {{accepted}} in information retrieval industry. It is interesting to investigate a feasible and practical technique to improve the retrieval performance of these conventional <b>IR</b> <b>methods.</b> In this paper, {{we are going to}} introduce a good alternative approach that uses passage-based ranking as the second stage of the retrieval process in them. 1...|$|R
40|$|Ice {{water path}} (IWP) {{determination}} is often complicated because of cloud overlap. Current satellite IWP retrievals are usually {{based on the}} assumption that all clouds are single-layered, despite the relatively frequent occurrence of overlapped cloud systems. Although IWP can be inferred from retrievals of cloud optical depth and effective ice particle sizes using visible (VIS) and infrared (<b>IR)</b> <b>methods</b> (Minni...|$|R
40|$|Abstract—In {{the last}} few decades, image {{registration}} (IR) has been established as a very active research area in computer vision. Over the years, it {{has been applied to}} a broad range of real-world problems ranging from remote sensing to medical imaging, artificial vision, and computer-aided design. IR has been usually tackled by iterative approaches considering numerical optimization methods which are likely to get stuck in local optima. Recently, a large number of <b>IR</b> <b>methods</b> based on the use of metaheuristics and evolutionary computation paradigms has been proposed providing outstanding results. In this contribution, we aim to develop a preliminary experimental study on some of the most recognized feature-based <b>IR</b> <b>methods</b> considering evolutionary algorithms. To do so, the IR framework is first presented and a brief description of some prominent evolutionary-based IR proposals are reviewed. Finally, a selection {{of some of the most}} representative methods are benchmarked facing challenging 3 D medical image registration problem instances. I...|$|R
