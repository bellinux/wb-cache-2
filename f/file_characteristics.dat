19|106|Public
5000|$|Enhanced wildcards and {{the ability}} to filter by file sizes, date and time stamps, and other <b>file</b> <b>characteristics</b> ...|$|E
5000|$|Apple ISO 9660 Extensions adds {{support for}} classic Mac OS-specific and macOS-specific <b>file</b> <b>characteristics</b> such as {{resource}} forks, file backup date and more.|$|E
5000|$|Several Microsoft {{operating}} systems provided {{a set of}} modifiable <b>file</b> <b>characteristics</b> that could be accessed and changed through a low-level system call. For example, as of release 4.0, the first six bits of the file attribute byte indicated {{whether or not a}} file was read-only (as opposed to writeable), hidden, a system file, a volume label, a subdirectory, or if the file had been [...] "archived" [...] (with the bit being set if the file had changed since the last use of the BACKUP command). However, initial releases of the operating system did not provide user-level method for reading or changing these values.|$|E
30|$|Unix-like, text-based {{configuration}} <b>files</b> to setup <b>characteristics</b> {{and behavior}} of the nodes.|$|R
40|$|This {{research}} {{explored the}} efficacy of the Volunteer Return Preparation Program (VRPP) for low-income taxpayers. VRPP facilitates free tax return preparation assistance to low-income taxpayers through partnering organizations in local communities across the United States. The project consisted of the following objectives: 1) to examine the demographic and tax <b>filing</b> <b>characteristics</b> of taxpayers that use VRPP and how they compare to a random sample of non-users, 2) to identify the extent to which VRPP users and non-users claimed select tax credits {{and the extent to which}} the utilization is explained by profile characteristics, and 3) to assess neighborhood level factors that influence VRPP participation. By focusing on a government program that is structured to provide assistance to individuals that are economically disadvantaged, this research sheds light on {{the efficacy of}} the program, while explaining participation...|$|R
40|$|The FGGE/ERBZ tape {{contains}} 5 parameters {{which are}} extracted and reformatted from the Nimbus- 7 ERB Zonal Means Tape. There are {{three types of}} files on a FGGE/ERBZ tape: a tape header file, and data <b>files.</b> Physical <b>characteristics,</b> gross format, and file specifications are given. A sample tape check/document printout (shipping letter) is included...|$|R
40|$|A {{description}} of the machine readable catalog, including detailed format and tape <b>file</b> <b>characteristics,</b> is given. The machine file is a computation of mean values for position and magnitude at a mean epoch of observation for each unique star in the Oxford, Paris, Bordeaux, Toulouse and Northern Hemisphere Algiers zone. The format was changed to effect more efficient data searching by position and additional duplicate entries were removed. The final catalog contains data for 997311 stars...|$|E
40|$|The modern {{operating}} systems must integrate various Internet services, especially World-Wide Web facilities to access Web resources using file systems mechanisms. In this paper {{we present a}} high-level model describing an abstract distributed file system. The proposed description is based on Resource Description Framework (RDF) recommendation of the World-Wide Web Consortium, a standardized foundation for processing metadata. To represent the RDF statements about various <b>file</b> <b>characteristics,</b> we propose an XML-based language called Extensible File Properties Markup Language...|$|E
40|$|The actual modern {{operating}} systems must incorporate {{a variety of}} Internet services, especially World-Wide Web facilities to access distributed resources using file systems mechanisms. In this paper we present a high-level model describing a general distributed file system. The proposed description is based on Resource Description Framework (RDF) recommendation of the World-Wide Web Consortium – a general purpose XML-based technology that enables the semantic description of resources on the Web. To represent the RDF statements about various <b>file</b> <b>characteristics,</b> an XML-based language...|$|E
5000|$|To {{optimize}} the storage {{and reduce the}} I/O overhead for the very common case of attributes with very small associated value, NTFS prefers to place the value within the attribute itself (if {{the size of the}} attribute does not then exceed the maximum size of an MFT record), instead of using the MFT record space to list clusters containing the data; in that case, the attribute will not store the data directly but will just store an allocation map (in the form of data runs) pointing to the actual data stored elsewhere on the volume. When the value can be accessed directly from within the attribute, it is called [...] "resident data" [...] (by computer forensics workers). The amount of data that fits is highly dependent on the <b>file's</b> <b>characteristics,</b> but 700 to 800 bytes is common in single-stream files with non-lengthy filenames and no ACLs.|$|R
40|$|Scientific {{codes are}} usually parallelised by {{partitioning}} a grid among processors. Irregular grids (see figure 1) must be partitioned carefully {{in order to}} balance the workload. Solution methods for time-varying prob-lems often change the grid as the computation progresses; furthermore, the grid structure is not generally manifest until load-time, when the code reads the problem from a <b>file.</b> These <b>characteristics</b> make paral...|$|R
40|$|This paper {{studies the}} <b>file</b> caching <b>characteristics</b> of the industry-standard webserving {{benchmark}} SPECweb 99 and develops an optimal cost model for balancing performance and disk and RAM costs, where &quot;cost &quot; {{can be very}} broadly defined. The model is applied to a realistic 32 -bit address hardware configuration to demonstrate a solution that eliminates file accesses as a potential webserving bottleneck for very high workload levels...|$|R
40|$|Cataloging Internet Resources {{came out}} of a project that OCLC {{initiated}} with support from the U. S. Department of Education to have libraries volunteer to identify, select and catalog Internet resources using the USMARC format and AACR 2 cataloging rules. Changes from the first to the second edition include adopting the ISBD(ER) expanded list of terms {{to be used in the}} <b>File</b> <b>characteristics</b> area, including examples that include a broader variety of types of resources, and noting whether MARC fields are repeatable or non-repeatable. The examples in the book follow OCLC MARC rather than USMARC. The guidelines have been expanded in some cases to 2 ̆ 2 include CONSER and ISBD(ER) modifications to practice. 2 ̆...|$|E
40|$|Abstract. In this paper, the {{research}} focus on NC process information integration methods based on database and process information extraction from macro files of heterogeneous commercial CAM platforms. First, analyzed the macro <b>file</b> <b>characteristics</b> of studied CAM system to build heterogeneous systems variables mapping table and establish standard macro file for each studied CAM systems; Second, designed macro file self-learning mechanism based on {{characteristics of the}} macro file, extracted implicit process information in macro files and saved to build General process Database; Finally, used the General process Database and the standard macro files to generate target macro files to achieve automatic programming. The process Information integration method in this research improved the efficiency and processing quality of NC machining, and made excellent process experience well inheritance and reuse...|$|E
40|$|As part of {{its work}} to explore {{emerging}} issues associated with characterisation of digital materials, Planets has explored vocabularies and information structures for expressing the properties integral {{to the value of}} digital art. Value encompasses those qualities that must be understood and captured {{in order to ensure that}} art works’ sensory, emotional, mental and spiritual resonance remain. Facets of interactivity, modularity and temporality associated with digital art present some critical questions that the preservation community must increasingly be equipped to answer. Because digital art materials exhibit fundamental multidimensionality, validating the successful preservation of creative experience demands the explication of more than just <b>file</b> <b>characteristics.</b> Understanding relationships between objects also implies an understanding of their respective functional qualities. This paper presents a Planets’ vocabulary for encapsulating contextual and implicit characteristics of digital art, optimised for preservation planning and validation...|$|E
5000|$|The Full-Year Population <b>Characteristics</b> <b>files</b> (released {{annually}} in May) - A skeleton {{version of the}} person-level consolidated file, which gets released {{six months before the}} Consolidated file of the same year but does not include any income or medical expenditure variables.|$|R
40|$|Abstract—This paper explores {{a hidden}} {{communication}} channel using a game save file that {{is created by}} the game Prison Architect. The file gets modified by a proof of concept tool developed to encode/decode messages within the <b>file.</b> The <b>characteristics</b> (such as throughput, detection, mechanism, robustness, etc.) of this channel are outlined. A discussion of whether to call this channel a covert channel or a type of steganography follows by outlining some definitions in other publications and proposing a different definition...|$|R
40|$|AbstractIn power systems, {{extraordinary}} {{amounts of}} data collected from smart grid application systems need to be stored efficiently and effectively in real time. A new data storage method was proposed {{to deal with the}} huge data storage problem in embedded Linux operation system. The new method combined the advantages of Linux kernel system customization feature and operation system <b>file</b> management <b>characteristics</b> to fully utilize the hardware resources. The simulation results demonstrated that the proposed method can improve the real-time database system's storage and query performance...|$|R
40|$|This paper {{extends the}} {{analysis}} of Extendible Hashing to cover Partial Expansions with elastic buckets. Although previous studies of elastic buckets {{can be adapted to}} extendible hashing, the approach taken here provides another view to the problem. We provide a correspondence between fixed bucket capacities and elastic buckets. Furthermore, the results are based on easy and straightforward approximations. Keywords: Analysis of algorithms, data management, data and file structures, hashing. 1 Introduction Extendible hashing (EH) was introduced and analyzed by Fagin et al. [3] and has been further studied by Mendelson [8], Yao [12] and Flajolet [4]. Their analysis concentrated on the asymptotic behavior for the different EH and Trie <b>file</b> <b>characteristics,</b> but followed the usual fixed bucket capacity for the leaf pages. This paper extends {{the analysis of}} EH found in [8] to cover Partial Expansions (PE) with elastic buckets [7]. Partial expansions were proposed and studied by Lomet [7] [...] ...|$|E
40|$|This paper {{describes}} a simulation model {{which is used}} as a tool for designing and evaluating literature searching systems. The simulation program creates a well specified collection of documents and analyzes the effect of changes in query <b>file</b> <b>characteristics</b> on system output performance. First a thesaurus of term relations is generated. Then, employing the thesaurus, routines generate pseudo-documents and pseudo-queries. These pseudo-documents and pseudo-queries are then compared to see the effect of various query file parameter changes on the quantity of material retrieved. Evaluation of the simulation output indicates that there are small differences between the results of the experimental runs. It is concluded that one method for generating pseudo-queries is not deafly better than another. It is believed, however, that the simulation model as an approach to the evaluation of retrieval systems provides a limited but useful framework for the evaluation of information retrieval systems...|$|E
40|$|Let AxB be {{the product}} space of two sets A and B which {{is divided into}} a (pairs {{representing}} the same entity) and nonmatches (pairs representing different entities). Linkage rules are those that divide AxB into links (designated matches), possible links (pairs for which we delay a decision), and nonlinks (designated nonmatches). Under fixed bounds on the error rates, Fellegi and Sunter (1969) provided a linkage rule that is optimal {{in the sense that}} it minimizes the set of possible links. The optimality is dependent on knowledge of certain joint inclusion probabilities that are used in a crucial likelihood ratio. In applying the record linkage model, assumptions are often made that allow estimation of weights that are a function of the joint inclusion probabilities. If the assumptions are not met, then the linkage procedure using estimates computed under the assumptions may not be optimal. This paper describes a method for estimating weights using the EM Algorithm under less restrictive assumptions. The weight computation automatically incorporates a Bayesian adjustment based on <b>file</b> <b>characteristics...</b>|$|E
40|$|Abstract. As {{the number}} of nodes in cluster systems {{continues}} to grow, leveraging scalable algorithms {{in all aspects of}} such systems becomes key to maintaining performance. While scalable algorithms have been applied successfully in some areas of parallel I/O, many operations are still performed in an uncoordinated manner. In this work we consider, in three file system scenarios, the possibilities for applying scalable algorithms to the many operations that make up the MPI-IO interface. From this evaluation we extract a set of <b>file</b> system <b>characteristics</b> that aid in developing scalable MPI-IO implementations. ...|$|R
40|$|The {{emerging}} widespread {{adoption of}} the Digital Imaging Communications in Medicine (DICOM) standard will increase the demand for radiologic image transfer between radiologic image acquisition, archive, display and printing devices. Unfortunately, there are {{and will continue to}} be many devices that do not and will not support this standard, especially older radiologic equipment and devices from nonradiologic vendors. Determining the image <b>file</b> format <b>characteristics</b> of images from such equipment is often difficult, and done on an ad hoc basis. We have developed a software tool that assists users in determining the image file format parameters of unknown radiologic images...|$|R
50|$|Text {{entered into}} a search bar by the user is {{compared}} to the search engine's database. Matching results are accompanied by {{a brief description of}} the audio <b>file</b> and its <b>characteristics</b> such as sample frequency, bit rate, type of file, length, duration, or coding type. The user is given the option of downloading the resulting files.|$|R
40|$|CRUSH is a data {{compression}} utility {{that provides the}} user with several lossless compression techniques available in a single application. It is intended that the future development of CRUSH will depend upon feedback from the user community to identify new features and capabilities desired by the users. CRUSH provides an extension to the UNIX Compress program and the various VMS implementations of Compress that many users are familiar with. An important capability added by CRUSH is the addition of additional compression techniques and the option of automatically determining the best technique for a given data file. The CRUSH software is written in C and is designed to run on both VMS and UNIX systems. VMS files that are compressed will regain their full <b>file</b> <b>characteristics</b> upon decompression. To the extent possible, compressed files can be transferred between VMS and UNIX systems, and thus be decompressed on a different system than they were compressed on. Version 1 of CRUSH is currently available. This version is a VAX VMS implementation. Version 2, which has {{the full range of}} capabilities for both VMS and UNIX implementations, will be available shortly...|$|E
40|$|P. {{study was}} {{conducted}} by an electronic bulletin board system (BBS) team to explore e variety of approaches to electronic communication between the GOvernment Printing Office's (GPO) Library Programs Service and the depository libraries. After reviewing various dissemination technologies, the BBS team recramended that an electronic bulletin board system become the Federal Depository Library Program's (FDLP) telecommunications vehicle cl choice. It also recommended that the FDLP BBS be established by using existing Project Hermes hardware and software, a telecommunications vehicle currently used in the electronic dissemination of Supreme Court slip opinions. This report provides an executive summary, {{a discussion of the}} study methodology and study recommendations, descriptions of the FDLP and of the general characteristics of bulletin board systems, and a profile of the FDLP BBS, including its <b>file</b> <b>characteristics</b> and size, usage characteristics, staffing, telecommunications, system configuration options and costa, and an implementation schedule. Concluding the report are a 12 -item bibliography, the Depository Library Council recommendations, the names and addresses of the study consultants, suggestions for the BBS file content, and a iiscussion of alternative dissemination technologies, i. e., online services, electronic mail, fax technology, and direct media dissemination. (NAB) * Reproductions supplied by EDRS are the best that can te made * * from the original document. * cs...|$|E
40|$|Distributed file {{systems have}} been {{extensively}} studied in the past, {{but they are still}} far from wide acceptance over heterogeneous network environments. Most traditional network file systems target the tight-couple highspeed networks only, and do not work well in the wide-area setting. Several communication optimization techniques are proposed in the context of wide-area file systems, but these approaches do not take into consideration the <b>file</b> <b>characteristics</b> and may instead introduce extra computing overhead when the network condition is good. We envision that the capability of providing adaptive, seamless file access to personal documents across diverse network connections {{plays an important role in}} the success of future distributed file systems. In this paper, we propose to build an adaptive distributed file system which provides the “ClosE and Go, Open and Resume ” (Cegor) semantics across heterogeneous network connections, ranging from high-bandwidth local area network to low-bandwidth dial-up connection. Our approach relies on a set of new techniques for managing adaptive access to remote files, including three components: system support for secure, transparent reconnection at different places, semantic-view based caching to reduce communication frequencies in the system, and type-specific communication optimization to minimize the bandwidth requirement of synchronizations between clients and servers. 1...|$|E
50|$|According to {{reviews and}} {{benchmarks}} {{of the available}} filesystems for Linux, JFS is fast and reliable, with consistently good performance under different kinds of load, contrary to other filesystems that seem to perform better under particular usage patterns, for instance with small or large <b>files.</b> Another <b>characteristic</b> often mentioned, {{is that it is}} light and efficient with available system resources and even heavy disk activity is realized with low CPU usage. Especially for databases which need synchronous writes to survive a hardware crash, JFS with external journal seems to be the best option.File fragmentation on JFS impairs filesystem performance less than on more traditional Linux ext3 filesystems.|$|R
50|$|TorChat is a {{decentralized}} anonymous instant messenger {{that uses}} Tor hidden services as its underlying network. It provides cryptographically secure text messaging and <b>file</b> transfers. The <b>characteristics</b> of Tor's hidden services {{ensure that all}} traffic between the clients is encrypted {{and that it is}} very difficult to tell who is communicating with whom and where a given client is physically located.|$|R
40|$|Knowledge of how cesarean {{birth rates}} vary by {{hospital}} characteristics may aid in understanding and perhaps modifying {{some of the}} structural and process components of newborn delivery services to decrease the necessity of birth by cesarean procedure. To examine the influence of select hospital characteristics, data on hospital newborn deliveries in Illinois for 1986 among women 10 - 50 years of age inclusive (N = 130, 249) were obtained from computerized hospital discharge abstract <b>files.</b> <b>Characteristics</b> of the hospitals {{were obtained from the}} annual American Hospital Association survey. Adjusting for mother's age at delivery; presence of pregnancy, labor, and delivery complications; expected primary payer; and size of hospital, women delivering in hospitals with teaching status were less likely (odds ratio = 0. 76, p less than. 001, 95 percent CL: 0. 73, 0. 79) to have a primary cesarean birth than women delivering in hospitals without this designation. A significantly lower cesarean birth rate in teaching hospitals was also observed in women of all age groups, in Medicaid and non-Medicaid women, and for most categories of delivery complications. These data suggest the need to identify the programmatic, technologic, and manpower functions associated with hospital teaching status that could decrease the likelihood of a primary cesarean delivery. The study also suggests that changes aimed at the manner of diagnosis, monitoring, and/or management of pregnancy/delivery complications may reduce the cesarean birth rate because of large differences in the primary cesarean birth rate found between teaching and other hospitals for most categories of newborn delivery complications...|$|R
40|$|As {{collections}} become {{larger in}} size, more complex in structure and increasingly diverse in composition, new approaches {{are needed to}} help curators assess digital files and make decisions about their long-term preservation. We present research {{on the use of}} interactive visualization to analyze file characterization information for the purpose of assessing the preservation condition of a vast collection of complex electronic records. The case study collection contains over 1, 000, 000 files of diverse formats arranged in varied record structures and record groups. The visualization application uses tree maps and a relational database management system (RDBMS) to represent the collection’s arrangement and to show available characterization information at different levels of aggregation, classification and abstraction. Through this visualization interface curators can interact dynamically with the collections ’ characterization information to discover trends, as well as compare and contrast various <b>file</b> <b>characteristics</b> across the collection. Curators may select and weight the variables that they want to analyze. They can pursue analysis workflows that go from a high-level overview of the collection’s preservation condition based on file format risks, to obtaining more detailed results about the condition of record groups and individual records. While there are various digital preservation planning tools available, to our knowledge none have been designed specifically to visually present assessment information across vast and complex collections. We present research to address the need for such a tool. 1...|$|E
40|$|The Goddard Space Flight Center (GSFC) Version 0 (V 0) Distributed Active Archive Center (DAAC) {{has been}} {{developed}} to support existing and pre Earth Observing System (EOS) Earth science datasets, facilitate the scientific research, and test Earth Observing System Data and Information System (EOSDIS) concepts. To ensure that no data is ever lost, each product received at GSFC DAAC is archived on two different media (VHS and Digital Linear Tape (DLT)). The first copy is made on VHS tape and is {{under the control of}} UniTree. The second and third copies are made to DLT and VHS media under a custom built software package named "Archer". While Archer provides only a subset of the functions available with commercial software like UniTree, it supports migration between near-line and off-line media and offers much greater performance and flexibility to satisfy the specific needs of a Data Center. Archer is specifically designed to maximize total system throughput, rather than focusing on the turn-around time for individual files. The Commercial Off the Shelf Software (COTS) Hierarchical Storage Management (HSM) products evaluated were mainly concerned with transparent, interactive, file access to the end-user, rather than as a batch-oriented, optimizable (based on known data <b>file</b> <b>characteristics)</b> data archive and retrieval system. This is critical to the distribution requirements of the GSFC DAAC where orders for 5000 or more files at a time are received. Archer has the ability to queue many thousands of file requests and to sort these requests into internal processing schedules that optimize overall throughput. Specifically, mount and dismount, tape load and unload cycles, and tape motion are minimized. This feature {{did not seem to be}} available in many COTS packages. Archer also [...] ...|$|E
40|$|Organizing the {{information}} that we call personal files such as files in a home directory, web pages found on the Internet, images, emails for later revisiting is currently required by many computer users. Several information retrieval models are proposed to fullfil this requirement. Each model is characterized by the types of personal files, their organization and the searching method used. Traditional file systems let a user organize his files into a directory tree and retrieve them later by browsing the directory tree. Desktop seach tools such as Google Desktop or Beagle automatically index file properties and file content (keywords) to provide the user with file retrieval by querying on file properties or on file content. Semantic file systems propose a searching method that combines querying with browsing to give to the users the advantages of both searching methods. For informations on the Internet, tagging systems are more and more used to facilitate the re-finding of these Internet ressources. Among personal <b>file</b> <b>characteristics</b> (properties, content, context) exploited by the above retrieval models, the working context of the user has been proved to be relevant to help a user to successfully retrieve his personal files. This work proposes a model for personal file retrieval, called context-based model for personal file retrieval. Our model allows a user to associate his personal files with a working context using tags. To retrieve a personal file, the user must describe the working context corresponding to each file. The searching method in our model gives to the users the advantages of both searching methods: browsing and querying. We develop our model by improving traditional tagging models. Based on tag relationships and popularities, we organize tags created by a user into a directed acyclic graph (DAGoT). This DAGoT is used as the basic data model to realize our context-based model for personal file retrieval. We use this graph to recognize working contexts associated to personal files, guide a user to reconstruct his working contexts, refine the searching requests, and retrieve personal files by context. EVRY-BU (912282101) / SudocEVRY-INT (912282302) / SudocSudocFranceF...|$|E
30|$|In our framework, we {{employed}} Unix-like text based configuration <b>files</b> {{to configure}} <b>characteristics</b> {{and behavior of}} each node in the simulation. In the configuration files, mechanisms are provided to set {{the role of the}} node as either OBU or RSU, or to define the number of RSU, and their unique IDs. A script is also provided for generating configuration files automatically for large deployments of VANETs.|$|R
40|$|Microarray data {{is subject}} to noise and {{systematic}} variation that negatively affects the resolution of copy number analysis. We describe Rawcopy, an R package for processing of Affymetrix CytoScan HD, CytoScan 750 k and SNP 6. 0 microarray raw intensities (CEL <b>files).</b> Noise <b>characteristics</b> {{of a large number}} of reference samples are used to estimate log ratio and B-allele frequency for total and allele-specific copy number analysis. Rawcopy achieves better signal-to-noise ratio and higher proportion of validated alterations than commonly used free and proprietary alternatives. In addition, Rawcopy visualizes each microarray sample for assessment of technical quality, patient identity and genome-wide absolute copy number states. Software and instructions are available at [URL]...|$|R
40|$|AbstractThe data in {{this paper}} {{are related to the}} {{research}} article entitled “Taxonomic status and phylogenetic relationship of tits based on mitogenomes and nuclear segments” (X. J. Li et al., 2016) [1]. The mitochondrial genomes and nuclear segments of tits were sequenced to analyze mitochondrial characteristics and phylogeny. In the data, the analyzed results are presented. The data holds the resulting <b>files</b> of mitochondrial <b>characteristics,</b> heterogeneity, best schemes, and trees...|$|R
