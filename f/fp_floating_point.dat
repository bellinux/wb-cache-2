1|5053|Public
40|$|The key {{to protect}} {{huge amount of}} {{multimedia}} data in ubiquitous networks is to introduce safety aware high-performed single VLSI processor sys-tems embedded with cipher process. Thus, we ex-ploited the architecture of a hardware cryptography-embedded multimedia mobile processor named HC-gorilla by sophisticatedly unifying up-to-date proces-sor techniques. Although it was provided with care-fully selected Java bytecodes and cipher codes, <b>FP</b> (<b>floating</b> <b>point)</b> expression was omitted due to the restriction of hardware resource. Considering recent trend of embedded applications like voice recognition, 3 D graphics, and image/vision processing, FP hard-ware is crucial for further enhancing HCgorillafs Java functions. We focus in this article {{the development of a}} compact FPU (Floating point number Processing Unit). A compact FP format specific for HCgorilla is IEEE 754 compatible except the bit width represen-tation of FP data. Prioritizing the latency of FPU, it has only 5 stages. The compact FPU is built in HCgorilla by adding 16 FP arithmetic codes and im-proving the decode stage of the previous HCgorilla. By using a 0. 18 -µm standard cell CMOS technology supported by VDEC, we have so far accomplished the logic synthesis and behavior simulation. The 400 MHz of clock frequency is justified from delay analysis...|$|E
30|$|Thus in a {{conventional}} butterfly, to compute output X, two <b>floating</b> <b>point</b> adders/subtractors are required. To compute output Y, four <b>floating</b> <b>point</b> adders/subtractors and four <b>floating</b> <b>point</b> multipliers are required. These <b>floating</b> <b>point</b> multipliers consume more dynamic power and occupy more area.|$|R
40|$|This paper aims on {{the design}} of a {{reversible}} fused 32 -Point Radix- 2 single precision <b>floating</b> <b>point</b> FFT unit using 3 : 2 compressor. The work focuses on the realization of three reversible fused <b>floating</b> <b>point</b> units: reversible <b>floating</b> <b>point</b> add-sub unit, reversible <b>floating</b> <b>point</b> multiply-add unit and reversible <b>floating</b> <b>point</b> multiply-subtract unit. The proposed work requires the design of a reversible single precision <b>floating</b> <b>point</b> adder, a reversible single precision <b>floating</b> <b>point</b> subtractor and a reversible single precision <b>floating</b> <b>point</b> multiplier. A reversible single precision <b>floating</b> <b>point</b> adder and subtractor is designed with less quantum cost, less number of reversible gates and less constant inputs. A reversible single precision <b>floating</b> <b>point</b> multiplier is implemented using 3 : 2 compressor as the 24 x 24 bit multiplier based on 3 : 2 compressor is highly efficient when compared with the design using 4 : 3 compressors. A reversible fused 32 -Point Radix- 2 <b>floating</b> <b>point</b> FFT Unit using 3 : 2 compressor consumes less number of resources, operates at a slightly greater speed and dissipates less power when compared with the reversible discrete 32 -Point Radix- 2 <b>floating</b> <b>point</b> FFT Unit. The proposed Fused 32 -Point Radix- 2 <b>floating</b> <b>point</b> FFT unit using 3 : 2 compressor dissipates 2. 074 W while the same design as a discrete implementation dissipates 2. 176 W...|$|R
30|$|The <b>floating</b> <b>point</b> pixel {{calculation}} module {{performs the}} algorithm outlined in Figure[*] 1 and writes the resulting high-resolution pixel {{back to the}} portion of the M 4 k memory block reserved for the final high-resolution frame. This module mainly consists of <b>floating</b> <b>point</b> multiplication and <b>floating</b> <b>point</b> addition modules for the resizing stages, <b>floating</b> <b>point</b> subtraction and <b>floating</b> <b>point</b> addition modules for the comparison stage to generate the error frame, and <b>floating</b> <b>point</b> subtraction modules for the hypothesis adjustment stage. Note that we used a combined <b>floating</b> <b>point</b> addition and subtraction unit, in which a single bit was used to designate the module as either a subtract operation or an add operation. All of the <b>floating</b> <b>point</b> operations are single precision (32 -bit) <b>floating</b> <b>point</b> operations. The shared add and subtract unit requires seven clock cycles to complete, and the multiplication unit requires five clock cycles to complete.|$|R
50|$|Above {{method of}} {{representing}} the <b>floating</b> <b>point</b> numbers is complex method of representing the <b>floating</b> <b>point</b> numbers, but with it we can represent {{wide range of}} <b>floating</b> <b>points</b> numbers by this method. As well as it is convenient to manipulate the <b>floating</b> <b>point</b> numbers by this method. It is difficult for a human being to convert <b>floating</b> <b>point</b> (real numbers in mathematical language) number into the format {{in which they are}} stored into the memory. As well as it is difficult to convert back <b>floating</b> <b>point,</b> looking its hex or binary value, into the real number. Due to the conversion problem fixed <b>floating</b> <b>point</b> numbers are used. Fixed <b>floating</b> <b>point</b> numbers could be easily converted by the human into its memory representation and from memory representation into the actual number.|$|R
40|$|We {{will study}} {{arbitrary}} precision integers, rationals, fixed precision <b>floating</b> <b>point</b> numbers, and arbitrary precision <b>floating</b> <b>point</b> numbers. In later lectures, we will learn about algebraic expressions and general algebraic numbers. We {{start out with}} a short discussion of arbitrary precision integers and rationals. The bulk of the lecture will be about <b>floating</b> <b>point</b> numbers. <b>Floating</b> <b>point</b> numbers are of the form s · m · 2 e where s is a sign bit (− 1 or + 1), m is a non-negative number called mantissa and e is an integer called exponent. The number of digits available for the mantissa is either fixed (all hardware <b>floating</b> <b>point</b> systems) or arbitrary (most software <b>floating</b> <b>point</b> systems). The exponent either comes from a fixed range (hardware <b>floating</b> <b>point</b> numbers and some software <b>floating</b> <b>point</b> systems) or is arbitrary (some software <b>floating</b> <b>point</b> systems). Already the first programmable computer offered <b>floating</b> <b>point</b> numbers. In 1938, Konrad Zuse completed the ”Z 1 ”, the first programmable computer. It worked with 22 -bit floating-point numbers having a 7 -bit exponent, 15 -bit significant (including one implicit bit), and a sign bit. The Z 3, completed in 1941, implemented <b>floating</b> <b>point</b> arithmetic exceptions with representations for plus and minus infinity and undefined. The first commercial computers offering <b>floating</b> <b>point</b> arithmetic in hardware are Zuse’s Z...|$|R
5000|$|<b>Floating</b> <b>point</b> performance: 6.2 GFLOPS (single {{precision}} 32-bit <b>floating</b> <b>point)</b> ...|$|R
50|$|This gives 440400B16, which when {{converted}} {{back to a}} <b>floating</b> <b>point</b> number (by dividing {{again by}} 216, but holding the result as <b>floating</b> <b>point)</b> gives 6.71999.The correct <b>floating</b> <b>point</b> result is 6.72.|$|R
25|$|All <b>floating</b> <b>point</b> {{operations}} are largely implemented in microcode across {{the family and}} thus and are fairly slow. On the V60/V70 the 32-bit <b>floating</b> <b>point</b> operations took 120/116/137 cycles for addition/multiplication/division, while the corresponding 64-bit <b>floating</b> <b>point</b> operations took 178/270/590 cycles. The V80 had some limited hardware assist for parts of the <b>floating</b> <b>point</b> operations, e.g. decomposition into sign, exponent and mantissa, thus its <b>floating</b> <b>point</b> unit was claimed up to 3 times {{as effective as the}} one of the V70, with 32-bit operations taking 36/44/74 cycles while 64-bit <b>floating</b> <b>point</b> operations taking 75/110/533 cycles on the V80 (again, for addition/multiplication/division).|$|R
5000|$|<b>Floating</b> <b>point,</b> general {{discussion}} of accuracy issues in <b>floating</b> <b>point</b> arithmetic ...|$|R
5000|$|... 1998 Display system having <b>floating</b> <b>point</b> {{rasterization}} and <b>floating</b> <b>point</b> [...].|$|R
40|$|With gate counts {{approaching}} {{ten million}} gates, FPGAs are quickly becoming suitable for major <b>floating</b> <b>point</b> computations. However, to date, few comprehensive tools {{to allow for}} <b>floating</b> <b>point</b> unit tradeoffs have been developed. Most commercial or academic <b>floating</b> <b>point</b> libraries provide {{only a small number}} of <b>floating</b> <b>point</b> modules with fixed parameters of bit-width, area, and speed. With this limitation, user designs must be modified to meet the available units. The balance between FPGA <b>floating</b> <b>point</b> unit resource...|$|R
40|$|<b>Floating</b> <b>point</b> units form an {{important}} component of many reconfigurable computing applications. The creation of <b>floating</b> <b>point</b> units under a collection of area, latency, and throughput constraints is {{an important}} consideration for system designers. Given the range of possible tradeoffs, most commercial or academic <b>floating</b> <b>point</b> libraries for FPGAs provide a small fraction of possible <b>floating</b> <b>point</b> units. In contrast, the floating unit generation approach outlined in this paper allows for the creation of more than 200 different <b>floating</b> <b>point</b> units, with differing area, throughput, and latency characteristics. These variations are supported through selection of a <b>floating</b> <b>point</b> architecture and the use of <b>floating</b> <b>point</b> unit pipelining. Each of these <b>floating</b> <b>point</b> units can be generated with a variable number of bits for the mantissa and the exponent. Given requirements on throughput, area and latency, our generation flow automatically chooses the proper algorithm and architecture to create a <b>floating</b> <b>point</b> unit which fulfills design requirements. Our approach is fully integrated into standard C++ using ASC, a stream compiler for FPGAs, and the underlying PAM-Blox II module generation environment [13]. The <b>floating</b> <b>point</b> units created by our approach are competitive in size and performance with ones created by commercial vendors...|$|R
40|$|Most {{commercial}} and academic <b>floating</b> <b>point</b> libraries for FPGAs provide {{only a small}} fraction of all possible <b>floating</b> <b>point</b> units. In contrast, the <b>floating</b> <b>point</b> unit generation approach outlined in this paper allows for the creation of a vast collection of <b>floating</b> <b>point</b> units with differing throughput, latency, and area characteristics. Given performance requirements, our generation tool automatically chooses the proper implementation algorithm and architecture to create a compliant <b>floating</b> <b>point</b> unit. Our approach is fully integrated into standard C++ using ASC, a stream compiler for FPGAs, and the PAM-Blox II module generation environment. The <b>floating</b> <b>point</b> units created by our approach exhibit a factor of two latency improvement versus commercial FPGA <b>floating</b> <b>point</b> units, while consuming only half of the FPGA logic area...|$|R
50|$|SSE <b>floating</b> <b>point</b> SIMD. Four {{single-precision}} <b>floating</b> <b>point</b> numbers per clock cycle.|$|R
30|$|MICAz motes do {{not support}} <b>floating</b> <b>point</b> {{operations}} by hardware, and they are needed by the algorithm, which uses matrices of <b>floating</b> <b>point</b> data. <b>Floating</b> <b>point</b> variables are, then, managed by software emulation when the application is compiled, but passing <b>floating</b> <b>point</b> parameters by reference to functions are not supported; we need this feature to compute matrix operations.|$|R
40|$|A {{system for}} the rapid {{prototyping}} of <b>floating</b> <b>point</b> hardware designs is presented. This system, called Float, consists of a <b>floating</b> <b>point</b> class for the simulation of quantization effects associated with low precision <b>floating</b> <b>point</b> operators; an optimizer which can automatically determine the minimal number of exponent and fraction bits required for a specified degree of accuracy; and a parameterized <b>floating</b> <b>point</b> library which can generate <b>floating</b> <b>point</b> operators with arbitrary precision. A digital sine-cosine generator is used as an example. ...|$|R
50|$|Group 6 is <b>floating</b> <b>point</b> {{instructions}} (if a <b>floating</b> <b>point</b> unit is installed).|$|R
50|$|Misc - <b>floating</b> <b>point,</b> vector, signal processing, cache locking, decimal <b>floating</b> <b>point,</b> etc.|$|R
50|$|The floating-point {{arithmetic}} feature provides 4 64-bit <b>floating</b> <b>point</b> registers {{and instructions}} {{to operate on}} 32 and 64 bit hexadecimal <b>floating</b> <b>point</b> numbers. The 360/85 and 360/195 also support 128 bit extended precision <b>floating</b> <b>point</b> numbers.|$|R
5000|$|... {{enhanced}} <b>floating</b> <b>point</b> - IBM offered extended-precision 128-bit quadruple-precision <b>floating</b> <b>point</b> on the 360/85 ...|$|R
40|$|Data {{structures}} such as *BMDs, HDDs, and K*BMDs provide compact representations for functions which map Boolean vectors into integer values, but not <b>floating</b> <b>point</b> values. In this paper, {{we propose}} a new data structure, called Multiplicative Power Hybrid Decision Diagrams (*PHDDs), to provide a compact representation for functions that map Boolean vectors into integer or <b>floating</b> <b>point</b> values. The size of the graph to represent the IEEE <b>floating</b> <b>point</b> encoding is linear with the word size. The complexity of <b>floating</b> <b>point</b> multiplication grows linearly with the word size. The complexity of <b>floating</b> <b>point</b> addition grows exponentially {{with the size of}} the exponent part, but linearly with the size of the mantissa part. We applied *PHDDs to verify integer multipliers and <b>floating</b> <b>point</b> multipliers before the rounding stage, based on a hierarchical verification approach. For integer multipliers, our results are at least 6 times faster than *BMDs. Previous attempts at verifying <b>floating</b> <b>point</b> multipliers required manual intervention. We verified <b>floating</b> <b>point</b> multipliers before the rounding stage automatically...|$|R
40|$|<b>Floating</b> <b>point</b> {{numbers are}} used in many {{applications}} that  would be well suited to a higher parallelism than that offered in a CPU. In  these cases, an FPGA, with its ability to handle multiple calculations  simultaneously, could be the solution. Unfortunately, floating point  operations which are implemented in an FPGA is often resource intensive,  which means that many developers avoid <b>floating</b> <b>point</b> solutions in FPGAs or  using FPGAs for <b>floating</b> <b>point</b> applications. Here the potential to get less expensive <b>floating</b> <b>point</b> operations by using ahigher radix for the <b>floating</b> <b>point</b> numbers and using and expand the existingDSP block in the FPGA is investigated. One of the goals is that the FPGAshould be usable for both the users that have <b>floating</b> <b>point</b> in their designsand those who do not. In order to motivate hard <b>floating</b> <b>point</b> blocks in theFPGA, these must not consume {{too much of the}} limited resources. This work shows that the <b>floating</b> <b>point</b> addition will become smaller withthe use of the higher radix, while the multiplication becomes smaller by usingthe hardware of the DSP block. When both operations are examined at the sametime, it turns out {{that it is possible to}} get a reduced area, compared toseparate <b>floating</b> <b>point</b> units, by utilizing both the DSP block and higherradix for the <b>floating</b> <b>point</b> numbers...|$|R
50|$|The Linaro Linux {{project had}} a Linux x11 {{software}} <b>floating</b> <b>point</b> GPU driver available, but all current efforts with ARM Linux {{seem to be}} utilizing the hardware <b>floating</b> <b>point</b> libraries. The soft/hard <b>floating</b> <b>point</b> systems are not compatible.|$|R
50|$|Note that 64-bit <b>floating</b> <b>point</b> {{operations}} consumes {{both the}} first two execution columns. This implies that an SM can issue up to 32 single-precision (32-bit) <b>floating</b> <b>point</b> operations or 16 double-precision (64-bit) <b>floating</b> <b>point</b> operations at a time.|$|R
50|$|The CFPA (CVAX <b>Floating</b> <b>Point</b> Accelerator) is a <b>floating</b> <b>point</b> {{coprocessor}} for the CVAX 78034.|$|R
40|$|<b>Floating</b> <b>point</b> {{arithmetic}} {{is widely}} used in many areas, especially scientific computation and signal processing. The main applications of <b>floating</b> <b>points</b> today are {{in the field of}} medical imaging, biometrics, motion capture and audio applications. The IEEE <b>floating</b> <b>point</b> standard defines both single precision and double precision formats. Multiplication is a core operation in many signal processing computations, and as such efficient implementation of <b>floating</b> <b>point</b> multipliers is an important concern. Until now there is the implementation of the low precision <b>floating</b> <b>point</b> formats, but this piece of work considers the implementation of 64 -bit double precision multiplier. This paper presents the FPGA implementation of double precision <b>floating</b> <b>point</b> multiplier using Xilinx Coregen Tool...|$|R
5000|$|Color depth supported: 8 bits integer, 16 bits integer, 16 bits <b>floating</b> <b>point,</b> 32 bits <b>floating</b> <b>point</b> ...|$|R
5000|$|... full <b>floating</b> <b>point</b> {{operations}} on single- or double-precision operands, selected by single/double bit in <b>Floating</b> <b>Point</b> Status Register ...|$|R
40|$|This work {{describes}} {{the effect of}} architectural/system level design decisions on the performance, of <b>floating</b> <b>point</b> arithmetic units. By modeling with VHDL and using design synthesis techniques, different architectures of <b>floating</b> <b>point</b> adders, multipliers and multiply-accumulate fused units, are compared using different technologies and cell libraries. Some modifications to recent published works have been proposed to minimize the energy delay product with special emphasis on power reduction. A new low power, high performance, transition activity scaled, double data path <b>floating</b> <b>point</b> multiplier has been proposed and its validity is proved by comparing it to a single data path <b>floating</b> <b>point</b> multiplier. A transition activity scaled, triple data path <b>floating</b> <b>point</b> adder has been compared with a high speed, single data path <b>floating</b> <b>point</b> adder using an optimized Leading Zero Anticipatory logic. Three different architectures of <b>floating</b> <b>point</b> multiply-accumulate fused units are evaluated for their desirability for high speed, low power and minimum area. The findings of this work validate different higher level design methodologies of <b>floating</b> <b>point</b> arithmetic units irrespective of the rapidly changing underneath technology...|$|R
40|$|Abstract—We {{identify}} a timing channel in the <b>floating</b> <b>point</b> instructions of modern x 86 processors: the running time of <b>floating</b> <b>point</b> addition and multiplication instructions can vary by two {{orders of magnitude}} depending on their operands. We develop a benchmark measuring the timing variability of <b>floating</b> <b>point</b> operations and report on its results. We use <b>floating</b> <b>point</b> data timing variability to demonstrate practi-cal attacks on {{the security of the}} Firefox browser (versions 23 through 27) and the Fuzz differentially private database. Finally, we initiate the study of mitigations to <b>floating</b> <b>point</b> data timing channels with libfixedtimefixedpoint, a new fixed-point, constant-time math library. Modern <b>floating</b> <b>point</b> standards and implementations are sophisticated, complex, and subtle, a fact that has not been sufficiently recognized by the security community. More work is needed to assess the implications of the use of <b>floating</b> <b>point</b> instructions in security-relevant software. I...|$|R
40|$|An {{algorithm}} is described for multiplying multiprecision <b>floating</b> <b>point</b> numbers. The returned result {{is equal to}} the <b>floating</b> <b>point</b> number obtained by rounding the exact product. Software implementations of multiprecision <b>floating</b> <b>point</b> multiplication can reduce the computing time by a factor of two if they do not compute the low order digits of the product of the two mantissas. However, these algorithms do not necessarily provide exactly rounded results. The algorithm described in this paper is guaranteed to produce exactly rounded results and typically obtains the same savings. 1 Introduction We present an algorithm for multiplying multiprecision <b>floating</b> <b>point</b> numbers. The returned result {{is equal to the}} <b>floating</b> <b>point</b> number obtained by rounding the exact product. A rounding operation which satisfies this requirement is called exact rounding. Exact rounding provides a well defined, implementation independent semantics for <b>floating</b> <b>point</b> arithmetic. For this reason, <b>floating</b> <b>point</b> [...] ...|$|R
40|$|We {{address the}} single source {{shortest}} path planning problem (SSSP) {{in the case}} of <b>floating</b> <b>point</b> edge weights. We show how any integer based Dijkstra solution that relies on a monotone integer priority queue to create a full ordering over path lengths in order to solve integer SSSP can be used as an oracle to solve <b>floating</b> <b>point</b> SSSP with positive edge weights (<b>floating</b> <b>point</b> P-SSSP). <b>Floating</b> <b>point</b> P-SSSP is of particular interest to the robotics community. This immediately yields a handful of faster runtimes for <b>floating</b> <b>point</b> P-SSSP; for example, O(m + nC/δ), where C is the largest weight and δ is the minimum edge weight in the graph. It also ensures that many future advances for integer SSSP will be transferable to <b>floating</b> <b>point</b> P-SSSP...|$|R
5000|$|... {{images in}} 8, 16 or 32 bit integer, 32 or 64 bit <b>floating</b> <b>point</b> and <b>floating</b> <b>point</b> complex data ...|$|R
50|$|<b>Floating</b> <b>point</b> numbers: Holders for <b>floating</b> <b>point</b> {{numerical}} values are typically either {{a word or}} a multiple of a word.|$|R
40|$|BMDs, HDDs, and K*BMDs provide compact {{representations}} for functions which map Boolean vectors into integer values, but not <b>floating</b> <b>point</b> values. In this paper, {{we propose}} a new data structure, called Multiplicative Power Binary Hybrid Diagrams (*PBHDs), {{to provide a}} compact representation for functions that map Boolean vectors into integer or <b>floating</b> <b>point</b> values. The size of the graph to represent the IEEE <b>floating</b> <b>point</b> encoding is linear with the word size. The complexity of <b>floating</b> <b>point</b> multiplication grows linearly with the word size. The complexity of <b>floating</b> <b>point</b> addition grows exponentially {{with the size of}} the exponent part, but linearly with the size of the mantissa part. We applied *PBHDs to verify integer multipliers and <b>floating</b> <b>point</b> multipliers before the rounding stage, based on a hierarchical verification approach. For integer multipliers, our results are at least 6 times faster than *BMD's. Previous attempts at verifying <b>floating</b> <b>point</b> multipliers required man [...] ...|$|R
