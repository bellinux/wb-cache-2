5|10000|Public
40|$|Analog-to-information {{converters}} and Compressed Sampling (CS) sensor front-ends try to only {{extract the}} relevant, information-bearing elements of an incoming <b>data</b> <b>stream.</b> <b>Information</b> extraction and recognition tasks can run {{directly on the}} compressed data stream without needing full signal reconstruction. The accuracy of the extracted information or classification is strongly determined by the front-end settings and tolerated level of hardware impairments. Exploiting this, allows to dynamically tune accuracy for power consumption. This paper discusses this trade-off and introduces a theoretical framework to guide the selection of optimal hardware settings under given power or accuracy constraints. This is illustrated with two circuit realizations: 1) an analog-to-information converter for voice activity detection (VAD), and 2) a CS photopletysmographic (PPG) heart rate (HR) extraction application. status: publishe...|$|E
40|$|Focusing on {{problems}} such as complexities existing in compressed storage structures of the current data stream Top-k closed frequent itemsets algorithm and inaccuracy in the algorithm, the paper puts forward an algorithm of MTKCFI-SW by designing compact prefix pattern trees for compression and storage of effective information in data stream sliding windows. The CFP-tree, capable of promptly capturing newly added <b>data</b> <b>stream</b> <b>information</b> under circumstances of any sliding window sizes, {{does not need to}} fix the sizes of sliding windows and thus improves the flexibility of this algorithm. Research in dynamic determination of mining threshold and pruning threshold also helps to improve accuracy of this algorithm by adopting an effective approach in mining Top-k closed frequent itemsets in the environment of data stream. Â </p...|$|E
40|$|Fluctuations in {{the local}} Newtonian {{gravitational}} field present a limit to high precision measurements, including searches for gravitational waves using laser interferometers. In this work, we present a model of this perturbing gravitational field and evaluate schemes {{to mitigate the effect}} by estimating and subtracting it from the interferometer <b>data</b> <b>stream.</b> <b>Information</b> about the Newtonian noise is obtained from simulated seismic data. The method is tested on causal as well as acausal implementations of noise subtraction. In both cases it is demonstrated that broadband mitigation factors close to 10 can be achieved removing Newtonian noise as a dominant noise contribution. The resulting improvement in the detector sensitivity will substantially enhance the detection rate of gravitational radiation from cosmological sources. Comment: 29 pages, 11 figure...|$|E
30|$|To {{study the}} impact of {{applying}} partial reconfiguration {{in the context of}} reconfigurable radios, we have implemented an auto-adaptive OFDM transmitter in the proposed system. The OFDM is an advanced multi-carrier modulation technique used in the new generations of fixed and mobile wireless communication systems (ex: Wifi, LTE,...). This technique divides high-rate <b>data</b> <b>streams</b> into multiple low-rate <b>data</b> <b>streams.</b> The <b>information</b> is then modulated and transmitted over multiple sub-carriers.|$|R
30|$|Lane {{detection}} warning is {{a typical}} computer vision process. Analyzing the relationship between <b>data</b> <b>streams</b> and <b>information</b> included in this <b>data</b> <b>stream,</b> we divide the vision process into two levels: the data process level and the symbol process level. The vision tasks in the data process level are characterized by a large <b>data</b> <b>stream</b> with simple operations, such as convolution, edge extraction, and line detection. By contrast, the vision tasks in the symbol process level are characterized by a small <b>data</b> <b>stream</b> with complex operations.|$|R
40|$|Adaptive Information Filtering (AIF) is {{concerned}} with filtering <b>information</b> <b>streams</b> in changing environments. The changes may occur both on the transmission side (the nature of the streams can change) and on the reception side (the interests of a user can change). The research described in this paper details the progress made in a prototype AIF system based on weighted n- gram analysis and evolutionary computation. A major advance is the design and implementation of an n-gram class library allowing experimentation with different values of n instead of solely with 3 -grams as in the past. The new prototype system was tested on the Reuters- 21578 text categorization test collection. 1 Introduction Information Filtering (IF) {{is the process of}} filtering <b>data</b> <b>streams</b> {{in such a way that}} only particular data are preserved, depending on certain information needs. The IF environment is the combination of <b>data</b> <b>stream</b> and <b>information</b> needs. When the <b>data</b> <b>stream</b> and the <b>information</b> needs [...] ...|$|R
40|$|A secure {{communication}} {{system based on}} the error-feedback synchronization of the electronic model of the particle-in-a-box system is proposed. This circuit allows a robust and simple electronic emulation of the mechanical behavior of the collisions of a particle inside a box, exhibiting rich chaotic behavior. The required nonlinearity to emulate the box walls is implemented in a simple way when compared with other analog electronic chaotic circuits. A master/slave synchronization of two circuits exhibiting a rich chaotic behavior demonstrates the potentiality of this system to {{secure communication}}. In this system, binary <b>data</b> <b>stream</b> <b>information</b> modulates the bifurcation parameter of the particle-in-a-box electronic circuit in the transmitter. In the receiver circuit, this parameter is estimated using Pecora-Carroll synchronization and error-feedback synchronization. The performance of the demodulation process is verified through the eye pattern technique applied on the recovered bit stream. During the demodulation process, the error-feedback synchronization presented better performance compared with the Pecora-Carroll synchronization. The application of the particle-in-a-box electronic circuit in a secure communication system is demonstrated...|$|E
40|$|Smart home {{technology}} is a better choice for the people to care about security, comfort and power saving as well. It is required to develop technologies that recognize the Activities of Daily Living (ADLs) of the residents at home and detect the abnormal behavior in the individual's patterns. Data mining techniques such as Frequent pattern mining (FPM), High Utility Pattern (HUP) Mining were used to find those activity patterns from the collected sensor data. But applying the above technique for Activity Recognition from the temporal sensor data stream is highly complex and challenging task. So, a new approach is proposed for activity recognition from sensor data stream {{which is achieved by}} constructing Frequent Pattern Stream tree (FPS-tree). FPS is a sliding window based approach to discover the recent activity patterns over time from data streams. The proposed work aims at identifying the frequent pattern of the user from the sensor data streams which are later modeled for activity recognition. The proposed FPM algorithm uses a data structure called Linked Sensor Data Stream (LSDS) for storing the sensor <b>data</b> <b>stream</b> <b>information</b> which increases the efficiency of frequent pattern mining algorithm through both space and time. The experimental results show the efficiency of the proposed algorithm and this FPM is further extended for applying for power efficiency using HUP to detect the high usage of power consumption of residents at smart home...|$|E
40|$|International audienceThe growing {{emergence}} of new applications where the data changes rapidly has boosted the development of several researches related to <b>data</b> <b>streams</b> processing. Preference reasoning {{is an example of}} a useful task that can be used to monitor <b>data</b> <b>streams</b> for <b>information</b> that best fit the users wishes. In this paper, we revisited the formalism TPref in order to propose a new approach for processing <b>data</b> <b>streams</b> according to temporal conditional preferences. Our approach, named StreamPref, covers important issues not addressed yet in preference reasoning with temporal conditional preferences...|$|R
30|$|In {{addition}} to software realizations (code), {{there will be}} much more and diverse data in smart grids. There are already a lot of new digital data sources originating for instance from the smart metering (AMR). The data must be acquired and exchanged, resulting in new <b>data</b> <b>streams</b> and <b>information</b> flows across the grid. Furthermore, all the data must be stored and shared for example with data hubs.|$|R
40|$|A {{representation}} of a video sequence having a first <b>data</b> <b>stream</b> comprising first <b>data</b> portions, the first data portions comprising first timing information and a second <b>data</b> <b>stream,</b> the second <b>data</b> <b>stream</b> comprising a second data portion having second timing information, may be derived. Association information is associated to a second data portion of the second <b>data</b> <b>stream,</b> the association <b>information</b> indicating a predetermined first dat portion of the first <b>data</b> <b>stream.</b> A transport stream comprising {{the first and the}} second <b>data</b> <b>stream</b> as the {{representation of}} the video sequence is generated...|$|R
40|$|In {{this work}} we propose a novel {{approach}} to anomaly detection in <b>streaming</b> communication <b>data.</b> We first build a stochastic model for the system based on temporal communication patterns across each edge, which we call the REWARDS (REneWal theory Approach for Real-time <b>Data</b> <b>Streams)</b> model. We then define a measure of anomaly for an arbitrary subgraph based on the likelihood of its recent activity given past behavior. Finally, we develop an algorithm to efficiently identify subgraphs with the most anomalous activity. Although our work has until now focused on the cybersecurity domain, the model we present is more broadly applicable to <b>information</b> retrieval in <b>data</b> <b>streams</b> and <b>information</b> networks...|$|R
40|$|LISA {{will be able}} {{to detect}} {{gravitational}} waves from inspiralling massive black hole (MBH) binaries out to redshifts z > 10. If the binary masses and luminosity distances can be extracted from the Laser Interferometer Space Antenna (LISA) <b>data</b> <b>stream,</b> this <b>information</b> can be used to reveal the merger history of MBH binaries and their host galaxies in the evolving universe. Since this parameter extraction generally requires that LISA observe the inspiral for a significant fraction of its yearly orbit, carrying out this program requires adequate sensitivity at low frequencies, f 1...|$|R
40|$|In {{the real}} world data is often non stationary. In {{predictive}} analytics, machine learning and data mining the phenomenon of unexpected change in underlying data over time is known as concept drift. Changes in underlying data might occur due to changing personal interests, changes in population, adversary activities or they {{can be attributed to}} a complex nature of the environment. When there is a shift in data, the predictions might become less accurate as the time passes or opportunities to improve the accuracy might be missed. Thus the learning models need to be adaptive to the changes. The problem of concept drift is of increasing importance to machine learning and data mining as more and more data is organized in the form of <b>data</b> <b>streams</b> rather than static databases, and it is rather unusual that concepts and data distributions stay stable {{over a long period of}} time. It is not surprising that the problem of concept drift has been studied in several research communities including but not limited to machine learning and <b>data</b> mining, <b>data</b> <b>streams,</b> <b>information</b> retrieval, and recommender systems. Different approaches for detecting and handling concept drift have been proposed in the literature, and many of them have already proved their potential in a wide range of application domains, e. g. fraud detection, adaptive system control, user modeling, information retrieval, text mining, biomedicine...|$|R
40|$|It is envisaged {{that the}} Global Earth Observation System of Systems (GEOSS) will provide <b>data</b> <b>streams</b> to {{decision-support}} tools {{for a wide}} variety of users and applications. As with the Internet, GEOSS will be a global and flexible network of content providers facilitating decision makers' access to an extraordinary range of information products at their desk. However, the challenge for the global users community is to have access to fast and reliable tools to process the <b>data</b> <b>streams</b> into actionable <b>information...</b>|$|R
40|$|Continuous <b>data</b> <b>streams</b> are <b>information</b> {{sources in}} which data arrives in high-volume, in un-predictable rapid bursts. Processing <b>data</b> <b>streams</b> is a {{challenging}} task due to (1) {{the problem of}} random access to fast and large <b>data</b> <b>streams</b> using present storage technologies, (2) the exact answers from <b>data</b> <b>streams</b> are often too expensive. A framework of building a Grid-based Zero-Latency <b>Data</b> <b>Stream</b> Warehouse (GZLDSWH) to overcome the resource limitation issues in <b>data</b> <b>stream</b> processing without using approximation approaches is specified. The GZLDSWH is built upon a set of Open Grid Service Infrastructure (OGSI) -based services and Globus Toolkit 3 (GT 3) with the capability of capturing and storing continuous <b>data</b> <b>streams,</b> performing analytical processing, and reacting autonomously in near real time to some kinds of events based on well-established Knowledge Base. The requirements of a GZLDSWH, its Grid-based conceptual architecture, and the operations of its service are described in this paper. Furthermore, several challenges and issues in building a GZLDSWH such as the Dynamic Collaboration Model between the grid services, the Analytical Model, the Design and Evaluation aspects of the Knowledge Base Rules are discussed and investigated...|$|R
40|$|In this paper, we {{proposed}} an online algorithm, called FQT-Stream (Frequent Query Trees of Streams), to mine {{the set of}} all frequent tree patterns over a continuous XML <b>data</b> <b>stream.</b> A new numbering method is proposed to represent the tree structure of a XML query tree. An effective sub-tree numeration approach is developed to extract the essential information from the XML <b>data</b> <b>stream.</b> The extracted <b>information</b> is stored in an effective summary data structure. Frequent query trees are mined from the current summary data structure by a depth-first-search manner...|$|R
50|$|Asynchronous serial {{communication}} {{is a form}} of {{serial communication}} in which the communicating endpoints' interfaces are not continuously synchronized by a common clock signal. Instead of a common synchronization signal, the <b>data</b> <b>stream</b> contains synchronization <b>information</b> in form of start and stop signals, before and after each unit of transmission, respectively. The start signal prepares the receiver for arrival of data and the stop signal resets its state to enable triggering of a new sequence.|$|R
30|$|One of our {{underlying}} objectives {{has been}} to make our proposed design collect raw <b>data</b> and <b>stream</b> the <b>information</b> back to a cloud-based system. We realized that the efficacy of our design can be enhanced by introduction of an efficient and environment-friendly cloud-based solution. A mobile cloud computing approach as discussed {{in the work of}} Sarddar et al. (2015) was examined (Sarddar et al. 2015) and adopted in our work.|$|R
40|$|DE 19907964 C UPAB: 20000907 NOVELTY - The {{encryption}} device provides an encrypted <b>data</b> <b>stream</b> from audio and/or video signals using a coder (204 - 210) {{which provides a}} <b>data</b> <b>stream</b> with a pre-defined syntax, coupled to an encryption stage (18). This uses an encryption key (k) for scrambling the output signal from the coder, so that the encrypted <b>data</b> <b>stream</b> contains useful <b>information</b> which differs from the useful <b>information</b> in the <b>data</b> <b>stream</b> at {{the output of the}} coder while maintaining the pre-defined syntax. DETAILED DESCRIPTION - Also included are INDEPENDENT CLAIMS for the following: (a) a de-{{encryption device}}; (b) an encryption method for an audio and/or video signal; (c) a de-encryption method for an encrypted audio and/or video signal USE - The device is used for encryption of audio and/or video signals for copy-right protection. ADVANTAGE - The pre-defined syntax of the <b>data</b> <b>stream</b> is not altered...|$|R
40|$|The {{database}} {{community is}} exploring {{more and more}} multidisciplinary avenues: Data semantics overlaps with ontology management; reasoning tasks venture into the domain of artificial intelligence; and <b>data</b> <b>stream</b> management and <b>information</b> retrieval shake hands, e. g., when processing Web click-streams. These new research avenues become evident, for example, in the topics that doctoral students choose for their dissertations. This paper surveys the emerging multidisciplinary research by doctoral students in database systems and related areas. It {{is based on the}} PIKM 2010, which is the 3 r...|$|R
40|$|We propose {{and analyze}} a {{distributed}} learning system to classify data captured from distributed and dynamic <b>data</b> <b>streams.</b> Our scheme consists of multiple distributed learners that are interconnected via an exogenously-determined net-work. Each learner observes a specific <b>data</b> <b>stream,</b> which is correlated {{to a common}} event {{that needs to be}} classified, and maintains a set of local classifiers and a weight for each local classifier. We propose a cooperative online learning scheme in which the learners exchange information through the network both to compute an aggregate prediction and to adapt the weights to the dynamic characteristics of the <b>data</b> <b>streams.</b> The <b>information</b> dissemination protocol is designed to minimize the time required to compute the final prediction. We determine an upper bound for the worst-case misclas-sification probability of our scheme, which depends on the misclassification probability of the best (unknown) static ag-gregation rule. Importantly, such bound tends to zero if the misclassification probability of the best static aggregation rule tends to zero. When applied to well-known data sets ex-periencing concept drifts, our scheme exhibits gains ranging from 20 % to 70 % with respect to state-of-the-art solutions...|$|R
5000|$|NTBackup {{supports}} Encrypting File System, NTFS hard {{links and}} junction points, alternate <b>data</b> <b>streams,</b> disk quota <b>information,</b> mounted drive and remote storage information. It saves NTFS permissions, audit entries and ownership settings, respects the archive bit attribute on files and folders and can create normal, copy, [...] differential, incremental and daily backups, backup catalogs, {{as well as}} Automated System Recovery. It supports logging and excluding files from the backup per-user or for all users. Hardware compression is supported if the tape drive supports it. Software compression is not supported, even in Backup to files.|$|R
40|$|The paper {{focuses on}} {{temporal}} synchronization of various <b>data</b> <b>streams</b> in multimedia <b>information</b> (e. g., voice, video, graphics and text) exchanged between user entities distributed over a network. During delivery of such data at users for processing (i. e., data play-out), maintaining the required temporal association between {{points in the}} <b>data</b> across various <b>streams</b> is necessary {{in the presence of}} transport delays through the network. This synchronization problem has two aspects in an application: i) Framing of <b>data</b> <b>streams</b> which refers to identifying various points in the <b>data</b> <b>streams</b> that are distinctly perceivable, and ii) Temporal presentation control which refers to ordering of various points in the <b>data</b> <b>streams</b> over real-time as required. In our solution approach, the temporal axis of an application is segmented into intervals, where each interval is a unit of synchronization. Temporal presentation in a real-time interval involves play-out of the media data segments belonging to t [...] ...|$|R
40|$|In {{the real}} world data is often non stationary. In {{predictive}} analytics, machine learning and data mining the phenomenon of unexpected change in underlying data over time is known as concept drift. Changes in underlying data might occur due to changing personal interests, changes in population, adversary activities or they {{can be attributed to}} a complex nature of the environment. When there is a shift in data, the predictions might become less accurate as the time passes or opportunities to improve the accuracy might be missed. Thus the learning models need to be adaptive to the changes. The problem of concept drift is of increasing importance to machine learning and data mining as more and more data is organized in the form of <b>data</b> <b>streams</b> rather than static databases, and it is rather unusual that concepts and data distributions stay stable {{over a long period of}} time. It is not surprising that the problem of concept drift has been studied in several research communities including but not limited to machine learning and <b>data</b> mining, <b>data</b> <b>streams,</b> <b>information</b> retrieval, and recommender systems. Different approaches for detecting and handling concept drift have been proposed in the literature, and many of them have already proved their potential in a wide range of application domains, e. g. fraud detection, adaptive system control, user modeling, information retrieval, text mining, biomedicine. HaCDAIS 2010 workshop 1 organized in conjunction with ECML/PKDD 2010 and held on 24 September 2010 in Barcelona, Spain provides a focused international forum for researchers to discuss new, we aim to attract researchers with an interest in handling concept drift and recurring contexts in adaptive information systems. Topics discussed at the workshop include classification and clustering on <b>data</b> <b>streams</b> and evolving <b>data,</b> change and novelty detection in online, semi-online and offline settings, adaptive ensembles, adaptive sampling and instance selection, incremental learning and model adaptivity, delayed labeling in <b>data</b> <b>streams,</b> dynamic feature selection, handling local and complex concept drift, qualitative and quantitative evaluation of concept drift handling performance, reoccurring contexts and context-aware approaches, application-specific and domain driven approaches within the areas of information retrieval, recommender systems, pattern recognition, user modeling, decision support and adaptive information systems These proceedings include abstract of the invited talk, invited software report and six peer-reviewed papers accepted to the workshop, two as full papers and four as short papers...|$|R
40|$|Abstract. RFID {{middleware}} is a {{key technology}} of Internet of things. It achieves data recognition and data filtering between hardware devices and software applications. A design scheme for RFID middleware framework {{is presented in the}} paper. According to functional analysis and layered design ideas, we define a hierarchy of RFID middleware and construct an application framework, in which RFID event manager module is responsible for processing <b>data</b> <b>streams</b> and RFID <b>information</b> service module is responsible for system integration. Simulation results verify the correctness of the design project. Its recognition rate, redundant data filtering capability and other performance indicators have reached the preset requirements...|$|R
40|$|The recent {{advances}} in hardware and software have enabled the capture of different measurements of data {{in a wide range}} of fields. These measurements are generated continuously and in a very high fluctuating data rates. Examples include sensor networks, web logs, and computer network traffic. The storage, querying and mining of such data sets are highly computationally challenging tasks. Mining <b>data</b> <b>streams</b> is concerned with extracting knowledge structures represented in models and patterns in non stopping <b>streams</b> of <b>information.</b> The research in <b>data</b> <b>stream</b> mining has gained a high attraction due to the importance of its applications and the increasing generation of <b>streaming</b> <b>information.</b> Applications of <b>data</b> <b>stream</b> analysis can vary from critical scientific and astronomical applications to importan...|$|R
40|$|This paper {{discusses}} some optimization algorithms {{intended for}} usage {{in the process}} of scheduling transmissions between a base station and mobile terminals by allocating time-slots to the different mobiles. The purpose of the scheduling is {{to make use of the}} fast fading characteristics of the radio channel, instead of alleviating the effects with over-pessimistic channel coding. By using information about the individual <b>data</b> <b>streams,</b> together with <b>information</b> about future wireless channel characteristics for the different mobile hosts, it is possible to plan the transmission, so that the requirements meet the limitations. The algorithms described are compared with respect to throughput, computational complexity, and user demand satisfaction...|$|R
50|$|In many cases, files or <b>data</b> <b>streams</b> {{contain more}} <b>information</b> than {{is needed for}} a {{particular}} purpose. For example, a picture may have more detail than the eye can distinguish when reproduced at the largest size intended; likewise, an audio file does not {{need a lot of}} fine detail during a very loud passage. Developing lossy compression techniques as closely matched to human perception as possible is a complex task. Sometimes the ideal is a file that provides exactly the same perception as the original, with as much digital information as possible removed; other times, perceptible loss of quality is considered a valid trade-off for the reduced data.|$|R
40|$|Several {{research}} areas today {{overlap between}} {{the tracks of}} databases, information retrieval and knowledge management, such as natural language processing, semantic web, digital libraries, visualization, information quality and data mining. Inter-disciplinary research across these tracks encourages advances {{in the development of}} databases, the extraction of information and the discovery of knowledge. This is precisely the focus of our article. We explain the research issues addressed in a Ph. D. workshop recently held at the ACM Conference on Information and Knowledge Management. This workshop had presentations on novel ideas addressing challenges in information and knowledge management. It covered a broad range of topics such as XML architectures, sensor <b>data</b> <b>streams,</b> personal <b>information</b> managers and text pre-processing. In this article, we provide an overview of the research problems and solutions discussed in the Ph. D. workshop. Our article thus describes the latest technological developments in information and knowledge management as seen by academia. This cutting edge technology also finds practical applications in the corporate world...|$|R
40|$|Blockchain {{represents}} a technology for establishing a shared, immutable {{version of the}} truth between a network of participants that do not trust one another, and therefore {{has the potential to}} disrupt any financial or other industries that rely on third-parties to establish trust. Recent trends in computing including: prevalence of Free and Open Source Software (FOSS); easy access to High Performance Computing (HPC i. e. 'The Cloud'); and increasingly advanced analytics capabilities such as Natural Language Processing (NLP) and Machine Learning (ML) allow for rapidly prototyping applications for analysis of trends in the emergence of Blockchain technology. A scaleable proof-of-concept pipeline that lays the groundwork for analysis of multiple <b>streams</b> of semi-structured <b>data</b> posted on social media is demonstrated. Preliminary analysis and performance metrics are presented and discussed. Future work is described that will scale the system to cloud-based, real-time, analysis of multiple <b>data</b> <b>streams,</b> with <b>Information</b> Extraction (IE) (ex. sentiment analysis) and Machine Learning capability...|$|R
40|$|Streams {{of events}} appear {{increasingly}} today in various Web {{applications such as}} blogs, feeds, sensor <b>data</b> <b>streams,</b> geospatial <b>information,</b> on-line financial data, etc. Event Processing (EP) is concerned with timely detection of compound events within streams of simple events. State-of-the-art EP provides on-the-fly analysis of event streams, but cannot combine streams with background knowledge and cannot perform reasoning tasks. On the other hand, semantic tools can effectively handle background knowledge and perform reasoning thereon, but cannot deal with rapidly changing data provided by event streams. To bridge the gap, we propose Event Processing SPARQL (EP-SPARQL) as a new language for complex events and Stream Reasoning. We provide syntax and formal semantics of the language and devise an effective execution model for the proposed formalism. The execution model is grounded on logic programming, and features effective event processing and inferencing capabilities over temporal and static knowledge. We provide an open-source prototype implementation and present a set of tests to show the usefulness and effectiveness of our approach...|$|R
40|$|International audienceThe {{database}} {{community is}} exploring {{more and more}} multidisciplinary avenues: Data semantics overlaps with ontology management; reasoning tasks venture into the domain of artificial intelligence; and <b>data</b> <b>stream</b> management and <b>information</b> retrieval shake hands, e. g., when processing Web click-streams. These new research avenues become evident, for example, in the topics that doctoral students choose for their dissertations. This paper surveys the emerging multidisciplinary research by doctoral students in database systems and related areas. It {{is based on the}} PIKM 2010, which is the 3 rd Ph. D. workshop at the International Conference on Information and Knowledge Management (CIKM). The topics addressed include ontology development, <b>data</b> <b>streams,</b> natural language processing, medical databases, green energy, cloud computing, and exploratory search. In addition to core ideas from the workshop, we list some open research questions in these multidisciplinary areas...|$|R
40|$|Anomaly {{detection}} has a {{wide range}} of real-world applications, including: monitoring computer network usage, virus detection (computer or human), credit card fraud detection, and natural disaster prediction. However, unprecedented growth in the capability to collect massive amounts of data has introduced new challenges in efficiency and scalability. Furthermore, communication data is highly dynamic, so a comprehensive solution should exploit temporal as well as relational aspects of network communication. In this work we propose a novel approach to anomaly detection in <b>streaming</b> communication <b>data</b> that is able to leverage the wealth of temporal and relational information inherent in the data. We first build a stochastic model for the system based on temporal communication patterns across each edge, which we call the REWARDS (REneWal theory Approach for Real-time <b>Data</b> <b>Streams)</b> model. We then define a measure of anomaly for an arbitrary subgraph based on the likelihood of its recent activity given past behavior. Finally, we develop an algorithm to efficiently identify subgraphs with the most anomalous activity. Experiments on a variety of realworld data show the effectiveness and scalability of our approach. Although our work has until now focused on the cybersecurity domain, the model we present is more broadly applicable to <b>information</b> retrieval in <b>data</b> <b>streams</b> and <b>information</b> networks...|$|R
40|$|International Telemetering Conference Proceedings / October 30 -November 02, 1989 / Town & Country Hotel & Convention Center, San Diego, CaliforniaThe {{unique set of}} {{problems}} encountered when analyzing MILSTD- 1553 Bus data acquired in realtime from a remote source are indentified and discussed and one possible approach to their solution is offered. Such problems are associated with realtime test range support using monitored (eavesdropped) 1553 bus data telemetered in its natural (asynchronous) form rather than stuffed into a traditional synchronous PCM frame map. Attached devices acting as Bus Controllers (BC) or Remote Terminals (RT) utilize a handshaking command/response protocol to communicate packets (messages) over associated 1553 bus(es). Thus attached devices are able to decode/respond to messages occurring in any of ten possible formats because they participate in handshaking. Analysis must be preceeded by <b>data</b> <b>stream</b> decoding. Monitoring (eavesdropping) is not interactive and thus eliminates handshakes as external points of reference. This forces the process of decoding a monitored <b>data</b> <b>stream</b> to rely on intrinsic (<b>data</b> <b>stream</b> content) <b>information</b> coupled with any externally supplied configuration information. Remote monitoring must {{deal with the problem}} of re-acquisition after drop-out being asynchronous with message occurance, thus making the decoding process assume an unknown state at both start-up and re-acquisition. The proposed solution attempts to define a minimum system made up of the component for intrinsic analysis and coupled with external configuration information necessary to accurately decode monitored 1553 data in realtime...|$|R
40|$|Real-time {{remote sales}} {{assistance}} is an underdeveloped component of online sales services. Solutions involving web page text chat, telephony and video support prove problematic when seeking to remotely guide customers in their sales processes, especially with configurations of physically complex artefacts. Recently, {{there has been}} great interest {{in the application of}} virtual worlds and augmented reality to create synthetic environments for remote sales of physical artefacts. However, {{there is a lack of}} analysis and development of appropriate software services to support these processes. We extend our previous work with the detailed design of configuration context services to support the management of an interactive sales session using augmented reality. We detail the context and configuration services required, presenting a novel <b>data</b> service <b>streaming</b> configuration <b>information</b> to the vendor for business analytics. We expect that a fully implemented configuration management service, based on our design, will improve the remote sales experience for both customers and vendors alike via analysis of the <b>streamed</b> <b>information...</b>|$|R
