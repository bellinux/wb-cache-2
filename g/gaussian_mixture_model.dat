2934|10000|Public
25|$|Since W is orthogonal, the {{estimation}} problem amounts to recovery of a signal in iid Gaussian noise. As p is sparse, one {{method is to}} apply a <b>Gaussian</b> <b>mixture</b> <b>model</b> for p.|$|E
25|$|Mixture of Gaussians method {{approaches}} by modelling each pixel as {{a mixture}} of Gaussians and uses an on-line approximation to update the model. In this technique, {{it is assumed that}} every pixel's intensity values in the video can be modeled using a <b>Gaussian</b> <b>mixture</b> <b>model.</b> A simple heuristic determines which intensities are most probably of the background. Then the pixels which do not match to these are called the foreground pixels.|$|E
50|$|Subspace <b>Gaussian</b> <b>Mixture</b> <b>Model</b> (SGMM) is an {{acoustic}} modeling approach {{in which all}} phonetic states share a common <b>Gaussian</b> <b>Mixture</b> <b>Model</b> structure, and the means and mixture weights vary in a subspace of the total parameter space.|$|E
40|$|This article proposes the {{distribution}} similarity measure–based <b>Gaussian</b> <b>mixtures</b> <b>model</b> for the contact-state (CS) modelling in force-guided robotic assembly processes of flexible rubber parts. The wrench (Cartesian force and torque) signals of the manipulated object are captured for different {{states of the}} given assembly process. The distribution similarity measure–based <b>Gaussian</b> <b>mixtures</b> <b>model</b> CS modelling scheme is employed in modelling the captured wrench signals for different CSs. The proposed distribution similarity measure–based <b>Gaussian</b> <b>mixtures</b> <b>model</b> CS modelling scheme uses the <b>Gaussian</b> <b>mixtures</b> <b>model</b> in modelling the captured signals. The parameters of the <b>Gaussian</b> <b>mixtures</b> <b>models</b> are computed using expectation maximisation. The optimal number of <b>Gaussian</b> <b>mixtures</b> <b>model</b> components for each CS model is determined by considering the classification success rate as an index for the similarity measure between {{the distribution}} of the captured signals and the developed models. The optimal number of <b>Gaussian</b> <b>mixtures</b> <b>model</b> components corresponds to the highest classification success rate; hence, object elasticity variation would be accommodated by properly choosing the optimal number of <b>Gaussian</b> <b>mixtures</b> <b>model</b> components. The performance of the proposed distribution similarity measure–based <b>Gaussian</b> <b>mixtures</b> <b>model</b> CS modelling strategy is evaluated by a test stand composed of a KUKA lightweight robot doing peg-in-hole assembly processes for flexible rubber objects. Two rubber objects with different elasticity are considered for two experiments; in the first experiment, an elastic peg of 30 Shore A hardness is considered and that of the second experiment has hardness of 6 Shore A which is even softer than the one used in experiment 1. Employing the proposed distribution similarity measure–based <b>Gaussian</b> <b>mixtures</b> <b>model</b> CS modelling strategy excellent classification success rate was obtained for both experiments. However, more <b>Gaussian</b> <b>mixtures</b> <b>model</b> components are required for the softer one that gives a strong impression of the non-stationarity behaviour increment for softer materials. Comparison is performed with the available CS modelling schemes and the distribution similarity measure–based <b>Gaussian</b> <b>mixtures</b> <b>model</b> is shown to provide the best classification success rate performance with a reduced computational time...|$|R
40|$|We {{present a}} novel method forrepresenting "{{extruded}}" distributions. An extruded distribution is an M-dimensional manifoldin the parameter {{space of the}} component distribution. Representations of that manifoldare "continuous mixture models". We present a method forforming one-dimensional continuous <b>Gaussian</b> <b>mixture</b> <b>models</b> of sampled extruded Gaussian distributions viaridgk of gOBOLEd"qkOLUd Using Monte Carlo simulations and ROC analysis, we explore the utility of a variety ofbinning techniques andgd,qkEDd"qqU, functions. We demonstrate that extruded Gaussian distributions are more accurately andconsistently representedby continuous <b>Gaussian</b> <b>mixture</b> <b>models</b> than by #nite <b>Gaussian</b> <b>mixture</b> <b>models</b> formedvia maximum likelihoodexpectation maximization. ? 2002 PatternRecogdkkEU Society. Published by Elsevier Science Ltd. Allrig,, reserved...|$|R
40|$|Hidden Markov <b>models</b> using <b>Gaussian</b> <b>mixture</b> <b>models</b> {{as their}} hidden state {{distributions}} {{have been successfully}} applied in text-dependent speaker identification applications. Nevertheless, it is well-known that <b>Gaussian</b> <b>mixture</b> <b>models</b> are very vulnerable {{to the presence of}} outliers in the fitting set used for their estimation. Student's-t <b>mixture</b> <b>models</b> have been proposed recently as a heavy-tailed, tolerant to outliers alternative t...|$|R
5000|$|A typical non-Bayesian <b>Gaussian</b> <b>mixture</b> <b>model</b> {{looks like}} this: ...|$|E
5000|$|A Bayesian {{version of}} a <b>Gaussian</b> <b>mixture</b> <b>model</b> is as follows: ...|$|E
5000|$|Use a <b>Gaussian</b> <b>mixture</b> <b>model</b> (with 5-8 components) {{to model}} those 2 distributions.|$|E
30|$|We have {{implemented}} speech enhancement using <b>Gaussian</b> <b>mixtures</b> <b>models</b> with sizes 16, 64, and 256 mixtures.|$|R
40|$|When the {{projection}} {{of a collection of}} samples onto a subset of basis feature vectors has a Gaussian distribution, those samples have a generalized projective Gaussian distribution (GPGD). GPGDs arise in a variety of medical images as well as some speech recognition problems. We will demonstrate that GPGDs are better represented by continuous <b>Gaussian</b> <b>mixture</b> <b>models</b> (CGMMs) than finite <b>Gaussian</b> <b>mixture</b> <b>models</b> (FGMMs). This pape...|$|R
40|$|We propose an {{extension}} of <b>Gaussian</b> <b>mixture</b> <b>models</b> in the statistical-mechanical point of view. The conventional <b>Gaussian</b> <b>mixture</b> <b>models</b> are formulated to divide all points in given data to some kinds of classes. We introduce some quantum states constructed by superposing conventional classes in linear combinations. Our extension can provide a new algorithm in classifications of data by means of linear response formulas in the statistical mechanics...|$|R
5000|$|The {{equations}} {{here are}} incorrect {{and need to}} be corrected.Imagine a Bayesian <b>Gaussian</b> <b>mixture</b> <b>model</b> described as follows: ...|$|E
5000|$|... #Caption: The {{steps of}} the EM {{algorithm}} on a two component <b>Gaussian</b> <b>mixture</b> <b>model</b> on the Old Faithful dataset ...|$|E
5000|$|Another set {{of methods}} for {{determining}} the number of clusters are information criteria, such as the Akaike information criterion (AIC), Bayesian information criterion (BIC), or the Deviance information criterion (DIC) [...] - [...] if {{it is possible to}} make a likelihood function for the clustering model. For example: The k-means model is [...] "almost" [...] a <b>Gaussian</b> <b>mixture</b> <b>model</b> and one can construct a likelihood for the <b>Gaussian</b> <b>mixture</b> <b>model</b> and thus also determine information criterion values.|$|E
40|$|Abstract: <b>Gaussian</b> <b>mixture</b> <b>models</b> are {{commonly}} used in speaker identification and verification systems. However, owing to their non discriminant nature, <b>Gaussian</b> <b>mixture</b> <b>models</b> still give greater identification errors in the evaluation process. Partitioning speakers database in clusters based on some proximity criteria where only a single cluster <b>Gaussian</b> <b>mixture</b> <b>models</b> is run in every test, have been suggested in literature generally {{to speed up the}} identification process for very large databases. In this paper, we propose a hierarchical clustering scheme using the discriminant power of support vector machines. Speakers are divided into small subsets and evaluation is then processed by GMMs. Experimental results show that the proposed method reduced significantly the error in overall speaker identification tests. Keywords: Speaker identification, GMM, SVM...|$|R
40|$|Abstract. In this paper, {{the problem}} of face {{authentication}} using salient facial features together with statistical generative models is adressed. Actually, classical generative <b>models,</b> and <b>Gaussian</b> <b>Mixture</b> <b>Models</b> in particular make strong assumptions on the way observations derived from face images are generated. Indeed, systems proposed so far consider that local observations are independent, which is obviously {{not the case in}} a face. Hence, we propose a new generative model based on Bayesian Networks using only salient facial features. We compare it to <b>Gaussian</b> <b>Mixture</b> <b>Models</b> using the same set of observations. Conducted experiments on the BANCA database show that our model is suitable for the face authentication task, since it outperforms not only <b>Gaussian</b> <b>Mixture</b> <b>Models,</b> but also classical appearance-based methods, such as Eigenfaces and Fisherfaces. ...|$|R
40|$|Hidden Markov <b>models</b> using finite <b>Gaussian</b> <b>mixture</b> <b>models</b> {{as their}} hidden state {{distributions}} {{have been applied}} in modeling of time series that result from various noisy signals. Nevertheless, <b>Gaussian</b> <b>mixture</b> <b>models</b> are well-known to be highly intolerant {{to the presence of}} outliers within the fitting sets used for their estimation. Finite Student's-t <b>mixture</b> <b>models</b> have recently emerged as a heaviertailed, robust alternative to <b>Gaussian</b> <b>mixture</b> <b>models,</b> overcoming these hurdles. To exploit those merits of Student's-t <b>mixture</b> <b>models,</b> we introduce in this paper a novel hidden Markov chain model where the hidden state distributions are considered to be finite mixtures of multivariate Student's-t densities and we derive an algorithm for the model parameters estimation under a maximum likelihood framework. We apply this novel approach in automatic gesture recognition and we show that our model provides a substantial improvement in data representation performance and computational efficiency over the standard Gaussian model. © 2008 IEEE...|$|R
5000|$|... k-means clustering, and its {{associated}} expectation-maximization algorithm, is a special case of a <b>Gaussian</b> <b>mixture</b> <b>model,</b> specifically, the limit of taking all covariances as diagonal, equal, and small. It is often easy to generalize a k-means problem into a <b>Gaussian</b> <b>mixture</b> <b>model.</b> [...] Another generalization of the k-means algorithm is the K-SVD algorithm, which estimates data points as a sparse linear combination of [...] "codebook vectors". K-means corresponds to the special case of using a single codebook vector, with a weight of 1.|$|E
5000|$|Now, {{considering}} the factor , {{note that it}} automatically factors into [...] due {{to the structure of}} the graphical model defining our <b>Gaussian</b> <b>mixture</b> <b>model,</b> which is specified above.|$|E
5000|$|... #Caption: An {{animation}} {{demonstrating the}} EM algorithm fitting a two component <b>Gaussian</b> <b>mixture</b> <b>model</b> to the Old Faithful dataset. The algorithm steps through from a random initialization to convergence.|$|E
5000|$|<b>Gaussian</b> <b>mixture</b> <b>models</b> {{trained with}} expectation-maximization {{algorithm}} (EM algorithm) maintains probabilistic assignments to clusters, instead of deterministic assignments, and multivariate Gaussian distributions instead of means.|$|R
3000|$|... 2009), {{in order}} to {{classify}} pulmonary sounds, used cepstral analysis and <b>Gaussian</b> <b>mixture</b> <b>models</b> and achieved prediction accuracy close to 90 %. Oz et al. ([...] [...]...|$|R
3000|$|As {{background}} model,we use two improved <b>Gaussian</b> <b>mixture</b> <b>models</b> {{as described}} in [16] initialized with identical parameters except for the learning rate, a short-term background model [...]...|$|R
50|$|Since W is orthogonal, the {{estimation}} problem amounts to recovery of a signal in iid Gaussian noise. As p is sparse, one {{method is to}} apply a <b>Gaussian</b> <b>mixture</b> <b>model</b> for p.|$|E
5000|$|... #Caption: Bayesian <b>Gaussian</b> <b>mixture</b> <b>model</b> using plate notation. Smaller squares {{indicate}} fixed parameters; larger circles indicate random variables. Filled-in shapes indicate known values. The indication K means a vector of size K.|$|E
5000|$|Note {{that these}} steps {{correspond}} {{closely with the}} standard EM algorithm to derive a maximum likelihood or maximum a posteriori (MAP) solution for the parameters of a <b>Gaussian</b> <b>mixture</b> <b>model.</b> The responsibilities [...] in the E step correspond closely to the posterior probabilities of the latent variables given the data, i.e. the computation of the statistics , , and [...] corresponds closely to the computation of corresponding [...] "soft-count" [...] statistics over the data; {{and the use of}} those statistics to compute new values of the parameters corresponds closely to the use of soft counts to compute new parameter values in normal EM over a <b>Gaussian</b> <b>mixture</b> <b>model.</b>|$|E
3000|$|..., q[*]=[*] 1, … Q, where Q is {{the number}} of parts in the feature pool. The {{parameters}} of <b>Gaussian</b> <b>mixture</b> <b>models</b> are estimated using a typical EM algorithm.|$|R
30|$|Various feature {{combinations}} {{have been}} applied both for <b>Gaussian</b> <b>mixture</b> <b>modeling</b> and i-vector-based speaker diarization systems. The experiments {{have been carried out}} on Augmented Multi-party Interaction meeting corpus. The best result, in terms of diarization error rate, is reported by using i-vector-based cosine-distance clustering together with a signal parameterization consisting of a combination of static cepstral coefficients, delta, voice-quality, and prosodic features. The best result shows about 24 % relative diarization error rate improvement compared to the baseline system which is based on <b>Gaussian</b> <b>mixture</b> <b>modeling</b> and short-term static cepstral coefficients.|$|R
40|$|A general {{framework}} {{for dealing with}} both linear regression and clustering problems is described. It includes Gaussian clusterwise linear regression analysis with random covariates and cluster analysis via <b>Gaussian</b> <b>mixture</b> <b>models</b> with variable selection. It also admits a novel approach for detecting multiple clusterings from possibly correlated sub-vectors of variables, based on a model defined {{as the product of}} conditionally independent <b>Gaussian</b> <b>mixture</b> <b>models.</b> A necessary condition for the identifiability of such a model is provided. The usefulness and effectiveness of the described methodology are illustrated using simulated and real datasets...|$|R
5000|$|A Bayesian <b>Gaussian</b> <b>mixture</b> <b>model</b> is {{commonly}} extended {{to fit a}} vector of unknown parameters (denoted in bold), or multivariate normal distributions. In a multivariate distribution (i.e. one modelling a vector [...] with N random variables) one may model a vector of parameters (such as several observations of a signal or patches within an image) using a <b>Gaussian</b> <b>mixture</b> <b>model</b> prior distribution on the vector of estimates given bywhere the ith vector component is characterized by normal distributions with weights , means [...] and covariance matrices [...] To incorporate this prior into a Bayesian estimation, the prior is multiplied with the known distribution [...] of the data [...] conditioned on the parameters [...] to be estimated. With this formulation, the posterior distribution [...] is also a <b>Gaussian</b> <b>mixture</b> <b>model</b> of the form with new parameters [...] and [...] that are updated using the EM algorithm. [...] Although EM-based parameter updates are well-established, providing the initial estimates for these parameters is currently an area of active research. Note that this formulation yields a closed-form solution to the complete posterior distribution. Estimations of the random variable [...] may be obtained via one of several estimators, such as the mean or maximum of the posterior distribution.|$|E
5000|$|Currently, {{the most}} {{frequently}} used classifiers are linear discriminant classifiers (LDC), k-nearest neighbor (k-NN), <b>Gaussian</b> <b>mixture</b> <b>model</b> (GMM), support vector machines (SVM), artificial neural networks (ANN), decision tree algorithms and hidden Markov models (HMMs). Various studies showed that choosing the appropriate classifier can significantly enhance the overall performance of the system. The list below gives a brief description of each algorithm: ...|$|E
5000|$|... #Caption: Animation of the {{clustering}} {{process for}} one-dimensional data using a Bayesian <b>Gaussian</b> <b>mixture</b> <b>model</b> where normal distributions {{are drawn from}} a Dirichlet process. The histograms of the clusters are shown in different colours. During the parameter estimation process, new clusters are created and grow on the data. The legend shows the cluster colours {{and the number of}} datapoints assigned to each cluster.|$|E
40|$|We {{present an}} optimal mass {{transport}} framework {{on the space}} of <b>Gaussian</b> <b>mixture</b> <b>models,</b> which are widely used in statistical inference. Our method leads to a natural way to compare, interpolate and average <b>Gaussian</b> <b>mixture</b> <b>models.</b> Basically, we study such models on a certain submanifold of probability densities with certain structure. Different aspects of this framework are discussed and several examples are presented to illustrate the results. This method represents our first attempt to study optimal transport problems for more general probability densities with structures. Comment: 8 pages, 6 figure...|$|R
40|$|Autonomous {{navigation}} and picture compilation tasks require robust feature descriptions or models. Given the non Gaussian nature of sensor observations, {{it will be}} shown that <b>Gaussian</b> <b>mixture</b> <b>models</b> provide a general probabilistic representation allowing analytical solutions to the update and prediction operations in the general Bayesian filtering problem. Each operation in the Bayesian filter for <b>Gaussian</b> <b>mixture</b> <b>models</b> multiplicatively increases the number of parameters in the representation leading {{to the need for}} a re-parameterisation step. A computationally efficient re-parameterisation step will be demonstrated resulting in a compact and accurate estimate of the true distribution...|$|R
40|$|Abstract—Subspace <b>Gaussian</b> <b>mixture</b> <b>models</b> (SGMMs) {{provide a}} compact {{representation}} of the Gaussian parameters in an acoustic model, but may still suffer from over-fitting with insufficient training data. In this letter, the SGMM state parameters are estimated using a penalized maximum-likelihood objective, based on and regularization, {{as well as their}} combination, referred to as the elastic net, for robust model estimation. Experiments on the 5000 -word Wall Street Journal transcription task show word error rate reduction and improved model robustness with regularization. Index Terms—Acoustic modeling,-norm penalty, elastic net, regularization, sparsity, subspace <b>Gaussian</b> <b>mixture</b> <b>models...</b>|$|R
