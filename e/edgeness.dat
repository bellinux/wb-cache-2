30|0|Public
500|$|Tapping's former Stargate SG-1 co-star Michael Shanks guest starred as Jimmy in [...] "Penance". Shanks had an {{interest}} in playing Jimmy because of the <b>edgeness</b> of the character. Shanks also suggested Aleks Paunovic, actor and part-time boxer, for the part as the diacon Duke; being a boxer fit into the character's violent nature. Paul McGillion returns as Wexford in the season finale. McGillion first appeared as Wexford in the original webisodes, and since then the producers wanted him to return. Tapping believed that placing him in the finale was [...] "the perfect place for him." [...] Callum Blue was cast as the finale's antagonist, Edward Forsythe, as the producers believed Blue could portray somebody who is both charming and evil. In the same episode, Sahar Biniaz was cast as Kali; Biniaz was chosen as the producers believed her physical appearance was goddess-like. Balinder Johal played a cult member; she {{was the first person to}} audition for the part, and the producers cast her as she was spiritual, calm, and soft in her appearance. Johal taught the cast how to speak Hindi in parts of the dialogue. The producers were hoping to cast David Hewlett, another Stargate alum, in a guest spot; however, this was met by scheduling issues, as he was shooting a film at the time. Despite this the producers were confident they would cast Hewlett [...] "eventually." ...|$|E
5000|$|Consider {{a region}} with N pixels. the {{gradient-based}} edge detector {{is applied to}} this region by producing two outputs for each pixel p: the gradient magnitude Mag(p) and the gradient direction Dir(p). The <b>edgeness</b> per unit area can be defined by [...] for some threshold T.|$|E
5000|$|To include {{orientation}} with <b>edgeness</b> {{we can use}} histograms {{for both}} gradient magnitude and gradient direction. Let Hmag(R) denote the normalized histogram of gradient magnitudes of region R, and let Hdir(R) denote the normalized histogram of gradient orientations of region R. Both are normalized according to the size NR Then [...] is a quantitative texture description of region R.|$|E
5000|$|Tapping's former Stargate SG-1 co-star Michael Shanks guest starred as Jimmy in [...] "Penance". Shanks had an {{interest}} in playing Jimmy because of the <b>edgeness</b> of the character. Shanks also suggested Aleks Paunovic, actor and part-time boxer, for the part as the diacon Duke; being a boxer fit into the character's violent nature. Paul McGillion returns as Wexford in the season finale. McGillion first appeared as Wexford in the original webisodes, and since then the producers wanted him to return. Tapping believed that placing him in the finale was [...] "the perfect place for him." [...] Callum Blue was cast as the finale's antagonist, Edward Forsythe, as the producers believed Blue could portray somebody who is both charming and evil. In the same episode, Sahar Biniaz was cast as Kali; Biniaz was chosen as the producers believed her physical appearance was goddess-like. Balinder Johal played a cult member; she {{was the first person to}} audition for the part, and the producers cast her as she was spiritual, calm, and soft in her appearance. Johal taught the cast how to speak Hindi in parts of the dialogue. The producers were hoping to cast David Hewlett, another Stargate alum, in a guest spot; however, this was met by scheduling issues, as he was shooting a film at the time. Despite this the producers were confident they would cast Hewlett [...] "eventually." ...|$|E
30|$|An {{adaptive}} threshold, varying {{for each}} DCT block, is defined combining robustness and <b>edgeness,</b> {{as described in}} the following subsections.|$|E
3000|$|... {{estimate}} the distance among the <b>edgeness</b> measures, which are {{indicator of the}} presence of a master direction in the block under examination.|$|E
30|$|The <b>edgeness</b> {{value is}} used to drive the noise {{reduction}} intensity, which depends on the threshold computation defined in the following subsection. Several cases could happen.|$|E
3000|$|Edgeness" [...] of the block: if a block {{contains}} an edge or a detail, its information content should be maintained untouched, whilst a homogeneous block should be strongly corrected. In this paper an <b>edgeness</b> measure has been used. It {{is basically a}} fuzzy measure describing {{the probability that a}} block {{contains an}} edge.|$|E
40|$|This paper gives {{details of}} a series of low level image {{processing}} routines which successfully break an image into a set of coloured regions. The first stage in the process is multi-scale edge detection. A fixed set of different sized kernels are used with the results being put into a single 'edgeness' image. A fixed lower threshold is applied to the <b>edgeness</b> image. A non-maximum suppression step is then applied. The histogram of edge strength in the non-max-suppressed image is used to set the high threshold for a hysteresis edge tracking routine. Points in the non-maxsuppressed image above the high threshold are used as seed points to grow edges with the full <b>edgeness</b> image being used as the search domain. The edge growing algorithm therefore suffers less from the topological damage resulting from nonmaximum suppression. A saliency filter is used to reject short crinkled edge chains. The second stage in the process uses the edge image to generate a series of Voronoi peaks. These are u [...] ...|$|E
40|$|A b m t- I n {{this survey}} we review the impge {{processing}} {{literature on the}} various approaches and models investigators have uaed for texture. These include st. tbticrl approaches of autocordation function, optical transforms, digital h d o n n s, textural <b>edgeness,</b> structural element, gray tone cooccuaence, run lensuls, and auto- modela We discuss and generalize some structural approaches to texture based on more complex primitives than gray tone. We cwdude with some niquestothestnrcturplpn'mitipea strudud-~atktid genenliution ~ which apply tfie stntistial tech-I...|$|E
40|$|Edges {{represent}} significant {{boundary information}} between objects or classes. Various methods, {{which are based}} on differential operation, such as Sobel, Prewitt, Roberts, Canny, and etc. have been proposed and widely used. The methods are based on a linear convolution of mask with pre-assigned coefficients. In this paper, we propose an edge detection method based on Dempster-Shafer's evidence theory to evaluate <b>edgeness</b> of the given pixel. The effectiveness of the proposed method is shown through experimental results on several test images and compared with conventional methods...|$|E
40|$|Google Glass has {{potential}} to be a real-time data capture and annotation tool. With professional sports as a use-case, we present a platform which helps a football coach capture and annotate interesting events using Google Glass. In our implementation, an interesting event is indicated by a predefined hand gesture or motion, and our platform can automatically detect these gestures in a video without training any classifier. Three event detectors are examined and our experiment shows that the detector with combined <b>edgeness</b> and color moment features gives the best detection performance. ...|$|E
40|$|Geometrical image {{features}} like edges and ridges in digital images may be extracted by convolving the images with appropriate derivatives of Gaussians. The {{choice of the}} convolution operator and of {{the parameters of the}} Gaussian involved defines a specific feature image. In this paper, various feature images derived from CT and MR brain images are defined and tested for usability and robustness in a correlation-based two and three dimensional matching algorithm. A number of these feature images is shown to furnish accurate matching results. The best results are obtained using gradient magnitude <b>edgeness</b> images...|$|E
40|$|In this paper, {{the initial}} {{work on a}} fuzzy {{approach}} for detecting multi-junctions in <b>edgeness</b> images (points where two or more edges meet) is presented. An 11 x 11 pixel processing window is divided into overlapping sectors, in which the presence and orientation of line segments is determined by forming the intersection of fuzzy sets, each carrying uncertain information about the orientation of a line. The number of line segments gives the type of pixel. Some experiments are shown, illustrating the performance {{as well as a}} rough estimate of the resources required for FPGA implementation, for which the method has been developed...|$|E
40|$|We {{demonstrate}} how {{the formulation of}} a nonlinear scale-space filter {{can be used for}} edge detection and junction analysis. By casting edge-preserving filtering in terms of maximizing information content subject to an average cost function, the computed cost at each pixel location becomes a local measure of <b>edgeness.</b> This computation depends on a single scale parameter and the given image data. Unlike previous approaches which require careful tuning of the filter kernels for various types of edges, our scheme is general {{enough to be able to}} handle different edges, such as lines, step-edges, corners and junctions. Anisotropy in the data is handled automatically by the nonlinear dynamics...|$|E
40|$|Abstract—The {{purpose of}} this article is to give a new {{analysis}} of the Anisotropic Diffusion (AD) and propose an adaptive nonlinear filtering based on a judicious choice of the Conductance Function (CF) and the <b>edgeness</b> threshold. A new undesirable effect, which we call the “pinhole effect, ” may result when AD is introduced for the first time. A robust solution to this effect is proposed and evaluated through experimental data. The evolution of the diffused signal is analyzed through a physical model using the Optical Flow Technique (OFT). The overall strategy is evaluated through experimental results obtained on synthetic and actual images. Index Terms—Nonlinear anisotropic diffusion, fluid mechanics, homogeneity, image enhancement, noise smoothing, optical flow. æ...|$|E
40|$|Meshfree {{particle}} method (MPM) exhibits improved {{flexibility and}} accuracy {{in dealing with}} problems with large deformation, complex geometry and material discontinuities. In this paper, we present a MPM based framework for the simultaneous shape recovery and motion tracking of the left ventricle. The myocardium is modeled as an anisotropic elastic body accounting for the fiber directions, represented by sampling nodes bounded by endocardial and epicardial boundaries. Cardiac dynamics is then driven by external force constructed individually for each node through integration of measures of image <b>edgeness,</b> image derived salient feature coherence, prior tissue disuibution model, and temporal motion model. The displacement field throughout the cardiac cycle is obtained when the total energy of the elastic body is minimized to reach equilibrium state. Experiments with 3 D canine MR images illustrate the benefits and potentials of such effort. © 2007 IEEE...|$|E
40|$|International audienceTracking-by-detection {{approaches}} {{are some of}} the most successful object trackers in recent years. Their success is largely determined by the detector model they learn initially and then update over time. However, under challenging conditions where an object can undergo transformations, e. g., severe rotation, these methods are found to be lacking. In this paper, we address this problem by formulating it as a proposal selection task and making two contributions. The first one is introducing novel proposals estimated from the geometric transformations undergone by the object, and building a rich candidate set for predicting the object location. The second one is devising a novel selection strategy using multiple cues, i. e., detection score and <b>edgeness</b> score computed from state-of-the-art object edges and motion boundaries. We extensively evaluate our approach on the visual object tracking 2014 challenge and online tracking benchmark datasets, and show the best performance...|$|E
40|$|A novel {{scheme for}} developing, at low {{computational}} cost, neural-fuzzy classifiers based on large-scale, model-based exemplars is outlined. The new method extends {{the approach that}} Bezdek applied to train a neural net (NN) Sobel edge classifier by training the NN on the complete population of 3 x 3 binary image prototypes scored to fuzzy values by a classical operator. We first show that, replacing the fuzzy values of <b>edgeness</b> of the exemplaers, by crisp defuzzified values vastly improved computational speed. A complexity analysis proves however that for operators based on larger windows, the use of complete binary exemplars sets will be computationally intractable. In the new scheme the NN classifier is trained over a hybrid set { selected binary image exemplars with crisp outputs | sampled pixels within a realistic image, these pixels being crisply scored by use of a classic operator. } We demonstrate the scheme by deriving a 5 x 5 neural fuzzy Plessy operator, far superior to the classic Plessy...|$|E
40|$|The various {{tasks of}} {{computer}} vision dealing with objects, such as recognition, registration, and measurement, have typically required the intermediate step of finding an object edge, or equivalently {{the list of}} pixels in the object. This paper proposes a means for characterizing object structure and shape that avoids the need to find an explicit edge but rather operates directly from the image intensity distribution in the object and its background, using operators that do indeed respond to 11 <b>edgeness</b> 11 • The means involves a generalization of medial axis descriptions from objects defined by characteristic functions to those described by intensity distributions. The generalized axis is called the multiscale medial axis because it {{is defined as a}} branching curve in scale space. The result is stable to calculate and can be used to subdivide an image object into subobjects and detail subshapes as well as to characterize the shape properties of the objects, subobjects, and detail subshapes...|$|E
40|$|Abstract—Segmentation of {{coronary}} arteries in X-ray angiography {{is a fundamental}} tool to evaluate arterial diseases and choose proper coronary treatment. The accurate segmentation {{of coronary}} arteries has become an important topic for the registration of different modalities, which allows physicians rapid access to different medical imaging information from computed tomography (CT) scans or magnetic resonance imaging (MRI). In this paper, we propose an accurate fully automatic algorithm based on Graph-cuts for vessel centerline extraction, caliber estimation, and catheter detection. Vesselness, geodesic paths, and a new multiscale <b>edgeness</b> map are combined to customize the Graph-cuts approach to the segmentation of tubular structures, {{by means of a}} global optimization of the Graph-cuts energy function. Moreover, a novel supervised learning methodology that integrates local and contextual information is proposed for automatic catheter detection. We evaluate the method performance on three datasets coming from different imaging systems. The method performs as good as the expert observer with respect to centerline detection and caliber estimation. Moreover, the method discriminates between arteries and catheter with an accuracy of 96. 5 %, sensitivity of 72 %, and precision of 97. 4 %. Index Terms—Angiography, caliber, catheter, centerline (CL), Graph-cuts (GC), quantitative coronary angiography (QCA) ...|$|E
40|$|In this paper, we {{consider}} the colorization problem of grayscale images in which some color scribbles are initially given. Our proposed method {{is based on the}} weighted color blending of the scribbles. Unlike previous works which utilize the shortest distance as the blending weights, we employ a new intrinsic distance measure based on the Random Walks with Restart (RWR), known as a very successful technique for defining the relevance between two nodes in a graph. In our work, we devise new modified data-driven RWR framework that can incorporate locally adaptive and data-driven restarting probabilities. In this new framework, the restarting probability of each pixel becomes dependent on its <b>edgeness,</b> generated by the canny detector. Since this data-driven RWR enforces color consistency in the areas bounded by the edges, it produces more reliable edge-preserving colorization results that are less sensitive to the size and position of each scribble. Moreover, if the additional information about the scribbles which indicate the foreground object is available, our method can be readily applied to the object segmentation and matting. Experiments on several synthetic, cartoon and natural images demonstrate that our method achieves much high quality colorization results compared with the state-of-the-art methods. Index Terms — Data-Driven Random Walks with Restart, color blending, edge-preserving colorization...|$|E
40|$|We {{present an}} {{investigation}} {{on the use}} of Tensor Voting for categorizing LIDAR data into outliers, line elements (e. g. high-voltage power lines), surface patches (e. g. roofs) and volumetric elements (e. g. vegetation). The Reconstruction of man-made objects is a main task of photogrammetry. With the increasing quality and availability of LIDAR sensors, range data {{is becoming more and more}} important. With LIDAR sensors it is possible to quickly aquire huge amounts of data. But in contrast to classical systems, where the measurement points are chosen by an operator, the data points do not explicitly correspond to meaningful points of the object, i. e. edges, corners, junctions. To extract these features it is necessary to segment the data into homogeneous regions wich can be processed afterwards. Our approach consists of a two step segmentation. The first one uses the Tensor Voting algorithm. It encodes every data point as a particle which sends out a vector field. This can be used to categorize the pointness, <b>edgeness</b> and surfaceness of the data points. After the categorization of the given LIDAR data points also the regions between the data points are rated. Meaningful regions like edges and junctions, given by the inherent structure of the data, are extracted. In a second step the so labeled points are merged due to a similarity constraint. This similarity constraint is based on a minimum description length principle, encoding and comparing different geometrical models. The output of this segmentation consists of non overlapping geometric objects in three dimensional space. The aproach is evaluated with some examples of Lidar data. ...|$|E
40|$|This work {{presents}} an image segmentation method for range data that uses multi-scale wavelet analysis {{in combination with}} pattern recognition. To segment range images we develop PASSEF (pattern analysis of scale space {{for the detection of}} features). PASSEF creates a fuzzy edge map and we then apply a morphological watershed algorithm to this map to create a segmentation. The PASSEF system uses pattern recognition to classify points in an image based on response to a feature detector over scale. A scale-space signature is the vector of measurements at different scales taken at a single point in an image. We train PASSEF with scale-space signatures from the edge points of a training image. Once trained, the system can determine the degree of <b>edgeness</b> of points in a new image. A feature-detection framework based on multi-scale analysis and pattern-recognition has several potential advantages over other feature-detection systems. Our goal is to create a system that exploits the advantages of a multi-scale, pattern-recognition framework. These advantages are detection of features at different scales (i. e. features of all sizes), robustness to noise, and few or no free parameters. We discuss these advantages in relation {{to the development of the}} PASSEF system and provide a critical analysis of the system based on these three goals. The PASSEF system achieves the stated goals for the detection of step-edge features. Our results also show that this technique might be useful in the detection of other features such as crease edges. We suggest future work for extending the capabilities of the system...|$|E
40|$|Background Accurate {{automatic}} segmentation of the caudate nucleus in {{magnetic resonance}} images (MRI) {{of the brain}} is of great interest {{in the analysis of}} developmental disorders. Segmentation methods based on a single atlas or on multiple atlases have been shown to suitably localize caudate structure. However, the atlas prior information may not represent the structure of interest correctly. It may therefore be useful to introduce a more flexible technique for accurate segmentations. Method We present Cau-dateCut: a new fully-automatic method of segmenting the caudate nucleus in MRI. CaudateCut combines an atlas-based segmentation strategy with the Graph Cut energy-minimization framework. We adapt the Graph Cut model to make it suitable for segmenting small, low-contrast structures, such as the caudate nucleus, by defining new energy function data and boundary potentials. In particular, we exploit information concerning the intensity and geometry, and we add supervised energies based on contextual brain structures. Furthermore, we reinforce boundary detection using a new multi-scale <b>edgeness</b> measure. Results We apply the novel CaudateCut method to the segmentation of the caudate nucleus to a new set of 39 pediatric attention-deficit/hyperactivity disorder (ADHD) patients and 40 control children, as well as to a public database of 18 subjects. We evaluate the quality of the segmentation using several volumetric and voxel by voxel measures. Our results show improved performance in terms of segmentation compared to state-of-the-art approaches, obtaining a mean overlap of 80. 75 %. Moreover, we present a quantitative volumetric analysis of caudate abnormalities in pediatric ADHD, the results of which show strong correlation with expert manual analysis. Conclusion CaudateCut generates segmentation results that are comparable to gold-standard segmentations and which are reliable in the analysis of differentiating neuroanatomical abnormalities between healthy controls and pediatric ADHD...|$|E
40|$|The {{problem of}} {{detecting}} edges between differently textured regions {{is important in}} the process of segmentation of remotely sensed images. However, the concept of texture is difficult to be precisely defined {{and there seems to be}} no unique way to characterize texture. One possible approach is through the use of edge detection algorithms and i n these terms texture is conceived as <b>edgeness</b> per unit area. Regions that possess a wide variation in tone will be characterized by a high concentration of local edges and the opposite is true for smooth regions. The algorithm for texture edge detection that is proposed in this paper is based on the detection of local edges followed by the processes of propagation and shrinking of regions and the determination of the periphery of the resulting sets. The local edge detection method is formulated in statistical terms and it leads to the solution of a hypothesis testing problem. Similar results could be obtained by using differentiation techniques like the gradient, for example. After the local edges are detected, a propagation process, followed by a shrinking process, will tend to eliminate the holes and the isolated points in the binary image defined by the local edges. The border of the textured regions is obtained by determining the periphery of the resulting connected components sets S 2 ̆ 7 s through the computation of the set of points with unitary distance to S (the complement of S). This set can be obtained through a shrinking process. Variations of the methods of propagation and shrinking were also attempted. These variations are based on thresholds on the number of neighbors (on a 8 -neighborhood) of a point, that determine whether the point remain in S, move from S to S or vice-versa. The methods were tested on Landsat images of the State of Mato Grosso, Brasil. Preliminary results seem to indicate that the variations of the propagation and shrinking methods based on the use of thresholds tend to give edges that are more continuous...|$|E
40|$|Visual {{recognition}} of three-dimensional objects {{is a fundamental}} task in content-based retrieval applications that follow a query-by-example approach. It helps to provide comfortable and efficient ways to access databases via visual descriptions of objects. As the role of product and object databases steadily increases, the development of effective and efficient recognition systems gains in importance. One of the problems to be solved in visual recognition is the image segmentation, whose goal is to find an image partition composed of regions that have a correspondence to real objects. As a general vision problem, the segmentation task is ill-posed, and can only be solved under consideration of additional information that is not contained in the images. This work presents a segmentation framework based on a general model of visual processing. The task is split into three stages, each dealing with knowledge {{at different levels of}} abstraction. The image-based segmentation stage exclusively considers low-level image information to detect homogeneous regions. The surface-based stage incorporates knowledge about the scene composition in order to find segments that correspond to expected surfaces. The object-based stage identifies regions as parts of the objects known to the application. This is achieved through the interaction with different recognition processes that provide the necessary additional information about the objects. A multi-objective optimization algorithm is employed to evaluate the first two segmentation stages. This evaluation concept allows to quantitatively compare optimal parameterizations for each algorithm, where the optimality criterion is given through an aggregate fitness measure that considers several aspects of the segmentation result simultaneously. The object-based segmentation stage can be indirectly evaluated using the recognition rates of the complete retrieval system. The proposed framework is tested with object sets containing up to 200 objects. Modern image processing techniques have been combined and enhanced in the algorithmic specification of the framework. These include feature-space clustering, color <b>edgeness</b> detection based on color contrast techniques, watershed transform for color images, split and merge methods based on adjacency graph image representations, a color zooming approach relying on the whitening transformation, combination of several information sources using Bayesian Belief Networks, and the detection of relevant image locations by means of a scale-space analysis. The optimization process used in the evaluation of the algorithms relies on an evolutionary approach that finds a front of optimal parameterizations for a given reference set. It allows to objectively verify the adequateness of the proposed methods over several state-of-the-art algorithms. Additionally, the three-staged segmentation framework permits to concentrate the optimization of the segmentation into different aspects of the algorithms, which increases the robustness and improves the results...|$|E
40|$|This {{dissertation}} examines word-prosody of ciTonga, a Malawian Bantu {{language spoken}} by lakeshore people of northern Malawi. It {{is argued that}} the real word-prosody in this language (and perhaps many Bantu languages) revolves around the idea of Strong Accent Constituency, power relations between segments, syllables and between lower and higher prosodic categories as determined by Universal Guidelines such as SONORITY, FINALITY, <b>EDGENESS</b> and PROSODIC HIERARCHY as well as constraints which favour language- or context-specific Strong Accent Constituents (PROSODIC STEM, ACCENT FOOT, STEM-SYLLABLE 1, PENULTIMATE SYLLABLE or FINAL SYLLABLE). Tone and prosodic morphemes such as Minimal Prosodic Words and Reduplicative Prosodic Morphemes also seem to be heavily regulated by Strong Accent Constituency. The empirical bases are three speech styles found in ciTonga (Nkhata-Bay Variety) namely, formal, common, and elderly speech styles. It {{is one of several}} understudied and endangered languages in Malawi. This study therefore is in line with {{one of the goals of}} the University of Malawi's Centre for Language Studies, where this candidate serves as a member, which is to prioritize research activities on such languages. The candidate is a native speaker of ciTonga and, as such, he is primary source of most of the data. Other methods such as elicitation and focus group discussions were conducted with informants not only to get to the bottom of the matter, but to also understand social issues underlying language variation. The dissertation has been presented in 6 chapters. Chapter 1 presents introductory remarks. These include the problem statement, a note on methodology, summary of findings, theoretical precedents, and, finally, organization of the dissertation. Chapter 2 presents basic facts about the language under study. These include language classification, previous works on ciTonga, speech sounds, the syllable, tone, as well as nominal and verbal morphology. Chapter 3 presents a proposal for the theory of Strong Accent Constituency. It presents the data on vowel and consonant deletion which motivates this theory analysis. Then attempts are made to account for the facts in terms of stress-accent theory and Downing's (2006 b) Morpheme-Based Templates Theory both of which are found to be slightly problematic to account for segment deletion and preservation patterns exhibited in ciTonga. Finally, the chapter introduces the theory and accounts for the facts in terms of Strong Accent Constituent Theory. Chapter 4 presents formally the theory of Interaction between Tone and Strong Accent Constituents. The chapter presents the data on tone assignment in basic verbs, simple past tense verbs and present progressive aspect verbs which motivate this type of theory analysis. Attempts are then made to account for the facts in terms of tone alignment theory (as argued by Mtenje 2006), autosegmental accent (Clements and Goldsmith 1984) and 'pitch-accent' or accentual properties of tone in Bantu languages (as hinted upon by Downing 2004). All these theoretical perspectives are found to be slightly inadequate to account for tone distribution patterns in ciTonga. On the other hand, a theory based on Interaction between Tone and Strong Accent Constituents is shown to account for the facts slightly better. Chapter 5 presents formally proposals for Strong Accent Constituent-Based Templates as a theory of morphology-prosody interfaces in ciTonga and perhaps many other Bantu languages with a Strong Accent Constituent system. It presents the data on general phonological words, Minimal Prosodic Words and Reduplicative Prosodic Morphemes. It then reviews two competing theories in the literature within the Generalized Templates Theory namely, the Prosodic Hierarchy-Based Templates Theory and the Morpheme-Based Templates Theory both of which have a goal to account for morphology-prosody interfaces. Both these theories are shown to be slightly inadequate to account for parameters exhibited by phonological words in ciTonga. On the other hand, it is suggested that a theory of Strong Accent Constituent-Based Templates may account for the facts slightly better. Chapter 6 summarizes and concludes the dissertation.   PhD i språkvitenskapPhD in Language and Linguistic...|$|E

