0|2673|Public
30|$|From {{the current}} <b>image</b> <b>frame</b> to the still <b>image</b> <b>frame</b> between the target, search and the {{extracted}} {{features of the}} <b>image</b> <b>frame</b> the most similar to the target motion characteristics.|$|R
40|$|Augmented desk {{interfaces}} {{and other}} vir-tual reality systems depend on accurate, real-time hand and fingertip tracking for seamless inte-gration between real objects and associated digital information. We introduce {{a method for}} discerning fingertip locations in <b>image</b> <b>frames</b> and measuring fin-gertips trajectories across <b>image</b> <b>frames.</b> We also pro-pose a mechanism for combining direct manipulation and symbolic gestures based on multiple finger-tip motions. Our method uses a filtering tech-nique, in addition to detecting fin-gertips in each <b>image</b> <b>frame,</b> to predict fingertip locations in succes-sive <b>image</b> <b>frames</b> and to examin...|$|R
3000|$|...,[*]α) {{represents}} transition algorithm. It can {{be found}} that the complex <b>image</b> <b>frames</b> are proportional to the degree and feature matching. This shows that more space, time, and computation must be paid {{in order to get}} more features to match the <b>image</b> <b>frames.</b>|$|R
40|$|Abstract: This paper {{proposes a}} new {{approach}} to water flow algorithm for the text line segmentation. Original method assumes hypothetical water flows under a few specified angles to the document <b>image</b> <b>frame</b> from left to right and vice versa. As a result, unwetted <b>image</b> <b>frames</b> are extracted. These areas are of major importance for text line segmentation. Method modifications mean extension values of water flow angle and unwetted <b>image</b> <b>frames</b> function enlargement. Results are encouraging due to text line segmentation improvement which is the most challenging process stage in document image processing...|$|R
30|$|A few <b>image</b> <b>frames</b> {{with minimal}} camera motion is {{selected}} from the image sequence. By doing this, image redundancy is addressed as <b>image</b> <b>frames</b> having large camera motion between them have less amount of overlapping object regions. So, adding two frames with large camera motion does not give many corresponding points.|$|R
30|$|The {{characteristics}} of real-time target <b>image</b> <b>frames</b> would be extracted.|$|R
3000|$|The {{input to}} the system is a {{sequence}} of grey scale <b>image</b> <b>frames</b> from a video of a maritime scene. The output {{of the system is}} ideally the same set of <b>image</b> <b>frames</b> with various maritime objects of interest highlighted by a level set contour. An overview of how the system functions is as follows: [...]...|$|R
40|$|A {{vision system}} has been {{developed}} that recognizes and tracks multiple vehicles from sequences of gray-scale images taken from a moving car in hard real time. Recognition is accomplished by combining the analysis of single <b>image</b> <b>frames</b> with {{the analysis of the}} motion information provided by multiple consecutive <b>image</b> <b>frames.</b> In single <b>image</b> <b>frames,</b> cars are recognized by matching deformable gray-scale templates, by detecting image features, such as corners, and by evaluating how these features relate to each other. Cars are also recognized by differencing consecutive <b>image</b> <b>frames</b> and by tracking motion parameters that are typical for cars. The vision system utilizes the hard real-time operating system Maruti which guarantees that the timing constraints on the various vision processes are satisfied. The dynamic creation and termination of tracking processes optimizes the amount of computational resources spent and allows fast detection and tracking of multiple cars. Experimental results [...] ...|$|R
40|$|International audienceIn this paper, {{we propose}} an {{extraction}} method of lip movement images from successive <b>image</b> <b>frames</b> and present {{the possibility to}} utilize lip movement images in the speech activity extraction process of speech recognition phase. The <b>image</b> <b>frames</b> are acquired from the PC image camera {{with the assumption that}} facial movement is limited during talking. First of all, one new lip movement <b>image</b> <b>frame</b> is generated with comparing two successive <b>image</b> <b>frames</b> each other. Second, the fine image noises are removed. Each fitness rate is calculated by comparing the lip feature data as objectly separated images. It is analyzed {{whether or not there is}} the lip movement image through verification to the objects and three images which have higher rates in their fitnesses. As a result of linking the speech & image processing system, the interworking rate shows 99. 3 % even in the various illumination environments. It was visually confirmed that lip movement images are tracked and can be utilized in speech activity extraction process...|$|R
40|$|This paper {{introduces}} a spatio-temporal technique for selecting or filtering out lower quality digital <b>image</b> <b>frames.</b> The technique is demonstrated on Electro-Optical/Infrared image sequences which suggests it {{is a candidate}} for exploiting reconnaissance (recce) imagery or can be part of a recce subsystem. For human vision exploitation, a few poor quality <b>image</b> <b>frames</b> out of hundreds in a digital image sequence may be only a minor irritation when the sequence runs at the typical 30 frames per second. Of course, if that human needs to examine each frame, a system that automatically removes or enhances lower quality <b>image</b> <b>frames</b> could be beneficial. For machine vision subsystems, a few poor quality <b>image</b> <b>frames</b> could cause lower probability of recognition. The filtering technique introduced in this paper can improve input into machine vision algorithms. Another application for this technique is digital transmission to filter out unwanted images prior to transmission or to selectively enha [...] ...|$|R
30|$|We {{compared}} our method {{with other}} background subtraction algorithms on three publicly available datasets. The Wallflower dataset [31] contains six videos. Each video comes with one manually labeled ground truth. They {{have the same}} <b>image</b> <b>frame</b> size of 160 [*]×[*] 120 pixels. The Star dataset [32] contains nine videos. Each video comes with 20 manually labeled frames as ground truths. The videos have a different <b>image</b> <b>frame</b> size, from 160 [*]×[*] 120 pixels to 320 [*]×[*] 256 pixels. Finally, we tested our method and compared with other algorithms on ChangeDetection.net dataset [33]. Each video comes with large amount of labeled ground truths. They have larger <b>image</b> <b>frame</b> size, from 320 [*]×[*] 240 pixels to 720 [*]×[*] 576 pixels.|$|R
40|$|A 3 D {{volumetric}} display concept {{is presented in}} this paper. In comparison to stereoscopic 3 D imaging systems, real-time volumetric 3 D imaging systems require substantially higher image data transfer amounts. EuroLCDs proprietary 3 D volumetric image generation technology employs high speed projection of image depth planes into multilayer optical shutter screen. For transmission of flicker free real-time multilayer volumetric <b>image</b> <b>frame</b> it is required several times higher data bandwidth in comparison with stereoscopic 3 D <b>image</b> <b>frames.</b> Paper presents real-time 3 D <b>image</b> <b>frame</b> data transfer implementation with Xilinx Virtex- 6 FPGA's and high-speed video projection system based on Texas Instruments DLP technology, solidstate light sources and multiplanar optical shutter screen array...|$|R
40|$|A video based {{method to}} detect {{volatile}} organic compounds (VOC) leaking out of process equipments used in petrochemical refineries is developed. Leaking VOC plume from a damaged component causes edges present in <b>image</b> <b>frames</b> loose their sharpness. This leads to {{a decrease in the}} high frequency content of the image. The background of the scene is estimated and decrease of high frequency energy of the scene is monitored using the spatial wavelet transforms of the current and the background images. Plume regions in <b>image</b> <b>frames</b> are analyzed in low-band sub-images, as well. <b>Image</b> <b>frames</b> are compared with their corresponding low-band images. A maximum likelihood estimator (MLE) for adaptive threshold estimation is also developed in this paper. © 2008 IEEE...|$|R
3000|$|A {{digital input}} filter, RTSP, that captures <b>image</b> <b>frames</b> from an IP camera using RTSP/RTP for transmission; [...]...|$|R
40|$|We {{address the}} problem of {{restoring}} a high-quality image from an observed image sequence strongly distorted by atmospheric turbulence. A novel algorithm is proposed in this paper to reduce geometric distortion as well as space-and-time-varying blur due to strong turbulence. By considering a suitable energy functional, our algorithm first obtains a sharp reference image and a subsampled image sequence containing sharp and mildly distorted <b>image</b> <b>frames</b> with respect to the reference image. The subsampled image sequence is then stabilized by applying the Robust Principal Component Analysis (RPCA) on the deformation fields between <b>image</b> <b>frames</b> and warping the <b>image</b> <b>frames</b> by a quasiconformal map associated with the low-rank part of the deformation matrix. After <b>image</b> <b>frames</b> are registered to the reference image, the low-rank part of them are deblurred via a blind deconvolution, and the deblurred frames are then fused with the enhanced sparse part. Experiments have been carried out on both synthetic and real turbulence-distorted video. Results demonstrate that our method is effective in alleviating distortions and blur, restoring image details and enhancing visual quality. Comment: 21 pages, 24 figure...|$|R
2500|$|Teletext {{information}} is broadcast in the {{vertical blanking interval}} between <b>image</b> <b>frames</b> in a broadcast television signal, in numbered [...] "pages." ...|$|R
3000|$|Among some {{reported}} {{methods for}} vehicle tracking [36, 37, 38], we propose to combine region-based tracking [8] with kernel-based tracking [39, 40]. After vehicle segmentation, results are binary blobs {{in the image}} and these blobs are extracted and classified as vehicles if they meet at least the threshold size. A state vector is associated with each valid vehicle and it records the position of vehicle at each <b>image</b> <b>frame.</b> In addition to the positions, other information of the vehicle, such as size and vehicle shape/contour, can be recorded and be used to classify vehicles if needed. Given {{the current state of}} a vehicle at <b>image</b> <b>frame</b> [...] t, we use Kalman filtering to predict the state of a vehicle in the next <b>image</b> <b>frame</b> [...] t + 1 [...] [41]. To associate vehicles between frame [...] t [...] and frame [...] t + 1, the algorithm compares the segmented vehicles in frame [...] t + 1 [...] (i.e., the target) against the vehicles from frame [...] t [...] (i.e., the model) in joint feature-spatial spaces using the Kernel-based tracking algorithm [39, 40]. The feature-spatial model of a vehicle is characterized in <b>image</b> <b>frame</b> [...] t [...] and predicted for comparison against the target in frame [...] t + 1 [...]. Finally, note that the state vector, which contains the vehicle position at each <b>image</b> <b>frame,</b> gives the complete trajectory of a vehicle once a vehicle is tracked.|$|R
40|$|International audienceThis paper proposes an {{approach}} to water flow method modification for text segmentation and reference text line detection of sample text at almost any skew angle. Original water flow algorithm assumes hypothetical water flows under only a few specified angles of the document <b>image</b> <b>frame</b> {{from left to right}} and vice versa. As a result of water flow algorithm, unwetted <b>image</b> <b>frames</b> are extracted. These areas are of major importance for text line parameters extraction as well as for text segmentation. Water flow method modification means extension values of water flow specified angle and unwetted <b>image</b> <b>frames</b> function enlargement. Modified method is examined and evaluated under different sample text skew angles. Results are encouraged especially due to improving text segmentation which is the most challenging process stage...|$|R
50|$|A DynaScan 360 is a {{cylindrical}} LED display device, {{designed to}} minimize the number of light-emitting diodes used to display <b>image</b> <b>frames.</b>|$|R
40|$|Fitting {{an image}} block with {{ordinary}} discrete orthogonal polynomials results in large errors at {{and around the}} block center. However, using discrete orthogonal polynomials gen-eralized with a normal weight, we have smaller errors at and around the center. In this paper, we show a technique estimating block motion in two successive <b>image</b> <b>frames</b> rising thegeneralized polynomials. The technique fits blocks of two <b>image</b> <b>frames</b> with the polynomi-als and derives constraints from which translational motion components can be determined...|$|R
40|$|In this paper, {{we propose}} using signals {{collected}} from in-ertial sensors on cameras {{to speed up}} image alignment for panorama construction. Inertial sensors including accelerom-eters and gyroscopes are first calibrated to improve sensing accuracy. These sensors are then used to estimate the posi-tion and orientation of each captured <b>image</b> <b>frame.</b> By know-ing {{the relative displacement of}} <b>image</b> <b>frames,</b> alignment can be performed with good accuracy and computational effi-ciency. Through examples we illustrate the effectiveness of inertial-sensor assisted panorama...|$|R
40|$|A {{laser scanner}} computes a {{range from a}} laser line to an imaging sensor. The laser line {{illuminates}} a detail within an area covered by the imaging sensor, the area having a first dimension and a second dimension. The detail has a dimension perpendicular to the area. A traverse moves a laser emitter coupled to the imaging sensor, at a height above the area. The laser emitter is positioned at an offset along the scan direction {{with respect to the}} imaging sensor, and is oriented at a depression angle with respect to the area. The laser emitter projects the laser line along the second dimension of the area at a position where a <b>image</b> <b>frame</b> is acquired. The imaging sensor is sensitive to laser reflections from the detail produced by the laser line. The imaging sensor images the laser reflections from the detail to generate the <b>image</b> <b>frame.</b> A computer having a pipeline structure is connected to the imaging sensor for reception of the <b>image</b> <b>frame,</b> and for computing the range to the detail using height, depression angle and/or offset. The computer displays the range to the area and detail thereon covered by the <b>image</b> <b>frame...</b>|$|R
3000|$|... [18], {{which is}} used to {{describe}} the regional structure of an image that is used to model the regional change in the <b>image</b> <b>frames</b> over time.|$|R
3000|$|... {{possible}} values {{corresponding to the}} hypotheses ranging from [...] "no target present" [...] to [...] "all targets present" [...] in the <b>image</b> <b>frame</b> at instant [...]...|$|R
5000|$|... 85 mm: Portrait — A short {{telephoto}} lens {{that allows a}} longer subject to camera distance, to produce pleasing perspective effects, while maintaining useful <b>image</b> <b>framing.</b>|$|R
40|$|Abstract [...] A medical {{workstation}} {{has been}} developed for the efficient display and analysis of large sets of digital cineradiographic images. Various features aid the clinician in quickly identifying and extracting the image information relevant for diagnosis: animated viewing of <b>image</b> <b>frames,</b> a digital magnifying glass for local image enlargement and enhancement, a special review queue for critical <b>image</b> <b>frames,</b> and task-oriented <b>image</b> processing. Double <b>frame</b> buffering and direct memory addressing ensure fast, artifact-free image display and transfer. Much freedom is provided for adapting the system to one’s own preferences. A statistical analysis of extensive tests conducted by eight clinical expert reviewers is given...|$|R
50|$|In HTML, longdesc is an {{attribute}} used within the <b>image</b> element, <b>frame</b> element, or iframe element. It {{is supposed to}} be a URL to a document that provides a long description for the <b>image,</b> <b>frame,</b> or iframe in question. Note that this attribute should contain a URL, and not as is commonly mistaken, the text of the description itself.|$|R
3000|$|For each <b>image</b> <b>frame</b> {{of a video}} {{activity}} {{containing a}} certain class of activity performed by a single person, a pair of 2 -D hand points {p [...]...|$|R
50|$|C-HTML {{does not}} support tables, image maps, {{multiple}} fonts and styling of fonts, background colors and <b>images,</b> <b>frames,</b> or style sheets, and {{is limited to a}} monochromatic display.|$|R
5000|$|GIF: Support for {{decoding}} and rendering compressed Graphics Interchange Format (GIF) images, in its single-frame variants only. Loading a multi-frame GIF will display {{only the}} first <b>image</b> <b>frame.</b>|$|R
40|$|We {{present a}} system for {{automatic}} 2 D analysis of the tongue movement from digital ultrasound image sequences. The system focuses on extraction, tracking {{and analysis of the}} tongue surface during speech production and swallowing. The input to the system is provided by a Head and Transducer Support System (HATS), which is developed for use in ultrasound imaging of tongue movement. We developed a novel active contour (snakes) model that uses several adjacent images during the extraction of the tongue surface contour for an <b>image</b> <b>frame.</b> The user supplies an initial contour model for a single <b>image</b> <b>frame</b> in the sequence. This initial contour is a form of expert knowledge input to the system, which is used to find the candidate contour points in the adjacent images. Subsequently, the new snake mechanism is applied to estimate optimal contours for each <b>image</b> <b>frame</b> using these candidate points. Finally, the system uses a postprocessing method to refine the positions of the contours by utilizing more spatiotemporal information. We exten...|$|R
40|$|This paper {{presents}} results demonstrating real-time six {{degree of}} freedom localization, mapping, navigation and obstacle avoidance in an outdoor environment using only a low-cost off-the-shelf inertial measurement unit and a monocular camera. This navigation system is intended for operation of small unmanned aerial vehicles when GPS signals are unavailable due to obstructions or jamming and when operating in cluttered environments such as urban canyons or forests. A small radio-controlled car {{is used as a}} test bed. A bearings-only Simultaneous Localization and Mapping algorithm was implemented to localize both the vehicle and obstacles. This paper describes: (a) the hardware used; (b) a two-step approach for data association (<b>image</b> <b>frame</b> to <b>image</b> <b>frame</b> followed by <b>image</b> <b>frame</b> to map); (c) a technique for landmark initialization for the case where landmarks are located on the ground. Hardware test results demonstrating navigation to a goal in an obstacle strewn environment are presented. The effect of unmodelled sensor biases is examined in simulation. I...|$|R
40|$|Efficient {{representation}} of the background texture in video <b>image</b> <b>frames,</b> motivates com-pression strategies based on good perceptual reconstruction quality, instead of just bit-accurate reconstruction. This {{is especially true for}} video <b>image</b> <b>frames</b> in applications such as videos with structural patterns, and Bi-Directional Reflectance Distribution Func-tion (BRDF) <b>image</b> <b>frames</b> of an object, where different images of an object in a single pose are taken in different illumination conditions. This paper investigates a new approach for an efficient {{representation of}} a class of images from textured videos and different BRDF images of an object, using sparse {{representation of the}} Directional Empirical Mode Decom-position (DEMD) residue of the frame. The efficient representation of the DEMD residue is achieved as a sparse coding solution based on a Discrete Wavelet Transform (DWT) -based sparsification. Experimental results demonstrate the effectiveness of the algorithm showing higher compression as compared to standard wavelet-based image compression schemes in a Compressive Sensing (CS) framework and JPEG 2000, at similar perceptual reconstruction quality...|$|R
40|$|In {{this article}} a scheme for image {{transmission}} over Wireless Sensor Networks (WSN) with an adaptive compression factor is introduced. The proposed control architecture affects {{the quality of}} the transmitted images according to: (a) the traffic load within the network and (b) the level of details contained in an <b>image</b> <b>frame.</b> Given an approximate transmission period, the adaptive compression mechanism applies Quad Tree Decomposition (QTD) with a varying decomposition compression factor based on a gradient adaptive approach. For the initialization of the proposed control scheme, the desired a priori maximum bound for the transmission time delay is being set, while a tradeoff among {{the quality of the}} decomposed <b>image</b> <b>frame</b> and the time needed for completing the transmission of the frame should be taken under consideration. Based on the proposed control mechanism, the quality of the slowly varying transmitted <b>image</b> <b>frames</b> is adaptively deviated based on the measured time delay in the transmission. The efficacy of the adaptive compression control scheme is validated through extended experimental results...|$|R
3000|$|... where R and t are the {{rotation}} and translation matrix, respectively, m and n are the height and {{width of the}} <b>image</b> <b>frame,</b> and k is the inlier point number.|$|R
3000|$|At each user-click of the tip-electrode, {{automatic}} tracking commences between the diastolic and systolic <b>image</b> <b>frames</b> determined in Section 2.1. Two cropped window sizes are first defined: (i) a [...]...|$|R
