369|117|Public
50|$|Rizzuto and Kahana (2001) {{were able}} to show that the neural network model can account for {{repetition}} on recall accuracy by incorporating a probabilistic-learning algorithm. During the retrieval process, no learning occurs. As a result, the weights of the network remain fixed, showing that the model is able to switch from a learning stage to a recall stage. By adding contextual drift we are able to show the rapid forgetting that occurs in a <b>Hopfield</b> <b>model</b> during a cued-recall task. The entire network contributes to the change in the activation of any single node.|$|E
50|$|Hopfield {{dielectric}} - {{in quantum}} mechanics {{a model of}} dielectric consisting of quantum harmonic oscillators interacting with the modes of the quantum electromagnetic field. The collective interaction of the charge polarization modes with the vacuum excitations, photonsleads to the perturbation of both the linear dispersion relation of photons and constant dispersion of charge waves by the avoided crossing between the two dispersion lines of polaritons. Similarly to the acoustic and the optical phonons and far from the resonance one branch is photon-like while the other charge wave-like.Mathematically the Hopfield dielectric for the one mode of excitation {{is equivalent to the}} Trojan wave packet in the harmonicapproximation. The <b>Hopfield</b> <b>model</b> of the dielectric predicts the existence of eternal trapped frozen photons similar to the Hawking radiation inside the matter with the density proportional to the strength of the matter-field coupling.|$|E
50|$|The <b>Hopfield</b> <b>model</b> {{accounts}} for associative memory through {{the incorporation of}} memory vectors. Memory vectors can be slightly used, and this would spark the retrieval of the most similar vector in the network. However, {{we will find out}} that due to this process, intrusions can occur. In associative memory for the Hopfield network, {{there are two types of}} operations: auto-association and hetero-association. The first being when a vector is associated with itself, and the latter being when two different vectors are associated in storage. Furthermore, both types of operations are possible to store within a single memory matrix, but only if that given representation matrix is not one or the other of the operations, but rather the combination (auto-associative and hetero-associative) of the two. It is important to note that Hopfield’s network model utilizes the same learning rule as Hebb’s (1949) learning rule, which basically tried to show that learning occurs as a result of the strengthening of the weights by when activity is occurring.|$|E
40|$|We {{obtain a}} result on the {{behavior}} of the solutions of a general nonautonomous <b>Hopfield</b> neural network <b>model</b> with delay, assuming some general bound for the product of consecutive terms in the sequence of neuron charging times and some conditions to control the nonlinear part of the equations. We then apply this result to improve some existent results on the stability of <b>Hopfield</b> <b>models.</b> Our results are based on a new abstract result {{on the behavior}} of nonautonomous delayed equations...|$|R
40|$|Based {{on a new}} {{abstract}} {{result on}} the behavior of nonautonomous delayed equations, we obtain a stability result for the solutions of a general discrete nonautonomous <b>Hopfield</b> neural network <b>model</b> with delay. As an application we improve some existing results on the stability of <b>Hopfield</b> <b>models.</b> António J. G. Bento and César M. Silva were partially supported by FCT through CMA-UBI (project PEst-OE/MAT/UI 0212 / 2014). José J. Oliveira was supported by the Research Centre of Mathematics of the University of Minho with the Portuguese Funds from the Fundação para a Ciência e a Tecnologia, through the Project PEstOE/MAT/UI 0013 / 2014. info:eu-repo/semantics/publishedVersio...|$|R
40|$|In this paper, {{artificial}} {{neural networks}} for solving multiobjective optimization {{problems have been}} considered. The Tank-Hopfield model for linear programming has been extended, and then the neural model for finding Pareto-optimal solutions in the linear multi-criterion optimization problem with continuous decision variables has been discussed. Furthermore, the model for solving quasi-quadratic multiobjective optimization problems has been studied. What is more, some <b>models</b> of the <b>Hopfield</b> neural network for solving NP-hard combinatorial multi-criterion optimization problems have been proposed. Finally, the family of extended <b>Hopfield</b> <b>models</b> for finding Pareto-optimal solutions has been developed [...] Key words: Neural networks, efficient solutions, multi-criterion optimization. 1...|$|R
50|$|The Network {{capacity}} of the Hopfield network model is determined by neuron amounts and connections within a given network. Therefore, the number of memories {{that are able to}} be stored is dependent on neurons and connections. Furthermore, it was shown that the recall accuracy between vectors and nodes was 0.138 (approximately 138 vectors can be recalled from storage for every 1000 nodes) (Hertz et al., 1991). Therefore, it is evident that many mistakes will occur if one tries to store a large number of vectors. When the <b>Hopfield</b> <b>model</b> does not recall the right pattern, it is possible that an intrusion has taken place, since semantically related items tend to confuse the individual, and recollection of the wrong pattern occurs. Therefore, the Hopfield network model is shown to confuse one stored item with that of another upon retrieval. Perfect recalls and high capacity, >0.14, can be loaded in the network by Hebbian learning method.|$|E
50|$|Building on his 1964 Rockefeller PhD thesis, in the 1960s and 1970s, Grossberg {{generalized}} the Additive and Shunting {{models to}} a class of dynamical systems that included these models as well as non-neural biological models, and proved content addressable memory theorems for this more general class of models. As part of this analysis, he introduced a Liapunov functional method to help classify the limiting and oscillatory dynamics of competitive systems by keeping track of which population is winning through time. This Liapunov method led him and Michael Cohen to discover in 1981 and publish in 1982 and 1983 a Liapunov function {{that they used to}} prove that global limits exist in a class of dynamical systems with symmetric interaction coefficients that includes the Additive and Shunting models. John Hopfield published this Liapunov function for the Additive model in 1984. Some scientists started to call Hopfield’s contribution the <b>Hopfield</b> <b>model.</b> In an attempt to correct this historical error, other scientists called the more general model and Liapunov function the Cohen-Grossberg model. Still other scientists call it the Cohen-Grossberg-Hopfield model. In 1987, Bart Kosko adapted the Cohen-Grossberg model and Liapunov function, which proved global convergence of STM, to define an Adaptive Bidirectional Associative Memory that combines STM and LTM and which also globally converges to a limit.|$|E
50|$|As {{noted in}} the section on Education and Early Research, Grossberg has studied how brains give rise to minds since he took the {{introductory}} psychology course as a freshman at Dartmouth College in 1957. At that time, Grossberg introduced the paradigm of using nonlinear systems of differential equations to show how brain mechanisms can give rise to behavioral functions. This paradigm is helping to solve the classical mind/body problem, and is the basic mathematical formalism that is used in biological neural network research today. In particular, in 1957-1958, Grossberg discovered widely used equations for (1) short-term memory (STM), or neuronal activation (often called the Additive and Shunting models, or the <b>Hopfield</b> <b>model</b> after John Hopfield's 1984 application of the Additive model equation); (2) medium-term memory (MTM), or activity-dependent habituation (often called habituative transmitter gates, or depressing synapses after Larry Abbott's 1997 introduction of this term); and (3) long-term memory (LTM), or neuronal learning (often called gated steepest descent learning). One variant of these learning equations, called Instar Learning, was introduced by Grossberg in 1976 into Adaptive Resonance Theory and Self-Organizing Maps for the learning of adaptive filters in these models. This learning equation was also used by Kohonen in his applications of Self-Organizing Maps starting in 1984. Another variant of these learning equations, called Outstar Learning, was used by Grossberg starting in 1967 for spatial pattern learning. Outstar and Instar learning were combined by Grossberg in 1976 in a three-layer network for the learning of multi-dimensional maps from any m-dimensional input space to any n-dimensional output space. This application was called Counter-propagation by Hecht-Nielsen in 1987.|$|E
40|$|The electron-transfer {{reaction}} between ferrocytochrome c and ferricyanide {{has been}} studied by the method of photoexcitation. The observed transfer rate shows saturation behaviour at high ferricyanide concentration. Data analysis indicates {{that there are two}} binding sites of vastly different affinities at which electron transfer occurs. The binding constant for the strong binding site decreases from 1600 M - 1 to 80 M - 1 as the ionic strength increases from 15 mM to 140 mM. At 20 °C, the intramolecular electron-transfer rate for this site is 4. 65 · 10 4 s - 1, which gives an electron-transfer distance of approx. 9. 7 Å according to <b>Hopfield's</b> <b>model.</b> link_to_subscribed_fulltex...|$|R
40|$|In this paper, an {{identification}} method is proposed for discrete-time nonlinear systems using a Hopfield neural network (HNN) as a coefficient learning mechanism to obtain optimized coefficients over {{a set of}} Gaussian basis functions. The outputs of the HNN, which are coefficients over a set of Gaussian basis functions, are discretized to be a discrete <b>Hopfield</b> learning <b>model</b> and completely approximated by the learning model if the sampled step size approaches zero. The main contribution {{of this paper is}} that the convergence condition of the discrete <b>Hopfield</b> learning <b>model</b> is derived. Finally, to demonstrate the effectiveness of the proposed methods, simulation results are illustrated in this paper. Keywords:Hopfield neural networks...|$|R
40|$|We {{analyze the}} storage {{capacity}} of the <b>Hopfield</b> <b>models</b> on classes of random graphs. While such a setup has been analyzed for {{the case that the}} underlying random graph model is an Erdös-Renyi graph, other architectures, including those investigated in the recent neuroscience literature, have not been studied yet. We develop a notion of storage capacity that highlights the influence of the graph topology and give results on the storage capacity for not too irregular random graph models. The class of models investigated includes the popular power law graphs for some parameter values. Comment: Published at [URL] in the Bernoulli ([URL] by the International Statistical Institute/Bernoulli Society ([URL]...|$|R
40|$|We {{introduce}} {{a form of}} the <b>Hopfield</b> <b>model</b> that is able to store {{an increasing number of}} biased i. i. d. patterns (it is well known that the standard <b>Hopfield</b> <b>model</b> fails to work properly in this context). We show that this new form of the <b>Hopfield</b> <b>model</b> with N neurons can store N fl log N or ffN biased patterns (depending on which notion of storage is used). The quantity fl increases with an increasing bias of the patterns, while ff decreases when the bias gets large. 2 Hopfield Models with Biased Patterns 1 Introduction Neural networks, especially the so [...] called <b>Hopfield</b> <b>model,</b> have been in the centre of interest in the recent probabilistic and physical literature. Originally introduced by Pastur and Figotin [FP 77] as a simplified model of a spin glass the <b>Hopfield</b> <b>model</b> connects the ideas of neural computing with those of statistical mechanics. However, it was its re-interpretation as a neural network by Hopfield [Ho 82] that created the interest in the model. The basic idea behind [...] ...|$|E
40|$|The {{so-called}} swapping algorithm {{was designed}} to simulate from spin glass distributions, among others. In this note we show that it mixes rapidly, in a very simple disordered system, the <b>Hopfield</b> <b>model</b> with two patterns. Swapping algorithm Metropolis algorithm <b>Hopfield</b> <b>model</b> Markov chain Monte Carlo methods Neural networks Spin glasses...|$|E
40|$|Abstract. We {{investigate}} the retrieval phase diagrams of an asynchronous fully-connected attractor network with non-monotonic transfer function {{by means of}} a mean-field approximation. We find for the noiseless zero-temperature case that this non-monotonic Hopfield network can store more patterns than a network with monotonic transfer function investigated by Amit et al. Properties of retrieval phase diagrams of non-monotonic networks agree with the results obtained by Nishimori and Opris who treated synchronous networks. We also {{investigate the}} optimal storage capacity of the non-monotonic <b>Hopfield</b> <b>model</b> with state-dependent synaptic couplings introduced by Zertuche et al. We show that the non-monotonic <b>Hopfield</b> <b>model</b> with state-dependent synapses stores more patterns than the conventional <b>Hopfield</b> <b>model.</b> Our Statistical mechanical approaches were successful for investigation of equilibrium properties of associative memories or attractor networks. The <b>Hopfield</b> <b>model</b> [1][2] which updates its state asynchronously was investigated fro...|$|E
40|$|The {{phase diagram}} of Little's model is {{determined}} {{when the number}} of stored patterns p grows as ρ = αN, where N is the number of neurons. We duplicate phase space in order to accomodate cycles of length two within the framework of equilibrium statistical mechanics. Using the replica symmetry scheme we determine the phase diagram including a parameter J 0 able to control the occurrence of cycles. The second order transition between the paramagnetic and ferromagnetic phase becomes first order at a tricritical point. The retrieval region is some what larger than in <b>Hopfield's</b> <b>model.</b> We also find a low temperature paramagnetic phase with unphysical properties...|$|R
40|$|We {{present an}} {{analytically}} solvable random graph {{model in which}} the connections between the nodes can evolve in time, adiabatically slowly compared to {{the dynamics of the}} nodes. We apply the formalism to finite connectivity attractor neural network (<b>Hopfield)</b> <b>models</b> and we show that due to the minimisation of the frustration effects the retrieval region of the phase diagram can be significantly enlarged. Moreover, the fraction of misaligned spins is reduced by this effect, and is smaller than in the infinite connectivity regime. The main cause of this difference is found to be the non-zero fraction of sites with vanishing local field when the connectivity is finite. Comment: 17 pages, 8 figure...|$|R
40|$|In {{this paper}} {{we look at}} a class of random {{optimization}} problems that arise in the forms typically known as <b>Hopfield</b> <b>models.</b> We view two scenarios which we term as the positive Hopfield form and the negative Hopfield form. For both of these scenarios we define the binary optimization problems that essentially emulate what would typically {{be known as the}} ground state energy of these models. We then present a simple mechanism {{that can be used to}} create a set of theoretical rigorous bounds for these energies. In addition to purely theoretical bounds, we also present a couple of fast optimization algorithms that can also be used to provide solid (albeit a bit weaker) algorithmic bounds for the ground state energies...|$|R
40|$|We {{consider}} the <b>Hopfield</b> <b>model</b> with {{a finite number}} of randomly chosen patterns above and below the critical temperature and prove an almost sure conditional central limit theorem for the vector of overlap parameters. For this purpose we analyse the almost sure asymptotic behaviour of the partition function. Fluctuations <b>Hopfield</b> <b>model</b> Overlap parameter Neural networks Laplace's method...|$|E
40|$|We {{study the}} Kac {{version of the}} <b>Hopfield</b> <b>model</b> and prove a Lebowitz-Penrose theorem for the {{distributions}} of the overlap parameters. At the same time, we prove a large deviation principle for the standard <b>Hopfield</b> <b>model</b> with infinitely many patterns. (orig.) Available from TIB Hannover: RR 5549 (95) +a / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|E
40|$|We give {{a review}} on the {{rigorous}} results concerning the storage {{capacity of the}} <b>Hopfield</b> <b>model.</b> We distinguish between two different concepts of storage both of them guided {{by the idea that}} the retrieval dynamics in a Monte-Carlo dynamics (possibly at zero temperature). We recall the results of McEliece et al. [MPRV 87] as well as those by Newman [N 88] for the storage capacity of the <b>Hopfield</b> <b>model</b> with unbiased i. i. d. patterns and comprehend some recent development concerning the <b>Hopfield</b> <b>model</b> with semantically correlated or biased patterns. (orig.) Available from TIB Hannover: RR 5549 (254) +a / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|E
40|$|We {{evaluate}} the high temperature {{limit of the}} free energy of spin glasses on the hypercube with Hamiltonian H_N(σ) = σ^T J σ, where the coupling matrix J is drawn from certain symmetric orthogonally invariant ensembles. Our derivation relates the annealed free energy of these models to a spherical integral, and expresses the limit of the free energy {{in terms of the}} limiting spectral measure of the coupling matrix J. As an application, we derive the limiting free energy of the Random Orthogonal Model (ROM) at high temperatures, which confirms non-rigorous calculations of Marinari et al. (1994). Our methods also apply to other well-known models of disordered systems, including the SK and Gaussian <b>Hopfield</b> <b>models.</b> Comment: 15 pages, 1 figur...|$|R
40|$|We study Generalised Restricted Boltzmann Machines with generic priors {{for units}} and weights, interpolating between Boolean and Gaussian variables. We present a {{complete}} {{analysis of the}} replica symmetric phase diagram of these systems, which {{can be regarded as}} Generalised <b>Hopfield</b> <b>models.</b> We underline the role of the retrieval phase for both inference and learning processes and we show that retrieval is robust for a large class of weight and unit priors, beyond the standard Hopfield scenario. Furthermore we show how the paramagnetic phase boundary {{is directly related to the}} optimal size of the training set necessary for good generalisation in a teacher-student scenario of unsupervised learning. Comment: 5 pages, 4 figures; extensive simulations and 2 new figures added; corrected typos; added reference...|$|R
5000|$|The quantum {{associative}} memory algorithm [...] {{has been introduced}} by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory, but propose an algorithm for a circuit-based quantum computer that simulates {{associative memory}}. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition, and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. An advantage lies in the exponential storage capacity of memory states, however the question remains whether the model has significance regarding the initial purpose of <b>Hopfield</b> <b>models</b> as a demonstration of how simplified artificial neural networks can simulate features of the brain.|$|R
40|$|We {{analyze the}} storage {{capacity}} of {{different forms of}} the <b>Hopfield</b> <b>model</b> with semantically correlated patterns ¸ i (i. e. the patterns are correlated in but independent in i). We show that the standard <b>Hopfield</b> <b>model</b> of neural networks with N neurons can store N fl log N or ffN semantically correlated patterns (depending on which notion of storage is used), provided that the correlation comes from a homogeneous Markov chain. The quantity fl increases with an increasing correlation of the patterns, while ff decreases with large correlations. This answers the open question whether the standard Hopfield can store any increasing number of correlated patterns {{at all in the}} affirmative. Moreover, we show that there is a version of the <b>Hopfield</b> <b>model</b> that can store a number of semantically correlated data that is even larger than the bound for the number of independent patterns that can be stored in the standard <b>Hopfield</b> <b>model</b> provided we require that the patterns are exactly reproduced. [...] ...|$|E
40|$|We {{have applied}} the {{generating}} functional analysis (GFA) to the continuous <b>Hopfield</b> <b>model.</b> We have also {{confirmed that the}} GFA predictions in some typical cases exhibit good consistency with computer simulation results. When a retarded self-interaction term is omitted, the GFA result becomes identical to that obtained using the statistical neurodynamics {{as well as the}} case of the sequential binary <b>Hopfield</b> <b>model.</b> Comment: 4 pages, 2 figure...|$|E
40|$|This note {{relates the}} storage {{capacity}} of the <b>Hopfield</b> <b>model</b> of neural networks with possibly correlated patterns to a moderate deviation upper bound for the empirical correlation of the patterns. Examples are, among others, independent patterns with spins that are correlated as in an Ising model or as in a Curie-Weiss model. <b>Hopfield</b> <b>model</b> Neural networks Storage capacity Moderate deviations Large deviations Spin glasses...|$|E
40|$|In {{this work}} we explain how to {{properly}} use mean-field methods {{to solve the}} inverse Ising problem when the phase space is clustered, that is many states are present. The clustering of the phase space can occur for many reasons, e. g. when a system undergoes a phase transition. Mean-field methods for the inverse Ising problem are typically used without {{taking into account the}} eventual clustered structure of the input configurations and may led to very bad inference (for instance in the low temperature phase of the Curie-Weiss model). In the present work we explain how to modify mean-field approaches when the phase space is clustered and we illustrate the effectiveness of the new method on different clustered structures (low temperature phases of Curie-Weiss and <b>Hopfield</b> <b>models).</b> Comment: 6 pages, 5 figure...|$|R
40|$|Many popular {{probabilistic}} {{models for}} temporal sequences assume simple hidden dynamics or low dimensionality of discrete variables. For higher dimensional discrete hidden variables, recourse is often made to approximate mean field theories, which {{to date have}} been applied to models with only simple hidden unit dynamics. We consider a class of models in which the discrete hidden space is defined by parallel dynamics of densely connected high-dimensional stochastic Hopfield networks. For these Hidden <b>Hopfield</b> <b>Models</b> (HHMs), mean field methods are derived for learning discrete and continuous temporal sequences. We discuss applications of HHMs to classification and reconstruction of non-stationary time series. We also demonstrate a few problems (learning of incomplete binary sequences and reconstruction of 3 D occupancy graphs) where distributed discrete hidden space representation may be useful. We show that while these problems cannot be easily solved by other dynamic belief networks, they are efficiently addressed by HHMs...|$|R
40|$|We rigorously {{define and}} study the {{limiting}} dynamics for PasturFigotin -Hopfield models of neural networks with N nodes and p patterns in the (thermodynamic) limit N ! 1, p j const. We study local and global properties of this limiting dynamics. This {{author would like to}} thank ESI for their hospitality and their financial support y Postal address: INRIA, Domaine de Voluceau, Rocquencourt, B. P. 105, 78153 Le Chesnay, France z The research of this author has been supported by a fellowship from the Royal Netherlands Academy for Arts and Sciences x Postal address: Dept. of Mathematics and Computer Science, University of Leiden, P. O. Box 9512, 2300 RA Leiden, Holland 1 Introduction There are sufficiently many mathematical results concerning equilibrium states of <b>Hopfield</b> <b>models,</b> but results on the dynamics of these networks are scattered throughout the physical literature. Moreover, no rigorous exposition nor even a definition of the dynamics in the thermodynamic limit seem to ex [...] ...|$|R
40|$|The mean field <b>Hopfield</b> <b>model</b> is the {{paradigm}} for serial processing networks: a system able to retrieve, {{one at a}} time, previously stored patterns of information. On the other side, mul-titasking associative networks (retrieving several patterns of informations at the same time) are crucial for the understunding of real biological systems and {{for the development of}} parallel processing artificial machines. In this thesis, I will introduce two different ways to build paral-lel processing associative networks: by diluting the patterns entries (Diluted <b>Hopfield</b> <b>Model)</b> or by introducing a topological structure (Hierarchical <b>Hopfield</b> <b>Model)</b> towards a non mean-field interaction among agents. From a statistical mechanics perspective, passing through the analogy between the <b>Hopfield</b> <b>model</b> and a bipartite spin glass system, we analyze, in the former case, the capability of the network by varying the level of load and dilution (from low to high storage, from fully connected to finite connectivity regime), while, in the last case, we investigate the possibility of switching from a serial to a parallel processing regime by varying the level of the temperature...|$|E
40|$|We {{investigate}} the retrieval phase diagrams of an asynchronous fully-connected attractor network with non-monotonic transfer function {{by means of}} a mean-field approximation. We find for the noiseless zero-temperature case that this non-monotonic Hopfield network can store more patterns than a network with monotonic transfer function investigated by Amit et al. Properties of retrieval phase diagrams of non-monotonic networks agree with the results obtained by Nishimori and Opris who treated synchronous networks. We also {{investigate the}} optimal storage capacity of the non-monotonic <b>Hopfield</b> <b>model</b> with state-dependent synaptic couplings introduced by Zertuche et el. We show that the non-monotonic <b>Hopfield</b> <b>model</b> with state-dependent synapses stores more patterns than the conventional <b>Hopfield</b> <b>model.</b> Our formulation can be easily extended to a general transfer function. Comment: Latex 13 pages using IOP style fil...|$|E
40|$|We {{construct}} a perturbative series for order parameters and other interesting observables of the <b>Hopfield</b> <b>model.</b> It is {{shown that the}} L(2) norm of the n-th term of the expansion is bounded by alpha(n). We apply this construction to the L(2) fluctuation of the order parameter r and {{we find that the}} coefficients of its expansion vanish in the thermodynamic limit. Using a recent result [1] we deduce that the replica symmetric solution of the <b>Hopfield</b> <b>model</b> holds at any order of alpha...|$|E
40|$|Hopfield Neural Networks {{have been}} used as {{universal}} identifiers of non-linear systems, because of their inherent dynamic properties. However the design decision of the number of neurons in the Hopfield network is not easy to make, in order for the network model to have the necessary complexity, extra neurons are required. This poses a problem since the role of the states that these neurons represent is not clear. Adding a hidden layer in the <b>Hopfield</b> network <b>model</b> increases the complexity of the model without posing the extra states problem. Alternatively breaking the problem down by having different interconnected <b>Hopfield</b> networks <b>modeling</b> each state, also increase the complexity of the problem. A comparison between the three approaches (traditional Hopfield, Hopfield with a hidden layer, and multiple interconnected Hopfield networks) indicates equivalence between the three structures, but with the alternative cases having increased connectivity in the feedback matrix, and limited connectivity in the weight matrices. 1...|$|R
40|$|In {{our recent}} work StojnicHopBnds 10 {{we looked at}} a class of random {{optimization}} problems that arise in the forms typically known as <b>Hopfield</b> <b>models.</b> We viewed two scenarios which we termed as the positive Hopfield form and the negative Hopfield form. For both of these scenarios we defined the binary optimization problems whose optimal values essentially emulate what would typically {{be known as the}} ground state energy of these models. We then presented a simple mechanisms {{that can be used to}} create a set of theoretical rigorous bounds for these energies. In this paper we create a way more powerful set of mechanisms that can substantially improve the simple bounds given in StojnicHopBnds 10. In fact, the mechanisms we create in this paper are the first set of results that show that convexity type of bounds can be substantially improved in this type of combinatorial problems. Comment: arXiv admin note: substantial text overlap with arXiv: 1306. 376...|$|R
30|$|The {{importance}} of group polarization is significant as {{it helps to}} explain group behavior {{in a variety of}} real-life situations. Examples of these situations include public voting, terrorism, and violence. In our former studies, we investigated the group polarization based on <b>Hopfield</b> attractor <b>model,</b> and revealed a very interesting connection between global patterns and local structure balance [10]. Next, we will concentrate on the phenomenon of group opinions convergence and consensus.|$|R
