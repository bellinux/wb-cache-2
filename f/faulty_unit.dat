16|89|Public
5000|$|Five {{original}} spares, now Four spares - ESP-1 FRAM-1 [...] Plus {{the original}} 4 spares, 2 on ITS-S6, 2 on ITS-P6, {{one of which}} was swapped out for a <b>faulty</b> <b>unit</b> during an Exp 35 EVA May 11, 2013.|$|E
50|$|Krikalev and Currie {{replaced}} a <b>faulty</b> <b>unit</b> in Zarya which {{controlled the}} discharging of stored energy {{from one of}} the module's six batteries. The battery had not been working properly in its automatic configuration, but the new unit was functioning normally shortly after it was installed.|$|E
50|$|The {{technology}} {{and maintenance of}} the UCC-4 was taught to airman of the career field 304X0 at Keesler Air Force Base, Mississippi.The UCC-4 was unique {{because all of the}} modules were hermetically sealed. The technician maintaining the piece of equipment merely had to take out the faulty module {{and replace it with a}} functional one, and this was the start of what became known as black box maintenance. Before the development of this technology, technicians would be required to troubleshoot and repair the <b>faulty</b> <b>unit.</b> The black box technique of maintenance meant the equipment was put back on line quickly, and the faulty module was sent to a depot where it was repaired by a team of technicians.|$|E
40|$|AbstractIn this paper, a {{comparison}} model is considered for multiprocessor fault diagnosis. In this approach, system tasks {{are assigned to}} pairs of processors (or units) {{and the results are}} compared. These agreements and disagreements among units are the basis for identifying <b>faulty</b> <b>units.</b> Such a system is said to be t 1 -diagnosable if, given any complete collection of comparison outcomes, the set of <b>faulty</b> <b>units</b> can be isolated to within a set of at most t 1 units, assuming that no more than t 1 <b>units</b> can be <b>faulty.</b> This paper shows an optimal O(|E|) algorithm (where |E| corresponds to the number of comparisons), by which, {{on the basis of the}} collection of comparison outcomes, all the <b>faulty</b> <b>units</b> except at most one can be correctly identified and all the <b>faulty</b> <b>units</b> can be isolated to within a set of t 1 or fewer units in which at most one can possibly be fault-free...|$|R
40|$|Consider {{a network}} G of n units, {{each of which}} can be tested by those units from which there is a test connection. The outcome of each test is binary (good or faulty) and is the {{judgment}} of the testing unit on the tested unit. We present an algorithm for identifying the minimum number of <b>faulty</b> <b>units</b> based on the test outcomes. It works in time proportional to τ(G) m, provided the number of <b>faulty</b> <b>units</b> is no more than τ(G), where m is the number of test connections and τ(G) is a parameter of G such that if the number of <b>faulty</b> <b>units</b> is no more than τ(G), then they are uniquely identifiable...|$|R
40|$|We {{propose a}} {{distributed}} disabling algorithm for a multiprocessing {{system in which}} each processor or unit is prevented from doing computation when it fails some number of tests by other units. The goal is to disable all <b>faulty</b> <b>units</b> and to enable all fault-free units. Specifically, a unit is disabled iff it fails d or more tests by enabled units (d-disabling rule). A multiprocessor system is c-correctable using the d-disabling ntle iff all <b>faulty</b> <b>units</b> are permanently disabled and all fault-free units are permanently enabled after {{a finite number of}} applications of the disabling rule, pro-vided there are no more than c <b>faulty</b> <b>units.</b> This models an unattended system where the removal of <b>faulty</b> <b>units</b> is done locally by simple and reliable circuitry. We give a sufficient condition for c-correctability in general systems and a necessary and sufficient condition in general systems where c <d. Then, we give necessary and sufficient conditions for c-correctability of two types of systems, (1) complete digraphs and (2) a new class of systems called segmented systems. I...|$|R
5000|$|The {{fault in}} the ADF system made it {{impossible}} to be used as an approach aid. The VOR flag alarm was set too high and the glide-slope deflection sensitivity indication was set 50% more than its usual operational setting. It didn't prevent the system, but was rather [...] "exacting for the pilots". The faulty settings of the glide-slope alarm circuit prevented the flag appearing until the two signals were unusable or not received. If the <b>faulty</b> <b>unit</b> received an [...] "unsatisfactory signal", the pointer could move to the mid-position. The pilot on the left seat could not notice this failure. This is dangerous. If the pointer moved to the mid-position, the crew would have thought that they were on the correct glide path. Investigators then questioned the airworthiness of the aircraft due to these faults.|$|E
50|$|Lee {{had ordered}} the French prisoners to hide, but they {{remained}} outside and fought alongside the American and Wehrmacht soldiers. Throughout the night, the defenders were harried by a reconnaissance force sent to assess their strength and probe the fortress for weaknesses. In the morning of 5 May, a force of 100-150 Waffen-SS launched their attack. Before the main assault began, Gangl was able to phone Alois Mayr, the Austrian resistance leader in Wörgl, and request reinforcements. Only two more German soldiers under his command and a teenage Austrian resistance member, Hans Waltl, could be spared, and they quickly drove to the castle. The Sherman tank provided machine-gun fire support until it was destroyed by German fire from an 88 mm gun; it was occupied at the time only by a radioman seeking to repair the tank's <b>faulty</b> <b>unit,</b> who escaped without injury.|$|E
50|$|An {{equipment}} practice was needed quickly, {{and it was}} realised that a matrix of reed relays would be {{about the same size}} as a crossbar switch. Therefore, the {{equipment practice}} of AT&E's crossbar system was adopted for the TXE1 apart from the STC common control, which had its own equipment practice. The STC common control consisted of 14 racks and made up a complete suite of the exchange. It was made entirely from discrete components as integrated circuits were not yet in common use. There was much discussion by all contractors as to whether at the time there was a reliable connector so as to provide the ability to withdraw and replace units. STC decided to have the units that could be withdrawn and AT&E and AEI did not. It turned out that the connectors used by STC were reliable and they had a great advantage in fault finding. It also allowed the STC engineers to place a suspect <b>faulty</b> <b>unit</b> in an outrigger so it could be tested in situ.|$|E
3000|$|... [...]. The set of test outcomes, {{known as}} the syndrome, is decoded by a {{centralized}} system supervisor. Preparata et al. [19] and later Hakimi and Amin [14] characterized the PMC model stating the topological properties of the diagnostic graph under which a system is diagnosable. An efficient syndrome decoding algorithm for the PMC model able to identify all <b>faulty</b> <b>units</b> was proposed by Dahbura–Masson [12]; another algorithm able to identify almost all <b>faulty</b> <b>units</b> was later proposed in [4].|$|R
40|$|A novel {{approach}} to the diagnosis of hypercubes, called Self-Validating Diagnosis, is introduced. An algorithm based on this approach, called SVD algorithm, is presented and evaluated. Given any fault set and the resulting syndrome, the algorithm returns a diagnosis and a syndrome-dependent bound, Ts, with the property that diagnosis is correct (although possibly incomplete) if {{the actual number of}} <b>faulty</b> <b>units</b> is less than Ts. The average of Ts is very large and the diagnosis is almost complete even when the percentage of <b>faulty</b> <b>units</b> in the system approaches 50 %. Moreover, the diagnosis correctness can be validated deterministically by individually probing {{a very small number of}} units. These results suggest that the SVD algorithm is suitable for applications requiring a large degree of diagnosability, as it is the case of wafer-scale testing of VLSI chips, where the percentage of <b>faulty</b> <b>units</b> may be as large as 50 %...|$|R
40|$|International audienceFuture chip multiprocessors (CMPs) will {{be capable}} of deconfiguring <b>faulty</b> <b>units</b> in order to permit {{continued}} operation {{in the presence of}} wear-out failures. However, the unforeseen downside is pipeline imbalance due to other portions of the pipeline now being overprovisioned with respect to the deconfigured functionality. We propose PowerTransfer, a novel CMP architecture that dynamically redistributes the chip power under pipeline imbalances that arise from deconfiguring <b>faulty</b> <b>units.</b> Through rebalancing – achieved by temporary, symbiotic deconfiguration of additional functionality within the degraded core – power is harnessed for use elsewhere on the chip. This additional power is dynamically transferred to portions of the multi-core chip that can realize a performance boost from turning on previously dormant microarchitectural features. We demonstrate that a realistic PowerTransfer manager achieves chip-wide performance improvements of up to 25 % compared to architectures that simply deconfigure <b>faulty</b> <b>units</b> without regard to the resulting inefficiency...|$|R
40|$|Abstract- The {{present day}} systems {{are in need}} of high level of fault {{tolerance}} in the Multicore processors without substantial loss of overall performance. The commodity processors that are available now have handled mainly soft errors (transient errors) and a very small amount of work is done for handling hard faults. In this paper we propose to include a Reconfigurable Hardware Unit (RHU) inside the core which can detect and isolate the faults in the functional units inside a core using stored test patterns. Once the <b>faulty</b> <b>unit</b> is isolated, a part of the RHU is reconfigured by loading stored configuration to perform the functions of the <b>faulty</b> <b>unit</b> and the register values of the <b>faulty</b> <b>unit</b> is forwarded to the reconfigured RHU. The test patterns and configurations should be stored in fast access non-volatile storage devices such as flash memory. This improved architecture will help to solve many fault tolerance issues with no visible loss of performance at low cost and space...|$|E
40|$|Long {{duration}} {{missions to}} the Moon and Mars pose {{a number of}} challenges to mission designers, controllers, and the crews. Among these challenges are planning for corrective maintenance actions which often require a repair. Current repair strategies on the International Space Station (ISS) rely primarily {{on the use of}} Orbital Replacement Units (ORUs), where a <b>faulty</b> <b>unit</b> is replaced with a spare, and the <b>faulty</b> <b>unit</b> typically returns to Earth for analysis and possible repair. The strategy of replace to repair has posed challenges even for the ISS program. Repairing faulty hardware at lower levels such as the component level can help maintain system availability in situations where no spares exist and potentially reduce logistic resupply mass. This report provides recommendations to help enable manual replacement of electronics at the component-level for future manned space missions. The recommendations include hardware, tools, containment options, and crew training. The recommendations are {{based on the work of}} the Component Level Electronics Assembly Repair (CLEAR) task of the Exploration Technology Development Program from 2006 to 2009. The recommendations are derived based on the experience of two experiments conducted by the CLEAR team aboard the International Space Station as well as a group of experienced Miniature/Microminiature (2 M) electronics repair technicians and instructors from the U. S. Navy 2 M Project Office. The emphasis of the recommendations is the physical repair. Fault diagnostics and post-repair functional test are discussed in other CLEAR reports...|$|E
40|$|An {{integrated}} {{modeling and}} analysis of error detection and recovery is presented. When fault latency and/or error latency exist, the system may suffer from multiple faults or error propagations which seriously deteriorate the fault-tolerant capability. Several detection models that enable analysis {{of the effect of}} detection mechanisms on the subsequent error handling operations and the overall system reliability were developed. Following detection of the <b>faulty</b> <b>unit</b> and reconfiguration of the system, the contaminated processes or tasks have to be recovered. The strategies of error recovery employed depend on the detection mechanisms and the available redundancy. Several recovery methods including the rollback recovery are considered. The recovery overhead is evaluated as an index of the capabilities of the detection and reconfiguration mechanisms...|$|E
40|$|Abstract—In this paper, we {{introduce}} a new model for diagnosable systems called t; k-diagnosable system which guarantees that at least k <b>faulty</b> <b>units</b> (processors) in a system are detected provided {{that the number of}} <b>faulty</b> <b>units</b> does not exceed t. This system includes classical one-step diagnosable systems and sequentially diagnosable systems. We prove a necessary and sufficient condition for 8 ̆ 5 t; k-diagnosable system, and discuss a lower bound for diagnosability. Finally, we deal with a relation between 8 ̆ 5 t; k-diagnosability and diagnosability of classical basic models. Index Terms—Fault diagnosis, PMC model, one-step t-diagnosis, sequential t-diagnosis, diagnosability, Cartesian product...|$|R
40|$|This {{publication}} {{is a work}} of the U. S. Government {{as defined}} in Title 17, United States Code, Section 101. As such, it is in the public domain, and under the provisions of Title 17, United States Code, Section 105, may not be copyrighted. Proceedings of the 26 th Annual Allerton Conference on Communication, Control, and Computing, Sept. 1988, regular (full) paper, pp. 408 - 416 (Unrefereed) We propose a distributed disabling algorithm for a multiprocessing system in which each processor or unit is prevented from doing computation when it fails some number of tests by other units. The goal is to disable all <b>faulty</b> <b>units</b> and to enable all fault-free units. Specifically, a unit is disabled if it fails d or more tests by enabled units (d-disabling rule). A multiprocessor system is c-correctable using the d-disabling rule if all <b>faulty</b> <b>units</b> are permanently disabled and all fault-free units are permanently enabled after a finite number of applications of the disabling rule, provided there are no more thna c <b>faulty</b> <b>units.</b> This models an unattended system where the removal of <b>faulty</b> <b>units</b> is done locally by simple and reliable circuitry. We give a sufficient condition for c-correctability in general systems and a necessary and sufficient condition in general systems where c < d. Then, we give necessary and sufficient conditions for c-correctability of two types of systems, (1) complete digraphs and (2) a new class of systems called segmented systems...|$|R
30|$|The present work {{considers}} {{the problem of}} building a connection assignment of the sensors in a WSN {{in order to ensure}} an energy-aware diagnosable system. The PMC model defines a system of n units as t-diagnosable if all <b>faulty</b> <b>units</b> can be diagnosed provided the number of <b>faulty</b> <b>units</b> does not exceed t [19] (t is also called the diagnosability of the system). In order to diagnose t units, the following conditions must hold: (c 1) the number n of units in the system must be {{greater than or equal to}} 2 t+ 1, and (c 2) a unit must be tested by at least t other units [19].|$|R
40|$|The {{collimation}} {{system in}} the beam cleaning insertion IR 7 of the future Large Hadron Collider (LHC) cleans the primary halo and the secondary radiation of a beam with unprecedented energy and intensity. Accidental beam losses can therefore entail severe consequences to the hardware of the machine. Thus, protection mechanisms, e. g. beam abort, must be instanta-neously triggered {{by a set of}} Beam Loss Monitors (BLM's). The readings in the BLM's couple the losses from various irradiated objects, which renders the identication of any <b>faulty</b> <b>unit</b> rather complex. In the present study the detailed geometry of IR 7 [1] was upgraded with the insertion of the BLM's, and the Monte Carlo FLUKA transport code was used to estimate the indi-vidual contribution of every collimator to the showers detected in each BLM. ...|$|E
40|$|As devices {{continue}} to scale, future shipped hardware will likely fail due to in-the-field hardware faults. As traditional redundancy-based hardware reliability solutions that tackle these faults {{will be too}} expensive to be broadly deployable, recent {{research has focused on}} low-overhead reliability solutions. One approach is to employ lowoverhead (“always-on”) detection techniques that catch high-level symptoms and pay a higher overhead for (rarely invoked) diagnosis. This paper presents trace-based fault diagnosis, a diagnosis strategy that identifies permanent faults in microarchitectural units by analyzing the faulty core’s instruction trace. Once a fault is detected, the faulty core is rolled back and re-executes from a previous checkpoint, generating a faulty instruction trace and recording the microarchitecture-level resource usage. A diagnosis process on another fault-free core then generates a fault-free trace which it compares with the faulty trace to identify the <b>faulty</b> <b>unit.</b> Our result shows that this approach successfully diagnoses 98 % of the faults studied and is a highly robust and flexible way for diagnosing permanent faults. 1...|$|E
40|$|This paper proposes two fault {{detection}} and diagnosis methods for VAV units without a sensor of supply air volume, {{and the results}} of applying these methods to a real building are presented. One method detects faults by applying a statistical method to four values calculated using the room air temperatures and the demand values of VAV damper opening of each unit during a steady state operation period. From the results of case studies, the method can reduce the number of units to be checked as faulty ones down to 12 % of the total number and all the units that really have a fault are included in this group. The other method judges the faults by applying dynamic system analysis to the operational data when the VAV system starts up. From the result of the case studies, the method can reduce the number of units down to 30 %, among which five units actually have a fault and only one <b>faulty</b> <b>unit</b> was not included in this group. Both methods can reduce time and cost for commissioning of VAV units significantly by the help of BEMS...|$|E
40|$|In {{the present}} paper, {{the concept of}} a hybrid fault {{situation}} is introduced, which specifies bounded combinations of permanently faulty and intermittently <b>faulty</b> <b>units</b> in a system. The general class of hybrid fault situations includes, as special cases, the all permanent fault case and the unrestricted intermittent fault case, which have been previously considered with PMC models. An approach compatible with the diagnosis of permanent fault situations is then applied to the diagnosis of hybrid fault situation. The motivation for doing so is the common practice of testing for the presence of intermittent faults in systems by means of repeated applications of tests that are designed for the detection of permanent faults. The testing assignment of PMC models of system is characterized, and interrelationships between the number of intermittently and permanently <b>faulty</b> <b>units</b> that can be diagnosed is established...|$|R
50|$|Distribution of the {{propulsion}} {{among the}} cars also {{results in a}} system that is less vulnerable to single-point-of-failure outages. Many classes of DMU are capable of operating with <b>faulty</b> <b>units</b> still in the consist. Because of the self-contained nature of diesel engines, {{there is no need to}} run overhead electric lines or electrified track, which can result in lower system construction costs.|$|R
40|$|Cascaded H-Bridge (CHB) {{converters}} can {{be directly}} connected to medium-voltage grids without using transformers and they possess the advantages of large capacity and low harmonics. They are significant tools for providing grid connections in large-capacity renewable energy systems. However, the reliability of a grid-connected CHB converter can be seriously influenced {{by the number of}} power switching devices that exist in the structure. This paper proposes a fault-tolerant control strategy based on double zero-sequence voltage injection and DC voltage optimization to improve the reliability of star-connected CHB converters after one or more power units have been bypassed. By injecting double zero-sequence voltages into each phase cluster, the DC voltages of the healthy units can be rapidly balanced after the <b>faulty</b> <b>units</b> are bypassed. In addition, optimizing the DC voltage increases the number of <b>faulty</b> <b>units</b> that can be tolerated and improves the reliability of the converter. Simulations and experimental results are shown for a seven-level three-phase CHB converter to validate the efficiency and feasibility of this strategy...|$|R
40|$|Miniaturized commercial-of-the-shelf {{components}} are very susceptible for space radiation. To provide nevertheless a reliable satellite performance, a concept based on hot redundant electronics and an “intelligent” watchdog using fault detection, isolation and recovery (FDIR) algorithms was realized. This principle proved its performance {{now for more}} than 3 years in orbit and so far no interruption was encountered. Realization with power-efficient microprocessors enables efficient implementation of redundancy concepts even at pico-satellite level (mass about 1 kg). At {{the example of the}} on-board-data-handling system (OBDH) of UWE- 3 a first version of this approach is in orbit since November 2013. The microprocessors as well as the storage units are running in hot redundancy. As soon as deviations in outputs are detected, a watchdog is activated to identify by its FDIR software the faulty component. In almost real-time it directs responsibility to the correct unit and starts a recovery/rebooting process for the <b>faulty</b> <b>unit.</b> All this is handled internally in the OBDH, thus the functionality of the subsystem is not interrupted by this process and the satellite provides a continuous nominal operation. Similar approaches are transferred to the electronics of other safety critical subsystems like AOCS and power control, increasing the reliability of the overall satellite system despite just employing commercial components...|$|E
40|$|An optimum {{operation}} scheduling {{method is}} reported regarding a {{fault of the}} thermal units as probabilistic. 0 ptimum operation is considered as which the reliability limit and the NO_x emission constraint can be economically satisfied. The reliability limit is satisfied by increasing kinds of the power supply states. The power supply is considered as possible only when every line’s capacity is satisfied. A strict method and an approximate method are introduced to estimate the change of thermal output power after a unit fault. The strict method is a simple application of the economic load dispatch. The approximate method is an economical dispatch of the output power of a <b>faulty</b> <b>unit</b> using the demand supply balance. From model system simulations where the number of thermal units is 5, the estimating speed of the approximate method is shown to be 15 {{times faster than the}} strict one. Estimating error of the approximate results is also shown to be small. Line capacity is satisfied by modifying the economical power flow. The changed value of the thermal output power is estimated to satisfy the line capacity by using an exceeding flow value from the capacity. Many simulated results are shown applying the proposed method to a model system. It is concretely shown that the proposed method can estimate the optimum operation to satisfy the various reliability limits and the various emission constraints...|$|E
40|$|The {{testing of}} logic {{circuits}} {{is accomplished by}} applying test stimuli and checking {{the response of the}} unit under test against that of the good unit. In compact testing the test response is compressed and compared against the 2 ̆ 2 signature 2 ̆ 2 of the good unit. The technique is particularly appropriate for field testing of bus oriented systems to isolate hardware failures down to the chip-level. ^ This thesis studies the use of the Rademacher-Walsh spectral techniques for compact testing of digital logic. These techniques are well known in the analog domain but their use in the digital domain is relatively recent. We identify the spectral properties which relate to testing and testable designs. It is shown that the parity of the function weight {{plays an important role in}} improving the testability of the circuit using the spectral techniques. ^ Both deterministic and random compact testing methods are considered. For deterministic testing several commonly applicable fault models are considered and guidelines provided for choosing one or two spectral coefficients for data compression. The results lead to a suggestion for improving the testability of two-level circuits, such as the programmable logic arrays (PLA 2 ̆ 7 s). ^ Two measures of performance of random compact testing are considered: the probability of rejecting a good unit and the probability of accepting a <b>faulty</b> <b>unit.</b> A new test scheme is proposed to improve the performance of random compact testing using a spectral coefficients, and a mathematical analysis is given for determining the performance. ...|$|E
40|$|This thesis {{consists}} of two relate but self-sustaining parts. In Part I a new diagnosability measure, t/ $-$ 1 -diagnosability, is proposed for interconnected systems. This new diagnosability assures that all <b>faulty</b> <b>units,</b> except for at most one, can be correctly identified in one step, {{as long as the}} total number of <b>faulty</b> <b>units</b> does not exceed t. The class of t/ $-$ 1 -diagnosable systems is fully characterized. A polynomial algorithm is presented for determining the degree of t/ $-$ 1 -diagnosability for any given system. A polynomial diagnosis algorithm is also given for any t/ $-$ 1 -diagnosable systems. It is shown that the degree of t/ $-$ 1 -diagnosability could be twice as large as the degree of t-diagnosability for a given system. In Part II a probabilistic diagnosis algorithm is presented for constant degree structures such as grids. It is shown that almost all <b>faulty</b> <b>units</b> can be correctly identified under a binomial failure distribution even when the probability of failure is rather high. The performance is very insensitive to yield variations under a negative binomial failure distribution. The application of this algorithm to the production testing of chips and wafers is explored. A simple test structure is provided for wafer testing, which utilizes the test access port of each die to facilitate comparison testing. The diagnosis algorithm is localized and incorporated into the test structure to determine the status of each die. The scheme is unique in that it is shown to work well when faults are clustered and even when the yield is low...|$|R
5000|$|... #Caption: The {{alleyway}} {{between the}} Forum 28 and Wilkinson where the <b>faulty</b> AC <b>unit</b> was located ...|$|R
40|$|A novel {{approach}} to the diagnosis of hypercubes, called Self-Validating Diagnosis, is introduced. An algorithm based on this approach, called SVD algorithm, is presented and evaluated. Given any fault set and the resulting syndrome, the algorithm returns a diagnosis and a syndrome-dependent bound, To, with the property that diagnosis is correct (although possibly incomplete) if {{the actual number of}} <b>faulty</b> <b>units</b> is less than To...|$|R
40|$|Army Cost Efficient Spaceflight Research Experiments and Demonstrations (ACES RED) is an iterative, {{periodic}} flight {{experiment and}} demonstration effort to test singular phenomenologies, technologies, and concepts for future (S 2 ̆ 6 T) {{projects that are}} directly related to and in support of the United States Army Space S 2 ̆ 6 T Roadmap Programs. The first ACES RED experiment’s main focus is to generously expand the available dataset to verify long-duration performance of the MAI- 400 attitude determination features and actuator durability as well as mature various commercial-off-the-shelf (COTS) technologies that will reduce the cost and complexity while maintaining performance of Army small satellites. This experiment will be mounted on the International Space Station with operation and access to continuous on-orbit data for greater than one year with reliable reference instrumentation. Realization with power-efficient microprocessors enables efficient implementation of redundancy concepts even at pico-satellite level (mass about 1 kg). At the example of the on-board-data-handling system (OBDH) of UWE- 3 a first version of this approach is in orbit since November 2013. The microprocessors as well as the storage units are running in hot redundancy. As soon as deviations in outputs are detected, a watchdog is activated to identify by its FDIR software the faulty component. In almost real-time it directs responsibility to the correct unit and starts a recovery/rebooting process for the <b>faulty</b> <b>unit.</b> All this is handled internally in the OBDH, thus the functionality of the subsystem is not interrupted by this process and the satellite provides a continuous nominal operation. Similar approaches are transferred to the electronics of other safety critical subsystems like AOCS and power control, increasing the reliability of the overall satellite system despite just employing commercial components...|$|E
40|$|We {{show that}} the {{adaptive}} diagnosis approach {{can be applied to}} probabilistically diagnosable systems. Three adaptive diagnosis algorithms are developed under both the symmetric and asymmetric test invalidation assumptions. A test selection strategy based on a probabilistic measure of test results is derived and used in each algorithm. We {{show that the}} adaptive algorithms are efficient in identifying all <b>faulty</b> <b>units</b> in a system...|$|R
40|$|This {{paper is}} devoted to study an {{iterative}} estimation/classification algorithm over a sensor network with <b>faulty</b> <b>units</b> recently appeared in the literature. We here present a complete analysis {{of the performance of}} the algorithm when the number of units goes to infinity both in terms of estimation and of classification error. In particular it is shown that the algorithm solution converges to the optimal Maximum Likelihood estimato...|$|R
40|$|We {{consider}} {{problems of}} fault diagnosis in multiprocessor systems. Preparata, Metze and Chien (1967) introduced a graph theoretical model for system-level diagnosis, in which processors perform tests {{on one another}} via links in the system. Faultfree processors correctly identify the status of tested processors, while the faulty processors can give arbitrary test results. The goal is to identify faulty processors based on the test results. A system {{is said to be}} t-diagnosable if <b>faulty</b> <b>units</b> can be identified, provided the number of <b>faulty</b> <b>units</b> present does not exceed t. We explore here diagnosis problems for n-cube systems and give bounds for diagnosability of the n-cube. We also describe a simple diagnosis algorithm A which is linear in time and which can be used for sequential diagnosis as well as for incomplete diagnosis in one step. In particular the algorithm applied to arbitrary topology based interconnection systems G with N processors improves previously known ones. It has sequential diagnosability tA(G) ≥ ⌈ 2 N 1 2 ⌉ − 3, which is optimal in the worst case. ...|$|R
40|$|In {{this paper}} we {{introduce}} a self-diagnosis algorithm for hypercube-connected systems. The algorithm produces a diagnosis which is provably correct {{if the number}} of <b>faulty</b> <b>units</b> in the system is less than a thereshold T&# 417; asserted by the algorithm itself. Although the diagnosis may be incomplete, simulations show that the expected number of unidentified units is very small. The application of the diagnosis strategy to the manufacturing test of VLSI chips is also considered...|$|R
40|$|AbstractWe {{consider}} {{problems of}} fault diagnosis in multiprocessor systems. Preparata, Metze and Chien [F. P. Preparata, G. Metze, R. T. Chien, On the connection assignment problem of diagnosable systems, IEEE Trans. Comput. EC 16 (12) (1967) 848 – 854] introduced a graph theoretical model for system-level diagnosis, in which processors perform tests {{on one another}} via links in the system. Fault-free processors correctly identify the status of tested processors, while the faulty processors can give arbitrary test results. The goal is to identify faulty processors based on the test results. A system {{is said to be}} t-diagnosable if <b>faulty</b> <b>units</b> can be identified, provided the number of <b>faulty</b> <b>units</b> present does not exceed t. We explore here diagnosis problems for n-cube systems and give bounds for diagnosability of the n-cube. We also describe a simple diagnosis algorithm A which is linear in time and which can be used for sequential diagnosis as well as for incomplete diagnosis in one step. In particular, the algorithm applied to arbitrary topology based interconnection systems G with N processors improves previously known ones. It has sequential diagnosability tA(G) ≥⌈ 2 N 12 ⌉− 3, which is optimal in the worst case...|$|R
