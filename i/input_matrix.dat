646|878|Public
5000|$|The {{product of}} [...] and [...] is a matrix with 10000 rows and 500 columns, the same shape as the <b>input</b> <b>matrix</b> [...] and, if the {{factorization}} worked, it {{is a reasonable}} approximation to the <b>input</b> <b>matrix</b> [...]|$|E
5000|$|Sparse matrix-vector {{multiplication}} (SpMV) of {{the form}} [...] is a widely used computational kernel existing in many scientific applications.The <b>input</b> <b>matrix</b> [...] is sparse. The input vector [...] and the output vector [...] are dense.In case of repeated [...] operation involving the same <b>input</b> <b>matrix</b> [...] but possibly changing numerical values of its elements, [...] can be preprocessed to reduce both the parallel and sequential run time of the SpMV kernel.|$|E
5000|$|It is {{possible}} to find a low rank approximation to the LU decomposition using a randomized algorithm. Given an <b>input</b> <b>matrix</b> [...] and a desired low rank , the randomized LU returns permutation matrices [...] and lower/upper trapezoidal matrices [...] of size [...] and [...] respectively, such that with high probability , where [...] is a constant {{that depends on the}} parameters of the algorithm and [...] is the th singular value of the <b>input</b> <b>matrix</b> [...]|$|E
40|$|We propose {{several new}} {{schedules}} for Strassen-Winograd's matrix multiplication algorithm, they reduce the extra memory allocation requirements by three different means: by introducing a few pre-additions, by overwriting the <b>input</b> <b>matrices,</b> or {{by using a}} first recursive level of classical multiplication. In particular, we show two fully in-place schedules: one having {{the same number of}} operations, if the <b>input</b> <b>matrices</b> can be overwritten; the other one, slightly increasing the constant of the leading term of the complexity, if the <b>input</b> <b>matrices</b> are read-only. Many of these schedules have been found by an implementation of an exhaustive search algorithm based on a pebble game...|$|R
3000|$|Data for the <b>input</b> {{coefficient}} <b>matrices</b> {{representing the}} production {{structure of the}} Japanese economy in 1995 and 2005 {{were obtained from the}} fixed-price tables of the Japanese Linked Input–Output Tables (SB-MIAC [2011]). This particular study period is chosen because it is covered by the latest available linked IOTs. In addition to these <b>input</b> coefficient <b>matrices,</b> we also utilized “trimmed” versions of the <b>input</b> coefficient <b>matrices</b> to check the robustness of our analysis. The trimmed <b>input</b> coefficient <b>matrix</b> was constructed by setting [...]...|$|R
3000|$|... {{where we}} {{generalized}} f {{to act on}} the columns of its <b>input</b> <b>matrices.</b> Apparently, the EnKF time update amounts to a one-step-ahead simulation of X [...]...|$|R
5000|$|The {{state-of-the-art}} guaranteed algorithm for {{the robust}} PCA problem (with the <b>input</b> <b>matrix</b> being [...] ) is an alternating minimization type algorithm. The computational complexity is [...] where the input is the superposition of a low-rank (of rank [...] ) and a sparse matrix of dimension [...] and [...] is the desired {{accuracy of the}} recovered solution, i.e., [...] where [...] is the true low-rank component and [...] is the estimated or recovered low-rank component. Intuitively, this algorithm performs projections of the residual on to the set of low-rank matrices (via the SVD operation) and sparse matrices (via entry-wise hard thresholding) in an alternating manner - that is, low-rank projection of the difference the <b>input</b> <b>matrix</b> and the sparse matrix obtained at a given iteration followed by sparse projection of the difference of the <b>input</b> <b>matrix</b> and the low-rank matrix obtained in the previous step, and iterating the two steps until convergence.|$|E
5000|$|Assume an <b>input</b> <b>matrix</b> of 3×3 pixels {{where the}} center most pixel is the pixel to be scaled, and an output matrix of 2×2 pixels (i.e., the scaled pixel) ...|$|E
5000|$|The {{realization}} of the causal Wiener filter {{looks a lot like}} the solution to the least squares estimate, except in the signal processing domain. The least squares solution, for <b>input</b> <b>matrix</b> [...] and output vector is ...|$|E
50|$|The best {{classical}} algorithm known runs {{polynomial time}} in the worst case.The quantum algorithm provides a quadratic improvement in the general case, and an exponential improvement when the <b>input</b> <b>matrices</b> are of low rank.|$|R
40|$|We {{present a}} novel three-step modified-signed-digit optical {{computing}} {{system based on}} the disrete correlation of space-coded <b>input</b> <b>matrices</b> that are multiply-imaged by using optical fan-out elements. The advantages of our method over other approaches include a larger throughput, more flexible operation, and an easier optical implementation...|$|R
40|$|Optimizing the <b>input</b> {{covariance}} <b>matrix</b> of a multiple-antenna transmit {{system with}} partial channelstructure feedback {{is an important}} issue to fully exploit the channel capacity. Efficient design of the optimal <b>input</b> covariance <b>matrix,</b> however, remains unavailable although its eigenvector structure was clearly revealed in a recent publication. In this paper, we obtain an explicit derivative function forming a solid basis for optimizing the optimal <b>input</b> covariance <b>matrix.</b> This new derivative expression enables us to further develop an efficient iterative algorithm for determining the optimal eigenvalues. The technique is illustrated through numerical examples...|$|R
50|$|The {{decision}} {{version of}} the travelling salesman problem is in NP. Given an <b>input</b> <b>matrix</b> of distances between n cities, the problem is {{to determine if there}} is a route visiting all cities with total distance less than k.|$|E
50|$|There {{are many}} {{algorithms}} for computing the Hermite normal form {{dating back to}} 1851. It was not until 1979 when an algorithm for computing the Hermite normal form that ran in strongly polynomial time was first developed; that is, the number of steps to compute the Hermite normal form is bounded above by a polynomial in {{the dimensions of the}} <b>input</b> <b>matrix,</b> and the space used by the algorithm (intermediate numbers) is bounded by a polynomial in the binary encoding size of the numbers in the <b>input</b> <b>matrix.</b> One class of algorithms is based on Gaussian elimination in that special elementary matrices are repeatedly used. The LLL algorithm {{can also be used to}} efficiently compute the Hermite normal form.|$|E
5000|$|Consider {{a matrix}} [...] {{to be learned}} from a set of examples, , where [...] goes from [...] to , and [...] goes from [...] to [...] Let each <b>input</b> <b>matrix</b> [...] be , and let [...] be of size [...] A general model for the output [...] can be posed as ...|$|E
40|$|This paper {{establishes}} a `turnpike theorem' for a closed linear model of production with a primitive <b>input</b> requirement <b>matrix.</b> Optimal programs of resource allocation have a `turnpike property' if the growth factor of every {{sector in the}} economy converges, in the long run, to a common value. The usefulness of such a theorem {{is due to the}} fact that the <b>input</b> requirement <b>matrix</b> for an economy with a large number of goods may be primitive (some power of the matrix is strictly positive). Turnpike theorem, <b>Input</b> output <b>matrix.</b> ...|$|R
40|$|We {{consider}} {{the problem of}} approximate joint triangularization {{of a set of}} noisy jointly diagonalizable real matrices. Approximate joint triangularizers are commonly used in the estimation of the joint eigenstructure of a set of matrices, with applications in signal processing, linear algebra, and tensor decomposition. By assuming the <b>input</b> <b>matrices</b> to be perturbations of noise-free, simultaneously diagonalizable ground-truth matrices, the approximate joint triangularizers are expected to be perturbations of the exact joint triangularizers of the ground-truth matrices. We provide a priori and a posteriori perturbation bounds on the `distance' between an approximate joint triangularizer and its exact counterpart. The a priori bounds are theoretical inequalities that involve functions of the ground-truth matrices and noise matrices, whereas the a posteriori bounds are given in terms of observable quantities that can be computed from the <b>input</b> <b>matrices.</b> From a practical perspective, the problem of finding the best approximate joint triangularizer of a set of noisy matrices amounts to solving a nonconvex optimization problem. We show that, under a condition on the noise level of the <b>input</b> <b>matrices,</b> it is possible to find a good initial triangularizer such that the solution obtained by any local descent-type algorithm has certain global guarantees. Finally, we discuss the application of approximate joint matrix triangularization to canonical tensor decomposition and we derive novel estimation error bounds. Comment: 19 page...|$|R
5000|$|... {{leading to}} the {{algebraic}} expression QM v for the composed output from v <b>input.</b> The <b>matrix</b> transformations mount up to the left in this use of a column vector for <b>input</b> to <b>matrix</b> transformation. The natural bias to read left-to-right, as subsequent transformations are applied in linear algebra, stands against column vector inputs.|$|R
5000|$|Let the <b>input</b> <b>matrix</b> (the matrix to be factored) be [...] with 10000 {{rows and}} 500 columns where words are in rows and {{documents}} are in columns. That is, we have 500 documents indexed by 10000 words. It follows that a column vector [...] in [...] represents a document.|$|E
5000|$|... {{where the}} {{subscript}} [...] {{indicates that the}} system is faulty. This approach models multiplicative faults by modified system matrices. Specifically, actuator faults are represented by the new <b>input</b> <b>matrix</b> , sensor faults are represented by the output map , and internal plant faults are represented by the system matrix [...]|$|E
50|$|During the {{execution}} of Bareiss algorithm, every integer that is computed is the determinant of a submatrix of the <b>input</b> <b>matrix.</b> This allows, using the Hadamard inequality, to bound the size of these integers. Otherwise, the Bareiss algorithm {{may be viewed as}} a variant of Gaussian elimination and needs roughly the same number of arithmetic operations.|$|E
40|$|In this work, {{we present}} a novel {{continuous}} robust controller for a class of multi-input/multi-output nonlinear systems that contains unstructured uncertainties in their drift vectors and <b>input</b> <b>matrices.</b> The proposed controller compensates uncertainties in the system dynamics and achieves asymptotic tracking while requiring only {{the knowledge of the}} sign of the leading principal minors of the <b>input</b> gain <b>matrix.</b> A Lyapunov-based argument backed up with an integral inequality is applied to prove the asymptotic stability of the closed-loop system. Simulation results are presented to illustrate the viability of the proposed method. Scientific and Technological Research Council of Turkey (113 E 147...|$|R
5000|$|Let the [...] {{denote the}} <b>input</b> data <b>matrix,</b> [...] {{the number of}} columns {{corresponding}} {{with the number of}} samples of mixed signals and [...] the number of rows corresponding with the number of independent source signals. The <b>input</b> data <b>matrix</b> [...] must be prewhitened, or centered and whitened, before applying the FastICA algorithm to it.|$|R
30|$|For {{comparison}} purposes, we {{have used}} the Bewoulf B 1 since it uses a configuration {{that is similar to}} the other implementations [16, 17]. Our implementation (Bewoulf B 1) also considers <b>input</b> <b>matrices</b> that include the same input sizes used in [16, 17], namely, 480 × 480, 960 × 960, and 1, 920 × 1, 920.|$|R
5000|$|More formally, a set [...] {{is called}} a (combinatorial) {{rectangle}} if whenever [...] and [...] then [...] Equivalently, R can also {{be viewed as a}} submatrix of the <b>input</b> <b>matrix</b> A such that [...] where [...] and [...] Consider the case when [...] bits are already exchanged between the parties. Now, for a particular , let us define a matrix ...|$|E
5000|$|The {{training}} set {{is defined as}} , where [...] is the [...] <b>input</b> <b>matrix</b> and [...] is the output vector. Where applicable, the kernel function is denoted by , and the [...] kernel matrix is denoted by [...] which has entries [...] and [...] denotes the Reproducing Kernel Hilbert Space (RKHS) with kernel [...] The regularization parameter is denoted by [...]|$|E
5000|$|Using {{the above}} definition, {{it is useful}} to think of the {{function}} [...] as a matrix [...] (called the <b>input</b> <b>matrix)</b> where each row of the matrix corresponds to [...] and each column corresponds to [...] An entry in the <b>input</b> <b>matrix</b> is [...] Initially both Alice and Bob have a copy of the entire matrix A (assuming the function [...] is known to both). Then, the problem of computing the function value can be rephrased as [...] "zeroing-in" [...] on the corresponding matrix entry. This problem can be solved if either Alice or Bob knows both [...] and [...] At the start of communication, the number of choices for the value of the function on the inputs is the size of matrix, i.e[...] Then, as and when each party communicates a bit to the other, the number of choices for the answer reduces as this eliminates a set of rows/columns resulting in a submatrix of A.|$|E
3000|$|The MV {{adaptive}} beamforming algorithm {{starts with}} the <b>input</b> <b>matrices</b> which contain unaligned ultrasonic echo signals {{from a series of}} ultrasonic echo channels. The number of the ultrasonic echo channels is denoted as M, which is also called the receive aperture of the ultrasonic echo signals. In order to obtain the aligned (M× 1) echo signal vector e [...]...|$|R
40|$|Using the {{notation}} of our original paper,' we summarize differ- ent techniques by (nXn) <b>input</b> <b>matrices</b> a, b, c,... and the cor- responding labor requirement vectors ao, bo, [...] . When one sets the wage w= 1 as a normalization, the price vector Pa, {{associated with any}} technique a, {{is a function of}} the interest rate r: [...] ...|$|R
40|$|The MAP {{inference}} {{problem in}} many graphical models {{can be solved}} efficiently using a fast algorithm for computing min-sum products of n × n matrices. The class of models in question includes cyclic and skip-chain models that arise in many applications. Although the worst-case complexity of the min-sum product operation is not known to be much better than O(n 3), an O(n 2. 5) expected time algorithm was recently given, subject to some constraints on the <b>input</b> <b>matrices.</b> In this paper we give an algorithm that runs in O(n 2 log n) expected time, assuming that the entries in the <b>input</b> <b>matrices</b> are independent samples from a uniform distribution. We also show that two variants of our algorithm are quite fast for inputs that arise in several applications. This leads to significant performance gains over previous methods in applications within computer vision and natural language processing. ...|$|R
5000|$|Cannon's algorithm, {{also known}} as the 2D algorithm, {{partitions}} each <b>input</b> <b>matrix</b> into a block matrix whose elements are submatrices of size [...] by , where [...] is the size of fast memory. The naïve algorithm is then used over the block matrices, computing products of submatrices entirely in fast memory. This reduces communication bandwidth to , which is asymptotically optimal (for algorithms performing [...] computation).|$|E
50|$|The causal finite impulse {{response}} (FIR) Wiener filter, {{instead of using}} some given data matrix X and output vector Y, finds optimal tap weights by using the statistics of the input and output signals. It populates the <b>input</b> <b>matrix</b> X with estimates of the auto-correlation of the input signal (T) and populates the output vector Y with estimates of the cross-correlation between the output and input signals (V).|$|E
50|$|This method {{operates}} on distance data, computes {{a transformation of}} the <b>input</b> <b>matrix</b> and then computes the minimum distance of the pairs of languages. It operates correctly even if the languages do not evolve with a lexical clock. A weighted version of the method may also be used. The method produces an output tree. It is {{claimed to be the}} closest method to manual techniques for tree construction.|$|E
40|$|The {{model-free}} {{control of}} aeroelastic vibrations of a non-linear 2 -D wing-flap system operating in supersonic flight speed regimes {{is discussed in}} this paper. A novel continuous robust controller design yields asymptotically stable vibration suppression in both the pitching and plunging degrees of freedom using the flap deflection as a control input. The controller also ensures that all system states remain bounded at all times during closed-loop operation. A Lyapunov method is used to obtain the global asymptotic stability result. The unsteady aerodynamic load is considered by resourcing to the non-linear Piston Theory Aerodynamics (PTA) modified {{to account for the}} effect of the flap deflection. Simulation results demonstrate the performance of the robust control strategy in suppressing dynamic aeroelastic instabilities, such as non-linear flutter and limit cycle oscillations. Key words: Nonlinear aeroelastic control, model-free control, robust and neural control, Supersonic aerodynamic Nomenclature A, G, Gd State and <b>input</b> <b>matrices</b> Ac, Bc State and <b>input</b> <b>matrices</b> of transformed system A*c Matrix of the system zero dynamic...|$|R
40|$|In this thesis, a novel {{method for}} control of non-square {{dynamical}} systems using a model following approach is developed. Control methodologies such as dynamic inversion and sliding mode control require an inversion of the <b>input</b> influence <b>matrix.</b> However, if the system <b>input</b> influence <b>matrix</b> is non-square direct inversion is not possible. Pseudo inversion of the <b>input</b> influence <b>matrix</b> may be performed for control allocation. However, pseudo inversion limits the control to states where the controller is directly applied. The pseudoinverse method does not permit the engineer to designate a particular state to control or track. When accurate tracking of states that are not directly controlled (“remaining states”) is required the pseudo inversion method is not useful. Current methods such as dynamic extension {{can be used to}} generate a square <b>input</b> influence <b>matrix,</b> essentially, creating an <b>input</b> influence <b>matrix</b> that is invertible. However, this method is tedious for large systems. In this work, a new transformation is applied to the original dynamical system model to develop an <b>input</b> influence <b>matrix</b> that is square. Assuming the system is controllable, the proposed transformation allows for accurate tracking of selectable states. Selection of the new transformation matrix is used to develop accurate tracking of certain states compared to the remaining states. A method based on optimal control theory is used to define the transformation matrix. The new approach is first applied to control a two mass system with simulation results presented showing the advantage of the proposed new control strategy. Finally, simulation results are presented for longitudinal control of an aircraft using one control input...|$|R
40|$|Recently, Pagh {{presented}} a randomized approximation algorithm for the multiplication of real-valued matrices building upon work for detecting {{the most frequent}} items in data streams. We continue this line of research and present new deterministic matrix multiplication algorithms. Motivated by applications in data mining, we first {{consider the case of}} real-valued, nonnegative n-by-n <b>input</b> <b>matrices</b> A and B, and show how to obtain a deterministic approximation of the weights of individual entries, as well as the entrywise p-norm, of the product AB. The algorithm is simple, space efficient and runs in one pass over the <b>input</b> <b>matrices.</b> For a user defined b ∈ (0, n 2) the algorithm runs in time O(nb+ n · Sort(n)) and space O(n+ b) and returns an approximation of the entries of AB within an additive factor of ‖AB‖E 1 /b, where ‖C‖E 1 =∑i,j |Cij | is the entrywise 1 -norm of a matrix C and Sort(n) is the time required to sort n real numbers in linear space. Building upon a result by Berinde et al. we show that for skewed matrix products (a common situation in many real-life applications) the algorithm is more efficient and achieves better approximation guarantees than previously known randomized algorithms. When the <b>input</b> <b>matrices</b> are not restricted to nonnegative entries, we present a new deterministic group testing algorithm detecting nonzero entries in the matrix product with large absolute value. The algorithm is clearly outperformed by randomized matrix multiplication algorithms, but as a byproduct we obtain the first O(n 2 +ε) -time deterministic algorithm for matrix products with O(n) nonzero entries. ...|$|R
