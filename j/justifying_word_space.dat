0|904|Public
5000|$|The 2008 {{edition of}} the Government Printing Office's (GPO) Style Manual is unequivocal in its {{guidance}} regarding this convention: [...] "A single <b>justified</b> <b>word</b> <b>space</b> will be used between sentences. This applies to all types of composition." ...|$|R
5000|$|Justification of text. If {{the text}} is not <b>justified,</b> the <b>word</b> <b>spacing</b> is fixed and so only the {{protrusion}} elements of microtypography {{are likely to be}} useful.|$|R
5000|$|<b>Word</b> <b>spaces,</b> {{preceding}} or following punctuation, {{should be}} optically adjusted {{to appear to}} be of the same value as a standard <b>word</b> <b>space.</b> If a standard <b>word</b> <b>space</b> is inserted after a full point or a comma, then, optically, this produces a space of up to 50% wider than that of other <b>word</b> <b>spaces</b> within a line of type. This is because these punctuation marks carry space above them, which, when added to the adjacent standard <b>word</b> <b>spaces,</b> combines to create a visually larger space. Some argue that the [...] "additional" [...] space after a comma and full point serves as a [...] "pause signal" [...] for the reader. But this is unnecessary (and visually disruptive) since the pause signal is provided by the punctuation mark itself.|$|R
40|$|Dimensionality {{reduction}} {{has been}} shown to improve processing and information extraction from high dimensional data. <b>Word</b> <b>space</b> algorithms typically employ linear reduction techniques that assume the space is Euclidean. We investigate the effects of extracting nonlinear structure in the <b>word</b> <b>space</b> using Locality Preserving Projections, a reduction algorithm that performs manifold learning. We apply this reduction to two common <b>word</b> <b>space</b> models and show improved performance over the original models on benchmarks. ...|$|R
40|$|We {{present the}} S-Space Package, an open source {{framework}} for developing and evaluating <b>word</b> <b>space</b> algorithms. The package implements well-known <b>word</b> <b>space</b> algorithms, such as LSA, {{and provides a}} comprehensive set of matrix utilities and data structures for extending new or existing models. The package also includes <b>word</b> <b>space</b> benchmarks for evaluation. Both algorithms and libraries are designed for high concurrency and scalability. We demonstrate {{the efficiency of the}} reference implementations and also provide their results on six benchmarks. ...|$|R
50|$|Open Juncture {{is marked}} by <b>word</b> <b>space.</b>|$|R
40|$|Abstract — A {{hierarchical}} model incorporating motion patterns, proto symbols and words is proposed. The proto symbols abstract motion patterns, while {{the words are}} associated with the proto symbols stochastically. This paper describes the construction of a <b>word</b> <b>space,</b> where <b>words</b> are located in a multidimensional space based on dissimilarities among the words. The dissimilarity between two words can be calculated by using association probabilities that the words generate motion proto symbols. The <b>word</b> <b>space</b> encapsulates relations among the words such as similar or dissimilar pairs of <b>words.</b> The <b>word</b> <b>space</b> also allows motion recognition based on words. The validity of the constructed <b>word</b> <b>space</b> is demonstrated on a motion capture database. Moreover, the addition of the word associations is found to change the conventional proto symbol space so that the discrimination among the proto symbols is improved. I...|$|R
2500|$|... 1999: —modern {{mass-production}} commercial printing: {{a single}} <b>word</b> <b>space</b> between sentences ...|$|R
5000|$|One widened space, {{typically}} one-and-a-third {{to slightly}} less than twice as wide as a <b>word</b> <b>space.</b> This spacing was sometimes used in typesetting before the 19th century. It has also been used in other non-typewriter typesetting systems such as the Linotype machine and the TeX system. Modern computer-based digital fonts can adjust the spacing after terminal punctuation as well, creating a space slightly wider than a standard <b>word</b> <b>space.</b>|$|R
50|$|Language {{can also}} be a factor that typographers would take into {{consideration}} for <b>word</b> <b>spacing.</b> For a language like Latin, “most boundaries are marked by grammatical tags, and a smaller space is therefore sufficient”. In English, the ability to read a line easily, instead of needing {{to make sense of it}} first, is also attributed by good <b>word</b> <b>spacing.</b>|$|R
5000|$|<b>Word</b> <b>spacing</b> in {{typography}} {{refers to}} the size of the <b>space</b> between <b>words.</b> It should be distinguished from letter-spacing (the spacing between the letters within each <b>word)</b> and sentence <b>spacing</b> (the spacing between sentences). Typographers may modify the spacing of letters or words in a body of type to aid readability and copy fit, or for aesthetic effect. In web browsers and standardized digital typography the <b>word</b> <b>spacing</b> is controlled by the CSS1 word-spacing property.|$|R
5000|$|... 1999: the Badger-in-the-bag game—modern {{mass-production}} commercial printing: {{a single}} <b>word</b> <b>space</b> between sentences ...|$|R
40|$|Abstract. We {{describe}} {{a system to}} tackle the Lexical Substitution task that exploits, as its only resource, co-occurrence statistics from a large PoS-tagged corpus. The system exploits the <b>word</b> <b>space</b> model formalism, and represents the word to be substituted by a composite vector {{that takes into account}} both the overall distribution of the word in the input corpus and its local context. As far as the precision and recall are concerned, the system is ranked among the highest positions in the Evalita competition, while it results winner in the mode p and mode r ranking. Key words: <b>word</b> <b>space</b> models, composition in <b>word</b> <b>space</b> models, corpus-based semantics...|$|R
40|$|We {{present the}} results of {{clustering}} experiments {{with a number of}} different evaluation sets using dependency based <b>word</b> <b>spaces.</b> Contrary to previous results we found a clear advantage using a parsed corpus over <b>word</b> <b>spaces</b> constructed with the help of simple patterns. We achieve considerable gains in performance over these spaces ranging between 9 and 13 % in absolute terms of cluster purity. ...|$|R
40|$|<b>Word</b> <b>space</b> models, in {{the sense}} of vector space models built on {{distributional}} data taken from texts, are used to model semantic relations between words. We argue that the high dimensionality of typical vector space models lead to unintuitive effects on modeling likeness of meaning and that the local structure of <b>word</b> <b>spaces</b> is where interesting semantic relations reside. We show that the local structure of <b>word</b> <b>spaces</b> has substantially different dimensionality and character than the global space and that this structure shows potential to be exploited for further semantic analysis using methods for local analysis of vector space structure rather than globally scoped methods typically in use today such as singular value decomposition or principal component analysis. ...|$|R
40|$|This paper {{proposes a}} <b>word</b> <b>spacing</b> model using a hidden Markov model (HMM) for re ning Korean raw text corpora. Previous {{statistical}} approaches for automatic <b>word</b> <b>spacing</b> have used models that {{make use of}} inaccurate probabilities {{because they do not}} consider the previous spacing state. We consider <b>word</b> <b>spacing</b> problem as a classi cation problem such as Part-of-Speech (POS) tagging and have experimented with various models considering extended context. Experimental result shows that the performance of the model becomes better as the more context considered. In case of the same number of parameters are used with other method, it is proved that our model is more eective by showing the better results...|$|R
50|$|Distributional {{semantic}} {{models that}} use linguistic items as context {{have also been}} referred to as <b>word</b> <b>space</b> models.|$|R
40|$|In {{collaboration}} with Gavagai, {{a company that}} develops automated and scalable methods for retrieving actionable intelligence from dynamic data, I have been studying semantic <b>word</b> <b>spaces</b> and topology. In this bachelor’s thesis, with help from computational topology, I introduce new ways to describe properties of these semantic <b>word</b> <b>spaces,</b> so called barcodes. I develop a measure to describe barcodes of betti number zero, prove its validity and discuss its implications...|$|R
40|$|Abstract. In this paper, we {{will analyze}} the {{behavior}} of several parameters, namely type of contexts, similarity measures, and <b>word</b> <b>space</b> models, in the task of word similarity extraction from large corpora. The main objective of the paper will be to describe experiments comparing different extraction systems based on all possible combinations of these parameters. Special attention will {{be paid to the}} comparison between syntax-based contexts and windowing techniques, binary similarity metrics and more elaborate coefficients, as well as baseline <b>word</b> <b>space</b> models and Singular Value Decomposition strategies. The evaluation leads us to conclude that the combination of syntax-based contexts, binary similarity metrics, and a baseline <b>word</b> <b>space</b> model makes the extraction much more precise than other combinations with more elaborate metrics and complex models. ...|$|R
40|$|Recent {{work has}} pointed out the differ-ence between the {{concepts}} of semantic similarity and semantic relatedness. Im-portantly, some NLP applications depend on measures of semantic similarity, while others work better with measures of se-mantic relatedness. It has also been ob-served that methods of computing simi-larity measures from text corpora produce <b>word</b> <b>spaces</b> that are biased towards either semantic similarity or relatedness. De-spite these findings, there has been lit-tle work that evaluates the effect of vari-ous techniques and parameter settings in the <b>word</b> <b>space</b> construction from corpora. The present paper experimentally investi-gates how the choice of context, corpus preprocessing and size, and dimension re-duction techniques like singular value de-composition and frequency cutoffs influ-ence the semantic properties of the result-ing <b>word</b> <b>spaces.</b> ...|$|R
5000|$|... a <b>word</b> <b>space,</b> or letter space (the <b>space</b> {{between two}} <b>words</b> on a line, two letter spaces being ##) ...|$|R
50|$|Improve hand-eye {{coordination}} in writing following writing skills, e.g., clockwise and anti clockwise circling, angular writing, keeping symmetry and inter <b>word</b> <b>spacing.</b>|$|R
5000|$|.....French spacing. The {{insertion}} of fixed space {{such as an}} en or an em between sentences instead of a variable <b>word</b> <b>space.</b>|$|R
50|$|Marks of {{omission}} should {{consist of}} three full points. These {{should be set}} without any spaces, but be preceded and followed by <b>word</b> <b>spaces.</b>|$|R
50|$|Instead of em rules without spaces, use en rules preceded and {{followed}} by the <b>word</b> <b>space</b> of the line, as in the third paragraph above.|$|R
25|$|Additional {{space at}} the ends of {{sentences}} is called French spacing,... In typesetting, a thin space set in addition to the <b>word</b> <b>space</b> achieves French spacing.|$|R
5000|$|<b>Word</b> <b>spacing</b> {{is crucial}} for the written form because it {{illustrates}} the sound of speech where audible gaps or pauses take place [...] With typography, <b>word</b> <b>spacing</b> shows this unspoken aspect of speech. Otherwise, {{it would be difficult}} for people to read one long continuous line of letters. It is hard to determine how much spacing should be put in between words, but a good typographer is able to determine proper spacing. When text and spacing are consistent, this makes it easier to read.|$|R
25|$|Syriac {{is written}} {{from right to}} left in {{horizontal}} lines. It is a cursive script, but not all letters connect within a <b>word.</b> <b>Spaces</b> separate individual <b>words.</b>|$|R
50|$|In other <b>words,</b> <b>space</b> {{and time}} are {{a form of}} perceiving and {{causality}} {{is a form of}} knowing. Both space and time and conceptual principles and processes pre-structure experience.|$|R
40|$|Abstract. <b>Word</b> <b>space</b> models, in {{the sense}} of vector space models built on {{distributional}} data taken from texts, are used to model semantic relations between words. We argue that the high dimensionality of typical vector space models lead to unintuitive effects on modeling likeness of meaning and that the local structure of <b>word</b> <b>spaces</b> is where interesting semantic relations reside. We show that the local structure of <b>word</b> <b>spaces</b> has substantially different dimensionality and character than the global space and that this structure shows potential to be exploited for further semantic analysis using methods for local analysis of vector space structure rather than globally scoped methods typically in use today such as singular value decomposition or principal component analysis. Vector space models Vector space models are frequently used in information access, both for research experiments and as a building block for systems in practical use. There are numerous implementations of methods for modeling topical variation in text usin...|$|R
40|$|An {{automatic}} <b>word</b> <b>spacing</b> {{is one of}} {{the important}} tasks in Korean language processing and information retrieval. Since {{there are a number of}} confusing cases in <b>word</b> <b>spacing</b> of Korean, there are some mistakes in many texts including news articles. This paper presents a high-accurate method for automatic <b>word</b> <b>spacing</b> based on self-organizing-gram model. This method is basically a variant of-gram model, but achieves high accuracy by automatically adapting context size. In order to find the optimal context size, the proposed method automatically increases the context size when the contextual distribution after increasing it dose not agree with that of the current context. It also decreases the context size when the distribution of reduced context is similar to that of the current context. This approach achieves high accuracy by considering higher dimensional data in case of necessity, and the increased computational cost are compensated by the reduced context size. The experimental results show that the self-organizing structure of-gram model enhances the basic model. ...|$|R
40|$|Children's eye {{movements}} were recorded {{to examine the}} role of <b>word</b> <b>spacing</b> and positional character frequency {{on the process of}} Chinese lexical acquisition during reading. Three types of two-character novel pseudowords were constructed: words containing characters in positions in which they frequently occurred (congruent), words containing characters in positions they do not frequently occur in (incongruent) and words containing characters that do not have a strong position bias (balanced). There were two phases within the experiment, a learning phase and a test phase. There were also two learning groups: half the children read sentences in a word-spaced format and the other half read the sentences in an unspaced format during the learning phase. All the participants read normal, unspaced text at test. A benefit of <b>word</b> <b>spacing</b> was observed in the learning phase, but not at test. Also, facilitatory effects of positional character congruency were found both in the learning and test phase; however, this benefit was greatly reduced at test. Furthermore, we did not find any interaction between <b>word</b> <b>spacing</b> and positional character frequencies, indicating that these two types of cues affect lexical acquisition independently. With respect to theoretical accounts of lexical acquisition, we argue that <b>word</b> <b>spacing</b> might facilitate the very earliest stages of word learning by clearly demarking word boundary locations. In contrast, we argue that characters' positional frequencies might affect relatively later stages of word learnin...|$|R
40|$|Application of Random Indexing (RI) to {{extractive}} text summarization {{has already}} been proposed in literature. RI is an approximating technique to deal with high-dimensionality problem of <b>Word</b> <b>Space</b> Models (WSMs). However, the distinguishing feature of RI from other WSMs (e. g. Latent Semantic Analysis (LSA)) is the near-orthogonality of the word vectors (index vectors). The near-orthogonality property of the index vectors helps in reducing the dimension of the underlying <b>Word</b> <b>Space.</b> The present work focuses on studying in detail the near-orthogonality property of random index vectors, {{and its effect on}} extractive text summarization. A probabilistic definition of near-orthogonality of RI-based <b>Word</b> <b>Space</b> is presented, and a thorough discussion on the subject is conducted in this paper. Our experiments on DUC 2002 data show that while quality of summaries produced by RI with Euclidean distance measure is almost invariant to near-orthogonality of the underlying Word Space; the quality of summaries produced by RI with cosine dissimilarity measure is strongly affected by near-orthogonality. Also, it is found that RI with Euclidean distance measure performs much better than many LSA-based summarization techniques. This improved performance of RI-based summarizer over LSA-based summarizer is significant because RI is computationally inexpensive as compared to LSA which uses Singular Value Decomposition (SVD) - a computationally complex algebraic technique for dimension reduction of the underlying <b>Word</b> <b>Space...</b>|$|R
40|$|The SICS team used a resource-thrifty ap-proach for valence {{annotation}} {{based on}} a word-space model {{and a set of}} seed words. The model was trained on newsprint, and va-lence was computed using proximity to one of two manually defined points in a high-dimensional <b>word</b> <b>space</b> — one represent-ing positive valence, the other representing negative valence. By projecting each head-line into this space, choosing as valence the similarity score to the point that was closer to the headline, the experiment provided re-sults with high recall of negative or positive headlines. These results show that working without a high-coverage lexicon is a viable approach to content analysis of textual data. 1 Working without a lexicon Our approach takes as its starting point the obser-vation that lexical resources always are noisy, out of date, and most often suffer simultaneously from being both too specific and too general. For our ex-periments, our only lexical resource consists of a list of eight positive words and eight negative words, as shown below in Table 1. We use a medium-sized corpus of general newsprint to build a general <b>word</b> <b>space,</b> and use our minimal lexical resource to orient ourselves in it. 2 <b>Word</b> <b>space</b> A <b>word</b> <b>space</b> is a high-dimensional vector space built from distributional statistics, in which each word in the vocabulary is represented as a context vector ~v of occurrence frequencies, such as: ~vi = [fj, · · ·, fn] where f is the (normally transformed) frequency of occurrence of word i in context j. The point of this representation is that semantic similarity between words can be computed using vector similarity mea-sures. The semantics of the <b>word</b> <b>space</b> are deter-mined by the data from which the occurrence infor-mation has been collected. The data in the SemEval Affective Text task con-sists of news headlines, which means that a relevant <b>word</b> <b>space</b> should be produced from topically sim-ilar texts, such as newswire documents. For this reason, we trained our model on a corpus of US newsprint which is available for experimentation for participants in the Cross Language Evaluation Fo-rum (CLEF). 1 The corpus consists of some 100, 00...|$|R
5000|$|One <b>word</b> <b>space</b> (French Spacing). This is {{the current}} {{convention}} in most countries that use the ISO basic Latin alphabet for published and final written work, as well as digital media.|$|R
5000|$|Not all [...] "flush left" [...] {{settings}} in traditional typography were identical. In flush left text, words are separated {{on a line}} by the default <b>word</b> <b>space</b> built into the font.|$|R
