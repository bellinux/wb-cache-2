72|13|Public
25|$|The first {{complete}} Lisp compiler, {{written in}} Lisp, was implemented in 1962 by Tim Hart and Mike Levin at MIT. This compiler introduced the Lisp model of <b>incremental</b> <b>compilation,</b> in which compiled and interpreted functions can intermix freely. The language used in Hart and Levin's memo is {{much closer to}} modern Lisp style than McCarthy's earlier code.|$|E
25|$|The Burroughs/Unisys APLB {{interpreter}} (1982) was {{the first}} to use dynamic <b>incremental</b> <b>compilation</b> to produce code for an APL-specific virtual machine. It recompiled on-the-fly as identifiers changed their functional meanings. In addition to removing parsing and some error checking from the main execution path, such compilation also streamlines the repeated entry and exit of user-defined functional operands. This avoids the stack setup and take-down for function calls made by APL's built-in operators such as Reduce and Each.|$|E
25|$|Visual Studio {{features}} background compilation (also called <b>incremental</b> <b>compilation).</b> As code {{is being}} written, Visual Studio compiles {{it in the}} background {{in order to provide}} feedback about syntax and compilation errors, which are flagged with a red wavy underline. Warnings are marked with a green underline. Background compilation does not generate executable code, since it requires a different compiler than the one used to generate executable code. Background compilation was initially introduced with Microsoft Visual Basic but has now been expanded for all included languages.|$|E
50|$|Chez Scheme is a Scheme {{implementation}} by R. Kent Dybvig, first {{released in}} 1985, which uses <b>incremental</b> native-code <b>compilation</b> to produce native binaries for the PowerPC, SPARC, IA-32, and x86-64 processor architectures. Chez Scheme supports R6RS since version 7.9.1.|$|R
50|$|This {{allows the}} {{developer}} {{the freedom of}} working with compiled code {{without the need for}} the traditional compile-link-run cycle. This is like a specialized form of <b>incremental</b> or dynamic <b>compilation.</b>|$|R
40|$|Knowledge {{compilation}} {{by using}} the extension rule (KCER) has been recognized as a novel compilation approach, although this method can only deal with static knowledge bases. This paper proposes a novel <b>incremental</b> knowledge <b>compilation</b> method {{by using the}} extension rule {{so that it can}} tackle knowledge compilation problems in the dynamic environment. The method does not recompile the whole knowledge base, while it is to use the information of compiling original knowledge base to speed up the compiling process. The experimental results show that this method can deal with dynamic knowledge bases efficientl...|$|R
50|$|Mesa {{introduced}} {{several other}} innovations in language design and implementation, {{notably in the}} handling of software exceptions, thread synchronization, and <b>incremental</b> <b>compilation.</b>|$|E
50|$|Because of its strict {{separation}} between interface and implementation, Mesa allows true <b>incremental</b> <b>compilation</b> and encourages architecture- and platform-independent programming. They also simplified source-level debugging, including remote debugging via the Ethernet.|$|E
5000|$|Scala's {{commercial}} outlet, Lightbend Inc., {{has called}} sbt [...] "arguably the best tool for building Scala projects", saying that its two most prominent features are <b>incremental</b> <b>compilation</b> and an interactive shell. When continuous compilation mode is entered, the Scala compiler is instantiated only once, which eliminates subsequent startup costs, and source file changes are tracked {{so that only}} affected dependencies are recompiled. The interactive console allows modifying build settings on the fly and entering the Scala REPL along with all class files of the project. The popularity of the <b>incremental</b> <b>compilation</b> has prompted Typesafe to extract this feature {{in the form of}} an independent component called Zinc.|$|E
40|$|International audienceUser {{interfaces}} for interactive proof assistants {{have always}} lagged behind those for mainstream programming languages. Whereas integrated development environments (IDEs) have support for features like project management, version control, dependency analysis and <b>incremental</b> project <b>compilation,</b> " IDE " s for proof assistants typically only operate on files in isolation, relying on external tools to integrate those files into larger projects. In this paper we present Coqoon, an IDE for Coq projects integrated into Eclipse. Coqoon manages proofs as projects rather than isolated source files, and compiles these projects using the Eclipse common build system. Coqoon {{takes advantage of}} the latest features of Coq, including asynchronous and parallel processing of proofs, and—when used together with a third-party OCaml extension for Eclipse—can even be used to work on large developments containing Coq plugins...|$|R
40|$|This paper {{presents}} {{techniques for}} improving compilation and run-time performance in compiler-based multiple instruction retry. <b>Incremental</b> updating enhances <b>compilation</b> time when new instructions {{are added to}} the program. Post-pass code rescheduling and spill register reassignment algorithms improve the run-time performance and decrease the code growth across the application programs studied. Branch hazards are shown to be resolvable by simple modifications to the incremental updating schemes during the pseudo register phase and to the spill register reassignment algorithm during the post-pass phas...|$|R
40|$|Abstract {{interpretation}} {{is a technique}} for flow analysis widely used {{in the area of}} logic programming. Until now it has been used only for the global compilation of Prolog programs. But the programming language Prolog enables the programmer to change the program during run time using the built-in predicates assert and retract. To support the generation of efficient code for programs using these dynamic database predicates we extended abstract interpretation to be executed incrementally. The aim of incremental abstract {{interpretation is}} to gather program information with minimal reinterpretation at a program change. In this paper we describe the implementation of incremental abstract interpretation and the integration in our VAM 1 P based Prolog compiler. Preliminary results show that <b>incremental</b> global <b>compilation</b> can achieve the same optimization quality as global compilation with little additional cost. 1 Introduction Since several years we do {{research in the area of}} imple [...] ...|$|R
50|$|The first {{complete}} Lisp compiler, {{written in}} Lisp, was implemented in 1962 by Tim Hart and Mike Levin at MIT. This compiler introduced the Lisp model of <b>incremental</b> <b>compilation,</b> in which compiled and interpreted functions can intermix freely.|$|E
50|$|Not all metaprogramming {{involves}} generative programming. If {{programs are}} modifiable at runtime or if <b>incremental</b> <b>compilation</b> is available (such as in C#, Forth, Frink, Groovy, JavaScript, Lisp, Lua, Perl, PHP, Python, REBOL, Ruby, Smalltalk, and Tcl), then techniques {{can be used}} to perform metaprogramming without actually generating source code.|$|E
50|$|The first {{complete}} Lisp compiler, {{written in}} Lisp, was implemented in 1962 by Tim Hart and Mike Levin at MIT. This compiler introduced the Lisp model of <b>incremental</b> <b>compilation,</b> in which compiled and interpreted functions can intermix freely. The language used in Hart and Levin's memo is {{much closer to}} modern Lisp style than McCarthy's earlier code.|$|E
40|$|The {{programming}} language Prolog has built-in predicates which enable the modification {{of a program}} at runtime. This makes the global compilation of Prolog programs a complex task. This paper presents an intermediate representation which enables fast <b>incremental</b> global <b>compilation</b> of Prolog. This intermediate representation {{is based on the}} Vienna Abstract Machine (VAM) with its two versions, the VAM 2 P and the VAM 1 P. The VAM 2 P has two instruction pointers and is well suited for intermediate code interpretation. Since the VAM 1 P has only one instruction pointer it can be expanded to machine code. Abstract interpretation can be used to collect information to generate specialized machine code which reduces the code size and allows faster execution. The VAM 2 P has been modified to an abstract machine for abstract interpretation, the VAM AI. This VAM AI is an intermediate representation which can be used to reconstruct the source program, which can be executed for dataflow analysis and w [...] ...|$|R
40|$|General Terms Languages Keywords {{standard}} ml, separate <b>compilation,</b> <b>incremental</b> compi-lation, types Introduction We propose {{an extension}} to Standard ML called SMLSC. SMLSCsupports separate compilation {{in the sense}} that it gives a static semantics to individual program fragments, which we call units. Aunit may depend on other units, and can be type-checked independently of those units by specifying what it expects of them. Theseexpectations are given in the form of interfaces for those other units. When unit A is checked against another unit B via a medi-ating interface, we need not have access to B at all. Therefore wesay that A is separately compiled (SC) against B. It is also useful to allow uni...|$|R
40|$|We {{demonstrate}} the pragmatic {{value of the}} principal typing property, a property more general than ML's principal type property, by studying a type system with principal typings. The type system is based on rank 2 intersection types and {{is closely related to}} ML. Its principal typing property provides elegant support for separate compilation, including "smartest recompilation" and incremental type inference, and for accurate type error messages. Moreover, it motivates a novel rule for typing recursive definitions that can type many examples of polymorphic recursion. Type inference remains decidable; this is surprising, since type inference for ML plus polymorphic recursion is undecidable. Keywords: Polymorphic recursion, separate <b>compilation,</b> <b>incremental</b> type inference, error messages, intersection types. 1 Introduction We would like to make a careful distinction between the following two properties of type systems. Property A Given: a term M typable in type environment A. There ex [...] ...|$|R
50|$|When {{building}} C language projects, it {{is imperative}} for <b>incremental</b> <b>compilation</b> (and useful for clean compilation) {{to be able to}} track dependencies between compilation units. C expresses interfaces between compilation units via header files; as such, it is often necessary to rebuild a compilation unit when a header it includes is changed. make needs to be informed of these dependencies.|$|E
50|$|A {{closely related}} {{technique}} is <b>incremental</b> <b>compilation.</b> An incremental compiler {{is used in}} POP-2, POP-11, Forth, some versions of Lisp, e.g. Maclisp {{and at least one}} version of the ML programming language (Poplog ML).This requires the compiler for the programming language {{to be part of the}} runtime system. In consequence, source code can be read in at any time, from the terminal, from a file, or possibly from a data-structure constructed by the running program, and translated into a machine code block or function (which may replace a previous function of the same name), which is then immediately available for use by the program. Because of the need for speed of compilation during interactive development and testing, the compiled code is likely not to be as heavily optimised as code produced by a standard 'batch compiler', which reads in source code and produces object files that can subsequently be linked and run. However an incrementally compiled program will typically run much faster than an interpreted version of the same program. <b>Incremental</b> <b>compilation</b> thus provides a mixture of the benefits of interpreted and compiled languages. To aid portability it is generally desirable for the incremental compiler to operate in two stages, namely first compiling to some intermediate platform-independent language, and then compiling from that to machine code for the host machine. In this case porting requires only changing the 'back end' compiler. Unlike dynamic compilation, as defined above, <b>incremental</b> <b>compilation</b> does not involve further optimisations after the program is first run.|$|E
5000|$|Pollock {{received}} a B.S. in Computer Science and a B.S. in Economics from Allegheny College in 1981. She {{received a}} M.S. in Computer Science from the University of Pittsburgh in 1983 and a Ph.D in Computer Science from the University of Pittsburgh in 1986. Her Thesis was called [...] "An approach to <b>incremental</b> <b>compilation</b> of optimized code" [...] and her thesis advisor was Mary Lou Soffa.|$|E
40|$|Previous work on compiler-basedmultiple {{instruction}} retry has utilized {{a series}} of compiler transformations, loop protection, node splitting, and loop expansion, to eliminate anti-dependencies of length N in the pseudo register, the machine register, and the post-pass resolver phases of compilation 1. The results have provided a means of rapidly recovering from transient processor failures by rolling back N instructions. This paper presents techniques for improving compilation and run-time performance in compiler-based multiple instruction retry. <b>Incremental</b> updating enhances <b>compilation</b> time when new instructions {{are added to the}} program. Post-pass code rescheduling and spill register reassignment algorithms improve the run-time performance and decrease the code growth across the application programs studied. Branch hazards are shown to be resolvable by simple modifications to the incremental updating schemes during the pseudo register phase and to the spill register reassignment algorithm during the post-pass phase. KEY WORDS Rollback recovery Fault-tolerant computing Instruction retr...|$|R
40|$|Abstract Applications {{ranging from}} {{algorithmic}} trading to scientific data analysis require realtime analytics based on views over databases receiving thousands of updates each second. Such views {{have to be}} kept fresh at millisecond la-tencies. At the same time, these views have to support clas-sical SQL, rather than window semantics, to enable applica-tions that combine current with aged or historical data. In this article, we present the DBToaster system, which keeps materialized views of standard SQL queries continu-ously fresh as data changes very rapidly. This is achieved {{by a combination of}} aggressive compilation techniques and DBToaster’s original recursive finite differencing technique which materializes a query and a set of its higher-order del-tas as views. These views support each other’s incremental maintenance, leading to a reduced overall view maintenance cost. This article provides a first description of the complete system, and a thorough experimental evaluation of its per-formance. DBToaster supports tens of thousands of com-plete view refreshes a second {{for a wide range of}} queries. Keywords <b>incremental</b> view maintenance, <b>compilation...</b>|$|R
40|$|We have {{developed}} and mechanically verified an ML system called CakeML, which supports a substantial subset of Standard ML. CakeML is implemented as an interactive read-eval-print loop (REPL) in x 86 - 64 machine code. Our correctness theorem ensures that this REPL implementation prints only those results permitted by the semantics of CakeML. Our verification effort touches on a breadth of topics including lexing, parsing, type checking, <b>incremental</b> and dynamic <b>compilation,</b> garbage collection, arbitraryprecision arithmetic, and compiler bootstrapping. Our contributions are twofold. The first is simply {{in building a}} system that is end-to-end verified, demonstrating that each piece of such a verification effort can in practice be composed with the others, and ensuring that none of the pieces rely on any over-simplifying assumptions. The second is developing novel approaches to some of the more challenging aspects of the verification. In particular, our formally verified compiler can bootstrap itself: we apply the verified compiler to itself to produce a verified machine-code implementation of the compiler. Additionally, our compiler proof handles diverging input programs with a lightweight approach based on logical timeout exceptions. The entire development was carried out in the HOL 4 theorem prover...|$|R
50|$|The Burroughs/Unisys APLB {{interpreter}} (1982) was {{the first}} to use dynamic <b>incremental</b> <b>compilation</b> to produce code for an APL-specific virtual machine. It recompiled on-the-fly as identifiers changed their functional meanings. In addition to removing parsing and some error checking from the main execution path, such compilation also streamlines the repeated entry and exit of user-defined functional operands. This avoids the stack setup and take-down for function calls made by APL's built-in operators such as Reduce and Each.|$|E
50|$|Visual Studio {{features}} background compilation (also called <b>incremental</b> <b>compilation).</b> As code {{is being}} written, Visual Studio compiles {{it in the}} background {{in order to provide}} feedback about syntax and compilation errors, which are flagged with a red wavy underline. Warnings are marked with a green underline. Background compilation does not generate executable code, since it requires a different compiler than the one used to generate executable code. Background compilation was initially introduced with Microsoft Visual Basic but has now been expanded for all included languages.|$|E
50|$|The Rational Environment was {{organized}} around a persistent intermediate representation (DIANA), providing users with syntactic and semantic completion, <b>incremental</b> <b>compilation,</b> and integrated configuration management and version control. To overcome {{a conflict between}} strong typing and iterative development that produced recompilation times proportional to system size rather than size-of-change, the Rational Environment supported the definition of subsystems with explicit architectural imports and exports; this mechanism later proved useful in protecting application architectures from inadvertent degradation. The Environment's Command Window mechanism {{made it easy to}} directly invoke Ada functions and procedures, which encouraged developer-driven unit testing.|$|E
50|$|Third, to {{understand}} language use {{we must be}} able to distinguish correct editing operations from wrong ones in our development environment. Traditional development environments have long provided guidance during the writing of a program. <b>Incremental</b> <b>compilation</b> allows the environment to offer detailed suggestions to the developer such as how to complete a statement. More intrusive kinds of guidance also exist such as syntax-oriented editors where only input conforming to the grammar can be entered. Generic text-editors that can be parameterized with the grammar of a language have existed for a long time.|$|E
50|$|Common Lisp {{implementations}} may use any mix {{of native}} code compilation, byte code compilation or interpretation. Common Lisp {{has been designed}} to support incremental compilers, file compilers and block compilers. Standard declarations to optimize compilation (such as function inlining or type specialization) are proposed in the language specification. Most Common Lisp implementations compile source code to native machine code. Some implementations can create (optimized) stand-alone applications. Others compile to interpreted bytecode, which is less efficient than native code, but eases binary-code portability. There are also compilers that compile Common Lisp code to C code. The misconception that Lisp is a purely interpreted language is most likely because Lisp environments provide an interactive prompt and that code is compiled one-by-one, in an incremental way. With Common Lisp <b>incremental</b> <b>compilation</b> is widely used.|$|E
50|$|Key to {{its success}} has been {{engineering}} and language decisions that make the language usable and programs deployable. The original Xerox AspectJ implementation used source weaving, which required access to source code. When Xerox contributed the code to Eclipse, AspectJ was reimplemented using the Eclipse Java compiler and a bytecode weaver based on BCEL, so developers could write aspects for code in binary (.class) form. At this time the AspectJ language was restricted to support a per-class model essential for <b>incremental</b> <b>compilation</b> and load-time weaving. This made IDE integrations as responsive as their Java counterparts, and it let developers deploy aspects without altering the build process. This led to increased adoption, as AspectJ became usable for impatient Java programmers and enterprise-level deployments. Since then, the Eclipse team has increased performance and correctness, upgraded the AspectJ language to support Java 5 language features like generics and annotations, and integrated annotation-style pure-java aspects from AspectWerkz.|$|E
50|$|Pop-11 is {{the core}} {{language}} of the Poplog system. The fact that the compiler and compiler subroutines are available at run-time (a requirement for <b>incremental</b> <b>compilation)</b> gives it the ability to support a far wider range of extensions than would be possible using only a macro facility. This {{made it possible for}} incremental compilers to be added for Prolog, Common Lisp and Standard ML, which could be added as required to support either mixed language development or development in the second language without using any Pop-11 constructs. This made it possible for Poplog to be used by teachers, researchers, or developers who were interested in only one of the languages. The most successful product developed in Pop-11 was the Clementine data-mining system, developed by ISL, as described in the entry on Poplog. After SPSS bought ISL they decided to port Clementine to C++ and Java, and eventually succeeded with great effort (and perhaps some loss of the flexibility provided by the use of an AI language!).|$|E
40|$|The {{methodology}} and developmental history of <b>incremental</b> <b>compilation</b> is discussed. The implementation of <b>incremental</b> <b>compilation</b> in the PECAN programmmg environment generator is discussed m detail. The PECAN environment generated for Pascal has been modified to support procedure-byprocedure compilation, and complete (traditional) compilation. The time efficiency of these compilation methods is compared with that, of <b>incremental</b> <b>compilation.</b> ii Declaration Except where otherwise indicated this thesis {{is my own}} work...|$|E
40|$|For high-density, {{high-performance}} FPGA designs, {{the ability}} to iterate rapidly during design and debugging stages is critical to meet time-to-market demands. Today’s FPGA designers are encountering problems traditionally associated with ASIC designs, especially long place-and-route compilation times and difficulties achieving timing closure. To address these issues, FPGA and EDA vendors are beginning to offer incremental design and compilation capabilities previously available only with ASIC design tools. These capabilities include top-down methodologies that support evolving designs and engineering change orders (ECO), as well as bottom-up design methodologies including team-based design flows. <b>Incremental</b> <b>compilation</b> improves productivity by dramatically reducing design iteration times and preserving results to reach timing closure more easily. This paper presents <b>incremental</b> <b>compilation</b> methodologies using Mentor Graphics ® Precision RTL Synthesis and Altera ® Quartus ® II software, including user scenarios and design recommendations to maximize the benefits and ensure good quality of results in an <b>incremental</b> <b>compilation</b> flow. 2 <b>Incremental</b> <b>Compilation</b> Benefits Conventionally, a hierarchical design is flattened into a single netlist before logic synthesis and fitting (or placement and routing). The entire design is then recompiled every time there {{is a change in}} the design. One reason for this behavior is to obtain optimal quality of results. By processing the entire design, the compiler can perform global optimizations to improve area and performance. However, this leads to long placement and routing times, and typically causes major changes in the placement results and the design’s performance even for minor changes in source code or settings. There are many situations in which a more <b>incremental</b> <b>compilation</b> flow is desirable. <b>Incremental</b> <b>compilation</b> allows a design to be organized into logical partitions for synthesis and fitting...|$|E
40|$|Although {{simulation}} {{remains an}} important part of application-specific integrated circuit (ASIC) validation, hardware-assisted parallel verification is becoming a larger part of the overall ASIC verification flow. In this paper, we describe and analyze a set of <b>incremental</b> <b>compilation</b> steps that can be directly applied to a range of parallel logic verification hardware, including logic emulators. Important aspects of this work include the formulation and analysis of two incremental design mapping steps: the partitioning of newly added design logic onto multiple logic processors and the communication scheduling of newly added design signals between logic processors. To validate our <b>incremental</b> <b>compilation</b> techniques, the developed mapping heuristics have been integrated into the compilation flow for a field-programmable gate-array-based Ikos VirtuaLogic emulator [1]. The modified compiler has been applied to five large benchmark circuits that have been synthesized from register-transfer level and mapped to the emulator. It is shown that our incremental approach reduces verification compile time for modified designs by up to a factor of five versus complete design recompilation for benchmarks of over 100 000 gates. In most cases, verification run-time following <b>incremental</b> <b>compilation</b> of a modified design matches the performance achieved with complete design recompilation...|$|E
40|$|Abstract—Although {{simulation}} {{remains an}} important part of application-specific integrated circuit (ASIC) validation, hardware-assisted parallel verification is becoming a larger part of the overall ASIC verification flow. In this paper, we describe and analyze a set of <b>incremental</b> <b>compilation</b> steps that can be directly applied to a range of parallel logic verification hardware, including logic emulators. Important aspects of this work include the formulation and analysis of two incremental design mapping steps: the partitioning of newly added design logic onto multiple logic processors and the communication scheduling of newly added design signals between logic processors. To validate our <b>incremental</b> <b>compilation</b> techniques, the developed mapping heuristics have been integrated into the compilation flow for a field-programmable gate-array-based Ikos VirtuaLogic emulator [1]. The modified compiler has been applied to five large benchmark circuits that have been synthesized from register-transfer level and mapped to the emulator. It is shown that our incremental approach reduces verification compile time for modified designs by up to a factor of five versus complete design recompilation for benchmarks of over 100 000 gates. In most cases, verification run-time following <b>incremental</b> <b>compilation</b> of a modified design matches the performance achieved with complete design recompilation. Index Terms—Incremental compilation, incremental partitioning, incremental routing, logic emulation. I...|$|E
40|$|Over {{the past}} decade, the steady {{growth rate of}} FPGA device {{capacities}} has enabled the development of multi-FPGA prototyping environments capable of implementing millions of logic gates. While software support for translating new user designs from gate and RTL-level netlists to FPGA bitstreams has improved steadily, little {{work has been done}} in developing techniques to support the translation of incremental design changes at the netlist level to a set of replacement bitstreams for a small number of FPGAs in a multi-FPGA system. As system sizes and design compilation times increase, the need to support rapid, <b>incremental</b> <b>compilation</b> grows progressively important. In this paper we describe and analyze a set of <b>incremental</b> <b>compilation</b> steps, including incremental design partitioning and incremental inter-FPGA routing, for two specific classes of multi-FPGA emulation systems. These classes are defined by the techniques that emulation software systems use to determine inter-FPGA communica [...] ...|$|E
