76|913|Public
50|$|<b>Homogeneity</b> <b>testing</b> for a {{candidate}} reference material typically involves replicated measurements on multiple units or subsamples of the material.|$|E
50|$|In meteorology, weather {{datasets}} are acquired {{over many}} years of record and, as part of this, measurements at certain stations may cease occasionally while, at around the same time, measurements may start at nearby locations. There are then questions as to whether, if the records are combined to form a single longer set of records, those records can be considered homogeneous over time. An example of <b>homogeneity</b> <b>testing</b> of wind speed and direction data {{can be found in}} Romanić et al., 2015.|$|E
5000|$|The {{quality control}} {{procedures}} lead to each data being flagged as either [...] "OK", [...] "suspect" [...] or [...] "missing" [...] and <b>homogeneity</b> <b>testing</b> {{results in the}} classification of series as [...] "useful", [...] "doubtful" [...] or [...] "suspect". It is recommended to use {{the results of the}} homogeneity tests for selecting appropriate series and time intervals since the [...] "useful", [...] "doubtful" [...] and [...] "suspect" [...] categories only hold for the particular time interval for which the tests are applied. Information on the specifics of quality control and homogeneity procedures {{can be found in the}} Algorithm Theoretical Basis Document [...] and elsewhere.|$|E
5000|$|... #Subtitle level 3: Incongruence length {{difference}} <b>test</b> (or partition <b>homogeneity</b> <b>test)</b> ...|$|R
30|$|Therefore, three {{absolute}} <b>homogeneity</b> <b>tests</b> {{are used}} to detect any variation in the time series to extract meaningful statistics and characteristics from these data, using AnClim software which has been specially created for climatologic purposes (Stepanek 2008). The tests {{that are used in}} this research are; Pettitt’s <b>test,</b> standard normal <b>homogeneity</b> <b>test</b> (SNHT), and von Neumann’s test (Taxak et al. 2014; Carvalho et al. 2014).|$|R
40|$|A {{density ratio}} {{is defined by}} the ratio of two {{probability}} densities. We study the inference problem of density ratios and apply a semi-parametric density-ratio estimator to the two-sample <b>homogeneity</b> <b>test.</b> In the proposed test procedure, the f-divergence between two probability densities is estimated using a density-ratio estimator. The f-divergence estimator is then exploited for the two-sample <b>homogeneity</b> <b>test.</b> We derive an optimal estimator of f-divergence {{in the sense of the}} asymptotic variance in a semiparametric setting, and provide a statistic for two-sample <b>homogeneity</b> <b>test</b> based on the optimal estimator. We prove that the proposed test dominates the existing empirical likelihood score test. Through numerical studies, we illustrate the adequacy of the asymptotic theory for finite-sample inference...|$|R
50|$|The {{scheme was}} {{established}} in 1974 by the Department of the Environment {{with much of the}} initial planning and implementation undertaken by the Water Research Centre. It involved the creation of a network of sites across the UK. Most sites were at the downstream freshwater limits of the larger rivers although some of the largest rivers also included additional locations below major tributary confluences. Before inclusion of any location in the survey, <b>homogeneity</b> <b>testing</b> was carried out to characterise quality variations laterally and vertically within the river section chosen. For each designated point a specific sampling location was identified {{based on the results of}} the homogeneity exercise. In addition laboratories who were to undertake the analysis of samples were identified and a comprehensive AQC exercise was conducted to ensure comparability of data derived from the monitoring programme.|$|E
40|$|Nonparametric order {{tests for}} {{homogeneity}} and component independence are proposed, {{which are based}} on data compressors. For <b>homogeneity</b> <b>testing</b> the idea is to compress the word obtained by ordering the combined samples and writing the number of the sample in place of each element. H 0 should be rejected if the string is compressed to a certain degree and accepted otherwise. We show that such a test obtained from an ideal data compressor is valid against all alternatives. Component independence is reduced to <b>homogeneity</b> <b>testing...</b>|$|E
40|$|An {{international}} {{interlaboratory trial}} {{was conducted to}} validate thermoluminescence methods for detecting irradiated fruits and vegetables. Five products were used in this study. This paper presents the results from prestudy material, <b>homogeneity</b> <b>testing,</b> details of sample preparation, and participants= results. Prestudy results provided a basis for cross comparison of instruments in different laboratories. A wide range of sensitivities, reproducibilities, and signal?to?background ratios were observed. <b>Homogeneity</b> <b>testing</b> showed that the method can distinguish between nonirradiated and irradiated products, including those bleached with 100 J/cm 2 artificial daylight, provided that sensitivity rejection criteria are rigorously applied. Blind results were returned by 9 participants {{in the form of}} first and second glow integrals and glow ratios for all samples and a qualitative classification for each product. Of the 387 results reported, 327 valid results were obtained from participants. Where valid data were obtained, correct qualitative identifications were made by participants in all cases. Participants= results and <b>homogeneity</b> <b>testing</b> both confirm the validity of the thermoluminescence method for detecting irradiated fruits and vegetables...|$|E
3000|$|Variables OB and NB groups, {{separated}} by age (n 1, n 2, n 3), {{were evaluated by}} the <b>homogeneity</b> <b>test</b> (chi-square-x [...]...|$|R
5000|$|... #Caption: Schematic of a {{balanced}} nested design for a CRM <b>homogeneity</b> <b>test.</b> Large bottles show packaged individual CRM units; small vials show subsamples prepared for measurement.|$|R
40|$|Classical {{variance}} <b>homogeneity</b> <b>tests</b> (Fisher's, Bartlett's, Cochran's, Hart- ley's and Levene's tests) and nonparametric {{tests for}} dispersion characteristics homogeneity (Ansari-Bradley's, Mood's, Siegel-Tukey's tests) have been consid- ered. Distributions of classical tests statistics {{have been investigated}} under vio- lation of assumption that samples belong to the normal law. The comparative analysis of power of classical variance <b>homogeneity</b> <b>tests</b> with power of nonpara- metric tests has been carried out. Tables of percentage points for Cochran's test have been obtained in case of distributions which are di®erent from normal...|$|R
40|$|Abstract: All studies {{concerning}} climate variability {{reveal the}} necessity of the <b>homogeneity</b> <b>testing</b> of data {{as the first step}} in further research. Inhornogeneities caused by station relocation, installation of new instruments, etc. can be the cause of misleading conclusions that do not correspond to real changes. It is quite obvious that data must be tested in order to locate possible inhomogeneities/discont in uities. This paper describes the <b>homogeneity</b> <b>testing</b> of the mean monthly and annual air temperature in Croatia. Several 50 -year temperature time series during the period from 1949 to 1998 were tested using the Standard Normal Homogeneity Test (SNHT). The exact cause of discontinuities was searched from the meta data...|$|E
40|$|Aims: To {{develop a}} {{technique}} for <b>homogeneity</b> <b>testing</b> of serum aliquot samples {{suitable for use}} in the Quality Assurance Program in Clinical Immunology (QAP Pty Ltd). Methods: Albumin was selected as the surrogate protein marker for the product to be tested and the coefficient of dispersion (COD) calculated as the measure of homogeneity. To detect changes in the average level of homogeneity, cumulative sum control (cusum) charts were used. Results: The COD(%) for each triplicate reading of albumin obtained from 34 specimens was normally distributed with a mean of 0. 49 % and a standard deviation of 0. 25 %. In industrial quality control schemes the action line is generally set at the upper 99 % confidence limits, hence any triplicate sample would be considered to have acceptable homogeneity if the COD was ≤ 1. 08 %. Cusum charts were created to monitor albumin homogeneity over time. Conclusions: The use of albumin measurement as the surrogate appears statistically suitable for <b>homogeneity</b> <b>testing</b> in QAP programs for immunodiagnostic testing. CUSUM charts are particularly useful to monitor such <b>homogeneity</b> <b>testing.</b> Adrian Esterman, Sue Javanovich, Robert McEvoy and Peter Roberts-Thomson...|$|E
40|$|International audienceNonparametric rank {{tests for}} {{homogeneity}} and component independence are proposed, {{which are based}} on data compressors. For <b>homogeneity</b> <b>testing</b> the idea is to compress the binary string obtained by ordering the two joint samples and writing 0 if the element is from the first sample and 1 if it is from the second sample and breaking ties by randomization (extension to the case of multiple samples is straightforward). $H_ 0 $ should be rejected if the string is compressed (to a certain degree) and accepted otherwise. We show that such a test obtained from an ideal data compressor is valid against all alternatives. Component independence is reduced to <b>homogeneity</b> <b>testing</b> by constructing two samples, one of which is {{the first half of the}} original and the other is the second half with one of the components randomly permuted...|$|E
3000|$|..., {{we compare}} {{two sets of}} data which are drawn from the populations. The <b>homogeneity</b> <b>test</b> becomes a problem of testing whether two samples of random {{variables}} are generated from the same distribution.|$|R
40|$|In {{this paper}} we will check the homogeneity/heterogeneity of Levy {{processes}} using some non-parametric <b>homogeneity</b> <b>tests.</b> First we create two samples from two Levy processes {{starting from the}} definition of the Levy process, and next we test if the two samples have the same distribution. Using the Levy—Ito decomposition we will perform the <b>homogeneity</b> <b>tests</b> for given parts of the Levi processes. The study of the homogeneity of stock markets shocks is usefull because the eventualy homogeneity can produce a phenomenon analogue to the resonance that can be observed in mechanics. This resonance increase the idiosyncratic risk. ...|$|R
40|$|Seagrass (Syringodium isoetifolium) is a {{plant that}} lives {{submerged}} at sea, this floweringplant, leaves, and stems that are plugged into a powerful in water. The chemical content ofthe leaves of the Seagrass is a flavonoid, phenol, hydroquinone, and antioxidants. This studyaims to determine {{the quality of the}} cream of seagrass leaf extracts using a concentration of 5 %, 10 %, 20 % dan 40 %. on organoleptic <b>testing</b> conducted cream, <b>homogeneity</b> <b>test,</b> testdispersive power, pH and absorbance test. results obtained in the organoleptic seen from thecolor green, semi-solid dosage forms and distinctive smell seagrass cream. <b>homogeneity</b> <b>test.</b> <b>homogeneity</b> <b>test</b> cream seagrass leaf extract with a concentration of 5 % 10 % 20 % and 40 %did not experience any clumping or phase separation dispersive power test creams rangedfrom 2. 5 cm - 3. 5 cm. pH ranged from 4. 93 - 5. 96 and test absorption ranged from 3. 7 ml - 5 ml. seagrass leaf extract cream with type W/O creams that meet the <b>test</b> of <b>homogeneity</b> testquality, dispersive power test, test and test pH absorptio...|$|R
40|$|This paper {{describes}} the studies {{performed with the}} candidate Certified Reference Material (CRM) of captopril, the first CRM of an active pharmaceutical ingredient (API) in Brazil, including determination of impurities (organic, inorganic and volatiles), <b>homogeneity</b> <b>testing,</b> short- and long-term stability studies, calculation of captopril content using the mass balance approach, and estimation of the associated measurement uncertainty...|$|E
40|$|Abstract — Regularized Maximum Mean Discrepancy (RMMD), our novel {{measure for}} kernel-based {{hypothesis}} testing, excels at hypothesis tests involving multiple comparisons with power control even when sample sizes are small. We derive asymptotic distributions under the null and alternative hypotheses, and assess power control. Outstanding results are obtained on challenging benchmark datasets. Keywords- kernel-based hypothesis testing, <b>Homogeneity</b> <b>testing,</b> Multiple comparisons, Power I...|$|E
40|$|A {{modified}} {{approach to}} <b>homogeneity</b> <b>testing</b> of reference materials for microanalytical techniques, {{based on the}} concept of the information mass and the prescription of <b>homogeneity</b> <b>testing</b> for bulk material (sample mass>lOO mg), is presented. A microanalytical instrumental set-up is used to study homogeneity at mass level below 1 mg and a modified version of T and F statistical tests is applied. The sensitivity of the method to identify contaminated materials is assessed with Monte Carlo simulations and a good accuracy in the identification of contaminants is demonstrated. As an example of the application of the approach the analysis of the IAEA-SLl reference material is performed and the homogeneity test shows homogeneous distribution of most of the elements for sub pg masses. For these elements the calculation of the mass levels required for a desired accuracy is presented. A method to determine the mass level for homogeneous distribution for the remaining elements identified is also suggested...|$|E
40|$|Mixtures of {{binomial}} distributions {{are used}} to model linkage between genes and markers. In several classical models, we study the likelihood ratio test for linkage, which are <b>homogeneity</b> <b>tests</b> in mixture models. Using ad hoc reparametrizations, we prove that the corresponding test statistics converge to the supremum of squared truncated Gaussian processes under the null hypothesis. Binomial mixture <b>Homogeneity</b> Likelihood ratio <b>test</b> Linkage...|$|R
40|$|The {{learning}} outcomes of mathematic problem at MTs Thamrin yahya had a problem. There were 50 % who got score under KKM. This research gives solution by appiying Teams Games Tournaments (TGT) model. Hypothesis testing used t test. before conducting t <b>test,</b> <b>homogeneity</b> and normality <b>test</b> was conducted firstly. Normality test used liliefors <b>test</b> and <b>homogeneity</b> <b>test</b> used test wich {{was the result}} or normal distribution sample has homogeny varians. Then, hypothesis test was conducted which was the calculation of result was tcount = 5, 26 and ttable = 2, 008. It show that tcount > ttable at the 5 % significan level effect of TGT learning model toward mathematics studi result...|$|R
40|$|Abstrack : This {{research}} {{aimed to}} know the effectiveness of guided inquiry learning model in improving the student achievement in class XI IPA SMAN 5 Pekanbaru. This research was a quasi experiment with randomized group pretest-posttes design. The sample {{of this study was}} XI IPA 5 as experiment class and XI IPA 6 as control class after being tested for normality and <b>homogeneity</b> <b>tests.</b> Normality test was performed using Liliefors test. The results of the Liliefors normality test obtained for all the data (material prerequisites, pretest, and posttest) normal distribution where Lmaks ≤ Ltabel. The results of the <b>homogeneity</b> <b>test,</b> of Fhitung 1. 67 with dk = n 1 + n 2 - 2 and α = 0. 05) means that the application of guided inquiry learning model gave effect of 8. 02 % to the improvement of the student achievement...|$|R
40|$|Mushroom {{reference}} material has been prepared and characterized {{for use in}} proficiency test exercises within the frame of an IAEA Interregional Technical Cooperation Project. Laboratories from 14 countries provided results for <b>homogeneity</b> <b>testing</b> and the assignment of property values. The contents of 11 elements have been assigned. The material was used for conducting a proficiency test in Poland and the results obtained by Polish laboratories are presented and discussed...|$|E
40|$|<b>Homogeneity</b> <b>testing</b> of {{materials}} used in analytical proficiency tests is usually carried out by selecting at random {{a number of the}} distribution units (commonly 10) and analysing them in duplicate. The results are assessed by one-way analysis of variance. Because of the limited amount of information available in these data, this analysis provides an imprecise estimate of the analytical variance σ 2 an, an even more imprecise estimate of the sampling variance σ 2 sam, and a test of low statistical power of the hypothesis σ 2 sam = 0. When a succession of similar materials are prepared for successive rounds, under conditions where statistical control of the whole process is a reasonable assumption, the greatly increased number of degrees of freedom obtained by pooling the data allows both variance components to be well estimated, even when the sampling variance is small. A new approach to <b>homogeneity</b> <b>testing</b> is proposed, in which, after an initial phase of data collection and variance estimation, a control chart is set up and the experimental design is reduced to 10 singleton measurements. This gives a considerable reduction in expenditure, with little or no loss in statistical power...|$|E
40|$|This paper {{presents}} the results of the studies carried out with the candidate certified reference material (CRM) of metronidazole, first CRM of this active pharmaceutical ingredient (API) available on the market and second Brazilian CRM of an API. The investigation includes the determination of organic impurities, inorganic impurities and volatiles, validation of the HPLC-DAD method, stability studies under transport and storage conditions, <b>homogeneity</b> <b>testing,</b> calculation of metronidazole content by mass balance, confirmation of the certified value by differential scanning calorimetry (DSC), and estimation of measurement uncertainties...|$|E
40|$|The <b>test</b> of <b>homogeneity</b> {{developed}} by L. V. Hedges (1982) for the fixed effects model is frequently used in quantitative meta-analyses {{to test whether}} effect sizes are equal. Despite its widespread use, evidence {{of the behavior of}} this test for the less-than-ideal case of small study iample sizes paired with large numbers of studies is contradictory, and its behavior for nonnormal score distributions in primary studies is an open question. The results of a Monte Carlo study indicated that the Type I error rate and power of the <b>homogeneity</b> <b>test</b> were insensitive to skewed score distributions, but were very sensitive to smaller study sample sizes paired with larger numbers of studies. These findings extend earlier results and help to clarify the statistical behavior of the <b>homogeneity</b> <b>test.</b> Specifically, the pairing of small study sample sizes with large numbers of studies tends to produce conservative Type I error rates for the <b>homogeneity</b> <b>test</b> and underestimates its power, increasing the likelihood of Type II errors. (Contains 2 tables and 23 references.) (Author/SLD) Reproductions supplied by EDRS are the best that can be made from the original document...|$|R
40|$|The {{assessment}} of regional homogeneity {{is a critical}} point in regional frequency analysis. To this end, many <b>homogeneity</b> <b>tests</b> have been proposed, even though a general comparison among them is still lacking. Commonly used <b>homogeneity</b> <b>tests,</b> based on L moments ratios, are considered here in a comparison with two rank tests that do not rely on particular assumptions regarding the parent distribution. The performance of these tests is assessed {{in a series of}} Monte Carlo simulation experiments. In particular, the power and type I error of each test are determined for different scale and shape parameters of the regional parent distributions. The tests are also evaluated by varying the number of sites belonging to the region, the series length, the type of the parent distributions and the degree of heterogeneity. We find that L moments based tests are more powerful when the samples are slightly skewed, while the rank tests have better performances in case of high skewness. On the basis of these findings we propose a simple method to guide the choice of the <b>homogeneity</b> <b>test</b> to be used for the different possible case...|$|R
40|$|In {{statistical}} meta-analysis paradigm {{dealing with}} the techniques of pooling of evidence across several studies, the notion of effect size is fundamental and {{it is necessary to}} perform appropriate tests for the equality of population effect sizes arising out of these studies before performing any meta-analysis. A well known <b>test</b> for <b>homogeneity</b> in this context is based on Cochran's chi-square statistic. However Cochran's test might be inaccurate in <b>testing</b> <b>homogeneity</b> for some effect sizes in the sense of not maintaining the stipulated Type I errors. In this dissertation, three such scenarios are identified: 1) <b>testing</b> <b>homogeneity</b> of standardized mean difference, 2) <b>testing</b> <b>homogeneity</b> of correlations and correlation matrices, and 3) <b>testing</b> <b>homogeneity</b> of proportions with low event rates, and in each case the severe inadequacy of Cochran's <b>homogeneity</b> <b>test</b> is demonstrated. Apppropriate bootstrap procedures for <b>testing</b> <b>homogeneity</b> hypotheses are suggested, and applications in each problem are indicated. Two additional problems are also dealt with in the dissertation: Mantel-Haenszel and Peto weights in random effects meta-analysis, and extraction methods in statistical meta-analysis...|$|R
40|$|International audienceA metric between time-series {{distributions}} {{is proposed}} {{that can be}} evaluated using binary classification methods, which were originally developed to work on i. i. d. data. It is shown how this metric {{can be used for}} solving statistical problems that are seemingly unrelated to classification and concern highly dependent time series. Specifically, the problems of time-series clustering, <b>homogeneity</b> <b>testing</b> and the three-sample problem are addressed. Universal consistency of the resulting algorithms is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data...|$|E
40|$|A metric between time-series {{distributions}} {{is proposed}} {{that can be}} evaluated using binary classification methods, which were originally developed to work on i. i. d. data. It is shown how this metric {{can be used for}} solving statistical problems that are seemingly unrelated to classification and concern highly dependent time series. Specifically, the problems of time-series clustering, <b>homogeneity</b> <b>testing</b> and the three-sample problem are addressed. Universal consistency of the resulting algorithms is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data. Keywords: distributions time series, reductions, stationary ergodic, clustering, metrics between probability 1...|$|E
40|$|We {{suggest a}} new {{approach}} to hypothesis testing for ergodic and stationary processes. In contrast to standard methods, the suggested approach gives a possibility to make tests, based on any lossless data compression method even if the distribution law of the codeword lengths is not known. We apply this approach to the following four problems: goodness-of-fit testing (or identity testing), testing for independence, testing of serial independence and <b>homogeneity</b> <b>testing</b> and suggest nonparametric statistical tests for these problems. It {{is important to note that}} practically used so-called archivers can be used for suggested testing. Comment: accepted for "Statistical Methodology" (Elsevier...|$|E
30|$|Pre-treatment <b>homogeneity</b> <b>test.</b> A Student’s t-test {{was used}} to compare the means for the {{experimental}} and control groups because the distribution of assessments for both groups was normalised (non-significant Kolmogorov-Smirnov test results). Fisher’s least significant difference (LSD) method was applied to determine which means were significantly different from others.|$|R
40|$|Meta-analysis {{seeks to}} combine the results of several {{experiments}} {{in order to improve}} the accuracy of decisions. It is common to use a <b>test</b> for <b>homogeneity</b> to determine if the results of the several experiments are sufficiently similar to warrant their combination into an overall result. Cochran'sQstatistic is frequently used for this <b>homogeneity</b> <b>test.</b> It is often assumed thatQfollows a chi-square distribution under the null hypothesis of homogeneity, but it has long been known that this asymptotic distribution forQis not accurate for moderate sample sizes. Here, we present an expansion for the mean ofQunder the null hypothesis that is valid when the effect and the weight for each study depend on a single parameter, but for which neither normality nor independence of the effect and weight estimators is needed. This expansion represents an orderO(1 /n) correction to the usual chi-square moment in the one-parameter case. We apply the result to the <b>homogeneity</b> <b>test</b> for meta-analyses in which the effects are measured by the standardized mean difference (Cohen'sd-statistic). In this situation, we recommend approximating the null distribution ofQby a chi-square distribution with fractional degrees of freedom that are estimated from the data using our expansion for the mean ofQ. The resulting <b>homogeneity</b> <b>test</b> is substantially more accurate than the currently used test. We provide a program available at the Paper Information link at theBiometricswebsitefor making the necessary calculations...|$|R
40|$|Here is an ado {{file and}} a help file for calculating {{asymptotic}} and exact tests of symmetry for NxN contingency tables from dependent samples. This test {{is known in}} genetics as the TDT test. "symmetry" performs symmetry and marginal <b>homogeneity</b> <b>tests</b> on square NxN tables {{where there is a}} 1 to 1 matching of cases and controls (non-independence). ...|$|R
