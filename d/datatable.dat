20|16|Public
5000|$|... ${var.age}. In Data Item Controls like <b>DataTable</b> and List Table, custom {{rendering}} of Data Column {{can be done}} by using ItemTemplate. in order to access the current Database record a variable should be declared in the <b>DataTable</b> with name var for example. Then inside the ItemTemplate the values inside the var variable can be accessed as shown in the EL.|$|E
50|$|Some of jspx {{features}} are used like Master/Content pages, Ajax, <b>DataTable,</b> Validators and web forms.|$|E
5000|$|Used in {{combination}} with Profiler to provide rich visualizations of your profiling data — both graphically (using the Charts Control) and in tabular format (using <b>DataTable).</b>|$|E
40|$|Moved to Bootstrap framework. 	Moved many {{functions}} to Ajax. 	Updated <b>DataTables</b> library. 	Improved error messages. 	More consistent labeling and small UI fixes. 	Added Twitter and Feedback buttons. 	New Local and Beta modes. 	Old Select tool replaced with new Manage tool. 	Scrubber: Added Remove whitespace, Keep Ampersands, Keepwords lists, and markup tag scrubbing functions (full support for XML, partial for HTML/SGML). 	Tokenizer: New version uses serverside processing to handle large DTMs efficiently. New Show/Hide columns function. 	WordCloud, Multicloud, Bubbleviz: Improved tooltips. 	WordCloud: Term counts table switched to <b>DataTables</b> with search and sort functions. 	Multicloud: New toggle switch for topic clouds. 	Rolling Windows: Reorganized UI display. 	Statistics: Switched to <b>DataTables</b> with client-side download buttons for multiple formats. 	Hierarchical Clustering: New Silhouette Score interface, dendrogram {{is now a}} PNG by default, new Newick format export button. 	K-Means Clustering: Interactive tooltips for Voronoi cells for better readability. 	Similarity Query: More efficient code. Output now uses <b>DataTables.</b> 	TopWords: Enhanced functionality and clearer interface...|$|R
40|$|What's New: 	Compatibility with Tethys 2. 0. 1. 	Added API for {{programmatic}} data queries. @msouff 	Use Plotly {{instead of}} HighCharts (Fully open source!). 	Use <b>DataTables.</b> js for management tables. 	Use Tethys Platform Settings ([URL] 	Improve stream clicking functionality. @msouff 	Improved error handling. 	Code cleanup...|$|R
40|$|New paint for the UI {{look and}} feel. Reworked Rolling Window Analysis Kmeans Clustering got some love Document Milestones if {{inserted}} beforehand {{can be detected}} on cut Tokenize moved to <b>DataTables</b> Top-Words added Citation information: Kleinman, S., LeBlanc, M. D., Zhang, C. (2015). Lexos. v. 2. 5. [URL]...|$|R
5000|$|ColdFusion 8 {{natively}} supports [...]NET {{within the}} CFML syntax. ColdFusion developers can simply call any [...]NET assembly without needing to recompile or alter the assemblies in any way. Data types are automatically translated between ColdFusion and [...]NET (example: [...]NET <b>DataTable</b> → ColdFusion Query).|$|E
50|$|SensorThings HcDT is a {{javascript}} charting {{library for}} the OGC SensorThings API. It {{is based on}} the open source Highchart library and <b>Datatable.</b> It is a front-end charting library enable developers to connect to datastreams from any OGC SensorThings API service, and display the sensor observations in charts, tables, or dashboard widgets for web applications.|$|E
50|$|A DataReader can in {{some cases}} be used {{in place of a}} <b>DataTable,</b> however many programmers have {{experienced}} connection bloat when following this approach. A DataReader can only be used against an (already) open database connection; that connection isn't closed until the DataReader's Dispose method is called. If an exception is thrown while the data is being processed, for example as described in Strong vs Weak Typing, above, the Dispose method will never be called if the developer writes code explicitly declaring and disposing the DataReader without the use of a try-finally block. The C# using construct {{is a good way to}} avoid this problem, as shown below in the code example.|$|E
5000|$|Business objects created using CSLA [...]NET fully support {{data binding}} for all Microsoft [...]NET UI technologies, {{including}} Windows Runtime (WinRT), WPF, Web Forms, ASP.NET MVC, Windows Phone, Silverlight, and Windows Forms. Data-bound controls like DataGrids and ListBoxes can be bound to business objects instead of more generalized database objects like ADO.NET DataSets and <b>DataTables.</b>|$|R
40|$|The Chart Update Mash up (ChUM™) {{consists}} of various APIs/Technologies that when brought together allow for intuitive georeferenced visualization of the Critical Chart Updates {{published by the}} Office of Coast Survey (OCS). Each component of ChUM™ will be described in this paper with an explanation of how each piece works together to form ChUM™. Figure 1 shows the basic building blocks that make up ChUM™. At {{the base of the}} structure (in blue) is the data that OCS disseminates to the public via its website/web-services: Raster Nautical Charts, the CRIT data (critical corrections to the chart), and the Coast Pilot. There are three 3 rd party API’s (Application Programming Interface) /plug-ins that are used: the Google Maps API v 3, the <b>DataTables</b> plug-in, and the NauticalChartsAPI (NCAPI). The NCAPI is an API created by the University of California, San Diego’s Coastal Observing Research and Development Center (CORDC) and it deals directly with fetching and preparing the Raster Nautical charts for use in Google Maps. The <b>DataTables</b> is a plug-in for the jQuery JavaScript library and it is populated with data from the OCS CRIT data. The NauticalChartsAPI UI (User Interface), NCAPIUI, was created by combining the NCAPI, <b>DataTables,</b> and the Coast Pilot data into one API that contained widgets for an intuitive interface that is similar in look and function as the Google Maps widgets. Google Map API is used for its geo-referenced environment and for its familiar controls that are built-in: namely, the panning, zooming, markers/info boxes and various background-overlay options...|$|R
40|$|The CERN MINUIT library has {{recently}} been ported to C++, facilitating the incorporation of that familiar package into a general purpose fitting application called XYfit with a graphical user interface built with Qt 3. Custom theory functions can easily be added as “plugins” to XYfit, which reads in <b>DataTables</b> in XML format generated by the μView spreadsheet utility (which see), thus allowing more flexible fitting of μSR spectra...|$|R
40|$|Main changes: {{customizable}} indexing (allows {{to index}} strings, numbers, dates [...] . many other {{different types of}} data); display search results in categories (node types) search results can be accessed through URLs and therefore shareable through URLs. search results for table search are displayed in <b>datatable</b> ([URL] Generate a download link for table search results...|$|E
40|$|International audienceLaplacian {{low-rank}} approximations {{are much}} appreciated {{in the context}} of graph spectral methods and Correspondence Analysis. We address here the problem of determining the dimensionality K* of the relevant eigenspace of a general binary <b>datatable</b> by a statistically well-founded method. We propose 1) a general framework for graph adjacency matrices and any rectangular binary matrix, 2) a randomization test for fixing K*. We illustrate with both artificial and real data...|$|E
40|$|CaWO_ 4 (scheelite) and CaMoO_ 4 (powellite) series {{minerals}} have {{a continuous}} solid solution. The solid solution were synthesized by dry method at 850 ℃. The synthetic minerals {{in this series}} are aggregate of polyhedral crystals. X-ray powder diffraction <b>data(Table</b> 1) data for synthetic scheelite and powellite are examined by Guinier method and unit cell parameters (Table 2) obtained by a least squares calculation. These values (Figure 2) increased linearly with increasing Mo contents...|$|E
25|$|In October 2013, The Weekly Standard {{reported}} {{the site was}} violating the copyrights of SpryMedia, a UK-based technology company, by utilizing their software with the copyright notices removed. The software was <b>DataTables,</b> a free and open-source plugin for jQuery designed to improve presentation of data, and was dual-licensed under the GNU GPL version 2 and a modified 3-clause BSD license. HealthCare.gov subsequently rectified the license violation by providing appropriate attribution, license and copyright notices.|$|R
40|$|The dataset is {{composed}} of 50 years of approximately monthly mesozooplankton sampling at station Stoncica. The volume of filtered water was estimated assuming 70 % efficiency of the Hensen 330 micrometer mesh size net according to Laevastu (1958). Subsamples amounting to 1 / 20 of the catch were counted for the most representative groups. The whole catch was examined for rare species. The results in the <b>datatables</b> are recalculated to (#/m** 3) ...|$|R
50|$|In October 2013, The Weekly Standard {{reported}} {{the site was}} violating the copyrights of SpryMedia, a UK-based technology company, by utilizing their software with the copyright notices removed. The software was <b>DataTables,</b> a free and open-source plugin for jQuery designed to improve presentation of data, and was dual-licensed under the GNU GPL version 2 and a modified 3-clause BSD license. HealthCare.gov subsequently rectified the license violation by providing appropriate attribution, license and copyright notices.|$|R
40|$|Abstract — Laplacian {{low-rank}} approximations {{are much}} ap-preciated {{in the context}} of graph spectral methods and Corre-spondence Analysis. We address here the problem of determin-ing the dimensionality K * of the relevant eigenspace of a gen-eral binary <b>datatable</b> by a statistically well-founded method. We propose 1) a general framework for graph adjacency ma-trices and any rectangular binary matrix, 2) a randomization test for fixing K*. We illustrate with both artificial and real data. Keywords-dimensionality reduction; intrinsic dimension; randomization test; low-rank approximation; graph Laplacian; bipartite graph; Correspondence Analysis; Cattell’s scree; binary matrix. I. INTRODUCTION AND STATE-OF-THE-ART Spectral methods are used for optimally condensin...|$|E
40|$|Epidemiologic {{notes and}} reports: Polychlorinated {{biphenyl}} exposure - Indiana [...] Pneumonic tularemia - Washington [...] International notes: Infant botulism - England [...] Quarantine measures [...] Current trends: Primary and secondary syphilis - United States, January 1978. Table 1. Summary of reported {{primary and secondary}} syphilis cases by reporting area, January 1978 and January 1977, provisional <b>dataTable</b> I. Summary-Cases of specified notifiable diseases: United States [...] Table II. Notifiable diseases of low frequency: United States [...] Table III. Cases of specified notifiable diseases: United States, weeks ending March 18, 1978, and March 19, 1977 (11 th week) [...] Table IV. Deaths in 121 U. S. cities, week ending March 18, 1978 (11 th week) ...|$|E
40|$|In {{the present}} work five {{different}} hard coatings have been deposited on five qualities {{of tool steel}} by different means of deposition (CVD, PVD, Arc). Substrates were highly polished planar steel samples, on the one hand, and special testing-tools on the other hand. Planar steel substrates were used {{to set up a}} <b>datatable</b> of thin film characteristics such as film thickness, hardness, coefficient of friction, adhesion and Young's modulus. Coated testing-tools were used to evaluate their performance in the `steel-strip-drawing-test'. This testing routine is explained in detail. Three different qualities of steel sheets and two different lubricants were tested. In order to evaluate the compatibility of a hard coated tool for steel sheet drawing two criteria from the `steel-strip-drawing-test', the coefficient of friction and the stick-slip-effect, were estimated...|$|E
40|$|We present our fast {{ionisation}} routine used {{to study}} transient softX-raylasers with ARWEN, a two-dimensional hydrodynamic code incorporating adaptative mesh refinement (AMR) and radiative transport. We compute global rates between ion stages assuming an effective temperature between singly-excited levels of each ion. A two-step method is used to obtain in a straightforward manner the variation of ion populations over long hydrodynamic time steps. We compare our model with existing theoretical results both stationary and transient, finding that the discrepancies are moderate except for large densities. We simulate an existing Molybdenum Ni-like transient softX-raylaser with ARWEN. Use of the fast ionisation routine leads to a larger increase in temperature and a larger gain zone than when LTE <b>datatables</b> are used...|$|R
40|$|International audienceBeside CA and log-linear model, {{issued from}} the {{statistics}} domain, other research streams originating in Artificial Intelligence have coped with the interacting variables problem: we will present here the extension to categorical variables of our results on extracting and statistically validating " itemsets " in boolean <b>datatables.</b> We coined MIDOVA (Multidimensional Interaction Differential of Variation) our method for highlighting and representing complex links between qualitative variables, which includes interaction, well-suited to socio-economic data. We will {{compare it to}} the CA and log-linear model approaches, using the same 3 -way example as Escofier and her colleagues. We will show that out method is effective for general N-way interactions (N may be far greater than 3), whether symmetrically or not, and results both in easy and detailed interpretability, as CA does, and in statistical significance testing, as the log-linear model does in the case of few variables...|$|R
40|$|Multimodal {{clustering}} is an unsupervised {{technique for}} mining interesting patterns in $n$-adic binary relations or $n$-mode networks. Among {{different types of}} such generalized patterns one can find biclusters and formal concepts (maximal bicliques) for 2 -mode case, triclusters and triconcepts for 3 -mode case, closed $n$-sets for $n$-mode case, etc. Object-attribute biclustering (OA-biclustering) for mining large binary <b>datatables</b> (formal contexts or 2 -mode networks) arose {{by the end of}} the last decade due to intractability of computation problems related to formal concepts; this type of patterns was proposed as a meaningful and scalable approximation of formal concepts. In this paper, our aim is to present recent advance in OA-biclustering and its extensions to mining multi-mode communities in SNA setting. We also discuss connection between clustering coefficients known in SNA community for 1 -mode and 2 -mode networks and OA-bicluster density, the main quality measure of an OA-bicluster. Our experiments with 2 -, 3 -, and 4 -mode large real-world networks show that this type of patterns is suitable for community detection in multi-mode cases within reasonable time even though the number of corresponding $n$-cliques is still unknown due to computation difficulties. An interpretation of OA-biclusters for 1 -mode networks is provided as well...|$|R
40|$|ISBN : 978 - 3 - 7908 - 2603 - 6 International audienceDetermining {{the number}} of {{relevant}} dimensions in the eigen-space of a data matrix is a central issue in many data-mining applications. We tackle here the sub-problem of finding the ''right'' dimensionality of a type of data matrices often encountered in the domains of text or usage mining: large, sparse, high-dimensional binary datatables. We present here {{the application of a}} randomization test to this problem. We validate our approach first on artificial datasets, then on a real documentary data collection, i. e. 1900 documents described in a 3600 keywords dataspace, where the actual, intrinsic dimension appears to be 28 times less than {{the number of}} keywords - an important information when preparing to cluster or discriminate such data. We also present preliminary results on the problem of clearing the <b>datatable</b> from non-essential information bits...|$|E
40|$|International audienceOur goal is twofold: 1) {{we want to}} {{mine the}} only {{statistically}} valid 2 -itemsets out of a boolean <b>datatable,</b> 2) on this basis, we want to build the only higher-order non-redundant itemsets compared to their sub-itemsets. For the first task we have designed a randomization test (Tournebool) respectful {{of the structure of}} the data variables and independant from the specific distributions of the data. In our test set (193 texts and 888 terms), this leads to a reduction from 400, 000 2 -itemsets to 4000 significant ones, at the 95 % confidence interval. For the second task, we have devised a hierarchical stepwise procedure (MIDOVA) for evaluating the residual amount of variation devoted to higher-order itemsets, yielding new possible positive or negative high-order relations. On our example, this leads to 2300 3 -itemsets, 41 4 -itemsets, and no higher-order ones, in a computationally efficient way...|$|E
40|$|ISBN : 978 - 1 - 4244 - 4839 - 5 International audienceWe present here an {{algorithm}} for decomposing any binary <b>datatable</b> into {{a set of}} “sufficient itemsets”, i. e. a non-redundant list of itemsets {{adequate for}} reconstructing the whole table up to a permutation of the rows. For doing so, we have replaced the “support” threshold criterion of the well-known Apriori algorithm by a “number of liberties”: the liberty count expresses how a (k+ 1) -level itemset is constrained by its k-level “parents”, till the level when the situation turns frozen. Our algorithm is symmetric: we {{take into account the}} absence of items as well as their presence in our itemsets. Conversely, we present a method for reconstituting the original data starting from our exact MIDOVA representation. We illustrate these points with the examples of Breast Cancer and Mushroom datasets from UCI Repository. We validate our approach by deriving a learning classifier approach and applying it to three discrimination problems drawn from the above-mentioned repository...|$|E
40|$|Correspondence Analysis (CA) is {{particularly}} suited to categorical variables, {{as long as}} 2 -way contingency tables are concerned. (Mourad 1983) {{has pointed out that}} its extension to 3 -way contingency tables is far from trivial, due to interaction effects between the variables. (Escofier 1983) has provided a non-symmetric solution to this problem, through the example of a 3 -way qualification×profession×gender table, disregarding interaction in this first approach. Then (Escofier & Pagès 1988) took interaction into account, still in a non-symmetric scheme, and illustrated with the same example, and in (Abdessemed & Escofier 2000) this CA approach was contrasted with the log-linear model one. Beside CA and log-linear model, issued from the statistics domain, other research streams originating in Artificial Intelligence have coped with the same problem: we will present here the extension to categorical variables of our results on extracting and statistically validating « itemsets » in boolean <b>datatables,</b> results first published in (Cadot 2006) – for a survey on itemset approaches, see (Han 2001). We coined MIDOVA (Multidimensional Interaction Differential of Variation) our method for highlighting and representing complex links between qualitative variables, which includes interaction, well-suited to socio-economic data (Haj Ali & Cadot 2010). We will compare it to the CA and log-linear model approaches, using the same 3 -way example as Escofier and her colleagues. We will show that out method is effective for general N-way interactions (N may be far greater tha...|$|R
40|$|We {{present a}} new {{object-oriented}} radiative-magnetogasdynamics (RMHD) code based on unstructured grid technology {{which is expected}} to be a convenient tool for investigations of plasma dynamics in various pulsed-power facilities. High-performance computing is a proper tool for simulations of complex multiscale nonlinear processes like transient flows of strongly radiative multicharged plasmas. The first tests of this new code have showed promising results. The code performs calculations in (r-z) and (x-y) coordinates. We accept a single-fluid MHD model [1]. The MHD-equations are written in a 2. 5 D approximation, i. e. the main flowfield vectors are presented by all three components. Anisotropy of dissipative processes in the presence of magnetic field is accounted. The electron-ion energy relaxation is included. Radiative energy transfer is described by the equation for spectral radiation intensity. To overcome the inhomogeneity of magnetically-driven plasmas we solve the radiative transport equation by means of semi-analytical characteristic algorithm. An additional grid of rays is necessary for its implementation. A modified FE-DSN-method [2] as well as the method based on the adjoint radiative transfer problem solution [3] are also developed and put into practice. The radiative energy transfer is calculated via multigroup spectral approximation. For practical applications the <b>datatables</b> of plasma properties as to thermodynamics, ionisation state, opacities and emissivities are used. The MHD equations written as conservation laws are discretized by means of a finite-volume method. The set of finite volumes is built for basic unstructured triangular mesh. All the physical variables are located in the triangular grid nodes. A splitting scheme is applied to the full MHD system. The procedure of time-advance is an explicit second-order predictor-corrector. A general monotonous reconstruction of mesh-defined functions was designed with respect to the dependence on two variables and a generalized quasi-monotonous Lax-Friedrichs-Tadmor scheme was developed [4]. The explicit difference scheme allows quite natural parallel implementation for distributed computer systems. A technique of margins was suggested for data exchange between subdomains with unstructured grids located at different processors. The designed code was applied to simulations of a dense strongly radiative Z-pinches formed of an exploded multiwire array. Comparisons with experimental data obtained in Sandia National Laboratory (USA) are presented...|$|R
40|$|School of Information and Communication Technology (ICT) at Royal Institute of Technology (KTH) have 16 {{computers}} {{that they are}} not using, which they bought from PDC at KTH a couple of years ago. Teachers of KTH are interested in using these computers to let their students work on a cloud computer environment. There is an open source system called OpenNebula, which is used on many universities and by many other persons setting up a cloud computer environment. OpenNebula is an distributed virtual machine manager that allows virtualization of ITinfrastructure, provides good environment for user management and setting up storage sub systems and has other desirable characterics suitable for laborations in classes. One of the goals of this project was to install OpenNebula on 16 computers, with one of the computers being the front end. I call this cluster of nodes Cloudelia. These are the requirements of the system: In order for a user to use the system, authentication {{needs to be done to}} ensure that it has a KTHaccount. When an administrator sees a get permission-request from an user and is deciding on whether to approve the user or not, it must be able to rely on that an authentication-mechanism ensures that the user really is the user with the specific KTH-user name shown in the interface. This ensures that any user outside of KTH with an intent to use Cloudelia in a malicious way doesn’t get access to the system. The teachers should be provided an interface in which they can handle the granting/denying of permission of the users whom have requested permission to use Cloudelia. They should be able to do this for all of the users in an efficient way and upon granting access to the users, new user accounts should be created in OpenNebula. This reduces the work load for the teachers. There were certain design choices that were made, including the choice of whether to run OpenNebula with a shared or non-shared file system, whether to use Kerberos or Central Authentication Service (CAS) for authentication and the choice of which virtual machine to use. The web interfaces were implemented using PHP, AJAX and MySQL. The web interface for teachers used an AJAX-framework called <b>DataTables</b> [1], which facilitates and minimizes the code amount required for presenting data from e. g. MySQL in tables on a web page. It was chosen to be used for the presentation of the users of the system in the administration interface for this reason. AJAX was used because it provides good capabilities of creating a website with interaction with the user. The back end on the server side was implemeted in PHP. It receives arguments by POST and GET. There are different php-files receiving data from the web interfaces with different responsibilities...|$|R
40|$|International audienceIn {{standard}} multivariate data analysis, individuals × variables {{data table}} is usually considered (two-way <b>datatable).</b> However, {{from a practical}} view point this simple data structure appears to be somehow limitated. It isthe case for instance when individuals are charaterized by the temperature at different locations sampled overdifferent times, leading to a three-way data structure. Such multi-way structure {{can be viewed as}} a stack ofmatrices X = Xi jk 1 ≤i≤I, 1 ≤j≤J, 1 ≤k≤K from which the I horizontal slices describe the individuals i = 1, [...] ., I, theJ lateral slices describe the variables (temperature) j = 1, [...] .,J and the K frontal slices describe the different timepoints k = 1, [...] .,K. Many two-way data analysis methods have been extended to the multi-way configuration. Forinstance, a multi-way formulation of Partial Least Squares Regression (N-PLS) has been proposed in [1]. N-PLSrelies on the maximization of a covariance criterion but explicitely takes into acount the multi-way structure ofthe input data. In this paper, we present a Multi-way formulation of Fisher Discriminant Analysis (MFDA) in anattempt to improve the interpretability of the resulting model compared with the results obtained with unfoldedmethods. MFDA is illustrated on a real multi-modalMagnetic Resonance Brain Imaging (MRI) datase...|$|E
40|$|NLPCA-Viz This {{is a basic}} {{application}} meant to do data imputation and perform an Non-Linear Principal Component. This is an R Shiny Application that requires R and will install numerous dependencies to make run. Run the following {{to bring up the}} applet on your computer. ''' install. packages('shiny') library(shiny) runGitHub('NLPCA-Viz','pavan 8632 ') ''' News about NLPCA-Viz The first iteration is in use and capable of taking you through all steps from initial input of a. csv file to a visualization of the multiple imputed data sets and a novel visualization developed by the Ferguson Lab in UCSF's Brain and Spinal Injury Center. Future iterations will take this framework and port the R-Shiny application to an open source, PHP/JS driven webpage that optimizes efficiency. Outline of Workflow First input a file, in a. csv format, using the sidebar on the left. Then you can choose functions to apply to parse and filter your data. The first tab allows the user to remove columns with no variance(No Vary) and remove columns and rows with only NA values (NA Col, NA Row) Second, the user can choose which columns to use for the imputation by choosing column names and then viewing their final data table. Third, the user is told to classify their columns as either categorical or numerical variables. This is important for the NL-PCA and imputation techniques. Fourth, the user can use two methods of imputation. An iterative PCA based approach, and a Monte-Carlo Multiple Chained Equation approach. All data tables are stored and the user can view summary statistics of each. In the final tab, the user can choose the <b>datatable</b> to use [...] either the original data or one of the imputed data sets and run an NL-PCA. The NL-PCA used is an extension of the homals packaged published by the GIFI group. [URL] All visualizations and data tables used are downloadable for reproducibility and publishing...|$|E
40|$|According to the Virial Theorem, all {{gravitational}} {{systems in}} equilibrium {{sit on a}} plane in the 3 D parameter space defined by their mass, size and second moment of the velocity tensor. While these quantities cannot be directly observed, there are suitable proxies: the luminosity Lk, half-light radius Re and dispersion sigma_e. These proxies indeed lie on a very tight Fundamental Plane (FP). How do the black holes in the centers of galaxies relate to the FP? Their masses are known to exhibit no strong correlation with total galaxy mass, but they do correlate weakly with bulge mass (when present), and extremely well with the velocity dispersion through the Mbh = sigma_e^ 5. 4 relation. These facts together imply that a tight plane must also exist defined by black hole mass, total galaxy mass and size. Here I show that this is indeed the case using a heterogeneous set of 230 black holes. The sample includes BHs from zero to 10 billion solar masses and host galaxies ranging from low surface brightness dwarfs, through bulge-less disks, to brightest cluster galaxies. The resulting BH-size-luminosity Mbh=(Lk/Re) ^ 3. 8 has {{the same amount of}} scatter as the M-sigma relation and is aligned with the galaxy FP, such that it is just a re-projection of sigma. The inferred BH-size-mass relation is Mbh=(M_star/Re) ^ 2. 9. These relationships are universal and extend to galaxies without bulges. This implies that the black hole is primarily correlated with its global velocity dispersion and not with the properties of the bulge. I show that the classical bulge [...] mass relation is a projection of the M-sigma relation. When the velocity dispersion cannot be measured (at high-z or low dispersions), the BH-size-mass relation should be used as a proxy for black hole mass in favor of just galaxy or bulge mass. Comment: 13 pages, 8 figures, and a large table. Accepted to ApJ. <b>Datatable</b> and code at [URL]...|$|E
40|$|Accord. NET Framework 3. 8. 0 release notes 22. 10. 2017. Version updates and fixes: 	GH- 82 : Add {{support for}} {{weighted}} PCA; 	GH- 127 : Fast KMeans (Request); 	GH- 145 : MovingNormalStatistics; 	GH- 157 : Issue in Survival analysis using VB. NET; 	GH- 184 : Add an Example for Graylevel coocurrences; 	GH- 211 : Any samples {{on how to}} use Boosted Decision Trees; 	GH- 257 : DFT functions in AForge. Math. FourierTransform and Accord. Math. Transforms; 	GH- 262 : C 45 Learning Discrete vs Real; 	GH- 374 : Dictionary of video capabilities doesn't take into account the video framerate; 	GH- 376 : Add an Example for VideoCaptureDevice Class; 	GH- 377 : Add an Example for LevenbergMarquardt Class; 	GH- 415 : Add an Example for AdaBoost(TModel); 	GH- 421 : Add an Example for KalmanFilter 2 D Class; 	GH- 422 : Add an Example for DecisionStump. Learn Method; 	GH- 424 : Add an Example for AdaBoost(TModel) Class; 	GH- 430 : Add an Example for GeneralConfusionMatrix Constructor (Int 32, Int 32 [], Int 32 []); 	GH- 440 : BagOfWords for audio signals; 	GH- 441 : Add Mel Frequency Cepstral Coefficients (MFCC); 	GH- 466 : Add an Example for Distance. Mahalanobis Method (Double[], Double[]); 	GH- 467 : Add an Example for Spline Class; 	GH- 473 : Add an Example for IParallel. ParallelOptions Property; 	GH- 476 : Add an Example for TwoWayAnova Constructor (Double[][][], TwoWayAnovaModel); 	GH- 483 : Add an Example for Haralick Class; 	GH- 486 : Add an Example for LibSvmModel. Save Method (String); 	GH- 498 : Add an Example for QLearning Class; 	GH- 500 : Add an Example for FourierTransform 2. FFT Method (Complex[], FourierTransform. Direction); 	GH- 505 : Add an Example for ConfusionMatrix Constructor (Int 32 [], Int 32 [], Int 32, Int 32); 	GH- 517 : Add an Example for Sarsa Class; 	GH- 519 : Add an Example for NelderMead Class; 	GH- 530 : Add an Example for Matrix. Inverse Method (Double[][]); 	GH- 532 : Add an Example for GaussNewton Class; 	GH- 547 : Add an Example for HaralickDescriptor Class; 	GH- 554 : Add an Example for BinaryTree(TNode) Class; 	GH- 557 : Add an Example for Matrix. Sort(TKey, TValue) Method (TKey[], TValue[,], IComparer(TKey)); 	GH- 560 : Add an Example for FourierTransform 2. FFT Method (Double[], Double[], FourierTransform. Direction); 	GH- 566 : Add an Example for Distance. GetDistance(T) Method; 	GH- 569 : Add an Example for Distance. Euclidean Method (Double, Double, Double, Double); 	GH- 575 : Add an Example for LuDecomposition Class; 	GH- 576 : RandomForestLearning: Examples can't run with SampleRatio not equal 1. 0; 	GH- 582 : Add an Example for Matrix. Multiply Method (Double[], Double[,]); 	GH- 610 : Add an Example for UnivariateContinuousDistribution. Fit Method (Double[]); 	GH- 616 : Add an Example for LevenbergMarquardt Class; 	GH- 618 : Add an Example for Apriori Class; 	GH- 629 : Add an Example for AdaBoost(TModel) Class; 	GH- 636 : Add an Example for Measures. Correlation Method (Double[][]); 	GH- 640 : Add an Example for GaussianKernel Class; 	GH- 642 : Add an Example for Matrix. PseudoInverse Method (Decimal[,]); 	GH- 653 : Add an Example for HistogramsOfOrientedGradients Class; 	GH- 656 : Add an Example for MatReader. Read Method (String); 	GH- 660 : Add an Example for LogLikelihoodLoss Class; 	GH- 665 : Add an Example for FourierTransform. FFT Method; 	GH- 687 : Add an Example for ShapiroWilkTest Class; 	GH- 695 : Add an Example for TFIDF. Transform Method (String[][]); 	GH- 703 : Add an Example for Imputation Class; 	GH- 717 : Possible issue with DynamicTimeWarp kernel class; 	GH- 718 : Add an Example for Cosine. Distance Method; 	GH- 723 : Procrustes analysis is giving weird/wrong results; 	GH- 727 : Add an Example for IRadialBasisKernel Interface; 	GH- 730 : Binary-Split with normalized FREAK; 	GH- 739 : Add an Example for MultipleLinearRegression. CoefficientOfDetermination Method; 	GH- 756 : Add an Example for ProportionalHazardsAnalysis. LogLikelihood Property; 	GH- 764 : Add an Example for AndersonDarlingTest Class; 	GH- 769 : Issue using visual bag of words with large images; 	GH- 783 : Add an Example for LocalBinaryPattern Class; 	GH- 785 : Add an Example for Tools. RandomGroups Method (Int 32, Double); 	GH- 787 : Add an Example for HiddenMarkovModel(TDistribution, TObservation). Predict Method (TObservation[]); 	GH- 789 : Add support for OS X; 	GH- 792 : Add an Example for FisherExactTest Class; 	GH- 793 : Add an Example for HoughLineTransformation Class; 	GH- 798 : System. AccessViolationException in FastBoxBlur; 	GH- 800 : Missing dependency for Accord. Neuro in NuGet; 	GH- 802 : Index outside of the bounds of the array in Naive Bayes; 	GH- 803 : NaN probabilities from large features with MultinomialLogisticRegression; 	GH- 805 : Unsafe keyword being exposed in the public API; 	GH- 807 : Add an Example for CrossValidating NaiveBayes; 	GH- 809 : The Codification filter should honor the value of DefaultMissingValueReplacement unless overriden; 	GH- 811 : Naive Bayes should provide better argument checking for negative symbols; 	GH- 812 : ZhangSuenSkeletonization filter not exist to use; 	GH- 814 : Add an Example for MulticlassSupportVectorMachine(TKernel, TInput) Class; 	GH- 818 : Add an Example for LinearConstraint Class; 	GH- 819 : Quadratic Objective Function to support basic vector operations; 	GH- 820 : Augmented Lagrangian to support linear constraints; 	GH- 824 : Improve number of class inference in ZeroOneLoss; 	GH- 825 : Replace multi-dimentional with jagged arrays in IntegralImage. cs; 	GH- 828 : Accord. Neuro under. Net Standard 2. 0; 	GH- 830 : Read PGM image pending; 	GH- 831 : Index outside of the bounds of the array in Naive Bayes; 	GH- 843 : Where is Accord. NET AdaBoost Decide method; 	GH- 845 : Add an Example for Decision Structure; 	GH- 848 : Wilcoxon Signed Rank Test for PAIRED samples: TwoSampleWilcoxonSignedRankTest; 	GH- 849 : TwoSampleWilcoxonSignedRankTest crashing when sample vectors are exactly the same values; 	GH- 852 : Add an Example for DecisionSet Class; 	GH- 853 : Access to last Hessian in BoundedBroydenFletcherGoldfarbShanno; 	GH- 856 : Add an overload to IsSymmetric that accepts a tolerance; 	GH- 857 : Mann-Whitney-U Test producing strange results; 	GH- 862 : Accord. Math -> Vector -> T[] Sample (T[] values, int size) incorrect; 	GH- 865 : Measures. Quartiles: value for Q 1 (lower quartile) wrong in QuantileMethod. R; 	GH- 873 : Add an Example for DecisionRule Class; 	GH- 876 : Allow the maximum frame rate possible in DirectShow for VideoCaptureDevice; 	GH- 877 : Add an Example for HaarCascadeWriter Class; 	GH- 878 : Accord. Math. Transforms. FourierTransform 2. DFT 2 Is bugged; 	GH- 882 : Adding lazy evaluation to matrix decompositions; 	GH- 885 : Add an Example for Signal. GetEnergy Method; 	GH- 890 : Add an Example for MultinomialLogisticRegressionAnalysis Class; 	GH- 897 : Wrong status text in ImageView from DebugVisualizer; 	GH- 898 : The Range method is producing some unexpected results; 	GH- 899 : Add an Example for IntegralImage 2 Class; 	GH- 900 : Add an Example for ExhaustiveTemplateMatching Class; 	GH- 901 : Add an Example for HSL Class; 	GH- 911 : Character case of folder name; 	GH- 913 : KNearestNeighbors can not be serialized; 	GH- 917 : Two C++ projects require "Platform toolset v 141 " which is only available on VS 2017; 	GH- 919 : Build failed for Samples. sln on VS 2015; 	GH- 921 : Fix for the normal random number generator when a seed is specified; 	GH- 924, GH- 925 : Fixing the seeded exponential generator; 	GH- 927 : Broadcasting dimension seems counter-intuitive; 	GH- 929 : Add an Example for SpeededUpRobustFeaturesDescriptor Class; 	GH- 930 : Add an Example for FastCornersDetector Class; 	GH- 931 : Add an Example for MatchingTracker Class; 	GH- 937 : Add an Example for LogisticRegression Class; 	GH- 948 : Accord. Video. FFMPEG. VideoFileReader should provide frame-based random access; 	GH- 949 : Add the Free Spoken Digits Dataset to Accord. DataSets; 	GH- 950 : Add a dataset for example test videos; 	GH- 955 : KalmanFilter 2 D throws System. NullReferenceException; 	GH- 956 : Integrate AForge. NET fixes (up to September 6); 	 	General 	 		The libsonly script is now in RAR 4 format instead of RAR 5 so they will not be listed as corrupted files by Linux/MacOSX decompressors; 	 	 	 	Core 	 		Splitting ITransform into ITransform and ICovariantTransform (to support generic covariance); 	 	 	 	Video. FFMPEG 	 		Standardizing the C++ projects to depend on VS 2015 runtime instead of VS 2017 to keep compatibility with VS 2015; 		Adding a static constructor in the FFMPEG project to check whether the system has those dependencies installed; 	 	 	 	Audio 	 		Adding the initial version for a MelFrequencyCepstrumCoefficients audio feature extractor; 		Adding a IAudioFeatureExtractor interface (akin to the IImageFeatureExtractor for Accord. Imaging); 		Adding a Mono filter to convert multi-channel audio signals into single channel (mono) signals; 		Adding a Signal. FromFile() method to load audio signals from file similarly to Bitmap. FromFile(); 		Adding an AudioDecoder class akin to ImageDecoder to find audio format decoders based on file extension; 	 	 	 	Audition 	 		Adding BagOfAudioWords class to compute bag-of-word representations from audio signals; 	 	 	 	Imaging 	 		Adding support for decoding PNM files in format P 2 and P 3 (besides the already supported P 5 and P 6); 		Updating Haralick's to use the same normalization method as HOG and LBP; 		Updating the color classes (RGB, HSL, YCbCr) to be structs; 		Adding conversion operators between different color classes; 		Updating ImageDecoder to find decoders using reflection instead of manual registration; 		Updating the feature extraction framelet to implement the ITransform interfaces and deprecating IFeatureDetector; 		Updating all feature descriptors to be classes rather than structs so the generics covariance can work; 		Integrating AForge. NET's texture generation classes: Adding a base class for texture generation methods, updating them to use the framework-wide random number generator, and deprecating their Reset method; 	 	 	 	DataSets: 	 		Adding the Yin-Yang dataset {{as an example of a}} non-linear 2 D binary classification problem; 		Adding the Servo dataset as an example of a mixed discrete/continuous dataset for regression; 	 	 	 	Math 	 		Adding a methods for the numerical calculation of the Hessian in the FiniteDifferences class; 		Updating Vector. Interval and Vector. Range to behave similar to NumPy's linspance and arange functions; 		Adding new overloads in element-wise operations that accept a VectorType enumeration instead of a integer for specifying to which dimension the element-wise operation should be performed; 		Updating the Digamma and Trigamma functions to handle negative values; 	 	 	 	MachineLearning 	 		Updating the AdaBoost classes to implement the more recent classification framelet; 		Adding a Error property in ConfusionMatrix and GeneralConfusionMatrix (1. 0 - Accuracy); 		Adding named constructors to ConfusionMatrix to create matrices directly from classifiers, their inputs and expected outputs; 		Adding a new IsColor 8 bpp extension method to detect whether an 8 -bpp image is a color image (non-grayscale); 		Adding a new ConvertColor 8 bppToGrayscale 8 bpp extension methods to convert these into grayscale 8 -bpp images; 		Fixing Codification filter transformation for <b>DataTables</b> when only some columns should be converted; 		NumberOfOutputs and NumberOfSymbols should have different implementations depending on the variable type; 		Enforcing alphabetical/default sorted order for symbols in Codification filter (this is a breaking change); 		Codification filter should now transform columns in the same order as specified by the user; 	 	 	 	Statistics 	 		 		Adding exponentially weighted moving average (ewma) methods in Statistics. Measures partial classes; 		 		 		Sample applications 		 			Adding a new sample application demonstrating how to use the framework in Unity 3 D...|$|R
40|$|This is {{a map of}} the {{movement}} of the Great Epizootic generated from a large-scale historical geographic information systems project built in ArcGIS. The author compiled evidence of the first accounts of the arrival of the epizootic in 164 Canadian and US cities and towns based on over 480 newspaper accounts and reports published between 1872 and 1873. This evidence was placed in a GIS <b>datatable,</b> featuring both geographic and time-date fields. Using the ArcGIS Online Time Aware tool, {{the movement}} of the Great Epizootic can be observed in an interactive animated map at [URL] the course of 50 weeks, an outbreak of what was believed to be equine influenza spread from Toronto to nearly every major city in Canada and the United States, infecting an enormous population of urban horses. The disease also infected horses in Mexico and other parts of Latin America. Nineteenth-century cities in Canada and the US were filled with horses. Equine labour provided the power for intra-urban transportation and shipping. They pulled streetcars, delivered goods, and even powered machinery. In 1872 - 73, cities in Canada and the US were connected by an expanding network of railways. The Grand Trunk spanned the most populous provinces of Canada and the Union Pacific recently connected the Atlantic and Pacific urban centres of the US. Railways sped the Great Epizootic across the continent, linking the bodies of horses in Toronto to nearly every city in Canada and the US. This is {{a map of the}} spread of the 1872 - 73 Great Epizootic. It also displays the approximate railway networks in Canada and the US. Each point on the map documents when the disease was first reported to have arrived in that city. Click on the points for details and source information about the arrival of the disease in each city. Use the timeline at the bottom to see how the epizootic spread over time week-by-week...|$|E
40|$|Concurrent with globalization, South Korea has {{experienced}} a gradual shortage of human capital in the form og well-educated engineers. One {{solution to this problem}} is to look to expatriate labor or "global talent", which could provide not only skilled individuals for the emerging services sector, but also replenish a diminishing local pool of engineering talent(also know as "brain drain"). Further, increased economic globalization has intensified competition for such "global talent" and Korea must compete with other advanced countries already lobbying for these individuals. This study investigates two potential sources of skilled expatriate labor for the Korean domestic market: 1) second and third generation ethnic Koreans residing overseas and 2) foreigners studying at Korean universities for PhD degrees, primarily in engineering fields. In order to become a competitive recruiter, however, South Korea must revamp and implement certain immigration policies to attract these pools of individuals to stay in the country. After discussing the status quo in Korea, in terms of the underlying situation as well as existing policies that deal with expatriate labor issues, we look at 'four other advanced countries' (the U. S., Canada, Germany, and Japan) policies to see which approaches have been the most successful and which best apply to the Korean case. In general, the most important variations between these four countries are first, how intensively countries have recruited expatriate labor and second their emphasis on a single national culture versus a "melting pot" of diverse cultures. Statistics on expatriate labor in Korea are used as a type of barometer to gauge where Korea stands in comparison to other advanced nations and where certain improvements might be warranted. We conclude by offering certain policy recommendations that will bolster Korea's competitiveness in recruiting and retaining global talent. ○ Key Points: - Korea is experiencing a depleting pool of local engineering science talent. - Korea must recruit capital to its market in order to remain competitive in ther global economy. - Expatriate labor offers a viable source to remedy the "brain drain" and increase Korea's competitiveness. - Two potential groups of expatriate labor that Korea might focus on are 1) second and third generation ethnic Koreans residing overseas and 2) foreign PhD students, studying at Korean universities. - Korea must thereby revamp and implement certain immigration policies to attract these two groups by learning from the experiences of other advanced nations who have been successful in expatriate labor recruitment and retention. - Specifically Korea should:· restrict its OPT-like program to bebefit primarily graduates of four-year institutions, majoring in seienc and engineering- related fields· closely monitor the success of the Science Card· increase support for foreign postdoctorate students in engineering and the sciences, ad funded by the Ministry of Educations and awarded to top researches at individual universities · create a national database of information on foreign students residing in Korea. This will help answer key questions to the existing <b>dataTable</b> of Contents Ⅰ. Introduction 1 1. Background 1 2. Expatriate Labor and Korea as a Global/Regional Hub 2 3. Expatriate Labor and Engineering Humal Capital 4 4. Paper Objectives and Methods 8 Ⅱ. Market Segmentation 11 1. Unskilled Labor 12 2. Skilled Labor 15 2 - 1. First-Generation Ethnic Koreans Living Overseas 18 2 - 2. Second- and Third-Generation Ethnic Koreans Living Overseas 20 2 - 3. Foreign Students in Korea 21 2 - 4. Skilled and Semi-Skilled Foreigners Working in Korea 26 3. Summary: Focal Points of this Research Project 27 4. Case Selection 29 4 - 1. Canada: Multicultural and Aggressive 31 4 - 2. United States: Multicultural but Passive 36 4 - 3. Germany: Monocultural but Aggressive 42 4 - 4. Japan: Monocultural and Passive 48 Ⅲ. Ethnic Koreans in the Services Sector 53 1. Current Situation in Korea 54 2. Current Policies in Korea 56 3. Best Practices in Other Countries 59 3 - 1. Germany 59 3 - 2. Japan 61 4. Policy Recommendations 65 Ⅳ. Foreign Science and Engineering Students in Korea 73 1. Current Situation in Korea 74 2. Best Practices in Other Countries 79 2 - 1. The United States 80 2 - 2. Canada 81 2 - 3. Germany 82 2 - 4. Japan 82 2 - 5. Summary 82 3. Policy Recommendations 83 Ⅴ. Conclusion 87 Summary 89 References 9...|$|E
