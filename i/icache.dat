14|2|Public
5000|$|Citing an {{internal}} Intel study that tracked kernel releases, Bottomley said Linux performance had dropped about two per centage points at every release, for a cumulative drop of about 12 per cent {{over the last}} ten releases. [...] "Is this a problem?" [...] he asked. -We're getting bloated and huge. Yes, it's a problem ... Uh, I'd love to say we have a plan ... I mean, sometimes it's a bit sad that we are definitely not the streamlined, small, hyper-efficient kernel that I envisioned 15 years ago ... The kernel is huge and bloated, and our <b>icache</b> footprint is scary. I mean, {{there is no question}} about that. And whenever we add a new feature, it only gets worse.|$|E
40|$|This paper {{presents}} a compression scheme for embedded RISC microprocessors' code. The scheme achieves better compression ratios (around 0. 57) than other reported implementations, {{and as the}} Instruction Cache (<b>ICache)</b> holds compressed instructions its effective size is increased and the hit ratio is improved. Moreover, the processor remains unaware of the compression and its functionality is fully preserved. Other important benefits are the reduction in power consumption and improvement of performance brought about by instruction cache compression, which are still to be quantified. The scheme has required the resolutions of issues that arise from both memory and <b>ICache</b> data misalignment and from the compressed to uncompressed address space mapping. These resolutions are briefly described here...|$|E
40|$|Abstract. This paper {{focuses on}} the {{instruction}} fetch resources in a real-time SMT processor to provide an energy-efficient configuration for a soft real-time application running as a high priority thread {{as fast as possible}} while still offering decent progress in low priority or non-real-time thread(s). We propose a fetch mechanism, Fetch-around, where a high priority thread accesses the L 1 <b>ICache,</b> and low priority threads directly access the L 2. This allows both the high and low priority threads to simultaneously fetch instructions, while preventing the low priority threads from thrashing the high priority thread’s <b>ICache</b> data. Overall, we show an energy-performance metric that is 13 % better than the next best policy when the high performance thread priority is 10 x that of the low performance thread...|$|E
50|$|The {{universal}} card movement {{began when}} NFC based mobile wallet solutions failed to gain ground and presented numerous difficulties, {{the two biggest}} being the cost of replacing hardware at the merchants POS and customers needing a NFC capable phone. Since <b>iCache's</b> financial blunder with their Geode card in mid 2012, {{there have been many}} new companies trying to perfect and capitalize on a universal card type product.|$|R
40|$|Deep-submicron CMOS designs {{maintain}} high transistor switching speeds by {{scaling down}} the supply voltage and proportionately reducing the transistor threshold voltage. Lowering the threshold voltage increases leakage energy dissipation due to subthreshold leakage current {{even when the}} transistor is not switching. Estimates suggest a five-fold increase in leakage energy in every future generation. In modern microarchitectures, much of the leakage energy is dissipated in large on-chip cache memory structures with high transistor densities. While cache utilization varies both within and across applications, modern cache designs are fixed in size resulting in transistor leakage inefficiencies. This paper explores an integrated architectural and circuit-level approach to reducing leakage energy in instruction caches (<b>icaches).</b> At the architecture level, we propose the Dynamically ResIzable i-cache (DRI i-cache), a novel i-cache design that dynamically resizes and adapts to an application’s required size. At the circuit-level, we use gated-V dd, a novel mechanism that effectively turns off the supply voltage to, and eliminates leakage in, the SRAM cells in a DRI i-cache’s unused sections. Architectural and circuitlevel simulation results indicate that a DRI i-cache successfully and robustly exploits the cache size variability both within and across applications. Compared to a conventional i-cache using an aggressively-scaled threshold voltage a 64 K DRI i-cache reduces on average both the leakage energy-delay product and cache size by 62 %, with less than 4 % impact on execution time. Our results also indicate that a wide NMOS dual-V t gated-V dd transistor with a charge pump offers the best gating implementation and virtually eliminates leakage energy with minimal increase in an SRAM cell read time area as compared to an i-cache with an aggressively-scaled threshold voltage...|$|R
40|$|Aggressive {{function}} inlining {{can lead}} to significant improvements in execution time. This potential is reduced by extensive instruction cache (<b>Icache)</b> misses caused by code expansion involved with aggressive inlining. It {{is very difficult to}} predict which inlinings will lead to <b>Icache</b> conflicts, as the exact location of code lines in the executable depends on completing the inlining first. In this work we propose a new method for selective inlining called “Icache Loop Blockings” (ILB). In ILB we allow inlinings that do not create multiple inlined copies of the same function in hot execution cycles. This is done to prevent any increase in the <b>Icache</b> footprint of hot execution cycles. This method is significantly more aggressive than previous ones, experiments show it is also better. Results on a server level processor and on an embeded CPU, running SPEC CINT 2000, show an improvement of 10 % in the execution time of the ILB in comparison to other inlining methods. The scheme was implemented in a post link tool (fdpr). This was achieved without bloating the size of the hot code executed at any single point of execution, which is crucial for the embedded processor domain. We have also considered the synergy between code reordering and inlining focusing on the way inlining can help code reordering. This aspect of inlining has not been studied in previous works. ...|$|E
40|$|This paper {{focuses on}} I-cache {{behaviour}} enhancement {{through the application}} of high-level code transformations. Specifically, a flow for the iterative application of the I-Cache performance optimizing transformations is proposed. The procedure of applying transformation is driven by a set of analytical equations, which receive parameters related to code and I-cache structure and predict the number of I-cache misses. Experimental results from a real-life demonstration application shows that order of magnitude reductions of the number of <b>Icache</b> misses can be achieved by the application of the proposed methodology...|$|E
40|$|Low {{power has}} been {{considered}} as an important issue in instruction cache (I-cache) designs. Several {{studies have shown that}} the I-cache can be tuned to reduce power. These techniques, however, exclusively focus on user-level applications, even though there is evidence that many commercial and emerging workloads often involve heavy use of the operating system (OS). This study goes beyond previous work to explore the opportunities to design energy-efficient <b>Icache</b> for system workloads. Employing a full-system experimental framework and a wide range of workloads, we characterize user and OS I-cache accesses and motivate OSaware I-cache tuning to save power. We then present two techniques (OS-aware cache way lookup and OS-aware cache set drowsy mode) to reduce the dynamic and the static power consumption of I-cache. The proposed OS-aware cache way lookup reduces the number of parallel tag comparisons and data array read-outs for cache accesses to save dynamic <b>Icache</b> power in a given operation mode. The proposed OSaware cache set drowsy mode puts I-cache regions that are only heavily used by another operation mode to reduce leakage power. The proposed mechanisms require minimal hardware modification and addition. Simulation based experiments show that with no or negligible impact on performance, applying OS-aware tuning techniques yields significant dynamic and static power savings across the experimented applications. To our knowledge, this is the first work to explore cache power optimization by considering the interactions of application-OS-hardware. It is our belief that the proposed techniques can be applied to improve the I-cache energy efficiency on server processors mostly targeting on modern and commercial applications that heavily invoke OS activities. 1...|$|E
40|$|This is a {{companion}} note to Elementary Microarchitecture Algebra [1] and outlines an algebraic simplication {{proof of the}} pipelined microarchitecture described in that paper. 1 Transforming the Microarchitecture The laws presented in Elementary Microarchitecture Algebra [1], {{as well as others}} introduced in this note, can be used for aggressively restructuring microarchitectures while retaining behavioral equivalence. The example we present here contains three levels of forwarding logic, resolves hazards by stalling the pipeline, and performs branch speculation. The block diagram for this microarchitecture is shown in Figure 1. branch_misp regFile alu mem kill <b>ICache</b> hazard Fig. 1. Microarchitecture before simplication By just using algebraic laws, {{we have been able to}} reduce most of the complexity leaving essentially an unpipelined microarchitecture. We have implemented some of the algebraic laws as a rewrite system in Isabelle. The proof proceeds in stages, according to th [...] ...|$|E
40|$|We {{present an}} {{architecture}} that features dynamic multithreading {{execution of a}} single program. Threads are created automatically by hardware at procedure and loop boundaries and executed speculatively on a simultaneous multithreading pipeline. Data prediction is used to alleviate dependency constraints and enable lookahead execution of the threads. A two-level hierarchy significantly enlarges the instruction window. Efficient selective recovery from the second level instruction window takes place after a mispredicted input to a thread is corrected. The second level is slower to access but {{has the advantage of}} large storage capacity. We show several advantages of this architecture: (1) it minimizes the impact of <b>ICache</b> misses and branch mispredictions by fetching and dispatching instructions out-of-order, (2) it uses a novel value prediction and recovery mechanism to reduce artificial data dependencies created by the use of a stack to manage run-time storage, and (3) it improves the exe [...] ...|$|E
40|$|Abstract. Continuing {{advances}} in semiconductor technology {{and demand for}} higher performance {{will lead to more}} powerful, superpipelined and wider issue processors. Instruction caches in such processors will consume a significant fraction of the on-chip energy due to very wide fetch on each cycle. This paper proposes a new energy-effective design of the fetch unit that exploits the fact that not all instructions in a given I-cache fetch line are used due to taken branches. A Fetch Mask Determination unit is proposed to detect which instructions in an I-cache access will actually be used to avoid fetching any of the other instructions. The solution is evaluated for a 4 -, 8 - and 16 -wide issue processor in 100 nm technology. Results show an average improvement in the <b>Icache</b> Energy-Delay product of 20 % for the 8 -wide issue processor and 33 % for the 16 -wide issue processor for the SPEC 2000, with no negative impact on performance. ...|$|E
40|$|In this paper, {{we present}} two methods to reduce leakage energy by {{dynamically}} resizing the cache during program execution. The first method monitors the miss {{rate of the}} individual subbanks (in a subbanked cache structure) and selectively shuts them if their miss rate falls below a predetermined threshold. Simulations on SPECJVM 98 benchmarks show that for a 64 K <b>Icache,</b> this method results in a leakage reduction of 17 - 69 % for a 4 subbank structure and 18 - 75 % for a 8 subbank structure when the performance penalty is < 1 %. The second method dynamically resizes the cache based on whether the macroblocks (a group of adjacent cache blocks) are being heavily accessed or not. This method has higher area overhead but greater leakage energy reduction. Simulations on {{the same set of}} benchmarks show that this method results in a leakage reduction of 22 - 81 % for the I-cache when the performance penalty is < 0. 1 %, and 17 - 85 % for the D-cache when the performance penalty is < 1 %. 1...|$|E
40|$|An {{architecture}} that features dynamic multithreading {{execution of a}} single program is studied in this dissertation. Threads are created automatically by hardware at procedure and loop boundaries and executed speculatively on a simultaneous multithreading pipeline. Data prediction is used to alleviate dependency constraints and enable lookahead execution of the threads. A two-level hierarchy significantly enlarges the instruction window. Selective recovery from the second level instruction window takes place after a midspredicted input to a thread is corrected. The second level is slower to access but {{has the advantage of}} large storage capacity. We show several advantages of this architecture: (1) it increases a superscalar’s execution throughput and functional units ' utilization, (2) it adds minimal complexity to the critical execution path, therefore, throughput increase is realizable at little or no cycle time cost, (3) it minimizes the impact of <b>ICache</b> misses and branch mispredictions by fetching and dispatching instructions out-of-order, (4) it uses a novel value prediction and recovery mechanism to reduce artificial data dependencies created by the use of a stack to manage run-time storage. (5) it achieves a speedup of almost 30 % on the integer SPEC 95 benchmarks, with equal execution units. This speedup was measured on single thread binaries, without any compiler support, and using a detailed performance simulator...|$|E
40|$|Although modern superscalar {{processors}} achieve high {{branch prediction}} accuracy, certain branches either are inherently {{difficult to predict}} or incur destructive interference in prediction tables, causing significant performance loss due to mispredictions. We propose a novel microarchitecture, called Skipper, to handle such difficult branches by exploiting control-flow independence. Previous approaches to handling difficult branches, one way or another, amount to executing incorrect instructions, squandering cycles and resources such as the i-cache bandwidth. Skipper altogether avoids incorrect instructions by skipping over, without even fetching, the control-flow dependent computation conditioned by a difficult branch. Instead, Skipper fetches and executes the control-flow independent instructions, which are past {{the point where the}} branch's taken and not-taken paths reconverge, and which need to be executed irrespective of the branch outcome. Because Skipper executes the correct control-flow dependent instructions after the difficult branch is resolved, it conserves the valuable resources. Skipper is the first proposal to exploit control-flow independence by skipping over control-flow dependent computation in a superscalar pipeline. Skipper fetches the skipped control-flow dependent instructions after the post-reconvergent instructions, out of program order. We describe key mechanisms to implement Skipper without unduly complicating the pipeline despite out-of-order fetch. SPECint 95 simulations show that Skipper performs 10 % and 8 % better than superscalar and the previously-proposed Polypath, respectively, when all three microarchitectures have equal <b>icache</b> bandwidth and hardware resources...|$|E
40|$|Dynamic binary {{translation}} systems {{enable a}} wide range of applications such as program instrumentation, optimization, and security. DBTs use a software code cache to store previ-ously translated instructions. The code layout in the code cache greatly differs from the code layout of the original program. This paper provides an exhaustive analysis of the performance of the instruction/trace cache and other structures of the micro-architecture while executing DBTs that focus on program instr-umentation, such as DynamoRIO and Pin. We performed our evaluation along two axes. First, we di-rectly accessed the hardware performance counters to deter-mine actual cache miss counts. Second, we used simulation to analyze the spatial locality of the translated application. Our re-sults show that when executing an application under the control of Pin or DynamoRIO, the <b>icache</b> miss counts actually increase over 2 X. Surprisingly, the L 2 cache and the L 1 data cache show a much lower performance degradation or even break even with the native application. We also found that overall performance degradations are due to the instructions added by the DBT it-self, and that these extra instructions outweigh any possible spatial locality benefits exhibited in the code cache. Our obser-vations held regardless of the trace length, code cache size, or the presence of a hardware trace cache. These results provide {{a better understanding of the}} efficiency of current instrumen-tation tools and their effects on instruction/trace cache perfor-mance and other structures of the microarchitecture. 1...|$|E
40|$|This paper {{describes}} {{a framework for}} modeling macroscopic program behavior and applies it to optimizing prescient instruction prefetch [...] -a novel technique that uses helper threads to improve single-threaded application performance by performing judicious and timely instruction prefetch. A helper thread is initiated when the main thread encounters a spawn point, and prefetches instructions starting at a distant target point. The target identifies a code region tending to incur I-cache misses that the main thread is likely to execute soon, even though intervening control flow may be unpredictable. The optimization of spawn-target pair selections is formulated by modeling program behavior as a Markov chain based on profile statistics. Execution paths are considered stochastic outcomes, and aspects of program behavior are summarized via path expression mappings. Mappings for computing reaching, and posteriori probability; path length mean, and variance; and expected path footprint are presented. These are used with Tarjan's fast path algorithm to e#ciently estimate the benefit of spawn-target pair selections. Using this framework we propose a spawn-target pair selection algorithm for prescient instruction prefetch. This algorithm has been implemented, and evaluated for the Itanium # Processor Family architecture. A limit study finds 4. 8 % to 17 % speedups on an in-order simultaneous multithreading processor with eight contexts, over nextline and streaming I-prefetch {{for a set of}} benchmarks with high <b>Icache</b> miss rates. The framework in this paper is potentially applicable to other thread speculation techniques...|$|E

