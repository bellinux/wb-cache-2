56|4|Public
50|$|A multikernel {{operating}} system treats a multi-core machine as {{a network of}} independent cores, {{as if it were}} a distributed system. It does not assume shared memory but rather implements <b>inter-process</b> <b>communications</b> as message-passing.|$|E
50|$|The kernel {{provides}} an {{application programming interface}} that is used for all <b>inter-process</b> <b>communications</b> and system commands. Driver processes access this interface to manage memory or request direct resource access. The interface functions are C-compatible.|$|E
50|$|Non-persisted {{files are}} not {{associated}} with a file on a disk. When the last process has finished working with the file, the data is lost. These files are suitable for creating shared memory for <b>inter-process</b> <b>communications</b> (IPC).|$|E
40|$|The aim of Para++ is {{to provide}} a user-level C++ {{interface}} to message passing libraries, by encapsulating the notions of processes and <b>inter-processes</b> <b>communications</b> into specific C++ objects and streams. Actually, this abstraction level allows to implement Para++ with any kind of message passing library. Para++'s main idea is to add new C++ io-streams to allows inter-tasks communications. These streams support all generic scalar datatype (int, float, double, [...] .), plus some mathematical datatypes (Vectors, Matrix, [...] .) in order to exchange data between co-operating tasks. Para++ has been implemented on top of PVM and MPI...|$|R
40|$|The aim of Para++ is {{to provide}} a user-level C++ {{interface}} to message passing libraries, by encapsulating the notions of processes and <b>inter-processes</b> <b>communications</b> into specificC++ objects and streams. Actually, this abstraction level allows to implement Para++ with any kind of message passing library. Para++'s main idea is to add new C++ io-streams allowing inter-tasks communications. These streams support all generic scalar datatype (int, float, double, [...] .), plus somemathematical datatypes (Vectors, Matrix, [...] .). Para++ has been implemented on top of PVM [1] and MPI [2]. 1 Introduction In this paper, we present a new abstraction level for message passing parallelism. The model of communications between tasks still remains unchanged but is simplified by a new C++ interface. This idea was first proposed in [3], but our approach changes on several points. All kind of message passing libraries are always based on identical sequences of primitive calls like : ffl an init function : [...] ...|$|R
5000|$|... "Everything is a file" [...] {{describes}} one of {{the defining}} features of Unix, and its derivatives [...] - [...] that {{a wide range of}} input/output resources such as documents, directories, hard-drives, modems, keyboards, printers and even some <b>inter-process</b> and network <b>communications</b> are simple streams of bytes exposed through the filesystem name space.|$|R
50|$|Some SSI systems allow {{processes}} {{on different}} nodes to communicate using <b>inter-process</b> <b>communications</b> mechanisms {{as if they}} were running on the same machine. On some SSI systems this can even include shared memory (can be emulated with Software Distributed shared memory).|$|E
50|$|LINX {{is a new}} {{generation}} of the OSE Link Handler for <b>Inter-process</b> <b>communications</b> (IPC) that provides high performance and robust functionality developed by ENEA AB in Sweden. LINX was released as free and open-source software, subject to the requirements of the GNU General Public License (GPL), version 2.|$|E
50|$|Aleph used <b>inter-process</b> <b>communications</b> to move data between {{programs}} and the kernel, so applications could transparently access resources on any machine on the local area network (which {{at the time was}} a 3-Mbit/s experimental Xerox Ethernet). The project eventually petered out after several years due to rapid changes in the computer hardware market, but the ideas {{led to the creation of}} Accent at Carnegie Mellon University, leading in turn to Mach.|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimited. The Multibackend Database Supercomputer (MDBS) pioneered in the Naval Postgraduate School Laboratory for Database Systems Research offers an elegant {{solution to the}} four most pressing {{problems associated with the}} traditional approach to very large database management systems: capacity growth, performance improvement, data sharing, and resource consolidation. The purpose of this thesis is to develop a theory of system software portability for this large and complex network application which will facilitate others in the installation and utilization of MDBS. The first challenge is the almost total lack of documentation about MDBS software of use to system porters. A second set of issues revolves around the use of hardware by MDBS, particularly the use of mass storage devices for the storage and manipulation of base- and meta-data. A third challenge concerns the portability of system calls, shell programs, and the C language implementation. A final set of portability issues arises from the extensive use of <b>inter-process</b> and inter-machine <b>communications</b> by MDBS. Major, United States Marine Corp...|$|R
50|$|In {{computer}} science and information science, {{system is a}} software system which has components as its structure and observable <b>inter-process</b> <b>communications</b> as its behavior. Again, an example will illustrate: There are systems of counting, as with Roman numerals, and various systems for filing papers, or catalogues, and various library systems, of which the Dewey Decimal System is an example. This still fits with the definition of components which are connected together (in this case {{in order to facilitate}} the flow of information).|$|E
50|$|The VxWorks Core Platform {{consists}} {{of a set of}} runtime components and development tools. The run time components are an operating system (UP and SMP; 32- and 64-bit), software for applications support (file system, core network stack, USB stack and <b>inter-process</b> <b>communications)</b> and hardware support (architecture adaptor, processor support library, device driver library and board support packages). VxWorks core development tools are compilers such as Diab, GNU, and Intel C++ Compiler (ICC)) and its build and config tools. The system also includes productivity tools such as its Workbench development suite and Intel tools and development support tools for asset tracking and host support.|$|E
5000|$|Monitor used a pipe-like {{system of}} shared memory {{as the basis}} of its <b>inter-process</b> <b>communications.</b> Data to be sent from one process to another was copied into an empty memory buffer, and when the {{receiving}} program was ready, back out again. The buffer was then returned to the pool. Programs had a very simple API for passing data, using an [...] set of four methods. Client applications send data with [...] and could optionally block using [...] Servers used a mirroring set of calls, [...] and [...] Note that messages had an implicit [...] "return path" [...] for every message sent, making the semantics more like a remote procedure call than Mach's completely I/O-based system.|$|E
50|$|A key {{concept to}} almost all microkernels is {{breaking}} down a single large kernel into {{a set of}} communicating servers. Instead of having a single large program {{in control of the}} entire hardware side of the computer system, these sorts of duties are handed out to smaller programs that are given rights to control different parts of the machine. For instance, a particular server might be given control of the networking hardware, while another has the task of managing the hard drives. Another server would handle the file system, calling both of these lower-level servers. User applications ask for services by sending messages to these servers, using some form of <b>inter-process</b> <b>communications</b> (IPC), as opposed to asking the kernel to do this work via a syscall or trap.|$|E
40|$|Communication-avoiding {{algorithms}} allow redundant computations {{to minimize}} the number of <b>inter-process</b> <b>communications.</b> In this paper, we propose to exploit this redundancy for fault-tolerance purpose. We illustrate this idea with QR factorization of tall and skinny matrices, and we evaluate the number of failures our algorithm can tolerate under different semantics...|$|E
30|$|Multi-spin coding {{refers to}} all {{techniques}} that store and process multiple spins in one memory word. In this paper, we apply the multi-spin coding technique to the 2 D Ising model. In general, multi-spin coding technique {{results in a}} faster algorithm {{as a consequence of}} updating multiple spins simultaneously. However, we mainly employ this technique to reduce the <b>inter-process</b> <b>communications.</b>|$|E
40|$|This paper {{presents}} a proposal of a language, based on synchronous active objects that introduces the CSP primitives into Java. The proposal {{does not use}} channels to realise the <b>inter-process</b> <b>communications,</b> but is shown to offer the same expressive power as the channel based solutions. The paper also shows that the rendezvous defined by CSP or our synchronous objects are very well adapted to the industrial development of event driven applications, handling simultaneously GUIs and accesses to remote applications...|$|E
40|$|Abstract—In FPGA-based {{adaptive}} computing, <b>Inter-Process</b> <b>Communications</b> (IPC) {{are required}} to exchange information among hardware processes which time-multiplex the resources in a same reconfigurable region. In this paper, we use pipes for IPC and analyze the performance in terms of throughput, throughput efficiency and latency in switching contexts. We also present two practical implementations using FPGA BRAM and external DDR memory. Experimental results expose the key role that context switching plays in determining the IPC performance at various pipe sizes and data rates. I...|$|E
30|$|Although it is a {{parallel}} procedure, the analysis results {{of all the}} groups have to be centralized and subsequently redistributed. This is because the procedures before and after this step are often parallelized by partitioning the region of study according to some geological properties (e.g., parameter assignment for geologically homogeneous zones, zonal flow path identification and flow fluid simulations along hydraulic gradients). Thus, the inconsistency in parallel computing designs between the related procedures may influence the algorithm’s performance {{as a result of}} excessive data transformations and <b>inter-process</b> <b>communications.</b>|$|E
40|$|Commercially {{available}} {{flow sheet}} simulators cannot perform optimization runs {{with more than}} one criterion. To overcome this problem, the flow sheet simulator CHEMCAD is combined with an external optimization solver, and the Excel VBA client is used to arrange <b>inter-process</b> <b>communications.</b> The resulting tool for multi-criteria optimization in chemical process engineering is applied to two separation problems. First, a dividing-wall column is used as a theoretical example to test different starting points and different scalarization techniques. Second, a significant decrease in energy demand is achieved for a real continuous world-scale facility...|$|E
40|$|In this paper, we {{describe}} the essential elements of a parallel algorithm for the FDTD method using the MPI (Message Passing Interface) library. To simplify and accelerate the algorithm, an MPI Cartesian 2 D topology is used. The <b>inter-process</b> <b>communications</b> are optimized {{by the use of}} derived data types. A general approach is also explained for parallelizing the auxiliary tools, such as far-field computation, thin-wire treatment, etc. For PMLs, we have used a new method that makes it unnecessary to split the field components. This considerably simplifies the computer programming, and is compatible with the parallel algorithm. Revie...|$|E
40|$|Requirements for {{systems to}} {{continue}} to operate satisfactorily {{in the presence of}} faults has {{led to the development of}} techniques for the construction of fault tolerant software. This thesis addresses the problem of error detection and recovery in distributed systems which consist of a set of communicating sequential processes. A method is presented for the `a priori' design of conversations for this class of distributed system. Petri nets are used to represent the state and to solve state reachability problems for concurrent systems. The dynamic behaviour of the system can be characterised by a state-change table derived from the state reachability tree. Systematic conversation generation is possible by defining a closed boundary on any branch of the state-change table. By relating the state-change table to process attributes it ensures all necessary processes are included in the conversation. The method also ensures properly nested conversations. An implementation of the conversation scheme using the concurrent language occam is proposed. The structure of the conversation is defined using the special features of occam. The proposed implementation gives a structure which is independent of the application and is independent of the number of processes involved. Finally, the integrity of <b>inter-process</b> <b>communications</b> is investigated. The basic communication primitives used in message passing systems are seen to have deficiencies when applied to systems with safety implications. Using a Petri net model a boundary for a time-out mechanism is proposed which will increase the integrity of a system which involves <b>inter-process</b> <b>communications...</b>|$|E
40|$|In {{this paper}} we {{describe}} Mobile-IT Education (MIT. EDU) classroom applications, which demonstrate {{the potential of}} using the MIThril architecture for rapid prototyping of wireless mobile multi-user applications for use in classroom settings. MIThril is a stable, accessible system that combines inexpensive, commodity hardware, a flexible sensor/peripheral interconnection bus, and a powerful, light-weight distributed sensing, classification, and <b>inter-process</b> <b>communications</b> software architecture to facilitate the development of distributed real-time multi-modal and context-aware applications. We demonstrate the power and functionality of this platform by describing a number of MIT. EDU application deployments in educational settings. Initial evaluations of these experiments demonstrate the potential of using the system for realworld interactive m-learning applications...|$|E
30|$|We use C++ {{programming}} language {{to implement the}} algorithm. In our algorithm, every process creates a sub-lattice and the energy is calculated after each Monte Carlo iteration. Each process communicates with its two neighbor processes during the job, and they exchange the boundary spin variables. Finally, the total energy of lattice is calculated by map-reduce method versus the temperature. We use multi-spin coding technique to reduce the <b>inter-process</b> <b>communications.</b> This algorithm has been designed {{in a way that}} an appropriate load-balancing and good scalability exist. It has been executed on the cluster computer of Plasma Physics Research Center which includes 9 nodes and each node consists of two quad-core CPUs. Our results show that this algorithm is more efficient for large lattices and more iterations.|$|E
30|$|In our algorithm, each {{individual}} process creates its own sub-lattice, initializes it, gets all Monte Carlo iterations done and calculates {{the energy of}} the sub-lattice for a specific temperature. Each process communicates with its two neighbor processes during the job and they exchange the boundary spin variables. Finally, the total energy of lattice is calculated by map-reduce method. Since in multi-spin coding technique each spin is stored by 3 bits, <b>inter-process</b> <b>communications</b> are reduced considerably. Because computational load of each sub-lattice is assigned to each process and size of all sub-lattices is equal, an appropriate load balancing exists. Since each process—independent of number of processes—only communicates with its two neighbor processes and the lattice is decomposed into sub-lattices, the algorithm benefits a good scalability.|$|E
40|$|Model-checking {{has proved}} {{effective}} to verify lowlevel properties on isolated parts of operating {{systems such as}} scheduling algorithms or implementations of <b>inter-process</b> <b>communications.</b> In such situations, the relevant implementation is well-localized in the source code, and the modeling language usually lends itself very well to formal paraphrase. However, there are high-level properties of operating systems that require modeling of {{various parts of the}} implementation. For example, task isolation, the property that user threads cannot access kernel memory, requires modeling of thread management, memory management, hardware protection mechanisms, etc. In this paper, we show how to build in the Spin model-checker a model of a multi-threaded operating system (namely, Topsy) that covers most parts of the implementation, thus enabling verification of properties such as task isolation. ...|$|E
40|$|We {{introduce}} UVM, a new {{virtual memory}} subsystem for 4. 4 BSD that makes {{better use of}} existing hardware memory management features to reduce overhead and improve performance. Our novel approach focuses on allowing processes to pass memory to and from other processes and the kernel, and to share memory. This approach reduces or eliminates the need to copy data thus reducing the time spent within the kernel and freeing up cycles for application processing. Unlike the approaches that focus exclusively on the networking and <b>inter-process</b> <b>communications</b> (IPC) subsystems, our approach provides a general framework for solutions that can improve efficiency of the entire I/O subsystem. Our primary objective in creating UVM was to produce a virtual memory system that provides a Unix-like operating system kernel's I/O and IPC subsystems with ef [...] ...|$|E
40|$|Computational Grids are {{emerging}} as a new paradigm for sharing and aggregation of geographically distributed resources for solving large-scale computational problems in science, engineering and commerce. Web services are new distributed application components that conform to the standards that make them externally available and solve certain kinds of widespread industry problems in various disciplines. We have demonstrated in this paper a way to integrate the execution locations and the data locations seamlessly with web services. We call this approach a GridFiles approach. GridFiles is a novel mechanism, which allows manipulation of files or sockets on a remote location {{as if it is}} local. We achieved this by over loading the IO primitives in conventional languages so that they support <b>inter-process</b> <b>communications</b> as well as file operations across a distributed infrastructure similar to the local environment...|$|E
40|$|<b>Inter-process</b> <b>communications</b> (IPC) are {{ubiquitous}} in today’s software systems, {{yet they are}} rarely treated as first-class programming concepts. Implementing crosscutting concerns for message-based IPC are difficult, even using aspect-oriented programming languages (AOPL) such as AspectJ. Many of these challenges are because {{the context of a}} communication-related crosscutting concern is often a conversation consisting of message sends and receives. Hence, developers typically have to implement communication protocols manually using primitive operations, such as connect, send, receive, and close. This dissertation describes an extension to AspectJ, called CommJ, with which developers can implement communication-related concerns in cohesive and loosely coupled aspects. It then presents preliminary, but encouraging results from a subsequent study that begin by defining a reuse and maintenance quality model. Subsequently the results show seven different ways in which CommJ can improve the reusability and maintainability of applications requiring network communications...|$|E
40|$|It {{has been}} years since the {{introduction}} of the Dynamic Network Optimization (DNO) concept, yet the DNO development is still at its infant stage, largely {{due to a lack of}} breakthrough in minimizing the lengthy optimization runtime. Our previous work, a distributed parallel solution, has achieved a significant speed gain. To cater for the increased optimization complexity pressed by the uptake of smartphones and tablets, however, this paper examines the potential areas for further improvement and presents a novel asynchronous distributed parallel design that minimizes the <b>inter-process</b> <b>communications.</b> The new approach is implemented and applied to real-life projects whose results demonstrate an augmented acceleration of 7. 5 times on a 16 -core distributed system compared to 6. 1 of our previous solution. Moreover, there is no degradation in the optimization outcome. This is a solid sprint towards the realization of DNO...|$|E
40|$|Van Emden's {{incremental}} queries {{address the}} inadequacy of current Prolog-style querying mechanism in most logic programming systems for interactive problem-solving. In the context of constraint logic programming, incremental queries involve adding new constraints or deleting old ones from a query after a solution is found. This paper presents an implementation scheme IQ of incremental queries in Constraint Pandora, which defines a class of non-deterministic concurrent constraint logic programming languages. We use Van Hentenryck and Le Provost's scheme (VHLP-scheme hereafter), a re-execution approach, as a starting point. Re-execution is costly in concurrent languages, in which process creation and <b>inter-process</b> <b>communications</b> are common operations. The main idea of IQ is that the basic trail unwinding operation used in backtracking is more e#cient than re-execution in reaching an execution context along a recorded execution path. We modify the conventional trail-unwinding operation in [...] ...|$|E
40|$|In {{the last}} five years, Distributed Shared Memory (DSM) systems have {{received}} increasing attention. Indeed, by releasing the programmer from the management of <b>inter-process</b> <b>communications,</b> they offer a very intuitive and easyto -use programming paradigm. In compensation, such systems often appear, from the programmer point of view, as a "black box" since no information about the actual communications is available. Consequently, {{in the absence of}} visualization and monitoring tools, optimizing, debugging or evaluating the performance of DSM applications is very difficult. In that framework, this paper proposes an original monitoring model based on two new concepts: meta-objects and event manager processes. This model constitutes the basis of an actual monitoring system, called DOSMOS-Trace, that has been designed and implemented to monitor applications developed on top of the DOSMOS DSM system 1. This monitoring environment is analysed in terms of functionalities, protocols and user [...] ...|$|E
40|$|In the CHORUS/MiX R fl {{distributed}} {{operating system}} architecture the microkernel provides system servers with generic services which {{are independent of}} a particular operating system; these services include processor scheduling, memory management and <b>inter-process</b> <b>communications.</b> In turn, co-operating system servers provide at the application programmer 's interface a particular operating system personality. The CHORUS/MiX implementation of UNIX R fl is based on AT&T source code, but is significantly re-structured into a set of system servers. This re-structuring {{has resulted in a}} modular and adaptable system which is well suited to distribution across a loosely coupled parallel architecture. The CHORUS/MiX system is further being developed to provide what has been termed single site semantics (SSS). This will make it possible to create the illusion of UNIX running on a single processor whilst taking advantage of the availability of a number of loosely coupled processors. The IMS T [...] ...|$|E
40|$|This paper {{proposes a}} way to elegantly {{approach}} a problem {{that used to be}} ugly by its nature: to save and restore (serialize) objects to and from magnetic support, in a portable, machine- and language-independent fashion. The solution also addresses intra- and <b>inter-process</b> <b>communications,</b> because the problems raised by interchangeability and portability and are similar in both cases. To achieve this, a high-level description language that addresses low-level issues is designed. Translation from a high-level specification to a certain programming language can be done automatically, thus considerably reducing the effort that programmers used to spend when implementing complicated, standard or non-standard serialization protocols, and increasing the code reliability at the same time. 1 Introduction To persist is to hold to a state or a goal. In computer programs, an example of persistence is retaining information between application invocations. Objects that are created when [...] ...|$|E
40|$|The goal in {{developing}} a robust parallelization tool {{is that it is}} easy to use, it requires minimal modifications to the original serial code, it is extensible {{to a wide variety of}} applications, and that it provide good portable performance. A directive-based parallelization tool is described called the Parallel Pre-processor (PPP) that meets most of these goals. The user inserts directives, in the form of comments, into existing Fortran code. PPP translates the code and directives into a parallel version that runs efficiently on shared and distributed memory high-performance computing platforms including: SGI Origin, IBM SP 2, Cray T 3 E, SUN, Alpha and Intel Clusters. Twenty directives are available to support operations including array re-declarations, <b>inter-process</b> <b>communications,</b> loop transformations, and parallel I/O operations. PPP also provides support for incremental parallelization and parallel debugging. Keywords: Directive-based parallelization, Weather and ocean models, parallel compiler, Fortran source to source translator, distributed memory computer...|$|E
40|$|The growing {{software}} {{content in}} various battery-driven embedded systems {{has led to}} significant interest in technologies for energy-efficient embedded software. While lowenergy software research has, in the past, focused on energy optimization at the instruction and source-code levels, approaches targeted at a higher software level are beginning to gain attention. In this work, we propose a methodology to refine a control-data flow diagram (CDFD) model of an embedded software program into an energy-efficient multi-process software architecture graph (SAG). Our starting representation, the CDFD, is capable of modeling data-dependent control flow. Energy efficiency is achieved by reducing the energy wastage due to context switches and <b>inter-process</b> <b>communications</b> (IPCs). Conditionally-unused computational operations, and their corresponding energy, are also avoided by a condition-aware static scheduler. Finally, code generation is performed from the energy-efficient SAG to produce a multi-process program that implements the original CDFD specification. Experimental results establish {{the efficacy of the}} proposed approach. ...|$|E
