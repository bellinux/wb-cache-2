558|4697|Public
5|$|Over {{the next}} three decades, Kilburn led the {{development}} of a succession of innovative Manchester computers. The first, commenced in 1951, was a development of the Mark I known as the megacycle machine or Meg, that replaced the vacuum tube diodes with solid state ones. This permitted an order of magnitude increase in the clock rate. To add further speed, Kilburn provided for 10-bit parallel CRT memory. It {{was also one of the}} first computers, if not the first, to have <b>floating</b> <b>point</b> <b>arithmetic.</b> Meg operated for the first time in 1954, and nineteen were sold by Ferranti, six of them to customers overseas.|$|E
25|$|Microsoft Math and Mathematica return ComplexInfinity for 1/0. Maple and SageMath return {{an error}} message for 1/0, and {{infinity}} for 1/0.0 (0.0 tells these systems to use <b>floating</b> <b>point</b> <b>arithmetic</b> instead of algebraic arithmetic).|$|E
25|$|This {{formula is}} also {{sometimes}} used {{in connection with}} the sample variance. While useful for hand calculations, it is not advised for computer calculations as it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude and <b>floating</b> <b>point</b> <b>arithmetic</b> is used. This is discussed in the article Algorithms for calculating variance.|$|E
5000|$|... a 16-lane integer and <b>floating</b> <b>point</b> vector <b>Arithmetic</b> Logic Unit (ALU) ...|$|R
5000|$|<b>Floating</b> <b>point</b> - <b>arithmetic</b> {{instructions}} supported an eight-digit mantissa and two-digit characteristic (offset exponent) - MMMMMMMMCC, {{providing a}} range of ±0.00000001E-50 to ±0.99999999E+49. (7 extra operation codes) ...|$|R
50|$|Binary scaling is a {{computer}} programming technique used mainly by embedded C, DSP and assembler programmers to perform a pseudo <b>floating</b> <b>point</b> using integer <b>arithmetic.</b>|$|R
25|$|While Java's <b>floating</b> <b>point</b> <b>arithmetic</b> {{is largely}} based on IEEE 754 (Standard for Binary Floating-Point Arithmetic), certain {{features}} are not supported even when using the strictfp modifier, such as Exception Flags and Directed Roundings — capabilities mandated by IEEE Standard 754. Additionally, the extended precision floating-point types permitted in 754 and present in many processors are not permitted in Java.|$|E
25|$|One of the {{simplest}} problems is the evaluation of a function at a given point. The most straightforward approach, of just plugging in the number in the formula is sometimes not very efficient. For polynomials, a better approach is using the Horner scheme, since it reduces the necessary number of multiplications and additions. Generally, {{it is important to}} estimate and control round-off errors arising from the use of <b>floating</b> <b>point</b> <b>arithmetic.</b>|$|E
25|$|The IEEE floating-point standard, {{supported}} by almost all modern floating-point units, specifies that every <b>floating</b> <b>point</b> <b>arithmetic</b> operation, including division by zero, has a well-defined result. The standard supports signed zero, {{as well as}} infinity and NaN (not a number). There are two zeroes: +0 (positive zero) and −0 (negative zero) and this removes any ambiguity when dividing. In IEEE 754 arithmetic, a÷+0 is positive infinity when a is positive, negative infinity when a is negative, and NaN when a=±0. The infinity signs change when dividing by −0 instead.|$|E
40|$|Small devices {{used in our}} day {{life are}} {{constructed}} with powerful architectures {{that can be used}} for industrial applications when requiring portability and communication facilities. We present in this paper an example of the use of an embedded system, the Zeus epic 520 single board computer, for defect detection in textiles using image processing. We implement the Haar wavelet transform using the embedded visual C++ 4. 0 compiler for Windows CE 5. The algorithm was tested for defect detection using images of fabrics with five types of defects. An average of 95 % in terms of correct defect detection was obtained, achieving a similar performance than using processors with <b>float</b> <b>point</b> <b>arithmetic</b> calculations...|$|R
25|$|The {{subroutine}} concept led to {{the availability}} of a substantial subroutine library. By 1951, 87 subroutines in the following categories were available for general use: <b>floating</b> <b>point</b> arithmetic; <b>arithmetic</b> operations on complex numbers; checking; division; exponentiation; routines relating to functions; differential equations; special functions; power series; logarithms; miscellaneous; print and layout; quadrature; read (input); nth root; trigonometric functions; counting operations (simulating repeat until loops, while loops and for loops); vectors; and matrices.|$|R
40|$|The {{contents}} {{of this book}} include chapters on <b>floating</b> <b>point</b> computer <b>arithmetic,</b> natural and generalized interpolating polynomials, uniform approximation, numerical integration, polynomial splines and many more. This book is intended for undergraduate and graduate students in institutes, colleges, universities and academies who want to specialize in this field. The readers will develop a solid understanding of the concepts of numerical methods and their application. The inclusion of Lagrane and Hermite approximation by polynomials, Trapezian rule, Simpsons rule, Gauss methods and Romberg`s me...|$|R
25|$|The 6809 {{design team}} {{believed}} that future system integrators would look to off-the-shelf code in ROMs to handle common tasks. Motorola's official programming manual contains the full listing of assist09, a so-called monitor, a miniature operating system {{intended to be}} burned in ROM. Another example of ROM code might be binary <b>floating</b> <b>point</b> <b>arithmetic,</b> which is a common requirement in many systems. In order to speed time to market, common code modules would be purchased, rather than developed in-house, and integrated into systems with code from other manufacturers. Since a CPU designer could hardly guarantee where this code would be located in a future system, the 6809 design focused heavily on support of position-independent, reentrant code that could be freely located anywhere in the memory map. This expectation was, in reality, never quite met: Motorola's only released example of a ROM'd software module was the MC6839 floating-point ROM. However, the decisions made by the design team made for a very powerful processor and made possible advanced operating systems like OS-9 and UniFlex, which {{took advantage of the}} position-independent, re-entrant nature of the 6809 to create multi-user multitasking operating systems.|$|E
2500|$|... "Transreal arithmetic" [...] {{closely resembles}} IEEE <b>floating</b> <b>point</b> <b>arithmetic,</b> a <b>floating</b> <b>point</b> <b>arithmetic</b> {{commonly}} used on computers. IEEE <b>floating</b> <b>point</b> <b>arithmetic,</b> like transreal arithmetic, uses affine infinity (two separate infinities, one positive and one negative) rather than projective infinity (a single unsigned infinity, turning the number line into a loop).|$|E
2500|$|Torres 1913 paper, [...] "Essays on Automatics," [...] also {{introduced}} {{the idea of}} <b>floating</b> <b>point</b> <b>arithmetic,</b> which historian Randell says was described [...] "almost casually," [...] apparently without recognizing {{the significance of the}} discovery.|$|E
50|$|Two {{versions}} {{existed for}} the 650s with a 2000 word memory drum: FOR TRANSIT I (S) and FOR TRANSIT II, the latter for machines equipped with indexing registers and automatic <b>floating</b> <b>point</b> decimal (bi-quinary) <b>arithmetic.</b> Appendix A of the manual included wiring diagrams for the IBM 533 card reader/punch control panel.|$|R
40|$|According to {{scientific}} applications, level of precision is more demanding computational point {{which can be}} double precision <b>floating</b> <b>point</b> <b>arithmetic‘s</b> or quadruple precision floating-point arithmetic‘s. Here we analyze the evolution of double-precision floating-point & quadruple precision floating-point computing. Since last few years this application has more demand. Modern science and Engineering models mostly depend on supercomputer simulation to reduce experimentation requirements. The results show that peak-performance for precision addition, Subtraction, multiplication and division on FPGAs is already better than general-purpose processors (GPPs). The canonical signed digit (CSD) representation {{is one of the}} existing signed digit representations with unique features which make it useful in certain DSP applications focusing on low power, area efficient and high speed arithmetic. Canonical signed digit is a recoding technique, which recodes a number with minimum number of non-zero digits. As the number of partial products depends on the number of non-zero digits, by using Canonical recoding, the number of non-zero digits will be reduced, thereby reducing the number of partial products. In this paper, Double & quadruple precisio...|$|R
40|$|Hardware {{implementation}} in FPGA enables parallel operation facility and high speed. This paper proposes an efficient method for retrieval of compressive measurements through an advanced L 1 optimization algorithm known as Linearized Bregman algorithm. Hardware used <b>floating</b> <b>point</b> unit <b>arithmetic</b> operation, {{so that it}} gives the same results as that obtained in microcontroller and Matlab simulation. Linearized Bregman Algorithm (LBA) has good convergence properties and gives fast retrieval of the original information. Our Hardware implementation shows that the recovered images are truly representative of what is obtained by computation...|$|R
2500|$|A {{mnemonic}} for {{the above}} expression is [...] "mean of square minus square of mean". This equation {{should not be}} used for computations using <b>floating</b> <b>point</b> <b>arithmetic</b> because it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude. There exist numerically stable alternatives.|$|E
2500|$|This {{is also a}} repeating binary {{fraction}} 0.0... It {{may come}} as a surprise that terminating decimal fractions can have repeating expansions in binary. It {{is for this reason that}} many are surprised to discover that 0.1 + ... + 0.1, (10 additions) differs from 1 in <b>floating</b> <b>point</b> <b>arithmetic.</b> In fact, the only binary fractions with terminating expansions are of the form of an integer divided by a power of 2, which 1/10 is not.|$|E
2500|$|The older {{term for}} number theory is arithmetic. By {{the early twentieth}} century, it had been {{superseded}} by [...] "number theory". [...] (The word [...] "arithmetic" [...] {{is used by the}} general public to mean [...] "elementary calculations"; it has also acquired other meanings in mathematical logic, as in Peano arithmetic, and computer science, as in <b>floating</b> <b>point</b> <b>arithmetic.)</b> The use of the term arithmetic for number theory regained some ground {{in the second half of}} the 20th century, arguably in part due to French influence. In particular, arithmetical is preferred as an adjective to number-theoretic.|$|E
5|$|Each {{background}} processor {{consisted of}} a computation section, a control section and local memory. The computation section performed 64-bit scalar, <b>floating</b> <b>point</b> and vector <b>arithmetic.</b> The control section provided instruction buffers, memory management functions, and a real-time clock. 16 kwords (128 kbytes) of high-speed local memory was incorporated into each background processor for use as temporary scratch memory.|$|R
50|$|Mathomatic {{can be used}} as a <b>floating</b> <b>point</b> or integer <b>arithmetic</b> code {{generating}} tool, simplifying and converting equations into optimized assignment {{statements in}} the Python, C, and Java programming languages. The output can be made compatible with most other mathematics programs, except TeX and MathML format input/output are currently not available. The ASCII characters that are allowed in Mathomatic variable names is configurable, allowing TeX format variable names.|$|R
50|$|The S/360-67 {{operated}} {{with a basic}} internal {{cycle time}} of 200 nanoseconds and a basic 750 nanosecond magnetic core storage cycle, {{the same as the}} S/360-65. The 200 ns cycle time put the S/360-67 {{in the middle of the}} S/360 line, between the Model 30 at the low end and the Model 195 at the high end. From 1 to 8 bytes (8 data bits and 1 parity bit per byte) could be read or written to processor storage in a single cycle. A 60-bit parallel adder facilitated handling of long fractions in floating-point operations. An 8-bit serial adder enabled simultaneous execution of <b>floating</b> <b>point</b> exponent <b>arithmetic,</b> and also handled decimal arithmetic and variable field length (VFL) instructions.|$|R
2500|$|Anderson quickly gained {{publicity}} in December 2006 in the United Kingdom {{when the}} regional BBC South Today reported his claim of [...] "having solved a 1200 year old problem", namely that of division by zero. However, commentators quickly responded that his ideas {{are just a}} variation of the standard IEEE 754 concept of NaN (Not a Number), which has been commonly employed on computers in <b>floating</b> <b>point</b> <b>arithmetic</b> for many years. Dr Anderson defended against the criticism of his claims on BBC Berkshire on 12 December 2006, saying, [...] "If anyone doubts me I can hit them over the head with a computer that does it." ...|$|E
2500|$|Fractions in binary only {{terminate}} if the denominator has 2 as {{the only}} prime factor. As a result, 1/10 {{does not have a}} finite binary representation, and this causes 10 × 0.1 not to be precisely equal to 1 in <b>floating</b> <b>point</b> <b>arithmetic.</b> As an example, to interpret the binary expression for 1/3 = [...]010101..., this means: 1/3 = 0 × 2−1 + 1 × 2−2 + 0 × 2−3 + [...] 1 × 2−4 + ... = 0.3125 + ... An exact value cannot be found with a sum of a finite number of inverse powers of two, the zeros and ones in the binary representation of 1/3 alternate forever.|$|E
2500|$|The IA32, x86-64, and Itanium {{processors}} {{support an}} 80-bit [...] "double extended" [...] extended precision format with a 64-bit significand. [...] The Intel 8087 math coprocessor {{was the first}} x86 device which supported <b>floating</b> <b>point</b> <b>arithmetic</b> in hardware. [...] It was designed to support a 32-bit [...] "single precision" [...] format and a 64-bit [...] "double precision" [...] format for encoding and interchanging floating point numbers. [...] The temporary real (extended) format was designed not to store data at higher precision as such, but rather primarily {{to allow for the}} computation of double results more reliably and accurately by minimising overflow and roundoff-errors in intermediate calculations: for example, many floating point algorithms (e.g. exponentiation) suffer from significant precision loss when computed using the most direct implementations. [...] To mitigate such issues the internal registers in the 8087 were designed to hold intermediate results in an 80-bit [...] "extended precision" [...] format. [...] The 8087 automatically converts numbers to this format when loading floating point registers from memory and also converts results back to the more conventional formats when storing the registers back into memory. To enable intermediate subexpressions results to be saved in extended precision scratch variables and continued across programming language statements, and otherwise interrupted calculations to resume where they were interrupted, it provides instructions which transfer values between these internal registers and memory without performing any conversion, which therefore enables access to the extended format for calculations- also reviving the issue of the accuracy of functions of such numbers, but at a higher precision.|$|E
40|$|Iterative {{algorithms}} {{based on}} runs and runs of runs are presented {{to calculate the}} cells of the two-dimensional latice intersected by a line of real slope and intercept. The technique is applied to the problem of traversing a ray through a two-dimensional grid. Using runs or runs of runs provides a significant improvement in the efficiency of ray traversal for all but very short path lengths when compared to the DDA algorithm implemented using <b>floating</b> or fixed <b>point</b> <b>arithmetic...</b>|$|R
50|$|The KSR-1 {{processor}} {{was implemented}} as a four-chip set in 1.2 micrometer complementary metal-oxide-semiconductor (CMOS). These chips were: the cell execution unit, the <b>floating</b> <b>point</b> unit, the <b>arithmetic</b> logic unit, and the external I/O unit (XIO). The CEU handled instruction fetch (two per clock), and all operations involving memory, such as loads and stores. 40-bit addresses were used, going to full 64-bit addresses later. The integer unit had 32, 64-bit-wide registers. The <b>floating</b> <b>point</b> unit is discussed below. The XIO {{had the capacity}} of 30 MB/s throughput to I/O devices. It included 64 control and data registers.|$|R
40|$|In {{addition}} to specifying details of <b>floating</b> <b>point</b> formats and <b>arithmetic</b> operations, the IEEE 754 and IEEE 854 standards recommend conforming systems include several utility functions {{to aid in}} writing portable programs. For example, <b>floating</b> <b>point</b> constants associated with a format, such as the largest representable number, can be generated in a format-independent manner using the recommended functions {{and a handful of}} <b>floating</b> <b>point</b> literals. The functions perform useful tasks such as returning the <b>floating</b> <b>point</b> number adjacent to a given value and scaling a <b>floating</b> <b>point</b> number by a power of two. However, the descriptions of the functions are very brief and the two related standards give slightly different specifications for some of the functions. This paper describes robust implementations of the IEEE recommended functions written in " 100 % Pure Java. " These functions work under all the dynamic rounding and trapping modes supported by IEEE 754. Writing such functions in Java pr [...] ...|$|R
5000|$|... "Transreal arithmetic" [...] {{closely resembles}} IEEE <b>floating</b> <b>point</b> <b>arithmetic,</b> a <b>floating</b> <b>point</b> <b>arithmetic</b> {{commonly}} used on computers. IEEE <b>floating</b> <b>point</b> <b>arithmetic,</b> like transreal arithmetic, uses affine infinity (two separate infinities, one positive and one negative) rather than projective infinity (a single unsigned infinity, turning the number line into a loop).|$|E
50|$|Although Java's <b>floating</b> <b>point</b> <b>arithmetic</b> {{is largely}} based on IEEE 754 (Standard for Binary Floating-Point Arithmetic), certain {{features}} are unsupported even when using the strictfp modifier, such as Exception Flags and Directed Roundings, abilities mandated by IEEE Standard 754 (see Criticism of Java, <b>Floating</b> <b>point</b> <b>arithmetic).</b>|$|E
5000|$|... 128-bit (hexadecimal) <b>floating</b> <b>point</b> <b>arithmetic</b> on all models.|$|E
40|$|Independent Component Analysis (ICA) is a {{statistical}} signal processing technique having emerging new practical application areas, such as blind signal separation such as mixed voices or images, analysis of {{several types of}} data or feature extraction. Fast independent component analysis (Fast ICA) {{is one of the}} most efficient ICA technique. Fast ICA algorithm separates the independent sources from their mixtures by measuring non-gaussian. Fast ICA is a common method to identify aircrafts and interference from their mixtures such as electroencephalogram (EEG), magnetoencephalography (MEG), and electrocardiogram (ECG). Therefore, it is valuable to implement Fast ICA for real-time signal processing. In this thesis, the Fast ICA algorithm is implemented by hand coding HDL code. In addition, in order to increase the number of precision, the <b>floating</b> <b>point</b> (FP) <b>arithmetic</b> units are also implemented by HDL coding. To verify the algorithm, MATLAB simulations are also performed for both off line signal rocessing and real-time signal processing...|$|R
40|$|This paper {{presents}} an area efficient architecturefor quadruple precision division arithmetic on the FPGAplatform. Many application demands {{for the higher}} precisioncomputation (like quadruple precision) than the single anddouble precision. Division is an important arithmetic, butrequires {{a huge amount of}} hardware resources with increasingprecision, for a complete hardware implementation. So, thispaper {{presents an}} iterative architecture for quadruple precisiondivision arithmetic with small area requirement and promisingspeed. The implementation follows the standard processingsteps for the <b>floating</b> <b>point</b> division <b>arithmetic,</b> including processing of sub-normal operands and exceptional case handling. The most dominating part of the architecture, the mantissadivision, is based on the series expansion methodology ofdivision, and designed in an iterative fashion to minimize thehardware requirement. This unit requires a 114 x 114 bit integermultiplier, and thus, a FPGA based area-efficient integermultiplier is also proposed with better design metrics thanprior art on it. These proposed architectures are implementedon the Xilinx FPGA platform. The proposed quadruple precision division architecture shows a small hardware usage withpromising speed...|$|R
40|$|Abstract — In this paper, low end Digital Signal Processors (DSPs) {{are applied}} to {{accelerate}} integer neural networks. The use of DSPs to accelerate neural networks has been a topic of study for some time, and has demonstrated significant performance improvements. Recently, {{work has been done}} on integer only neural networks, which greatly reduces hardware requirements, and thus allows for cheaper hardware implementation. DSPs with Arithmetic Logic Units (ALUs) that support <b>floating</b> or fixed <b>point</b> <b>arithmetic</b> are generally more expensive than their integer only counterparts due to increased circuit complexity. However if the need for <b>floating</b> or fixed <b>point</b> math operation can be removed, then simpler, lower cost DSPs can be used. To achieve this, an integer only neural network is created in this paper, which is then accelerated by using DSP instructions to improve performance...|$|R
