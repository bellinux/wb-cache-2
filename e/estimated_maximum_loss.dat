7|6832|Public
40|$|As {{with any}} {{situation}} that involves economical risk refineries may share their risk with insurers. The decision process generally includes modelling to determine to which extent the process area can be damaged. On the extreme end of modelling the so-called <b>Estimated</b> <b>Maximum</b> <b>Loss</b> (EML) scenarios are found. These scenarios predict the maximum loss a particular installation can sustain. Unfortunately no standard model for this exists. Thus the insurers reach different results due to applying different models and different assumptions. Therefore, a {{study has been}} conducted on a case in a Swedish refinery where several scenarios previously had been modelled by two different insurance brokers using two different softwares, ExTool and SLAM. This study reviews the concept of EML and analyses the used models to see which parameters are most uncertain. Also a third model, EFFECTS, was employed {{in an attempt to}} reach a conclusion with higher reliability...|$|E
40|$|This paper {{explains}} {{in detail the}} method behind the insurance database Estimated Maximum information technology Loss (EMitL). The database has been a crucial tool {{to make it possible}} to insure IT perils. It helps to insure IT-perils financially in the same professional way as consequences of traditional perils like fire, flood, and robbery are insured, and thereby secures shareholders ' investments. EMitL estimates the security awareness in an existing IT-platform. Based on that information, existing security measures can be &quot;priced &quot; as they may reduce the <b>estimated</b> <b>maximum</b> <b>loss</b> figures- and thereby the costs for the insurance. In addition, a more cost-effective decision can be made on additional security measures. Furthermore, the costs for the loss exposure inherent in a business service/product can be estimated in a better way, and thereby be incorporated in the product's price. The IT insurances are based on the traditional industries ’ classes: Liability, Loss o...|$|E
30|$|The worst {{credible}} consequence (WCC) {{scenario is}} the first step in evaluating a scenario. It considers the scenario when all active fire protection systems are impaired. Active fire protection systems can be active fire suppression systems such as sprinklers, evacuation alarm, self closing doors activated by smoke detection etc. This scenario has similarities with maximum foreseeable loss or <b>estimated</b> <b>maximum</b> <b>loss</b> within the insurance industry. However, it should be pointed out that different insurance companies have different definitions, some assumes that fire walls are not breeched and others require a special fire wall or physical separation if it should be assumed to limit the fire damage. The benefit of analyzing this scenario is that it provides information to whether active fire protection systems are needed to meet the protection objectives. If the protection objectives are met without any active fire protection system there is no need to conduct any further analysis of the scenario. Further, if protection objectives are met, there is no need to analyze impairment of the fire protection systems due to any reason (as part of antagonistic attack or common failures), hence there is no need to analyze the availability and reliability of the systems.|$|E
40|$|The {{results of}} {{simulation}} of trapped modes in LHC (phase 1) secondary collimators are presented. Both monopole and dipole modes have been analyzed giving {{estimates of the}} longitudinal and transverse impedances for different values of the collimator gap. The comparison with available measurement data shows good agreement. It {{has been found that}} a monopole mode at 1. 25 GHz gives the main contribution to the longitudinal impedance resulting mainly in heat deposition in the region of sliding RF finger. <b>Estimated</b> <b>maximum</b> <b>losses</b> are 65 mW per finger for nominal LHC beam intensity. Several dipole modes which give non-negligible contribution to the transverse impedance at frequencies below 2 GHz have been found and analyzed...|$|R
40|$|In this study, {{the crop}} yield {{response}} of rainfed crops {{to climate change}} was evaluated focusing on the most representative crops in the Guadiana river basin. The quantification of crop yields was performed using a simple soil water balance model framework. The herbaceous crop yields were evaluated with the ISAREG model, implementing a water balance approach combined with the Stewart method. A similar water balance approach was used to estimate the yields for the most representative permanent rain-fed woody crops in the region using an alternative spreadsheet-based model, but implementing a more detailed water stress evaluation through the crop cycles. Yields were simulated for two future periods(2011 – 2040) and (2041 – 2070) using, as climate inputs, temperature and precipitation series, reflecting a combination of five climate change scenarios (CCS) created using the ensemble-delta technique applied to CMIP 3 climate projections datasets to represent different alternative climate change bracketing conditions for rainfall and air temperature. The results showed that comparatively with the reference period climate (1961 – 1990) rainfed crop yields will decrease in future period 1 (2011 – 2040) and reach even higher losses in future period 2 (2041 – 2070). Within the herbaceous crops, sunflower and winter wheat were the most susceptible to yield losses under climate change, reaching <b>estimated</b> <b>maximum</b> <b>losses</b> for future period 2 of respectively 18. 5 % and 13. 6 %, followed by natural grown pastures with 11. 5 %. For woody crops, <b>maximum</b> <b>estimated</b> yields <b>losses</b> were considerably higher for almond (27. 2 %) than grapevine (5. 4 %) and olive (14. 9 %) ...|$|R
40|$|The {{procedure}} for <b>estimating</b> Probable <b>Maximum</b> <b>Loss</b> (PML) for natural catastrophes {{has evolved over}} the past few decades from a rather simplistic deterministic basis to a more sophisticated methodology based on loss exceedance probability curves, generated using catastrophe modelling software. This development process is reviewed, with an emphasis on the earthquake peril, which, because of its widespread threat to critical industrial installations, has been at the forefront of most PML advances. The coherent risk definition of PML is advocated as an improvement over standard quantile methods, which can give rise to anomalous aggregation results failing to satisfy the fundamental axiom of subadditivity, and so discouraging the pooling of risks...|$|R
40|$|We {{reviewed}} the literature {{to examine the}} vulnerability to water stress-induced embolism of Pinaceae relative to other conifers and to study the inter-relationships among the main traits involved in the hydraulic function within the Pinaceae. Results showed that Pinaceae (particularly the genus Pinus) are more vulnerable to xylem embolism, and show less variability in this character, than other conifers. Detailed data from 12 populations of Pinaceae (11 species) from three different areas (Piñol and Sala 2000; Martínez-Vilalta and Piñol 2002; Oliveras et al. 2003) was used to study the relationships among hydraulic properties of stems. These included: leaf-to-wood area ratio (AL:AW), wood- and leaf-specific hydraulic conductivity (KW and KL, respectively), vulnerability to xylem embolism (50 PLC), carbon isotope composition of needles (δ 13 C) and minimum needle water potential (minimum L). Results showed that hydraulic properties {{tended to be more}} correlated among each other than with indicators of environmental (precipitation to potential evapotranspiration ratio, P/E) or physiological water stress (minimum L). The only exception was an increase of δ 13 C with decreasing minimum L and P/E. Overall, AL:AW ratio decreased with increasing vulnerability to xylem embolism, and with increasing KW and KL (P< 0. 05). We found a strong positive relationship between carbon isotope composition and the <b>estimated</b> <b>maximum</b> <b>loss</b> of conductivity due to xylem embolism under field conditions, suggesting stronger stomatal control in more vulnerable specie...|$|E
40|$|In recent years, immense {{power system}} outage events have {{happened}} across the world. This is not exceptional to the Malaysia power system whereby on 27 Jun 2013 the system blackout {{occurred in the}} state of Sarawak, due to sudden dropping of frequency. Hence, power system risk assessment has become an important and mandatory task in planning, operation, maintenance and asset management of utilities. There have been efforts devoted in searching for new methods and procedures that effectively evaluate the risk of a power system. The objective {{of this study is to}} rank and determine the most common cause of power loss outages in the grid. This study implements multi criteria decision-making methods such as Analytic Hierarchy Process (AHP) and Technique for Order Preference by Similarity to Ideal Solution (TOPSIS). For data collection, it employed interviews of key participants, review of documents including unpublished official reports and annual reports. From the data collected there are four criteria identified, namely Duration Time (min), <b>Estimated</b> <b>Maximum</b> <b>Loss</b> of load (MW), Estimated Energy No Supplied (MW-min) and System Minutes. On the other hand, seven causes of power loss outages are identified, they are Treat To System Security, Equipment Failure, Fire or Explosion, Switching Risk, Tower Collapse, Accelerated Ageing of Equipment and Supervisory Control System Failure. Results of data analysis show that both methods have identified that Equipment Failure is the major cause, followed by Supervisory Control System Failure...|$|E
40|$|The aim of {{this paper}} is to present the basic aspects of theory of risk and insurance. Furthermore, there are some {{observations}} about applications of this theory in modern Polish economy. The paper consists of two main parts. The first one is a short definition of risk. The second one presents the process of risk management and methods of handling risk. It is noticeable that all stages of risk management are strictly connected. The first stage of risk management is risk identifying. At this time organization tries to discover all possible risks which can interrupt a business process. It is advisable to look for all possible dangers in a strict relation to at least following issues: fixed assets, liquid assets and current assets of the organization, moreover organization's human resources and legal liability. The next stage is risk analyzing (risk quantifying) when the organization makes efforts to measure the risk. Some statistical methods can be used to find a likelihood of hazard occurring and an impact of hazard occurring. In case if there are few data, non-statistical methods are used. The most popular are anticipating of PML (possible maximum loss) and EML (<b>estimated</b> <b>maximum</b> <b>loss).</b> At the and of the process of risk management organization takes a decision about handling of risk and financing risk. Organization should consider all important features and consequences of using different methods of handling and financing the risk and then make a choice. Zadanie pt. „Digitalizacja i udostępnienie w Cyfrowym Repozytorium Uniwersytetu Łódzkiego kolekcji czasopism naukowych wydawanych przez Uniwersytet Łódzki” nr 885 /P-DUN/ 2014 zostało dofinansowane ze środków MNiSW w ramach działalności upowszechniającej nauk...|$|E
3000|$|Huang [3] applied Monte Carlo {{simulation}} method and Brownian motion to calculate VAR and obtained optimal VAR by adjustment coefficient, and his study {{results showed that}} optimal VAR was efficient and <b>estimated</b> the <b>maximum</b> expected <b>loss</b> with high confidence interval. In this paper, we begin with considering {{the review of the}} literature followed by the theoretical basics of study. Next, a comparison is made between quasi-random sequences and pseudo-random sequences, then definitions of [...] the terms that are used regarding stock market are given, and finally conclusions are drawn.|$|R
40|$|The {{concept of}} Value at Risk(VaR) <b>estimates</b> the <b>maximum</b> <b>loss</b> of a {{financial}} position {{at a given time}} for a given probability. This paper considers the adequacy of the methods that are the basis of extreme value theory in the Montenegrin emerging market before and during the global financial crisis. In particular, the purpose of the paper is to investigate whether the peaks-over-threshold method outperforms the block maxima method in evaluation of Value at Risk in emerging stock markets such as the Montenegrin market. The daily return of the Montenegrin stock market index MONEX 20 is analyzed for the period January 2004 - February 2014. Results of the Kupiec test show that the peaks-over-threshold method is significantly better than the block maxima method, but both methods fail to pass the Christoffersen independence test and joint test {{due to the lack of}} accuracy in exception clustering when measuring Value at Risk. Although better, the peaks-over-threshold method still cannot be treated as an accurate VaR model for the Montenegrin frontier stock market...|$|R
40|$|The World Trade Center {{disaster}} was a stark {{reminder to}} the insurance industry of the potential dire consequences of accumulating high concentrations of insured value and underestimating a hazard {{to which they are}} exposed. By imposing strict coverage limits, or stopping to offer terrorism cover for large commercial policies, initial steps can be taken to address the accumulation problem. Subsequently, exploration of the impact of some hypothetical future terrorist scenarios can guide the control of risk accumulations. But <b>estimating</b> Probable <b>Maximum</b> <b>Loss,</b> as well as the pricing of terrorism risk, require the hazard issue also to be addressed. This will never be resolved as well as hurricane or earthquake hazard, but some insight into its ranking as a peril is urgently needed. With the insurance industry struggling with the uncertainty of how to deal with terrorism risks, hopes will be placed on a reduction of the terrorist threat, now that there is a concerted global will to combat terrorism. Terrorism hazard is considered here in the wake of this international governmental resolution, and the destruction of the al-Qaeda training camps in Afghanistan. The frequency and severity of attacks depend crucially on organizationa...|$|R
40|$|Petroleum {{refineries}} {{are at risk}} due to the flammability of {{both the}} raw material and the products. In addition to {{the threat to the}} working force there is also an apparent economical risk associated with the processing of flammable compounds. This economical risk is not only due to the direct impact of a fire or explosion but also includes the cost of business interruption in case of a shutdown due to an accident. As with any situation that involves economical risks refineries may share their risk with insurers. The premium is then based {{on the size of the}} financial risk. Thus a decision has to be made by the operator how to share the risk of economical losses with insurance companies at a fair price. However, the decision process is not easy and it generally includes modelling of various scenarios to determine to which extent the process area can be damaged if, for example, a pipe rupture occurs. On the extreme end of modelling the so called <b>Estimated</b> <b>Maximum</b> <b>Loss</b> (EML) scenarios are found. These scenarios try to predict the maximum loss a particular installation can sustain due to an accident. Unfortunately a standard model for estimation of maximum loss does not exist. It means that brokers reach different results on the same scenario due to applying different models and different assumptions. Thus an operator may face uncertainty during the decision process. Hence improvements should be made not only to the models used but also to the concept of EML itself. Therefore, a study has been conducted on a case "Preem Refinery" where several scenarios previously had been modelled by two different brokers using two different softwares, ExTool and SLAM. The aim of this paper is to review the concept of EML and to analyse the used models to see which parameters that influenced the results. The results of the study show that: Overpressure damage threshold values, cloud weights, and releasable inventory are the main sources of deviation in the modelled scenarios; Clear cut-off values for the probabilities of an accident should be used to avoid the "not plausible" argument sometimes heard. As for improving the models themselves, no clear reason for working with threshold values when it comes to overpressure damage can be found. A continuous curve seems more fitting in the age of computers. The possibility to shift such a curve to account for the difference in overpressure sustainability between different types of process equipment could also be explored; and, there are many aspects to investigate further in order to make potential loss predictions more reliable, and this should be well worthwhile since a lot of money is at stake when plant owners and insurers decide on insurance limits and premiums...|$|E
40|$|This paper {{develops}} {{a framework for}} stress testing the credit exposures of Hong Kong's retail banks to macroeconomic shocks. It involves the construction of macroeconomic credit risk models, each consisting of a multiple regression model explaining the default rate of banks, {{and a set of}} autoregressive models explaining the macroeconomic environment estimated by the method of seemingly unrelated regression. Specifically, two macroeconomic credit risk models are built. One model is specified for the overall loan portfolios of banks and, to illustrate how the same framework can be applied for stress testing loans to different economic sectors, the other model is specified for the banks' mortgage exposures only. The empirical results suggest a significant relationship between the default rates of bank loans and key macroeconomic factors including Hong Kong¡¦s real GDP, real interest rates, real property prices and Mainland China's real GDP. Macro stress testing is then performed to assess the vulnerability and risk exposures of banks' overall loan portfolios and mortgage exposures. By using the framework, a Monte Carlo method is applied to estimate the distribution of possible credit losses conditional on an artificially introduced shock. Different shocks are individually introduced into the framework for the stress tests. The magnitudes of the shocks are specified according to those occurred during the Asian financial crisis. The result shows that even for the Value-at-Risk (VaR) at the confidence level of 90 %, banks would continue to make a profit in most stressed scenarios, suggesting that the current credit risk of the banking sector is moderate. However, under the extreme case for the VaR at the confidence level of 99 %, banks' credit loss would range from a maximum of 3. 22 % to a maximum of 5. 56 % of the portfolios, and if a confidence level of 99. 9 % is taken, it could range from a maximum of 6. 08 % to a maximum of 8. 95 %. These <b>estimated</b> <b>maximum</b> <b>losses</b> are very similar to what the market experienced one year after the Asian financial crisis shock. However, the probability of such losses and beyond is very low. ...|$|R
40|$|International audienceThe {{increasing}} need {{to reduce}} power consumption and interconnection complexity in optical network on chip requires new configurations and strategies to interconnect cores in one chip. In this paper we present a new configuration of an optical network on chip (ONoC) in which the routing of control and payload data is done optically. So we can reduce the use of electric signals in the router itself. To validate and confirm our choice, we conducted a study {{of all types of}} losses that may happen to the signal across the network using FDTD-based simulators, and finally taking into account the results of this study, we present an algorithm that allows us to <b>estimate</b> the <b>maximum</b> <b>loss</b> of each connection between any processors of the network. This estimation allows us to assess the reliability of such configuration. In a network, the number of rings in each router was reduced to 4, allowing 30 % reduction of power consumption compared to Huaxi Gu et al. We will study too active microresonator behavior, mainly resonance frequency in order to find the most suitable frequency to modulate signals. The use of such routers in ONoC has several benefits such as a static and simple routing algorithm and more interconnection capacity compared to other proposed routers...|$|R
40|$|Abstract: The {{feasibility}} of employing the indigenously developed Ferroboron alloy as an alternate neutron shield material {{in combination with}} 9 Cr-based ferritic steel clad in future Indian Fast Breeder Reactors (FBR), has been investigated from a metallurgical perspective. In this regard, extensive studies have been undertaken to estimate quantitatively the nature of interaction between Ferroboron and P 91 -ferritic steel at high temperatures. It is found that in the temperature range 550 to 600 o C, 9 Cr-based ferritic steel is fully compatible with Ferroboron. However, at higher temperatures, Feroboron interacts with ferritic steel; but the <b>maximum</b> <b>estimated</b> <b>loss</b> of clad thickness is restricted to about 250 m for 60 years of service...|$|R
40|$|The DST Integrity Plan {{requires}} the ultrasonic {{wall thickness measurement}} of two vertical scans of the tank primary wall from a single riser. The resulting measurements are then used in an extreme value methodology to predict the minimum wall thickness expected for the entire tank. The methodology was developed in previous work by {{the authors of this}} report. A component of the methodology is to consider the possible impact of riser differences had multiple risers instead been used. The approach is based on previous analyses of Tank AY- 101 which had measurements taken from multiple risers. This report presents <b>estimated</b> <b>maximum</b> wall thickness <b>loss</b> for five DST's with associated uncertainty estimation and confidence bounds. Several sources of variability are incorporated since the individual sources cannot be separated. These sources include original manufacturing plate thickness and the precision of the measurement process, as well as loss due to corrosion, the actual feature of interest...|$|R
40|$|The {{aim of this}} {{research}} paper is to evaluate hedge fund returns Value-at-Risk by using GARCH models. To perform the empirical analysis, one uses the HFRX daily performance hedge fund strategy subindexes and spans the period March 2003 – March 2008. I found that skewness and kurtosis are substantial in the hedge fund returns distribution and the clustering phenomenon is pointed out. These features suggest the use of GARCH models to model the volatility of hedge fund return indexes. Hedge fund return conditional variances are estimated by using linear models (GARCH) and non-linear asymmetric models (EGARCH and TGARCH). Performance of several Value at Risk models is compared; the Gaussian VaR, the student VaR, the cornish fisher VaR, the normal GARCH-type VaR, the student GARCH-type VaR and the cornish fisher GARCH-type VaR. Our results demonstrate that the normal VaR underestimates accurate hedge fund risks while the student and the cornish fisher GARCH-type VaR are more reliable to <b>estimate</b> the potential <b>maximum</b> <b>loss</b> of hedge funds. Hedge Fund, Value at Risk, GARCH models. ...|$|R
30|$|Potential <b>maximum</b> <b>loss</b> (PMLt). PMLt=ln(Ht)-ln(Ct). The {{potential}} <b>maximum</b> <b>loss</b> {{measures the}} possible <b>maximum</b> <b>loss</b> {{from the high}} price extreme to the closing price.|$|R
40|$|The {{main focus}} of this work is on hedging of {{currency}} risks with special emphasis {{on the case of}} Czech export. In the first chapter, I create a motivation for further studying of the problem. I describe the state of export industries and {{the economy as a whole}} and how these aspects are connected to the exchange rates. In the second chapter, I explain how firms create their assumptions about future exchange rates. I also run a Monte Carlo analysis on historical data and come with predictions of my own. In the third chapter, I am discussing the relevance of using VaR models for <b>estimating</b> the <b>maximum</b> possible <b>loss</b> of funds due to unwanted moves in the exchange rate. Furthermore, I describe various instruments usable for hedging of currency exposure including forwards, options, swaps and other derivatives. In the final chapter of this work, I am asking financial and sales directors of 51 Czech firms about how currency risks influence their businesses and how they protect themselves against these threats...|$|R
5000|$|In {{this case}} the <b>maximum</b> <b>loss</b> is from 100 to 20 = 80, so the {{discounted}} <b>maximum</b> <b>loss</b> is simply ...|$|R
30|$|The {{paper is}} {{structured}} {{in the following}} way: In “Multiperiod Maximum Loss” section we recapitulate basic facts about <b>Maximum</b> <b>Loss</b> and introduce the multiperiod version of <b>Maximum</b> <b>Loss.</b> “Main results” section proves the main results about time unit invariance of <b>Maximum</b> <b>Loss.</b> In “Examples and counterexamples” section we give an example with multiperiod <b>Maximum</b> <b>Loss</b> applied to linear and quadratic portfolios and analyze time unit invariance for further risk measures. Finally, “Conclusions” section summarizes the findings.|$|R
40|$|The common {{practice}} {{for managing the}} credit risk of lending portfolios is to the calculate the <b>maximum</b> <b>loss</b> within the "value at risk" framework. Most financial institutions use large-scale Monte Carlo simulations to do this. However, such simulations may impose heavy calculation loads. This paper proposes a simplified method that approximates <b>maximum</b> <b>loss</b> with minimal simulation burden. Our method divides a portfolio into subportfolios at each credit rating level and calculates the <b>maximum</b> <b>loss</b> of each subportfolio. We assume that the subportfolio's structure provokes little fluctuation in the ratio between the <b>maximum</b> <b>loss</b> and the standard deviation. We therefore begin with a subportfolio in which each exposure is of the same amount (a homogeneous subportfolio). Simple calculations provide the standard deviation for both the heterogeneous subportfolio whose risk is to be measured and the homogeneous subportfolio. The <b>maximum</b> <b>loss</b> for the homogeneous subportfolio {{can be obtained by}} using analytical techniques rather than simulations. The <b>maximum</b> <b>loss</b> for a heterogeneous subportfolio is then approximated by multiplying the ratio of the <b>maximum</b> <b>loss</b> and standard deviation of the homogeneous subportfolio by the standard deviation of the heterogeneous subportfolio. Simulation examples indicate that this approximation is effective in all portfolios except those including extremely large exposures. This paper also describes a technique for using the total <b>maximum</b> <b>loss</b> of all subportfolios to find the <b>maximum</b> <b>loss</b> for the entire portfolio. ...|$|R
30|$|From Eqs. (4) and (5), {{it can be}} {{observed}} that the <b>maximum</b> <b>loss</b> is considered equal to 1 when the beneficial attribute is at minimum and the non-beneficial attributes at maximum values. But, all the attributes are not equally weighted in MADM problems. Hence, the <b>maximum</b> <b>losses</b> cannot {{be considered to be}} equal to 1 for all attributes. Incorporating the weights of different attributes, the <b>maximum</b> <b>loss</b> for each attribute is obtained by multiplying the <b>maximum</b> <b>loss</b> (equal to one) by the respective weight of the attribute.|$|R
3000|$|Step 4 Since the {{attribute}} VC is {{a beneficial}} attribute, the <b>maximum</b> <b>loss</b> {{occurs when the}} attribute takes on a minimum value. Similarly, the <b>maximum</b> <b>losses</b> occur when CF and PI are at their minimum values.|$|R
40|$|Common {{practice}} {{for managing the}} credit risk of lending portfolios is to calculate <b>maximum</b> <b>loss</b> within the "value at risk" framework. Most financial institutions use largescale Monte Carlo simulations to do this. However, such simulations may impose heavy calculation loads. This paper proposes a simplified method that approximates <b>maximum</b> <b>loss</b> with minimal simulation burden. Our method divides a portfolio into sub-portfolios at each credit rating level and calculates the <b>maximum</b> <b>loss</b> of each sub-portfolio. We assume that the sub-portfolio's structure provokes little fluctuation in the ratio between the <b>maximum</b> <b>loss</b> and the standard deviation. We therefore begin with a sub-portfolio in which each exposure is of the same amount (a homogeneous sub-portfolio). Simple calculations provide the standard deviation for both the heterogeneous sub-portfolio whose risk is to be measured and the homogeneous subportfolio. The <b>maximum</b> <b>loss</b> for the homogeneous sub-portfolio {{can be obtained by}} using an [...] ...|$|R
50|$|<b>Maximum</b> <b>loss</b> = {{difference}} in strike prices - net credit.|$|R
50|$|<b>Maximum</b> <b>loss</b> = net debit, {{realized}} {{when both}} options expire worthless.|$|R
40|$|Abstract This paper {{suggests}} two new heuristic algorithms for optimization of Value-at-Risk (VaR). By definition, VaR is an <b>estimate</b> of the <b>maximum</b> portfolio <b>loss</b> {{during a}} standardized period with some confidencelevel. The optimization algo-rithms {{are based on}} the minimization of the closely related risk measure Condi...|$|R
50|$|A long iron {{butterfly}} will attain <b>maximum</b> <b>losses</b> {{when the}} stock price falls at or below the lower strike price of the put or rise above or equal to the higher strike of the call purchased. The difference in strike price between the calls or puts subtracted by the premium received when entering the trade is the <b>maximum</b> <b>loss</b> accepted.|$|R
40|$|The {{identification}} of scenarios {{which have a}} particularly low or high P&L helps {{to get a better}} understanding of the portfolio's risk exposure. Therefore, the notions of safe (resp. dangerous) regions are introduced, which represent sets where the P&L is greater (resp. less) than a given critical level. In order to describe such sets in an easily interpretable way, one [...] dimensional intervals are used. Such intervals can be determined by solving a sequence of restricted <b>maximum</b> <b>loss</b> problems. Keywords: Risk Management [...] - <b>Maximum</b> <b>Loss</b> Optimization 1 Introduction <b>Maximum</b> <b>Loss</b> (ML) was introduced as a method for measuring market risks of nonlinear portfolios (cf. [Studer]). The basic idea of ML is to determine the worst case out of a specific set A of scenarios, called "trust region". <b>Maximum</b> <b>Loss</b> is a coherent risk measure (cf. [Artzner et al. ]) and it is always more conservative than the corresponding VAR (for a more detailed discussion of VAR refer to [Beckstrom and Campbell] and [R [...] ...|$|R
3000|$|... (1) Given a radius K for the <b>maximum</b> {{aggregated}} <b>losses</b> over [0, T], it {{is possible}} to find radii k_t for each time step, such that MML equals one period <b>Maximum</b> <b>Loss</b> with the given radius K. Here, ∑ k_t=K holds.|$|R
30|$|<b>Maximum</b> <b>Loss</b> is a {{coherent}} risk measure (see Artzner et al. 1999; Föllmer and Schied 2004) and a decision maker, trying to minimize it, is ambiguity averse {{in the sense}} of Gilboa and Schmeidler (1989). Special instances of <b>Maximum</b> <b>Loss</b> have been used already in Friedman (2002 a) and Hansen and Sargent (2008), who considered linear and quadratic portfolios depending on normally distributed risk factors.|$|R
40|$|In this study, it is {{theoretically}} {{proven that}} the expected value of <b>maximum</b> <b>loss</b> of fractional Brownian motion (fBm) up to time 1 with Hurst parameter [1 / 2, 1) is bounded above by 2 /√(π) and below by 1 /√(π). This result is generalized for fBm with H∈[1 / 2, 1) up to any fixed time, t. This also {{leads us to}} the bounds related to the distribution of <b>maximum</b> <b>loss</b> of fBm. As numerical study some lower bounds on the expected value of <b>maximum</b> <b>loss</b> of fBm up to time 1 are obtained by discretization. Simulation study is conducted with Cholesky method. Finally, comparison of the established bounds with simulation results is given...|$|R
5000|$|The {{discounted}} <b>maximum</b> <b>loss</b> is {{the expected}} shortfall at level [...] It {{is therefore a}} coherent risk measure.|$|R
40|$|This study {{quantified}} individual phenotypic {{variation in}} live weight and live-weight changes {{during the first}} three lactations and estimated the effects of age, lactation week and pregnancy on live weight. Data comprised weekly averaged live weight (calculated from daily observations) during 452 lactations of 239 Holstein-Friesian cows. Unadjusted mean live weights were 553 (s. d. 50), 611 (s. d. 55) and 654 (s. d. 57) kg during first, second and third parity, respectively. Estimated effect of growth during parity was 46, 52 and 23 kg for the first three parities. Mean <b>maximum</b> weight <b>loss</b> was 26, 22 and 22 kg for first, second and third parity and variation was large among individuals. Week of lactation when cows had their <b>maximum</b> weight <b>loss</b> ranged from 7 weeks in first lactation to 13 weeks in third lactation. <b>Estimated</b> <b>maximum</b> effect of pregnancy on live weight during the lactation varied from 27 to 59 kg. Phenotypic variance in live weight increased with parity. Repeatabilities of live-weight observations within parity were 0. 85. Across parities, high repeatabilities were found for calving weight and mean live weight but not for parameters associated with <b>maximum</b> weight <b>loss.</b> Correlations between weekly means and mean live weight during the whole of lactation were high. It was concluded that single live-weight observations of heifers are a good measurement of mean live weight {{during the first three}} parities. Peer reviewe...|$|R
