7807|2169|Public
5|$|Hubble {{data can}} be {{analyzed}} using many different packages. STScI maintains the custom-made Space Telescope Science Data Analysis System (STSDAS) software, which contains all the programs needed to run pipeline reduction on raw data files, {{as well as many}} other astronomical image processing tools, tailored to the requirements of Hubble data. The software runs as a module of IRAF, a popular astronomical <b>data</b> <b>reduction</b> program.|$|E
5|$|A {{limitation}} of the radial velocity method used to detect 47 Ursae Majoris b is that only a lower limit on the planet's mass can be obtained. Preliminary astrometric measurements made by the Hipparcos satellite suggest the planet's orbit is inclined at an angle of 63.1° to {{the plane of the}} sky, which would imply a true mass 12% greater than the lower limit determined by radial velocity measurements. However, subsequent investigation of the <b>data</b> <b>reduction</b> techniques used suggests that the Hipparcos measurements are not precise enough to adequately characterise the orbits of substellar companions, and the true inclination of the orbit (and hence the true mass) are regarded as unknown.|$|E
25|$|From project engineer, he was {{selected}} to be chief of the Test Engineering Branch. From this post he was subsequently promoted to Chief, Flight Test Engineering Laboratory. The responsibility of this division-level organization was {{to carry out the}} research and engineering phases of all of the experimental flight test programs assigned to the AFFTC, including overseeing such details as the human factors program and overseeing the weighing and balancing of aircraft. Ridley's new organization included separate branches for <b>Data</b> <b>Reduction,</b> Performance Engineering and Flight Research. It was in this post that he made his longest-lasting contributions to the science of flight testing.|$|E
5000|$|Recommendations {{for storage}} {{security}} services (sanitization, data confidentiality, and <b>data</b> <b>reductions)</b> ...|$|R
40|$|We present MRNet, a {{software-based}} multicast/reduction {{network for}} building scalable performance and system administration tools. MRNet supports multiple simultaneous, asynchronous collective communication operations. MRNet is flexible, allowing tool builders to tailor its process network topology {{to suit their}} tool?s requirements and the underlying system?s capabilities. MRNet is extensible, allowing tool builders to incorporate custom <b>data</b> <b>reductions</b> to augment its collection of built-in reductions. We evaluated MRNet in a simple test tool and also integrated into an existing, real-world performance tool with up to 512 tool back-ends. In the realworld tool, we used MRNet not only for multicast and simple <b>data</b> <b>reductions</b> but also with custom histogram and clock skew detection reductions. In our experiments, the MRNet-based tools showed significantly better performance than the tools without MRNet for average message latency and throughput, overall tool start-up latency, and performance data processing throughput...|$|R
5000|$|... 6 to 1 loss-less <b>data</b> volume <b>reduction</b> on SAR Nadir Altimetry dataset.|$|R
25|$|According to some {{conspiracy}} theorists, {{during the}} military investigation of green fireballs in New Mexico, UFOs were photographed by a tracking camera over White Sands Proving Grounds on April 27, 1949. They {{claim that the}} final report in 1951 on the green fireball investigation claimed there was insufficient data to determine anything. Conspiracy theorists claim that documents later uncovered by Dr. Bruce Maccabee indicate that triangulation was accomplished. The conspiracy theorists also claim that the <b>data</b> <b>reduction</b> and photographs showed four objects about 30 feet in diameter flying in formation at high speed {{at an altitude of}} about 30 miles. According to conspiracy theorists, Maccabee says this result was apparently suppressed from the final report.|$|E
500|$|The first commercially {{available}} model rocket telemetry transmitter {{is among the}} first items to be offered by MITS. Accessory modules including a tone beacon, temperature sensor, and a roll rate sensor, as well as tracking lights, ground systems for <b>data</b> <b>reduction,</b> and light weight, water activated batteries will soon be available.|$|E
500|$|In 1963, the USAF {{asked for}} {{proposals}} for an Airborne Warning and Control System (AWACS) to replace its EC-121 Warning Stars, which {{had served in}} the airborne early warning role for over a decade. The new aircraft would take advantage of improvements in radar technology and in computer aided radar data analysis and <b>data</b> <b>reduction.</b> These developments allowed airborne radars to [...] "look down", detect the movement of low-flying aircraft (see Look-down/shoot-down), and discriminate, even over land, target aircraft's movements—previously this had been impossible, due to the inability to discriminate an aircraft's track from ground clutter. Contracts were issued to Boeing, Douglas, and Lockheed, the latter being eliminated in July 1966. In 1967, a parallel program was put into place to develop the radar, with Westinghouse Electric and the Hughes Aircraft being asked to compete in producing the radar system. In 1968, it was referred to as Overland Radar Technology (ORT) during development tests on the modified EC-121Q. The Westinghouse radar antenna was going to be used by whichever company won the radar competition, since Westinghouse had pioneered in the design of high-power RF phase-shifters.|$|E
40|$|An atlas of all {{magnetic}} field observations {{made during the}} Skylab missions with the Real Time Solar Magnetograph system located at the Marshall Space Flight Center is presented. Also included are {{a description of the}} system and its operation; an outline of the <b>data</b> <b>reductions</b> performed; and a discussion of probable errors, noise, magnetic sensitivity, and system reliability...|$|R
40|$|The {{aim of this}} {{research}} is too seek and analyze murabahah financing system at PT. Bank Syariah Mandiri, Tadulako Branch Office, Palu. This is aqualitative research in which the respondents selected through purposive sampling method. Data collection methods are participant observation, in depth interviews, documentation, and triangulation. Data analysis technique includes <b>data</b> <b>reductions,</b> <b>data</b> presentation, and conclusions. The result of this study shows that murabahah financing system at PT. Bank Syariah Mandiri, Tadulako branch Office, Palu has been implemented based on laws and requerements in accordance with islamic syariah through murabahah system...|$|R
40|$|This paper {{describes}} the hardware realization of a <b>data</b> rate <b>reduction</b> codec used {{for increasing the}} playing time of a digital video cassette recorder. The experimental codec is designed for reduction factors between 2 and 5. Algorithms utilized for <b>data</b> rate <b>reduction</b> are DCT, adaptive quantization and variable length encoding. Interframe editing and shuttle mode are supported by a special codec architecture...|$|R
2500|$|In {{reflection}} seismology, {{an array}} of seismometers image sub-surface features. [...] The data are reduced to images using algorithms similar to tomography. [...] The <b>data</b> <b>reduction</b> methods resemble those of computer-aided tomographic medical imaging X-ray machines (CAT-scans), or imaging sonars.|$|E
2500|$|It {{is often}} used as <b>data</b> <b>reduction</b> step in systems genetic {{applications}} where modules are represented by [...] "module eigengenes" [...] e.g. Module eigengenes {{can be used to}} correlate modules with clinical traits. Eigengene networks are coexpression networks between module eigengenes (i.e. networks whose nodes are modules) [...]|$|E
2500|$|A lot of {{research}} is currently being carried out {{in order to make}} EEG devices smaller, more portable and easier to use. So called [...] "Wearable EEG" [...] is based upon creating low power wireless collection electronics and ‘dry’ electrodes which do not require a conductive gel to be used. Wearable EEG aims to provide small EEG devices which are present only on the head and which can record EEG for days, weeks, or months at a time, as ear-EEG. Such prolonged and easy-to-use monitoring could make a step change in the diagnosis of chronic conditions such as epilepsy, and greatly improve the end-user acceptance of BCI systems. Research is also being carried out on identifying specific solutions to increase the battery lifetime of Werable EEG devices through the use of the <b>data</b> <b>reduction</b> approach. For example, in the context of epilepsy diagnosis, <b>data</b> <b>reduction</b> has been used to extend the battery lifetime of Werable EEG devices by intelligently selecting, and only transmitting, diagnostically relevant EEG data.|$|E
40|$|MERRA AS is a cyberinfrastructure {{resource}} that will combine iRODS-based Climate Data Server (CDS) capabilities with Coudera MapReduce to serve MERRA analytic products, store the MERRA reanalysis data collection in an HDFS to enable parallel, high-performance, storage-side <b>data</b> <b>reductions,</b> manage storage-side driver, mapper, reducer code sets and realized objects for users, {{and provide a}} library of commonly used spatiotemporal operations that can be composed to enable higher-order analyses...|$|R
40|$|In Gayon & Bois (2008) and Gayon etal (2009), (i) {{we studied}} the {{theoretical}} feasibility {{and efficiency of}} retrograde mean motion resonances (i. e. two planets are both in orbital resonance and in counter-revolving configuration), (ii) we showed that retrograde resonances can generate interesting mechanisms of stability, and (iii) we obtained a dynamical fit involving a counter-revolving configuration {{that is consistent with}} the observations of the HD 73526 planetary system. In the present paper, we present and analyze <b>data</b> <b>reductions</b> assuming counter-revolving configurations for eight compact multi-planetary systems detected through the radial velocity method. In each case, we select the best fit leading to a dynamically stable solution. The resulting <b>data</b> <b>reductions</b> obtained in rms and chi values for counter-revolving configurations are of the same order, and sometimes slightly better, than for prograde configurations. In the end, these fits tend to show that, over the eight studied multi-planetary systems, six of them could be regulated by a mechanism involving a counter-revolving configuration. Comment: 4 pages, 1 figure, 2 tables, accepted for publication in MNRAS letters (August 10, 2009...|$|R
40|$|With {{continuous}} {{developments in}} LiDAR technologies high point cloud densities have been attainable but accompanied by challenges for processing big volumes of <b>data.</b> <b>Reductions</b> in high point cloud densities {{are expected to}} lower data acquisition and data processing costs; however this could affect {{the characteristics of the}} generated Digital Elevation Models (DEMs). This research aimed to evaluate the effects of reductions in airborne LiDAR point cloud data densities on the visual and statistical characteristics of the generated DEMs. DEMs have been created from a dataset which constitutes last returns of raw LiDAR data that was acquired at bare lands for Gilmer County, USA between March and April 2004, where qualitative and quantitative testing analyses have been performed. Visual analysis has shown that the DEM can withstand a considerable degree of quality with reduced densities down to 0. 128  pts/m 2 (47  % of the data remaining), however degradations in the DEM visual characteristics appeared in coarser tones and rougher textures have occurred with more reductions. Additionally, the statistical analysis has indicated that the standard deviations of the DEM elevations have decreased by only 22  % of the total decrease with <b>data</b> density <b>reductions</b> down to 0. 101  pts/m 2 (37  % of the data remaining) while greater rate of decreasing in the standard deviations has occurred with more reductions referring to greater rate of surface smoothing and elevation approximating. Furthermore, the accuracy analysis testing has given that the DEM accuracy has degraded by only 4. 83  % of the total degradations with <b>data</b> density <b>reductions</b> down to 0. 128  pts/m 2, however great deteriorations in the DEM accuracy have occurred with more <b>data</b> <b>reductions.</b> Finally, it is recommended that LiDAR data can withstand point density reductions down to 0. 128  pts/m 2 (about 50  % of the data) without big deteriorations in the visual and statistical characteristics of the generated DEMs...|$|R
2500|$|A {{worldwide}} {{array of}} seismometers can actually image {{the interior of}} the Earth in wave-speed and transmissivity. [...] This type of system uses events such as earthquakes, impact events or nuclear explosions as wave sources. [...] The first efforts at this method used manual <b>data</b> <b>reduction</b> from paper seismograph charts. [...] Modern digital seismograph records are better adapted to direct computer use. [...] With inexpensive seismometer designs and internet access, amateurs and small institutions have even formed a [...] "public seismograph network".|$|E
2500|$|The 9th ADD {{established}} the temporary 1962 [...] "Cuban Missile Early Warning System" [...] for the missile crisis. [...] Responsibility for a USAFSS squadron's AN/FPS-17 radar station in Turkey for missile test monitoring transferred to ADC on 1 July 1963, the same date the site's AN/FPS-79 achieved IOC. [...] By January 1963, ADC's Detachment 3 of the 9th Aerospace Defense Division (9th ADD) was providing space surveillance {{data from the}} Moorestown BMEWS station [...] "to a Spacetrack Analysis Center at Colorado Springs." [...] On 31 December 1965, Forward Scatter Over-the-Horizon network data from the 440L <b>Data</b> <b>Reduction</b> Center was being received by ADC for missile warning, and a NORAD plan for 1 April 1966 was for ADC to [...] "reorganize its remaining 26th, 28th, 29th, and 73d Air Divisions into four air forces." ...|$|E
2500|$|The term [...] "smoker" [...] {{is given}} {{to a person who}} habitually smokes tobacco on a daily basis. This {{category}} {{has been the focus of}} the vast majority of tobacco studies. However, the health effects of less-than-daily smoking are far less well understood. Studies have often taken the data of [...] "occasional smokers" [...] (those who have never smoked daily) and grouped them with those who have never smoked. A 2006 European study on occasional smoking published findings that the risk of the major smoking-related cancers for occasional smokers was 1.24 times that of those who have never smoked at all but the result was not statistically significant. (For a confidence interval of 95%, this data showed an incidence rate ratio of 0.80 to 1.94.) (<b>Data</b> <b>reduction</b> used Cox proportional hazard model, stratified by gender and country.) This compares to studies showing that habitual heavy smokers have greater than 50 times the incidence of smoking-related cancers.|$|E
5000|$|Optimization of the product's overall {{operations}} {{based on}} actual performance <b>data,</b> and <b>reduction</b> of downtimes through predictive maintenance and remote service.|$|R
40|$|In 1998 The Optical Gravitational Lensing Experiment (OGLE) {{successfully}} implemented automated <b>data</b> <b>reductions</b> for QSO 2237 + 0305. Using a {{new image}} subtraction method we achieved a differential photometry scatter of 1 – 5 % for images A–D respectively. Combined with a time sampling of 1 – 2 times a week this is sufficient for early detection of caustic crossings. Nearly real time photometry of QSO 2237 + 0305 {{is available from the}} OGLE web sit...|$|R
40|$|Abstract — We {{present a}} {{flexible}} and inexpensive system for analyzing gesture {{data in a}} computer. Several basic <b>data</b> <b>reductions</b> are introduced, and the tradeoff between latency and reliability is discussed. An example of the application of this technology to musical performance is presented via {{the example of the}} radio drum, a musical controller that produces eight channels of gestural data. The advantages of analyzing this data stream in software are exposed, and future applications of the technology are presented. I...|$|R
2500|$|Weighted {{correlation}} network analysis, {{also known}} as weighted gene co-expression network analysis (WGCNA), is a widely used data mining method especially [...] for studying [...] biological networks based on pairwise [...] correlations between variables. While it {{can be applied to}} most [...] high-dimensional data sets, it has been most widely used in [...] genomic applications. [...] It allows one to define modules (clusters), intramodular hubs, and network nodes with regard to module membership, to study the relationships between co-expression modules, and to compare the network topology of different networks (differential network analysis). WGCNA can be used as [...] <b>data</b> <b>reduction</b> technique (related to oblique [...] factor analysis [...] ), as [...] clustering method (fuzzy clustering), as [...] feature selection method (e.g. as gene screening method), as framework for integrating complementary (genomic) data (based on weighted correlations between quantitative variables), and as [...] data exploratory technique. Although WGCNA incorporates traditional data exploratory techniques, its intuitive network language and analysis framework transcend any standard analysis technique. [...] Since it uses network methodology and is well suited for integrating complementary genomic data sets, it can be interpreted as systems biologic or systems genetic data analysis method. By selecting intramodular hubs in consensus modules, WGCNA also gives rise to network based [...] meta analysis techniques ...|$|E
2500|$|In special relativity, an {{observer}} will, in most cases, mean {{a frame of}} reference from which a set of objects or events are being measured. This usage differs significantly from the ordinary English meaning of the term. Reference frames are inherently nonlocal constructs, and according to this usage of the term, {{it does not make}} sense to speak of {{an observer}} as having a location. In Fig.1‑1, imagine that a scientist is in control of a dense lattice of clocks, synchronized within her reference frame, that extends indefinitely throughout the three dimensions of space. Her location within the lattice is not important. She uses her latticework of clocks to determine the time and position of events taking place within its reach. The term observer refers to the entire ensemble of clocks associated with one inertial frame of reference. In this idealized case, every point in space has a clock associated with it, and thus the clocks register each event instantly, with [...] no time delay between an event and its recording. A real observer, however, will see a delay between the emission of a signal and its detection due to the speed of light. To synchronize the clocks, in the <b>data</b> <b>reduction</b> following an experiment, the time when a signal is received will be corrected to reflect its actual time were it to have been recorded by an idealized lattice of clocks.|$|E
2500|$|Miller {{worked on}} {{increasingly}} larger interferometers, culminating in {{one with a}} [...] (effective) arm length that he tried at various sites, including {{on top of a}} mountain at the Mount Wilson Observatory. To avoid the possibility of the aether wind being blocked by solid walls, his mountaintop observations used a special shed with thin walls, mainly of canvas. From noisy, irregular data, he consistently extracted a small positive signal that varied with each rotation of the device, with the sidereal day, and on a yearly basis. His measurements in the 1920s amounted to approximately [...] instead of the nearly [...] expected from the Earth's orbital motion alone. He remained convinced this was due to partial entrainment or aether dragging, though he did not attempt a detailed explanation. He ignored critiques demonstrating the inconsistency of his results and the refutation by the Hammar experiment. Miller's findings were considered important at the time, and were discussed by Michelson, Lorentz and others at a meeting reported in 1928. There was general agreement that more experimentation was needed to check Miller's results. Miller later built a non-magnetic device to eliminate magnetostriction, while Michelson built one of non-expanding Invar to eliminate any remaining thermal effects. Other experimenters from around the world increased accuracy, eliminated possible side effects, or both. So far, no one has been able to replicate Miller's results, and modern experimental accuracies have ruled them out. Roberts (2006) has pointed out that the primitive <b>data</b> <b>reduction</b> techniques used by Miller and other early experimenters, including Michelson and Morley, were capable of creating apparent periodic signals even when none existed in the actual data. After reanalyzing Miller's original data using modern techniques of quantitative error analysis, Roberts found Miller's apparent signals to be statistically insignificant.|$|E
40|$|Abstract. Complementing recent {{progress}} on classical complexity and polynomial-time approximability of feedback set problems in (bipartite) tournaments, we extend and partially improve fixed-parameter tractability results for these problems. We show that Feedback Vertex Set in tournaments is amenable {{to the novel}} iterative compression technique. Moreover, we provide <b>data</b> <b>reductions</b> and problem kernels for Feedback Vertex Set and Feedback Arc Set in tournaments, and a depthbounded search tree for Feedback Arc Set in bipartite tournaments based on a new forbidden subgraph characterization. ...|$|R
40|$|Fixed-parameter {{algorithms}} can efficiently find optimal {{solutions to}} some NP-hard problems, including several problems {{that arise in}} graphmodeled data clustering. This survey provides a primer about practical techniques to develop such algorithms; in particular, we discuss the design of kernelizations (<b>data</b> <b>reductions</b> with provable performance guarantees) and depth-bounded search trees. Our investigations are circumstantiated by three concrete problems {{from the realm of}} graph-modeled data clustering for which fixed-parameter algorithms have been implemented and experimentally evaluated, namely Clique, Cluster Editing, and Clique Cover...|$|R
40|$|The {{dielectric}} properties of polluted waters are {{measured with a}} reflection-type resonant cavity at 1. 43 GHz. Very small water samples in quartz tubes of known volume are placed {{in the center of}} the maximum electric field. Measurement of the resonance-frequency variation and a change of the cavity's quality factor are used to determine the {{dielectric properties}}. The microwave emissivity of the polluted water is then calculated via the Fresnel equation and applied to <b>data</b> <b>reductions</b> of microwave radiometer measurements...|$|R
50|$|<b>Data</b> <b>reduction</b> of Agency-collected ELINT signals.|$|E
50|$|These {{are common}} {{techniques}} used in <b>data</b> <b>reduction.</b>|$|E
5000|$|... #Subtitle level 4: <b>Data</b> <b>reduction</b> {{obtained}} from normalization ...|$|E
40|$|This paper {{introduces}} a new algorithm for clustering data in high-dimensional feature spaces, called GARDENHD. The algorithm is {{organized around the}} notion of <b>data</b> space <b>reduction,</b> i. e. the process of detecting dense areas (dense cells) in the space. It performs effective and efficient elimination of empty areas that characterize typical high-dimensional spaces and an efficient adjacency-connected agglomeration of dense cells into larger clusters. It produces a compact representation that can effectively {{capture the essence of}} data. GARDENHD is a hybrid of cell-based and density-based clustering. However, unlike typical clustering methods in its class, it applies a recursive partition of sparse regions in the space using a new space-partitioning strategy. The properties of this partitioning strategy greatly facilitate <b>data</b> space <b>reduction.</b> The experiments on synthetic and real data sets reveal that GARDENHD and its <b>data</b> space <b>reduction</b> are effective, efficient, and scalable...|$|R
40|$|The {{uncertainty}} in relay satellite sate {{is a significant}} error source which cannot be ignored in the reduction of satellite-to-satellite tracking data. Based on simulations and real <b>data</b> <b>reductions,</b> it is numerically impractical to use simultaneous unconstrained solutions to determine both relay and user satellite epoch states. A Bayesian or least squares estimation technique with an a priori procedure is presented which permits the adjustment of relay satellite epoch state in the reduction of satellite-to-satellite tracking data without the numerical difficulties introduced by an ill-conditioned normal matrix...|$|R
30|$|As {{described}} above, the GDA is a nonlinear <b>data</b> dimension <b>reduction</b> method {{based on}} kernel function learning technique, {{which will be}} used to deal with the normalized feature vectors.|$|R
