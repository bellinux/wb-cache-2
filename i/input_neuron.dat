33|1329|Public
25|$|An <b>input</b> <b>neuron</b> has no predecessor, but {{serves as}} an input {{interface}} for the whole network. Similarly, an output neuron has no successor, and thus serves as the output interface of the whole network.|$|E
5000|$|Hebb's {{rule has}} {{synaptic}} weights approaching infinity {{with a positive}} learning rate. We can stop this by normalizing the weights so that each weight's magnitude is restricted between 0, corresponding to no weight, and 1, corresponding to being the only <b>input</b> <b>neuron</b> with any weight. We do this by normalizing the weight vector to be of length one: ...|$|E
5000|$|... where [...] is {{the output}} of the [...] th neuron, [...] is the th <b>input</b> <b>neuron</b> signal, [...] is the {{synaptic}} weight (or strength of connection) between the neurons [...] and , and [...] is the activation function. While this model has seen success in machine-learning applications, it is a poor model for real (biological) neurons, because it lacks the time-dependence that real neurons exhibit. Some of the earliest biological models took this form until kinetic models such as the Hodgkin-Huxley model became dominant.|$|E
40|$|Abstract—In this paper, {{we propose}} {{a new type}} of information-theoretic method for the {{self-organizing}} maps (SOM), taking into account competition between competitive (output) neurons as well as <b>input</b> <b>neurons.</b> The method is called ”double competition”, as it considers competition between outputs as well as <b>input</b> <b>neurons.</b> By increasing information in <b>input</b> <b>neurons,</b> we expect to obtain more detailed information on input patterns through the information-theoretic method. We applied the information-theoretic methods to two well-known data sets from the machine learning database, namely, the glass and dermatology data sets. We found that the information-theoretic method with double competition explicitly separated the different classes. On the other hand, without considering <b>input</b> <b>neurons,</b> class boundaries could not be explicitly identified. In addition, without consid-ering <b>input</b> <b>neurons,</b> quantization and topographic errors were inversely related. This means that when the quantization errors decreased, topographic errors inversely increased. However, with double competition, this inverse relation between quantization and topographic errors was neutralized. Experimental results show that by incorporating information in <b>input</b> <b>neurons,</b> class structure could be clearly identified without degrading the map quality to severely. Keywords—double competition, self-organizing maps, mutual information, class structure I...|$|R
5000|$|It {{consists}} of one output neuron, K hidden <b>neurons</b> and K*N <b>input</b> <b>neurons.</b> <b>Inputs</b> {{to the network}} take 3 values: The weights between <b>input</b> and hidden <b>neurons</b> take the values: Output value of each hidden neuron is calculated as a sum of all multiplications of <b>input</b> <b>neurons</b> and these weights: Signum is a simple function, which returns -1,0 or 1: ...|$|R
5000|$|<b>Input</b> layer: One <b>neuron</b> {{appears in}} the input layer for each {{predictor}} variable. In the case of categorical variables, N-1 neurons are used where N {{is the number of}} categories. The <b>input</b> <b>neurons</b> standardizes the value ranges by subtracting the median and dividing by the interquartile range. The <b>input</b> <b>neurons</b> then feed the values to each of the neurons in the hidden layer.|$|R
5000|$|Natural or {{pharmacological}} <b>input</b> <b>neuron</b> models - The {{models in}} this category connect between the input stimulus which can be either pharmacological or natural, to {{the probability of a}} spike event. The input stage of these models is not electrical, but rather has either pharmacological (chemical) concentration units, or physical units that characterize an external stimulus such as light, sound or other forms of physical pressure. Furthermore, the output stage represents the probability of a spike event and not an electrical voltage. Typically, this output probability is normalized (divided by) a time constant, and the resulting normalized probability is called the [...] "firing rate" [...] and has units of Hertz. The probabilistic description taken by the models {{in this category}} was inspired from laboratory experiments involving either natural or pharmacological stimulation which exhibit variability in the resulting spike pattern. Nevertheless, when averaging these experimental results across several trials, a clear pattern is often revealed.|$|E
5000|$|This {{expression}} {{contains a}} product of partial similarities, l(X(n)), over all bottom-up signals; therefore it forces the NMF system to account for every signal (even if one term in the product is zero, the product is zero, the similarity is low and the knowledge instinct is not satisfied); this {{is a reflection of}} the first principle. Second, before perception occurs, the mind does not know which object gave rise to a signal from a particular retinal neuron. Therefore a partial similarity measure is constructed so that it treats each model as an alternative (a sum over concept-models) for each <b>input</b> <b>neuron</b> signal. Its constituent elements are conditional partial similarities between signal X(n) and model Mm, l(X(n)|m). This measure is “conditional” on object m being present, therefore, when combining these quantities into the overall similarity measure, L, they are multiplied by r(m), which represent a probabilistic measure of object m actually being present. Combining these elements with the two principles noted above, a similarity measure is constructed as follows: ...|$|E
5000|$|The term {{receptive}} field {{is also used}} {{in the context of}} artificial neural networks, most often in relation to convolutional neural networks (CNNs). When used in this sense, the term adopts a meaning reminiscent of {{receptive field}}s in actual biological nervous systems. CNNs have a distinct architecture, designed to mimic the way in which real animal brains are understood to function; instead of having every neuron in each layer connect to all neurons in the next layer (Multilayer perceptron), the neurons are arranged in a 3-dimensional structure {{in such a way as}} to take into account the spatial relationships between different neurons with respect to the original data. Since CNNs are used primarily in the field of computer vision, the data that the neurons represent is typically an image; each <b>input</b> <b>neuron</b> represents one pixel from the original image. The first layer of neurons is composed of all the input neurons; neurons in the next layer will receive connections from some of the input neurons (pixels), but not all, as would be the case in a MLP and in other traditional neural networks. Hence, instead of having each neuron receive connections from all neurons in the previous layer, CNNs use a receptive field-like layout in which each neuron receives connections only from a subset of neurons in the previous (lower) layer. The receptive field of a neuron in one of the lower layers encompasses only a small area of the image, while the receptive field of a neuron in subsequent (higher) layers involves a combination of receptive fields from several (but not all) neurons in the layer before (i. e. a neuron in a higher layer [...] "looks" [...] at a larger portion of the image than does a neuron in a lower layer). In this way, each successive layer is capable of learning increasingly abstract features of the original image. The use of receptive fields in this fashion is thought to give CNNs an advantage in recognizing visual patterns when compared to other types of neural networks.|$|E
40|$|Many {{immediate}} early genes (IEG) have activity-dependent induction in {{a subset}} of brain subdivisions or neuron types. However, none have been reported yet with regulation specific to thalamic-recipient sensory <b>input</b> <b>neurons</b> of the telencephalon or in the thalamic sensory <b>input</b> <b>neurons</b> themselves. Here, we report the first such gene, dual specificity phosphatase 1 (dusp 1). Dusp 1 is an inactivator of mitogen-activated protein kinase (MAPK), and MAPK in turn activates expression of egr 1, {{one of the most}} commonly studied IEGs, as determined in cultured cells. We found that in the brain of naturally behaving songbirds and other avian species, hearing song, seeing visual stimuli, or performing motor behavior caused high dusp 1 up-regulation respectively in auditory, visual, and somatosensory input cell populations of the thalamus and thalamic-recipient sensory <b>input</b> <b>neurons</b> of the telencephalic pallium, whereas high egr 1 up-regulation occurred only in subsequently connected secondary and tertiary sensory neuronal populations of these same pathways. Motor behavior did not induce high levels of dusp 1 expression in the motor-associated areas adjacent to song nuclei, where egr 1 is up-regulated in response to movement. Our analysis of dusp 1 expression in mouse brain suggests similar regulation in the sensory <b>input</b> <b>neurons</b> of the thalamus and thalamic-recipient layer IV and IV neurons of the cortex. These findings suggest that dusp 1 has specialized regulation to sensory <b>input</b> <b>neurons</b> of the thalamus and telencephalon; they further suggest that this regulation may serve to attenuate stimulus-induced expression of egr 1 and other IEGs, leading to unique molecular properties of forebrain sensory <b>input</b> <b>neurons...</b>|$|R
5000|$|Output {{value of}} each hidden neuron is {{calculated}} as a sum of all exclusive disjunctions (exclusive or) of <b>input</b> <b>neurons</b> and these weights: ...|$|R
5000|$|Considering [...] as a {{function}} of the <b>inputs</b> of all <b>neurons</b> [...] receiving <b>input</b> from <b>neuron</b> , ...|$|R
3000|$|... b. A {{hidden layer}} This layer {{associates}} one neuron for each {{row in the}} training data set matrix. The neuron stores {{the values of the}} independent variables for a particular row with the corresponding dependent variable values. During training, the GRNN calculates the Euclidean distances for each <b>input</b> <b>neuron</b> of the input vector (X). The distances from each <b>input</b> <b>neuron</b> are fed with a smoothing factor into a nonlinear exponential activation function, as specified by equation Eq. (12). The resulting value is passed to the neurons in the summation layer (Fig.  7).|$|E
40|$|The present work {{exploits}} {{the possibility}} of using a Self-Organizing Neural Network to obtain the endmembers (class prototypes) on hyperspectral images. The Self-Organizing neural network has the advantage that obtains by competitive procedures this endmembers on hyperspectral images. We propose a neural network for processing the spectral information for each pixel. The neural model consists of a Self-Organizing Neural Network. This net has an <b>input</b> <b>neuron</b> for each image channel. The output neuron number is related with the endmember number. Different distances and learning functions are used to obtain a better endmember extraction. The result discussion also includes the neighborhood function and the influence of noise in the endmember quality. ...|$|E
30|$|In a Hopfield neural network, a neuron can {{not only}} be used for an <b>input</b> <b>neuron,</b> but also an output neuron. Every Hopfield neural network has a {{so-called}} cost function (or an energy function), which is used for measuring stability of a Hopfield neural network. Signals were circularly transmitted in the whole network. The operation course {{can be regarded as}} a recovered and strengthened processing for an input signal. In the course, the network approach gradually to a stable state when the cost function is minimized. If a problem can be mapped to the task of minimizing a cost function, the Hopfield neural network will be implemented to obtain an optimal (or near optimal) solution.|$|E
30|$|The NN was {{designed}} to work well if it was built with 20 <b>input</b> <b>neurons,</b> 10 neurons in the hidden layer and 3 neurons in the output layer.|$|R
30|$|The <b>input</b> <b>neurons</b> nodes {{are equal}} to input vector {{dimension}} {{in the learning}} sample, each node which is a simple distribution unit directly takes the input vectors into the pattern layer.|$|R
40|$|AbstractWe {{explore the}} {{precision}} of neural timing in a model neural system with n identical <b>input</b> <b>neurons</b> whose firing time in response to stimulation is chosen from a density f. These <b>input</b> <b>neurons</b> stimulate a target cell which fires when it receives m hits within ɛ msec. We prove that {{the density of the}} firing time of the target cell converges as ɛ→ 0 to the input density f raised to the mth and normalized. We give conditions for convergence of the density in L 1, pointwise, and uniformly as well as conditions for the convergence of the standard deviations...|$|R
40|$|Abstract. An {{improved}} algorithm using B-splines as weight {{functions for}} training neural networks is proposed. There {{is no need}} for training neural networks or solving linear equations. The most important advantage is that we can get the forms of weight functions by the given patterns directly. Each of weight function is a one-variable function and takes one associated input point (<b>input</b> <b>neuron)</b> as its argument. The form of each weight function is a linear combination of some B-splines defined on the sets of given input variables (input knots or input patterns), whose coefficients are associated with the given output patterns. Some examples are presented to illustrate good performance of the new algorithm...|$|E
40|$|I {{consider}} a topographic projection between two neuronal layers with dif-ferent densities of neurons. Given {{the number of}} output neurons con-nected to each <b>input</b> <b>neuron</b> (divergence or fan-out) {{and the number of}} input neurons synapsing on each output neuron (convergence or fan-in) I determine the widths of axonal and dendritic arbors which minimize the total volume ofaxons and dendrites. My analytical results can be sum-marized qualitatively in the following rule: neurons of the sparser layer should have arbors wider than those of the denser layer. This agrees with the anatomical data from retinal and cerebellar neurons whose morphol-ogy and connectivity are known. The rule may be used to infer connec-tivity of neurons from their morphology. ...|$|E
40|$|Cyclone track {{prediction}} is a two dimensional time series prediction problem that involves latitudes and longitudes which define {{the position of}} a cyclone. Recurrent neural networks have been suitable for time series prediction due to their architectural properties in modeling temporal sequences. Coevolutionary recurrent neural networks have been used for time series prediction and also applied to cyclone track prediction. In this paper, we present an architecture for encoding two dimensional time series problem into Elman recurrent neural networks composed of a single <b>input</b> <b>neuron.</b> We use cooperative coevolution and back-propagation through-time algorithms for training. Our experiments show an improvement in the accuracy when compared to previous results using a different recurrent network architecture...|$|E
30|$|The methods {{proposed}} previously for estimating different well-testing variables {{require a}} complex rule definition; the functional links {{defined in the}} <b>input</b> <b>neurons</b> of ANNs are determined subjectively, not based on a scientific benchmark.|$|R
40|$|Abstract. The {{influence}} of a weight-dependent spike-timing dependent plasticity (STDP) rule on the temporal evolution and equilibrium state of a certain synapse is investigated. We show that under certain conditions, a spike-induced rate-learning scheme could be achieved. Through studying the situation when a single Hodgkin-Huxley neuron is driven by a large ensemble of <b>input</b> <b>neurons,</b> we find that synchronized firing of a sub population of <b>input</b> <b>neurons</b> may be important to information processing in the nervous system. Using simulations, we show that the temporal structure of the spike trains of these synchronized <b>input</b> <b>neurons</b> can be transmitted reliably; further, synapses from these neurons will increase stably due to the STDP rule and this may provide a mechanism for learning and information storage in biologically plausible network models. PACS. 87. 18. Sn Neural networks – 87. 16. Xa Signal transduction – 87. 17. Aa Theory and modeling; computer simulation In real nervous systems, information is encoded and transmitted via spatiotemporal patterns of action potentials fired by each neuron; and the synaptic weights between neurons can be modified by these firing events. Thi...|$|R
3000|$|... 1]. The {{neural network}} has 60 <b>input</b> <b>neurons,</b> two {{for each of}} the 30 points of the {{resampled}} snake, and only one output neuron that provides the probability that the contour represents a pedestrian; such probability will be, again, in the range [[...]...|$|R
30|$|The {{generalized}} {{structure of}} NN {{consists of the}} three types of neuron layers, i.e., input layer, hidden layer, and output layer. Generally, the input layer acquires the data from the outside world and the output layer returns the data after passing through the hidden layer(s). NN structure can either be of feedforward or recurrent[20]. In feedforward structure, data flow from the input layer to the output layer without having any feedback connection to the backward layer(s). The classical example of feedforward structure is the perceptron listed in[21]. However in recurrent NN, there are connections between the output neuron to the <b>input</b> <b>neuron</b> of the same or the backward layer. The example being the Elman's recurrent neural network[22] in which the concept of recurrent NN is employed. In this study, we employ MLP, a feedforward structure of NN, which has been used widely in time series prediction[23] and binary prediction[24].|$|E
40|$|We {{investigate}} {{the learning of}} deterministic finite-state automata (DFA's) with recurrent networks with a single <b>input</b> <b>neuron,</b> where each input symbol is represented as a temporal pattern and strings as sequences of temporal patterns. We empirically demonstrate that obvious temporal encodings can make learning very difficult or even impossible. Based on preliminary results, we formulate some hypotheses about 'good' temporal encoding, i. e. encodings which do not significantly increase training time compared to training of networks with multiple input neurons. I. INTRODUCTION Recurrent neural networks (RNN's) can be trained to behave like deterministic finite-state automata (DFA's) [2, 3, 4, 6, 8, 9, 10]. The symbols of the input alphabet have to be encoded into the network's set of input neurons. We investigate whether it is feasible to learn DFA's with a temporal encoding scheme where the example strings are fed into a network via a single input neurons...|$|E
40|$|Abstract. This paper {{presents}} a new statistical approach for learning automatic color image correction. The {{goal is to}} parameterize color independently of illumination and to correct color for changes of illumination. This is useful in many image processing applications, such as color image segmentation or background subtraction. The motivation for using a learning approach is to deal with changes of lighting typical of indoor environments such as home and office. The method is based on learning color invariants using a modified multi-layer perceptron (MLP). The MLP is odd-layered and the central bottleneck layer includes two neurons that estimates the color invariants and one <b>input</b> <b>neuron</b> proportional to the luminance desired in output of the MLP(luminance being strongly correlated with illumination). The advantage of the modified MLP over a classical MLP is better performance and the estimation of invariants to illumination. Results compare the approach with other color correction approaches from the literature. ...|$|E
40|$|AbstractThis paper {{proposes a}} {{neuronal}} circuitry layout and synaptic plasticity principles {{that allow the}} (pyramidal) neuron {{to act as a}} “combinatorial switch”. Namely, the neuron learns to be more prone to generate spikes given those combinations of firing <b>input</b> <b>neurons</b> for which a previous spiking of the neuron had been followed by a positive global reward signal. The reward signal may be mediated by certain modulatory hormones or neurotransmitters, e. g., the dopamine. More generally, a trial-and-error learning paradigm is suggested in which a global reward signal triggers long-term enhancement or weakening of a neuron’s spiking response to the preceding neuronal input firing pattern. Thus, rewards provide a feedback pathway that informs neurons whether their spiking was beneficial or detrimental for a particular input combination. The neuron’s ability to discern specific combinations of firing <b>input</b> <b>neurons</b> is achieved through a random or predetermined spatial distribution of input synapses on dendrites that creates synaptic clusters that represent various permutations of <b>input</b> <b>neurons.</b> The corresponding dendritic segments, or the enclosed individual spines, are capable of being particularly excited, due to local sigmoidal thresholding involving voltage-gated channel conductances, if the segment’s excitatory and absence of inhibitory inputs are temporally coincident. Such nonlinear excitation corresponds to a particular firing combination of <b>input</b> <b>neurons,</b> and it is posited that the excitation strength encodes the combinatorial memory and is regulated by long-term plasticity mechanisms. It is also suggested that the spine calcium influx that may result from the spatiotemporal synaptic input coincidence may cause the spine head actin filaments to undergo mechanical (muscle-like) contraction, with the ensuing cytoskeletal deformation transmitted to the axon initial segment where it may modulate the global neuron firing threshold. The tasks of pattern classification and generalization are discussed within the presented framework...|$|R
40|$|This paper {{proposes a}} {{neuronal}} circuitry layout and synaptic plasticity principles {{that allow the}} (pyramidal) neuron {{to act as a}} "combinatorial switch". Namely, the neuron learns to be more prone to generate spikes given those combinations of firing <b>input</b> <b>neurons</b> for which a previous spiking of the neuron had been followed by a positive global reward signal. The reward signal may be mediated by certain modulatory hormones or neurotransmitters, e. g., the dopamine. More generally, a trial-and-error learning paradigm is suggested in which a global reward signal triggers long-term enhancement or weakening of a neuron's spiking response to the preceding neuronal input firing pattern. Thus, rewards provide a feedback pathway that informs neurons whether their spiking was beneficial or detrimental for a particular input combination. The neuron's ability to discern specific combinations of firing <b>input</b> <b>neurons</b> is achieved through a random or predetermined spatial distribution of input synapses on dendrites that creates synaptic clusters that represent various permutations of <b>input</b> <b>neurons.</b> The corresponding dendritic segments, or the enclosed individual spines, are capable of being particularly excited, due to local sigmoidal thresholding involving voltage-gated channel conductances, if the segment's excitatory and absence of inhibitory inputs are temporally coincident. Such nonlinear excitation corresponds to a particular firing combination of <b>input</b> <b>neurons,</b> and it is posited that the excitation strength encodes the combinatorial memory and is regulated by long-term plasticity mechanisms. It is also suggested that the spine calcium influx that may result from the spatiotemporal synaptic input coincidence may cause the spine head actin filaments to undergo mechanical (muscle-like) contraction, with the ensuing cytoskeletal deformation transmitted to the axon initial segment where it may [...] . Comment: Version 5 : added computer code in the ancillary files sectio...|$|R
40|$|The {{processing}} of visual data in area 17 of the mammalian cortex is mainly performed by cells with receptive fields which are tuned to different orientations of input stimuli. The mechanisms underlying {{the emergence of}} receptive field properties of orientation selective cells are not well understood up to now. Recently, some models for the prenatal development of the receptive fields of orientation selective simple cells have been proposed, which emerge in neural networks trained by Hebb type unsupervised learning rules. These models, however, use different network architectures and are restricted {{to the case of}} identical <b>input</b> <b>neurons.</b> In this work, a biologically motivated neural network model with a general architecture is presented. It is trained with a Hebb type updating rule and with uncorrelated <b>input.</b> The <b>input</b> <b>neurons</b> are identified with retinal ganglion cells and exhibit mature Mexican hat type receptive fields. If the receptive fields of the <b>input</b> <b>neurons</b> have identical properties (deterministic model), a set of parameter domains is found, which characterize different kinds of receptive field maturation behaviour of the network. Results obtained by other authors with similar models are contained in this description as special cases. In addition, the more general and rarely investigated stochastic model, where random variations of the parameters describing the receptive fields of the <b>input</b> <b>neurons</b> occur, is investigated. A high sensitivity of the network against these random variations is obtained. In case of large variations of receptive field parameters of the ganglion cells, a qualitatively new kind of maturation behaviour appears. A significant part of the synaptic connections from ganglion cells to the cortical cell is removed and small simple cell receptive fields with only few lobes emerge. The stochastic model is found to provide a better description of the size, scatter and structure of receptive fields present in biological systems, than the deterministic model...|$|R
40|$|Based on the T-S fuzzy neural network, {{the types}} of corona {{discharge}} are recognized. In this paper，three types of corona discharge test model are designed, then characteristic signals of corona discharge, the maximum, minimum and mean value and including the fractal dimension, are extracted, and then used as the input vectors of neural network, furthermore, accordance with the numbers of input vector and speciality of membership function, the T-S FNN topological structure is investigated and constructed, at the same time, the 4 neurons are selected used as <b>input</b> <b>neuron</b> and the 3 Gaussian functions are choose membership function, then the T-S FNN topological, which is trained and tested for this network, the structure is confirmed as the pattern recognition network in {{the types of}} corona discharge, and results show that the network is great effective for pattern recognition of corona discharge. Key words: T-S fuzzy neural network; membership function; corona discharge; pattern recognition 1...|$|E
40|$|An {{algorithm}} for {{the training}} of multilayered feedforward neural networks is presented. The strategy {{is very similar to}} the well-known tiling algorithm, yet the resulting architecture is completely different. New hidden units are added to one layer only in order to correct the errors of the previous ones; standard perceptron learning can be applied. The output of the network is given by the product of these k (± 1) neurons (parity machine). In a special case with two hidden units, the capacity αc and stability of the network can be derived exactly by means of a replica-symmetric calculation. Correlations between the two sets of couplings vanish exactly. For the case of arbitrary k, estimates of αc are given. The asymptotic capacity per <b>input</b> <b>neuron</b> of a network trained according to the proposed algorithm is found to be αc ~ k lnk for k→∞ in the estimation. This is in agreement with recent analytic results for the algorithm-independent capacity of a parity machine. ...|$|E
40|$|The goal of {{this work}} is to {{benchmark}} the custom design computational architecture supporting Artificial Neural Network (ANN) acceleration. The custom design optimizes the frequently used: multiply and accumulate (MAC) operations. In this work {{the performance of the}} custom design is compared with ARM and MIPS architectures supporting basic Single Instruction Multiple Data (SIMD) instructions. Benchmarking is performed by verifying the number of instructions required to compute n <b>input</b> <b>neuron.</b> The custom design is implemented using Very High Speed Integrated Circuits Hardware Description Language (VHDL) for Xilinx Spartan 6 Series FPGA Family. The execution speed is not considered as benchmarking parameter since the custom design is verified on the FPGA family. The ARM and MIPS architectures referenced here are basic architectures supporting SIMD instructions. Loading the inputs into registers and storing the results back into the memory depends upon the bus architecture supported and varies from architecture to architecture. Load and store instructions {{are not part of the}} benchmarkin...|$|E
5000|$|... #Subtitle level 2: [...] Pharmacological <b>input</b> {{stimulus}} <b>neuron</b> models ...|$|R
50|$|The NEAT {{approach}} {{begins with}} a perceptron-like feed-forward network of only <b>input</b> <b>neurons</b> and output neurons. As evolution progresses through discrete steps, {{the complexity of the}} network's topology may grow, either by inserting a new neuron into a connection path, or by creating a new connection between (formerly unconnected) neurons.|$|R
3000|$|The type of the {{controlling}} neural network was chosen a three-layer feedforward network with a sigmoidal transfer function, {{which has been}} adapted by backpropagation. Here are the <b>input</b> <b>neurons</b> labeled X_i, hidden neurons Z_i, and output neurons Y_i [...]. Hidden and output neurons have a threshold equaled to value “ 1 ”.|$|R
