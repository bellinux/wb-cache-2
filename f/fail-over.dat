172|0|Public
5000|$|Integrated {{repository}} synchronisation (for <b>fail-over</b> servers).|$|E
50|$|<b>Fail-over</b> {{times are}} demonstrably {{in the region}} of 50ms.|$|E
5000|$|Oracle TAF (Transparent Application <b>Fail-over)</b> and HA (High availability) support) ...|$|E
50|$|Automatic multi-copy {{replication}} for {{the entire}} database {{is built into the}} Clusterpoint database software. It is active replication, with workload sharing within a cluster. Clusterpoint supports high-performance OLTP transactions, ACID-compliant, within a main cluster in a single data center, while providing <b>fail-over</b> to more datacenters running database replica clusters. <b>Fail-over</b> takes only few seconds, if communication latency among data centers is minor.|$|E
5000|$|A {{number of}} JobScheduler agents can be {{configured}} in an agent cluster for redundancy and <b>fail-over.</b>|$|E
5000|$|Fault Tolerance (almost instant {{stateful}} <b>fail-over</b> of a VM in {{the event}} of a physical host failure) ...|$|E
50|$|For {{any given}} failure, a <b>fail-over</b> or {{redundancy}} can almost always be designed and {{incorporated into a}} system.|$|E
50|$|CUBRID High Availability {{provides}} load-balanced, fault-tolerant {{and continuous}} service availability through its shared-nothing clustering, <b>fail-over</b> and fail-back automated mechanisms.|$|E
50|$|Finally, most {{electric}} locking hardware still have mechanical keys as a <b>fail-over.</b> Mechanical key locks {{are vulnerable to}} bumping.|$|E
5000|$|... #Caption: RMDS Sample Complex System with <b>fail-over</b> capabilities, cross-site resiliency, RTIC-enabled caching and {{distribution}} across RV for select clients.|$|E
5000|$|... 8692omSF Switch Fabric and CPU 8692 with Expansion Mezzanine card, Supports 50 ms <b>fail-over</b> on NNI trunks with MultiLink Trunking ...|$|E
5000|$|Oracle NoSQL Database leverages Paxos-based {{automated}} <b>fail-over</b> {{election process}} {{in the event of}} a master replica node failure to minimize downtime.|$|E
50|$|RTR {{manages the}} {{messages}} sent between client-server to provide node and network <b>fail-over</b> for increased reliability, transactional integrity, and interoperability between dissimilar systems.|$|E
5000|$|No special {{hardware}} {{is required}} for building <b>fail-over</b> systems: in case the primary host PC fails, the secondary host PC may start polling IP controllers.|$|E
5000|$|Multihoming {{support in}} which one or both {{endpoints}} of a connection can consist {{of more than one}} IP address, enabling transparent <b>fail-over</b> between redundant network paths.|$|E
50|$|The {{system was}} {{grounded}} in the principle of <b>fail-over</b> and in case of power loss or other unforeseen circumstances would switch over to a fully functioning secondary site.|$|E
50|$|In May 2013, Wyless Group {{introduced}} Wyless Connect, a Bundled Solution offering, combining {{routing equipment}} and a managed solution for connectivity {{and implementation of}} <b>fail-over</b> and back-up requirements for multi-branch businesses.|$|E
50|$|Processing unit: The unit of {{scalability}} and <b>fail-over.</b> Normally, a {{processing unit}} is {{built out of}} a POJO (Plain Old Java Object) container, such as that provided by the Spring Framework.|$|E
50|$|HDFS {{added the}} high-availability capabilities, as {{announced}} for version 2.0 in May 2012, letting the main metadata server (the NameNode) manually <b>fail-over</b> onto a backup. The project has also started developing automatic fail-overs.|$|E
50|$|Sept 24 2013 - Official {{release of}} the eponymous NoMachine, or NX 4, sees the {{introduction}} of new products, native support for accessing Windows and Mac hosts, browser-based access, <b>fail-over</b> and clustering capabilities, screen recording, bidirectional file-transfer, H.264, SSL-encryption.|$|E
5000|$|Hybrid Cloud {{solutions}} that replicate both on-site and to off-site data centers. These solutions provide {{the ability to}} instantly <b>fail-over</b> to local on-site hardware, but {{in the event of}} a physical disaster, servers can be brought up in the cloud data centers as well.|$|E
50|$|According to a {{study by}} Wikibon in 2012, Aerospike is the leading data-in-flash {{database}} for transactional analytic applications, and it can answer over 200 thousand transactions per second per node. Additionally, with automatic <b>fail-over,</b> replication, and cross data center synchronization, the Aerospike database can store terabytes of data.|$|E
50|$|SLA-driven {{container}}: The SLA-driven container {{enables the}} deployment of the application on a dynamic pool of machines based on Service Level Agreements. SLA definitions include the number of instances that need to run in order {{to comply with the}} application scaling and <b>fail-over</b> policies, as well as other policies.|$|E
50|$|CNR is a DHCP/DHCPv6 server used by cable-based {{and similar}} network service {{providers}} {{because of its}} support for <b>fail-over</b> between redundant servers, Dynamic DNS updates so that DHCP leases are reflected in DNS data, integration with directory services using LDAP Version 3, and ability to handle high request rates. It is also extensible.|$|E
50|$|The NSC {{system was}} {{designed}} for high availability—all system services were either redundant or would <b>fail-over</b> from one node to another in {{the advent of a}} node crash. The disk subsystem was either accessible from multiple nodes (using a Fibre Channel SAN or dual-ported SCSI) or used cross-node mirroring in a similar fashion to DRBD.|$|E
50|$|Using this {{technology}} allows or enables {{the use of}} several links (from 2 up to 8) and combined them to create increased bandwidth and several <b>fail-over</b> paths. This produces server to switch or switch to switch connections that are up to 8 times faster. In the past redundant links were unused due to Spanning Tree’s loop protection.|$|E
50|$|CloudStore (KFS, {{previously}} Kosmosfs) was Kosmix's C++ {{implementation of}} the Google File System. It parallels the Hadoop project, which is implemented in the Java programming language. CloudStore supports incremental scalability, replication, checksumming for data integrity, client side <b>fail-over</b> and access from C++, Java and Python. There is a FUSE module so that the file system can be mounted on Linux.|$|E
50|$|SpaceWire and IEEE 1355 DS-DE {{allows for}} a wider set of speeds for data transmission, and some new {{features}} for automatic failover. The <b>fail-over</b> features let data find alternate routes, so a spacecraft can have multiple data buses, and be made fault-tolerant. SpaceWire also allows the propagation of time interrupts over SpaceWire links, {{eliminating the need for}} separate time discretes.|$|E
5000|$|The Location API is {{designed}} to work with many different positioning methods. Generic interfaces lets application developers implement systems that can retrieve location data from multiple sources, based on their availability on the device. It has advantages over a single means of tracking, including <b>fail-over,</b> indoor/outdoor transparency, and a choice between the speed and accuracy trade-offs between GPS, cellular, or other positioning methods. The API supports: ...|$|E
50|$|A similar risk exists {{inside the}} data center. Servers must be sized {{correctly}} {{in order to}} deliver adequate performance to end users. In a cloud-based computing model, the servers can also represent a {{single point of failure}} risk. If a server fails, end users lose access to all of the resources supported by that server. This risk can be mitigated by building redundancies, <b>fail-over</b> processes, backups, and load balancing utilities into the system.|$|E
50|$|BigFix Relays act as {{concentration}} {{points for}} Fixlet messages on network infrastructures. Relays are a software module that execute as a shared service on non-dedicated hardware. Relays help reduce network bandwidth requirements for distribution of BigFix Fixlets and content such as software, patches, updates, and other information. Relays {{also offer a}} <b>fail-over</b> mechanism to keep BigFix-managed clients {{in touch with the}} BigFix console should “normal” communications channels go dark or become overloaded with other traffic.|$|E
50|$|OS4000 Rel 3 arrived around 1980, and {{included}} Linked-OS — support for Linked OS4000 operating systems to enable multi-node systems to be constructed. The main customer {{for this was}} the central computing service of University College London (Euclid), where a multi-node system consisting of a Hub file server and multiple Rim multi-access compute server systems provided service for over 100 simultaneous users. Linked-OS was also used to construct <b>fail-over</b> Process control systems with higher resilience.|$|E
50|$|VIP {{addresses}} {{are also}} used for connection redundancy by providing alternative <b>fail-over</b> options for one machine. For this to work, the host has to run an interior gateway protocol like OSPF, and appear as a router {{to the rest of}} the network. It advertises virtual links connected via itself to all of its actual network interfaces. If one network interface fails, normal OSPF topology reconvergence will cause traffic to be sent via another interface.|$|E
50|$|Tibero RDBMS {{enables a}} stable and {{efficient}} management of DBMSs and guarantees high-performance transaction processing, using the Tibero Active Cluster (hereafter TAC) technology, which is a failover operation based on a shared disk clustering system environment. TAC allows instances on different nodes to share the same data via the shared disk. It supports stable system operation (24x365) with the <b>fail-over</b> function, and optimal transaction processing by guaranteeing the integrity of data in each instance’s memory.|$|E
50|$|The RTR {{software}} {{has three}} logical entities {{and referred to}} as front-end (FE), back-end (BE) and transaction-router(TR). The router is a software component that provides the <b>fail-over</b> intelligence and manages connections to the back-end.The client applications running on the Front-End combined with Router and Server applications running on back-end interact to provide transaction integrity and reliability. The three logical entities can exist on the same node but are usually deployed on different nodes to achieve modularity, scalability and high availability.|$|E
5000|$|Aerospike's {{database}} is {{a combination}} of three layers: the Client Layer, the Distribution Layer, and the Data Storage Layer. The Aerospace Client Layer is designed for speed, and includes open source client libraries that utilize Aerospike APIs, track nodes, and keep track of data. The Distribution Layer is a self-managing attribute that automates <b>fail-over,</b> replication, and data migration. The Data Storage Layer is flash-optimized and stores data in both RAM and Flash. Data is stored in policy containers referred to as [...] "namespaces”.|$|E
