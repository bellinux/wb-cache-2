7152|2940|Public
25|$|It is {{noteworthy}} that working in a higher-dimensional <b>feature</b> <b>space</b> increases the generalization error of support vector machines, although given enough samples the algorithm still performs well.|$|E
25|$|The {{original}} maximum-margin hyperplane algorithm {{proposed by}} Vapnik in 1963 constructed a linear classifier. However, in 1992, Bernhard E. Boser, Isabelle M. Guyon and Vladimir N. Vapnik suggested {{a way to}} create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.) to maximum-margin hyperplanes. The resulting algorithm is formally similar, except that every dot product {{is replaced by a}} nonlinear kernel function. This allows the algorithm to fit the maximum-margin hyperplane in a transformed <b>feature</b> <b>space.</b> The transformation may be nonlinear and the transformed space high dimensional; although the classifier is a hyperplane in the transformed <b>feature</b> <b>space,</b> it may be nonlinear in the original input space.|$|E
25|$|Recent {{algorithms}} {{for finding}} the SVM classifier include sub-gradient descent and coordinate descent. Both techniques {{have proven to}} offer significant advantages over the traditional approach when dealing with large, sparse datasets—sub-gradient methods are especially efficient when there are many training examples, and coordinate descent when the dimension of the <b>feature</b> <b>space</b> is high.|$|E
40|$|In this paper, {{we propose}} {{a class of}} graphs, the {{tripartite}} directed acyclic graphs (tDAGs), to model first-order rule <b>feature</b> <b>spaces</b> for sentence pair classification. We introduce an algorithm for computing the similarity in first-order rewrite rule <b>feature</b> <b>spaces.</b> Our algorithm is extremely efficient and, as it computes the similarity of instances that can be represented in explicit <b>feature</b> <b>spaces,</b> it is a valid kernel function. ...|$|R
40|$|We {{describe}} {{an approach to}} the combination of music similarity <b>feature</b> <b>spaces</b> {{in the context of}} music classification. The approach is based on taking the product of posterior probabilities obtained from separate classifiers for the different <b>feature</b> <b>spaces.</b> This allows for a different influence of the classifiers per song and an overall classification accuracy improving those resulting from individual <b>feature</b> <b>spaces</b> alone. This is demonstrated by combining spectral and rhythmic similarity for classification of ballroom dance music...|$|R
40|$|Abstract — This paper {{discusses}} {{the effectiveness of}} deep autoencoder neural networks in visual reinforcement learning (RL) tasks. We propose a framework for combining deep autoencoder neural networks (for learning compact <b>feature</b> <b>spaces)</b> with recently-proposed batch-mode RL algorithms (for learning policies). An emphasis is put on the data-efficiency of this combination and on studying {{the properties of the}} <b>feature</b> <b>spaces</b> automatically constructed by the deep auto-encoders. These <b>feature</b> <b>spaces</b> are empirically shown to adequately resemble existing similarities between observations and allow to learn useful policies. We propose several methods for improving the topology of the <b>feature</b> <b>spaces</b> making use of task-dependent information in order to further facilitate the policy-learning. Finally, we present first results on successfully learning good control policies using synthesized and real images. I...|$|R
25|$|The National Development and Reform Commission has {{formally}} approved Chengdu's proposed {{establishment of}} a national bio-industry base there. The government of Chengdu has recently unveiled a plan to create a 90 billion CNY bio pharmaceutical sector by 2012. China's aviation industries have begun construction of a high-tech industrial park {{in the city that}} will <b>feature</b> <b>space</b> and aviation technology. The local government plans to attract overseas and domestic companies for service outsourcing and become a well-known service outsourcing base in China and worldwide.|$|E
25|$|Another core idea of M-Theory {{is close}} in spirit to {{ideas from the}} field of {{compressed}} sensing. An implication from Johnson–Lindenstrauss lemma says that a particular number of images can be embedded into a low-dimensional <b>feature</b> <b>space</b> with the same distances between images by using random projections. This result suggests that dot product between the observed image and some other image stored in memory, called template, {{can be used as a}} feature helping to distinguish the image from other images. The template need not to be anyhow related to the image, it could be chosen randomly.|$|E
500|$|Goldenheart is a post-breakup concept {{album that}} explores themes of {{imagination}} and dreams. In discussing trials of relationships, it portrays personal subjects as epic tales of battle and salvation. Gerrick D. Kennedy of the Los Angeles Times writes that its stories of romantic and professional heartbreak are [...] "tightly intertwined through Richard's imagery". Her lyrics employ religious imagery, battle motifs, and allusions to high fantasy and science fiction tropes, including heroic last stands, world-dominating empires, parted oceans, starflights, vampiric lovers, and military deployment, all used as metaphors for internal landscape and personal conflict. [...] "Northern Lights" [...] and [...] "Frequency" [...] <b>feature</b> <b>space</b> travel and cybernetic imagery, respectively, {{with the latter}} song featuring bandwidth references such as [...] "your signal's found a home" [...] and [...] "stimulation makes it flow". Jesse Cataldo from Slant Magazine observes [...] "a kind of feverish mysticism" [...] on the album, which he views is [...] "concerned with magical imagery and the self-restorative properties of the human heart." [...] "'86" [...] is titled after the slang term and is about ridding oneself of barriers.|$|E
40|$|A {{real-time}} on-road {{vehicle tracking}} method {{is presented in}} this work. The tracker builds statistical models for the target in color and shape <b>feature</b> <b>spaces</b> and continuously evaluates each of the <b>feature</b> <b>spaces</b> by computing the similarity score between the probabilistic distributions of the target and the model. Based on the similarity scores, the final location of the target is determined by fusing the potential locations found in different <b>feature</b> <b>spaces</b> together. The proposed method has been evaluated on real data, illustrating good performance...|$|R
50|$|A subtype <b>featuring</b> <b>space,</b> {{science fiction}} and horror in film.|$|R
40|$|Entity classification, {{like many}} other {{important}} problems in NLP, involves learning classifiers over sparse high-dimensional <b>feature</b> <b>spaces</b> that result from the conjunction of elementary fea-tures of the entity mention and its context. In this paper we develop a low-rank reg-ularization framework for training max-entropy models in such sparse conjunctive <b>feature</b> <b>spaces.</b> Our approach handles con-junctive <b>feature</b> <b>spaces</b> using matrices and induces an implicit low-dimensional rep-resentation via low-rank constraints. We show that when learning entity classifiers under minimal supervision, using a seed set, our approach is more effective in con-trolling model capacity than standard tech-niques for linear classifiers. ...|$|R
2500|$|Using a {{different}} annealing technology based on {{nuclear magnetic resonance}} (NMR), a quantum Hopfield network was implemented in 2009 that mapped the input data and memorized data to Hamiltonians, allowing the use of adiabatic quantum computation. NMR technology also enables universal quantum computing, and it {{was used for the}} first experimental implementation of a quantum support vector machine to distinguish hand written number ‘6’ and ‘9’ on a liquid-state [...] quantum computer in 2015. The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors to represent the images as the states of a qubit. The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image. Once the vectors are defined on the <b>feature</b> <b>space,</b> the quantum support vector machine was implemented to classify the unknown input vector. The readout avoids costly quantum tomography by reading out the final state in terms of direction (up/down) of the NMR signal.|$|E
2500|$|Whereas the {{original}} {{problem may be}} stated in a finite dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that {{the original}} finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products may be computed easily {{in terms of the}} variables in the original space, by defining them in terms of a kernel function [...] selected to suit the problem. The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters [...] of images of feature vectors [...] that occur in the data base. With this choice of a hyperplane, the points [...] in the <b>feature</b> <b>space</b> that are mapped into the hyperplane are defined by the relation: [...] Note that if [...] becomes small as [...] grows further away from , each term in the sum measures the degree of closeness of the test point [...] to the corresponding data base point [...] In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in {{one or the other of}} the sets to be discriminated. Note the fact that the set of points [...] mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets which are not convex at all in the original space.|$|E
50|$|The {{vector space}} {{associated}} with these vectors is often called the <b>feature</b> <b>space.</b> In {{order to reduce the}} dimensionality of the <b>feature</b> <b>space,</b> a number of dimensionality reduction techniques can be employed.|$|E
5000|$|... "Daily News" [...] (<b>featuring</b> <b>Space</b> Ghost Purp, Earl Sweatshirt & Action Bronson) ...|$|R
40|$|This paper investigates how to {{automatically}} classify schemaless XML data into a user-defined topic directory. The main {{focus is on}} constructing appropriate <b>feature</b> <b>spaces</b> on which a classifier operates. In addition to the usual text-based term frequency vectors, we study XML twigs and tag paths as extended features that can be combined with text term occurrences in XML elements. Moreover, we show how to leverage ontological background information, more specifically, the WordNet thesaurus, {{for the construction of}} more expressive <b>feature</b> <b>spaces.</b> For efficiency our implementation computes features incrementally and caches ontology entries. Our experiments demonstrate the improved accuracy of automatic classification based on the enhanced <b>feature</b> <b>spaces...</b>|$|R
40|$|This paper {{investigates the}} {{practicality}} {{and effectiveness of}} conceptual spaces {{as a framework for}} knowledge representation. We empirically compares and contrasts two popular quantitative lazy learning systems (nearest neighbor learning and prototype learning) within conceptual spaces and mere multidimensional <b>feature</b> <b>spaces.</b> Experimental results demonstrates conceptual spaces are superior to mere multidimensional <b>feature</b> <b>spaces</b> in concept learning and confirm the virtue of conceptual spaces...|$|R
5000|$|A mapping {{function}} [...] {{is required to}} map each query and the element of database to a <b>feature</b> <b>space.</b> Then each point in the <b>feature</b> <b>space</b> is labelled with certain rank by ranking method.|$|E
50|$|Definition 2: Visual term: {{it is the}} {{clustering}} {{result in}} the <b>feature</b> <b>space</b> (centers of the clusters), more than one patch can give nearest information in <b>feature</b> <b>space,</b> so we can consider {{it in the same}} term.|$|E
5000|$|... #Caption: Left: A {{partitioned}} two-dimensional <b>feature</b> <b>space.</b> These partitions {{could not}} have resulted from recursive binary splitting. Middle: A partitioned two-dimensional <b>feature</b> <b>space</b> with partitions that did result from recursive binary splitting. Right: A tree corresponding to the partitioned <b>feature</b> <b>space</b> in the middle. Notice the convention that when the expression at the split is true, the tree follows the left branch. When the expression is false, the right branch is followed.|$|E
40|$|Abstract. This work {{serves as}} an initial {{investigation}} into improvements to classification accuracy of an imagined movement-based Brain Computer Interface (BCI) by combining the <b>feature</b> <b>spaces</b> of two unique measurement modalities: functional near infrared spectroscopy (fNIRS) and electroencephalography (EEG). Our dual-modality system recorded concurrent and co-locational hemodynamic and electrical responses in the motor cortex during an imagined movement task, participated in by two subjects. Offline analysis and classification of fNIRS and EEG data was performed using leave-one-out cross-validation (LOOCV) and linear discriminant analysis (LDA). Classification of 2 -dimensional fNIRS and EEG <b>feature</b> <b>spaces</b> was performed separately and then their <b>feature</b> <b>spaces</b> were combined for further classification. Results of our investigation indicate that by combining <b>feature</b> <b>spaces,</b> modest gains in classification accuracy of an imagined movement-based BCI {{can be achieved by}} employing a supplemental measurement modality. It is felt that this technique may be particularly useful in the design of BCI devices for the augmentation of rehabilitation therapy. 1...|$|R
50|$|Episode 93 of Gin Tama <b>features</b> <b>Space</b> Woman, a {{gigantic}} alien with similar appearance to Ultraman.|$|R
5000|$|Ter Mur — Land of the Gargoyles. The capital, Ter Mur, <b>features</b> <b>space</b> for player homes.|$|R
5000|$|To extend LDA to {{non-linear}} mappings, the data, {{given as}} the [...] points [...] can be mapped {{to a new}} <b>feature</b> <b>space,</b> [...] via some function [...] In this new <b>feature</b> <b>space,</b> the function {{that needs to be}} maximized is ...|$|E
50|$|Rough fuzzy {{hybridization}} is {{a method}} of hybrid intelligent system or soft computing, where Fuzzy set theory is used for linguistic representation of patterns, leading to a fuzzy granulation of the <b>feature</b> <b>space.</b> Rough set theory is used to obtain dependency rules which model informative regions in the granulated <b>feature</b> <b>space.</b>|$|E
5000|$|The {{signs and}} {{marketing}} {{for the museum}} <b>feature</b> <b>Space</b> Invaders.|$|E
40|$|Feature design most {{difficult}} aspect in designing a learning system complex and difficult phase, e. g., structural feature representation: deep knowledge and intuitions are required design problems when {{the phenomenon is}} described by many features Motivation (2) Kernel methods alleviate such problems Structures represented in terms of substructures High dimensional <b>feature</b> <b>spaces</b> Implicit and abstract <b>feature</b> <b>spaces</b> Generate high number of features Support Vector Machines “select ” the relevant features Automatic Feature engineering side-effec...|$|R
40|$|The {{issue of}} seeking {{efficient}} and effective methods for classifying unstructured text in large document corpora has received much attention in recent years. Traditional document representation like bag-of-words encodes documents as feature vectors, which usually leads to sparse <b>feature</b> <b>spaces</b> with large dimensionality, thus {{making it hard to}} achieve high classification accuracies. This paper addresses the problem of classifying unstructured documents on the Web. A classification approach is proposed that utilizes traditional feature reduction techniques along with a collaborative filtering method for augmenting document <b>feature</b> <b>spaces.</b> The method produces <b>feature</b> <b>spaces</b> with an order of magnitude less features compared with a baseline bag-of-words feature selection method. Experiments on both real-world data and benchmark corpus indicate that our approach improves classification accuracy over the traditional methods for both Support Vector Machines and AdaBoost classifiers. ...|$|R
3000|$|To asses if the {{differences}} in performance between pose normalization applied in different <b>feature</b> <b>spaces</b> are statistically significant, we compute p [...]...|$|R
5000|$|Expand the <b>feature</b> <b>space</b> {{of machine}} {{learning}} / text mining systems ...|$|E
5000|$|PCA or Fisher LDA {{projection}} into <b>feature</b> <b>space,</b> {{followed by}} k-NN classification ...|$|E
50|$|Kernel matrix {{defines the}} {{proximity}} of the input information. For example, in Gaussian Radial basis function, determines the dot product of the inputs in a higher-dimensional space, called <b>feature</b> <b>space.</b> It is believed that the data become more linearly separable in the <b>feature</b> <b>space,</b> and hence, linear algorithms can be applied on the data with a higher success.|$|E
40|$|Typical tasks: from text {{categorization}} to syntactic and semantic parsing Feature {{design for}} NLP applications: complex and difficult phase, e. g., structural feature representation: deep knowledge and intuitions are required design problems when {{the phenomenon is}} described by many features Kernels for Natural Language Processing (2) Kernel methods alleviate such problems Structures represented in terms of substructures High dimensional <b>feature</b> <b>spaces</b> Implicit and abstract <b>feature</b> <b>spaces</b> Generate high number of features Support Vector Machines “select ” the relevant feature...|$|R
40|$|Image {{retrieval}} using {{relevance feedback}} {{can be considered}} as a classification process. In practice, the generalization of classifier is often constrained by the insufficiency of training samples. In this paper, we propose a novel relevance feedback approach capable of collecting more representative samples. Image labeling and classifier training are conducted in two complementary image <b>feature</b> <b>spaces.</b> The complementarities between <b>feature</b> <b>spaces</b> are also studied. Our experimental result based on 10, 000 images indicates that the proposed approach significantly improves image retrieval performance...|$|R
40|$|Low-cost {{inertial}} {{and motion}} sensors embedded on smartphones {{have provided a}} new platform for dynamic activity pattern inference. In this research, a comparison has been conducted on different sensor data, <b>feature</b> <b>spaces</b> and <b>feature</b> selection methods {{to increase the efficiency}} and reduce the computation cost of activity recognition on the smartphones. We evaluated a variety of <b>feature</b> <b>spaces</b> and a number of classification algorithms from the area of Machine Learning, including Naive Bayes, Decision Trees, Artificial Neural Networks and Support Vector Machine classifiers. A smartphone app that performs activity recognition is being developed to collect data and send them to a server for activity recognition. Using extensive experiments, the performance of various <b>feature</b> <b>spaces</b> has been evaluated. The results showed that the Bayesian Network classifier yields recognition accuracy of 96. 21 % using four features while requiring fewer computations...|$|R
