455|681|Public
2500|$|Suppose [...] {{is a set}} of {{independent}} random variables from a set of [...] Poisson distributions, each with a parameter , , and we would like to estimate these parameters. Then, Clevenson and Zidek show that under the normalized squared <b>error</b> <b>loss</b> , when , then, similar as in Stein's famous example for the Normal means, the MLE estimator [...] is inadmissible.|$|E
2500|$|The {{algorithm}} starts by a stochastic {{mapping of}} [...] to [...] through , {{this is the}} corrupting step. Then the corrupted input [...] passes through a basic auto-encoder process and is mapped to a hidden representation [...] From this hidden representation, we can reconstruct [...] In the last stage, a minimization algorithm runs {{in order to have}} z {{as close as possible to}} uncorrupted input [...] The reconstruction error [...] might be either the cross-entropy loss with an affine-sigmoid decoder, or the squared <b>error</b> <b>loss</b> with an affine decoder.|$|E
5000|$|... #Caption: Huber loss (green, [...] ) and squared <b>error</b> <b>loss</b> (blue) as a {{function}} of ...|$|E
50|$|The {{format is}} robust to {{transmission}} <b>errors.</b> <b>Loss</b> of whole packets {{as well as}} bit errors can be masked with a steady degradation of audio quality (packet loss concealment, PLC).|$|R
40|$|Abstract ∗ In this paper, {{we propose}} a {{transport}} control performance improvement algorithm in heterogeneous networks. This paper discusses some novel algorithms {{to estimate the}} available bandwidth and to distinguish congestion <b>error</b> <b>losses</b> from wireless transmission <b>error</b> <b>losses.</b> The main characteristics of the algorithms {{is that it is}} non intrusive, i. e., it does not need to inject the traffic into the network in order to estimate the available bandwidth. Thus, our algorithms proposed can improve the throughput and can also be applied to different types of networks adaptively. We report experimental results to efficiency of the proposed algorithm. 1...|$|R
50|$|Connection-oriented protocols, such as {{the widely}} used TCP protocol, {{generally}} watch for packet <b>errors,</b> <b>losses,</b> or delays (see Quality of Service) to adjust the transmit speed. Various network congestion avoidance processes, support different trade-offs.|$|R
5000|$|Under squared <b>error</b> <b>loss</b> (SEL), the {{conditional}} expectation E(θi | Yi = yi) {{is a reasonable}} quantity to use for prediction. For the Poisson compound sampling model, this quantity is ...|$|E
50|$|In statistics, the Huber loss {{is a loss}} {{function}} used in robust regression, that is less sensitive to outliers in data than the squared <b>error</b> <b>loss.</b> A variant for classification is also sometimes used.|$|E
5000|$|Suppose [...] {{is a set}} of {{independent}} random variables from a set of [...] Poisson distributions, each with a parameter , , and we would like to estimate these parameters. Then, Clevenson and Zidek show that under the normalized squared <b>error</b> <b>loss</b> , when , then, similar as in Stein's famous example for the Normal means, the MLE estimator [...] is inadmissible.|$|E
40|$|In {{this article}} we study the {{simultaneous}} estimation of the Poisson means in J-way multiplicative models and a decomposable model for three-way layouts. The estimators which improve on the maximum likelihood estimators under the normal-ized squared <b>error</b> <b>losses</b> are provided for each model. The proposed estimator...|$|R
40|$|AbstractFor the p-variate Poisson mean, {{under the}} sum of {{weighted}} squared <b>error</b> <b>losses,</b> weights being reciprocals of variances, a class of proper Bayes minimax estimates dominating the usual estimate, namely the sample mean is produced. An example is given to illustrate this. The interrelation of our results with those of Clevenson and Zidek is pointed out...|$|R
40|$|Rapport de {{recherche}} n ° 5306 — Septembre 2004 — 39 pages Abstract: This paper {{analyzes the}} performance of a large population composed of several classes of long lived TCP flows experiencing packet losses due to random transmission errors and to congestion created by the sharing of a common tail-drop or AQM bottleneck router. Each class has a different transmission error rate. This setting is used to analyze the competition between wired and wireless users in an access network, where one class (the wired class) has no or small (like BER in DSL) transmission <b>error</b> <b>losses</b> whereas the other class has higher transmission <b>error</b> <b>losses,</b> or the competition between DSL flows using different coding schemes. We propose a natural and simple model for the joint throughput evolution of several classes of TCP flows under such a mix of losses. Two types of random transmission <b>error</b> <b>losses</b> are considered: one where losses are Poisson and independent of the rate of the flow, and one where the losses are still Poisson but with an intensity that is proportional to the rate of the source. We show that the large population model where the population tends to infinity has a threshold on the transmission error rate (given in closed form) above which there are no congestion losses at all in steady state, and below which the stationary state is a periodic congestion regime in which we compute both the mean value and the distribution of th...|$|R
50|$|To {{ensure that}} bearer traffic in LTE {{networks}} is appropriately handled, a mechanism {{is needed to}} classify {{the different types of}} bearers into different classes, with each class having appropriate QoS parameters for the traffic type. Examples of the QoS parameters include Guaranteed Bit Rate (GBR) or non-Guaranteed Bit Rate (non-GBR), Priority Handling, Packet Delay Budget and Packet <b>Error</b> <b>Loss</b> rate. This overall mechanism is called QCI.|$|E
50|$|The James-Stein {{estimator}} is a nonlinear estimator {{which can}} be shown to dominate, or outperform, the ordinary least squares technique {{with respect to a}} mean-square <b>error</b> <b>loss</b> function. Thus least squares estimation is not necessarily an admissible estimation procedure. Some others of the standard estimates associated with the normal distribution are also inadmissible: for example, the sample estimate of the variance when the population mean and variance are unknown.|$|E
50|$|In statistics, {{the mean}} squared error (MSE) or mean squared {{deviation}} (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures {{the average of the}} squares of the errors or deviations—that is, the difference between the estimator and what is estimated. MSE is a risk function, corresponding to the expected value of the squared <b>error</b> <b>loss</b> or quadratic loss. The difference occurs because of randomness or because the estimator doesn't account for information that could produce a more accurate estimate.|$|E
40|$|The {{problem of}} {{simultaneous}} {{estimation of the}} eigenvalues of a covariance matrix is considered under a sum of squared <b>errors</b> <b>loss.</b> A new class of estimators which is a generalization of Dey's (1988) class of estimators is given. As an immediate consequence, {{a new class of}} estimators of trace of the covariance matrix is obtained. Wishart identity estimation of eigenvalues covariance matrix...|$|R
40|$|Transmission Control Protocol (TCP) {{performance}} degrades in broadband {{geostationary satellite}} networks due to long propagation delays and high bit error rates. In this thesis, we propose TCP with algorithm modifications for adaptive delay and loss response (TCP-ADaLR) to improve TCP performance. TCP-ADaLR incorporates delayed acknowledgement mechanism recommended for Internet hosts. We evaluate {{and compare the}} performance of TCP-ADaLR, TCP SACK, and TCP NewReno, with and without delayed acknowledgements. In the ideal channel case, TCP-ADaLR exhibits the lowest user-perceived latency for FTP and HTTP applications. In the presence of congestion, TCP-ADaLR shows comparable performance to TCP SACK and TCP NewReno. In the presence of <b>error</b> <b>losses,</b> TCP-ADaLR exhibits improvements up to 61 % and 76 % in throughput and utilization, respectively. In the presence of both congestion and <b>error</b> <b>losses,</b> TCP-ADaLR exhibits goodput and throughput improvements up to 43 %. TCP-ADaLR exhibits better fairness and friendliness than TCP NewReno and maintains TCP end-to-end semantics...|$|R
30|$|Therefore, the TCP {{throughput}} for wireless networks can {{be improved}} by accurately identifying the cause of packet loss [34, 62, 490] and reducing the TCP transmission rate only when congestion is detected. However, TCP congestion control has no mechanism for identifying the cause of packet loss. We term this problem as packet loss classification and various efforts {{have been made to}} propose solutions to this problem. In general, the solutions for packet loss classification fall in two broad categories, depending on where the solution is implemented in the network, that is, at intermediate nodes or in end-systems. The former requires additional implementation at the intermediate nodes that either hide the <b>error</b> <b>losses</b> from the sender [32, 33], or communicate to the sender extra statistics about the network state, such as congestion notification [483] and burst acknowledgment (ACK) [490]. It is important to mention that hiding <b>error</b> <b>losses</b> may violate TCP end-to-end principle as it may require splitting the TCP connection by sending an ACK to the sender before the packet arrives at the receiver [129].|$|R
5000|$|Consider {{the problem}} of {{estimating}} a deterministic (not Bayesian) parameter [...] from noisy or corrupt data [...] related through the conditional probability distribution [...] Our goal {{is to find a}} [...] "good" [...] estimator [...] for estimating the parameter , which minimizes some given risk function [...] Here the risk function is the expectation of some loss function [...] with respect to [...] A popular example for a loss function is the squared <b>error</b> <b>loss</b> , and the risk function for this loss is the mean squared error (MSE).|$|E
50|$|When sailing in high winds, a {{small boat}} or dinghy can capsize shortly after a jibe due to {{helmsman}} <b>error</b> (<b>loss</b> of direction control, or suddenly rounding into the wind too far) or tripping over the centerboard. It is partly for this second reason that centerboards are often lifted while sailing downwind even in non-planing hulls, the main reason being that a centreboard/keel is not needed for sailing downwind and simply adds to the drag of the hull. Raising the centreboard reduces drag and increases the boat's speed.|$|E
5000|$|The {{algorithm}} starts by a stochastic {{mapping of}} [...] to [...] through , {{this is the}} corrupting step. Then the corrupted input [...] passes through a basic auto-encoder process and is mapped to a hidden representation [...] From this hidden representation, we can reconstruct [...] In the last stage, a minimization algorithm runs {{in order to have}} z {{as close as possible to}} uncorrupted input [...] The reconstruction error [...] might be either the cross-entropy loss with an affine-sigmoid decoder, or the squared <b>error</b> <b>loss</b> with an affine decoder.|$|E
5000|$|Catastrophic - Failure {{may cause}} a crash. <b>Error</b> or <b>loss</b> of {{critical}} function required to safely fly and land aircraft.|$|R
40|$|For the p-variate Poisson mean, {{under the}} sum of {{weighted}} squared <b>error</b> <b>losses,</b> weights being reciprocals of variances, a class of proper Bayes minimax estimates dominating the usual estimate, namely the sample mean is produced. An example is given to illustrate this. The interrelation of our results with those of Clevenson and Zidek is pointed out. p-variate Poisson means Bayes estimation admissibility minimaxity two stage prior exponential type II beta Dirichlet monotone likelihood ratio...|$|R
40|$|This paper {{analyzes}} {{the performance of}} a large population composed of several classes of long lived TCP flows experiencing packet losses due to random transmission errors and to congestion created by the sharing of a common tail-drop or AQM bottleneck router. Each class has a different transmission error rate. This setting is used to analyze the competition between wired and wireless users in an access network, where one class (the wired class) has no or small (like BER in DSL) transmission <b>error</b> <b>losses</b> whereas the other class has higher transmission <b>error</b> <b>losses,</b> or the competition between DSL flows using different coding schemes. We propose a natural and simple model for the joint throughput evolution of several classes of TCP flows under such a mix of losses. Two types of random transmission <b>error</b> <b>losses</b> are considered: one where losses are Poisson and independent of the rate of the flow, and one where the losses are still Poisson but with an intensity that is proportional to the rate of the source. We show that the large population model where the population tends to infinity has a threshold on the transmission error rate (given in closed form) above which there are no congestion losses at all in steady state, and below which the stationary state is a periodic congestion regime in which we compute both the mean value and the distribution of the rate obtained by each class of flow. We also show that the maximum mean value for the aggregated rate is achieved at the threshold. For the finite population model and models based on other classes of point processes, a sufficient condition is obtained for the existence of congestion times in the case of arbitrary transmission error point processes...|$|R
5000|$|Therefore, {{gradient}} boosting {{will fit}} [...] to the residual [...] Like in other boosting variants, each [...] learns to correct its predecessor [...] A generalization {{of this idea}} to loss functions other than squared error - and to classification and ranking problems - follows from the observation that residuals [...] for a given model are the negative gradients (with respect to [...] ) of the squared <b>error</b> <b>loss</b> function [...] So, gradient boosting is a gradient descent algorithm; and generalizing it entails [...] "plugging in" [...] a different loss and its gradient.|$|E
5000|$|Squared <b>error</b> <b>loss</b> {{is one of}} {{the most}} widely used loss {{functions}} in statistics, though its widespread use stems more from mathematical convenience than considerations of actual loss in applications. Carl Friedrich Gauss, who introduced the use of mean squared error, was aware of its arbitrariness and was in agreement with objections to it on these grounds. [...] The mathematical benefits of mean squared error are particularly evident in its use at analyzing the performance of linear regression, as it allows one to partition the variation in a dataset into variation explained by the model and variation explained by randomness.|$|E
50|$|According to 3GPP TS 23.203, 9 QCI {{values in}} Rel-8 (13 QCIs Rel-12, 15 QCIs Rel-14) are {{standardized}} and associated with QCI characteristics {{in terms of}} packet forwarding treatment that the bearer traffic receives edge-to-edge between the UE and the P-GW. Scheduling priority, resource type, packet delay budget and packet <b>error</b> <b>loss</b> rate are the set of characteristics defined by the 3GPP standard {{and they should be}} understood as guidelines for the pre-configuration of node specific parameters to ensure that applications/services mapped to a given QCI receive the same level of QoS in multi-vendor environments as well as in roaming scenarios. The QCI characteristics are not signalled on any interface.|$|E
40|$|In {{reflector}} system design, achieving high {{stability of}} phase center position {{with changes in}} frequency in reflector feed antennas is highly desired. However, obtaining highly stable phase center is not possible for UWB feed antennas, specially for planar ones. Thus, an optimum positioning for the UWB feed antenna should be defined. Optimization of the positioning of the feed antenna is essential since this process lowers resulting phase <b>error</b> <b>losses</b> significantly. In this work, a novel method for optimizing the UWB feed position of a prime focus reflector antenna from phase and amplitude recordings of the measured radiated field is introduced. An automatic and fast design procedure, based on Genetic Algorithms, is described. The proposed methodology has been numerically and experimentally assessed. The procedure is introduced by an application example {{to one of the}} most commonly used UWB feed antennas in high-performance reflector antenna systems: Linear Tapered Slot Antenna (LTSA). A LTSA antenna operating in 6 – 25 [*]GHz frequency band is designed and manufactured. The performance of the method is quantified in terms of its phase <b>error</b> <b>losses</b> in E- and H-planes for reflector illumination...|$|R
40|$|Long {{propagation}} {{delays and}} high bit error rates in heterogeneous networks with {{geostationary earth orbit}} (GEO) satellite links have {{negative impact on the}} performance of Transmission Control Protocol (TCP). In this paper, we propose modifications to TCP by introducing adaptive delay and loss response (TCP-ADaLR) to mitigate the adverse effects of satellite link characteristics. The proposed modifications incorporate delayed acknowledgment (ACK) recommended for Internet hosts. TCP-ADaLR introduces adaptive window increase and loss recovery mechanisms to address TCP performance degradation in satellite networks. We evaluate and compare the performance of TCP-ADaLR, TCP SACK, and TCP NewReno, with delayed ACK enabled and disabled. In the absence of losses, TCP-ADaLR exhibits the shortest user-perceived latency for HTTP and FTP applications. In the presence of only congestion losses, TCP-ADaLR shows comparable performance to TCP SACK and TCP NewReno. In the presence of only <b>error</b> <b>losses,</b> TCP-ADaLR exhibits improvements up to 61 % and 76 % in throughput and utilization, respectively. In the presence of both congestion and <b>error</b> <b>losses,</b> TCP-ADaLR exhibits goodput and throughput improvements up to 43 %. TCP-ADaLR exhibits the best performance in the absence of losses and in the presence of losses due to both congestion and errors. It also friendly to TCP NewReno, exhibits better fairness, and maintains TCP end-to-end semantics...|$|R
30|$|The {{usage of}} HFS from {{and up to}} the {{application}} level is justified in several cases by the interest and necessity of representing true bit reality. One of these cases corresponds to the introduction of forward error correction codes in the higher layers of the protocol stack, as for instance, with RTP-FEC or more generally IETF FecFrame approaches, which aim at overcoming remaining <b>losses</b> or <b>errors</b> at transport or application layers, without requiring a full TCP integrity mechanism. Another case corresponds to the transmission of multimedia data, whose codecs are often resilient to small <b>errors</b> or <b>losses,</b> and for which <b>errors</b> or <b>losses</b> positions are critical to evaluate their real impact on the end-user and measure the PQoS. This is the case considered by the French ANR DITEMOI project, in which <b>error</b> and <b>loss</b> resilient H. 264 /AVC decoders were introduced [31], and new strategies for limiting retransmission in video sessions in a multiple users context are being studied.|$|R
50|$|The frequentist {{procedures}} of significance testing and confidence intervals {{can be constructed}} without regard to utility functions. However, some elements of frequentist statistics, such as statistical decision theory, do incorporate utility functions. In particular, frequentist developments of optimal inference (such as minimum-variance unbiased estimators, or uniformly most powerful testing) make use of loss functions, which {{play the role of}} (negative) utility functions. Loss functions need not be explicitly stated for statistical theorists to prove that a statistical procedure has an optimality property. However, loss-functions are often useful for stating optimality properties: for example, median-unbiased estimators are optimal under absolute value loss functions, in that they minimize expected loss, and least squares estimators are optimal under squared <b>error</b> <b>loss</b> functions, in that they minimize expected loss.|$|E
50|$|Kalman-based Stochastic Gradient Descent (kSGD) is {{an online}} and offline {{algorithm}} for learning parameters from statistical problems from quasi-likelihood models, which include linear models, non-linear models, generalized linear models, and neural networks with squared <b>error</b> <b>loss</b> as special cases. For online learning problems, kSGD {{is a special}} case of the Kalman Filter for linear regression problems, a special case of the Extended Kalman Filter for non-linear regression problems, and {{can be viewed as}} an incremental Gauss-Newton method. The benefits of kSGD, in comparison to other methods, are (1) it is not sensitive to the condition number of the problem , (2) it has a robust choice of hyperparameters, and (3) it has a stopping condition. The drawbacks of kSGD is that the algorithm requires storing a dense covariance matrix between iterations, and requires a matrix-vector product at each iteration.|$|E
50|$|Regularization {{perspectives}} on support vector machines provide {{a way of}} interpreting support vector machines (SVMs) {{in the context of}} other machine learning algorithms. SVM algorithms categorize multidimensional data, with the goal of fitting the training set data well, but also avoiding overfitting, so that the solution generalizes to new data points. Regularization algorithms also aim to fit training set data and avoid overfitting. They do this by choosing a fitting function that has low error on the training set, but also is not too complicated, where complicated functions are functions with high norms in some function space. Specifically, Tikhonov regularization algorithms choose a function that minimize the sum of training set error plus the function's norm. The training set error can be calculated with different loss functions. For example, regularized least squares is a special case of Tikhonov regularization using the squared <b>error</b> <b>loss</b> as the loss function.|$|E
40|$|Optical burst {{switching}} {{is one of}} {{the most}} promising next-generation all-optical data transport paradigms. In this paper, we discuss forward error correction as a candidate for providing loss recovery in an optical burstswitched network. We develop a network-level analytical model to evaluate the packet loss probability of forward <b>error</b> correction <b>loss</b> recovery mechanism. We also develop a simulation model to investigate the proposed forward <b>error</b> correction <b>loss</b> recovery mechanism and to compare the performance of our proposed mechanism with the existing burst retransmission loss recovery mechanism. Our results show that the proposed mechanism significantly reduces the packet loss in an optical burst-switched network...|$|R
40|$|Abstract: Challenges in {{monitoring}} optically-transparent networks are highlighted for dynamically-controlled Raman amplification systems. We use models of amplifier physics together with statistical estimation to automatically discriminate between measurement <b>errors,</b> anomalous <b>losses,</b> and pump failures...|$|R
40|$|Comparisons of {{estimates}} between Bayes and frequentist {{methods are}} inter-esting and challenging topics in statistics. In this paper, Bayes estimates and predictors are derived for a normal distribution. The commonly used frequentist predictor {{such as the}} maximum likelihood estimate (MLE) is a “plug-in ” procedure by substituting the MLE of µ into the predictive dis-tribution. We examine Bayes prediction under the α-absolute <b>error</b> <b>losses,</b> the LINEX losses and the entropy loss as special case of the α-absolute er-ror losses. If the variance is unknown, the joint conjugate prior is used to estimate the unknown mean for the α-absolute <b>error</b> <b>losses</b> and an ad hoc method by replacing the unknown variance by the sample variance for the LINEX losses. Bayes estimates are also extended to the linear combinations of regression coefficients. Under certain assumptions for a design matrix, the asymptotic expected losses are derived. Under suitable priors, Bayes esti-mate and predictor perform better than the MLE. Under the LINEX loss, the Bayes estimate under the Jeffreys prior is superior to the MLE. However, for prediction, {{it is not clear}} whether Bayes prediction or MLE performs better. Under some circumstances, even when one loss is the “true ” loss function, Bayes estimate under another loss performs better than the Bayes estimate under the “true ” loss. This serves as a warning to naive Bayesians who assume that Bayes methods always perform well regardless of circumstances...|$|R
