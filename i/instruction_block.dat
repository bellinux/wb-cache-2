23|138|Public
2500|$|In PowerPC G1, G2, G3, and G4 {{pages are}} {{normally}} [...] After a TLB miss, the standard PowerPC MMU begins two simultaneous lookups. [...] One lookup attempts {{to match the}} address with one of four or eight data block address translation (DBAT) registers, or four or eight <b>instruction</b> <b>block</b> address translation registers (IBAT), as appropriate. [...] The BAT registers can map linear chunks of memory as large as [...] and are normally used by an OS to map large portions of the address space for the OS kernel's own use. [...] If the BAT lookup succeeds, the other lookup is halted and ignored.|$|E
50|$|WFL {{also has}} an <b>instruction</b> <b>block</b> command {{which is used to}} give {{operators}} instructions needed to run the current job. These instructions are displayed using the 'IB' operator command.|$|E
50|$|On 23 February 1967, in Hương Trà, Vietnam, Major Badcoe rescued, {{under heavy}} fire, a United States Medical Adviser. On 7 March, he led his company {{in an attack}} and turned {{what seemed to be}} certain defeat into a victory. Again, on 7 April, he {{attempted}} to lead his company against more powerful opposition. This final act of bravery resulted in his death. The main hall at Officer Cadet School, Portsea was named in his honour, as is the main lecture theatre in the Military <b>Instruction</b> <b>Block</b> at the Royal Military College, Duntroon.|$|E
40|$|In usual Genetic Programming (GP) schemes, {{only the}} best {{programs}} survive {{from one generation to}} the next. This implies that useful code, that might be hidden inside in-trons in low fitness individuals, is often lost. In this paper, we propose a new representation borrowing from Linear GP (LGP), called PhenoGP, where solutions are coded as or-dered lists of <b>instruction</b> <b>blocks.</b> The main goal of evolution is then to find the best ordering of the <b>instruction</b> <b>blocks,</b> with possible repetitions. When the fitness remains stalled, ignored <b>instruction</b> <b>blocks,</b> which have a low probability to be useful, are replaced. Experiments show that PhenoGP achieve competitive results against standard LGP...|$|R
50|$|Captain Adam Mohamed, {{in charge}} of {{security}} unit, was informed of this uprising. He issued <b>instructions</b> to <b>block</b> the premises. Upon Adam Mohamed's <b>instruction</b> to <b>block</b> the premises, Jail Security Unit personnel were issued riot gear. Prisoners on the loose confronted with this unit. Since prisoners highly out numbered them; security personnel in riot gear soon retreated.|$|R
40|$|Future {{processors}} combining {{out-of-order execution}} with aggressive speculation techniques {{will need to}} fetch multiple non-consecutive <b>instruction</b> <b>blocks</b> in a single cycle to achieve high-performance. Several high-bandwidth instruction fetching schemes have been proposed {{in the past few}} years. The Two-Block Ahead (TBA) branch predictor predicts two non-consecutive <b>instruction</b> <b>blocks</b> per cycle while relying on a conventional instruction cache. The trace cache (TC) records traces of instructions and delivers multiple non-consecutive <b>instruction</b> <b>blocks</b> to the execution core. The aim {{of this paper is to}} investigate the pros and cons of both approaches. Maintaining consistency between memory and TC is not a straightforward issue. We propose a simple hardware scheme to maintain consistency at a reasonable performance loss (1 to 5 %). We also introduce a new fill unit heuristic for TC, the mispredict hint, that leads to significantly better performance (up to 20 %). This is mainly due to better prediction accuracy results and TC miss ratios. TBA requires double-ported or bank-interleaved structures to supply two non-consecutive blocks in a single cycle. We show that a 4 -way interleaving scheme is cost-effective since it impairs performance by only 3 to 5 %. Finally, simulation results show that such an enhanced TC scheme delivers higher performance than TBA when caches are large, due to a lower branch misprediction penalty and a higher instruction bandwidth on mispredictions. When the hardware budget is smaller, TBA outperforms TC because of a higher TC miss ratio and branch misprediction rate...|$|R
50|$|In PowerPC G1, G2, G3, and G4 {{pages are}} {{normally}} 4 KB. After a TLB miss, the standard PowerPC MMU begins two simultaneous lookups. One lookup attempts {{to match the}} address with one of four or eight data block address translation (DBAT) registers, or four or eight <b>instruction</b> <b>block</b> address translation registers (IBAT), as appropriate. The BAT registers can map linear chunks of memory as large as 256 MB, and are normally used by an OS to map large portions of the address space for the OS kernel's own use. If the BAT lookup succeeds, the other lookup is halted and ignored.|$|E
50|$|Mill uses a {{very long}} {{instruction}} word (VLIW)-style encoding to store up to 33 simple operations in wide instruction words, termed opcodes. Mill uses two program counters, and every wide instruction is split into two parts. One of the program counters counts backward. So, the code of every linear <b>instruction</b> <b>block</b> is executed from its middle to outside by two almost independent decoders. Unused operations are deleted by a small fixed-format data item {{in the center of}} each instruction. This helps maintain code density by reducing the incidence of no-operation codes in Mill code. It also allows each functional unit to start speculatively executing its instruction field, and then discard its result if it has no instruction.|$|E
40|$|A basic rule in {{computer}} architecture {{is that a}} processor cannot execute an application faster than it fetches its instructions. This paper presents a novel costeffective mechanism called the two-block ahead branch predictor. Information from the current <b>instruction</b> <b>block</b> is not used for predicting {{the address of the}} next <b>instruction</b> <b>block,</b> but rather for predicting the block following the next <b>instruction</b> <b>block.</b> This approach overcomes the instruction fetch bottleneck exhibited by wide-dispatch "brainiac" processors by enabling them to efficiently predict addresses of two instruction blocks in a single cycle. Furthermore, pipelining the branch prediction process can also be done by means of our predictor for "speed demon" processors to achieve higher clock rate or to improve the prediction accuracy by means of bigger prediction structures. Moreover, and unlike the previously-proposed multiple predictor schemes, multiple-block ahead branch predictors can use any of the branch predictio [...] ...|$|E
30|$|Instruction: Num. Chars {{captures the}} number of {{characters}} in the <b>instruction</b> text <b>block</b> of a recipes.|$|R
50|$|Sir Charles Tupper Secondary School {{operates}} on a linear timetable on a Day 1 / Day 2 rotation, from September to June. Each day has three blocks of 1 hour and 15 minutes of <b>instruction</b> in each <b>block,</b> and one block of 1 hour and 17 minutes of <b>instruction.</b> The <b>blocks</b> rotate {{four times a}} year.|$|R
50|$|Step 2: As {{the block}} is already {{present in the}} cache and in an {{exclusive}} state so it directly modifies that without any bus <b>instruction.</b> The <b>block</b> {{is now in a}} modified state.|$|R
40|$|We propose {{and analyze}} an {{adaptive}} instruction prefetch scheme, called threaded prefetching, that {{makes use of}} history information to guide the prefetching. The scheme {{is based on the}} observation that control flow paths are likely to repeat themselves. In the proposed scheme, we associate with each <b>instruction</b> <b>block</b> a number of threads that indicate the instruction blocks that have been brought into the cache by the current block. These threads later trigger the prefetching of the indicated instruction blocks once the <b>instruction</b> <b>block</b> containing them are re-accessed by the processor. These pointers, in effect, encode the causal relationship between an <b>instruction</b> <b>block</b> and the instruction blocks that have been brought into the cache by the block. The results from trace-driven simulations using SPEC benchmarks show that the proposed scheme improves the prefetch accuracy by more than 100 % on average for 32 Kbyte cache. They also show that the scheme significantly (80 % on average for [...] ...|$|E
40|$|One of the {{obstacles}} to using RISC processors in a hard real-time environment is the unpredictability of caches. This unpredictability stems from basing them on a design that tries to optimize the average case execution time. In this paper, we propose a dual mode instruction prefetch scheme {{as an alternative to}} instruction caching schemes. In the proposed scheme, a thread is associated with each <b>instruction</b> <b>block.</b> The thread indicates the <b>instruction</b> <b>block</b> that is to be prefetched once the block containing it is accessed by the processor. The proposed scheme operates in two different modes: real-time and non real-time modes. In the real-time mode, the prefetching of instruction blocks is made in the direction that improves the worst case execution time. For this purpose, the thread is generated by the compiler through an analysis of the worst case execution path. In the non real-time mode, the thread is dynamically updated so that it indicates the <b>instruction</b> <b>block</b> that is most likely [...] ...|$|E
40|$|Block {{scheduling}} {{is a topic}} {{selected by}} this researcher. This researcher investigated the effects of block scheduling in relationship to Mathematics <b>instruction.</b> <b>Block</b> scheduling is a very hotly contested subject for schools at this time and this researcher {{was concerned about the}} effects of this type of scheduling on high school Mathematics <b>instruction.</b> <b>Block</b> scheduling was developed in response to criticisms regarding the ineffective use of school time. With block scheduling, students take four classes per day, each lasting 90 minutes. Classes are completed in one semester rather than lasting for an entire year under a traditional schedule. Benefits of block scheduling are that students are able to take more courses throughout high school and have more time available for electives. Less time is wasted on transitioningii between classes and administrative duties, allowing this time to be used for instruction. Teachers are able to use innovative instructional strategies as they have a longer time period in which to complete their lessons. Issues with block scheduling include students having to focus their attention for longer periods of time. Some teachers continue to use lecturing as their only instructional tool, and 9...|$|E
40|$|Abstract. The {{interaction}} of register allocation and instruction scheduling is a well-studied problem: Certain ways of arranging <b>instructions</b> within basic <b>blocks</b> reduce overlaps of live ranges, {{leading to the}} insertion of less costly spill code. However, there is little previous research on the extension of this problem to global code motion, i. e., the motion of <b>instructions</b> between <b>blocks.</b> We present an algorithm that models global code motion as an optimization problem {{with the goal of}} minimizing overlaps between live ranges in order to minimize spill code. Our approach analyzes the program to identify the live range overlaps for all possible placements of <b>instructions</b> in basic <b>blocks</b> and all orderings of <b>instructions</b> within <b>blocks.</b> Using this information, we formulate an optimization problem to determine code motions and partial local schedules that minimize the overall cost of live range overlaps. We evaluate solutions of this optimization problem using integer linear programming, where feasible, and a simple greedy heuristic. We conclude that global code motion with the sole goal of avoiding spills rarely leads to performance improvements because code is placed too conservatively. On the other hand, purely local optimal instruction scheduling for minimal spilling is e ective at improving performance when compared to a heuristic scheduler for minimal register use. ...|$|R
50|$|The {{institution}} is near Gandhi Nagar {{on the outskirts}} of the city, 3 km from the airport on the Gwalior bypass road. The campus is spread over 247 acre. The main buildings start 1 km from the main gate. The campus has <b>instruction</b> <b>blocks,</b> workshop, library block, nanotechnology block, School of Energy and Environment block and Administrative block.The campus has residential facilities for the staff and separate hostels for boys and girls. The campus is full of greenery and sightseeing locations specially in rainy season. Besides the above-mentioned blocks, the campus includes an Energy Park, Bio-diesel Plant, Medicinal Garden and a number of windmills and solar panels which contribute to the energy sources of the university.|$|R
40|$|International audienceThe high {{availability}} of {{a huge number of}} documents on the Web makes plagiarism very attractive and easy. This plagiarism concerns any kind of document, natural language texts as well as more structured information such as programs. In order to cope with this problem, many tools and algorithms have been proposed to ﬁnd similarities. In this paper we present a new algorithm designed to detect similarities in source codes. Contrary to existing methods, this algorithm relies on the notion of function and focuses on obfuscation with inlining and outlining of functions. This method is also eﬃcient against insertions, deletions and permutations of <b>instruction</b> <b>blocks.</b> It is based on code factorization and uses adapted pattern matching algorithms and structures such as suffix arrays...|$|R
40|$|This {{case study}} {{explored}} {{the journey of}} four first grade teachers in their pursuit {{to improve the quality}} of their small group instruction time through increased engagement of students away from the small groups, thus allowing for quality instruction taking place in the small group. The teachers participated in professional development on literacy work stations that included video and an accompanying text. Based on the qualitative data from observations and interviews, all four teachers believed that the quality of their small group instruction improved with the implementation of literacy work stations through increased student engagement and motivation and the subsequent decrease in interruptions to the small group instruction. ^ Four themes emerged: The “I Can…” List, Schedule Issues, Group Numbers, and Professional Development – Teachers Seeking Feedback. The “I can…” lists allowed students to make decisions, work independently, increase engagement, and allowed the teacher to teach in the small group without interruptions. ^ Because schedule issues impacted the effectiveness of the small group <b>instruction</b> <b>block</b> and the implementation of literacy stations, teachers believed that the quality of the small group <b>instruction</b> <b>block</b> could be improved with longer periods of uninterrupted instruction, a decrease in the flow of students in and out of the class, and the inclusion of a paraprofessional in the classroom. ^ An additional theme regarding professional development emerged from this study. The four teachers continued to seek feedback and support in order to fully implement the instructional practices, making the professional development stronger. ...|$|E
30|$|The core {{implementation}} of the Xeon is configured using VS to operate at a frequency of 2 [*]GHz and has 128 [*]kB of L 1 cache (64 [*]kB data and 64 [*]kB instruction cache), 4 [*]MB of unified and shared L 2 cache [5], floating point, integer, and load/store execution units. Here as well, the instruction queue length is set to 6 and instructions {{are included in the}} instruction set of both the cores, so as to make the memory access comparison void of all other differences in the architectures. These instructions are defined in the <b>instruction</b> <b>block</b> that is described in a later section.|$|E
30|$|Each core of Opteron {{implemented}} in the project using VS is configured to a frequency of 2 [*]GHz and has 128 [*]kB of L 1 cache (64 [*]kB data and 64 [*]kB instruction cache), 2 [*]MB of L 2 cache, and the floating point, integer, and load/store execution units. This 2 [*]MB of L 2 cache per core {{is compatible with the}} 4 [*]MB of shared cache used in the Intel Xeon memory architecture. The instruction queue length is set to 6 and instructions are included in the instruction set of both the cores, so as to make the memory access comparison void of all other differences in the architectures. These instructions are defined in the <b>instruction</b> <b>block</b> that is further described in a later section.|$|E
40|$|This paper {{makes two}} {{observations}} {{that lead to}} a new heterogeneous core design. First, we observe that most serial code exhibits fine-grained heterogeneity: at the scale of tens or hundreds of instructions, regions of code fit different microarchitectures better (at the same point or {{at different points in}} time). Second, we observe that by grouping contiguous regions of <b>instructions</b> into <b>blocks</b> that are executed atomically, a core can exploit this fine-grained heterogeneity: atomicity allows each block to be executed independently on its own execution backend that fits its characteristics best. Based on these observations, we propose a fine-grained hetero-geneous core design, called the heterogeneous block architecture (HBA), that combines heterogeneous execution backends into one core. HBA breaks the program into blocks of code, determines the best backend for each block, and specializes the block for that backend. As an example HBA design, we combine out-of-order, VLIW, and in-order backends, using simple heuristics to choose backends for different dynamic <b>instruction</b> <b>blocks.</b> Our extensive evaluations compare this example HBA design to multiple baseline core designs (including monolithic out-of-order, clustered out-of-order, in-order and a state-of-the-art heterogeneous core design) and show that it provides significantly better energy efficiency than all designs at similar performance. I...|$|R
25|$|The Hitachi 6309 was an {{enhanced}} {{version of the}} 6809 with extra registers and additional <b>instructions,</b> including <b>block</b> move, additional multiply instructions and hardware-implemented division. It was used in unofficially-upgraded Tandy Color Computer 3 computers and a version of OS-9 was written to take advantages of the 6309's extra features: NitrOS-9.|$|R
3000|$|... of {{application}} profiles, each representing {{the behaviors of}} {{a part of the}} application, which are possibly periodic. Our model does not presume a certain granularity for application profiles and thus can be chosen according to the individual needs. For example, ranging from fine to coarse grained, application profiles can represent individual <b>instructions,</b> basic <b>blocks,</b> functions, or groups thereof.|$|R
40|$|With more {{computing}} platforms {{connected to}} the Internet each day, computer system security has become a critical issue. One of the major security problems is execution of malicious injected code. In this paper we propose new processor extensions that allow execution of trusted instructions only. The proposed extensions verify <b>instruction</b> <b>block</b> signatures in run-time. Signatures are generated during a trusted installation process, using a multiple input signature register (MISR), and stored in an encrypted form. The coefficients of the MISR and the key used for signature encryption {{are based on a}} hidden processor key. Signature verification is done in the background, concurrently with program execution, thus reducing negative impact on performance. The preliminary results indicate that the proposed processor extensions will prevent execution of any unauthorized code at a relatively small increase in system complexity and execution time. 1...|$|E
40|$|Software {{integrity}} and confidentiality {{play a central}} role in making embedded computer systems resilient to various malicious actions, such as software attacks; probing and tampering with buses, memory, and I/O devices; and reverse engineering. In this paper we describe an efficient hardware mechanism that protects software {{integrity and}} guarantees software confidentiality. To provide software integrity, each <b>instruction</b> <b>block</b> is signed during program installation with a cryptographically secure signature. The signatures embedded in the code are verified during program execution. Software confidentiality is provided by encrypting instruction blocks. To achieve low performance overhead, the proposed mechanism combines several architectural enhancements: a variation of one-time-pad encryption, parallelizable signatures, and conditional execution of unverified instructions. A relatively high memory overhead due to embedded signatures can be reduced by protecting multiple instruction blocks with one signature, with minimal effects on complexity and performance overhead. 1...|$|E
40|$|Cache {{memories}} {{have been}} extensively used {{to bridge the}} speed gap between high speed processors and relatively slow main memory. However, they are not widely used in real-time systems due to their unpredictable performance. This paper proposes an instruction prefetching scheme called threaded prefetching {{as an alternative to}} instruction caching in real-time systems. In the proposed threaded prefetching, an <b>instruction</b> <b>block</b> pointer called a thread is assigned to each instruction memory block and is made to point to the next block on the worst case execution path that is determined by a compile-time analysis. Also, the thread is not updated throughout the entire program execution to guarantee predictability. This paper also compares the worst case performances of various previous instruction prefetching schemes with that of the proposed threaded prefetching. By analyzing several benchmark programs, we show that the worst case performance of the proposed scheme is significantly better [...] ...|$|E
40|$|As {{processors}} {{continue to}} experience relatively rapid clock speed increases, the gap widens between cpu and memory performance. Unlike other studies that collect memory traces and analyze them for compile time optimization or propose cache organization {{best suited for}} an application group, this paper tackles the problem at its roots, namely analyzing data access patterns and optimizing them before implementation. Optimization done by today's compilers is mostly loop level. Function level optimization is limited to inlining code that often leads to poor instruction cache utilization, affecting code performance adversely. In this study, an algorithm to solve compressible Euler equations is studied with regard to temporal and spatial access of data. Data and <b>instruction</b> <b>blocks,</b> which are used most often, are isolated. The algorithm is then coded to utilize the characteristics of hierarchial memories with as much as 45 % improvement over conventional optimization techniques. 1 Introductio [...] ...|$|R
40|$|Programs {{written in}} a typed {{language}} are guaranteed to satisfy the safety properties of the type system without runtime checks. A type system for an intermediate language allows static verification of safety properties independent of source languages, and opens up opportunities for advanced compiler optimizations. This paper surveys three major intermediate languages: Java bytecode, typed assembly language and proof-carrying code. Java bytecode requires minimal type annotation but sophisticated verification algorithms. Typed assembly language permits low-level constructs such as registers and <b>instruction</b> <b>blocks,</b> yet still enforces control- flow safety and memory safety. Proof-carrying code provides a general framework for any safety properties definable in a meta-logical framework. We motivate the use of typed intermediate languages, illustrate the type systems of the three languages mentioned above with examples, and compare their tradeoffs of expressiveness versus complexity. Additionally, we {{assess the impact of}} the three languages and identify research directions for future work...|$|R
40|$|The {{functions}} were discribed and {{the operating}} <b>instructions,</b> the <b>block</b> diagram {{and the proposed}} versions are given for modifying the program {{in order to obtain}} the statistical characteristics of multi-channel video information. The program implements certain man-machine methods for investigating video information. It permits representation of the material and its statistical characteristics in a form which is convenient for the user...|$|R
40|$|Measuring and {{modeling}} instantaneous current consumption or current dynamics of a processor {{is important in}} embedded system designs, wireless communications, low energy mobile computing, security of communications, and reliability. This thesis introduced a new instruction-level based macro-modeling approach for instantaneous current consumption in a complex processor core along with new instantaneous current measurement techniques at the instruction and program level. Current consumption and voltage supply waveforms of a processor core were acquired by a sampling oscilloscope through an external interrupt-based setup. Accurate measurements of current, power and energy consumption at the <b>instruction,</b> <b>block,</b> or program level were obtained from analyzing the stored current and voltage waveforms. The simulated instantaneous current waveform at the program level was generated in two steps. First, a base waveform of the current simulation was generated {{by the use of}} four basic current superposition principles applied to current approximating functions at the instruction level. Secondly, a final waveform of the simulated current was generated from the base waveform by applying a factorial adjustment {{as a function of the}} instruction parallelism and sequencing. Step-by-step current modeling procedure...|$|E
40|$|This paper {{describes}} a new instruction-supply mechanism, called the eXtended Block Cache (XBC). The {{goal of the}} XBC is to improve on the Trace Cache (TC) hit rate, while providing the same bandwidth. The improved hit rate is achieved by having the XBC a nearly redundant free structure. The basic unit recorded in the XBC is the extended block (XB), which is a multiple-entry single-exit <b>instruction</b> <b>block.</b> A XB is a sequence of instructions ending on a conditional or an indirect branch. Unconditional direct jumps do not end a XB. In order to enable multiple entry points per XB, the XB index {{is derived from the}} IP of its ending instruction. Instructions within the XB are recorded in reverse order, enabling easy extension of XBs. The multiple entry-points remove most of the redundancy. Since there is at most one conditional branch per XB, we can fetch up to n XBs per cycle by predicting n branches. The multiple fetch enables the XBC to match the TC bandwidth. 1. Introduction A processor m [...] ...|$|E
40|$|Abstract—Embedded system {{designers}} face {{a unique}} set of challenges in making their systems more secure, as these systems often have stringent resource constraints or must operate in harsh or physically insecure environments. One of the security issues that have recently drawn attention is software integrity, which ensures that the programs in the system have not been changed either by an accident or an attack. In this paper we propose an efficient hardware mechanism for runtime verification of software integrity using encrypted <b>instruction</b> <b>block</b> signatures. We introduce several variations of the basic mechanism, and give details of three techniques that are most suitable for embedded systems. Performance evaluation using selected MiBench, Mediabench, and Basicrypt benchmarks indicates that the considered techniques impose a relatively small performance overhead. The best overall technique has performance overhead in the range 0 - 8 %, when protecting 128 byte instruction blocks with 16 -byte signatures. With 64 -byte instruction blocks, the overhead is in the range 0 - 15 %; the average overhead with 8 KB cache is 1 %. With additional investment in a signature cache, this overhead can be almost completely eliminated. Index Terms—Computer architecture, embedded systems, secure computing, processor design, performance evaluation, security attacks, decryption...|$|E
40|$|INSTRUCTION LINE ASSOCIATIVE REGISTERS) Due to {{the growing}} {{mismatch}} between processor performance and memory latency, many dynamic mechanisms which are “invisible ” to the user have been proposed: for example, trace caches and automatic pre-fetch units. However, these dynamic mechanisms have become inadequate due to implicit memory accesses that have become so expensive. On the other hand, compiler-visible mechanisms like SWAR (SIMD Within A Register) and LARs (Line Associative Registers) are potentially more effective at improving data access performance. This thesis investigates applying the same ideas to improve instruction access. ILAR (Instruction LARs) store instructions in wide registers. <b>Instruction</b> <b>blocks</b> are explicitly loaded into ILAR, using block compression to enhance memory bandwidth. The control flow of the program then refers to instructions directly by their position within an ILAR, rather than by lengthy memory addresses. Because instructions are accessed directly from within registers, there is no implicit instruction fetch from memory. This thesis proposes an instruction set architecture for ILAR, investigates...|$|R
40|$|ScratchPad Memories (SPMs) are {{commonly}} used in embedded systems {{because they are more}} energy-efficient than caches and enable tighter application control on the memory hierarchy. Optimally mapping code and data to SPMs is, however, still a challenge. This paper proposes an optimal scratchpad mapping approach for code segments, which has the distinctive characteristic of working directly on application binaries, thus requiring no access to either the compiler or the application source code- a clear advantage for legacy or proprietary, IP-protected applications. The mapping problem is solved by means of a Dynamic Programming algorithm applied to the execution traces of the target application. The algorithm is able to find the optimal set of <b>instructions</b> <b>blocks</b> to be moved into a dedicated SPM, either minimizing energy consumption or execution times. A patching tool, which can use the output of the optimal mapper, modifies the binary of the application and moves the relevant portions of its code segments to memory locations inside of the SPM...|$|R
30|$|In program analysis, {{the program}} is {{composed}} by basic blocks. Basic blocks are code snippets with a single entry and exit point, <b>instructions</b> in basic <b>blocks</b> will be sequentially executed and will only be executed once. In code coverage measuring, state-of-the-art methods take basic block as the best granularity. The reasons include that, (1) basic block is the smallest coherent units of program execution, (2) measuring function or instruction would result in information loss or redundancy, (3) basic block could be identified by {{the address of the}} first <b>instruction</b> and basic <b>block</b> information could be easily extracted through code instrumentation.|$|R
