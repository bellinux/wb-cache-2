1003|10000|Public
5|$|Climate {{research}} on live coral species {{is limited to}} a few studied species. Studying Porites coral provides a stable foundation for geochemical interpretations that is much simpler to physically <b>extract</b> <b>data</b> in comparison to Platygyra species where the complexity of Platygyra species skeletal structure creates difficulty when physically sampled, which happens {{to be one of the}} only multidecadal living coral records used for coral paleoclimate modeling.|$|E
5|$|Two {{years after}} the Battle of New York, Steve Rogers works in Washington, D.C. for the espionage agency S.H.I.E.L.D. under Director Nick Fury, while {{adjusting}} to contemporary society. Rogers and Agent Natasha Romanoff are sent with S.H.I.E.L.D.'s counter-terrorism S.T.R.I.K.E. team, led by Agent Rumlow, to free hostages aboard a S.H.I.E.L.D. vessel from Georges Batroc and his mercenaries. Mid-mission, Rogers discovers Romanoff has another agenda: to <b>extract</b> <b>data</b> from the ship's computers for Fury. Rogers returns to the Triskelion, S.H.I.E.L.D.'s headquarters, to confront Fury and is briefed about Project Insight: three Helicarriers linked to spy satellites, designed to preemptively eliminate threats. Unable to decrypt the data recovered by Romanoff, Fury becomes suspicious about Insight and asks senior S.H.I.E.L.D. official Alexander Pierce to delay the project.|$|E
25|$|Continuous {{auditing}} {{is often}} confused with computer-aided auditing. The {{purpose and scope}} of the two techniques, however, are quite different. Computer-aided auditing employs end user technology including spreadsheet software, such as Microsoft Excel, to allow traditional auditors to run audit-specific analyses as they conduct the periodic audit. Continuous auditing, on the other hand, involves advanced analytical tools that automate a majority of the auditing plan. Where auditors manually <b>extract</b> <b>data</b> and run their own analyses in computer-aided auditing during the course of their traditional audit, high-powered servers automatically extract and analyze data at specified intervals as a part of continuous auditing.|$|E
30|$|Two authors {{developed}} a structured data extraction form and <b>extracted</b> <b>data</b> from each eligible study. This structured form ensured that all <b>data</b> were being <b>extracted</b> and recorded consistently and completely by all reviewers. They <b>extracted</b> <b>data</b> on study characteristics, including location of study, measurements of correction angles, method of data collection, etc. We <b>extracted</b> <b>data</b> directly comparing accuracy of correction of navigated HTO versus the conventional procedure.|$|R
5000|$|The UEFwalk script validates and <b>extracts</b> <b>data</b> from UEF files.|$|R
50|$|The private class data {{design pattern}} solves the {{problems}} above by <b>extracting</b> a <b>data</b> class for the target class {{and giving the}} target class instance an instance of the <b>extracted</b> <b>data</b> class.|$|R
25|$|Continuous data {{assurance}} verifies {{the integrity}} of data flowing through the information systems. Continuous data assurance uses software to <b>extract</b> <b>data</b> from IT systems for analysis at the transactional level to provide more detailed assurance. CDA systems provide the ability to design expectation models for analytical procedures at the business-process level, {{as opposed to the}} current practice of relying on ratio or trend analysis at higher levels of data aggregation. CDA software can continuously and automatically monitor transactions, comparing their generic characteristics with predetermined benchmarks, thereby identifying anomalous situations. When significant discrepancies occur, alarms are triggered and routed to appropriate stakeholders and auditors.|$|E
2500|$|Data mining [...] is the {{computing}} {{process of}} discovering patterns in large data sets involving methods {{at the intersection}} of machine learning, statistics, and database systems. It is an essential process where intelligent methods are applied to <b>extract</b> <b>data</b> patterns. It is an interdisciplinary subfield of computer science. The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use. Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating. Data mining is the analysis step of the [...] "knowledge discovery in databases" [...] process, or KDD.|$|E
2500|$|The Spanish {{government}} confirmed on 10 May 2015 {{that the}} plane’s {{flight data recorder}} and cockpit voice recorder had been recovered. Despite being examined by a joint team from the Spanish ministries of development and defense, the Spanish authorities subsequently passed the recorders to the French military air accident investigation agency BEAD to extract and analyze the data. On 13 May 2015 it emerged that technical issues were slowing retrieval of the crash data; General Bruno Caïtucoli, head of BEAD, reported that [...] "there are technical issues in reading the system, {{and it is a}} question of compatibility between systems, so we are still trying to <b>extract</b> <b>data.</b> The extracting system we are using belongs to the French defense procurement agency DGA,” Caïtucoli said, noting that the problem appeared to be a compatibility issue between the recorders and the DGA’s data reading system, rather than an issue with the condition of the recorders themselves.|$|E
5000|$|PDBsum [...] - [...] <b>extracts</b> <b>data</b> {{from other}} {{databases}} about PDB structures ...|$|R
40|$|In this paper, {{we report}} our initial {{investigations}} {{on the problems}} of automatically <b>extracting</b> <b>data</b> objects from a given hidden-web source (i. e., the web site with an HTML search form) and automatically assigning semantics to the <b>extracted</b> <b>data.</b> We also propose some future work {{to address the problem of}} information discovery and integration for hidden-web sources...|$|R
50|$|A {{form filler}} is the {{opposite}} of a screen scraper, which <b>extracts</b> <b>data</b> from a form.|$|R
5000|$|<b>Extract</b> <b>data</b> from a {{web page}} (e.g., hyperlinks, images, text, etc.) ...|$|E
5000|$|Auto-extraction - Automatically <b>extract</b> <b>data</b> from {{web pages}} into a {{structured}} dataset ...|$|E
5000|$|... {{provides}} {{a simple and}} efficient index slice syntax to <b>extract</b> <b>data</b> from large arrays.|$|E
30|$|We {{searched for}} and {{gathered}} data from MEDLINE, Elsevier, Cochrane Central Register of Controlled Trials and Web of Science databases. Studies were eligible if they compared {{the effects of}} EGDT versus control care on mortality in adult patients with severe sepsis and septic shock. Two reviewers <b>extracted</b> <b>data</b> independently. Data including mortality, sample size of the patients with severe sepsis and septic shock, and resuscitation endpoints were <b>extracted.</b> <b>Data</b> were analyzed by the methods recommended by the Cochrane Collaboration Review Manager 4.2 software.|$|R
40|$|Abstract. The paper {{addresses}} {{a problem of}} extraction of semantic information from Czech texts from the Web. The method described in this paper exploits existing linguistic tools created originally for a syntactically annotated corpus, Prague Dependency Treebank (PDT 2. 0). We are working on development of a system which captures text of web-pages, annotates it linguistically by linguistic tools, <b>extracts</b> <b>data</b> and interprets the <b>extracted</b> <b>data</b> semantically in terms of web ontologies. The proposed extraction method is based on extraction rules – tree queries, which are adopted from the Netgraph application. Semantic interpretation of these rules provides semantics of the <b>extracted</b> <b>data.</b> We present some initial experiments {{in the domain of}} reports of traffic accidents...|$|R
5000|$|Accessing {{data sources}} in the {{internet}} from within your SAP system (e.g. <b>extracting</b> <b>data</b> from online catalogs) ...|$|R
5000|$|Are there {{staff members}} trained to <b>extract</b> <b>data</b> {{elements}} {{that can be}} reused in metadata structures? ...|$|E
5000|$|<b>Extract</b> <b>data</b> {{from many}} common {{document}} formats like Microsoft Word, Excel, RTF, PowerPoint and Adobe Acrobat PDF ...|$|E
5000|$|... "Complaint Formulator", an {{electronic}} interface to let litigants <b>extract</b> <b>data</b> from their problem situation and assemble it into various legal documents; ...|$|E
30|$|Three reviewers <b>extracted</b> the <b>data</b> {{from the}} {{included}} studies independently. The <b>extracted</b> <b>data</b> included study type, number of included patients, patient characteristics, fracture characteristics {{according to the}} AO/OTA classification, details on the intervention and control group, length of follow-up and outcome measures.|$|R
50|$|<b>Data</b> can be <b>extracted</b> {{from any}} ODBC data source. Users enter their own SQL {{statement}} to <b>extract</b> the <b>data.</b> <b>Extracted</b> <b>data</b> {{can be saved}} into data files in many formats (CSV, Tab delimited, HTML, delimited text, etc.) or reused by a load.|$|R
50|$|Data {{scraping}} is {{a technique}} in which a computer program <b>extracts</b> <b>data</b> from human-readable output coming from another program.|$|R
50|$|Jackson {{also created}} CROWN, a Columbia Law School {{initiative}} to introduce data science techniques to <b>extract</b> <b>data</b> from legal filings for empirical research.|$|E
5000|$|Diffbot [...] - [...] uses {{computer}} vision and machine learning to automatically <b>extract</b> <b>data</b> from web pages by interpreting pages visually {{as a human}} being might.|$|E
5000|$|Regarding data integration, Rainer states, [...] "It is {{necessary}} to <b>extract</b> <b>data</b> from source systems, transform them, and load them into a data mart or warehouse".|$|E
30|$|Two {{researchers}} independently searched data sources, screened {{studies for}} eligibility, evaluated risk of bias, and <b>extracted</b> <b>data</b> using predefined criteria.|$|R
40|$|Version 1. 0. 0 Title Example dataset {{of input}} data for shinyMethyl Description <b>Extracted</b> <b>data</b> from 369 TCGA Head and Neck Cancer DNA {{methylation}} samples. The <b>extracted</b> <b>data</b> {{serve as an}} example dataset for the package shinyMethyl. Original samples are from 450 k methylation arrays, and were obtained from The Cancer Genome Atlas (TCGA). 310 samples are from tumor, 50 are matched normals and 9 are technical replicates of a control cell line...|$|R
30|$|Apart from <b>extracting</b> <b>data</b> access {{patterns}} and node profiles, {{we believe that}} more information is needed for a better data placement solution.|$|R
50|$|It is also {{possible}} to extract new model starting from a particular time during the simulation, open trends and <b>extract</b> <b>data</b> from the simulation in Microsoft Excel documents.|$|E
5000|$|HOWLERMONKEY: (see {{image at}} right) A RF {{transceiver}} {{that makes it}} possible (in conjunction with digital processors and various implanting methods) to <b>extract</b> <b>data</b> from systems or allow them to be controlled remotely.|$|E
50|$|Atos {{developed}} {{a computer system}} that would <b>extract</b> <b>data</b> from GP's computers Nationwide. Costs rose from £14 million to £40 million and it was felt Atos had taken insufficient care how it spent taxpayers' money.|$|E
5000|$|Multi-Format Support: The {{platform}} automatically <b>extracts</b> <b>data</b> from {{documents that}} would normally be processed manually, and can transform them into any other format.|$|R
5000|$|... parses {{the most}} recent entry and <b>extracts</b> <b>data</b> from it in a [...] format similar to [...] It is {{primarily}} used in scripts.|$|R
50|$|A Perl {{program for}} <b>extracting</b> <b>data</b> from the {{catalogue}} {{is available from}} http://archive.eso.org/ASTROM/. Tycho-2 File Formats WCSTools software uses the files catalog.dat and index.dat.|$|R
