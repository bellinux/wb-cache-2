7|10000|Public
40|$|Many {{statistics}} texts tend {{to focus}} more on the theory and mathematics underlying statistical tests than on their applications and interpretation. This can leave readers with little understanding of how to apply statistical tests or how to interpret their findings. While the SPSS statistical software has done much to alleviate the frustrations of social science professionals and students who must analyze data, they still face daunting challenges in selecting the proper tests, <b>executing</b> <b>the</b> <b>tests,</b> and interpreting the test results. With emphasis firmly on such practical matters, this handbook s...|$|E
40|$|Adaptive {{test cases}} {{are often used}} when a {{specification}} permits several possible correct outputs from an implementation under test for a given input. In this paper, we propose an algorithm to apply such adaptive test cases which is optimal {{in terms of the}} number of inputs and is more efficient than previously published algorithm for <b>executing</b> <b>the</b> <b>tests.</b> Our solution comes at a cost, a pre–processing step that must be executed once. The solution presented here is particularly interesting in situations where the same set of adaptive test cases will be applied a large number of times...|$|E
40|$|Ankara : Department of Mathematics and Institute of Engineering and Science, Bilkent Univ., 1994. Thesis (Master's) [...] Bilkent University, 1994. Includes bibliographical {{references}} leaves 74 - 76 This thesis {{consists of}} two parts. The first part, which is Chapter 2, is a survey on some aspects of Fibonacci numbers. In this part, we tried to gather some interesting properties of these numbers and some topics related to the Fibonacci sequence from various references, so that the reader may get {{an overview of the}} subject. After giving the basic concepts about the Fibonacci numbers, their arithmetical properties are studied. These include divisibility and periodicity properties, the Zeckendorf Theorem, Fibonacci trees and their relations to the representations of integers, polynomials used for deriving new identities for Fibonacci numbers and Fibonacci groups. Also in Chapter 2, natural phenomena related to the golden section, such as certain plants having Fibonacci numbers for the number of petals, or the relations of generations of bees with the Fibonacci numbers are recounted. In {{the second part of the}} thesis. Chapter 3, we focused on a Fibonacci based random number sequence. We analyzed and criticized the generator Sfc = k(j>—[k(j) ] by applying some standart tests for randomness on it. Chapter 5, the Appendix consists of Fortran programs used for <b>executing</b> <b>the</b> <b>tests</b> of Chapter 3. Yücel, GülnihalM. S...|$|E
25|$|<b>The</b> {{framework}} then <b>executes</b> <b>the</b> <b>test</b> {{for each}} scenario, with the parameters from that scenario.|$|R
3000|$|When <b>executed,</b> <b>the</b> <b>tests</b> pass, but it {{is known}} that the {{validity}} of the response is not being verified correctly; [...]...|$|R
5000|$|<b>Execute</b> <b>the</b> <b>test</b> through <b>the</b> use of <b>testing</b> tools (SW test) or {{instruments}} (HW <b>test),</b> while showing <b>the</b> {{progress and}} accepting control from the operator (for example to Abort) ...|$|R
40|$|This paper {{shows the}} first {{implementation}} of the methodology developed in [18] to a superscalar {{state of the art}} PowerPC implementation[17][16]. The experiment, which is described in detail, includes modeling of parts of that processor in SMV[10], generating abstract tests from the model using CFSM [7], converting the abstract tests into restric- tions on architectural tests, converting the restrictions into directive for a test generation tool, generating the tests using Genesys [4], <b>executing</b> <b>the</b> <b>tests</b> on the real implementation and verifying, clock by clock, that the real tests executed match the abstract tests. Similar methodologies that use formal verification to drive test generation has been sugested in the past [15][13]. As the goal {{of this paper is to}} describe an experiment, the difference between the methodologies (can be seen in [18]) is not elaborated. The results from the experiment were very encouraging both from the theoretical and practical consideration. We showed that using the methodology suggested it is possible to generate architctural tests with known micro architectural properties for a real, very large, design. We showed that using a very small model we can generate tests on the real pipelines that have exactly the same timing. This is a very important achievement because it lets us generate directly, and not by trial and error, the cases in which the window of opportunity for an event is very narrow. We compar our results to the current methodology and show its benefits. The rest of this paper is organized as follows: In section 2 we give an overview of the methodology used. I n section 3 we describe the processor to which we apply this methodology. In section 4 we describe the experiment with its many stages. Section 5 details some of the probl [...] ...|$|E
40|$|Web {{services}} only expose interface level information, abstracting away implementation details. Testing is a {{time consuming}} and resource-intensive activity. Therefore, {{it is important to}} minimize the set of test cases executed without compromising quality. Since white-box testing techniques and traditional structural coverage criteria require access to code, we require a model-based approach for web service testing. Testing relies on oracles to provide expected outcomes for test cases and, if implemented manually, they depend on testers’ understanding of functional requirements to decide the correct response of the system on every given test case. As a result, they are costly in creation and maintenance and their quality depends on the correct interpretation of the requirements. Alternatively, if suitable specifications are available, oracles can be generated automatically at lower cost and with better quality. We propose to specify service operations as visual contracts with executable formal specifications as rules of a typed attributed graph transformation system. We associate operation signatures with these rules for providing test oracles. We analyze dependencies and conflicts between visual contracts to develop a dependency graph. We propose model-based coverage criteria, considering this dependency graph, to assess the completeness of test suites. We also propose a mechanism to find out which of the potential dependencies and the conflicts were exercised by a given test case. While <b>executing</b> <b>the</b> <b>tests,</b> the model is simulated and coverage is recorded as well as measured against the criteria. The criteria are formalized and the dynamic detection of conflicts and dependencies is developed. This requires keeping track of occurrences and overlaps of pre- and post-conditions, their enabling and disabling, in successive model states, and interpreting these in terms of the static dependency graph. Systems evolve over time and need retesting each time there is a change. In order to verify that the quality of the system is maintained, we use regression testing. Since regression test suites tend to be large, we isolate the affected part in the system only retesting affected parts by rerunning a selected subset of the total test suite. We analyze the test cases that were executed on both versions and propose a mechanism to transfer the coverage provided by these test cases. This information helps us to assess the completeness of the test suite on the new version without executing all of it...|$|E
40|$|TeleFOT is a Large Scale Collaborative Project {{under the}} Seventh Framework Programme, co-funded by the European Commission DG Information Society and Media within the {{strategic}} objective “ICT for Cooperative Systems”: [URL] overall objectives of TeleFOT are {{to assess the}} impacts of aftermarket and nomadic devices used in vehicles for driver support and to raise the awareness of the functions and potential that these devices offer. This report starts from the scientific and technological objectives that will make these overall objectives more concrete. They are 1. Build, mobilise and integrate European test communities for long term testing and assessment of driver support functions through aftermarket and nomadic devices 2. Create a methodological framework for <b>executing</b> <b>the</b> <b>tests</b> and analysing the data 3. Study aftermarket and nomadic devices in different technical contexts 4. Study different levels of impacts on drivers and society 5. Focus on functions and services for safe, efficient and economical travel 6. Investigate the contents of functions provided for cooperative driver support 7. Develop effective procedures of enhancing awareness and take-up of driver support ICT systems among the public 8. Focus also on aspects {{in the use of}} aftermarket and nomadic devices that may decrease safety TeleFOT is supposed adopt the approach of Field Operational Test (FOT). When going into the actual work in TeleFOT, as laid out in the DoW, it was seen as a useful step {{to make use of the}} structure of the deliverable D 2. 2. 1 Testing and Evaluation strategy I, based on the FESTA FOT Chain (from the FESTA Handbook). This was done in order to identify what findings in the TeleFOT project (so far) has a unique and from the FESTA Handbook deviating approach. The intention has been to highlight these deviations (or improvements) in order to widen the potential use of the FOT methodology in the future. This IP-level deliverable is therefore focused on what constitutes the unique features of TeleFOT that could have an impact also on a more general level. This is especially important as new FOTs are planned in the area “cooperative driving”. Most of the TeleFOT deliverables until today have been consulted including the first series of deliverables addressing the Data Analysis Plans for all the impact areas to be covered by TeleFOT; they are Efficiency, Environment, Mobility, Safety and User Uptake. It is concluded that the first of the overall objectives has been met to quite a high degree, even if there still are some important steps that must be finished. The second overall objective is not yet addressed in a systematic way. However, there are WPs in the TeleFOT DoW that are supposed to cover these aspects in the last part of the project...|$|E
30|$|We <b>execute</b> <b>the</b> <b>test</b> {{programs}} following <b>the</b> characterization approach. Then, we {{vary the}} scheduling {{policy and the}} frequency, we note the power and performance variations and we extract energy models.|$|R
50|$|Test Environment Management (TEM) is a {{function}} in the software delivery process which aids <b>the</b> software <b>testing</b> cycle by providing a validated, stable and usable <b>test</b> environment to <b>execute</b> <b>the</b> <b>test</b> scenarios or replicate bugs.|$|R
50|$|One reason teams avoid {{continuous}} {{testing is}} that their infrastructure is not scalable enough to continuously <b>execute</b> <b>the</b> <b>test</b> suite. This problem can be addressed by focusing <b>the</b> <b>tests</b> on <b>the</b> business's priorities, splitting <b>the</b> <b>test</b> base, and parallelizing <b>the</b> <b>testing</b> with application release automation tools.|$|R
40|$|We {{present a}} dynamic test {{prioritization}} technique {{with the objective}} to speed up uncovering updates to existing software and therefore, increase {{the rate at which}} faulty software can be debugged. Our technique utilizes two types of data [...] -the results of executing tests on prior version of the software; and the results of executing tests on the new version which determines the next test to be executed. The contributions of the thesis are two-fold: understanding what constitutes an effective ordering of tests and developing an algorithm that can and efficiently generate such order. At its cores, the proposed dynamic ordering technique relies on two basic conjectures. Firstly, tests that are closely related are likely to uncover similar updates/faults and tests that are not related are likely to widen the search for updates/faults. In other words, if a test uncovers updates in a software, i. e., its execution behavior (in terms coverage) differs considerably between prior and current version of the software, then selecting a test closely related to {{it is likely to be}} beneficial. Similarly, if a test does not uncover updates in a software, it would be good to select an unrelated test to execute next to increase the chances of better coverage. The relationship between tests are determined from the execution of tests while testing prior versions of the software. The second conjecture is that selecting tests in the above order will speed up uncovering bugs in the software. We develop a baseline ordering using complete knowledge about the results of executing tests in two different versions of the software. The baseline ordering arranges the tests in descending order in terms of amount of changes the tests uncover between the prior and new version of the software. We evaluate the effectiveness of this ordering (i. e., the validity of the conjectures) by computing the rate at which the order can identify (seeded) bugs in a software [...] the measurement is referred to as APFD. The baseline order produces high APFD values indicating that the order is indeed effective. However, note that the baseline ordering can be only obtained if the tests are already executed in two versions of the software; the challenge is to identify the ordering before <b>executing</b> <b>the</b> <b>tests</b> on the version being tested. We have developed an algorithm that estimates the baseline ordering. We evaluate the quality of the estimates using a rank relationship measure refer to as Order-Relationship Measure (ORM). We find that the ORM is high when call-sequences resulting from executing tests are used for estimation. We also find that low ORM implies low APFD values for the estimate. We have evaluated our algorithm on two non-trivial software repositories. We have investigated the role of two important parameters (thresholds capturing the closeness relationship between tests) in identifying high quality (high APFD) ordering and outlines how these parameters can be statically determined based on executing tests on the prior versions of the software. Finally, we have showed that the application of our algorithm in generating the test orders dynamically has close to 3 % overhead...|$|E
3000|$|In {{the example}} shown in Listing 1, when <b>executing</b> <b>the</b> <b>test</b> project, <b>the</b> newRule is called {{before and after}} the testMethod (...) [...]. The {{annotation}} @Rule is responsible for adding the desired behaviors before and after each test.|$|R
50|$|Interface {{engines are}} built on top of Interface Environment. Interface engine {{consists}} of a parser and a <b>test</b> runner. <b>The</b> parser is present to parse the object files coming from the object repository into <b>the</b> <b>test</b> specific scripting language. <b>The</b> <b>test</b> runner <b>executes</b> <b>the</b> <b>test</b> scripts using a test harness.|$|R
30|$|C 4 B {{was capable}} of {{generating}} C code for all B modules, so, all positive tests generated by BETA were <b>executed.</b> <b>The</b> <b>test</b> results revealed problems in the C code generated for the Timetracer module. This problem was reported to the C 4 B development team.|$|R
50|$|Regression testing {{can be used}} {{not only}} for <b>testing</b> <b>the</b> {{correctness}} of a program, but often also for tracking {{the quality of its}} output. For instance, in the design of a compiler, regression <b>testing</b> could track <b>the</b> code size, and {{the time it takes to}} compile and <b>execute</b> <b>the</b> <b>test</b> suite cases.|$|R
40|$|Performance {{evaluation}} for biometric {{systems can}} be classified into three types: technology, scenario and operational evaluation [1]. The technology evaluation is normally performed under <b>the</b> offline <b>testing</b> scenario and suitable for the algorithm evaluation. On the other hand, the scenario and operational evaluation <b>execute</b> <b>the</b> <b>test</b> under <b>the</b> real <b>testing</b> environments with pre-determined online testing scenario and real operational situations (o...|$|R
50|$|For running <b>the</b> OAT <b>test</b> cases, <b>the</b> tester {{normally}} has {{exclusive access}} to the system or environment. This means that a single tester would be <b>executing</b> <b>the</b> <b>test</b> cases at a single point of time. For OAT the exact Operational Readiness quality gates are defined: both entry and exit gates. The primary emphasis of OAT {{should be on the}} operational stability, portability and reliability of the system.|$|R
5000|$|Dry run <b>the</b> <b>tests</b> - before {{actually}} <b>executing</b> <b>the</b> load <b>test</b> with predefined users, a dry run {{is carried}} out in order to check the correctness of the script ...|$|R
5000|$|Total Ship Amphibibious - This {{section is}} {{responsible}} for planning and <b>executing</b> <b>the</b> operational <b>testing</b> and evaluation of amphibious warfare ships, including: ...|$|R
30|$|Thinking about <b>the</b> <b>testing</b> {{process as}} a whole, one {{important}} metric is <b>the</b> time to <b>execute</b> <b>the</b> <b>test</b> suite which eventually {{may be even more}} relevant than other metrics. Hence, we need to run multi-objective controlled experiments where we <b>execute</b> all <b>the</b> <b>test</b> suites (TTR 1.1 × TTR 1.2; TTR 1.2 × other solutions) probably assigning different weights to the metrics. We also need to investigate the parallelization of our algorithm so that it can perform even better when subjected to a more complex set of parameters, values, strengths. One possibility is to use the Compute Unified Device Architecture/Graphics Processing Unit (CUDA/GPU) platform (Ploskas and Samaras 2016). We must develop other multi-objective controlled experiment addressing effectiveness (ability to detect defects) of our solution compared with the other five greedy approaches.|$|R
40|$|International audienceThis paper {{presents}} a software-based approach for testing IEEE 1500 -compliant SoCs. In the proposed approach, <b>the</b> <b>test</b> program {{is no more}} <b>executed</b> by <b>the</b> external-traditional tester but by the SoC itself. The novel feature {{is the use of}} a dedicated test processor called T-Proc embedded onto <b>the</b> SoC to <b>test</b> <b>the</b> components. Under the control of the embedded SoC microprocessor, <b>the</b> <b>test</b> processor <b>executes</b> <b>the</b> <b>test</b> programs stored in the outside external memory, through a functional embedded external RAM controller interface. Using the ITC 02 SoC benchmarks a comparison is done between T-Proc and a classical bus-based test strategy...|$|R
40|$|This paper {{describes}} an environment for remote testing of IN-Services (programs for Intelligent Networks). This system is realised as a Client/Server Architecture. A programmer of IN-Services {{is connected to}} a Test Center via Internet. A Graphical User Interface (GUI) is running on a Windows PC. An FTP connection to a local UNIX machine with program source files is established to download these files. A Test Requirement can be created and then {{be sent to the}} Test Center. After <b>executing</b> <b>the</b> <b>test</b> <b>the</b> results are stored and sent back to the Client. The final part describes a generalisation of this Remote-Testing system to a Unified Architecture for Tele-Experimenting. Key word...|$|R
40|$|A {{language}} is presented for describing tests of integrated circuits. The language {{has a high}} abstractive capability that enables test specifications to follow the structural or logical organization of a design. <b>The</b> <b>test</b> {{language is}} applied {{to a number of}} current design styles in a series of examples. Methods for designing integrated circuits for testability are demonstrated. An implementation of <b>the</b> <b>test</b> language through a test language interpreter and a tester is discussed. Tester designs are presented that will <b>execute</b> <b>the</b> <b>test</b> language with unusually high efficiency...|$|R
5000|$|Point Defense Systems - This {{section is}} {{responsible}} for planning and <b>executing</b> <b>the</b> operational <b>testing</b> and evaluation of U. S. Navy short-range air and surface defense systems, including: ...|$|R
30|$|Different Handover (HO) types may be <b>executed</b> in <b>the</b> <b>testbed</b> as <b>the</b> MN {{moves along}} the scenario: Horizontal Handover (HHO), Intra-IR Vertical Handover (VHO), and Inter-IR VHO.|$|R
50|$|FitNesse {{testing is}} based around the {{notation}} of black-box testing, {{in which a}} system under test {{is considered to be}} a black box and is tested in terms of the outputs generated in response to predetermined inputs. A functional tester is responsible for designing <b>the</b> <b>tests</b> in a functional sense and expressing them within the FitNesse tool, whereas the software developer is responsible for connecting the FitNesse tool to <b>the</b> system under <b>test</b> so that FitNesse can <b>execute</b> <b>the</b> <b>test</b> and compare <b>the</b> actual output to the expected output.|$|R
3000|$|... 0 {{with respect}} to the already known basic blocks <b>executed</b> by <b>the</b> <b>test</b> cases within I'. Intuitively, E(x_ 0, I') gives us a quality measure for input x [...]...|$|R
40|$|The unit RTH, Radio Access Network Transmission and Home at Ericsson site in Linkoping uses today {{different}} test tool {{which are}} divided between different projects. Today {{they do not}} have any optimal solution for in an easy way <b>execute</b> <b>the</b> <b>test</b> cases from different projects concurrently, and share <b>the</b> <b>test</b> tools between <b>the</b> these projects. All <b>the</b> execution of <b>test</b> cases which a test tool is needed needs to be configured and started manually which cost both time and money. Since <b>the</b> <b>test</b> tools are very expensive to use, it is desirable to increase the utilization. The purpose of this thesis is to provide RTH a working prototype which can in an intelligent way schedule and then automatically <b>execute</b> <b>the</b> <b>test</b> cases. <b>The</b> prototype shall consist of a web user interface and a scheduler part. The web user interface is going to be that part which the user works in, and the scheduler handles the prioritization and make sure that <b>the</b> <b>test</b> cases are <b>executed.</b> To reach <b>the</b> goal with a working prototype, PHP and Java were picked as framework for the prototype. The theory behind these programming languages and more can be read in the theory chapter, and all the different methods which were used. The result of the working process can be read in the Result chapter. The end prototype fulfills the customer’s requirement. Improvement and new functionalities are given as suggestion {{at the end of this}} thesis, where we also discuss the working process...|$|R
50|$|Concordion {{specifications}} {{are written}} in Markdown, HTML or Excel and then instrumented with special links, attributes or comments respectively. When <b>the</b> corresponding <b>test</b> fixture class is run, Concordion interprets <b>the</b> instrumentation to <b>execute</b> <b>the</b> <b>test.</b> Rather than forcing product owners to specify requirements in a specially structured language, Concordion lets you write them in normal language using paragraphs, tables and proper punctuation. This makes the specifications much more natural to read and write, and helps everyone to understand and agree about what a feature is supposed to do.|$|R
30|$|A vital {{source of}} threats is the {{difficulty}} of setting up the libraries used in the experiments. To add a new library to Optimizer, we first needed to measure its test suite statement coverage. Measuring coverage of test suites designed for JavaScript programs is not a standardized procedure, that is, we lack a well-known “recipe” to follow. Once <b>the</b> <b>test</b> suite statement coverage is identified (>[*] 90 %), {{it is necessary to}} locally setup the library environment and its dependencies to allow the Optimizer to make changes in the source code and <b>execute</b> <b>the</b> <b>test</b> suites.|$|R
50|$|When <b>test</b> {{cases are}} <b>executed,</b> <b>the</b> <b>test</b> leader and <b>the</b> project manager must know, where exactly the project stands {{in terms of}} testing activities. To know where the project stands, the inputs from the {{individual}} testers must come to <b>the</b> <b>test</b> leader. This will include, what test cases are executed, how long it took, how many test cases passed, how many failed, and how many are not executable. Also, how often the project collects the status is to be clearly stated. Some projects will have a practice of collecting the status {{on a daily basis}} or weekly basis.|$|R
40|$|Regression {{testing is}} a {{significant}} but a very expensive testing process. Test case prioritization is a technique to schedule and <b>execute</b> <b>the</b> <b>test</b> cases in such an order that results in increasing their ability to meet some performance goal. One of the main goal {{is to increase the}} rate of fault detection –i. e. to detect the faults as early as possible during <b>the</b> <b>testing</b> process. Test case prioritization is used to minimize the expenses of regression testing. This paper proposes a technique to select and prioritize <b>the</b> <b>test</b> cases and results in improving the rate of fault detection. Indexing terms/Keywords Regression Testing, Test case prioritization, Prioritization techniques. ...|$|R
30|$|Once test {{input data}} is obtained, the {{original}} model is animated using these inputs to generate oracle data (expected <b>test</b> case results). <b>The</b> approach currently supports four strategies for oracle verifications: exception checking (<b>executes</b> <b>the</b> <b>test</b> and verifies if any exception is raised), invariant checking (verifies if the invariant is preserved), state variables checking (verifies if the {{values for the}} state variables are the ones expected), and return variables checking (verifies if the values returned by the operation are the ones expected). These strategies can be combined to make weaker or stronger verifications.|$|R
30|$|In {{order to}} {{automate}} tests of randomized logic, we have proposed an approach based on three software patterns: Deterministic Characteristic Assertion - {{to create an}} assertion that verifies {{the validity of the}} algorithm result; Re-test With Different Seeds - to <b>execute</b> <b>the</b> <b>test</b> several times with different seeds; and Recycle Failed Seeds - to persist seeds used in failed tests to be reused in future test executions. Next, we present an overview of each pattern. And, after the explanation of the patterns, we present the application of the proposed approach through an illustrative example.|$|R
