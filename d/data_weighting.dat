132|2517|Public
40|$|Modeling the Earth’s gravity eld from {{observed}} satellite accelerations is discussed. The {{emphasis is}} on deriving satellite accelerations from precise orbit data {{as well as on}} the optimal <b>data</b> <b>weighting</b> (including the case of time-dependent noise in orbit data). Numerical examples illustrate the importance of the proper <b>data</b> <b>weighting.</b> It is shown, in particular, that low-frequency noise in orbit data can be efciently suppressed this way (contrary to the traditional approach based on the integration of variational equations). ...|$|E
30|$|For the {{observatory}} <b>data</b> <b>weighting,</b> {{we use a}} simpler scheme based on a scaling of the instrument accuracy and zenith angle weighting. The application of LAVA and along-track SD {{has the effect of}} significantly down-weighting the satellite data. To prevent {{the observatory}} data dominating the model post LAVA/SD, we apply a scaling to the observatory weights. This scaling is chosen such that the mean satellite-observatory <b>data</b> <b>weighting</b> post LAVA/SD is the same as that found before LAVA and along-track SD are included.|$|E
40|$|In sample surveys {{weighting}} {{is applied}} to data to increase the quality of estimates. <b>Data</b> <b>weighting</b> {{can be used for}} several purposes. Sample design weights can be used to adjust the differences in selection probabilities for non-self weighting sample designs. Sample design weights, adjusted for nonresponse and non-coverage through the sequential <b>data</b> <b>weighting</b> process. The unequal selection probability designs represented the complex sampling designs. Among many reasons of weighting, the most important reasons are weighting for unequal probability of selection, compensation for nonresponse, and post-stratification. Many highly efficient estimation methods in survey sampling require strong information about auxiliary variables, x. The most common estimation methods using auxiliary information in estimation stage are regression and ratio estimator. This paper proposes a sequential <b>data</b> <b>weighting</b> procedure for the estimators of combined ratio mean in complex sample surveys and general variance estimation for the population ratio mean. To illustrate the utility of the proposed estimator, Turkish Demographic and Health Survey 2003 real life data is used. It is shown that the use of auxiliary information on weights can considerably improve the efficiency of the estimates...|$|E
5000|$|... where N is {{the number}} of samples, [...] are random {{variables}} as samples of the stochastic [...] (noisy signal), and the first degree polynomial <b>data</b> <b>weights</b> are ...|$|R
5000|$|... {{where the}} hat (^) denotes an estimate, N {{is the number}} of samples in the data window, [...] is the time of the desired estimate, and the <b>data</b> <b>weights</b> are ...|$|R
40|$|Discovering {{interesting}} rules {{from financial}} data In this paper problem of mining <b>data</b> with <b>weights</b> and finding association rules is presented. Some applications are discussed, especially focused on financial data. Solutions {{of the problem}} are analyzed. A few approaches are proposed and compared. Pruning based on measures of rules interestingness is described and some measures proposed in literature are shown. Influence of <b>data</b> <b>weights</b> on these measures is also discussed. 1...|$|R
30|$|All {{statistical}} {{analyses were performed}} using GraphPad Prism software version 5.04 (GraphPad Software Inc., USA). Standard curves were generated using a 4 -parameter logistic equation (sigmoidal dose–response curve with variable slope) and a 1 /Y 2 <b>data</b> <b>weighting</b> [27].|$|E
40|$|Microbial {{fuel cell}} {{is a kind}} of {{promising}} new source of green energy. Because of its complicated reaction mechanism and its inherent characteristics of time-varying, uncertainty, strong-coupling and nonlinearity, there are complex control challenges in modelling and control of microbial fuel cells. This paper studies on performance improvement of microbial fuel cells by the approach of model predictive control. A numerical simulation platform for microbial fuel cell is established, and a traditional model predictive controller is designed for MFC first; then model predictive controllers which use Laguerre function and exponential <b>data</b> <b>weighting</b> are designed subsequently to compare with the traditional model predictive controller. Simulation results show that the proposed improved model predictive controller modified by exponential <b>data</b> <b>weighting</b> can give the system both good steady-state behavior and satisfactory dynamic property...|$|E
40|$|In this correspondence, a nonlinearly {{weighted}} {{least-squares method}} is developed for robust modeling of sensor array <b>data.</b> <b>Weighting</b> functions for various observation noise scenarios are determined using maximum likelihood estimation theory. Computational {{complexity of the}} new method is comparable with the standard least-squares estimation procedures. Simulation examples of direction-of-arrival estimation are presented. © 1998 IEEE...|$|E
30|$|In this section, we derive the inhomogeneously {{screened}} Poisson equation {{from the}} energy function in (3), which includes spatially varying <b>data</b> <b>weight.</b> Then we present an iterative solver for the equation and analyze its convergence rate.|$|R
3000|$|Nash {{equilibrium}} is {{a strategy}} profile where every <b>data</b> <b>weight</b> is underlined. This suggests an alternative definition for Nash equilibrium involving best-response functions. The best-response function for data item i[*]∈[*]N is set-weight valued function B [...]...|$|R
30|$|We {{determine}} hypocenters {{using the}} arrival times of P-wave and S-wave at seismic stations in Japan. Iterative method (Hamada et al., 1983) {{is used to}} calculate hypocenters by taking into consideration the <b>data</b> <b>weight</b> related to the hypocentral distance (Ueno et al., 2002).|$|R
40|$|Note: {{before using}} this routine, please read the Users ’ Note for your {{implementation}} {{to check the}} interpretation of bold italicised terms and other implementation-dependent details. 1 Purpose G 01 AAF calculates the mean, standard deviation, coefficients of skewness and kurtosis, and the maximum and minimum values {{for a set of}} ungrouped <b>data.</b> <b>Weighting</b> may be used...|$|E
40|$|Cataloged from PDF {{version of}} article. In this correspondence, a nonlinearly {{weighted}} least-squares method is developed for robust modeling of sensor array <b>data.</b> <b>Weighting</b> functions for various observation noise scenarios are determined using maximum likelihood estimation theory. Computational {{complexity of the}} new method is comparable with the standard least-squares estimation procedures. Simulation examples of direction-of-arrival estimation are presented...|$|E
40|$|Progress in the {{following}} areas is described: refining altimeter and altimeter crossover measurement models for precise orbit determination and for {{the solution of the}} earth's gravity field; performing experiments using altimeter data for the improvement of precise satellite ephemerides; and analyzing an optimal relative <b>data</b> <b>weighting</b> algorithm to combine various data types in the solution of the gravity field...|$|E
40|$|Linear {{estimation}} theory, {{along with}} a new technique to compute relative <b>data</b> <b>weights,</b> {{was applied to the}} determination of the Earth's geopotential field and other geophysical model parameters using a combination of satellite ground-based tracking data, satellite altimetry data, and the surface gravimetry data. The relative <b>data</b> <b>weights</b> for the inhomogeneous data sets are estimated simultaneously with the gravity field and other geophysical and orbit parameters in a least squares approach to produce the University of Texas gravity field models. New techniques to perform calibration of the formal covariance matrix for the geopotential solution were developed to obtain a reliable gravity field error estimate. Different techniques, which include orbit residual analysis, surface gravity anomaly residual analysis, subset gravity solution comparisons and consider covariance analysis, were applied to investigate the reliability of the calibration...|$|R
40|$|We {{investigate}} {{here the}} behavior of the standard k-means clustering algorithm and several alternatives to it: the k- harmonic means algorithm due to Zhang and colleagues, fuzzy k-means, Gaussian expectation-maximization, and two new variants of k-harmonic means. Our aim is to nd which aspects of these algorithms contribute to nding good clusterings, as opposed to converging to a low-quality local optimum. We describe each algorithm in a uni ed framework that introduces separate cluster membership and <b>data</b> <b>weight</b> functions. We then show that the algorithms do behave very dierently from each other on simple low-dimensional synthetic datasets and image segmentation tasks, and that the k-harmonic means method is superior. Having a soft membership function is essential for nding high-quality clusterings, but having a non-constant <b>data</b> <b>weight</b> function is useful also...|$|R
40|$|International audienceContinuous <b>weight</b> and {{temperature}} <b>data</b> were collected for honey bee hives in two locations in Arizona, and those data were evaluated {{with respect to}} separate measurements of hive phenology to develop methods for noninvasive hive monitoring. <b>Weight</b> {{and temperature}} <b>data</b> were divided into the 25 -h running average and the daily within-day changes, or “detrended” data. Data on adult bee and brood masses from hive evaluations were regressed on the amplitudes of sine curves fit to the detrended <b>data.</b> <b>Weight</b> <b>data</b> amplitudes were significantly correlated with adult bee populations during nectar flows, and temperature amplitudes were found inversely correlated with the log of colony brood weight. The relationships were validated using independent datasets. In addition, the effects of an adult bee kill on hive <b>weight</b> <b>data</b> were contrasted with published <b>data</b> on <b>weight</b> changes during swarming. Continuous data {{were found to be}} rich sources of information about colony health and activity...|$|R
40|$|We {{present an}} {{improved}} mascon approach to transform monthly spherical harmonic solutions based on GRACE satellite data into mass anomaly estimates in Greenland. The GRACE-based spherical harmonic coefficients {{are used to}} synthesize gravity anomalies at satellite altitude, which are then inverted into mass anomalies per mascon. The limited spectral content of the gravity anomalies is properly accounted for by applying a low-pass filter {{as part of the}} inversion procedure to make the functional model spectrally consistent with the data. The full error covariance matrices of the monthly GRACE solutions are properly propagated using the law of covariance propagation. Using numerical experiments, we demonstrate the importance of a proper <b>data</b> <b>weighting</b> and of the spectral consistency between functional model and data. The developed methodology is applied to process real GRACE level- 2 data (CSR RL 05). The obtained mass anomaly estimates are integrated over five drainage systems, as well as over entire Greenland. We find that the statistically optimal <b>data</b> <b>weighting</b> reduces random noise by 35 – 69 %, depending on the drainage system. The obtained mass anomaly time-series are de-trended to eliminate the contribution of ice discharge and are compared with de-trended surface mass balance (SMB) time-series computed with the Regional Atmospheric Climate Model (RACMO 2. 3). We show that when using a statistically optimal <b>data</b> <b>weighting</b> in GRACE data processing, the discrepancies between GRACE-based estimates of SMB and modelled SMB are reduced by 24 – 47 %. </p...|$|E
40|$|Abstract — The paper {{presents}} a complete system for building an improved picture with greater {{high dynamic range}} by using different pictures of the same scene acquired under different exposure settings. The image data fusion is achieved by merging the original <b>data</b> <b>weighting</b> each single contribute on pixel basis by suitable data function. Experiments confirm the effectiveness of such approach...|$|E
40|$|A {{systolic}} algorithm/array {{is described}} for recursive least squares (RLS) estimation, which achieves an O(n(0)) throughput rate with O(n(2)) parallelism. The array is also useful for several other applications, such as, e. g., SVD updating and Kalman filtering. An additional {{advantage is that}} unlike with other RLS-arrays, {{it is now possible}} to incorporate alternative <b>data</b> <b>weighting</b> strategies, such as directional weighting, without sacrificing speed. status: publishe...|$|E
40|$|The {{problem of}} {{nonlinear}} weighted least squares fitting of the three-parameter Weibull distribution to the given data (wi,ti,yi), i= 1, [...] .,n, is considered. The part wi> 0 {{of the data}} stands for the <b>data</b> <b>weights.</b> It is shown that the best least squares estimate exists provided that the data satisfy just the following two natural conditions: (i) 0...|$|R
40|$|Depends lattice, latticeExtra, clv Description Entropy {{weighted}} kmeans (ewkm) is {{a weighted}} subspace clustering algorithm {{that is well}} suited to very high dimensional <b>data.</b> <b>Weights</b> are calculated as {{the importance of a}} variable with regard to cluster membership. The feature group weighted kmenas (fgkm) extends this concept by grouping features and weighting the group in addition to weihgting individual features...|$|R
40|$|Between 1975 and 1988, {{demographic}} <b>data,</b> <b>weight</b> {{and height}} measurements, {{and blood pressure}} readings were obtained for 77 890 residents of Saskatchewan (about 7. 6 % of the population). High readings were present in 7. 8 % of those surveyed, but prevalence fell over the lifetime of the survey. Subjects whose drug therapy was modified had a larger fall in blood pressure than those whose medical regimen was unchanged...|$|R
30|$|To Estimate the {{coefficients}} describing the magneto-spheric field, we {{make use of}} a spherical harmonic expansion by solving simultaneously for coefficients in the GSM and SM frames. Since satellites sample the sphere more densely near the poles than at the equator, an equal-area <b>data</b> <b>weighting</b> is performed prior to the least-squares inversion. Subsequently we {{make use of the}} SHA property to derive {{the coefficients}} of an external poloidal field.|$|E
30|$|In Section 2 we {{describe}} the data used, their selection criteria, and briefly outline the <b>data</b> <b>weighting</b> scheme. In Section 3 {{we describe}} the parent model and the process used to fit its parameters, while in Section 4 we evaluate the model coefficients. In Section 5 we extrapolate forwards in time and extract the IGRF- 11 MF and SV candidate models. We provide concluding remarks in Section 6.|$|E
40|$|We propose sliding-window multiedge {{detectors}} and reflectivity estimators for complex SAR images. The novel {{detectors and}} estimators allow {{to take into}} account additive observation noise and colored signal (speckle) and noise processes; furthermore, they employ an exponential <b>data</b> <b>weighting</b> to improve spatial resolution. In the multiedge case, simulation results demonstrate a substantial performance improvement over existing methods when the speckle is colored and additive noise is present. 1...|$|E
40|$|A new {{technique}} {{has been developed}} for the <b>weighting</b> of <b>data</b> from satellite tracking systems {{in order to obtain}} an optimum least squares solution and an error calibration for the solution parameters. Data sets from optical, electronic, and laser systems on 17 satellites in GEM-TI (Goddard Earth Model, 36 x 36 spherical harmonic field) have been employed toward application of this technique for gravity field parameters. Also GEM-T 2 (31 satellites) was recently computed as a direct application of the method and is summarized here. The method employs subset solutions of the data associated with the complete solution and uses an algorithm to adjust the <b>data</b> <b>weights</b> by requiring the differences of parameters between solutions to agree with their error estimates. With the adjusted weights the process provides for an automatic calibration of the error estimates for the solution parameters. The <b>data</b> <b>weights</b> derived are generally much smaller than corresponding weights obtained from nominal values of observation accuracy or residuals. Independent tests show significant improvement for solutions with optimal weighting as compared to the nominal weighting. The technique is general and may be applied to orbit parameters, station coordinates, or other parameters than the gravity model...|$|R
40|$|Technical {{models and}} {{analytical}} approaches {{used to develop}} the <b>weight</b> <b>data</b> for vehicle system concepts using advanced technology are reported. <b>Weight</b> <b>data</b> are supplied for the following major system elements: engine, pressurization, propellant containers, structural shells and secondary structure, and environmental protection shields for the meteoroid and thermal design requirements. Scaling laws, improved and a simplified set, are developed from the system <b>weight</b> <b>data.</b> The laws consider {{the implications of the}} major design parameters and mission requirements on the stage inert mass...|$|R
3000|$|To {{obtain the}} {{adsorption}} isotherm, {{the degree of}} surface coverage (θ) for various concentrations of the P(2 ABT) is calculated according to (θ = %IE/ 100). Because, the <b>data</b> of <b>weight</b> loss and polarization are close to each other, we use the <b>data</b> obtained from <b>weight</b> loss to study the adsorption of P(2 ABT) on MS electrode in hydrochloric acid solution, different isotherms are studied as follows: [...]...|$|R
40|$|Given {{the adverse}} impact of image noise on the {{perception}} of important clinical details in digital mammography, routine quality control measurements should include an evaluation of noise. The European Guidelines, for example, employ a second-order polynomial fit of pixel variance {{as a function of}} detector air kerma (DAK) to decompose noise into quantum, electronic and fixed pattern (FP) components and assess the DAK range where quantum noise dominates. This work examines the robustness of the polynomial method against an explicit noise decomposition method. The two methods were applied to variance and noise power spectrum (NPS) data from six digital mammography units. Twenty homogeneously exposed images were acquired with PMMA blocks for target DAKs ranging from 6. 25 to 1600 [*]µGy. Both methods were explored for the effects of <b>data</b> <b>weighting</b> and squared fit coefficients during the curve fitting, the influence of the additional filter material (2 [*]mm Al versus 40 [*]mm PMMA) and noise de-trending. Finally, spatial stationarity of noise was assessed. <b>Data</b> <b>weighting</b> improved noise model fitting over large DAK ranges, especially at low detector exposures. The polynomial and explicit decompositions generally agreed for quantum and electronic noise but FP noise fraction was consistently underestimated by the polynomial method. Noise decomposition as a function of position in the image showed limited noise stationarity, especially for FP noise; thus the position of the region of interest (ROI) used for noise decomposition may influence fractional noise composition. The ROI area and position used in the Guidelines offer an acceptable estimation of noise components. While there are limitations to the polynomial model, when used with care and with appropriate <b>data</b> <b>weighting,</b> the method offers a simple and robust means of examining the detector noise components as a function of detector exposure. status: publishe...|$|E
40|$|A <b>data</b> <b>weighting</b> {{method for}} {{combining}} different geodetic data based on statistical {{tests of the}} residuals is discussed. By requiring the residuals of combination solution to be white Gaussian as a sample of random noise, the relative weight factors can be determined internally, independent of external comparisons. To achieve this goal and to assess the solution, the Pearson chi(2) and higher-order moments of residuals are used to measure quantitatively the discrepancies between the residuals and white Gaussian noise...|$|E
30|$|After {{successful}} data {{selection and}} import, the respective data analysis configuration {{needs to take}} place. This includes the weighting of input data for automated data analysis and identification of attributes relevant for key indicators, as well as settings regarding the system’s policy recommendation behavior. Regarding the input <b>data</b> <b>weighting,</b> human IAM engineers could e.g. decide to give more weight to data values that are constantly updated, maintained and revised and thus have a high accuracy during the consecutive algorithmic analysis.|$|E
3000|$|... [...]. The {{parameter}} δ in (6), which {{determines the}} degree of the overlap between the <b>data</b> reliability <b>weights,</b> was set to 0.25. The parameters C [...]...|$|R
3000|$|... is {{assigned}} to a class ci {{in order to maximize}} P([...] c_i| d_j^ * [...].). The logarithm of probabilities are summed up to classify an opinion document. It is preferred over product of probabilities to avoid underflow. It addresses the missing value problem as well. Slack variables add smoothing effect against noisy <b>data.</b> <b>Weights</b> can also be assigned to features which define their contribution towards the classification. It is a biased approach, where prominent features are given high weights {{to play a major role}} in choose a sentiment label.|$|R
40|$|Description Entropy {{weighted}} k-means (ewkm) is {{a weighted}} subspace clustering algorithm {{that is well}} suited to very high dimensional <b>data.</b> <b>Weights</b> are calculated as {{the importance of a}} variable with regard to cluster membership. The two-level variable weighting clustering algorithm TW-k-means (twkm) introduces two types of weights, the weights on individual variables and the weights on variable groups, and they are calculated during the clustering process. The feature group weighted k-means (fgkm) extends this concept by grouping features and weighting the group in addition to weighting individual features...|$|R
