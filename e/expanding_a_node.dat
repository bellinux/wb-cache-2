4|10000|Public
40|$|A * {{is often}} {{described}} as being ‘optimal’, in that it expands the minimum number of unique nodes. But, A * may generate many extra nodes which are never expanded. This is a performance loss, especially when the branching factor is large. Partial Expansion A * (PEA*) (Yoshizumi, Miura, and Ishida 2000) addresses this problem when <b>expanding</b> <b>a</b> <b>node,</b> n, by generating all the children of n but only storing children with the same f-cost as n. n is re-inserted into the OPEN list, but with the f-cost of the next best child. This paper introduces an enhanced version of PEA * (EPEA*). Given a priori domain knowledge, EPEA * generates only the children with the same f-cost as the parent. EPEA* is generalized to its iterative-deepening variant, EPE-IDA*. For some domains, these algorithms yield substantial performance improvements. State-of-the-art results were obtained for the pancake puzzle and for some multi-agent pathfinding instances. Drawbacks of EPEA * are also discussed...|$|E
40|$|Heuristic or {{informed}} {{search is}} very useful {{in quite a}} lot of applications. There are several algorithms performing heuristic search such as A ∗ (Hart, Nilsson and Raphael (1968)) IDA ∗ (Korf (1985)) and several memory restricted algorithms such as DBIDA ∗ and MEIDA ∗ (Eckerle and Schurier (1995)). Evaluating search algorithms used for heuristic state space problems the (n 2 − 1) –puzzle is often used as the only benchmark. This problem has two major advantages. It has a fairly easy optimistic heuristic (lower bound func-tion) and leads to a huge search space of (n 2 − 1) !/ 2 possible configurations. But on the other hand we find a lot of side effects that are not admirable. For example the shortest cycle length bigger than two is twelve such that using the good Manhattan estimate the danger of a re–expansion of the same node is small. There are no dead ends, such that even human solvers find good approximate solutions. The search time is measured in the number of expanded nodes but the CPU time for <b>expanding</b> <b>a</b> <b>node</b> is small compared to other data structure operations (only two positions need to be swapped and there is n...|$|E
40|$|When solving {{instances}} of problem domains that feature a large branching factor, A * may generate {{a large number}} of nodes whose cost is greater than the cost of the optimal solution. We designate such nodes as surplus. Generating surplus nodes and adding them to the OPEN list may dominate both time and memory of the search. A recently introduced variant of A * called Partial Expansion A * (PEA*) deals with the memory aspect of this problem. When <b>expanding</b> <b>a</b> <b>node</b> n, PEA * generates all of its children and puts into OPEN only the children with f = f(n). n is re-inserted in the OPEN list with the f-cost of the best discarded child. This guarantees that surplus nodes are not inserted into OPEN. In this paper, we present a novel variant of A * called Enhanced Partial Expansion A * (EPEA*) that advances the idea of PEA * to address the time aspect. Given a priori domain- and heuristic-specific knowledge, EPEA * generates only the nodes with f = f(n). Although EPEA * is not always applicable or practical, we study several variants of EPEA*, which make it applicable to {{a large number of}} domains and heuristics. In particular, the ideas of EPEA * are applicable to IDA * and to the domains where pattern databases are traditionally used. Experimental studies show significant improvements in run-time and memory performance for several standard bench-mark applications. We provide several theoretical studies to facilitate an understanding of the new algorithm...|$|E
40|$|We {{present an}} {{algorithm}} for on-line, incremental discovery of temporal-difference (TD) networks. The key contribution is {{the establishment of}} three criteria to <b>expand</b> <b>a</b> <b>node</b> in TD network: <b>a</b> <b>node</b> is <b>expanded</b> when the node is well-known, independent, and has a prediction error that requires further explanation. Since none of these criteria requires centralized calculation operations, they are easily computed in a parallel and distributed manner, and scalable for bigger problems compared to other discovery methods of predictive state representations. Through computer experiments, we demonstrate the empirical effectiveness of our algorithm. 1...|$|R
50|$|At line 1, the if {{condition}} checks if curr contains any {{formula to}} be expanded.If the curr is empty then at line 2 the if condition checks if there already exists a state q' with {{same set of}} expanded formulas.If that is the case, then we do not add <b>a</b> redundant <b>node,</b> but we add parameter incoming in Incoming(q') at line 3.Otherwise, we add <b>a</b> new <b>node</b> q using the parameters at lines 5-9 and we start <b>expanding</b> <b>a</b> successor <b>node</b> of q using next(q) as its unexpanded set of formulas at line 10.|$|R
40|$|In this paper, {{we present}} an {{attribute}} graph grammar for image parsing on scenes with man-made objects, such as buildings, hallways, kitchens, and living rooms. We choose one class of primitives – 3 D planar rectangles projected on images, and six graph grammar production rules. Each production rule not only <b>expands</b> <b>a</b> <b>node</b> into its components but {{also includes a}} number of equations that constrain the attributes of <b>a</b> parent <b>node</b> and those of its children. Thus our graph grammar is context sensitive. The grammar rules are used recursively to produce {{a large number of}} objects and patterns in images and thus is a type of generative model. The inference algorithm integrates bottomup rectangle detection which activates top-down prediction using the grammar rules. The final results are validated in a Bayesian framework. The output of the inference is a hierarchical parsing graph with objects, surfaces, rectangles, and their spatial relations. In the inference, the acceptance of a grammar rule means a recognition of an object, and actions are taken to pass the attributes between <b>a</b> <b>node</b> and its parent through the constraint equations associated with this production rule. When an attribute is passed from <b>a</b> child <b>node</b> to <b>a</b> parent <b>node,</b> it is called bottom-up, and the opposite is called top-down. 1...|$|R
40|$|This paper reports several {{properties}} of heuristic best-first search strategies whose scoring functions f depend {{on all the}} information available from each candidate path, not merely on the current cost g and the estimated completion cost h. It is shown that several known {{properties of}} A * retain their form (with the minmax offplaying {{the role of the}} optimal cost), which helps establish general tests of admissibility and general conditions for node expansion for these strategies. On the basis of this framework the computational optimality of A*, in the sense of never <b>expanding</b> <b>a</b> <b>node</b> that can be skipped by some other algorithm having access to the same heuristic information that A* uses, is examined. A hierarchy of four optimality types is defined and three classes of algorithms and four domains of problem instances are considered. Computational performances relative to these algorithms and domains are appraised. For each class-domain combination, we then identify the strongest type of optimality that exists and the algorithm for achieving it. The main results of this paper relate to the class of algorithms that, like A*, return optimal solutions (i. e., admissible) when all cost estimates are optimistic (i. e., h 5 h*). On this class, A * is shown to be not optimal and it is also shown that no optimal algorithm exists, but if the performance tests are confirmed to cases in which the estimates are also consistent, then A * is indeed optimal. Additionally, A * is also shown to be optimal over a subset of the latter class containing all best-first algorithms that are guided by path-dependent evaluation functions...|$|E
40|$|MREC {{is a new}} {{recursive}} {{best-first search}} algorithm which combines the good features of A * and IDA*. It Is closer in operation to IDA*, and does not use an OPEN list. In order to execute, all MREC needs is sufficient memory for its implicit stack. But {{it can also be}} fed at runtime a parameter M which tells it how much additional memory is available for use. In this extra memory, MREC stores as much as possible of the explicit graph. When M = 0, MREC is identical to IDA*. But when M> 0, it can make far fewer node expansions than IDA*. This can be advantageous for problems where the time to <b>expand</b> <b>a</b> <b>node</b> is significant. Extensive runs on a variety of search problems, involving search graphs {{that may or may not}} be trees, indicate that MREC with M = 0 is as good as IDA * on problems such as the 15 puzzle for which IDA * is suitable, while MREC with large M is as fast as A * on problems for which node expansion time is not negligible. ...|$|R
40|$|The {{most basic}} {{activity}} performed by an intelligent agent is {{deciding what to}} do next. Usually, the decision {{takes the form of}} selecting, from among many applicable methods, one method to try first, or opting to <b>expand</b> <b>a</b> particular <b>node</b> in <b>a</b> simple search. The most primitive case is selecting between two independent alternatives. Below, this case is examined and the value of the control knowledge that makes the decision is determined. Another result derived is the sensitivity of the expected value of control knowledge {{as a function of the}} accuracy of the parameters used to make these control decisions...|$|R
40|$|A * with {{lookahead}} (AL*) is {{a variant}} of A * that performs a cost-bounded DFS lookahead from <b>a</b> <b>node</b> when it is gener-ated. We show that the original version of AL * (AL∗ 0) can, in some circumstances, fail to return an optimal solution be-cause of the move pruning it does. We present two new ver-sions, AL∗ 1 and ELH, that we prove to always be correct and give conditions in which AL∗ 0 is guaranteed to be cor-rect. In our experiments with unit costs, AL∗ 0 was usually the fastest AL * version, but its advantage was usually small. In our experiments with non-unit costs, AL∗ 0 substantially out-performs both A * and IDA*. We also evaluate the idea of immediately <b>expanding</b> <b>a</b> generated <b>node</b> if {{it has the same}} f-value as its parent. We find that doing so causes AL * to require more memory and sometimes slows AL * down...|$|R
30|$|Figure 4 shows {{part of the}} h-hop keyterm graph for {{the cluster}} {{in the middle of}} Fig.  3, where ellipses are anchor nodes and {{rectangles}} are <b>expanded</b> <b>nodes.</b> <b>As</b> the dotted bordered rectangle shows, although “History of the United States Republican Party” connects with “republicans” and “george bush”, it is less associated with other anchor nodes. Hence, it is removed in the expansion process.|$|R
30|$|An <b>expanding</b> front passes <b>a</b> <b>node</b> {{only once}} and reaches nodes further away at later times. Because of this causal dependency, {{the parts of}} the {{solution}} holding larger values depend on nodes with smaller values. Numerical errors introduced close to the origin will propagate to the whole domain. It is therefore important that the computed solution is particularly accurate close to the initial condition. To ensure that all initiated subdomains fully converge, they are computed with one sweep of 3 D PMM {{in the beginning of the}} algorithms.|$|R
40|$|This paper studies {{a simple}} {{attribute}} graph grammar as a generative image representation for image parsing in a Bayesian framework. This grammar has one class of primitives as its terminal nodes - 3 D planar rectangles projected on images, and six production {{rules for the}} spatial layout of the rectangular objects. All the terminal and non-terminal nodes in the grammar are described by attributes for their geometric properties and image appearance. Each production rule either instantiates <b>an</b> non-terminal <b>node</b> as <b>a</b> rectangular primitive or <b>expands</b> <b>a</b> <b>node</b> into its components. A production rule {{is associated with a}} number of equations that constrain the attributes of <b>a</b> parent <b>node</b> and those of its children. This grammar, albeit simple, can produce a combinatorial number of configurations and objects in made-made scenes, such as building, hallways, kitchens, and living rooms etc. A configuration refers to a planar graphic representation produced by a series of grammar rules in a parsing graph (augmented from a parsing tree by including horizontal constraints). A given image ins then generated from a configuration by a primal sketch model [5] which is formulated as the likelihood in the Bayesian framework. The paper will be focused on designing an effective inference algorithm which computes (or construct) a hierarchical parsing graph from an input image in the process of maximizing a Bayesian posterior probability or equivalently minimizing a description length (MDL). It is worth clarifying that the parsing graph here is only a generic interpretation for the object layout and it {{is beyond the scope of}} this paper to deal with explicit scene and object recognition. The inference algorithm integrates bottom-up rectangle detection as weighted candidates (particles) which activate the grammar rules for top-down predictions of occluded or missing components. Intuitively each grammar rule maintains a list of particles as in an "assembly line". The top-down process chooses the most promising particle (with heaviest weight) at each step. The acceptance of a grammar rule means a recognition of a certain sub-configuration so that the description length is decreased in a greedy way, and it also activates a number of actions: (i) creating new "top-down" particles and insert them into the lists; (ii) reweighing some particles in the lists; (iii) passing attributes between <b>a</b> <b>node</b> and its parent through the constraint equations associated with this production rule. When an attribute is passed from a child to <b>a</b> <b>node</b> to <b>a</b> parent <b>node,</b> it is called bottom-up; and the opposite is called top-down. The whole procedure is, in spirit, similar to the data-drive Markov chain Monte Carlo paradigm [20], [16], except that a greedy algorithm is adopted for simplicity...|$|R
5000|$|In 2004 {{the network}} was <b>expanded</b> by adding <b>a</b> <b>node</b> {{on top of the}} 15 story ‘Faraday Building’ to the east of the campus. This node allowed line of site access to the main student areas of Southampton and <b>a</b> further three <b>nodes</b> in peoples homes were {{connected}} to the network. All the links in the network were wireless because the university would not allow unauthenticated traffic to pass over their network and at the time SOWN was an open network with no authentication. It was proposed that the network be secured using 802.1x authentication which would allow it to tie in with Eduroam and thus authenticate academics from around the world.|$|R
40|$|Best-first {{search is}} a general search {{algorithm}} that, always <b>expands</b> next <b>a</b> frontier <b>node</b> of lowest cost. Its applicability, however, is limited by its exponential memory requirement. Iterative deepening, a previous approach to this problem, does not expand nodes in best-first, order if t’he cost, function can decrease along a path. We present a linear-space best-first search algorithm (RBFS) that always explores new nodes in best-first order, regardless of the cost function, and expands fewer nodes than iterative deepening with a. nondecreasing cost function. On the sliding-tile puzzles, RBFS with a weighted evaluation function dramatically reduces computation time with onl...|$|R
5000|$|Expansion and pruning of nodes {{is driven}} by keeping two values of [...] for every node. <b>Node</b> [...] stores <b>a</b> value [...] which {{estimates}} the cost of reaching the goal by taking a path through that node. The lower the value, the higher the priority. As in A* this value is initialized to , but will then be updated to reflect changes to this estimate when its children are <b>expanded.</b> <b>A</b> fully <b>expanded</b> <b>node</b> will have <b>an</b> [...] value at least as high as that of its successors. In addition, the node stores the [...] value of the best forgotten successor. This value is restored if the forgotten successor is revealed {{to be the most}} promising successor.|$|R
3000|$|We {{compute the}} {{semantic}} relatedness between each expanded node and the anchor node set, denoted as μ([...] EV.L, AN) [...] in Sect.  5.2. Then we rank all the expanded nodes {{in descending order}} of their semantic relatedness scores. We filter the less related <b>expanded</b> <b>nodes</b> given <b>a</b> relatedness threshold τ. We introduce another parameter α to easily represent τ. We set τ based on α (a smaller α results in a larger τ). For each document, we set the number of expanded nodes no larger {{than the number of}} anchor nodes. Figure  13 shows the effects of α on keyphrase extraction, where the cluster number r is 1 and the keyphrase number K is 10. From the figure, we can see a suitable semantic relatedness threshold could improve the performance since it filters some noisy <b>expanded</b> <b>nodes.</b> <b>A</b> large semantic relatedness threshold would decrease the improvement of performance since it filters too much semantic relations including meaningful relations and expanded nodes.|$|R
50|$|Sometimes it is {{possible}} to use anchor ties (also called bolt ties), these are ties fitted into holes drilled in the structure. A common type is a ring bolt with <b>an</b> <b>expanding</b> wedge which is then tied to <b>a</b> <b>node</b> point.|$|R
5000|$|Splitting <b>a</b> <b>node.</b> Splitting <b>a</b> <b>node</b> {{corresponds}} to splitting <b>a</b> <b>node</b> into two half <b>nodes,</b> one being <b>a</b> sink {{and the other}} a source.|$|R
3000|$|... (at <b>an</b> {{interior}} <b>node).</b> <b>A</b> tip <b>node</b> is <b>a</b> <b>node</b> {{with one}} edge, whereas <b>an</b> interior <b>node</b> {{has more than}} one edge.|$|R
40|$|A Conditional Tree Pattern (CTP) <b>expands</b> <b>an</b> XML tree {{pattern with}} labels {{attached}} to the descendant edges. These labels can be XML element names or Boolean CTPs. The meaning of a descendant edge labelled by A and ending in <b>a</b> <b>node</b> labelled by B is a path of child steps ending in <b>a</b> B <b>node</b> such that all intermediate <b>nodes</b> are <b>A</b> <b>nodes.</b> In effect this expresses the until B, A holds construction from temporal logic. This paper studies the containment problem for CTP. For tree patterns (TP), this problem {{is known to be}} coNP-complete. We show that it is PSPACE-complete for CTP. This increase in complexity {{is due to the fact}} that CTP is expressive enough to encode an unrestricted form of label negation: *∖ <b>a,</b> meaning "any <b>node</b> except <b>an</b> a-node". Containment of TP expanded with this type of negation is already PSPACE-hard. CTP is a positive, forward, first order fragment of Regular XPath. Unlike TP, CTP expanded with disjunction is not equivalent to unions of CTP's. Like TP, CTP is a natural fragment to consider: CTP is closed under intersections and CTP with disjunction is equally expressive as positive existential first order logic expanded with the until operator...|$|R
50|$|Child: <b>A</b> child <b>node</b> is <b>a</b> <b>node</b> {{extending}} from another <b>node.</b> For example, <b>a</b> computer with internet access {{could be considered}} <b>a</b> child <b>node</b> of <b>a</b> <b>node</b> representing the internet. The inverse relationship is that of <b>a</b> parent <b>node.</b> If <b>node</b> C is <b>a</b> child of <b>node</b> <b>A,</b> then A is the parent node of C.|$|R
25|$|The {{following}} code inserts <b>a</b> <b>node</b> after <b>an</b> existing <b>node</b> in <b>a</b> singly linked list. The diagram {{shows how}} it works. Inserting <b>a</b> <b>node</b> before <b>an</b> existing one cannot be done directly; instead, one must {{keep track of}} the previous <b>node</b> and insert <b>a</b> <b>node</b> after it.|$|R
5000|$|Splitting <b>a</b> <b>node</b> <b>A</b> <b>node</b> {{is split}} by {{replacing}} it with its children.|$|R
50|$|Assume {{a failure}} free state for a path from <b>a</b> <b>node</b> B to <b>a</b> <b>node</b> <b>A.</b> <b>Node</b> B bridges the signal destined to <b>A</b> from other <b>nodes</b> on the ring, both on working and {{protecting}} routes. At <b>node</b> <b>A,</b> signals {{from these two}} routes are continuously monitored for path layer defects and the better quality signal is selected.Now consider a failure state where fiber between <b>node</b> <b>A</b> and <b>node</b> B is cut. The selector switches traffic on the standby route when the active route between <b>node</b> <b>A</b> and <b>node</b> B is failed.|$|R
3000|$|Since the {{certificates}} {{issued by}} <b>a</b> <b>node</b> {{are stored in}} its local repository, one of the tasks that <b>a</b> <b>node</b> may perform during idle periods is the renewal of certificates issued by it to those nodes that might still be considered as trusted. Otherwise, certificate renewal may be developed on demand. It means that when an expired certificate {{is included in the}} non-updated repository of <b>a</b> <b>node,</b> such <b>a</b> <b>node</b> should request <b>a</b> renewal for that certificate. When a certificate for <b>a</b> <b>node</b> [...]...|$|R
5000|$|<b>A</b> sensor <b>node,</b> {{also known}} as a mote (chiefly in North America), is <b>a</b> <b>node</b> in <b>a</b> sensor network that is capable of {{performing}} some processing, gathering sensory information and communicating with other connected nodes in the network. A mote is <b>a</b> <b>node</b> but <b>a</b> <b>node</b> is not always a mote.|$|R
5000|$|... {{where every}} path from <b>a</b> <b>node</b> in [...] to <b>a</b> <b>node</b> in [...] passes through [...]|$|R
3000|$|... ∀i,j is the {{probability}} that <b>a</b> <b>node</b> sends in contention slot j. Note that <b>a</b> <b>node</b> may send more than one reply during the selection. The reply contains <b>a</b> <b>node</b> identifier and, optionally, a metric indicating the node’s suitability for selection.|$|R
30|$|If <b>a</b> <b>node</b> is <b>a</b> {{member of}} a {{community}} represented by a base station, {{it is very likely}} that <b>a</b> <b>node</b> will meet the base station in near future. Considering all the parameters the same, if <b>a</b> <b>node</b> say <b>A</b> contacts <b>a</b> <b>node,</b> say B, of the same community as the message destination, then node B’s utility to deliver that message will be higher than A.|$|R
5000|$|When <b>a</b> <b>node</b> in <b>a</b> cluster fails, {{strategies}} such as [...] "fencing" [...] may {{be employed}} to keep {{the rest of the}} system operational. Fencing is the process of isolating <b>a</b> <b>node</b> or protecting shared resources when <b>a</b> <b>node</b> appears to be malfunctioning. There are two classes of fencing methods; one disables <b>a</b> <b>node</b> itself, and the other disallows access to resources such as shared disks.|$|R
5000|$|Tree: The full input Newick Format for {{a single}} tree Subtree: <b>an</b> {{internal}} <b>node</b> (and its descendants) or <b>a</b> leaf <b>node</b> Leaf: <b>a</b> <b>node</b> with no descendants Internal: <b>a</b> <b>node</b> and its one or more descendants BranchSet: a set {{of one or more}} Branches Branch: a tree edge and its descendant subtree. Name: the name of <b>a</b> <b>node</b> Length: the length of a tree edge.|$|R
5000|$|... {{while looking}} at the {{transitive}} closure of <b>a</b> system (all <b>nodes</b> downstream from <b>a</b> <b>node),</b> <b>a</b> <b>node</b> in its own transitive closure indicates a circularity; ...|$|R
5000|$|... where <b>a</b> <b>node</b> x {{is said to}} {{dominate}} <b>node</b> y in <b>a</b> directed graph if every path from start to y includes x. <b>A</b> <b>node</b> x is said to postdominate <b>a</b> <b>node</b> y if every path from y to end includes x.|$|R
3000|$|We {{consider}} a directed graph G of N <b>nodes</b> (<b>A,</b> B,...). In a directed graph, an edge between two <b>nodes,</b> <b>A</b> and B, has a direction, either from A to B or from B to A. We denote an edge from <b>a</b> <b>node</b> <b>A</b> to <b>a</b> <b>node</b> B by <b>A</b> → B. A directed path, in G, from <b>a</b> <b>node</b> <b>A</b> to <b>a</b> <b>node</b> B is <b>a</b> set of edges leading from A to B, possibly through other nodes {{as depicted in}} Figure 12 a. For example, the edge set (A → B, B → C, [...]. [...]. [...]., Y → Z), form a directed path from <b>node</b> <b>A</b> to <b>node</b> Z. We consider that {{there is always a}} trivial directed path from any <b>node</b> <b>A</b> to itself.|$|R
5000|$|<b>A</b> <b>node</b> d {{strictly}} dominates <b>a</b> <b>node</b> n if d dominates n and d {{does not}} equal n.|$|R
