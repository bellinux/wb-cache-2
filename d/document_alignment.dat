16|40|Public
40|$|International audienceThis paper {{presents}} {{our first}} attempt at constructing a Vietnamese-French statistical machine translation system. Since Vietnam-ese is an under-resourced language, we concentrate on building a large Vietnamese-French parallel corpus. A <b>document</b> <b>alignment</b> method based on publication date, special words and sentence alignment result is proposed. The paper also presents an application of the obtained parallel corpus {{to the construction of}} a Vietnamese-French statistical machine translation system, where the use of different units for Vietnamese (syllables, words, or their combinations) is discussed...|$|E
40|$|In this paper, {{we present}} an {{unsupervised}} hybrid model which combines statistical, lexical, linguistic, contextual, and temporal features in a generic EMbased framework to harvest bilingual terminology from comparable corpora through comparable <b>document</b> <b>alignment</b> constraint. The model is configurable for any language and is extensible for additional features. In overall, it produces considerable improvement in performance over the baseline method. On top of that, our model has shown promising capability to discover new bilingual terminology with limited usage of dictionaries. ...|$|E
40|$|This paper {{presents}} {{our first}} attempt at constructing a Vietnamese-French statistical machine translation system. Since Vietnamese is an under-resourced language, we concentrate on building a large Vietnamese-French parallel corpus. A <b>document</b> <b>alignment</b> method based on publication date, special words and sentence alignment result is proposed. The paper also presents an application of the obtained parallel corpus {{to the construction of}} a Vietnamese-French statistical machine translation system, where the use of different units for Vietnamese (syllables, words, or their combinations) is discussed. ...|$|E
40|$|We {{propose a}} new {{approach}} to improving named entity recognition (NER) in broadcast news speech data. The approach proceeds in two key steps: (1) we automatically detect <b>document</b> <b>alignments</b> between highly similar speech documents and corresponding written news stories that are easily obtainable from the Web; (2) we employ term expansion techniques commonly used in information retrieval to recover named entities that were initially missed by the speech transcriber. We show that our method is able to find named entities missing in the transcribed speech data, and additionally to correct incorrectly assigned named entity tags. Consequently, our novel approach improves state-of-the-art NER results from speech data both in terms of recall and precision. status: publishe...|$|R
40|$|Mapping {{documents}} into an interlingual representation {{can help}} bridge {{the language barrier}} of a cross-lingual corpus. Previous approaches use aligned documents as training data to learn an interlingual representation, making them sensitive to {{the domain of the}} training data. In this paper, we learn an interlingual representation in an unsupervised manner using only a bilingual dictionary. We first use the bilingual dictionary to find candidate <b>document</b> <b>alignments</b> and then use them to find an interlingual representation. Since the candidate alignments are noisy, we develop a robust learning algorithm to learn the interlingual representation. We show that bilingual dictionaries generalize to different domains better: our approach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain. ...|$|R
40|$|In {{the context}} of a {{multimodal}} application, this article proposes an image-based method for bridging the gap between document excerpts and video extracts. The approach, further called <b>document</b> image <b>alignment,</b> takes advantage of the observable events related to documents that are visible during meetings. In particular, the article presents a new method for detecting slide changes in slideshows, its evaluation, and a preliminary work on document identification. 1...|$|R
40|$|The WMT Bilingual <b>Document</b> <b>Alignment</b> Task {{requires}} {{systems to}} assign source pages to their “translations”, {{in a big}} space of possible pairs. We present four methods: The first one uses the term position similarity between candidate document pairs. The second method requires automatically translated versions of the target text, and matches them with the candidates. The third and fourth methods try to overcome some of the challenges presented {{by the nature of}} the corpus, by considering the string similarity of source URL and candidate URL, and combining the first two approaches...|$|E
40|$|In this paper, {{we present}} a {{validation}} approach of detected alignment links between dialog transcript and discussed documents, {{in the context of}} a multimodal <b>document</b> <b>alignment</b> framework of multimedia events (meetings and lectures). The validation approach consists in an entailment process of the detected alignment links. This entailment process exploits several features, from the structural level of aligned documents to the linguistic level of their tokens. The implemented entailment strategies were evaluated on several multimodal corpora. The obtained results prove that the choice of the relevant entailment strategy depends on the types of documents that are available in the corpus, on their content, and also on the nature of the corpus...|$|E
40|$|This paper {{presents}} the results of the WMT 16 shared tasks, which included five machine translation (MT) tasks (standard news, IT-domain, biomedical, multimodal, pronoun), three evaluation tasks (metrics, tuning, run-time estimation of MT quality), and an automatic post-editing task and bilingual <b>document</b> <b>alignment</b> task. This year, 102 MT systems from 24 institutions (plus 36 anonymized online systems) were submitted to the 12 translation directions in the news translation task. The IT-domain task received 31 submissions from 12 institutions in 7 directions and the Biomedical task received 15 submissions systems from 5 institutions. Evaluation was both automatic and manual (relative ranking and 100 -point scale assessments) ...|$|E
40|$|Topic {{models have}} been studied {{extensively}} {{in the context of}} monolingual corpora. Though there are some attempts to mine topical structure from cross-lingual corpora, they require clues about <b>document</b> <b>alignments.</b> In this paper we present a generative model called JointLDA which uses a bilingual dictionary to mine multilingual topics from an unaligned corpus. Experiments conducted on different data sets confirm our conjecture that jointly modeling the cross-lingual corpora offers several advantages compared to individual monolingual models. Since the JointLDA model merges related topics in different languages into a single multilingual topic: a) it can fit the data with relatively fewer topics. b) it has the ability to predict related words from a language different than that of the given document. In fact it has better predictive power compared to the bag-of-word based translation model leaving the possibility for JointLDA to be preferred over bag-of-word model for cross-lingual IR applications. We also found that the monolingual models learnt while optimizing the cross-lingual copora are more effective than the corresponding LDA models...|$|R
40|$|We {{propose a}} new {{approach}} to improving named entity recognition (NER) in broadcast news speech data and to adapting NER to changing name patterns in news speech data. NER refers here to the recognition of person, location and organization names. The approach proceeds in two key steps: (1) we automatically detect <b>document</b> <b>alignments</b> between highly similar speech documents and corresponding written news stories that are easily obtainable from the Web; (2) we employ term expansion techniques inspired by information retrieval methods to recover NEs that were initially missed by the speech transcriber. We have developed several models, which are able to correct wrongly transcribed NEs in the speech data, to suggest missing NEs and to correctly assign their semantic tag. The methods differ in how relevant NEs are selected from related written documents. We have downloaded 40 short broadcast news and retrieved 5532 related written news stories from Web. The automatic speech recognition system (ASR) of FBK is used to transcribe the 40 broadcast news stories. The Stanford NER system is applied on the transcribed speech data and this method forms our baseline NER. The transcribed speech data misses many NEs (106 NEs missing out of th...|$|R
40|$|This thesis {{describes}} and evaluates pathology test ordering {{by general}} practitioners (GPs) in Australia, using data {{collected in the}} BEACH program (2000 - 2010). From 2000 - 02 to 2006 - 08, the total increase in volume of GP-ordered pathology tests was due to: increased likelihood of GPs’ ordering test(s), increased number of tests ordered per episode, increased number of problems managed at encounters, and increased population attendance rates. Significant independent predictors {{of the volume of}} pathology ordered by GPs were investigated. The principal explanatory variable was the type of problem being managed. For six problems, appropriateness of ordering was assessed, by measuring alignment of GPs’ ordering with guidance <b>documents.</b> <b>Alignment</b> was good for: hypertension, Type 2 diabetes, lipid disorders and weakness/tiredness; and poor for ‘health checks’ and overweight/obesity. Of the total volume of tests for these problems, only a small proportion was deemed inappropriate. I found no evidence to support concerns raised in the literature about widespread inappropriate ordering. For the ongoing management of chronic problems, pathology testing guidance was poor. Australia has an ageing population and therefore chronic problem management, and the testing associated with it, will inevitably increase. Improved pathology guidance could help support GPs’ to order appropriately in this high growth area...|$|R
40|$|In this paper, {{we present}} a feature-based method to align {{documents}} with similar content across two sets of bilingual comparable corpora from daily news texts. We evaluate the contribution of each individual feature and investigate the incorporation of these diverse statistical and heuristic features for the task of bilingual <b>document</b> <b>alignment.</b> Experimental results on the English-Chinese and English-Malay comparable news corpora show that our proposed Discrete Fourier Transformbased term frequency distribution feature is very effective. It contributes 4. 1 % and 8 % to performance improvement over Pearson’s correlation method on the two comparable corpora. In addition, when more heuristic and statistical features {{as well as a}} bilingual dictionary are utilized, our method shows an absolute performance improvement of 23. 2 % and 15. 3 % on the two sets of bilingual corpora when comparing with a prior information retrieval-based method. ...|$|E
40|$|Abstract. Document {{security}} {{does not}} only {{play an important}} role in specific domains e. g. passports, checks and degrees but also in every day documents e. g. bills and vouchers. Using special high-security features for this class of documents is not feasible due to the cost and the complexity of these methods. We present an approach for detecting falsified docu-ments using a document signature obtained from its intrinsic features: bounding boxes of connected components are used as a signature. Using the model signature learned from a set of original bills, our approach can identify documents whose signature significantly differs from the model signature. Our approach uses globally optimal <b>document</b> <b>alignment</b> to build a model signature that can be used to compute the probability of a new document being an original one. Preliminary evaluation shows that the method is able to reliably detect faked documents. ...|$|E
40|$|Abstract We {{present a}} {{multimodal}} <b>document</b> <b>alignment</b> framework, which highlights existing alignment relationships between documents that are discussed and recorded during multimedia {{events such as}} meetings. These relationships that should help indexing the archives of these events are detected using various techniques from natural language processing and information retrieval. The main alignment strategies studied are based on thematic, quotation and reference relationships. At the analysis level, the alignment framework was applied at several levels of granularity of documents, requiring specific document segmentation techniques. Our framework that is language independent was evaluated on corpora in French and English, including meetings and scientific presentations. The satisfactory evaluation results obtained at several stages show the importance of our approach in bridging the gap between meeting documents, independently from the language and domain. They highlight also {{the utility of the}} multimodal alignment in advanced applications, e. g. multimedia document browsing, contentbased / temporal-based searching, etc...|$|E
40|$|The {{development}} of a common syntax for EDI (Electronic Data Interchange), XML (eXtensible Markup Language), opened new formalization perspectives for interorganizational data exchanges over the Internet. Many of the organizations involved in the normalizaEDI, IAS/IFRS norms, financial reporting, XBRL, taxonomies specifications, <b>document</b> instance, strategic <b>alignment,</b> XBRL platform. ...|$|R
40|$|Thin {{sections}} (0. 1 - 0. 25 micron) {{of isolated}} chicken erythrocyte nuclei were examined at various tilt angles. Stereo pairs of electron micrographs <b>document</b> the parallel <b>alignment</b> of 25 -nm chromatin fibers {{adjacent to the}} nuclear envelope, and demonstrate a fiber substructure consistent with close-packed arrays of nucleosomes...|$|R
50|$|Text: Notes can {{be created}} by typing, {{importing}} text or files, or by dragging from other notes, other programs, or files. The sliding text toolbar {{can be used to}} select font, size, style, text color, emphasis color, <b>document</b> color, or <b>alignment.</b> There are no limits on the number of notes or note size.|$|R
40|$|This paper {{presents}} a multimodal <b>document</b> <b>alignment</b> framework {{that aims to}} a cross-indexing of the various documents within multimodal applications (e. g. meetings and lectures). These documents might be either present during the event (e. g. static documents) or generated after the event (e. g. speech transcript). In the current study, three documents types have been considered, static documents, speech transcript and slideshows. This work {{is an extension of}} a previous bimodal alignment framework, between static documents and speech transcript of meetings [10]. The obtained results in this paper prove that the slideshows are a considerable alternative for speech recordings to temporally index static documents. Moreover, it is shown that the integration of slideshows has improved and reinforced the bimodal alignment between static documents and speech transcript. Furthermore, several features have been considered and studied in our enhanced alignment framework, such as effect of the TF. IDF metric, the WordNet thesaurus and the noisy speech on our alignment process...|$|E
40|$|We apply cross-lingual Latent Semantic Indexing to the Bilingual <b>Document</b> <b>Alignment</b> Task at WMT 16. Reduced-rank {{singular}} value {{decomposition of}} a bilingual term-document matrix derived from known English/French page pairs {{in the training}} data allows us to map monolingual documents into a joint semantic space. Two variants of cosine similarity between the vectors that place each document into the joint semantic space are combined {{with a measure of}} string similarity between corresponding URLs to produce 1 : 1 alignments of English/French web pages in a variety of domains. The system achieves a recall of ca. 88 % if no in-domain data is used for building the latent semantic model, and 93 % if such data is included. Analysing the system’s errors on the training data, we argue that evaluating aligner performance based on exact URL matches under-estimates their true performance and propose an alternative that is able to account for duplicates and near-duplicates in the underlying data...|$|E
40|$|We are {{presenting}} a new highly multilingual document-aligned parallel corpus called DCEP- Digital Corpus of the European Parliament. It consists of various document types covering {{a wide range}} of subject domains. With a total of 1. 37 billion words in 23 languages (253 language pairs), gathered in the course of ten years, this is the largest single release of documents by a European Union institution. DCEP contains most of the content of the European Parliament's official Website. It includes different document types produced between 2001 and 2012, excluding only the documents already exist in the Europarl corpus to avoid overlapping. We are presenting the typical acquisition steps of the DCEP corpus: data access, <b>document</b> <b>alignment,</b> sentence splitting, normalisation and tokenisation, and sentence alignment efforts. The sentence-level alignment is still in progress but based on some first experiments; we showed that DCEP is very useful for NLP applications, in particular for Statistical Machine Translation...|$|E
50|$|The Carrollton Road-Carrollton Segment is a {{historic}} 19th-century road in Boone County, Arkansas. It extends for more than 4 mi eastward from Carrollton, which was once the county seat of Carroll County. The roadways matching the <b>documented</b> 1837 <b>alignment</b> of the road are (from west to east, beginning at U.S. Route 412) County Road 417, Terrapin Creek Road, and Dunkard Road, up to its junction with Green Hill Road. This stretch of dirt road has retained a 19th-century rural character, and is further significant as a route taken by Native American parties on the Trail of Tears in 1838.|$|R
40|$|This {{document}} {{contains a}} brief summary {{of research on the}} Success for All comprehensive school reform program and the <b>alignment</b> <b>document</b> that outlines the correlation between Success for All and the objectives and outcomes described in the Stanford Achievement Test: Ninth Edition (SAT- 9) as designed by Harcourt, Inc. Research has found Success for All to be one of only two elementary school models of comprehensive school reform to receive the highest ratings for research quality and outcomes. The purpose of the alignment is to assure educators that teaching the Success for All program effectively will not only help children improve their reading ability but will also help them demonstrate what they have learned on state accountability measures. The <b>alignment</b> <b>document</b> is organized into a 2 -column chart with the SAT- 9 test objectives on the left side and the corresponding Success for All components on the right for grades 1 through 6. An Activities Glossary that briefly describes the various components of the program i...|$|R
40|$|The CORBA-based Simulator was a Laboratory Directed Research and Development (LDRD) {{project that}} applied {{simulation}} techniques to explore critical questions about distributed control systems. The simulator project used a three-prong approach that studied object-oriented distribution tools, computer network modeling, and simulation of key control system scenarios. The National Ignition Facility's (NIF) optical alignment system was modeled to study control system operations. The alignment of NIF's 192 beamlines {{is a large}} complex operation involving more than 100 computer systems and 8000 mechanized devices. The alignment process is defined by a detailed set of procedures; however, many of the steps are deterministic. The alignment steps for a poorly aligned component are {{similar to that of}} a nearly aligned component; however, additional operations/iterations are required to complete the process. Thus, the same alignment operations will require variable amounts of time to perform depending on the current alignment condition as well as other factors. Simulation of the alignment process is necessary to understand beamline alignment time requirements and how shared resources such as the Output Sensor and Target Alignment Sensor effect alignment efficiency. The simulation has provided alignment time estimates and other results based on <b>documented</b> <b>alignment</b> procedures and alignment experience gained in the laboratory. Computer communication time, mechanical hardware actuation times, image processing algorithm execution times, etc. have been experimentally determined and incorporated into the model. Previous analysis of alignment operations utilized average implementation times for all alignment operations. Resource sharing becomes rather simple to model when only average values are used. The time required to actually implement the many individual alignment operations will be quite dynamic. The simulation model estimates the time to complete an operation using distributions rather than static values. The only way to accurately understand resource utilization and time requirements for a complex industrial application such as alignment, is to utilize simulation tools such as Simprocess to model the system...|$|R
40|$|Research {{findings}} are often transmitted both as written documents and narrated slide presentations. As these {{two forms of}} media contain both unique aspects and replicated information, {{it is useful to}} combine and align the two to create a single unified medium. We discuss the problem and explore algorithms to tackle the presentation to <b>document</b> <b>alignment</b> problem, based only on the text extractable from both media. We investigate three different similarity metrics and two alignment procedures on the domain to assess their efficacy for the problem. Although edit distance is widely used in alignment a standard greedy algorithm outperformed such an edit distance based approach on our problem set. As some slides in presentations should not be aligned to any paragraphs (e. g., outline slides), we further enhanced the greedy approach with an supervised classifier to distinguish these nil alignments from others. We have evaluated our approach on a corpus of 20 computer science presentation-document pairs, and demonstrated our highest accuracy rate of 62 %...|$|E
40|$|Version 1. 2 : This version {{draws from}} {{numerous}} discussions, workshops and planning sessions {{over the past}} decade, along with an extensive review of the published literature and the approaches adopted by colleagues in other large landscape programs. While a complete document, and ready for use providing additional guidance to work underway, it {{will be subject to}} extensive review by collaborating groups and scientific peers during the remainder of 2014 and the first half of 2015. We expect Version 2. 0 to be a substantially improved <b>document.</b> <b>Alignment</b> of Australian wildlife corridors with the National Wildlife Corridors Plan is supported through funding from the Australian Government’s National Wildlife Corridors Plan, which supported development of Version 1. 1 Acknowledgements The hard yards of drawing this document together, and wrestling with the many nuances of what ecological resilience and function mean over this particular large landscape, has been admirably undertaken by Paula Deegan, working with the Gondwana Link Ltd CEO and Information Manager and drawing on the efforts and thoughts {{of a wide range of}} groups and individuals...|$|E
40|$|Abstract. Ontology {{matching}} and mapping is {{of critical}} importance to effective consumption of distributed and heterogeneous data-sets in today’s Web of Data. Since 2004 the Ontology Alignment Evaluation Initiative (OAEI) provides a number of complex challenges to evaluate {{the performance of the}} increasing number of matching tools and methods. This leads to the question how the individual OAEI challenges and the individual alignment results can be documented best for effective online consumption, management and further analysis. In this paper, we argue that the current documentation of alignment creation lifecycle aspects within OAEI would benefit from more formal model support. In this paper we present a case study to show how our ontology-based meta-data model for ontology mapping reuse (OM 2 R) can be applied for the OAEI to <b>document</b> <b>alignment</b> challenges and some quantification on the likely benefits in terms of helping challenge administrators and participants create consistent documentation in terms of high correctness and less inconsistent statements as well as results that are explicit, predictable and easy to interpret...|$|E
40|$|Includes bibliographical {{references}} (pages 208 - 222) Archaeoastronomical investigations at LAn- 357, Burro Flats (sites Ven- 551 -Ven 561), and Bell Canyon (sites LAn- 413 and LAn- 511) revealed evidence strongly {{suggesting that}} these sites were astronomically significant {{for both the}} Chumash and Fernandeno groups which occupied the west San Fernando Valley {{at the time of}} historic contact. Field research conducted at these sites during the winter and summer solstices of 1979 - 1980 revealed sunrise and sunset alignments which are believed to be ritually significant, as well as perhaps calendrically important. Two types of astronomical solstitial alignments were discovered within the study area: (1) direct <b>alignments,</b> <b>documented</b> for LAn- 357, Burro Flats and Bell Canyon, and (2) indirect <b>alignments,</b> <b>documented</b> at Burro Flats. LAn- 357 and Burro Flats are village/habitation sites which consist of numerous rock art loci [...] pictographs and petroglyphs. Bell Canyon consists of an historic village (Huwam - LAn- 413) and an ethnohistoric shrine (Tswaya tsuqele - LAn- 511) used for the Winter Solstice Ceremony. This thesis combines archaeoastronomical data, rock art interpretation and an intensive review of the ethnographic/ ethnohistoric literature in an attempt to analyze the significance of these alignments in reference to Chumash and Fernandeno ceremonialism...|$|R
40|$|The {{quality of}} a {{statistical}} machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training. In recent years, {{there have been several}} approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages. We advance {{the state of the art}} in parallel sentence extraction by modeling the <b>document</b> level <b>alignment,</b> motivated by the observation that parallel sentence pairs are often found in close proximity. We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model. Results for both accuracy in sentence extraction and downstream improvement in an SMT system are presented. ...|$|R
40|$|Annotating the regions, text {{lines and}} {{characters}} of document images is an important, but tedious and expensive task. A ground-truthing tool may largely alleviate the human burden in this process. This paper describes an automated recognition-based tool GTLC for finding the best alignment between the text transcript and the connected components of unconstrained handwritten <b>document</b> image. The <b>alignment</b> process is formulated as an optimization problem involving candidate character segmentation and recognition. We have validated {{the effectiveness of}} this tool and have used it for annotating a large number of handwritten Chinese documents...|$|R
40|$|In machine translation, <b>document</b> <b>alignment</b> {{refers to}} finding correspondences between {{documents}} which are exact translations of each other. We define pseudo-alignment as {{the task of}} finding topical—as opposed to exact—correspondences between documents in different languages. We apply semisupervised methods to pseudo-align multilingual corpora. Specifically, we construct a topicbased graph for each language. Then, given exact correspondences between a subset of documents, we project the unaligned documents into a shared lower-dimensional space. We demonstrate that close documents in this lower-dimensional space tend to share the same topic. This has applications in machine translation and cross-lingual information analysis. Experimental results show that pseudo-alignment of multilingual corpora is feasible and that the document alignments produced are qualitatively sound. Our technique requires no linguistic knowledge of the corpus. On average when 10 % of the corpus consists of exact correspondences, an on-topic correspondence occurs within the top 5 foreign neighbors in the lowerdimensional space while the exact correspondence occurs within the top 10 foreign neighbors in this this space. We also show how to substantially improve these results with a novel method for incorporating language-independent information. ...|$|E
40|$|This paper {{describes}} {{a way of}} performing alignment of Portuguese-Chinese bilingual pairs from the given two <b>documents.</b> Extracting the <b>alignment</b> pairs is a critical step for building Portuguese-Chinese bilingual corpus for Example Based Machine Translation Systems (EBMT). In short, the proposed alignment system performs four steps: break down the document into sentences level, score each pair of sentence by different features, apply Singular Value Decomposition on the results, extract the aligned pair base on a similarity function. Keywords: Bilingual document alignment; singular value decomposition; segmentation; cross sentence alignment 1...|$|R
40|$|Information {{extraction}} (IE) from semi-structured Web documents {{plays an}} important role for a variety of information agents. Over the past decade, researchers have developed a rich family of generic IE techniques based on supervised approach which learn extraction rules from userlabelled training examples. However, annotating training data can be expensive when a lot of data sources need to be extracted. In this article, we introduce annotation-free IE using pattern mining and string alignment techniques. We describe OLERA, a semi-supervised IE system that produces extraction rules by aligning similar contents of multiple input records together and presents the result in a spreadsheet-like table. Therefore, users do not need to annotate the input documents but only to specify the scheme for the extracted data after the extraction pattern is discovered. Another plus is that this approach works not only for multi-record Web pages (as a limitation of some unsupervised IE approaches) but also single-record Web pages. KEY WORDS information extraction, semi-structured <b>documents,</b> string <b>alignment,</b> approximate matching...|$|R
40|$|This study {{presents}} funding {{opportunities to}} support investment and rural development and fisheries sector. Adoption {{of the national}} program and rural development as a strategic document for implementing European programs meant for rural Romania assumption of an intervention model {{is based on a}} medium-term strategy {{on the development of the}} village world. The work is based on the analysis of official <b>documents</b> indicating <b>alignment</b> programs of intervention policies in the field and on their analysis of available data that refers to the current state of implementation of programs referring to Romanian rural development. Given that the Common Agricultural Policythe (CAP) budget for 2014 - 2020 is higher than the amounts allocated for the 2007 - 2013 program, consider that in determining the coordinates of financial allocation for the next year must take into account the problems encountered in previous period to eliminate them, because the efficiency of the implementation of funds to support rural development objectives depends largely on rural transformation internally and reducing disparities in the regions and communities, and externally in relation to Member States of the European Union...|$|R
40|$|A Curriculum <b>Alignment</b> <b>Document</b> directs {{secondary}} {{choral music}} {{curriculum and instruction}} in the Metropolitan School District of Decatur Township (Indianapolis, Indiana). Using the content and educational priorities of that document as a guide, this paper provides strategies for assessment and grading policies and procedures {{in the middle and}} high schools of Decatur Township. This paper first considers the literature and summarizes the substantial findings of researchers and the consensus of pedagogy experts related to grading and assessment practices in music classes. Secondly, a number of specific grading and assessment guidelines are presented and discussed, with special concern noted for validity and reliability in assessment. Finally, the appendix to this document provides sample assessment materials. School of MusicThesis (M. M. ...|$|R
40|$|We {{present a}} novel Bayesian topic model for {{learning}} discourse-level document structure. Our model leverages insights from discourse theory to constrain latent topic assignments {{in a way}} that reflects the underlying organization of document topics. We propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents. We show that this space of orderings can be effectively represented using a distribution over permutations called the Generalized Mallows Model. We apply our method to three complementary discourse-level tasks: cross-document <b>alignment,</b> <b>document</b> segmentation, and information ordering. Our experiments show that incorporating our permutation-based model in these applications yields substantial improvements in performance over previously proposed methods. 1 1...|$|R
