1|35|Public
40|$|The Radiation Force Balance (RFB) {{technique}} {{is well established}} and most widely used for the measurement of total ultrasonic power radiated by ultrasonic transducer. The {{technique is}} used as a primary standard for calibration of ultrasonic transducers with relatively <b>fair</b> <b>uncertainty</b> in the low power (below 1 W) regime. In this technique, uncertainty comparatively increases in the range of few watts wherein the effects such as thermal heating of the target, cavitations, and acoustic streaming dominate. In addition, error in the measurement of ultrasonic power is also caused due to movement of absorber at relatively high radiated force which occurs at high power level. In this article a new technique is proposed which does not measure the balance output during transducer energized state as done in RFB. It utilizes the change in buoyancy of the absorbing target due to local thermal heating. The linear thermal expansion of the target changes the apparent mass in water due to buoyancy change. This forms the basis for the measurement of ultrasonic power particularly in watts range. The proposed method comparatively reduces uncertainty caused by various ultrasonic effects that occur at high power such as overshoot due to momentum of target at higher radiated force. The functionality of the technique has been tested and compared with the existing internationally recommended RFB technique...|$|E
5000|$|Uncertainty {{is often}} an {{important}} factor in economics. According to economist Frank Knight, it is different from risk, where there is a specific probability assigned to each outcome (as when flipping a <b>fair</b> coin). <b>Uncertainty</b> involves a situation that has unknown probabilities, while the estimated probabilities of possible outcomes need not add to unity.|$|R
40|$|Life cycle cost (LCC) {{is used as}} a {{cost-effective}} decision support for maintenance of railway track infrastructure. However, a <b>fair</b> degree of <b>uncertainty</b> associated with the estimation of LCC is due to the statistical characteristics of reliability and maintainability parameters. This paper presents a methodology for estimation of uncertainty linked with LCC, by a combination of design of experiment and Monte Carlo simulation. The proposed methodology is illustrated by a case study of Banverket (Swedish National Rail Administration). The paper also includes developed maintenance cost models for track. Validerad; 2009; 20081128 (yuafuq...|$|R
40|$|The {{range and}} {{bandwidth}} {{of the services}} which must be offered by access networks have increased significantly over recent years. Furthermore, there is a <b>fair</b> degree of <b>uncertainty</b> about the trend of services which might be demanded by users in the future. Therefore, the architecture of access network becomes an important development area in order to adapt to these trends and make it scalable for future growth. In this paper we {{provide an overview of}} emerging architectures for broadband optical access networks. We also discuss challenges that component manufacturers might address to keep in pace with the market's rapid growth...|$|R
50|$|It {{is common}} for two signals to share an {{identical}} path, branch and follow different paths for a while, then converge {{back to the same}} point to produce a result. When this happens, you can remove a <b>fair</b> amount of <b>uncertainty</b> from the total delay because you know that they shared a common path for a while. Even though each signal has an uncertain delay, because their delays were identical for part of the journey the total uncertainty can be reduced. This tightens up the worst-case estimation for the signal delay, and usually allows a small but important speedup of the overall device.|$|R
50|$|Adaptation {{to climate}} change in Jordan {{describes}} measures with the objective to prepare {{the country for the}} impacts of climate change. Water resources in Jordan are scarce. Besides the rapid population growth, the impacts of climate change are likely to further exacerbate the problem. Temperatures will increase and the total annual precipitation is likely to decrease, however with a <b>fair</b> share of <b>uncertainty.</b> Hence, existing and new activities with the objective to minimize the gap between water supply and demand contribute to adapt Jordan to tomorrow’s climate. This might be accompanied by activities improving Jordan’s capacity to monitor and project meteorological and hydrological data and assess its own vulnerability {{to climate change}}. This article focuses on the impacts of climate change on the Jordanian water sector.|$|R
40|$|There is {{an ongoing}} {{discussion}} in the literature whether personal uncertainty is indeed a threatening experience or whether people actually try to maintain it or even seek it. The current paper argues that personal uncertainty is particularly a negative experience when approaching it. Furthermore, this work will demonstrate that this negative experience will also have consequences for self-regulatory processes, namely the motivation to regulate personal uncertainty. Building {{on the work of}} the social cognition field, I propose that people process information in a more detailed manner when they are approaching uncertainty (Study 1). Moreover, people will judge procedures, where they can voice their opinion as more <b>fair</b> when approaching <b>uncertainty</b> (Study 2). Findings collected in different experimental paradigms provide evidence that personal uncertainty is indeed a negative experience when approaching it...|$|R
40|$|The {{analysis}} of default probabilities and correlations within credit risky portfolios is usually strongly {{affected by the}} scarce availability of data. High standard deviations and a <b>fair</b> amount of <b>uncertainty</b> in the derived estimates are well known consequences of this. However, when deriving predictions in a second stage these volatilities are usu-ally ignored and only point estimators are used, giving a false appearance of accuracy. The aim {{of this paper is}} to show how a consideration of these uncertainties will affect this second stage analysis. Besides the introduction of a new Bayesian credit portfolio approach, for this purpose in a Bayesian framework the joint posterior distribution of default probabilities and correlation parameters will be derived. Further, the effects are quantified a consideration of this distribution would have, with respect to the prediction of portfolio risk figures and also for pricing of structured derivatives...|$|R
40|$|Extract: In today’s world, {{patients}} and hospitals are increasingly {{looking across the}} ocean to find the ‘right’ doctors for their needs. Already {{it is possible to}} recognize two modes of commerce for practicing transoceanic medicine. First there is medical tourism, which involves patients travelling to foreign countries to purchase healthcare services at a discount compared with their home country. Second there is international telemedicine, which involves doctors making virtual house calls to patients in remote country via internet-derived technology. Common to both of these practices is a <b>fair</b> degree of <b>uncertainty</b> as to the extent of the healthcare provider’s liability for medical malpractice due to undecided jurisdictional law in cyberspace. This article outlines the growth of transoceanic medicine — and the potential for disputes — and suggests that ADR may {{play a vital role in}} this brave new world of healthcare...|$|R
40|$|In {{business}} processes, knowledge-intensive {{tasks are}} ones {{in which the}} people performing such tasks {{are involved in a}} <b>fair</b> degree of <b>uncertainty.</b> These people are required to apply and bring together their experience, training, expertise and judgement. In particular, they are concerned about issues or problems that might arise and how these are best dealt with or avoided. Current workflow technology does not support such tasks, as it deals only with predictable and easily automated decision making. In particular, it fails to deliver the right information to the user at the right time based on the context of the process instance, thus not taking the opportunity to forewarn users of potential problems. Context-aware workflows are a way to overcome shortcomings of workflow management systems. This paper proposes an approach for the dynamic integration of knowledge and workflow processes by offering proper support for the realtime handling of the both the current context of a process and its execution path. ...|$|R
40|$|The {{numerical}} {{investigation of}} airflow and chemical transport characteristics {{for a general}} class of buildings involves identifying values for model parameters, such as e. ective leakage areas and temperatures, for which a <b>fair</b> amount of <b>uncertainty</b> exists. A Monte Carlo simulation, with parameter values drawn from likely distributions using Latin Hypercube sampling, helps to account for these uncertainties by generating a corresponding distribution of simulated results. However, conducting large numbers of model runs can challenge a simulation program, not only by increasing the need for fast algorithms, but also by proposing specific combinations of parameter values that may define difficult numerical problems. The paper describes several numerical approaches to improving the speed and reliability of the COMIS multizone airflow simulation program. Selecting a broad class of algorithms based on the mathematical properties of the airflow systems (symmetry and positive-definiteness), it evaluates new solution methods for possible inclusion in the COMIS code. In addition, it discusses further changes that will likely appear in future releases of the program...|$|R
40|$|SUMMARY: Many {{landfills}} in Sweden {{as well as}} in Europe {{are subject}} to closure in the near future. Roughly 2, 000 hectares of landfill area in Sweden have to be covered, equivalent to almost one hundred million tonnes of construction material. In addition to material costs in the order of tens of billions Euro, this also puts a strain on the environment through the exploitation of virgin materials. The deadline for this adoption to the EU Landfill directive is due in 2008 but it is already apparent that many landfills will not meet the deadline. Many landfill operators are considering alternative cover designs in order to reduce resource spending. However, there is a <b>fair</b> amount of <b>uncertainty</b> with regard to functional and environmental consequences of using alternative materials, both {{from the side of the}} companies and the authorities. It is necessary to characterise these materials in terms of chemical and physical properties. For example, the leaching of the materials is of great interest in order to estimate the requirements and design of future leachate treatment. 1...|$|R
30|$|Historical {{time series}} {{generated}} from GPS sites reveal significant seasonal {{variations in the}} vertical direction. It {{is well known that}} continental waters (soil moisture, snow, ground water) mass redistribution is one of the potential contributors to these observed seasonal variations although their actual loading effects on GPS results are least well understood. A number of hydrology model outputs exist with a <b>fair</b> degree of <b>uncertainty.</b> Studies of interrelations between anomalous vertical variations observed at geodetic sites and hydrology model outputs are useful, in particular, as the hydrology models continue to be refined. In this paper, artificial neural networks is proposed for correlating GPS height residuals with the soil moisture variability. Time series from eight sites of the global GPS network are used to correlate with the soil moisture information from the US National Oceanographic and Atmospheric Administration (NOAA) Climate Prediction Center’s land data assimilation system (CPC LDAS). The results show the feasibility of the neural interpretation in terms of the correlation coefficients (∼ 0.6) and root mean square errors (about 15 % of residual range). Other geodetic time series can be used for the same purpose, such as from SLR, VLBI, and absolute gravity.|$|R
40|$|Elsewhere {{in these}} proceedings, Schneider et al. discuss {{compositional}} constraints on hematite-rich spherule (blueberry) formation at Meridiani Planum. Schneider et al. provide the background for work done to date {{to understand the}} composition and mineralogy of the spherules and devise a test of possible concretion growth processes. They also report the results of area analyses of spherules in targets analyzed with the Alpha Particle X-ray Spectrometer (APXS) and test several possible models for included components other than hematite. In this abstract, we use the compositional trends for spherule-rich targets to compute possible elemental compositions of the spherules. This approach differs from that of, which also used a determination of the area of spherules in APXS targets, coupled with a correction for the radial acceptance function, to try to un-mix the compositions directly, using 2 and 3 -component models and mass balance. That approach contained a <b>fair</b> amount of <b>uncertainty</b> owing to problems associated with irregular and heterogeneous target geometry, unknown composition of non-spherule lithic components, and variable dust coatings on spherules. Since then, Opportunity has analyzed additional spherule-rich targets, and the compositional trends so obtained permit a more direct assessment of the data...|$|R
40|$|This study {{estimated}} energy, {{environmental and}} consumer economic impacts of U. S. federal residential energy efficiency standards that became {{effective in the}} 1988 - 2001 period or will take effect {{by the end of}} 2007. These standards {{have been the subject of}} in-depth analyses conducted as part of DOE's standards rulemaking process. This study drew on those analyses, but updated certain data and developed a common framework and assumptions for all of the products in order to estimate realized impacts and to update projected impacts. We estimate that the considered standards will reduce residential primary energy consumption and CO{sub 2 } emissions in 2020 by 8 % compared to the levels expected without any standards. They will save a cumulative total of 34 quads by 2020, and 54 quads by 2030. The estimated cumulative net present value of consumer benefit amounts to $ 93 billion by 2020, and grows to $ 125 billion by 2030. The overall benefit/cost ratio of cumulative consumer impacts is 2. 45 to 1. While the results of this study are subject to a <b>fair</b> degree of <b>uncertainty,</b> we believe that the general conclusions [...] DOE's energy efficiency standards save significant quantities of energy (and associated carbon emissions) and reduce consumers' net costs [...] are robust...|$|R
40|$|Article 78 (a) VAT Directive {{clearly states}} that taxes, duties, levies and charges must be {{included}} in the taxable amount for VAT. The CJEU has substantially mitigated the scope of this rule. Following the CJEU’s ruling in Lisboagás, the amount of a tax, duty, levy or charge may need to {{be included in the}} taxable amount under Article 73 if it cannot be included in the taxable amount on the basis of Article 78 (a). A <b>fair</b> amount of <b>uncertainty</b> still exists as to the relation between Article 73, Article 78 (a) and Article 79 (c) VAT Directive is unclear. This interrelationship is the main topic of this article. The authors provide for criteria to establish whether (amounts of) taxes, duties, levies or charges are {{to be included in the}} taxable amount. They show that – contrary to CJEU case law – Article 78 (a) cannot be interpreted in the light of Article 73 VAT Directive. Also, they clarify that if the amount of a tax, duty, levy or charge {{must be included}} in the taxable amount on the basis of Article 73, it cannot be excluded from it on the basis of Article 79 (c). Furthermore, they conclude that the reference in Article 84 to Article 78 (a) VAT Directive has become outdated...|$|R
40|$|I would {{typically}} {{think it}} should be 24 hours. Even if they don’t have the answer they should at least say, ‘I got your email’ [...] . ” [12] “I don’t want the people to think that they can get an immediate [response] [...] . [that] I’ll drop anything for you [...] . ” [12] Over the last few decades, email has become an important communication and collaboration tool. As a global, networked messaging system, email allows individuals to exchange information and carry on conversations, without requiring participants to gather at a physical location (as in face-to-face communication), at a particular point in time (as in telephone conversations), and with relative alacrity (as opposed to postal mail). Yet, email communication carries a <b>fair</b> degree of <b>uncertainty,</b> {{due to the nature of}} asyn-chronous messaging and the interaction design of the email system. Between the time that an email is sent and a response is received, there are few meaningful cues available to the sender about how the recipient is handling the message. “When will I get a response to this email? ” is a common question for email senders [12]. Unless the participants have some other means of estab-lishing context or a prior history of email interaction, answering this question is difficult. Decidin...|$|R
40|$|An {{increase}} in temperatures and {{a decrease in}} precipitations could be prejudicial to maintaining a premium wine production in regions with a Mediterranean climate. The present article offers to assess recent and future exposure to such changes in two Mediterranean wine producing regions: Roussillon (France) and McLaren Vale (Australia); as well as to frame elements increasing their vulnerability to such changes, and reversely elements that could help increase their adaptive capacity. A study of temperature and rainfall data observed (1956 - 2010) in Perpignan and Adelaide and simulated (2001 - 2060) by ARPEGE-RETIC-V 4 and CSIRO Mk 3. 5 was completed by sixty-one interviews with key stakeholders of the wine industry in both regions. Results show that producers had to face an {{increase in}} temperature {{and a decrease in}} rainfall in the last decade, and this tendency, according to climate models, is likely to continue through to 2060, with a <b>fair</b> level of <b>uncertainty.</b> This has negative impacts on grape maturation, and traditional vineyard management may not be sufficient to deal with it in the long term. Implementing new strategies of adaptation to uncertainty in climate, as well as in economy, depends on: a sufficient financial capital, a sustainable management of water resources, and flexibility in legislation. To achieve this, diversification and entrepreneurship of producers is particularly important. Anne-Laure Lereboullet, Douglas Bardsley et Gérard Beltrand...|$|R
40|$|Approved {{for public}} release; {{distribution}} in unlimited. As mesoscale models increase in resolution {{there is a}} greater need to understand predictability on smaller scales. The predictability of a model is related to forecast skill. It is possible that the uncertainty of one scale of motion can affect the other scales due to the nonlinearity of the atmosphere. Some suggest that topography is one factor that can lead to an increase of forecast skill and therefore predictability. This study examines the uncertainty of a mesoscale model and attempts to characterize the predictability of the wind field. The data collected is from the summer, when the synoptic forcing is relatively benign. Mesoscale Model 5 (MM 5) lagged forecasts are used to create a three-member ensemble over a 12 -hour forecast cycle. The differences in these forecasts are used to determine the spread of the wind field. Results show that some mesoscale features have high uncertainty and others have low uncertainty, shedding light on the potential predictability of these features with a mesoscale model. Results indicate that topography is a large source of uncertainty. This is seen in all data sets, contrary to other studies. The ability of the model to properly forecast the diurnal cycle also impacted substantially on the character and evolution of forecast spread. The persistent mesoscale features were represented reasonably well, however the detailed structure of these features had a <b>fair</b> amount of <b>uncertainty.</b> Lieutenant Junior Grade, United States Nav...|$|R
40|$|Uncertainty {{information}} for global {{leaf area index}} (LAI) products is important for global modeling studies but usually difficult to systematically obtain at a global scale. Here, we present a new method that cross-validates existing global LAI products and produces consistent uncertainty information. The method {{is based on a}} triple collocation error model (TCEM) that assumes errors among LAI products are not correlated. Global monthly absolute and relative uncertainties, in 0. 05 ° spatial resolutions, were generated for MODIS, CYCLOPES, and GLOBCARBON LAI products, with reasonable agreement in terms of spatial patterns and biome types. CYCLOPES shows the lowest absolute and relative uncertainties, followed by GLOBCARBON and MODIS. Grasses, crops, shrubs, and savannas usually have lower uncertainties than forests in association with the relatively larger forest LAI. With their densely vegetated canopies, tropical regions exhibit the highest absolute uncertainties but the lowest relative uncertainties, the latter of which tend to increase with higher latitudes. The estimated uncertainties of CYCLOPES generally meet the quality requirements (± 0. 5) proposed by the Global Climate Observing System (GCOS), whereas for MODIS and GLOBCARBON only non-forest biome types have met the requirement. Nevertheless, none of the products seems to be within a relative uncertainty requirements of 20 %. Further independent validation and comparative studies are expected to provide a <b>fair</b> assessment of <b>uncertainties</b> derived from TCEM. Overall, the proposed TCEM is straightforward and could be automated for the systematic processing of real time remote sensing observations to provide theoretical uncertainty {{information for}} a wider range of land products...|$|R
40|$|This study {{estimated}} energy, {{environmental and}} consumer economic impacts of U. S. Federal residential energy efficiency standards that became {{effective in the}} 1988 - 2006 period, and of energy efficiency standards for fluorescent lamp ballasts and distribution transformers. These standards {{have been the subject}} of in-depth analyses conducted as part of DOE's standards rulemaking process. This study drew on those analyses, but updated certain data and developed a common framework and assumptions for all of the products in order to estimate realized impacts and to update projected impacts. It also performed new analysis for the first (1990) fluorescent ballast standards, which had been introduced in the NAECA legislation without a rulemaking. We estimate that the considered standards will reduce residential/ commercial primary energy consumption and carbon dioxide emissions in 2030 by 4 percent compared to the levels expected without any standards. The reduction for the residential sector is larger, at 8 percent. The estimated cumulative energy savings from the standards amount to 39 quads by 2020, and 63 quads by 2030. The standards will also reduce emissions of carbon dioxide by considerable amounts. The estimated cumulative net present value of consumer benefit amounts to $ 241 billion by 2030, and grows to $ 269 billion by 2045. The overall ratio of consumer benefits to costs (in present value terms) in the 1987 - 2050 period is 2. 7 to 1. Although the estimates made in this study are subject to a <b>fair</b> degree of <b>uncertainty,</b> we believe they provide a reasonable approximation of the national benefits resulting from Federal appliance efficiency standards...|$|R
40|$|Present-day {{practice}} is inadequate for developing justified confidence in software’s impact on system reliability. The inadequacies are especially obvious {{when dealing with}} systems of systems. As will be shown in this paper, there is a <b>fair</b> amount of <b>uncertainty</b> among system engineers about how to determine the impact of software on overall system reliability, and this uncertainty is especially clear when attempting to evaluate the impact of software on system of systems (SoS) reliability. This paper discusses the uncertainty that is evident today, based on presentations given at a reliability, availability, maintainability, and testability (RAM-T) summit for a large system of systems. Clearly, new approaches (or at least, better guides) are needed to deal adequately with software aspects of system and SoS reliability. A few suggestions are provided in this paper (the need for giving software failures consideration when doing system-level FME-CAs, 1 the need for specifying failure definitions and scoring criteria at the SoS level (not just at the constituent system, or platform, level), {{and the need for}} Software Reliability Improvement Programs undertaken during system design), but the main point {{is that it is not}} enough to simply formulate software reliability goals or to collect statistics on detected defects. Some Observations We start with a real life example. At a RAM-T Summit for a major DoD SoS, various contractors responsible for producing different constituents of the system briefed their approach and findings regarding system reliability, availability, and maintainability. 2 As part of the briefing template, each contractor was asked to discuss their approach to software reliability and its contribution to overall system reliability goals. There were a wide variety of statements in response to this requirement...|$|R
40|$|Hundred Talent Program of the Chinese Academy of Sciences Uncertainty {{information}} for global {{leaf area index}} (LAI) products is important for global modeling studies but usually difficult to systematically obtain at a global scale. Here, we present a new method that cross-validates existing global LAI products and produces consistent uncertainty information. The method {{is based on a}} triple collocation error model (TCEM) that assumes errors among LAI products are not correlated. Global monthly absolute and relative uncertainties, in 0. 05 degrees spatial resolutions, were generated for MODIS, CYCLOPES, and GLOBCARBON LAI products, with reasonable agreement in terms of spatial patterns and biome types. CYCLOPES shows the lowest absolute and relative uncertainties, followed by GLOBCARBON and MODIS. Grasses, crops, shrubs, and savannas usually have lower uncertainties than forests in association with the relatively larger forest LAI. With their densely vegetated canopies, tropical regions exhibit the highest absolute uncertainties but the lowest relative uncertainties, the latter of which tend to increase with higher latitudes. The estimated uncertainties of CYCLOPES generally meet the quality requirements (+/- 0. 5) proposed by the Global Climate Observing System (GCOS), whereas for MODIS and GLOBCARBON only non-forest biome types have met the requirement. Nevertheless, none of the products seems to be within a relative uncertainty requirements of 20 %. Further independent validation and comparative studies are expected to provide a <b>fair</b> assessment of <b>uncertainties</b> derived from TCEM. Overall, the proposed TCEM is straightforward and could be automated for the systematic processing of real time remote sensing observations to provide theoretical uncertainty {{information for}} a wider range of land products. (C) 2012 Elsevier Inc. All rights reserve...|$|R
40|$|Polymerase Chain Reaction, or PCR, is an {{invaluable}} tool in molecular biology for amplification {{of a specific}} genetic sequence. Since Karry Mullis and others applied the heat-stable polymerase enzyme of the microorganism Thermus aquaticus to the amplification of short stretches of DNA, {{it is hard to}} imagine doing science in biology without it. PCR is accomplished with a machine that sequentially heats and cools samples of DNA in solution with T. aquaticus polymerase and other components, takes up about the space of a large toaster and costs a few thousand dollars. This heating cycle is repeated approximately thirty times, and takes around two or three hours. Hence, a long lunch is required. The proposed project is a device which performs the operations of a PCR thermocycler at a reduced scale, decreasing the use of consumable reagents and substantially reducing the amount of time required for PCR amplification of target DNA. This will be accomplished with a device about the size of a paperback novel capable of maneuvering, heating and cooling microliter-scale quantities of liquid on a Printed Circuit Board treated to be highly-water repellent. The individual articulation of droplets will be accomplished utilizing a phenomenon of altering the interface between droplets and the hydrophobic surface. The effective automation and volume reduction of a PCR thermocycler of the proposed project opens up the technique for formerly precluded applications, such as rural or third-world medical diagnosis and field usage. In addition, person-hours and total reagents used in a traditional bench-top PCR assay will be reduced. This project entails a <b>fair</b> number of <b>uncertainties</b> that will be experimentally investigated. Project planning and management include stages to determine certain characteristics and capabilities of the device that will influence the final design...|$|R
40|$|The thesis {{aimed to}} explore how various social groups {{perceived}} mild traumatic brain injury (mTBI) in the British military. A literature review on brain provided insights on what constituted mTBI and the key issues arising from mTBI. Social representations theory was the overarching framework of inquiry. The thesis consists of four empirical studies. Three types of data - an official report on mTBI in the British military, {{a sample of the}} British public’s perception of the injury and a former British service personnel’s experience of the injury were subjected to analysis. A pluralistic approach was employed. The first type of data was subjected to two different types of analysis: a preliminary investigation and a deconstruction. In the first study discourse analysis was employed. Authors of the project report constructed mTBI as a phenomenon with a <b>fair</b> amount of <b>uncertainty.</b> Care plans for those with mTBI were constructed as already in place and of superior standards to their American counterpart. In the second study the researcher employed critical discourse analysis to deconstruct how the report’s authors framed the uncertain aspects of the injury in the first study to work up certainty about the controversial aspects of mTBI by discrediting the labelling of mTBI and marginalizing those presenting persistent symptoms. In the third study thematic analysis was employed. The general public constructed mTBI as having varied causes with a spectrum of symptoms and consequences besides being an occupational hazard the government should deal with. In the fourth study interpretative phenomenological analysis was employed. The former serviceman felt divorced from the military post injury. Despite not having his difficulties acknowledged by the military, he coped with the changes brought on by the injury positively framing his experiences. Throughout the empirical studies mTBI was represented as an uncertain phenomenon with many facets...|$|R
40|$|We {{present results}} {{concerning}} the nonperturbative {{evaluation of the}} renormalization constant for the quark field, Z(q), from lattice simulations with twisted-mass quarks and three values of the lattice spacing. We use the regularization-invariant momentum-subtraction (RI'-MOM) scheme. Z(q) has very large lattice spacing artefacts; it is considered here as a test bed to elaborate accurate methods which {{will be used for}} other renormalization constants. We recall and develop the nonperturbative correction methods and propose tools to test the quality of the correction. These tests are also applied to the perturbative correction method. We check that the lattice-spacing artefacts indeed scale as a(2) p(2). We then study the running of Z(q) with particular attention to the nonperturbative effects, presumably dominated by the dimension-two gluon condensate in Landau gauge. We show indeed that this effect is present, and not small. We check its scaling in physical units, confirming that it is a continuum effect. It gives a similar to 4 % contribution at 2 GeV. Different variants are used in order to test the reliability of our result and estimate the systematic uncertainties. Finally, combining all our results and using the known Wilson coefficient of, we find g(2) (mu(2)) (mu 2 CM) = 2. 01 (11) (+ 0. 61 - 0. 73) GeV 2 at mu = 10 GeV, the local operator A(2) being renormalized in the (MS) over bar scheme. This last result is in <b>fair</b> agreement within <b>uncertainties</b> with the value independently extracted from the strong coupling constant. We convert the nonperturbative part of Z(q) from the regularization-invariant momentum-subtraction (RI'-MOM) scheme to (MS) over bar. Our result for the quark field renormalization constant in the (MS) over bar scheme is Z(q) ((MS) over bar pert) ((2 GeV) (2); g(bare) (2)) = 0. 750 (3) (7) - 0. 313 (20) (g(bare) (2) - 1. 5) for the perturbative contribution and Z(q) ((MS) over bar nonperturbative) ((2 GeV) (2); g(bare) (2)) 0. 781 (6) (21) - 0. 313 (20) (g(bare) (2) - 1. 5) when the nonperturbative contribution is included...|$|R
40|$|Information and its derived {{knowledge}} are not static. Instead, information is changing {{over time and}} our understanding of it evolves with our ability and willingness to consume the information. When compared to humans, current computer systems seem very limited {{in their ability to}} really understand the meaning of things. On the other hand, they are very powerful {{when it comes down to}} performing exact computations. One aspect which sets humans apart from machines when trying to understand the world is that we will often make mistakes, forget information, or choose what to focus on. To put this in another perspective, it seems like humans can behave somehow more randomly and still outperform machines in knowledge related tasks. In computer science there is a branch of research concerned with allowing randomness or inaccuracy in algorithms, which are then called approximate algorithms. The main beneﬁt of using these algorithms is that they are often much faster than their exact counterparts, at the cost of producing wrong or inexact results, once in a while. So, these algorithms could be used in contexts where erring once in while does not harm. If the chance of making a mistake is very slim, say lower than the chance of a memory error, then the expected precision will rival their exact counterparts. Furthermore, the input data to the algorithms often already contains a <b>fair</b> amount of <b>uncertainty,</b> such that the small error which the approximate algorithm introduces becomes more or less insigniﬁcant. In this dissertation, the author investigates the use of familiar and new approximate algorithms to knowledge discovery and evolution. The main contributions of the dissertation are a) an abstract formulation of what it means for an ontology to be and stay optimal over time, b) a contribution to a vision paper regarding the future of evolving knowledge ecosystems, c) an investigation of the application of locality-sensitive hashing (LSH) in the context of ontology matching and semantic search, d) the twister tries algorithm which is a novel approximate hierarchical clustering approach with linear space and time constraints, and e) an extension on the twister tries algorithm which trades a longer, but adaptable running time for a likely improvement of the clustering result...|$|R
40|$|This thesis {{examines}} the factors affecting audit and non-audit fees {{and the effects}} of the joint provision of audit and non-audit services on auditing. The first essay focuses on environmental factors. Using data for Swedish listed companies over a six year span, including pre-crisis, crisis and post-crisis periods, the essay investigates whether changing economic conditions affect the level of fees paid for audit and non-audit services. The finding suggests that auditors increase their risk premium for auditing during a financial crisis andtend to charge higher audit fees as a response to lower risk levels in the post-crisis period. On the other hand, a significant reduction in non-audit fees suggests that companies are less willing to invest in consulting services during thecrisis and post-crisis periods. The second essay also studies the effects of environmental factors on audit pricing. Using data for financial institutions in 24 European countries, the study examines whether the level of effort spent on the evaluation of fair values is higher for more uncertain fair values. The result suggests that an increasing level of complexity and risk requires greater audit effort. Furthermore, the results showthat the strength of a country’s institutional setting is positively associated with the effort spent on the evaluation of high <b>uncertainty</b> <b>fair</b> value estimates. The finding implies that auditors spend more effort in stronger regulated countries, possibly due to higher potential litigation costs. The third essay focuses on the factors related to an individual audit partner. Based on the data of publicly listed Swedish companies, it investigates whether partner special competencies are reflected in the prices charged for auditing. The findings show that partner industry expertise and client-specific expertise are associated with higher audit fees. A further finding isthat female partners are considerably under-represented among specialists. However,the under-representation of females among higher qualified partners does not seem to negatively affect their possibilities to earn higher fees. The fourth essay investigates how the joint provision of audit and non-audit services affects perceived knowledge spillover and audit efficiency. The essay makesuse of survey data from a large sample of Swedish auditors and finds that the levels of communication and trust are positively associated with knowledge spillover. The result further suggests that the information gained from the provision of non-audit services can reduce auditors’ effort (time) spent on different audit procedures, thereby increasing audit efficiency...|$|R
40|$|Extract] Rationale and scope: Stream 2 of the Regional NRM Planning for Climate Change Fund {{supports}} the project "Knowledge to manage land and sea: A {{framework for the}} future" run by a consortium of scientists from James Cook University (JCU) and CSIRO. This report is the first major product of the consortium project. It is not an in-depth {{review of the literature}} that already exists for some NRM sectors. Rather, it is syntheses of current knowledge through expert opinion about the threats and potential impacts of climate change in the Wet Tropics Cluster (WTC) region across all sectors. This report focuses on four geographically distinct NRM regions grouped in the WTC: Mackay-Whitsunday, Wet Tropics, Cape York, and the Torres Strait regions, which are managed by Reef Catchments NRM, Terrain NRM, Cape York NRM, and the Torres Strait Regional Authority respectively. The report is framed by the specific topics and issues defined by the NRM groups in the WTC region, reflecting their planning processes and priorities of these groups as well as the characteristics of their regional communities. The focus of this report is on possible impacts and threats, not adaptation options that will be discussed in a future report. Chapter 9 discusses the science of adaptation in a general sense and mitigation opportunities relating to carbon storage are discussed in a section of Chapter 6. The report presents key messages around each topic and issue in bold type in each chapter. Key messages for NRM groups are also summarised {{at the beginning of each}} chapter. These key messages represent our syntheses of expected threats and impacts based on expert opinion but also substantiated by published sources. Each key message is followed by a brief explanation of the underlying scientific support with a small number of key citations to the relevant literature. In most cases there is a <b>fair</b> amount of <b>uncertainty</b> associated with the key messages and they should be understood as best estimates based on expert opinion. Much of the uncertainty about potential impacts is due to the climate model uncertainty about changes in rainfall amount and timing that are critical variables that will influence many sectors in the WTC region. Another source of uncertainty for some topics and issues is limited or lack of direct research on climate change impacts across several of the key NRM sectors in the WTC region. The main conclusions and summaries for each synthesis chapter (by NRM sector) are presented below with a final chapter on adaptation science...|$|R
40|$|Today's railway {{sector is}} {{imposing}} high demands for service quality on railway infrastructure managers. Since railway infrastructure {{has a long}} asset life, it requires efficient maintenance planning to perform effectively throughout its life cycle to meet these high demands. Traditionally maintenance decisions for the railway infrastructure {{have been based on}} past experience and expert estimations. The application of RAMS (Reliability, Availability, Maintainability and Safety) analysis for railway infrastructure is limited. The focus of this thesis is to demonstrate the applicability of RAMS analysis in effective maintenance planning. Within the scope of this research, various case studies associated with Banverket (the Swedish National Rail Administration and ALSTOM Transport have been carried out. The research presents approaches and models for estimating RAMS targets based on the service quality requirements of the railway infrastructure. The availability target of the infrastructure has been estimated by considering the capacity and punctuality requirements of the infrastructure, whereas the safety goal of the track has been estimated by calculating the probability of derailment by means of undetected rail breaks and poor track quality. Effective estimation of the RAMS targets will help infrastructure managers to predict the maintenance investment in the railway infrastructure needed {{over a period of time}} in order to achieve the targets. Nevertheless, the availability target of the infrastructure can lead to train delay. A model has been developed to achieve the availability target in both the scheduled and the condition based maintenance regimes by choosing an effective maintenance interval and detection probability respectively. This has been illustrated by a case study on track circuits. Different maintenance strategies can help in achieving the RAMS targets. In order to determine the cost-effective solution, LCC (life cycle cost) should be used. The maintenance strategy with lowest LCC will be the cost effective maintenance strategy. This has been demonstrated by a case study on a signalling system. Sensitivity analyses have been performed to calculate the maximum cost effectiveness of the system for different maintenance parameters. LCC estimation for a maintenance strategy should always consider the risks associated with the strategy. A <b>fair</b> degree of <b>uncertainty</b> is also associated with LCC estimation due to the statistical characteristics of RAMS parameters. An approach has been developed in this thesis to calculate the uncertainties associated with LCC estimation. Petri-Net analyses, Monte Carlo simulations, Design of Experiment have been used to develop models to achieve the objectives of this thesis. This thesis discusses the applicability of RAMS and LCC analyses for railway infrastructure and demonstrates models for effective infrastructure maintenance planning. Godkänd; 2009; 20091027 (ambpat); DISPUTATION Ämnesområde: Drift och underhållsteknik/Operation and Maintenance Opponent: Professor Ajit Kumar Verma, International Institute of Information Technology Pune, Bombay, Indien Ordförande: Professor Uday Kumar, Luleå tekniska universitet Tid: Måndag den 30 november 2009, kl 10. 00 Plats: F 1031, Luleå tekniska universite...|$|R
6000|$|... "It was {{to talk to}} me," [...] Maria returned; {{on which}} he was further sure that she was {{practically}} in possession of what he himself hadn't yet told her. He was even sure she was in possession of things he himself couldn't have told; for the consciousness of them was now all in her face and accompanied there with a shade of sadness that marked in her the close of all uncertainties. It came out for him more than ever yet {{that she had had}} from the first a knowledge she believed him not to have had, a knowledge the sharp acquisition of which might be destined to make a difference for him. The difference for him might not inconceivably be an arrest of his independence and a change in his attitude--in other words a revulsion in favour of the principles of Woollett. She had really prefigured the possibility of a shock that would send him swinging back to Mrs. Newsome. He hadn't, it was true, week after week, shown signs of receiving it, but the possibility had been none the less in the air. What Maria accordingly had had now to take in was that the shock had descended and that he hadn't, all the same, swung back. He had grown clear, in a flash, on a point long since settled for herself; but no reapproximation to Mrs. Newsome had occurred in consequence. Madame de Vionnet had by her visit held up the torch to these truths, and what now lingered in poor Maria's face was the somewhat smoky light of the scene between them. If the light however wasn't, as we have hinted, the glow of joy, the reasons for this also were perhaps discernible to Strether even through the blur cast over them by his natural modesty. She had held herself for months with a firm hand; she hadn't interfered on any chance--and chances were specious enough--that she might interfere to her profit. She had turned her back on the dream that Mrs. Newsome's rupture, their friend's forfeiture--the engagement the relation itself, broken beyond all mending--might furnish forth her advantage; and, to stay her hand from promoting these things, she had on private, difficult, but rigid, lines, played strictly fair. She couldn't therefore but feel that, though, as the end of all, the facts in question had been stoutly confirmed, her ground for personal, for what might have been called interested, elation remained rather vague. Strether might easily have made out that she had been asking herself, in the hours she had just sat through, if there were still for her, or were only not, a <b>fair</b> shade of <b>uncertainty.</b> Let us hasten to add, however, that what he at first made out on this occasion he also at first kept to himself. He only asked what in particular Madame de Vionnet had come for, and as to this his companion was ready.|$|R
40|$|This {{document}} is a diploma thesis from the Norwegian University of Science and Technology, Department of Machine Design and Materials Technology, and covers {{the following research}} questions: What is the composition of plastics household waste in Trondheim, Norway, how should it be collected out of the alternative curbside containers plastdunk (container for plastics only) and miljødunk (container for plastics, metal and other fractions not suitable for incineration), and how should it be processed, out of the alternatives mechanical and chemical/feedstock recycling? The report includes the results from {{a detailed analysis of}} the plastics content in the plastdunk, as well as the findings in a literature study of mechanical and feedstock recycling. These are the main conclusions: - The calculated plastics amount from the examined area of Heimdal in Trondheim, shows an increase of 48 %. As this is unlikely, there is probably one or more errors in the analysis from this year, or the similar analysis done last year. This adds some uncertainty to some of the conclusions. The calculated total amount of households plastics in Trondheim per year is 3736 tons. This is a very different amount from the previous estimate of 1174 tons (packaging only). As only 25, 4 % of the total plastics ends up in the plastdunk, there is a loss of 74, 5 % of the plastics because of erroneous sorting from consumers. - Plastic shopping and garbage bags amount to 40, 2 % of the plastic film fraction. Of the bottle fraction, plastics from dairy products amount to 21, 7 %, while plastics from packaging of car-maintenance products (motor oil etc.) add up to 33, 6 %. There is 5, 5 % more plastics in absolute amounts in the plastdunk compared to the miljødunk. When implementing a curbside system based on the plastdunk in Trondheim, 41 % more film and 75 % more bottles can be recovered from the wasteflow, compared to the existing system based on the miljødunk. The {{reason for this is that}} it is easier to sort plastics from the plastdunk. Thus, I can conclude that the plastdunk is the better alternative compared to th miljødunk. - A comparison of the environmental properties of the alternatives mechanical and feedstock recycling shows that mechanical recycling has the least environmental load. Comparing by total costs shows that the two alternatives cost roughly the same per ton treated, while feedstock recycling has the better potential for cost savings over time, as technology progresses. The cost-data involve a <b>fair</b> degree of <b>uncertainty.</b> The smallest facilities for feedstock recycling, with a annual capacity of 15 000 tons, are based on the method of BASF thermolysis. This method has a relatively low environmental load. However, the method demands a refinery for its products. Summing up, the report concludes that feedstock recycling is unsuitable for plastics from households in Trondheim...|$|R
40|$|In recent years, {{public-to-private}} transaction (going private transaction) {{has become a}} common practice in Taiwan''s business models. However, what are the problems of this business model? How should the legal system regulate it? In the U. S. A., the development and academic studies of the concept and regulations of {{public-to-private transaction}} are mature. This thesis hopes to help readers to understand the problems and regulation of public-to-private transaction by researching the concept and regulations of public-to-private transaction in the United States. Chapter one includes the motive, the purpose, the range, the methodology of the research and the analysis of related domestic papers {{in order to make}} readers understand where the questions are in the topic. Chapter two describes the concept, motive, and means of the public-to-private transaction in the U. S. A., and based on protecting the interests of minority shareholders, points out the problems of the public-to-private transaction and the related regulations of the U. S., such as information asymmetry and Securities Exchange Act of information disclos ure standards; <b>fair</b> price <b>uncertainty</b> and the appraisal rights; conflicts of interest and the fiduciary duties. Chapter three covers the information disclosure standards of the public-to-private transaction, namely the United States Securities and Exchange Act Rule 13 e- 3 and Schedule 13 e- 13 and the relationship between the foregoing rules and the anti-fraud provisions. Chapter four discusses the relations between the public-to-private transaction and the appraisal rights. It starts with the process of the development and the function of the appraisal rights in the public-to-private transaction; and then discusses the regulations and the operations of the court of the appraisal rights. Chapter five describes the fiduciary duties of corporate insiders, such as controlling shareholders and managements. It includes the meaning and the standards of the judicial review of the public-to-private transaction. The discussion focuses on the formation and the judicial review of the fiduciary obligations of “long-form merger” and “tender offer followed by short-form merger”, that are the two main approaches of the public-to-private transaction. In the last chapter, there are some reviews about the the information disclosure regulations, the appraisal rights, and the fiduciary duty of the public-to-private transaction in the U. S [...] Based on the regulations of the U. S., the thesis points out the problems of the public-to-private transaction that may happen in Taiwan. 近年來下市私有化交易之商業模式已成為我國實務上常見之交易型態，惟該類型交易之問題點何在？法制上應如何加以規範？實有加以研究之必要。由於美國對於下市私有化交易之概念及其規範，其不論其實務發展及學者研究均較為成熟，因此本文希望藉由對美國下市私有化交易之概念與規範進行研究，能有助於國人瞭解下市私有化交易之問題所在及規範重點。 論文第一章介紹本文之研究動機與目的、研究範圍與方法及國內相關論文之文獻分析，俾使讀者明瞭本文之研究議題所在。第二章中主要是說明美國下市私有化交易之概念、進行下市私有化交易之動機及途徑，以及基於少數股東權益保護之立場，指出下市私有化交易之相關弊病及其相對應規範制度，諸如：資訊不對稱/證券交易法資訊揭露規範、公平價格之不確定性/股份收買請求權制度，及利益衝突/信賴義務等，作為後續章節延伸討論之啟承。第三章係探討下市私有化交易中之資訊揭露規範，即美國證券交易法Rule 13 e- 3 、Schedule 13 e- 13 等規範之內容及其與反詐欺條款（anti-fraud provisions）之關係。第四章係探討下市私有化交易與股份收買請求權制度之關連性，首先說明股份收買請求權制度之發展歷程及其於下市私有化交易中所能發揮的功能，再就股份收買請求權制度之規範內容與法院評價運作方式，作一深入討論。第五章係探討發動下市私有化交易之公司內部人（控制股東、管理階層）信賴義務規範，包括其內涵與司法審查標準，探討之重點是下市私有化交易所循之二種主要途徑：制式合併（long-form merger）與公開收購加上簡式合併（tender offer followed by short-form merger）之途徑形成原因與其信賴義務司法審查標準。第六章結論，首先依序就美國下市私有化交易中之資訊揭露規範、股份收買請求權制度及信賴義務規範等，提出本文之研究檢討；其次，再以美國法規範之觀點，指出我國下市私有化交易可能涉及之規範制度問題點。第一章 緒論	 1 第一節 研究動機與目的	 1 第二節 研究範圍與方法	 3 第三節 文獻分析	 4 第四節 研究架構	 5 第二章 下市私有化交易	 9 第一節 下市私有化交易之概念	 9 第二節 下市私有化交易之動機	 11 第一項 公司上市之價值	 11 壹、公司立場	 11 貳、股東立場	 12 第二項 下市私有化之動機	 12 壹、脫離公開市場干擾（Effective Closure of Public Capital Markets） 12 貳、免除上市交易相關費用（Avoidance of Regulatory, Compliance, and Insurance Expenses）	 13 參、公司長期利益最大化（Long-term Value Maximization）	 14 肆、降低代理成本（Reducing Agency Costs）	 15 伍、節稅利益（Tax-related Benefits）	 16 陸、敵意併購防禦措施（Defensive Tactics for Hostile Takeover） 16 柒、獨佔公司未來獲利（Exclusive Company''s Future Benefits）	 17 第三節 下市私有化交易之途徑	 17 第一項 途徑規劃所需考量之法律因素	 17 壹、股東會同意（Stockholder Vote）	 17 貳、股份收買請求權（Appraisal Right）	 18 參、反併購規定（Anti-Takeover Statutes）	 18 肆、信賴義務（Fiduciary Duty）	 19 伍、證券團體訴訟（Securities Class Action）	 20 第二項 交易途徑之類型	 21 壹、制式合併（Long-form Merger）	 21 貳、公開收購加上簡式合併（Tender Offer followed by Short-form Merger）	 22 參、股份合併（Reverse Stock Split）	 22 肆、資產出售與解散（Asset Sale and Dissolution）	 23 第四節 下市私有化交易之弊端與規範	 24 第一項 下市私有化交易之弊端	 25 壹、資訊不對稱	 25 貳、公平價格之不確定性	 26 參、利益衝突	 27 第二項 下市私有化交易之規範	 27 壹、證券交易法之資訊揭露規範	 27 貳、股份收買請求權（Appraisal Rights）	 28 參、信賴義務（Fiduciary Duty）	 29 第三章 聯邦證券交易法之資訊揭露規範	 31 第一節 Rule 13 e- 3 	 31 第一項 Rule 13 e- 3 之制訂緣由	 31 壹、造成目標公司少數股東權益受損	 32 貳、影響證券市場投資信心	 33 參、州法未提供足夠救濟措施	 33 肆、避免利益衝突發生	 34 第二項 Rule 13 e- 3 之規範內容	 34 壹、發行人（Issuer）或關係人（Affiliate）	 35 貳、Rule 13 e- 3 交易（Rule 13 e- 3 Transactions）	 36 參、豁免事由（Exemptions）	 39 第二節 Schedule 13 E- 3 	 41 第一項 Schedule 13 E- 3 之揭露項目	 41 壹、揭露事項	 41 貳、分析說明	 50 第二項 Schedule 13 E- 3 之運作	 51 壹、向美國證券交易委員會（SEC）提出時間	 51 貳、向目標公司股東揭露之特別注意事項	 52 參、美國證券交易委員會（SEC）之審查	 53 第三項 Schedule 13 E- 3 之功用	 54 壹、幫助目標公司少數股東為交易決定	 54 貳、將申報人置於兩難困境（Horns of A Dilemma）	 56 第三節 反詐欺條款（Anti-Fraud Provisions）	 57 第一項 Rule 10 b- 5 	 58 壹、Rule 10 b- 5 之規範內容	 58 貳、Santa Fe Industries, Inc. v. Green案	 59 一、案件事實	 59 二、法院判決	 60 三、小結	 63 第二項 Rule 13 e- 3 (b) 	 63 壹、Rule 13 e- 3 (b) 之規範內容	 63 貳、Howing Company v. Nationwide Corporation案	 65 一、案件事實	 65 二、法院判決	 66 （一）原告是否有默示訴權存在	 66 （二）有無違反Rule 13 e- 3 （項目 7 、 8 、 9 ）資訊揭露義務	 67 （三）是否屬反欺詐條款之詐欺行為	 68 三、小結	 69 第四章 股份收買請求權	 71 第一節 股份收買請求權之概念	 71 第一項 股份收買請求權之源起	 72 第二項 股份收買請求權之功能	 74 壹、傳統理論	 74 一、補償反對股東喪失否決權之功能	 74 二、提供變現力之功能	 75 三、提供合憲性之功能	 75 四、提升多數股東控制權之功能	 76 貳、晚近理論	 76 一、事前觀點	 76 （一）解決協調能力不足問題之功能	 77 （二）設定股票基礎底價之功能	 77 二、事後觀點	 78 （一）保護股份邊際價值之功能	 78 （二）評斷管理階層表現之功能	 78 （三）發現管理階層不法行為之功能	 79 參、小結－下市私有化交易中之監督功能	 80 第二節 德拉瓦州公司法與法院見解	 81 第一項 德拉瓦州公司法之規範	 81 壹、股份收買請求權之適用範圍	 82 一、原因事由	 82 二、例外限制	 82 （一）存續公司法則（Surviving Corporation）	 83 （二）市場例外法則（Market Exception）	 84 貳、股份收買請求權之行使程序	 86 一、公司於合併前之通知義務	 86 二、股東會決議前之書面請求	 86 三、未投票贊成合併決議	 87 四、公司於合併後之通知義務	 87 五、聲請法院裁定之不變期間	 87 六、公司於審前之資訊提供義務	 87 七、法院審查反對股東是否適格	 88 八、法院審前證據開示程序	 88 第二項 德拉瓦州法院對公平價格之認定	 89 壹、評價方法	 91 一、德拉瓦塊狀法（The Delaware Block method）	 91 （一）市場價值（Market Value）	 91 （二）盈餘價值（Earnings Value）	 92 （三）淨資產價值（Net Asset Value）	 93 二、現金流量折現法（Discounted Cash Flow Method）	 93 （一）自由現金流量（Free Cash Flow; FCF）	 94 （二）終值（Terminal Value；TV）	 94 （三）折算率（Discount Rate）	 95 三、小結－評價結果之不確定性	 95 貳、評價考量因素之爭議	 96 一、合併後之綜效利得（Synergistic Gains）	 96 二、少數股權折價（Minority Discount）	 98 三、不公平行為（Inequitable Conduct）	 100 第五章 信賴義務	 103 第一節 信賴義務與審查標準	 103 第一項 董事之信賴義務	 103 壹、注意義務（Duty of Care）	 105 一、建立內控系統之義務（Duty to Monitor）	 106 二、提出質詢調查之義務（Duty to Make Inquiry）	 107 三、基於資訊充足為決策之義務（Duty to Make Informed Business Judgement）	 108 貳、忠誠義務（Duty of Loyalty）	 110 一、利害關係人交易（Conflict of Interest Transaction）	 111 二、董事報酬（Compensation）	 111 三、奪取公司機會（Take Opportunity）	 113 第二項 控制股東之信賴義務	 114 壹、注意義務（Duty of Care）	 116 貳、忠誠義務（Duty of Loyalty）	 117 一、關係人交易（Related Party Transaction）	 118 二、出售控制權（Sales of Control）	 118 三、壓迫行為（Oppresive Measures）	 120 第三項 信賴義務之司法審查標準	 121 壹、經營判斷法則（Business Judgement Rule,BJR）	 122 貳、加強審查標準（Enhanced Scrutiny Standard）	 124 參、完全公平標準（Entire Fairness Standard）	 126 第二節 下市私有化交易之信賴義務審查－以德拉瓦州法院判決為中心	 128 第一項 途徑一：制式合併	 128 壹、制式合併途徑之發展歷程	 128 一、Coyne v. Park & Tilford Distillers Corp. 案與Stauffer v. Standard Brands, Inc. 案－認許簡式合併實行現金逐出合併之合法性	 129 二、David J. Greene & Co. v. Schenley Industries, Inc. 案－認許制式合併實行現金逐出合併之合法性	 130 三、Singer v. Magnavox Co. 案－以雙重測試標準（商業目的與完全公平）審查現金逐出合併之合法性	 131 四、Weinberger v. UOP, Inc. 案－僅以完全公平標準審查現金逐出合併之合法性	 132 貳、制式合併途徑之司法審查標準：完全公平標準	 133 一、完全公平標準之內涵	 134 （一）公平交易（Fair Dealing）	 135 （二）公平價格（Fair Price）	 135 二、舉證責任轉換	 136 （一）少數股東多數同意（approval by a majority of the minority shareholders）	 137 （二）獨立委員會同意（approval by an independent committee of the directors ）	 138 第二項 途徑二：公開收購加上簡式合併	 139 壹、公開收購加上簡式合併路徑之發展歷程	 139 一、In re Siliconix Inc. Shareholders Litigation案－自願性公開收購將不受完全公平標準的審查	 140 二、Glassman v. Unocal Exploration Corp. 案－股份收買請求權是反對簡式合併之少數股東的唯一救濟手段	 141 貳、公開收購加上簡式合併途徑之信賴義務審查標準－經營判斷法則	 143 一、少數股東權益保障相關規範是否足夠？	 143 （一）公開收購中之自願性顯有疑義	 144 （二）簡式合併中股份收買請求權之效用顯有疑義	 145 二、法院與學者之檢討意見	 145 （一）法院判決－In Re Pure Resources, Inc., Shareholders Litigation案	 146 （二）學者意見	 148 第六章 結論	 153 第一節 美國下市私有化交易規範之檢討	 153 第二節 我國下市私有化交易規範之問題點	 155 參考文獻	 15...|$|R
40|$|Brownfields {{redevelopment}} {{has been}} encouraged by governments or {{the real estate}} market because of economic, social and environmental benefits. However, uncertainties in contaminated land redevelopment may cause massive investment risk and need to be managed so that contaminated land redevelopment is facilitated. This study was designed to address hydrogeological as well as economic uncertainty in a hypothetical contaminated land redevelopment project and manage the risk from these uncertainties through the integration of the hydrogeological and economic uncertainties. Hydrogeological uncertainty is derived from incomplete site information, including aquifer heterogeneity, and must be assessed with scientific expertise, given the short history of redevelopment projects and their unique hydrogeological characteristics. Hydrogeological uncertainty has not yet been incorporated in one framework with the economic uncertainty that has been relatively well observed in financial markets. Two cases of Non-Aqueous Phase Liquid (NAPL) contamination were simulated using a physically-based hydrogeological model to address hydrogeological uncertainty: one concerns the effect of an ethanol spill on a light NAPL (LNAPL) contaminated area in the vadose zone, and the other is regarding the vapour phase intrusion of volatile organic compounds, in particular, Trichloroethylene (TCE), a dense NAPL (DNAPL), into indoor air through a variably saturated heterogeneous aquifer. The first simulation replicated experimental observations in the laboratory, such as the capillary fringe depressing and the NAPL pool remobilizing and collecting in a reduced area exhibiting higher saturations than observed prior to an ethanol injection. However, the data gap, in particular, on the chemical properties between the model and the experiment caused the uncertainty in the model simulation. The second NAPL simulation has been performed based on a hypothetical scenario where new dwellings in a redeveloped area have the potential risk of vapour phase intrusion from a subsurface source into indoor air because remediation or foundation design might fail. The simulation results indicated that the aquifer heterogeneity seemed the most significant factor controlling the indoor air exposure risk from a TCE source in the saturated zone. Then, the exposure risk was quantified using Monte Carlo simulations with 50 statistically equivalent heterogeneous aquifer permeability fields. The quantified risk (probability) represents the hydrogeological uncertainty in the scenario and gives the information on loss occurrence intensity of redevelopment failure. Probability of failure (or loss occurrence intensity) was integrated with cost of failure (or loss magnitude) to evaluate the risk capital in the hypothetical brownfields redevelopment project. The term “risk capital” is adopted from financial literature and is the capital you can lose from high risk investment. Cost of failure involves economic uncertainty and can be defined based on a developer’s financial agreement with new dwellers to prevent litigation in the case of certain events, such as an environmental event where indoor air concentrations of pollutants exceed regulatory limits during periodic inspections. The developer makes such a financial agreement with new dwellers because new dwellings have been constructed founded on flawed site information, and municipalities may require it if a land use planning approval is required. An agreement was presumed that the developer would repurchase the affected houses from new dwellers immediately, if indoor air contamination exceeded the regulatory limit. Furthermore, the developer would remediate any remaining contamination, demolish the affected houses and build new houses if they were worth investing in. With this financial plan assumed, the stochastic housing price, stochastic inflation rate and stochastic interest rate have been considered to cause the uncertainty in the cost of failure, and the information on these stochastic variables was obtained from the financial market due to its long history of observations. This research reviewed appropriate risk capital valuation methods for hydrogeologists to apply straightforwardly to their projects, with integrating probability of failure (hydrogeological uncertainty) and cost of failure (economic uncertainty). The risk capital is essentially the probability of failure times the cost of failure with safety loading added to compensate investors against hydrogeological and financial <b>uncertainty.</b> <b>Fair</b> market prices of risk capital have been valuated using financial mathematics and actuarial premium calculations, and each method has a specific safety loading term to reflect investors’ level of risk aversion. Risk capital results indicated that the price of the risk capital was much more sensitive to hydrogeological uncertainty than financial uncertainty. Developers can manage the risk capital by saving a contingency fee for future events or paying an insurance premium, given that the price of this risk capital is the price of a contingent claim, subsequent to failure in remediation or in foundation design, and equivalent to an environmental insurance premium if there is an insurance company to indemnify the liability for the developer. The optimal framework of investment strategy in brownfields redevelopment can be built by linkage of addressing and integrating uncertainties and valuating risk capital from the uncertainties. This framework involves balancing the costs associated with each step while maximizing a net profit from land redevelopment. The optimal investment strategy, such as if or when to remediate or redevelop and to what degree, is given when the future price of the land minus time and material costs as well as the contingency fee or insurance premium maximizes a net profit...|$|R
40|$|Skripsie (MEd) [...] PU vir CHO, 1988 This {{study had}} a dual objective. The first was to {{determine}} which factors influenced the career choice of girls and the second whether• their• actual choices coincided with their aptitudes. Socialization has a direct influence on the identification of girls with their careers. From an early age their behaviour {{is in line with}} the traditional roles (femininity, attractiveness, etc.) which society holds up to them. This is also true about occupations regarded as being suitable for girls. Girls therefore tend to choose their careers from a limited sphere. The style of parents' upbringing of their children also has an influence on the development of a girl's sex role identity, and therefore her career orientation. The peer group, especially male friends, has a tremendous influence during adolescence on the formation of a girl's sex role identity and career orientation. Pressure from the peer group tends to make girls feel that popularity and attractiveness are more important than academic achievement and high professional aspirations. It also appears that teachers do not {{play a significant role in}} the career orientation of girls. The influence of television on the career orientation of girls can be traced back to the influence on the patterns of interest of television viewers. It emerges that television programmes represent sex roles in a stereotyped manner. Through that, patterns of interest in the fields of "Social work" and "Practical-female" (19 -Field Interest Questionnaire) are reinforced. School counsellors do not have a big influence on the career choices of girls. Boys and girls both regard the aspects of interest highly in making a choice of career. Girls tend, however, to give high credence to the rendering of service and to welfare. Boys tend to look more strongly at good income and good opportunities for promotion. The training of girls is directed more at the development of aptitudes in a field in the Humanities, in secretarial and clerking positions. The result of this is that girls mostly follow a domestic, caring and administrative course of study. The career values of girls and their self-image have a negative influence on their career orientation. This leads to the situation that fear of success inhibits them from developing their full professional potential. Other factors, such as manpower shortages, economic and political conditions, the improvement of the level of education and professional knowledge separately or in conjunction, have an influence on girls' career preferences and expectations. For the purposes of this study the Senior Aptitude Test, professional profiles identified with the aid of the Senior Aptitude Test and a questionnaire were used. Three aspects are covered by the questionnaire: * Field of study * Career expectations * Choice of career All the Afrikaans-speaking matriculants of four Afrikaans-medium high schools in a particular metropolitan area participated in the project. The data were statistically processed with an SAS computer programme. Fields of study and aptitudes of the study group do not correlate. Most respondents' strongest aptitudes are in the Humanities, while they are in fact following courses in the field of Economics. Most respondents are of the opinion that there are adequate work opportunities for white girls in the RSA, but feel that career counselling did not make adequate provision for girls. The study group are of the opinion that girls can do traditionally male jobs. There is a <b>fair</b> amount of <b>uncertainty,</b> however, about the professional future of white girls in a changing RSA. Professional training is thus regarded as being very important. More than half of the group (67, 2 %) expert to marry and still to have a career. Most of the respondents (176 out of 277) have made a fin 3 l career choice. The reason why the rest have not made career choices could largely be attributed to too little knowledge about careers. Five careers, viz. secretarial, teaching, nursing, social work and clerking, represent the largest percentage (35, 4 %) of the study group's career choices. The study group mainly made their choices on their own volition, and most parents are satisfied with their daughters' choices of career. Training is essential for most careers, mainly at universities and technikons. Too little knowledge of careers and financial considerations gave rise to the fact that some respondents (71 out of 277) planned to study later. In the consideration of the results of the investigation, the following conclusions could be reached: The process of socialization still holds up the caring role, service and femininity to girls as the ideal. In that way girls' aptitudes in the fields of the Humanities and in medicine are reinforced. In spite of that, the greatest percentage of girls followed a commercial course. The conclusion that is reached is that they feel "safe" in this field of study, because the traditional expectation is that they will go and work as a typist or a secretary. Girls therefore choose a field of study in Standard seven which is not in line with their aptitudes. It emerges that girls do not receive adequate counselling in Standard seven, and that being a secretary or a teacher is still held up to them by die Guidance teacher. The career choices of the study group therefore correlate with their fields of study, but not with their aptitudes. The conclusion that can be reached is that a large percentage of the study group would seem to have made an unrealistic choice of career, probably because they did this from a limited professional awareness. This can probably be ascribed to the traditional values and roles held for girls. Other conclusions which emerge from this study: * The respondents are not properly informed about the changes occurring in the RSA. * The study group dispose of too little information about professional training, although they seem to be aware of its importance. * Girls want to move away from the traditional sex role of wife and mother, and want to reconcile career and marriage. * Most respondents do not dispose of adequate self- knowledge to come to a realistic choice of career without some supportive action. * The female work force in South Africa is very valuable and an important source of executive and highly qualified manpower which should not be neglected. The place which women fill in the career world and its implications should enjoy special attention in school guidance. It is especially necessary that girls be made aware of their career potential but also of career options and problems. A school guidance teacher should take note of the fact that girls' career orientation is different from that of boys, and should adjust the counselling programme accordingly. One cannot, therefore, have the same programme for boys and girls. If it should be accepted that the role of woman in the business world is important, it is essential that career and course guidance in school be improved on an ongoing basis. Such improvement can only take place if the process of career counselling makes provision for the dynamic as well as the scientific dimensions of career choice and career development of girls in particular. The career orientation of girls is a lifelong process. It goes much further than mere career counselling. It is comprehensive, encompassing the girl in her totality. It also points to co-ordinated planning by all persons and institutions involved. It is only in this way that the optimal utilization of available manpower in its full implications can be attained. Master...|$|R

