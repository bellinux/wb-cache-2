14|184|Public
5000|$|A {{graduate}} from the Massachusetts Institute of Technology in 1983, his graduate thesis was “A Local Area <b>Disk</b> <b>Server</b> for RT-11.” (RT-11 {{was one of the}} Digital Equipment Corporation's operating systems). Per the abstract, his thesis focused on “the potential of the fast-growing field of Local Area Networking… explored through the creation of an Ethernet-based <b>disk</b> <b>server</b> for the popular standalone RT-11 operating system.” ...|$|E
5000|$|When first developed, {{nearly all}} LAN storage {{was based on}} the <b>disk</b> <b>server</b> model. This meant that if a client {{computer}} wanted to read a particular block from a particular file it would have to issue the following requests across the relatively slow LAN: ...|$|E
5000|$|A {{version of}} Pixi was {{incorporated}} into the Picasso system from Post Impressions Ltd (later purchased by Snell & Wilcox). This was an HDTV capable <b>disk</b> <b>server</b> designed to handle 10bit RGB signals. Industrial Light & Magic purchased one of these systems for use on Star Wars episode I [...] "The Phantom Menace" [...] and another for episode II [...] "Attack of the Clones".|$|E
40|$|We {{report on}} our {{investigations}} on some technologies {{that can be}} used to build <b>disk</b> <b>servers</b> and networks of <b>disk</b> <b>servers</b> using commodity hardware and software solutions. It focuses on the performance that can be achieved by these systems and gives measured figures for different configurations. It is divided into two parts : iSCSI and other technologies and hardware and software RAID solutions. The first part studies different technologies {{that can be used}} by clients to access <b>disk</b> <b>servers</b> using a gigabit ethernet network. It covers block access technologies (iSCSI, hyperSCSI, ENBD). Experimental figures are given for different numbers of clients and servers. The second part compares a system based on 3 ware hardware RAID controllers, a system using linux software RAID and IDE cards and a system mixing both hardware RAID and software RAID. Performance measurements for reading and writing are given for different RAID levels. Comment: Paper associated to a poster from the 2003 Computing in High Energy and Nuclear Physics (CHEP 03), La Jolla, Ca, USA, March 2003, 4 pages, LaTeX. PSN TUDP 00...|$|R
50|$|A Storage Element (SE) {{provides}} uniform {{access to}} data storage resources. The Storage Element may control simple <b>disk</b> <b>servers,</b> large <b>disk</b> arrays or tape-based Mass Storage Systems (MSS). Most WLCG/EGEE sites provide at least one SE.|$|R
40|$|This {{document}} {{concerns the}} award of two contracts for {{the supply of}} <b>disk</b> <b>servers</b> for physics data processing. The Finance Committee is invited {{to agree to the}} negotiation of contracts for the supply of <b>disk</b> <b>servers</b> for physics data processing with: 1. ELONEX (CH), the lowest bidder complying with the specification, for a total usable disk space of not less than 513 Terabytes, for an amount of 966 382 Swiss francs, not subject to revision; 2. TRANSTEC (CH), the second lowest bidder complying with the specification, for a total usable disk space of not less than 491 Terabytes, for an amount of 954 520 Swiss francs, not subject to revision. 2006 / 96 / 5 /...|$|R
5000|$|The Over 5 is a {{software}} package for transferring files between a C64 and an host machine (which {{can be an}} Amiga, PC or Unix box). It works in two different ways, either using the C64 as a server to the host for accessing floppy disks or using the host as a hard <b>disk</b> <b>server</b> for the C64. Over5 was developed in ????-2002 by Daniel Kahlin. It was later ported to Win32 by Martin Sikström and to Unix by Andreas Anderson.|$|E
5000|$|... 1978's MCM/900 was faster yet, {{included}} 24 kB RAM, and an {{included a}} monitor. The MCM/1000, {{also called the}} MCM Power was a re-packaged /900, and was later re-packaged again as the MCM MicroPower. The real change for the /900 and /1000 was to support the HDS-10 <b>disk</b> <b>server,</b> which included an 8.4 MB 8 inch Shugart hard drive, an 8-inch floppy disk drive, and a 64 k Zilog Z80 to control it. Up to 8 /900's /1000's could be plugged into the HDS-10.|$|E
5000|$|On November 29, 2012, Sony {{announced}} the 4K Ultra HD Video Player—a hard <b>disk</b> <b>server</b> preloaded with ten 4K movies and several 4K video clips that {{they planned to}} include with the Sony XBR-84X900. The preloaded 4K movies are The Amazing Spider-Man, Total Recall (2012), The Karate Kid (2010), Salt, Battle: Los Angeles, The Other Guys, Bad Teacher, That's My Boy, Taxi Driver, and The Bridge on the River Kwai. Additional 4K movies and 4K video clips will be offered for the 4K Ultra HD Video Player in the future [...]|$|E
50|$|The {{defendant}} {{was ordered to}} produce, at its own expense, all responsive email existing on its optical <b>disks,</b> <b>servers,</b> and five backup tapes as selected by the plaintiff. The court would only conduct a cost-shifting analysis after {{the review of the}} contents of the backup tapes.|$|R
40|$|As network latency {{drops below}} disk latency, access {{time to a}} remote disk will begin to {{approach}} local disk access time. The performance of I/O may then be improved by spreading disk pages across several remote <b>disk</b> <b>servers</b> and accessing <b>disk</b> pages in parallel. To research this we have prototyped a data page server called a Page File. This persistent data type provides a set of methods to access disk pages stored on a cluster of remote machines acting as <b>disk</b> <b>servers.</b> The goal {{is to improve the}} throughput of database management system or other I/O intensive application by accessing pages from remote disks and incurring disk latency in parallel. This report describes the conceptual foundation and the methods of access for our prototype. (Also cross-referenced as UMIACS-TR- 93 - 47...|$|R
40|$|We {{describe}} the architecture, implementation, use, and potential {{use of a}} scale, high-performance, distributed-parallel data storage system developed in the ARPA funded MAGIC gigabit testbed. A collection of wide area distributed <b>disk</b> <b>servers</b> operate in parallel to provide logical block level access to large data sets. Operated primarily as a network-based cache, the architecture supports cooperation among independently owned resources to provide fast, large-scale, on-demand storage to support data handling, simulation, and computation...|$|R
5000|$|Altos 586 (despite {{what its}} name might suggest today) used a 10 MHz 8086 {{processor}}, among the fastest for a 1983 microcomputer. An 8089 chip {{aided by a}} Z80 queuing processor supported up to eight terminals. Ran Xenix or MP/M-86. The 586 had 512 KB standard memory and came with six RS-232C serial port and one RS-422, which was intended for networking rather than terminal attachment. The Altos 986 was a variant with 1 MB RAM and four extra serial ports. 3Com developed their new Ethernet card for the 986 model, running Xenix 3.0 and sold as a network <b>disk</b> <b>server</b> for IBM PC, XT computers installed with 3Com Ethernet expansion cards.|$|E
40|$|The CMS {{experiment}} at CERN {{will produce}} {{large amounts of}} data in short time periods. Because the data buffers at the experiment are not large enough, this data needs {{to be transferred to}} other storages. The CMS Tier 0 will be an enormous job processing and storage facility at the CERN site. One part of this Tier 0, called the Tier 0 input buffer, has the task to readout the experiment data buffers and to supply these data to other tasks that need to be carried out with it (such as storing). It has to make sure that no data is lost. This thesis compares different scenarios to work with a set of disk servers in order to accomplish the Tier 0 input buffer tasks. To increase the performance per <b>disk</b> <b>server,</b> write and read actions on the same <b>disk</b> <b>server</b> are separated. To find the optimal moments a <b>disk</b> <b>server</b> should change from accepting and writing items to supplying items to other tasks, the combination of various parameters, such as the usage of a particular queuing discipline (like FIFO, LIFO, LPTF and SPTF) and the state of the <b>disk</b> <b>server</b> has been studied. To make the actual comparisons a simulation of dataflow models of the different scenarios has been used. These simulations have been performed with the Yasper simulation tool. This tool uses Petri Net models as its input. To be more certain that the models represent the real situation, some model parts have been remodelled in a tool called GPSS. This tool is not using Petri Nets as its input model; instead it uses queuing models described in a special GPSS language. The results of the simulations show that the best queuing discipline to be used with the Tier 0 input buffer is the LPTF discipline. In particular in combination with a change moment as soon as a <b>disk</b> <b>server</b> has been readout completely...|$|E
40|$|Though {{there have}} been several recent efforts to develop disk based video servers, these {{approaches}} have all ignored the topic of updates and <b>disk</b> <b>server</b> crashes. In this paper, we present a priority based model for building video servers that handle two classes of events: user events that could include enter, play, pause, rewind, fast-forward, exit, as well as system events such as insert, delete, server-down, server-up that correspond to uploading new movie blocks onto the disk(s), eliminating existing blocks from the disk(s), and/or experiencing a <b>disk</b> <b>server</b> crash. We will present algorithms to handle such events. Our algorithms are provably correct, and computable in polynomial time. Furthermore, we guarantee that under certain reasonable conditions, continuing clients experience jitter free presentations. We further justify the efficiency of our techniques with a prototype implementation and experimental results. (Also cross-referenced as UMIACS-TR- 97 - 47...|$|E
50|$|In {{the context}} of a gender {{discrimination}} and retaliation lawsuit, the plaintiff Laura Zubulake moved to obtain from defendants UBS Warburg LLC, UBS Warburg and UBS AG ('UBS') “all documents concerning any communication by or between UBS employees concerning the Plaintiff.” UBS responded by providing several documents, including e-mail records totaling 100 pages, but did not search its backup tapes or other archives for responsive e-mails. The Plaintiff requested emails from UBS' optical <b>disks,</b> <b>servers</b> and backup tapes.|$|R
50|$|OpenStack Image (Glance) {{provides}} discovery, registration, {{and delivery}} services for <b>disk</b> and <b>server</b> images. Stored {{images can be}} used as a template. It can also be used to store and catalog an unlimited number of backups. The Image Service can store <b>disk</b> and <b>server</b> images in a variety of back-ends, including Swift. The Image Service API provides a standard REST interface for querying information about disk images and lets clients stream the images to new servers.|$|R
40|$|The RS/ 6000 {{performed}} well {{in our test}} environment. The potential exists for the RS/ 6000 {{to act as a}} departmental server for a small number of users, rather than as a high speed archival <b>server.</b> Multiple UniTree <b>Disk</b> <b>Server's</b> utilizing one UniTree <b>Disk</b> <b>Server's</b> utilizing one UniTree Name Server could be developed that would allow for a cost effective archival system. Our performance tests were clearly limited by the network bandwidth. The performance gathered by the LibUnix testing shows that UniTree is capable of exceeding ethernet speeds on an RS/ 6000 Model 550. The performance of FTP might be significantly faster if asked to perform across a higher bandwidth network. The UniTree Name Server also showed signs of being a potential bottleneck. UniTree sites that would require a high ratio of file creations and deletions to reads and writes would run into this bottleneck. It is possible to improve the UniTree Name Server performance by bypassing the UniTree LibUnix Library altogether and communicating directly with the UniTree Name Server and optimizing creations. Although testing was performed in a less than ideal environment, hopefully the performance statistics stated in this paper will give end-users a realistic idea as to what performance they can expect in this type of setup...|$|R
40|$|This {{particular}} {{object was}} used up until 2012 in the Data Centre. It slots {{into one of}} the <b>Disk</b> <b>Server</b> trays. Hard disks were invented in the 1950 s. They started as large disks up to 20 inches in diameter holding just a few megabytes (link is external). They were originally called "fixed disks" or "Winchesters" (a code name used for a popular IBM product). They later became known as "hard disks" to distinguish them from "floppy disks (link is external). " Hard disks have a hard platter that holds the magnetic medium, as opposed to the flexible plastic film found in tapes and floppies...|$|E
40|$|A {{video server}} allows to deliver {{multiple}} video streams to different clients. To provide the required amount of storage and bandwidth, a video server must contain {{a large number}} of disks. Since the retrieval rate of the disk and the consumption rate of the clients differ, the data retrieved from disk need to be temporarily stored in main memory, which is an important cost factor in a video server. Video data must be retrieved from disks {{in such a way that}} neither buffer starvation nor overflow occurs for all concurrent video streams. In this paper we calculate the required buffer for the GSS scheduling algorithm for multiple streams retrieved from multiple <b>disk</b> <b>server</b> nodes. We prove analytically that shared buffer management reduces, in comparison with the dedicated buffer management, the required buffer by up to 50 %. ...|$|E
40|$|In 2013, CERN IT {{evaluated}} then deployed a petabyte-scale Ceph {{cluster to}} support OpenStack use-cases in production. With {{now more than}} a year of smooth operations, we will present our experience and tuning best-practices. Beyond the cloud storage use-cases, we have been exploring Ceph-based services to satisfy the growing storage requirements during and after Run 2. First, we have developed a Ceph back-end for CASTOR, allowing this service to deploy thin <b>disk</b> <b>server</b> nodes which act as gateways to Ceph, this feature marries the strong data archival and cataloging features of CASTOR with the resilient and high performance Ceph subsystem for disk. Second, we have developed RADOSFS, a lightweight storage API which builds a POSIX-like filesystem on top of the Ceph object layer. When combined with Xrootd, RADOSFS can offer a scalable object interface compatible with our HEP data processing applications. Lastly the same object layer is being used to build a scalable and inexpensive NFS service for several user communities...|$|E
40|$|In {{this paper}} we {{describe}} {{the current state of}} the art in equipment, software and methods for transferring large scientific datasets at high speed around the globe. We first present a short introductory history of the use of networking in HEP, some details on the evolution, current status and plans for the Caltech/CERN/DataTAG transAtlantic link, and a description of the topology and capabilities of the research networks between CERN and HEP institutes in the USA. We follow this with some detailed material on the hardware and software environments we have used in collaboration with international partners (including CERN and DataTAG) to break several Internet 2 land speed records over the last couple of years. Finally we describe our recent developments in collaboration with Microsoft, Newisys, AMD, Cisco and other industrial partners, in which we are attempting to transfer HEP data files from <b>disk</b> <b>servers</b> at CERN via a 10 Gbit network path to <b>disk</b> <b>servers</b> at Caltech's Center for Advanced Computing Research (a total distance of over 11, 000 kilometres), at a rate exceeding 1 GByte per second. We describe some solutions being used to overcome networking and hardware performance issues. Whilst such transfers represent the bleeding edge of what is possible today, they are expected to be commonplace at the start of LHC operations in 2007. Index Terms—High performance networking, High speed data transfer, TCP...|$|R
50|$|Usually <b>server</b> hard <b>disks</b> are {{specified}} {{for less}} spinup cycles than desktop hard <b>disks,</b> because <b>servers</b> are only very rarely rebooted. On the other hand, <b>server</b> hard <b>disks</b> have much higher MTBF.|$|R
5000|$|... #Caption: Storage servers housing 24 SAS hard <b>disk</b> drives per <b>server</b> ...|$|R
40|$|The ideal {{storage system}} is {{globally}} accessible, always available, provides unlimited performance and capacity {{for a large}} number of clients, and requires no management, This paper describes the design, implementation, and performance of Petal, a system that attempts to approximate this ideal in practice through a novel combination of features. Petal consists of a collection of networkconnected servers that cooperatively manage a pool of physical disks. To a Petal client, this collection appearsasa highly available block-level storage system that provides large abstract containers called virtuul disks. A virtual disk is globally accessible to all Petal clients on the network. A client can create a virtual disk on demand to tap the entire capacity and performance of the underlying phys-ical resources. Furthermore, additional resources, such as servers and disks, can be automatically incorporated into PetaL We have an initial Petal prototype consisting of four 225 MHz DEC 3000 / 700 workstations running Digitrd Unix and connected by a 155 Mbit/s ATM network. The prototype provides clients with virtual disks that tolerate and recover from <b>disk,</b> <b>server,</b> and network failures. Latency is comparable to a locally attached disk, and throughput scales with the number of servers. The prototype can achieve 1 / 0 rates of up to 3150 requests/seeand bandwidth up to 43. 1 Mbytes/sec. ...|$|E
40|$|Contributions {{were made}} to several projects. Howard Nguyen was {{assisted}} in developing the Space Station RPS (Rack Power Supply). The RPS is a computer controlled power supply that helps test equipment used for experiments before the equipment is installed on Space Station Freedom. Ron Bennett of General Electric Government Services was assisted {{in the design and}} analysis of the Standard Interface Rack Controller hardware and software. An analysis was made of the GPIB (General Purpose Interface Bus), looking for any potential problems while transmitting data across the bus, such as the interaction of the bus controller with a data talker and its listeners. An analysis was made of GPIB bus communications in general, including any negative impact the bus may have on transmitting data back to Earth. A study was made of transmitting digital data back to Earth over a video channel. A report was written about the study and a revised version of the report will be submitted for publication. Work was started on the design of a PC/AT compatible circuit board that will combine digital data with a video signal. Another PC/AT compatible circuit board is being designed to recover the digital data from the video signal. A proposal was submitted to support the continued development of the interface boards after the author returns to Memphis State University in the fall. A study was also made of storing circuit board design software and data on the hard <b>disk</b> <b>server</b> of a LAN (Local Area Network) that connects several IBM style PCs. A report was written that makes several recommendations. A preliminary design review was started of the AIVS (Automatic Interface Verification System). The summer was over before any significant contribution could be made to this project...|$|E
5000|$|Traditional Data Recovery {{following}} a <b>server</b> <b>disk</b> drive crash is impossible ...|$|R
40|$|A {{large portion}} of the power budget in server environments goes into the I/O subsystem- the disk array in particular. Traditional {{approaches}} to disk power management involve completely stopping the disk rotation, which can take a considerable amount of time, making them less useful in cases where idle times between disk requests may not be long enough to outweigh the overheads. This paper presents a new approach called DRPM to modulate disk speed (RPM) dynamically, and gives a practical implementation to exploit this mechanism. Extensive simulations with different workload and hardware parameters show that DRPM can provide significant energy savings without compromising much on performance. This paper also discusses practical issues when implementing DRPM on <b>server</b> <b>disks.</b> Keywords: <b>Server</b> <b>Disks,</b> Power Management. ...|$|R
40|$|We {{present the}} design and {{implementation}} of a VOD server that addresses the issues of storage, retrieval, and scheduling {{of a large number}} of video objects as well as the reliability aspects related to striping (distributing) video objects over several hard disks. The result is a multi-platform, distributed video server built from off-the-shelf components that is able to cope with various kinds of heterogeneity (number of disks, storage volume of disks, number of <b>disk</b> <b>servers,</b> variety of striping techniques, range of redundancy codecs). Further, our video server runs on multiple platforms (SOLARIS, Windows NT). Reliability is guaranteed through the optional use of parityor mirroring-based reliability techniques...|$|R
5000|$|Robb, Drew (2003) <b>Server</b> <b>Disk</b> Management in a Windows Environment Chapter 7 - AUERBACH (...) ...|$|R
50|$|ChronoSync is a macOS {{application}} for periodic backups, bootable drive clones and folder synchronizations. It offers {{the ability to}} target any device or folder visible in Finder such as a Volume, Thumb Drive, NAS, <b>Disk</b> Image, <b>Server,</b> or another Mac.|$|R
40|$|Performance, {{reliability}} and scalability in data access are key {{issues in the}} context of HEP data processing and analysis applications. In this paper we present the results of a large scale performance measurement performed at the INFN-CNAF Tier- 1, employing some storage solutions presently available for HEP computing, namely CASTOR, GPFS, Scalla/Xrootd and dCache. The storage infrastructure was based on Fibre Channel systems organized in a Storage Area Network, providing 260 TB of total disk space, and 24 <b>disk</b> <b>servers</b> connected to the computing farm (280 worker nodes) via Gigabit LAN. We also describe the deployment of a StoRM SRM instance at CNAF, configured to manage a GPFS file system, presenting and discussing its performances...|$|R
40|$|Traditional {{correspondence}} {{system has}} now been replaced by internet, which has now become indispensable in everyone’s life. With {{the advent of the}} internet, majority of people correspond through emails several times in a day. However, as internet has evolved, email is being exploited by spammers so as to disturb the recipients’. The entire internet community pays the price, every time there pops a spam mail. Online privacy of the users is compromised when spam disturbs a network by crashing mail servers and filling up hard <b>disks.</b> <b>Servers</b> classified as spam sites are forfeited from sending mails to the recipients’. This paper gives the broader view of spam, issues challenges and statistical losses occurred on account of spams...|$|R
5000|$|Serial Storage Architecture (SSA) was {{a serial}} {{transport}} protocol used to attach <b>disk</b> drives to <b>server</b> computers.|$|R
40|$|The {{designers}} {{of a large}} scale video-on-demand system face an optimization problem of deciding how to assign movies to multiple <b>disks</b> (<b>servers)</b> such that the request blocking probability is minimized subject to capacity constraints. To solve this problem, {{it is essential to}} develop scalable and accurate analytical means to evaluate the blocking performance of the system for a given file assignment. The performance analysis is made more {{complicated by the fact that}} the request blocking probability depends also on how disks are selected to serve user requests for multicopy movies. In this paper, we analyze several efficient resource selection schemes. Numerical results demonstrate that our analysis is scalable and sufficiently accurate to support the task of file assignment optimization in such a system. © 2008 IEEE...|$|R
5000|$|<b>Disk</b> protection, a <b>server</b> {{feature that}} {{discards}} changes {{made to the}} server during user sessions similar to Windows SteadyState ...|$|R
