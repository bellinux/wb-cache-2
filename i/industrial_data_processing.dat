8|10000|Public
50|$|Another current {{definition}} of <b>industrial</b> <b>data</b> <b>processing</b> {{is that it}} concerns those computer programs whose variables in some way represent physical quantities; for example the temperature and pressure of a tank, {{the position of a}} robot arm, etc.|$|E
50|$|<b>Industrial</b> <b>data</b> <b>processing</b> is {{a branch}} of applied {{computer}} science that covers the area of design and programming of computerized systems which are not computers as such — {{often referred to as}} embedded systems (PLCs, automated systems, intelligent instruments, etc.). The products concerned contain at least one microprocessor or microcontroller, as well as couplers (for I/O).|$|E
50|$|Cloud {{computing}} dynamically allocates {{the needed}} resources, releasing them once a task is finished, requiring users to pay only for needed services, often via a service-level agreement. Cloud computing and cluster computing paradigms {{are becoming increasingly}} important to <b>industrial</b> <b>data</b> <b>processing</b> and scientific applications such as astronomy and physics, which frequently require the availability {{of large numbers of}} computers to carry out experiments.|$|E
50|$|Dietz Computer Systems was a German {{minicomputer}} manufacturer {{with its}} main office in Mülheim an der Ruhr, Germany. The systems {{were used for}} <b>industrial</b> and business <b>data</b> <b>processing,</b> {{as well as for}} technical and scientific purposes. A popular computer-aided design software, Technovision, ran on the systems produced by Dietz.|$|R
30|$|The <b>industrial</b> big <b>data</b> {{pipeline}} {{focuses on}} supporting data-driven analytics applications for predictive and intelligent equipment maintenance. These types of applications were chosen given their alignment {{with the goals}} of smart manufacturing, such as enabling predictive capabilities, promoting machine availability, and optimising energy consumption. The main facets of these applications were identified as <b>industrial</b> <b>data</b> integration, real-time <b>processing,</b> and data-driven analysis.|$|R
40|$|As we {{approach}} {{the beginning of a}} new millennium, planning for social equity remains an important and relevant subject of planning inquiry and practice. Amazing technological advances in the tools for communication, <b>industrial</b> production, <b>data</b> <b>processing</b> and research, combined with the rise of democratic governance processes, have transformed the ways in which society functions. Devastating wars, shifts in political and economic borders, population movements, and environmental disasters have contributed to changes in social and physical landscapes. To;:y we are faced with a process of uneven development and persistent concentrated pockets of poverty within dynamic and growing regions...|$|R
5000|$|In November 1975, R2E signed Warner & Swasey Company as the {{exclusive}} manufacturer and marketer of the Micral {{line in the}} United States and Canada. Warner & Swasey marketed its Micral-based system for <b>industrial</b> <b>data</b> <b>processing</b> applications such as engineering data analysis, accounting and inventory control. [...] R2E and Warner & Swasey displayed the Micral M multiple microcomputer system at the June 1976 National Computer Conference. The Micral M consists of up to eight Micral S microcomputers, {{each with its own}} local memory and sharing the common memory so the local and common memory look like one monolithic memory for each processor. The system has a distributed multiprocessor operating system R2E said was based on sharing common resources and real-time task management.Some time after the July 1976 introduction of the Zilog Z80, came the Z80-based Micral CZ. The 8080-based Micral C, an intelligent CRT terminal designed for word processing and automatic typesetting, was introduced in July 1977. [...] It has two Shugart SA400 minifloppy drives and a panel of system control and sense switches below the minifloppy drives. Business application language (BAL) and FORTRAN are supported. By October, R2E had set up an American subsidiary, R2E of America, in Minneapolis. [...] The Micral V Portable (1978) could run FORTRAN and assembler under the Sysmic operating system, or BAL. [...] The original Sysmic operating system was renamed Prologue in 1978. Interestingly, Prologue was able to perform real-time multitasking, and was a multi-user system. R2E offered CP/M for the Micral C in 1979.|$|E
40|$|System {{level testing}} of <b>industrial</b> <b>data</b> <b>processing</b> {{software}} poses several challenges. Input {{data can be}} very large, even {{in the order of}} gigabytes, and with complex constraints that define when an input is valid. Generating the right input data to stress the system for robustness properties (e. g. to test how faulty data is handled) is hence very complex, tedious and error prone when done manually. Unfortunately, this is the current practice in industry. In previous work, we defined a methodology to model the structure and the constraints of input data by using UML class diagrams and OCL constraints. Tests were automatically derived to cover predefined fault types in a fault model. In this paper, to obtain more effective system level test cases, we developed a novel search-based test generation tool. Experiments on a real-world, large <b>industrial</b> <b>data</b> <b>processing</b> system show that our automated approach can not only achieve better code coverage, but also accomplishes this using significantly smaller test suites...|$|E
40|$|This article {{reports on}} the results of a {{questionnaire}} passed to students in a higher training course in a vocational route, in electrical engineering and <b>industrial</b> <b>data</b> <b>processing.</b> The objective is to evaluate the benefit they got from an instruction in project management techniques, together with the use they made of these techniques, both during a series of practical activities based on a project-based teaching, and during their final placement in companies. ...|$|E
40|$|Abstract. Gaia is {{an ambitious}} space {{astrometry}} mission of ESA with a main objective {{to map the}} sky in astrometry and photometry down to a magnitude 20 {{by the end of}} the next decade. While the mission is built and operated by ESA and an <b>industrial</b> consortium, the <b>data</b> <b>processing</b> is entrusted to a consortium formed by the scientific community, which was formed in 2006 and formally selected by ESA one year later. The satellite will downlink around 100 TB of raw telemetry data over a mission duration of 5 years from which a very complex iterative processing will lead to the final science output: astrometry with a final accuracy of a few tens of microarcseconds, epoch photometry in wide and narrow bands, radial velocity and spectra for the stars brighter than 17 mag. We discuss the general principles and main difficulties of this very large <b>data</b> <b>processing</b> and present the organisation of the European Consortium responsible for its design and implementation...|$|R
40|$|Gaia is {{an ambitious}} space {{astrometry}} mission of ESA with a main objective {{to map the}} sky in astrometry and photometry down to a magnitude 20 {{by the end of}} the next decade. While the mission is built and operated by ESA and an <b>industrial</b> consortium, the <b>data</b> <b>processing</b> is entrusted to a consortium formed by the scientific community, which was formed in 2006 and formally selected by ESA one year later. The satellite will downlink around 100 TB of raw telemetry data over a mission duration of 5 years from which a very complex iterative processing will lead to the final science output: astrometry with a final accuracy of a few tens of microarcseconds, epoch photometry in wide and narrow bands, radial velocity and spectra for the stars brighter than 17 mag. We discuss the general principles and main difficulties of this very large <b>data</b> <b>processing</b> and present the organisation of the European Consortium responsible for its design and implementation. Comment: 7 pages, 2 figures, Proceedings of IAU Symp. 24...|$|R
40|$|Abstract. Identifying {{various types}} of {{aluminum}} wheels was traditionally performed manually, which could result in low efficiency, limited reliability, poor accuracy, and high labor cost. This paper presents the design and implementation of an on-line automatic recognition system for aluminum wheels based on laser trigonometry principle. The system mainly consists of a recognition station, two laser displacement sensors, a ball-screw, an <b>industrial</b> computer with <b>data</b> <b>processing</b> software, and a programmable logic controller (PLC). Robust algorithms, {{as well as an}} ingenious database for storing information of wheels in different styles and size are introduced for identifying the wheels. The feature data of a special wheel, such as the diameter, thickness and number of spokes, can be determining accurately by scanning the wheel using two laser displacement sensors and compare well with those in the database. Results show that the system identifies each wheel correctly in less than 20 seconds. The stability of this system is excellent. Significant cost saving, low error rate, and high efficiency can be achieved...|$|R
40|$|This article {{gives an}} account of the results of a {{questionnaire}} to be filled in by students of electrical engineering and <b>industrial</b> <b>data</b> <b>processing</b> who were in a higher training course in their vocational route. The objective is to evaluate the benefit they got from an instruction in project management techniques, together with the use made of these techniques, both during a series of practical activities based on a projectbased teaching, and during their final placement in companies...|$|E
40|$|International audienceIn France, {{technicians}} graduate after a two-­years {{study at}} Technological University Institutes with a French acronym IUT. After graduating, some students decide {{to apply for}} a job in industry or in service sector, others decide to carry on their studies in a Master or Bachelor Engineering program. The IUT undergraduate studies are divided into four semesters, namely S 1, S 2, S 3, and S 4, each one having multiple modules some of them mandatory and others optional. This organization leads students to choose some modules which fit their personal and professional development plan. The interdisciplinary teaching experience presented in this paper concerns students who wish to continue their studies in the Master or Bachlor of Engineering program. It involves <b>industrial</b> <b>data</b> <b>processing,</b> mathematics, and automatic control...|$|E
40|$|International audienceIn {{the context}} of {{embedded}} systems design, the growing het erogeneity of systems leads to increasingly complex and unreliable tool chains. The Model Driven Engineering (MDE) community has been mak-ing considerable eff orts to abstract tool languages in meta-models, and to o ffer model transformation mechanisms for models exchanges. However, the interoperability problems are recurring and still not consistently addressed. For instance, {{when it comes to}} executable models exchanges, it is very di fficult to ensure the preservation of the models behavior from one tool to another. This is mainly {{due to a lack of}} understanding of the Models of Computation (MoC) and execution semantics behind the models within di erent environments. In this paper, we introduce a methodology and a framework to: make explicit the execution semantics of models (based on the theory of MoC); provide semantics enrichment mechanisms to ensure the preservation of the execution semantics of models between tools. Our case study is an integration between a UML speci cation tool and an <b>industrial</b> Intensive <b>Data</b> Flow <b>processing</b> tool. This contribution helps to highlight execution semantics concerns within tool integration context...|$|R
40|$|<b>Data</b> <b>Processing</b> {{discusses}} the principles, practices, and associated tools in <b>data</b> <b>processing.</b> The book {{is comprised of}} 17 chapters that are organized into three parts. The first part covers the characteristics, systems, and methods of <b>data</b> <b>processing.</b> Part 2 deals with the <b>data</b> <b>processing</b> practice; this part {{discusses the}} data input, output, and storage. The last part discusses topics related to systems and software in <b>data</b> <b>processing,</b> which include checks and controls, computer language and programs, and program elements and structures. The text will be useful to practitioners of computer-re...|$|R
40|$|This paper {{proposes a}} {{conceptual}} matrix model with algorithms for biological <b>data</b> <b>processing.</b> The required elements for constructing a matrix model are discussed. The representative matrix-based methods and algorithms which have potentials in biological <b>data</b> <b>processing</b> are presented / proposed. Some application {{cases of the}} model in biological <b>data</b> <b>processing</b> are studied, which show the applicability of this model in various kinds of biological <b>data</b> <b>processing.</b> This conceptual model established a framework within which biological <b>data</b> <b>processing</b> and mining could be conducted. The model is also heuristic to other applications. <br /...|$|R
30|$|Quality <b>data</b> <b>processing</b> defines {{configuration}} {{and processing}} parameters, which are utilized {{in the evaluation}} process of quality attributes. Examples of configuration parameters include location of binary or configuration files, and environmental execution parameters (e.g. number of CPU cores, size of memory). Processing parameters refer to input data, which should be provided to <b>data</b> <b>processing</b> executables (e.g. Spark streaming). Supported quality <b>data</b> <b>processing</b> defines processing, which can be performed with a specific <b>data</b> <b>processing</b> tool. Especially, it can be specified what kind of quality attributes for a data source can be evaluated with a specific <b>data</b> <b>processing</b> tool.|$|R
40|$|Abstract. UnifiedViews is an Extract-Transform-Load (ETL) frame-work {{that allows}} users – publishers, consumers, or analysts – to define, execute, monitor, debug, schedule, and share RDF <b>data</b> <b>processing</b> tasks. The <b>data</b> <b>processing</b> tasks may use custom plugins created by users. UnifiedViews differs from other ETL {{frameworks}} by natively supporting RDF data and ontologies. The practical demonstration of UnifiedViews {{at the conference}} will (1) clearly demonstrate how UnifiedViews helps RDF/Linked Data users with RDF <b>data</b> <b>processing</b> (2) and show the real instance of UnifiedViews with tens of <b>data</b> <b>processing</b> tasks and DPUs motivated by real <b>data</b> <b>processing</b> use cases. ...|$|R
40|$|Significant {{numbers of}} {{physicians}} are using <b>data</b> <b>processing</b> services {{and a large}} number of firms are offering an increasing variety of services. This paper quantifies user dissatisfaction with office practice <b>data</b> <b>processing</b> systems and analyzes factors affecting dissatisfaction in large group practices. Based on this analysis, a proposal is made for a more structured approach to obtaining <b>data</b> <b>processing</b> services in order to lower the risks and increase satisfaction with <b>data</b> <b>processing...</b>|$|R
40|$|DE 102007026480 A 1 UPAB: 20081222 NOVELTY - The method {{involves}} attaching mobile <b>data</b> <b>processing</b> {{units to}} collection containers contained with products to be commissioned. The <b>data</b> <b>processing</b> units command over micro-controllers, local memory, sensor interfaces and wireless communication devices. The mobile <b>data</b> <b>processing</b> unit is addressed on the <b>data</b> <b>processing</b> {{system in a}} commissioning controlling system to identify the current collection containers. The numbers of products, which can be inferred, are indicated. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for the execution of a method for the commissioning of goods. USE - Method for the commissioning of goods. ADVANTAGE - The method involves attaching mobile <b>data</b> <b>processing</b> units to collection containers contained with products to be commissioned, where the mobile <b>data</b> <b>processing</b> unit is addressed on the <b>data</b> <b>processing</b> system in a commissioning controlling system to identify the current collection containers, and hence ensures to simplify the commissioning work by retrofitting the existing stock option and shelving systems...|$|R
5000|$|Pure {{communications}} and pure <b>data</b> <b>processing</b> {{have very different}} characteristics that led to different policy results. The markets that the technology existed on assisted the FCC make its policy decisions. [...] "The pure <b>data</b> <b>processing</b> market was viewed as an innovative, competitive market with low barriers to entry and little chance of monopolization." [...] The FCC established that no additional regulation or safeguards where required for the pure <b>data</b> <b>processing</b> market. The pure communications market {{on the other hand}} was being managed by an incumbent monopoly. The FCC had four concerns about the incumbent telephone companies which were: [...] "the sale of <b>data</b> <b>processing</b> services by carriers should not hurt the provision of common carrier services, the costs of such <b>data</b> <b>processing</b> services should not be passed on to telephone rate payers, revenues derived from common carrier services should not be used to cross subsidize <b>data</b> <b>processing</b> services, and the furnishing of such <b>data</b> <b>processing</b> services by carriers should not hurt the competitive computer market." ...|$|R
50|$|The IEA <b>Data</b> <b>Processing</b> and Research Center (DPC) is the <b>data</b> <b>processing</b> and {{research}} department of IEA, located in Hamburg, Germany.|$|R
40|$|Moore's law {{was first}} {{postulated}} in 1968, and it loosely {{says that the}} cost of making calculations on a computer falls by 50 % each year. Securities markets are, in essence, a form of <b>data</b> <b>processing.</b> Consequently, Moore’s law has driven important changes in those markets over the past forty years. Faster <b>data</b> <b>processing</b> was essential for major changes in securities trading. Increased turnover of portfolios was a result of faster <b>data</b> <b>processing.</b> Consequently, the criticism of that turnover may be misplaced. The effectiveness of regulatory changes, such as the lowering of brokerage commissions and the reduction in bid ask spreads, depended on reduced <b>data</b> <b>processing</b> costs, that is on Moore's Law. Deregulation of brokerage commissions could not have reduced rates by as much as it did if we had not had decreasing costs of <b>data</b> <b>processing.</b> The reduction in bid ask spreads which followed decimalization of securities quotes depended on improved <b>data</b> <b>processing.</b> Continued reductions in <b>data</b> <b>processing</b> costs will require a new regulatory approach. Regulators should consider the improvements in <b>data</b> <b>processing</b> and <b>data</b> transmission when they establish capital requirements and haircuts. ...|$|R
40|$|<b>Data</b> <b>processing</b> complexity, partitionability, {{locality}} and provenance play {{a crucial}} role in the effectiveness of distributed <b>data</b> <b>processing.</b> Dynamics in <b>data</b> <b>processing</b> necessitates effective modeling which allows the understanding and reasoning of the fluidity of <b>data</b> <b>processing.</b> Through virtualization, resources have become scattered, heterogeneous, and dynamic in performance and networking. In this paper, we propose a new distributed <b>data</b> <b>processing</b> model based on automata where <b>data</b> <b>processing</b> is modeled as state transformations. This approach falls within a category of declarative concurrent paradigms which are fundamentally different than imperative approaches in that communication and function order are not explicitly modeled. This allows an abstraction of concurrency and thus suited for distributed systems. Automata give us a way to formally describe <b>data</b> <b>processing</b> independent from underlying processes while also providing routing information to route data based on its current state in a P 2 P fashion around networks of distributed processing nodes. Through an implementation, named Pumpkin, of the model we capture the automata schema and routing table into a <b>data</b> <b>processing</b> protocol and show how globally distributed resources can be brought together in a collaborative way to form a <b>processing</b> plane where <b>data</b> objects are self-routable on the plane...|$|R
40|$|This paper {{reviews the}} {{historical}} development of <b>data</b> <b>processing,</b> discerning three approximate decades of distinct evolutionary cycles. Each cycle {{is seen as}} forking, two contrasting styles of <b>data</b> <b>processing</b> forming and separating during the cycle. On this basis, a classification of the major general areas of <b>data</b> <b>processing</b> is suggested. For each decade, characteristic aspects are discussed and both the lines of development are described. Finally, some observations regarding the present decade and its requirements are given, and some predictions relating to the next decade and its prerequisites are made. DESCRIPTORS: Classification of <b>data</b> <b>processing.</b> Evolution of <b>data</b> <b>processing.</b> Philosophical implications. Computing milieu. Terminology. CR CATEGORIES: 1. 2, 1. 3, 2. ...|$|R
40|$|A {{system for}} {{assessing}} vestibulo-ocular function includes a motion sensor system adapted to be coupled to a user's head; a <b>data</b> <b>processing</b> system configured {{to communicate with}} the motion sensor system to receive the head-motion signals; a visual display system configured {{to communicate with the}} <b>data</b> <b>processing</b> system to receive image signals from the <b>data</b> <b>processing</b> system; and a gain control device arranged to be operated by the user and to communicate gain adjustment signals to the <b>data</b> <b>processing</b> system...|$|R
5000|$|Mivar-based {{technology}} of <b>data</b> <b>processing</b> {{is a method}} of creating logical inference system or automated algorithm construction from modules, services or procedures {{on the basis of}} active trained mivar network of rules with the linear computational complexity. Mivar-based {{technology of}} <b>data</b> <b>processing</b> is designed for <b>data</b> <b>processing</b> including logical inference, computational procedures and services.|$|R
5000|$|Roger Lee Sisson (June 24, 1926 [...] - [...] January 22, 1992) was {{an early}} <b>data</b> <b>processing</b> pioneer. Sisson worked on Project Whirlwind while a {{graduate}} student at MIT, co-founded the first consulting firm devoted to electronic <b>data</b> <b>processing,</b> and published a number of the earliest books and periodicals on computers and <b>data</b> <b>processing.</b>|$|R
40|$|Fourier {{transform}} spectrometry {{is a type}} {{of novel}} information obtaining technology, which integrated the functions of imaging and spectra, but the data that the instrument acquired is the interference data of the target, which is an intermediate data and couldn&# 39;t be used directly, so <b>data</b> <b>processing</b> must be adopted for the successful application of the interferometric data. In the present paper, <b>data</b> <b>processing</b> techniques are divided into two classes: general-purpose and special-type. First, the advance in universal interferometric <b>data</b> <b>processing</b> technique is introduced, then the special-type interferometric data extracting method and <b>data</b> <b>processing</b> technique is illustrated according to the classification of Fourier transform spectroscopy. Finally, the trends of interferogram <b>data</b> <b>processing</b> technique are discussed...|$|R
50|$|The term <b>Data</b> <b>Processing</b> (DP) {{has also}} been used {{previously}} {{to refer to a}} department within an organization responsible for the operation of <b>data</b> <b>processing</b> applications.|$|R
40|$|A {{field survey}} was {{conducted}} to study the factors {{associated with the use}} of <b>data</b> <b>processing</b> charge-back information in organizations. The aim of this research was to identify the organizational and budgetary characteristics associated with how the output of a chargeback system is used by user-managers to control their <b>data</b> <b>processing</b> costs. It was found that involvement in budget preparation, accountability for meeting the <b>data</b> <b>processing</b> budget, and cost variability of the charges were the- most important factors to consider when designing <b>data</b> <b>processing</b> chargeback systems...|$|R
40|$|Scientific {{workflow}} {{systems have}} become a necessary tool for many applications, enabling the composition and execution of complex analysis. CO(2) flux data observed by eddy covariance technique is large in quantity and the procedure of flux data is complex, scientific workflow technique plays {{a very important role}} in the sharing, reusing and automatic calculation of flux <b>data</b> <b>processing</b> method. In this paper, we discuss the feasibility and validity of applying scientific workflow technique to flux <b>data</b> <b>processing</b> and make a tentative approach to construct a scientific workflow system for CO(2) flux <b>data</b> <b>processing</b> by taking Kepler scientific workflow system as the development platform. CO(2) flux data of Changbai Mountain in 2003 is used to verify the scientific workflow system. The results show that scientific workflow system for CO(2) flux <b>data</b> <b>processing</b> can solve many problems of too much multifarious calculation, inconsistent development platform and complicated procedure in flux <b>data</b> <b>processing.</b> This approach indicates that the scientific workflow system applied to CO(2) flux <b>data</b> <b>processing</b> can provide an automatic calculation platform for flux <b>data</b> <b>processing</b> and prompt the communication and sharing of international flux <b>data</b> <b>processing</b> method, which make it easier for scientists to focus on their research and not computation management...|$|R
40|$|In this {{research}} aims {{to change the}} personnel staffing <b>data</b> <b>processing,</b> which is stillperformed in the conventional / manual, {{with the help of}} computer hardware for fastdata processing, the company makes computerized <b>data</b> <b>processing</b> personnel in orderto accelerate the process of doing <b>data</b> <b>processing</b> personnel and presenting reports -reports / information to the right, quickly and accurately...|$|R
40|$|In {{carrying}} out real work practices and preparing this paper, the authors obtain abroader knowledge about <b>data</b> <b>processing</b> {{system on the}} computerized system and cancompare with the <b>data</b> <b>processing</b> system manually. Given the <b>data</b> <b>processing</b> systemwith a computerized warehouse section can then be expected to facilitate thewarehouse stock control of goods entering derta out systematically and efficiently aspossible...|$|R
40|$|PREFACE Many {{books and}} {{articles}} generally recognize that management personnel other {{than those in the}} <b>data</b> <b>processing</b> installation need to be oriented towards <b>data</b> <b>processing</b> if effective utilization of these costly computers is to evolve. Present emphasis in the U. S. Marine Corps is on training of <b>data</b> <b>processing</b> personnel with limited orienta-tion or training of other managerial personnel in the capabilities and limitations of computers. The author carries an additional military occupational specialty as a <b>Data</b> <b>Processing</b> Officer and has been involved with Marine Corps <b>data</b> <b>processing</b> for a number of years. He found this lack of training of other officers a perplexing problem to him in the conduct of everyday tasks. It soon became evident to him that all officers dealing with him should have adequate training in <b>data</b> <b>processing.</b> This problem has been recognized by some Marine Corps officials, but to date a standard syste...|$|R
