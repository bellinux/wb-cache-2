174|224|Public
25|$|Every {{context-free}} grammar {{can be transformed}} into an equivalent nondeterministic pushdown automaton. The derivation process of the grammar is simulated in a leftmost way. Where the grammar rewrites a nonterminal, the PDA takes the topmost nonterminal from its stack and replaces it by the right-hand part of a grammatical rule (expand). Where the grammar generates a terminal symbol, the PDA reads a symbol from input when it is the topmost symbol on the stack (match). In a sense the stack of the PDA contains the unprocessed data of the grammar, corresponding to a pre-order traversal of a <b>derivation</b> <b>tree.</b>|$|E
5000|$|... #Caption: Example <b>derivation</b> <b>tree</b> from G1 in linear (upper left table) and {{graphical}} (main picture) notation ...|$|E
50|$|The image {{shows the}} {{corresponding}} derivation tree; it is a tree of trees (main picture), whereas a <b>derivation</b> <b>tree</b> in word grammars is a tree of strings (upper left table).|$|E
40|$|This paper {{introduces}} well-ordered <b>derivation</b> <b>trees</b> {{and makes}} use of this concept in a novel axiomatization of the TAG parsing problem as a constraint satisfaction problem. Contrary to prior approaches, our axiomatization focuses on the <b>derivation</b> <b>trees</b> rather than the derived <b>trees.</b> Well-ordered <b>derivation</b> <b>trees</b> are our pri-mary models, whereas the derived trees serve solely to determine word order. ...|$|R
40|$|This paper {{describes}} {{a method of}} semi-automatically acquiring an En-glish HPSG grammar from the Penn Treebank. First, heuristic rules are employed to annotate the treebank with partially-specified <b>derivation</b> <b>trees.</b> Lexical entries are automatically extracted from the annotated corpus by inversely applying schemata to partially-specified <b>derivation</b> <b>trees.</b> ...|$|R
40|$|International audienceThis paper {{gives an}} Abstract Categorial Grammar (ACG) account of (Kallmeyer and Kuhlmann, 2012) 's process of {{transformation}} of the <b>derivation</b> <b>trees</b> of Tree Adjoining Grammar (TAG) into dependency trees. We make explicit how the requirement of keeping a direct interpretation of dependency trees into strings results into lexical ambiguity. Since the ACG framework has already been used to provide a logical semantics from TAG <b>derivation</b> <b>trees,</b> we have a unified picture where <b>derivation</b> <b>trees</b> and dependency trees are related but independent equivalent ways {{to account for the}} same surface [...] meaning relation...|$|R
50|$|Genetic editing {{is named}} by analogy with {{genetics}} manuscripts (individuals) {{are derived from}} other manuscripts (or previous states of the same manuscript) and with the <b>derivation</b> <b>tree</b> being a partial ordered tree.|$|E
5000|$|Two [...] "acceleration" [...] theorems due to Imbert {{permit the}} {{elimination}} of redundant inequalities based solely on syntactic properties of the formula <b>derivation</b> <b>tree,</b> thus curtailing the need to solve linear programs or compute matrix ranks.|$|E
50|$|Practical uses of {{context-free}} languages require also {{to produce}} a <b>derivation</b> <b>tree</b> that exhibits the structure that the grammar associates with the given string. The process of producing this tree is called parsing. Known parsers have a time complexity that is cubic {{in the size of}} the string that is parsed.|$|E
40|$|AbstractTransformations of {{relational}} structures {{by applying}} productions have been studied. Processes of transforming structures are considered and {{a method of}} composing such processes is developed. The processes generalize <b>derivation</b> <b>trees</b> of context-free grammars. The method of composing processes generalizes the operations on <b>derivation</b> <b>trees.</b> Church-Rosser properties of processes are stated...|$|R
40|$|Model theoretic syntax is {{concerned}} with studying the descriptive complexity of grammar formalisms for natural languages by defining their <b>derivation</b> <b>trees</b> in suitable logical formalisms. The central tool for model theoretic syntax has been monadic second-order logic (MSO). Much of the recent {{research in this area}} has been concerned with finding more expressive logics to capture the <b>derivation</b> <b>trees</b> of grammar formalisms that generate non-context-free languages. The motiva-tion behind this search for more expressive logics is to describe formally certain mildly context-sensitive phenomena of natural languages. Several extensions to MSO have been proposed, most of which no longer define the <b>derivation</b> <b>trees</b> of grammar formalisms directly, while others introduce logically odd restrictions. We therefore propose to consider first-order transitive closure logic. In this logic, <b>derivation</b> <b>trees</b> can be defined in a direct way. Our main result is that transi-tive closure logic, even deterministic transitive closure logic, is more expressive in defining classes of tree languages than MSO. (Deterministic) transitive closure logics are capable of defining non-regular tree languages that are of interest to linguistics. ...|$|R
40|$|Semantic role {{labeling}} (SRL) methods typically use features from syntactic parse trees. We {{propose a}} novel method that uses Lexicalized Tree-Adjoining Grammar (LTAG) based features for this task. We convert parse <b>trees</b> into LTAG <b>derivation</b> <b>trees</b> where the semantic roles {{are treated as}} hidden information learned by supervised learning on annotated data derived from PropBank. We extracted various features from the LTAG <b>derivation</b> <b>trees</b> and trained a discriminative decision list model to predict semantic roles. We present our results on the full CoNLL 2005 SRL task...|$|R
50|$|A {{parse tree}} or parsing tree or <b>derivation</b> <b>tree</b> or {{concrete}} syntax tree is an ordered, rooted tree {{that represents the}} syntactic structure of a string according to some context-free grammar. The term parse tree itself is used primarily in computational linguistics; in theoretical syntax, the term syntax tree is more common.|$|E
5000|$|... #Caption: Proof idea: If s is {{sufficiently}} long, its <b>derivation</b> <b>tree</b> w.r.t. a Chomsky normal form grammar must contain some nonterminal N twice on some tree path (upper picture). Repeating n times the derivation part N ⇒...⇒ vNx obtains a derivation for uvnwxny (lower {{left and right}} picture for n=0 and 2, respectively).|$|E
50|$|At {{a higher}} level of {{complexity}} are the context-free grammars (type 2). The derivation of a sentence by such a grammar can be depicted as a <b>derivation</b> <b>tree.</b> Linguists working within generative grammar often view such trees as a primary object of study. According to this view, a sentence is not merely a string of words, but rather a hierarchy with subordinate and superordinate branches connected at nodes.|$|E
40|$|Abstract. String-token Petri net was {{introduced}} by labeling the tokens as strings over an alphabet. Languages in regular and linear families which are two basic classes in the Chomsky hierarchy are generated by these Petri nets. An extension of string-token Petri net called Tree-token Petri net (TTPN) {{was introduced}} by labeling the tokens with trees. It was proved that the set of <b>derivation</b> <b>trees</b> obtained by any regular language is accepted by a TTPN. In this paper, we prove that the set of <b>derivation</b> <b>trees</b> obtained by any linear language is accepted by a TTPN...|$|R
40|$|In {{this paper}} we {{consider}} <b>derivation</b> <b>trees</b> whose edges have attached weights 0 or 1 {{and that are}} generated by weighted context-free grammars with context-free level control language. If the level control language is regular, the generating function that counts these <b>derivation</b> <b>trees</b> according to number of nodes, length of yield and weighted height is rational. The asymptotic behaviour of the average weighted height and {{the average number of}} nodes is of the form g 1 (n) · n + g 2 (n), where g 1 (n) and g 2 (n) are bounded periodic functions...|$|R
40|$|Abstract. A {{context-free}} grammar G is ambiguous {{if there is}} a word that can be generated by G with at least two different <b>derivation</b> <b>trees.</b> Ambiguous grammars are often distinguished by their degree of ambiguity, which is the maximal number of <b>derivation</b> <b>trees</b> for the words generated by them. If there is no such upper bound G is said to be ambiguous of infinite degree. By considering how many <b>derivation</b> <b>trees</b> a word of at most length n may have, we can distinguish {{context-free grammar}}s with infinite degree of ambiguity by the growth-rate of their ambiguity with respect to the length of the words. It is known that each cycle-free context-free grammar G is either exponentially ambiguous or its ambiguity is bounded by a polynomial. Until now there have only been examples of context-free languages with inherent ambiguity 2 Θ(n) and Θ(n d) for each d ∈ N 0. In this paper first examples of (linear) context-free languages with nonconstant sublinear ambiguity are presented. ...|$|R
50|$|Parse trees and/or {{derivation}} {{trees are}} {{encountered in the}} study of phrase structure grammars such as context-free grammars or linear grammars. The leaves of a <b>derivation</b> <b>tree</b> for a formal grammar G are the terminal symbols of that grammar, and the internal nodes the nonterminal or variable symbols. One can read off the corresponding terminal string by performing an ordered tree traversal and recording the terminal symbols in the order they are encountered. The resulting sequence of terminals is a string of the language L(G) generated by the grammar G.|$|E
50|$|Every {{context-free}} grammar {{can be transformed}} into an equivalent nondeterministic pushdown automaton. The derivation process of the grammar is simulated in a leftmost way. Where the grammar rewrites a nonterminal, the PDA takes the topmost nonterminal from its stack and replaces it by the right-hand part of a grammatical rule (expand). Where the grammar generates a terminal symbol, the PDA reads a symbol from input when it is the topmost symbol on the stack (match). In a sense the stack of the PDA contains the unprocessed data of the grammar, corresponding to a pre-order traversal of a <b>derivation</b> <b>tree.</b>|$|E
5000|$|One of {{the chief}} goals of GPSG {{is to show that}} the syntax of natural {{languages}} can be described by context-free grammars (written as ID/LP grammars), with some suitable conventions intended to make writing such grammars easier for syntacticians. Among these conventions are a sophisticated feature structure system and so-called [...] "meta-rules", which are rules generating the productions of a context-free grammar. GPSG further augments syntactic descriptions with semantic annotations {{that can be used to}} compute the compositional meaning of a sentence from its syntactic <b>derivation</b> <b>tree.</b> However, it has been argued (for example by Robert Berwick) that these extensions require parsing algorithms of a higher order of computational complexity than those used for basic CFGs.|$|E
40|$|We {{present an}} {{incremental}} evaluation algorithm for materialized views in relational and deductive database systems. The algorithm computes, in an incremental fashion, {{the changes to}} the materialized view in response to changes (insertions, deletions, and updates) to the base relations. The view may be defined in SQL or in Datalog, and may use UNION, negation, aggregation (e. g. SUM, MIN), linear recursion, and general recursion. The algorithm is optimal in that it computes exactly those view tuples that are inserted or deleted. The algorithm works by tracking the number of <b>derivation</b> <b>trees</b> ([Mum 91, MS 92]) for each tuple in the view. The number of <b>derivation</b> <b>trees</b> for a tuple corresponds to the count of a tuple in the duplicate semantics used in relational systems such as those based on SQL. For deductive databases using set semantics, we show {{that the number of}} <b>derivation</b> <b>trees</b> for nonrecursive queries can be computed at little or no cost above the cost of evaluating the quer [...] ...|$|R
40|$|Inderpal Singh Mumick AT&T Bell Laboratories 600 Mountain Avenue Murray Hill, NJ 07974, USA mumick@research. att. com Oded Shmueli AT&T Bell Laboratories 600 Mountain Avenue Murray Hill, NJ 07974, USA oshmu@cs. Technion. AC. IL Abstract We {{investigate}} {{the problem of}} checking {{whether the number of}} <b>derivation</b> <b>trees</b> of a Datalog program with duplicate semantics is finite or not. We show that given a safe stratified query and an edb, it is possible to check, in polynomial time, whether the query has a finite number of <b>derivation</b> <b>trees.</b> However, it is undecidable to check whether a safe stratified query has a finite number of <b>derivation</b> <b>trees</b> for every possible edb. We identify two classes of queries for which checking finiteness over all edbs is decidable. We also define duplicate semantics for Datalog (and thereby for the newly proposed variants of recursive SQL) queries with stratified negation. Since evaluation involving counting of duplicates may not terminate in recursive SQL, this [...] ...|$|R
40|$|This {{doctoral}} thesis studies theoretical properties of grammars with restricted <b>derivation</b> <b>trees.</b> After presenting {{the state of}} the art concerning this investigation area, the research is focused on the three main kinds of the restrictions placed upon the <b>derivation</b> <b>trees.</b> First, it introduces completely new investigation area represented by cut-based restriction and examines the generative power of the grammars restricted in this way. Second, it investigates several new properties of path-based restriction placed upon the <b>derivation</b> <b>trees.</b> Specifically, it studies the impact of erasing productions on the generative power of grammars with restricted path and introduces two corresponding normal forms. Then, it describes a new relation between grammars with restricted path and some pseudoknots. Next, it presents a counterargument to the generative power of grammars with controlled path that has been considered as well-known so far. Finally, it introduces a generalization of path-based restriction to not just one but several paths. The model generalized in this way is studied, namely its pumping, closure, and parsing properties...|$|R
50|$|ALF was {{designed}} to be genuine integration of both programming paradigms, and thus any functional expression {{can be used in a}} goal literal and arbitrary predicates can occur in conditions of equations. ALF's operational semantics is based on the resolution rule to solve literals and narrowing to evaluate functional expressions. In order to reduce the number of possible narrowing steps, a leftmost-innermost basic narrowing strategy is used which, it is claimed, can be efficiently implemented. Terms are simplified by rewriting before a narrowing step is applied and equations are rejected if the two sides have different constructors at the top. Rewriting and rejection are supposed to result in a large reduction of the search tree and produce an operational semantics that is more efficient than Prolog's resolution strategy. Similarly to Prolog, ALF uses a backtracking strategy corresponding to a depth-first search in the <b>derivation</b> <b>tree.</b>|$|E
40|$|This work {{provides}} a TAG account of gapping in English, {{based on a}} novel deletion-like op-eration that {{is referred to as}} de-anchoring. De-anchoring applies onto elementary trees, but it is licensed by the <b>derivation</b> <b>tree</b> in two ways. Firstly, de-anchored trees must be linked to the root of the <b>derivation</b> <b>tree</b> by a chain of adjunc-tions, and the sub-graph of de-anchored nodes in a <b>derivation</b> <b>tree</b> must satisfy certain inter-nal constraints. Secondly, de-anchoring must be licensed by the presence of a homomorphic antecedent <b>derivation</b> <b>tree.</b> ...|$|E
40|$|The goal of {{this thesis}} is to design and {{implement}} the parser of grammars, whose <b>derivation</b> <b>tree</b> is limited by inspection of levels. Common parsing procedures {{are studied in detail}} and then it is discussed, how they could be extended by inspection of <b>derivation</b> <b>tree.</b> The {{most important part of the}} thesis is a draft of continuous inspection of the <b>derivation</b> <b>tree</b> simultaneously with its construction, which allows close cooperation between these two processes. This approach enables significant increasing of the parser power...|$|E
40|$|International audienceFeature-based regular tree grammars (FRTG) {{can be used}} to {{generate}} the <b>derivation</b> <b>trees</b> of a feature-based tree adjoining grammar (FTAG). We make use of this fact to specify and implement both an FTAG-based sentence realiser and a benchmark generator for this realiser. We argue furthermore that the FRTG encoding enables us to improve on other proposals based on a grammar of TAG <b>derivation</b> <b>trees</b> in several ways. It preserves the compositional semantics that can be encoded in feature-based TAGs; it increases efﬁciency and restricts overgeneration; and it provides a uniform resource for generation, benchmark construction, and parsing...|$|R
40|$|Several {{authors have}} pointed out that the {{correspondence}} between LTAG <b>derivation</b> <b>trees</b> and dependency structures is not as direct as it may seem at first glance, and various proposals have been made to over-come this divergence. In this paper we pro-pose to view the correspondence between <b>derivation</b> <b>trees</b> and dependency structures as a tree transformation during which the direction of some of the original edges is re-versed. We show that, under this transform-ation, LTAG is able to induce both ill-nes-ted dependency trees and dependency trees with gap-degree greater than 1, which is not possible under the direct reading of deriva-tion trees as dependency trees. ...|$|R
40|$|We present {{three ways}} of {{inducing}} probability distributions on <b>derivation</b> <b>trees</b> produced by Minimalist Grammars, and give their maximum likelihood estimators. We {{argue that a}} parameterization based on locally normalized log-linear models balances competing requirements for modeling expressiveness and computational tractability. ...|$|R
40|$|Most of the {{proposals}} for semantics in the Tree Adjoining Grammar (TAG) framework suppose that the <b>derivation</b> <b>tree</b> serves as basis for semantics. However, {{in some cases the}} <b>derivation</b> <b>tree</b> does not provide the semantic links one needs. This paper concentrates on one of these cases, namely the analysis of quantifiers as adjuncts. The paper proposes to enrich the TAG <b>derivation</b> <b>tree</b> and use the resulting structure as basis for semantics. This allows to deal with quantifiers, even in PPs embedded into NPs, such that an adequate semantics with appropriate scope orders is obtained. The enriched derivation structure allows also to treat other cases that are problematic for the assumption that a TAG semantics can be based on the <b>derivation</b> <b>tree...</b>|$|E
40|$|Most of the {{proposals}} for semantics in the Tree Adjoining Grammar (TAG) framework suppose that the <b>derivation</b> <b>tree</b> serves as basis for semantics. However, sometimes the <b>derivation</b> <b>tree</b> does not provide the semantic links one needs. In this paper, some of these cases are inspected, in particular the analysis of quanti ers and of unbounded wh-movement in embedded interrogatives. The paper proposes to enrich the TAG <b>derivation</b> <b>tree</b> and use the resulting structure as basis for semantics. This allows {{to deal with the}} problematic cases...|$|E
40|$|A {{notion of}} <b>derivation</b> <b>tree</b> is {{introduced}} for ground term rewriting systems and new proofs are given for some old results. 1 Introduction A ground term rewriting {{system is a}} term rewriting system of which the rules do not contain variables. We will show that a natural concept of <b>derivation</b> <b>tree</b> can be defined for these rewriting systems, {{in such a way}} that a tree t 1 can be (iteratively) rewritten to a tree t 2 iff there is a <b>derivation</b> <b>tree</b> of which the "yield" is the pair (t 1; t 2), with an appropriate definition of `yield'. Derivations that differ only in the order of independent rule applications, correspond to the same <b>derivation</b> <b>tree.</b> Moreover, the set of derivation trees forms a regular tree language. Thus, the situation is analogous to (and, in fact, generalizes) the situation for contextfree grammars. Using this concept of <b>derivation</b> <b>tree,</b> and the well-known closure properties of the regular tree languages, we give a new proof for (a slight extension of) the main result [...] ...|$|E
40|$|This paper {{develops}} {{a version of}} Natural Logic – an inference system that works directly on natural language syntactic representations, with no intermediate translation to logical formulae. Following work by Sánchez, we develop a small fragment that computes semantic order relations between <b>derivation</b> <b>trees</b> in Categorial Grammar. The proposed system has the following new characteristics: (i) It uses orderings between <b>derivation</b> <b>trees</b> as purely syntactic units, derivable by a formal calculus. (ii) The system is extended for conjunctive phenomena like coordination and relative clauses. This allows a simple account of non-monotonic expressions that are reducible to conjunctions of monotonic ones. (iii) A decision procedure for provability is developed for a fragment of Natural Logic...|$|R
40|$|A {{context-free}} grammar G is ambiguous if {{and only if}} there is a word that can be generated by G with at least two different <b>derivation</b> <b>trees.</b> Ambiguous grammars are often distinguished by their degree of ambiguity, which is the maximal number of <b>derivation</b> <b>trees</b> for the words generated by them. If there is no such upper bound G is said to be ambiguous of infinite degree. Here as a new tool for examining the ambiguity of cycle-free {{context-free grammar}}s the ambiguity function is introduced. This function maps the natural number n to the maximal number of <b>derivation</b> <b>trees</b> which a word of length at most n may have. This provides the possibility to distinguish infinitely ambiguous context-free grammars by the growth-rate of their ambiguity functions. We present a necessary and sufficient, but in general undecidable, criterion for exponential ambiguity. In fact violation of this criterion leads to a polynomial upper bound for the ambiguity, which can be effectively constructed from the grammar. Hence for cycle-free context-free grammars the ambiguity function is either an element of 2 ^&Theta;(n) or of O(n^d) for some d &isin; N 0 which can be effectively constructed from G...|$|R
40|$|AbstractThe surface tree {{languages}} {{obtained by}} top-down finite state transformation of monadic trees {{are exactly the}} frontier-preserving homomorphic images of sets of <b>derivation</b> <b>trees</b> of ETOL systems. The corresponding class of tree transformation languages is therefore equal to the class of ETOL languages...|$|R
